{"id": "1610.04213", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Oct-2016", "title": "Reset-free Trial-and-Error Learning for Robot Damage Recovery", "abstract": "The high probability of hardware failures prevents many advanced robots (e.g. legged robots) to be confidently deployed in real-world situations (e.g post-disaster rescue). Instead of attempting to diagnose the failure(s), robots could adapt by trial-and-error in order to be able to complete their tasks. However, the best trial-and-error algorithms for robotics are all episodic: between each trial, the robot needs to be put back in the same state, that is, the robot is not learning autonomously. In this paper, we introduce a novel learning algorithm called \"Reset-free Trial-and-Error\" (RTE) that allows robots to recover from damage while completing their tasks. We evaluate it on a hexapod robot that is damaged in several ways (e.g. a missing leg, a shortened leg, etc.) and whose objective is to reach a sequence of targets in an arena. Our experiments show that the robot can recover most of its locomotion abilities in a few minutes, in an environment with obstacles, and without any human intervention. Overall, this new algorithm makes it possible to contemplate sending robots to places that are truly too dangerous for humans and in which robots cannot be rescued.", "histories": [["v1", "Thu, 13 Oct 2016 19:39:58 GMT  (2119kb,D)", "http://arxiv.org/abs/1610.04213v1", "8 pages, 6 figures, 3 pseudocodes/algorithms"], ["v2", "Wed, 12 Apr 2017 23:08:17 GMT  (8279kb,D)", "http://arxiv.org/abs/1610.04213v2", "18 pages, 16 figures, 3 tables, 6 pseudocodes/algorithms"]], "COMMENTS": "8 pages, 6 figures, 3 pseudocodes/algorithms", "reviews": [], "SUBJECTS": "cs.RO cs.AI", "authors": ["konstantinos chatzilygeroudis", "vassilis vassiliades", "jean-baptiste mouret"], "accepted": false, "id": "1610.04213"}, "pdf": {"name": "1610.04213.pdf", "metadata": {"source": "CRF", "title": "Reset-free Trial-and-Error Learning for Data-Efficient Robot Damage Recovery", "authors": ["Konstantinos Chatzilygeroudis", "Vassilis Vassiliades", "Jean-Baptiste Mouret"], "emails": ["jean-baptiste.mouret@inria.fr", "konstantinos.chatzilygeroudis@inria.fr,", "vassilis.vassiliades@inria.fr,", "jean-baptiste.mouret@inria.fr"], "sections": [{"heading": null, "text": "The question is to what extent the robots are actually robots that are used in environments that are too risky for humans. [1] Even the best robots will encounter unforeseen situations: hardware failures will always be a possibility, especially with highly complex robots in environments that are too risky for humans. [2] For example: C. Atkeson et al. reports that in DARPA robotics, the Atlas robot used a \"mean time between failures of hours or days.\""}, {"heading": "II. STATE-OF-THE-ART", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. Learning in Robotics", "text": "While there is a consensus that robots should be able to learn new tasks and improve their behavior over time, there is much less agreement about the best place to insert learning into a robot architecture, and many approaches rely on supervised learning algorithms that learn forward or reverse models. Once learned, such models can be combined with control or planning algorithms to accomplish the task. A critical aspect of model learning is data collection: supervised learning algorithms need labeled data, but marginal babbling is often insufficient to generate behaviors that are of interest to robots [16], [17]; for example, with a robotic manipulator, random movements are unlikely to generate touch-like behaviors. Active learning can help alleviate this problem by improving behaviors that improve the model \"in the right place.\" Instead of learning a model, robots can use an RL algorithm to figure out how to behave."}, {"heading": "B. Fault Tolerance and Recovery in Robotics", "text": "The classic approaches to fault tolerance are based on updating the robot model, either directly with self-diagnosis [25] or indirectly with machine learning [26], [5]; the model is then used to plan and / or control it. For example, if a hexapod robot detects that one of its legs is not functioning as expected, it can adjust the use and adapt the controller to use only the legs [27]. However, due to the many cognitive ambiguities in a robot, these approaches require many sensors and / or strong hypotheses about the type of possible error. An alternative approach is to let a damaged robot learn a new policy through trial-and-error that circumvents the need for diagnosis [7], [9], [8], [10]. In this way of working, the biggest challenge is to design algorithms that are as data-efficient as possible, because too many attempts both damage the robot and make a too slow prediction, as well as applying a reversible behavior in multiple situations."}, {"heading": "III. PROBLEM FORMULATION", "text": "Our problem can be placed within the general framework of the Markov decision-making processes (MDP) [18]. An MDP is a tuple (S, A, T, r), where S is the state space (continuous or discrete), A is the room for action (continuous or discrete), T (st, at, st + 1) is the state transitional function, which indicates the likelihood of transition to the state st + 1% S if the actor in the state takes action st + S, and r: S \u2192 R the immediate reward function (defining the role of the actor), where r (st + 1) the immediate reward of the state st + 1 and st + 1 can contain both internal variables (such as the body position) and external variables (such as obstacles). The goal of the actor (i.e. the robot) is to find a specific goal (st) to find a deterministic policy procedure, i.e. a mapping of states to maximize the problem (the action)."}, {"heading": "IV. APPROACH", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. Learning the Action Repertoire", "text": "We assume that it is a \"random\" action. () We assume that there is a \"random\" variation. (...) We assume that we have a \"random\" variation. (...) We assume that we have a \"random\" variation. (...) We assume that we have a \"random\" variation. (...) We assume that we have a \"random\" variation. (...) We assume that we have a \"random\" variation. (...) We assume that we have a random variation. (...) We assume that we have a random variation. (...) We assume that we have a random variation. (...) We assume that we have a random variation. (...) We assume that we have a random variation. (...) We assume that we have a random variation."}, {"heading": "B. Learning with Gaussian Processes", "text": "MAP-Elites provides not only the set of actions to be used by politics, but also a previous procedure for how an action changes the state variables, i.e. an assignment of actions to relative results, f: A \u2192 O. These actions are well organized in the task area, so it is possible to define distances between them. Since this procedure comes from a simulator, it is only an approximation. More important is that the simulator uses a model of the intact robot while the real model is corrupted. In other words, in order to make the real robot powerful, there must be a way to correct this assignment. To do this, we use n Gaussian processes (GP) (where n is the number of dimensions of O) with an intermediate function corresponding to the previous one of MAP-Elites. In other words, the mapping compressed with the simulator (2f) serves as a procedure for the GPs.GP distribution, which is an extension of the Gaussian function, which is an end-dimensional distribution by GP."}, {"heading": "C. Probabilistic Optimal Planning using MCTS", "text": "We are interested in solving an MDP with a package of measures containing thousands of measures. Since GPs are probabilistic models, they provide both a prediction and uncertainty about any prediction that can be exploited by probabilistic planners. \u2022 Here, we are using Monte Carlo Tree Search (MCTS) [32], as it has already been successfully used to solve (partially observable) -MDPs with stochastic transition functions [33], [34], continuous state spaces and high branching factors [34]. MCTS is a high-ranking, sample-based search algorithm to find optimal decisions in a given area by taking random samples in the decision room and building a search tree according to the results. Each state in the search tree is evaluated based on the average result of Monte Carlo Kind rollouts from that state. These rollouts are typically random or guided by a simple, domain-dependent heuristic search."}, {"heading": "D. Reset-free Trial-and-Error Learning Algorithm", "text": "When connecting all parts together, RTE first generates an action repertoire using the MAP elite algorithm (algo. 3, lines 2-3); then, during the mission, it plans each step using MCTS and the current belief in the result of the actions (prediction of the GPs) taking into account the uncertainty of the predictions and potential end states (e.g. collisions with obstacles) (lines 9-13); at the end of each step, the GPs are updated with the recorded data (lines 14-15). Algorithm 3 Reset-free Trial-and-Error Learning 1: Procedure RTE 2: Create Action Repertoire, A, using MAP elite (sec. IV-A) 3: Construct mean function M from MAP elite data 4: for i = 1 \u2192 dim (O) do 5: GPi: A \u2192 Oi with Mi as prior (sec. IV-B, V-C) for 6 ecute function from MAP-Elites data 4: Est: Est (Est: 1: Est: Est: 1: Est: Est: 1)."}, {"heading": "V. EXPERIMENTAL SETUP", "text": "To test the performance of our algorithm, we use a hexapod robot, which has to reach random equidistant targets in an environment one after the other (Fig. 1). We carry out experiments both in simulation (to collect extensive statistics) and on a physical robot."}, {"heading": "A. Parametric Low-level Controller", "text": "The low-level controller is the same as in [8], [29]. It is deliberately kept simple so that this work can concentrate on the learning algorithm. In this open-loop controller, the position of the first two joints of each of the 6 legs is determined by a periodic function with 3 parameters: a phase shift, an offset and an amplitude (the frequency is fixed).The position of the third joint of each leg is the opposite of the position of the second leg, so that the last segment is always vertical. This results in 36 newly evaluated parameters. At each step of the learning algorithm, the low-level controller is executed for 3 seconds with the specified parameters."}, {"heading": "B. Learning the Action Repertoire", "text": "The task of the robot is to reach points in the Cartesian space (x, y), therefore the MAP elites should produce a repertoire of actions, each reaching a different point in the Cartesian space. As many controllers can reach the same position, we select those, which make the robot follow a continuous curve and for which the body points to the tangent of the total curve at the end of a 3-second period. Therefore, the performance function of the MAP elites for the individual is: pi = | \u03b1i \u2212 \u03b1d | (7), where \u03b1i is the yaw angle of the robot and \u03b1d is the desired yaw angle of the robot at the end of the movement. To describe the circular paths, we just have to maintain the (x, y) position of the robot at the end of the movement (since we can calculate the desired angle for any point in the 2-D space)."}, {"heading": "C. Learning with Gaussian Processes", "text": "The GP inputs are the 2-D descriptors of the actions, and the outputs are predictions of relative x, y and \u03b1 shifts. To avoid angle discontinuities, we learn cos\u03b1 and sin\u03b1 instead of the raw angle \u03b1. Thus, we learn a mapping of actions to relative results: a \u2192 (\u2206 x, \u2206 y, cos \u0445 \u03b1, sin \u2206 \u03b1) (9) We use the quadratic exponential core as covariance function [37]. We use the Limbo C + + 11 library2 for GP regression."}, {"heading": "D. Solving the problem with MCTS", "text": "We start with MCTS for our problem by making the following decisions: 1https: / / dartsim.github.io 2https: / / github.com / resibots / limboa) Selection Policy: We choose the selection policy of the company Rolet et et al. [38], which properly handles cases where there is a large number of actions or the action space is continuous.b) Selection Policy: We choose the selection policy of the company Coue \ufffd toux et al. [39], which properly handles cases where the state space is continuous.c) Selection Policy: We use A * for a simplified problem to guide the selection process (see section V-E) d) Generative Model: We construct the generative model using the prediction of the GPs (with their variance)."}, {"heading": "E. Guiding MCTS using A* on a simplified problem", "text": "To speed this up, we first discredit the space and create a grid map; then we simulate a virtual point robot with 8 actions (one of which is for3https: / / github.com / resibots / mctseach neighboring cell - allows diagonal movements) and solve the problem of orbit planning with A *. Solving this simplified task is very accountable. We only keep the first step of the optimized path and use it to know an approximate direction for the next MCTS action. Next, we take N (in our case 100) random actions from the repertoire and return the one that best fits that direction. Note that we use GPS prediction to decide which action to choose. This simple procedure reduces the runtime of MCTS to near real time (a few seconds to select the next action) and still get high quality results."}, {"heading": "VI. RESULTS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. Simulation results", "text": "We count the number of steps (3s actions) required to achieve 30 equidistant (distances of 3.5 m or 3 m for each environment) random targets, and we count the number of collisions with obstacles (including the walls of the arena). We compare RTE (Sec. IV) with MCTS-based planning (with the same action repertoire as RTE). This second treatment corresponds to a classical planning algorithm with a re-planning after each step. We examine 3 different types of damage (leg shortening and removal), 2 different environments and 2 different action repertoires (Fig. 4). Each scenario is replicated 50 times for the statistics. Results show that RTE takes fewer steps to achieve each target and leads to fewer collisions than the re-planing algorithm (Fig. 4): Compared to MCTS alone, RTE requires between 1.18 (in the simplest damages, Fig. 4A) and 2.04 (most severe damage, Fig. 4C) to reach the 430 and 4C steps (less damage)."}, {"heading": "B. Physical robot results", "text": "Subsequently, we evaluated RTE on the physical robot with a single damage (we removed the right metatarsal), in two environments (with and without central obstacle) and a single action repertoire; the number of targets to be reached was 10 and 5 targets for each environment, respectively, and the distance between targets was 2 \u221a 2. Each scenario is replicated 5 times. The environment (location of obstacles) and the position of the robot are recorded with a motion capture system (Optitrack), although a SLAM and / or a visual odometry algorithm could be used [8], [42]. Results show that RTE takes fewer steps to reach each target (environment 1: 13.0 steps, 25th and 75th percentiles [12.0, 14.0], environment 2: 18.0 steps, [14.0, 19.0]) than MCTS alone (environment 1: 28.0 steps, [24.0, 31.0], environment 2: 25.0 steps, 43.0] (Fig. 6)."}, {"heading": "VII. DISCUSSION AND CONCLUSION", "text": "With robots, as with many complex systems, \"we should not ask ourselves if any mishap can happen, but rather ask what we will do about it if it happens.\" [43] This advice is especially important if we want to be able to send advanced and expensive robots to risky places like a destroyed nuclear power plant. [4] The RTE learning algorithm enables robots to overcome such mistakes within minutes. We have successfully tested it on a hexapodic robot that has been damaged in multiple ways. Unlike most previous work, our algorithm does not require putting the robot back into the same position after each attempt: the robot learns autonomously while we consider its surroundings (obstacles)."}, {"heading": "ACKNOWLEDGMENTS", "text": "This work was funded by the European Research Council (ERC) within the framework of the European Union's Horizon 2020 research and innovation programme (funding agreement no. 637972, \"ResiBots\" project). The authors thank Dorian Goepp, Antoine Cully and Olivier Buffet."}], "references": [{"title": "No falls, no resets: Reliable humanoid behavior in the DARPA robotics challenge", "author": ["C. Atkeson"], "venue": "Proc. of IEEE Humanoid, 2015.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2015}, {"title": "How UGVs physically fail in the field", "author": ["J. Carlson", "R.R. Murphy"], "venue": "IEEE Trans. on Robotics, vol. 21, no. 3, pp. 423\u2013437, 2005.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2005}, {"title": "Fukushima robot operator writes tell-all blog", "author": ["E. Guizzo"], "venue": "IEEE Spectrum, August, vol. 23, 2011.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2011}, {"title": "Real-time fault diagnosis", "author": ["V. Verma", "G. Gordon", "R. Simmons", "S. Thrun"], "venue": "Robotics & Automation Magazine, IEEE, vol. 11, no. 2, pp. 56\u201366, 2004.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2004}, {"title": "Generation of whole-body optimal dynamic multi-contact motions", "author": ["S. Lengagne", "J. Vaillant", "E. Yoshida", "A. Kheddar"], "venue": "Int. Journal of Robotics Research, vol. 32, no. 9-10, pp. 1104\u20131119, 2013.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2013}, {"title": "Fault-tolerant gait learning and morphology optimization of a polymorphic walking robot", "author": ["D.J. Christensen", "J.C. Larsen", "K. Stoy"], "venue": "Evolving Systems, pp. 1\u201312, 2013.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2013}, {"title": "Robots that can adapt like animals", "author": ["A. Cully", "J. Clune", "D. Tarapore", "J.-B. Mouret"], "venue": "Nature, vol. 521, no. 7553, pp. 503\u2013507, 2015.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "Fast Damage Recovery in Robotics with the T-Resilience Algorithm", "author": ["S. Koos", "A. Cully", "J.-B. Mouret"], "venue": "Int. Journal of Robotics Research, vol. 32:14, pp. 1700\u20131723, 2013.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2013}, {"title": "Multiple chaotic central pattern generators with learning for legged locomotion and malfunction compensation", "author": ["G. Ren", "W. Chen", "S. Dasgupta", "C. Kolodziejski", "F. W\u00f6rg\u00f6tter", "P. Manoonpong"], "venue": "Information Sciences, vol. 294, pp. 666\u2013682, 2015.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "Reinforcement learning in robotics: A survey", "author": ["J. Kober", "J.A. Bagnell", "J. Peters"], "venue": "Int. Journal of Robotics Research, vol. 32, no. 11, pp. 1238\u20131274, 2013.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2013}, {"title": "Gaussian processes for data-efficient learning in robotics and control", "author": ["M.P. Deisenroth", "D. Fox", "C.E. Rasmussen"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell., vol. 37, no. 2, pp. 408\u2013423, 2015.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2015}, {"title": "Efficient reinforcement learning for robots using informative simulated priors", "author": ["M. Cutler", "J.P. How"], "venue": "Proc. of ICRA, 2015.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "Combining modelbased policy search with online model learning for control of physical humanoids", "author": ["I. Mordatch", "N. Mishra", "C. Eppner", "P. Abbeel"], "venue": "Proc. of ICRA, 2016.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2016}, {"title": "Illuminating search spaces by mapping elites", "author": ["J.-B. Mouret", "J. Clune"], "venue": "arXiv preprint arXiv:1504.04909, 2015.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2015}, {"title": "Model learning for robot control: a survey", "author": ["D. Nguyen-Tuong", "J. Peters"], "venue": "Cognitive Processing, vol. 12, no. 4, pp. 319\u2013340, 2011.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2011}, {"title": "Active learning of inverse models with intrinsically motivated goal exploration in robots", "author": ["A. Baranes", "P.-Y. Oudeyer"], "venue": "Robotics and Autonomous Systems, vol. 61, no. 1, pp. 49\u201373, 2013.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2013}, {"title": "Reinforcement learning: An introduction", "author": ["R.S. Sutton", "A.G. Barto"], "venue": "MIT press,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1998}, {"title": "Robot skill learning: From reinforcement learning to evolution strategies", "author": ["F. Stulp", "O. Sigaud"], "venue": "Paladyn, Journal of Behavioral Robotics, vol. 4, no. 1, pp. 49\u201361, 2013.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2013}, {"title": "Reinforcement learning of motor skills with policy gradients", "author": ["J. Peters", "S. Schaal"], "venue": "Neural Networks, vol. 21, no. 4, pp. 682\u2013697, 2008.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2008}, {"title": "Encouraging behavioral diversity in evolutionary robotics: an empirical study", "author": ["J.-B. Mouret", "S. Doncieux"], "venue": "Evolutionary Computation, vol. 20, no. 1, pp. 91\u2013133, 2012.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2012}, {"title": "An experimental comparison of Bayesian optimization for bipedal locomotion", "author": ["R. Calandra", "A. Seyfarth", "J. Peters", "M.P. Deisenroth"], "venue": "Proc. of ICRA. IEEE, 2014.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2014}, {"title": "Automatic gait optimization with gaussian process regression.", "author": ["D.J. Lizotte", "T. Wang", "M.H. Bowling", "D. Schuurmans"], "venue": "in Proc. of IJCAI,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2007}, {"title": "Using trajectory data to improve bayesian optimization for reinforcement learning", "author": ["A. Wilson", "A. Fern", "P. Tadepalli"], "venue": "The Journal of Machine Learning Research, vol. 15, no. 1, pp. 253\u2013282, 2014.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2014}, {"title": "Diagnosis and fault-tolerant control", "author": ["M. Blanke", "J. Schr\u00f6der"], "venue": null, "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2003}, {"title": "Resilient machines through continuous self-modeling", "author": ["J.C. Bongard", "V. Zykov", "H. Lipson"], "venue": "Science, vol. 314, no. 5802, pp. 1118\u2013 1121, 2006.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2006}, {"title": "Alternative gaits for multiped robots with leg failures to retain maneuverability", "author": ["K. Mostafa", "C. Tsai", "I. Her"], "venue": "International Journal of Advanced Robotic Systems, vol. 7, no. 4, p. 31, 2010.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2010}, {"title": "Taking the human out of the loop: A review of bayesian optimization", "author": ["B. Shahriari", "K. Swersky", "Z. Wang", "R.P. Adams", "N. de Freitas"], "venue": "Proceedings of the IEEE, vol. 104, no. 1, pp. 148\u2013175, 2016.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2016}, {"title": "Evolving a behavioral repertoire for a walking robot", "author": ["A. Cully", "J.-B. Mouret"], "venue": "Evolutionary Computation, 2015.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2015}, {"title": "EvoRBC: evolutionary repertoire-based control for robots with arbitrary locomotion complexity", "author": ["M. Duarte", "J. Gomes", "S.M. Oliveira", "A.L. Christensen"], "venue": "Proc. of GECCO, 2016.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2016}, {"title": "Deep neural networks are easily fooled: High confidence predictions for unrecognizable images", "author": ["A. Nguyen", "J. Yosinski", "J. Clune"], "venue": "Proc. of CVPR. IEEE, 2015, pp. 427\u2013436.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2015}, {"title": "Monte-carlo tree search: A new framework for game ai.", "author": ["G. Chaslot", "S. Bakkes", "I. Szita", "P. Spronck"], "venue": "AIIDE,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2008}, {"title": "Monte-carlo planning in large POMDPs", "author": ["D. Silver", "J. Veness"], "venue": "Proc. of NIPS, 2010, pp. 2164\u20132172.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2010}, {"title": "A survey of monte carlo tree search methods", "author": ["C.B. Browne"], "venue": "IEEE Trans. on Computational Intelligence and AI in Games, vol. 4, no. 1, pp. 1\u201343, 2012.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2012}, {"title": "Monte carlo tree search for continuous and stochastic sequential decision making problems", "author": ["A. Cou\u00ebtoux"], "venue": "Ph.D. dissertation, Universit\u00e9 Paris Sud-Paris XI, 2013.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2013}, {"title": "Sferes v2: Evolvin\u2019in the multi-core world", "author": ["J.-B. Mouret", "S. Doncieux"], "venue": "Proc. of IEEE CEC, 2010.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2010}, {"title": "Gaussian processes for machine learning", "author": ["C.E. Rasmussen", "C.K.I. Williams"], "venue": null, "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2006}, {"title": "Optimal active learning through billiards and upper confidence trees in continous domains", "author": ["P. Rolet", "M. Sebag", "O. Teytaud"], "venue": "Proc. of ECML, 2009, pp. 302\u2013317.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2009}, {"title": "Continuous upper confidence trees", "author": ["A. Cou\u00ebtoux", "J.-B. Hoock", "N. Sokolovska", "O. Teytaud", "N. Bonnard"], "venue": "Proc. of LION, 2011.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2011}, {"title": "On the parallelization of UCT", "author": ["T. Cazenave", "N. Jouandeau"], "venue": "Proc. of the Computer Games Workshop, 2007, pp. 93\u2013101.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2007}, {"title": "Continuous rapid action value estimates", "author": ["A. Couetoux", "M. Milone", "M. Brendel", "H. Doghmen", "M. Sebag", "O. Teytaud"], "venue": "Proc. of the 3rd Asian Conference on Machine Learning (ACML2011), 2011.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2011}, {"title": "Simultaneous localization and mapping: part I", "author": ["H. Durrant-Whyte", "T. Bailey"], "venue": "IEEE robotics & automation magazine, vol. 13, no. 2, pp. 99\u2013110, 2006.", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2006}, {"title": "On Building Systems That Will Fail", "author": ["F. Corbato"], "venue": "ACM Turing award lectures, vol. 34, no. 9, pp. 72\u201381, 2007.", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2007}, {"title": "A unifying view of sparse approximate Gaussian process regression", "author": ["J. Qui\u00f1onero-Candela", "C.E. Rasmussen"], "venue": "Journal of Machine Learning Research, vol. 6, no. Dec, pp. 1939\u20131959, 2005.", "citeRegEx": "44", "shortCiteRegEx": null, "year": 1939}], "referenceMentions": [{"referenceID": 0, "context": "During the recent DARPA Robotics Challenge (2015), many robots had to be \u201crescued\u201d by humans because of hardware failures [1], [2]; this is paradoxical for robots that were designed to operate in environments that are too risky for humans! While these robots could certainly have been more robust and could have prevented some falls [1], even the best robots will encounter unforeseen situations: hardware failures will always be a possibility, especially with highly complex robots in post-disaster environments [3], [4].", "startOffset": 122, "endOffset": 125}, {"referenceID": 0, "context": "During the recent DARPA Robotics Challenge (2015), many robots had to be \u201crescued\u201d by humans because of hardware failures [1], [2]; this is paradoxical for robots that were designed to operate in environments that are too risky for humans! While these robots could certainly have been more robust and could have prevented some falls [1], even the best robots will encounter unforeseen situations: hardware failures will always be a possibility, especially with highly complex robots in post-disaster environments [3], [4].", "startOffset": 333, "endOffset": 336}, {"referenceID": 1, "context": "During the recent DARPA Robotics Challenge (2015), many robots had to be \u201crescued\u201d by humans because of hardware failures [1], [2]; this is paradoxical for robots that were designed to operate in environments that are too risky for humans! While these robots could certainly have been more robust and could have prevented some falls [1], even the best robots will encounter unforeseen situations: hardware failures will always be a possibility, especially with highly complex robots in post-disaster environments [3], [4].", "startOffset": 513, "endOffset": 516}, {"referenceID": 2, "context": "During the recent DARPA Robotics Challenge (2015), many robots had to be \u201crescued\u201d by humans because of hardware failures [1], [2]; this is paradoxical for robots that were designed to operate in environments that are too risky for humans! While these robots could certainly have been more robust and could have prevented some falls [1], even the best robots will encounter unforeseen situations: hardware failures will always be a possibility, especially with highly complex robots in post-disaster environments [3], [4].", "startOffset": 518, "endOffset": 521}, {"referenceID": 0, "context": "report that the Atlas robot they used in the DARPA Robotics challenge had a \u201cmean time between failures of hours or, at most, days\u201d [1].", "startOffset": 132, "endOffset": 135}, {"referenceID": 3, "context": "The traditional method for damage recovery is to first diagnose the failure [5], then update the plans to bypass it [6].", "startOffset": 76, "endOffset": 79}, {"referenceID": 4, "context": "The traditional method for damage recovery is to first diagnose the failure [5], then update the plans to bypass it [6].", "startOffset": 116, "endOffset": 119}, {"referenceID": 5, "context": "To overcome these challenges, robots can avoid the diagnosis step and directly learn a compensatory behavior by trial and error [7], [8], [9], [10].", "startOffset": 128, "endOffset": 131}, {"referenceID": 6, "context": "To overcome these challenges, robots can avoid the diagnosis step and directly learn a compensatory behavior by trial and error [7], [8], [9], [10].", "startOffset": 133, "endOffset": 136}, {"referenceID": 7, "context": "To overcome these challenges, robots can avoid the diagnosis step and directly learn a compensatory behavior by trial and error [7], [8], [9], [10].", "startOffset": 138, "endOffset": 141}, {"referenceID": 8, "context": "To overcome these challenges, robots can avoid the diagnosis step and directly learn a compensatory behavior by trial and error [7], [8], [9], [10].", "startOffset": 143, "endOffset": 147}, {"referenceID": 9, "context": "In that case, damage recovery is a reinforcement learning (RL) problem in which a damaged robot has to maximize its performance for the task at hand [11].", "startOffset": 149, "endOffset": 153}, {"referenceID": 10, "context": "State-of-the-art RL algorithms, however, are either too constrained to be easily used with arbitrary robots and arbitrary tasks [12], or they require too many trials that prohibit realistic damage recovery [11].", "startOffset": 128, "endOffset": 132}, {"referenceID": 9, "context": "State-of-the-art RL algorithms, however, are either too constrained to be easily used with arbitrary robots and arbitrary tasks [12], or they require too many trials that prohibit realistic damage recovery [11].", "startOffset": 206, "endOffset": 210}, {"referenceID": 7, "context": "Fortunately, damage recovery is not a standard RL problem: standard algorithms assume that the robot starts with no knowledge, whereas a trial-and-error damage recovery algorithm can rely on the knowledge of the intact robot [9], [13], [14], [8].", "startOffset": 225, "endOffset": 228}, {"referenceID": 11, "context": "Fortunately, damage recovery is not a standard RL problem: standard algorithms assume that the robot starts with no knowledge, whereas a trial-and-error damage recovery algorithm can rely on the knowledge of the intact robot [9], [13], [14], [8].", "startOffset": 230, "endOffset": 234}, {"referenceID": 12, "context": "Fortunately, damage recovery is not a standard RL problem: standard algorithms assume that the robot starts with no knowledge, whereas a trial-and-error damage recovery algorithm can rely on the knowledge of the intact robot [9], [13], [14], [8].", "startOffset": 236, "endOffset": 240}, {"referenceID": 6, "context": "Fortunately, damage recovery is not a standard RL problem: standard algorithms assume that the robot starts with no knowledge, whereas a trial-and-error damage recovery algorithm can rely on the knowledge of the intact robot [9], [13], [14], [8].", "startOffset": 242, "endOffset": 245}, {"referenceID": 6, "context": "Error algorithm (IT&E) allowed, for instance, a hexapod robot to learn to walk after several injuries with a dozen of episodes [8].", "startOffset": 127, "endOffset": 130}, {"referenceID": 9, "context": "The main limitation of IT&E and other state-of-the-art algorithms for robot learning is their reliance on episodes [11]: after each trial, the robot needs to be reset to the same state.", "startOffset": 115, "endOffset": 119}, {"referenceID": 13, "context": "2): (1) a pre-computed action repertoire of the intact robot that also creates a mapping between the task space and the parameters of the low-level controller (generated by MAP-Elites [15] in our case, Fig.", "startOffset": 184, "endOffset": 188}, {"referenceID": 14, "context": "A critical aspect of model learning is data acquisition: supervised learning algorithms need labeled data, but random babbling is often insufficient to generate behaviors that are interesting for robots [16], [17]; for instance, with a robotic manipulator, random movements are unlikely to generate grasp-like behaviors.", "startOffset": 203, "endOffset": 207}, {"referenceID": 15, "context": "A critical aspect of model learning is data acquisition: supervised learning algorithms need labeled data, but random babbling is often insufficient to generate behaviors that are interesting for robots [16], [17]; for instance, with a robotic manipulator, random movements are unlikely to generate grasp-like behaviors.", "startOffset": 209, "endOffset": 213}, {"referenceID": 15, "context": "Active learning can help alleviating this issue by exploring behaviors that improve the model \u201cat the right place\u201d [17].", "startOffset": 115, "endOffset": 119}, {"referenceID": 9, "context": "Instead of learning a model, robots can use a RL algorithm to discover how to behave [11].", "startOffset": 85, "endOffset": 89}, {"referenceID": 16, "context": "Classic RL approaches are, however, designed for discrete state and action spaces [18], [11], whereas robots almost always have to solve continuous tasks, for example balancing by controlling the joint torques.", "startOffset": 82, "endOffset": 86}, {"referenceID": 9, "context": "Classic RL approaches are, however, designed for discrete state and action spaces [18], [11], whereas robots almost always have to solve continuous tasks, for example balancing by controlling the joint torques.", "startOffset": 88, "endOffset": 92}, {"referenceID": 17, "context": "A popular alternative is to view RL as an optimization of the parameters of a policy [19], which can be solved with gradient-based methods [20], [11], evolutionary algorithms [21] or other optimization methods like Bayesian Optimization [8], [22], [23].", "startOffset": 85, "endOffset": 89}, {"referenceID": 18, "context": "A popular alternative is to view RL as an optimization of the parameters of a policy [19], which can be solved with gradient-based methods [20], [11], evolutionary algorithms [21] or other optimization methods like Bayesian Optimization [8], [22], [23].", "startOffset": 139, "endOffset": 143}, {"referenceID": 9, "context": "A popular alternative is to view RL as an optimization of the parameters of a policy [19], which can be solved with gradient-based methods [20], [11], evolutionary algorithms [21] or other optimization methods like Bayesian Optimization [8], [22], [23].", "startOffset": 145, "endOffset": 149}, {"referenceID": 19, "context": "A popular alternative is to view RL as an optimization of the parameters of a policy [19], which can be solved with gradient-based methods [20], [11], evolutionary algorithms [21] or other optimization methods like Bayesian Optimization [8], [22], [23].", "startOffset": 175, "endOffset": 179}, {"referenceID": 6, "context": "A popular alternative is to view RL as an optimization of the parameters of a policy [19], which can be solved with gradient-based methods [20], [11], evolutionary algorithms [21] or other optimization methods like Bayesian Optimization [8], [22], [23].", "startOffset": 237, "endOffset": 240}, {"referenceID": 20, "context": "A popular alternative is to view RL as an optimization of the parameters of a policy [19], which can be solved with gradient-based methods [20], [11], evolutionary algorithms [21] or other optimization methods like Bayesian Optimization [8], [22], [23].", "startOffset": 242, "endOffset": 246}, {"referenceID": 21, "context": "A popular alternative is to view RL as an optimization of the parameters of a policy [19], which can be solved with gradient-based methods [20], [11], evolutionary algorithms [21] or other optimization methods like Bayesian Optimization [8], [22], [23].", "startOffset": 248, "endOffset": 252}, {"referenceID": 20, "context": "about 100 trials to learn the 4 parameters that control a minimalistic biped robot [22]), and most of them are episodic, meaning that the robot has to be put back in the same state after each trial.", "startOffset": 83, "endOffset": 87}, {"referenceID": 10, "context": "In particular, PILCO is a RL algorithm that is based on learning a probabilistic dynamics model, which is then used to optimize a policy [12].", "startOffset": 137, "endOffset": 141}, {"referenceID": 22, "context": "Nonetheless, PILCO has a very high computational cost (after a dozen of episodes, PILCO typically needs a few hours to select the next trial, see [24]), works only for lowdimensional state spaces (10-20 dimensions), and strongly constrains the type of policy [22] (for instance, it is unclear if PILCO can be used to learn a walking controller like in [22], [23], [8]).", "startOffset": 146, "endOffset": 150}, {"referenceID": 20, "context": "Nonetheless, PILCO has a very high computational cost (after a dozen of episodes, PILCO typically needs a few hours to select the next trial, see [24]), works only for lowdimensional state spaces (10-20 dimensions), and strongly constrains the type of policy [22] (for instance, it is unclear if PILCO can be used to learn a walking controller like in [22], [23], [8]).", "startOffset": 259, "endOffset": 263}, {"referenceID": 20, "context": "Nonetheless, PILCO has a very high computational cost (after a dozen of episodes, PILCO typically needs a few hours to select the next trial, see [24]), works only for lowdimensional state spaces (10-20 dimensions), and strongly constrains the type of policy [22] (for instance, it is unclear if PILCO can be used to learn a walking controller like in [22], [23], [8]).", "startOffset": 352, "endOffset": 356}, {"referenceID": 21, "context": "Nonetheless, PILCO has a very high computational cost (after a dozen of episodes, PILCO typically needs a few hours to select the next trial, see [24]), works only for lowdimensional state spaces (10-20 dimensions), and strongly constrains the type of policy [22] (for instance, it is unclear if PILCO can be used to learn a walking controller like in [22], [23], [8]).", "startOffset": 358, "endOffset": 362}, {"referenceID": 6, "context": "Nonetheless, PILCO has a very high computational cost (after a dozen of episodes, PILCO typically needs a few hours to select the next trial, see [24]), works only for lowdimensional state spaces (10-20 dimensions), and strongly constrains the type of policy [22] (for instance, it is unclear if PILCO can be used to learn a walking controller like in [22], [23], [8]).", "startOffset": 364, "endOffset": 367}, {"referenceID": 23, "context": "Classic approaches to fault tolerance rely on updating the model of the robot, either directly with a self-diagnosis [25], or indirectly with machine learning [26], [5]; the model is then used for planning and/or control.", "startOffset": 117, "endOffset": 121}, {"referenceID": 24, "context": "Classic approaches to fault tolerance rely on updating the model of the robot, either directly with a self-diagnosis [25], or indirectly with machine learning [26], [5]; the model is then used for planning and/or control.", "startOffset": 159, "endOffset": 163}, {"referenceID": 3, "context": "Classic approaches to fault tolerance rely on updating the model of the robot, either directly with a self-diagnosis [25], or indirectly with machine learning [26], [5]; the model is then used for planning and/or control.", "startOffset": 165, "endOffset": 168}, {"referenceID": 25, "context": "as expected, it can stop using it and adapt the controller to use only the working legs [27].", "startOffset": 88, "endOffset": 92}, {"referenceID": 5, "context": "An alternative approach is to let a damaged robot learn a new policy by trial-and-error, which bypasses the need for a diagnosis [7], [9], [8], [10].", "startOffset": 129, "endOffset": 132}, {"referenceID": 7, "context": "An alternative approach is to let a damaged robot learn a new policy by trial-and-error, which bypasses the need for a diagnosis [7], [9], [8], [10].", "startOffset": 134, "endOffset": 137}, {"referenceID": 6, "context": "An alternative approach is to let a damaged robot learn a new policy by trial-and-error, which bypasses the need for a diagnosis [7], [9], [8], [10].", "startOffset": 139, "endOffset": 142}, {"referenceID": 8, "context": "An alternative approach is to let a damaged robot learn a new policy by trial-and-error, which bypasses the need for a diagnosis [7], [9], [8], [10].", "startOffset": 144, "endOffset": 148}, {"referenceID": 6, "context": "To minimize the number of trials, several algorithms rely on the transferability hypothesis [8], [9]: the behaviors that do not use the damaged parts are likely to be similar with the damaged and the undamaged robot, therefore simulations of the intact robot can help searching for a new behavior on the damaged robot.", "startOffset": 92, "endOffset": 95}, {"referenceID": 7, "context": "To minimize the number of trials, several algorithms rely on the transferability hypothesis [8], [9]: the behaviors that do not use the damaged parts are likely to be similar with the damaged and the undamaged robot, therefore simulations of the intact robot can help searching for a new behavior on the damaged robot.", "startOffset": 97, "endOffset": 100}, {"referenceID": 6, "context": "Starting with this hypothesis, the IT&E algorithm [8] exploits a dynamic simulation of the intact robot to create a behaviorperformance map that predicts the performance of thousands of different behaviors.", "startOffset": 50, "endOffset": 53}, {"referenceID": 26, "context": "If a damage occurs, this map is used as a prior for a Bayesian optimization algorithm that searches for a compensatory behavior [28], [23], [22].", "startOffset": 128, "endOffset": 132}, {"referenceID": 21, "context": "If a damage occurs, this map is used as a prior for a Bayesian optimization algorithm that searches for a compensatory behavior [28], [23], [22].", "startOffset": 134, "endOffset": 138}, {"referenceID": 20, "context": "If a damage occurs, this map is used as a prior for a Bayesian optimization algorithm that searches for a compensatory behavior [28], [23], [22].", "startOffset": 140, "endOffset": 144}, {"referenceID": 6, "context": "Overall, the experimental results show that IT&E can allow various types of robots (a hexapod robot and an 8-DOF manipulator) to compensate for many different types of injuries in less than 2 minutes [8].", "startOffset": 200, "endOffset": 203}, {"referenceID": 16, "context": "Our problem can be cast in the general framework of Markov Decision Processes (MDP) [18].", "startOffset": 84, "endOffset": 88}, {"referenceID": 6, "context": "We would like to create a repertoire that covers the task space as well as possible [8], [29], [30], i.", "startOffset": 84, "endOffset": 87}, {"referenceID": 27, "context": "We would like to create a repertoire that covers the task space as well as possible [8], [29], [30], i.", "startOffset": 89, "endOffset": 93}, {"referenceID": 28, "context": "We would like to create a repertoire that covers the task space as well as possible [8], [29], [30], i.", "startOffset": 95, "endOffset": 99}, {"referenceID": 13, "context": "As a consequence, instead of using an inverse model, we learn the action repertoire with an iterative algorithm called MAP-Elites [15], [8] and a forward model (e.", "startOffset": 130, "endOffset": 134}, {"referenceID": 6, "context": "As a consequence, instead of using an inverse model, we learn the action repertoire with an iterative algorithm called MAP-Elites [15], [8] and a forward model (e.", "startOffset": 136, "endOffset": 139}, {"referenceID": 13, "context": "Essentially, MAP-Elites discretizes the na-dimensional task space to an na-dimensional grid, and then attempts to fill each of the cells thanks to a variation-selection loop [15], [8].", "startOffset": 174, "endOffset": 178}, {"referenceID": 6, "context": "Essentially, MAP-Elites discretizes the na-dimensional task space to an na-dimensional grid, and then attempts to fill each of the cells thanks to a variation-selection loop [15], [8].", "startOffset": 180, "endOffset": 183}, {"referenceID": 6, "context": "So far, it has been successfully used to generate behaviors for legged robots [8], wheeled robots controlled by neural networks [30], and even adversarial images for deep neural networks [31].", "startOffset": 78, "endOffset": 81}, {"referenceID": 28, "context": "So far, it has been successfully used to generate behaviors for legged robots [8], wheeled robots controlled by neural networks [30], and even adversarial images for deep neural networks [31].", "startOffset": 128, "endOffset": 132}, {"referenceID": 29, "context": "So far, it has been successfully used to generate behaviors for legged robots [8], wheeled robots controlled by neural networks [30], and even adversarial images for deep neural networks [31].", "startOffset": 187, "endOffset": 191}, {"referenceID": 26, "context": "A GP is an extension of the multivariate Gaussian distribution to an infinite-dimension stochastic process for which any finite combination of dimensions will be a Gaussian distribution [28].", "startOffset": 186, "endOffset": 190}, {"referenceID": 30, "context": "Here we use Monte Carlo Tree Search (MCTS) [32], as it has already been succesfully used to solve (Partially Observable)-MDPs with stochastic transition functions [33], [34], continuous state spaces, and high branching factors [34], [35].", "startOffset": 43, "endOffset": 47}, {"referenceID": 31, "context": "Here we use Monte Carlo Tree Search (MCTS) [32], as it has already been succesfully used to solve (Partially Observable)-MDPs with stochastic transition functions [33], [34], continuous state spaces, and high branching factors [34], [35].", "startOffset": 163, "endOffset": 167}, {"referenceID": 32, "context": "Here we use Monte Carlo Tree Search (MCTS) [32], as it has already been succesfully used to solve (Partially Observable)-MDPs with stochastic transition functions [33], [34], continuous state spaces, and high branching factors [34], [35].", "startOffset": 169, "endOffset": 173}, {"referenceID": 32, "context": "Here we use Monte Carlo Tree Search (MCTS) [32], as it has already been succesfully used to solve (Partially Observable)-MDPs with stochastic transition functions [33], [34], continuous state spaces, and high branching factors [34], [35].", "startOffset": 227, "endOffset": 231}, {"referenceID": 33, "context": "Here we use Monte Carlo Tree Search (MCTS) [32], as it has already been succesfully used to solve (Partially Observable)-MDPs with stochastic transition functions [33], [34], continuous state spaces, and high branching factors [34], [35].", "startOffset": 233, "endOffset": 237}, {"referenceID": 32, "context": "These rollouts are typically random or directed by a simple, domain-dependent heuristic [34].", "startOffset": 88, "endOffset": 92}, {"referenceID": 33, "context": "see [35]", "startOffset": 4, "endOffset": 8}, {"referenceID": 6, "context": "The low-level controller is the same as in [8], [29].", "startOffset": 43, "endOffset": 46}, {"referenceID": 27, "context": "The low-level controller is the same as in [8], [29].", "startOffset": 48, "endOffset": 52}, {"referenceID": 34, "context": "We use the Sferes2 framework [36] and the DART simulator1 for our implementation.", "startOffset": 29, "endOffset": 33}, {"referenceID": 35, "context": "We use the Squared Exponential Kernel as the covariance function [37].", "startOffset": 65, "endOffset": 69}, {"referenceID": 36, "context": "[38] that properly handles cases where there is a large number of actions or the action space is continuous.", "startOffset": 0, "endOffset": 4}, {"referenceID": 37, "context": "[39] that properly handles cases where the state space is continuous.", "startOffset": 0, "endOffset": 4}, {"referenceID": 33, "context": "d) Generative Model: We construct the generative model using the prediciton of the GPs (with their variance) [35].", "startOffset": 109, "endOffset": 113}, {"referenceID": 32, "context": "e) Default Policy for evaluation: We sample random actions from the repertoire [34].", "startOffset": 79, "endOffset": 83}, {"referenceID": 32, "context": ", we select the action that has the maximum average value [34].", "startOffset": 58, "endOffset": 62}, {"referenceID": 38, "context": "To make the search faster, we implemented root parallelization in MCTS [40] and we ran 4 parallel trees and gave to each one a budget of 5000 iterations.", "startOffset": 71, "endOffset": 75}, {"referenceID": 33, "context": "More generic approaches would be the Blind Value action sampling approach or cRAVE [35], [41].", "startOffset": 83, "endOffset": 87}, {"referenceID": 39, "context": "More generic approaches would be the Blind Value action sampling approach or cRAVE [35], [41].", "startOffset": 89, "endOffset": 93}, {"referenceID": 6, "context": "The environment (location of the obstacles) and the position of the robot is acquired using a Motion Capture system (Optitrack), although a SLAM and/or a visual odometry algorithm could have been used [8], [42].", "startOffset": 201, "endOffset": 204}, {"referenceID": 40, "context": "The environment (location of the obstacles) and the position of the robot is acquired using a Motion Capture system (Optitrack), although a SLAM and/or a visual odometry algorithm could have been used [8], [42].", "startOffset": 206, "endOffset": 210}, {"referenceID": 41, "context": "one will do about it when it occurs\u201d [43].", "startOffset": 37, "endOffset": 41}, {"referenceID": 2, "context": "This advice is especially important if we want to be able to send advanced and expensive robots in risky places like a destroyed nuclear plant [4], even if the robots are tele-operated.", "startOffset": 143, "endOffset": 146}, {"referenceID": 42, "context": "In future work, we will therefore consider using a time-window to keep this complexity low and/or using sparse GPs [44].", "startOffset": 115, "endOffset": 119}, {"referenceID": 40, "context": "In the future, we will design a more realistic experiment in which the robot discovers its environment with a SLAM algorithm [42].", "startOffset": 125, "endOffset": 129}], "year": 2017, "abstractText": "The high probability of hardware failures prevents many advanced robots (e.g. legged robots) to be confidently deployed in real-world situations (e.g post-disaster rescue). Instead of attempting to diagnose the failure(s), robots could adapt by trial-and-error in order to be able to complete their tasks. However, the best trial-and-error algorithms for robotics are all episodic: between each trial, the robot needs to be put back in the same state, that is, the robot is not learning autonomously. In this paper, we introduce a novel learning algorithm called \u201cReset-free Trial-and-Error\u201d (RTE) that allows robots to recover from damage while completing their tasks. We evaluate it on a hexapod robot that is damaged in several ways (e.g. a missing leg, a shortened leg, etc.) and whose objective is to reach a sequence of targets in an arena. Our experiments show that the robot can recover most of its locomotion abilities in a few minutes, in an environment with obstacles, and without any human intervention. Overall, this new algorithm makes it possible to contemplate sending robots to places that are truly too dangerous for humans and in which robots cannot be rescued.", "creator": "LaTeX with hyperref package"}}}