{"id": "1610.04265", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Oct-2016", "title": "Fast, Scalable Phrase-Based SMT Decoding", "abstract": "The utilization of statistical machine translation (SMT) has grown enormously over the last decade, many using open-source software developed by the NLP community. As commercial use has increased, there is need for software that is optimized for commercial requirements, in particular, fast phrase-based decoding and more efficient utilization of modern multicore servers.", "histories": [["v1", "Thu, 13 Oct 2016 21:25:34 GMT  (242kb,D)", "https://arxiv.org/abs/1610.04265v1", null], ["v2", "Tue, 18 Oct 2016 22:32:18 GMT  (242kb,D)", "http://arxiv.org/abs/1610.04265v2", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["hieu hoang", "nikolay bogoychev", "lane schwartz", "marcin junczys-dowmunt"], "accepted": false, "id": "1610.04265"}, "pdf": {"name": "1610.04265.pdf", "metadata": {"source": "CRF", "title": "Fast, Scalable Phrase-Based SMT Decoding", "authors": ["Hieu Hoang", "Nikolay Bogoychev", "Marcin Junczys-Dowmunt"], "emails": ["hieu@moses-mt.org", "s1031254@sms.ed.ac.uk", "lanes@illinois.edu", "junczys@amu.edu.pl"], "sections": [{"heading": null, "text": "In this paper, we examine the most important components of phrase-based decoding and decoder implementation with a particular focus on speed and scalability on multicore machines. The result is a replacement for the Moses decoder, which is up to fifteen times faster and scales monotonously with the number of cores."}, {"heading": "1 Introduction", "text": "Over the past decade, SMT has steadily evolved from a research discipline to commercial viability, as can be seen from services like Google and Microsoft Translation Services. Besides general services like these, there are a large number of companies that offer customized translation systems, as well as companies and organizations that implement in-house solutions. Many of these customized solutions use Moses as their SMT engineering. For many users, decoding is the time-critical part of the translation process. Using the multiple cores that are ubiquitous in servers today is a common strategy to mitigate this problem. However, it has been noted that the Moses decoder, among other things, is unable to efficiently use multiple cores (Ferna-ndez et al., 2016). That is, the decoding speed does not increase significantly when more cores are used, in fact, it can decrease when more cores are used."}, {"heading": "1.1 Prior Work", "text": "Most of the previous work on increasing decoding speed is similar to optimizing specific components of the decoder or decoding algorithms (2014). Heafield (2011) and Yasuhara et al. (2013) describe fast, efficient data structures for language models. Zens and Ney (2007) describe an implementation of a phrase table for an SMT decoder that is loaded on demand and reduces initial loading time and storage requirements. Junczys-Dowmunt (2012) extends this by compressing the phrase table and lexicalizing the re-order model. Chiang (2007) describes the cube-pruning and cube-growing algorithms that adjust the compromise between speed and translation quality to the individual parameters. Wuebker et al. (2012b) Note that language model is one of the most expensive operations in decoding. They have tried to improve decoding speed."}, {"heading": "1.2 Phrase-Based Model", "text": "The objective of decoding is to find the target translation with the highest probability, since a source sentence is given. In other words, for a source sentence, the goal is to find a target translation with the highest conditional probability p (t | s). Formally, this is written as: t = argmax t p (t | s) (1), where the arg-max function is the search. The log-linear model generalizes Equation 1 to include more component models and the weighting of each model according to the contribution of each model to the overall probability. p (t | s) = 1 Z exp (\u2211 m \u03bbmhm (t, s) (2), where \u03bbm is the weight, and hm is the characteristic function or \"score\" for model m. Z is the partition function that can be ignored for the optimization of the model. The standard functions of the phrase-based model include: 1. a distortion penalty, 3. a word penalty, 6. a translation penalty (6., 6.)."}, {"heading": "1.3 Beam Search", "text": "A translation of a source sentence is created by applying a set of translation rules that translate each source word together once and only once. Each partial translation is known as a hypothesis that is created by applying a rule to an existing hypothesis. This hypothesis expansion process starts with a hypothesis that has not translated a source word and ends with completed hypotheses that have translated all source words. The hypotheses with the highest scoring value are considered to be the best translation according to the model evaluation. In the phrase-based model, each rule translates a coherent sequence of source words. Successive applications of translation rules do not need to be adjacent to the source side, depending on the distortion limit. The target output is constructed strictly to the left of the target side using this sequence of translation rules. A bar search algorithm is used to efficiently build the complete hypotheses. The hypotheses are grouped into stacks that contain a comparable set of hypotheses based on each of a number of cardinals."}, {"heading": "2 Proposed Improvements", "text": "We will also focus on four main areas of optimisation."}, {"heading": "2.1 Efficient Memory Allocation", "text": "The search algorithm generates and destroys a large number of intermediate objects such as hypotheses and functional states, putting a strain on the operating system due to the need to synchronize memory access, especially when using a large number of threads. Libraries such as tcmalloc (Ghemawat and Menage, 2009) are designed to reduce the locking of storage applications, but in our case this is still not enough. We will try to improve the decryption speed by replacing the memory management of the operating system with our own custom memory management scheme. Memory is allocated from a storage pool rather than using the general allocation functions of the operating system. A memory pool is a large block given to the application by the operating system, and the application is then responsible for allocating portions of that memory to its components when they are requested."}, {"heading": "2.2 Stack Configurations", "text": "The most popular stack configuration for phrase-based models is cover cardinality, i.e. hypotheses that have translated the same number of source words are stored on the same stack. This is implemented in Pharaoh, Moses, and Joshua. However, there are alternatives to this configuration. Och et al. (2001) use a single stack for all hypotheses, Brown et al. (1993) use cover stacks (i.e. one stack per unique cover vector), while Wuebker et al. (2012a) and Zens and Ney (2008) use both cover and cardinality snippets. Although useful, this earlier work represents only one specific stack configuration each. Ortiz-Mart\u00ed'nez et al. (2006) examine a number of stack configurations by defining a granularity parameter that controls the maximum number of stacks needed to decode a sentence so we can re-examine the overlay configuration with a particular question we will be able to decode it."}, {"heading": "2.3 Phrase-Table Optimizations", "text": "For each phrase table of realistic size, memory and load time constraints, we need to use a load-on-demand implementation. Moses has several to fall back on, each with different performance characteristics. Figure 1 shows the decoding speed for the two fastest implementations. It shows that the probing phrase table (Bogoychev and Lopez, 2016) has the fastest translation rule, especially with a large number of cores, so from now on we will focus exclusively on this implementation.We propose two optimizations: First, the translation rule caching mechanism in Moses saves the most recently used rules. However, this requires locking and active management when clearing old rules. The result is a slower decoding, Table 1. We will examine a simpler caching mechanism by creating a static cache of the most likely translation rules, which will be used to start the decoding speed of the second algorithm."}, {"heading": "2.4 Lexicalized Re-ordering Model Optimizations", "text": "Similar to the phrase table, the lexicalized reorder model is trained on parallel data and a resulting model file is then queried during decoding. However, the need for random searches during the query inevitably leads to a slower decoding speed. Previous work such as Junczys-Dowmunt (2012) improved the query speed with more compact data structures. However, the query keys of the model are the source and target phrase of each translation rule. Instead of storing the lexicalized reorder model separately, we will integrate it into the translation model, eliminating the need for a separate file. However, the model remains the same under the loglinear framework, including its own weights.This optimization has a precedent in Wuebker et al. (2012a), but the effects on decoding speed have not been published. We will compare the results with the use of a separate model in this paper."}, {"heading": "3 Experimental Setup", "text": "The training data consisted of most publicly available Arabic-English data from Opus (Tiedemann, 2012), which contained over 69 million parallel sentences, and matched to a given set; the phrase table was then truncated using only the top 100 entries per source phrase, according to the compact data structure (Junczys-Dowmunt, 2012); all model files were then binarized; the language models were selected for their best multithreaded performance using KenLM (Heafield, 2011); the phrase table using the sample phrase table; lexicalized re-sorting model using the compact data structure (Junczys-Dowmunt, 2012); these binary formats were selected for their best multithreaded performance; Table 2 gives details on the resulting sizes of the model files. To test the decoding speed, we used a subset of training data, Table 3.For verification with another data set, we also used a second set of French sets."}, {"heading": "4 Results", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Optimizing Memory", "text": "Memory management accounts for more than 24% of the lifetime of the Moses decoder (Table 4), which increases to 39% when 32 threads are used, which reduces the scalability of the decoder. In contrast, our decoder spends 11% on memory management and does not significantly increase with more threads. Figure 2 compares the decoding speed for Moses and our decoder with the same models, parameters and test sets. Our decoder is 4.4 times faster with one thread and 5.0 times faster with all cores. However, like Moses, performance actually deteriorates after about 15 threads. The commit hash was bc5f8d15c6ce4bc678ba992860bfd4be6719cee8 6http: / / boost.org /"}, {"heading": "4.2 Stack Configuration", "text": "We investigated the effects of the following three stack configurations on the model score and decoding speed: 1. Coverage Cardinality, 2. Coverage, 3. Coverage and final position of the last translated source word. Coverage Cardinality is the same as Moses and Joshua. Coverage Configuration uses one stack per unique coverage vector. Coverage and final position of the last translated source word extend the coverage configuration by separating hypotheses in which the position of the last translated word is different, even if the coverage vectors are identical. This is an optimization to reduce the number of checks of the distortion limit depending on the position of the last word. Verification is a binary function d (Ch, ehypo, ranger) in which Ch is the coverage vector of the hypothesis h, eh the final position of the most current source word that has been translated, and the ranking of the individual groups is consistent with the coverage speed of the hypothesis."}, {"heading": "4.3 Translation Model", "text": "During the first optimization, we create a static translation model cache with translation rules that translates the most common source phrases, which is then loaded when the decoder starts. No effort is required to manage an active cache, but there is still some effort when using a cache. However, overall, using a static cache reduces the decoding time by 10% when using the optimal cache size, Table 5. For the second optimization, we disable compression of the target side of the translation rules, increasing the size of the binary files from 17 GB to 23 GB, but the time saving when decompressing the data resulted in a 1.5% reduction in decoding time for 1 thread and nearly 7% for saturated CPUs, Table 6."}, {"heading": "4.4 Lexicalized Re-ordering Model", "text": "The lexicalized reorder model requires a probability distribution of the reorder behavior of each translation rule learned from the training data, which is represented in the model file as a fixed number of probabilities for each rule, exactly how many probabilities depend on the parameterization of the model during the training. In decryption, each hypothesis is assigned a probability from this distribution according to the reorganization of the translation rule. In decryption, the probability distribution is then taken from the translation model rather than retrieving a separate file.This resulted in a significant reduction in the decryption time, especially with a high number of cores, Figure 5. The decryption speed increased by 40% when using a thread, but is 5 times faster when using 32 threads."}, {"heading": "4.5 Scalability", "text": "Figure 6 shows the decoding speed relative to the number of threads used. In our work, the decoding speed increases steadily as more threads are used, and decreases slightly after 16 threads when virtual cores are used by the CPU. Overall, decoding is 16 times faster than decoding with only one thread when all 16 cores (32 hyperthreads) are at full capacity. In contrast to Moses, where the speed increases to about 16 threads, but slows thereafter. using the tcmalloc library has a small positive effect on the decoding speed, but does little to improve scalability. Our work is 4.3 times faster than Moses with a single thread and 10.4 times faster when all cores are used."}, {"heading": "4.6 Other Models and Even More Cores", "text": "Our decoder has no scaling problems if we have tested it with the same model and tested it on a larger server, Figure 7. We verify the results with the French-English phrase system and test set. Speed gains are even greater than the Arabic-English test scenario, Figure 8. Our decoder is 5.4 times faster than Moses with a single thread and 14.5 times faster when all cores are saturated. It was suggested that using a larger language model would outweigh the improvements in decoding speed. We tested this assumption by replacing the language model in the ar-en experiment with a 96 GB language model. Loading time of the language model is significant (394 sec) and was excluded from the translation speed. Results show that our decoder is 7 times faster than Moses and still scaled monotonously until all CPUs are saturated, Figure 9."}, {"heading": "5 Conclusion", "text": "We have introduced a new decoder that is compatible with Moses. By investigating the shortcomings of the current implementation, we are able to optimize the speed, especially for multicore operation, resulting in double-digit increases over Moses on the same hardware. Our implementation is also unaffected by scalability issues that have affected Moses. In the future, we will investigate other key components of the decoding algorithm, especially the language model that was not touched in this essay. We are also interested in investigating the underlying reasons for the scalability problems in Moses to get a better understanding of where potential performance problems may occur."}, {"heading": "Acknowledgments", "text": "This work is sponsored by the Air Force Research Laboratory, Main Contract FA8650-11-C6160. The views and conclusions contained in this document are those of the authors and should not be construed as representative of the official policy of the Air Force Research Laboratory or the U.S. Government, neither express nor implied. Thanks to Kenneth Heafield for advice and code."}], "references": [{"title": "Fast and highly parallelizable phrase table for statistical machine translation", "author": ["N. Bogoychev", "A. Lopez"], "venue": "Proceedings of the First Conference on Statistical Machine Translation WMT16, Berlin, Germany.", "citeRegEx": "Bogoychev and Lopez,? 2016", "shortCiteRegEx": "Bogoychev and Lopez", "year": 2016}, {"title": "The mathematics of statistical machine translation", "author": ["P.F. Brown", "S.A. Della-Pietra", "V.J. Della-Pietra", "R.L. Mercer"], "venue": "Computational Linguistics, 19(2):263\u2013313.", "citeRegEx": "Brown et al\\.,? 1993", "shortCiteRegEx": "Brown et al\\.", "year": 1993}, {"title": "Hierarchical phrase-based translation", "author": ["D. Chiang"], "venue": "Computational Linguistics, 33(2):201\u2013228.", "citeRegEx": "Chiang,? 2007", "shortCiteRegEx": "Chiang", "year": 2007}, {"title": "Boosting performance of a statistical machine translation system using dynamic parallelism", "author": ["M. Fern\u00e1ndez", "J.C. Pichel", "J.C. Cabaleiro", "T.F. Pena"], "venue": "Journal of Computational Science, 13:37\u201348.", "citeRegEx": "Fern\u00e1ndez et al\\.,? 2016", "shortCiteRegEx": "Fern\u00e1ndez et al\\.", "year": 2016}, {"title": "Tcmalloc: Thread-caching malloc", "author": ["S. Ghemawat", "P. Menage"], "venue": null, "citeRegEx": "Ghemawat and Menage,? \\Q2009\\E", "shortCiteRegEx": "Ghemawat and Menage", "year": 2009}, {"title": "KenLM: faster and smaller language model queries", "author": ["K. Heafield"], "venue": "Proceedings of the EMNLP 2011 Sixth Workshop on Statistical Machine Translation, pages 187\u2013197, Edinburgh, Scotland, United Kingdom.", "citeRegEx": "Heafield,? 2011", "shortCiteRegEx": "Heafield", "year": 2011}, {"title": "Faster Phrase-Based decoding by refining feature state", "author": ["K. Heafield", "M. Kayser", "C.D. Manning"], "venue": "Proceedings of the Association for Computational Linguistics, Baltimore, MD, USA.", "citeRegEx": "Heafield et al\\.,? 2014", "shortCiteRegEx": "Heafield et al\\.", "year": 2014}, {"title": "A space-efficient phrase table implementation using minimal perfect hash functions", "author": ["M. Junczys-Dowmunt"], "venue": "Sojka, P., Hork, A., Kopecek, I., and Pala, K., editors, 15th International Conference on Text, Speech and Dialogue (TSD), volume 7499 of Lecture Notes in Computer Science, pages 320\u2013327. Springer.", "citeRegEx": "Junczys.Dowmunt,? 2012", "shortCiteRegEx": "Junczys.Dowmunt", "year": 2012}, {"title": "Moses: Open source toolkit for statistical machine translation", "author": ["P. Koehn", "H. Hoang", "A. Birch", "C. Callison-Burch", "M. Federico", "N. Bertoldi", "B. Cowan", "W. Shen", "C. Moran", "R. Zens", "C. Dyer", "O. Bojar", "A. Constantin", "E. Herbst"], "venue": "Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedings of the Demo and Poster Sessions, pages 177\u2013180, Prague, Czech Republic. Association for Computational Linguistics.", "citeRegEx": "Koehn et al\\.,? 2007", "shortCiteRegEx": "Koehn et al\\.", "year": 2007}, {"title": "Joshua: An open source toolkit for parsing-based machine translation", "author": ["Z. Li", "C. Callison-Burch", "C. Dyer", "S. Khudanpur", "L. Schwartz", "W. Thornton", "J. Weese", "O. Zaidan"], "venue": "Proceedings of the Fourth Workshop on Statistical Machine Translation, pages 135\u2013139, Athens, Greece. Association for Computational Linguistics.", "citeRegEx": "Li et al\\.,? 2009", "shortCiteRegEx": "Li et al\\.", "year": 2009}, {"title": "An efficient A* search algorithm for statistical machine translation", "author": ["F.J. Och", "N. Ueffing", "H. Ney"], "venue": "Workshop on Data-Driven Machine Translation at 39th Annual Meeting of the Association of Computational Linguistics (ACL).", "citeRegEx": "Och et al\\.,? 2001", "shortCiteRegEx": "Och et al\\.", "year": 2001}, {"title": "Generalized stack decoding algorithms for statistical machine translation", "author": ["D. Ortiz-Mart\u0131\u0301nez", "I. Garc\u0131\u0301a-Varea", "F. Casacuberta"], "venue": "In Proceedings on the Workshop on Statistical Machine Translation,", "citeRegEx": "Ortiz.Mart\u0131\u0301nez et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Ortiz.Mart\u0131\u0301nez et al\\.", "year": 2006}, {"title": "Phrasal: A toolkit for new directions in statistical machine translation", "author": ["D.C. Spence Green", "C.D. Manning"], "venue": "Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 114\u2013121. Citeseer.", "citeRegEx": "Green and Manning,? 2014", "shortCiteRegEx": "Green and Manning", "year": 2014}, {"title": "Parallel data, tools and interfaces in opus", "author": ["J. Tiedemann"], "venue": "LREC, pages 2214\u20132218.", "citeRegEx": "Tiedemann,? 2012", "shortCiteRegEx": "Tiedemann", "year": 2012}, {"title": "Jane 2: Open source phrase-based and hierarchical statistical machine translation", "author": ["J. Wuebker", "M. Huck", "S. Peitz", "M. Nuhn", "M. Freitag", "Peter", "J.-T.", "S. Mansour", "H. Ney"], "venue": "24th International Conference on Computational Linguistics, page 483. Citeseer.", "citeRegEx": "Wuebker et al\\.,? 2012a", "shortCiteRegEx": "Wuebker et al\\.", "year": 2012}, {"title": "Fast and scalable decoding with language model lookahead for phrase-based statistical machine translation", "author": ["J. Wuebker", "H. Ney", "R. Zens"], "venue": "Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Short Papers-Volume 2, pages 28\u201332. Association for Computational Linguistics.", "citeRegEx": "Wuebker et al\\.,? 2012b", "shortCiteRegEx": "Wuebker et al\\.", "year": 2012}, {"title": "An efficient language model using double-array structures", "author": ["M. Yasuhara", "T. Tanaka", "Norimatsu", "J.-y.", "M. Yamamoto"], "venue": "Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 222\u2013232, Seattle, Washington, USA. Association for Computational Linguistics.", "citeRegEx": "Yasuhara et al\\.,? 2013", "shortCiteRegEx": "Yasuhara et al\\.", "year": 2013}, {"title": "Efficient phrase-table representation for machine translation with applications to online mt and speech translation", "author": ["R. Zens", "H. Ney"], "venue": "HLT-NAACL, pages 492\u2013499.", "citeRegEx": "Zens and Ney,? 2007", "shortCiteRegEx": "Zens and Ney", "year": 2007}, {"title": "Improvements in dynamic programming beam search for phrase-based statistical machine translation", "author": ["R. Zens", "H. Ney"], "venue": "Proc. of IWSLT.", "citeRegEx": "Zens and Ney,? 2008", "shortCiteRegEx": "Zens and Ney", "year": 2008}], "referenceMentions": [{"referenceID": 3, "context": "However, it has been noticed that the Moses decoder, amongst others, is unable to efficiently use multiple cores (Fern\u00e1ndez et al., 2016).", "startOffset": 113, "endOffset": 137}, {"referenceID": 8, "context": "The most well known is Moses (Koehn et al., 2007) which implements a number of speed optimizations, including cubepruning.", "startOffset": 29, "endOffset": 49}, {"referenceID": 9, "context": "Joshua (Li et al., 2009) also supports cube-pruning for phrase-based models.", "startOffset": 7, "endOffset": 24}, {"referenceID": 14, "context": "Jane (Wuebker et al., 2012a) supports the language model look-ahead described in Wuebker et al.", "startOffset": 5, "endOffset": 28}, {"referenceID": 3, "context": "Heafield (2011) and Yasuhara et al.", "startOffset": 0, "endOffset": 16}, {"referenceID": 3, "context": "Heafield (2011) and Yasuhara et al. (2013) describe fast, efficient data structures for language models.", "startOffset": 0, "endOffset": 43}, {"referenceID": 3, "context": "Heafield (2011) and Yasuhara et al. (2013) describe fast, efficient data structures for language models. Zens and Ney (2007) describes an implementation of a phrase-table for an SMT decoder that is loaded on demand, reducing the initial loading time and memory requirements.", "startOffset": 0, "endOffset": 125}, {"referenceID": 3, "context": "Heafield (2011) and Yasuhara et al. (2013) describe fast, efficient data structures for language models. Zens and Ney (2007) describes an implementation of a phrase-table for an SMT decoder that is loaded on demand, reducing the initial loading time and memory requirements. Junczys-Dowmunt (2012) extends this by compressing the on-disk phrase table and lexicalized re-ordering model.", "startOffset": 0, "endOffset": 298}, {"referenceID": 2, "context": "Chiang (2007) describes the cube-pruning and cube-growing algorithm which allows the tradeoff between speed and translation quality to the adjusted with a single parameter.", "startOffset": 0, "endOffset": 14}, {"referenceID": 2, "context": "Chiang (2007) describes the cube-pruning and cube-growing algorithm which allows the tradeoff between speed and translation quality to the adjusted with a single parameter. Wuebker et al. (2012b) note that language model querying is amongst the most expensive operation in decoding.", "startOffset": 0, "endOffset": 196}, {"referenceID": 2, "context": "Chiang (2007) describes the cube-pruning and cube-growing algorithm which allows the tradeoff between speed and translation quality to the adjusted with a single parameter. Wuebker et al. (2012b) note that language model querying is amongst the most expensive operation in decoding. They sought to improved decoding speed by caching score computations early pruning of translation options. This work is similar to Heafield et al. (2014) which group hypotheses with identical language model context and incrementally expand them, reducing LM querying.", "startOffset": 0, "endOffset": 437}, {"referenceID": 2, "context": "Chiang (2007) describes the cube-pruning and cube-growing algorithm which allows the tradeoff between speed and translation quality to the adjusted with a single parameter. Wuebker et al. (2012b) note that language model querying is amongst the most expensive operation in decoding. They sought to improved decoding speed by caching score computations early pruning of translation options. This work is similar to Heafield et al. (2014) which group hypotheses with identical language model context and incrementally expand them, reducing LM querying. Fern\u00e1ndez et al. (2016) was concerned with multi-core speed but treated decoding as a black box within a parallelization framework.", "startOffset": 0, "endOffset": 575}, {"referenceID": 2, "context": "Chiang (2007) describes the cube-pruning and cube-growing algorithm which allows the tradeoff between speed and translation quality to the adjusted with a single parameter. Wuebker et al. (2012b) note that language model querying is amongst the most expensive operation in decoding. They sought to improved decoding speed by caching score computations early pruning of translation options. This work is similar to Heafield et al. (2014) which group hypotheses with identical language model context and incrementally expand them, reducing LM querying. Fern\u00e1ndez et al. (2016) was concerned with multi-core speed but treated decoding as a black box within a parallelization framework. There are a number of phrase-based decoding implementations, many of which implements the extensions to the phrase-based model described above. The most well known is Moses (Koehn et al., 2007) which implements a number of speed optimizations, including cubepruning. It is widely used for MT research and commercial use. Joshua (Li et al., 2009) also supports cube-pruning for phrase-based models. Phrasal (Spence Green and Manning, 2014) supports a number of variants of the phrase-based model. Jane (Wuebker et al., 2012a) supports the language model look-ahead described in Wuebker et al. (2012b) in addition to other tools to speed up decoding such as having a separate fast, lightweight decoder.", "startOffset": 0, "endOffset": 1283}, {"referenceID": 2, "context": "Chiang (2007) describes the cube-pruning and cube-growing algorithm which allows the tradeoff between speed and translation quality to the adjusted with a single parameter. Wuebker et al. (2012b) note that language model querying is amongst the most expensive operation in decoding. They sought to improved decoding speed by caching score computations early pruning of translation options. This work is similar to Heafield et al. (2014) which group hypotheses with identical language model context and incrementally expand them, reducing LM querying. Fern\u00e1ndez et al. (2016) was concerned with multi-core speed but treated decoding as a black box within a parallelization framework. There are a number of phrase-based decoding implementations, many of which implements the extensions to the phrase-based model described above. The most well known is Moses (Koehn et al., 2007) which implements a number of speed optimizations, including cubepruning. It is widely used for MT research and commercial use. Joshua (Li et al., 2009) also supports cube-pruning for phrase-based models. Phrasal (Spence Green and Manning, 2014) supports a number of variants of the phrase-based model. Jane (Wuebker et al., 2012a) supports the language model look-ahead described in Wuebker et al. (2012b) in addition to other tools to speed up decoding such as having a separate fast, lightweight decoder. mtplz is a specialized decoder developed to implement the incremental decoding described in Heafield et al. (2014). The Moses, Joshua and Phrasal decoders implement multithreading, however, they all report scalability problems, either in the paper (Phrasal) or via social media (Moses2 and Joshua3).", "startOffset": 0, "endOffset": 1499}, {"referenceID": 5, "context": "We use KenLM (Heafield, 2011) due to its popularity and consistent performance, but as with Moses, other language model implementations can be added later.", "startOffset": 13, "endOffset": 29}, {"referenceID": 3, "context": "Unlike Fern\u00e1ndez et al. (2016), we will optimize decoding speed by looking inside the black box.", "startOffset": 7, "endOffset": 31}, {"referenceID": 3, "context": "Unlike Fern\u00e1ndez et al. (2016), we will optimize decoding speed by looking inside the black box. We will compare multicore performance the best-of-breed phrase-table described in Junczys-Dowmunt (2012) with our own implementation.", "startOffset": 7, "endOffset": 202}, {"referenceID": 4, "context": "Libraries such as tcmalloc (Ghemawat and Menage, 2009) are designed to reduce locking contention for multi-threaded application but in our case, this is still not enough.", "startOffset": 27, "endOffset": 54}, {"referenceID": 9, "context": "Och et al. (2001) uses a single stack for all hypotheses, Brown et al.", "startOffset": 0, "endOffset": 18}, {"referenceID": 1, "context": "(2001) uses a single stack for all hypotheses, Brown et al. (1993) uses coverage stacks (ie.", "startOffset": 47, "endOffset": 67}, {"referenceID": 1, "context": "(2001) uses a single stack for all hypotheses, Brown et al. (1993) uses coverage stacks (ie. one stack per unique coverage vector) while Wuebker et al. (2012a) and Zens and Ney (2008) apply both coverage and cardinality pruning.", "startOffset": 47, "endOffset": 160}, {"referenceID": 1, "context": "(2001) uses a single stack for all hypotheses, Brown et al. (1993) uses coverage stacks (ie. one stack per unique coverage vector) while Wuebker et al. (2012a) and Zens and Ney (2008) apply both coverage and cardinality pruning.", "startOffset": 47, "endOffset": 184}, {"referenceID": 1, "context": "(2001) uses a single stack for all hypotheses, Brown et al. (1993) uses coverage stacks (ie. one stack per unique coverage vector) while Wuebker et al. (2012a) and Zens and Ney (2008) apply both coverage and cardinality pruning. While useful, these prior works present only one particular stack configuration each. Ortiz-Mart\u0131\u0301nez et al. (2006) explore a range of stack configurations by defining a granularity parameter which controls the maximum number of stacks required to decode a sentence.", "startOffset": 47, "endOffset": 345}, {"referenceID": 0, "context": "From this, it appears that the Probing phrase-table (Bogoychev and Lopez, 2016) has the fastest translation rule lookup, especially with large number of cores, therefore, we will concentrate exclusively on this implementation from hereon.", "startOffset": 52, "endOffset": 79}, {"referenceID": 0, "context": "From this, it appears that the Probing phrase-table (Bogoychev and Lopez, 2016) has the fastest translation rule lookup, especially with large number of cores, therefore, we will concentrate exclusively on this implementation from hereon. We propose two optimizations. Firstly, the translation rule caching mechanism in Moses saves the most recently used rules. However, this require locking and active management in clearing of old rules. The result is slower decoding, Table 1. We shall explore a simpler caching mechanism by creating a static cache of the most likely translation rules to be used at the start of decoding. Secondly, the Probing phrase-table use a simple compression algorithm to compress the target side of the translation rule. Compression was championed by Junczys-Dowmunt (2012) as the main reason behind the speed of their phrase-table but as we saw in Figure 1, this comes at the cost of scalability to large number of threads.", "startOffset": 53, "endOffset": 802}, {"referenceID": 7, "context": "Previous work such as Junczys-Dowmunt (2012) improve querying speed with more compact data structures.", "startOffset": 22, "endOffset": 45}, {"referenceID": 7, "context": "Previous work such as Junczys-Dowmunt (2012) improve querying speed with more compact data structures. However, the model\u2019s query keys are the source and target phrase of each translation rule. Rather than storing the lexicalized re-ordering model separately, we shall integrating it into the translation model, eliminating the need to query a separate file. However, the model remains the same under the log-linear framework, including having its own weights. This optimization has precedent in Wuebker et al. (2012a) but the effect on decoding speed", "startOffset": 22, "endOffset": 519}, {"referenceID": 13, "context": "The training data consisted of most of the publicly available Arabic-English data from Opus (Tiedemann, 2012) containing over 69 million parallel sentences, and tuned on a held out set.", "startOffset": 92, "endOffset": 109}, {"referenceID": 5, "context": "All model files were then binarized; the language models were binarized using KenLM (Heafield, 2011), the phrase table using the Probing phrase-table, lexicalized reordering model using the compact data structure (Junczys-Dowmunt, 2012).", "startOffset": 84, "endOffset": 100}, {"referenceID": 7, "context": "All model files were then binarized; the language models were binarized using KenLM (Heafield, 2011), the phrase table using the Probing phrase-table, lexicalized reordering model using the compact data structure (Junczys-Dowmunt, 2012).", "startOffset": 213, "endOffset": 236}, {"referenceID": 2, "context": "Standard Moses phrase-based configurations are used, except that we use the cube-pruning algorithm (Chiang, 2007) with a pop-limit of 4004, rather than the basic phrase-based algorithm.", "startOffset": 99, "endOffset": 113}], "year": 2016, "abstractText": "The utilization of statistical machine translation (SMT) has grown enormously over the last decade, many using open-source software developed by the NLP community. As commercial use has increased, there is need for software that is optimized for commercial requirements, in particular, fast phrase-based decoding and more efficient utilization of modern multicore servers. In this paper we re-examine the major components of phrase-based decoding and decoder implementation with particular emphasis on speed and scalability on multicore machines. The result is a drop-in replacement for the Moses decoder which is up to fifteen times faster and scales monotonically with the number of cores.", "creator": "LaTeX with hyperref package"}}}