{"id": "1610.04286", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Oct-2016", "title": "Sim-to-Real Robot Learning from Pixels with Progressive Nets", "abstract": "Applying end-to-end learning to solve complex, interactive, pixel-driven control tasks on a robot is an unsolved problem. Deep Reinforcement Learning algorithms are too slow to achieve performance on a real robot, but their potential has been demonstrated in simulated environments. We propose using progressive networks to bridge the reality gap and transfer learned policies from simulation to the real world. The progressive net approach is a general framework that enables reuse of everything from low-level visual features to high-level policies for transfer to new tasks, enabling a compositional, yet simple, approach to building complex skills. We present an early demonstration of this approach with a number of experiments in the domain of robot manipulation that focus on bridging the reality gap. Unlike other proposed approaches, our real-world experiments demonstrate successful task learning from raw visual input on a fully actuated robot manipulator. Moreover, rather than relying on model-based trajectory optimisation, the task learning is accomplished using only deep reinforcement learning and sparse rewards.", "histories": [["v1", "Thu, 13 Oct 2016 22:42:10 GMT  (1603kb,D)", "http://arxiv.org/abs/1610.04286v1", null]], "reviews": [], "SUBJECTS": "cs.RO cs.LG", "authors": ["andrei a rusu", "matej vecerik", "thomas roth\\\"orl", "nicolas heess", "razvan pascanu", "raia hadsell"], "accepted": false, "id": "1610.04286"}, "pdf": {"name": "1610.04286.pdf", "metadata": {"source": "CRF", "title": "Sim-to-Real Robot Learning from Pixels with Progressive Nets", "authors": ["Andrei A. Rusu", "Matej Vecerik", "Thomas Roth\u00f6rl", "Nicolas Heess", "Razvan Pascanu", "Raia Hadsell"], "emails": ["raia}@google.com"], "sections": [{"heading": "1 Introduction", "text": "Deep Reinforcement Learning offers a new promise for reaching the human level in robotics domains, especially for pixel-to-action scenarios where government estimates of high-dimensional sensors and environmental interaction and feedback are critical. With deep RL, a new set of algorithms has evolved that can achieve sophisticated, precise control over demanding tasks, but these achievements have been demonstrated primarily in simulation, rather than in actual robotic platforms. While recent advances in simulation are profound RL impressive, demonstrating learning skills on real robots remains the yardstick by which we must measure the practical applicability of these methods."}, {"heading": "2 Transfer Learning from Simulation to Real", "text": "Our approach is based on the progressive network architecture, which enables the transfer of learning content through lateral connections that connect each layer of previously learned network columns with each new column, thus supporting a rich composition of functions. We first summarize progressive networks and then discuss their application for transfer to robotic domains."}, {"heading": "2.1 Progressive Networks", "text": "Progressive networks are ideal for the simulation transfer of policies into robot control domains, for several reasons. First, functions learned for one task can be transferred to many new tasks without being destroyed by fine-tuning. Second, the columns can be implicitly heterogeneous, which can be important for solving different tasks, including different input modalities, or simply to improve the learning speed when transferring them to the real robot. Third, progressive networks add new capacities, including new input connections, when transferring them to new tasks. This is beneficial for bridging the reality gap in order to accommodate different inputs between simulation and real sensors. a progressive network starts with a single column: a deep neural network with hidden activations h (1) i Rni of the number of units on the layer i. \""}, {"heading": "2.2 Approach", "text": "The proposed approach to the transfer of simulated to real robot domains is based on a progressive network with some specific changes. First, the columns of a progressive network do not have to have identical capacities or structures, and this can be an advantage in simulation situations. Second, the layered adapters proposed for progressive networks are unnecessary for the output layers of complementary task sequences so that they are not used. Third, the output layer of the robot-trained column is initialized from the simulated column to improve the growth of the overall parameters. These architectural features are shown in Fig. 1. The biggest risk in this approach to the transfer of learning layers is that the rewards in the real column are so sparse or non-existent that the output parameters of the simulated column are not executed."}, {"heading": "3 Related Literature", "text": "There are many different paradigms for domain transfer and many approaches specifically designed for deep neural models, but far fewer approaches for transferring simulation to reality for robotic domains. Even rarer are methods that can be used to transfer into interactive, rich sensor domains by learning end-to-end (pixel-to-action) data. A growing portion of the work examines the ability of deep networks to transfer between domains. Some research [16, 21] simply looks at the augmentation of target domain data with data from the source domain where an alignment exists. Building on this work, the observation begins that as you look at higher layers in the model, the transferability of traits rapidly decreases. To correct this effect, a soft constraint is added that forces the distribution of traits to become more similar. [13] A \"loss of confusion\" is proposed that forces the model to ignore variations in the two domains."}, {"heading": "4 Experiments", "text": "In fact, most people who are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to"}, {"heading": "4.1 Training in simulation", "text": "The first pillar is trained in the simulation with A3C, as already mentioned, with a broad feedback or recurring network. Intuitively, it makes sense to use a larger capacity network for training in the simulation to achieve maximum performance. We verified this intuition by comparing broad and narrow network architectures and found that the narrow network exhibits slower learning and poorer performance (see Figure 4). We also see that the LSTM model performs the feedback model by an average of 3 points per episode. Even with this relatively simple task, full performance is only achieved after significant interaction with the environment, in the order of 50 million steps - a number that is not feasible with a real robot. Compared to the real robot, simulation training is accelerated by fast rendering, multithreaded learning algorithms, and the ability to train continuously without human involvement."}, {"heading": "4.2 Transfer to the robot", "text": "To train on the real Jaco, a flat target is manually repositioned within a 40 by 30 cm area every third episode. Rewards are automatically given by tracking the colored target and issuing rewards based on the position of the Jaco gripper in relation to it. We train a baseline from scratch, a finely tuned first column and a progressive second column. Each experiment runs over about 60,000 steps (about four hours). The baseline is trained by randomly initializing a narrow network and then training it. We also try a randomly initialized wide network. As seen in Figure 5 (green curve), the randomly initialized column does not achieve the same result as the progressive column, and the agent does not receive a reward during the entire training. The progressive second column reaches 34 points, while the experiment begins with the finetuning, which begins with the simulated column and continues further training on the robot, does not achieve the same score as the progressive column."}, {"heading": "4.3 Transfer to a dynamic robot task with proprioception", "text": "Unlike the finetuning paradigm, which is unable to accommodate a changing network morphology or new input modalities, progressive networks offer flexibility that is advantageous for transferring to new data sources, while at the same time using prior knowledge. To demonstrate this, we train a second column on the Reacher task, but add proprioceptive features in addition to the RGB images. Proprioceptive features are common angles and speeds for each of the 9 joints of the arm and fingers, a total of 18, inputs to an MLP (a single linear layer plus ReLu) and connect it to the results of the twisted stack. Then, a third progressive column is added that learns only from the proprioceptive features, while visual input is passed through the previous columns and the features are used via the lateral connections."}, {"heading": "5 Discussion", "text": "Progressive neural networks provide a framework that can be used for continuous learning of many tasks and that facilitates transfer learning, even across the dividing line between simulation and robotics. We took full advantage of the flexibility and computational scaling that simulation provides, comparing many hyperparameters and architectures for a random start, a random target control task with visual input, and then successfully transferred the skills to agent training on the real robot. To exploit the potential of deep reinforcement learning applied in real robot domains, learning needs to become much more efficient. One way to achieve this is through the transfer of learning by simulation-trained agents. We have described an initial series of experiments that demonstrate that progressive networks can be used to achieve reliable, rapid transfer of pixel-to-action RL strategies."}], "references": [{"title": "Transfer learning for reinforcement learning on a physical robot", "author": ["Samuel Barrett", "Matthew E. Taylor", "Peter Stone"], "venue": "In Ninth International Conference on Autonomous Agents and Multiagent Systems - Adaptive Learning Agents Workshop (AAMAS - ALA),", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2010}, {"title": "Continuous deep q-learning with model-based acceleration", "author": ["Shixiang Gu", "Timothy P. Lillicrap", "Ilya Sutskever", "Sergey Levine"], "venue": "In ICML 2016,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2016}, {"title": "Learning continuous control policies by stochastic value gradients", "author": ["Nicolas Heess", "Gregory Wayne", "David Silver", "Timothy P. Lillicrap", "Tom Erez", "Yuval Tassa"], "venue": "In Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2015}, {"title": "3D Simulation for Robot Arm Control with Deep Q-Learning", "author": ["S. James", "E. Johns"], "venue": "ArXiv e-prints,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2016}, {"title": "Learning neural network policies with guided policy search under unknown dynamics", "author": ["Sergey Levine", "Pieter Abbeel"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2014}, {"title": "End-to-end training of deep visuomotor policies", "author": ["Sergey Levine", "Chelsea Finn", "Trevor Darrell", "Pieter Abbeel"], "venue": "CoRR, abs/1504.00702,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2015}, {"title": "Learning hand-eye coordination for robotic grasping with deep learning and large-scale data collection", "author": ["Sergey Levine", "Peter Pastor", "Alex Krizhevsky", "Deirdre Quillen"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2016}, {"title": "Learning contact-rich manipulation skills with guided policy search", "author": ["Sergey Levine", "Nolan Wagener", "Pieter Abbeel"], "venue": "In IEEE International Conference on Robotics and Automation,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2015}, {"title": "Continuous control with deep reinforcement learning", "author": ["Timothy P. Lillicrap", "Jonathan J. Hunt", "Alexander Pritzel", "Nicolas Heess", "Tom Erez", "Yuval Tassa", "David Silver", "Daan Wierstra"], "venue": "CoRR, abs/1509.02971,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2015}, {"title": "Learning transferable features with deep adaptation networks", "author": ["Mingsheng Long", "Yue Cao", "Jianmin Wang", "Michael I. Jordan"], "venue": "In Proceedings of the 32nd International Conference on Machine Learning, ICML 2015,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2015}, {"title": "Human-level control through deep reinforcement learning", "author": ["V. Mnih", "Kk Kavukcuoglu", "D. Silver", "A. Rusu", "J. Veness", "M. Bellemare", "A. Graves", "M. Riedmiller", "A. Fidjeland", "G. Ostrovski", "S. Petersen", "C. Beattie", "A. Sadik", "I. Antonoglou", "H. King", "D. Kumaran", "D. Wierstra", "S. Legg", "D. Hassabis"], "venue": "Nature, 518(7540):529\u2013533,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2015}, {"title": "Asynchronous methods for deep reinforcement learning", "author": ["Volodymyr Mnih", "Adri\u00e0 Puigdom\u00e8nech Badia", "Mehdi Mirza", "Alex Graves", "Timothy P. Lillicrap", "Tim Harley", "David Silver", "Koray Kavukcuoglu"], "venue": "In Int\u2019l Conf. on Machine Learning (ICML),", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2016}, {"title": "Learning deep object detectors from 3d models", "author": ["Xingchao Peng", "Baochen Sun", "Karim Ali", "Kate Saenko"], "venue": "In 2015 IEEE International Conference on Computer Vision,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2015}, {"title": "Supersizing self-supervision: Learning to grasp from 50k tries and 700 robot hours", "author": ["Lerrel Pinto", "Abhinav Gupta"], "venue": "In ICRA 2016,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2016}, {"title": "Progressive neural networks", "author": ["Andrei Rusu", "Neil Rabinowitz", "Guillaume Desjardins", "Hubert Soyer", "James Kirkpatrick", "Koray Kavukcuoglu", "Razvan Pascanu", "Raia Hadsell"], "venue": "arXiv preprint arXiv:1606.04671,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2016}, {"title": "Trust region policy optimization", "author": ["John Schulman", "Sergey Levine", "Philipp Moritz", "Michael I. Jordan", "Pieter Abbeel"], "venue": "In Proceedings of the 32nd International Conference on Machine Learning (ICML),", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2015}, {"title": "High-dimensional continuous control using generalized advantage estimation", "author": ["John Schulman", "Philipp Moritz", "Sergey Levine", "Michael Jordan", "Pieter Abbeel"], "venue": "In Proceedings of the International Conference on Learning Representations (ICLR),", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2016}, {"title": "Render for CNN: viewpoint estimation in images using cnns trained with rendered 3d model views", "author": ["Hao Su", "Charles Ruizhongtai Qi", "Yangyan Li", "Leonidas J. Guibas"], "venue": "In 2015 IEEE International Conference on Computer Vision,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2015}, {"title": "Mujoco: A physics engine for model-based control", "author": ["E. Todorov", "T. Erez", "Y. Tassa"], "venue": "In International Conference on Intelligent Robots and Systems IROS,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2012}, {"title": "Towards adapting deep visuomotor representations from simulated to real environments", "author": ["Eric Tzeng", "Coline Devin", "Judy Hoffman", "Chelsea Finn", "Xingchao Peng", "Sergey Levine", "Kate Saenko", "Trevor Darrell"], "venue": "CoRR, abs/1511.07111,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2015}, {"title": "Simultaneous deep transfer across domains and tasks", "author": ["Eric Tzeng", "Judy Hoffman", "Trevor Darrell", "Kate Saenko"], "venue": "In 2015 IEEE International Conference on Computer Vision,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2015}, {"title": "Deep domain confusion: Maximizing for domain invariance", "author": ["Eric Tzeng", "Judy Hoffman", "Ning Zhang", "Kate Saenko", "Trevor Darrell"], "venue": "CoRR, abs/1412.3474,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2014}, {"title": "Target-driven visual navigation in indoor scenes using deep reinforcement learning", "author": ["Yuke Zhu", "Roozbeh Mottaghi", "Eric Kolve", "Joseph J. Lim", "Abhinav Gupta", "Li Fei-Fei", "Ali Farhadi"], "venue": "CoRR, abs/1609.05143,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2016}], "referenceMentions": [{"referenceID": 4, "context": "While recent advances in simulation-driven deep RL are impressive [8, 19, 6, 12, 20, 15, 5], demonstrating learning capabilities on real robots remains the bar by which we must measure the practical applicability of these methods.", "startOffset": 66, "endOffset": 91}, {"referenceID": 15, "context": "While recent advances in simulation-driven deep RL are impressive [8, 19, 6, 12, 20, 15, 5], demonstrating learning capabilities on real robots remains the bar by which we must measure the practical applicability of these methods.", "startOffset": 66, "endOffset": 91}, {"referenceID": 2, "context": "While recent advances in simulation-driven deep RL are impressive [8, 19, 6, 12, 20, 15, 5], demonstrating learning capabilities on real robots remains the bar by which we must measure the practical applicability of these methods.", "startOffset": 66, "endOffset": 91}, {"referenceID": 8, "context": "While recent advances in simulation-driven deep RL are impressive [8, 19, 6, 12, 20, 15, 5], demonstrating learning capabilities on real robots remains the bar by which we must measure the practical applicability of these methods.", "startOffset": 66, "endOffset": 91}, {"referenceID": 16, "context": "While recent advances in simulation-driven deep RL are impressive [8, 19, 6, 12, 20, 15, 5], demonstrating learning capabilities on real robots remains the bar by which we must measure the practical applicability of these methods.", "startOffset": 66, "endOffset": 91}, {"referenceID": 11, "context": "While recent advances in simulation-driven deep RL are impressive [8, 19, 6, 12, 20, 15, 5], demonstrating learning capabilities on real robots remains the bar by which we must measure the practical applicability of these methods.", "startOffset": 66, "endOffset": 91}, {"referenceID": 1, "context": "While recent advances in simulation-driven deep RL are impressive [8, 19, 6, 12, 20, 15, 5], demonstrating learning capabilities on real robots remains the bar by which we must measure the practical applicability of these methods.", "startOffset": 66, "endOffset": 91}, {"referenceID": 14, "context": "Progressive nets have been shown to produce positive transfer between disparate tasks such as Atari games by utilizing lateral connections to previously learnt models [18].", "startOffset": 167, "endOffset": 171}, {"referenceID": 11, "context": "First, an actor-critic network is trained in simulation using multiple asynchronous workers [15].", "startOffset": 92, "endOffset": 96}, {"referenceID": 12, "context": "Some research [16, 21] considers simply augmenting the target domain data with data from the source domain where an alignment exists.", "startOffset": 14, "endOffset": 22}, {"referenceID": 17, "context": "Some research [16, 21] considers simply augmenting the target domain data with data from the source domain where an alignment exists.", "startOffset": 14, "endOffset": 22}, {"referenceID": 9, "context": "Building on this work, [13] starts from the observation that as one looks at higher layers in the model, the transferability of the features decreases quickly.", "startOffset": 23, "endOffset": 27}, {"referenceID": 9, "context": "In [13], a \u2018confusion\u2019 loss is proposed which forces the model to ignore variations in the data that separate the two domains [24, 25].", "startOffset": 3, "endOffset": 7}, {"referenceID": 20, "context": "In [13], a \u2018confusion\u2019 loss is proposed which forces the model to ignore variations in the data that separate the two domains [24, 25].", "startOffset": 126, "endOffset": 134}, {"referenceID": 21, "context": "In [13], a \u2018confusion\u2019 loss is proposed which forces the model to ignore variations in the data that separate the two domains [24, 25].", "startOffset": 126, "endOffset": 134}, {"referenceID": 20, "context": "Based on [24], [23] attempts to address the simulation to reality gap by using aligned data.", "startOffset": 9, "endOffset": 13}, {"referenceID": 19, "context": "Based on [24], [23] attempts to address the simulation to reality gap by using aligned data.", "startOffset": 15, "endOffset": 19}, {"referenceID": 17, "context": "Demonstrating the difficulty of the problem, [21] provides evidence that a simple application of a model trained on synthetic data on the real robot fails.", "startOffset": 45, "endOffset": 49}, {"referenceID": 0, "context": "Partial success on transferring from simulation to a real robot has been reported [2, 7, 26].", "startOffset": 82, "endOffset": 92}, {"referenceID": 3, "context": "Partial success on transferring from simulation to a real robot has been reported [2, 7, 26].", "startOffset": 82, "endOffset": 92}, {"referenceID": 22, "context": "Partial success on transferring from simulation to a real robot has been reported [2, 7, 26].", "startOffset": 82, "endOffset": 92}, {"referenceID": 5, "context": "[9],[11]).", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[9],[11]).", "startOffset": 4, "endOffset": 8}, {"referenceID": 13, "context": "Alternative approaches embrace big data ideas for robotics ([17, 10]).", "startOffset": 60, "endOffset": 68}, {"referenceID": 6, "context": "Alternative approaches embrace big data ideas for robotics ([17, 10]).", "startOffset": 60, "endOffset": 68}, {"referenceID": 11, "context": "For training in simulation, we use the Async Advantage Actor-Critic (A3C) framework introduced in [15].", "startOffset": 98, "endOffset": 102}, {"referenceID": 10, "context": "Compared to DQN [14], the model simultaneously learns a policy and a value function for predicting expected future rewards, and can be trained with CPUs, using multiple threads.", "startOffset": 16, "endOffset": 20}, {"referenceID": 18, "context": "The MuJoCo physics simulator [22] is used to train the first column for our experiments, with a rendered camera view to provide observations.", "startOffset": 29, "endOffset": 33}], "year": 2016, "abstractText": "Applying end-to-end learning to solve complex, interactive, pixel-driven control tasks on a robot is an unsolved problem. Deep Reinforcement Learning algorithms are too slow to achieve performance on a real robot, but their potential has been demonstrated in simulated environments. We propose using progressive networks to bridge the reality gap and transfer learned policies from simulation to the real world. The progressive net approach is a general framework that enables reuse of everything from low-level visual features to high-level policies for transfer to new tasks, enabling a compositional, yet simple, approach to building complex skills. We present an early demonstration of this approach with a number of experiments in the domain of robot manipulation that focus on bridging the reality gap. Unlike other proposed approaches, our real-world experiments demonstrate successful task learning from raw visual input on a fully actuated robot manipulator. Moreover, rather than relying on model-based trajectory optimisation, the task learning is accomplished using only deep reinforcement learning and sparse rewards.", "creator": "LaTeX with hyperref package"}}}