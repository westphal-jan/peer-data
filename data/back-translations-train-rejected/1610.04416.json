{"id": "1610.04416", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Oct-2016", "title": "Distributional Inclusion Hypothesis for Tensor-based Composition", "abstract": "According to the distributional inclusion hypothesis, entailment between words can be measured via the feature inclusions of their distributional vectors. In recent work, we showed how this hypothesis can be extended from words to phrases and sentences in the setting of compositional distributional semantics. This paper focuses on inclusion properties of tensors; its main contribution is a theoretical and experimental analysis of how feature inclusion works in different concrete models of verb tensors. We present results for relational, Frobenius, projective, and holistic methods and compare them to the simple vector addition, multiplication, min, and max models. The degrees of entailment thus obtained are evaluated via a variety of existing word-based measures, such as Weed's and Clarke's, KL-divergence, APinc, balAPinc, and two of our previously proposed metrics at the phrase/sentence level. We perform experiments on three entailment datasets, investigating which version of tensor-based composition achieves the highest performance when combined with the sentence-level measures.", "histories": [["v1", "Fri, 14 Oct 2016 11:52:19 GMT  (28kb)", "http://arxiv.org/abs/1610.04416v1", "To appear in COLING 2016"]], "COMMENTS": "To appear in COLING 2016", "reviews": [], "SUBJECTS": "cs.CL cs.AI", "authors": ["dimitri kartsaklis", "mehrnoosh sadrzadeh"], "accepted": false, "id": "1610.04416"}, "pdf": {"name": "1610.04416.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["d.kartsaklis@qmul.ac.uk", "mehrnoosh.sadrzadeh@qmul.ac.uk"], "sections": [{"heading": null, "text": "ar Xiv: 161 0.04 416v 1 [cs.C L] 14 Oct 201 6"}, {"heading": "1 Introduction", "text": "It's not the first time you've changed your name in another country, it's the second time you've changed your name in another country, it's the first time you've changed your name in another country, it's the second time you've changed your name in another country, it's the second time you've changed your name in another country, it's the second time you've changed your name in another country, it's the third time you've changed your name in another country, it's the third time you've changed your name in another country, it's the third time you've changed your name in another country, it's the third time you've changed your name in another country."}, {"heading": "2 Compositional distributional semantics", "text": "The purpose of a compositional distribution model is to generate a vector that represents the meaning of a phrase or sentence by combining the vectors of its words. In the simplest case, this is achieved by elementary operations on the vectors of the words (Mitchell and Lapata, 2010). Specifically, the vector representation \u2212 \u2192 w of a word sequence w1,.. In a linguistically motivated approach, relational words such as verbs and adjectives are treated as linear or multilinear maps, which are then applied to the vectors of their arguments by following the rules of grammar (Coecke et al, 2010; Grefenstette and Sadrzadeh, 2011; Baroni and Zamparelli, 2010)."}, {"heading": "3 Distributional inclusion hypothesis", "text": "The distribution hypothesis (DIH) (Dagan et al., 1999; Geffet and Dagan, 2005; Herbelot and Ganesalingam, 2013) is based on the fact that if a word u contains a word v, it makes sense to replace instances of u with v. For example, \"cat\" contains \"animal,\" i.e. in the sentence \"a cat is asleep,\" it makes no sense to do a similar substitution and get the sentence \"a butterfly is asleep.\" The hypothesis is that it only makes sense if there are monotonous contexts in upward contexts. For example, the substitution of u for v would not lead to judgments that have negations or quantifiers like \"all\" none. \""}, {"heading": "4 Measuring feature inclusion at the phrase/sentence level", "text": "In the most recent work, the authors of this paper have introduced a variation of the APinc and balAPinc aimed at addressing the additional complications imposed when assessing commitments at the phrase / sentence level (Kartsaklis and Sadrzadeh, 2016). The modified measures differ from the original in two respects. First, in a compositional distribution model, the practice of viewing non-zero elements of vectors as characteristics becomes too restrictive and thus suboptimal for assessing the vector space and the applied compositional operator (especially in intersectional models, see Sections 5 and 6), an element can obtain very low values without ever reaching zero. The new measures exploit this vagueness of the notion of \"decay\" to the limit by including F (\u2212 \u2192 w) all dimensions of the distribution models."}, {"heading": "5 Generic feature inclusion in compositional models", "text": "In fact, most of them are able to move around in search of a suitable candidate."}, {"heading": "6 Feature inclusion in concrete constructions of tensor-based models", "text": "While the previous section provided a general analysis of the characteristic inclusion behaviour of tensor-based models, the exact characteristic inclusion characteristics of these models depend on the specific concrete structures and are in principle more refined than the simple intersective or union-based composition. In this section, we will examine a range of grade-based models in terms of feature inclusion and derive their characteristics."}, {"heading": "6.1 Relational model", "text": "As a starting point we use the model of Grefenstette and Sadrzadeh (2011), which takes an extended approach and builds the tensor of a relationship word from the vectors of its arguments. Specifically, the tensors for adjectives, intransitive verbs, and transitive verbs are defined as below, or: adj = \u2211 i \u2212 \u2212 \u2212 nouni verbIN = \u2211 i \u2212 \u2192 Sbji verbTR = \u2211 i \u2212 \u2192 Sbji \u2212 Obji (7), where \u2212 \u2212 \u2212 nouni, \u2212 sbji, and \u2212 obji refer to the distribution vectors of the nouns, subjects, and objects that have occurred as arguments for the adjective and the verb over the training corpus vector vector vector vector vector vector vector. In the case of a subject verb-object-vector vector vector vector vector vector vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-result-vector-vector-string-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-result-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-"}, {"heading": "6.2 Frobenius models", "text": "In fact, most people are able to move to another world in which they are able to live, in which they want to live."}, {"heading": "6.3 Projective models", "text": "In this section we provide an alternative solution and solve the problem of having dimensions lower than those required by arguing that the sentence / phrase space should be spanned by the vectors of the verbs arguments by the corpus. Thus, we generate verb matrices for intransitive sentences and verb phrases by summarizing the projectors of the argument vector as follows: When these verbs are assembled with any subject / object to form a phrase / sentence, each vector in the encompassing space is weighted by its similarity (assuming normalized vectors) to the vector of this object / object, which is: \u2212 Objective Objective Obstructive \u2212 Obstructive \u2212 Obstructive \u2212 Obstructive \u2212 Obstructive \u2212 Obstructive \u2212 Obstructive \u2212 Obstructive \u2212 Obstructive \u2212 Obstructive \u2212 Obstructive \u2212 Obstructive \u2212 Obstructive \u2212 Obstruction (Objective)"}, {"heading": "6.4 Inclusion of verb vectors", "text": "The models of the previous sections offer a multitude of possibilities to represent the meaning of a verb using its arguments. We solve this problem by embedding the missing information in the existing tensors; for example, we can add the tensors of the projective model as follows: vitv = \u2211 i \u2212 \u2212 \u2212 Sbji (\u2212 Sbji-v) vvvvp = \u2211 i (\u2212 \u2192 Obji-v) \u2212 \u2212 \u2192 Obji (18) with \u2212 v, which designates the distribution vector of the verb. In the context of an intransitive sentence, we have now implemented the following interaction: \u2212 \u2212 \u2212 \u2212 sv = T \u00b7 vitv = \u2212 \u2212 s T \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212, \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212"}, {"heading": "7 Experimental setting", "text": "We evaluate the characteristic inclusion behavior of the Tensor-based models of Section 6 in three different categories: \"Subject-Subject-Agreement,\" \"Subject-Agreement,\" \"Contract,\" \"Contract,\" \"Contract,\" \"Contract,\" \"Contract,\" \"Contract,\" \"Contract,\" \"Contract,\" \"\" Contract, \"\" \",\" \"\" \"\" Contract, \"\" \"\" \"\", \"\" \"\" \"\" \"\" \"\", \"\" \"\" \"\" \"\", \"\" \"\" \"\" \",\" \"\" \"\" \"\" \"\", \"\" \"\" \"\" \"\" \"\", \"\" \"\" \"\" \"\" \",\" \"\" \"\" \"\" \",\" \"\" \"\" \"\" \",\" \"\" \"\" \"\" \",\" \"\" \"\" \"\", \"\" \"\" \"\" \"\", \"\" \"\" \"\", \"\" \"\" \"\" \"\" \",\" \"\" \"\" \"\" \"\", \"\" \"\" \"\" \"\" \",\" \"\" \"\" \"\" \"\" \",\" \"\" \"\" \"\" \"\" \",\" \"\" \"\" \",\" \"\" \"\" \"\", \"\" \"\" \"\" \",\" \"\" \"\" \"\" \",\" \"\" \"\" \",\" \"\" \"\" \"\" \"\" \"\" \"\", \"\" \",\" \"\" \"\" \"\" \",\" \"\" \"\" \",\" \"\" \"\" \"\" \"\" \"\" \",\", \"\" \"\" \"\" \",\" \"\" \"\", \"\" \"\" \"\", \"\" \"\" \",\" \"\" \"\" \",\" \"\" \"\" \"\" \",\" \"\" \",\" \"\" \"\" \",\" \"\" \"\" \"\" \"\" \"\", \"\" \"\": \",\", \",\", \"\", \",\" \"\" \",\" \",\" \"\" \"\" \",\" \"\" \"\", \"\" \"\" \"\" \"\" \"\" \"\" \"\""}, {"heading": "8 Results and discussion", "text": "The results are presented in Table 1 (Subject-Verb and Verb-Object-Task) and Table 2 (Subject-Verb-Object-Task). In all cases, a combination of a Frobenius tensor model with one of the units of measurement at the sentence level (SAPinc) delivers the highest performance. In general, SAPinc and SBalAPinc work very well with all tested compositional models and achieve cross-model performance in all three tasks that is higher than that of any other measurement variable. From the perspective of feature inclusion, we see that models that use an element of interesectional composition (vector multiplication, MIN, relational and Frobenius tensor models) provide consistent performance in all measured variables. The reason for this may be that intersectional filtering avoids the generation of very dense vectors and thus facilitates tailment judgments based on the DIH. On the other hand, union-based models, these compositional factors, such as MAX or very short tentoric models, are the least likely to generate compositional factors based on DIH."}, {"heading": "9 Conclusion and future work", "text": "In this paper, we examined the application of the distribution hypothesis to the evaluation of the interaction between phrase and sentence vectors generated by compositional operators with an emphasis on tensor-based models. Our results showed that intersectional composition in general, and the Frobenius tensor models in particular, perform best in evaluating monotonous interactions upwards, especially in combination with the scale of the sentence level (Kartsaklis and Sadrzadeh, 2016). Experimenting with different versions of tensor models for interactions is an interesting topic that we intend to pursue in a future paper. Furthermore, the extension of word-based interaction to phrases and sentences establishes links to natural logic (MacCartney and Manning, 2007), a topic that deserves separate treatment and represents a future direction."}, {"heading": "Acknowledgments", "text": "The authors thank the EPSRC for Career Acceleration Fellowship EP / J002607 / 1 and AFOSR International Scientific Collaboration Grant FA9550-14-1-0079 for their support."}], "references": [{"title": "Sentence Entailment in Compositional Distributional Semantics", "author": ["E. Balk\u0131r", "D. Kartsaklis", "M. Sadrzadeh."], "venue": "Proceedings of the International Symposium on Artificial Intelligence and Mathematics (ISAIM), Fort Lauderdale, FL, January.", "citeRegEx": "Balk\u0131r et al\\.,? 2016a", "shortCiteRegEx": "Balk\u0131r et al\\.", "year": 2016}, {"title": "2016b. Topics in Theoretical Computer Science: The First IFIP WG", "author": ["Esma Balk\u0131r", "Mehrnoosh Sadrzadeh", "Bob Coecke"], "venue": "TTCS", "citeRegEx": "Balk\u0131r et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Balk\u0131r et al\\.", "year": 2015}, {"title": "Using density matrices in a compositional distributional model of meaning", "author": ["Esma Balk\u0131r."], "venue": "Master\u2019s thesis, University of Oxford.", "citeRegEx": "Balk\u0131r.,? 2014", "shortCiteRegEx": "Balk\u0131r.", "year": 2014}, {"title": "Graded entailment for compositional distributional semantics", "author": ["Desislava Bankova", "Bob Coecke", "Martha Lewis", "Daniel Marsden."], "venue": "arXiv preprint arXiv:1601.04908.", "citeRegEx": "Bankova et al\\.,? 2016", "shortCiteRegEx": "Bankova et al\\.", "year": 2016}, {"title": "Nouns are Vectors, Adjectives are Matrices", "author": ["M. Baroni", "R. Zamparelli."], "venue": "Proceedings of Conference on Empirical Methods in Natural Language Processing (EMNLP).", "citeRegEx": "Baroni and Zamparelli.,? 2010", "shortCiteRegEx": "Baroni and Zamparelli.", "year": 2010}, {"title": "An empirical study of smoothing techniques for language modeling", "author": ["Stanley F. Chen", "Joshua Goodman."], "venue": "Proceedings of the 34th Annual Meeting on Association for Computational Linguistics, ACL \u201996, pages 310\u2013318, Stroudsburg, PA, USA. Association for Computational Linguistics.", "citeRegEx": "Chen and Goodman.,? 1996", "shortCiteRegEx": "Chen and Goodman.", "year": 1996}, {"title": "Syntax-aware multi-sense word embeddings for deep compositional models of meaning", "author": ["Jianpeng Cheng", "Dimitri Kartsaklis."], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1531\u20131542, Lisbon, Portugal, September. Association for Computational Linguistics.", "citeRegEx": "Cheng and Kartsaklis.,? 2015", "shortCiteRegEx": "Cheng and Kartsaklis.", "year": 2015}, {"title": "Context-theoretic semantics for natural language: an overview", "author": ["Daoud Clarke."], "venue": "Proceedings of the workshop on geometrical models of natural language semantics, pages 112\u2013119. Association for Computational Linguistics.", "citeRegEx": "Clarke.,? 2009", "shortCiteRegEx": "Clarke.", "year": 2009}, {"title": "Mathematical foundations for a compositional distributional model of meaning", "author": ["Bob Coecke", "Mehrnoosh Sadrzadeh", "Stephen Clark."], "venue": "Linguistic Analysis, 36.", "citeRegEx": "Coecke et al\\.,? 2010", "shortCiteRegEx": "Coecke et al\\.", "year": 2010}, {"title": "Similarity-based models of word cooccurrence probabilities", "author": ["Ido Dagan", "Lillian Lee", "Fernando C.N. Pereira."], "venue": "Mach. Learn., 34(1-3):43\u201369.", "citeRegEx": "Dagan et al\\.,? 1999", "shortCiteRegEx": "Dagan et al\\.", "year": 1999}, {"title": "A Synopsis of Linguistic Theory, 1930-1955", "author": ["John R. Firth."], "venue": "Studies in Linguistic Analysis, pages 1\u201332.", "citeRegEx": "Firth.,? 1957", "shortCiteRegEx": "Firth.", "year": 1957}, {"title": "The distributional inclusion hypotheses and lexical entailment", "author": ["Maayan Geffet", "Ido Dagan."], "venue": "Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL\u201905), pages 107\u2013114, Ann Arbor, Michigan, June. Association for Computational Linguistics.", "citeRegEx": "Geffet and Dagan.,? 2005", "shortCiteRegEx": "Geffet and Dagan.", "year": 2005}, {"title": "Experimental support for a categorical compositional distributional model of meaning", "author": ["Edward Grefenstette", "Mehrnoosh Sadrzadeh."], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 1394\u20131404. Association for Computational Linguistics.", "citeRegEx": "Grefenstette and Sadrzadeh.,? 2011", "shortCiteRegEx": "Grefenstette and Sadrzadeh.", "year": 2011}, {"title": "Measuring semantic content in distributional vectors", "author": ["Aur\u00e9lie Herbelot", "Mohan Ganesalingam."], "venue": "Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, volume 2, pages 440\u2013445. Association for Computational Linguistics.", "citeRegEx": "Herbelot and Ganesalingam.,? 2013", "shortCiteRegEx": "Herbelot and Ganesalingam.", "year": 2013}, {"title": "A convolutional neural network for modelling sentences", "author": ["Nal Kalchbrenner", "Edward Grefenstette", "Phil Blunsom."], "venue": "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 655\u2013665, Baltimore, Maryland, June. Association for Computational Linguistics.", "citeRegEx": "Kalchbrenner et al\\.,? 2014", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2014}, {"title": "A study of entanglement in a categorical framework of natural language", "author": ["Dimitri Kartsaklis", "Mehrnoosh Sadrzadeh."], "venue": "B. Coecke, I. Hasuo, and P. Panangaden, editors, Quantum Physics and Logic 2014 (QPL 2014). EPTSC 172, pages 249\u2013261.", "citeRegEx": "Kartsaklis and Sadrzadeh.,? 2014", "shortCiteRegEx": "Kartsaklis and Sadrzadeh.", "year": 2014}, {"title": "A Compositional Distributional Inclusion Hypothesis", "author": ["Dimitri Kartsaklis", "Mehrnoosh Sadrzadeh."], "venue": "Proceedings of the 9th Conference on Logical Aspects of Computational Linguistics (LACL), Lecture Notes in Computer Science. Springer. To appear.", "citeRegEx": "Kartsaklis and Sadrzadeh.,? 2016", "shortCiteRegEx": "Kartsaklis and Sadrzadeh.", "year": 2016}, {"title": "A unified sentence space for categorical distributional-compositional semantics: Theory and experiments", "author": ["Dimitri Kartsaklis", "Mehrnoosh Sadrzadeh", "Stephen Pulman."], "venue": "COLING 2012, 24th International Conference on Computational Linguistics, Proceedings of the Conference: Posters, 8-15 December 2012, Mumbai, India, pages 549\u2013558.", "citeRegEx": "Kartsaklis et al\\.,? 2012", "shortCiteRegEx": "Kartsaklis et al\\.", "year": 2012}, {"title": "Directional distributional similarity for lexical inference", "author": ["Lili Kotlerman", "Ido Dagan", "Idan Szpektor", "Maayan Zhitomirsky-Geffet."], "venue": "Natural Language Engineering, 16(04):359\u2013389.", "citeRegEx": "Kotlerman et al\\.,? 2010", "shortCiteRegEx": "Kotlerman et al\\.", "year": 2010}, {"title": "Measures of distributional similarity", "author": ["Lillian Lee."], "venue": "Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics on Computational Linguistics, pages 25\u201332.", "citeRegEx": "Lee.,? 1999", "shortCiteRegEx": "Lee.", "year": 1999}, {"title": "An information-theoretic definition of similarity", "author": ["Dekang Lin."], "venue": "Proceedings of the International Conference on Machine Learning, pages 296\u2013304.", "citeRegEx": "Lin.,? 1998", "shortCiteRegEx": "Lin.", "year": 1998}, {"title": "Natural logic for textual inference", "author": ["Bill MacCartney", "Christopher D. Manning."], "venue": "ACL Workshop on Textual Entailment and Paraphrasing. Association for Computational Linguistics.", "citeRegEx": "MacCartney and Manning.,? 2007", "shortCiteRegEx": "MacCartney and Manning.", "year": 2007}, {"title": "Evaluating neural word representations in tensor-based compositional settings", "author": ["Dmitrijs Milajevs", "Dimitri Kartsaklis", "Mehrnoosh Sadrzadeh", "Matthew Purver."], "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 708\u2013719, Doha, Qatar, October. Association for Computational Linguistics.", "citeRegEx": "Milajevs et al\\.,? 2014", "shortCiteRegEx": "Milajevs et al\\.", "year": 2014}, {"title": "Composition in distributional models of semantics", "author": ["Jeff Mitchell", "Mirella Lapata."], "venue": "Cognitive Science, 34(8):1388\u20131439.", "citeRegEx": "Mitchell and Lapata.,? 2010", "shortCiteRegEx": "Mitchell and Lapata.", "year": 2010}, {"title": "Semantic compositionality through recursive matrix-vector spaces", "author": ["R. Socher", "B. Huval", "C. Manning", "Ng. A."], "venue": "Conference on Empirical Methods in Natural Language Processing 2012.", "citeRegEx": "Socher et al\\.,? 2012", "shortCiteRegEx": "Socher et al\\.", "year": 2012}, {"title": "From frequency to meaning: Vector space models of semantics", "author": ["Peter D Turney", "Patrick Pantel."], "venue": "Journal of artificial intelligence research, 37(1):141\u2013188.", "citeRegEx": "Turney and Pantel.,? 2010", "shortCiteRegEx": "Turney and Pantel.", "year": 2010}, {"title": "Characterising measures of lexical distributional similarity", "author": ["Julie Weeds", "David Weir", "Diana McCarthy."], "venue": "Proceedings of the 20th international conference on Computational Linguistics, number 1015. Association for Computational Linguistics.", "citeRegEx": "Weeds et al\\.,? 2004", "shortCiteRegEx": "Weeds et al\\.", "year": 2004}], "referenceMentions": [{"referenceID": 10, "context": "1 Introduction Distributional hypothesis asserts that words that often occur in the same contexts have similar meanings (Firth, 1957).", "startOffset": 120, "endOffset": 133}, {"referenceID": 25, "context": "Naturally these models are used extensively to measure the semantic similarity of words (Turney and Pantel, 2010).", "startOffset": 88, "endOffset": 113}, {"referenceID": 9, "context": "Distributional inclusion hypothesis is exactly about such relationships, and particularly, about entailment (Dagan et al., 1999; Geffet and Dagan, 2005; Herbelot and Ganesalingam, 2013).", "startOffset": 108, "endOffset": 185}, {"referenceID": 11, "context": "Distributional inclusion hypothesis is exactly about such relationships, and particularly, about entailment (Dagan et al., 1999; Geffet and Dagan, 2005; Herbelot and Ganesalingam, 2013).", "startOffset": 108, "endOffset": 185}, {"referenceID": 13, "context": "Distributional inclusion hypothesis is exactly about such relationships, and particularly, about entailment (Dagan et al., 1999; Geffet and Dagan, 2005; Herbelot and Ganesalingam, 2013).", "startOffset": 108, "endOffset": 185}, {"referenceID": 16, "context": "In recent work, we have applied the same compositionality principles to lift the entailment relation from word level to phrase/sentence level (Kartsaklis and Sadrzadeh, 2016; Balk\u0131r et al., 2016a; Balk\u0131r et al., 2016b; Balk\u0131r, 2014).", "startOffset": 142, "endOffset": 232}, {"referenceID": 0, "context": "In recent work, we have applied the same compositionality principles to lift the entailment relation from word level to phrase/sentence level (Kartsaklis and Sadrzadeh, 2016; Balk\u0131r et al., 2016a; Balk\u0131r et al., 2016b; Balk\u0131r, 2014).", "startOffset": 142, "endOffset": 232}, {"referenceID": 2, "context": "In recent work, we have applied the same compositionality principles to lift the entailment relation from word level to phrase/sentence level (Kartsaklis and Sadrzadeh, 2016; Balk\u0131r et al., 2016a; Balk\u0131r et al., 2016b; Balk\u0131r, 2014).", "startOffset": 142, "endOffset": 232}, {"referenceID": 2, "context": "The work in (Balk\u0131r, 2014; Balk\u0131r et al., 2016b) was focused on the use of entropic measures on density matrices and compositional operators thereof, but no experimental results were considered; similarly, Bankova et al.", "startOffset": 12, "endOffset": 48}, {"referenceID": 0, "context": "In (Balk\u0131r et al., 2016a), we showed how entropic and other measures can be used on vectors as well as on density matrices and supported this claim with experimental results.", "startOffset": 3, "endOffset": 25}, {"referenceID": 16, "context": "In (Kartsaklis and Sadrzadeh, 2016), we focused on making the distributional inclusion hypothesis compositional, worked", "startOffset": 3, "endOffset": 35}, {"referenceID": 0, "context": "In recent work, we have applied the same compositionality principles to lift the entailment relation from word level to phrase/sentence level (Kartsaklis and Sadrzadeh, 2016; Balk\u0131r et al., 2016a; Balk\u0131r et al., 2016b; Balk\u0131r, 2014). The work in (Balk\u0131r, 2014; Balk\u0131r et al., 2016b) was focused on the use of entropic measures on density matrices and compositional operators thereof, but no experimental results were considered; similarly, Bankova et al. (2016) use a specific form of density matrices to represent words for entailment purposes, focusing only on theory.", "startOffset": 175, "endOffset": 462}, {"referenceID": 23, "context": "One can broadly classify the compositional distributional models to three categories: ones that are based on simple element-wise operations between vectors, such as addition and multiplication (Mitchell and Lapata, 2010); tensor-based models in which relational words such as verbs and adjectives are tensors and matrices contracting and multiplying with noun (and noun-phrase) vectors (Coecke et al.", "startOffset": 193, "endOffset": 220}, {"referenceID": 8, "context": "One can broadly classify the compositional distributional models to three categories: ones that are based on simple element-wise operations between vectors, such as addition and multiplication (Mitchell and Lapata, 2010); tensor-based models in which relational words such as verbs and adjectives are tensors and matrices contracting and multiplying with noun (and noun-phrase) vectors (Coecke et al., 2010; Grefenstette and Sadrzadeh, 2011; Baroni and Zamparelli, 2010); and models in which the compositional operator is part of a neural network (Socher et al.", "startOffset": 386, "endOffset": 470}, {"referenceID": 12, "context": "One can broadly classify the compositional distributional models to three categories: ones that are based on simple element-wise operations between vectors, such as addition and multiplication (Mitchell and Lapata, 2010); tensor-based models in which relational words such as verbs and adjectives are tensors and matrices contracting and multiplying with noun (and noun-phrase) vectors (Coecke et al., 2010; Grefenstette and Sadrzadeh, 2011; Baroni and Zamparelli, 2010); and models in which the compositional operator is part of a neural network (Socher et al.", "startOffset": 386, "endOffset": 470}, {"referenceID": 4, "context": "One can broadly classify the compositional distributional models to three categories: ones that are based on simple element-wise operations between vectors, such as addition and multiplication (Mitchell and Lapata, 2010); tensor-based models in which relational words such as verbs and adjectives are tensors and matrices contracting and multiplying with noun (and noun-phrase) vectors (Coecke et al., 2010; Grefenstette and Sadrzadeh, 2011; Baroni and Zamparelli, 2010); and models in which the compositional operator is part of a neural network (Socher et al.", "startOffset": 386, "endOffset": 470}, {"referenceID": 24, "context": ", 2010; Grefenstette and Sadrzadeh, 2011; Baroni and Zamparelli, 2010); and models in which the compositional operator is part of a neural network (Socher et al., 2012; Kalchbrenner et al., 2014) and is usually optimized against a specific objective.", "startOffset": 147, "endOffset": 195}, {"referenceID": 14, "context": ", 2010; Grefenstette and Sadrzadeh, 2011; Baroni and Zamparelli, 2010); and models in which the compositional operator is part of a neural network (Socher et al., 2012; Kalchbrenner et al., 2014) and is usually optimized against a specific objective.", "startOffset": 147, "endOffset": 195}, {"referenceID": 12, "context": "Models of this form have been used in the past with success in a number of NLP tasks, such as head-verb disambiguation (Grefenstette and Sadrzadeh, 2011), term-definition classification (Kartsaklis et al.", "startOffset": 119, "endOffset": 153}, {"referenceID": 17, "context": "Models of this form have been used in the past with success in a number of NLP tasks, such as head-verb disambiguation (Grefenstette and Sadrzadeh, 2011), term-definition classification (Kartsaklis et al., 2012), and generic sentence similarity (Kartsaklis and Sadrzadeh, 2014).", "startOffset": 186, "endOffset": 211}, {"referenceID": 15, "context": ", 2012), and generic sentence similarity (Kartsaklis and Sadrzadeh, 2014).", "startOffset": 41, "endOffset": 73}, {"referenceID": 12, "context": "In this paper we extend this work to entailment, by investigating (theoretically and experimentally) the properties of feature inclusion on the phrase and sentence vectors produced in four different tensor-based compositional distributional models: the relational models of (Grefenstette and Sadrzadeh, 2011), the Frobenius models of (Kartsaklis et al.", "startOffset": 274, "endOffset": 308}, {"referenceID": 17, "context": "In this paper we extend this work to entailment, by investigating (theoretically and experimentally) the properties of feature inclusion on the phrase and sentence vectors produced in four different tensor-based compositional distributional models: the relational models of (Grefenstette and Sadrzadeh, 2011), the Frobenius models of (Kartsaklis et al., 2012; Milajevs et al., 2014), the projective models of (Kartsaklis and Sadrzadeh, 2016), and the holistic linear regression model of (Baroni and Zamparelli, 2010).", "startOffset": 334, "endOffset": 382}, {"referenceID": 22, "context": "In this paper we extend this work to entailment, by investigating (theoretically and experimentally) the properties of feature inclusion on the phrase and sentence vectors produced in four different tensor-based compositional distributional models: the relational models of (Grefenstette and Sadrzadeh, 2011), the Frobenius models of (Kartsaklis et al., 2012; Milajevs et al., 2014), the projective models of (Kartsaklis and Sadrzadeh, 2016), and the holistic linear regression model of (Baroni and Zamparelli, 2010).", "startOffset": 334, "endOffset": 382}, {"referenceID": 16, "context": ", 2014), the projective models of (Kartsaklis and Sadrzadeh, 2016), and the holistic linear regression model of (Baroni and Zamparelli, 2010).", "startOffset": 34, "endOffset": 66}, {"referenceID": 4, "context": ", 2014), the projective models of (Kartsaklis and Sadrzadeh, 2016), and the holistic linear regression model of (Baroni and Zamparelli, 2010).", "startOffset": 112, "endOffset": 141}, {"referenceID": 16, "context": "A newly proposed adaptation of these metrics, recently introduced by the authors in (Kartsaklis and Sadrzadeh, 2016), is also detailed.", "startOffset": 84, "endOffset": 116}, {"referenceID": 16, "context": "We experiment with these models and evaluate them on entailment relations between simple intransitive sentences, verb phrases, and transitive sentences on the datasets of (Kartsaklis and Sadrzadeh, 2016).", "startOffset": 171, "endOffset": 203}, {"referenceID": 16, "context": "On a more general note, the experimental results of this paper support that of previous work (Kartsaklis and Sadrzadeh, 2016) and strongly indicate that compositional models employing some form of intersective feature selection, i.", "startOffset": 93, "endOffset": 125}, {"referenceID": 4, "context": ", 2010; Grefenstette and Sadrzadeh, 2011; Baroni and Zamparelli, 2010); and models in which the compositional operator is part of a neural network (Socher et al., 2012; Kalchbrenner et al., 2014) and is usually optimized against a specific objective. Tensor-based models stand in between the two extremes of element-wise vector mixing and neural net-based methods, offering a sufficiently powerful alternative that allows for theoretical reasoning at a level deeper than it is usually possible with black-box statistical approaches. Models of this form have been used in the past with success in a number of NLP tasks, such as head-verb disambiguation (Grefenstette and Sadrzadeh, 2011), term-definition classification (Kartsaklis et al., 2012), and generic sentence similarity (Kartsaklis and Sadrzadeh, 2014). In this paper we extend this work to entailment, by investigating (theoretically and experimentally) the properties of feature inclusion on the phrase and sentence vectors produced in four different tensor-based compositional distributional models: the relational models of (Grefenstette and Sadrzadeh, 2011), the Frobenius models of (Kartsaklis et al., 2012; Milajevs et al., 2014), the projective models of (Kartsaklis and Sadrzadeh, 2016), and the holistic linear regression model of (Baroni and Zamparelli, 2010). Contrary to formal semantic models, and customary to distributional models, our entailments are nonboolean and come equipped with degrees. We review a number of measures that have been developed for evaluating degrees of entailment at the lexical level, such as the APinc measure and its newer version, balAPinc of Kotlerman et al. (2010), which is considered as state-of-the-art for word-level entailment.", "startOffset": 42, "endOffset": 1669}, {"referenceID": 23, "context": "In the simplest case, this is done by element-wise operations on the vectors of the words (Mitchell and Lapata, 2010).", "startOffset": 90, "endOffset": 117}, {"referenceID": 8, "context": "These are then applied to the vectors of their arguments by following the rules of the grammar (Coecke et al., 2010; Grefenstette and Sadrzadeh, 2011; Baroni and Zamparelli, 2010).", "startOffset": 95, "endOffset": 179}, {"referenceID": 12, "context": "These are then applied to the vectors of their arguments by following the rules of the grammar (Coecke et al., 2010; Grefenstette and Sadrzadeh, 2011; Baroni and Zamparelli, 2010).", "startOffset": 95, "endOffset": 179}, {"referenceID": 4, "context": "These are then applied to the vectors of their arguments by following the rules of the grammar (Coecke et al., 2010; Grefenstette and Sadrzadeh, 2011; Baroni and Zamparelli, 2010).", "startOffset": 95, "endOffset": 179}, {"referenceID": 24, "context": "\u2212\u2192 svo = (verb \u00d7 \u2212\u2192 obj)\u00d7 \u2212\u2212\u2192 subj Finally, phrase and sentence vectors have been also produced by the application of neural architectures, such as recursive or recurrent neural networks (Socher et al., 2012; Cheng and Kartsaklis, 2015) and convolutional neural networks (Kalchbrenner et al.", "startOffset": 187, "endOffset": 236}, {"referenceID": 6, "context": "\u2212\u2192 svo = (verb \u00d7 \u2212\u2192 obj)\u00d7 \u2212\u2212\u2192 subj Finally, phrase and sentence vectors have been also produced by the application of neural architectures, such as recursive or recurrent neural networks (Socher et al., 2012; Cheng and Kartsaklis, 2015) and convolutional neural networks (Kalchbrenner et al.", "startOffset": 187, "endOffset": 236}, {"referenceID": 14, "context": ", 2012; Cheng and Kartsaklis, 2015) and convolutional neural networks (Kalchbrenner et al., 2014).", "startOffset": 70, "endOffset": 97}, {"referenceID": 9, "context": "3 Distributional inclusion hypothesis The distributional inclusion hypothesis (DIH) (Dagan et al., 1999; Geffet and Dagan, 2005; Herbelot and Ganesalingam, 2013) is based on the fact that whenever a word u entails a word v, then it makes sense to replace instances of u with v.", "startOffset": 84, "endOffset": 161}, {"referenceID": 11, "context": "3 Distributional inclusion hypothesis The distributional inclusion hypothesis (DIH) (Dagan et al., 1999; Geffet and Dagan, 2005; Herbelot and Ganesalingam, 2013) is based on the fact that whenever a word u entails a word v, then it makes sense to replace instances of u with v.", "startOffset": 84, "endOffset": 161}, {"referenceID": 13, "context": "3 Distributional inclusion hypothesis The distributional inclusion hypothesis (DIH) (Dagan et al., 1999; Geffet and Dagan, 2005; Herbelot and Ganesalingam, 2013) is based on the fact that whenever a word u entails a word v, then it makes sense to replace instances of u with v.", "startOffset": 84, "endOffset": 161}, {"referenceID": 11, "context": "Despite this and other limitations, the DIH has been subject to a good amount of study in the distributional semantics community and its predictions have been validated (Geffet and Dagan, 2005; Kotlerman et al., 2010).", "startOffset": 169, "endOffset": 217}, {"referenceID": 18, "context": "Despite this and other limitations, the DIH has been subject to a good amount of study in the distributional semantics community and its predictions have been validated (Geffet and Dagan, 2005; Kotlerman et al., 2010).", "startOffset": 169, "endOffset": 217}, {"referenceID": 5, "context": "Examples of measures used here are entropy-based measures such as KL-divergence (Chen and Goodman, 1996).", "startOffset": 80, "endOffset": 104}, {"referenceID": 19, "context": "To overcome this restriction, a variant referred to by \u03b1-skew (Lee, 1999) has been proposed (for \u03b1 \u2208 (0, 1] a smoothing parameter).", "startOffset": 62, "endOffset": 73}, {"referenceID": 25, "context": "Measures developed by Weeds et al. (2004) and Clarke (2009) advance this simple methods by arguing that not all features play an equal role in representing words and hence they should not be treated equally when it comes to measuring entailment.", "startOffset": 22, "endOffset": 42}, {"referenceID": 7, "context": "(2004) and Clarke (2009) advance this simple methods by arguing that not all features play an equal role in representing words and hence they should not be treated equally when it comes to measuring entailment.", "startOffset": 11, "endOffset": 25}, {"referenceID": 18, "context": "Pertinence is computed by various different measures, the most recent of which is balAPinc (Kotlerman et al., 2010), where LIN is Lin\u2019s similarity (Lin, 1998) and APinc is an asymmetric measure:", "startOffset": 91, "endOffset": 115}, {"referenceID": 20, "context": ", 2010), where LIN is Lin\u2019s similarity (Lin, 1998) and APinc is an asymmetric measure:", "startOffset": 39, "endOffset": 50}, {"referenceID": 16, "context": "4 Measuring feature inclusion at the phrase/sentence level In recent work, the authors of this paper introduced a variation of the APinc and balAPinc measures aiming to address the extra complications imposed when evaluating entailment at the phrase/sentence level (Kartsaklis and Sadrzadeh, 2016).", "startOffset": 265, "endOffset": 297}, {"referenceID": 18, "context": "According to (Kotlerman et al., 2010), the rationale of including a symmetric measure was that APinc tends to return unjustifyingly high scores when the entailing word is infrequent, that is, when the feature vector of the entailing word is very short; the purpose of the symmetric measure was to penalize the result, since in this case the similarity of the narrower term with the broader one is usually low.", "startOffset": 13, "endOffset": 37}, {"referenceID": 16, "context": "As shown in (Kartsaklis and Sadrzadeh, 2016), element-wise composition of this form lifts naturally from the word level to phrase/sentence level; specifically, for two sentences s1 = u1 .", "startOffset": 12, "endOffset": 44}, {"referenceID": 0, "context": "This kind of lifting of the entailment relationship from words to the phrase/sentence level also holds for tensor-based models (Balk\u0131r et al., 2016a).", "startOffset": 127, "endOffset": 149}, {"referenceID": 12, "context": "1 Relational model As a starting point we will use the model of Grefenstette and Sadrzadeh (2011), which adopts an extensional approach and builds the tensor of a relational word from the vectors of its arguments.", "startOffset": 64, "endOffset": 98}, {"referenceID": 12, "context": "For a transitive sentence, the model of Grefenstette and Sadrzadeh (2011) returns a matrix, computed in the following way:", "startOffset": 40, "endOffset": 74}, {"referenceID": 17, "context": "2 Frobenius models As pointed out in (Kartsaklis et al., 2012), the disadvantage of the relational model is that their resulting representations of verbs have one dimension less than what their types dictate.", "startOffset": 37, "endOffset": 62}, {"referenceID": 17, "context": "A solution presented in (Kartsaklis et al., 2012) suggested the use of Frobenius operators in order to expand vectors and matrices into higher order tensors by embedding them into the the diagonals.", "startOffset": 24, "endOffset": 49}, {"referenceID": 15, "context": "Below we define the feature sets of two variations, where this combination is achieved via addition (we refer to this model as Frobenius additive) and element-wise multiplication (Frobenius multiplicative) of the vectors produced by the individual models (Kartsaklis and Sadrzadeh, 2014):", "startOffset": 255, "endOffset": 287}, {"referenceID": 16, "context": "We use the entailment datasets introduced in (Kartsaklis and Sadrzadeh, 2016), which consist of 135 subject-verb pairs, 218 verb-object pairs, and 70 subject-verbobject pairs, the phrases/sentences of which stand in a fairly clear entailment relationship.", "startOffset": 45, "endOffset": 77}, {"referenceID": 16, "context": "We experimented with a variety of entailment measures, including SAPinc and SBalAPinc as in (Kartsaklis and Sadrzadeh, 2016), their word-level counterparts (Kotlerman et al.", "startOffset": 92, "endOffset": 124}, {"referenceID": 18, "context": "We experimented with a variety of entailment measures, including SAPinc and SBalAPinc as in (Kartsaklis and Sadrzadeh, 2016), their word-level counterparts (Kotlerman et al., 2010), KL-divergence (applied to smoothed vectors as in Chen and Goodman (1996)), \u03b1-skew with \u03b1 = 0.", "startOffset": 156, "endOffset": 180}, {"referenceID": 4, "context": "We also present results for a least squares fitting model, which approximates the distributional behaviour of holistic phrase/sentence vectors along the lines of (Baroni and Zamparelli, 2010).", "startOffset": 162, "endOffset": 191}, {"referenceID": 4, "context": ", 2010), KL-divergence (applied to smoothed vectors as in Chen and Goodman (1996)), \u03b1-skew with \u03b1 = 0.", "startOffset": 58, "endOffset": 82}, {"referenceID": 4, "context": ", 2010), KL-divergence (applied to smoothed vectors as in Chen and Goodman (1996)), \u03b1-skew with \u03b1 = 0.99 as in Kotlerman et al. (2010), WeedsPrec as in Weeds et al.", "startOffset": 58, "endOffset": 135}, {"referenceID": 4, "context": ", 2010), KL-divergence (applied to smoothed vectors as in Chen and Goodman (1996)), \u03b1-skew with \u03b1 = 0.99 as in Kotlerman et al. (2010), WeedsPrec as in Weeds et al. (2004), and ClarkeDE as in Clarke (2009).", "startOffset": 58, "endOffset": 172}, {"referenceID": 4, "context": ", 2010), KL-divergence (applied to smoothed vectors as in Chen and Goodman (1996)), \u03b1-skew with \u03b1 = 0.99 as in Kotlerman et al. (2010), WeedsPrec as in Weeds et al. (2004), and ClarkeDE as in Clarke (2009). We use strict feature inclusion as a baseline; in this case, entailment holds only when F( \u2212\u2212\u2212\u2212\u2192 phrase1) \u2286 F( \u2212\u2212\u2212\u2212\u2192 phrase2).", "startOffset": 58, "endOffset": 206}, {"referenceID": 16, "context": "Our results showed that intersective composition in general, and the Frobenius tensor models in particular, achieve the best performance when evaluating upward monotone entailment, especially when combined with the sentence-level measures of (Kartsaklis and Sadrzadeh, 2016).", "startOffset": 242, "endOffset": 274}, {"referenceID": 21, "context": "Furthermore, the extension of word-level entailment to phrases and sentences provides connections with natural logic (MacCartney and Manning, 2007), a topic that is worth a separate treatment and constitutes a future direction.", "startOffset": 117, "endOffset": 147}], "year": 2016, "abstractText": "According to the distributional inclusion hypothesis, entailment between words can be measured via the feature inclusions of their distributional vectors. In recent work, we showed how this hypothesis can be extended from words to phrases and sentences in the setting of compositional distributional semantics. This paper focuses on inclusion properties of tensors; its main contribution is a theoretical and experimental analysis of how feature inclusion works in different concrete models of verb tensors. We present results for relational, Frobenius, projective, and holistic methods and compare them to the simple vector addition, multiplication, min, and max models. The degrees of entailment thus obtained are evaluated via a variety of existing wordbased measures, such as Weed\u2019s and Clarke\u2019s, KL-divergence, APinc, balAPinc, and two of our previously proposed metrics at the phrase/sentence level. We perform experiments on three entailment datasets, investigating which version of tensor-based composition achieves the highest performance when combined with the sentence-level measures.", "creator": "dvips(k) 5.991 Copyright 2011 Radical Eye Software"}}}