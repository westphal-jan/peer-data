{"id": "1610.04631", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Oct-2016", "title": "A Harmonic Mean Linear Discriminant Analysis for Robust Image Classification", "abstract": "Linear Discriminant Analysis (LDA) is a widely-used supervised dimensionality reduction method in computer vision and pattern recognition. In null space based LDA (NLDA), a well-known LDA extension, between-class distance is maximized in the null space of the within-class scatter matrix. However, there are some limitations in NLDA. Firstly, for many data sets, null space of within-class scatter matrix does not exist, thus NLDA is not applicable to those datasets. Secondly, NLDA uses arithmetic mean of between-class distances and gives equal consideration to all between-class distances, which makes larger between-class distances can dominate the result and thus limits the performance of NLDA. In this paper, we propose a harmonic mean based Linear Discriminant Analysis, Multi-Class Discriminant Analysis (MCDA), for image classification, which minimizes the reciprocal of weighted harmonic mean of pairwise between-class distance. More importantly, MCDA gives higher priority to maximize small between-class distances. MCDA can be extended to multi-label dimension reduction. Results on 7 single-label data sets and 4 multi-label data sets show that MCDA has consistently better performance than 10 other single-label approaches and 4 other multi-label approaches in terms of classification accuracy, macro and micro average F1 score.", "histories": [["v1", "Fri, 14 Oct 2016 20:36:57 GMT  (1784kb)", "http://arxiv.org/abs/1610.04631v1", "IEEE 28th International Conference on Tools with Artificial Intelligence, ICTAI 2016"], ["v2", "Mon, 24 Oct 2016 16:38:29 GMT  (221kb,D)", "http://arxiv.org/abs/1610.04631v2", "IEEE 28th International Conference on Tools with Artificial Intelligence, ICTAI 2016"]], "COMMENTS": "IEEE 28th International Conference on Tools with Artificial Intelligence, ICTAI 2016", "reviews": [], "SUBJECTS": "cs.CV cs.AI", "authors": ["shuai zheng", "feiping nie", "chris ding", "heng huang"], "accepted": false, "id": "1610.04631"}, "pdf": {"name": "1610.04631.pdf", "metadata": {"source": "CRF", "title": "A Harmonic Mean Linear Discriminant Analysis for Robust Image Classification", "authors": ["Shuai Zheng", "Feiping Nie", "Chris Ding", "Heng Huang"], "emails": ["zhengs123@gmail.com,", "feipingnie@gmail.com,", "chqding@uta.edu,", "heng@uta.edu"], "sections": [{"heading": null, "text": "In biology, researchers and engineers nowadays have larger and larger data with very high dimensions that need to be processed on a daily basis [1]. However, there is always an underlying low-dimensional structure that can capture the underlying main characteristics of high-dimensional data. What's more, high-dimensional data costs a lot of computer resources. Dimensionality reduction algorithms have been proposed to extract important features from high-dimensional data. Dimensionality reduction algorithms have been proposed to extract high-dimensional features from high-dimensional data. Dimensionality reductions are important in many applications of statistics, pattern recognition and machine learning."}, {"heading": "II. LIMITATIONS OF NLDA", "text": "NLDA has some limitations. First, for many datasets in which p is the data dimension, there is no zero space of Sw, where n is the sample number, K is the class number. Thus, the zero space dimension of Sw is at least p \u2212 (n \u2212 K). If p \u2264 (n \u2212 K) does not exist, the zero space of Sw cannot exist. Second, NLDA takes into account all class distances equally, whereby larger class distances can dominate the objective function and thus limit the performance of NLDA. NLDA solves the problem of the equation (5), which maximizes the distance between mk (the center of class k) and m (the center of all data). However, there is equal consideration between the class distances. Figure 1 shows two solutions on a toy datas.These toy data have 3 classes and each class contains 10 points. Figure 1GDA-1 shows the distance from S1 class S1, where S1 is the zero class."}, {"heading": "III. A HARMONIC MEAN BETWEEN-CLASS DISTANCE OBJECTIVE", "text": "As we can see from the demonstration in Figure 1, pairs between class distances play an important role in the result. Figure 1b is a better solution than Figure 1a, because all 3 classes in the solution are clearly separated and no two classes are too close to each other. To achieve this goal, we introduce the use of pairs between class distances. To include pairs between class distances in our goal, we define pairs between class distances Bk1k2 for classes k1 and k2 as: Bk1k2 = (mk1 \u2212 mk2) T - we want to maximize pairs of class distances between class distances. Instead of the traditional approach of maxTr (GTSbG) in Eq. (5), we maximize all pairs of class distance 1: max. kK \u2212 kkkkkkkk.kkkk.kkkkk.kkkkk.kkkk.kkkk.kkkkk.kkkk.kkkk.kkkkk.kkkk1 = kkkkkkkkkkkkkkkk.k.k.kkkk.kk.kkkk2 = kkkkkkkkkkkk:::: kkkkkkkkkkkkkkkkkkkkkkkkk::: kkkkkkkkkkkkkkk:::: kkkkkkkkkkkkk:::: kkk: kkkkkkkkkkk: kk:: kk: kk: kk: kkk = kk: kk = kk = kk = kk = kkk = kk = kk = kk = kk: kk = kk = kk = kk = kk = kk = kk = kk = kk = kk = kk = kk = kk = kk = kk = kk = kk = kk = kk = kk = kk = kk = kk = kk = kk = kk = kk = kk"}, {"heading": "IV. MULTI-CLASS DISCRIMINANT ANALYSIS (MCDA)", "text": "In this section we propose a generalized, efficient and stable LDA formulation, Multi-Class Discriminant Analysis (MCDA) = 1. Since in NLDA for many applications the zero space of the scatter matrix within the class does not exist, which means that there is no such transformation matrix G, so GTSwG = I. (9) Algorithm 1 Gradient Descent Algorithm for MCDA.Input: Data matrix X p \u00b7 n with n data points and the P dimension; Class indicator matrix Y n \u00d7 K, K is the number of classes; Subspace dimension k Output: Projection matrix G p \u00b7 k1: Initialization G2: Update Sw and Bk1k2 using Eq (GTK \u00d7 K, K is the number of classes; Subspace dimension Output: Projection matrix G \u00b7 k1: Initialization of class G1)."}, {"heading": "V. ALGORITHM", "text": "We use the parentage of Eq. (10) The gradient is: \u2202 JMCDA \u2202 G = 2\u03b3SwG \u2212 K \u2211 k1 = 1K \u2211 k2 = k1 + 12nk2Bk2G (TrGTBk2G) 2. (11) To enforce the constraints, we use SVD decomposition to make G orthonormally every few iterations.) If we use SVD in every iteration, it becomes expensive. Thus, we can apply SVD every few iterations that are on projection matrix G (subspace dimension is very small), and is very fast."}, {"heading": "VII. CONNECTION WITH TRACE RATIO", "text": "The track ratio was proposed in [6], [7], [8] to solve the following problem: Max G Tr (GTSbG) Tr (GTStG) s.t. GTG = I. (12) Since Equation (12) maximizes Tr (GTSbG), it also maximizes the arithmetic mean of the distance between classes and also suffers from the robustness problem discussed in Equation (7). The track ratio can be reduced to NLDA if the reduced partial dimension k is not greater than the dimension of the zero space of Sw. Using St = Sb + Sw equals Equation (12) maxGTr (GTSbG) Tr (GTSbG) + Tr (GTSwG) = maxG 11 + Tr (GTSwG) / Tr (GTSwG) / Tr (GTSw).Since Sb, Sw and St are all semi-positive, the maximum target value can be reached if this class is within the SW."}, {"heading": "VIII. MULTI-LABEL MCDA", "text": "In the image and video annotation, each image is usually associated with several different word classes. However, let's take two sample images from the MSRC data in Figure 4 as an example. Figure 4a is commented with 3 words: sky, plane, and grass; Figure 4b is commented with 3 words: car, buildings, road. In machine learning, each data point is classified into only one category. (Multi-label multi-class problem is more generalized than single-label multi-class problem.An important difference between single-label classification and multi-label classification is that class memberships in single-label classification are mutually exclusive, while class memberships in multi-label classification are overlapped with 2 or more classes."}, {"heading": "IX. EXPERIMENTS", "text": "In this section, we will first examine the convergence of algorithm 1 and the effect of the reduced dimensional number of MCDA. Then, we will compare the classification accuracy of MCDA with 10 other subspace learning algorithms using the reduced dimensional number K \u2212 1 (K is class number). Finally, we will experiment with classification accuracy and the macro and micro-average F score for multi-label datasets. Caltech101 [14] contains 101 object categories. We will then use VLFeat [15] to extract the HOG function. Caltech07-HOG contains 7 categories randomly selected from Caltech101, and each category has 30 images. Caltech20-HOG contains 20 categories randomly selected from Caltech101, and each category has 30 images. MSRC [16] comes from the MSRC database v1 and contains 7 classes of 30 images in each class."}, {"heading": "A. Convergence of Algorithm 1", "text": "To verify the convergence of algorithm 1, we use the first 4 single mark data sets, Caltech07HOG, Caltech20-HOG, MSRC-HOG, MSRC-GIST, as examples. To find a reasonable guess for \u03b3, the first part \u03b3Tr (GTSwG) of equation (10) and the second part \u2211 K \u2212 1k1 = 1 \u2211 K k2 = k1 + 1nk1nk1nk2 Tr (GTBk1k2G) should be on a similar scale. To get an approximate value, we insert G = I into equation (10) if \u03b3Tr (GTSwG) = \u2211 K \u2212 1 k1 k1 k1 = 1 \u2211 K k2 = 1 \u2211 K k1 = k1 + 1nk1nk2 TrBk1k2 TrBk1k2 Tr (GTBk1k2G), so that we can see G = 1TrSwK \u2212 1 \u2211 k2 = k1 + n2 Trk2 Trk2 Trk2 Trk2 Trk2 Trk2 (GTB1k2G) during the Convergence of Algorithm 1, we can quickly verify the convergence of algorithm 1."}, {"heading": "B. Effect of subspace dimension", "text": "Standard LDA can find subspace dimensions from 1 to K \u2212 1. MCDA has no dimensional limitation. Therefore, in this part we examine the performance of the MCDA subspace classification in relation to the subspace dimension and compare the performance with standard LDA and the trace ratio. For standard LDA, we only calculate a reduced dimension from 1 to K \u2212 1. After using the dimension reduction, KNN classifier (knn = 3) is used for the classification. Classification accuracy is the average of the 5x cross validation results. Figure 6 shows the classification accuracy of MCDA, LDA and Trace Ratio. The result shows that MCDA has a higher classification accuracy than LDA and Trace Ratio when the same number of dimensions is used for all 4 datasets."}, {"heading": "C. Single-label classification experiment", "text": "On 7 single-label data sets, we compare MCDA with 10 other methods, including LDA, Zero Margin LDA (NLDA) [5], Trace Ratio LDA (TraceRatio) [6], Semi Definite Positive LDA (spdLDA) [21], Maximum Margin Criteria (MMC) [22], LDA (RLDA) [23], Uncorrelated LDA (ULDA) [24], Orthogonal LDA (OLDA) [24], Orthogonal Centroid Method (OCM) [25] and Orthogonal Least Squares LDA (OLSLDA) [26]. In the experiment, we reduce the raw data to K \u2212 1 dimension. We use 5-fold cross-validation to select training and test data. After selecting training and test data, we adjust the binary parameters based on the selected training data."}, {"heading": "D. Multi-label classification experiment", "text": "We compare the performance of multi-label MCDA with 4 other multi-label algorithms on 4 multi-label data sets in terms of macro accuracy (Table IV), macro-averaged F1score (Table V), and micro-averaged F1 score (Table VI). F1 score is defined as: F1 = 2 x precision x recall accuracy + recall. Macro-averaged is the mean based on the entire test data set, while micro-averaged is the mean that gives each class the same weight. Macro-averaged and micro-averaged F1 score are widely used as a measurement to evaluate classification performance [27]. Multi-label Least Square (MLLS) algorithms include Multi-Informed Latent Semantic Indexing (MLSI) [9], Multi-label Dimensionality Reduction via Dependence Maximization (MDDM) [Laast Square], Multi-Label (Least) [10] (Least)."}, {"heading": "X. RELATED WORK", "text": "Researchers and engineers today have larger and larger data with very high dimensions that need to be processed every day (DA DA = DA 36). Many big data technologies such as cloud computing, dimension reduction, acceleration algorithms have been proposed [28] [29] [31] [32] [33]. The trace ratio problem has been studied in detail in recent years. Many dimension reduction algorithms can be reduced to a trace ratio lens, but the trace ratio problem has no solution in a closed form. Thus, the trace ratio problem efficiently becomes an interesting research topic. Wang [6] proposed an efficient iterative algorithm to obtain an approximate solution. Shen [34] proposed a formulation to solve the trace ratio problem directly. Never proposed a trace ratio criterion the selection of features. Each feature subset has a feature score that is calculated by an algorithmic subset to optimize the global trace ratio."}, {"heading": "XI. CONCLUSIONS", "text": "In this paper, we propose a harmonic mean based on Linear Discriminant Analysis, Multi-Class Discriminant Analysis (MCDA), which uses the weighted harmonic mean of the paired distance between classes and gives higher priority to maximizing small distances between classes. MCDA has been expanded to reduce dimensions with multiple labels. Extensive experiments on 7 single-label data sets and 4 multi-label data sets show that MCDA consistently outperforms 10 other single-label algorithms and 4 multi-label algorithms in terms of classification accuracy, macro and micro-mean F1 values."}, {"heading": "ACKNOWLEDGMENT", "text": "This work is partially supported by NSF1356628, NSF1633753."}], "references": [{"title": "A survey of dimension reduction techniques", "author": ["I.K. Fodor"], "venue": "2002.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2002}, {"title": "Dimension reduction for classification with gene expression microarray data", "author": ["J.J. Dai", "L. Lieu", "D. Rocke"], "venue": "Statistical applications in genetics and molecular biology, vol. 5, no. 1, 2006.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2006}, {"title": "Principal component analysis", "author": ["I. Jolliffe"], "venue": "Wiley Online Library,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2002}, {"title": "Introduction to statistical pattern recognition", "author": ["K. Fukunaga"], "venue": "Academic press,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2013}, {"title": "A new lda-based face recognition system which can solve the small sample size problem", "author": ["L.-F. Chen", "H.-Y.M. Liao", "M.-T. Ko", "J.-C. Lin", "G.-J. Yu"], "venue": "Pattern recognition, vol. 33, no. 10, pp. 1713\u20131726, 2000.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2000}, {"title": "Trace ratio vs. ratio trace for dimensionality reduction", "author": ["H. Wang", "S. Yan", "D. Xu", "X. Tang", "T. Huang"], "venue": "Computer Vision and Pattern Recognition, 2007. CVPR\u201907. IEEE Conference on. IEEE, 2007, pp. 1\u20138.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2007}, {"title": "Trace ratio problem revisited", "author": ["Y. Jia", "F. Nie", "C. Zhang"], "venue": "Neural Networks, IEEE Transactions on, vol. 20, no. 4, pp. 729\u2013735, 2009.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2009}, {"title": "Neighborhood minmax projections.", "author": ["F. Nie", "S. Xiang", "C. Zhang"], "venue": "in IJCAI,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2007}, {"title": "Multi-label informed latent semantic indexing", "author": ["K. Yu", "S. Yu", "V. Tresp"], "venue": "Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval. ACM, 2005, pp. 258\u2013265.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2005}, {"title": "Multilabel dimensionality reduction via dependence maximization", "author": ["Y. Zhang", "Z.-H. Zhou"], "venue": "ACM Transactions on Knowledge Discovery from Data (TKDD), vol. 4, no. 3, p. 14, 2010.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2010}, {"title": "Extracting shared subspace for multi-label classification", "author": ["S. Ji", "L. Tang", "S. Yu", "J. Ye"], "venue": "Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining. ACM, 2008, pp. 381\u2013389.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2008}, {"title": "Multi-label linear discriminant analysis", "author": ["H. Wang", "C. Ding", "H. Huang"], "venue": "Computer Vision\u2013ECCV 2010. Springer, 2010, pp. 126\u2013139.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2010}, {"title": "Semi-supervised orthogonal discriminant analysis via label propagation", "author": ["F. Nie", "S. Xiang", "Y. Jia", "C. Zhang"], "venue": "Pattern Recognition, vol. 42, no. 11, pp. 2615\u20132627, 2009.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2009}, {"title": "Learning generative visual models from few training examples: An incremental bayesian approach tested on 101 object categories", "author": ["L. Fei-Fei", "R. Fergus", "P. Perona"], "venue": "Computer Vision and Image Understanding, vol. 106, no. 1, pp. 59\u201370, 2007.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2007}, {"title": "Vlfeat: An open and portable library of computer vision algorithms", "author": ["A. Vedaldi", "B. Fulkerson"], "venue": "Proceedings of the international conference on Multimedia. ACM, 2010, pp. 1469\u20131472.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2010}, {"title": "Locus: Learning object classes with unsupervised segmentation", "author": ["J. Winn", "N. Jojic"], "venue": "Computer Vision, 2005. ICCV 2005. Tenth IEEE International Conference on, vol. 1. IEEE, 2005, pp. 756\u2013763.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2005}, {"title": "Parameterisation of a stochastic model for human face identification", "author": ["F.S. Samaria", "A.C. Harter"], "venue": "Applications of Computer Vision, 1994., Proceedings of the Second IEEE Workshop on. IEEE, 1994, pp. 138\u2013142.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1994}, {"title": "Gradientbased learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE, vol. 86, no. 11, pp. 2278\u20132324, 1998.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1998}, {"title": "A kernel method for multilabelled classification", "author": ["A. Elisseeff", "J. Weston"], "venue": "Advances in neural information processing systems, 2001, pp. 681\u2013687.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2001}, {"title": "Learning multi-label scene classification", "author": ["M.R. Boutell", "J. Luo", "X. Shen", "C.M. Brown"], "venue": "Pattern recognition, vol. 37, no. 9, pp. 1757\u20131771, 2004.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2004}, {"title": "A semi-definite positive linear discriminant analysis and its applications", "author": ["D. Kong", "C. Ding"], "venue": "Data Mining (ICDM), 2012 IEEE 12th International Conference on. IEEE, 2012, pp. 942\u2013947.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2012}, {"title": "Efficient and robust feature extraction by maximum margin criterion", "author": ["H. Li", "T. Jiang", "K. Zhang"], "venue": "Neural Networks, IEEE Transactions on, vol. 17, no. 1, pp. 157\u2013165, 2006.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2006}, {"title": "Regularized linear discriminant analysis and its application in microarrays", "author": ["Y. Guo", "T. Hastie", "R. Tibshirani"], "venue": "Biostatistics, vol. 8, no. 1, pp. 86\u2013100, 2007.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2007}, {"title": "Characterization of a family of algorithms for generalized discriminant analysis on undersampled problems", "author": ["J. Ye"], "venue": "Journal of Machine Learning Research, 2005, pp. 483\u2013502.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2005}, {"title": "Lower dimensional representation of text data based on centroids and least squares", "author": ["H. Park", "M. Jeon", "J.B. Rosen"], "venue": "BIT Numerical mathematics, vol. 43, no. 2, pp. 427\u2013 448, 2003.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2003}, {"title": "Orthogonal vs. uncorrelated least squares discriminant analysis for feature extraction", "author": ["F. Nie", "S. Xiang", "Y. Liu", "C. Hou", "C. Zhang"], "venue": "Pattern Recognition Letters, vol. 33, no. 5, pp. 485\u2013491, 2012.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2012}, {"title": "Evaluation: From precision, recall and f-measure to roc., informedness, markedness & correlation", "author": ["D. Powers"], "venue": "Journal of Machine Learning Technologies, vol. 2, no. 1, pp. 37\u201363, 2011.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2011}, {"title": "Analysis and modeling of social influence in high performance computing workloads", "author": ["S. Zheng", "Z.-Y. Shae", "X. Zhang", "H. Jamjoom", "L. Fong"], "venue": "European Conference on Parallel Processing. Springer Berlin Heidelberg, 2011, pp. 193\u2013204.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2011}, {"title": "Virtual machine migration in an over-committed cloud", "author": ["X. Zhang", "Z.-Y. Shae", "S. Zheng", "H. Jamjoom"], "venue": "2012 IEEE Network Operations and Management Symposium. IEEE, 2012, pp. 196\u2013203.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2012}, {"title": "Tidewatch: Fingerprinting the cyclicality of big data workloads", "author": ["D. Williams", "S. Zheng", "X. Zhang", "H. Jamjoom"], "venue": "IEEE INFOCOM 2014-IEEE Conference on Computer Communications. IEEE, 2014, pp. 2031\u20132039.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2014}, {"title": "Kernel alignment inspired linear discriminant analysis", "author": ["S. Zheng", "C. Ding"], "venue": "Joint European Conference on Machine Learning and Knowledge Discovery in Databases. Springer Berlin Heidelberg, 2014, pp. 401\u2013416.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2014}, {"title": "A closed form solution to multi-view low-rank regression.", "author": ["S. Zheng", "X. Cai", "C.H. Ding", "F. Nie", "H. Huang"], "venue": "in AAAI,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2015}, {"title": "Accelerating deep learning with shrinkage and recall", "author": ["S. Zheng", "A. Vishnu", "C. Ding"], "venue": "arXiv preprint arXiv:1605.01369, 2016.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2016}, {"title": "A convex programming approach to the trace quotient problem", "author": ["C. Shen", "H. Li", "M.J. Brooks"], "venue": "Computer Vision\u2013 ACCV 2007. Springer, 2007, pp. 227\u2013235.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2007}, {"title": "Trace ratio criterion for feature selection.", "author": ["F. Nie", "S. Xiang", "Y. Jia", "C. Zhang", "S. Yan"], "venue": "in AAAI,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2008}, {"title": "Subspace linear discriminant analysis for face recognition", "author": ["W. Zhao", "R. Chellappa", "P.J. Phillips"], "venue": null, "citeRegEx": "36", "shortCiteRegEx": "36", "year": 1999}], "referenceMentions": [{"referenceID": 0, "context": "INTRODUCTION Researchers and engineers nowadays have larger and larger data with very high dimension to be processed everyday [1].", "startOffset": 126, "endOffset": 129}, {"referenceID": 1, "context": "In biology science, high-dimensional gene expression data is used to predict tumor [2].", "startOffset": 83, "endOffset": 86}, {"referenceID": 2, "context": "Many methods have been proposed for dimensionality reduction, such as principal component analysis (PCA) [3] and linear discriminant analysis (LDA) [4].", "startOffset": 105, "endOffset": 108}, {"referenceID": 3, "context": "Many methods have been proposed for dimensionality reduction, such as principal component analysis (PCA) [3] and linear discriminant analysis (LDA) [4].", "startOffset": 148, "endOffset": 151}, {"referenceID": 4, "context": "In null space based LDA (NLDA) [5], the between-class distance is maximized in the null space of within-class scatter matrix Sw, max G Tr(GSbG), s.", "startOffset": 31, "endOffset": 34}, {"referenceID": 5, "context": "CONNECTION WITH TRACE RATIO Trace ratio was proposed in [6], [7], [8] to solve the following problem:", "startOffset": 56, "endOffset": 59}, {"referenceID": 6, "context": "CONNECTION WITH TRACE RATIO Trace ratio was proposed in [6], [7], [8] to solve the following problem:", "startOffset": 61, "endOffset": 64}, {"referenceID": 7, "context": "CONNECTION WITH TRACE RATIO Trace ratio was proposed in [6], [7], [8] to solve the following problem:", "startOffset": 66, "endOffset": 69}, {"referenceID": 8, "context": "It has stimulated many multi-label learning algorithms [9] [10] [11] [12].", "startOffset": 55, "endOffset": 58}, {"referenceID": 9, "context": "It has stimulated many multi-label learning algorithms [9] [10] [11] [12].", "startOffset": 59, "endOffset": 63}, {"referenceID": 10, "context": "It has stimulated many multi-label learning algorithms [9] [10] [11] [12].", "startOffset": 64, "endOffset": 68}, {"referenceID": 11, "context": "It has stimulated many multi-label learning algorithms [9] [10] [11] [12].", "startOffset": 69, "endOffset": 73}, {"referenceID": 11, "context": "Wang proposed a multi-label formulation of scatter matrices for multi-label data in [12].", "startOffset": 84, "endOffset": 88}, {"referenceID": 11, "context": "Multi-label between-class scatter matrix S\u0303b and within-class scatter matrix S\u0303w are defined as follows [12]:", "startOffset": 104, "endOffset": 108}, {"referenceID": 12, "context": "(28, 29, 30) in [13].", "startOffset": 16, "endOffset": 20}, {"referenceID": 13, "context": "Caltech101[14] contains 101 object categories.", "startOffset": 10, "endOffset": 14}, {"referenceID": 14, "context": "We then use VLFeat [15] to extract HOG feature.", "startOffset": 19, "endOffset": 23}, {"referenceID": 15, "context": "MSRC [16] is from MSRC data base v1 and contains 7 classes with 30 images in each class.", "startOffset": 5, "endOffset": 9}, {"referenceID": 16, "context": "[17], digit datasets MNIST [18] and handwritten alphabets Binalpha.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "[17], digit datasets MNIST [18] and handwritten alphabets Binalpha.", "startOffset": 27, "endOffset": 31}, {"referenceID": 18, "context": "Yeast is a multi-label data from [19].", "startOffset": 33, "endOffset": 37}, {"referenceID": 15, "context": "MSRC [16] is MSRC multi-label data base v2 provided by Microsoft Research Cambridge, which has 591 images annotated by 23 classes.", "startOffset": 5, "endOffset": 9}, {"referenceID": 19, "context": "Scene is a multi-label image data from [20].", "startOffset": 39, "endOffset": 43}, {"referenceID": 4, "context": "Single-label classification experiment On 7 single-label dataset, we compare MCDA with 10 other different methods, including LDA, null space LDA(NLDA) [5], Trace Ratio LDA(TraceRatio) [6], Semi Definite Positive LDA(spdLDA) [21], Maximum Margin Criteria(MMC) [22], regularized LDA(RLDA) [23], Uncorrelated LDA(ULDA) [24], Orthogonal LDA (OLDA) [24], Table IV: Multi-label classification accuracy (best results are in bold).", "startOffset": 151, "endOffset": 154}, {"referenceID": 5, "context": "Single-label classification experiment On 7 single-label dataset, we compare MCDA with 10 other different methods, including LDA, null space LDA(NLDA) [5], Trace Ratio LDA(TraceRatio) [6], Semi Definite Positive LDA(spdLDA) [21], Maximum Margin Criteria(MMC) [22], regularized LDA(RLDA) [23], Uncorrelated LDA(ULDA) [24], Orthogonal LDA (OLDA) [24], Table IV: Multi-label classification accuracy (best results are in bold).", "startOffset": 184, "endOffset": 187}, {"referenceID": 20, "context": "Single-label classification experiment On 7 single-label dataset, we compare MCDA with 10 other different methods, including LDA, null space LDA(NLDA) [5], Trace Ratio LDA(TraceRatio) [6], Semi Definite Positive LDA(spdLDA) [21], Maximum Margin Criteria(MMC) [22], regularized LDA(RLDA) [23], Uncorrelated LDA(ULDA) [24], Orthogonal LDA (OLDA) [24], Table IV: Multi-label classification accuracy (best results are in bold).", "startOffset": 224, "endOffset": 228}, {"referenceID": 21, "context": "Single-label classification experiment On 7 single-label dataset, we compare MCDA with 10 other different methods, including LDA, null space LDA(NLDA) [5], Trace Ratio LDA(TraceRatio) [6], Semi Definite Positive LDA(spdLDA) [21], Maximum Margin Criteria(MMC) [22], regularized LDA(RLDA) [23], Uncorrelated LDA(ULDA) [24], Orthogonal LDA (OLDA) [24], Table IV: Multi-label classification accuracy (best results are in bold).", "startOffset": 259, "endOffset": 263}, {"referenceID": 22, "context": "Single-label classification experiment On 7 single-label dataset, we compare MCDA with 10 other different methods, including LDA, null space LDA(NLDA) [5], Trace Ratio LDA(TraceRatio) [6], Semi Definite Positive LDA(spdLDA) [21], Maximum Margin Criteria(MMC) [22], regularized LDA(RLDA) [23], Uncorrelated LDA(ULDA) [24], Orthogonal LDA (OLDA) [24], Table IV: Multi-label classification accuracy (best results are in bold).", "startOffset": 287, "endOffset": 291}, {"referenceID": 23, "context": "Single-label classification experiment On 7 single-label dataset, we compare MCDA with 10 other different methods, including LDA, null space LDA(NLDA) [5], Trace Ratio LDA(TraceRatio) [6], Semi Definite Positive LDA(spdLDA) [21], Maximum Margin Criteria(MMC) [22], regularized LDA(RLDA) [23], Uncorrelated LDA(ULDA) [24], Orthogonal LDA (OLDA) [24], Table IV: Multi-label classification accuracy (best results are in bold).", "startOffset": 316, "endOffset": 320}, {"referenceID": 23, "context": "Single-label classification experiment On 7 single-label dataset, we compare MCDA with 10 other different methods, including LDA, null space LDA(NLDA) [5], Trace Ratio LDA(TraceRatio) [6], Semi Definite Positive LDA(spdLDA) [21], Maximum Margin Criteria(MMC) [22], regularized LDA(RLDA) [23], Uncorrelated LDA(ULDA) [24], Orthogonal LDA (OLDA) [24], Table IV: Multi-label classification accuracy (best results are in bold).", "startOffset": 344, "endOffset": 348}, {"referenceID": 24, "context": "Orthogonal Centroid Method(OCM) [25] and Orthogonal Least Squares LDA(OLSLDA)[26].", "startOffset": 32, "endOffset": 36}, {"referenceID": 25, "context": "Orthogonal Centroid Method(OCM) [25] and Orthogonal Least Squares LDA(OLSLDA)[26].", "startOffset": 77, "endOffset": 81}, {"referenceID": 26, "context": "Macro-averaged and micro-averaged F1-score are widely used as a metric to evaluate classification performance [27].", "startOffset": 110, "endOffset": 114}], "year": 2017, "abstractText": "Linear Discriminant Analysis (LDA) is a widelyused supervised dimensionality reduction method in computer vision and pattern recognition. In null space based LDA (NLDA), a well-known LDA extension, between-class distance is maximized in the null space of the within-class scatter matrix. However, there are some limitations in NLDA. Firstly, for many data sets, null space of within-class scatter matrix does not exist, thus NLDA is not applicable to those datasets. Secondly, NLDA uses arithmetic mean of between-class distances and gives equal consideration to all between-class distances, which makes larger between-class distances can dominate the result and thus limits the performance of NLDA. In this paper, we propose a harmonic mean based Linear Discriminant Analysis, Multi-Class Discriminant Analysis (MCDA), for image classification, which minimizes the reciprocal of weighted harmonic mean of pairwise between-class distance. More importantly, MCDA gives higher priority to maximize small between-class distances. MCDA can be extended to multi-label dimension reduction. Results on 7 single-label data sets and 4 multi-label data sets show that MCDA has consistently better performance than 10 other single-label approaches and 4 other multi-label approaches in terms of classification accuracy, macro and micro average F1 score. Keywords-Dimensionality Reduction; Linear Discriminant Analysis; Image Classification", "creator": "LaTeX with hyperref package"}}}