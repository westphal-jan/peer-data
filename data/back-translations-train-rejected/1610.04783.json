{"id": "1610.04783", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Oct-2016", "title": "Similarity Learning for Time Series Classification", "abstract": "Multivariate time series naturally exist in many fields, like energy, bioinformatics, signal processing, and finance. Most of these applications need to be able to compare these structured data. In this context, dynamic time warping (DTW) is probably the most common comparison measure. However, not much research effort has been put into improving it by learning. In this paper, we propose a novel method for learning similarities based on DTW, in order to improve time series classification. Making use of the uniform stability framework, we provide the first theoretical guarantees in the form of a generalization bound for linear classification. The experimental study shows that the proposed approach is efficient, while yielding sparse classifiers.", "histories": [["v1", "Sat, 15 Oct 2016 20:37:52 GMT  (482kb,D)", "http://arxiv.org/abs/1610.04783v1", "Techreport"]], "COMMENTS": "Techreport", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["maria-irina nicolae", "\\'eric gaussier", "amaury habrard", "marc sebban"], "accepted": false, "id": "1610.04783"}, "pdf": {"name": "1610.04783.pdf", "metadata": {"source": "CRF", "title": "Similarity Learning for Time Series Classification", "authors": ["Maria-Irina Nicolae", "\u00c9ric Gaussier", "Amaury Habrard", "Marc Sebban"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "The presence of time series in numerous fields of application makes them the object of considerable research efforts for their classification or prediction. Classification of time series is a challenging problem, with multiple applications in areas such as speech recognition, energy consumption, object identification, bioinformatics, patient care, etc. To solve such tasks, one is per se led to compare time series by pairs to determine their proximity or common patterns. However, time series coming from real applications are mostly not directly comparable, due to differences in length, phase or sampling frequency. An important subsequent task for solving the previous problems will be to find the right alignment between moments of time.Dynamic time distortion [13] is the most well-known algorithm for measuring similarity between two time series by finding the best alignment between them. Its popularity is based on its ability to work with series of different lengths and phases, and its performance, and its performance, in general, better than the climate."}, {"heading": "2 Related Work", "text": "This method is designed to solve the problem of comparing time series of different lengths and phases. The warping method found by DTW is calculated by DTW. A cost matrix (typically constructed using the Euclidean distance) in square time by dynamic programming. Aligning two time series means finding all matching moments of time between them (Figure 1). The alignment is well constructed if all indices are used in both time series and the warping path is continuous and ascending."}, {"heading": "3 Similarity Learning for Time Series Classification", "text": "This section introduces the proposed method for learning temporal similarity functions. Let's start by defining the similarity to be used with time series, and then introduce the method for learning them. Let's leave A-RtA-D a multivariate time series of length tA and dimension d. We use X to denote the space of all time series of finite length. Let's consider the following problem of binary classification: We get the designation multivariate time series (A, l) drawn from a distribution P over X-1-1-1, possibly of different length, but the same dimension d."}, {"heading": "3.1 Bilinear Similarity for Time Series", "text": "For a pair of time series A and B is CM (A, B). \u00b7 RtA \u00b7 tB is a pair matrix of the cost of aligning a moment in A to one in B under the metric parameters of matrix M. Since we use a similarity function, CM (A, B) represents the affinity values that we want to maximize rather than minimize the cost. We will refer to the lines of A as a1,.., atA and those of B as b1,.., btB. Without loss of generality, the data will be as | ai | 2 = 1, i... tA}. We will focus on an affinity matrix of form: CM (A, B) i, j = a of generalization x."}, {"heading": "3.2 Learning Good Similarities", "text": "Our goal is to learn the matrix M, which parameterizes the similarity function KM for use in classification. To this end, we have a training set S of the time series M {(Ai, li)} mi = 1, which is drawn from the same distribution according to P and a set L of n boundary stones {(Bj, l \u2032 j) nj = 1. We want to optimize the (, \u03b3, \u03c4) quality of the proposed similarity function as it is represented in definition 1: E (A, l) [1 \u2212 E (B, l \u2032), R (B, l \u2032) [ll \u2032 KM (A, B))) -quality of the proposed similarity function, as defined in definition 1: E (B, l)."}, {"heading": "4 Theoretical Guarantees", "text": "In this section, we derive a generalization for SLTS using the concept of uniform stability [5]. Let's use the empirical loss function for an example (A, l) between the empirical loss we minimize under the regularization in Equation (3) and the value we want to minimize for the actual loss. Let's show the empirical loss function for an example (A, l) of how stable we will be. \"(M, l). (M, l). (M, l). (M) = 1 \u2212 n. (A, j). (3), SLTS minimizes the empirical risk of the learned matrix M over the entire training set S: E (M, l)."}, {"heading": "5 Experiments", "text": "In this section, we present the results of the experiments carried out to assess the performance of the proposed method. In the first experiment, we show that learning the matrix M provides additional information for linear classification. We also analyze the influence of the number of landmarks on SLTS. The second study offers a comparison of the SLTS algorithms, while the third part illustrates the ability of SLTS to learn a discriminatory metric in the space created by the landmarks. Finally, we offer a discussion on the selection of landmarks, followed by an additional experiment that meant comparing some of the heuristics used for the selection of landmarks. We conduct the experimental study on multivariate time series coming from UCI Machine Repository, which contain between 47-8800 instances. We begin by giving the description of the datasets used for the experiments in Table 1."}, {"heading": "6 Conclusion and Perspectives", "text": "In this paper, we address the problem of learning a global linear classifier for multivariate time series through similarity learning. We propose a bilinear similarity function that takes into account the optimal orientation. Our method is linked to a generalization linked to the error of the metric and classifier, and is the first to guarantee classification services for learning similarity in the case of time series. The experimental study demonstrates the usefulness of the (, \u03b3, \u03c4) good framework and the importance of metric learning in this environment. Future work should include learning Mahalanobis metrics, as suggested by the results of the LDMLT. We also plan to try to capture local temporal information by learning multiple metrics, as well as the effects of different regulators on the matrix M."}, {"heading": "Acknowledgments", "text": "This project was financed by a grant from the Rh\u00f4ne-Alpes region."}, {"heading": "A Proofs of Lemmas", "text": "This section contains the findings of Lemmas (A, l) and Lemmas (A, l), as well as the definition of other Lemmas (A, l) necessary for these findings. To prove that Lemma (B, l) and Lemma (B, l), that we (B, l) and (B, l) have a connection to the Frobenius norm of a subrange of the similarity function, we need two additional Lemmas (A, l). Since M) is the optimal solution of Problem (3), we have: RS (0). Lemma A. If M (3) is the optimal solution of Problem (3), we have the optimal solution of Problem (B, l). Lemma A. If M (M, l) is the optimal solution of Problem (3)."}], "references": [{"title": "Sparsedtw: A novel approach to speed up dynamic time warping", "author": ["G. Al-Naymat", "S. Chawla", "J. Taheri"], "venue": "CoRR, abs/1201.2969,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2012}, {"title": "Improved guarantees for learning via similarity functions", "author": ["M.-F. Balcan", "A. Blum", "N. Srebro"], "venue": "COLT, pages 287\u2013298. Omnipress,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2008}, {"title": "Similarity learning for provably accurate sparse linear classification", "author": ["A. Bellet", "A. Habrard", "M. Sebban"], "venue": "ICML, pages 1871\u20131878,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2012}, {"title": "Metric Learning", "author": ["A. Bellet", "A. Habrard", "M. Sebban"], "venue": "Morgan & Claypool Publishers,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2015}, {"title": "Stability and generalization", "author": ["O. Bousquet", "A. Elisseeff"], "venue": "JMLR, 2:499\u2013526,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2002}, {"title": "Information-theoretic metric learning", "author": ["J.V. Davis", "B. Kulis", "P. Jain", "S. Sra", "I.S. Dhillon"], "venue": "ICML, pages 209\u2013216. ACM,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2007}, {"title": "Learning multiple temporal matching for time series classification", "author": ["C. Frambourg", "A.D. Chouakria", "\u00c9. Gaussier"], "venue": "IDA, volume 8207 of LNCS, pages 198\u2013209. Springer,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2013}, {"title": "Minimum prediction residual principle applied to speech recognition", "author": ["F. Itakura"], "venue": "IEEE Trans. on ASSP, 23(1):67\u201372,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1975}, {"title": "Weighted dynamic time warping for time series classification", "author": ["Y.-S. Jeong", "M.K. Jeong", "O.A. Omitaomu"], "venue": "Pattern Recogn., 44(9):2231\u20132240, Sept.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2011}, {"title": "Similarity-based learning via data driven embeddings", "author": ["P. Kar", "P. Jain"], "venue": "NIPS, pages 1998\u20132006. Curran Associates, Inc.,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2011}, {"title": "Clustering by means of medoids", "author": ["L. Kaufman", "P. Rousseeuw"], "venue": "Statistical Data Analysis Based on the L1-Norm and Related Methods, pages 405\u2013416. North-Holland,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1987}, {"title": "On the need for time series data mining benchmarks: A survey and empirical demonstration", "author": ["E. Keogh", "S. Kasetty"], "venue": "Data Mining and Knowledge Discovery, 7(4):349\u2013371,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2003}, {"title": "The symmetric time-warping problem: From continuous to discrete", "author": ["J.B. Kruskall", "M. Liberman"], "venue": "Time Warps, String Edits and Macromolecules: The Theory and Practice of String Comparison. Addison-Wesley,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1983}, {"title": "Metric learning: A survey", "author": ["B. Kulis"], "venue": "Foundations and Trends in Machine Learning, 5(4):287\u2013364,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2013}, {"title": "Metric learning for temporal sequence alignment", "author": ["R. Lajugie", "D. Garreau", "F. Bach", "S. Arlot"], "venue": "NIPS, pages 1817\u20131825. Curran Associates, Inc.,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning a mahalanobis distance-based dynamic time warping measure for multivariate time series classification", "author": ["J. Mei", "M. Liu", "Y.F. Wang", "H. Gao"], "venue": "IEEE Trans. on Cybernetics, 46(6):1363\u20131374,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2015}, {"title": "Time series classification by class-specific Mahalanobis distance measures", "author": ["Z. Prekopcs\u00e1k", "D. Lemire"], "venue": "Adv. Data Analysis and Classification, 6(3):185\u2013200,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2012}, {"title": "Online and batch learning of generalized cosine similarities", "author": ["A.M. Qamar", "\u00c9. Gaussier"], "venue": "ICDM, pages 926\u2013931,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2009}, {"title": "Dynamic-programming algorithm optimization for spoken word recognition", "author": ["H. Sakoe", "S. Chiba"], "venue": "Trans. on ASSP, 26(1):43\u201349,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 1978}, {"title": "FastDTW: toward accurate dynamic time warping in linear time and space", "author": ["S. Salvador", "P. Chan"], "venue": "KDD workshop on mining temporal and sequential data,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2004}, {"title": "Sample complexity of learning mahalanobis distance metrics", "author": ["N. Verma", "K. Branson"], "venue": "NIPS, pages 2584\u20132592,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2015}, {"title": "Distance metric learning for large margin nearest neighbor classification", "author": ["K. Weinberger", "L. Saul"], "venue": "JMLR, 10:207\u2013244,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2009}, {"title": "Distance metric learning, with application to clustering with side-information", "author": ["E.P. Xing", "A.Y. Ng", "M.I. Jordan", "S. Russell"], "venue": "NIPS, volume 15, pages 505\u2013512,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2002}, {"title": "1-norm support vector machines", "author": ["J. Zhu", "S. Rosset", "T. Hastie", "R. Tibshirani"], "venue": "NIPS, 16(1):49\u201356,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2004}], "referenceMentions": [{"referenceID": 12, "context": "Dynamic time warping [13] is the most well-known algorithm for measuring the similarity between two time series by finding the best alignment between them.", "startOffset": 21, "endOffset": 25}, {"referenceID": 7, "context": "The majority of previous results in time series classification concerns the adjustment of the constraints for finding the best alignment between time series for the task at hand [8, 20, 9, 1, 21].", "startOffset": 178, "endOffset": 195}, {"referenceID": 18, "context": "The majority of previous results in time series classification concerns the adjustment of the constraints for finding the best alignment between time series for the task at hand [8, 20, 9, 1, 21].", "startOffset": 178, "endOffset": 195}, {"referenceID": 8, "context": "The majority of previous results in time series classification concerns the adjustment of the constraints for finding the best alignment between time series for the task at hand [8, 20, 9, 1, 21].", "startOffset": 178, "endOffset": 195}, {"referenceID": 0, "context": "The majority of previous results in time series classification concerns the adjustment of the constraints for finding the best alignment between time series for the task at hand [8, 20, 9, 1, 21].", "startOffset": 178, "endOffset": 195}, {"referenceID": 19, "context": "The majority of previous results in time series classification concerns the adjustment of the constraints for finding the best alignment between time series for the task at hand [8, 20, 9, 1, 21].", "startOffset": 178, "endOffset": 195}, {"referenceID": 11, "context": "Most of these approaches are designed for univariate time series [12, 18], which record the value of only one feature per time moment.", "startOffset": 65, "endOffset": 73}, {"referenceID": 16, "context": "Most of these approaches are designed for univariate time series [12, 18], which record the value of only one feature per time moment.", "startOffset": 65, "endOffset": 73}, {"referenceID": 3, "context": "Metric learning [4, 23, 6, 24] can address exactly this problem as it allows one to learn the weights of features and the correlations between them from the available training data.", "startOffset": 16, "endOffset": 30}, {"referenceID": 21, "context": "Metric learning [4, 23, 6, 24] can address exactly this problem as it allows one to learn the weights of features and the correlations between them from the available training data.", "startOffset": 16, "endOffset": 30}, {"referenceID": 5, "context": "Metric learning [4, 23, 6, 24] can address exactly this problem as it allows one to learn the weights of features and the correlations between them from the available training data.", "startOffset": 16, "endOffset": 30}, {"referenceID": 22, "context": "Metric learning [4, 23, 6, 24] can address exactly this problem as it allows one to learn the weights of features and the correlations between them from the available training data.", "startOffset": 16, "endOffset": 30}, {"referenceID": 1, "context": "Our method is based on the ( , \u03b3, \u03c4)-good similarities learning framework [2].", "startOffset": 74, "endOffset": 77}, {"referenceID": 12, "context": "Dynamic time warping [13] computes the optimal alignment between two time series under a metric by finding the pairs of time indices to align.", "startOffset": 21, "endOffset": 25}, {"referenceID": 19, "context": "In order to overcome the computational complexity of DTW, faster alternatives were introduced, like FastDTW [21] and SparseDTW [1].", "startOffset": 108, "endOffset": 112}, {"referenceID": 0, "context": "In order to overcome the computational complexity of DTW, faster alternatives were introduced, like FastDTW [21] and SparseDTW [1].", "startOffset": 127, "endOffset": 130}, {"referenceID": 18, "context": "aligning the beginning of a series with the end of another), of which we mention the Sakoe-Chiba band [20] and the Itakura parallelogram [8].", "startOffset": 102, "endOffset": 106}, {"referenceID": 7, "context": "aligning the beginning of a series with the end of another), of which we mention the Sakoe-Chiba band [20] and the Itakura parallelogram [8].", "startOffset": 137, "endOffset": 140}, {"referenceID": 13, "context": "Metric learning [14, 4] focuses on learning the parameters of a distance or similarity function from data.", "startOffset": 16, "endOffset": 23}, {"referenceID": 3, "context": "Metric learning [14, 4] focuses on learning the parameters of a distance or similarity function from data.", "startOffset": 16, "endOffset": 23}, {"referenceID": 21, "context": "Large Margin Metric Learning (LMNN) [23] and Information-Theoretic Metric Learning (ITML) [6] are probably the most well-", "startOffset": 36, "endOffset": 40}, {"referenceID": 5, "context": "Large Margin Metric Learning (LMNN) [23] and Information-Theoretic Metric Learning (ITML) [6] are probably the most well-", "startOffset": 90, "endOffset": 93}, {"referenceID": 6, "context": "For time series, the notion of learning a metric has mostly been used in the sense of learning the right alignment for univariate time series [7].", "startOffset": 142, "endOffset": 145}, {"referenceID": 14, "context": "In [15], the authors propose to learn a Mahalanobis metric for multivariate time series alignment of audio data.", "startOffset": 3, "endOffset": 7}, {"referenceID": 15, "context": "Recently, LDMLT [17] was designed to learn a Mahalanobis distance for multivariate time series from triplet constraints.", "startOffset": 16, "endOffset": 20}, {"referenceID": 14, "context": "Moreover, neither the method from [15] nor LDMLT come with guarantees that learning the metric improves performance for the given task.", "startOffset": 34, "endOffset": 38}, {"referenceID": 1, "context": "[2] K : X \u00d7 X \u2192 [\u22121, 1] is a ( , \u03b3, \u03c4)-good similarity function in hinge loss for a learning problem P if there exists a random indicator function R(x) defining a probabilistic set of \"reasonable points\" such that the following conditions hold: 1.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "[2] Let K be an ( , \u03b3, \u03c4)-good similarity function in hinge loss for a learning problem P.", "startOffset": 0, "endOffset": 3}, {"referenceID": 23, "context": "This formulation is equivalent to a relaxed L1-norm SVM [25].", "startOffset": 56, "endOffset": 60}, {"referenceID": 2, "context": "This issue has been addressed in [3] only for feature vectors.", "startOffset": 33, "endOffset": 36}, {"referenceID": 17, "context": "For the pair of indices i and j, the affinity is equivalent to computing the generalized cosine similarity [19], as ai and bj are already normalized.", "startOffset": 107, "endOffset": 111}, {"referenceID": 4, "context": "In this section, we derive a generalization bound for SLTS using the notion of uniform stability [5].", "startOffset": 97, "endOffset": 100}, {"referenceID": 4, "context": "Definition 3 (Uniform stability [5]).", "startOffset": 32, "endOffset": 35}, {"referenceID": 20, "context": "According to [22], the presence of the number of features d in the numerator of the bound is to be expected and shows that the approach may suffer from the curse of dimensionality.", "startOffset": 13, "endOffset": 17}, {"referenceID": 15, "context": "In the case of Auslan, we only use the 25 first classes instead of the total of 95, as done in precedent studies [17].", "startOffset": 113, "endOffset": 117}, {"referenceID": 1, "context": "We compare our method against the following classic algorithms: \u2022 Standard nearest neighbor classifier (1NN); \u2022 Linear SVM under L2 regularization; \u2022 Linear classifier from [2], presented in Equation (2) (called BBS from now on); \u2022 LDMLT [17] with a nearest neighbor classifier; \u2022 SLTS, the similarity learning method proposed in this chapter, which is then used to learn a global linear classifier using the formulation in [2].", "startOffset": 173, "endOffset": 176}, {"referenceID": 15, "context": "We compare our method against the following classic algorithms: \u2022 Standard nearest neighbor classifier (1NN); \u2022 Linear SVM under L2 regularization; \u2022 Linear classifier from [2], presented in Equation (2) (called BBS from now on); \u2022 LDMLT [17] with a nearest neighbor classifier; \u2022 SLTS, the similarity learning method proposed in this chapter, which is then used to learn a global linear classifier using the formulation in [2].", "startOffset": 238, "endOffset": 242}, {"referenceID": 1, "context": "We compare our method against the following classic algorithms: \u2022 Standard nearest neighbor classifier (1NN); \u2022 Linear SVM under L2 regularization; \u2022 Linear classifier from [2], presented in Equation (2) (called BBS from now on); \u2022 LDMLT [17] with a nearest neighbor classifier; \u2022 SLTS, the similarity learning method proposed in this chapter, which is then used to learn a global linear classifier using the formulation in [2].", "startOffset": 424, "endOffset": 427}, {"referenceID": 17, "context": "The comparison between the two should thus be taken with caution as distances and similarities can yield very different results [19].", "startOffset": 128, "endOffset": 132}, {"referenceID": 10, "context": "K-Medoids [11] is a classical clustering technique.", "startOffset": 10, "endOffset": 14}, {"referenceID": 9, "context": "Dselect [10] was proposed as a landmarks selection algorithm that optimizes a criterion of diversity.", "startOffset": 8, "endOffset": 12}, {"referenceID": 0, "context": "Then we have, for t \u2208 [0, 1]: ||M||F \u2212 ||M\u2212 t\u2206M||F + ||M||F \u2212 ||M + t\u2206M||F \u2264 2kt \u03bbm ||\u2206M||F .", "startOffset": 22, "endOffset": 28}, {"referenceID": 4, "context": "The proof is similar to the one of Lemma 20 in [5], thus we shall omit it.", "startOffset": 47, "endOffset": 50}], "year": 2016, "abstractText": "Multivariate time series naturally exist in many fields, like energy, bioinformatics, signal processing, and finance. Most of these applications need to be able to compare these structured data. In this context, dynamic time warping (DTW) is probably the most common comparison measure. However, not much research effort has been put into improving it by learning. In this paper, we propose a novel method for learning similarities based on DTW, in order to improve time series classification. Making use of the uniform stability framework, we provide the first theoretical guarantees in the form of a generalization bound for linear classification. The experimental study shows that the proposed approach is efficient, while yielding sparse classifiers.", "creator": "LaTeX with hyperref package"}}}