{"id": "1610.04841", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Oct-2016", "title": "Translation Quality Estimation using Recurrent Neural Network", "abstract": "This paper describes our submission to the shared task on word/phrase level Quality Estimation (QE) in the First Conference on Statistical Machine Translation (WMT16). The objective of the shared task was to predict if the given word/phrase is a correct/incorrect (OK/BAD) translation in the given sentence. In this paper, we propose a novel approach for word level Quality Estimation using Recurrent Neural Network Language Model (RNN-LM) architecture. RNN-LMs have been found very effective in different Natural Language Processing (NLP) applications. RNN-LM is mainly used for vector space language modeling for different NLP problems. For this task, we modify the architecture of RNN-LM. The modified system predicts a label (OK/BAD) in the slot rather than predicting the word. The input to the system is a word sequence, similar to the standard RNN-LM. The approach is language independent and requires only the translated text for QE. To estimate the phrase level quality, we use the output of the word level QE system.", "histories": [["v1", "Sun, 16 Oct 2016 10:54:23 GMT  (23kb)", "https://arxiv.org/abs/1610.04841v1", "7 pages, published at First Conference on Machine Translation"], ["v2", "Fri, 21 Oct 2016 07:01:05 GMT  (23kb,D)", "http://arxiv.org/abs/1610.04841v2", "7 pages, published at First Conference on Machine Translation"]], "COMMENTS": "7 pages, published at First Conference on Machine Translation", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["raj nath patel", "sasikumar m"], "accepted": false, "id": "1610.04841"}, "pdf": {"name": "1610.04841.pdf", "metadata": {"source": "CRF", "title": "Translation Quality Estimation using Recurrent Neural Network", "authors": ["Raj Nath Patel"], "emails": ["rajnathp@cdac.in", "sasi@cdac.in"], "sections": [{"heading": "1 Introduction", "text": "In fact, it is the case that most of us are able to abide by the rules that they have established over the last five years. (...) In fact, it is the case that they are able to change the rules. (...) In fact, it is the case that they are able to change the rules. (...) In fact, it is the case that they are able to change the rules of the market. (...) In fact, it is the case that they are able to determine the rules of the market. (...) In fact, it is the case that they do not abide by the rules of the market. (...) The rules of the market are such that the rules of the market are not to abide by the rules of the market. (...) The rules of the market are such that the rules of the market are not to abide by. (...) The rules of the market are not to abide by the rules. (...) The rules of the market are not to abide by the rules. (...) The rules of the market are not to abide by the rules. (...) The rules of the market are not to abide by the rules. (... The rules of the market are not to abide by the rules. (...) The rules of the market are not to abide by the rules. (...) The rules of the market are not to abide by the rules. (...) The rules of the market are to abide by the rules."}, {"heading": "2 Related Work", "text": "Most of these approaches require manually designed features (Bojar et al., 2014), similar to the features provided by the organizers. Logacheva et al. (2015) modeled the QE word level using the CRF + + data selection and data bootstrapping tool, in which data selection filters out the sentences with the lowest percentage of faulty tokens and assumes they are less useful for the task. Bootstrapping technology creates additional data instances and increases the importance of BAD labels that appear in the training data. Shang et al. (2015) tried to address the issue of label imbalance by creating sublabels such as OK B (start), OK I (middle), OK E (end). Shah et al. (2015) have used word embedding as an additional feature (+ 25 features) with SVM classifiers. Bilingual Deep Neural Network (Utility Layer DNN) architecture was proposed for 2015-Qao."}, {"heading": "3 RNN Models for QE", "text": "For this task we used the RNN extensions Long Short-Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997) and Gated Recurrent Unit (GRU) (Cho et al., 2014). LSTM and GRU have shown that they perform better than the simple RNN in modelling long-distance dependencies in the data. Simple RNN also suffers from the problem of exploding and disappearing gradients (Bengio et al., 1994). LSTM and GRU solve this problem by introducing a gating mechanism. LSTM includes input, output and oblivion stores with a memory cell, while GRU has only reset and updated gates (no memory cell). Detailed description of each model is given in the following subsections."}, {"heading": "3.1 LSTM", "text": "Different researchers use slightly different LSTM variants (Graves, 2013; Yao et al., 2014; Jozefowicz et al., 2015). We implemented the version of LSTM described by the following equations: it = sigm (Wxixt + Whiht \u2212 1 + bi) ot = sigm (Wxoxt + Whoht \u2212 1 + bo) ft = sigm (Wxfxt + Whfht \u2212 1 + bf) jt = tanh (Wxjxt + Whjht \u2212 1 + bj) ct = ct \u2212 1 ft + it jt ht = tanh (ct) otwhere sigm is the logistic sigmoid function and tanh is the hyperbolic tangent function to add non-linearity in the network. is the elementwise multiplication of vectors. i, o, f are input, output, forget gates respectively j is the new memory content while c is the updated memory content."}, {"heading": "3.2 Deep LSTM", "text": "In this thesis we used Deep LSTM with two layers. Deep LSTM is created by stacking several LSTM on top of each other. We pass the output of the lower LSTM as input to the upper LSTM. For example, if it is the output of the lower LSTM, we apply a matrix transformation to format the input text for the upper LSTM. Matrix transformation allows us to have two successive LSTM layers of different sizes."}, {"heading": "3.3 GRU", "text": "GRU is an architecture very similar to LSTM. Chung et al. (2014) found that GRU outperforms LSTM in a number of tasks. GRU is defined by the following equations: rt = sigm (Wxrxt + Whrht \u2212 1 + br) zt = sigm (Wxzxt + Whzht \u2212 1 + bz) h = tanh (Wxhxt + Whh (rt ht \u2212 1) + bh) ht = zt ht \u2212 1 + (1 \u2212 zt) h. In the above equations W \u0445 are the weight matrices and b \u0445 the bias vectors. r and z are known as reset or update gates. GRU does not use a separate memory cell as used in LSTM. However, the gated mechanism controls the flow of information in the unit."}, {"heading": "3.4 Implementation Details", "text": "We implemented all models (LSTM, deep LSTM and GRU) with 1THEANO Framework (Bergstra et al., 2010; Bastien et al., 2012) as described above. For all models in the work, the size of a hidden layer is 100, the word embedding dimensionality is 100 and the context word window size is 5. We initialized all square weight matrices as random orthogonal matrices. All bias vectors were initialized to zero. Other weight matrices were scanned from a Gaussian distribution with mean 0 and variance 0.012. To update the model parameters, we used Truncated Back-Propagation-Through-Time (T-BPTT) (Werbos, 1990) with stochastic gradient descent. We set the depth of BPTT to 7 for all models."}, {"heading": "4 Experiments and Results", "text": "In this section we describe the experiments carried out for the common task and present the experimental results."}, {"heading": "4.1 Data distribution", "text": "The split for the train-1http: / / deeplearning.net / software / theano / # downloading / development / testing is described in Table 1. The split for the train-1http: / / deeplearning.net / software / theano / # downloading / development / testing was used to evaluate the various experiments we conducted for the joint task. The evaluation results shown in the Results section refer only to Test1. The organizers provided another set of test data (Test2) to evaluate all submitted systems."}, {"heading": "4.2 Methodology", "text": "In the following subsections, we discuss our approaches to assessing the quality of words and phrases."}, {"heading": "4.2.1 Word Level QE", "text": "Our experiments mainly focus on the word level QE. We have used the output of the word level QE system for estimating the phrase level quality.As mentioned above, we have used the modified RNN-LM architecture for the experiments.Baseline (LSTM) system was developed by training word embedding from scratch with other parameters of the model. In another set of experiments we have pre-programmed the word embedding with word2vec (Mikolov et al., 2013b) and further aligned it with the training of the model parameters. For the pre-training we have used an additional corpus (2M sentences approx.) from English Europarl data (Koehn, 2005).For bilingual models we have restructured the source sentence (English) according to the target (German) by using the word alignment ratio of word alignment designation (sSQE)."}, {"heading": "4.2.2 Phrase Level QE", "text": "For the phrase level QE, we have not formed an explicit system. As it was mentioned by the organizers that a phrase is marked as \"BAD\" if any word in the phrase is a wrong translation, we have taken the output of the word level QE system and marked the phrase as \"BAD\" if any word in the phrase boundary is marked as \"BAD.\" And other phrases (all words have the OK sign) are simply marked as \"OK.\""}, {"heading": "4.3 Results", "text": "In order to develop a basic system for QE at the word and phrase level, the organizers used the basic features (22 features) to train a conditional random field (CRF) model using the CRFSuite tool.The results of the experiments against Test2 are evaluated in Tables 2 and 3.We have evaluated our systems based on the F1Score. Because the data is dominated by the \"OK\" class and a naive system that marks all the words \"OK\" achieves a high value, the F1 score of the \"BAD\" class was used as the primary measurement for system valuation.The result tables indicate that the GRU LSTM, as reported by Cho et al. (2014), exceeds the LSTM at the word level. Results for QE at the phrase level are shown in Table 3. The result tables indicate that the LSTM, as reported by Cho et al. (2014), exceeds the QE at the word level."}, {"heading": "4.4 Submission to the shared task", "text": "We participated in Task-2, which includes the quality assessment of word and phrase. System settings submitted were: GRU + pretrain + sublabels, which are also marked in the result tables (2 and 3). Tables 4 and 5 describe the 2 results of the submission on Test2 corpus. Submission results were provided by the organizers.2http: / / www.quest.dcs.shef.ac.uk / wmt16 _ files _ qe / wmt16 _ task2 _ results.pdf"}, {"heading": "5 Discussion", "text": "The approach is language-independent and uses only the vector of context words to predict the tag for a word. In other words, we check whether any word (grammatically) fits into the given slot of words or not. However, we could use language-specific traits to increase classification accuracy. Experiments with bilingual models are similar to the concept of adding more traits to any machine learning algorithm. In monolingual models, we only use the vector of target words as a trait, while in bilingual models we also use the vector of source words (English). A challenge that machine learners often face is dealing with distorted classes in classification problems. Class distribution (OK / BAD) is also distorted in our case. To solve the problem, we have tried to balance the distribution of classes by introducing the subtable.LSTM and GRU, with the exception of the gating mechanism."}, {"heading": "6 Conclusion and Future Work", "text": "We have developed a language-independent word / phrase-level quality estimation system using RNN. We have used RNN-LM architecture with LSTM, Deep LSTM and GRU. We have shown that these models benefit from pre-training and the introduction of sublabels. Furthermore, models with bilingual features outperform monolingual models. We can extend the work to quality estimation at the sentence and document level. Improving word-level quality estimation through data selection and boot strapping (Logacheva et al., 2015), more effective methods for dealing with label imbalances, training larger models using language-specific features, other variations of the LSTM architecture, etc. are other possibilities."}], "references": [{"title": "Theano: new features and speed improvements", "author": ["Pascal Lamblin", "Razvan Pascanu", "James Bergstra", "Ian Goodfellow", "Arnaud Bergeron", "Nicolas Bouchard", "David WardeFarley", "Yoshua Bengio"], "venue": null, "citeRegEx": "Bastien et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Bastien et al\\.", "year": 2012}, {"title": "Learning long-term dependencies with gradient descent is difficult", "author": ["Bengio et al.1994] Yoshua Bengio", "Patrice Simard", "Paolo Frasconi"], "venue": "In IEEE Transactions on Neural Networks,", "citeRegEx": "Bengio et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 1994}, {"title": "A neural probabilistic language model", "author": ["Bengio et al.2003] Yoshua Bengio", "Rejean Ducharme", "Pascal Vincent", "Christian Jauvin"], "venue": "In Journal of Machine Learning Reseach,", "citeRegEx": "Bengio et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2003}, {"title": "Theano: a CPU and GPU math expression compiler", "author": ["Olivier Breuleux", "Frederic Bastien", "Pascal Lamblin", "Razvan Pascanu", "Guillaume Desjardins", "Joseph Turian", "David WardeFarley", "Yoshua Bengio"], "venue": null, "citeRegEx": "Bergstra et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Bergstra et al\\.", "year": 2010}, {"title": "Confidence Estimation for Machine Translation", "author": ["Blatz et al.2004] John Blatz", "Erin Fitzgerald", "George Foster", "Simona Gandrabur", "Cyril Goutte", "Alex Kulesza", "Alberto Sanchis", "Nicola Ueffing"], "venue": "In Proceedings of the 20th international con-", "citeRegEx": "Blatz et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Blatz et al\\.", "year": 2004}, {"title": "Findings of the 2012 workshop on statistical machine translation", "author": ["Soricut", "Lucia Specia."], "venue": "Proceedings of the Seventh Workshop on Statistical Machine Translation, pages 10\u201351, Montreal, Canada. Association for Computational Linguistics.", "citeRegEx": "Soricut and Specia.,? 2012", "shortCiteRegEx": "Soricut and Specia.", "year": 2012}, {"title": "Learning phrase representations using RNN encoder-decoder for statistical machine", "author": ["Cho et al.2014] Kyunghyun Cho", "Bart Van Merrinboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio"], "venue": null, "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Empirical evaluation of gated recurrent neural networks on sequence modeling", "author": ["Chung et al.2014] Junyoung Chung", "Caglar Gulcehre", "KyungHyun Cho", "Yoshua Bengio"], "venue": null, "citeRegEx": "Chung et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chung et al\\.", "year": 2014}, {"title": "FBK-UPV-UEdin participation in the WMT14 Quality Estimation shared-task", "author": ["U Politecnica de Valencia", "Christian Buck", "Marco Turchi", "Matteo Negri"], "venue": "In Proceedings of the Ninth Workshop on Statisti-", "citeRegEx": "Souza et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Souza et al\\.", "year": 2014}, {"title": "Generating sequences with recurrent neural networks. arXiv:1308.0850", "author": ["Alex Graves"], "venue": null, "citeRegEx": "Graves.,? \\Q2013\\E", "shortCiteRegEx": "Graves.", "year": 2013}, {"title": "Long short-term memory", "author": ["Hochreiter", "Schmidhuber1997] Sepp Hochreiter", "Jurgen Schmidhuber"], "venue": "In Neural computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "An empirical exploration of recurrent network architectures", "author": ["Wojciech Zaremba", "Ilya Sutskever"], "venue": "In Proceedings of the 32nd International Conference on Machine Learning,", "citeRegEx": "Jozefowicz et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Jozefowicz et al\\.", "year": 2015}, {"title": "Europarl: A Parallel Corpus for Statistical Machine Translation", "author": ["Philipp Koehn"], "venue": "In MT summit,", "citeRegEx": "Koehn.,? \\Q2005\\E", "shortCiteRegEx": "Koehn.", "year": 2005}, {"title": "QUality Estimation from ScraTCH (QUETCH): Deep Learning for Word-level Translation Quality Estimation", "author": ["Shigehiko Schamoni", "Stefan Riezler"], "venue": "In Proceedings of the Tenth Workshop on Statistical Ma-", "citeRegEx": "Kreutzer et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kreutzer et al\\.", "year": 2015}, {"title": "Data enhancement and selection strategies for the word-level Quality Estimation", "author": ["Chris Hokamp", "Lucia Specia"], "venue": "In Proceedings of the Tenth Workshop on Statistical Machine Translation,", "citeRegEx": "Logacheva et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Logacheva et al\\.", "year": 2015}, {"title": "Recurrent neural network based language model", "author": ["Martin Karafiat", "Lukas Burget", "Jan Cernocky", "Sanjeev Khudanpur"], "venue": "In Proceedings of Interspeech,", "citeRegEx": "Mikolov et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2010}, {"title": "Exploiting Similarities among Languages for Machine Translation", "author": ["Quoc V Le", "Ilya Sutskever"], "venue": "In CoRR,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "A scalable hierarchical distributed language model", "author": ["Mnih", "Hinton2009] Andriy Mnih", "Geoffrey E. Hinton"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Mnih et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2009}, {"title": "Hierarchical Probabilistic Neural Network Language Model", "author": ["Morin", "Bengio2005] Frederic Morin", "Yoshua Bengio"], "venue": "In Aistats,", "citeRegEx": "Morin et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Morin et al\\.", "year": 2005}, {"title": "Connectionist language modeling for large vocabulary continuous speech recognition", "author": ["Schwenk", "Gauvain2002] Holger Schwenk", "Jean-Luc Gauvain"], "venue": "In ICASSP. IEEE,", "citeRegEx": "Schwenk et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Schwenk et al\\.", "year": 2002}, {"title": "Continuous space language models", "author": ["Holger Schwenk"], "venue": "In Computer Speech and Language,", "citeRegEx": "Schwenk.,? \\Q2007\\E", "shortCiteRegEx": "Schwenk.", "year": 2007}, {"title": "SHEF-NN: Translation Quality Estimation with Neural Networks", "author": ["Shah et al.2015] Kashif Shah", "Varvara Logacheva", "G Paetzold", "Frederic Blain", "Daniel Beck", "Fethi Bougares", "Lucia Specia"], "venue": "In Proceedings of the Tenth Workshop", "citeRegEx": "Shah et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Shah et al\\.", "year": 2015}, {"title": "Strategy-Based Technology for Estimating MT Quality", "author": ["Shang et al.2015] Liugang Shang", "Dongfeng Cai", "Duo Ji"], "venue": "In Proceedings of the Tenth Workshop on Statistical Machine Translation,", "citeRegEx": "Shang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Shang et al\\.", "year": 2015}, {"title": "Parsing With Compositional Vector Grammars", "author": ["John Bauer", "Christopher D. Manning", "Andrew Y. Ng"], "venue": "In Proceedings of the ACL", "citeRegEx": "Socher et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Recursive deep models for semantic compositionality over a sentiment treebank", "author": ["Alex Perelygin", "Jy Wu"], "venue": "In Proceedings of EMNLP,", "citeRegEx": "Socher et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Estimating the Sentence-Level Quality of Machine Translation Systems", "author": ["Specia et al.2009] Lucia Specia", "Marco Turchi", "Nicola Cancedda", "Marc Dymetman", "Nello Cristianini"], "venue": "In 13th Conference of the European Association for Machine Translation,", "citeRegEx": "Specia et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Specia et al\\.", "year": 2009}, {"title": "Backpropagation through time: what it does and how to do it", "author": ["Paul J. Werbos"], "venue": "In IEEE,", "citeRegEx": "Werbos.,? \\Q1990\\E", "shortCiteRegEx": "Werbos.", "year": 1990}, {"title": "Recurrent neural networks for language understanding", "author": ["Yao et al.2013] Kaisheng Yao", "Geoffrey Zweig", "MeiYuh Hwang", "Yangyang Shi", "Dong Yu"], "venue": "In INTERSPEECH,", "citeRegEx": "Yao et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Yao et al\\.", "year": 2013}, {"title": "Spoken language understanding using long shortterm memory neural networks", "author": ["Yao et al.2014] Kaisheng Yao", "Baolin Peng", "Yu Zhang", "Dong Yu", "Geoffrey Zweig", "Yangyang Shi"], "venue": "In Spoken Language Technology Workshop (SLT),", "citeRegEx": "Yao et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Yao et al\\.", "year": 2014}, {"title": "ADADELTA: an adaptive learning rate method", "author": ["Matthew D. Zeiler"], "venue": null, "citeRegEx": "Zeiler.,? \\Q2012\\E", "shortCiteRegEx": "Zeiler.", "year": 2012}], "referenceMentions": [{"referenceID": 4, "context": "Quality estimation is the process to predict the quality of translation without any reference translation (Blatz et al., 2004; Specia et al., 2009).", "startOffset": 106, "endOffset": 147}, {"referenceID": 26, "context": "Quality estimation is the process to predict the quality of translation without any reference translation (Blatz et al., 2004; Specia et al., 2009).", "startOffset": 106, "endOffset": 147}, {"referenceID": 15, "context": "tions (Mikolov et al., 2010; Mikolov et al., 2013a; Mikolov et al., 2013b; Socher et al., 2013a; Socher et al., 2013b).", "startOffset": 6, "endOffset": 118}, {"referenceID": 2, "context": "as feed forward neural network language models (Schwenk and Gauvain, 2002; Bengio et al., 2003; Morin and Bengio, 2005; Schwenk, 2007) and Hierarchical Log-Bi-linear language models (Mnih and Hinton, 2009).", "startOffset": 47, "endOffset": 134}, {"referenceID": 21, "context": "as feed forward neural network language models (Schwenk and Gauvain, 2002; Bengio et al., 2003; Morin and Bengio, 2005; Schwenk, 2007) and Hierarchical Log-Bi-linear language models (Mnih and Hinton, 2009).", "startOffset": 47, "endOffset": 134}, {"referenceID": 20, "context": "Shang et al. (2015) tried to solve the problem of label imbalance with creating sub-labels like OK B (begin), OK I (intermediate), OK E (end).", "startOffset": 0, "endOffset": 20}, {"referenceID": 20, "context": "Shah et al. (2015) have used word embedding as an additional feature (+25 features) with SVM classifier.", "startOffset": 0, "endOffset": 19}, {"referenceID": 12, "context": "Bilingual Deep Neural Network (DNN) based model for word level QE was proposed by Kreutzer et al. (2015), in which word embedding was pre-trained and fine-tuned with other parameters of the network using stochastic gradient descent.", "startOffset": 82, "endOffset": 105}, {"referenceID": 8, "context": "de Souza et al. (2014) have used Bidirectional LSTM as a classifier for word level QE.", "startOffset": 3, "endOffset": 23}, {"referenceID": 28, "context": "The architecture of RNN-LM has been used for Natural Language Understanding (NLU) (Yao et al., 2013; Yao et al., 2014) earlier.", "startOffset": 82, "endOffset": 118}, {"referenceID": 29, "context": "The architecture of RNN-LM has been used for Natural Language Understanding (NLU) (Yao et al., 2013; Yao et al., 2014) earlier.", "startOffset": 82, "endOffset": 118}, {"referenceID": 13, "context": "Our approach is quite similar to the Kreutzer et al. (2015), but we are using RNN instead of DNN.", "startOffset": 37, "endOffset": 60}, {"referenceID": 13, "context": "Our approach is quite similar to the Kreutzer et al. (2015), but we are using RNN instead of DNN. We have also tried to address the problem of label-imbalance, introducing sub-labels as suggested by Shang et al. (2015).", "startOffset": 37, "endOffset": 219}, {"referenceID": 6, "context": "For this task, we exploited RNN\u2019s extensions, Long Short-Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997) and Gated Recurrent Unit (GRU) (Cho et al., 2014).", "startOffset": 142, "endOffset": 160}, {"referenceID": 1, "context": "Simple RNN also suffers from the problem of exploding and vanishing gradient (Bengio et al., 1994).", "startOffset": 77, "endOffset": 98}, {"referenceID": 9, "context": "Different researchers use slightly different LSTM variants (Graves, 2013; Yao et al., 2014; Jozefowicz et al., 2015).", "startOffset": 59, "endOffset": 116}, {"referenceID": 29, "context": "Different researchers use slightly different LSTM variants (Graves, 2013; Yao et al., 2014; Jozefowicz et al., 2015).", "startOffset": 59, "endOffset": 116}, {"referenceID": 11, "context": "Different researchers use slightly different LSTM variants (Graves, 2013; Yao et al., 2014; Jozefowicz et al., 2015).", "startOffset": 59, "endOffset": 116}, {"referenceID": 7, "context": "Chung et al. (2014) found that GRU outperforms LSTM on a suit of tasks.", "startOffset": 0, "endOffset": 20}, {"referenceID": 3, "context": "We implemented all the models (LSTM, deep LSTM and GRU) with 1THEANO framework (Bergstra et al., 2010; Bastien et al., 2012) as described above.", "startOffset": 79, "endOffset": 124}, {"referenceID": 0, "context": "We implemented all the models (LSTM, deep LSTM and GRU) with 1THEANO framework (Bergstra et al., 2010; Bastien et al., 2012) as described above.", "startOffset": 79, "endOffset": 124}, {"referenceID": 27, "context": "To update the model parameters, we have used Truncated Back-Propagation-Through-Time (T-BPTT) (Werbos, 1990) (Werbos, 1990) with stochastic gradient descent.", "startOffset": 94, "endOffset": 108}, {"referenceID": 27, "context": "To update the model parameters, we have used Truncated Back-Propagation-Through-Time (T-BPTT) (Werbos, 1990) (Werbos, 1990) with stochastic gradient descent.", "startOffset": 109, "endOffset": 123}, {"referenceID": 30, "context": "We used Ada-delta (Zeiler, 2012) (Zeiler, 2012) to adapt the learning rate of each parameter automatically ( = 10\u22126 and \u03c1 = 0.", "startOffset": 18, "endOffset": 32}, {"referenceID": 30, "context": "We used Ada-delta (Zeiler, 2012) (Zeiler, 2012) to adapt the learning rate of each parameter automatically ( = 10\u22126 and \u03c1 = 0.", "startOffset": 33, "endOffset": 47}, {"referenceID": 12, "context": ") from EnglishGerman Europarl data (Koehn, 2005).", "startOffset": 35, "endOffset": 48}, {"referenceID": 12, "context": ") from EnglishGerman Europarl data (Koehn, 2005). For bilingual models, we restructured the source sentence (English) according to the target (German) using word alignment provided by the organizers. For many-to-one mapping in the alignment (English-German), we chose the first alignment only. The \u2018NULL\u2019 token was assigned to the words where were not aligned with any word on the target side. The input of the model is constructed by concatenating context words of source and target. For example, consider the source word sequence s1s2s3, and the target word sequence t1t2t3, then the input to the network will be s1s2s3t1t2t3. In the training data, the distribution of the labels (OK/BAD) is skewed (OK to BAD ratio is approx. 4:1). To handle the issue, we tried one of the strategies proposed by Shang et al. (2015),", "startOffset": 36, "endOffset": 819}, {"referenceID": 6, "context": "From the result tables, it is evident that GRU outperforms LSTM as reported by Cho et al. (2014) for this task as well.", "startOffset": 79, "endOffset": 97}, {"referenceID": 13, "context": "The results of Bilingual models are better than monolingual models, as reported by Kreutzer et al. (2015).", "startOffset": 83, "endOffset": 106}, {"referenceID": 7, "context": "It is hard to say which model will perform better in what conditions or in general (Chung et al., 2014).", "startOffset": 83, "endOffset": 103}], "year": 2016, "abstractText": "This paper describes our submission to the shared task on word/phrase level Quality Estimation (QE) in the First Conference on Statistical Machine Translation (WMT16). The objective of the shared task was to predict if the given word/phrase is a correct/incorrect (OK/BAD) translation in the given sentence. In this paper, we propose a novel approach for word level Quality Estimation using Recurrent Neural Network Language Model (RNN-LM) architecture. RNN-LMs have been found very effective in different Natural Language Processing (NLP) applications. RNN-LM is mainly used for vector space language modeling for different NLP problems. For this task, we modify the architecture of RNNLM. The modified system predicts a label (OK/BAD) in the slot rather than predicting the word. The input to the system is a word sequence, similar to the standard RNN-LM. The approach is language independent and requires only the translated text for QE. To estimate the phrase level quality, we use the output of the word level QE system.", "creator": "LaTeX with hyperref package"}}}