{"id": "1610.04900", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Oct-2016", "title": "Convergence rate of stochastic k-means", "abstract": "We analyze online and mini-batch k-means variants. Both scale up the widely used Lloyd 's algorithm via stochastic approximation, and have become popular for large-scale clustering and unsupervised feature learning. We show, for the first time, that they have global convergence towards local optima at $O(\\frac{1}{t})$ rate under general conditions. In addition, we show if the dataset is clusterable, with suitable initialization, mini-batch k-means converges to an optimal k-means solution with $O(\\frac{1}{t})$ convergence rate with high probability. The k-means objective is non-convex and non-differentiable: we exploit ideas from non-convex gradient-based optimization by providing a novel characterization of the trajectory of k-means algorithm on its solution space, and circumvent its non-differentiability via geometric insights about k-means update.", "histories": [["v1", "Sun, 16 Oct 2016 18:59:59 GMT  (326kb,D)", "http://arxiv.org/abs/1610.04900v1", null], ["v2", "Mon, 7 Nov 2016 18:20:06 GMT  (423kb,D)", "http://arxiv.org/abs/1610.04900v2", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["cheng tang", "claire monteleoni"], "accepted": false, "id": "1610.04900"}, "pdf": {"name": "1610.04900.pdf", "metadata": {"source": "CRF", "title": "Convergence rate of stochastic k-means", "authors": ["Cheng Tang", "Claire Monteleoni"], "emails": ["tangch@gwu.edu", "cmontel@gwu.edu"], "sections": [{"heading": "1 Introduction", "text": "x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x,"}, {"heading": "2 Related work and our contributions", "text": "This year it is more than ever before in the history of the city."}, {"heading": "3.3 Local Lipschitzness and clusterability", "text": "As discussed in Section 3.1, boundary points are boundary points for defining mapping v >, but how likely do they arise in practice? We answer this question by revealing the geometric implication of a boundary point that will lead us to discover the link between local lipschitzness of v and clusterability."}, {"heading": "4.1 Local convergence in the presence of stochastic noise", "text": "In contrast to a convex problem, the difficulty of establishing a local convergence in our case is that the solution of the algorithm is displaced from the current environment of the attraction by stochastic noise to any iteration. Determining a (b0, \u03b1) stable stationary point C *, provided that the algorithm is in the vicinity of the attraction of C * at the time of iteration, can be formalized as follows: \"t: = (Ci, C *) \u2264 b0\u0445, (4) Letting t > < (4) Letting t \u00b2 leads to the following definition:\" So we must show Pr (Ci, C *) \u2264 b0\u0445, (5), even with stoic noise."}, {"heading": "4.1.1 Inequality for a martingale-like process", "text": "We use \"t\" (Ct, C) as an abbreviation and we leave \"e\" (\"t\") (\"t\") (\"t\") (\"t\") (\"t\") (\"t\") (\"t\") (\"t\") (\"t\") (\"t\") (\"t\") (\"t\") (\"t\") (\"t\") (\"t\") (\"t\") (\"t\") (\"t\") (\"t\") (\"t\" (t \") (\" t \") (\" t \"(t\") (\"t\") (\"t\") (\"t\") (t \"t\" (t) (t) (t) (t (t) (t) (\"t\" (t) (t) (t) (t (t) (t) (t) (t) (\"t) (t) (t) (t) (t) (t) (t) (t) (t) (\" t) (t) (t) (\"t) (t) (\" t \"t\" (t) (t) (\"t) (t) (\" t) (\"t) (\" t) (\"t\" (t \"(t\") (t) (\"(t) (\" (t) (\"(t) (t) (\" (t) (t) (\"(t) (t) (t) (\" (t) (\"(t) (t) (\" (t) (t) (\"(t) (\" (t) (t) (t) (t) (t) (\"(\" (t) (t) (\"(t) (\" (t) (t) (t) (\"(t) (t) (t) (t) (\" (t) (\"(t) (t) (\" (t) (t) (\"(t) (t) (t) (\" (t) (t) (\"(t) (t) (t) (t) (\" (t) (t) (\"(t) (t) (\" (t) (\"(t) (t) (t) (\" (t) (\"(t) ("}, {"heading": "4.1.2 Proof sketch of main theorems", "text": "Equipped with the necessary ingredients, we are ready to explain the analysis that leads to our main theories (> r). Theorem 1 is a global convergence result. To prove it, we divide our analysis of algorithm 1 into two phases, which are characterized by global convergence and local convergence, indicated by the distance from the current solution to stationary points. (Ct, C) We define the global convergence phase as a time interval of random length so that global convergence is so advanced that global convergence is so advanced that global convergence is so advanced that global convergence is so advanced that global convergence is so advanced that global convergence is so advanced that global convergence is so advanced that global convergence is so advanced that global convergence is so advanced that global convergence is so advanced that global convergence is defined as global convergence is (Cf, Cf, C)."}, {"heading": "5 Experiments", "text": "To verify the O (1t) convergence rate of theorem 1, we run stochastick averages with varying learning rates, mini-batch size, and k on RCV1 [15]. Thedataset has manually categorized 804414 Newswire stories with 103 topics, each story being a 47236-dimensional sparse vector; it was used in [20] for empirical evaluation of mini-batch-k averages. We experimented with both the flat learning rate in (3) and the adaptive learning rate (6), which we call the BBS rate, as proposed by the authors of [6, 20]. Figure 2 shows the convergence in k averages of k-mean algorithms over 100 iterations for varying m and k; fix each pair (m, k) that we initialize with the 1-line, with a series of k-centricity and stostic averages running with varying learning parameters."}, {"heading": "6 Discussion", "text": "This paper provides the first analysis of the convergence rate of stochastic means, but several questions remain unanswered. Firstly, our analysis refers to the flat learning rate in (3), while adaptive learning rate in (6) is more common in practice. From our experiments, we assume that O (1t) convergence can also be achieved in the latter case. As discussed in Section 4.1.1, the key question is whether we are OK with the adaptive rate, a random quantity that depends on all the information before t. Secondly, we provide two examples of assumptions that imply the lipschitzness of v. Can we find other assumptions? In particular, there is a spectrum of assumptions between our assumptions (A) and (B) that imply varying degrees of lipschitziness? We also believe that further investigations of batch k means can be made using our framework in Section 3.1."}, {"heading": "7 Appendix A: supplementary materials to Sec-", "text": "In this part of the appendix, we provide details of the construction of our framework that are not included in Section 3.1 (due to space constraints); in this part of the appendix, we provide details of the construction of our framework that are not included in Section 3.1 (due to space constraints); one problem with k means is that we consider an extended cluster space that represents the unification of all k clusters with 1 \u2264 k clusters; to handle the degenerated cases, we use the pre-image v \u2212 1 (A) to denote the non-boundaries C, so that v (C) = A, i.e., these are the sets of non-boundaries in the equivalence class."}, {"heading": "7.1 Local Lipschitzness and clusterability", "text": "The proof for Lemma 2. \".\" 1 \".\" 2. \".\" A. \"\". \"A.\" \".\" A. \"\". \".\" A. \"\". \"\" A. \"\". \".\" \"A.\" \".\". \"\" A. \"\". \"\" A. \"\". \"\" A. \"\". \".\" \"A.\". \"\". \"\". \"\" A. \"\". \"\". \"\". \"\". \".\" A. \"\". \"\". \"\". \"A.\" \".\" \"\". \".\". \"\" A. \"\". \"\". \"\" \".\" \"\". \"\". \"\" \".\" \"\". \"\". \"\" \".\" A. \"\" \".\" \"\". \"\" \".\""}, {"heading": "8 Appendix B: Proof of Theorem 3", "text": "Theorem 3. Attach any 0 < \u03b4 \u2264 1e. Suppose C is stable (bo, \u03b1). If we run algorithm 1 with parameters that are satisfactory > ln (1 \u2212 \u221a \u03b1) ln (1 \u2212 45p \u0445 min) c \"> \u03b22 [1 \u2212 \u221a \u03b1 \u2212 (1 \u2212 45p \u0445 min) m] with \u03b2 \u2265 2to \u2265 768 (c\") 2 (1 + 1bo) 2n2 ln21\u03b4Then, if at some point in the iteration i, \u0432 i \u2264 12bo\u03c6, we have the following parameters: t > i, Pr (\u0445t) \u2265 1 \u2212 \u03b4 and Et [\u0441t] \u2264 (to + i + 1to + t + 1) \u03b2 \u0445 i + (c \") 2B\u03b2 \u2212 1 (to + i + 2 to + i + 1) \u03b2 + 11to + t + 1where B: = 4 (bo + 1) n\u03c6."}, {"heading": "8.1 Proofs leading to Theorem 3", "text": "In the subsequent analysis, we use Et [\u00b7] as a shortened notation for E [\u00b7 t], in which we deal with the question of where we are. \u2212 s The noise concepts that occur in our analysis are: E [str] x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x (x x x x x x x x x x x x x x x x x (x x x x x x x x x x x x (x x x x x x x x x x x x x x x x x x (x x x x x x x x x x x x x x x x x x x x x x x x x x (x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x"}, {"heading": "9 Proofs of Theorem 1 and Theorem 2", "text": "In our analysis we therefore treat each cluster r as an updated learning rate, and differentiate between a sampled and non-sampled cluster only by defining c-tr.pt / tr.Proof leading to theorem 1 Lemma 14, we are not able to rate the 1-to-1-to-1-to-1-to-1-to-1-to-1-to-1-to-1-to-1-to-1-to-1-to-1-to-1-to-1-to-to-1-to-to-1-to-to-1-to-to-1-to-to-1-to-to-to-1-to-to-to-1-to-to-1-to-to-to-1-to-to-to-1-to-to-to-to-1-to-to-to-1-to-to-to-to-1-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-"}, {"heading": "G := {\u2203T \u2265 1,\u2203A\u2217 \u2208 {A\u2217}[k], s.t. At = A\u2217,\u2200t \u2265 T}", "text": "Secondly, we want to determine the expected convergence rate of the algorithm to this stationary cluster formation. (To prove that event G has a high probability, we first consider the random variable. (This is the first time that the algorithm \"hits\" a stationary cluster formation. \"(This is a stop time, since it is a stop time, since it is a Ft measurable number.) From Lemma 15Pr ({S < S}) {S {S} n {S} n {S} n {S} n {S} n = 0, {S} n {S) the stationary cluster formation is measured. From Lemma 15Pr ({S < S}) = Pr ({S} N) = Pr (T \u00b2) 0 {S = 0 {S})) we will designate the stationary cluster formation. (We call the stationary cluster formation, that the algorithm\" hits. \""}, {"heading": "9.1 Existence of stable stationary point under geometric assumptions on the dataset", "text": "First, we note that our assumption (B) contains two lower limits on the line connecting c + c + s and its orthogonal component: x = 12 (c + c) s (c + c) s (c) s (c) s (c) s (c) s (c) s (c) s (c) s (c) s (c) s (c) s (c) s (c) s (c) s (c) s (c) s (c) s (c) s (c) s) s (c) s (c) s) s (c) s (c) s (c) s (c) s (c) s (c) s (c) s (c) s (c) s (c) s (c) s (c) s (c) s) c (c) s (c) s (c) s (s) c (s) s (c) s (c) s (c) s (c) s (c) s (c) s (c) s (c) s (c) s (c) s (c) s (c) s (c) s (c) s (c) s) s (c) s (c) s (c) s (c) s (c) s (c) s (c) s (c) s (c) s (c) s (c) s (c) s (c) s (c) s (c) s (c) s (c) s (c) s (c) s (c) s (c) s (c) s (c) s (c (c) s) s (c (c) s (c) s (c) s (c) s (c) s (c (c) s (c) s (s (c) s (c) s (c (s) s (s (s) c) c) c (s (s (s) c (s (s) c) c (s (s) c (s (s) c (s (s (s) c) c) c (s (s (s (s (s) c) c) c (s (s (s) c (s (s) c (s (s (s (s) c (s)"}, {"heading": "10 Appendix D: auxiliary lemmas", "text": "It is not the first time that we are in a country, in which we are in a world, in which we are in a world, in which we are in a world, in which we are in a world, in which we are in a world, in which we are in a world, in which we are in a world, in which we are in a world, in which we are in a world, in which we are in a world, in which we live in a world, in which we live in a world, in which we live in a world, in which we live in a world, in which we live in a world, in which we live in a world, in which we live in a world, in which we live in a world, in which we live in a world, in which we live, in which we live in a world, in which we live in a world, in which we live, in which we live in a world, in which we live, in which we live in a world, in which we live, in which we live in a world, in which we live in a world, in which we live in which we live, in which we live in which we live in a world, in which we live in which we live in which we live in a world, in which we live in which we live in which we live in which we live in a world, in which we live in which we live in which we live in which we live in a world, in which we live in which we live in which we live in which we live in a world, in which we live in which we live in which we live in which we live in which we live in a world, in which we live in which we live in which we live in which we live in a world, in which we live in which we live in which we live in which we live in which we live in a world in which we live in a world, in which we live in which we live in which we live in which we live in which we live in a world in which we live in which we live in a world, in which we live in which we live in which we live in which we live in a world in which we live in which we live in which we live in which we live in a world in which we live in which we live in which we live in a world in which we live in which we live in which we live in which we live in"}], "references": [], "referenceMentions": [], "year": 2017, "abstractText": "<lb>We analyze online [6] and mini-batch [20] k-means variants. Both<lb>scale up the widely used Lloyd\u2019s algorithm via stochastic approximation,<lb>and have become popular for large-scale clustering and unsupervised<lb>feature learning. We show, for the first time, that they have global<lb>convergence towards \u201clocal optima\u201d at rate<lb>O(1t ) under general condi-<lb>tions. In addition, we show if the dataset is clusterable, with suitable<lb>initialization, mini-batch k-means converges to an optimal k-means<lb>solution at rate<lb>O(1t ) with high probability. The k-means objective is<lb>non-convex and non-differentiable: we exploit ideas from non-convex<lb>gradient-based optimization by providing a novel characterization of the<lb>trajectory of k-means algorithm on its solution space, and circumvent<lb>its non-differentiability via geometric insights about k-means update.", "creator": "LaTeX with hyperref package"}}}