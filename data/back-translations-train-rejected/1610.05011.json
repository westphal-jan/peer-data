{"id": "1610.05011", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Oct-2016", "title": "Interactive Attention for Neural Machine Translation", "abstract": "Conventional attention-based Neural Machine Translation (NMT) conducts dynamic alignment in generating the target sentence. By repeatedly reading the representation of source sentence, which keeps fixed after generated by the encoder (Bahdanau et al., 2015), the attention mechanism has greatly enhanced state-of-the-art NMT. In this paper, we propose a new attention mechanism, called INTERACTIVE ATTENTION, which models the interaction between the decoder and the representation of source sentence during translation by both reading and writing operations. INTERACTIVE ATTENTION can keep track of the interaction history and therefore improve the translation performance. Experiments on NIST Chinese-English translation task show that INTERACTIVE ATTENTION can achieve significant improvements over both the previous attention-based NMT baseline and some state-of-the-art variants of attention-based NMT (i.e., coverage models (Tu et al., 2016)). And neural machine translator with our INTERACTIVE ATTENTION can outperform the open source attention-based NMT system Groundhog by 4.22 BLEU points and the open source phrase-based system Moses by 3.94 BLEU points averagely on multiple test sets.", "histories": [["v1", "Mon, 17 Oct 2016 08:33:20 GMT  (562kb)", "http://arxiv.org/abs/1610.05011v1", "Accepted at COLING 2016"]], "COMMENTS": "Accepted at COLING 2016", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["fandong meng", "zhengdong lu", "hang li", "qun liu"], "accepted": false, "id": "1610.05011"}, "pdf": {"name": "1610.05011.pdf", "metadata": {"source": "CRF", "title": "Interactive Attention for Neural Machine Translation", "authors": ["Fandong Meng", "Zhengdong Lu", "Hang Li", "Qun Liu"], "emails": ["fandongmeng@tencent.com", "Lu.Zhengdong@huawei.com", "HangLi.HL@huawei.com", "qliu@computing.dcu.ie"], "sections": [{"heading": null, "text": "ar Xiv: 161 0.05 011v 1 [cs.C L] 17 Oct 201 6Interactive Attention for Neural Machine TranslationFandong Meng1 \u0445 Zhengdong Lu2 Hang Li2 Qun Liu3,41AI Platform Department, Tencent Technology Co., Ltd. fandongmeng @ tencent.com2Noah's Ark Lab, Huawei Technologies {Lu.Zhengdong, HangLi.HL} @ huawei.com3ADAPT Centre, School of Computing, Dublin City 4Key Laboratory of Intelligent Information Processing, Institute of Computing Technology, CASqliu @ computing.dcu.ie AbstractConventional attention-based Neural Machine Translation (NMT) performs a dynamic alignment in generating the target. By repeatedly reading the representation of the source set generated after the encoder (Bahdanau, 2015), attention can be significantly increased on this mechanism."}, {"heading": "1 Introduction", "text": "However, the results of this study are based on the translation of words from the source code (Li et al., 2016; Tu et al., 2016; Shen et al., 2016; Zhou et al., 2016), in which the attention model plays an increasingly important role. Attention-based NMT represents the source sentence as a sequence of vectors working according to an RNN or bi-directional RNN model (Schuster and Paliwal, 1997), and then simultaneously performs a dynamic alignment with a neural network and generating the target sentence with another RNN. Usually, NMT is more efficient with an attention model than its attention-free counterpart: it can achieve comparable results with far fewer parameters and training."}, {"heading": "2 Background", "text": "Our work builds on the attention-based NMT (Bahdanau et al., 2015), which takes as input a sequence of vector representations of the source sentence generated by an RNN or a bidirectional RNN, and then collectively learns to align and translate the vector representations during translation using an RNN decoder. Therefore, in this section we will take a look at the attention-based NMT before discussing the NMTIA in more detail in the next section."}, {"heading": "2.1 Attention-based Neural Machine Translation", "text": "Formally speaking, for an input source sequence x = {x1, x2, \u00b7 \u00b7, xN} and the previously generated target sequence y < t = {y1, y2, \u00b7 \u00b7, yt \u2212 1}, the probability of the next target word yt isp (yt | y < t, x) = softmax (f (ct, yt \u2212 1, st))) (1), where f (\u00b7) is a non-linear function and st is the state of the decoder RNN at time step t, which is calculated, ass = g (st \u2212 1, yt \u2212 1, ct) (2), where g (\u00b7) can be any activation function, we assume a more complex dynamic operator like in Gated Recurrent Unit (GRU et et et et al). (Cho et al., 2014)."}, {"heading": "2.2 Improved Attention Model", "text": "The alignment model at, j evaluates how well the output at position t matches the input around position j based on st \u2212 1 and hj. Intuitively, it should be advantageous to use the information from yt \u2212 1 directly when reading from the representation of the source set that is not implemented in the original attention-based NMT (Bahdanau et al., 2015). As shown in Figure 2, we insert this implementation into the attention model inspired by the recent implementation of the attention-based NMT1. This type of attention model can find a more effective alignment path by including both the previous hidden state st \u2212 1 and the previous context word yt \u2212 1. Then the calculation of e (t, j) a tanh (What, t \u2212 1 + Uahj) (5), where s-t \u2212 1 = U (queyst \u2212 1, 1 \u2212 dl) is \u2212 1 and \u2212 1 is \u2212 1."}, {"heading": "3 Interactive Attention", "text": "In this section, we will refer to the proposed interactivity of ATTENTION-based NMT, called NMTIA. Figure 3 shows the framework of the NMTIA with two rounds of interactive read / write operations (indicated by the yellow and red arrows), which assumes the same predictive model (Eq. 1) with improved attention-based NMT. Figure 3, h2,.. hN} the original set x = x2, \u00b7 xN}, we take H as memory, which contains N cells with the jth cell, which contains two key parts in Figure 3, hUr-ATTIVE ATTENTION."}, {"heading": "3.1 Read and Write of Interactive Attention", "text": "t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t, t"}, {"heading": "3.2 Optimization", "text": "Parameters to be optimized include the embedding of words in the source and target languages, the parameters for the encoder, decoder and other operations of NMTIA. Optimization is done via the standard backpropagation (BP), which aims to maximize the probability of the target sequence. In practice, we use the standard stochastic gradient descent (SGD) and mini-batch with learning rate controlled by AdaDelta (Zeiler, 2012)."}, {"heading": "4 Experiments", "text": "In this section we report on our empirical study on NMTIA on the translation task between Chinese and English. The experiments aim to answer the following questions: \u2022 Can NMTIA achieve significant improvements over conventional attention-based NMT? \u2022 Can NMTIA surpass the attention-based NMT coverage model (Tu et al., 2016)?"}, {"heading": "4.1 Data and Metric", "text": "Our training data consists of 1.25M set pairs extracted from the LDC corpora3, with 27.9M Chinese words and 34.5m English words, respectively. As a development set, we select the NIST 2002 (MT02) dataset, which is used to monitor the training process and determine the state of the early stop. As a test set, we use the NIST 2003 (MT03), 2004 (MT04), 2005 (MT05), 2006 (MT06) datasets, and the non-fall 4 gram NIST BLEU4 datasets with statistical significance check (character test (Collins et al., 2005) between the proposed models and the baselines. 3Companies include LDC2002E18, LDC2003E07, DCLC3LC32004T42007- and HansLC42004ftl resources."}, {"heading": "4.2 Training Details", "text": "In the formation of neural networks, we limit the source and target vocabulary to the most common 30K words in both Chinese and English, covering approximately 97.7% and 99.3% of the two corpus, respectively. All words outside the vocabulary are assigned to a special UNK token. We initialize the recurring weight matrices as random orthogonal matrices. All preconfiguration vectors are initialized to zero. For other parameters, we initialize them by selecting each element from the Gaussian distribution of mean 0 and variance 0.012. Parameters are updated by SGD and mini-batch (size 80), with the learning rate controlled by AdaDelta (Zeiler, 2012)."}, {"heading": "4.3 Comparison Systems", "text": "We compare our NMTIA with four systems: \u2022 Moses (Koehn et al., 2007): an open source phrase-based translation system5 with standard configuration. Word adaptations are achieved with GIZA + + (Och and Ney, 2003) on the training corpus in both directions, using the \"grow-diag-final-and\" balance strategy (Koehn et al., 2003). \u2022 The 4 gram language model with modified Kneser-Ney smoothing is trained with the SRILM toolkit (Stolcke et al., 2002) on the target part of the training data. \u2022 Groundhog: an open source NMT system6 implemented with the conventional attention model (Bahdanau et al., 2015). \u2022 RNsearch: our in-house implementation of the NMT system with the improved conventional attention model of TMT, as described in the \"attention search\" section 2 of the NMT during the coverage variants."}, {"heading": "4.4 Main Results", "text": "Before moving on to more detailed comparisons, we first note that \u2022 RNNsearch, implemented with the conventional attention model described in Section 2.1, outperforms Groundhog by an average of 2.38 BLEU points in four sets of tests; \u2022 RNNsearch, which only evaluates sentences of up to 50 words with 30K vocabulary, but can average 2.10 BLEU points over the open source phrase system Moses, which is trained with complete training data. Clearly from Table 1, NMTIA can achieve significant improvements over RNNSearch, by an average of 1.84 BLEU points in four sets of tests. We suspect this is because our INTERACTIVE ATTENTION mechanism can track the interaction history between the decoder and the display of the source set during translation, which can be helpful for the decoder to automatically distinguish which parts have been translated and which have not."}, {"heading": "4.5 INTERACTIVE ATTENTION Vs. Coverage Model", "text": "In fact, we have a coverage vector for each hidden state that attracts attention and feeds the coverage vector into the attention model. Therefore, we compare our coverage model with the coverage model in (Tu et al., 2016). There are two coverage models proposed in (Tu et al., 2016), including coverage models as special cases. We compare our coverage models with the coverage model in (Tu et al., 2016)."}, {"heading": "5 Related Work", "text": "Our work refers to recent work that focuses on improving attention models (Luong et al., 2015a; Cohn et al., 2016; Feng et al., 2016). Luong et al. (2015a) proposed using global and local attention models to improve translation performance, using a global one to take care of all source words, and a local one to look at a subset of source words at once. Cohn et al. (2016) expanded attention-based NMT to include structural distortions of word-based alignment models that have made improvements across multiple language pairs. Feng et al. (2016) added implicit distortions and fertility models to attention-based NMT to achieve translation improvements. This work differs from our INTERACTIVE ATTENTION approaches, as we use a more merit-based reading while we use more generic writing."}, {"heading": "6 Conclusion", "text": "We propose a simple but effective approach to INTERACTIVE ATTENTION, which models the interaction between the decoder and the representation of the source sentence during translation by reading and writing operations. Our empirical study on Chinese-English translation shows that INTERACTIVE ATTENTION can significantly improve the performance of NMT."}, {"heading": "Acknowledgements", "text": "Liu is partially supported by the Science Foundation Ireland (Grant 13 / RC / 2106) as part of the ADAPT Centre at Dublin City University. We warmly thank the anonymous reviewers for their thorough review and valuable suggestions."}, {"heading": "APPENDIX: Actual Translation Examples", "text": "In Appendix, we are giving some sample translations of RNsearch-80, NN-Cover-80 and NMTIA-80 to other countries, the problem of North Korean interference in the affairs of North Korea would be solved only by reference. We are highlighting some correct translation segments (or underlined by basic systems) in blue and false ones in red. Example Translationssrc! # $% # # # # - # - # - # - # - # # - # - # 82- # # # 82- # 82- # 82- # # # # # - # - # - # - # - # - # - # - # - # - # - # - # - # - # - # - # - # - # - # - # - # - # - # - # - # - # - # - # - # - # - # - # - # - # - # - # - # - # - # - # - # - # - # - # - # - # - # - # - # - # - # - # - # - # - # - # - # - # - # - # - # - # - # - # - # - # - # - # - # - # - # - - # - - # - - # - # - # - - - - # - - # - - # - - # - - - - # - - - # - - - - # - - - # - - - - # - - - - - # - - - - # - - - - - # - - - - # - - - - - # - - - - - # - - - - - - - # - - - - - - # - - - - - - # - - - - - - - - - - # - - - - - - - - # - - - - - - # - - - - - - - - - - - - # - - - - - - - - - - - - - - - - # - - - - - - - - - - - - - - - - - - # - - - - - - - - - - - - - - - - - - - - # - - - - - - - - - - - - - - - - - - - - - - - - - - - - -"}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "Proceedings of ICLR.", "citeRegEx": "Bahdanau et al\\.,? 2015", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "author": ["Kyunghyun Cho", "Bart van Merrienboer", "Caglar Gulcehre", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio."], "venue": "Proceedings of EMNLP, pages 1724\u20131734.", "citeRegEx": "Cho et al\\.,? 2014", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Incorporating structural alignment biases into an attentional neural translation model", "author": ["Trevor Cohn", "Cong Duy Vu Hoang", "Ekaterina Vymolova", "Kaisheng Yao", "Chris Dyer", "Gholamreza Haffari."], "venue": "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 876\u2013885, San Diego, California, June.", "citeRegEx": "Cohn et al\\.,? 2016", "shortCiteRegEx": "Cohn et al\\.", "year": 2016}, {"title": "Clause restructuring for statistical machine translation", "author": ["Michael Collins", "Philipp Koehn", "Ivona Ku\u010derov\u00e1."], "venue": "Proceedings of ACL, pages 531\u2013540.", "citeRegEx": "Collins et al\\.,? 2005", "shortCiteRegEx": "Collins et al\\.", "year": 2005}, {"title": "Implicit distortion and fertility models for attention-based encoder-decoder NMT model", "author": ["Shi Feng", "Shujie Liu", "Mu Li", "Ming Zhou."], "venue": "CoRR, abs/1601.03317.", "citeRegEx": "Feng et al\\.,? 2016", "shortCiteRegEx": "Feng et al\\.", "year": 2016}, {"title": "Neural turing machines", "author": ["Alex Graves", "Greg Wayne", "Ivo Danihelka."], "venue": "arXiv preprint arXiv:1410.5401.", "citeRegEx": "Graves et al\\.,? 2014", "shortCiteRegEx": "Graves et al\\.", "year": 2014}, {"title": "Reducing the dimensionality of data with neural networks", "author": ["Geoffrey E Hinton", "Ruslan R Salakhutdinov."], "venue": "Science, 313(5786):504\u2013507.", "citeRegEx": "Hinton and Salakhutdinov.,? 2006", "shortCiteRegEx": "Hinton and Salakhutdinov.", "year": 2006}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors", "author": ["Geoffrey E Hinton", "Nitish Srivastava", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan R Salakhutdinov."], "venue": "arXiv preprint arXiv:1207.0580.", "citeRegEx": "Hinton et al\\.,? 2012", "shortCiteRegEx": "Hinton et al\\.", "year": 2012}, {"title": "On using very large target vocabulary for neural machine translation", "author": ["S\u00e9bastien Jean", "Kyunghyun Cho", "Roland Memisevic", "Yoshua Bengio."], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 1\u201310, Beijing, China, July.", "citeRegEx": "Jean et al\\.,? 2015", "shortCiteRegEx": "Jean et al\\.", "year": 2015}, {"title": "Statistical phrase-based translation", "author": ["Philipp Koehn", "Franz Josef Och", "Daniel Marcu."], "venue": "Proceedings of NAACL, pages 48\u201354.", "citeRegEx": "Koehn et al\\.,? 2003", "shortCiteRegEx": "Koehn et al\\.", "year": 2003}, {"title": "Moses: Open source toolkit for statistical machine translation", "author": ["Philipp Koehn", "Hieu Hoang", "Alexandra Birch", "Chris Callison-Burch", "Marcello Federico", "Nicola Bertoldi", "Brooke Cowan", "Wade Shen", "Christine Moran", "Richard Zens", "Chris Dyer", "Ondrej Bojar", "Alexandra Constantin", "Evan Herbst."], "venue": "Proceedings of ACL on interactive poster and demonstration sessions, pages 177\u2013180, Prague, Czech Republic, June.", "citeRegEx": "Koehn et al\\.,? 2007", "shortCiteRegEx": "Koehn et al\\.", "year": 2007}, {"title": "Towards zero unknown word in neural machine translation", "author": ["Xiaoqing Li", "Jiajun Zhang", "Chengqing Zong."], "venue": "Proceedings of IJCAI.", "citeRegEx": "Li et al\\.,? 2016", "shortCiteRegEx": "Li et al\\.", "year": 2016}, {"title": "Effective approaches to attention-based neural machine translation", "author": ["Thang Luong", "Hieu Pham", "Christopher D. Manning."], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1412\u20131421.", "citeRegEx": "Luong et al\\.,? 2015a", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Addressing the rare word problem in neural machine translation", "author": ["Thang Luong", "Ilya Sutskever", "Quoc Le", "Oriol Vinyals", "Wojciech Zaremba."], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 11\u201319, Beijing, China, July.", "citeRegEx": "Luong et al\\.,? 2015b", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Neural transformation machine: A new architecture for sequence-to-sequence learning", "author": ["Fandong Meng", "Zhengdong Lu", "Zhaopeng Tu", "Hang Li", "Qun Liu."], "venue": "CoRR, abs/1506.06442.", "citeRegEx": "Meng et al\\.,? 2015", "shortCiteRegEx": "Meng et al\\.", "year": 2015}, {"title": "A systematic comparison of various statistical alignment models", "author": ["Franz Josef Och", "Hermann Ney."], "venue": "Computational linguistics, 29(1):19\u201351.", "citeRegEx": "Och and Ney.,? 2003", "shortCiteRegEx": "Och and Ney.", "year": 2003}, {"title": "Bidirectional recurrent neural networks", "author": ["Mike Schuster", "Kuldip K Paliwal."], "venue": "Signal Processing, IEEE Transactions on, 45(11):2673\u20132681.", "citeRegEx": "Schuster and Paliwal.,? 1997", "shortCiteRegEx": "Schuster and Paliwal.", "year": 1997}, {"title": "Minimum risk training for neural machine translation", "author": ["Shiqi Shen", "Yong Cheng", "Zhongjun He", "Wei He", "Hua Wu", "Maosong Sun", "Yang Liu."], "venue": "Proceedings of ACL, pages 1683\u20131692, Berlin, Germany, August.", "citeRegEx": "Shen et al\\.,? 2016", "shortCiteRegEx": "Shen et al\\.", "year": 2016}, {"title": "SRILM-an extensible language modeling toolkit", "author": ["Andreas Stolcke"], "venue": "Proceedings of ICSLP, volume 2, pages 901\u2013904.", "citeRegEx": "Stolcke,? 2002", "shortCiteRegEx": "Stolcke", "year": 2002}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc VV Le."], "venue": "Advances in Neural Information Processing Systems, pages 3104\u20133112.", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Neural machine translation with external phrase memory", "author": ["Yaohua Tang", "Fandong Meng", "Zhengdong Lu", "Hang Li", "Philip L.H. Yu."], "venue": "CoRR, abs/1606.01792.", "citeRegEx": "Tang et al\\.,? 2016", "shortCiteRegEx": "Tang et al\\.", "year": 2016}, {"title": "Modeling coverage for neural machine translation", "author": ["Zhaopeng Tu", "Zhengdong Lu", "Yang Liu", "Xiaohua Liu", "Hang Li."], "venue": "Proceedings of ACL, pages 76\u201385, Berlin, Germany, August. Association for Computational Linguistics.", "citeRegEx": "Tu et al\\.,? 2016", "shortCiteRegEx": "Tu et al\\.", "year": 2016}, {"title": "Memory-enhanced decoder for neural machine translation", "author": ["Mingxuan Wang", "Zhengdong Lu", "Hang Li", "Qun Liu."], "venue": "Proceedings of EMNLP.", "citeRegEx": "Wang et al\\.,? 2016", "shortCiteRegEx": "Wang et al\\.", "year": 2016}, {"title": "Memory networks", "author": ["Jason Weston", "Sumit Chopra", "Antoine Bordes."], "venue": "CoRR, abs/1410.3916.", "citeRegEx": "Weston et al\\.,? 2014", "shortCiteRegEx": "Weston et al\\.", "year": 2014}, {"title": "Adadelta: an adaptive learning rate method", "author": ["Matthew D Zeiler."], "venue": "arXiv preprint arXiv:1212.5701.", "citeRegEx": "Zeiler.,? 2012", "shortCiteRegEx": "Zeiler.", "year": 2012}, {"title": "Deep recurrent models with fast-forward connections for neural machine translation", "author": ["Jie Zhou", "Ying Cao", "Xuguang Wang", "Peng Li", "Wei Xu."], "venue": "CoRR, abs/1606.04199.", "citeRegEx": "Zhou et al\\.,? 2016", "shortCiteRegEx": "Zhou et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 0, "context": "By repeatedly reading the representation of source sentence, which keeps fixed after generated by the encoder (Bahdanau et al., 2015), the attention mechanism has greatly enhanced state-of-the-art NMT.", "startOffset": 110, "endOffset": 133}, {"referenceID": 21, "context": ", coverage models (Tu et al., 2016)).", "startOffset": 18, "endOffset": 35}, {"referenceID": 19, "context": "1 Introduction Neural Machine Translation (NMT) has made promising progress in recent years (Sutskever et al., 2014; Bahdanau et al., 2015; Luong et al., 2015a; Jean et al., 2015; Luong et al., 2015b; Tang et al., 2016; Wang et al., 2016; Li et al., 2016; Tu et al., 2016; Shen et al., 2016; Zhou et al., 2016), in which attention model plays an increasingly important role.", "startOffset": 92, "endOffset": 310}, {"referenceID": 0, "context": "1 Introduction Neural Machine Translation (NMT) has made promising progress in recent years (Sutskever et al., 2014; Bahdanau et al., 2015; Luong et al., 2015a; Jean et al., 2015; Luong et al., 2015b; Tang et al., 2016; Wang et al., 2016; Li et al., 2016; Tu et al., 2016; Shen et al., 2016; Zhou et al., 2016), in which attention model plays an increasingly important role.", "startOffset": 92, "endOffset": 310}, {"referenceID": 12, "context": "1 Introduction Neural Machine Translation (NMT) has made promising progress in recent years (Sutskever et al., 2014; Bahdanau et al., 2015; Luong et al., 2015a; Jean et al., 2015; Luong et al., 2015b; Tang et al., 2016; Wang et al., 2016; Li et al., 2016; Tu et al., 2016; Shen et al., 2016; Zhou et al., 2016), in which attention model plays an increasingly important role.", "startOffset": 92, "endOffset": 310}, {"referenceID": 8, "context": "1 Introduction Neural Machine Translation (NMT) has made promising progress in recent years (Sutskever et al., 2014; Bahdanau et al., 2015; Luong et al., 2015a; Jean et al., 2015; Luong et al., 2015b; Tang et al., 2016; Wang et al., 2016; Li et al., 2016; Tu et al., 2016; Shen et al., 2016; Zhou et al., 2016), in which attention model plays an increasingly important role.", "startOffset": 92, "endOffset": 310}, {"referenceID": 13, "context": "1 Introduction Neural Machine Translation (NMT) has made promising progress in recent years (Sutskever et al., 2014; Bahdanau et al., 2015; Luong et al., 2015a; Jean et al., 2015; Luong et al., 2015b; Tang et al., 2016; Wang et al., 2016; Li et al., 2016; Tu et al., 2016; Shen et al., 2016; Zhou et al., 2016), in which attention model plays an increasingly important role.", "startOffset": 92, "endOffset": 310}, {"referenceID": 20, "context": "1 Introduction Neural Machine Translation (NMT) has made promising progress in recent years (Sutskever et al., 2014; Bahdanau et al., 2015; Luong et al., 2015a; Jean et al., 2015; Luong et al., 2015b; Tang et al., 2016; Wang et al., 2016; Li et al., 2016; Tu et al., 2016; Shen et al., 2016; Zhou et al., 2016), in which attention model plays an increasingly important role.", "startOffset": 92, "endOffset": 310}, {"referenceID": 22, "context": "1 Introduction Neural Machine Translation (NMT) has made promising progress in recent years (Sutskever et al., 2014; Bahdanau et al., 2015; Luong et al., 2015a; Jean et al., 2015; Luong et al., 2015b; Tang et al., 2016; Wang et al., 2016; Li et al., 2016; Tu et al., 2016; Shen et al., 2016; Zhou et al., 2016), in which attention model plays an increasingly important role.", "startOffset": 92, "endOffset": 310}, {"referenceID": 11, "context": "1 Introduction Neural Machine Translation (NMT) has made promising progress in recent years (Sutskever et al., 2014; Bahdanau et al., 2015; Luong et al., 2015a; Jean et al., 2015; Luong et al., 2015b; Tang et al., 2016; Wang et al., 2016; Li et al., 2016; Tu et al., 2016; Shen et al., 2016; Zhou et al., 2016), in which attention model plays an increasingly important role.", "startOffset": 92, "endOffset": 310}, {"referenceID": 21, "context": "1 Introduction Neural Machine Translation (NMT) has made promising progress in recent years (Sutskever et al., 2014; Bahdanau et al., 2015; Luong et al., 2015a; Jean et al., 2015; Luong et al., 2015b; Tang et al., 2016; Wang et al., 2016; Li et al., 2016; Tu et al., 2016; Shen et al., 2016; Zhou et al., 2016), in which attention model plays an increasingly important role.", "startOffset": 92, "endOffset": 310}, {"referenceID": 17, "context": "1 Introduction Neural Machine Translation (NMT) has made promising progress in recent years (Sutskever et al., 2014; Bahdanau et al., 2015; Luong et al., 2015a; Jean et al., 2015; Luong et al., 2015b; Tang et al., 2016; Wang et al., 2016; Li et al., 2016; Tu et al., 2016; Shen et al., 2016; Zhou et al., 2016), in which attention model plays an increasingly important role.", "startOffset": 92, "endOffset": 310}, {"referenceID": 25, "context": "1 Introduction Neural Machine Translation (NMT) has made promising progress in recent years (Sutskever et al., 2014; Bahdanau et al., 2015; Luong et al., 2015a; Jean et al., 2015; Luong et al., 2015b; Tang et al., 2016; Wang et al., 2016; Li et al., 2016; Tu et al., 2016; Shen et al., 2016; Zhou et al., 2016), in which attention model plays an increasingly important role.", "startOffset": 92, "endOffset": 310}, {"referenceID": 16, "context": "Attention-based NMT represents the source sentence as a sequence of vectors after a RNN or bi-directional RNN (Schuster and Paliwal, 1997), and then simultaneously conducts dynamic alignment with a gating neural network and generation of the target sentence with another RNN.", "startOffset": 110, "endOffset": 138}, {"referenceID": 8, "context": "Usually NMT with attention model is more efficient than its attention-free counterpart: it can achieve comparable results with far less parameters and training instances (Jean et al., 2015).", "startOffset": 170, "endOffset": 189}, {"referenceID": 19, "context": "This superiority in efficiency comes mainly from the mechanism of dynamic alignment, which avoids the need to represent the entire source sentence with a fixed-length vector (Sutskever et al., 2014).", "startOffset": 174, "endOffset": 198}, {"referenceID": 0, "context": "However, conventional attention model is conducted on the representation of source sentence (fixed after generated) only with reading operation (Bahdanau et al., 2015; Luong et al., 2015a).", "startOffset": 144, "endOffset": 188}, {"referenceID": 12, "context": "However, conventional attention model is conducted on the representation of source sentence (fixed after generated) only with reading operation (Bahdanau et al., 2015; Luong et al., 2015a).", "startOffset": 144, "endOffset": 188}, {"referenceID": 21, "context": "This may let the decoder tend to ignore past attention information, and lead to over-translation and undertranslation (Tu et al., 2016).", "startOffset": 118, "endOffset": 135}, {"referenceID": 0, "context": ", 2014; Bahdanau et al., 2015; Luong et al., 2015a; Jean et al., 2015; Luong et al., 2015b; Tang et al., 2016; Wang et al., 2016; Li et al., 2016; Tu et al., 2016; Shen et al., 2016; Zhou et al., 2016), in which attention model plays an increasingly important role. Attention-based NMT represents the source sentence as a sequence of vectors after a RNN or bi-directional RNN (Schuster and Paliwal, 1997), and then simultaneously conducts dynamic alignment with a gating neural network and generation of the target sentence with another RNN. Usually NMT with attention model is more efficient than its attention-free counterpart: it can achieve comparable results with far less parameters and training instances (Jean et al., 2015). This superiority in efficiency comes mainly from the mechanism of dynamic alignment, which avoids the need to represent the entire source sentence with a fixed-length vector (Sutskever et al., 2014). However, conventional attention model is conducted on the representation of source sentence (fixed after generated) only with reading operation (Bahdanau et al., 2015; Luong et al., 2015a). This may let the decoder tend to ignore past attention information, and lead to over-translation and undertranslation (Tu et al., 2016). To address this problem, Tu et al. (2016) proposed to maintain tag vec\u2217The majority of this work was completed when the first author studied at Institute of Computing Technology, Chinese Academy of Sciences.", "startOffset": 8, "endOffset": 1302}, {"referenceID": 5, "context": "Inspired by neural turing machines (Graves et al., 2014), we propose INTERACTIVE ATTENTION model from the perspective of memory reading-writing, which provides a conceptually simpler and practically more effective mechanism for attention-based NMT.", "startOffset": 35, "endOffset": 56}, {"referenceID": 0, "context": "Experiment results show that NMTIA can significantly outperform both the conventional attention-based NMT baseline (Bahdanau et al., 2015) and coverage models (Tu et al.", "startOffset": 115, "endOffset": 138}, {"referenceID": 21, "context": ", 2015) and coverage models (Tu et al., 2016).", "startOffset": 28, "endOffset": 45}, {"referenceID": 0, "context": "2 Background Our work is built upon the attention-based NMT (Bahdanau et al., 2015), which takes a sequence of vector representations of the source sentence generated by a RNN or bi-directional RNN as input, and then jointly learns to align and translate by reading the vector representations during translation with a RNN decoder.", "startOffset": 60, "endOffset": 83}, {"referenceID": 1, "context": "st = g(st\u22121, yt\u22121, ct) (2) where g(\u00b7) can be any activation function, here we adopt a more sophisticated dynamic operator as in Gated Recurrent Unit (GRU) (Cho et al., 2014).", "startOffset": 155, "endOffset": 173}, {"referenceID": 16, "context": "Formally, hj = [ \u2212\u2192 hj T , \u2190\u2212 hj T ] is the annotation of xj , which is computed by a bi-directional RNN (Schuster and Paliwal, 1997) with GRU and contains information about the whole input sequence with a strong focus on the parts surrounding xj .", "startOffset": 105, "endOffset": 133}, {"referenceID": 0, "context": "This is called automatic alignment (Bahdanau et al., 2015) or attention model (Luong et al.", "startOffset": 35, "endOffset": 58}, {"referenceID": 12, "context": ", 2015) or attention model (Luong et al., 2015a), but it is essentially reading with content-based addressing defined in (Graves et al.", "startOffset": 27, "endOffset": 48}, {"referenceID": 5, "context": ", 2015a), but it is essentially reading with content-based addressing defined in (Graves et al., 2014).", "startOffset": 81, "endOffset": 102}, {"referenceID": 19, "context": "With the attention model, it releases the need to summarize the entire sentence with a single fixed-length vector (Sutskever et al., 2014; Cho et al., 2014).", "startOffset": 114, "endOffset": 156}, {"referenceID": 1, "context": "With the attention model, it releases the need to summarize the entire sentence with a single fixed-length vector (Sutskever et al., 2014; Cho et al., 2014).", "startOffset": 114, "endOffset": 156}, {"referenceID": 0, "context": "Intuitively, it should be beneficial to directly exploit the information of yt\u22121 when reading from the representation of source sentence, which is not implemented in the original attention-based NMT (Bahdanau et al., 2015).", "startOffset": 199, "endOffset": 222}, {"referenceID": 21, "context": "Clearly, INTERACTIVE ATTENTION can subsume the coverage models in (Tu et al., 2016) as special cases while conceptually simpler.", "startOffset": 66, "endOffset": 83}, {"referenceID": 5, "context": "We can use contentbased addressing to determine w t as described in (Graves et al., 2014) or (quite similarly) use the reading mechanism such as the attention model in Section 2.", "startOffset": 68, "endOffset": 89}, {"referenceID": 5, "context": "Attentive Write Inspired by the writing operation of neural turing machines (Graves et al., 2014), we define two types of operation on writing to the memory: FORGET and UPDATE.", "startOffset": 76, "endOffset": 97}, {"referenceID": 5, "context": "Attentive Write Inspired by the writing operation of neural turing machines (Graves et al., 2014), we define two types of operation on writing to the memory: FORGET and UPDATE. FORGET is similar 2 Wang et al. (2016) verified the former one for the read operation on the external memory.", "startOffset": 77, "endOffset": 216}, {"referenceID": 24, "context": "In practice, we use the standard stochastic gradient descent (SGD) and mini-batch with learning rate controlled by AdaDelta (Zeiler, 2012).", "startOffset": 124, "endOffset": 138}, {"referenceID": 21, "context": "The experiments are designed to answer the following questions: \u2022 Can NMTIA achieve significant improvements over the conventional attention-based NMT? \u2022 Can NMTIA outperform the attention-based NMT with coverage model (Tu et al., 2016)? 4.", "startOffset": 219, "endOffset": 236}, {"referenceID": 3, "context": "We use the case-insensitive 4-gram NIST BLEU4 as our evaluation metric, with statistical significance test (sign-test (Collins et al., 2005)) between the proposed models and the baselines.", "startOffset": 118, "endOffset": 140}, {"referenceID": 24, "context": "The parameters are updated by SGD and mini-batch (size 80) with learning rate controlled by AdaDelta (Zeiler, 2012) (\u01eb = 1e\u22126 and \u03c1 = 0.", "startOffset": 101, "endOffset": 115}, {"referenceID": 0, "context": "We train the NMT systems with the sentences of length up to 50 words in training data, and set the dimension of word embedding to 620 and the size of the hidden layer to 1000, following the settings in (Bahdanau et al., 2015).", "startOffset": 202, "endOffset": 225}, {"referenceID": 7, "context": "We also use dropout for our baseline NMT systems and NMTIA to avoid over-fitting (Hinton et al., 2012).", "startOffset": 81, "endOffset": 102}, {"referenceID": 6, "context": "Inspired by the effort on easing the training of very deep architectures (Hinton and Salakhutdinov, 2006), we use a simple pre-training strategy to train our NMTIA.", "startOffset": 73, "endOffset": 105}, {"referenceID": 0, "context": "First we train a regular attention-based NMT model (Bahdanau et al., 2015).", "startOffset": 51, "endOffset": 74}, {"referenceID": 10, "context": "3 Comparison Systems We compare our NMTIA with four systems: \u2022 Moses (Koehn et al., 2007): an open source phrase-based translation system5 with default configuration.", "startOffset": 69, "endOffset": 89}, {"referenceID": 15, "context": "The word alignments are obtained with GIZA++ (Och and Ney, 2003) on the training corpora in both directions, using the \u201cgrow-diag-final-and\u201d balance strategy (Koehn et al.", "startOffset": 45, "endOffset": 64}, {"referenceID": 9, "context": "The word alignments are obtained with GIZA++ (Och and Ney, 2003) on the training corpora in both directions, using the \u201cgrow-diag-final-and\u201d balance strategy (Koehn et al., 2003).", "startOffset": 158, "endOffset": 178}, {"referenceID": 0, "context": "The 4-gram language model with modified Kneser-Ney smoothing is trained on the target portion of training data with the SRILM toolkit (Stolcke and others, 2002), \u2022 Groundhog: an open source NMT system6 implemented with the conventional attention model (Bahdanau et al., 2015).", "startOffset": 252, "endOffset": 275}, {"referenceID": 21, "context": "\u2022 Coverage Model: state-of-the-art variants of attention-based NMT model (Tu et al., 2016) which improve the attention mechanism through modeling a soft coverage on the source representation http://www.", "startOffset": 73, "endOffset": 90}, {"referenceID": 21, "context": "Table 2: BLEU-4 scores (%) of the conventional attention-based model (RNNsearch-80), the neural network based coverage model (NN-Cover-80) (Tu et al., 2016) and our INTERACTIVE ATTENTION model (NMTIA-80).", "startOffset": 139, "endOffset": 156}, {"referenceID": 21, "context": "\u201c-80\u201d means the models are trained with the sentences of length up to 80 words, which is consistent with the setting in (Tu et al., 2016).", "startOffset": 120, "endOffset": 137}, {"referenceID": 21, "context": "We hence compare our INTERACTIVE ATTENTION model with the coverage model in (Tu et al., 2016).", "startOffset": 76, "endOffset": 93}, {"referenceID": 21, "context": "There are two coverage models proposed in (Tu et al., 2016), including linguistic coverage model and neural network based coverage model (NN-Cover).", "startOffset": 42, "endOffset": 59}, {"referenceID": 21, "context": "Although the coverage models are originally implemented on Groundhog in (Tu et al., 2016), they can be easily adapted to the \u201cRNNsearch\u201d.", "startOffset": 72, "endOffset": 89}, {"referenceID": 21, "context": "Following the setting in (Tu et al., 2016), we conduct the comparison with the training sentences of length up to 80 words.", "startOffset": 25, "endOffset": 42}, {"referenceID": 21, "context": "A more detailed comparison between conventional attention model (RNNsearch-80), neural network based coverage model (NN-Cover-80) (Tu et al., 2016) and NMTIA-80 suggests that our NMTIA80 is quite consistent on outperforming the conventional attention model and the coverage model.", "startOffset": 130, "endOffset": 147}, {"referenceID": 21, "context": "Coverage Model Tu et al. (2016) proposed two coverage models to let the NMT system to consider more about untranslated source words.", "startOffset": 15, "endOffset": 32}, {"referenceID": 12, "context": "5 Related Work Our work is related to recent works that focus on improving attention models (Luong et al., 2015a; Cohn et al., 2016; Feng et al., 2016).", "startOffset": 92, "endOffset": 151}, {"referenceID": 2, "context": "5 Related Work Our work is related to recent works that focus on improving attention models (Luong et al., 2015a; Cohn et al., 2016; Feng et al., 2016).", "startOffset": 92, "endOffset": 151}, {"referenceID": 4, "context": "5 Related Work Our work is related to recent works that focus on improving attention models (Luong et al., 2015a; Cohn et al., 2016; Feng et al., 2016).", "startOffset": 92, "endOffset": 151}, {"referenceID": 5, "context": "Our work is inspired by recent efforts on attaching an external memory to neural networks, such as neural turing machines (Graves et al., 2014), memory networks (Weston et al.", "startOffset": 122, "endOffset": 143}, {"referenceID": 23, "context": ", 2014), memory networks (Weston et al., 2014; Meng et al., 2015) and exploiting an external memory (Tang et al.", "startOffset": 25, "endOffset": 65}, {"referenceID": 14, "context": ", 2014), memory networks (Weston et al., 2014; Meng et al., 2015) and exploiting an external memory (Tang et al.", "startOffset": 25, "endOffset": 65}, {"referenceID": 20, "context": ", 2015) and exploiting an external memory (Tang et al., 2016; Wang et al., 2016) during translation.", "startOffset": 42, "endOffset": 80}, {"referenceID": 22, "context": ", 2015) and exploiting an external memory (Tang et al., 2016; Wang et al., 2016) during translation.", "startOffset": 42, "endOffset": 80}, {"referenceID": 2, "context": ", 2015a; Cohn et al., 2016; Feng et al., 2016). Luong et al. (2015a) proposed to use global and local attention models to improve translation performance.", "startOffset": 9, "endOffset": 69}, {"referenceID": 2, "context": ", 2015a; Cohn et al., 2016; Feng et al., 2016). Luong et al. (2015a) proposed to use global and local attention models to improve translation performance. They use a global one to attend to all source words and a local one to look at a subset of source words at a time. Cohn et al. (2016) extended the attentionbased NMT to include structural biases from word-based alignment models, which achieved improvements across several language pairs.", "startOffset": 9, "endOffset": 289}, {"referenceID": 2, "context": ", 2015a; Cohn et al., 2016; Feng et al., 2016). Luong et al. (2015a) proposed to use global and local attention models to improve translation performance. They use a global one to attend to all source words and a local one to look at a subset of source words at a time. Cohn et al. (2016) extended the attentionbased NMT to include structural biases from word-based alignment models, which achieved improvements across several language pairs. Feng et al. (2016) added implicit distortion and fertility models to attention-based NMT to achieve translation improvements.", "startOffset": 9, "endOffset": 462}, {"referenceID": 2, "context": ", 2015a; Cohn et al., 2016; Feng et al., 2016). Luong et al. (2015a) proposed to use global and local attention models to improve translation performance. They use a global one to attend to all source words and a local one to look at a subset of source words at a time. Cohn et al. (2016) extended the attentionbased NMT to include structural biases from word-based alignment models, which achieved improvements across several language pairs. Feng et al. (2016) added implicit distortion and fertility models to attention-based NMT to achieve translation improvements. These works are different with our INTERACTIVE ATTENTION approach, as we use a rather generic attentive reading while at the same time performing attentive writing. Our work is inspired by recent efforts on attaching an external memory to neural networks, such as neural turing machines (Graves et al., 2014), memory networks (Weston et al., 2014; Meng et al., 2015) and exploiting an external memory (Tang et al., 2016; Wang et al., 2016) during translation. Tang et al. (2016) exploited a phrase memory for NMT, which stores phrase pairs in symbolic form.", "startOffset": 9, "endOffset": 1048}, {"referenceID": 2, "context": ", 2015a; Cohn et al., 2016; Feng et al., 2016). Luong et al. (2015a) proposed to use global and local attention models to improve translation performance. They use a global one to attend to all source words and a local one to look at a subset of source words at a time. Cohn et al. (2016) extended the attentionbased NMT to include structural biases from word-based alignment models, which achieved improvements across several language pairs. Feng et al. (2016) added implicit distortion and fertility models to attention-based NMT to achieve translation improvements. These works are different with our INTERACTIVE ATTENTION approach, as we use a rather generic attentive reading while at the same time performing attentive writing. Our work is inspired by recent efforts on attaching an external memory to neural networks, such as neural turing machines (Graves et al., 2014), memory networks (Weston et al., 2014; Meng et al., 2015) and exploiting an external memory (Tang et al., 2016; Wang et al., 2016) during translation. Tang et al. (2016) exploited a phrase memory for NMT, which stores phrase pairs in symbolic form. They let the decoder utilize a mixture of word-generating and phrase-generating component, to generate a sequence of multiple words all at once. Wang et al. (2016) extended the NMT decoder by maintaining an external memory, which is operated by reading and writing opera-", "startOffset": 9, "endOffset": 1291}, {"referenceID": 5, "context": "tions of neural turing machines (Graves et al., 2014), while keeping a read-only copy of the original source annotations along side the \u201cread-write\u201d memory.", "startOffset": 32, "endOffset": 53}, {"referenceID": 5, "context": "More specially, our model inherited the notation and some simple operations for writing from (Graves et al., 2014), while NMTIA extends it to \u201cunbounded\u201d memory for representing the source.", "startOffset": 93, "endOffset": 114}, {"referenceID": 5, "context": "In addition, although the read-write operations in INTERACTIVE ATTENTION are not exactly the same with those in (Graves et al., 2014; Wang et al., 2016), our model can also achieve good performance.", "startOffset": 112, "endOffset": 152}, {"referenceID": 22, "context": "In addition, although the read-write operations in INTERACTIVE ATTENTION are not exactly the same with those in (Graves et al., 2014; Wang et al., 2016), our model can also achieve good performance.", "startOffset": 112, "endOffset": 152}], "year": 2016, "abstractText": "Conventional attention-based Neural Machine Translation (NMT) conducts dynamic alignment in generating the target sentence. By repeatedly reading the representation of source sentence, which keeps fixed after generated by the encoder (Bahdanau et al., 2015), the attention mechanism has greatly enhanced state-of-the-art NMT. In this paper, we propose a new attention mechanism, called INTERACTIVE ATTENTION, which models the interaction between the decoder and the representation of source sentence during translation by both reading and writing operations. INTERACTIVE ATTENTION can keep track of the interaction history and therefore improve the translation performance. Experiments on NIST Chinese-English translation task show that INTERACTIVE ATTENTION can achieve significant improvements over both the previous attention-based NMT baseline and some state-of-the-art variants of attention-based NMT (i.e., coverage models (Tu et al., 2016)). And neural machine translator with our INTERACTIVE ATTENTION can outperform the open source attention-based NMT system Groundhog by 4.22 BLEU points and the open source phrase-based system Moses by 3.94 BLEU points averagely on multiple test sets.", "creator": "gnuplot 4.6 patchlevel 3"}}}