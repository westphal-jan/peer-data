{"id": "1610.05182", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Oct-2016", "title": "Learning and Transfer of Modulated Locomotor Controllers", "abstract": "We study a novel architecture and training procedure for locomotion tasks. A high-frequency, low-level \"spinal\" network with access to proprioceptive sensors learns sensorimotor primitives by training on simple tasks. This pre-trained module is fixed and connected to a low-frequency, high-level \"cortical\" network, with access to all sensors, which drives behavior by modulating the inputs to the spinal network. Where a monolithic end-to-end architecture fails completely, learning with a pre-trained spinal module succeeds at multiple high-level tasks, and enables the effective exploration required to learn from sparse rewards. We test our proposed architecture on three simulated bodies: a 16-dimensional swimming snake, a 20-dimensional quadruped, and a 54-dimensional humanoid. Our results are illustrated in the accompanying video at", "histories": [["v1", "Mon, 17 Oct 2016 16:03:42 GMT  (8333kb,D)", "http://arxiv.org/abs/1610.05182v1", "Supplemental video available atthis https URL"]], "COMMENTS": "Supplemental video available atthis https URL", "reviews": [], "SUBJECTS": "cs.RO cs.AI", "authors": ["nicolas heess", "greg wayne", "yuval tassa", "timothy lillicrap", "martin riedmiller", "david silver"], "accepted": false, "id": "1610.05182"}, "pdf": {"name": "1610.05182.pdf", "metadata": {"source": "CRF", "title": "Learning and Transfer of Modulated Locomotor Controllers", "authors": ["Nicolas Heess", "Greg Wayne", "Yuval Tassa", "Timothy Lillicrap", "Martin Riedmiller", "David Silver"], "emails": [], "sections": [{"heading": null, "text": "A high-frequency, low-threshold \"spinal\" network with access to proprioceptive sensors learns sensorimotor primitives through training on simple tasks. This pre-trained module is fixed and connected to a low-frequency, high-threshold \"cortical\" network, with access to all sensors, which controls behavior by modulating the entrances to the spinal network. Where a monolithic end-to-end architecture fails completely, learning with a pre-trained spinal module succeeds in multiple high-threshold tasks and enables the effective exploration required to learn from sparse rewards. We test our proposed architecture on three simulated bodies: a 16-dimensional swim snake, a 20-dimensional quadrupedal, and a 54-dimensional humanoid (see attached video)."}, {"heading": "1 Introduction", "text": "In fact, the fact is that most of them are a very limited group, able to move around without being able to achieve their objectives."}, {"heading": "2 Architecture", "text": "The setup is the standard interaction model between agent and environment. At each point in time, an agent leads an action and then receives a reward rt and a new observation ot + 1. Its goal is to maximize the expected sum of discounted future rewards Rt = \u2211 t = t \u03b3t \u2032 \u2212 trt \u2032, known as return. The agent is characterized by a policy \u03c0 with parameters that allocate actions as a function of the history ht = (o1, a1,... ot \u2212 1, at \u2212 1, ot), i.e. at pension level.1 Stochastic policy is defined by the composition of the networks for the high-level controller FH and the low-level controller FH."}, {"heading": "3 Learning locomotor controllers with policy gradients", "text": "We use a framework of actors and critics for pre-training learning and transfer, which takes into account both fully observed (MDPs) and partially observed problems (POMDPs)."}, {"heading": "3.1 Generalized advantage estimation", "text": "We perform gradient ascent in the expected discounted yield J = E [R0], taking the expectation with respect to the trajectory distribution induced by politics and environmental dynamics, (6) where bt is any baseline that does not depend on at \u00b2. In this thesis, we use a learned parameterized value function Vt (ht; \u03c9) with parameters \u03c9 to reduce the variance of the estimate. We replace Rt with the p \u00b2 -weighted yield R\u03bbt = \u0445 k = 0 \u03bb kRkt, where R k = 0 \u03b3 k = 0 \u03b3 k jrt + j + \u03b3 t + k + k + 1V (ht + k + 1). The parameter \u03bb deals with distortion in the value function against deviation in the yield. We also use estimates of the return R\u043c \u00b2 t as target values for the function, so that the loss for the function + k + 1 (ht + k + 1) (ht +) for the training function is different."}, {"heading": "3.2 Policy gradient with hierarchical noise", "text": "The political gradient framework outlined above leads to learning processes at the political level, in which the stochasticity of politics is used for exploration (constant approach).The selection of action distribution (constant approach) \u03c0 as diagonal Gaussian is common due to its simplicity. At the same time, as we will show below, it can lead to very poor exploratory behavior, especially in high-dimensional action spaces. Due to its limited form, it is unable to describe correlations across action dimensions or time steps.The triggering of physical bodies with this form of white noise tends to generate uneven motions, which are attenuated by the dynamics of the second order. On the other hand, our low-level controllers are feedback controllers that produce preformed locomotor behavior. Modelling of this behavior can adequately lead to explorative behavior that is more consistent in space and time. Thus, we allow feedback controllers not only at the low level, but also at the high level of the controller."}, {"heading": "4 Experiments", "text": "We evaluate our framework in three physical areas: a floating snake, a four-legged snake and a humanoid. The snake has 6-legged limbs with a 5-dimensional action space and can move forward using frictional forces. The four-legged snake has an 8-dimensional action space with two joints per leg. The humanoid has 21 action dimensions. In the following motor control problems, the core challenge is to learn basic motion sequences. In more complex behaviors such as navigation, the movement pattern can be reused. In addition to the following description, we encourage the reader to watch the complementary videos3."}, {"heading": "4.1 Training methodology", "text": "We train our hierarchical motor controller on a simple pre-training task and then compare its performance in one or more transfer tasks, including training the low-level controller (which will be reused later) along with a preliminary high-level controller, which provides task-specific information during pre-training, thus ensuring the controllability of the low-level controller. The pre-training task, which facilitates the development of generic locomotion skills, is described by an informative forming reward and requires the controller to move each creature from a random initial configuration to a randomly positioned target. After pre-training, the preliminary high-level controller is discarded and the weights of the low-level controller are frozen. For each transfer task, a new high-level controller is trained to modulate the input of the frozen low-level controller."}, {"heading": "4.2 6-link snake", "text": "This year is the highest in the history of the country."}, {"heading": "4.3 Quadruped", "text": "This year, more than ever before in the history of a country in which it is a country in which it is a country in which it is a country in which it is a country in which it is a country, a country in which it is a country, a country in which it is a city, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a city, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a"}, {"heading": "4.4 Humanoid", "text": "In a final set of experiments, we applied the approach to a particularly challenging 27 degree offreedom control problem: the humanoid model shown in Figure 8. With 21 actors, the problem is much larger than the others. Furthermore, while the snake and the quadrupeds are passively stable, humanoid trapping is a non-trivial control problem in itself. The training task consists of a simple multi-task setup: in each episode, the humanoid is initiated in a upright position standing in the direction of the x-axis and it is required to either move straight or follow a right-hand circle of a fixed radius (5m). The reward function consists of a small square control penalty, a positive constant reward to stay alive, the speed in the desired direction (forward or along the circle)."}, {"heading": "5 Related Work", "text": "The notion that biological motor systems are hierarchical is ancient and dates back to the 19th century [10]. In the 20th century, Bernstein popularized the concept of hierarchical control through \"motor synergies\" or stereotypical combinations to activate muscles with multiple joints [3]. More recently, the concept of spinal motor primitives has been passed on by Mussa-Ivaldi and Bizzi [19] and others [17]. Motor-driven primitives resonate in robotics, especially as primitives of dynamic motion [11], which are low-dimensional attractor systems that can facilitate the learning of robot movements, and hierarchical robot control abstractions originate at least from the 1980s [4]. Modern control theorists have also applied abstract hierarchical skills to manipulation [27] and more biomechanical descriptions of motion sequences [24] as a way of transferring a large variety of motion to [28]."}, {"heading": "6 Conclusion", "text": "Our architecture contains two levels of abstraction that differ in both their access to sensory information and the timescales on which they operate. Our design encourages the low-level controller to focus on the specifics of reactive motor control, while a high-level controller directs behavior toward the task by communicating a modular signal. Our study moves away from the common but unnatural environment in which an agent is trained on a single task. Instead, we take advantage of the fact that many complex motor behaviors share a low structure by building reusable low-level controllers for a variety of tasks. We found our method to be particularly effective when attempting challenging transfer tasks with sparse rewards, where exploration via \"motor chatter\" occurs, we are unlikely to accumulate rewards at all levels."}, {"heading": "A Network Parameters", "text": "We use the same hierarchical neural network architecture for all problems and only change the number of hidden units. The outputs of both the highest and lowest levels parameterise means and standard deviations. The standard deviations are generated by a linear plane with sigmoidal nonlinearity. The lowest controller is a standard feed network with three hidden layers using tanh () nonlinearity. The second layer generates deviations with the performance of the high-level controller. The high-level controller gives out only 10 units as a bottle neck. For the snake and quadruplets there are 150 hidden units per layer next to the linking layer; for the humanoid 300 hidden units. During the pre-training, the high-level controller is an LSTM with a perceptual encoder. The perceptual encoder has 30, 40 and 100 hidden units with tanh () nonlinity for the shifting centre."}, {"heading": "B Experimental procedure for transfer tasks", "text": "In fact, most people who are able are able to move, move and turn. A smooth design of the reward was used during the pre-training. For the humanoid, we experimented with several pre-training tasks. The results in the main text were obtained by training a single low-level controller in a simple multi-task setup, using both the gear in a left circle with a fixed radius, and the gear in a right circle with a fixed radius. The task was randomly and uniformly sampled at the beginning of each episode. The low-level controller was distributed across the three tasks, but we used a separate high-level controller and a function for each result not to be removed at the level."}, {"heading": "C Additional analyses", "text": "In fact, most people are able to decide for themselves what they want and what they want."}], "references": [{"title": "State abstraction for programmable reinforcement learning agents", "author": ["D Andre", "S J Russell"], "venue": "AAAI/IAAI, pages 119\u2013125,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2002}, {"title": "Effective control knowledge transfer through learning skill and representation hierarchies", "author": ["M Asadi", "M Huber"], "venue": "IJCAI, volume 7, pages 2054\u20132059,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2007}, {"title": "The co-ordination and regulation of movements", "author": ["N A Bernstein"], "venue": "Pergamon Press Ltd.,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1967}, {"title": "A robust layered control system for a mobile robot", "author": ["R A Brooks"], "venue": "Robotics and Automation, IEEE Journal of, 2(1):14\u201323,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1986}, {"title": "Learning parameterized motor skills on a humanoid robot", "author": ["B C Da Silva", "G Baldassarre", "G Konidaris", "A Barto"], "venue": "Robotics and Automation (ICRA), 2014 IEEE International Conference on, pages 5239\u20135244. IEEE,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2014}, {"title": "Feudal reinforcement learning", "author": ["P Dayan", "G E Hinton"], "venue": "Advances in neural information processing systems, pages 271\u2013271. Morgan Kaufmann Publishers,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1993}, {"title": "Locomotor primitives in newborn babies and their development", "author": ["N Dominici", "Y P Ivanenko", "G Cappellini", "A d\u2019Avella", "V Mond\u00ec", "M Cicchese", "A Fabiano", "T Silei", "A Di Paolo", "C Giannini"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2011}, {"title": "Continuous deep q-learning with model-based acceleration", "author": ["S Gu", "T P Lillicrap", "I Sutskever", "S Levine"], "venue": "arXiv preprint arXiv:1603.00748,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2016}, {"title": "Learning continuous control policies by stochastic value gradients", "author": ["N Heess", "G Wayne", "D Silver", "T P Lillicrap", "T Erez", "Y Tassa"], "venue": "Advances in Neural Information Processing Systems, pages 2926\u20132934,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2015}, {"title": "On the comparative study of disease of the nervous system", "author": ["J Hughlings Jackson"], "venue": "Br Med J, 17:355\u2013362,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1889}, {"title": "Learning attractor landscapes for learning motor primitives", "author": ["A J Ijspeert", "J Nakanishi", "S Schaal"], "venue": "Advances in Neural Information Processing Systems 15,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2002}, {"title": "Auto-encoding variational Bayes", "author": ["D P Kingma", "M Welling"], "venue": "arXiv preprint arXiv:1312.6114,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2013}, {"title": "Efficient skill learning using abstraction selection", "author": ["G Konidaris", "A G Barto"], "venue": "IJCAI, volume 9, pages 1107\u20131112,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2009}, {"title": "End-to-end training of deep visuomotor policies", "author": ["S Levine", "C Finn", "T Darrell", "P Abbeel"], "venue": "arXiv preprint arXiv:1504.00702,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2015}, {"title": "Continuous control with deep reinforcement learning", "author": ["T P Lillicrap", "J J Hunt", "A Pritzel", "N Heess", "T Erez", "Y Tassa", "D Silver", "D Wierstra"], "venue": "arXiv preprint arXiv:1509.02971,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}, {"title": "A hierarchical foundation for models of sensorimotor control", "author": ["G E Loeb", "I E Brown", "E J Cheng"], "venue": "Experimental brain research, 126(1):1\u201318,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1999}, {"title": "Asynchronous methods for deep reinforcement learning", "author": ["V Mnih", "A P Badia", "M Mirza", "A Graves", "T P Lillicrap", "T Harley", "D Silver", "K Kavukcuoglu"], "venue": "CoRR, abs/1602.01783,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2016}, {"title": "Motor learning through the combination of primitives", "author": ["F A Mussa-Ivaldi", "E Bizzi"], "venue": "Philosophical Transactions of the Royal Society of London B: Biological Sciences, 355(1404):1755\u20131769,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2000}, {"title": "Die Schreitbewegungen der Neugeborenen [The walking movements of newborns", "author": ["A Peiper"], "venue": "Monatsschrift fur Kinderheilkunde, 45:444,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 1929}, {"title": "Relativized options: Choosing the right transformation", "author": ["B Ravindran", "A G Barto"], "venue": "ICML, pages 608\u2013615,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2003}, {"title": "Stochastic Backpropagation and Approximate Inference in Deep Generative Models", "author": ["D Rezende", "S Mohamed", "D Wierstra"], "venue": "Tony Jebara and Eric P. Xing, editors, International Conference on Machine Learning,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2014}, {"title": "High-dimensional continuous control using generalized advantage estimation", "author": ["J Schulman", "P Moritz", "S Levine", "M Jordan", "P Abbeel"], "venue": "arXiv preprint arXiv:1506.02438,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2015}, {"title": "A neural circuitry that emphasizes spinal feedback generates diverse behaviours of human locomotion", "author": ["S Song", "H Geyer"], "venue": "The Journal of physiology, 593(16):3493\u20133511,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2015}, {"title": "Between mdps and semi-mdps: A framework for temporal abstraction in reinforcement learning", "author": ["R S Sutton", "D Precup", "S Singh"], "venue": "Artificial intelligence, 112(1):181\u2013211,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 1999}, {"title": "Motor primitive discovery", "author": ["P S Thomas", "A G Barto"], "venue": "ICDL-EPIROB, pages 1\u20138,", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2012}, {"title": "From task parameters to motor synergies: A hierarchical framework for approximately optimal control of redundant manipulators", "author": ["E Todorov", "W Li", "X Pan"], "venue": "Journal of robotic systems, 22(11):691\u2013710,", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2005}, {"title": "Strategic attentive writer for learning macro-actions", "author": ["Alexander Vezhnevets", "Volodymyr Mnih", "John Agapiou", "Simon Osindero", "Alex Graves", "Oriol Vinyals", "Koray Kavukcuoglu"], "venue": null, "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2016}, {"title": "Hierarchical control using networks trained with higher-level forward models", "author": ["G Wayne", "LF Abbott"], "venue": "Neural computation,", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 18, "context": "A human baby, when lifted so that its feet just graze the ground, will step in a cyclical walking motion [20, 7].", "startOffset": 105, "endOffset": 112}, {"referenceID": 6, "context": "A human baby, when lifted so that its feet just graze the ground, will step in a cyclical walking motion [20, 7].", "startOffset": 105, "endOffset": 112}, {"referenceID": 21, "context": "And indeed, several recent studies have shown that end-to-end reinforcement learning approaches are capable of generating high-quality motor control policies using generic neural networks [23, 15, 9, 16, 8].", "startOffset": 188, "endOffset": 206}, {"referenceID": 13, "context": "And indeed, several recent studies have shown that end-to-end reinforcement learning approaches are capable of generating high-quality motor control policies using generic neural networks [23, 15, 9, 16, 8].", "startOffset": 188, "endOffset": 206}, {"referenceID": 8, "context": "And indeed, several recent studies have shown that end-to-end reinforcement learning approaches are capable of generating high-quality motor control policies using generic neural networks [23, 15, 9, 16, 8].", "startOffset": 188, "endOffset": 206}, {"referenceID": 14, "context": "And indeed, several recent studies have shown that end-to-end reinforcement learning approaches are capable of generating high-quality motor control policies using generic neural networks [23, 15, 9, 16, 8].", "startOffset": 188, "endOffset": 206}, {"referenceID": 7, "context": "And indeed, several recent studies have shown that end-to-end reinforcement learning approaches are capable of generating high-quality motor control policies using generic neural networks [23, 15, 9, 16, 8].", "startOffset": 188, "endOffset": 206}, {"referenceID": 11, "context": "The particular approach we take relies on the re-parameterization trick recently applied in the probabilistic modeling literature [13, 22] and in a policy gradient framework by [9]: 3", "startOffset": 130, "endOffset": 138}, {"referenceID": 20, "context": "The particular approach we take relies on the re-parameterization trick recently applied in the probabilistic modeling literature [13, 22] and in a policy gradient framework by [9]: 3", "startOffset": 130, "endOffset": 138}, {"referenceID": 8, "context": "The particular approach we take relies on the re-parameterization trick recently applied in the probabilistic modeling literature [13, 22] and in a policy gradient framework by [9]: 3", "startOffset": 177, "endOffset": 180}, {"referenceID": 16, "context": "We implemented our experiments using the asynchronous actor-critic framework introduced in [18].", "startOffset": 91, "endOffset": 95}, {"referenceID": 16, "context": "The results for learning from scratch with a FF network are sensitive to the initialization and regularization of the policy: Both, a larger initial value of the (learned) standard deviation (\u03c3init) as well as adding a regularizing term that encourages entropy in the per-step action distribution (as in [18]) improve the results (see (a)).", "startOffset": 304, "endOffset": 308}, {"referenceID": 9, "context": "5 Related Work The notion that biological motor systems are hierarchical is ancient, dating to the 19th century [10].", "startOffset": 112, "endOffset": 116}, {"referenceID": 2, "context": "In the 20th century, Bernstein promulgated the notion of hierarchical control through \u201cmotor synergies,\u201d or stereotyped, multi-joint muscle activation combinations [3].", "startOffset": 164, "endOffset": 167}, {"referenceID": 17, "context": "More recently, the notion of spinal motor primitives has been forwarded by Mussa-Ivaldi and Bizzi [19] and others [17].", "startOffset": 98, "endOffset": 102}, {"referenceID": 15, "context": "More recently, the notion of spinal motor primitives has been forwarded by Mussa-Ivaldi and Bizzi [19] and others [17].", "startOffset": 114, "endOffset": 118}, {"referenceID": 10, "context": "Motor primitives have resonated in robotics, especially as Dynamic Movement Primitives [11], which are low-dimensionality attractor systems that can simplify the learning of robot movements, and hierarchical robot control abstractions date to at least the 1980s [4].", "startOffset": 87, "endOffset": 91}, {"referenceID": 3, "context": "Motor primitives have resonated in robotics, especially as Dynamic Movement Primitives [11], which are low-dimensionality attractor systems that can simplify the learning of robot movements, and hierarchical robot control abstractions date to at least the 1980s [4].", "startOffset": 262, "endOffset": 265}, {"referenceID": 25, "context": "Modern control theorists have also considered abstract hierarchical architectures for manipulation [27] and more bio-mechanical descriptions of locomotion [24].", "startOffset": 99, "endOffset": 103}, {"referenceID": 22, "context": "Modern control theorists have also considered abstract hierarchical architectures for manipulation [27] and more bio-mechanical descriptions of locomotion [24].", "startOffset": 155, "endOffset": 159}, {"referenceID": 23, "context": "The reinforcement learning literature has also explored a wide variety of temporal abstractions that wrap low-level control into options [25] or skills [14].", "startOffset": 137, "endOffset": 141}, {"referenceID": 12, "context": "The reinforcement learning literature has also explored a wide variety of temporal abstractions that wrap low-level control into options [25] or skills [14].", "startOffset": 152, "endOffset": 156}, {"referenceID": 4, "context": "These temporal abstractions may be applied to motor control [5, 26, 29], may be transferred to new tasks [1, 2, 21], and may also incorporate information hiding principles [6, 14].", "startOffset": 60, "endOffset": 71}, {"referenceID": 24, "context": "These temporal abstractions may be applied to motor control [5, 26, 29], may be transferred to new tasks [1, 2, 21], and may also incorporate information hiding principles [6, 14].", "startOffset": 60, "endOffset": 71}, {"referenceID": 27, "context": "These temporal abstractions may be applied to motor control [5, 26, 29], may be transferred to new tasks [1, 2, 21], and may also incorporate information hiding principles [6, 14].", "startOffset": 60, "endOffset": 71}, {"referenceID": 0, "context": "These temporal abstractions may be applied to motor control [5, 26, 29], may be transferred to new tasks [1, 2, 21], and may also incorporate information hiding principles [6, 14].", "startOffset": 105, "endOffset": 115}, {"referenceID": 1, "context": "These temporal abstractions may be applied to motor control [5, 26, 29], may be transferred to new tasks [1, 2, 21], and may also incorporate information hiding principles [6, 14].", "startOffset": 105, "endOffset": 115}, {"referenceID": 19, "context": "These temporal abstractions may be applied to motor control [5, 26, 29], may be transferred to new tasks [1, 2, 21], and may also incorporate information hiding principles [6, 14].", "startOffset": 105, "endOffset": 115}, {"referenceID": 5, "context": "These temporal abstractions may be applied to motor control [5, 26, 29], may be transferred to new tasks [1, 2, 21], and may also incorporate information hiding principles [6, 14].", "startOffset": 172, "endOffset": 179}, {"referenceID": 12, "context": "These temporal abstractions may be applied to motor control [5, 26, 29], may be transferred to new tasks [1, 2, 21], and may also incorporate information hiding principles [6, 14].", "startOffset": 172, "endOffset": 179}, {"referenceID": 26, "context": "Recent work [28] proposes an architecture for discovering temporally extended macro actions from scratch.", "startOffset": 12, "endOffset": 16}, {"referenceID": 0, "context": "References [1] D Andre and S J Russell.", "startOffset": 11, "endOffset": 14}, {"referenceID": 1, "context": "[2] M Asadi and M Huber.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[3] N A Bernstein.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "[4] R A Brooks.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[5] B C Da Silva, G Baldassarre, G Konidaris, and A Barto.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[6] P Dayan and G E Hinton.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[7] N Dominici, Y P Ivanenko, G Cappellini, A d\u2019Avella, V Mond\u00ec, M Cicchese, A Fabiano, T Silei, A Di Paolo, C Giannini, et al.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[8] S Gu, T P Lillicrap, I Sutskever, and S Levine.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "[9] N Heess, G Wayne, D Silver, T P Lillicrap, T Erez, and Y Tassa.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "[10] J Hughlings Jackson.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[11] A J Ijspeert, J Nakanishi, and S Schaal.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[13] D P Kingma and M Welling.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[14] G Konidaris and A G Barto.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[15] S Levine, C Finn, T Darrell, and P Abbeel.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[16] T P Lillicrap, J J Hunt, A Pritzel, N Heess, T Erez, Y Tassa, D Silver, and D Wierstra.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[17] G E Loeb, I E Brown, and E J Cheng.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "[18] V Mnih, A P Badia, M Mirza, A Graves, T P Lillicrap, T Harley, D Silver, and K Kavukcuoglu.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "[19] F A Mussa-Ivaldi and E Bizzi.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "[20] A Peiper.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "[21] B Ravindran and A G Barto.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "[22] D Rezende, S Mohamed, and D Wierstra.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "[23] J Schulman, P Moritz, S Levine, M Jordan, and P Abbeel.", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "[24] S Song and H Geyer.", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "[25] R S Sutton, D Precup, and S Singh.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "[26] P S Thomas and A G Barto.", "startOffset": 0, "endOffset": 4}, {"referenceID": 25, "context": "[27] E Todorov, W Li, and X Pan.", "startOffset": 0, "endOffset": 4}, {"referenceID": 26, "context": "[28] Alexander Vezhnevets, Volodymyr Mnih, John Agapiou, Simon Osindero, Alex Graves, Oriol Vinyals, and Koray Kavukcuoglu.", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": "[29] G Wayne and LF Abbott.", "startOffset": 0, "endOffset": 4}], "year": 2016, "abstractText": "We study a novel architecture and training procedure for locomotion tasks. A high-frequency, low-level \u201cspinal\u201d network with access to proprioceptive sensors learns sensorimotor primitives by training on simple tasks. This pre-trained module is fixed and connected to a low-frequency, high-level \u201ccortical\u201d network, with access to all sensors, which drives behavior by modulating the inputs to the spinal network. Where a monolithic end-to-end architecture fails completely, learning with a pre-trained spinal module succeeds at multiple high-level tasks, and enables the effective exploration required to learn from sparse rewards. We test our proposed architecture on three simulated bodies: a 16-dimensional swimming snake, a 20-dimensional quadruped, and a 54-dimensional humanoid (see attached video).", "creator": "LaTeX with hyperref package"}}}