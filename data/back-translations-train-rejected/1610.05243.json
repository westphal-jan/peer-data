{"id": "1610.05243", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Oct-2016", "title": "Pre-Translation for Neural Machine Translation", "abstract": "Recently, the development of neural machine translation (NMT) has significantly improved the translation quality of automatic machine translation. While most sentences are more accurate and fluent than translations by statistical machine translation (SMT)-based systems, in some cases, the NMT system produces translations that have a completely different meaning. This is especially the case when rare words occur.", "histories": [["v1", "Mon, 17 Oct 2016 18:14:24 GMT  (81kb,D)", "http://arxiv.org/abs/1610.05243v1", "9 pages. To appear in COLING 2016"]], "COMMENTS": "9 pages. To appear in COLING 2016", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["jan niehues", "eunah cho", "thanh-le ha", "alex waibel"], "accepted": false, "id": "1610.05243"}, "pdf": {"name": "1610.05243.pdf", "metadata": {"source": "CRF", "title": "Pre-Translation for Neural Machine Translation", "authors": ["Jan Niehues", "Eunah Cho"], "emails": ["firstname.surname@kit.edu"], "sections": [{"heading": null, "text": "Although most sentences are more precise and fluent than translations by statistical machine translation systems (SMT), in some cases the NMT system produces translations that have a completely different meaning, especially when rare words occur. By using statistical machine translation, it has already been shown that significant benefits can be achieved by simplifying input in a pre-processing step. A commonly used example is the pre-sorting approach. In this work, we used phrase-based machine translation to pre-translate input into the target language. Afterwards, a neural machine translation system builds the final hypothesis using pre-translation, using either the output of phrase-based machine translation (PBMT) or a combination of source analysis and source sentence."}, {"heading": "1 Introduction", "text": "In recent years, the system of statistical translation (SMT) has become state-of-the-art for most language pairs. Recently, neural machine translation (NMT) systems have the ability to outperform SMT systems in several assessments. These models are able to produce a fluent and accurate translation for most sentences. The segment in the table is fluent. However, the weakness of the NMT systems is that they lose the original meaning of the source words during translation. An example from the first Conference of Machine Translation (WMT16) is the segment in the table. The English word goalie is not translated, but the German word goalie, but that means God. One problem might be that we need to cut off the vocabulary to train the model."}, {"heading": "2 Related Work", "text": "The idea of a linear combination of machine translation systems using different paradigms has already been used successfully for SMT and rule-based machine translation (RBMT) (Dugast et al., 2007; Simard et al., 2007), and they are building an SMT system that post-processes the output of an RBMT system. By combining SMT and RBMT, they could outperform both individual systems. These experiments promote the field of automatic post-processing (Bojar et al., 2015). Recently, it has been shown that models based on neuronal MT are very successful in this task (Junczys-Dowmunt and Grundkiewicz, 2016).For PBMT, there have been several attempts to apply pre-processing to improve the performance of the translation system. A commonly used pre-processing step is morphological splitting, such as compound splitting in German (Koehn and Knight, 2003). Another example would be the use of pre-reordering to achieve more monotoring."}, {"heading": "3 Phrase-based and Neural Machine Translation", "text": "Based on the initial work on a word-based translation system (Brown et al., 1993), sentence-based machine translation (Koehn et al., 2003; Och and Ney, 2004) was broken down into consecutive sentences used as basic translation units, allowing for many-too-many alignments. Based on this segmentation, the probability of translation is calculated using a log-linear combination of different features: P (eI, f I) = exp (\u2211 N = 1 nhn (eI, f I)). e \u2032 I exp (\u2211 N = 1 nhn (e \u2032 I, f I)). (1) In the initial model, the features are based on linguistic and translation model probabilities as well as a few count-based features. In advanced PBMT systems, several additional features are used to better model the translation process."}, {"heading": "4 PBMT Pre-translation for NMT (PreMT)", "text": "In this thesis, we want to combine the advantages of PBMT and NMT. With the combined system, we should be able to generate a translation for all the words that appear at least once in the training data, while maintaining high-quality translations for most sentences from NMT. Motivated by several approaches to simplify the translation process for PBMT through pre-processing, we will translate the source as a pre-processing step using the phrase-based machine translation system.The main translation task is performed by the neural translation model, which can choose between the output of the PBMT system or the original input when generating the translation."}, {"heading": "4.1 Pipeline", "text": "In our first experiment, we combined the phrase-based MT and the neural MT in a pipeline, as shown in Figure 1a. Input is initially processed by the phrase-based machine translation system from the input language f into the target language e \u00b2. Since the machine translation system is not perfect, the output of the system may contain incorrect translations. Therefore, we will call the output language of the PBMT system e \u2032. In a second step, we will train a neural monolingual translation system that translates from the output of the PBMT system e \u2032 into a better target sentence e \u2032."}, {"heading": "4.2 Mixed Input", "text": "One disadvantage of the pipeline approach is that the PBMT system could introduce some errors in the translation from which the NMT cannot recover. For example, it is possible that some information is lost from the source sentence because the word is completely deleted during the translation of the PBMT system. We are trying to overcome this problem by building an NMT system that not only takes over the output of the PBMT system, but also the original source sentence. An advantage of the NMT system is that we can easily encode different input information.The architecture of our system is in Figure 1b. Implementing mixed input for the NMT system is easy. Given the source input f = f1,... fI and the output of the PBMT system e \u2032 = e \u2032 1,... e \u2032 J \u2032, we have generated the input for the NMT system. Firstly, we have ensured that the vocabulary of f = 1, the BMT system and the output of the PT I..."}, {"heading": "4.3 Training", "text": "In both cases, we can no longer train the NMT system on the basis of source and target language data, but on the basis of the output of the PBMT system and target language data. Therefore, we must generate translations of all parallel training data using the PBMT system. Due to its ability to use very long phrases, a PBMT system usually performs significantly better on training data than on invisible test data. Of course, this detracts from the performance of our approach, as the NMT system underestimates the number of improvements it needs to make on the test data. To limit this effect, we have not used the entire phrase table when translating training data. If a phrase pair occurs only once, we cannot learn it from another sentence pair. In the following (Niehues and Waibel, 2013), we have removed all phrase pairs that occur only once for the translation of the corpus."}, {"heading": "5 Experiments", "text": "We analyze the methodology of the Conference on Statistical Machine Translation (WMT) for translating messages from English into German. First, we describe the system and analyze the translation quality measured in BLEU. Then, we analyze the performance depending on the frequency of the words and finally show some sample translations."}, {"heading": "5.1 System description", "text": "For pre-translation, we used a PBMT system. To analyze the impact of the quality of the PBMT system, we used two different systems, a base system and an advanced model system, which were trained on all the parallel data available for the WMT 20161. The news commentary corpus, the meetings of the European Parliament and the joint crawl corpus add up to 3.7M sentences and about 90M words. In the base system, we use three language models, a word-based, a bilingual (Niehues et al., 2011) and a cluster-based language model that uses 100 automatically generated clusters using MKCLS (Och., 1999).The advanced system uses pre-reodering (Herrmann et al., 2013) and lexicalized reordering. In addition, it uses a discriminatory word lexicon (Niehues and Waibel, 2013) and a language model that is optimized on the large monolingual data system."}, {"heading": "5.2 English - German Machine Translation", "text": "In fact, most of them will be able to play by the rules they have established in the past."}, {"heading": "5.3 System Comparison", "text": "After evaluating the approach, we continue to analyze the various machine translation techniques. To do this, we compared the single NMT system, the advanced PBMT system, and the mixed system using the advanced PBMT system as input. Our original idea was that PBMT systems are better suited for translating rare words, while the NMT produces a more fluid translation. To confirm this assumption, we edited the output of the entire system. For all systems analyzed, we replaced all target words that occur less than N times in the training data with the UNK token. Therefore, for large N we only have the most common words in the reference, while for lower N more and more words are used. The results for N-1, 10, 100, 1K, 10K, 100K} are presented in Figure 2. Of course, with lower N tokens we will have fewer UNK tokens in the output, while for lower N tokens in the EU output, we normalize the performance of the MT system through the BLT."}, {"heading": "5.4 Examples", "text": "In Table 3 we show the output of the PBMT, NMT and PreMT system. First, for the PBMT system we see a typical error in the translation from and into German. The verb of the parsed subparticle is at the second position in English, but in the German sentence it must be located at the end of the sentence. Often, the PBMT system is unable to perform this far-reaching reordering. For the NMT system we see two more errors. Both the words goalie and parried are quite rare in the training data and therefore they are divided into several parts by the BPE algorithm. In this case, the NMT makes more mistakes. For the first word, the NMT system generates a complete wrong translation of God instead of the goalkeeper. The second word is simply dropped and does not appear in the translation model. The example shows that the pre-translation system prevents both errors."}, {"heading": "6 Conclusion", "text": "Motivated by success in statistical machine translation, we used phrase-based machine translation to pre-translate the input and then generate the final translation using neural machine translation. While a simple serial combination of the two models could not produce a better translation than the neural machine translation system, we are able to improve the quality of machine translation measured in BLEU. The single pre-translated system could even surpass the ensemble NMT system. In the ensemble system, the PreMT system could trump the NMT system by up to 1.8 BLEU points. With the combined approach, we can produce a smoother translation typical of the NMT system, but also translate rare words, which are often easier to translate by PBMT."}, {"heading": "Acknowledgments", "text": "The project that led to this application was funded by the European Union's Horizon 2020 research and innovation programme under Funding Agreement No. 645452, which was supported by the Carl Zeiss Foundation."}], "references": [{"title": "Incorporating discrete translation lexicons into neural machine translation", "author": ["Arthur et al.2016] Philip Arthur", "Graham Neubig", "Satoshi Nakamura"], "venue": "In Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Arthur et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Arthur et al\\.", "year": 2016}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Kyunghyun Cho", "Yoshua Bengio"], "venue": null, "citeRegEx": "Bahdanau et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Findings of the 2015 workshop on statistical machine translation", "author": ["Bojar et al.2015] Ond\u0159ej Bojar", "Rajen Chatterjee", "Christian Federmann", "Barry Haddow", "Matthias Huck", "Chris Hokamp", "Philipp Koehn", "Varvara Logacheva", "Christof Monz", "Matteo Negri", "Matt Post", "Carolina Scarton", "Lucia Specia", "Marco Turchi"], "venue": "In Proceedings of the Tenth Workshop on Statistical Machine Translation,", "citeRegEx": "Bojar et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bojar et al\\.", "year": 2015}, {"title": "The mathematics of statistical machine translation: Parameter estimation", "author": ["Brown et al.1993] Peter F. Brown", "Vincent J. Della Pietra", "Stephen A. Della Pietra", "Robert L. Mercer"], "venue": "Comput. Linguist.,", "citeRegEx": "Brown et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Brown et al\\.", "year": 1993}, {"title": "Statistical post-editing on systran\u2019s rule-based translation system", "author": ["Dugast et al.2007] Lo\u0131\u0308c Dugast", "Jean Senellart", "Philipp Koehn"], "venue": "In Proceedings of the Second Workshop on Statistical Machine Translation,", "citeRegEx": "Dugast et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Dugast et al\\.", "year": 2007}, {"title": "The karlsruhe institute of technology systems for the news translation task in wmt 2016", "author": ["Ha et al.2016] Thanh-Le Ha", "Eunah Cho", "Jan Niehues", "Mohammed Mediani", "Matthias Sperber", "Alexandre Allauzen", "Alexander Waibel"], "venue": "In Proceedings of the First Conference on Machine Translation,", "citeRegEx": "Ha et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Ha et al\\.", "year": 2016}, {"title": "Combining Word Reordering Methods on different Linguistic Abstraction Levels for Statistical Machine Translation", "author": ["Jan Niehues", "Alex Waibel"], "venue": "In Proceedings of the Seventh Workshop on Syntax, Semantics and Structure in Statistical Translation,", "citeRegEx": "Herrmann et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Herrmann et al\\.", "year": 2013}, {"title": "Loglinear combinations of monolingual and bilingual neural machine translation models for automatic post-editing", "author": ["Junczys-Dowmunt", "Roman Grundkiewicz"], "venue": "In Proceedings of the First Conference on Machine Translation,", "citeRegEx": "Junczys.Dowmunt et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Junczys.Dowmunt et al\\.", "year": 2016}, {"title": "Empirical Methods for Compound Splitting", "author": ["Koehn", "Knight2003] Philipp Koehn", "Kevin Knight"], "venue": null, "citeRegEx": "Koehn et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Koehn et al\\.", "year": 2003}, {"title": "Statistical phrase-based translation", "author": ["Koehn et al.2003] Philipp Koehn", "Franz Josef Och", "Daniel Marcu"], "venue": "In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology - Volume", "citeRegEx": "Koehn et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Koehn et al\\.", "year": 2003}, {"title": "Addressing the rare word problem in neural machine translation", "author": ["Luong et al.2015] Thang Luong", "Ilya Sutskever", "Quoc V. Le", "Oriol Vinyals", "Wojciech Zaremba"], "venue": "In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing,", "citeRegEx": "Luong et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "An MT Error-Driven Discriminative Word Lexicon using Sentence Structure Features", "author": ["Niehues", "Waibel2013] Jan Niehues", "Alex Waibel"], "venue": "In Proceedings of the Eighth Workshop on Statistical Machine", "citeRegEx": "Niehues et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Niehues et al\\.", "year": 2013}, {"title": "Wider Context by Using Bilingual Language Models in Machine Translation", "author": ["Niehues et al.2011] Jan Niehues", "Teresa Herrmann", "Stephan Vogel", "Alex Waibel"], "venue": "In Sixth Workshop on Statistical Machine Translation (WMT", "citeRegEx": "Niehues et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Niehues et al\\.", "year": 2011}, {"title": "The alignment template approach to statistical machine translation", "author": ["Och", "Ney2004] Franz Josef Och", "Hermann Ney"], "venue": "Comput. Linguist.,", "citeRegEx": "Och et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Och et al\\.", "year": 2004}, {"title": "An Efficient Method for Determining Bilingual Word Classes", "author": ["Franz Josef Och"], "venue": "In Proceedings of the Ninth Conference of the European Chapter of the Association for Computational Linguistics (EACL", "citeRegEx": "Och.,? \\Q1999\\E", "shortCiteRegEx": "Och.", "year": 1999}, {"title": "Minimum Error Rate Training in Statistical Machine Translation", "author": ["Franz Josef Och"], "venue": "In 41st Annual Meeting of the Association for Computational Linguistics (ACL),", "citeRegEx": "Och.,? \\Q2003\\E", "shortCiteRegEx": "Och.", "year": 2003}, {"title": "Word Reordering in Statistical Machine Translation with a POS-Based Distortion Model", "author": ["Rottmann", "Vogel2007] Kay Rottmann", "Stephan Vogel"], "venue": "In Proceedings of the 11th International Conference on Theoretical and Methodological Issues in Machine Translation (TMI", "citeRegEx": "Rottmann et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Rottmann et al\\.", "year": 2007}, {"title": "Neural machine translation of rare words with subword units", "author": ["Barry Haddow", "Alexandra Birch"], "venue": "In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "Sennrich et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Sennrich et al\\.", "year": 2016}, {"title": "Statistical phrase-based post-editing", "author": ["Simard et al.2007] Michel Simard", "Cyril Goutte", "Pierre Isabelle"], "venue": "Proceedings of NAACL", "citeRegEx": "Simard et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Simard et al\\.", "year": 2007}, {"title": "Sequence to sequence learning with neural networks", "author": ["Oriol Vinyals", "Quoc V Le"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Training phrase translation models with leaving-one-out", "author": ["Arne Mauser", "Hermann Ney"], "venue": "In Annual Meeting of the Assoc. for Computational Linguistics,", "citeRegEx": "Wuebker et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Wuebker et al\\.", "year": 2010}], "referenceMentions": [{"referenceID": 17, "context": "We used Byte Pair Encoding (BPE) (Sennrich et al., 2016) to represent the text using a fixed size vocabulary.", "startOffset": 33, "endOffset": 56}, {"referenceID": 4, "context": "The idea of linear combining of machine translation systems using different paradigms has already been used successfully for SMT and rule-based machine translation (RBMT) (Dugast et al., 2007; Simard et al., 2007).", "startOffset": 171, "endOffset": 213}, {"referenceID": 18, "context": "The idea of linear combining of machine translation systems using different paradigms has already been used successfully for SMT and rule-based machine translation (RBMT) (Dugast et al., 2007; Simard et al., 2007).", "startOffset": 171, "endOffset": 213}, {"referenceID": 2, "context": "Those experiments promote the area of automatic post-editing (Bojar et al., 2015).", "startOffset": 61, "endOffset": 81}, {"referenceID": 20, "context": "The translations have been used to re-train the translation model (Wuebker et al., 2010) or to train additional discriminative translation models (Niehues and Waibel, 2013).", "startOffset": 66, "endOffset": 88}, {"referenceID": 10, "context": "In order to improve the translation of rare words in NMT, authors try to translate words that are not in the vocabulary in a post-processing step (Luong et al., 2015).", "startOffset": 146, "endOffset": 166}, {"referenceID": 17, "context": "In (Sennrich et al., 2016), a method to split words into sub-word units was presented to limit the vocabulary size.", "startOffset": 3, "endOffset": 26}, {"referenceID": 0, "context": "Also the integration of lexical probabilities into NMT was successfully investigated (Arthur et al., 2016).", "startOffset": 85, "endOffset": 106}, {"referenceID": 3, "context": "Starting with the initial work on word-based translation system (Brown et al., 1993), phrase-based machine translation (Koehn et al.", "startOffset": 64, "endOffset": 84}, {"referenceID": 8, "context": ", 1993), phrase-based machine translation (Koehn et al., 2003; Och and Ney, 2004) segments the sentence into continuous phrases that are used as basic translation units.", "startOffset": 42, "endOffset": 81}, {"referenceID": 19, "context": "In a second step, the decoder is initialized by the representation of the source sentence and is then generating the target sequence one word after the other using the last generated word as input for the RNN (Sutskever et al., 2014).", "startOffset": 209, "endOffset": 233}, {"referenceID": 1, "context": "To overcome this problem, (Bahdanau et al., 2014) introduced the soft attention mechanism.", "startOffset": 26, "endOffset": 49}, {"referenceID": 1, "context": "A detailed description of the NMT framework can be found in (Bahdanau et al., 2014).", "startOffset": 60, "endOffset": 83}, {"referenceID": 12, "context": "In the baseline system, we use three language models, a word-based, a bilingual (Niehues et al., 2011) and a cluster based language model, using 100 automatically generated clusters using MKCLS (Och, 1999).", "startOffset": 80, "endOffset": 102}, {"referenceID": 14, "context": ", 2011) and a cluster based language model, using 100 automatically generated clusters using MKCLS (Och, 1999).", "startOffset": 99, "endOffset": 110}, {"referenceID": 6, "context": "The advanced system use pre-reodering (Herrmann et al., 2013) and lexicalized reordering.", "startOffset": 38, "endOffset": 61}, {"referenceID": 15, "context": "Both systems were optimized on the tst2014 using Minimum error rate training (Och, 2003).", "startOffset": 77, "endOffset": 88}, {"referenceID": 5, "context": "A detailed description of the systems can be found in (Ha et al., 2016).", "startOffset": 54, "endOffset": 71}, {"referenceID": 17, "context": "described in (Sennrich et al., 2016) with 40K operations.", "startOffset": 13, "endOffset": 36}], "year": 2016, "abstractText": "Recently, the development of neural machine translation (NMT) has significantly improved the translation quality of automatic machine translation. While most sentences are more accurate and fluent than translations by statistical machine translation (SMT)-based systems, in some cases, the NMT system produces translations that have a completely different meaning. This is especially the case when rare words occur. When using statistical machine translation, it has already been shown that significant gains can be achieved by simplifying the input in a preprocessing step. A commonly used example is the pre-reordering approach. In this work, we used phrase-based machine translation to pre-translate the input into the target language. Then a neural machine translation system generates the final hypothesis using the pre-translation. Thereby, we use either only the output of the phrase-based machine translation (PBMT) system or a combination of the PBMT output and the source sentence. We evaluate the technique on the English to German translation task. Using this approach we are able to outperform the PBMT system as well as the baseline neural MT system by up to 2 BLEU points. We analyzed the influence of the quality of the initial system on the final result.", "creator": "LaTeX with hyperref package"}}}