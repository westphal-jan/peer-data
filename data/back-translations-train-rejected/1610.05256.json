{"id": "1610.05256", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Oct-2016", "title": "Achieving Human Parity in Conversational Speech Recognition", "abstract": "Conversational speech recognition has served as a flagship speech recognition task since the release of the DARPA Switchboard corpus in the 1990s. In this paper, we measure the human error rate on the widely used NIST 2000 test set, and find that our latest automated system has reached human parity. The error rate of professional transcriptionists is 5.9% for the Switchboard portion of the data, in which newly acquainted pairs of people discuss an assigned topic, and 11.3% for the CallHome portion where friends and family members have open-ended conversations. In both cases, our automated system establishes a new state-of-the-art, and edges past the human benchmark. This marks the first time that human parity has been reported for conversational speech. The key to our system's performance is the systematic use of convolutional and LSTM neural networks, combined with a novel spatial smoothing method and lattice-free MMI acoustic training.", "histories": [["v1", "Mon, 17 Oct 2016 18:40:50 GMT  (85kb,D)", "http://arxiv.org/abs/1610.05256v1", null], ["v2", "Fri, 17 Feb 2017 07:32:58 GMT  (86kb,D)", "http://arxiv.org/abs/1610.05256v2", "Revised for publication, updated results"]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["w xiong", "j droppo", "x huang", "f seide", "m seltzer", "a stolcke", "d yu", "g zweig"], "accepted": false, "id": "1610.05256"}, "pdf": {"name": "1610.05256.pdf", "metadata": {"source": "CRF", "title": "ACHIEVING HUMAN PARITY IN CONVERSATIONAL SPEECH RECOGNITION", "authors": ["W. Xiong", "J. Droppo", "X. Huang", "F. Seide", "M. Seltzer", "A. Stolcke", "D. Yu"], "emails": [], "sections": [{"heading": null, "text": "In recent years, the number of people who are able to succeed in the field of the art of speech and speech has increased significantly, from the games of the past to the simple tasks of the art of speech and speech. [1] A series of carefully designed tasks, carried out publicly by the LDC and NGOs, have resulted in most of them being able to position themselves in the public domain. [2] First, simple tasks such as \"resource management\" are driven by a small vocabulary and carefully controlled grammar; then we read the language in the Wall Street Journal [8]; then we become increasingly difficult for automated systems."}, {"heading": "3.1. CNNs", "text": "We use three CNN variants. The first is the VGG architecture of [41]. Compared to the networks previously used in image recognition, this network uses small (3x3) filters, is deeper and applies up to five revolutionary layers before pooling. The second network is based on the ResNet architecture [42], which adds highway connections [43], i.e. a linear transform of the input of each layer to the output of the layer [43, 44]. The only difference is that we apply batch standardization before calculating ReLU activations. The last CNN variant is the LACE model (Layer-wise context expansion with attention) [45]. LACE is a TDNN variant, in which each higher layer is a weighted sum of nonlinear transformations of a window of lower layer frames. In other words, each higher layer uses the broader context than lower layers."}, {"heading": "3.2. LSTMs", "text": "We use a bidirectional architecture [47] without frame tipping [29]. The core model structure is the LSTM defined in [28]. We found that using networks with more than six layers did not improve the error rate during development and chose 512 hidden units per direction, per layer, as this provided a reasonable balance between training time and final model accuracy."}, {"heading": "3.3. Spatial Smoothing", "text": "The results of the study show that the individual models are two different approaches based on the different approaches of the different approaches. First, each activation vector has been reinterpreted as a two-dimensional image; second, this image is highly filtered; the filter is implemented as a circular convolution with a three-dimensional image; the central tapping of the core has a value of 1, and the remaining eight have a value of \u2212 1 / 8; and third, the energy of this highly filtered image is compensated and added to the training target function. We have empirically found that a suitable scale for this energy 0.1 is objective in relation to the existing cross-entropy lens."}, {"heading": "6.1. RNN-LM setup", "text": "Our RNN-LM configuration has secondary distinguishing features, as described below. 1. We have performed both standard, predictive and reverse calculations of RNNLMs that predict words in reverse order. In addition, we have created a second RNN-LM for each direction, starting with different random initial weights. At the word level, the two RNN-LM probabilities are interpolated with corresponding Ngram-LM probabilities (separately for the forward and backward models). In addition, we have created a second RNN-LM for each direction. The two RNN-LMs and the N-LM probabilities for each direction are interpolated."}, {"heading": "6.2. LSTM-LM setup", "text": "After achieving good results with RNN-LMs, we also explored the recursive network architecture for speech modeling, inspired by recent work that yields gains over RNN-LMs1However, by adding more hidden layers, we did not produce any more gains.For applying the lessons from our RNN-LM experiments, we explored additional alternatives as described below. There are two types of input vectors that our LSTM LMs take, word-based on a hot vector and a letter vector. [59] Including, we explored both forward and backward models, training four different LSTM LMs as a whole."}, {"heading": "6.3. Training data", "text": "The 4 gram decoding language model was trained using the available CTS transcripts from the DARPA-EARS program: Switchboard (3M words), BBN Switchboard 2 transcripts (850k), Fisher (21M), English CallHome (200k), and the University of Washington Conversation Webcorpus (191M). A separate N gram model was trained from each source and interpolated with weights optimized for RT-03 transcripts. An additional LM component was added to the untrimmed 4 gram rescoring major components, which was trained on 133M words of LDC broadcast news texts. The N gram LM configuration is modeled according to the model described in [50], except for maximized smoothing."}, {"heading": "6.4. RNN-LM and LSTM-LM performance", "text": "Table 6 shows perplexity and word errors for various recurrent neural networks, from simple to complex. The acoustic model used was ResNet CNN.As can be seen, each of the measures described above adds up incremental gains, which add up to a significant overall improvement. Overall, the gain compared to a pure N-gram-based system is a relative error reduction of 20% for RNN-LMs and 23% for LSTM-LMs. As shown later (see Table 7), the gains are similar for different acoustic models."}, {"heading": "6.5. System Combination", "text": "The rescored N leaderboards from each subsystem are then combined into a single confusion network [62] using the SRILM nbest-rover tool. However, the number of potential candidate systems is too large to allow an all-encompassing combination both for practical reasons and due to excessive adaptation problems. Instead, we conduct a greedy search, starting with the best system and gradually adding additional systems to find a small set of systems that complement each other to the maximum. RT-02 Switchboard was used for this search method. Relative weighting (for confusion network-mediated tuning) of the different systems is optimized using an EM algorithm, using the same data, and is smoothed hierarchically by interpolating each set of system weights with the previous one in the search procedure."}, {"heading": "7.1. Flexible, Terse Model Definition", "text": "In CNTK, a neural network (and the training criteria) are specified by its formula, using a user-defined functional language (BrainScript) or Python. A graph-based execution engine, which enables automatic differentiation, then trains the parameters of the model by SGD. By using a master library of common layer types, networks can be specified very briefly. Examples can be found in [63]."}, {"heading": "7.2. Multi-Server Training using 1-bit SGD", "text": "CNTK enabled the training times by parallelizing the SGD training with our 1-bit SGD parallelization technique [65]. This data-parallel method distributes minicharges across multiple worker nodes and then aggregates the sub-gradients. While the required communication time would otherwise be prohibitive, the 1-bit SGD method eliminates the bottleneck by two techniques: 1-bit quantification of gradients and automatic scaling of the minibatch size. In [65] we showed that the gradient values can be quantified to only one bit by transferring the quantization error from one minibatch to the next. Each time a sub-gradient is quantified, the quantization error is calculated and stored and then fed to the next minibatch, reducing the required bandwidth by 32 times with minimal loss of accuracy."}, {"heading": "8.1. Speech corpora", "text": "We train with the commonly used English CTS (Switchboard and Fisher) corpora. The evaluation is done with the NIST 2000 CTS test set, which includes both Switchboard (SWB) and CallHome (CH) subsets. The Switchboard-1 part of the NIST 2002 CTS test set was used for tuning and development.The acoustic training data consists of the LDC Korpora 97S62, 2004S13, 2005S13, 2004S11 and 2004S09; a complete description can be found under [12]."}, {"heading": "8.2. Acoustic Model Details", "text": "The CNN models used window sizes as shown in Figure 1, and the LSTMs processed one input image after another every 10 milliseconds. Most of our models use three left-right three-phonic models with 9000 bound states. In addition, we have trained several models with 27k bound states. The phonetic inventory includes specialized models for noise, vowel noise, laughter, and silence. We use a 30k vocabulary derived from the most common words in the switchboard and Fisher Corporation. The decoder uses a statically generated unigram graph and dynamically applies the language model evaluation. The unigram graph shows about 300k states and 500k slurs. Table 3 shows the result of i-vector adjustment and LFMMI training on several of our early systems. We achieve a relative improvement of 5-8% in i-vectors, including CNN advances and reduction of the large MI models."}, {"heading": "8.3. Comparative System Performance", "text": "In fact, it is the case that most of them are able to move into another world, in which they are able to move, in which they move, in which they move, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they, in which they live, in which they live, in which they, in which they are able to move, in which they are able to move, in which they are able to move, in which they are able to move, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they are able"}, {"heading": "Acknowledgments", "text": "We thank Arul Menezes for accessing the Microsoft transcription pipeline; Chris Basoglu, Amit Agarwal and Marko Radmilac for their invaluable support with CNTK; Jinyu Li and Partha Parthasarathy for many helpful conversations. We also thank X. Chen from Cambridge University for valuable support with the CUED RNLM toolkit, and ICSI for computing and data resources. [2] D. Silver, A. Huang, C. J. Maddison, L. Sifre, G. Van Den Driessche, J. Schrittwieser, I. Antonoglou, V. Panneershelvam, M. Lanctot, et al. \""}], "references": [{"title": "A", "author": ["M. Campbell"], "venue": "J. Hoane, and F.-h. Hsu, \u201cDeep Blue\u201d, Artificial intelligence, vol. 134, pp. 57\u201383", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2002}, {"title": "G", "author": ["D. Silver", "A. Huang", "C.J. Maddison", "A. Guez", "L. Sifre"], "venue": "Van Den Driessche, J. Schrittwieser, I. Antonoglou, V. Panneershelvam, M. Lanctot, et al., \u201cMastering the game of Go with deep neural networks and tree search\u201d, Nature, vol. 529, pp. 484\u2013489", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2016}, {"title": "G", "author": ["D. Amodei", "R. Anubhai", "E. Battenberg", "C. Case", "J. Casper", "B. Catanzaro", "J. Chen", "M. Chrzanowski", "A. Coates"], "venue": "Diamos, et al., \u201cDeep Speech 2: End-toend speech recognition in English and Mandarin\u201d, arXiv preprint arXiv:1512.02595", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2015}, {"title": "Super-human multi-talker speech recognition: the IBM 2006 Speech Separation Challenge system", "author": ["T.T. Kristjansson", "J.R. Hershey", "P.A. Olsen", "S.J. Rennie", "R.A. Gopinath"], "venue": "Proc. Interspeech, vol. 12, p. 155", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2006}, {"title": "Singlechannel mixed speech recognition using deep neural networks", "author": ["C. Weng", "D. Yu", "M.L. Seltzer", "J. Droppo"], "venue": "Proc. IEEE ICASSP, pp. 5632\u20135636. IEEE", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2014}, {"title": "A look at NIST\u2019s benchmark ASR tests: past", "author": ["D.S. Pallett"], "venue": "present, and future\u201d, in IEEE Automatic Speech Recognition and Understanding Workshop, pp. 483\u2013 488. IEEE", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2003}, {"title": "The DARPA 1000-word resource management database for continuous speech recognition", "author": ["P. Price", "W.M. Fisher", "J. Bernstein", "D.S. Pallett"], "venue": "Proc. IEEE ICASSP, pp. 651\u2013654. IEEE", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1988}, {"title": "The design for the wall street journal-based csr corpus", "author": ["D.B. Paul", "J.M. Baker"], "venue": "Proceedings of the workshop on Speech and Natural Language, pp. 357\u2013 362. Association for Computational Linguistics", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1992}, {"title": "The 1996 broadcast news speech and language-model corpus", "author": ["D. Graff", "Z. Wu", "R. MacIntyre", "M. Liberman"], "venue": "Proceedings of the DARPA Workshop on Spoken Language technology, pp. 11\u201314", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1997}, {"title": "Switchboard: Telephone speech corpus for research and development", "author": ["J.J. Godfrey", "E.C. Holliman", "J. McDaniel"], "venue": "Proc. IEEE ICASSP, vol. 1, pp. 517\u2013520. IEEE", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1992}, {"title": "The Fisher corpus: a resource for the next generations of speech-to-text", "author": ["C. Cieri", "D. Miller", "K. Walker"], "venue": "LREC, vol. 4, pp. 69\u201371", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2004}, {"title": "Advances in speech transcription at IBM under the DARPA EARS program", "author": ["S.F. Chen", "B. Kingsbury", "L. Mangu", "D. Povey", "G. Saon", "H. Soltau", "G. Zweig"], "venue": "IEEE Transactions on Audio, Speech, and Language Processing, vol. 14, pp. 1596\u20131608", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2006}, {"title": "T", "author": ["S. Matsoukas", "J.-L. Gauvain", "G. Adda"], "venue": "Colthurst, C.- L. Kao, O. Kimball, L. Lamel, F. Lefevre, J. Z. Ma, J. Makhoul, et al., \u201cAdvances in transcription of broadcast news and conversational telephone speech within the combined ears bbn/limsi system\u201d, IEEE Transactions on Audio, Speech, and Language Processing, vol. 14, pp. 1541\u20131556", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2006}, {"title": "X", "author": ["A. Stolcke", "B. Chen", "H. Franco", "V.R.R. Gadde", "M. Graciarena", "M.-Y. Hwang", "K. Kirchhoff", "A. Mandal", "N. Morgan"], "venue": "Lei, et al., \u201cRecent innovations in speech-to-text transcription at SRI-ICSI-UW\u201d, IEEE Transactions on Audio, Speech, and Language Processing, vol. 14, pp. 1729\u20131744", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2006}, {"title": "The AT&T 2001 LVCSR system", "author": ["A. Ljolje"], "venue": "NIST LVCSR Workshop", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2001}, {"title": "Conversational telephone speech recognition", "author": ["J.-L. Gauvain", "L. Lamel", "H. Schwenk", "G. Adda", "L. Chen", "F. Lefevre"], "venue": "Proc. IEEE ICASSP, vol. 1, pp. I\u2013212. IEEE", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2003}, {"title": "Development of the 2003 cu-htk conversational telephone speech transcription system", "author": ["G. Evermann", "H.Y. Chan", "M.J.F. Gales", "T. Hain", "X. Liu", "D. Mrva", "L. Wang", "P.C. Woodland"], "venue": "Proc. IEEE ICASSP, vol. 1, pp. I\u2013249. IEEE", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2004}, {"title": "Conversational speech transcription using context-dependent deep neural networks", "author": ["F. Seide", "G. Li", "D. Yu"], "venue": "Proc. Interspeech, pp. 437\u2013440", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2011}, {"title": "Speech recognition by machines and humans", "author": ["R.P. Lippmann"], "venue": "Speech Communication, vol. 22, pp. 1\u201315", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1997}, {"title": "The Microsoft 2016 conversational speech recognition system", "author": ["W. Xiong", "J. Droppo", "X. Huang", "F. Seide", "M. Seltzer", "A. Stolcke", "D. Yu", "G. Zweig"], "venue": "submitted to ICASSP", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2017}, {"title": "Generalization of back-propagation to recurrent neural networks", "author": ["F.J. Pineda"], "venue": "Physical Review Letters, vol. 59, pp. 2229", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1987}, {"title": "A learning algorithm for continually running fully recurrent neural networks", "author": ["R.J. Williams", "D. Zipser"], "venue": "Neural Computation, vol. 1, pp. 270\u2013280", "citeRegEx": "22", "shortCiteRegEx": null, "year": 1989}, {"title": "Phoneme recognition using time-delay neural networks", "author": ["A. Waibel", "T. Hanazawa", "G. Hinton", "K. Shikano", "K.J. Lang"], "venue": "IEEE transactions on acoustics, speech, and signal processing, vol. 37, pp. 328\u2013339", "citeRegEx": "23", "shortCiteRegEx": null, "year": 1989}, {"title": "Convolutional networks for images", "author": ["Y. LeCun", "Y. Bengio"], "venue": "speech, and time series\u201d, The handbook of brain theory and neural networks, vol. 3361, pp. 1995", "citeRegEx": "24", "shortCiteRegEx": null, "year": 1995}, {"title": "Backpropagation applied to handwritten zip code recognition", "author": ["Y. LeCun", "B. Boser", "J.S. Denker", "D. Henderson", "R.E. Howard", "W. Hubbard", "L.D. Jackel"], "venue": "Neural computation, vol. 1, pp. 541\u2013551", "citeRegEx": "25", "shortCiteRegEx": null, "year": 1989}, {"title": "A recurrent error propagation network speech recognition system", "author": ["T. Robinson", "F. Fallside"], "venue": "Computer Speech & Language, vol. 5, pp. 259\u2013274", "citeRegEx": "26", "shortCiteRegEx": null, "year": 1991}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural Computation, vol. 9, pp. 1735\u20131780", "citeRegEx": "27", "shortCiteRegEx": null, "year": 1997}, {"title": "Long shortterm memory recurrent neural network architectures for large scale acoustic modeling", "author": ["H. Sak", "A.W. Senior", "F. Beaufays"], "venue": "Proc. Interspeech, pp. 338\u2013342", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2014}, {"title": "Fast and accurate recurrent neural network acoustic models for speech recognition", "author": ["H. Sak", "A. Senior", "K. Rao", "F. Beaufays"], "venue": "arXiv preprint arXiv:1507.06947", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2015}, {"title": "The IBM 2015 English conversational telephone speech recognition system", "author": ["G. Saon", "H.-K.J. Kuo", "S. Rennie", "M. Picheny"], "venue": "arXiv preprint arXiv:1505.05899", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2015}, {"title": "Very deep multilingual convolutional neural networks for lvcsr", "author": ["T. Sercu", "C. Puhrsch", "B. Kingsbury", "Y. LeCun"], "venue": "Proc. IEEE ICASSP, pp. 4955\u20134959. IEEE", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2016}, {"title": "Very deep convolutional neural networks for lvcsr", "author": ["M. Bi", "Y. Qian", "K. Yu"], "venue": "Proc. Interspeech", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2015}, {"title": "Very deep convolutional neural networks for noise robust speech recognition", "author": ["Y. Qian", "M. Bi", "T. Tan", "K. Yu"], "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. PP, pp. 1\u20131", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2016}, {"title": "Recurrent neural network based language model", "author": ["T. Mikolov", "M. Karafi\u00e1t", "L. Burget", "J. Cernock\u1ef3", "S. Khudanpur"], "venue": "Proc. Interspeech, vol. 2, p. 3", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2010}, {"title": "Context dependent recurrent neural network language model", "author": ["T. Mikolov", "G. Zweig"], "venue": "IEEE Speech and Language Technology Workshop, pp. 234\u2013239", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2012}, {"title": "Improving English conversational telephone speech recognition", "author": ["I. Medennikov", "A. Prudnikov", "A. Zatvornitskiy"], "venue": "Proc. Interspeech, pp. 2\u20136", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2016}, {"title": "Linguistic regularities in continuous space word representations", "author": ["T. Mikolov", "W.-t. Yih", "G. Zweig"], "venue": "in HLT-NAACL,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2013}, {"title": "Sequence to sequence learning with neural networks", "author": ["I. Sutskever", "O. Vinyals", "Q.V. Le"], "venue": "Advances in Neural Information Processing Systems, pp. 3104\u2013 3112", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2014}, {"title": "A", "author": ["A. Hannun", "C. Case", "J. Casper", "B. Catanzaro", "G. Diamos", "E. Elsen", "R. Prenger", "S. Satheesh", "S. Sengupta"], "venue": "Coates, et al., \u201cDeep speech: Scaling up end-to-end speech recognition\u201d, arXiv preprint arXiv:1412.5567", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2014}, {"title": "Transcription methods for consistency", "author": ["M.L. Glenn", "S. Strassel", "H. Lee", "K. Maeda", "R. Zakhary", "X. Li"], "venue": "volume and efficiency\u201d, in LREC", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2010}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "arXiv preprint arXiv:1409.1556", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep residual learning for image recognition", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "arXiv preprint arXiv:1512.03385", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2015}, {"title": "Highway networks", "author": ["R.K. Srivastava", "K. Greff", "J. Schmidhuber"], "venue": "CoRR, vol. abs/1505.00387", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2015}, {"title": "Linearly augmented deep neural network", "author": ["P. Ghahremani", "J. Droppo", "M.L. Seltzer"], "venue": "Proc. IEEE ICASSP, pp. 5085\u20135089. IEEE", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2016}, {"title": "Deep convolutional neural networks with layer-wise context expansion and attention", "author": ["D. Yu", "W. Xiong", "J. Droppo", "A. Stolcke", "G. Ye", "J. Li", "G. Zweig"], "venue": "Proc. Interspeech", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2016}, {"title": "Consonant recognition by modular construction of large phonemic time-delay neural networks", "author": ["A. Waibel", "H. Sawai", "K. Shikano"], "venue": "Proc. IEEE ICASSP, pp. 112\u2013115. IEEE", "citeRegEx": "46", "shortCiteRegEx": null, "year": 1989}, {"title": "Framewise phoneme classification with bidirectional LSTM and other neural network architectures", "author": ["A. Graves", "J. Schmidhuber"], "venue": "Neural Networks, vol. 18, pp. 602\u2013610", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2005}, {"title": "Front-end factor analysis for speaker verification", "author": ["N. Dehak", "P.J. Kenny", "R. Dehak", "P. Dumouchel", "P. Ouellet"], "venue": "IEEE Transactions on Audio, Speech, and Language Processing, vol. 19, pp. 788\u2013798", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2011}, {"title": "Speaker adaptation of neural network acoustic models using i-vectors", "author": ["G. Saon", "H. Soltau", "D. Nahamoo", "M. Picheny"], "venue": "IEEE Speech Recognition and Understanding Workshop, pp. 55\u201359", "citeRegEx": "49", "shortCiteRegEx": null, "year": 2013}, {"title": "The IBM 2016 English conversational telephone speech recognition", "author": ["G. Saon", "T. Sercu", "S.J. Rennie", "H.J. Kuo"], "venue": "system\u201d, in Proc. Interspeech,", "citeRegEx": "50", "shortCiteRegEx": "50", "year": 2016}, {"title": "Sequential classification criteria for NNs in automatic speech recognition", "author": ["G. Wang", "K. Sim"], "venue": "Proc. Interspeech", "citeRegEx": "51", "shortCiteRegEx": null, "year": 2011}, {"title": "Sequence-discriminative training of deep neural networks", "author": ["K. Vesel\u1ef3", "A. Ghoshal", "L. Burget", "D. Povey"], "venue": "Proc. Interspeech, pp. 2345\u20132349", "citeRegEx": "52", "shortCiteRegEx": null, "year": 2013}, {"title": "Purely sequence-trained neural networks for ASR based on lattice-free MMI", "author": ["D. Povey", "V. Peddinti", "D. Galvez", "P. Ghahrmani", "V. Manohar", "X. Na", "Y. Wang", "S. Khudanpur"], "venue": "Proc. Interspeech, pp. 2751\u20132755", "citeRegEx": "53", "shortCiteRegEx": null, "year": 2016}, {"title": "Parallelizing WFST speech decoders", "author": ["C. Mendis", "J. Droppo", "S. Maleki", "M. Musuvathi", "T. Mytkowicz", "G. Zweig"], "venue": "Proc. IEEE ICASSP, pp. 5325\u2013 5329. IEEE", "citeRegEx": "54", "shortCiteRegEx": null, "year": 2016}, {"title": "SRILM\u2014an extensible language modeling toolkit", "author": ["A. Stolcke"], "venue": "Proc. Interspeech, vol. 2002, p. 2002", "citeRegEx": "55", "shortCiteRegEx": null, "year": 2002}, {"title": "Efficient estimation of maximum entropy language models with N-gram features: An SRILM extension", "author": ["T. Alum\u00e4e", "M. Kurimo"], "venue": "Proc. Interspeech, pp. 1820\u20131823", "citeRegEx": "56", "shortCiteRegEx": null, "year": 2012}, {"title": "CUED-RNNLM: An open-source toolkit for efficient training and evaluation of recurrent neural network language models", "author": ["X. Chen", "X. Liu", "Y. Qian", "M.J.F. Gales", "P.C. Woodland"], "venue": "Proc. IEEE ICASSP, pp. 6000\u20136004. IEEE", "citeRegEx": "57", "shortCiteRegEx": null, "year": 2016}, {"title": "Noise-contrastive estimation: A new estimation principle for unnormalized statistical models", "author": ["M. Gutmann", "A. Hyv\u00e4rinen"], "venue": "AISTATS, vol. 1, pp. 6", "citeRegEx": "58", "shortCiteRegEx": null, "year": 2010}, {"title": "Learning deep structured semantic models for web search using clickthrough data", "author": ["P.-S. Huang", "X. He", "J. Gao", "L. Deng", "A. Acero", "L. Heck"], "venue": "Proceedings of the 22nd ACM international conference on Conference on information & knowledge management, pp. 2333\u20132338. ACM", "citeRegEx": "59", "shortCiteRegEx": null, "year": 2013}, {"title": "Using the output embedding to improve language models", "author": ["O. Press", "L. Wolf"], "venue": "arXiv preprint arXiv:1608.05859", "citeRegEx": "60", "shortCiteRegEx": null, "year": 2016}, {"title": "Self-stabilized deep neural network", "author": ["P. Ghahremani", "J. Droppo"], "venue": "Proc. IEEE ICASSP. IEEE", "citeRegEx": "61", "shortCiteRegEx": null, "year": 2016}, {"title": "H", "author": ["D. Yu", "A. Eversole", "M. Seltzer", "K. Yao", "Z. Huang", "B. Guenter", "O. Kuchaiev", "Y. Zhang", "F. Seide"], "venue": "Wang, et al., \u201cAn introduction to computational networks and the computational network toolkit\u201d, Technical report, Technical report, Tech. Rep. MSR, Microsoft Research, 2014, 2014. research. microsoft. com/apps/pubs", "citeRegEx": "64", "shortCiteRegEx": null, "year": 2014}, {"title": "1-bit stochastic gradient descent and its application to dataparallel distributed training of speech DNNs", "author": ["F. Seide", "H. Fu", "J. Droppo", "G. Li", "D. Yu"], "venue": "Proc. Interspeech, pp. 1058\u20131062", "citeRegEx": "65", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep convolutional neural networks for lvcsr", "author": ["T.N. Sainath", "A.-r. Mohamed", "B. Kingsbury", "B. Ramabhadran"], "venue": "in Proc. IEEE ICASSP,", "citeRegEx": "66", "shortCiteRegEx": "66", "year": 2013}, {"title": "Applying convolutional neural networks concepts to hybrid NN-HMM model for speech recognition", "author": ["O. Abdel-Hamid", "A.-r. Mohamed", "H. Jiang", "G. Penn"], "venue": "in Proc. IEEE ICASSP,", "citeRegEx": "67", "shortCiteRegEx": "67", "year": 2012}, {"title": "Stimulated deep neural network for speech recognition", "author": ["C. Wu", "P. Karanasou", "M.J. Gales", "K.C. Sim"], "venue": "Proc. Interspeech, pp. 400\u2013404", "citeRegEx": "68", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 0, "context": "Recent years have seen human performance levels reached or surpassed in tasks ranging from the games of chess and Go [1, 2] to simple speech recognition tasks like carefully read newspaper speech [3] and rigidly constrained smallvocabulary tasks in noise [4, 5].", "startOffset": 117, "endOffset": 123}, {"referenceID": 1, "context": "Recent years have seen human performance levels reached or surpassed in tasks ranging from the games of chess and Go [1, 2] to simple speech recognition tasks like carefully read newspaper speech [3] and rigidly constrained smallvocabulary tasks in noise [4, 5].", "startOffset": 117, "endOffset": 123}, {"referenceID": 2, "context": "Recent years have seen human performance levels reached or surpassed in tasks ranging from the games of chess and Go [1, 2] to simple speech recognition tasks like carefully read newspaper speech [3] and rigidly constrained smallvocabulary tasks in noise [4, 5].", "startOffset": 196, "endOffset": 199}, {"referenceID": 3, "context": "Recent years have seen human performance levels reached or surpassed in tasks ranging from the games of chess and Go [1, 2] to simple speech recognition tasks like carefully read newspaper speech [3] and rigidly constrained smallvocabulary tasks in noise [4, 5].", "startOffset": 255, "endOffset": 261}, {"referenceID": 4, "context": "Recent years have seen human performance levels reached or surpassed in tasks ranging from the games of chess and Go [1, 2] to simple speech recognition tasks like carefully read newspaper speech [3] and rigidly constrained smallvocabulary tasks in noise [4, 5].", "startOffset": 255, "endOffset": 261}, {"referenceID": 5, "context": "In the area of speech recognition, much of the pioneering early work was driven by a series of carefully designed tasks with DARPA-funded datasets publicly released by the LDC and NIST [6]: first simple ones like the \u201cresource management\u201d task [7] with a small vocabulary and carefully controlled grammar; then read speech recognition in the Wall Street Journal task [8]; then Broadcast News [9]; each progressively more difficult for automatic systems.", "startOffset": 185, "endOffset": 188}, {"referenceID": 6, "context": "In the area of speech recognition, much of the pioneering early work was driven by a series of carefully designed tasks with DARPA-funded datasets publicly released by the LDC and NIST [6]: first simple ones like the \u201cresource management\u201d task [7] with a small vocabulary and carefully controlled grammar; then read speech recognition in the Wall Street Journal task [8]; then Broadcast News [9]; each progressively more difficult for automatic systems.", "startOffset": 244, "endOffset": 247}, {"referenceID": 7, "context": "In the area of speech recognition, much of the pioneering early work was driven by a series of carefully designed tasks with DARPA-funded datasets publicly released by the LDC and NIST [6]: first simple ones like the \u201cresource management\u201d task [7] with a small vocabulary and carefully controlled grammar; then read speech recognition in the Wall Street Journal task [8]; then Broadcast News [9]; each progressively more difficult for automatic systems.", "startOffset": 367, "endOffset": 370}, {"referenceID": 8, "context": "In the area of speech recognition, much of the pioneering early work was driven by a series of carefully designed tasks with DARPA-funded datasets publicly released by the LDC and NIST [6]: first simple ones like the \u201cresource management\u201d task [7] with a small vocabulary and carefully controlled grammar; then read speech recognition in the Wall Street Journal task [8]; then Broadcast News [9]; each progressively more difficult for automatic systems.", "startOffset": 392, "endOffset": 395}, {"referenceID": 9, "context": "The Switchboard [10] and later Fisher [11] data collections of the 1990s and early 2000s provide what is to date the largest and best studied of the conversational corpora.", "startOffset": 16, "endOffset": 20}, {"referenceID": 10, "context": "The Switchboard [10] and later Fisher [11] data collections of the 1990s and early 2000s provide what is to date the largest and best studied of the conversational corpora.", "startOffset": 38, "endOffset": 42}, {"referenceID": 11, "context": "The history of work in this area includes key contributions by institutions such as IBM [12], BBN [13], SRI [14], AT&T [15], LIMSI [16], Cambridge University [17], Microsoft [18] and numerous others.", "startOffset": 88, "endOffset": 92}, {"referenceID": 12, "context": "The history of work in this area includes key contributions by institutions such as IBM [12], BBN [13], SRI [14], AT&T [15], LIMSI [16], Cambridge University [17], Microsoft [18] and numerous others.", "startOffset": 98, "endOffset": 102}, {"referenceID": 13, "context": "The history of work in this area includes key contributions by institutions such as IBM [12], BBN [13], SRI [14], AT&T [15], LIMSI [16], Cambridge University [17], Microsoft [18] and numerous others.", "startOffset": 108, "endOffset": 112}, {"referenceID": 14, "context": "The history of work in this area includes key contributions by institutions such as IBM [12], BBN [13], SRI [14], AT&T [15], LIMSI [16], Cambridge University [17], Microsoft [18] and numerous others.", "startOffset": 119, "endOffset": 123}, {"referenceID": 15, "context": "The history of work in this area includes key contributions by institutions such as IBM [12], BBN [13], SRI [14], AT&T [15], LIMSI [16], Cambridge University [17], Microsoft [18] and numerous others.", "startOffset": 131, "endOffset": 135}, {"referenceID": 16, "context": "The history of work in this area includes key contributions by institutions such as IBM [12], BBN [13], SRI [14], AT&T [15], LIMSI [16], Cambridge University [17], Microsoft [18] and numerous others.", "startOffset": 158, "endOffset": 162}, {"referenceID": 17, "context": "The history of work in this area includes key contributions by institutions such as IBM [12], BBN [13], SRI [14], AT&T [15], LIMSI [16], Cambridge University [17], Microsoft [18] and numerous others.", "startOffset": 174, "endOffset": 178}, {"referenceID": 18, "context": "In the past, human performance on this task has been widely cited as being 4% [19].", "startOffset": 78, "endOffset": 82}, {"referenceID": 18, "context": "However, the error rate estimate in [19] is attributed to a \u201cpersonal communication,\u201d and the actual source of this number is ephemeral.", "startOffset": 36, "endOffset": 40}, {"referenceID": 19, "context": "We improve on our recently reported conversational speech recognition system [20] by about 0.", "startOffset": 77, "endOffset": 81}, {"referenceID": 20, "context": "While the basic structures have been well known for a long period [21, 22, 23, 24, 25, 26, 27], it is only recently that they have emerged as the best models for speech recognition.", "startOffset": 66, "endOffset": 94}, {"referenceID": 21, "context": "While the basic structures have been well known for a long period [21, 22, 23, 24, 25, 26, 27], it is only recently that they have emerged as the best models for speech recognition.", "startOffset": 66, "endOffset": 94}, {"referenceID": 22, "context": "While the basic structures have been well known for a long period [21, 22, 23, 24, 25, 26, 27], it is only recently that they have emerged as the best models for speech recognition.", "startOffset": 66, "endOffset": 94}, {"referenceID": 23, "context": "While the basic structures have been well known for a long period [21, 22, 23, 24, 25, 26, 27], it is only recently that they have emerged as the best models for speech recognition.", "startOffset": 66, "endOffset": 94}, {"referenceID": 24, "context": "While the basic structures have been well known for a long period [21, 22, 23, 24, 25, 26, 27], it is only recently that they have emerged as the best models for speech recognition.", "startOffset": 66, "endOffset": 94}, {"referenceID": 25, "context": "While the basic structures have been well known for a long period [21, 22, 23, 24, 25, 26, 27], it is only recently that they have emerged as the best models for speech recognition.", "startOffset": 66, "endOffset": 94}, {"referenceID": 26, "context": "While the basic structures have been well known for a long period [21, 22, 23, 24, 25, 26, 27], it is only recently that they have emerged as the best models for speech recognition.", "startOffset": 66, "endOffset": 94}, {"referenceID": 27, "context": "Surprisingly, this is the case for both acoustic modeling [28, 29, 30, 31, 32, 33] and language modeling [34, 35, 36].", "startOffset": 58, "endOffset": 82}, {"referenceID": 28, "context": "Surprisingly, this is the case for both acoustic modeling [28, 29, 30, 31, 32, 33] and language modeling [34, 35, 36].", "startOffset": 58, "endOffset": 82}, {"referenceID": 29, "context": "Surprisingly, this is the case for both acoustic modeling [28, 29, 30, 31, 32, 33] and language modeling [34, 35, 36].", "startOffset": 58, "endOffset": 82}, {"referenceID": 30, "context": "Surprisingly, this is the case for both acoustic modeling [28, 29, 30, 31, 32, 33] and language modeling [34, 35, 36].", "startOffset": 58, "endOffset": 82}, {"referenceID": 31, "context": "Surprisingly, this is the case for both acoustic modeling [28, 29, 30, 31, 32, 33] and language modeling [34, 35, 36].", "startOffset": 58, "endOffset": 82}, {"referenceID": 32, "context": "Surprisingly, this is the case for both acoustic modeling [28, 29, 30, 31, 32, 33] and language modeling [34, 35, 36].", "startOffset": 58, "endOffset": 82}, {"referenceID": 33, "context": "Surprisingly, this is the case for both acoustic modeling [28, 29, 30, 31, 32, 33] and language modeling [34, 35, 36].", "startOffset": 105, "endOffset": 117}, {"referenceID": 34, "context": "Surprisingly, this is the case for both acoustic modeling [28, 29, 30, 31, 32, 33] and language modeling [34, 35, 36].", "startOffset": 105, "endOffset": 117}, {"referenceID": 35, "context": "Surprisingly, this is the case for both acoustic modeling [28, 29, 30, 31, 32, 33] and language modeling [34, 35, 36].", "startOffset": 105, "endOffset": 117}, {"referenceID": 17, "context": "In comparison to the standard feed-forward MLPs or DNNs that first demonstrated breakthrough performance on conversational speech recognition [18], these acoustic models have the ability to model a large amount of acoustic context with temporal invariance, and in the case of convolutional models, with frequency invariance as well.", "startOffset": 142, "endOffset": 146}, {"referenceID": 36, "context": "In language modeling, recurrent models appear to improve over classical N-gram models through the use of an unbounded word history, as well as the generalization ability of continuous word representations [37].", "startOffset": 205, "endOffset": 209}, {"referenceID": 37, "context": "In the meantime, ensemble learning has become commonly used in several neural models [38, 39, 35], to improve robustness by reducing bias and variance.", "startOffset": 85, "endOffset": 97}, {"referenceID": 38, "context": "In the meantime, ensemble learning has become commonly used in several neural models [38, 39, 35], to improve robustness by reducing bias and variance.", "startOffset": 85, "endOffset": 97}, {"referenceID": 34, "context": "In the meantime, ensemble learning has become commonly used in several neural models [38, 39, 35], to improve robustness by reducing bias and variance.", "startOffset": 85, "endOffset": 97}, {"referenceID": 19, "context": "This paper is an expanded version of [20], with the following additional features:", "startOffset": 37, "endOffset": 41}, {"referenceID": 39, "context": "Past work [40] reports inter-transcriber error rates for data taken from the later RT03 test set (which contains Switchboard and Fisher, but no CallHome data).", "startOffset": 10, "endOffset": 14}, {"referenceID": 40, "context": "The first is the VGG architecture of [41].", "startOffset": 37, "endOffset": 41}, {"referenceID": 41, "context": "The second network is modeled on the ResNet architecture [42], which adds highway connections [43], i.", "startOffset": 57, "endOffset": 61}, {"referenceID": 42, "context": "The second network is modeled on the ResNet architecture [42], which adds highway connections [43], i.", "startOffset": 94, "endOffset": 98}, {"referenceID": 42, "context": "form of each layer\u2019s input to the layer\u2019s output [43, 44].", "startOffset": 49, "endOffset": 57}, {"referenceID": 43, "context": "form of each layer\u2019s input to the layer\u2019s output [43, 44].", "startOffset": 49, "endOffset": 57}, {"referenceID": 44, "context": "The last CNN variant is the LACE (layer-wise context expansion with attention) model [45].", "startOffset": 85, "endOffset": 89}, {"referenceID": 22, "context": "LACE is a TDNN [23] variant in which each higher layer is a weighted sum of nonlinear transformations of a window of lower layer frames.", "startOffset": 15, "endOffset": 19}, {"referenceID": 22, "context": "[23, 46] in the use of a learned attention mask and ResNet like linear pass-through.", "startOffset": 0, "endOffset": 8}, {"referenceID": 45, "context": "[23, 46] in the use of a learned attention mask and ResNet like linear pass-through.", "startOffset": 0, "endOffset": 8}, {"referenceID": 46, "context": "We use a bidirectional architecture [47] without frameskipping [29].", "startOffset": 36, "endOffset": 40}, {"referenceID": 28, "context": "We use a bidirectional architecture [47] without frameskipping [29].", "startOffset": 63, "endOffset": 67}, {"referenceID": 27, "context": "The core model structure is the LSTM defined in [28].", "startOffset": 48, "endOffset": 52}, {"referenceID": 47, "context": "Speaker adaptive modeling in our system is based on conditioning the network on an i-vector [48] characterization of each speaker [49, 50].", "startOffset": 92, "endOffset": 96}, {"referenceID": 48, "context": "Speaker adaptive modeling in our system is based on conditioning the network on an i-vector [48] characterization of each speaker [49, 50].", "startOffset": 130, "endOffset": 138}, {"referenceID": 49, "context": "Speaker adaptive modeling in our system is based on conditioning the network on an i-vector [48] characterization of each speaker [49, 50].", "startOffset": 130, "endOffset": 138}, {"referenceID": 50, "context": "As noted in [51, 52], the necessary gradient for use in backpropagation is a simple function of the posterior probability of a particular acoustic model state at a given time, as computed by summing over all possible word sequences in an unconstrained manner.", "startOffset": 12, "endOffset": 20}, {"referenceID": 51, "context": "As noted in [51, 52], the necessary gradient for use in backpropagation is a simple function of the posterior probability of a particular acoustic model state at a given time, as computed by summing over all possible word sequences in an unconstrained manner.", "startOffset": 12, "endOffset": 20}, {"referenceID": 11, "context": "As first done in [12], and more recently in [53], this can be accomplished with a straightforward alphabeta computation over the finite state acceptor representing the decoding search space.", "startOffset": 17, "endOffset": 21}, {"referenceID": 52, "context": "As first done in [12], and more recently in [53], this can be accomplished with a straightforward alphabeta computation over the finite state acceptor representing the decoding search space.", "startOffset": 44, "endOffset": 48}, {"referenceID": 11, "context": "In [12], the search space is taken to be an acceptor representing the composition HCLG for a unigram language model L on words.", "startOffset": 3, "endOffset": 7}, {"referenceID": 52, "context": "In [53], a language model on phonemes is used instead.", "startOffset": 3, "endOffset": 7}, {"referenceID": 52, "context": "As in [53], we use cross-entropy regularization.", "startOffset": 6, "endOffset": 10}, {"referenceID": 53, "context": "An initial decoding is done with a WFST decoder, using the architecture described in [54].", "startOffset": 85, "endOffset": 89}, {"referenceID": 54, "context": "We use an N-gram language model trained and pruned with the SRILM toolkit [55].", "startOffset": 74, "endOffset": 78}, {"referenceID": 55, "context": "All Ngram LMs were estimated by a maximum entropy criterion as described in [56].", "startOffset": 76, "endOffset": 80}, {"referenceID": 56, "context": "Our RNN-LMs are trained and evaluated using the CUEDRNNLM toolkit [57].", "startOffset": 66, "endOffset": 70}, {"referenceID": 33, "context": "This produced lower perplexity and word error than the standard, single-hidden-layer RNN-LM architecture [34].", "startOffset": 105, "endOffset": 109}, {"referenceID": 57, "context": "Training used noise-contrastive estimation (NCE) [58].", "startOffset": 49, "endOffset": 53}, {"referenceID": 35, "context": "for conversational speech recognition [36].", "startOffset": 38, "endOffset": 42}, {"referenceID": 58, "context": "There are two types of input vectors our LSTM LMs take, word based one-hot vector input and letter trigram vector [59] input.", "startOffset": 114, "endOffset": 118}, {"referenceID": 59, "context": "For the word based input, we leveraged the approach from [60] to tie the input embedding and output embedding together.", "startOffset": 57, "endOffset": 61}, {"referenceID": 60, "context": "Convergence was improved through a variation of selfstabilization [61], in which each output vector x of nonlinearities are scaled by 1 4 ln(1 + e ), where a \u03b2 is a scalar that is learned for each output.", "startOffset": 66, "endOffset": 70}, {"referenceID": 49, "context": "The N-gram LM configuration is modeled after that described in [50], except that maxent smoothing was used.", "startOffset": 63, "endOffset": 67}, {"referenceID": 61, "context": "All neural networks in the final system were trained with the Microsoft Cognitive Toolkit, or CNTK [63, 64], on a Linux-based multi-GPU server farm.", "startOffset": 99, "endOffset": 107}, {"referenceID": 49, "context": "[50] LSTM 15.", "startOffset": 0, "endOffset": 4}, {"referenceID": 52, "context": "[53] LSTM 15.", "startOffset": 0, "endOffset": 4}, {"referenceID": 49, "context": "[50] Combination 13.", "startOffset": 0, "endOffset": 4}, {"referenceID": 62, "context": "1-bit SGD parallelization technique [65].", "startOffset": 36, "endOffset": 40}, {"referenceID": 62, "context": "In [65], we showed that gradient values can be quantized to just a single bit, if one carries over the quantization error from one minibatch to the next.", "startOffset": 3, "endOffset": 7}, {"referenceID": 11, "context": "The acoustic training data is comprised by LDC corpora 97S62, 2004S13, 2005S13, 2004S11 and 2004S09; see [12] for a full description.", "startOffset": 105, "endOffset": 109}, {"referenceID": 39, "context": "We speculate that this is due to the nature of the Fisher training corpus, where the \u201cquick transcription\u201d guidelines were predominately used [40].", "startOffset": 142, "endOffset": 146}, {"referenceID": 63, "context": "Compared to earlier applications of CNNs to speech recognition [66, 67], our networks are much deeper, and use linear bypass connections across convolutional layers.", "startOffset": 63, "endOffset": 71}, {"referenceID": 64, "context": "Compared to earlier applications of CNNs to speech recognition [66, 67], our networks are much deeper, and use linear bypass connections across convolutional layers.", "startOffset": 63, "endOffset": 71}, {"referenceID": 30, "context": "They are similar in spirit to those studied more recently by [31, 30, 50, 32, 33].", "startOffset": 61, "endOffset": 81}, {"referenceID": 29, "context": "They are similar in spirit to those studied more recently by [31, 30, 50, 32, 33].", "startOffset": 61, "endOffset": 81}, {"referenceID": 49, "context": "They are similar in spirit to those studied more recently by [31, 30, 50, 32, 33].", "startOffset": 61, "endOffset": 81}, {"referenceID": 31, "context": "They are similar in spirit to those studied more recently by [31, 30, 50, 32, 33].", "startOffset": 61, "endOffset": 81}, {"referenceID": 32, "context": "They are similar in spirit to those studied more recently by [31, 30, 50, 32, 33].", "startOffset": 61, "endOffset": 81}, {"referenceID": 44, "context": "We improve on these architectures with the LACE model [45], which iteratively expands the effective window size, layer-by-layer, and adds an attention mask to differentially weight distant context.", "startOffset": 54, "endOffset": 58}, {"referenceID": 65, "context": "Our spatial regularization technique is similar in spirit to stimulated deep neural networks [68].", "startOffset": 93, "endOffset": 97}, {"referenceID": 11, "context": "Our use of lattice-free MMI is distinctive, and extends previous work [12, 53] by proposing the use of a mixed triphone/phoneme history in the language model.", "startOffset": 70, "endOffset": 78}, {"referenceID": 52, "context": "Our use of lattice-free MMI is distinctive, and extends previous work [12, 53] by proposing the use of a mixed triphone/phoneme history in the language model.", "startOffset": 70, "endOffset": 78}, {"referenceID": 35, "context": "For our best CNN system, LSTM-LM rescoring yields a relative word error reduction of 23%, and a 20% relative gain for the combined recognition system, considerably larger than previously reported for conversational speech recognition [36].", "startOffset": 234, "endOffset": 238}], "year": 2016, "abstractText": "Conversational speech recognition has served as a flagship speech recognition task since the release of the DARPA Switchboard corpus in the 1990s. In this paper, we measure the human error rate on the widely used NIST 2000 test set, and find that our latest automated system has reached human parity. The error rate of professional transcriptionists is 5.9% for the Switchboard portion of the data, in which newly acquainted pairs of people discuss an assigned topic, and 11.3% for the CallHome portion where friends and family members have open-ended conversations. In both cases, our automated system establishes a new state-of-the-art, and edges past the human benchmark. This marks the first time that human parity has been reported for conversational speech. The key to our system\u2019s performance is the systematic use of convolutional and LSTM neural networks, combined with a novel spatial smoothing method and lattice-free MMI acoustic training.", "creator": "LaTeX with hyperref package"}}}