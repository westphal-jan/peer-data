{"id": "1610.05361", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Oct-2016", "title": "End-to-end attention-based distant speech recognition with Highway LSTM", "abstract": "End-to-end attention-based models have been shown to be competitive alternatives to conventional DNN-HMM models in the Speech Recognition Systems. In this paper, we extend existing end-to-end attention-based models that can be applied for Distant Speech Recognition (DSR) task. Specifically, we propose an end-to-end attention-based speech recognizer with multichannel input that performs sequence prediction directly at the character level. To gain a better performance, we also incorporate Highway long short-term memory (HLSTM) which outperforms previous models on AMI distant speech recognition task.", "histories": [["v1", "Mon, 17 Oct 2016 21:16:56 GMT  (70kb,D)", "http://arxiv.org/abs/1610.05361v1", "8 pages, 2 figures"]], "COMMENTS": "8 pages, 2 figures", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["hassan taherian"], "accepted": false, "id": "1610.05361"}, "pdf": {"name": "1610.05361.pdf", "metadata": {"source": "CRF", "title": "END-TO-END ATTENTION-BASED DISTANT SPEECH RECOGNITION WITH HIGHWAY LSTM", "authors": ["Hassan Taherian"], "emails": ["hasan.t@aut.ac.ir"], "sections": [{"heading": null, "text": "Keywords: remote voice recognition, attention, highway LSTM"}, {"heading": "1 Introduction", "text": "Further improvements have been reported through the use of more advanced models such as the Attention-based Recurrent Sequence Generator (ARSG) as part of an end-to-end speech recognition system [4]. Although these new techniques help to reduce the error rate of words on the automatic speech recognition system (ASR), Distant Speech Recognition (DSR) remains a challenging task due to reverberation and overlapping acoustic signals, even with advanced front-end processing techniques such as state-of-the-art beamforming [5]. It is reported that the use of multiple microphones with signal preprocessing techniques such as beamforming will improve the performance of the DSR [5]. However, the performance of such techniques is suboptimal as they depend heavily on the microphone array geometry and the location of the target source."}, {"heading": "2 Related Work", "text": "Kim et al. have already proposed an attention-based approach with multi-channel input [9]. In their work, an attention mechanism is embedded in a recursive neural network-based acoustic model to weight channel inputs for a number of images. The more reliable the channel input, the higher the score. By calculating the phase difference between pairs of microphones, they also integrated spatial information into their attention mechanism to speed up the learning of auditory attention. They reported that this model achieves similar performance to beam imaging techniques. An advantage of using such a model is that no prior knowledge of microphone array geometry is required and real-time processing is much faster than models with front-end pre-processing techniques. However, they have not exploited the full potential of the attention mechanism in their proposed model by using conventional DNN-HMM acoustic models."}, {"heading": "3 Proposed Model", "text": "Our model consists of three parts: an encoder, an attention mechanism, and a decoder. The model takes multi-channel input X and stochastically generates an output sequence (y1, y2,..., yT).The input X consists of N-channel inputs X = {Xch1, Xch2,..., XchN}, with each channel Xchi being a sequence of small overlapping audio frames Xchi = {Xchi1, Xchi2,..., XchiL}.The encoder transforms the input into a different representation using Recurrent Neural Network (RNN).Subsequently, the attention mechanism weighs elements of a new representation based on their relevance to the output y at each time step. Finally, the decoder, which in our model is an RNN, generates an output sequence (y1, y2,..., yT), with the sequence weighted based on its relevance to the output y."}, {"heading": "3.1 The Encoder", "text": "It is therefore a very slow path to an end-to-end scenario, since the attention mechanism of each channel has to be weighted at each step. Furthermore, phase difference calculations between pairs of microphones are required for scoring frames. Due to this computerized complexity, we decided to link all channel features together and feed them into the network, as Liu al. did. We used bidirectional long-term memory (BHLSTM) in the encoder. Zhang et al. showed that BHLSTM achieved a state-of-the-art performance in the AMI (SDM) task with dropout."}, {"heading": "3.2 The Attetion Mechanism", "text": "The attention mechanism is a subnetwork that weighs the encoded inputs h. It selects the time positions on the input sequence useful for updating the hidden state of the decoder. Specifically, at each step i the attention mechanism scalar energy ei, l is calculated for each time step of the encoded input hl and the hidden state of the decoder si. The scalar energy is converted into probability distribution, which is called alignment, via time steps using the Softmax function. Finally, the context vector ci is calculated by a linear combination of elements of encoded inputs hl [12].ei, l = w T tanh (Wsi + Vhl + b) (12) \u03b1i, l = exp (ei, l).L l = 1 exp (ei) (13)."}, {"heading": "3.3 The Decoder", "text": "The task of the decoder, which is an RNN within the ARSG framework, is to generate a probability distribution over the next character, depending on all characters already seen. This distribution is generated with an MLP and a softmax by the hidden state of the decoder si \u2212 1, the previously issued character yi \u2212 1 and the context vector of the attention mechanism ci.P (yi | X, y < i) = MLP (si \u2212 1, ci \u2212 1) (17) The hidden state of the decoder si \u2212 1 is a function of the previously emitted character yi \u2212 1 and the context vector of the attention mechanism ci \u2212 1: si \u2212 1, yi \u2212 1, ci \u2212 1) (18) Where f is a single layer of the standard LSTM. The ARSG with encoder can be trained jointly for end-to-end speech recognition. The objective function is to maximize the probability of the protocol sequence."}, {"heading": "4 Conclusion", "text": "In this paper, we show that end-to-end models based only on neural networks can be applied to the task of Distant Speech Recognition with the concatenation of multi-channel inputs, as well as higher accuracy when using highway LSTMs in the encoder network. In the future, we will investigate the integration of the Recurrent Neural Network Language Model (RNNLM) with the Attention-based Recurrent Sequence Generator (ARSG) with the Highway LSTM."}], "references": [{"title": "First-Pass Large Vocabulary Continuous Speech Recognition using Bi-Directional Recurrent DNNs", "author": ["A.Y. Hannun", "A.L. Maas", "D. Jurafsky", "A.Y. Ng"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2014}, {"title": "Connectionist Temporal Classification: Labelling Unsegmented Sequence Data with Recurrent Neural Networks", "author": ["A. Graves", "S. Fernandez", "F. Gomez", "J. Schmidhuber"], "venue": "Proc. 23rd Int. Conf. Mach. Learn.,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2006}, {"title": "Towards End-To-End Speech Recognition with Recurrent Neural Networks, in JMLR", "author": ["A. Graves", "N. Jaitly"], "venue": "Workshop and Conference Proceedings,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2014}, {"title": "End-To-End Attention-Based Large Vocabulary Speech Recognition, arXiv", "author": ["D. Bahdanau", "J. Chorowski", "D. Serdyuk", "P. Brakel", "Y. Bengio"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2015}, {"title": "Microphone array processing for distant speech recognition: From close-talking microphones to far-field sensors", "author": ["K. Kumatani", "J. McDonough", "B. Raj"], "venue": "IEEE Signal Process. Mag., vol. 29,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2012}, {"title": "Likelihood-maximizing beamforming for robust hands-free speech recognition", "author": ["M.L. Seltzer", "B. Raj", "R.M. Stern"], "venue": "IEEE Trans. Speech Audio Process.,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2004}, {"title": "Convolutional neural networks for distant speech recognition", "author": ["P. Swietojanski", "A. Ghoshal", "S. Renals"], "venue": "IEEE Signal Process. Lett., vol. 21,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2014}, {"title": "Highway long short-term memory RNNS for distant speech recognition, in ICASSP", "author": ["Y. Zhang", "G. Chen", "D. Yu", "K. Yaco", "S. Khudanpur", "J. Glass"], "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2016}, {"title": "Recurrent Models for auditory attention in multi-microphone distance speech recognition", "author": ["S. Kim", "I. Lane"], "venue": "Proc. Annu. Conf. Int. Speech Commun. Assoc. INTERSPEECH,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2016}, {"title": "Using neural network front-ends on far field multiple microphones based speech recognition, in ICASSP", "author": ["Y. Liu", "P. Zhang", "T. Hain"], "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2014}, {"title": "Long short-term memory based recurrent neural network architectures for large scale acoustic modeling, in Interspeech", "author": ["H. Sak", "A. Senior", "F. Beaufays"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2014}, {"title": "Listen, attend and spell: A neural network for large vocabulary conversational speech recognition, in ICASSP", "author": ["W. Chan", "N. Jaitly", "Q. Le", "O. Vinyals"], "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2016}, {"title": "End-to-end Continuous Speech Recognition using Attention-based Recurrent NN: First Results", "author": ["J. Chorowski", "D. Bahdanau", "K. Cho", "Y. Bengio"], "venue": "Deep Learn. Represent. Learn. Work. NIPS", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2014}, {"title": "Weighted finite-state transducers in speech recognition, Comput", "author": ["M. Mohri", "F. Pereira", "M. Riley"], "venue": "Speech Lang.,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2002}], "referenceMentions": [{"referenceID": 0, "context": "Recently end-to-end neural network speech recognition systems has shown promising results [1,2,3].", "startOffset": 90, "endOffset": 97}, {"referenceID": 1, "context": "Recently end-to-end neural network speech recognition systems has shown promising results [1,2,3].", "startOffset": 90, "endOffset": 97}, {"referenceID": 2, "context": "Recently end-to-end neural network speech recognition systems has shown promising results [1,2,3].", "startOffset": 90, "endOffset": 97}, {"referenceID": 3, "context": "Further improvement were reported by using more advanced models such as Attention-based Recurrent Sequence Generator (ARSG) as part of an end-to-end speech recognition system [4].", "startOffset": 175, "endOffset": 178}, {"referenceID": 4, "context": "Although these new techniques help to decrease the word error rate (WER) on the automatic speech recognition system (ASR), Distant Speech Recognition (DSR) remains a challenging task due to the reverberation and overlapping acoustic signals, even with sophisticated front-end processing techniques such as state-of-the-art beamforming [5].", "startOffset": 335, "endOffset": 338}, {"referenceID": 4, "context": "It is reported that using multiple microphones with signal preprocessing techniques like beamforming will improve the performance of the DSR [5].", "startOffset": 141, "endOffset": 144}, {"referenceID": 5, "context": "However the performance of such techniques are suboptimal since they are depended heavily on the microphone array geometry and the location of target source [6].", "startOffset": 157, "endOffset": 160}, {"referenceID": 6, "context": "Other works have shown that Deep Neural Networks with multichannel inputs can be used for learning a suitable representation for distant speech recognition without any front-end preprocessing [7].", "startOffset": 192, "endOffset": 195}, {"referenceID": 3, "context": "We use [4] as the baseline while extend its single channel input to multiple channel.", "startOffset": 7, "endOffset": 10}, {"referenceID": 3, "context": "We also integrate the language model in the decoder of ASRG with Weighted Finite State Transducer (WFST) framework as it was purposed in [4].", "startOffset": 137, "endOffset": 140}, {"referenceID": 7, "context": "To avoid slowness in convergence, we use Highway LSTM [8] in our model which allows us to have stacked LSTM layers without having gradient vanishing problem.", "startOffset": 54, "endOffset": 57}, {"referenceID": 8, "context": "already proposed an attention-based approach with multichannel input [9].", "startOffset": 69, "endOffset": 72}, {"referenceID": 9, "context": "In our model, attention mechanism is similar to [10] but it I also connected to the ARGS to produce an end-to-end results.", "startOffset": 48, "endOffset": 52}, {"referenceID": 8, "context": "proposed in [9], is to feed each channel separately in to the RNN at each time step.", "startOffset": 12, "endOffset": 15}, {"referenceID": 9, "context": "have done [10] in their model.", "startOffset": 10, "endOffset": 14}, {"referenceID": 7, "context": "showed that BHLSTM with dropout achieved state-of-theart performance in the AMI (SDM) task [8].", "startOffset": 91, "endOffset": 94}, {"referenceID": 7, "context": "Thus this model allows the network to go much deeper [8].", "startOffset": 53, "endOffset": 56}, {"referenceID": 10, "context": "BHLSTM is based on long short-term memory projected (LSTMP) which originally proposed by [11].", "startOffset": 89, "endOffset": 93}, {"referenceID": 10, "context": "This architecture, shown in the Figure 1, converges faster than standard LSTM and it has less parameters than standard LSTM while keeping the performance [11].", "startOffset": 154, "endOffset": 158}, {"referenceID": 11, "context": "Finally, the context vector ci is calculated by linear combination of elements of encoded inputs hl [12].", "startOffset": 100, "endOffset": 104}, {"referenceID": 3, "context": "showed that using context-based scheme for attention model is prone to error since similar elements of embedded input h is scored equally [4].", "startOffset": 138, "endOffset": 141}, {"referenceID": 12, "context": "suggested a windowing approach which limits the number of embedded inputs for computation of alignments [13].", "startOffset": 104, "endOffset": 108}, {"referenceID": 11, "context": "suggested to sample from previous predicted characters with the rate of 10% and use them as input for predicting next character instead of ground truth transcript [12].", "startOffset": 163, "endOffset": 167}, {"referenceID": 11, "context": "showed that end-to-end models for speech recognition can achieve good results without using language model but for reducing word error rate (WER), language model is essential [12].", "startOffset": 175, "endOffset": 179}, {"referenceID": 3, "context": "suggested, is to use Weighted Finite State Transducer (WFST) framework [4].", "startOffset": 71, "endOffset": 74}, {"referenceID": 13, "context": "By composition of grammar and lexicon, WFST defines the log probability for a character sequence ( see [14]) .", "startOffset": 103, "endOffset": 107}, {"referenceID": 3, "context": "(see [4]).", "startOffset": 5, "endOffset": 8}], "year": 2016, "abstractText": "End-to-end attention-based models have been shown to be competitive alternatives to conventional DNN-HMM models in the Speech Recognition Systems. In this paper, we extend existing end-to-end attentionbased models that can be applied for Distant Speech Recognition (DSR) task. Specifically, we propose an end-to-end attention-based speech recognizer with multichannel input that performs sequence prediction directly at the character level. To gain a better performance, we also incorporate Highway long short-term memory (HLSTM) which outperforms previous models on AMI distant speech recognition task.", "creator": "LaTeX with hyperref package"}}}