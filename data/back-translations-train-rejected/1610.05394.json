{"id": "1610.05394", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Oct-2016", "title": "Sequential Learning without Feedback", "abstract": "In many security and healthcare systems a sequence of features/sensors/tests are used for detection and diagnosis. Each test outputs a prediction of the latent state, and carries with it inherent costs. Our objective is to {\\it learn} strategies for selecting tests to optimize accuracy \\&amp; costs. Unfortunately it is often impossible to acquire in-situ ground truth annotations and we are left with the problem of unsupervised sensor selection (USS). We pose USS as a version of stochastic partial monitoring problem with an {\\it unusual} reward structure (even noisy annotations are unavailable). Unsurprisingly no learner can achieve sublinear regret without further assumptions. To this end we propose the notion of weak-dominance. This is a condition on the joint probability distribution of test outputs and latent state and says that whenever a test is accurate on an example, a later test in the sequence is likely to be accurate as well. We empirically verify that weak dominance holds on real datasets and prove that it is a maximal condition for achieving sublinear regret. We reduce USS to a special case of multi-armed bandit problem with side information and develop polynomial time algorithms that achieve sublinear regret.", "histories": [["v1", "Tue, 18 Oct 2016 01:15:57 GMT  (228kb,D)", "http://arxiv.org/abs/1610.05394v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["manjesh hanawal", "csaba szepesvari", "venkatesh saligrama"], "accepted": false, "id": "1610.05394"}, "pdf": {"name": "1610.05394.pdf", "metadata": {"source": "CRF", "title": "Sequential Learning without Feedback\u2217", "authors": ["Manjesh Hanawal", "Csaba Szepesvari", "Venkatesh Saligrama"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "This year it is more than ever before."}, {"heading": "1.1 Related Work", "text": "Unlike our USS setup, there is a large amount of literature that deals with sensor capture (see [4, 5, 6]). Like us, they also deal with cascade models with features / test costs, but their methodology is based on training decision strategies with fully monitored data for the use of prediction times. There are also several methods that work in an online bandit setting and train prediction models with feature costs [12], but again they need soil truth labels as reward feedback. A slightly more general version of [12] is developed in [13], where the learner can additionally decide whether to purchase soil truth labels for a price. Our work bears some similarity to the concept of active classification, which deals with learning strategies [7, 8] within a given sequence of tests. Similarly, as we look at this work, the cost of using tests and the goal is to learn when to stop making setup decisions related to our loss."}, {"heading": "2 Background", "text": "In this section, we will introduce a series of sequential decision problems, namely stochastic partial monitoring, bandits and bandits with ancillary observations, which we will build on lateral observations. (First, we will use a few words about our notation to denote random variables.) The set of real numbers is denoted by R. For positive integers n, which we can identify with the d-dimensional probability. (.) In a stochastic partial monitoring problem (SPM), a learner interacts with a stochastic environment in a sequential manner. In the round t = 1, 2,. the learner selects an action from an action and receives a feedback Yt. (.)"}, {"heading": "3 Unsupervised Sensor Selection", "text": "The set of real numbers is referred to by R. For positive integral numbers n, we will allow [n] s [n] s [n] s [n] s. We will allow M1 (X) to designate the number of probability distributions over a certain number of probability distributions. We will first consider the non-superordinate, cascaded sensor selection problems. We will throw it out as a special case of stochastic partial monitoring (SPM), which is described in the appendix. Later, we will briefly describe expansions to tree structures and contextual cases. Formally, a problem instance is specified by a pair (P, c) where P is a distribution over K + 1 dimensional hypercalculation, and is a K-rated problem."}, {"heading": "4 When is USS Learnable?", "text": "Unless there is a \"real\" problem class, if there is a \"maximum\" learnable problem class, if there is a \"maximum\" problem class, if there is a \"learnable\" class, and for each \"optimal\" class there is an \"optimal\" problem class, which is \"learnable,\" and for each \"optimal\" class there is an \"optimal\" class, which is \"unlearnable.\" In this section, we will examine two special learnable problem classes, in which the regular properties of the instances in the \"SD\" are intuitive, while the \"optimal\" WD is the maximum extension of the \"SD.\" Let's start with some definitions. \""}, {"heading": "5 Regret Equivalence", "text": "In this section, we note that USS with SD property as \"regret is equivalent\" to an example of multi-armed bandits (MAB) with side information (MAB). The corresponding MAB algorithm can then be imported accordingly to solve USS efficiently. Let PUSS be the set of USSs with action set A = [K]. The corresponding bandit problems will have the same plot, while for plot k [K] the neighborhood problem N (k) = [k]. Let's take each instance (P, c).PUSS and leave (Y, Y 1,., Y K).P the unobserved state of the environment. We let the reward distribution for arm k in the corresponding bandit problem be a shifted Bernoulli distribution. Specifically, the cost of arm k follow the distribution of I {Y = Y 1} \u2212 Ck (we use costs here to avoid inverse signs).The cost of different weapons are defined so that they are independent of each other."}, {"heading": "6 Algorithms", "text": "The reduction of the previous section suggests that an algorithm developed for stochastic bandits with lateral observations can be used. < p > p > p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p."}, {"heading": "6.1 Algorithm for Weak Dominance", "text": "The reduction scheme described above is optimal under the SD property and may fail under the more relaxed WD property. < i = > K = > K = < i = > K = < i = > K = < i = > K = (3) It follows for all two sensors i < j = (3) - (3) - (4) - K (6) - P (6) - K (6) - 1); and so we can calculate a boundary error between two sensors by tracking only the traces of the discrepancies relative to sensor 1. Under WD, the boundary error is lower and the traces of only the discrepancies relative to sensor 1 are no longer sufficient because P (Y i = Y 1) - a good estimate for P (Y i = Y) - (Y = Y j j) - 1). Our key finding is based on the fact that under WD - for a given set of discrepancies - probabilities - for all of j - Y - (Y - (Y) (K)."}, {"heading": "6.2 Extensions", "text": "Tree Structures: We can deal with trees where the sensors are now organized as a tree with root nodes corresponding to sensor 1, and we can select any path that comes from the root node. To see this, note that the marginal error condition of Equation 1 still characterizes the optimal sensor selection. Under a modified variant of Equation 2, this is in relation to the children of an optimal sensor, it follows that it is again sufficient to trace inconsistencies. Similarly, equality can also be modified in an appropriate way. This leads to sublinear regret algorithm for tree structures. Context-dependent sensor selection: Each example has a context x, Rd and a test card (partial) of the context ix."}, {"heading": "7 Experiments", "text": "In this section, we evaluate the performance of algorithms 1 and 2 on synthetic and real data sets (PIMA diabetes) and heart disease (Cleveland). Thus, the cost of each data set is very high, both in the way we allocate the cost of each application process to each application process, and in the way we allocate the cost of each application process to each application process. To ease the SD requirement, we set errors to 10% during data generation. Sensor outputs 2 and 3, if they base the properties on three \"sensors\" based on their cost. For PIMA diabetes dataset (size = 768), the first sensor is associated with the cost / profile ($6), the second sensor additionally uses."}, {"heading": "8 Conclusions", "text": "This type of problem occurs in a number of health and safety applications. In these applications, we have a collection of tests that are typically organized in a hierarchical architecture in which test results are sequenced. The goal is to identify the best balance between accuracy and test costs.The biggest challenge with these applications is that fundamentally truthful annotated examples are not available and it is often difficult to acquire them locally. We proposed a novel approach to sensor selection based on a new notion of weak and strong dominance characteristics. We demonstrated that a weak dominance condition is the maximum, as violations of this condition lead to a loss of learning ability. Our experiments show that weak dominance in practice for real data sets, and typically for these data sets, can be as effective as supervised (bandit) selection. Recognition: The authors thank Dr. Joe Wang for helpful discussions and in particular the concept of strong dominance."}, {"heading": "9 Appendix", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Stochastic Partial Monitoring Problem 3", "text": "In an SPM, a learner interacts with a stochastic environment in a sequential manner. In turn t = 1, 2,... the learner selects an action At from an action set A and receives a feedback Yt-Y from a distribution p, which depends on the chosen action and also on the environment instance identified by a \"parameter.\" The learner also receives a reward Rt, which is a function of the chosen action and the unknown parameter \u03b8: Rt = r (At, \u03b8). The reward can be part of the feedback for round t. The aim of the learner is to maximize all his expected rewards. The family of distributions (p (\u00b7; a, \u03b8) a, cycles and the family of rewards (r (a, \u03b8) a, cycles and the amount of possible parameters is known to the learner who uses this knowledge to sensibly select his next action in order to reduce his uncertainty, so that ultimately it is best possible to achieve a reward (b)."}, {"heading": "Proof of Proposition 1", "text": "Proof. \u21d2: Let A be an algorithm that achieves a sublinear regret and selects an instance for which an explanation is expected. Let P = PS (PY | S). The regret Rn (A, \u03b8) from A to instance \u03b8 can be written in formRn (A, \u03b8) = \u2211 k (K) EPS [Nk (n)] k (\u03b8), where Nk (n) is the number of times the action k of A is chosen during the n rounds, while A interacts with the Id sequence Y1."}, {"heading": "Proof of Corollary 2", "text": "Proof: Under Proposition 5, awd is well defined by WD, while under Proposition 6, awd is well defined by WD."}, {"heading": "Proof of Proposition 2", "text": "Proof: We construct a map as required by Proposition 1. Let us take an instance and let it be its decomposition as defined above. In order to identify an optimal action in \u03b8, it is clearly sufficient to use the sign of \u03b3i + Ci \u2212 (\u03b3i = P (Y i 6 = Y) \u2212 P (Y i 6 = Y) \u2212 P (Y j 6 = Y Y Y Y) = ((((((P (Y i 6 = Y)), Y i = Y) = Y (Y) = Y (Y) = Y (Y) = Y (Y), Y (Y) = Y (Y) = 6, Y (Y) = 6, Y (Y) = 6 (P), Y (Y) = 6 (Y), Y (Y) = 6 (P), Y (Y) = 6, Y (P) = 6, Y (P) = 6 (P)."}, {"heading": "Proof of Proposition 3", "text": "Proof. \u21d2: It follows from the premise that Cj \u2212 Ci \u2265 \u03b3i \u2212 \u03b3j. Thus, by (3), Cj \u2212 Ci \u2265 P (Y i 6 = Y j)."}, {"heading": "Proof of proposition 4", "text": "Proof. \u21d2: The condition \u03b3i + Ci \u2264 \u03b3j + Cj implies that \u03b3j \u2212 \u03b3i \u2265 Ci \u2212 Cj. Through the logical sequence 1 we get P (Y i 6 = Y j) \u2265 Ci \u2212 Cj. So: Let us leave Ci \u2212 Cj \u2264 P (Y i 6 = Y j)."}, {"heading": "Proof of Theorem ??", "text": "Proof. Let a as in the theorem statement. By Proposition 8, awd is the unique sound action-selector map over \u0445WD. So, for each \u03b8 = PS PY | S-WD, awd (PS) = a (PS)."}, {"heading": "Proof of Proposition 5", "text": "So the only question is whether (7a) is also valid. We prove this by contradiction: So let us assume that (7a) does not apply to some j < i, Ci \u2212 Cj \u2265 P (Y i 6 = Y j), i.e. to some j < i, Ci \u2212 Cj \u2265 P (Y i 6 = Y j)."}, {"heading": "Proof of Proposition 6", "text": "Proof. Let us take any \u03b8-WD with \u03b8 = PS PY | S, i = awd (PS), j = a \u043a (\u03b8). If i = j, there is nothing to prove. Therefore, let us first assume that j > i. Then through (7b), Cj \u2212 Ci \u2265 P (Y i 6 = Y j). As a logical consequence, let us assume that j < i. Then through (5), Ci \u2212 Cj \u2265 P (Y i 6 = Y j). If we combine these two inequalities, however, we get through (7a), Ci \u2212 Cj \u2264 \u03b3j + Cj, which contradicts the definition of j. Let us now assume that j < i. then through (5), Ci \u2212 Cj \u2265 P (Y i 6 = Y j).However, through (7a), Ci \u2212 Cj < P (Y i 6 = Y j j), so j < i cannot hold both and we must have j = i."}, {"heading": "Proof of Proposition 8", "text": "The reason for the change is as follows: Consider that in principle, PY | S can be changed so that on the new instance i is still an optimal action, while j is not an optimal action, while the new instance xi = P \u00b2 1 is. The change is as follows: Consider each y \u2212 j = (y1, yj + 1, yj + 1, yp = 0, yp = 1, yp = 1, y. yj \u2212 1, yj + 1, y. yK) an optimal action, while the new instance xi = PS \u00b2 1, yp = 1, yp = 0, yp = 0."}, {"heading": "Proof of Theorem 1", "text": "The proof for theorem 1: We construct a map as required by clause 1. Let us take an instance \u03b8 = PS PY | S, and let its decomposition be as before. Let us leave \u03b3i = P (Y i 6 = Y), (Y, Y 1,..., Y K) \u0445 \u03b8, Ci = c1 + \u00b7 \u00b7 \u00b7 + ci. To identify an optimal action in \u03b8, it is clearly sufficient to leave the sign of \u03b3i + Ci \u2212 (\u03b3j + Cj) = \u03b3i \u2212 \u03b3i \u2212 \u03b3j + (Ci \u2212 Cj) for all pairs i, j [K] 2. Without loss of universality (WLOG) let us leave i < j. by clause 2, \u03b3i \u2212 \u03b3i \u2212 \u03b3j = P (Y i 6 = Y j) \u2212 2P (Y j 6 = Y, Y i = Y).Now that the strong dominance condition (Y j 6 = Y, Y i = Y) is fulfilled."}, {"heading": "Proof of Theorem 3", "text": "Evidence. We first consider the case when K = 2 and arbitrarily C2 \u2212 C1 = 1 / 4. We consider two instances, \u03b8, \"\" in such a way that, for example, \u03b8, action k = 1 \"with an action gap of c (2, \u03b8) \u2212 c (1, \u03b8) = 1 / 4 is optimal between the cost of the second and the first action, while the entries in PS,\" k = 2 \"are the optimal action and the action gap c (1, \u03b8) \u2212 c, where 0 < < 3 / 8. Further, the entries in PS\" and PS \"are different,\" verifiable, \"at most. Hence, a standard consideration is that no algorithm can achieve sublinear minimax repentance over\" WD, \"because each algorithm is only able to identify PS. The constructions of\" y \"and\" are listed in Table 2: The input in a cell indicates the probability of the event as being through the column and line designation Y = probability. \"For example, Y = probability.\""}, {"heading": "Proof of Proposition 9", "text": "Evidence. First of all, it should be noted that the allocation of policies is such that the number of arm strokes k after n rounds by the policy \u03c0 on the problem instance f (\u03b8) is the same as the number of arm strokes k by \u03c0 \u2032 on the problem instance \u03b8. Remember that the mean of arm strokes k in the problem instance \u03b8 + Ck and the mean of the corresponding arm strokes in the problem instance f (\u03b8) \u03b31 \u2212 (\u03b3i + Ci) is Rn (\u03c0) = \u2211 k [K] EPS [Nk (n)] EPS [Nk (\u03b3k)] EPS [Nk (\u04211 \u2212 \u0421i \u2212 Ci} (\u04211 \u2212 \u0421i)) = \u0445 k [K] EPS [Nk (\u04211 \u2212 \u04211 \u2212 \u0421k \u2212 \u0421k \u2212 \u0421k)] EPS [Nk (n)] (\u0421k + Ck \u2212 min i \u0421i) \u2212 (\u04211 \u2212 \u0421i) (\u04211 \u2212 \u0421i) (\u0421i \u2212 \u0421i), \u04211) (\u0421i)."}], "references": [{"title": "Multi-stage classifier design", "author": ["K. Trapeznikov", "V. Saligrama", "D.A. Castanon"], "venue": "Machine Learning, vol. 39, pp. 1\u201324, 2014.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2014}, {"title": "Partial monitoring \u2013 classification, regret bounds, and algorithms", "author": ["G. Bart\u00f3k", "D. Foster", "D. P\u00e1l", "A. Rakhlin", "C. Szepesv\u00e1ri"], "venue": "Mathematics of Operations Research, vol. 39, pp. 967\u2013997, 2014.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2014}, {"title": "On the likelihood that one unknown probability exceeds another in view of the evidence of two samples", "author": ["W. Thompson"], "venue": "Biometrika, vol. 25, pp. 285\u2013294, 1933.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1933}, {"title": "Supervised sequential classification under budget constraints", "author": ["K. Trapeznikov", "V. Saligrama"], "venue": "AISTATS, 2013, pp. 235\u2013242.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2013}, {"title": "Directed acyclic graph for resource constrained prediction", "author": ["J. Wang", "K. Trapeznikov", "V. Saligrama"], "venue": "Proceeding of Conference on Neural Information Processing Systems, NIPS, 2015.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2015}, {"title": "Feature-budgeted random forest", "author": ["F. Nan", "J. Wang", "V. Saligrama"], "venue": "Proceedings of the International Conference on Machine Learning, 2015.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning when to stop thinking and do something!", "author": ["B. P\u00f3czos", "Y. Abbasi-Yadkori", "C. Szepesv\u00e1ri", "R. Greiner", "N. Sturtevant"], "venue": "in ICML,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2009}, {"title": "Learning cost-sensitive active classifiers", "author": ["R. Greiner", "A. Grove", "D. Roth"], "venue": "Artificial Intelligence, vol. 139, pp. 137\u2013174, 2002.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2002}, {"title": "Learning and classifying under hard budgets", "author": ["A. Kapoor", "R. Greiner"], "venue": "ECML, 2005.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2005}, {"title": "Adore: Adaptive object recognition", "author": ["B. Draper", "J. Bins", "K. Baek"], "venue": "International Conference on Vision Systems, 1999, pp. 522\u2013537.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1999}, {"title": "Efficient interpretation policies", "author": ["R. Isukapalli", "R. Greiner"], "venue": "International Joint Conference on Artificial Intelligence, 2001, pp. 1381\u20131387. 16", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2001}, {"title": "Prediction with limited advice and multiarmed bandits with paid observations", "author": ["Y. Seldin", "P. Bartlett", "K. Crammer", "Y. Abbasi-Yadkori"], "venue": "Proceeding of International Conference on Machine Learning, ICML, 2014, pp. 208\u2013287.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2014}, {"title": "Online learning with costly features and labels", "author": ["N. Zolghadr", "G. Bart\u00f3k", "R. Greiner", "A. Gy\u00f6rgy", "C. Szepesv\u00e1ri"], "venue": "NIPS, 2013, pp. 1241\u20131249.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2013}, {"title": "Asymptotically efficient adaptive allocation schemes for controlled i.i.d. processes: Finite parameter space", "author": ["R. Agrawal", "D. Teneketzis", "V. Anantharam"], "venue": "IEEE Transaction on Automatic Control, vol. 34, pp. 258\u2013267, 1989.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1989}, {"title": "From bandits to experts: On the value of side-observations", "author": ["S. Mannor", "O. Shamir"], "venue": "NIPS, 2011.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2011}, {"title": "Online learning with feedback graphs:beyond bandits", "author": ["N. Alon", "N. Cesa-Biancbi", "O. Dekel", "T. Koren"], "venue": "Proceeding of Conference on Learning Theory, 2015, pp. 23\u201335.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}, {"title": "From bandits to experts: A tale of domination and independence", "author": ["N. Alon", "N. Cesa-Biancbi", "C. Gentile", "Y. Mansour"], "venue": "Proceeding of Conference on Neural Information Processing Systems, NIPS, 2013, pp. 1610\u20131618.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2013}, {"title": "Online learning with gaussian payoffs and side observations", "author": ["Y. Wu", "A. Gy\u00f6rgy", "C. Szepesv\u00e1ri"], "venue": "NIPS, September 2015, pp. 1360\u20131368.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "A number of different imaging and non-imaging tests are sequentially processed (see [1]).", "startOffset": 84, "endOffset": 87}, {"referenceID": 1, "context": "3 we pose our problem as a version of stochastic partial monitoring problem [2] with atypical reward structure, where tests are viewed as actions and sequential observations serves as side information.", "startOffset": 76, "endOffset": 79}, {"referenceID": 2, "context": "Bandit problems [3] are a special case of partial monitoring, where the key missing information is the expected cost for each action, and the feedback is the noisy version of the expected cost of the action chosen.", "startOffset": 16, "endOffset": 19}, {"referenceID": 3, "context": "In contrast to our USS setup there exists a wide body of literature dealing with sensor acquisition (see[4, 5, 6]).", "startOffset": 104, "endOffset": 113}, {"referenceID": 4, "context": "In contrast to our USS setup there exists a wide body of literature dealing with sensor acquisition (see[4, 5, 6]).", "startOffset": 104, "endOffset": 113}, {"referenceID": 5, "context": "In contrast to our USS setup there exists a wide body of literature dealing with sensor acquisition (see[4, 5, 6]).", "startOffset": 104, "endOffset": 113}, {"referenceID": 11, "context": "There are also several methods that work in an online bandit setting and train prediction models with feature costs [12] but again they require ground-truth labels as reward-feedback.", "startOffset": 116, "endOffset": 120}, {"referenceID": 11, "context": "A somewhat more general version of [12] is developed in [13] where in addition the learner can choose to acquire ground-truth labels for a cost.", "startOffset": 35, "endOffset": 39}, {"referenceID": 12, "context": "A somewhat more general version of [12] is developed in [13] where in addition the learner can choose to acquire ground-truth labels for a cost.", "startOffset": 56, "endOffset": 60}, {"referenceID": 6, "context": "Our paper bears some similarity with the concept of active classification, which deals with learning stopping policies[7, 8] among a given sequence of tests.", "startOffset": 118, "endOffset": 124}, {"referenceID": 7, "context": "Our paper bears some similarity with the concept of active classification, which deals with learning stopping policies[7, 8] among a given sequence of tests.", "startOffset": 118, "endOffset": 124}, {"referenceID": 1, "context": "Our paper is related to the framework of finite partial monitoring problems[2], which deals with how to infer unknown key information and where tests/actions reveal different types of information about the unknown information.", "startOffset": 75, "endOffset": 78}, {"referenceID": 13, "context": "In this context [14] consider special cases where payoff/rewards for a subset of actions are observed.", "startOffset": 16, "endOffset": 20}, {"referenceID": 14, "context": "This is further studied as a side-observation problem in [15] and as graph-structured feedback [16, 17, 18]).", "startOffset": 57, "endOffset": 61}, {"referenceID": 15, "context": "This is further studied as a side-observation problem in [15] and as graph-structured feedback [16, 17, 18]).", "startOffset": 95, "endOffset": 107}, {"referenceID": 16, "context": "This is further studied as a side-observation problem in [15] and as graph-structured feedback [16, 17, 18]).", "startOffset": 95, "endOffset": 107}, {"referenceID": 17, "context": "This is further studied as a side-observation problem in [15] and as graph-structured feedback [16, 17, 18]).", "startOffset": 95, "endOffset": 107}, {"referenceID": 14, "context": "A finite armed bandit with side-observations [15] is also a special case of SPMs, where the learner upon choosing an action a \u2208 A receives noisy reward observations, namely, Yt = (Yt,a)a\u2208N(At), Yt,a \u223c pr(\u00b7; a, \u03b8), E [Yt,a] = r(a, \u03b8), from a neighbor-set N (a) \u2282 A, which is a priori known to the learner.", "startOffset": 45, "endOffset": 49}, {"referenceID": 17, "context": "In this paper we make use of Algorithm 1 of [18] that was proposed for stochastic bandits with Gaussian side observations.", "startOffset": 44, "endOffset": 48}, {"referenceID": 17, "context": "For the convenience of the reader, we give the algorithm resulting from applying the reduction to Algorithm 1 of [18] in an explicit form.", "startOffset": 113, "endOffset": 117}, {"referenceID": 17, "context": "The following result follows from Theorem 6 of [18]:", "startOffset": 47, "endOffset": 51}], "year": 2016, "abstractText": "In many security and healthcare systems a sequence of features/sensors/tests are used for detection and diagnosis. Each test outputs a prediction of the latent state, and carries with it inherent costs. Our objective is to learn strategies for selecting tests to optimize accuracy & costs. Unfortunately it is often impossible to acquire-in-situ ground truth annotations and we are left with the problem of unsupervised sensor selection (USS). We pose USS as a version of stochastic partial monitoring problem with an unusual reward structure (even noisy annotations are unavailable). Unsurprisingly no learner can achieve sublinear regret without further assumptions. To this end we propose the notion of weak-dominance. This is a condition on the joint probability distribution of test outputs and latent state and says that whenever a test is accurate on an example, a later test in the sequence is likely to be accurate as well.We empirically verify that weak dominance holds on real datasets and prove that it is a maximal condition for achieving sublinear regret. We reduce USS to a special case of multi-armed bandit problem with side information and develop polynomial time algorithms that achieve sublinear regret.", "creator": "LaTeX with hyperref package"}}}