{"id": "1610.05446", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Oct-2016", "title": "Provably Good Early Detection of Diseases using Non-Sparse Covariance-Regularized Linear Discriminant Analysis", "abstract": "To improve the performance of Linear Discriminant Analysis (LDA) for early detection of diseases using Electronic Health Records (EHR) data, we propose \\TheName{} -- a novel framework for \\emph{\\underline{E}HR based \\underline{E}arly \\underline{D}etection of \\underline{D}iseases} on top of \\emph{Covariance-Regularized} LDA models. Specifically, \\TheName\\ employs a \\emph{non-sparse} inverse covariance matrix (or namely precision matrix) estimator derived from graphical lasso and incorporates the estimator into LDA classifiers to improve classification accuracy. Theoretical analysis on \\TheName\\ shows that it can bound the expected error rate of LDA classification, under certain assumptions. Finally, we conducted extensive experiments using a large-scale real-world EHR dataset -- CHSN. We compared our solution with other regularized LDA and downstream classifiers. The result shows \\TheName\\ outperforms all baselines and backups our theoretical analysis.", "histories": [["v1", "Tue, 18 Oct 2016 06:11:23 GMT  (634kb,D)", "https://arxiv.org/abs/1610.05446v1", null], ["v2", "Wed, 19 Oct 2016 01:34:27 GMT  (403kb,D)", "http://arxiv.org/abs/1610.05446v2", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["haoyi xiong", "yanjie fu", "wenqing hu", "guanling chen", "laura e barnes"], "accepted": false, "id": "1610.05446"}, "pdf": {"name": "1610.05446.pdf", "metadata": {"source": "CRF", "title": "Provably Good Early Detection of Diseases using Non-Sparse Covariance-Regularized Linear Discriminant Analysis", "authors": ["Haoyi Xiong", "Yanjie Fu", "Wenqing Hu", "Guanling Chen", "Laura E. Barnes"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "In this context, it should be noted that this is one of the greatest challenges in the history of the European Union."}, {"heading": "2.1 LDA for Early Detection of Disease", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2 Preliminaries and Problem Formulation", "text": "The estimation of the covariance matrix converges with the covariance matrix of the population, so \u03a3 \u0432-1 cannot exist. Although some existing work suggests using a pseudo-inverse to approximate the inverse covariance matrix, the accuracy of the LDA may be low. Further introductions to the covariance matrix estimation under HDLSS settings can be found in the survey [14]."}, {"heading": "2.2 Performance Analysis of LDA", "text": "This section summarizes the series of studies [15, 16,24, 25] to estimate the theoretical error rate for LDA classifiers. Consider two p-dimensional Gaussian distributions N (\u00b5 +, \u03a3) and N (\u00b5 \u2212, \u03a3), which have the same covariance matrix but two different mean vectors \u00b5 + and \u00b5 \u2212, assuming that an error is unknown. Using samples from these two distributions, we can estimate the covariance matrix, mean vectors \u00b5 + and \u00b5 \u2212. The expected error rate of a linear discrimination analysis (i.e. the probability of missed classification) [15, 16] is: (2.3). (\u00b5 \u2212, \u00b5 \u2212, \u03a3, \u00b5 +, both the digit and the digit.). (Digit. \u2212 digit. \u2212 digit. \u2212 digit. \u2212 digit. \u2212 digit. \u2212 digit. \u2212 digit. \u2212 digit. \u2212 digit. \u2212 digit. \u2212 digit. \u2212 digit. \u2212 digit. \u2212 digit. \u2212 digit. \u2212 digit. \u2212 digit. \u2212 digit. \u2212 digit. \u2212 digit. \u2212 digit. \u2212 digit. \u2212 digit. \u2212 digit. \u2212 digit. \u2212 digit. \u2212 digit. \u2212 digit. \u2212 digit. \u2212 digit. \u2212 digit. \u2212 digit. \u2212 digit. \u2212 digit. \u2212 digit. \u2212 digit. \u2212 digit. \u2212 digit. \u2212 digit. \u2212 digit. \u2212 digit."}, {"heading": "2.3 Problem Definition", "text": "Inspired by the preliminary estimates above, we consider the problem of improving the performance of LDA for disease early detection to be minimizing the expected error rate in 2.3. However, the applicable parameters are not known in our research settings, thus minimizing the expected error rate under all possible applicable parameters. (xm-1, lm-1) The target can thus be further reduced to find the OPTIMAL estimate of assumptions that minimize the expectation of the expected error rate under all possible applicable parameters. (xm-1) The objective error rate using the proposed diagnoses will be further reduced to minimize the OPTIMAL estimate of assumptions. (2,5) The argmin-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-pb-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-"}, {"heading": "3.2 Integrated LDA Classification", "text": "Considering the estimated mean values of the vectors \u00b5- +, \u00b5-2, the inverse covariance matrix T and a vector of the new patient x, E2D2 decides whether the patient will develop the target disease, using an FDA model derived from Equation 2.2 as: (3.7) characters (logxT T-T-\u00b5-N + -12 \u00b5-T-N-N + + log \u03c0 + xT T-N-N-N-N-N-N-N-N-N). If the above equation yields + 1, then E2D2 classifies x as the patient who will develop the target disease. We refer to the above LDA derivation as Non-Sparse Covariance-Regularized Discriminant Analysis, in view of the sparse Covariance-Regularized Discriminant Analysis of Witen and Tibishirani [12], which is based on the Graphical Lasso."}, {"heading": "4 Algorithm Analysis", "text": "We report on the theoretical analysis of E2D2, as follows: 1. We are first introducing a new upper limit of the expected error rate. \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212"}, {"heading": "5.2 Experiment Results", "text": "Given that the estimate used by CRDA is very close to the non-sparse estimate, we compared the \"1 standard error\" of these two estimates, and the results in Table 2 show that no-sparse estimator can always exceed Graphical Lasso with fewer errors. Note that in our temporal experiment, we simulated a training set with a relatively large sample size (i.e., 10,000). Such a large number of samples is not normally available for realistic predictive modeling. 6 Related paper In this section, we will first summarize previous studies related to this paper."}, {"heading": "6.2 Extensions to LDA Models", "text": "In fact, most of them are able to survive themselves if they do not put themselves in a position to survive on their own. Most of them are not able to survive on their own, and most of them are not able to survive on their own. Most of them are able to survive on their own, and most of them are able to survive on their own."}, {"heading": "A Appendix", "text": "In the following tables, we present the performance comparison between E2D2 and baselines, in which we present the results both in terms of accuracy, F1 score, sensitivity, specificity and in terms of standard derivatives. Specifically, we compare performance using various experimental settings, such as the number of days in advance for early detection (e.g. 30 days, 60 days and 90 days) and by variing parameters for model training. E2D2 clearly outperforms other algorithms in terms of overall accuracy, F1Score and sensitivity (number of days in advance for early detection (e.g. 30 days, 60 days and 90 days)."}], "references": [{"title": "Supervised patient similarity measure of heterogeneous patient", "author": ["Jimeng Sun", "Fei Wang", "Jianying Hu", "Shahram Edabollahi"], "venue": "records. ACM SIGKDD Explorations Newsletter,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2012}, {"title": "Personalized predictive modeling and risk factor identification using patient similarity", "author": ["Jianying Hu Fei Wang Kenney Ng", "Jimeng Sun"], "venue": "AMIA Summit on Clinical Research Informatics (CRI),", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2015}, {"title": "Psf: A unified patient similarity evaluation framework through metric learning with weak supervision", "author": ["Fei Wang", "Jimeng Sun"], "venue": "Biomedical and Health Informatics, IEEE Journal of,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2015}, {"title": "MSEQ: Early detection of anxiety and depression via temporal orders of diagnoses in electronic health data", "author": ["Haoyi Xiong"], "venue": "In Big Data (Workshop),", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2015}, {"title": "Mining medical data for predictive and sequential patterns: Pkdd", "author": ["Susan Jensen", "UK SPSS"], "venue": "In Proceedings of the 5th European Conference on Principles and Practice of Knowledge Discovery in Databases,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2001}, {"title": "Temporal Phenotyping from Longitudinal Electronic Health Records: A Graph Based Framework", "author": ["Chuanren Liu", "Fei Wang", "Jianying Hu", "Hui Xiong"], "venue": "In Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2015}, {"title": "Clinical risk prediction by exploring high-order feature correlations", "author": ["Fei Wang", "Ping Zhang", "Xiang Wang", "Jianying Hu"], "venue": "In AMIA Annual Symposium Proceedings,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2014}, {"title": "Bayes optimality in linear discriminant analysis", "author": ["Onur C Hamsici", "Aleix M Martinez"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2008}, {"title": "Sparse linear discriminant analysis by thresholding for high dimensional data", "author": ["Jun Shao", "Yazhen Wang", "Xinwei Deng", "Sijian Wang"], "venue": "The Annals of statistics,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2011}, {"title": "Effective linear discriminant analysis for high dimensional, low sample size data", "author": ["Zhihua Qiao", "Lan Zhou", "Jianhua Z Huang"], "venue": "In Proceeding of the World Congress on Engineering,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2008}, {"title": "Covarianceregularized regression and classification for high dimensional problems", "author": ["Daniela M Witten", "Robert Tibshirani"], "venue": "Journal of the Royal Statistical Society: Series B (Statistical Methodology),", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2009}, {"title": "Marble: highthroughput phenotyping from electronic health records via sparse nonnegative tensor factorization", "author": ["Joyce C Ho", "Joydeep Ghosh", "Jimeng Sun"], "venue": "In Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2014}, {"title": "Estimating structured high-dimensional covariance and precision matrices: Optimal rates and adaptive estimation", "author": ["T Tony Cai", "Zhao Ren", "Harrison H Zhou"], "venue": "Electronic Journal of Statistics,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2016}, {"title": "Random matrix theory in pattern classification: An application to error estimation", "author": ["Amin Zollanvari", "Edward R Dougherty"], "venue": "In 2013 Asilomar Conference on Signals, Systems and Computers,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2013}, {"title": "Analytic study of performance of error estimators for linear discriminant analysis", "author": ["Amin Zollanvari", "Ulisses M Braga-Neto", "Edward R Dougherty"], "venue": "IEEE Transactions on Signal Processing,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2011}, {"title": "The use of shrinkage estimators in linear discriminant analysis", "author": ["Roger Peck", "John Van Ness"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1982}, {"title": "Regularization studies of linear discriminant analysis in small sample size scenarios with application to face recognition", "author": ["Juwei Lu", "Konstantinos N Plataniotis", "Anastasios N Venetsanopoulos"], "venue": "Pattern Recognition Letters,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2005}, {"title": "Statistics for high-dimensional data: methods, theory and applications", "author": ["Peter B\u00fchlmann", "Sara Van De Geer"], "venue": "Springer Science & Business Media,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2011}, {"title": "Sparse inverse covariance estimation with the graphical lasso", "author": ["Jerome Friedman", "Trevor Hastie", "Robert Tibshirani"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2008}, {"title": "Confidence intervals for high-dimensional inverse covariance estimation", "author": ["Jana Jankova", "Sara van de Geer"], "venue": "Electronic Journal of Statistics,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2015}, {"title": "Icd-9 codes and surveillance for clostridium difficile\u2013associated disease", "author": ["Erik R Dubberke", "Kimberly A Reske", "L Clifford McDonald", "Victoria J Fraser"], "venue": "Emerging infectious diseases,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2006}, {"title": "Estimation of error rates in discriminant analysis", "author": ["Peter A Lachenbruch", "M Ray Mickey"], "venue": null, "citeRegEx": "24", "shortCiteRegEx": "24", "year": 1968}, {"title": "Asymptotic generalization bound of fishers linear discriminant analysis", "author": ["Wei Bian", "Dacheng Tao"], "venue": "IEEE transactions on pattern analysis and machine intelligence,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2014}, {"title": "Large-sample theory: Parametric case", "author": ["Herman Chernoff"], "venue": "The Annals of Mathematical Statistics,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 1956}, {"title": "Sparse permutation invariant covariance estimation", "author": ["Adam J Rothman", "Peter J Bickel", "Elizaveta Levina", "Ji Zhu"], "venue": "Electronic Journal of Statistics,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2008}, {"title": "College Health Surveillance Network: Epidemiology and Health Care Utilization of College Students at U.S. 4-Year Universities", "author": ["James C. Turner", "Adrienne Keller"], "venue": "Journal of American college health: J of ACH,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2015}, {"title": "Life event dimensions of loss, humiliation, entrapment, and danger in the prediction of onsets of major depression and generalized anxiety", "author": ["Kenneth S Kendler", "John M Hettema", "Frank Butera", "Charles O Gardner", "Carol A Prescott"], "venue": "Archives of general psychiatry,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2003}, {"title": "An optimization criterion for generalized discriminant analysis on undersampled problems", "author": ["Jieping Ye", "Ravi Janardan", "Cheong Hee Park", "Haesun Park"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2004}, {"title": "Toward personalizing treatment for depression: predicting diagnosis and severity", "author": ["Sandy H Huang", "Paea LePendu", "Srinivasan V Iyer", "Ming Tai- Seale", "David Carrell", "Nigam H Shah"], "venue": "Journal of the American Medical Informatics Association,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2014}, {"title": "Class-based n-gram models of natural language", "author": ["Peter F Brown", "Peter V Desouza", "Robert L Mercer", "Vincent J Della Pietra", "Jenifer C Lai"], "venue": "Computational linguistics,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 1992}, {"title": "On the distribution of the largest eigenvalue in principal components analysis", "author": ["Iain M Johnstone"], "venue": "Annals of statistics,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2001}, {"title": "A direct lda algorithm for highdimensional datawith application to face recognition", "author": ["Hua Yu", "Jie Yang"], "venue": "Pattern recognition,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2001}], "referenceMentions": [{"referenceID": 0, "context": "The availability of Electronic Health Records (EHR) [1,2] in healthcare settings provides an unique opportunity for early detection of patients\u2019 potential diseases using their historical health records.", "startOffset": 52, "endOffset": 57}, {"referenceID": 1, "context": "The availability of Electronic Health Records (EHR) [1,2] in healthcare settings provides an unique opportunity for early detection of patients\u2019 potential diseases using their historical health records.", "startOffset": 52, "endOffset": 57}, {"referenceID": 0, "context": "To predict the patients\u2019 future disease using EHR data, existing work proposed to first extract useful features, such as diagnosis-frequencies [1\u20133], pairwise diagnosis transition [4,5], and graphs of diagnosis sequences [6], to represent each patient\u2019s EHR data using the representation learning techniques.", "startOffset": 143, "endOffset": 148}, {"referenceID": 1, "context": "To predict the patients\u2019 future disease using EHR data, existing work proposed to first extract useful features, such as diagnosis-frequencies [1\u20133], pairwise diagnosis transition [4,5], and graphs of diagnosis sequences [6], to represent each patient\u2019s EHR data using the representation learning techniques.", "startOffset": 143, "endOffset": 148}, {"referenceID": 2, "context": "To predict the patients\u2019 future disease using EHR data, existing work proposed to first extract useful features, such as diagnosis-frequencies [1\u20133], pairwise diagnosis transition [4,5], and graphs of diagnosis sequences [6], to represent each patient\u2019s EHR data using the representation learning techniques.", "startOffset": 143, "endOffset": 148}, {"referenceID": 3, "context": "To predict the patients\u2019 future disease using EHR data, existing work proposed to first extract useful features, such as diagnosis-frequencies [1\u20133], pairwise diagnosis transition [4,5], and graphs of diagnosis sequences [6], to represent each patient\u2019s EHR data using the representation learning techniques.", "startOffset": 180, "endOffset": 185}, {"referenceID": 4, "context": "To predict the patients\u2019 future disease using EHR data, existing work proposed to first extract useful features, such as diagnosis-frequencies [1\u20133], pairwise diagnosis transition [4,5], and graphs of diagnosis sequences [6], to represent each patient\u2019s EHR data using the representation learning techniques.", "startOffset": 180, "endOffset": 185}, {"referenceID": 5, "context": "To predict the patients\u2019 future disease using EHR data, existing work proposed to first extract useful features, such as diagnosis-frequencies [1\u20133], pairwise diagnosis transition [4,5], and graphs of diagnosis sequences [6], to represent each patient\u2019s EHR data using the representation learning techniques.", "startOffset": 221, "endOffset": 224}, {"referenceID": 0, "context": "Then, a series of supervised learning techniques have been adopted to train predictive models, such as Support Vector Machine (SVM), Random Forest (RF), Bayesian Network, Linear Discriminant Analysis (LDA) [1\u20134, 7], using well represented EHR data with the labels of the target disease.", "startOffset": 206, "endOffset": 214}, {"referenceID": 1, "context": "Then, a series of supervised learning techniques have been adopted to train predictive models, such as Support Vector Machine (SVM), Random Forest (RF), Bayesian Network, Linear Discriminant Analysis (LDA) [1\u20134, 7], using well represented EHR data with the labels of the target disease.", "startOffset": 206, "endOffset": 214}, {"referenceID": 2, "context": "Then, a series of supervised learning techniques have been adopted to train predictive models, such as Support Vector Machine (SVM), Random Forest (RF), Bayesian Network, Linear Discriminant Analysis (LDA) [1\u20134, 7], using well represented EHR data with the labels of the target disease.", "startOffset": 206, "endOffset": 214}, {"referenceID": 3, "context": "Then, a series of supervised learning techniques have been adopted to train predictive models, such as Support Vector Machine (SVM), Random Forest (RF), Bayesian Network, Linear Discriminant Analysis (LDA) [1\u20134, 7], using well represented EHR data with the labels of the target disease.", "startOffset": 206, "endOffset": 214}, {"referenceID": 6, "context": "Then, a series of supervised learning techniques have been adopted to train predictive models, such as Support Vector Machine (SVM), Random Forest (RF), Bayesian Network, Linear Discriminant Analysis (LDA) [1\u20134, 7], using well represented EHR data with the labels of the target disease.", "startOffset": 206, "endOffset": 214}, {"referenceID": 3, "context": "Among these methods, LDA with diagnosis-frequency vectors is frequently used as one of the common performance benchmarks [4, 7], because of LDA\u2019s provable bayesian optimality [8].", "startOffset": 121, "endOffset": 127}, {"referenceID": 6, "context": "Among these methods, LDA with diagnosis-frequency vectors is frequently used as one of the common performance benchmarks [4, 7], because of LDA\u2019s provable bayesian optimality [8].", "startOffset": 121, "endOffset": 127}, {"referenceID": 7, "context": "Among these methods, LDA with diagnosis-frequency vectors is frequently used as one of the common performance benchmarks [4, 7], because of LDA\u2019s provable bayesian optimality [8].", "startOffset": 175, "endOffset": 178}, {"referenceID": 8, "context": "However, recent studies demonstrate the limitation of LDA on high dimension data [9\u201312], such as the EHR records [13].", "startOffset": 81, "endOffset": 87}, {"referenceID": 9, "context": "However, recent studies demonstrate the limitation of LDA on high dimension data [9\u201312], such as the EHR records [13].", "startOffset": 81, "endOffset": 87}, {"referenceID": 10, "context": "However, recent studies demonstrate the limitation of LDA on high dimension data [9\u201312], such as the EHR records [13].", "startOffset": 81, "endOffset": 87}, {"referenceID": 11, "context": "However, recent studies demonstrate the limitation of LDA on high dimension data [9\u201312], such as the EHR records [13].", "startOffset": 113, "endOffset": 117}, {"referenceID": 12, "context": ", covariance matrix, from a relatively small number of samples [14].", "startOffset": 63, "endOffset": 67}, {"referenceID": 13, "context": "According to the expected rate estimation for LDA classifiers [15, 16], LDA performs poorly with high misclassification rate, when the parameter estimation is inaccurate, under high dimension settings.", "startOffset": 62, "endOffset": 70}, {"referenceID": 14, "context": "According to the expected rate estimation for LDA classifiers [15, 16], LDA performs poorly with high misclassification rate, when the parameter estimation is inaccurate, under high dimension settings.", "startOffset": 62, "endOffset": 70}, {"referenceID": 12, "context": "Even when the sample size is larger than the number of dimensions, the sample (inverse) covariance estimation could be quite different with the \u201ctrue\u201d (inverse) covariance matrix, as discussed in details in a recent survey [14].", "startOffset": 223, "endOffset": 227}, {"referenceID": 8, "context": "To improve LDA learning, several regularization-based methods have been proposed to accurately estimate the (inverse) covariance matrix [10,17,18] or linear coefficients [9, 10] under high dimension and low sample size (HDLSS) settings [19].", "startOffset": 136, "endOffset": 146}, {"referenceID": 15, "context": "To improve LDA learning, several regularization-based methods have been proposed to accurately estimate the (inverse) covariance matrix [10,17,18] or linear coefficients [9, 10] under high dimension and low sample size (HDLSS) settings [19].", "startOffset": 136, "endOffset": 146}, {"referenceID": 16, "context": "To improve LDA learning, several regularization-based methods have been proposed to accurately estimate the (inverse) covariance matrix [10,17,18] or linear coefficients [9, 10] under high dimension and low sample size (HDLSS) settings [19].", "startOffset": 136, "endOffset": 146}, {"referenceID": 8, "context": "To improve LDA learning, several regularization-based methods have been proposed to accurately estimate the (inverse) covariance matrix [10,17,18] or linear coefficients [9, 10] under high dimension and low sample size (HDLSS) settings [19].", "startOffset": 170, "endOffset": 177}, {"referenceID": 17, "context": "To improve LDA learning, several regularization-based methods have been proposed to accurately estimate the (inverse) covariance matrix [10,17,18] or linear coefficients [9, 10] under high dimension and low sample size (HDLSS) settings [19].", "startOffset": 236, "endOffset": 240}, {"referenceID": 10, "context": "[12] based on their previous contribution to the sparse inverse covariance estimation using Graphical Lasso [20].", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "[12] based on their previous contribution to the sparse inverse covariance estimation using Graphical Lasso [20].", "startOffset": 108, "endOffset": 112}, {"referenceID": 8, "context": "sparsity of parameter estimation [9, 10, 12, 20], in this work we introduce a novel non-sparse (de-sparsified) inverse covariance matrix estimator [21] for further performance im-", "startOffset": 33, "endOffset": 48}, {"referenceID": 10, "context": "sparsity of parameter estimation [9, 10, 12, 20], in this work we introduce a novel non-sparse (de-sparsified) inverse covariance matrix estimator [21] for further performance im-", "startOffset": 33, "endOffset": 48}, {"referenceID": 18, "context": "sparsity of parameter estimation [9, 10, 12, 20], in this work we introduce a novel non-sparse (de-sparsified) inverse covariance matrix estimator [21] for further performance im-", "startOffset": 33, "endOffset": 48}, {"referenceID": 19, "context": "sparsity of parameter estimation [9, 10, 12, 20], in this work we introduce a novel non-sparse (de-sparsified) inverse covariance matrix estimator [21] for further performance im-", "startOffset": 147, "endOffset": 151}, {"referenceID": 8, "context": "Different from the existing sparse LDA models, which regularize the covariance matrix [10] or linear classification coefficients [9] to leverage sparse estimation of parameters, the proposed method uses a non-sparse estimator based on graphical lasso [20] to work with LDA models.", "startOffset": 86, "endOffset": 90}, {"referenceID": 18, "context": "Different from the existing sparse LDA models, which regularize the covariance matrix [10] or linear classification coefficients [9] to leverage sparse estimation of parameters, the proposed method uses a non-sparse estimator based on graphical lasso [20] to work with LDA models.", "startOffset": 251, "endOffset": 255}, {"referenceID": 0, "context": "sentation - There are many existing approaches to represent EHR data including the use of diagnosis-frequencies [1\u20133], pairwise diagnosis transition [4, 5], and graph representations of diagnosis sequences [6].", "startOffset": 112, "endOffset": 117}, {"referenceID": 1, "context": "sentation - There are many existing approaches to represent EHR data including the use of diagnosis-frequencies [1\u20133], pairwise diagnosis transition [4, 5], and graph representations of diagnosis sequences [6].", "startOffset": 112, "endOffset": 117}, {"referenceID": 2, "context": "sentation - There are many existing approaches to represent EHR data including the use of diagnosis-frequencies [1\u20133], pairwise diagnosis transition [4, 5], and graph representations of diagnosis sequences [6].", "startOffset": 112, "endOffset": 117}, {"referenceID": 3, "context": "sentation - There are many existing approaches to represent EHR data including the use of diagnosis-frequencies [1\u20133], pairwise diagnosis transition [4, 5], and graph representations of diagnosis sequences [6].", "startOffset": 149, "endOffset": 155}, {"referenceID": 4, "context": "sentation - There are many existing approaches to represent EHR data including the use of diagnosis-frequencies [1\u20133], pairwise diagnosis transition [4, 5], and graph representations of diagnosis sequences [6].", "startOffset": 149, "endOffset": 155}, {"referenceID": 5, "context": "sentation - There are many existing approaches to represent EHR data including the use of diagnosis-frequencies [1\u20133], pairwise diagnosis transition [4, 5], and graph representations of diagnosis sequences [6].", "startOffset": 206, "endOffset": 209}, {"referenceID": 20, "context": "Given each patient\u2019s EHR data, as shown in Figure 1, this method first retrieves the diagnosis codes [22] recorded during each visit.", "startOffset": 101, "endOffset": 105}, {"referenceID": 12, "context": "More introduction to the covariance matrix estimation under HDLSS settings can be found in survey [14].", "startOffset": 98, "endOffset": 102}, {"referenceID": 13, "context": "2 Performance Analysis of LDA In this section, we summarize the series of studies [15, 16, 24, 25] in theoretical error rate estimation for LDA classifiers.", "startOffset": 82, "endOffset": 98}, {"referenceID": 14, "context": "2 Performance Analysis of LDA In this section, we summarize the series of studies [15, 16, 24, 25] in theoretical error rate estimation for LDA classifiers.", "startOffset": 82, "endOffset": 98}, {"referenceID": 21, "context": "2 Performance Analysis of LDA In this section, we summarize the series of studies [15, 16, 24, 25] in theoretical error rate estimation for LDA classifiers.", "startOffset": 82, "endOffset": 98}, {"referenceID": 22, "context": "2 Performance Analysis of LDA In this section, we summarize the series of studies [15, 16, 24, 25] in theoretical error rate estimation for LDA classifiers.", "startOffset": 82, "endOffset": 98}, {"referenceID": 13, "context": ", probability of the missed classification) [15, 16] is: (2.", "startOffset": 44, "endOffset": 52}, {"referenceID": 14, "context": ", probability of the missed classification) [15, 16] is: (2.", "startOffset": 44, "endOffset": 52}, {"referenceID": 10, "context": "We call above LDA derivation as Non-Sparse Covariance-Regularized Discriminant Analysis, with respect to Witen and Tibishirani\u2019s sparse Covariance-Regularized Discriminant Analysis [12], which was based on the Graphical Lasso.", "startOffset": 181, "endOffset": 185}, {"referenceID": 19, "context": "Then we introduce a key existing theory [21] on the desparsified graphical lasso estimation T\u0302 , which proves", "startOffset": 40, "endOffset": 44}, {"referenceID": 19, "context": "Stochastic Bound of ||T\u0302 \u2212 \u03a3\u22121||F - According to [21], suppose T\u0302 is the de-sparsified graphical lasso esti-", "startOffset": 49, "endOffset": 53}, {"referenceID": 19, "context": "mation and \u03a3 refers to the true population covariance matrix, under specific structural assumption [21]:", "startOffset": 99, "endOffset": 103}, {"referenceID": 23, "context": "d = max1\u2264i\u2264p|{j : \u03a3\u22121 i,j 6= 0}|; further o(\u00b7) and Op(\u00b7) are little-o notation and big-O in probability (the notations were defined in [26]) respectively.", "startOffset": 135, "endOffset": 139}, {"referenceID": 19, "context": ", EHR diagnosis-frequency vectors) should be gaussian or subgaussian; and 2) the population inverse covariance matrix should follow the structural assumption [21] listed in Equation 4.", "startOffset": 158, "endOffset": 162}, {"referenceID": 24, "context": "Note that [27] demonstrated that the Frobenius norm rate of convergence for graphical lasso is Op( \u221a (p+ d) log p/m) under a mild condition, which can also bound the maximal expected error but not as tight as Equation 4.", "startOffset": 10, "endOffset": 14}, {"referenceID": 25, "context": "1 Experiment Setups Data Description - In this study, to evaluate ED, we used the de-identified EHR data from the College Health Surveillance Network (CHSN), which contains over 1 million patients and 6 million visits from 31 student health centers across the United States [28].", "startOffset": 274, "endOffset": 278}, {"referenceID": 26, "context": "Note that in our research, we do not predict these four types of mental disorders separately, as these four disorders are usually correlated and heavily overlapped in clinical practices [29].", "startOffset": 186, "endOffset": 190}, {"referenceID": 27, "context": "Specifically, LDA uses the sample covariance estimation, and inverts the covariance matrix using pseudoinverse [30] when the matrix inverse is not available; Shrinkage is based on LDA, using a sparse estimation of sample covariance: \u03a3\u2217 = \u03b2 \u2217 \u03a3\u0304 + (1\u2212\u03b2)\u2217diag(\u03a3\u0304), where diag(\u03a3\u0304) refers to the diagonal matrix of the sample estimation \u03a3\u0304.", "startOffset": 111, "endOffset": 115}, {"referenceID": 10, "context": "[12], which leverage the inverse covariance matrix estimated by Graphical Lasso Estimator addressed in Equation 3.", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "\u2022 Support Vector Machine (SVM) \u2013 Inspired by the previous studies [1, 2, 4], we use a linear binary SVM classifier with fine-tuned parameters.", "startOffset": 66, "endOffset": 75}, {"referenceID": 1, "context": "\u2022 Support Vector Machine (SVM) \u2013 Inspired by the previous studies [1, 2, 4], we use a linear binary SVM classifier with fine-tuned parameters.", "startOffset": 66, "endOffset": 75}, {"referenceID": 3, "context": "\u2022 Support Vector Machine (SVM) \u2013 Inspired by the previous studies [1, 2, 4], we use a linear binary SVM classifier with fine-tuned parameters.", "startOffset": 66, "endOffset": 75}, {"referenceID": 28, "context": ") \u2013 Inspired by the recent progress in depression prediction [31], we use a Logistic Regression classifier.", "startOffset": 61, "endOffset": 65}, {"referenceID": 0, "context": "Specifically, the EHR data of each patient was represented as a vector consisting of the frequency of each diagnosis code that has been discovered in previous visits [1\u20133].", "startOffset": 166, "endOffset": 171}, {"referenceID": 1, "context": "Specifically, the EHR data of each patient was represented as a vector consisting of the frequency of each diagnosis code that has been discovered in previous visits [1\u20133].", "startOffset": 166, "endOffset": 171}, {"referenceID": 2, "context": "Specifically, the EHR data of each patient was represented as a vector consisting of the frequency of each diagnosis code that has been discovered in previous visits [1\u20133].", "startOffset": 166, "endOffset": 171}, {"referenceID": 29, "context": "EHR data can also be represented using N-gram-alike [33] graphs, through counting the pairwise transitions between each pair of diagnosis codes in every visit [4, 5].", "startOffset": 52, "endOffset": 56}, {"referenceID": 3, "context": "EHR data can also be represented using N-gram-alike [33] graphs, through counting the pairwise transitions between each pair of diagnosis codes in every visit [4, 5].", "startOffset": 159, "endOffset": 165}, {"referenceID": 4, "context": "EHR data can also be represented using N-gram-alike [33] graphs, through counting the pairwise transitions between each pair of diagnosis codes in every visit [4, 5].", "startOffset": 159, "endOffset": 165}, {"referenceID": 5, "context": "resent the EHR of a patient using the temporal graphs, in order to preserve the temporal order of diagnoses partially [6].", "startOffset": 118, "endOffset": 121}, {"referenceID": 5, "context": "discussed the method of dimensionality reduction for temporal EHR graphs through edge selection [6].", "startOffset": 96, "endOffset": 99}, {"referenceID": 3, "context": "Given EHR data represented with vectors and graphs, researchers have proposed to predict the target disease through supervised learning, using downstream classifiers [4] or similarity search [1\u20133].", "startOffset": 166, "endOffset": 169}, {"referenceID": 0, "context": "Given EHR data represented with vectors and graphs, researchers have proposed to predict the target disease through supervised learning, using downstream classifiers [4] or similarity search [1\u20133].", "startOffset": 191, "endOffset": 196}, {"referenceID": 1, "context": "Given EHR data represented with vectors and graphs, researchers have proposed to predict the target disease through supervised learning, using downstream classifiers [4] or similarity search [1\u20133].", "startOffset": 191, "endOffset": 196}, {"referenceID": 2, "context": "Given EHR data represented with vectors and graphs, researchers have proposed to predict the target disease through supervised learning, using downstream classifiers [4] or similarity search [1\u20133].", "startOffset": 191, "endOffset": 196}, {"referenceID": 4, "context": "Given the EHR data with rich structures, sub-sequential pattern matching and sub-graph pattern matching are also leveraged to identify the risk of patients [5, 6].", "startOffset": 156, "endOffset": 162}, {"referenceID": 5, "context": "Given the EHR data with rich structures, sub-sequential pattern matching and sub-graph pattern matching are also leveraged to identify the risk of patients [5, 6].", "startOffset": 156, "endOffset": 162}, {"referenceID": 30, "context": "2, LDA requires the inverse covariance matrix for calculation, but the sample covariance matrix used in typical LDA is usually singular (non-invertible); and 2) the difference between sample (inverse) covariance matrix and the population (inverse) covariance matrix is extremely large, simulation studies [34] showed that the eignvectors of the two matrices can be nearly orthogonal.", "startOffset": 305, "endOffset": 309}, {"referenceID": 27, "context": "[30] proposed to use the Pseudo-inverse, while Direct LDA [35] leveraged the simultaneous diagonalization, to replace the matrix inverse operator.", "startOffset": 0, "endOffset": 4}, {"referenceID": 31, "context": "[30] proposed to use the Pseudo-inverse, while Direct LDA [35] leveraged the simultaneous diagonalization, to replace the matrix inverse operator.", "startOffset": 58, "endOffset": 62}, {"referenceID": 8, "context": "On the other hand, to obtain accurate parameter estimation for LDA under HDLSS settings, several works have proposed to sparsify the inverse covariance matrix [10\u201312] and linear coefficients [9].", "startOffset": 159, "endOffset": 166}, {"referenceID": 9, "context": "On the other hand, to obtain accurate parameter estimation for LDA under HDLSS settings, several works have proposed to sparsify the inverse covariance matrix [10\u201312] and linear coefficients [9].", "startOffset": 159, "endOffset": 166}, {"referenceID": 10, "context": "On the other hand, to obtain accurate parameter estimation for LDA under HDLSS settings, several works have proposed to sparsify the inverse covariance matrix [10\u201312] and linear coefficients [9].", "startOffset": 159, "endOffset": 166}, {"referenceID": 0, "context": "First, compared to other data mining approaches for early diseases detection [1\u20136], ED is the first work that focuses on improving the performance of LDA model for EHR data classification with diagnosisfrequency vector data representation, by addressing the expected error rates under HDLSS settings.", "startOffset": 77, "endOffset": 82}, {"referenceID": 1, "context": "First, compared to other data mining approaches for early diseases detection [1\u20136], ED is the first work that focuses on improving the performance of LDA model for EHR data classification with diagnosisfrequency vector data representation, by addressing the expected error rates under HDLSS settings.", "startOffset": 77, "endOffset": 82}, {"referenceID": 2, "context": "First, compared to other data mining approaches for early diseases detection [1\u20136], ED is the first work that focuses on improving the performance of LDA model for EHR data classification with diagnosisfrequency vector data representation, by addressing the expected error rates under HDLSS settings.", "startOffset": 77, "endOffset": 82}, {"referenceID": 3, "context": "First, compared to other data mining approaches for early diseases detection [1\u20136], ED is the first work that focuses on improving the performance of LDA model for EHR data classification with diagnosisfrequency vector data representation, by addressing the expected error rates under HDLSS settings.", "startOffset": 77, "endOffset": 82}, {"referenceID": 4, "context": "First, compared to other data mining approaches for early diseases detection [1\u20136], ED is the first work that focuses on improving the performance of LDA model for EHR data classification with diagnosisfrequency vector data representation, by addressing the expected error rates under HDLSS settings.", "startOffset": 77, "endOffset": 82}, {"referenceID": 5, "context": "First, compared to other data mining approaches for early diseases detection [1\u20136], ED is the first work that focuses on improving the performance of LDA model for EHR data classification with diagnosisfrequency vector data representation, by addressing the expected error rates under HDLSS settings.", "startOffset": 77, "endOffset": 82}, {"referenceID": 3, "context": "Second, our contribution is complementary with the work in EHR data representation [4\u20136] and we can further improve ED by incorporating advanced EHR data representation methods.", "startOffset": 83, "endOffset": 88}, {"referenceID": 4, "context": "Second, our contribution is complementary with the work in EHR data representation [4\u20136] and we can further improve ED by incorporating advanced EHR data representation methods.", "startOffset": 83, "endOffset": 88}, {"referenceID": 5, "context": "Second, our contribution is complementary with the work in EHR data representation [4\u20136] and we can further improve ED by incorporating advanced EHR data representation methods.", "startOffset": 83, "endOffset": 88}, {"referenceID": 19, "context": "Third, when compared to existing LDA extensions, ED adopts a novel inverse covariance matrix estimator [21] to lower and bound the expected error rate of the LDA model with theoretical guarantee under HDLSS settings, while [9\u201312] all focus on regularizing the the parameters of LDA using the \u201cheuristics of sparsity\u201d.", "startOffset": 103, "endOffset": 107}, {"referenceID": 8, "context": "Third, when compared to existing LDA extensions, ED adopts a novel inverse covariance matrix estimator [21] to lower and bound the expected error rate of the LDA model with theoretical guarantee under HDLSS settings, while [9\u201312] all focus on regularizing the the parameters of LDA using the \u201cheuristics of sparsity\u201d.", "startOffset": 223, "endOffset": 229}, {"referenceID": 9, "context": "Third, when compared to existing LDA extensions, ED adopts a novel inverse covariance matrix estimator [21] to lower and bound the expected error rate of the LDA model with theoretical guarantee under HDLSS settings, while [9\u201312] all focus on regularizing the the parameters of LDA using the \u201cheuristics of sparsity\u201d.", "startOffset": 223, "endOffset": 229}, {"referenceID": 10, "context": "Third, when compared to existing LDA extensions, ED adopts a novel inverse covariance matrix estimator [21] to lower and bound the expected error rate of the LDA model with theoretical guarantee under HDLSS settings, while [9\u201312] all focus on regularizing the the parameters of LDA using the \u201cheuristics of sparsity\u201d.", "startOffset": 223, "endOffset": 229}, {"referenceID": 19, "context": "To best of our knowledge, this paper is the first study that integrates [21] with LDA for Non-sparse CovarianceRegularized Discriminant Analysis and presents its theoretical properties.", "startOffset": 72, "endOffset": 76}], "year": 2016, "abstractText": "To improve the performance of Linear Discriminant Analysis (LDA) for early detection of diseases using Electronic Health Records (EHR) data, we propose ED \u2013 a novel framework for EHR based Early Detection of Diseases on top of Covariance-Regularized LDA models. Specifically, ED employs a non-sparse inverse covariance matrix (or namely precision matrix) estimator derived from graphical lasso and incorporates the estimator into LDA classifiers to improve classification accuracy. Theoretical analysis on ED shows that it can bound the expected error rate of LDA classification, under certain assumptions. Finally, we conducted extensive experiments using a large-scale realworld EHR dataset \u2013 CHSN. We compared our solution with other regularized LDA and downstream classifiers. The result shows ED outperforms all baselines and backups our theoretical analysis.", "creator": "LaTeX with hyperref package"}}}