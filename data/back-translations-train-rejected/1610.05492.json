{"id": "1610.05492", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Oct-2016", "title": "Federated Learning: Strategies for Improving Communication Efficiency", "abstract": "Federated Learning is a machine learning setting where the goal is to train a high-quality centralized model with training data distributed over a large number of clients each with unreliable and relatively slow network connections. We consider learning algorithms for this setting where on each round, each client independently computes an update to the current model based on its local data, and communicates this update to a central server, where the client-side updates are aggregated to compute a new global model. The typical clients in this setting are mobile phones, and communication efficiency is of utmost importance. In this paper, we propose two ways to reduce the uplink communication costs. The proposed methods are evaluated on the application of training a deep neural network to perform image classification. Our best approach reduces the upload communication required to train a reasonable model by two orders of magnitude.", "histories": [["v1", "Tue, 18 Oct 2016 09:11:51 GMT  (82kb,D)", "http://arxiv.org/abs/1610.05492v1", null], ["v2", "Mon, 30 Oct 2017 20:52:14 GMT  (3035kb,D)", "http://arxiv.org/abs/1610.05492v2", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["jakub kone\\v{c}n\\'y", "h brendan mcmahan", "felix x yu", "peter richt\\'arik", "ananda theertha suresh", "dave bacon"], "accepted": false, "id": "1610.05492"}, "pdf": {"name": "1610.05492.pdf", "metadata": {"source": "CRF", "title": "Federated Learning: Strategies for Improving Communication Efficiency", "authors": ["Jakub Kone\u010dn\u00fd", "H. Brendan McMahan", "Felix X. Yu", "Peter Richt\u00e1rik", "Ananda Theertha Suresh", "Dave Bacon"], "emails": ["J.Konecny@sms.ed.ac.uk,", "felixyu}@google.com", "peter.richtarik@ed.ac.uk,", "dabacon}@google.com"], "sections": [{"heading": "1 Introduction", "text": "As datasets become larger and more complex, machine learning increasingly requires distributing the optimization of model parameters across multiple machines. Existing machine learning algorithms are designed for highly controlled environments (such as data centers) where data is distributed between machines in a balanced and i.e. way, and high-throughput networks are available. A motivating example of federated optimization is when training data is held locally on mobile devices, and the devices are used as computer nodes for performing their local data in order to update a global model. The above framework differs from conventionally distributed machine learning [18, 12, 20, 5] due to the large number of clients, highly unbalanced and not comparable."}, {"heading": "2 Structured Update", "text": "The first type of communication efficient update limits the Hit updates to a pre-defined structure. Two types of structures are considered in the work: low rank. We force that Hit Rd1-d2 matrices are low rank, where k is a fixed number. We express Hit as the product of two matrices: H i t = A i tB i t, where A i t-Rd1 \u00d7 k, Bit-Rk \u00d7 d2 and Ait are randomly and firmly generated and only bit optimized. Note that Ait can then be compressed in the form of a random start field and customers only need to send bit to the server. We have also tried to fix bit and train A i t, as well as train both A i t and B i t; both are not performed as well. Our approach seems to work as well as the best techniques being considered [6], random mask. We limit the Hit to sparse matrices that follow a pre-defined saving pattern."}, {"heading": "3 Sketched Update", "text": "The second method of communication-efficient updating, which we refer to as sketches, is first of all the complete, unrestricted updating of our strategy, and then the updating in a (loss-prone) compressed form, before we are sent to the server. The server decodes the updates before it performs the aggregation. Such sketching methods have application in many areas. We suggest two ways of carrying out the sketches: the sketches, the sketches, the sketches, the sketches, the sketches, the sketches, the sketches, the sketches, the sketches and the sketches, the sketches, the sketches, the sketches, the sketches, the sketches, the sketches, the sketches, the sketches, the sketches, the sketches, the sketches and the sketches, the sketches, the sketches, the sketches, the sketches, the sketches, the sketches, the sketches, the sketches and the sketches, the sketches, the sketches, the sketches, the sketches, the sketches, the sketches, the sketches, the sketches, the sketches, the sketches, the sketches, the sketches, the sketches, the sketches, the sketches, the sketches, the sketches, the sketches, the sketches, the sketches, the sketches, the sketches, the sketches, the sketches, the sketches, the sketches, the sketches, the sketches, the sketches, the sketches, the sketches, the sketches, the sketches, the sketches, the sketches, the sketches, the sketches, the sketches, the sketches, the sketches, the sketches and the sketches, the sketches, the sketches, the sketches and the sketches, the sketches, the sketches, the sketches, the sketches, the sketches, the sk"}, {"heading": "4 Experiments", "text": "There are 50,000 training examples, which we have divided into 100 clients, each with 500 training examples."}], "references": [{"title": "Revisiting distributed synchronous SGD", "author": ["Jianmin Chen", "Rajat Monga", "Samy Bengio", "Rafal Jozefowicz"], "venue": "In ICLR Workshop Track,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2016}, {"title": "Project adam: Building an efficient and scalable deep learning training system", "author": ["Trishul Chilimbi", "Yutaka Suzue", "Johnson Apacible", "Karthik Kalyanaraman"], "venue": "In 11th USENIX Symposium on Operating Systems Design and Implementation", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2014}, {"title": "Large scale distributed deep networks", "author": ["Jeffrey Dean", "Greg Corrado", "Rajat Monga", "Kai Chen", "Matthieu Devin", "Mark Mao", "Andrew Senior", "Paul Tucker", "Ke Yang", "Quoc V Le"], "venue": "In NIPS,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2012}, {"title": "Predicting parameters in deep learning", "author": ["Misha Denil", "Babak Shakibi", "Laurent Dinh", "Nando de Freitas"], "venue": "In NIPS,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2013}, {"title": "On randomized distributed coordinate descent with quantized updates", "author": ["Mostafa El Gamal", "Lifeng Lai"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2016}, {"title": "Large-scale learning with less ram via randomization", "author": ["Daniel Golovin", "D. Sculley", "H. Brendan McMahan", "Michael Young"], "venue": "In ICML,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2013}, {"title": "Federated optimization: Distributed optimization beyond the datacenter", "author": ["Jakub Kone\u010dn\u00fd", "H. Brendan McMahan", "Daniel Ramage"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2015}, {"title": "Federated optimization: Distributed machine learning for on-device intelligence", "author": ["Jakub Kone\u010dn\u00fd", "H. Brendan McMahan", "Daniel Ramage", "Peter Richt\u00e1rik"], "venue": "arXiv preprint arXiv:1610.02527,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2016}, {"title": "Learning multiple layers of features from tiny images", "author": ["Alex Krizhevsky"], "venue": "Technical report,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2009}, {"title": "Distributed optimization with arbitrary local solvers", "author": ["Chenxin Ma", "Jakub Kone\u010dn\u00fd", "Martin Jaggi", "Virginia Smith", "Michael I. Jordan", "Peter Richt\u00e1rik", "Martin Tak\u00e1\u010d"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2014}, {"title": "Adding vs. averaging in distributed primal-dual optimization", "author": ["Chenxin Ma", "Virginia Smith", "Martin Jaggi", "Michael Jordan", "Peter Richt\u00e1rik", "Martin Tak\u00e1\u010d"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1973}, {"title": "Federated learning of deep networks using model averaging", "author": ["H. Brendan McMahan", "Eider Moore", "Daniel Ramage", "Blaise Aguera y Arcas"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2016}, {"title": "Coordinate descent with arbitrary sampling I: algorithms and complexity", "author": ["Zheng Qu", "Peter Richt\u00e1rik"], "venue": "Optimization Methods and Software,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2016}, {"title": "Quartz: Randomized dual coordinate ascent with arbitrary sampling", "author": ["Zheng Qu", "Peter Richt\u00e1rik", "Tong Zhang"], "venue": "In NIPS,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2015}, {"title": "Quantized incremental algorithms for distributed optimization", "author": ["M.G. Rabbat", "R.D. Nowak"], "venue": "IEEE Journal on Selected Areas in Communications,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2005}, {"title": "AIDE: Fast and communication efficient distributed optimization", "author": ["Sashank J Reddi", "Jakub Kone\u010dn\u00fd", "Peter Richt\u00e1rik", "Barnab\u00e1s P\u00f3cz\u00f3s", "Alex Smola"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2016}, {"title": "Stochastic reformulation of linear systems and fast stochastic iterative methods", "author": ["Peter Richt\u00e1rik", "Martin Tak\u00e1\u010d"], "venue": "Technical report,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2016}, {"title": "Communication-efficient distributed optimization using an approximate Newton-type method", "author": ["Ohad Shamir", "Nathan Srebro", "Tong Zhang"], "venue": "In ICML,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2014}, {"title": "Sketching as a tool for numerical linear algebra", "author": ["David P. Woodruff"], "venue": "Foundations and Trends R  \u00a9 in Theoretical Computer Science,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2014}, {"title": "Orthogonal random features", "author": ["Felix X Yu", "Ananda Theertha Suresh", "Krzysztof Choromanski", "Daniel Holtmann-Rice", "Sanjiv Kumar"], "venue": "In NIPS,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2016}, {"title": "DiSCO: Distributed optimization for self-concordant empirical loss", "author": ["Yuchen Zhang", "Xiao Lin"], "venue": "In ICML,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2015}], "referenceMentions": [{"referenceID": 7, "context": "Federated learning [10, 14, 9] proposes an alternative setting, where we train a shared global model under the coordination of a central server, from a federation of participating devices.", "startOffset": 19, "endOffset": 30}, {"referenceID": 11, "context": "Federated learning [10, 14, 9] proposes an alternative setting, where we train a shared global model under the coordination of a central server, from a federation of participating devices.", "startOffset": 19, "endOffset": 30}, {"referenceID": 6, "context": "Federated learning [10, 14, 9] proposes an alternative setting, where we train a shared global model under the coordination of a central server, from a federation of participating devices.", "startOffset": 19, "endOffset": 30}, {"referenceID": 15, "context": "The above framework differs from conventional distributed machine learning [18, 12, 20, 23, 5, 4] due to the the large number of clients, highly unbalanced and non-i.", "startOffset": 75, "endOffset": 97}, {"referenceID": 9, "context": "The above framework differs from conventional distributed machine learning [18, 12, 20, 23, 5, 4] due to the the large number of clients, highly unbalanced and non-i.", "startOffset": 75, "endOffset": 97}, {"referenceID": 17, "context": "The above framework differs from conventional distributed machine learning [18, 12, 20, 23, 5, 4] due to the the large number of clients, highly unbalanced and non-i.", "startOffset": 75, "endOffset": 97}, {"referenceID": 20, "context": "The above framework differs from conventional distributed machine learning [18, 12, 20, 23, 5, 4] due to the the large number of clients, highly unbalanced and non-i.", "startOffset": 75, "endOffset": 97}, {"referenceID": 2, "context": "The above framework differs from conventional distributed machine learning [18, 12, 20, 23, 5, 4] due to the the large number of clients, highly unbalanced and non-i.", "startOffset": 75, "endOffset": 97}, {"referenceID": 1, "context": "The above framework differs from conventional distributed machine learning [18, 12, 20, 23, 5, 4] due to the the large number of clients, highly unbalanced and non-i.", "startOffset": 75, "endOffset": 97}, {"referenceID": 11, "context": "For simplicity, we consider synchronized algorithms for federated learning [14, 3], where a typical round consists of the following steps: 1.", "startOffset": 75, "endOffset": 82}, {"referenceID": 0, "context": "For simplicity, we consider synchronized algorithms for federated learning [14, 3], where a typical round consists of the following steps: 1.", "startOffset": 75, "endOffset": 82}, {"referenceID": 10, "context": "Recent works show that a careful choice of the server-side learning rate can lead to faster convergence [13, 12, 10].", "startOffset": 104, "endOffset": 116}, {"referenceID": 9, "context": "Recent works show that a careful choice of the server-side learning rate can lead to faster convergence [13, 12, 10].", "startOffset": 104, "endOffset": 116}, {"referenceID": 7, "context": "Recent works show that a careful choice of the server-side learning rate can lead to faster convergence [13, 12, 10].", "startOffset": 104, "endOffset": 116}, {"referenceID": 3, "context": "Our approach seems to perform as well as the best techniques considered in [6], Random mask.", "startOffset": 75, "endOffset": 78}, {"referenceID": 13, "context": "This strategy can be seen as the combination of the master training method and a randomized block coordinate minimization approach [16, 15].", "startOffset": 131, "endOffset": 139}, {"referenceID": 12, "context": "This strategy can be seen as the combination of the master training method and a randomized block coordinate minimization approach [16, 15].", "startOffset": 131, "endOffset": 139}, {"referenceID": 18, "context": "Such sketching methods have application in many domains [21].", "startOffset": 56, "endOffset": 60}, {"referenceID": 16, "context": "It was recently shown that, in a certain setting, the expected iterates of SGD converge to the optimal point [19].", "startOffset": 109, "endOffset": 113}, {"referenceID": 14, "context": "Incremental, randomized and distributed optimization algorithms can be similarly analyzed in a quantized updates setting [17, 8, 7].", "startOffset": 121, "endOffset": 131}, {"referenceID": 5, "context": "Incremental, randomized and distributed optimization algorithms can be similarly analyzed in a quantized updates setting [17, 8, 7].", "startOffset": 121, "endOffset": 131}, {"referenceID": 4, "context": "Incremental, randomized and distributed optimization algorithms can be similarly analyzed in a quantized updates setting [17, 8, 7].", "startOffset": 121, "endOffset": 131}, {"referenceID": 19, "context": "In this work, we use a type of structured rotation matrix which is the product of a Walsh-Hadamard matrix and a binary diagonal matrix, motivated by the recent advance in this topic [22].", "startOffset": 182, "endOffset": 186}, {"referenceID": 8, "context": "We conducted the experiments using federated learning to train deep neural networks for the CIFAR10 image classification task [11].", "startOffset": 126, "endOffset": 130}, {"referenceID": 11, "context": "We employ the Federated Averaging algorithm [14], which significantly decreases the number of rounds of communication required to train a good model.", "startOffset": 44, "endOffset": 48}], "year": 2016, "abstractText": "Federated Learning is a machine learning setting where the goal is to train a highquality centralized model with training data distributed over a large number of clients each with unreliable and relatively slow network connections. We consider learning algorithms for this setting where on each round, each client independently computes an update to the current model based on its local data, and communicates this update to a central server, where the client-side updates are aggregated to compute a new global model. The typical clients in this setting are mobile phones, and communication efficiency is of utmost importance. In this paper, we propose two ways to reduce the uplink communication costs. The proposed methods are evaluated on the application of training a deep neural network to perform image classification. Our best approach reduces the upload communication required to train a reasonable model by two orders of magnitude.", "creator": "LaTeX with hyperref package"}}}