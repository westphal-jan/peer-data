{"id": "1610.05522", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Oct-2016", "title": "Addressing Community Question Answering in English and Arabic", "abstract": "This paper studies the impact of different types of features applied to learning to re-rank questions in community Question Answering. We tested our models on two datasets released in SemEval-2016 Task 3 on \"Community Question Answering\". Task 3 targeted real-life Web fora both in English and Arabic. Our models include bag-of-words features (BoW), syntactic tree kernels (TKs), rank features, embeddings, and machine translation evaluation features. To the best of our knowledge, structural kernels have barely been applied to the question reranking task, where they have to model paraphrase relations. In the case of the English question re-ranking task, we compare our learning to rank (L2R) algorithms against a strong baseline given by the Google-generated ranking (GR). The results show that i) the shallow structures used in our TKs are robust enough to noisy data and ii) improving GR is possible, but effective BoW features and TKs along with an accurate model of GR features in the used L2R algorithm are required. In the case of the Arabic question re-ranking task, for the first time we applied tree kernels on syntactic trees of Arabic sentences. Our approaches to both tasks obtained the second best results on SemEval-2016 subtasks B on English and D on Arabic.", "histories": [["v1", "Tue, 18 Oct 2016 10:22:46 GMT  (314kb,D)", "http://arxiv.org/abs/1610.05522v1", "presented at Second WebQA workshop, SIGIR2016 (this http URL)"]], "COMMENTS": "presented at Second WebQA workshop, SIGIR2016 (this http URL)", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["giovanni da san martino", "alberto barr\\'on-cede\\~no", "salvatore romeo", "alessandro moschitti", "shafiq joty", "fahad a al obaidli", "kateryna tymoshenko", "antonio uva"], "accepted": false, "id": "1610.05522"}, "pdf": {"name": "1610.05522.pdf", "metadata": {"source": "CRF", "title": "Addressing Community Question Answering in English and Arabic", "authors": ["Giovanni Da San Martino", "Alberto Barr\u00f3n-Cede\u00f1o", "Salvatore Romeo", "Alessandro Moschitti", "Shafiq Joty", "Fahad A. Al Obaidli", "Kateryna Tymoshenko", "Antonio Uva", "Daniele Bonadiman"], "emails": ["amoschitti@qf.org.qa", "sromeo@qf.org.qa", "d.bonadiman}@unitn.it"], "sections": [{"heading": "1. INTRODUCTION", "text": "In recent years, there has been a renewed interest in retrieving information necessary to answer questions within the community. (...) This combines traditional questions with a modern web scenario in which users ask questions that expect answers from other users. (...) There are a number of questions we ask in this paper: http: / / iyas.qcri.org / qldemoPermission to make digital or hard copies of portions of this work. (...) There are a number of questions we ask in this paper: http: / / iyas.qcri.org / qldemoPermission to identify yourself, or to do all of this work. (...) There are a number of questions that are granted without fees, the copies are distributed for profit or commercial benefits, and that copies of these notes carry these notes and the full quotes on the first page. (...) Copyrights for third components of this work must be honored (...)."}, {"heading": "2. RELATED WORK", "text": "The first step for any system that aims to automatically answer questions on cQA pages is to retrieve a set of questions similar to the user's input, and the set of similar questions will later be used to extract possible answers to the initial question. However, determining question similarity remains one of the biggest challenges in cQA due to issues such as the \"lexical gap.\" Various approaches have been proposed to overcome this problem. Early methods used statistical machine translation techniques to calculate semantic similarities between two questions."}, {"heading": "3. PROBLEM DESCRIPTION", "text": "Among the four tasks of the Semester 2016 competition, we focus our attention on the two that deal with the question-question similarity. In Paper B, we only look at the text. In Paper D, we also use the information from the answer that is related to the question of the forum. In fact, the candidate is looking for a solution to the various aspects of daily life in Qatar.3 Paper D uses Arabic instances that are obtained from three medical forums: webteb, altibbi and consult islamweb.4 As these tasks are being re-evaluated, we evaluate our models with medium precision (MAP)."}, {"heading": "3.2 Task D: Arabic Question\u2013Comment Pairs Re-Ranking", "text": "The task is to rearrange the question-comment pairs into three classes: (i) direct: if it is a direct answer to the new question; (ii) relevant: if it is not a direct answer to the question but provides information on the topic; and (iii) irrelevant: if it is an answer to another question that has nothing to do with the topic. For evaluation purposes, both direct and relevant forum questions are considered good. Table 4 reports on the class distribution of the data. There are about 60% of forum questions that are classified as good, so the data set is not very unbalanced. As in the case of English, the Arabic collection questions are user-generated. As the latter was extracted from medical domains, they present specific challenges: a mix of medical terminology (used by physicists) and colloquial language (used by patients)."}, {"heading": "4. OUR L2R MODELS", "text": "The function r can be linear: r (qo, qs) = ~ w \u00b7 \u03c6 (qo, qs), where ~ w is a linear model and \u03c6 () provides a character vector representation of the pair qo, qs. We use binary SVMs to learn r from examples and ee-model \u03c6 (qo, qs) with different character sets, which we describe in sections 4.1 and 4.2."}, {"heading": "4.1 Tree Kernel Models", "text": "In contrast to [25], our questions can contain several sub-questions, one subject, greetings, and explanations, so that they consist of several sentences. We merge the entire question text into a macro tree using a false root node that connects the parse trees of all sentences. In both tasks, we present pairs of questions; therefore, we merge the components of two macro trees that correspond (qo, qs). Figure 1 shows an example in which the relationships between two Arabic questions are used to form a graph. Translation of the two questions is: qo \"What are the symptoms of irritable bowel syndrome (IBS)?\" qs \"What are the symptoms of an irritable bowel syndrome (qs, qs)?\" We link the two macro trees by connecting phrases, e.g. NP, VP, PP, if there is at least qical correspondence between the qs and the phrase qo that are linked to the irritable bowel syndrome (qo)."}, {"heading": "4.2 Feature Vectors", "text": "Our L2R approach relies on various subsets of features to derive the relationship between two texts: text similarity, PTK similarity, Google rank, embedding features and machine translation ratings. In task B, we use text similarity, PTK similarity and Google rank. In task D, we use text embedding features and machine translation ratings. We calculate a total of 20 similarities, sim (qo, qj), using verbs (n = [1,.., 4], after stopword removal, using greedy string tiling features [29], longest common sub-sequences [1], Jaccard coefficient [9], word containment [15] and cosmic similarity. PTK characteristics. Another similarity is achieved by comparing syntactical trees with PTK, i.e. TK (qis)."}, {"heading": "5. EXPERIMENTS", "text": "We follow the evaluation framework of SemEval 2016 Task 3 in order to compare our system with that of the competition [19]. We used various tools for pre-processing the text in English: OpenNLPs Tokenizer, POS taggers and Chunk-Annotator5 and Stanford's Lemmatizer [16], all accessible via DKPro Core [8] 6. For Arabic texts, first stop words were removed, retaining only the content and Latin words. To segment the texts, we used the MADAMIRA toolkit [23]. To split the texts into sentences, we used we5https: / / opennlp.apache.org / 6https: / / dkpro.github.io / dkpro-core / the Stanford splitter.7 For parsing Arabic texts into syntactic trees, we used the Berkeley parser [24]."}, {"heading": "5.1 Experiments on Task B", "text": "In a series of preliminary results, we first compared a reranker - SVM [12] - with a standard binary SVM [11]. Since the results were comparable, we used the KeLP toolkite8, which enabled us to combine our three sub-ranges within the experiments, namely with the parameters of the TKs and the RBF cores, either with linearity or with the RBF cores."}, {"heading": "5.2 Experiments on Task D", "text": "We have performed three test groups for Paper D, which match our entries to the competition and which are reported in Table 7. In the first experiment, cont1, we applied a linear kernel to the embedding features. A second experiment, cont2, also includes machine translation rating features (both features are described in Section 4.2). A third experiment, primarily, used a linear kernel to the embedding features in combination with the syntactic tree kernel (STK) [18], which is applied to the constituency trees of the question texts as described in Section 4.1. The submission contests 1, which uses embedding features from [4], are an average system. Adding the machine translation rating features (MTE), the MAP increases from 38.33 to 39.98, allowing us to jump from eleventh to seventh place in the contest. However, if we add tree kernel points to the four-and-and-and-tree features (MTE), the four-and-and-and-and-and-four-tree points are more than the absolute points in the proximity."}, {"heading": "6. CONCLUSIONS", "text": "We have shown that the combination of similarity characteristics, syntactic structures based on tree cores, and traits based on search engine rankings can increase the performance of a questioner on a real cQA dataset. Specifically, our results suggest that Google uses general models that can be on par with specific models trained on specific domains. However, if we also use advanced syntactic / semantic representations to model the structural relationships between questions, we can achieve better results. Furthermore, we have modeled and tested relative tree cores for cQA that are robust to noise, and thus can increase Google's ranking. In the future, we would like to better structure the representation of questions by first merging a section of the question text, such as surveys, elaborations, which could be used to improve our system for improving the flat representation of trees in the macro phase."}, {"heading": "Acknowledgments", "text": "This research is being developed by the Arabic Language Technologies (ALT) group at the Qatar Computing Research Institute (QCRI), HBKU, Qatar Foundation in collaboration with MIT and is part of the Interactive sYstems for Answer Search (IYAS) project."}, {"heading": "7. REFERENCES", "text": "[1] L. Allison and T. Dix. A bit-stringlongest-common-subsequence algorithm. Inf. Process. Lett., 23 (6): 305-310, December 1986. [2] C. F. Baker, C. J. Fillmore, and J. B. Lowe. The berkeley framenet project. In Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics and 17th International Conference on Computational Linguistics - Volume 1, ACL '98, pp. 86-90, Stroudsburg, PA, USA, 1998. Association for Computational Linguistics. [3] A. Barr\u00f3n-Cede\u00f1o, S. Filice. Da San Martino, S. Joty, L. M\u00e0rquez, P. Nakov, and A. Moschitti. Thread-level information for Comment classification in community question."}], "references": [{"title": "A bit-string longest-common-subsequence algorithm", "author": ["L. Allison", "T. Dix"], "venue": "Inf. Process. Lett., 23(6):305\u2013310, Dec.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1986}, {"title": "The berkeley framenet project", "author": ["C.F. Baker", "C.J. Fillmore", "J.B. Lowe"], "venue": "Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics - Volume 1, ACL \u201998, pages 86\u201390, Stroudsburg, PA, USA,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1998}, {"title": "Thread-level information for comment classification in community question answering", "author": ["A. Barr\u00f3n-Cede\u00f1o", "S. Filice", "G. Da San Martino", "S. Joty", "L. M\u00e0rquez", "P. Nakov", "A. Moschitti"], "venue": "In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 2: Short Papers),", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2015}, {"title": "VectorSLU: A continuous word vector approach to answer selection in community question answering systems", "author": ["Y. Belinkov", "M. Mohtarami", "S. Cyphers", "J. Glass"], "venue": "Proceedings of the 9th International Workshop on Semantic Evaluation, SemEval \u201915, Denver, Colorado, USA,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2015}, {"title": "The use of categorization information in language models for question retrieval", "author": ["X. Cao", "G. Cong", "B. Cui", "C.S. Jensen", "C. Zhang"], "venue": "CIKM, pages 265\u2013274,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2009}, {"title": "Automatic evaluation of machine translation quality using n-gram co-occurrence statistics", "author": ["G. Doddington"], "venue": "Proceedings of the Second International Conference on Human Language Technology Research, HLT \u201902, pages 138\u2013145,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2002}, {"title": "Searching questions by identifying question topic and question focus", "author": ["H. Duan", "Y. Cao", "C.-Y. Lin", "Y. Yu"], "venue": "ACL, pages 156\u2013164,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2008}, {"title": "A broad-coverage collection of portable nlp components for building shareable analysis pipelines", "author": ["R. Eckart de Castilho", "I. Gurevych"], "venue": "In Proceedings of the Workshop on Open Infrastructures and Analysis Frameworks for HLT,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2014}, {"title": "\u00c9tude comparative de la distribution florale dans une portion des Alpes et des Jura", "author": ["P. Jaccard"], "venue": "Bulletin del la Soci\u00e9t\u00e9 Vaudoise des Sciences Naturelles, pages 547\u2013579,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1901}, {"title": "Question-answer topic model for question retrieval in community question answering", "author": ["Z. Ji", "F. Xu", "B. Wang", "B. He"], "venue": "CIKM, pages 2471\u20132474,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2012}, {"title": "Making Large-scale Support Vector Machine Learning Practical", "author": ["T. Joachims"], "venue": "Advances in Kernel Methods. MIT Press, Cambridge, MA, USA,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1999}, {"title": "Optimizing search engines using clickthrough data", "author": ["T. Joachims"], "venue": "KDD, pages 133\u2013142,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2002}, {"title": "Global thread-level inference for comment classification in community question answering", "author": ["S. Joty", "A. Barr\u00f3n-Cede\u00f1o", "G. Da San Martino", "S. Filice", "L. M\u00e0rquez", "A. Moschitti", "P. Nakov"], "venue": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2015}, {"title": "The METEOR metric for automatic evaluation of machine translation", "author": ["A. Lavie", "M. Denkowski"], "venue": "Machine Translation, 23(2\u20133):105\u2013115,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2009}, {"title": "Detecting short passages of similar text in large document collections", "author": ["C. Lyon", "J. Malcolm", "B. Dickerson"], "venue": "EMNLP, pages 118\u2013125,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2001}, {"title": "The Stanford CoreNLP natural language processing toolkit", "author": ["C.D. Manning", "M. Surdeanu", "J. Bauer", "J. Finkel", "S.J. Bethard", "D. McClosky"], "venue": "Association for Computational Linguistics (ACL) System Demonstrations, pages 55\u201360,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2014}, {"title": "Linguistic Regularities in Continuous Space Word Representations", "author": ["T. Mikolov", "W.-t. Yih", "G. Zweig"], "venue": "In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human  Language Technologies, NAACL-HLT", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2013}, {"title": "Efficient Convolution Kernels for Dependency and Constituent Syntactic Trees", "author": ["A. Moschitti"], "venue": "ECML, pages 318\u2013329.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2006}, {"title": "SemEval-2016 task 3: Community question answering", "author": ["P. Nakov", "L. M\u00e0rquez", "A. Moschitti", "W. Magdy", "H. Mubarak", "A.A. Freihat", "J. Glass", "B. Randeree"], "venue": "Proceedings of SemEval \u201916. ACL,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2016}, {"title": "QCRI: Answer selection for community question answering - experiments for Arabic and English", "author": ["M. Nicosia", "S. Filice", "A. Barr\u00f3n-Cede\u00f1o", "I. Saleh", "H. Mubarak", "W. Gao", "P. Nakov", "G. Da San Martino", "A. Moschitti", "K. Darwish", "L. M\u00e0rquez", "S. Joty", "W. Magdy"], "venue": "In Proceedings of the 9th International Workshop on Semantic Evaluation,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2015}, {"title": "BLEU: a method for automatic evaluation of machine translation", "author": ["K. Papineni", "S. Roukos", "T. Ward", "W.-J. Zhu"], "venue": "Proceedings of 40th Annual Meting of the Association for Computational Linguistics, ACL \u201902, pages 311\u2013318, Philadelphia, Pennsylvania, USA,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2002}, {"title": "Arabic Gigaword Fifth Edition", "author": ["R. Parker", "D. Graff", "K. Chen", "J. Kong", "K. Maeda"], "venue": "Linguistic Data Consortium (LDC), Philadelphia,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2011}, {"title": "Madamira: A fast, comprehensive tool for morphological analysis and disambiguation of arabic", "author": ["A. Pasha", "M. Al-Badrashiny", "M. Diab", "A.E. Kholy", "R. Eskander", "N. Habash", "M. Pooleery", "O. Rambow", "R. Roth"], "venue": "Proceedings of the Ninth International Conference on Language Resources and Evaluation (LREC\u201914), Reykjavik, Iceland, may", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2014}, {"title": "Improved inference for unlexicalized parsing", "author": ["S. Petrov", "D. Klein"], "venue": "Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics; Proceedings of the Main Conference, pages 404\u2013411, Rochester, New York, April", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2007}, {"title": "Structural relationships for large-scale learning of answer re-ranking", "author": ["A. Severyn", "A. Moschitti"], "venue": "SIGIR, pages 741\u2013750,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2012}, {"title": "A study of translation edit rate with targeted human annotation", "author": ["M. Snover", "B. Dorr", "R. Schwartz", "L. Micciulla", "J. Makhoul"], "venue": "Proceedings of the 7th Biennial Conference of the Association for Machine Translation in the Americas, AMTA \u201906, Cambridge, Massachusetts, USA,", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2006}, {"title": "Assessing the impact of syntactic and semantic structures for answer passages reranking", "author": ["K. Tymoshenko", "A. Moschitti"], "venue": "Proceedings of CIKM \u201915, pages 1451\u20131460, New York, NY, USA,", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2015}, {"title": "A syntactic tree matching approach to finding similar questions in community-based qa services", "author": ["K. Wang", "Z. Ming", "T.-S. Chua"], "venue": "SIGIR, pages 187\u2013194,", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2009}, {"title": "Yap3: Improved detection of similarities in computer program and other texts", "author": ["M. Wise"], "venue": "SIGCSE, pages 130\u2013134,", "citeRegEx": "29", "shortCiteRegEx": null, "year": 1996}, {"title": "Question retrieval with high quality answers in community question answering", "author": ["K. Zhang", "W. Wu", "H. Wu", "Z. Li", "M. Zhou"], "venue": "CIKM, pages 371\u2013380,", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2014}, {"title": "Phrase-based translation model for question retrieval in community question answer archives", "author": ["G. Zhou", "L. Cai", "J. Zhao", "K. Liu"], "venue": "ACL, pages 653\u2013662,", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2011}], "referenceMentions": [{"referenceID": 2, "context": "problem in [3, 13, 20].", "startOffset": 11, "endOffset": 22}, {"referenceID": 12, "context": "problem in [3, 13, 20].", "startOffset": 11, "endOffset": 22}, {"referenceID": 19, "context": "problem in [3, 13, 20].", "startOffset": 11, "endOffset": 22}, {"referenceID": 18, "context": "Recently, a new resource has been released for the SemEval 2016 Task 3 on Answer Selection in cQA [19].", "startOffset": 98, "endOffset": 102}, {"referenceID": 26, "context": ", n-grams, skip-grams; (ii) syntactic/structural features injected by TKs, which have shown to achieve the state of the art in the related task of answer sentence retrieval [27]; and (iii) features for modeling the initial rank provided by a state-of-the-art search engine, which represents a strong baseline.", "startOffset": 173, "endOffset": 177}, {"referenceID": 30, "context": "For instance, [31] applied a phrase-based translation model.", "startOffset": 14, "endOffset": 18}, {"referenceID": 4, "context": "Algorithms that try to go beyond simple text representation are presented in [5] and [7].", "startOffset": 77, "endOffset": 80}, {"referenceID": 6, "context": "Algorithms that try to go beyond simple text representation are presented in [5] and [7].", "startOffset": 85, "endOffset": 88}, {"referenceID": 4, "context": "In [5] a similarity between two questions on Yahoo! Answers is computed by using a language model with a smoothing method based on the category structure of Yahoo! An-", "startOffset": 3, "endOffset": 6}, {"referenceID": 6, "context": "In [7], the authors search for semantically similar questions by identifying the topic and focus of the user\u2019s question.", "startOffset": 3, "endOffset": 6}, {"referenceID": 9, "context": "A different approach using topic modeling for question retrieval was introduced in [10] and [30].", "startOffset": 83, "endOffset": 87}, {"referenceID": 29, "context": "A different approach using topic modeling for question retrieval was introduced in [10] and [30].", "startOffset": 92, "endOffset": 96}, {"referenceID": 27, "context": "The most similar work to ours is [28], where Table 1: A re-ranking example for the English Question\u2013 Question Similarity dataset.", "startOffset": 33, "endOffset": 37}, {"referenceID": 18, "context": "Table 2 gives class-distribution statistics of the English dataset [19].", "startOffset": 67, "endOffset": 71}, {"referenceID": 24, "context": "We essentially used the model of [25], originally proposed to rank passages.", "startOffset": 33, "endOffset": 37}, {"referenceID": 24, "context": "Different from [25], our questions may contain multiple subquestions, a subject, greetings, and elaborations, thus they are composed of several sentences.", "startOffset": 15, "endOffset": 19}, {"referenceID": 17, "context": "Finally, we apply either a partial tree kernel (PTK) or the syntactic tree kernels (STK) [18] and obtain the following kernel:", "startOffset": 89, "endOffset": 93}, {"referenceID": 28, "context": ", 4]), after stopword removal, using greedy string tiling [29], longest common subsequences [1], Jaccard coefficient [9], word containment [15], and cosine similarity.", "startOffset": 58, "endOffset": 62}, {"referenceID": 0, "context": ", 4]), after stopword removal, using greedy string tiling [29], longest common subsequences [1], Jaccard coefficient [9], word containment [15], and cosine similarity.", "startOffset": 92, "endOffset": 95}, {"referenceID": 8, "context": ", 4]), after stopword removal, using greedy string tiling [29], longest common subsequences [1], Jaccard coefficient [9], word containment [15], and cosine similarity.", "startOffset": 117, "endOffset": 120}, {"referenceID": 14, "context": ", 4]), after stopword removal, using greedy string tiling [29], longest common subsequences [1], Jaccard coefficient [9], word containment [15], and cosine similarity.", "startOffset": 139, "endOffset": 143}, {"referenceID": 3, "context": "Embeddings We utilize the embedding vectors as obtained by [4]: employing word2vec [17] on the Arabic Gigaword corpus [22].", "startOffset": 59, "endOffset": 62}, {"referenceID": 16, "context": "Embeddings We utilize the embedding vectors as obtained by [4]: employing word2vec [17] on the Arabic Gigaword corpus [22].", "startOffset": 83, "endOffset": 87}, {"referenceID": 21, "context": "Embeddings We utilize the embedding vectors as obtained by [4]: employing word2vec [17] on the Arabic Gigaword corpus [22].", "startOffset": 118, "endOffset": 122}, {"referenceID": 20, "context": "MTE features We used machine translation evaluation features: BLEU [21], TER [26], Meteor [14], NIST [6], Precision and Recall, and length ratio between the question and the comment.", "startOffset": 67, "endOffset": 71}, {"referenceID": 25, "context": "MTE features We used machine translation evaluation features: BLEU [21], TER [26], Meteor [14], NIST [6], Precision and Recall, and length ratio between the question and the comment.", "startOffset": 77, "endOffset": 81}, {"referenceID": 13, "context": "MTE features We used machine translation evaluation features: BLEU [21], TER [26], Meteor [14], NIST [6], Precision and Recall, and length ratio between the question and the comment.", "startOffset": 90, "endOffset": 94}, {"referenceID": 5, "context": "MTE features We used machine translation evaluation features: BLEU [21], TER [26], Meteor [14], NIST [6], Precision and Recall, and length ratio between the question and the comment.", "startOffset": 101, "endOffset": 104}, {"referenceID": 18, "context": "We follow the evaluation framework of SemEval 2016 Task 3 in order to be able to compare our system with the ones of the competition [19].", "startOffset": 133, "endOffset": 137}, {"referenceID": 15, "context": "We used different tools for preprocessing the text in English: OpenNLP\u2019s tokenizer, POS-tagger and chunk annotator, and Stanford\u2019s lemmatizer [16], all accessible through DKPro Core [8].", "startOffset": 142, "endOffset": 146}, {"referenceID": 7, "context": "We used different tools for preprocessing the text in English: OpenNLP\u2019s tokenizer, POS-tagger and chunk annotator, and Stanford\u2019s lemmatizer [16], all accessible through DKPro Core [8].", "startOffset": 182, "endOffset": 185}, {"referenceID": 22, "context": "We used the MADAMIRA toolkit [23] for segmenting the texts.", "startOffset": 29, "endOffset": 33}, {"referenceID": 23, "context": "For parsing Arabic texts into syntactic trees, we used the Berkeley parser [24].", "startOffset": 75, "endOffset": 79}, {"referenceID": 11, "context": "In a set of preliminary experiments, we first compared a reranker \u2014SVMrank [12]\u2014 with a standard binary SVM [11].", "startOffset": 75, "endOffset": 79}, {"referenceID": 10, "context": "In a set of preliminary experiments, we first compared a reranker \u2014SVMrank [12]\u2014 with a standard binary SVM [11].", "startOffset": 108, "endOffset": 112}, {"referenceID": 18, "context": "UH-PRHLT-primary, the best system at competition time [19], obtained a MAP of 76.", "startOffset": 54, "endOffset": 58}, {"referenceID": 1, "context": "47; still that system uses knowledge bases, such as BabelNet and FrameNet [2], which we do not require.", "startOffset": 74, "endOffset": 77}, {"referenceID": 17, "context": "Finally, a third experiment, primary, used a linear kernel on the embedding features in combination with the syntactic tree kernel (STK) [18], applied as described in Section 4.", "startOffset": 137, "endOffset": 141}, {"referenceID": 3, "context": "The submission cont1, using embedding features from [4], is an average system.", "startOffset": 52, "endOffset": 55}, {"referenceID": 18, "context": "Baselines as provided reported in [19].", "startOffset": 34, "endOffset": 38}], "year": 2016, "abstractText": "This paper studies the impact of different types of features applied to learning to re-rank questions in community Question Answering. We tested our models on two datasets released in SemEval2016 Task 3 on \u201cCommunity Question Answering\u201d. Task 3 targeted real-life Web fora both in English and Arabic. Our models include bag-of-words features (BoW), syntactic tree kernels (TKs), rank features, embeddings, and machine translation evaluation features. To the best of our knowledge, structural kernels have barely been applied to the question reranking task, where they have to model paraphrase relations. In the case of the English question re-ranking task, we compare our learning to rank (L2R) algorithms against a strong baseline given by the Google-generated ranking (GR). The results show that (i) the shallow structures used in our TKs are robust enough to noisy data and (ii) improving GR is possible, but effective BoW features and TKs along with an accurate model of GR features in the used L2R algorithm are required. In the case of the Arabic question re-ranking task, for the first time we applied tree kernels on syntactic trees of Arabic sentences. Our approaches to both tasks obtained the second best results on SemEval-2016 subtasks B on English and D on Arabic.", "creator": "LaTeX with hyperref package"}}}