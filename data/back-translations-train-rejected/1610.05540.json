{"id": "1610.05540", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Oct-2016", "title": "SYSTRAN's Pure Neural Machine Translation Systems", "abstract": "Since the first online demonstration of Neural Machine Translation (NMT) by LISA, NMT development has recently moved from laboratory to production systems as demonstrated by several entities announcing roll-out of NMT engines to replace their existing technologies. NMT systems have a large number of training configurations and the training process of such systems is usually very long, often a few weeks, so role of experimentation is critical and important to share. In this work, we present our approach to production-ready systems simultaneously with release of online demonstrators covering a large variety of languages (12 languages, for 32 language pairs). We explore different practical choices: an efficient and evolutive open-source framework; data preparation; network architecture; additional implemented features; tuning for production; etc. We discuss about evaluation methodology, present our first findings and we finally outline further work.", "histories": [["v1", "Tue, 18 Oct 2016 11:32:42 GMT  (302kb)", "http://arxiv.org/abs/1610.05540v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["josep crego", "jungi kim", "guillaume klein", "anabel rebollo", "kathy yang", "jean senellart", "egor akhanov", "patrice brunelle", "aurelien coquard", "yongchao deng", "satoshi enoue", "chiyo geiss", "joshua johanson", "ardas khalsa", "raoum khiari", "byeongil ko", "catherine kobus", "jean lorieux", "leidiana martins", "dang-chuan nguyen", "alexandra priori", "thomas riccardi", "natalia segal", "christophe servan", "cyril tiquet", "bo wang", "jin yang", "dakun zhang", "jing zhou", "peter zoldan"], "accepted": false, "id": "1610.05540"}, "pdf": {"name": "1610.05540.pdf", "metadata": {"source": "CRF", "title": "SYSTRAN\u2019s Pure Neural Machine Translation Systems", "authors": ["Josep Crego", "Jungi Kim", "Guillaume Klein", "Anabel Rebollo", "Kathy Yang", "Jean Senellart", "Egor Akhanov", "Patrice Brunelle", "Aur\u00e9lien Coquard", "Yongchao Deng", "Satoshi Enoue", "Chiyo Geiss", "Joshua Johanson", "Bo Wang", "Jin Yang", "Dakun Zhang", "Jing Zhou", "Peter Zoldan"], "emails": ["firstname.lastname@systrangroup.com"], "sections": [{"heading": null, "text": "ar Xiv: 161 0.05 540v 1 [cs.C L] 18 October 201 6Since the first online demonstration of Neural Machine Translation (NMT) by LISA (Bahdanau et al., 2014), the development of NMT systems has shifted from laboratory to production systems, as several companies have shown, announcing the introduction of NMT engines to replace their existing technologies. NMT systems have a large number of training configurations, and the training process of such systems is usually very long, often a few weeks, so the role of experimentation is crucial and important to share. In this work, we present our approach to production-ready systems simultaneously with the publication of online demonstrators covering a wide variety of languages (12 languages, for 32 language pairs). We examine various practical options: an efficient and evolutionary open source framework; data preparation; network architecture; additional implemented functions; tuning for production processes; and finally, discuss our first evaluation results, etc."}, {"heading": "1 Introduction", "text": "In fact, it is that we are able to assert ourselves, that we are able to assert ourselves, that we are able to assert ourselves in the world, and that we are able to assert ourselves, that we are able to stay in the world, \"he said."}, {"heading": "2 System Description", "text": "We base our NMT system on the encoder decoder framework provided by the open source project seq2seq-attn. Rooted on a number of established open source projects such as Andrej Karpathys char-rnn, 3 Wojciech Zaremba's Standard Short-Term Memory (LSTM) 4 and the rnn library from Element Research, 5 the framework provides a solid NMT base consisting of LSTM, such as the recurring module and faithful rhyming of the global general attention model and the input feeding at each stage of the RNN decoder, as described by Luong et al. (2015) It also comes with a variety of features such as the ability to train bi-directional encoders and pre-formed word embeddings, the ability to handle unknown words during decoding, either by copying the source book with the greatest attention or by searching for a word."}, {"heading": "3 Training Resources", "text": "In fact, it is that we are able to assert ourselves, that we are able, that we are able to put ourselves at the top, and that we are able, that we will be able to assert ourselves, that we are able, that we are able to unite, \"he said."}, {"heading": "4 Technology", "text": "In this section, we will look at several experiments that have improved various aspects of our translation engines, ranging from pre-processing techniques to expand the network with the ability to handle named units, use multiple word characteristics, and design the attention module to resemble more word alignments, to report on different levels of translation customization."}, {"heading": "4.1 Tokenization", "text": "We use standard token delimiters (spaces, tabs, etc.) and a set of language-dependent linguistic rules. Several types of entities are recognized (url and number) and replace their content with the appropriate placeholder. However, a postal process is used to detoxify translation hypotheses that regenerate the original raw text format corresponding to the language in question. For each language, we have access to language-specific tokenization and normalization rules. However, our preliminary experiments showed that there was no obvious benefit in using these language-specific tokenization patterns, and that some of the hardcoded rules actually degrade performance, which would require more investigation, but for the release of our first batch systems, we used a generic tokenization model for most languages except Arabic, Chinese, and German."}, {"heading": "4.2 Word Features", "text": "Sennrich and Haddow (2016) showed that the use of additional input functions improves translation quality. Similarly to this work, we introduced support for any number of discrete word characteristics as additional inputs to the encoder. Since we do not limit the number of values that these characteristics can take at the same time, we represent them with continuous and normalized vectors. For example, the representation of a feature f in the time step t: x (t) i = {1 nfif f takes the item value 0 otherwise (1), where nf is the number of possible values that the attribute f can take, and with x (t) Rnf. These representations are then associated with the word embed to form the new input to the encoder. We expanded this work by supporting additional features on the target page that are predicted by the encoder."}, {"heading": "4.3 Named Entities (NE)", "text": "Similarly, we used the same internal NE module to detect numbers and named entities in the source set and temporarily replace them with their respective placeholders (Table 3). Both the source and destination pages of the training data set must be processed for NE placeholders. To ensure proper entity recognition, we validate the detected entities across parallel records, i.e. a valid entity detection in a set should have the same type of entity in its parallel pair, and the word or phrase covered by the entities must be aligned with each other. We used fast alignment (Dyer et al., 2013) to automatically align the source words to target words. Our data sets usually contain about a fifth of the training substances that contain one or more NE placeholders."}, {"heading": "4.4 Guided Alignments", "text": "In fact, we are going to be able to solve the problem without there being a solution, \"he said."}, {"heading": "4.5 Politeness Mode", "text": "Many languages have ways of expressing courtesy and deference to people addressed in sentences. In Indo-European languages, there are two pronouns that correspond to the English You; it is referred to as the T-V distinction between the informal Latin pronoun tu (T) and the polite Latin pronoun Vos (V. Asian languages, such as Japanese and Korean, use widely honorable words (respectful words), words that are normally appended to the ends of names or pronouns to indicate the relative age and social position of the speakers. The expression of courtesy can also affect the vocabulary of verbs, adjectives and nouns, as well as sentence structures. Following the work of Sennrich et al. (2016a), we implemented a courtesy feature in our NMT engine: A special token of courtesy is added during training, where the token is expressed in the target mode of courtesy mode, especially if such courtesy mode is not specifically observed in Korean."}, {"heading": "4.6 Customization", "text": "Domain adaptation is a key feature for our clients - it generally includes terminology, domain and style adaptations, but can also be seen as an extension of the translation memory for human post-translation workflows. SYSTRAN engines integrate multiple domain adaptation techniques, train complete Newin domain engines, and automatically create an existing translation model using translation memories, extracting and reusing terminology. With Neural Machine Translation, a new concept of \"specialization\" comes close to the concept of incremental translation developed for statistical machine translations such as Ortiz-Mart\u00ed'nez et al., 2010."}, {"heading": "4.6.1 Generic Specialization", "text": "It is known that a system optimized for a specific text genre achieves higher accuracy than a \"generic\" system, and the customization process can be performed before, during or after the training process. Our preliminary experiments follow the latter approach. We incrementally \"generate\" a neural MT to a specific area by adding additional training periods via newly available indomain data.Adaptation proceeds incrementally when new in-domain data becomes available, while post-editing, which is similar to the Computer Aided Translation Framework described in (Cettolo et al., 2014), is experimenting with a generic translation task. The generic model is a subsample of the company provided for the WMT15 translation task."}, {"heading": "4.6.2 Post-editing Engine", "text": "The recent success of Pure Neural Machine Translation has led to the application of this technology to various related tasks, in particular to automated post-processing (APE), the objective of which is to simulate the behavior of a human post-translator, to correct translation errors made by an MT system. Until recently, most APE approaches were based on phrase-based SMT systems, either monolingual (MT target for human post-translation) (Simard et al., 2007) or source-aware (Be \u0301 chara et al., 2011). For many years, SYSTRAN has offered hybrid statistical post-translation (SPE) to improve translation through its rules-based MT system (RBMT et al., 2007). Following the success of Neural PostEditing (NPE) in the APE task of WMT '16 (Junczys-Dowmunt and Grundkiewicz, 2016), we have implemented a series of automated post-translation systems, in particular in MPE."}, {"heading": "5 Performance", "text": "As outlined above, one of the major drawbacks of NMT engines is the need to use state-of-the-art hardware technology to meet the enormous computing requirements in training and runtime. In terms of training, there are two major problems: the full training time and the computing power required, i.e. server investment. For this version, most of our training was run on a single GTX GeForce 1080 GPU (about $2.5 billion), while in (Wu et al., 2016) the authors mention the use of 96 K80 GPU for a full week to train a single language pair (about $250 million). On our hardware, full training was limited to 2x5M sets (see Section 3). A reasonable goal is to maintain training time for each language pair under one week and maintain reasonable investment so that the entire research community can have competitive training, but also indirectly so that all of our customers can benefit from the training technology."}, {"heading": "5.1 Pruning", "text": "This approach has proved efficient for the NMT tasks in See et al. (2016). Inspired by this work, we introduced similar pruning techniques in seq2seq-attn. We reproduced that model parameters can be pruned up to 60% without loss of power after retraining, as shown in Table 8. With a large pruning factor, the weights of neural networks can also be represented with sparse matrices. This implementation can lead to lower computing times, but above all to a smaller memory requirement that allows us to target more surroundings. Figures 5 and 6 show experiments with sparse matrices using eigen9. For example, when using float precision, multiplication with a sparse matrix already begins to take up less memory when 35% of its parameters are pruned.In connection with this work, in Section 5.4 we present our alternative self-based decoders that enable us to support maturity."}, {"heading": "5.2 Distillation", "text": "Despite these surprisingly accurate, NMT systems require deep networks to perform well. Typically, a 4-layer LSTM with 1000 hidden units per shift (4 x 1000) is used to achieve state-of-the-art results. Such models require state-of-the-art 9http: / / eigen.tuxfamily.orghardware for training in reasonable time, while conclusions are difficult even on standard setups or on small devices such as mobile phones. Although compressing deep models into smaller networks has been an active area of research, after working in (Kim and Rush, 2016) we experimented with sequence level knowledge distillation in the context of an Anglo-French NMT task. Knowledge distillation relies on educating a smaller student network to perform better by providing the relevant parts of a larger teacher network."}, {"heading": "5.3 Batch Translation", "text": "To increase the translation speed of large texts, we support batch translation, which works in addition to beam search, which means that for a beam of size K and a stack of size B, we forward K-B sequences to the model. Subsequently, the decoder output is split among each stack and the beam path for each sentence is continuously updated. As sentences within a stack can vary greatly in size, special care is required to mask the output of the encoder and attention softmax appropriately across the source sequences. Figure 7 shows the acceleration achieved by batch decoding in a typical setup."}, {"heading": "5.4 C++ Decoder", "text": "Although Torch is a powerful and easy-to-use framework, we decided to develop an alternative C + + implementation for decoding on the CPU. It increases our control over the decoding process and paves the way for further improvements in memory and speed while simplifying decoding. Our implementation is graph-based and uses Own for efficient matrix computation. It can be loaded and derived from Torch models. For this version, experiments show that the decoding speed is equal to or faster than the Torch-based implementation, especially in a multithreaded context. Figure 8 shows better use of the parallelization of the Own-based implementation."}, {"heading": "6 Evaluation", "text": "Machine translation evaluation has always been a challenge and the subject of many essays and dedicated workshops (Bojar et al., 2016). While automated metrics are now used as a standard in the search world and correlate well with human evaluation, the industry tends to use ad hoc human evaluation or productivity analysis metrics (Blain et al., 2011). As a translation solution company, even if automated metrics are used throughout the training process (and we give evaluations in Section 6.1), we take care of the human evaluation of the results. Wu et al. (2016) mention human evaluation, but at the same time cast doubt on the human being referenced to translate or evaluate. In this context, the claim of \"almost indistinguishable from human translation\" is both strong and very vague. On our side, during all our experiments and the preparation of specialized models, we have observed that innovations, quality, and contexts we could claim in English translation."}, {"heading": "6.1 Automatic Scoring and system comparison", "text": "Figure 9 shows automatic accuracy results, BLEU and perplexities for all language pairs. Noteworthy is the high correlation between perplexity and BLEU values, which shows that language pairs with less helplessness provide higher BLEU values. Also, note that different amounts of training data were used for each system (see Table 2). BLEU values were calculated using an internal test set. Since the beginning of this report, we have used \"internal\" validation and testing sets, making it difficult to compare the performance of our systems with other research machines. However, we must remember that our goal is to take into account improvements in our production systems."}, {"heading": "6.2 Human Ranking Evaluation", "text": "For each language pair, 100 sentences are collected \"in domain\" (*), 2. These sentences are sent to human translation (* *), and translated using candidate model and translated using available online translation services (* * *). 3. Without notification of the mix of human and machine translation, a team of 3 professional translators or linguists, who are fluent in both source and target languages, is asked to rate 3 random results for each sentence as translation based on their preferences. Preference includes accuracy as a priority, but also the fluidity of the generated translation. You have the choice to assign them 3 different rankings, or you can also choose to assign 2 or 3 of them the same rank if you cannot decide. (*) For generic domain, sentences from recent news articles, we have been selected for technical (IT) sentences defined as part of the translation memory in section 4.6. (*) We have used human categories for translation agencies."}, {"heading": "6.3 Qualitative Evaluation", "text": "In Table 11, we report on the result of the error analysis for NMT, SMT and RBMT for the Anglo-French language pair. This review confirms the translation ranking carried out in the previous review, but also contains some interesting facts: \u2022 The most noticeable error stems from missing words or parts of sentences. However, it is interesting to see that half of these \"omissions\" were considered okay by the reviewers and most of the time were not regarded as errors - in fact, it shows the system's ability not only to translate, but to summarize and get to the point we would expect from human translation. Of course, we need to fix the cases where the \"omissions\" are out of order. \u2022 Another finding is that the machine manages quotes poorly, and we will make sure that we explicitly teach this in our next release. Other low-hanging fruits are the case generation that sometimes seems to go off track, and the handling of named entities that we have already introduced, but that we do not include selectivity for the positive side of the release, where the MT does not represent a positive influence."}, {"heading": "7 Practical Issues", "text": "During the course of the training and during our internal evaluation, we identified several practical problems that are worth sharing: 1. Translating very long sentences 2. Translating user input such as a short word or the title of a news article. alignmentNMT is strongly influenced by the train data on which NMT learns how to generate precise and fluent translations holistically. Since the maximum length of a training instance during the training of our models was limited to a certain length, NMT models are strongly influenced by sentences that exceed that length, with NMT learning how to generate precise and fluent translations holistically. Hard splitting of longer sentences has some side effects, as the model considers both parts to be full sentences. As a consequence, whatever the limit we set for sentence length, we must teach the neural network how to handle longer sentences. & eistic splitting of longer sentences has some side effects, as the model considers both parts to be full sentences."}, {"heading": "8 Further Work", "text": "In this section, we will outline further experiments that are currently being carried out. First, we extend the NMT decoding to include the possibility of using multiple models, both external models, in particular an n-gram language model, as well as decoding with multiple networks (complete ensemble). In addition, we are working on the use of external word embedding and the modeling of unknown words within the network."}, {"heading": "8.1 Extending Decoding", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "8.1.1 Additional LM", "text": "As proposed by (Gu \u00bc lc, Ehre et al., 2015), we are conducting experiments to integrate an n-gram language model estimated over a large dataset on our neural MT system. We followed a flat fusion integration similar to language models used in a standard phrase-based MT decoder. In the context of decoding the beam search in NMT, t candidate words x are hypothesized at each step and assigned to a score corresponding to the neural network, pNMT (x). Sorted by their respective scores, the K-best candidates are assigned based on the score assigned to them by the language model, pLM (x). The resulting probability of each candidate is determined by the weighted sum of each log probability protocol p (x) = Log pLM (x) = Log pNMT (x) + \u03b2 Log pNMT (x)."}, {"heading": "8.1.2 Ensemble Decoding", "text": "The encoding of the ensemble has been confirmed as a practical technique to further improve performance compared to a single encoder decoder model (Sennrich et al., 2016b; Wu et al., 2016; Zhou et al., 2016).The improvement stems from the variety of predictions from different neural network models, which are learned by random initialization seeds and shuffling examples during training or by different optimization methods with respect to the development set (Cho et al., 2015).As a result, 3-8 isolated models are trained and put together, taking into account the costs of memory and training speed. In addition, (Junczys-Dowmunt et al., 2016) provides some methods to accelerate training by selecting different checkpoints as final models."}, {"heading": "8.2 Extending word embeddings", "text": "Although NMT technology has recently made a major breakthrough in the field of machine translation, it remains limited due to the limited vocabulary and use of bilingual training data. To reduce the negative impact of both phenomena, experiments are currently being conducted on the use of external word embedding weights. These external word embeddings are learned by the NMT network not only from bilingual data, but from an external model (e.g. word2vec (Mikolov et al., 2013)), so they can be estimated by larger monolingual corporations, incorporating data from different areas. Another advantage is that, since external word embedding weights are not modified during NMT training, a different vocabulary can be used for this fixed part of the input during application or retraining of the model (provided that the weights for the words in the new vocabulary come from the same embedding space as the original)."}, {"heading": "8.3 Unknown word handling", "text": "When an unknown word is generated in the target output set, a generic attention mechanism encoder decoder uses heuristics based on attention weights so that the word with the most attention is either directly copied as it is, or looked up in a dictionary. In recent literature (Gu et al., 2016; Gulcehre et al., 2016), researchers have attempted to directly model the unknown word handling within the attention and decoder networks. If the model learns to master both decoding and handling of unknown words, this will be the most optimized way to solve the problem of the only unknown word, and we will implement and evaluate the existing approaches within our framework."}, {"heading": "9 Conclusion", "text": "There is no doubt that Neural MT is definitely a technology that will continue to have a major impact on science and industry, but in its current state it is not without limitations; for language pairs that have abundant monolingual and bilingual train data, Phrase-based MT still performs better than Neural MT because the vocabulary size and insufficient use of monolingual data is still limited. Neural MT is not a universal technology, so a general configuration of the model works universally across all language pairs. For example, subword tokenization, such as BPE, offers an easy way out of the limited vocabulary problem, but we have discovered that it is not always the best choice for all language pairs."}, {"heading": "Acknowledgments", "text": "We would like to thank Yoon Kim, Prof. Alexander Rush and the other members of the Harvard NLP group for their support with the open source code, their proactive advice and valuable insights into the extensions, CrossLang and Homin Kwon for their thorough and meaningful definition of the evaluation protocol, as well as their evaluation team, and Inah Hong, Weimin Jiang and SunHyung Lee for their work."}, {"heading": "A Remarkable Results", "text": "It is time for us to find a solution that we can find."}, {"heading": "B Online System Parameters", "text": "All systems were trained with 4 LSTM layers, the Word embedding vectors were 500 in size, the dropout was set to 0.3 and we used bi-directional RNN (BRNN). Column Guided Alignment indicates whether the network was trained with guided alignments and in which epoch the feature was stopped."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate. CoRR, abs/1409.0473", "author": ["Kyunghyun Cho", "Yoshua Bengio"], "venue": "Demoed at NIPS 2014: http://lisa.iro.umontreal.ca/mt-demo/", "citeRegEx": "Bahdanau et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Statistical post-editing for a statistical mt system", "author": ["Yanjun Ma", "Josef van Genabith"], "venue": "In MT Summit,", "citeRegEx": "B\u00e9chara et al\\.,? \\Q2011\\E", "shortCiteRegEx": "B\u00e9chara et al\\.", "year": 2011}, {"title": "Qualitative analysis of post-editing for high quality machine translation. MT Summit XIII: the Thirteenth Machine Translation Summit [organized", "author": ["Blain et al.2011] Fr\u00e9d\u00e9ric Blain", "Jean Senellart", "Holger Schwenk", "Mirko Plitt", "Johann Roturier"], "venue": null, "citeRegEx": "Blain et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Blain et al\\.", "year": 2011}, {"title": "Results of the wmt16 metrics shared task", "author": ["Bojar et al.2016] Ond\u0159ej Bojar", "Yvette Graham", "Amir Kamran", "Milo\u0161 Stanojevi\u0107"], "venue": "In Proceedings of the First Conference on Machine Translation,", "citeRegEx": "Bojar et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Bojar et al\\.", "year": 2016}, {"title": "Large language models in machine translation", "author": ["Ashok C Popat", "Peng Xu", "Franz J Och", "Jeffrey Dean"], "venue": "Proceedings of the Joint Conference on Empirical Methods in Natural Language Processing and Com-", "citeRegEx": "Brants et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Brants et al\\.", "year": 2007}, {"title": "Translation project adaptation for mt-enhanced computer assisted translation", "author": ["Nicola Bertoldi", "Marcello Federico", "Holger Schwenk", "Loic Barrault", "Chrstophe Servan"], "venue": "Machine Translation,", "citeRegEx": "Cettolo et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cettolo et al\\.", "year": 2014}, {"title": "Multiun v2: Un documents with multilingual alignments", "author": ["Chen", "Eisele2012] Yu Chen", "Andreas Eisele"], "venue": "In LREC,", "citeRegEx": "Chen et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2012}, {"title": "Guided alignment training for topic-aware neural machine translation. CoRR, abs/1607.01628v1", "author": ["Chen et al.2016] Wenhu Chen", "Evgeny Matusov", "Shahram Khadivi", "Jan-Thorsten Peter"], "venue": null, "citeRegEx": "Chen et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2016}, {"title": "On using very large target vocabulary for neural machine translation", "author": ["Roland Memisevic", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1412.2007", "citeRegEx": "Cho et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2015}, {"title": "A character-level decoder without explicit segmentation for neural machine translation. CoRR, abs/1603.06147", "author": ["Chung et al.2016] Junyoung Chung", "Kyunghyun Cho", "Yoshua Bengio"], "venue": null, "citeRegEx": "Chung et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Chung et al\\.", "year": 2016}, {"title": "Statistical Post-Editing on SYSTRAN\u2019s Rule-Based Translation System", "author": ["Dugast et al.2007] Lo\u0131\u0308c Dugast", "Jean Senellart", "Philipp Koehn"], "venue": "In Proceedings of the Second Workshop on Statistical Machine Translation,", "citeRegEx": "Dugast et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Dugast et al\\.", "year": 2007}, {"title": "A simple, fast, and effective reparameterization of ibm model 2", "author": ["Dyer et al.2013] Chris Dyer", "Victor Chahuneau", "Noah A. Smith"], "venue": "In Proceedings of the 2013 Conference of the North American Chapter of the Association", "citeRegEx": "Dyer et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Dyer et al\\.", "year": 2013}, {"title": "Incorporating copying mechanism in sequence-to-sequence learning", "author": ["Gu et al.2016] Jiatao Gu", "Zhengdong Lu", "Hang Li", "Victor O.K. Li"], "venue": "In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics", "citeRegEx": "Gu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Gu et al\\.", "year": 2016}, {"title": "On using monolingual corpora in neural machine", "author": ["Orhan Firat", "Kelvin Xu", "Kyunghyun Cho", "Lo\u0131\u0308c Barrault", "HueiChi Lin", "Fethi Bougares", "Holger Schwenk andYoshua Bengio"], "venue": null, "citeRegEx": "G\u00fcl\u00e7ehre et al\\.,? \\Q2015\\E", "shortCiteRegEx": "G\u00fcl\u00e7ehre et al\\.", "year": 2015}, {"title": "Pointing the unknown words", "author": ["Sungjin Ahn", "Ramesh Nallapati", "Bowen Zhou", "Yoshua Bengio"], "venue": "In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics", "citeRegEx": "Gulcehre et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Gulcehre et al\\.", "year": 2016}, {"title": "Log-linear combinations of monolingual and bilingual neural machine translation models for automatic post-editing", "author": ["Junczys-Dowmunt", "Roman Grundkiewicz"], "venue": null, "citeRegEx": "Junczys.Dowmunt et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Junczys.Dowmunt et al\\.", "year": 2016}, {"title": "Is Neural Machine Translation Ready for Deployment? A Case Study on 30 Translation Directions", "author": ["T. Dwojak", "H. Hoang"], "venue": null, "citeRegEx": "Junczys.Dowmunt et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Junczys.Dowmunt et al\\.", "year": 2016}, {"title": "Sequence-level knowledge distillation", "author": ["Kim", "Rush2016] Yoon Kim", "Alexander M. Rush"], "venue": "CoRR, abs/1606.07947", "citeRegEx": "Kim et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Kim et al\\.", "year": 2016}, {"title": "Moses: Open Source Toolkit for Statistical Machine Translation", "author": ["Herbst."], "venue": "Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedings of the Demo and Poster Sessions.", "citeRegEx": "Herbst.,? 2007", "shortCiteRegEx": "Herbst.", "year": 2007}, {"title": "Europarl: A parallel corpus for statistical machine translation", "author": ["Philipp Koehn"], "venue": "In MT summit,", "citeRegEx": "Koehn.,? \\Q2005\\E", "shortCiteRegEx": "Koehn.", "year": 2005}, {"title": "Achieving open vocabulary neural machine translation with hybrid word-character models. CoRR, abs/1604.00788", "author": ["Luong", "Manning2016] Minh-Thang Luong", "Christopher D. Manning"], "venue": null, "citeRegEx": "Luong et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Luong et al\\.", "year": 2016}, {"title": "Effective approaches to attention-based neural machine translation", "author": ["Luong et al.2015] Thang Luong", "Hieu Pham", "Christopher D. Manning"], "venue": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Process-", "citeRegEx": "Luong et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Efficient estimation of word representations in vector space. CoRR, abs/1301.3781", "author": ["Kai Chen", "Greg Corrado", "Jeffrey Dean"], "venue": null, "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Online learning for interactive statistical machine translation", "author": ["Ismael Garc\u0131\u0301a-Varea", "Francisco Casacuberta"], "venue": "In Human Language Technologies: The 2010 Annual Conference of the North", "citeRegEx": "Ortiz.Mart\u0131\u0301nez et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Ortiz.Mart\u0131\u0301nez et al\\.", "year": 2010}, {"title": "Compression of neural machine translation models via pruning", "author": ["See et al.2016] Abigail See", "Minh-Thang Luong", "Christopher D Manning"], "venue": "In the proceedings of The 20th SIGNLL Conference on Computational Natural Language Learning", "citeRegEx": "See et al\\.,? \\Q2016\\E", "shortCiteRegEx": "See et al\\.", "year": 2016}, {"title": "Linguistic input features improve neural machine translation. CoRR, abs/1606.02892", "author": ["Sennrich", "Haddow2016] Rico Sennrich", "Barry Haddow"], "venue": null, "citeRegEx": "Sennrich et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Sennrich et al\\.", "year": 2016}, {"title": "Controlling politeness in neural machine translation via side constraints", "author": ["Barry Haddow", "Alexandra Birch"], "venue": null, "citeRegEx": "Sennrich et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Sennrich et al\\.", "year": 2016}, {"title": "Neural machine translation of rare words with subword units", "author": ["Barry Haddow", "Alexandra Birch"], "venue": "In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume", "citeRegEx": "Sennrich et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Sennrich et al\\.", "year": 2016}, {"title": "Statistical phrase-based postediting", "author": ["Simard et al.2007] Michel Simard", "Cyril Goutte", "Pierre Isabelle"], "venue": "Proceedings of NAACL", "citeRegEx": "Simard et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Simard et al\\.", "year": 2007}, {"title": "The jrc-acquis: A multilingual aligned parallel corpus with 20+", "author": ["Bruno Pouliquen", "Anna Widiger", "Camelia Ignat", "Tomaz Erjavec", "Dan Tufis", "D\u00e1niel Varga"], "venue": null, "citeRegEx": "Steinberger et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Steinberger et al\\.", "year": 2006}, {"title": "Parallel data, tools and interfaces in opus", "author": ["J\u00f6rg Tiedemann"], "venue": "In LREC,", "citeRegEx": "Tiedemann.,? \\Q2012\\E", "shortCiteRegEx": "Tiedemann.", "year": 2012}, {"title": "Deep recurrent models with fast-forward connections for neural machine translation. Transactions of the Association for Computational Linguistics, 4:371\u2013383", "author": ["Zhou et al.2016] Jie Zhou", "Ying Cao", "Xuguang Wang", "Peng Li", "Wei Xu"], "venue": null, "citeRegEx": "Zhou et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Zhou et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 0, "context": "Since the first online demonstration of Neural Machine Translation (NMT) by LISA (Bahdanau et al., 2014), NMT development has recently moved from laboratory to production systems as demonstrated by several entities announcing rollout of NMT engines to replace their existing technologies.", "startOffset": 81, "endOffset": 104}, {"referenceID": 20, "context": "With its root on a number of established open-source projects such as Andrej Karpathy\u2019s char-rnn,3 Wojciech Zaremba\u2019s standard long short-term memory (LSTM)4 and the rnn library from Element-Research,5 the framework provides a solid NMT basis consisting of LSTM, as the recurrent module and faithful reimplementations of global-general-attention model and input-feeding at each time-step of the RNN decoder as described by Luong et al. (2015).", "startOffset": 423, "endOffset": 443}, {"referenceID": 19, "context": "Also available opensource corpora are domain specific: Europarl (Koehn, 2005), JRC (Steinberger et al.", "startOffset": 64, "endOffset": 77}, {"referenceID": 29, "context": "Also available opensource corpora are domain specific: Europarl (Koehn, 2005), JRC (Steinberger et al., 2006) or MultiUN (Chen and Eisele, 2012) are legal texts, ted talk are scientific presentations, open subtitles (Tiedemann, 2012) are colloquial, etc.", "startOffset": 83, "endOffset": 109}, {"referenceID": 30, "context": ", 2006) or MultiUN (Chen and Eisele, 2012) are legal texts, ted talk are scientific presentations, open subtitles (Tiedemann, 2012) are colloquial, etc.", "startOffset": 114, "endOffset": 131}, {"referenceID": 15, "context": "In Junczys-Dowmunt et al. (2016), authors mention using corpus of 5M sentences and training of 1.", "startOffset": 3, "endOffset": 33}, {"referenceID": 15, "context": "In Junczys-Dowmunt et al. (2016), authors mention using corpus of 5M sentences and training of 1.2M batches each having 40 sentences \u2013 meaning basically that each sentence of the full corpus is presented 10 times to the training. In Wu et al. (2016), authors mention 2M steps of 128 examples for English\u2013French, for a corpus of 36M sentences, meaning about 7 iterations on the complete corpus.", "startOffset": 3, "endOffset": 250}, {"referenceID": 9, "context": "A number of previous work such as characterlevel (Chung et al., 2016), hybrid word-characterbased (Luong and Manning, 2016) and subwordlevel (Sennrich et al.", "startOffset": 49, "endOffset": 69}, {"referenceID": 15, "context": ", 2016) and (Junczys-Dowmunt et al., 2016), it does not have significant side effects (for instance generation of impossible words) and is not optimal to deal with actual word morphology - since the same suffix (josa in Korea) depending on the frequency of the word ending it is integrated with, will be splitted in multiple representations.", "startOffset": 12, "endOffset": 42}, {"referenceID": 25, "context": "The sub-word encoding algorithm Byte Pair Encoding (BPE) described by Sennrich et al. (2016b)", "startOffset": 70, "endOffset": 94}, {"referenceID": 11, "context": "fast align (Dyer et al., 2013) to automatically align source words to target words.", "startOffset": 11, "endOffset": 30}, {"referenceID": 6, "context": "We re-reimplemented Guided alignment strategy described in Chen et al. (2016). Guided alignment enforces the attention weights to be more like alignments in the traditional sense (e.", "startOffset": 59, "endOffset": 78}, {"referenceID": 25, "context": "Following the work of Sennrich et al. (2016a), we implemented a politeness feature in our NMT engine: a special token is added to each source sentence during training, where the token indicates the politeness mode observed in the target sentence.", "startOffset": 22, "endOffset": 46}, {"referenceID": 23, "context": "With Neural Machine Translation, a new notion of \u201cspecialization\u201d comes close to the concept of incremental translation as developed for statistical machine translation like (Ortiz-Mart\u0131\u0301nez et al., 2010).", "startOffset": 174, "endOffset": 204}, {"referenceID": 5, "context": "Adaptation proceeds incrementally when new in-domain data becomes available, generated by human translators while post-editing, which is similar to the Computer Aided Translation framework described in (Cettolo et al., 2014).", "startOffset": 202, "endOffset": 224}, {"referenceID": 28, "context": "Until recently, most of the APE approaches have been based on phrase-based SMT systems, either monolingual (MT target to human post-edition) (Simard et al., 2007) or source-", "startOffset": 141, "endOffset": 162}, {"referenceID": 1, "context": "aware (B\u00e9chara et al., 2011).", "startOffset": 6, "endOffset": 28}, {"referenceID": 10, "context": "For many years now SYSTRAN has been offering a hybrid Statistical Post-Editing (SPE) solution to enhance the translation provided by its rule-based MT system (RBMT) (Dugast et al., 2007).", "startOffset": 165, "endOffset": 186}, {"referenceID": 24, "context": "This approach has been proven efficient for the NMT tasks in See et al. (2016). Inspired by this work, we introduced similar pruning techniques in seq2seq-attn.", "startOffset": 61, "endOffset": 79}, {"referenceID": 3, "context": "Evaluation of machine translation has always been a challenge and subject to many papers and dedicated workshops (Bojar et al., 2016).", "startOffset": 113, "endOffset": 133}, {"referenceID": 2, "context": "research world and have shown good correlation with human evaluation, ad-hoc human evaluation or productivity analysis metrics are rather used in the industry (Blain et al., 2011).", "startOffset": 159, "endOffset": 179}, {"referenceID": 4, "context": "bing In 2007, Google already mentions using 2 trillion words in their language models for machine translation (Brants et al., 2007).", "startOffset": 110, "endOffset": 131}, {"referenceID": 13, "context": "As proposed by (G\u00fcl\u00e7ehre et al., 2015), we conduct experiments to integrate an n-gram language model estimated over a large dataset on our Neural MT system.", "startOffset": 15, "endOffset": 38}, {"referenceID": 31, "context": "Ensemble decoding has been verified as a practical technique to further improve the performance compared to a single Encoder-Decoder model (Sennrich et al., 2016b; Wu et al., 2016; Zhou et al., 2016).", "startOffset": 139, "endOffset": 199}, {"referenceID": 8, "context": "The improvement comes from the diversity of prediction from different neural network models, which are learned by random initialization seeds and shuffling of examples during training, or different optimization methods towards the development set(Cho et al., 2015).", "startOffset": 246, "endOffset": 264}, {"referenceID": 15, "context": "Also, (Junczys-Dowmunt et al., 2016) provides some methods to accelerate the training by choosing different checkpoints as the final models.", "startOffset": 6, "endOffset": 36}, {"referenceID": 22, "context": "word2vec (Mikolov et al., 2013)).", "startOffset": 9, "endOffset": 31}, {"referenceID": 12, "context": "In the recent literature (Gu et al., 2016; Gulcehre et al., 2016), researchers have attempted to directly model the unknown word handling within the attention and decoder networks.", "startOffset": 25, "endOffset": 65}, {"referenceID": 14, "context": "In the recent literature (Gu et al., 2016; Gulcehre et al., 2016), researchers have attempted to directly model the unknown word handling within the attention and decoder networks.", "startOffset": 25, "endOffset": 65}], "year": 2016, "abstractText": "Since the first online demonstration of Neural Machine Translation (NMT) by LISA (Bahdanau et al., 2014), NMT development has recently moved from laboratory to production systems as demonstrated by several entities announcing rollout of NMT engines to replace their existing technologies. NMT systems have a large number of training configurations and the training process of such systems is usually very long, often a few weeks, so role of experimentation is critical and important to share. In this work, we present our approach to production-ready systems simultaneously with release of online demonstrators covering a large variety of languages (12 languages, for 32 language pairs). We explore different practical choices: an efficient and evolutive open-source framework; data preparation; network architecture; additional implemented features; tuning for production; etc. We discuss about evaluation methodology, present our first findings and we finally outline further work. Our ultimate goal is to share our expertise to build competitive production systems for \u201dgeneric\u201d translation. We aim at contributing to set up a collaborative framework to speed-up adoption of the technology, foster further research efforts and enable the delivery and adoption to/by industry of use-case specific engines integrated in real production workflows. Mastering of the technology would allow us to build translation engines suited for particular needs, outperforming current simplest/uniform systems.", "creator": "LaTeX with hyperref package"}}}