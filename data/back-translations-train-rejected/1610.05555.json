{"id": "1610.05555", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Oct-2016", "title": "Online Contrastive Divergence with Generative Replay: Experience Replay without Storing Data", "abstract": "Conceived in the early 1990s, Experience Replay (ER) has been shown to be a successful mechanism to allow online learning algorithms to reuse past experiences. Traditionally, ER can be applied to all machine learning paradigms (i.e., unsupervised, supervised, and reinforcement learning). Recently, ER has contributed to improving the performance of deep reinforcement learning. Yet, its application to many practical settings is still limited by the memory requirements of ER, necessary to explicitly store previous observations. To remedy this issue, we explore a novel approach, Online Contrastive Divergence with Generative Replay (OCD_GR), which uses the generative capability of Restricted Boltzmann Machines (RBMs) instead of recorded past experiences. The RBM is trained online, and does not require the system to store any of the observed data points. We compare OCD_GR to ER on 9 real-world datasets, considering a worst-case scenario (data points arriving in sorted order) as well as a more realistic one (sequential random-order data points). Our results show that in 64.28% of the cases OCD_GR outperforms ER and in the remaining 35.72% it has an almost equal performance, while having a considerably reduced space complexity (i.e., memory usage) at a comparable time complexity.", "histories": [["v1", "Tue, 18 Oct 2016 12:06:14 GMT  (746kb,D)", "http://arxiv.org/abs/1610.05555v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.NE", "authors": ["decebal constantin mocanu", "maria torres vega", "eric eaton", "peter stone", "antonio liotta"], "accepted": false, "id": "1610.05555"}, "pdf": {"name": "1610.05555.pdf", "metadata": {"source": "CRF", "title": "Online Contrastive Divergence with Generative Replay: Experience Replay without Storing Data", "authors": ["Decebal Constantin Mocanu", "Maria Torres Vega", "Eric Eaton", "Antonio Liotta"], "emails": ["d.c.mocanu@tue.nl"], "sections": [{"heading": null, "text": "Keywords generative replay \u00b7 experience replay \u00b7 unattended learning \u00b7 online learning \u00b7 density assessment \u00b7 limited Boltzmann machines \u00b7 deep learningDecebal Constantin Mocanu Department of Electrical Engineering, Eindhoven University of Technology, Eindhoven, Netherlands Tel.: + 31 40-247 5394 Email: d.c.mocanu @ tue.nlMaria Torres Vega Department of Electrical Engineering, Eindhoven, Netherlands Eric Eaton Department of Computer and Information Science, University of Pennsylvania, Philadelphia, USAPeter Stone Department of Computer Science, The University of Texas at Austin, Austin, USAAntonio Liotta Department of Electrical Engineering, Eindhoven University of Technology, Eindhoven, Netherlands Xiv: 161 0.05 555v 1 [cs.L G] 18 October 201 6"}, {"heading": "1 Introduction", "text": "In fact, it is the case that most of them are able to abide by the rules that they have imposed on themselves. (...) In fact, it is the case that they are able to understand the rules that they apply in practice. (...) In practice, it is the case that they are able to understand the rules. (...) In practice, it is the case that they are not applied in practice. (...) In practice, it is the case that they are not applied in practice. (...) In practice, it is the case that they are not applied in practice. (...) In practice, it is the case that they are not applied in practice. (...) In practice, it is the case that they are not applied in practice. (...) In practice, in practice, in practice, in practice, in practice, in practice, in practice, in practice, in practice, in practice, in practice. (...) In practice, in practice, in practice, in practice. (...) In practice, in practice, in practice, in practice, in practice."}, {"heading": "2 Background and Related Work", "text": "In this section, we first discuss the learning experience in two parts: one for the positive and one for the negative effects."}, {"heading": "3 Online Contrastive Divergence with Generative Replay", "text": "This section presents our new algorithms to analyze the experiences of the past. < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < > > < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < <; < < < < < < < < < <; < < < < <; < < < < < <; < < < < < <; < < < < < < < < < < < < < < < < < < < < < < < < < < < < <"}, {"heading": "4 Experiments and Results", "text": "First, we considered a toy scenario (i.e. an artificially generated data set) to illustrate the OCDGR behavior = 000 points. Second, we evaluated the performance of OCDGR on the MNIST dataset1 of the handwritten digits and on the UCI evaluation scenario (Germain et al, 2015). In total, the evaluation was performed on 9 datasets coming from different areas, which are detailed in Table 1. To simulate the online learning situation, each training instance was fed to the RBM training algorithm only once in sequence. (1) A worst-case scenario, in which the data instances are presented in the order of the classes, and 2.) a more realistic scenario, in which the instances are randomly ordered. The update process of the free RBM parameters was triggered each time after the system was observed and 100 data points collected (i.e. 100)."}, {"heading": "4.2.1 Worst Case Scenario: Sorted Order", "text": "In the first scenario, we used the binary MNIST dataset. During the training, the data instances were arranged sequentially in ascending order of the digits (0, 1,..., 9), making it a difficult scenario for online learning. For each algorithm, we took into account different numbers of hidden neurons (nh, 25, 250, 500) and 784 visible neurons (i.e. 28 x 28 binary images). Figure 4a shows that RBMOCD exceeds RBMER-ML in all cases, regardless of the number of hidden neurons. Furthermore, it even exceeds RBMER-IM if it has enough representational power (i.e. 250 and 500 hidden neurons). It is interesting to see that while the generative power of RBMOCD increases in all cases with the number of hidden neurons, RBMER-IM is not significantly affected if RBMER-IM provides more hidden neurons (i.e., 250 and 500 hidden neurons)."}, {"heading": "4.2.2 Realistic Scenario: Random Order", "text": "This year it has come to the point that it will only be a matter of time before it will happen, until it does."}, {"heading": "5 Conclusion", "text": "We have proposed a novel method, Online Contrastive Divergence with Generative Replay (OCDGR), to train RBMs in online environments. Contrary to the current replay mechanisms, which retrieve directly recorded observations from memory, OCDGR uses the generative capabilities of RBMs to dynamically simulate past experiences. Consequently, it does not need to store past observations in memory, which significantly reduces its memory requirements. We have shown that RBMs trained online with OCDGR outperform RBMs that use experience replay mechanisms with memory, with few exceptions where their performance is comparable. We highlight that in some cases they even exceed RBMs with a similar number of hidden neurons, but are trained offline with standard contrasting deviations.In future work, we intend to better understand the effects of the various OCDGR meta parameters (in particular the relationship between the number of samples and the number of samples being observed)."}], "references": [{"title": "A learning algorithm for boltzmann machines", "author": ["H Ackley", "E Hinton", "J Sejnowski"], "venue": "Cognitive Science", "citeRegEx": "Ackley et al\\.,? \\Q1985\\E", "shortCiteRegEx": "Ackley et al\\.", "year": 1985}, {"title": "Experience replay for real-time reinforcement learning control", "author": ["S Adam", "L Busoniu", "R Babuska"], "venue": "IEEE Transactions on Systems,", "citeRegEx": "Adam et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Adam et al\\.", "year": 2012}, {"title": "Automatically mapped transfer between reinforcement learning tasks via three-way restricted boltzmann machines", "author": ["HB Ammar", "DC Mocanu", "M Taylor", "K Driessens", "K Tuyls", "G Weiss"], "venue": "Notes in Computer Science,", "citeRegEx": "Ammar et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Ammar et al\\.", "year": 2013}, {"title": "An automated measure of mdp similarity for transfer in reinforcement learning", "author": ["HB Ammar", "E Eaton", "ME Taylor", "DC Mocanu", "K Driessens", "G Weiss", "K Tuyls"], "venue": "Workshops at the Twenty-Eighth AAAI Conference on Artificial Intelligence", "citeRegEx": "Ammar et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Ammar et al\\.", "year": 2014}, {"title": "Learning deep architectures for ai", "author": ["Y Bengio"], "venue": "Found Trends Mach Learn 2(1):1\u2013127,", "citeRegEx": "Bengio,? \\Q2009\\E", "shortCiteRegEx": "Bengio", "year": 2009}, {"title": "Spatial anomaly detection", "author": ["HH Bosman", "G Iacca", "A Tejada", "HJ W\u00f6rtche", "A Liotta"], "venue": null, "citeRegEx": "Bosman et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Bosman et al\\.", "year": 2017}, {"title": "The flip-the-state transition operator for restricted", "author": ["A Fischer", "C Igel"], "venue": null, "citeRegEx": "Br\u00fcgge et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Br\u00fcgge et al\\.", "year": 2013}, {"title": "Parallel tempering for", "author": ["G Desjardins", "A Courville", "Y Bengio", "P Vincent", "O Delalleau"], "venue": null, "citeRegEx": "Desjardins et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Desjardins et al\\.", "year": 2010}, {"title": "The rate adapting poisson model for information", "author": ["PV Gehler", "AD Holub", "M Welling"], "venue": null, "citeRegEx": "Gehler et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Gehler et al\\.", "year": 2010}, {"title": "MADE: masked autoencoder", "author": ["M Germain", "K Gregor", "I Murray", "H Larochelle"], "venue": "Machine Learning, ACM,", "citeRegEx": "Germain et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Germain et al\\.", "year": 2015}, {"title": "A practical guide to training restricted boltzmann machines", "author": ["G Hinton"], "venue": "Learning, ICML 2015,", "citeRegEx": "Hinton,? \\Q2015\\E", "shortCiteRegEx": "Hinton", "year": 2015}, {"title": "Batch reinforcement learning in a complex", "author": ["S Kalyanakrishnan", "P Stone"], "venue": null, "citeRegEx": "Kalyanakrishnan and Stone,? \\Q2007\\E", "shortCiteRegEx": "Kalyanakrishnan and Stone", "year": 2007}, {"title": "Classification using discriminative restricted boltzmann", "author": ["H Larochelle", "Y Bengio"], "venue": null, "citeRegEx": "Larochelle and Bengio,? \\Q2008\\E", "shortCiteRegEx": "Larochelle and Bengio", "year": 2008}, {"title": "Self-improving reactive agents based on reinforcement learning, planning", "author": ["LJ Lin"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1992}, {"title": "Human-level control through deep", "author": ["D Kumaran", "D Wierstra", "S Legg", "D Hassabis"], "venue": null, "citeRegEx": "Kumaran et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kumaran et al\\.", "year": 2015}, {"title": "A (2014) Deep learning for objective quality assessment", "author": ["DC Mocanu", "G Exarchakos", "Liotta"], "venue": null, "citeRegEx": "Mocanu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Mocanu et al\\.", "year": 2014}, {"title": "A (2016) A topological insight", "author": ["DC Mocanu", "E Mocanu", "PH Nguyen", "M Gibescu", "Liotta"], "venue": null, "citeRegEx": "Mocanu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Mocanu et al\\.", "year": 2016}, {"title": "Language understanding for text-based games", "author": ["K s10994-016-5570-z Narasimhan", "T Kulkarni", "R Barzilay"], "venue": null, "citeRegEx": "Narasimhan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Narasimhan et al\\.", "year": 2015}, {"title": "Online semi-supervised learning with deep hybrid boltzmann machines and denoising autoencoders", "author": ["AG Ororbia", "CL Giles", "D Reitter"], "venue": null, "citeRegEx": "Ororbia et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ororbia et al\\.", "year": 2015}, {"title": "Restricted boltzmann machines modeling human choice", "author": ["T Osogami", "M Otsuka"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Osogami and Otsuka,? \\Q2014\\E", "shortCiteRegEx": "Osogami and Otsuka", "year": 2014}, {"title": "On the quantitative analysis of deep belief networks", "author": ["R Salakhutdinov", "I Murray"], "venue": "Proceedings of the International Conference on Machine Learning,", "citeRegEx": "Salakhutdinov and Murray,? \\Q2008\\E", "shortCiteRegEx": "Salakhutdinov and Murray", "year": 2008}, {"title": "Restricted boltzmann machines for collaborative filtering", "author": ["R Salakhutdinov", "A Mnih", "G Hinton"], "venue": "Proceedings of the 24th International Conference on Machine Learning, ACM,", "citeRegEx": "Salakhutdinov et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Salakhutdinov et al\\.", "year": 2007}, {"title": "Prioritized experience replay", "author": ["T Schaul", "J Quan", "I Antonoglou", "D Silver"], "venue": null, "citeRegEx": "Schaul et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Schaul et al\\.", "year": 2015}, {"title": "Information processing in dynamical systems: Foundations of harmony theory", "author": ["P Smolensky"], "venue": null, "citeRegEx": "Smolensky,? \\Q1987\\E", "shortCiteRegEx": "Smolensky", "year": 1987}, {"title": "Dyna-style planning with linear function approximation and prioritized sweeping", "author": ["RS Sutton", "C Szepesv\u00e1ri", "A Geramifard", "M Bowling"], "venue": "Proceedings of the 24th Conference on Uncertainty in Artificial Intelligence", "citeRegEx": "Sutton et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Sutton et al\\.", "year": 2008}, {"title": "Two distributed-state models for generating high-dimensional time series", "author": ["GW Taylor", "GE Hinton", "ST Roweis"], "venue": "Journal of Machine Learning Research", "citeRegEx": "Taylor et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Taylor et al\\.", "year": 2011}, {"title": "Training restricted boltzmann machines using approximations to the likelihood gradient", "author": ["T Tieleman"], "venue": "Proceedings of the 25th International Conference on Machine Learning,", "citeRegEx": "Tieleman,? \\Q2008\\E", "shortCiteRegEx": "Tieleman", "year": 2008}, {"title": "Using fast weights to improve persistent contrastive divergence", "author": ["T Tieleman", "G Hinton"], "venue": "Proceedings of the 26th Annual International Conference on Machine Learning,", "citeRegEx": "Tieleman and Hinton,? \\Q2009\\E", "shortCiteRegEx": "Tieleman and Hinton", "year": 2009}], "referenceMentions": [{"referenceID": 11, "context": "Since ER uses recorded data in chunks, it has sometimes been deemed a batch learning approach (Kalyanakrishnan and Stone, 2007).", "startOffset": 94, "endOffset": 127}, {"referenceID": 23, "context": "At the same time, Restricted Boltzmann Machines (RBMs) (Smolensky, 1987), the original building blocks in deep learning models (Bengio, 2009), besides providing in an unsupervised manner good weights for the deep belief networks initialization (LeCun et al, 2015), have been shown to be very good density estimators and to have powerful generative capabilities (Salakhutdinov and Murray, 2008; Mocanu et al, 2016).", "startOffset": 55, "endOffset": 72}, {"referenceID": 4, "context": "At the same time, Restricted Boltzmann Machines (RBMs) (Smolensky, 1987), the original building blocks in deep learning models (Bengio, 2009), besides providing in an unsupervised manner good weights for the deep belief networks initialization (LeCun et al, 2015), have been shown to be very good density estimators and to have powerful generative capabilities (Salakhutdinov and Murray, 2008; Mocanu et al, 2016).", "startOffset": 127, "endOffset": 141}, {"referenceID": 20, "context": "At the same time, Restricted Boltzmann Machines (RBMs) (Smolensky, 1987), the original building blocks in deep learning models (Bengio, 2009), besides providing in an unsupervised manner good weights for the deep belief networks initialization (LeCun et al, 2015), have been shown to be very good density estimators and to have powerful generative capabilities (Salakhutdinov and Murray, 2008; Mocanu et al, 2016).", "startOffset": 361, "endOffset": 413}, {"referenceID": 19, "context": "Examples of these applications are: modeling human choice (Osogami and Otsuka, 2014), collaborative filtering (Salakhutdinov et al, 2007), information retrieval (Gehler et al, 2006), transfer learning (Ammar et al, 2013), or multiclass classification (Larochelle and Bengio, 2008).", "startOffset": 58, "endOffset": 84}, {"referenceID": 12, "context": "Examples of these applications are: modeling human choice (Osogami and Otsuka, 2014), collaborative filtering (Salakhutdinov et al, 2007), information retrieval (Gehler et al, 2006), transfer learning (Ammar et al, 2013), or multiclass classification (Larochelle and Bengio, 2008).", "startOffset": 251, "endOffset": 280}, {"referenceID": 10, "context": "Since ER uses recorded data in chunks, it has sometimes been deemed a batch learning approach (Kalyanakrishnan and Stone, 2007). In general, ER focuses on the reuse of observed data in its raw form as stored in memory, replaying it to the online learner. However, this causes ER to scale poorly, as the memory requirements increase as the environment and system requirements increase. One common practice is to limit the available memory of the ER mechanism and to either 1.) discard the oldest experiences as the memory buffer becomes full and/or 2.) prioritize the experiences (Schaul et al, 2015). From a biological sense of memory (i.e., hippocampal replay in McClelland et al (1995)), the human brain does not store all observations explicitly, but instead it dynamically generates approximate reconstructions of those experiences for recall.", "startOffset": 95, "endOffset": 688}, {"referenceID": 4, "context": "At the same time, Restricted Boltzmann Machines (RBMs) (Smolensky, 1987), the original building blocks in deep learning models (Bengio, 2009), besides providing in an unsupervised manner good weights for the deep belief networks initialization (LeCun et al, 2015), have been shown to be very good density estimators and to have powerful generative capabilities (Salakhutdinov and Murray, 2008; Mocanu et al, 2016). Due to these capabilities RBMs and models derived from them have been successfully applied to various problems also as standalone models. Examples of these applications are: modeling human choice (Osogami and Otsuka, 2014), collaborative filtering (Salakhutdinov et al, 2007), information retrieval (Gehler et al, 2006), transfer learning (Ammar et al, 2013), or multiclass classification (Larochelle and Bengio, 2008). However, in all of the above settings RBMs have been used offline using offline training algorithms. This reduces drastically their capabilities to tackle real-world problems which can not be handled on server clouds using GPU computing, and require fast training algorithms capable of continuous learning when the the environment is changing. For example, in the world of wireless sensor networks which is by definition an environment with low-resources (e.g., memory, computational power, low energy) devices to perform anomaly detection in time series directly in the wireless nodes would help improving the network capacity on various components (e.g., lifetime, avoid data traffic congestions), as exemplified in Bosman et al (2017). We then hypothesize that due to their density estimation capabilities RBMs which have been used recently to", "startOffset": 128, "endOffset": 1573}, {"referenceID": 11, "context": "A complete review of ER applicability does not constitute a goal of this paper, but as an example, Kalyanakrishnan and Stone (2007) showed that the standard RL and the batch approaches (ER) are easily comparable in terms of performance.", "startOffset": 99, "endOffset": 132}, {"referenceID": 11, "context": "A complete review of ER applicability does not constitute a goal of this paper, but as an example, Kalyanakrishnan and Stone (2007) showed that the standard RL and the batch approaches (ER) are easily comparable in terms of performance. Recently, ER and its variants have contributed to improving deep reinforcement learning (DRL) (Mnih et al, 2015). Narasimhan et al (2015) proposed a form of re-sampling in the context of DRL, separating the learner experience into two parts: one for the positive rewards and one for the negative.", "startOffset": 99, "endOffset": 375}, {"referenceID": 11, "context": "A complete review of ER applicability does not constitute a goal of this paper, but as an example, Kalyanakrishnan and Stone (2007) showed that the standard RL and the batch approaches (ER) are easily comparable in terms of performance. Recently, ER and its variants have contributed to improving deep reinforcement learning (DRL) (Mnih et al, 2015). Narasimhan et al (2015) proposed a form of re-sampling in the context of DRL, separating the learner experience into two parts: one for the positive rewards and one for the negative. Ororbia et al (2015) proposed an online semi-supervised learning algorithm using deep hybrid Boltzmann machines and denoising autoencoders.", "startOffset": 99, "endOffset": 555}, {"referenceID": 23, "context": "They were introduced by Smolensky (1987) as a powerful model to learn a probability distribution over its inputs.", "startOffset": 24, "endOffset": 41}, {"referenceID": 4, "context": "Due the binary state of the neurons, the free energy of the visible units may be computed as (Bengio, 2009):", "startOffset": 93, "endOffset": 107}, {"referenceID": 4, "context": "where P\u0302 represents the empirical distribution of D and EP is the expectation computed under the model distribution (Bengio, 2009).", "startOffset": 116, "endOffset": 130}, {"referenceID": 4, "context": "where P\u0302 represents the empirical distribution of D and EP is the expectation computed under the model distribution (Bengio, 2009). However, sampling from P to compute the free energy and running long Monte-Carlo Markov Chains (MCMC) to obtain an estimator of the log-likelihood gradient is usually intractable. Due to this intractability, Hinton (2002) proposed an approximation method called Contrastive Divergence (CD), which solves the above problem by making two approximations.", "startOffset": 117, "endOffset": 354}, {"referenceID": 26, "context": "Examples of these are: persistent contrastive divergence (Tieleman, 2008), fast persistent contrastive divergence (Tieleman and Hinton, 2009), parallel tempering (Desjardins et al, 2010), and the replace of the Gibbs sampling with a transition operator to obtain a faster mixing rate and an improved learning accuracy without affecting the computational costs (Br\u00fcgge et al, 2013).", "startOffset": 57, "endOffset": 73}, {"referenceID": 27, "context": "Examples of these are: persistent contrastive divergence (Tieleman, 2008), fast persistent contrastive divergence (Tieleman and Hinton, 2009), parallel tempering (Desjardins et al, 2010), and the replace of the Gibbs sampling with a transition operator to obtain a faster mixing rate and an improved learning accuracy without affecting the computational costs (Br\u00fcgge et al, 2013).", "startOffset": 114, "endOffset": 141}, {"referenceID": 4, "context": "At the same time, RBMs can generate good samples of the incorporated data distribution via Gibbs sampling (Bengio, 2009).", "startOffset": 106, "endOffset": 120}, {"referenceID": 10, "context": "Except for the two OCDGR specific parameters, the settings for the others are discussed by Hinton (2012). The algorithm first initializes the RBM\u2019s free parameters \u0398 and the discrete time step t (lines 3\u20135).", "startOffset": 91, "endOffset": 105}, {"referenceID": 10, "context": "Except for the two OCDGR specific parameters, the settings for the others are discussed by Hinton (2012). The algorithm first initializes the RBM\u2019s free parameters \u0398 and the discrete time step t (lines 3\u20135). Each time step, the algorithm observes a new data instance, collecting nB new data points into a mini-batch Bt (lines 8\u201310). After observing nB new data points, OCDGR updates the RBM\u2019s parameters (line 11\u201341). The update procedure proceeds in two phases: Dynamic generation of historical data (lines 14\u201322) As it has been shown in Desjardins et al (2010) and in Cho et al (2010) that RBMs can sample uniformly from the state space, we generate nB\u0302 new training data points to represent past observations based on the data distribution modeled by the RBM at time t \u2212 1; these generated data points are collected into the set B\u0302t.", "startOffset": 91, "endOffset": 563}, {"referenceID": 10, "context": "Except for the two OCDGR specific parameters, the settings for the others are discussed by Hinton (2012). The algorithm first initializes the RBM\u2019s free parameters \u0398 and the discrete time step t (lines 3\u20135). Each time step, the algorithm observes a new data instance, collecting nB new data points into a mini-batch Bt (lines 8\u201310). After observing nB new data points, OCDGR updates the RBM\u2019s parameters (line 11\u201341). The update procedure proceeds in two phases: Dynamic generation of historical data (lines 14\u201322) As it has been shown in Desjardins et al (2010) and in Cho et al (2010) that RBMs can sample uniformly from the state space, we generate nB\u0302 new training data points to represent past observations based on the data distribution modeled by the RBM at time t \u2212 1; these generated data points are collected into the set B\u0302t.", "startOffset": 91, "endOffset": 587}, {"referenceID": 20, "context": "To quantify the generative performance of the trained networks, we used Annealed Importance Sampling (AIS) with the same parameters as in the original paper (Salakhutdinov and Murray, 2008) to estimate the partition function of the RBMs and to calculate their log probabilities.", "startOffset": 157, "endOffset": 189}, {"referenceID": 20, "context": "01 nats even the state-of-the-art results reported by Salakhutdinov and Murray (Salakhutdinov and Murray, 2008) (see Table 2, third row) for an RBM with 500 hidden neurons trained completely offline with standard one-step contrastive divergence.", "startOffset": 79, "endOffset": 111}, {"referenceID": 20, "context": "On the MNIST dataset the offline RBM results are taken from Salakhutdinov and Murray (2008), while on the UCI evaluation suite the offline RBMs results are taken from Germain et al (2015).", "startOffset": 60, "endOffset": 92}, {"referenceID": 20, "context": "On the MNIST dataset the offline RBM results are taken from Salakhutdinov and Murray (2008), while on the UCI evaluation suite the offline RBMs results are taken from Germain et al (2015).", "startOffset": 60, "endOffset": 188}, {"referenceID": 20, "context": "Similarly to the RBM\u2019s behavior reported by Salakhutdinov and Murray (Salakhutdinov and Murray, 2008), further CD steps improved the generative performance of these models.", "startOffset": 69, "endOffset": 101}], "year": 2016, "abstractText": "Conceived in the early 1990s, Experience Replay (ER) has been shown to be a successful mechanism to allow online learning algorithms to reuse past experiences. Traditionally, ER can be applied to all machine learning paradigms (i.e., unsupervised, supervised, and reinforcement learning). Recently, ER has contributed to improving the performance of deep reinforcement learning. Yet, its application to many practical settings is still limited by the memory requirements of ER, necessary to explicitly store previous observations. To remedy this issue, we explore a novel approach, Online Contrastive Divergence with Generative Replay (OCDGR), which uses the generative capability of Restricted Boltzmann Machines (RBMs) instead of recorded past experiences. The RBM is trained online, and does not require the system to store any of the observed data points. We compare OCDGR to ER on 9 real-world datasets, considering a worst-case scenario (data points arriving in sorted order) as well as a more realistic one (sequential random-order data points). Our results show that in 64.28% of the cases OCDGR outperforms ER and in the remaining 35.72% it has an almost equal performance, while having a considerably reduced space complexity (i.e., memory usage) at a comparable time complexity.", "creator": "LaTeX with hyperref package"}}}