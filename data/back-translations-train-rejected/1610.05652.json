{"id": "1610.05652", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Oct-2016", "title": "Vietnamese Named Entity Recognition using Token Regular Expressions and Bidirectional Inference", "abstract": "This paper describes an efficient approach to improve the accuracy of a named entity recognition system for Vietnamese. The approach combines regular expressions over tokens and a bidirectional inference method in a sequence labelling model, which achieves an overall $F_1$ score of 89.66% on a test set of an evaluation compaign, organized in late 2016 by the Vietnamese Language and Speech Processing (VLSP) community.", "histories": [["v1", "Tue, 18 Oct 2016 14:44:01 GMT  (9kb)", "http://arxiv.org/abs/1610.05652v1", "Submitted to the VLSP Workshop 2016"], ["v2", "Wed, 19 Oct 2016 14:25:42 GMT  (10kb)", "http://arxiv.org/abs/1610.05652v2", "Submitted to the VLSP Workshop 2016"]], "COMMENTS": "Submitted to the VLSP Workshop 2016", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["phuong le-hong"], "accepted": false, "id": "1610.05652"}, "pdf": {"name": "1610.05652.pdf", "metadata": {"source": "CRF", "title": "Vietnamese Named Entity Recognition using Token Regular Expressions and Bidirectional Inference", "authors": ["Phuong Le-Hong"], "emails": ["phuonglh@vnu.edu.vn"], "sections": [{"heading": null, "text": "The aim is to identify noun phrases and classify each of them into a given class. In these evaluation tasks, four designated entities were considered, including names of persons, organizations, locations, and names of different entities that do not belong to these three types. More recently, the language-independent NICS systems were evaluated. In these evaluation tasks, four designated entities were considered, including names of persons, organizations, places, and names of entities that do not belong to these three types."}, {"heading": "II. MULTINOMIAL LOGISTIC REGRESSION", "text": "Unlike generative classifiers, discriminatory classifiers model the rear P (y | x) directly. One of the main advantages of discriminatory models is that we can integrate many heterogeneous features for prediction that are not necessarily independent of each other. Each feature corresponds to a constraint of the model. In this model, the conditional probability of a label y with an observation x is asP (y | x) = exp (x, y); each feature corresponds to a constraint of the model. In this model, the conditional probability of a label y with an observation x is defined asP (y | x) = exp (x, y)."}, {"heading": "III. FEATURE REPRESENTATIONS", "text": "Functions play an important role in discriminatory statistical classification models in general, and the maximum entropy model in particular, because they provide the discriminatory ability to efficiently disambiguate classes. To facilitate the extraction of different types of characteristics, each lexical token is associated with an interface word and an annotation card that contains different information about the text in the form of key and value pairs; the current annotation card contains values for parts of the language, chunks, regular expression types, and named entities; information about the interface word, part of the language, and chunk tags is given in connection with the named entity detection; and in a training phrase, named entity tags are also provided. In the next subsection, we describe the regular expression types assigned to each token to add some kind of semantic information useful for disambiguating named entities."}, {"heading": "A. Regular Expressions over Tokens", "text": "We use both character-level and symbol-level regular expressions to derive useful attributes for ambiguous named entities. While character-level regular expressions are used to detect word form information that was very important in NER, token-level regular expressions are very useful to detect word sequence information that matches many long-term entities. Common word form characteristics that our system uses include: \u2022 lower word formatting, such as \"iPhone\" \u2022 is uppercase, such as \"H.,\" \"US\" \u2022 Ends in digit, such as \"A9,\" \"B52\" \u2022 contains hyphae, such as \"H-P\" \u2022 is a number, such as \"100\" \u2022 is date, such as. \""}, {"heading": "B. Regular Expression Type Annotation", "text": "Once regular expressions are defined using tokens, we add a regular expression type for each word of an input set by commenting on the associated annotation card. Along with information about language components, regular expression types provide helpful information for better classification of named entities, as shown in the latter experiments. We use a greedy algorithm to comment on the regular expression type for each word, if it has it. Basically, the algorithm works as follows: (PatternName, PatternRegExp) we first look for all positions of the sentence that begin with a pattern, and select the longest match, say PatternName, and PatternName, which ranges from Token wi to Token wj. \u2212 Then all tokwi, wi + 1 tootters that match in another pattern, with pattern patternName, pattern patternName, and pattern PatternName, which range from Token to token wj."}, {"heading": "C. Feature Set", "text": "In this subsection, we describe the full range of functions used in our system to classify a token at any point in a sentence. \u2022 Basic features: current word w0, current part-of-voicp0, current chunk tag c0, previously named entity tags t \u2212 1 and t \u2212 2 (or a special padding token \"BOS\" - beginning of sentence); \u2022 Word form features, as described in the previous subsection; \u2022 Basic common features: previous word w \u2212 1 (or \"BOS\"), jointof current and previous word w0 + w \u2212 1, next word w + 1 (or \"EOS\" end of sentence), jointof current and next part-of speech w0 type type type + w + 1, former part-of-speech p \u2212 1, jointof current and next-part-of-of-speech p \u2212 1, jointp \u2212 rintp \u2212 1, rintp \u2212 1, rintp \u2212 of-of-speech p0 + 1."}, {"heading": "IV. BIDIRECTIONAL DECODING", "text": "The standard decoding algorithm for sequence marking is the Viterbi algorithm, which is a dynamic programming algorithm for searching for the most likely sequence of brand names. In this thesis, we also use the Viterbi algorithm to find the best tag sequence for a given word sequence, but we found a significant improvement in tagging accuracy when combining two decoding policies, both forward decoding and backward decoding. In this section, we describe our bi-directional decoding of T-words."}, {"heading": "V. EXPERIMENTS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. Datasets", "text": "We evaluate our system on the basis of the training data set provided by VLSP NER1. This data set contains 16 858 marked records, a total of 386 520 words. The data set contains four different types of designated units: Person (PER), Location (LOC), Organization (ORG) and Miscellaneous (MISC). As the real test set has not been published, we divide this training data set into two parts, one for training and one for development. The training data set has 306 512 characters (79.3% of the corpus), and the development data set has 80 007 characters (20.7% of the corpus)."}, {"heading": "B. Parameter Settings", "text": "The multinomial logistic regression models used in our system are trained by the L-BFGS optimization algorithm using the L2 regulation method with a regularization parameter of 10 \u2212 6.2. The convergence tolerance of the objective function is also set at 10 \u2212 6. The maximum number of iterations of the optimization algorithm is set at 300, i.e. the training ends either when the functional value converges or when the number of iterations exceeds 300. We use the feature hashing technique as a fast and space-saving method of vectorization characteristics. The number of characteristics for our models is set at 262, 144 (i.e. 218)."}, {"heading": "C. Main Results", "text": "The performance of our system is evaluated on the basis of development by executing the automatic evaluation script from1http: / / vlsp.org.vn / evaluation _ campaign _ NER 2. Using a larger regularization parameter underpins the model. The main results are shown in Table I. Our system as a whole achieves an F1 score of 89.66%. OFR names are the most difficult entity type for the system, the F1 of which is the lowest of 63.48%. Personal names are the simplest type for the system, whose accuracy and recall rates are high and the F1 score of this type is 93.77%."}, {"heading": "D. Effect of Bidirectional Inference", "text": "The performance of the forward model is shown in Table II and that of the reverse model is shown in Table III.We see that the reverse model is about 4.6 points better than the forward model in recognizing place names of F1. This is surprising, since the only difference between the two models is a reversal of the input records. One possible explanation for this effect is that when recognizing place names of a token sequence w1, w2,..., wn, if we are already aware of the type of proper names, it is easier to predict its previous tokens themselves \u2212 1 and so on. We project that this is due to the natural structure of Vietnamese place names. However, the reverse model undercuts the forward model in recognizing organization names by a large distance. Its F1 score of this type is only 52.28%, while that of the forward model is 63.48%. This is understandable because our current regular expressions on detection of regularities in many organizational names are not described as uniform in this section."}, {"heading": "E. Effect of Token Regular Expressions", "text": "In this subsection, we report on the effectiveness of regular token expressions in our model. We observe that using regular token expressions greatly improves the performance of the system. If the regular token expressions are not used for the ORG type, its value for the F1 value of the forward model is 62.94%. Adding regular token expressions for this type helps to increase this value to 65.01%. Likewise, in the forward model, its value is 82.19% if token regular expressions are not used for the LOC type. Adding six token regular expressions for this type improves its value to 83.07%. However, we observe that when all regular expressions are used together for these two entity types mentioned, they interact with each other and differ slightly in their values, as shown in Table II."}, {"heading": "F. Software", "text": "The entity recognition system developed in this paper has been integrated into the Vitk toolkit, which contains some basic tools for processing Vietnamese texts. It is developed in the Java and Scala programming languages, which are open source and freely downloadable for research purposes.4 An interesting feature of this toolkit is that it is an Apache Spark application, which is a fast and general engine for processing large amounts of data. Vitk is therefore a very fast and scalable toolkit for processing large amounts of text data."}, {"heading": "VI. CONCLUSION", "text": "We presented our approach and its experimental result in Entity Recognition for Vietnamese Text. We demonstrated the effectiveness of using regular token expressions, the bidirectional decoding method in a conditional Markov model for sequence marking, and the combination of reverse and forward models."}, {"heading": "ACKNOWLEDGEMENTS", "text": "This research is partially funded by Alt Inc.5, and in particular we would like to thank Dr. Nguyen Tuan Duc, head of Alt's R & D department, and the developers of the Apache Spark software."}], "references": [{"title": "Scalable training of l1-regularized log-linear models", "author": ["G. Andrew", "J. Gao"], "venue": "Proceedings of ICML, Oregon State University, Corvallis, USA, 2007, pp. 33\u201340.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2007}, {"title": "Numerical Optimization, 2nd ed", "author": ["J. Nocedal", "S.J. Wright"], "venue": "New York: Springer,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2006}], "referenceMentions": [{"referenceID": 0, "context": "The parameter \u03b8 \u2208 R can be estimated using iterative scaling algorithms or some more efficient gradient-based optimization algorithms like conjugate gradient or quasi-Newton methods [1].", "startOffset": 182, "endOffset": 185}, {"referenceID": 1, "context": "In this paper, we use the L-BFGS optimization algorithm [2] and L2-regularization technique to estimate the parameters of the model, with smooth term is fixed at 1.", "startOffset": 56, "endOffset": 59}], "year": 2017, "abstractText": "This paper describes an efficient approach to improve the accuracy of a named entity recognition system for Vietnamese. The approach combines regular expressions over tokens and a bidirectional inference method in a sequence labelling model, which achieves an overall F1 score of 89.66% on a test set of an evaluation compaign, organized in late 2016 by the Vietnamese Language and Speech Processing (VLSP) community.", "creator": "LaTeX with hyperref package"}}}