{"id": "1610.05688", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Oct-2016", "title": "Low-rank and Sparse Soft Targets to Learn Better DNN Acoustic Models", "abstract": "Conventional deep neural networks (DNN) for speech acoustic modeling rely on Gaussian mixture models (GMM) and hidden Markov model (HMM) to obtain binary class labels as the targets for DNN training. Subword classes in speech recognition systems correspond to context-dependent tied states or senones. The present work addresses some limitations of GMM-HMM senone alignments for DNN training. We hypothesize that the senone probabilities obtained from a DNN trained with binary labels can provide more accurate targets to learn better acoustic models. However, DNN outputs bear inaccuracies which are exhibited as high dimensional unstructured noise, whereas the informative components are structured and low-dimensional. We exploit principle component analysis (PCA) and sparse coding to characterize the senone subspaces. Enhanced probabilities obtained from low-rank and sparse reconstructions are used as soft-targets for DNN acoustic modeling, that also enables training with untranscribed data. Experiments conducted on AMI corpus shows 4.6% relative reduction in word error rate.", "histories": [["v1", "Tue, 18 Oct 2016 16:02:10 GMT  (2065kb,D)", "http://arxiv.org/abs/1610.05688v1", null]], "reviews": [], "SUBJECTS": "cs.CL cs.AI cs.HC cs.LG", "authors": ["pranay dighe", "afsaneh asaei", "herve bourlard"], "accepted": false, "id": "1610.05688"}, "pdf": {"name": "1610.05688.pdf", "metadata": {"source": "CRF", "title": "LOW-RANK AND SPARSE SOFT TARGETS TO LEARN BETTER DNN ACOUSTIC MODELS", "authors": ["Pranay Dighe", "Afsaneh Asaei", "Herv\u00e9 Bourlard"], "emails": [], "sections": [{"heading": null, "text": "Index Terms - Soft Targets, Main Component Analysis, Sparse Encoding, Automatic Voice Recognition, Untranscribed Data."}, {"heading": "1. INTRODUCTION", "text": "This year, it is only a matter of time before an agreement is reached."}, {"heading": "2. LOW-RANK AND SPARSE SOFT TARGETS", "text": "This section describes the novel approach for reliable soft target estimation. We investigate the reasons for regularities between posterior senons and examine two systematic approaches to obtain more accurate probabilities than soft targets for acoustic DNN modelling."}, {"heading": "2.1. Towards Better Targets for DNN Training", "text": "Previous research on the distillation of DNN knowledge shows the potential of soft targets for model compression and the sub-optimal nature of hard alignments [15, 20]. Although hard targets assign a specific senon label to a relatively long sequence of (\u0445 10 or more) acoustic frames, senon durations are usually shorter. A long context of input frames can lead to the presence of acoustic characteristics corresponding to several senons in the input (Fig. 1 (a)), so that the assumption of binary outputs is imprecise. In contrast, soft outputs quantify such sequential information using non-zero probabilities for several senon classes. Contextual senon dependencies arising in soft targets can be attributed to ambiguities due to phonetic transitions [20]. Furthermore, the method of senon extraction leads to acoustic correlations between several classes corresponding to the same telephone HMM states."}, {"heading": "2.2. Low-rank Reconstruction Using Eigenposteriors", "text": "s \".P (sK | xt) > P (sK | xt) > P (sK | xt) > P (sK | xt) > P (sK) > P (sK) > P (sK) > P (sK) > P (sK) > P (sK) > P (sK).P (sK).P (sK).P (sK).P (sK).P (sK).P (sK).P (sK).P (sK).P (sK).P (sK).P (sK).P (sK).P (sK).P (sK).P (sK).P (sK).P (sK).P (sK).S) (S) (S) (S) (S).K (S) (S) (S) (.K) > P (sK | xt) > P (sK | sK) > P (sK) > P (sK) > P (sK) > P (sK) > P (sK) > P (sK) > P (sK) > P (sK).P (sK).P (sK).P (sK).P (sK).P (sK).P (sK).P (sK).P (sK).P (sK).P (sK).P (sK (sK).P (sK).P (sK (sK).P (sK).P (sK (sK).P (sK (sK).P (sK (sK).P (sK (sK).P (sK (sK).P (sK (sK).P (sK (sK).P (sK (sK).P (sK (sK (sK).P (sK).P (sK (sK).P (sK (sK).P (sK (.P (.P).P (.P (.P).P (.P).P (sK ("}, {"heading": "2.3. Sparse Reconstruction Using Dictionary Learning", "text": "In contrast to PCA, supercomplete dictionary learning and sparse coding allow modeling of nonlinear low-dimensional multiplicities. Scant modeling assumes that senone posteriors can be generated as a sparse linear combination of senone-space representatives collected in a DSP dictionary. We use online dictionary learning algorithms [23] to learn a supercomplete dictionary for senone sk by using a collection of posterior N training data from senone sk, such as DSP = arg min D, A tN = t1-DNT = t1-DNT-1 (4), in which A = [\u03b1t1.."}, {"heading": "3. EXPERIMENTAL ANALYSIS", "text": "In this section, we examine the effectiveness of low and sparse soft targets to improve the performance of DNN-HMM speech recognition, as well as the importance of better acoustic DNN models to harness information from untranscribed data."}, {"heading": "3.1. Database and Speech Features", "text": "Experiments are performed on the AMI corpus [19], which contains recordings of spontaneous conversations in meeting scenarios. We use recordings of single head microphones (IHM), comprising approximately 67 hours of tension set, 9 hours of development, (dev) set, and 7 hours of test set. 10% of the training data is used for cross-validation during DNN training, while the dev set is used for adjusting regulation parameters \u03c3 and \u03bb. For experiments with non-transcribed additional training data, we use the ICSI meeting corpus [25] and the Librispeech corpus [26]. Data from the ICSI corpus consist of meeting recordings (approximately 70 hours). Librispeech data is read from audiobooks and we use a 100-hour subset thereof. Kaldi toolkit [27] is used for the formation of DNN HMM systems. All DNs have 9 frames + 4 contextual dimensions and 3S."}, {"heading": "3.2. Baseline DNN-HMM using Hard and Soft Targets", "text": "Our baseline is a hybrid DNN-HMM system trained with forced aligned targets (IT setup in [29]). WHO with baseline DNN is 32.4% on AMI test set. Another baseline is a DNN trained with unimproved soft targets from the baseline. This system yields a WHO of 32.0%. All soft target-based DNNs are randomly initialized and trained using cross entropy loss backpropagation."}, {"heading": "3.3. Generation of Low-rank and Sparse Soft Targets", "text": "We group DNN forward pass senone probabilities for the training data into class-specific senone matrices. To do this, senone labels from the ground truth based on GMM-HMM hard alignments are used. Each matrix is limited to having N = 104 vectors of K = 4007 senone probabilities to facilitate the calculation of major components and sparse dictionary learning. We found that the average rank of senone matrices, defined as the number of singular values required to obtain 95% variability, is 44. Dictionaries of size 500 columns were learned for each senone, making them nearly 10 times overcomplete. The procedure, as shown in Fig. 2, is implemented to generate low-grade and sparse soft targets. We also encountered memory problems, while large matrices of the senone probabilities for all training and cross-validation data are stored almost 10 times over. The procedure, as illustrated in Fig. 2, is used to generate low-grade and sparse soft targets. We also encountered memory problems, while large matrices of the senone probabilities for all training and cross-validation data are stored almost 10 times over. The method, as shown in Fig. 2, is implemented to generate low-grade and sparse soft targets, is used to generate low-grade and sparse soft targets. We also encountered memory problems, while large matrices of the senone probabilities for all training and cross-validation data are stored almost 10 times over."}, {"heading": "3.4. DNN-HMM Speech Recognition", "text": "Speech recognition using DNNs trained with the new soft targets from the reconstruction of low and sparse ranks is compared in Table 1. System-0 is the hard base target based on DNN. System-1 is based on the monitored improvement of soft outputs obtained from System-0 on AMI training data, as shown in Fig. 2. As expected, the soft target training delivers lower WHO than the hard base targets. We see that both PCA and sparse reconstruction lead to more accurate acoustic modeling, where a sparse reconstruction achieves an absolute reduction of the value by 0.8%. Sparse reconstruction works better than a low-level reconstruction for ASR. This may be due to the higher accuracy of the sparse model in characterizing non-linear senon subspaces [8]. In contrast to previous work [9, 10] where two phases of the DNN forward projection directly require a low-dimensional DNN, and a low-dimensional space is estimated here."}, {"heading": "3.5. Training with Untranscribed Data", "text": "This year, it has reached the stage where it will be able to take the lead."}, {"heading": "4. CONCLUSIONS AND FUTURE DIRECTIONS", "text": "We presented a novel approach to enhancing DNN acoustic model training using low and sparse soft targets. PCA and sparse encoding were used to identify senone subspaces and increase senone probabilities through low-dimensional reconstruction. PCA reconstruction of low ranks is based on the existence of self-posteriors that capture the local dynamics of senone subspaces. Although sparse reconstruction is more effective in achieving reliable soft targets when transcribed data is provided, low ranks reconstruction using non-transcribed data can be turned out to be generalizable for extra-domain specific data. DNN, trained in low-grade reconstruction, achieves a relative 4.6% reduction in water when transcribed data is provided, while DNN, trained with non-improved soft targets, does not draw additional information from additional data. Intrinsic posteriors can be improved with more robust modelers A [30] and sensoring of PCones."}, {"heading": "5. ACKNOWLEDGMENTS", "text": "Research leading to these results was funded by the SNSF project \"Parsimonious Hierarchical Automatic Speech Recognition (PHASER)\" with funding number 200021-153507."}, {"heading": "6. REFERENCES", "text": "[1] Geoffrey Hinton, Li Deng, Dong Yu, George E Dahl, Abdelrahman Mohamed, Navdeep Jaitly, Andrew Senior, Vincent Vanhoucke, Patrick Nguyen, Tara N Sainath Li, et al. [2] Steve J Young, Julian J Odell, and Philip C Woodland, \"Treebased state tying for high accuracy, i.S. Bourralings,\" in Proceedings of the Workshop on Human Language Technology, pp. 82-97, 2012. [3] Navdeep Jaitly, Vincent Vanhoucke, and Geoffrey Hinton, \"Autoregressive product of multi-frame predictions can improve the accuracy of hybrid models,\" 2014. [4] A. Heigold M. Bacani H., H. Bacao H. Gali."}], "references": [{"title": "Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups", "author": ["Geoffrey Hinton", "Li Deng", "Dong Yu", "George E Dahl", "Abdelrahman Mohamed", "Navdeep Jaitly", "Andrew Senior", "Vincent Vanhoucke", "Patrick Nguyen", "Tara N Sainath"], "venue": "IEEE Signal Processing Magazine, vol. 29, no. 6, pp. 82\u201397, 2012.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2012}, {"title": "Treebased state tying for high accuracy acoustic modelling", "author": ["Steve J Young", "Julian J Odell", "Philip C Woodland"], "venue": "Proceedings of the workshop on Human Language Technology. Association for Computational Linguistics, 1994.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1994}, {"title": "Autoregressive product of multi-frame predictions can improve the accuracy of hybrid models", "author": ["Navdeep Jaitly", "Vincent Vanhoucke", "Geoffrey Hinton"], "venue": "2014.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2014}, {"title": "Gmm-free dnn acoustic model training", "author": ["A. Senior", "G. Heigold", "M. Bacchiani", "H. Liao"], "venue": "IEEE ICASSP, 2014.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2014}, {"title": "REMAP: Recursive Estimation and Maximization of a Posteriori Probabilities; Application to Transition-based Connectionist Speech Recognition, ICSI", "author": ["Herve Bourlard", "Yochai Konig", "Nelson Morgan"], "venue": "Technical Report TR-94-064,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1994}, {"title": "Switching dynamic system models for speech articulation and acoustics", "author": ["Li Deng"], "venue": "Mathematical Foundations of Speech and Language Processing, pp. 115\u2013133. Springer New York, 2004.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2004}, {"title": "Speech production knowledge in automatic speech recognition", "author": ["Simon King", "Joe Frankel", "Karen Livescu", "Erik McDermott", "Korin Richmond", "Mirjam Wester"], "venue": "The Journal of the Acoustical Society of America, 2007.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2007}, {"title": "Sparse modeling of neural network posterior probabilities for exemplar-based speech recognition", "author": ["Pranay Dighe", "Afsaneh Asaei", "Herv\u00e9 Bourlard"], "venue": "Speech Communication, 2015.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "Exploiting low-dimensional structures to enhance dnn based acoustic modeling in speech recognition", "author": ["Pranay Dighe", "Gil Luyet", "Afsaneh Asaei", "Herve Bourlard"], "venue": "IEEE ICASSP, 2016.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2016}, {"title": "Low-rank representation of nearest neighbor phone posterior probabilities to enhance dnn acoustic modeling", "author": ["Gil Luyet", "Pranay Dighe", "Afsaneh Asaei", "Herv\u00e9 Bourlard"], "venue": "Interspeech, 2016.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2016}, {"title": "Restructuring of deep neural network acoustic models with singular value decomposition", "author": ["Jian Xue", "Jinyu Li", "Yifan Gong"], "venue": "INTERSPEECH, 2013.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2013}, {"title": "Low-rank matrix factorization for deep neural network training with high-dimensional output targets", "author": ["Tara N Sainath", "Brian Kingsbury", "Vikas Sindhwani", "Ebru Arisoy", "Bhuvana Ramabhadran"], "venue": "IEEE ICASSP, 2013.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2013}, {"title": "Exploiting sparseness in deep neural networks for large vocabulary speech recognition", "author": ["Dong Yu", "Frank Seide", "Gang Li", "Li Deng"], "venue": "IEEE ICASSP, 2012.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2012}, {"title": "Neuron sparseness versus connection sparseness in deep neural network for large vocabulary speech recognition", "author": ["Jian Kang", "Cheng Lu", "Meng Cai", "Wei-Qiang Zhang", "Jia Liu"], "venue": "ICASSP, April 2015, pp. 4954\u20134958.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2015}, {"title": "Distilling the knowledge in a neural network", "author": ["Geoffrey Hinton", "Oriol Vinyals", "Jeff Dean"], "venue": "arXiv preprint arXiv:1503.02531, 2015.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2015}, {"title": "Transferring knowledge from a rnn to a dnn", "author": ["William Chan", "Nan Rosemary Ke", "Ian Lane"], "venue": "Interspeech, 2015.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}, {"title": "Wise teachers train better dnn acoustic models", "author": ["Ryan Price", "Ken-ichi Iso", "Koichi Shinoda"], "venue": "EURASIP Journal on Audio, Speech, and Music Processing, , no. 1, pp. 1\u201319, 2016.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2016}, {"title": "A sparse plus low-rank exponential language model for limited resource scenarios", "author": ["Brian Hutchinson", "Mari Ostendorf", "Maryam Fazel"], "venue": "IEEE Transactions on Audio, Speech, and Language Processing, vol. 23, no. 3, pp. 494\u2013504, 2015.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2015}, {"title": "The ami meeting corpus", "author": ["Iain McCowan", "Jean Carletta", "W Kraaij", "S Ashby", "S Bourban", "M Flynn", "M Guillemot", "T Hain", "J Kadlec", "V Karaiskos"], "venue": "Proceedings of the 5th International Conference on Methods and Techniques in Behavioral Research, 2005, vol. 88.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2005}, {"title": "Don\u2019t multiply lightly: Quantifying problems with the acoustic model assumptions in speech recognition", "author": ["Dan Gillick", "Larry Gillick", "Steven Wegmann"], "venue": "IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU), 2011.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2011}, {"title": "A tutorial on principal component analysis", "author": ["Jonathon Shlens"], "venue": "arXiv preprint arXiv:1404.1100, 2014.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2014}, {"title": "Low-dimensional procedure for the characterization of human faces", "author": ["L. Sirovich", "M. Kirby"], "venue": "J. Opt. Soc. Am. A, pp. 519\u2013524, 1987.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 1987}, {"title": "Online learning for matrix factorization and sparse coding", "author": ["Julien Mairal", "Francis Bach", "Jean Ponce", "Guillermo Sapiro"], "venue": "Journal of Machine Learning Research (JMLR), vol. 11, pp. 19\u201360, 2010.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2010}, {"title": "Regression shrinkage and selection via the lasso", "author": ["Robert Tibshirani"], "venue": "Journal of the Royal Statistical Society. Series B (Methodological), pp. 267\u2013288, 1996.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 1996}, {"title": "The icsi meeting corpus", "author": ["Adam Janin", "Don Baron", "Jane Edwards", "Dan Ellis", "David Gelbart", "Nelson Morgan", "Barbara Peskin", "Thilo Pfau", "Elizabeth Shriberg", "Andreas Stolcke"], "venue": "IEEE ICASSP, 2003.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2003}, {"title": "Librispeech: an asr corpus based on public domain audio books", "author": ["Vassil Panayotov", "Guoguo Chen", "Daniel Povey", "Sanjeev Khudanpur"], "venue": "IEEE ICASSP, 2015.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2015}, {"title": "The kaldi speech recognition toolkit", "author": ["Daniel Povey", "Arnab Ghoshal", "Gilles Boulianne", "Luk\u00e1\u0161 Burget", "Ond\u0159ej Glembek", "Nagendra Goel", "Mirko Hannemann", "Petr Motl\u0131\u0301\u010dek", "Yanmin Qian", "Petr Schwarz"], "venue": "2011.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2011}, {"title": "Sparse modeling for image and vision processing", "author": ["Julien Mairal", "Francis Bach", "Jean Ponce"], "venue": "arXiv preprint arXiv:1411.3230, 2014.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning feature mapping using deep neural network bottleneck features for distant large vocabulary speech recognition", "author": ["I. Himawan", "P. Motlicek", "D. Imseng", "B. Potard", "N. Kim", "J. Lee"], "venue": "IEEE ICASSP, 2015, pp. 4540\u20134544.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2015}, {"title": "Robust recovery of subspace structures by low-rank representation", "author": ["Guangcan Liu", "Zhouchen Lin", "Shuicheng Yan", "Ju Sun", "Yong Yu", "Yi Ma"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, , no. 99, pp. 1\u20131, 2013.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2013}, {"title": "Sparse principal component analysis", "author": ["Hui Zou", "Trevor Hastie", "Robert Tibshirani"], "venue": "Journal of computational and graphical statistics, vol. 15, no. 2, pp. 265\u2013286, 2006.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2006}, {"title": "Unsupervised cross-lingual knowledge transfer in dnn-based lvcsr", "author": ["Pawel Swietojanski", "Arnab Ghoshal", "Steve Renals"], "venue": "IEEE Spoken Language Technology Workshop (SLT), 2012.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2012}], "referenceMentions": [{"referenceID": 0, "context": "DNN based acoustic models have been state-of-the-art for automatic speech recognition over the past few years [1].", "startOffset": 110, "endOffset": 113}, {"referenceID": 1, "context": "While DNN input consists of multiple frames of acoustic features, the target output is obtained from a frame level GMM-HMM forced alignment corresponding to the context dependent tied triphone states or senones [2].", "startOffset": 211, "endOffset": 214}, {"referenceID": 2, "context": "This procedure results in inefficiency in DNN acoustic modeling [3, 4].", "startOffset": 64, "endOffset": 70}, {"referenceID": 3, "context": "This procedure results in inefficiency in DNN acoustic modeling [3, 4].", "startOffset": 64, "endOffset": 70}, {"referenceID": 4, "context": "Earlier studies on optimal training of a neural network for HMM decoding provides rigorous theoretical analysis that supports this idea [5].", "startOffset": 136, "endOffset": 139}, {"referenceID": 5, "context": "phones, senones) on low-dimensional non-linear manifolds [6, 7].", "startOffset": 57, "endOffset": 63}, {"referenceID": 6, "context": "phones, senones) on low-dimensional non-linear manifolds [6, 7].", "startOffset": 57, "endOffset": 63}, {"referenceID": 7, "context": "In the context of DNN acoustic modeling, low-dimensional structures are exhibited in the space of DNN senone posteriors [8].", "startOffset": 120, "endOffset": 123}, {"referenceID": 8, "context": "Low-rank and sparse representations are found promising to characterize senone-specific subspaces [9, 10].", "startOffset": 98, "endOffset": 105}, {"referenceID": 9, "context": "Low-rank and sparse representations are found promising to characterize senone-specific subspaces [9, 10].", "startOffset": 98, "endOffset": 105}, {"referenceID": 10, "context": "In [11, 12] low-rank decomposition of the neural network\u2019s weight matrices enables reduction in DNN complexity and memory footprint.", "startOffset": 3, "endOffset": 11}, {"referenceID": 11, "context": "In [11, 12] low-rank decomposition of the neural network\u2019s weight matrices enables reduction in DNN complexity and memory footprint.", "startOffset": 3, "endOffset": 11}, {"referenceID": 12, "context": "Similar goals have been achieved by exploiting sparse connections [13] and sparse activations [14] in hidden layers of DNN.", "startOffset": 66, "endOffset": 70}, {"referenceID": 13, "context": "Similar goals have been achieved by exploiting sparse connections [13] and sparse activations [14] in hidden layers of DNN.", "startOffset": 94, "endOffset": 98}, {"referenceID": 14, "context": "In another line of research, soft targets based DNN training has been found effective for enabling model compression [15, 16] and knowledge transfer from an accurate complex model to a smaller network [17].", "startOffset": 117, "endOffset": 125}, {"referenceID": 15, "context": "In another line of research, soft targets based DNN training has been found effective for enabling model compression [15, 16] and knowledge transfer from an accurate complex model to a smaller network [17].", "startOffset": 117, "endOffset": 125}, {"referenceID": 16, "context": "In another line of research, soft targets based DNN training has been found effective for enabling model compression [15, 16] and knowledge transfer from an accurate complex model to a smaller network [17].", "startOffset": 201, "endOffset": 205}, {"referenceID": 17, "context": "Strength of PCA lies in capturing the linear regularities in the data [18] whereas an over-complete dictionary used for sparse coding learns to model the non-linear space as a union of lowdimensional subspaces.", "startOffset": 70, "endOffset": 74}, {"referenceID": 8, "context": "Dictionary based sparse reconstruction also reduces the rank of the senone posterior space [9].", "startOffset": 91, "endOffset": 94}, {"referenceID": 18, "context": "Experimental evaluations are conducted on AMI corpus [19], a collection of recordings of multi-party meetings for large vocabulary speech recognition.", "startOffset": 53, "endOffset": 57}, {"referenceID": 14, "context": "Earlier works on distillation of the DNN knowledge show the potential of soft targets for model compression and the sub-optimal nature of the hard alignments [15, 20].", "startOffset": 158, "endOffset": 166}, {"referenceID": 19, "context": "Earlier works on distillation of the DNN knowledge show the potential of soft targets for model compression and the sub-optimal nature of the hard alignments [15, 20].", "startOffset": 158, "endOffset": 166}, {"referenceID": 19, "context": "Contextual senone dependencies arising in soft targets can be attributed to the ambiguities due to phonetic transitions [20].", "startOffset": 120, "endOffset": 124}, {"referenceID": 1, "context": "Furthermore, the procedure of senone extraction leads to acoustic correlations among multiple classes corresponding to the same phone-HMM states [2], as they all share the same root in the decision tree (Fig.", "startOffset": 145, "endOffset": 148}, {"referenceID": 8, "context": "In other words, class-specific senones lie in low-dimensional subspaces with a dimension higher than unity [9], that violates the principal assumption of binary hard targets.", "startOffset": 107, "endOffset": 110}, {"referenceID": 9, "context": "Our previous work demonstrates that the erroneous estimations can be separated using low-rank or sparse representations [10, 9].", "startOffset": 120, "endOffset": 127}, {"referenceID": 8, "context": "Our previous work demonstrates that the erroneous estimations can be separated using low-rank or sparse representations [10, 9].", "startOffset": 120, "endOffset": 127}, {"referenceID": 20, "context": "Principal components of the senone space are obtained via eigenvector decomposition [21] of covariance matrix of M .", "startOffset": 84, "endOffset": 88}, {"referenceID": 21, "context": "The eigenvectors stored in the low-rank projection Pl are referred to as \u201ceigenposteriors\u201d of the senone space (in the same spirit as eigenfaces are defined for low-dimensional modeling of human faces [22]).", "startOffset": 201, "endOffset": 205}, {"referenceID": 22, "context": "We use online dictionary learning algorithm [23] to learn an over-complete dictionary for senone sk using a collection of N training data posteriors of senone sk, such that", "startOffset": 44, "endOffset": 48}, {"referenceID": 23, "context": "2) of senone posteriors is thus obtained by first estimating the sparse representation [24] as", "startOffset": 87, "endOffset": 91}, {"referenceID": 8, "context": "Sparse reconstructed senone posteriors have been previously found to be more accurate acoustic models for DNN-HMM speech recognition [9].", "startOffset": 133, "endOffset": 136}, {"referenceID": 18, "context": "Experiments are conducted on AMI corpus [19] which contains recordings of spontaneous conversations in meeting scenarios.", "startOffset": 40, "endOffset": 44}, {"referenceID": 24, "context": "For experiments using untranscribed additional training data, we use ICSI meeting corpus [25] and Librispeech corpus [26].", "startOffset": 89, "endOffset": 93}, {"referenceID": 25, "context": "For experiments using untranscribed additional training data, we use ICSI meeting corpus [25] and Librispeech corpus [26].", "startOffset": 117, "endOffset": 121}, {"referenceID": 26, "context": "Kaldi toolkit [27] is used for training DNN-HMM systems.", "startOffset": 14, "endOffset": 18}, {"referenceID": 27, "context": "For dictionary learning and sprase coding, SPAMS toolbox [28] is used.", "startOffset": 57, "endOffset": 61}, {"referenceID": 28, "context": "Our baseline is a hybrid DNN-HMM system trained using forced aligned targets (IHM setup in [29]).", "startOffset": 91, "endOffset": 95}, {"referenceID": 15, "context": "It requires enormous amounts of storage space (similar to [16]).", "startOffset": 58, "endOffset": 62}, {"referenceID": 7, "context": "It can be due to the higher accuracy of sparse model in characterizing the non-linear senone subspaces [8].", "startOffset": 103, "endOffset": 106}, {"referenceID": 8, "context": "Unlike previous works [9, 10] which required two stages of DNN forward pass and explicit low-dimensional projection, a single DNN is learned here that estimates the probabilities directly on a lowdimensional space.", "startOffset": 22, "endOffset": 29}, {"referenceID": 9, "context": "Unlike previous works [9, 10] which required two stages of DNN forward pass and explicit low-dimensional projection, a single DNN is learned here that estimates the probabilities directly on a lowdimensional space.", "startOffset": 22, "endOffset": 29}, {"referenceID": 14, "context": "This method is reminiscent of the knowledge transfer approach [15, 16] which is typically used for model compression.", "startOffset": 62, "endOffset": 70}, {"referenceID": 15, "context": "This method is reminiscent of the knowledge transfer approach [15, 16] which is typically used for model compression.", "startOffset": 62, "endOffset": 70}, {"referenceID": 29, "context": "Eigenposteriors can be better estimated using robust PCA [30] and sparse PCA [31] for better modeling of senone subspaces.", "startOffset": 57, "endOffset": 61}, {"referenceID": 30, "context": "Eigenposteriors can be better estimated using robust PCA [30] and sparse PCA [31] for better modeling of senone subspaces.", "startOffset": 77, "endOffset": 81}, {"referenceID": 31, "context": "In future, we plan to investigate their usage in cross-lingual knowledge transfer [32].", "startOffset": 82, "endOffset": 86}], "year": 2016, "abstractText": "Conventional deep neural networks (DNN) for speech acoustic modeling rely on Gaussian mixture models (GMM) and hidden Markov model (HMM) to obtain binary class labels as the targets for DNN training. Subword classes in speech recognition systems correspond to context-dependent tied states or senones. The present work addresses some limitations of GMM-HMM senone alignments for DNN training. We hypothesize that the senone probabilities obtained from a DNN trained with binary labels can provide more accurate targets to learn better acoustic models. However, DNN outputs bear inaccuracies which are exhibited as high dimensional unstructured noise, whereas the informative components are structured and lowdimensional. We exploit principle component analysis (PCA) and sparse coding to characterize the senone subspaces. Enhanced probabilities obtained from low-rank and sparse reconstructions are used as soft-targets for DNN acoustic modeling, that also enables training with untranscribed data. Experiments conducted on AMI corpus shows 4.6% relative reduction in word error rate.", "creator": "LaTeX with hyperref package"}}}