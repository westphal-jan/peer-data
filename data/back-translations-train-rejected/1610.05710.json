{"id": "1610.05710", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Oct-2016", "title": "Feasibility Based-Large Margin Nearest Neighbor Metric Learning", "abstract": "In the area of data classification, one of the prominent algorithms is the large margin nearest neighbor (LMNN) approach which is a metric learning to enhance the performance of the popular k-nearest neighbor classifier. In principles, LMNN learns a more efficient metric in the input space by using a linear mapping as the outcome of a convex optimization problem. However, one of the greatest weak points of LMNN is the strong reliance of its optimization paradigm on how the neighboring points are chosen. In this paper, it is mathematically proved for the first time that the regular way of choosing the target points can lead to non-feasible optimization conditions regardless of the number of chosen neighboring points. We present a mathematical approach to categorize the target points into feasible and infeasible exemplars, an also we provide a feasibility measure for preference of the target candidates. In our proposed Feasibility Based-LMNN algorithm, we use the above clue to construct the optimization problem based on the most promising general mapping directions in the input space. Our empirical results shows that via using the proposed FB-LMNN approach the optimization problem will converge in a better optimum point, and therefor leads to better classification on the well-known benchmark datasets.", "histories": [["v1", "Tue, 18 Oct 2016 17:06:26 GMT  (1717kb,D)", "http://arxiv.org/abs/1610.05710v1", "This is the preprint of a submitted conference paper as provided by the authors"]], "COMMENTS": "This is the preprint of a submitted conference paper as provided by the authors", "reviews": [], "SUBJECTS": "cs.DS cs.LG", "authors": ["babak hosseini", "barbara hammer"], "accepted": false, "id": "1610.05710"}, "pdf": {"name": "1610.05710.pdf", "metadata": {"source": "META", "title": "Feasibility Based-Large Margin Nearest Neighbor Metric Learning", "authors": ["Babak Hosseini"], "emails": ["bhammer@techfak.uni-bielefeld.de"], "sections": [{"heading": null, "text": "Keywords: Large margin for manoeuvre with the nearest neighbour, feasibility measure, convex optimisation, K-next neighbour classification."}, {"heading": "1 Introduction", "text": "Metric learning, as a widespread approach to discriminatory data mining, is attributed to the search for a suitable metric for the given data in order to improve the processing of certain data sets and improve classification accuracy. In basic terms, it attempts to remove similar class data closer and further from them. One of the known methods for metric learning is the large margin that the closest neighbor (LMNN) has, which is essentially designed to increase the performance of the k-nearest neighbor classifier [1, 2, 3] by transferring the maximum margin concept of the SVM classifier to the kNN framework. LMNar Xiv: 161 0.05 710v 1 [cs.D S] has been used in many real-world problems such as face recognition [4], motion classification [5] and pedestrian identification [6]."}, {"heading": "2 Large Margin Nearest Neighbor Algorithm", "text": "Wide margin of the nearest posterior algorithm is a metric algorithm of learning ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7"}, {"heading": "3 Feasibility of target neighborhoods", "text": "As explained in Section 2, the first step in the LMNN approach is to select the target neighbor ~ ~ xj for each input ~ xi before solving the optimization problem (3), and the convexity of this optimization problem promises its convergence to a global minimum point. Thus, in many real-world problems, it is possible to find another set of optimization problems where the optimization problem can reach another optimal point leading to a lower objective value. In other words, the global optimization point can be a local optimal point for general optimization."}, {"heading": "4 Feasibility Based Large Margin Nearest Neighbor", "text": "Based on the discussion in the previous section, Fk is composed of (~ xi, ~ xj) which are unfeasible due to the optimization limitation, and we consider them unique conditions for the optimization algorithm. One way to prevent the unfeasible target sentence Fk from affecting the optimization problem (4) is simply to exclude them as a pre-processing step before starting the optimization algorithm. To do this, we check the relevant nearby cheaters ~ xl for each pair (~ xi, ~ xj) and in the case of existing unfeasible triples in combination with (~ xi, ~ xj), they are eliminated from N ki by choosing the next neighbor ~ xi in (3) in Wij = 1 instead, to replace ~ xj with that neighbor."}, {"heading": "4.1 Feasibility Bounds.", "text": "In principle, this strategy is effective against unfeasible cases such as the imaginary situation in Fig. 1, but in real experiments the chances of observing a definitive case of unfeasibility are low, for example, the smallest eigenvalue of Q in theorem 1 may be small but not zero. Nevertheless, we will show that the feasibility concept introduced plays a direct rule in the performance of the LMNN algorithm and must therefore necessarily be taken into account in relation to the smallest eigenvalue of the matrix Q. For example, consider a more realistic 2-D situation such as Fig. 2, in which the ~ x4 is placed an imposter between ~ xi and ~ x1 and so close to its connecting line, but has a very small positive value according to its smallest eigenvalues of the matrix Q."}, {"heading": "4.2 Feasibility Bound Weights in the Optimization.", "text": "In light of LMNN's global solution to the optimization problem, we are looking for an authoritative solution that is the global optimum in terms of (4). So, although the first part of the objective is necessary to merge the optimization algorithms to an optimal point, in the end an ideal global optimum is considered in terms of the next target values. (4) If the problem has a linear solution for the kNN classifier, the intersection will not be empty. In other words, L \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s.\" s \"s\" s \"s\" s \"s\" s \"s\" s \"\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s. \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s."}, {"heading": "5 Experiments", "text": "In this section we will examine the performance of the described FB-LMNN algorithm. We will implement our algorithm on some real data with different structures and also on synthetic data to better represent the effective point of the approach. We will use the 10-fold cross-validation to divide the data into test and traction batches. To select parameter k as the number of closest neighbours, we will use cross-validation and fix the parameter m = 0, which usually results in good performance for practical applications. A summary of the data sets and the chosen meta parameter for the LMNN approach is shown in Table 1. We will first compare our FB-LMNN algorithm with kNN using Euclidean distance, and as a second approach we will use the original LMNN method introduced by [3], and we have also tried the regular LMNN algorithm in the multipass method as [13] to improve its performance."}, {"heading": "5.1 Synthetic Data", "text": "In order to better demonstrate the effectiveness of the approach, we have created the so-called zebra-stripe toy data sets, which are also used by the original and multiple versions of the LMNN algorithm. [3] As can be seen in the figure, the first suggestion is to solve the problem by focusing on the local distances between the data. [4] Therefore, the closest neighbor classifiers and the ideal choice should be based on their principles. The main difference between our data sets and those used in [3] and [5] is the ratio of local distances to the closest class data."}, {"heading": "5.2 Real Data", "text": "The selected data sets relate to different classification problems in different areas and also with different structures. The result of using kNN, multi-pass LMNN, SVM and feasibility-based LMNN is presented in Table 2. According to the results, the difference between regular LMNN and the FB-LMNN is subtle for some data sets such as Car Evaluation and Iris, but the FB-LMNN performs better for some other data sets than the multi-pass LMNN, which shows that taking into account the shape of the feasibility sets for N k was effective, or in other words, there were similarities between the data of the same class based on this property. Compared with SVM classifiers, SVM achieves a better result for few data sets than the LMNN approaches, which are based on the structure of the data that was more suitable for this category."}, {"heading": "6 Conclusion", "text": "Although the LMNN optimization problem converges to a global minimum due to its convexity, this point is optimal for the algorithm with respect to the selected target neighbors. We presented examples where the typical way of changing the distance in the neighborhood will not address this problem. We determined that, regardless of the distance of the same class data, there may be closed but impracticable neighboring points that can mislead the optimization path if we take them as target examples. We demonstrated mathematically how to detect and measure the impracticability of the targets and reorder the optimization problem to take into account the above conditions during its process. As a result, we proposed the FB-LMNN, which represents an alternating optimization framework that attempts to take into account directions in metric space where there is a higher problem of taking into account the above conditions during its process."}, {"heading": "7 Acknowledgment", "text": "This research was supported by the Cluster of Excellence for Cognitive Interaction Technology \"CITEC\" (EXC 277) of Bielefeld University, which was funded by the"}, {"heading": "B. Cancer 20 30 256 5 3", "text": "German Research Foundation (DFG)."}], "references": [{"title": "A survey on metric learning for feature vectors and structured data", "author": ["A. Bellet", "A. Habrard", "M. Sebban"], "venue": "arXiv preprint arXiv:1306.6709, 2013.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2013}, {"title": "Metric learning: A survey", "author": ["B. Kulis"], "venue": "Foundations and Trends in Machine Learning, vol. 5, no. 4, pp. 287\u2013364, 2012.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2012}, {"title": "Distance metric learning for large margin nearest neighbor classification", "author": ["K.Q. Weinberger", "L.K. Saul"], "venue": "Journal of Machine Learning Research, vol. 10, pp. 207\u2013244, 2009. [Online]. Available: http://doi.acm.org/10.1145/1577069.1577078", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2009}, {"title": "Is that you? metric learning approaches for face identification", "author": ["M. Guillaumin", "J. Verbeek", "C. Schmid"], "venue": "2009 IEEE 12th International Conference on Computer Vision. IEEE, 2009, pp. 498\u2013505.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2009}, {"title": "Efficient metric learning for the analysis of motion data", "author": ["B. Hosseini", "B. Hammer"], "venue": "Data Science and Advanced Analytics (DSAA), 2015. 36678 2015. IEEE International Conference on, Oct 2015, pp. 1\u201310.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2015}, {"title": "Pedestrian recognition with a learned metric", "author": ["M. Dikmen", "E. Akbas", "T.S. Huang", "N. Ahuja"], "venue": "Asian conference on Computer vision. Springer, 2010, pp. 501\u2013512. 14  Table 2: Classification accuracy(%) for the selected datasets and the chosen approaches Dataset kNN MP-LMNN FB-LMNN SVM Zebra 21.31 23.51 72.21 50.82 Wine 73.4 96.91 98.77 77.23 Balance 87.42 94.03 96.08 97.5 B. Cancer 94.66 96.68 97.07 78.49 Car Eval. 92.57 98.32 98.4 60.08 Tic-Tac-Toe 87.42 97.66 98.13 85 Hepatitis 84.16 84.46 90 79.11 iris 92.64 93.24 94.12 97.47", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2010}, {"title": "Analysis of flow cytometry data by matrix relevance learning vector quantization", "author": ["M. Biehl", "K. Bunte", "P. Schneider"], "venue": "PLoS One, vol. 8, no. 3, p. e59401, 2013.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2013}, {"title": "Classification in high-dimensional spectral data: Accuracy vs. interpretability vs. model size", "author": ["A. Backhaus", "U. Seiffert"], "venue": "Neurocomputing, vol. 131, pp. 15\u201322, 2014.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2014}, {"title": "Efficiently learning a distance metric for large margin nearest neighbor classification.", "author": ["K. Park", "C. Shen", "Z. Hao", "J. Kim"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2011}, {"title": "Distance metric learning with eigenvalue optimization", "author": ["Y. Ying", "P. Li"], "venue": "Journal of Machine Learning Research, vol. 13, no. Jan, pp. 1\u201326, 2012.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2012}, {"title": "Large margin multi-task metric learning", "author": ["S. Parameswaran", "K.Q. Weinberger"], "venue": "Advances in neural information processing systems, 2010, pp. 1867\u20131875.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2010}, {"title": "Hierarchical large margin nearest neighbor classification", "author": ["Q. Chen", "S. Sun"], "venue": "Pattern Recognition (ICPR), 2010 20th International Conference on. IEEE, 2010, pp. 906\u2013909.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2010}, {"title": "Convergence of multi-pass large margin nearest neighbor metric learning", "author": ["C. G\u00f6pfert", "B. Paassen", "B. Hammer"], "venue": "International Conference on Artificial Neural Networks. Springer, 2016, pp. 510\u2013517.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2016}, {"title": "Eigenvalue inequalities for matrix product", "author": ["F. Zhang", "Q. Zhang"], "venue": "IEEE Transactions on Automatic Control, vol. 51, no. 9, p. 1506, 2006.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2006}, {"title": "Trace bounds on the solution of the algebraic matrix riccati and lyapunov equation", "author": ["S.-D. Wang", "T.-S. Kuo", "C.-F. Hsu"], "venue": "IEEE Transactions on Automatic Control, vol. 31, no. 7, pp. 654\u2013656, 1986.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1986}, {"title": "UCI machine learning repository", "author": ["M. Lichman"], "venue": "2013. [Online]. Available: http://archive.ics.uci.edu/ml 15", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2013}], "referenceMentions": [{"referenceID": 0, "context": "One of the well-known methods for metric learning is the large margin nearest neighbor (LMNN) which is fundamentally destined to increase the performance of k-nearest neighbor classifier[1, 2, 3], by transferring the maximum margin concept of SVM classifier to the kNN framework.", "startOffset": 186, "endOffset": 195}, {"referenceID": 1, "context": "One of the well-known methods for metric learning is the large margin nearest neighbor (LMNN) which is fundamentally destined to increase the performance of k-nearest neighbor classifier[1, 2, 3], by transferring the maximum margin concept of SVM classifier to the kNN framework.", "startOffset": 186, "endOffset": 195}, {"referenceID": 2, "context": "One of the well-known methods for metric learning is the large margin nearest neighbor (LMNN) which is fundamentally destined to increase the performance of k-nearest neighbor classifier[1, 2, 3], by transferring the maximum margin concept of SVM classifier to the kNN framework.", "startOffset": 186, "endOffset": 195}, {"referenceID": 3, "context": "was used in many real problems such as face recognition [4], motion classification [5] and pedestrian identification [6].", "startOffset": 56, "endOffset": 59}, {"referenceID": 4, "context": "was used in many real problems such as face recognition [4], motion classification [5] and pedestrian identification [6].", "startOffset": 83, "endOffset": 86}, {"referenceID": 5, "context": "was used in many real problems such as face recognition [4], motion classification [5] and pedestrian identification [6].", "startOffset": 117, "endOffset": 120}, {"referenceID": 6, "context": "Furthermore, this adaptation of metric in this manner also enhances the model interpretability along with increasing the accuracy, and in some domain it is used for easier visualization of data [7, 8].", "startOffset": 194, "endOffset": 200}, {"referenceID": 7, "context": "Furthermore, this adaptation of metric in this manner also enhances the model interpretability along with increasing the accuracy, and in some domain it is used for easier visualization of data [7, 8].", "startOffset": 194, "endOffset": 200}, {"referenceID": 8, "context": "There has been several attempts to improve the performance of the original LMNN approach, such as complexity reduction of the optmization framework [9], eigenvalue based optimization [10], its extension to the multi-taksing problem [11] and a hierarchical prepossessing of data [12] all of which have interesting results.", "startOffset": 148, "endOffset": 151}, {"referenceID": 9, "context": "There has been several attempts to improve the performance of the original LMNN approach, such as complexity reduction of the optmization framework [9], eigenvalue based optimization [10], its extension to the multi-taksing problem [11] and a hierarchical prepossessing of data [12] all of which have interesting results.", "startOffset": 183, "endOffset": 187}, {"referenceID": 10, "context": "There has been several attempts to improve the performance of the original LMNN approach, such as complexity reduction of the optmization framework [9], eigenvalue based optimization [10], its extension to the multi-taksing problem [11] and a hierarchical prepossessing of data [12] all of which have interesting results.", "startOffset": 232, "endOffset": 236}, {"referenceID": 11, "context": "There has been several attempts to improve the performance of the original LMNN approach, such as complexity reduction of the optmization framework [9], eigenvalue based optimization [10], its extension to the multi-taksing problem [11] and a hierarchical prepossessing of data [12] all of which have interesting results.", "startOffset": 278, "endOffset": 282}, {"referenceID": 2, "context": "According to [3] the optimization problem is a category of semidefinite programming as:", "startOffset": 13, "endOffset": 16}, {"referenceID": 2, "context": "One way to get close to the global optimum of problem is to use the multiple pass technique suggested by [3], which tries to update the targets neighbors N k based on the learned metric L after the convergence of the optimization problem (3).", "startOffset": 105, "endOffset": 108}, {"referenceID": 12, "context": "It is proved in [13] that in each iteration t of multiple pass LMNN, the optimization problem will converge to a better or equal optimal pointL comparing to L.", "startOffset": 16, "endOffset": 20}, {"referenceID": 13, "context": "Also it is proven in [14] that for our set of {Q,M} \u03bbmin(Q)\u03bbmax(M) \u2264 \u03bbmin(QM)", "startOffset": 21, "endOffset": 25}, {"referenceID": 14, "context": "Also another supporting fact to the above claim is proved in [15] as \u03bbmin(Q) Tr(M) \u2264 Tr(QM) Which imposes the bound on all eigenvalues of M so if \u03bbmin(Q) becomes really small it forces \u2016\u03bb(M)\u20161 to shrink in size.", "startOffset": 61, "endOffset": 65}, {"referenceID": 2, "context": "1 We compare our FB-LMNN algorithm first with kNN using Euclidean distance, and as the second approach we used the original LMNN method introduced by [3], also we tried the regular LMNN algorithm in a multi-pass fashion as [13] in order to improve its performance.", "startOffset": 150, "endOffset": 153}, {"referenceID": 12, "context": "1 We compare our FB-LMNN algorithm first with kNN using Euclidean distance, and as the second approach we used the original LMNN method introduced by [3], also we tried the regular LMNN algorithm in a multi-pass fashion as [13] in order to improve its performance.", "startOffset": 223, "endOffset": 227}, {"referenceID": 2, "context": "To better demonstrate the effectiveness of the approach, we manually generated the so called zebra stripe toy data set which was also tried by the original and multi-pass versions of LMNN algorithm by [3] and [13].", "startOffset": 201, "endOffset": 204}, {"referenceID": 12, "context": "To better demonstrate the effectiveness of the approach, we manually generated the so called zebra stripe toy data set which was also tried by the original and multi-pass versions of LMNN algorithm by [3] and [13].", "startOffset": 209, "endOffset": 213}, {"referenceID": 2, "context": "However the main difference of our dataset and the ones used in [3] and [13], is the ratio of the local distance to the nearest class data comparing to the distance to the nearest different class, similar to the example data in Fig.", "startOffset": 64, "endOffset": 67}, {"referenceID": 12, "context": "However the main difference of our dataset and the ones used in [3] and [13], is the ratio of the local distance to the nearest class data comparing to the distance to the nearest different class, similar to the example data in Fig.", "startOffset": 72, "endOffset": 76}, {"referenceID": 15, "context": "The real data sets are all chosen from the UCI repository library [16] and the results are reproducible easily.", "startOffset": 66, "endOffset": 70}], "year": 2016, "abstractText": "In the area of data classification, one of the prominent algorithms is the large margin nearest neighbor (LMNN) approach which is a metric learning to enhance the performance of the popular k-nearest neighbor classifier. In principles, LMNN learns a more efficient metric in the input space by using a linear mapping as the outcome of a convex optimization problem. However, one of the greatest weak points of LMNN is the strong reliance of its optimization paradigm on how the neighboring points are chosen. In this paper, it is mathematically proved for the first time that the regular way of choosing the target points can lead to non-feasible optimization conditions regardless of the number of chosen neighboring points. We present a mathematical approach to categorize the target points into feasible and infeasible exemplars, an also we provide a feasibility measure for preference of the target candidates. In our proposed Feasibility Based-LMNN algorithm, we use the above clue to construct the optimization problem based on the most promising general mapping directions in the input space. Our empirical results shows that via using the proposed FB-LMNN approach the optimization problem will converge in a better optimum point, and therefor leads to better classification on the well-known benchmark datasets", "creator": "XeLaTeX"}}}