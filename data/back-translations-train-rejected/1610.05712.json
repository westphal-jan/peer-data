{"id": "1610.05712", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Oct-2016", "title": "Fast L1-NMF for Multiple Parametric Model Estimation", "abstract": "In this work we introduce a comprehensive algorithmic pipeline for multiple parametric model estimation. The proposed approach analyzes the information produced by a random sampling algorithm (e.g., RANSAC) from a machine learning/optimization perspective, using a \\textit{parameterless} biclustering algorithm based on L1 nonnegative matrix factorization (L1-NMF). The proposed framework exploits consistent patterns that naturally arise during the RANSAC execution, while explicitly avoiding spurious inconsistencies. Contrarily to the main trends in the literature, the proposed technique does not impose non-intersecting parametric models. A new accelerated algorithm to compute L1-NMFs allows to handle medium-sized problems faster while also extending the usability of the algorithm to much larger datasets. This accelerated algorithm has applications in any other context where an L1-NMF is needed, beyond the biclustering approach to parameter estimation here addressed. We accompany the algorithmic presentation with theoretical foundations and numerous and diverse examples.", "histories": [["v1", "Tue, 18 Oct 2016 17:20:38 GMT  (8404kb,D)", "https://arxiv.org/abs/1610.05712v1", null], ["v2", "Fri, 11 Nov 2016 15:54:14 GMT  (8404kb,D)", "http://arxiv.org/abs/1610.05712v2", null]], "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["mariano tepper", "guillermo sapiro"], "accepted": false, "id": "1610.05712"}, "pdf": {"name": "1610.05712.pdf", "metadata": {"source": "CRF", "title": "Fast L1-NMF for Multiple Parametric Model Estimation\u2217", "authors": ["Mariano Tepper", "Guillermo Sapiro"], "emails": ["mariano.tepper@duke.edu", "guillermo.sapiro@duke.edu"], "sections": [{"heading": null, "text": "Keywords: multiple parametric model estimation, robust fit, RANSAC, non-negative matrix factorization, biclustering, L1 optimization"}, {"heading": "1. Introduction", "text": "This is a common problem in, for example, computer vision, which has been raised in a wide range of applications such as finding lines / circles / ellipses in images, homography estimation in stereo vision, motion estimation and segmentation, and geometric analysis of 3D dot clouds. Finding a single instance of a parametric model in a dataset is a robust adjustment problem, which in itself is difficult; the difficulties are exacerbated when multiple instances in the datasets may be present due to the inevitable emergence of pseudo-outliers (data points that belong to one structure and are usually outliers to another structure). Thus, we face the problem of simultaneously estimating model parameters and assigning data points to the estimated models. These two problems are intrinsic."}, {"heading": "2. Random Sample Ensemble", "text": "Let X be a matrix. In the following, (X) ij, (X): j, (X) i: denote the (i, j) th entry of X, the j-th column of X, and the i-th line of X, correspondingly. Entering the algorithm contains a pool U = {C\u00b5 (X, \u03b8j, \u03b4) nj = 1 of the consensus sets (the output of algorithm 1).Definition 1 (preference matrix A) We define the m-n preference matrix A, whose rows and columns each contain the m = | data elements {xi} mi = 1 of the consensus sets (the output of algorithm 1).Definition 1 (preference matrix A) we define the m-n preference matrix A, whose rows and columns represent the M datasets {xi} mi = 1 and the n consensus sets, such as (A) ij = {1, if xi-\u00b5-j-reigj-matrix)."}, {"heading": "2.1 Analyzing the preference matrix", "text": "c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c"}, {"heading": "2.2 L1-NMF", "text": "The only missing component to formalize the problem is an appropriate prior one for O. Since the errors are also binary (and therefore inaccurate), it would be a reasonable choice to sanction its L1 norm. For convenience, we also leave the binary restriction on O. We can therefore asmin u, v \".A \u2212 uvT \u2212 O\".A \".A\".A \".A\".A \".A\".A \".A\".A \".A\".A. \".A.\".A. \".A\".A. \".A\".A \".A\".A \".A\".A \".A\".A \".A\".A \".A\".A \".A\".A \".A\" \".A\".D \"D\".D \"D\".D \".D\" D \".D\".D \"D\".D \"D\".D \"D\".D \"D\".D \"A\".A \".A\".A \".A\".A \".A\".D \".D\".D \"A\".D \".D\" A \".D\".D \".D\" D \".D\" D \".D\".D \"A\".D \".D\".D \".D\".D \".D\".D \".D\" \".D\".D \".D\".D \".D\".D \".D\".D \".D\".D \".D\".D \".D\" \".D\".D \".D\".D \".D\".D \".D\".D \".D\".D \".D\".D \".D\".D \".D\".D \".D\".D \".D\".D \".D\".D \".D\".D \".D\".D \".D\".D \".D\".D \".D\".D \".D\".D \".D\".D \".D\".D \".D\".D \".D\".D \".D\".D \".D\".D \".D\".D \".D\".D \".D\".D \".D\""}, {"heading": "2.3 Dealing with multiple biclusters", "text": "A challenge with biclustering (as with classic clustering) is that the number of biclusters is not easy to determine or estimate in advance. If we follow a standard approach in the literature [22, 32], we sift the information sequentially. Generally, we iterate two steps until a holding criterion is met: (1) we find a bicluster (u, v) and (2) we subtract the information encoded by (u, v) from A.Algorithm 3, summarizing the proposed bicluster approach. In line 6, we set the columns corresponding to the active set of vt to zero, forcing disjunctive active sentences between the consecutive Vt and subsequent orthogonality, which also ensures that the non-negativity is maintained throughout the iterations. The proposed algorithm is very efficient, easy to code, and shows that it works well in the experiments we present."}, {"heading": "2.4 Minimum description length as a stopping criterion", "text": "The core of algorithm 3 (lines 1 to 8) does not provide a reliable mean for determining the number of biclusters to be extracted from the preference matrix. In fact, lines 3 and 7 only provide a rough stop criterion if At + 1 is empty or if the bicluster is not the product of a consensus between at least two models; this criterion will output more models than are needed, and this section is devoted to truncating the collection {(ut, vt)} Tt = 1. For each bicluster, we are only interested in supporting its vectors ut and vt (which are sparse in design); we can then binarize them to avoid small numerical inaccuracies that might have occurred during the optimization process, i.e. ut = bicluster and vt = bicluster, where (bxc\u03b3) i = {1 if (x) i > q x \u00b2 x \u00b2, otherwise."}, {"heading": "2.5 Statistical validation for pre-processing", "text": "We assume that it is a model with an associated consensus, built with tolerance (Equation (4), page 2). We assume that many good model estimates emerge (composed of random elements), but also generate many bad models (composed largely of outliers). In general, the number of bad models would far exceed the number of good ones. It is not worth looking at these columns of A. The question is how we determine the minimum size for a good consensus set, based on a contrary testing mechanism presented in depth in [12], is a statistically useful method for discarding bad models. These models will typically contain only a handful of objects. The question is how we determine the minimum size for a good consensus set."}, {"heading": "2.6 Statistical validation for post-processing", "text": "Once the dual-luminous algorithm returns a collection of Inliers model pairs, we must verify that these models are statistically significant from a geometrical point of view. To this end, we use the test in Definition 2 once more. Algorithm 4: Exclusion Principle Input: Capture of Inliers model pairs {(Ct, \u03b8t)} Tt = 1. Output: Cropped Capture of Inliers model pairs {(Ct, \u03b8t)} Tt = 1.1 K \u2190 \u2205; 2 S \u2190 X; 3 foreach (Ct, \u03b8t) do 4 if NFA\u00b5 (S, \u03b8, \u03b4) is significant, then 5 K \u2190 K (Ct, \u0445t)}; 6 S \u2190 X\\ Ct; In addition, models may overlap and can therefore share elements that make situations such as those described in Figure 2 common. In short, a good model should separate itself from background noise regardless of its intersection with other models."}, {"heading": "3. Accelerated Random Sample Ensemble", "text": "The problem of multiple parametric model estimates does not escape the current trend of dataset size growth, which reveals the need for rapid techniques to address these massive datasets. (There are two computational bottlenecks in our approach.) The first is shared by virtually all the algorithms in the field. (page 3) Fortunately, all random samples can be calculated in parallel, reducing the problem to clever software engineering. (Alternatively, there are newer techniques to reduce the number of samples needed [7], at the expense of a less parallel algorithm. (The main component of the proposed technique is the biclustering algorithm, and this section is dedicated to describing how to solve this optimization problem efficiently. (Let's remember that we are trying to solve the problem.)"}, {"heading": "3.1 Fast `1 regression", "text": "The FCT can be used as a building block for the construction of fast solvers for \"1 regression problems.\" We describe this first before introducing the proposed algorithm L1-NMF. \u2212 Definition 4 ([26]) Let A-Rm \u00b7 n be a rank r. A-base B-Rn \u00b7 r for the range of A is (\u03b1, \u03b2) -conditioned if B-1 \u2264 and B-Rr) x-Rp-Rp-n is well conditioned. We say that B is well conditioned if \u03b1 and \u03b2 polynomials are low degree in r, independent of m and n.Definition 5 ([26]) Given a well conditioned base B for the range of A-Rm \u00b7 n, we define the \"1-Rp-values of A as m-dimensional vector \u03bb, with elements i-Rp-Rp-value (B): Ri-Rp-Rp-Rp-Rp-Rp-Rp."}, {"heading": "3.2 Fast `1 NMF", "text": "Given that we are dealing with a biconvex problem, it is standard practice to find a solution to this problem by having two \"1-1-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-"}, {"heading": "4. Experimental results", "text": "In fact, New York, New York, it is so that most of them are able to survive themselves, and that they are able to survive themselves, \"he said in an interview with the\" New York Times, \"in which he grappled with the\" New York Times. \"\" I do not believe that the world is in order, \"he said in an interview with the\" New York Times, \"\" I believe that the world is in order, \"he said in an interview with the\" New York Times. \"\" I do not believe that it will be able to change the world, \"he said in an interview with the\" New York Times, \"\" New York, \"New York Times,\" New York, \"New York,\" New York, \"New York,\" New York, \"New York,\" New York, New York, New York, New York, New York, New York, New York, New York, New York, New York, New York, New York, New York, New York, New York, New York, New York, New York, New York, New York, New York, New York, New York, New York, New York, New York, New York, New York, New York, New York, New York, New York, New York, New York, New York, New York, New York, New York, New York, New York, New York, New York, New York, New York, New York, New York, New York, New York, New York, New York, New York, New York, New York, New York, New York, New York, New York, New York, New York, New York, New York, New York, New York, New York, New York, New York, New York, New York, New York, New York,"}, {"heading": "5. Conclusions", "text": "In this paper, we have introduced a complete and comprehensive algorithmic pipeline for estimating multiple parametric models. The proposed approach takes the information generated by a random sample algorithm (e.g. RANSAC) and analyzes it from a machine learning / optimization perspective using a parametric biclustering algorithm based on L1 non-negative matrix factorization (L1-NMF).This new formulation conceptually changes the way the data generated by the popular RANSAC or related process for generating model candidates is analyzed, using it 3. See footnote 2 Consistencies that arise naturally during RANSAC execution while explicitly avoiding spurious inconsistencies. In addition, and contrary to major trends in the literature, the proposed modeling does not impose non-overlapping parametric models."}], "references": [{"title": "Approximate nearest neighbors and the fast Johnson- Lindenstrauss transform", "author": ["N. Ailon", "B. Chazelle"], "venue": "STOC,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2006}, {"title": "The fast Johnson-Lindenstrauss transform and approximate nearest neighbors", "author": ["N. Ailon", "B. Chazelle"], "venue": "SIAM J. Comput., 39(1):302\u2013322,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2009}, {"title": "Vanishing point detection without any a priori information", "author": ["A. Almansa", "A. Desolneux", "S. Vamech"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell., 25(4):502\u2013507,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2003}, {"title": "Vanishing point detection by segment clustering on the projective space", "author": ["F. Andal\u00f3", "G. Taubin", "S. Goldenstein"], "venue": "ECCV Workshops,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2012}, {"title": "Two points fundamental matrix", "author": ["G. Ben-Artzi", "T. Halperin", "M. Werman", "S. Peleg"], "venue": "Technical report,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2016}, {"title": "Image segmentation by a contrario simulation", "author": ["N. Burrus", "T.M. Bernard", "J.M. Jolion"], "venue": "Pattern Recognit., 42(7):1520\u20131532,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2009}, {"title": "Accelerated hypothesis generation for multistructure data via preference analysis", "author": ["T.J. Chin", "J. Yu", "D. Suter"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell., 34(4):625\u2013638,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2012}, {"title": "Performance evaluation of RANSAC family", "author": ["S. Choi", "T. Kim", "W. Yu"], "venue": "BMVC,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2009}, {"title": "The fast Cauchy transform and faster robust linear regression", "author": ["K.L. Clarkson", "P. Drineas", "M. Magdon-Ismail", "M.W. Mahoney", "X. Meng", "D.P. Woodruff"], "venue": "SIAM J. Comput., 45(3):763\u2013810,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2016}, {"title": "Enumerative source encoding", "author": ["T.M. Cover"], "venue": "IEEE Trans. Inf. Theory, 19(1):73\u201377,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1973}, {"title": "Efficient edge-based methods for estimating manhattan frames in urban imagery", "author": ["P. Denis", "J.H. Elder", "F. Estrada"], "venue": "ECCV,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2008}, {"title": "From Gestalt Theory to Image Analysis, volume 34", "author": ["A. Desolneux", "L. Moisan", "J.M. Morel"], "venue": "Springer-Verlag,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2008}, {"title": "Random sample consensus: A paradigm for model fitting with applications to image analysis and automated cartography", "author": ["M. Fischler", "R. Bolles"], "venue": "Commun. ACM, 24 (6):381\u2013395,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1981}, {"title": "LSD: A fast line segment detector with a false detection control", "author": ["R. Grompone von Gioi", "J. Jakubowicz", "J.M. Morel", "G. Randall"], "venue": "IEEE Trans Pattern Anal Mach Intell,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2010}, {"title": "Robust nonnegative matrix factorization using l21-norm", "author": ["D. Kong", "C. Ding", "H. Huang"], "venue": "CIKM,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2011}, {"title": "Detecting the overlapping and hierarchical community structure in complex networks", "author": ["A. Lancichinetti", "S. Fortunato", "J. Kert\u00e9sz"], "venue": "New J. Phys., 11(3):33015,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2009}, {"title": "BRISK: Binary robust invariant scalable keypoints", "author": ["S. Leutenegger", "M. Chli", "R.Y. Siegwart"], "venue": "ICCV,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2011}, {"title": "Finding vanishing points via point alignments in image primal and dual domains", "author": ["J. Lezama", "R. von Gioi", "G. Randall", "J.-M. Morel"], "venue": "In CVPR,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2014}, {"title": "Solving consensus and semi-supervised clustering problems using nonnegative matrix factorization", "author": ["T. Li", "C. Ding", "M.I. Jordan"], "venue": "ICDM,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2007}, {"title": "T-linkage: A continuous relaxation of J-linkage for multimodel fitting", "author": ["L. Magri", "A. Fusiello"], "venue": "CVPR,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2014}, {"title": "Robust multiple model fitting with preference analysis and low-rank approximation", "author": ["L. Magri", "A. Fusiello"], "venue": "BMVC,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2015}, {"title": "From K-means to higher-way coclustering: multilinear decomposition with sparse latent factors", "author": ["E.E. Papalexakis", "N.D. Sidiropoulos", "R. Bro"], "venue": "IEEE Trans. Signal Process., 61(2):493\u2013506,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2013}, {"title": "The random cluster model for robust geometric fitting", "author": ["T.T. Pham", "T.J. Chin", "J. Yu", "D. Suter"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell., 36(8):1658\u20131671,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2014}, {"title": "MAC-RANSAC: A robust algorithm for the recognition of multiple objects", "author": ["J. Rabin", "J. Delon", "Y. Gousseau", "L. Moisan"], "venue": "3DPTV,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2010}, {"title": "High-resolution stereo datasets with subpixel-accurate ground truth", "author": ["D. Scharstein", "H. Hirschm\u00fcller", "Y. Kitajima", "G. Krathwohl", "N. Ne\u0161i\u0107", "X. Wang", "P. Westling"], "venue": "GCPR,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2014}, {"title": "Subspace embeddings for the L1-norm with applications", "author": ["C. Sohler", "D. Woodruff"], "venue": "STOC,", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2011}, {"title": "Nonlinear mean shift for clustering over analytic manifolds", "author": ["R. Subbarao", "P. Meer"], "venue": "CVPR,", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2006}, {"title": "Non-iterative approach for fast and accurate vanishing point detection", "author": ["J.P. Tardif"], "venue": "ICCV,", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2009}, {"title": "Multi-temporal foreground detection in videos", "author": ["M. Tepper", "A. Newson", "P. Sprechmann", "G. Sapiro"], "venue": "ICIP,", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2015}, {"title": "A bi-clustering framework for consensus problems", "author": ["M. Tepper", "G. Sapiro"], "venue": "SIAM J. Imaging Sci., 7(4):2488\u20132525,", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2014}, {"title": "Robust multiple structures estimation with J-linkage", "author": ["R. Toldo", "A. Fusiello"], "venue": "ECCV,", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2008}, {"title": "A penalized matrix decomposition, with applications to sparse principal components and canonical correlation analysis", "author": ["D.M. Witten", "R. Tibshirani", "T. Hastie"], "venue": "Biostatistics, 10(3):515\u2013534,", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2009}, {"title": "Data Mining: Practical Machine Learning Tools and Techniques", "author": ["I.H. Witten", "E. Frank", "M. a. Hall"], "venue": null, "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2011}, {"title": "Dynamic and hierarchical multi-structure geometric model fitting", "author": ["H.S. Wong", "T.J. Chin", "J. Yu", "D. Suter"], "venue": "ICCV,", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2011}, {"title": "A new curve detection method: Randomized Hough transform (RHT)", "author": ["L. Xu", "E. Oja", "P. Kultanen"], "venue": "Pattern Recognit. Lett., 11(5):331\u2013338,", "citeRegEx": "35", "shortCiteRegEx": null, "year": 1990}, {"title": "Online Nonnegative Matrix Factorization with Outliers", "author": ["R. Zhao", "V.Y.F. Tan"], "venue": "Technical report,", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2016}, {"title": "The multiRANSAC algorithm and its application to detect planar homographies", "author": ["M. Zuliani", "C.S. Kenney", "B.S. Manjunath"], "venue": "ICIP,", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2005}], "referenceMentions": [{"referenceID": 7, "context": "MPME is an important but difficult problem, as standard robust estimators, like RANSAC (RANdom SAmple Consensus) [8, 13], are designed to extract a single model.", "startOffset": 113, "endOffset": 120}, {"referenceID": 12, "context": "MPME is an important but difficult problem, as standard robust estimators, like RANSAC (RANdom SAmple Consensus) [8, 13], are designed to extract a single model.", "startOffset": 113, "endOffset": 120}, {"referenceID": 12, "context": "The number n is an overestimation of the number of trials needed to obtain a certain number of \u201cgood\u201d models [13, 31, 37].", "startOffset": 109, "endOffset": 121}, {"referenceID": 30, "context": "The number n is an overestimation of the number of trials needed to obtain a certain number of \u201cgood\u201d models [13, 31, 37].", "startOffset": 109, "endOffset": 121}, {"referenceID": 36, "context": "The number n is an overestimation of the number of trials needed to obtain a certain number of \u201cgood\u201d models [13, 31, 37].", "startOffset": 109, "endOffset": 121}, {"referenceID": 23, "context": ", [24].", "startOffset": 2, "endOffset": 6}, {"referenceID": 36, "context": "However, this approach is known to be suboptimal [37].", "startOffset": 49, "endOffset": 53}, {"referenceID": 36, "context": "The multiRANSAC algorithm [37] provides a more effective alternative, although the number of models must be known a priori, imposing a very limiting constraint in many applications.", "startOffset": 26, "endOffset": 30}, {"referenceID": 34, "context": ", using the Randomized Hough Transform [35], or by using non-parametric density estimation techniques, like the mean-shift clustering algorithm [27].", "startOffset": 39, "endOffset": 43}, {"referenceID": 26, "context": ", using the Randomized Hough Transform [35], or by using non-parametric density estimation techniques, like the mean-shift clustering algorithm [27].", "startOffset": 144, "endOffset": 148}, {"referenceID": 30, "context": "These, however, are not intrinsically robust techniques, even if they can be robustified with outliers rejection heuristics [31].", "startOffset": 124, "endOffset": 128}, {"referenceID": 30, "context": "Moreover, the choice of the parametrization and its discretization are critical, among other important shortcomings [31].", "startOffset": 116, "endOffset": 120}, {"referenceID": 30, "context": "From a different perspective, J-linkage [31], T-linkage [20], and RPA [21] address the problem by clustering the dataset.", "startOffset": 40, "endOffset": 44}, {"referenceID": 19, "context": "From a different perspective, J-linkage [31], T-linkage [20], and RPA [21] address the problem by clustering the dataset.", "startOffset": 56, "endOffset": 60}, {"referenceID": 20, "context": "From a different perspective, J-linkage [31], T-linkage [20], and RPA [21] address the problem by clustering the dataset.", "startOffset": 70, "endOffset": 74}, {"referenceID": 29, "context": "We have previously introduced a novel framework and perspective to reach consensus in grouping problems by re-framing them as biclustering problems [30].", "startOffset": 148, "endOffset": 152}, {"referenceID": 28, "context": ", traffic analysis [29], eldercare [29], shadow removal for face detection [36], video surveillance [36]).", "startOffset": 19, "endOffset": 23}, {"referenceID": 28, "context": ", traffic analysis [29], eldercare [29], shadow removal for face detection [36], video surveillance [36]).", "startOffset": 35, "endOffset": 39}, {"referenceID": 35, "context": ", traffic analysis [29], eldercare [29], shadow removal for face detection [36], video surveillance [36]).", "startOffset": 75, "endOffset": 79}, {"referenceID": 35, "context": ", traffic analysis [29], eldercare [29], shadow removal for face detection [36], video surveillance [36]).", "startOffset": 100, "endOffset": 104}, {"referenceID": 30, "context": "The preference matrix was explicitly introduced in the context of MPME in [31].", "startOffset": 74, "endOffset": 78}, {"referenceID": 19, "context": "An extension to work with a non-binary (using soft versus hard element-model membership) version of A was proposed in [20].", "startOffset": 118, "endOffset": 122}, {"referenceID": 20, "context": "But this is in fact one of the parameters we are interested in discovering! In the context of MPME, matrix factorization has been recently applied [21] to a normalized version of B (using soft membership).", "startOffset": 147, "endOffset": 151}, {"referenceID": 14, "context": "Find initializations for u and v using the iterative re-weighting scheme in [15].", "startOffset": 76, "endOffset": 80}, {"referenceID": 14, "context": "The method in [15] is not particularly well suited for large-scale problems, as it deals with dense weighting matrices with the size of A.", "startOffset": 14, "endOffset": 18}, {"referenceID": 21, "context": "Following a standard approach in the literature [22, 32] we sieve the information in a sequential way.", "startOffset": 48, "endOffset": 56}, {"referenceID": 31, "context": "Following a standard approach in the literature [22, 32] we sieve the information in a sequential way.", "startOffset": 48, "endOffset": 56}, {"referenceID": 9, "context": "dimensional vector p, this can be efficiently described using an enumerative code [10],", "startOffset": 82, "endOffset": 86}, {"referenceID": 11, "context": "The question is how do we determine the minimum size for a good consensus set? This important computational contribution, based on the a contrario testing mechanism presented in depth in [12], is addressed next.", "startOffset": 187, "endOffset": 191}, {"referenceID": 11, "context": "and that the error in Equation (3) locally follows an uniform distribution; this type of simple approximations has proven successful for outlier rejection [12].", "startOffset": 155, "endOffset": 159}, {"referenceID": 0, "context": "On the left, an arrangement of points built from uniformly sampling 100 points in [0, 1]2, 100 points in [0.", "startOffset": 82, "endOffset": 88}, {"referenceID": 0, "context": "31]\u00d7 [0, 1], 100 points in [0.", "startOffset": 5, "endOffset": 11}, {"referenceID": 0, "context": "71]\u00d7[0, 1], 50 in [0.", "startOffset": 4, "endOffset": 10}, {"referenceID": 5, "context": "Alternatively, Ntests can be empirically set by analyzing a training dataset [6], providing a tighter bound for the expectation.", "startOffset": 77, "endOffset": 80}, {"referenceID": 11, "context": "Following an a contrario reasoning [12], we decide whether the event of interest has occurred if it has a very low probability of occurring by chance in the above defined random (background) model.", "startOffset": 35, "endOffset": 39}, {"referenceID": 6, "context": "Alternatively, there are recent techniques to reduce the number of needed samples [7], at the expense of a less parallelizable algorithm.", "startOffset": 82, "endOffset": 85}, {"referenceID": 0, "context": "The fast Johnson-Lindenstrauss transform [1, 2] provides a way to build a low dimensional embedding in the `2 case and has been widely used in many practical settings.", "startOffset": 41, "endOffset": 47}, {"referenceID": 1, "context": "The fast Johnson-Lindenstrauss transform [1, 2] provides a way to build a low dimensional embedding in the `2 case and has been widely used in many practical settings.", "startOffset": 41, "endOffset": 47}, {"referenceID": 8, "context": "Its `1-based analog is the fast Cauchy transform (FCT) [9], which", "startOffset": 55, "endOffset": 58}, {"referenceID": 8, "context": "Theorem 3 ([9]) Let A \u2208 Rm\u00d7n be a matrix of rank r (r n).", "startOffset": 11, "endOffset": 14}, {"referenceID": 25, "context": "Definition 4 ([26]) Let A \u2208 Rm\u00d7n be a matrix of rank r.", "startOffset": 14, "endOffset": 18}, {"referenceID": 25, "context": "Definition 5 ([26]) Given a well-conditioned basis B for the range of A \u2208 Rm\u00d7n, we define the `1 leverage scores of A as the m-dimensional vector \u03bb, with elements (\u03bb)i = \u2016(B):i\u20161.", "startOffset": 14, "endOffset": 18}, {"referenceID": 8, "context": "The leverage scores of A can be found with the following procedure [9, 26]: 1.", "startOffset": 67, "endOffset": 74}, {"referenceID": 25, "context": "The leverage scores of A can be found with the following procedure [9, 26]: 1.", "startOffset": 67, "endOffset": 74}, {"referenceID": 8, "context": "using Definition 5, compute the leverage scores \u03bb of B = AR\u2020 (R\u2020 denotes the pseudoinverse of R); The leverage scores are used in [9, 26] to speed up the algorithmic solution of x\u2217 = argminx \u2016Ax\u2212 b\u20161.", "startOffset": 130, "endOffset": 137}, {"referenceID": 25, "context": "using Definition 5, compute the leverage scores \u03bb of B = AR\u2020 (R\u2020 denotes the pseudoinverse of R); The leverage scores are used in [9, 26] to speed up the algorithmic solution of x\u2217 = argminx \u2016Ax\u2212 b\u20161.", "startOffset": 130, "endOffset": 137}, {"referenceID": 14, "context": "with the iterative re-weighting scheme in [15].", "startOffset": 42, "endOffset": 46}, {"referenceID": 8, "context": "We also found in our experimental results that instead of using random sampling for building the row and column compression matrices, as in [9], good results were obtained by simply selecting the r largest leverage scores.", "startOffset": 140, "endOffset": 143}, {"referenceID": 15, "context": "In the general case where models overlap, we also use, as an additional measure, the generalized normalized mutual information (GNMI) [16], which extends the normalized mutual information (also called symmetric uncertainty) [33, p.", "startOffset": 134, "endOffset": 138}, {"referenceID": 30, "context": "We start our experimental evaluation with a few small synthetic datasets [31] where 2D points are arranged forming lines and circles.", "startOffset": 73, "endOffset": 77}, {"referenceID": 30, "context": "Figure 4: ARSE results for 2D circle detection on a synthetic dataset [31].", "startOffset": 70, "endOffset": 74}, {"referenceID": 30, "context": "This is an intrinsic limitation of previous state-of-the-art competitors such as Jlinkage [31], T-linkage [20], RPA [21] and most multiple model estimation techniques, which are generally based on partitioning (clustering) the dataset.", "startOffset": 90, "endOffset": 94}, {"referenceID": 19, "context": "This is an intrinsic limitation of previous state-of-the-art competitors such as Jlinkage [31], T-linkage [20], RPA [21] and most multiple model estimation techniques, which are generally based on partitioning (clustering) the dataset.", "startOffset": 106, "endOffset": 110}, {"referenceID": 20, "context": "This is an intrinsic limitation of previous state-of-the-art competitors such as Jlinkage [31], T-linkage [20], RPA [21] and most multiple model estimation techniques, which are generally based on partitioning (clustering) the dataset.", "startOffset": 116, "endOffset": 120}, {"referenceID": 30, "context": "The datasets (figures 3 and 4) were created [31] by sampling points from different parametric models, adding a certain amount of noise to these points, and then further sampling outliers from a uniform distribution.", "startOffset": 44, "endOffset": 48}, {"referenceID": 10, "context": "Table 2: Comparative performance of RSE and ARSE to detect vanishing points on the York Urban database [11].", "startOffset": 103, "endOffset": 107}, {"referenceID": 13, "context": ", [14]).", "startOffset": 2, "endOffset": 6}, {"referenceID": 10, "context": "To evaluate the proposed framework for the task of detecting vanishing points, we use the York Urban database [11], which comes with ground truth assignments of image segments to three orthogonal directions in 3D.", "startOffset": 110, "endOffset": 114}, {"referenceID": 16, "context": "Any modern method to find those matches would work relatively well for our purposes; we use BRISK [17].", "startOffset": 98, "endOffset": 102}, {"referenceID": 10, "context": "Figure 5: Examples of vanishing points detection on the York Urban database [11].", "startOffset": 76, "endOffset": 80}, {"referenceID": 6, "context": "To show this, we have included experiments using the state-ofthe-art MultiGS algorithm [7].", "startOffset": 87, "endOffset": 90}, {"referenceID": 4, "context": "It is worth pointing out that a recent technique [5] has been able to reduce the size of the minimal sample set for fundamental matrices to b = 2; its use would significantly reduce the computational complexity of the sampling step, imposing an upper bound of O(m2) to the total number of possible combinations.", "startOffset": 49, "endOffset": 52}, {"referenceID": 33, "context": "We estimate multiple fundamental matrices (moving camera and moving objects) and multiple homographies (moving planar objects) on the images in the AdelaideRMF dataset [34].", "startOffset": 168, "endOffset": 172}, {"referenceID": 19, "context": "We include comparisons with T-linkage [20], RCMSA [23], and RPA [21].", "startOffset": 38, "endOffset": 42}, {"referenceID": 22, "context": "We include comparisons with T-linkage [20], RCMSA [23], and RPA [21].", "startOffset": 50, "endOffset": 54}, {"referenceID": 20, "context": "We include comparisons with T-linkage [20], RCMSA [23], and RPA [21].", "startOffset": 64, "endOffset": 68}, {"referenceID": 24, "context": "Figure 6: Fundamental matrix estimation on two stereo pairs of the Middlebury Stereo dataset [25].", "startOffset": 93, "endOffset": 97}, {"referenceID": 16, "context": "From top to bottom: original pair, matches obtained from BRISK keypoints and descriptors [17] (2518 on the left, 8999 on the right), ARSE inliers (1363/2518 \u2248 54% on the left, 5537/8999 \u2248 61% on the right), and ARSE outliers (the RSE inlier sets are very similar).", "startOffset": 89, "endOffset": 93}, {"referenceID": 33, "context": "Figure 7: The ground truth of the standard AdelaideRMF dataset [34] has non-negligible errors (detectable by simple ocular inspection).", "startOffset": 63, "endOffset": 67}, {"referenceID": 33, "context": "Figure 8: Results on the AdelaideRMF dataset [34] for the estimation of fundamental matrices and homographies.", "startOffset": 45, "endOffset": 49}], "year": 2016, "abstractText": "In this work we introduce a comprehensive algorithmic pipeline for multiple parametric model estimation. The proposed approach analyzes the information produced by a random sampling algorithm (e.g., RANSAC) from a machine learning/optimization perspective, using a parameterless biclustering algorithm based on L1 nonnegative matrix factorization (L1-NMF). The proposed framework exploits consistent patterns that naturally arise during the RANSAC execution, while explicitly avoiding spurious inconsistencies. Contrarily to the main trends in the literature, the proposed technique does not impose non-intersecting parametric models. A new accelerated algorithm to compute L1-NMFs allows to handle medium-sized problems faster while also extending the usability of the algorithm to much larger datasets. This accelerated algorithm has applications in any other context where an L1-NMF is needed, beyond the biclustering approach to parameter estimation here addressed. We accompany the algorithmic presentation with theoretical foundations and numerous and diverse examples.", "creator": "LaTeX with hyperref package"}}}