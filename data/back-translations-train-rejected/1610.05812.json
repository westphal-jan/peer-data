{"id": "1610.05812", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Oct-2016", "title": "Small-footprint Highway Deep Neural Networks for Speech Recognition", "abstract": "Morden state-of-the-art speech recognition systems usually employ neural networks for acoustic modeling. However, compared to the conventional Gaussian mixture models, deep neural network (DNN) based acoustic models usually have much larger number of model parameters, making it challenging for their applications in resource constrained platforms such as mobile devices. In this paper, we study the application of the recently proposed highway deep neural network (HDNN) for training small-footprint acoustic models. HDNN is a type of depth-gated feedforward neural network, which introduces two type of gate functions to facilitate the information flow through different layers. Our study demonstrates that HDNNs are more compact than plain DNNs for acoustic modeling, i.e., they can achieve comparable recognition accuracy with much less model parameters than plain DNN-based acoustic models. Furthermore, HDNNs are more controllable than plain DNNs. The gate functions of a HDNN largely control the behavior of the whole network with very small number of model parameters. And finally, HDNNs are more adaptable than plain DNNs. For example, simply updating the gate functions using the adaptation data can result in considerable gains. We demonstrate these aspect by experiments using the publicly available AMI meeting speech transcription corpus, which has around 80 hours of training data. Moreover, we also investigate the knowledge distillation technique to further improve the small-footprint HDNN acoustic models.", "histories": [["v1", "Tue, 18 Oct 2016 22:06:01 GMT  (118kb,D)", "https://arxiv.org/abs/1610.05812v1", "9 pages, 6 figures. arXiv admin note: substantial text overlap witharXiv:1608.00892,arXiv:1607.01963"], ["v2", "Mon, 24 Oct 2016 21:12:56 GMT  (131kb,D)", "http://arxiv.org/abs/1610.05812v2", "9 pages, 6 figures. arXiv admin note: substantial text overlap witharXiv:1608.00892,arXiv:1607.01963"], ["v3", "Wed, 25 Jan 2017 15:45:22 GMT  (131kb,D)", "http://arxiv.org/abs/1610.05812v3", "9 pages, 6 figures. arXiv admin note: substantial text overlap witharXiv:1608.00892,arXiv:1607.01963"], ["v4", "Tue, 25 Apr 2017 19:48:41 GMT  (942kb,D)", "http://arxiv.org/abs/1610.05812v4", "9 pages, 6 figures. Accepted to IEEE/ACM Transactions on Audio, Speech and Language Processing, 2017. arXiv admin note: text overlap witharXiv:1608.00892,arXiv:1607.01963"]], "COMMENTS": "9 pages, 6 figures. arXiv admin note: substantial text overlap witharXiv:1608.00892,arXiv:1607.01963", "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["liang lu", "steve renals"], "accepted": false, "id": "1610.05812"}, "pdf": {"name": "1610.05812.pdf", "metadata": {"source": "CRF", "title": "Small-footprint Highway Deep Neural Networks for Speech Recognition", "authors": ["Liang Lu", "Steve Renals"], "emails": ["llu@ttic.edu,", "s.renals@ed.ac.uk"], "sections": [{"heading": null, "text": "This year, it has reached the point where it will be able to put itself at the top of the list."}, {"heading": "2 IEEE/ACM TRANSACTIONS ON AUDIO, SPEECH AND LANGUAGE PROCESSING, VOL XXX, NO. XXX, 2017", "text": "For example, if we do not update the model parameters in the hidden layers and / or the Softmax layer during sequence training, and only update the gate functions, we can maintain most of the improvement through sequence training. Furthermore, the regularization term in the sequence training target is not required if only the gate functions are updated. As the size of the gate functions is relatively small, we can only achieve a significant gain by fine-tuning these parameters for unattended speaker fitting, which is a great advantage of this model. Finally, we study teacher-student training and their combination with sequence training, as well as speaker matching to further improve the accuracy of the small format HDNN acoustic models. Our teacher-student training experiments also provide more results to understand this technique in sequence training and adaptation adjustment. Overall, a small format HDNN acoustic model with 5 million model parameters can add slightly less accuracy to a smaller DNN in comparison to a larger model with a smaller DNN."}, {"heading": "II. HIGHWAY DEEP NEURAL NETWORKS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. Deep neural networks", "text": "In this study, we focus on Feed-forward Deep Neural Networks (DNNs), which can achieve higher recognition accuracy with fewer model parameters than recursive neuronal networks with long-term short-term storage units (LSTM-RNNs) and convolutionary neuronal networks (CNNs) [18], [19], but for applications on resource-limited platforms, their accuracy can be transferred to a DNN by teacher-pupil training [?], [20], [21]. A multi-layer network with hidden L layers, however, is assigned ash1 = \u03c3 (x, \u03b81) (1) hl = \u03c3 (hl \u2212 1, \u03b8l), for l = 2, represented., L (2) y = g (hL, \u03b8c) (3), whereby: x is an input vector, which is the input property of a vector; h (1) is the input \u00b7 \u00b7 h."}, {"heading": "B. Highway networks", "text": "There are a variety of training algorithms and model architectures that have been proposed to enable very deep multi-layer networks, including pre-training [24], [25], normalized initialization [23], deeply monitored networks [26], and batch normalization [27]. Highway deep neural networks (HDNNs) [17] were originally proposed to enable very deep networks that can be trained by augmenting the hidden layers with gate functions: hl \u2212 l (hl \u2212 1, WT) + hl \u2212 1, Wc \u2212 1 (4), where the hidden activations of the l layer are referred to as gate functions; T (\u00b7) is the transformation gate that scales the original hidden activations; C (\u00b7) is the carry gate that scales the input before passing it directly to the next hidden layer; and we call elementary multiplication."}, {"heading": "C. Related models", "text": "Both HDNNs and LSTM RNNNs [32] have gate functions. However, the gates in LSTMs are designed to control the flow of information over time and to model time dependencies; in HDNNs, the gates are used to facilitate the flow of information through the depth of the model. Combinations of the two architectures have recently been explored: highway LSTMs [33] use highway connections to train a stacked LSTM with multiple layers; recursive highway networks [34] share gate functions to control the flow of information both in time and model depth. On the other hand, it was recently proposed to use the residual network (ResNet) [35] to train very deep networks, which advances the state of the art in computer vision. ResNets are closely related to motorway networks in the sense that they also rely on skip connections to train very deep networks; however, gate functions are not used in nets (which can save some computing costs)."}, {"heading": "III. TRAINING", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. Cross-entropy training", "text": "The most common criterion used to train neural networks for classification is the cross entropy (CE) loss function, L (CE) = \u2212 \u2211 j y-jt log yjt, (9) where j is the index of the hidden Markov model (HMM), yt is the output of the neural network (3) at the time t and y-t = {y1t, \u00b7 \u00b7, yYt} denotes the Ground Truth Label, which is a single vector, where J is the number of HMM states. Note that the loss function is defined here for a training example on the simplicity of notation. (10) Suppose y-jt = \u03b4ij, where \u03b4ij is the Kronecker delta function and i at the time t is the Ground i is the Ground Truth class, the CE loss is defined here for a training example on the simplicity of notation. (10) Suppose that y-jt = \u03b4ij, where \u03b4ij is the Ground Truth function, and i at the time t is the Ground i is the Ground Truth class, the CE loss will be converted to L (CE) = Logyit. (10) In this case, the minimization of the probability of this class corresponds to the probability (the negative) of the probability of the result of the L and the other (the negative)."}, {"heading": "B. Teacher-Student training", "text": "Instead of using the basic truth markers, the teacher-student training approach defines the loss function asL (KL) (\u03b8) = \u2212 \u2211 j y-jt log yjt, (11) where y-jt is the output of the teacher model that functions as a pseudo-label. Minimizing this loss function corresponds to minimizing the difference between the rear probabilities of each class from the teacher and student model [8]. Here, however, y-jt is no longer a uniform vector; instead, the competing classes will have small but not zero-rear probabilities for each training example. Hinton et al. [38] suggested that the small rear probabilities are valuable information encoding correlations between different classes. However, their roles in the loss function may be very small, as these probabilities are close to zero due to the Softmax function. To address this problem, a temperature parameter, T-R +, can be used to place the polarithms between different classes."}, {"heading": "C. Sequence training", "text": "While the two previous loss functions are defined at the frame level, sequence training defines the loss at the sequence level, which normally leads to a significant improvement in speech recognition accuracy [39] - [41]. Considering a sequence of acoustic images, X = {x1,..., xT}, the length T and a sequence of labels, Y, the loss function is then defined from the Bayean minimum risk criterion (sMBR) [42], [43] as asL (sMBR) (\u03b8) = [W] p (X | W) kP (W) A (Y) A (Y, Y [S] p (X | W) kP (W), (15) where: A (Y, Y) measures the distance between the ground truth and predicted labels at the state level; \u0438 denotes the hypotheses. 1Only the increase in temperature in the teacher network resulted in significantly higher error rates in pilot tests."}, {"heading": "4 IEEE/ACM TRANSACTIONS ON AUDIO, SPEECH AND LANGUAGE PROCESSING, VOL XXX, NO. XXX, 2017", "text": "In this thesis, we focus on the sMBR criterion for sequence training, since it can achieve comparable or slightly better results than training with the criteria Maximum Mutual Information (MMI) or Minimum Telephone Error (MPE) [40]. To solve this problem, we interpolate the sMBR loss function with the CE loss [41], using the smoothing parameters p, R +, L (\u03b8) = L (sMBR) (\u03b8) + pL (CE) (\u03b8). (16) One motivation for this interpolation is that the acoustic model is usually trained first on the CE loss and then fine-tuned with sMBR parameters for a few iterations."}, {"heading": "D. Adaptation", "text": "Adapting deep neural networks is a challenge due to the large number of unstructured model parameters and the small amount of adaptation data. However, the HDNN architecture is more structured because the parameters in the gate functions are layer-independent and can control the behavior of all hidden layers. This motivates the study of the adaptation of highway gates by fine-tuning these model parameters. Although the number of parameters in the gate functions is still large compared to the amount of adaptation data per speaker, the size of the gate functions can be controlled by reducing the number of hidden units but maintaining the accuracy by increasing the depth [14]. In addition, loudspeaker adaptation can be applied to teacher-student training courses to further improve the accuracy of the compact HDNN acoustic models."}, {"heading": "IV. EXPERIMENTS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. System setup", "text": "Our experiments were conducted with AMI's Customized Headset Microphone (IHM), which hits speech transcription corpus [44], [45].2 The amount of training data is about 80 hours, which corresponds to about 28 million frames. We used 40-dimensional fMLLR-adapted feature vectors normalized at speaker level, which were then spliced through a 15-frame context window (i.e. \u00b1 7) for all systems, the number of bound HMM states is 3972, and all DNN systems were trained with the same alignment; the results reported in this paper were obtained with the CNTK toolkit [46] with the Kaldi decoder [47], and the networks were trained2http: / / corpus.amiproject.orgusing the cross-entropy (CE) criterion without pre-training kit [46] with the Kalcooldi coder [47], and the networks were trained2http: / / corpus.org.amiproject.orgusing (CE)."}, {"heading": "B. Baseline results", "text": "Table I shows CE and sequence training results for DNN and HDNN base models of different sizes. DNN systems were all trained with Kaldi and RBM pre-training (without pre-training, thin and deep DNN models did not cooperate with CNTK). However, we were able to train HDNNs with random initialization without pre-training and show that the gate functions in HDNNs facilitate the flow of information through the layers. For sequence training, we implemented the sMBR update for 4 iterations and used p = 0.2 in equivalent (16) to avoid overhauling. Table I shows that the HDNNs consistently achieved lower WERs compared to the DNNs; the profit margin also increases when the number of hidden units decreases. As the number of hidden units decreases, the accuracy of the DNNs rapids deteriorates and the loss of accuracy of the DNs cannot be compensated by the overall sequence results for both the sequencing model and the DNN."}, {"heading": "C. Transform and Carry gates", "text": "The results are shown in Table II, where we have disabled each of the gates in turn. We can see that with only one of the two gates, the HDNN can achieve even lower WHO compared to the regular DNN baseline, but the best results are achieved when both gates are active, suggesting that the two gating functions are are5complementary. Figure 1 shows that the convergence curves of training HDNN with and without the transformation and execution of gates are more important. We observed faster convergence when both gates are active, with significantly lower convergences when using the transformation gate. This suggests that the carry gate that controls the skip connections is more important for the convergence rate. We examined the restricted gates in which C-T (\u00b7) [17] reduces the computing costs, as the matrix vector multiplication for the carry gate gate gate is."}, {"heading": "D. Adaptation", "text": "Previous experiments show that the gate functions can largely control the behavior of a multi-layer neural mesh extractor with a relatively small number of model parameters. This observation inspired us to investigate loudspeaker matching using the gate functions. In our first experiments, we investigated unattended loudspeaker matching, where we decoded the evaluation set using loudspeaker independent models and then used the resulting pseudo-labels to refine the gating parameters (\u03b8g) in the second pass."}, {"heading": "6 IEEE/ACM TRANSACTIONS ON AUDIO, SPEECH AND LANGUAGE PROCESSING, VOL XXX, NO. XXX, 2017", "text": "This is a relatively small amount of adaptation data given the size of the epochs (0.5 million parameters in the HDNN-H512L10 system), we set the learning rate to 2 \u00d7 10 \u2212 4 per sample, and we update it for 5 adaptation epochs. Results show that the adaptation results are smaller than we observe for different model configurations (both for CE and sMBR), and the results suggest that updating all model parameters leads to minor improvements. By adapting the speakers and sequence training, the HDNN system with 5 million model parameters (H512L10) will work slightly better than the DNN baseline with 30 million parameters (24.1% from Table V)."}, {"heading": "E. Teacher-Student training", "text": "In fact, the number of those who are able to outtrump themselves is very high. (...) We have it in our power to outtrump us. \"(S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S."}, {"heading": "F. Teacher-Student training with adaptation", "text": "We then conducted similar adjustment experiments to Section IV-D for HDNNs, which were trained by the teacher-student approach. We used the second adjustment approach for the standalone HDNN model, i.e., we decoded the scores to get the hard labels first, and used those labels to adjust the model using the CE loss (10). However, when using the teacher-student loss (11), only a single decoding pass is required, because the pseudo-labels are provided for the adjustment by the teacher who does not require word-level transcription, which is a particular benefit of the teacher training technology. However, the student model should be adapted offline for resource-limited application scenarios, otherwise the teacher model needs to be accessed to generate the labels, requiring another set of unlabeled speaker-dependent data for the adjustment, which is not normally expensive."}, {"heading": "G. Summary", "text": "We summarize our key findings in Table IX. Overall, the HDNN acoustic model, with around 5 million model parameters after adjusting the door functions, can easily outperform the sequence-trained baseline; with less than 2 million model parameters, it performs slightly worse. If less than 0.8 million parameters are used, the gap is much larger than the DNN baseline. With adaptation and teacher training, we can close the gap by about 50%, with the difference in WHO falling from about 5% absolute to 2.5% absolute."}, {"heading": "V. CONCLUSIONS", "text": "In this paper, we investigated sequence training and adaptation of these networks for acoustic modeling. Specifically, we investigated the role of the parameters in the hidden layers, gate functions and classification layers in the case of sequence training. However, we show that the gate functions, which constitute only a small part of the entire parameter set, are able to control the flow of information and adjust the behavior of the neural network feature extractors. We demonstrate this in both sequence training and adaptation experiments where considerable improvements were made by updating the gate functions. With these techniques, we obtained comparable or slightly lower WERs with much smaller acoustic models compared to a strong base model determined by a conventional DNN acoustic model with sequence training. As the number of model parameters is relatively large compared to the amount of data normally used for adaptation of speakers, the number of adaptation techniques used by school students may be better suited to the amount of adaptation techniques expected."}, {"heading": "VI. ACKNOWLEDGEMENT", "text": "We thank NVIDIA Corporation for donating a Titan X GPU and anonymous reviewers for revealing comments and suggestions that have helped improve the quality of this work."}, {"heading": "10 IEEE/ACM TRANSACTIONS ON AUDIO, SPEECH AND LANGUAGE PROCESSING, VOL XXX, NO. XXX, 2017", "text": "[23] Xavier Glorot and Yoshua prevised nets, \"Understanding the difficulty of training deep feedforward neural networks,\" in International conference on artificial intelligence and statistics, 2010, pp. 249-256. [24] Geoffrey E Hinton and Ruslan R Salakhutdinov, \"Reducing the dimensionality of data with neural networks,\" Science, vol. 313, no. 5786, pp. 504-507, 2006. [25] Yoshua Bengio, Pascal Lamblin Dan Popovici, Hugo Larochelle, et al., \"Greedy layer wise training of deep networks,\" in Proc. NIPS, 2007, vol. 504-507, 2006. [26] Chen-Yu Lee, Saining Xie, Patrick Gallagher, Zhengyou Zhang, and Zhuowen Tu, \"Deeply-supervised nets: 1409.5185, 2014]."}], "references": [{"title": "Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups", "author": ["Geoffrey Hinton", "Li Deng", "Dong Yu", "George E Dahl", "Abdel-rahman Mohamed", "Navdeep Jaitly", "Andrew Senior", "Vincent Vanhoucke", "Patrick Nguyen", "Tara N Sainath", "Brain Kingsbury"], "venue": "Signal Processing Magazine, IEEE, vol. 29, no. 6, pp. 82\u201397, 2012.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2012}, {"title": "Conversational speech transcription using context-dependent deep neural networks", "author": ["Frank Seide", "Gang Li", "Dong Yu"], "venue": "Interspeech, 2011, pp. 437\u2013440.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2011}, {"title": "The IBM 2016 English Conversational Telephone Speech Recognition System", "author": ["George Saon", "Tom Sercu", "Steven Rennie", "Hong-Kwang J. Kuo"], "venue": "Proc. INTERSPEECH, 2016.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2016}, {"title": "Connectionist speech recognition: a hybrid", "author": ["Herve A Bourlard", "Nelson Morgan"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1994}, {"title": "Connectionist probability estimators in HMM speech recognition", "author": ["Steve Renals", "Nelson Morgan", "Herv\u00e9 Bourlard", "Michael Cohen", "Horacio Franco"], "venue": "IEEE Transactions on Speech and Audio Processing, vol. 2, no. 1, pp. 161\u2013174, 1994.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1994}, {"title": "Restructuring of deep neural network acoustic models with singular value decomposition", "author": ["Jian Xue", "Jinyu Li", "Yifan Gong"], "venue": "Proc. INTERSPEECH, 2013, pp. 2365\u20132369.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2013}, {"title": "Low-rank matrix factorization for deep neural network training with high-dimensional output targets", "author": ["Tara N Sainath", "Brian Kingsbury", "Vikas Sindhwani", "Ebru Arisoy", "Bhuvana Ramabhadran"], "venue": "Proc. ICASSP. IEEE, 2013, pp. 6655\u20136659.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2013}, {"title": "Learning smallsize DNN with output-distribution-based criteria", "author": ["Jinyu Li", "Rui Zhao", "Jui-Ting Huang", "Yifan Gong"], "venue": "Proc. INTER- SPEECH, 2014.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2014}, {"title": "Do deep nets really need to be deep", "author": ["Jimmy Ba", "Rich Caruana"], "venue": "Proc. NIPS, 2014, pp. 2654\u20132662.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2014}, {"title": "Fitnets: Hints for thin deep nets", "author": ["Romero Adriana", "Ballas Nicolas", "Kahou Samira Ebrahimi", "Chassang Antoine", "Gatta Carlo", "Bengio Yoshua"], "venue": "Proc. ICLR, 2015.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "Fastfood-approximating kernel expansions in loglinear time", "author": ["Quoc Le", "Tam\u00e1s Sarl\u00f3s", "Alex Smola"], "venue": "Proc. ICML, 2013.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2013}, {"title": "Structured transforms for small-footprint deep learning", "author": ["Vikas Sindhwani", "Tara N Sainath", "Sanjiv Kumar"], "venue": "Proc. NIPS, 2015.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2015}, {"title": "ACDC: A Structured Efficient Linear Layer", "author": ["Marcin Moczulski", "Misha Denil", "Jeremy Appleyard", "Nando de Freitas"], "venue": "Proc. ICLR, 2016.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2016}, {"title": "Small-footprint deep neural networks with highway connections for speech recognition", "author": ["Liang Lu", "Steve Renals"], "venue": "Proc. INTERSPEECH, 2016.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2016}, {"title": "Sequence training and adaptation of highway deep neural networks", "author": ["Liang Lu"], "venue": "Proc. SLT, 2016.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2016}, {"title": "Knowledge distillation for small-footprint highway networks", "author": ["Liang Lu", "Michelle Guo", "Steve Renals"], "venue": "Proc. ICASSP, 2017.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2017}, {"title": "Training very deep networks", "author": ["Rupesh Kumar Srivastava", "Klaus Greff", "J\u00fcrgen Schmidhuber"], "venue": "Proc. NIPS, 2015.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2015}, {"title": "Long shortterm memory recurrent neural network architectures for large scale acoustic modeling", "author": ["Hasim Sak", "Andrew W Senior", "Fran\u00e7oise Beaufays"], "venue": "Proc. INTERSPEECH, 2014.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2014}, {"title": "Convolutional neural networks for speech recognition", "author": ["Ossama Abdel-Hamid", "Abdel-Rahman Mohamed", "Hui Jiang", "Li Deng", "Gerald Penn", "Dong Yu"], "venue": "IEEE/ACM Transactions on Audio, Speech and Language Processing (TASLP), vol. 22, no. 10, pp. 1533\u20131545, 2014.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2014}, {"title": "Transferring knowledge from a RNN to a DNN", "author": ["William Chan", "Nan Rosemary Ke", "Ian Lane"], "venue": "Proc. INTERSPEECH, 2015.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2015}, {"title": "Sequence student-teacher training of deep neural networks", "author": ["Jeremy HM Wong", "Mark JF Gales"], "venue": "Proc. INTERSPEECH. 2016, International Speech Communication Association.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2016}, {"title": "The difficulty of training deep architectures and the effect of unsupervised pre-training", "author": ["Dumitru Erhan", "Pierre-Antoine Manzagol", "Yoshua Bengio", "Samy Bengio", "Pascal Vincent"], "venue": "International Conference on artificial intelligence and statistics, 2009, pp. 153\u2013160.  10  IEEE/ACM TRANSACTIONS ON AUDIO, SPEECH AND LANGUAGE PROCESSING, VOL XXX, NO. XXX, 2017", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2009}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["Xavier Glorot", "Yoshua Bengio"], "venue": "International conference on artificial intelligence and statistics, 2010, pp. 249\u2013256.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2010}, {"title": "Reducing the dimensionality of data with neural networks", "author": ["Geoffrey E Hinton", "Ruslan R Salakhutdinov"], "venue": "Science, vol. 313, no. 5786, pp. 504\u2013507, 2006.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2006}, {"title": "Greedy layer-wise training of deep networks", "author": ["Yoshua Bengio", "Pascal Lamblin", "Dan Popovici", "Hugo Larochelle"], "venue": "Proc. NIPS, 2007, vol. 19, p. 153.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2007}, {"title": "Deeply-supervised nets", "author": ["Chen-Yu Lee", "Saining Xie", "Patrick Gallagher", "Zhengyou Zhang", "Zhuowen Tu"], "venue": "arXiv preprint arXiv:1409.5185, 2014.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2014}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["S Ioffe", "C Szegedy"], "venue": "ICML, 2015.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2015}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors", "author": ["Geoffrey E Hinton", "Nitish Srivastava", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan R Salakhutdinov"], "venue": "arXiv preprint arXiv:1207.0580, 2012.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2012}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["Nitish Srivastava", "Geoffrey Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov"], "venue": "The Journal of Machine Learning Research, vol. 15, no. 1, pp. 1929\u20131958, 2014.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 1929}, {"title": "Learning hidden unit contributions for unsupervised speaker adaptation of neural network acoustic models", "author": ["Pawel Swietojanski", "Steve Renals"], "venue": "Proc. SLT. IEEE, 2014, pp. 171\u2013176.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning hidden unit contributions for unsupervised acoustic model adaptation", "author": ["P Swietojanski", "J Li", "S Renals"], "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 24, no. 8, pp. 1450\u2013 1463, 2016.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2016}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation, vol. 9, no. 8, pp. 1735\u20131780, 1997.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 1997}, {"title": "Highway Long Short-Term Memory RNNs for Distant Speech Recognition", "author": ["Yu Zhang", "Guoguo Chen", "Dong Yu", "Kaisheng Yao", "Sanjeev Khudanpur", "James Glass"], "venue": "Proc. ICASSP, 2015.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2015}, {"title": "Recurrent highway networks", "author": ["Julian Georg. Zilly", "Rupesh Kumar Srivastava", "Koutnik Jan", "J\u00fcrgen Schmidhuber"], "venue": "arXiv preprint arXiv:1607.03474, 2016.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2016}, {"title": "Deep residual learning for image recognition", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"], "venue": "arXiv preprint arXiv:1512.03385, 2015.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2015}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["Karen Simonyan", "Andrew Zisserman"], "venue": "Proc. ICLR, 2015.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2015}, {"title": "Very deep convolutional neural networks for noise robust speech recognition", "author": ["Yanmin Qian", "Mengxiao Bi", "Tian Tan", "Kai Yu"], "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 24, no. 12, pp. 2263\u20132276, 2016.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2016}, {"title": "Distilling the knowledge in a neural network", "author": ["Geoffrey Hinton", "Oriol Vinyals", "Jeff Dean"], "venue": "Proc. NIPS Deep Learning and Representation Learning Workshop, 2015.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2015}, {"title": "Scalable minimum bayes risk training of deep neural network acoustic models using distributed hessian-free optimization", "author": ["Brian Kingsbury", "Tara N Sainath", "Hagen Soltau"], "venue": "Proc. INTERSPEECH, 2012.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2012}, {"title": "Sequence-discriminative training of deep neural networks", "author": ["K Vesel\u00fd", "A Ghoshal", "L Burget", "D Povey"], "venue": "Proc. INTERSPEECH, 2013.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2013}, {"title": "Error back propagation for sequence training of context-dependent deep networks for conversational speech transcription", "author": ["Hang Su", "Gang Li", "Dong Yu", "Frank Seide"], "venue": "Proc. ICASSP. IEEE, 2013, pp. 6664\u20136668.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2013}, {"title": "Hypothesis spaces for minimum bayes risk training in large vocabulary speech recognition", "author": ["Matthew Gibson", "Thomas Hain"], "venue": "Proc. INTERSPEECH. Citeseer, 2006.", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2006}, {"title": "Lattice-based optimization of sequence classification criteria for neural-network acoustic modeling", "author": ["Brian Kingsbury"], "venue": "Proc. ICASSP. IEEE, 2009, pp. 3761\u20133764.", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2009}, {"title": "Recognition and understanding of meetings the AMI and AMIDA projects", "author": ["Steve Renals", "Thomas Hain", "Herv\u00e9 Bourlard"], "venue": "Proc. ASRU. IEEE, 2007, pp. 238\u2013247.", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2007}, {"title": "Distant speech recognition experiments using the AMI Corpus", "author": ["S Renals", "P Swietojanski"], "venue": "New Era for Robust Speech Recognition \u2013 Exploting Deep Learning, S Watanabe, M Delcroix, F Metze, and JR Hershey, Eds. Springer, 2016.", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2016}, {"title": "An introduction to computational networks and the computational network toolkit", "author": ["Dong Yu", "Adam Eversole", "Mike Seltzer", "Kaisheng Yao", "Zhiheng Huang", "Brian Guenter", "Oleksii Kuchaiev", "Yu Zhang", "Frank Seide", "Huaming Wang"], "venue": "Tech. Rep., Tech. Rep. MSR, Microsoft Research, 2014.", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2014}, {"title": "The Kaldi speech recognition toolkit", "author": ["D Povey", "A Ghoshal", "G Boulianne", "L Burget", "O Glembek", "N Goel", "M Hannemann", "P Motl\u0131cek", "Y Qian", "P Schwarz", "J Silovsk\u00fd", "G Semmer", "K Vesel\u00fd"], "venue": "Proc. ASRU, 2011.", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2011}], "referenceMentions": [{"referenceID": 0, "context": "DEEP Learning has significantly advanced the state-ofthe-art in speech recognition over the past few years [1]\u2013 [3].", "startOffset": 107, "endOffset": 110}, {"referenceID": 2, "context": "DEEP Learning has significantly advanced the state-ofthe-art in speech recognition over the past few years [1]\u2013 [3].", "startOffset": 112, "endOffset": 115}, {"referenceID": 3, "context": "and hidden Markov model (NN/HMM) hybrid architecture, first investigated in the early 1990s [4], [5].", "startOffset": 92, "endOffset": 95}, {"referenceID": 4, "context": "and hidden Markov model (NN/HMM) hybrid architecture, first investigated in the early 1990s [4], [5].", "startOffset": 97, "endOffset": 100}, {"referenceID": 5, "context": "rank matrices [6], [7], teacher-student training [8]\u2013[10], and structured linear layers [11]\u2013[13].", "startOffset": 14, "endOffset": 17}, {"referenceID": 6, "context": "rank matrices [6], [7], teacher-student training [8]\u2013[10], and structured linear layers [11]\u2013[13].", "startOffset": 19, "endOffset": 22}, {"referenceID": 7, "context": "rank matrices [6], [7], teacher-student training [8]\u2013[10], and structured linear layers [11]\u2013[13].", "startOffset": 49, "endOffset": 52}, {"referenceID": 9, "context": "rank matrices [6], [7], teacher-student training [8]\u2013[10], and structured linear layers [11]\u2013[13].", "startOffset": 53, "endOffset": 57}, {"referenceID": 10, "context": "rank matrices [6], [7], teacher-student training [8]\u2013[10], and structured linear layers [11]\u2013[13].", "startOffset": 88, "endOffset": 92}, {"referenceID": 12, "context": "rank matrices [6], [7], teacher-student training [8]\u2013[10], and structured linear layers [11]\u2013[13].", "startOffset": 93, "endOffset": 97}, {"referenceID": 13, "context": "In this paper, we present a comprehensive study of smallfootprint acoustic models using highway deep neural networks (HDNNs), building on our previous studies [14]\u2013[16].", "startOffset": 159, "endOffset": 163}, {"referenceID": 15, "context": "In this paper, we present a comprehensive study of smallfootprint acoustic models using highway deep neural networks (HDNNs), building on our previous studies [14]\u2013[16].", "startOffset": 164, "endOffset": 168}, {"referenceID": 16, "context": "HDNNs are multi-layer networks which have shortcut connections between hidden layers [17].", "startOffset": 85, "endOffset": 89}, {"referenceID": 16, "context": "These gate functions are central to training very deep networks [17] and to speeding up convergence [14].", "startOffset": 64, "endOffset": 68}, {"referenceID": 13, "context": "These gate functions are central to training very deep networks [17] and to speeding up convergence [14].", "startOffset": 100, "endOffset": 104}, {"referenceID": 13, "context": "Besides, in contrast to training regular multi-layer networks of the same depth and width, which typically requires careful pretraining, we demonstrate that HDNNs may be trained using standard stochastic gradient descent without any pretraining [14].", "startOffset": 245, "endOffset": 249}, {"referenceID": 16, "context": "Furthermore, The authors in [17] only studied the constrained carry gate setting for HDNNs, while in this work we provide detailed comparisons of different gate functions in the context of speech recognition.", "startOffset": 28, "endOffset": 32}, {"referenceID": 17, "context": "term memory units (LSTM-RNNs) and convolutional neural networks (CNNs) can obtain higher recognition accuracy with fewer model parameters compared to DNNs [18], [19], they are computationally more expensive for applications on resource constrained platforms.", "startOffset": 155, "endOffset": 159}, {"referenceID": 18, "context": "term memory units (LSTM-RNNs) and convolutional neural networks (CNNs) can obtain higher recognition accuracy with fewer model parameters compared to DNNs [18], [19], they are computationally more expensive for applications on resource constrained platforms.", "startOffset": 161, "endOffset": 165}, {"referenceID": 19, "context": "Moreover, their accuracy can be transferred to a DNN by teacher-student training [?], [20],", "startOffset": 86, "endOffset": 90}, {"referenceID": 20, "context": "[21].", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "However, as the number of hidden layers increases, the error surface becomes increasingly non-convex, and it becomes more likely to find a poor local minimum using gradientbased optimization algorithms with random initialization [22].", "startOffset": 229, "endOffset": 233}, {"referenceID": 22, "context": "Furthermore the variance of the back-propagated gradients may become small in the lower layers if the model parameters are not initialized properly [23].", "startOffset": 148, "endOffset": 152}, {"referenceID": 23, "context": "There have been a variety of training algorithms, and model architectures, proposed to enable very deep multi-layer networks including pre-training [24], [25], normalised initial-", "startOffset": 148, "endOffset": 152}, {"referenceID": 24, "context": "There have been a variety of training algorithms, and model architectures, proposed to enable very deep multi-layer networks including pre-training [24], [25], normalised initial-", "startOffset": 154, "endOffset": 158}, {"referenceID": 22, "context": "isation [23], deeply-supervised networks [26], and batch normalisation [27].", "startOffset": 8, "endOffset": 12}, {"referenceID": 25, "context": "isation [23], deeply-supervised networks [26], and batch normalisation [27].", "startOffset": 41, "endOffset": 45}, {"referenceID": 26, "context": "isation [23], deeply-supervised networks [26], and batch normalisation [27].", "startOffset": 71, "endOffset": 75}, {"referenceID": 16, "context": "Highway deep neural networks (HDNNs) [17] were proposed to enable very deep networks to be trained by augmenting the hidden layers with gate functions:", "startOffset": 37, "endOffset": 41}, {"referenceID": 0, "context": "The outputs of T (\u00b7) and C(\u00b7) are constrained to be within [0, 1], and we use a sigmoid function for each, parameterized by WT and Wc respectively.", "startOffset": 59, "endOffset": 65}, {"referenceID": 13, "context": "Following our previous work [14], we tie the parameters in the gate functions across all the hidden layers, which can significantly save model parameters.", "startOffset": 28, "endOffset": 32}, {"referenceID": 27, "context": "tion [28], which may be written as", "startOffset": 5, "endOffset": 9}, {"referenceID": 27, "context": "where p( i) is a Bernoulli distribution for the i-th element in as originally proposed in [28]; it was shown later that using a continuous distribution with well designed mean and", "startOffset": 90, "endOffset": 94}, {"referenceID": 28, "context": "variance works as well or better [29].", "startOffset": 33, "endOffset": 37}, {"referenceID": 29, "context": "models [30], [31], which may be represented as", "startOffset": 7, "endOffset": 11}, {"referenceID": 30, "context": "models [30], [31], which may be represented as", "startOffset": 13, "endOffset": 17}, {"referenceID": 31, "context": "Both HDNNs and LSTM-RNNs [32] employ gate functions.", "startOffset": 25, "endOffset": 29}, {"referenceID": 32, "context": "Combinations of the two architectures have been explored recently: highway LSTMs [33] employ highway connections to train a stacked LSTM with multiple layers; recurrent highway networks [34] share gate functions to control the information flow in both time and model depth.", "startOffset": 81, "endOffset": 85}, {"referenceID": 33, "context": "Combinations of the two architectures have been explored recently: highway LSTMs [33] employ highway connections to train a stacked LSTM with multiple layers; recurrent highway networks [34] share gate functions to control the information flow in both time and model depth.", "startOffset": 186, "endOffset": 190}, {"referenceID": 34, "context": "residual network (ResNet) [35] was recently proposed to train very deep networks, advancing the state-of-the-art in computer vision.", "startOffset": 26, "endOffset": 30}, {"referenceID": 35, "context": "adapting approaches developed for visual object recognition [36], very deep CNN architectures have been investigated for speech recognition [37].", "startOffset": 60, "endOffset": 64}, {"referenceID": 36, "context": "adapting approaches developed for visual object recognition [36], very deep CNN architectures have been investigated for speech recognition [37].", "startOffset": 140, "endOffset": 144}, {"referenceID": 7, "context": "Minimizing this loss function is equivalent to minimizing the Kullback-Leibler (KL) divergence between the posterior probabilities of each class from the teacher and student models [8].", "startOffset": 181, "endOffset": 184}, {"referenceID": 37, "context": "[38] suggested that the small posterior probabilities are valuable information that encode correlations among different classes.", "startOffset": 0, "endOffset": 4}, {"referenceID": 37, "context": "Following [38], we applied the same temperature to the softmax functions in both the teacher and student networks in our experiments.", "startOffset": 10, "endOffset": 14}, {"referenceID": 38, "context": "While the previous two loss functions are defined at the frame level, sequence training defines the loss at the sequence level, which usually yields a significant improvement in speech recognition accuracy [39]\u2013[41].", "startOffset": 206, "endOffset": 210}, {"referenceID": 40, "context": "While the previous two loss functions are defined at the frame level, sequence training defines the loss at the sequence level, which usually yields a significant improvement in speech recognition accuracy [39]\u2013[41].", "startOffset": 211, "endOffset": 215}, {"referenceID": 41, "context": ",xT }, of length T , and a sequence of labels, Y , then the loss function from the state-level minimum Bayesian risk criterion (sMBR) [42], [43] is defined as", "startOffset": 134, "endOffset": 138}, {"referenceID": 42, "context": ",xT }, of length T , and a sequence of labels, Y , then the loss function from the state-level minimum Bayesian risk criterion (sMBR) [42], [43] is defined as", "startOffset": 140, "endOffset": 144}, {"referenceID": 39, "context": "In this paper, we focus on the sMBR criterion for sequence training since it can achieve comparable or slightly better results than training using the maximum mutual information (MMI) or minimum phone error (MPE) criteria [40].", "startOffset": 222, "endOffset": 226}, {"referenceID": 39, "context": "Only applying the sequence training criterion without regularization may lead to overfitting [40], [41].", "startOffset": 93, "endOffset": 97}, {"referenceID": 40, "context": "Only applying the sequence training criterion without regularization may lead to overfitting [40], [41].", "startOffset": 99, "endOffset": 103}, {"referenceID": 40, "context": "To address this problem, we interpolate the sMBR loss function with the CE loss [41], with smoothing parameter p \u2208 R,", "startOffset": 80, "endOffset": 84}, {"referenceID": 13, "context": "Although the number of parameters in the gate functions is still large compared to the amount of per-speaker adaptation data, the size of the gate functions may be controlled by reducing the number of hidden units, but maintaining the accuracy by increasing the depth [14].", "startOffset": 268, "endOffset": 272}, {"referenceID": 43, "context": "Our experiments were performed on the individual headset microphone (IHM) subset of the AMI meeting speech transcription corpus [44], [45].", "startOffset": 128, "endOffset": 132}, {"referenceID": 44, "context": "Our experiments were performed on the individual headset microphone (IHM) subset of the AMI meeting speech transcription corpus [44], [45].", "startOffset": 134, "endOffset": 138}, {"referenceID": 45, "context": "The results reported in this paper were obtained using the CNTK toolkit [46] with the Kaldi decoder [47], and the networks were trained", "startOffset": 72, "endOffset": 76}, {"referenceID": 46, "context": "The results reported in this paper were obtained using the CNTK toolkit [46] with the Kaldi decoder [47], and the networks were trained", "startOffset": 100, "endOffset": 104}, {"referenceID": 16, "context": "We also investigated constrained gates, in which C(\u00b7) = 1\u2212T (\u00b7) [17], which reduces the computational cost since the matrix-vector multiplication for the carry gate is not required.", "startOffset": 64, "endOffset": 68}, {"referenceID": 37, "context": "may play a lesser role, which supports the argument that they encode useful information for training the student model [38].", "startOffset": 119, "endOffset": 123}, {"referenceID": 7, "context": "We found that the sMBR-based teacher model can significantly improve the performance of the student model (similar to the results reported in [8]).", "startOffset": 142, "endOffset": 145}, {"referenceID": 14, "context": "2 in (17) and the default learning rate to be 10\u22125 following our previous work [15].", "startOffset": 79, "endOffset": 83}, {"referenceID": 14, "context": "During adaptation, we updated the SI model using 5 iterations with a fixed learning rate of 2 \u00d7 10\u22124 per sample following our previous setup [15].", "startOffset": 141, "endOffset": 145}, {"referenceID": 37, "context": "These results are also in line with the argument in [38] that the soft targets can work as a regularizer and can prevent the student model from overfitting.", "startOffset": 52, "endOffset": 56}], "year": 2017, "abstractText": "State-of-the-art speech recognition systems typically employ neural network acoustic models. However, compared to Gaussian mixture models, deep neural network (DNN) based acoustic models often have many more model parameters, making it challenging for them to be deployed on resource-constrained platforms, such as mobile devices. In this paper, we study the application of the recently proposed highway deep neural network (HDNN) for training small-footprint acoustic models. HDNNs are a depth-gated feedforward neural network, which include two types of gate functions to facilitate the information flow through different layers. Our study demonstrates that HDNNs are more compact than regular DNNs for acoustic modeling, i.e., they can achieve comparable recognition accuracy with many fewer model parameters. Furthermore, HDNNs are more controllable than DNNs: the gate functions of an HDNN can control the behavior of the whole network using a very small number of model parameters. Finally, we show that HDNNs are more adaptable than DNNs. For example, simply updating the gate functions using adaptation data can result in considerable gains in accuracy. We demonstrate these aspects by experiments using the publicly available AMI corpus, which has around 80 hours of training data.", "creator": "LaTeX with hyperref package"}}}