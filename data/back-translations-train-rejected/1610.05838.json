{"id": "1610.05838", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Oct-2016", "title": "CuMF_SGD: Fast and Scalable Matrix Factorization", "abstract": "Matrix factorization (MF) has been widely used in e.g., recommender systems, topic modeling and word embedding. Stochastic gradient descent (SGD) is popular in solving MF problems because it can deal with large data sets and is easy to do incremental learning. We observed that SGD for MF is memory bound. Meanwhile, single-node CPU systems with caching performs well only for small data sets; distributed systems have higher aggregated memory bandwidth but suffer from relatively slow network connection. This observation inspires us to accelerate MF by utilizing GPUs's high memory bandwidth and fast intra-node connection. We present cuMF_SGD, a CUDA-based SGD solution for large-scale MF problems. On a single CPU, we design two workload schedule schemes, i.e., batch-Hogwild! and wavefront-update that fully exploit the massive amount of cores. Especially, batch-Hogwild! as a vectorized version of Hogwild! overcomes the issue of memory discontinuity. We also develop highly-optimized kernels for SGD update, leveraging cache, warp-shuffle instructions and half-precision floats. We also design a partition scheme to utilize multiple GPUs while addressing the well-known convergence issue when parallelizing SGD. On three data sets with only one Maxwell or Pascal GPU, cuMF_SGD runs 3.1X-28.2X as fast compared with state-of-art CPU solutions on 1-64 CPU nodes. Evaluations also show that cuMF_SGD scales well on multiple GPUs in large data sets.", "histories": [["v1", "Wed, 19 Oct 2016 01:28:11 GMT  (749kb,D)", "https://arxiv.org/abs/1610.05838v1", null], ["v2", "Thu, 20 Oct 2016 13:38:34 GMT  (596kb,D)", "http://arxiv.org/abs/1610.05838v2", null], ["v3", "Thu, 10 Nov 2016 01:16:40 GMT  (703kb,D)", "http://arxiv.org/abs/1610.05838v3", null]], "reviews": [], "SUBJECTS": "cs.LG cs.NA", "authors": ["xiaolong xie", "wei tan", "liana l fong", "yun liang"], "accepted": false, "id": "1610.05838"}, "pdf": {"name": "1610.05838.pdf", "metadata": {"source": "CRF", "title": "CuMF_SGD: Fast and Scalable Matrix Factorization", "authors": ["Xiaolong Xie", "Wei Tan", "Liana L. Fong", "Yun Liang", "Thomas J. Watson"], "emails": ["ericlyun@pku.edu.cn", "llfong@us.ibm.com"], "sections": [{"heading": null, "text": "We present cuMF SGD, a CUDA-based SGD solution for large MF problems. On a single GPU, we design two scheduling schemes (Batch-Hogwild! and Wavefront Update) that fully exploit the enormous amount of cores. In particular, Batch-Hogwild!, a vectorized version of Hogwild!, overcomes the problem of memory discontinuity. We develop highly optimized cores for SGD updates, the use of cache, warp shuffle instructions, semi-precise floats, etc. We also design a partitioning scheme to use multiple GPUs while addressing the well-known convergence problem when parallelizing SGD. Evaluations of three datasets with only one Maxwell or Pascal GPU show that cuMF SGD 3.1X-28.2X runs as fast as state-of-the-art CPU solutions on 1-C64 SGU nodes, as we believe that Pascal SGU evaluations are based on large GU nodes or three."}, {"heading": "1. INTRODUCTION", "text": "That is, we have a natural connection to the embedding layers in a deep neural network. Let's take the receiver system as an example. Figure 1 shows a rating matrix R of m \u00d7 n, with sparse ratings from m users to n elements. We assume that R can be included in the multiplication of two low-ranking characteristics in the multiplication of two low-ranking characteristics, the P (m \u00d7 n), so that the derived characteristics P and Q can be used to predict the missing ratings in R, or as characteristics of the corresponding users / elements in the downstream machine learning tasks. We often have large data sets. We can use the number of users / millions of articles to predict the missing ratings in R, or as characteristics of the corresponding users / elements in the downstream factorizations."}, {"heading": "2. BACKGROUND", "text": "This section first briefly introduces the GPU architecture and the SGD matrix factorization algorithm, and then discusses the parallelism schemes for MF and SGD. We argue that the current blocking and matrix blocking schemes are not scalable to massive GPU cores, leading to the innovations presented in sections 3 and 4."}, {"heading": "2.1 The Compute Architecture", "text": "In order to overcome the limited memory bandwidth of a single CPU node and the limited network bandwidth of distributed systems, we use a heterogeneous platform with GPUs, as shown in Figure 3. GPUs have high internal memory bandwidth and are connected via PCIe or NVLink [23] to high internal bandwidth. CPUs handle data pre-processing, data movement and work scheduling at the highest level, while GPUs handle functional updates with enormous parallels. GPUs are throughput-oriented processors [28] with thousands of cores and high bandwidth memory (200-800 GB / s). In order to exploit the full power potential, GPU applications must be carefully designed to use the data and the parallelism of computation. In the next two subsections, we show that SGD is not trivial, as SGD is serial by nature."}, {"heading": "2.2 Stochastic Gradient Descent", "text": "The aim of matrix factorization is to train a m \u00b7 k attribute matrix P and a k \u00b7 n attribute matrix Q in such a way that: R \u2248 P \u00b7 QThe training process of matrix factorization consists of minimizing the following cost function: [u, v \u2212 R (ru, v \u2212 puqv) 2 + \u03bbp | | Pu | | 2 + \u03bbq | | | 2, where \u03bbp and \u03bbq are regulation parameters to avoid overmatch, and N is the number of non-zero samples in matrix R. The key idea of the SGD is to randomly select a sample in each step, e.g. ru, v from R, in order to calculate the course."}, {"heading": "2.3 Parallelization Schemes", "text": "In fact, most of them are unable to play by the rules."}, {"heading": "3. SINGLE GPU IMPLEMENTATION", "text": "This section describes how cuMF solves SGD MF with a GPU. We assume that all the required data is in the memory of the GPU. We discuss the implementation of multi-GPU in Section 4. We need to solve two problems in a single GPU. Section 3.1 deals with the problem of calculation optimization, i.e. optimizing each and every SGD update by using GPU hardware. Section 3.2 deals with work scheduling, i.e. distributing the many SGD updates over thousands of concurrent GPU threads."}, {"heading": "3.1 Computation Optimization", "text": "This year it has come to the point that it has never come as far as this year."}, {"heading": "3.2 Workload Scheduling", "text": "This year it is so far that it will only take one year to move on to the next round."}, {"heading": "4. SCALE TO LARGE DATA SETS", "text": "Section 3 describes how to solve MF in a single GPU, provided the evaluation matrix and feature matrices are fully in GPU memory. However, the limited GPU storage capacity [34] prevents cuMF SGD from solving major problems. For example, the NVIDIA TITAN X GPU has 12GB of memory, which can store only 1 billion samples (one sample needs a float and two integers). Nowadays, real-world problems can have 1011 samples [6]. Techniques such as Unified Virtual Memory [28] allow the GPU to use CPU memory, but with high overhead. Considering these factors to solve major MF problems that do not fit into the memory of a GPU, we need to partition the data sets and divide the partitions on GPUs into batches. Furthermore, we should overlap the data transfer with the calculation to reduce the delay caused by PU memory transfer."}, {"heading": "4.1 Partition to Multiple GPUs", "text": "The main idea is to split the rating matrix R into several blocks; each block is small enough to fit into the memory of a GPU so that independent blocks can be updated simultaneously on different GPUs. The multiple GPU solution works as follows: 1. Divide the rating matrix R into i \u00d7 j blocks. 3. Divide the feature matrix p into i segments and the corresponding feature matrix q into j segments accordingly. 2. When a GPU is idle, randomly select a matrix block from these independent blocks and send it to the GPU. 3. Transfer the matrix block and the corresponding characteristics sub-matrices p and q to the GPU. Then update the matrix block using the individual GPU implementation discussed in Section 3. After the update, p and q are transferred back to the CPU. 4. Iterate from 2 to the number specified by the GPU implementation."}, {"heading": "4.2 Overlap Data Transfer and Compute", "text": "The GPU memory bandwidth is much larger than the CPUGPU memory transfer bandwidth. For example, NVIDIATITAN X GPUs offer a memory bandwidth of 360 GB / s, while the CPU GPU memory bandwidth is only 16 GB / s (PCIe v3 16x). In the single GPU implementation, the CPUGPU memory transfer occurs only at the beginning and end of the MF and is therefore not dominant. However, if the dataset cannot fit into the GPU memory, the memory transfer often happens and has a greater impact on overall performance. Given the amount of memory transfer time, we overlap the memory transfers and the calculation when solving major problems, as shown in Figure 10 (b). Due to space constraints, we can only draw one GPU block."}, {"heading": "4.3 Implementation Details", "text": "Multiple GPU Management. We implement it with multiple CPU threads within a process. Within the process, there is a host thread and multiple worker threads, with each GPU bound to a worker thread. The host thread manages workload planning and informs the worker threads about the planning decision. Each worker thread then starts data transfer and starts computing kernels on a GPU overlapp. each worker thread is responsible for overlapping the computing and CPU GPU memory transfers. To achieve this, we use CUDA streams. A stream contains a list of GPU commands that are executed serially, and commands in different streams are executed in parallel if hardware resources allow."}, {"heading": "5. EXPERIMENTS", "text": "We implement cuMF SGD using CUDA C (source code at http: / / github.com / cumf / cumf _ sgd /), evaluate its performance on public records, and demonstrate its advantage in terms of performance and cost. Section 5.1 introduces the experimental environment. The following experiments are designed to answer these questions: \u2022 Compared with modern SGD-based approaches on CPUs [7, 10], is cuMF SGD better and why? (Section 5.2) \u2022 What implies the use of different generations of GPUs? (Section 5.3) \u2022 Compared with the ALS-based GPU library cuMF ALS that we published earlier [6], what is the benefit of cuMF SGD? (Section 5.4) \u2022 Parallelizing SGD is always difficult and can lead to converging problems."}, {"heading": "5.1 Experimental Setup", "text": "Platform. We evaluate cuMF SGD on heterogeneous platforms with CPU and GPUs. Table 1 shows the configuration of the two servers used in experiments.Datasets. We use three public datasets: Netflix, Yahoo! Music and Hugewiki. Details are given in Table 2. Netflix and Yahoo! Music contain a test set, but Hugewiki does not. We sample and extract 1% of the dataset for testing purposes. Parameters. As mentioned in the introduction, this essay focuses on the system level, but not on optimization at the algorithmic level. Therefore, we have not invested much effort in rotating parameters. Instead, we use the parameters adopted from previous work [6, 10, 5, 7]. For the learning rate, we adopt the techniques used by Yun et al. [10] to plan the learning rate, where the learning rate is reduced monotonically in the epoch: st = 1 \u00d7 1.5 of Yun et."}, {"heading": "5.2 Comparison of SGD approaches", "text": "In fact, most of them are able to survive on their own, and they see themselves able to survive on their own."}, {"heading": "5.3 Implication of GPU Architectures", "text": "We believe that cuMF SGD is capable of scaling to future GPU architectures with little tuning effort. In this section, we explain the performance gap between Maxwell and Pascal in three aspects: computing resources, non-chip memory bandwidth, and CPU memory bandwidth. Results show that the Pascal platform scales to more parallel workers and achieves much higher # updates / s than Maxwell. This is because the Maxwell platform has 24 streaming multiprocessors (SMs) within each GPU, with each SM allowing up to 32 parallel workers (thread blocks)."}, {"heading": "5.4 Comparison with cuMF_ALS", "text": "We use a GPU for cuMF SGD and one and four GPUs for cuMF ALS-4. Figure 13 compares their performance with three sets of data on Maxwell. We observe that cuMF SGD is faster than cuMF ALS-1 and a similar performance with cuMF ALS-4 is faster than cuMF ALS-4 with only one GPU.It was expected that cuMF SGD is faster than cuMF ALS for the following reason. Each epoch of SGD needs memory access to O (N-k) and computation of O (N-k) and computation of O (N-k2 + (m + n). Each epoch of SGD needs memory access to O (m + n) and computation of O (N-k)."}, {"heading": "5.5 Convergence Analysis", "text": "The original SGD algorithm is serial. To speed it up, we will discuss how to split it between a GPU in Section 3.2 and several GPUs in Section 4.1. It is well known that SGD parallelism can have subtle effects on convergence [5, 10]. However, in the context of matrix factorization, the implication varies between the two schemes proposed in Section 3.2: Hogwild! and Matrix Blocking. Hogwild! For a GPU, Section 3.2.2 proposes the Batch Hogwild! schema for partitioning. As a vectorized version of Hogwild! inherits the restriction from Hogwild!"}, {"heading": "5.6 Scale Up to Multiple GPUs", "text": "System-wise, cuMF SGD is designed to scale to multiple GPUs. Algorithmically, however, scaling is limited by factors such as problem dimension and number of parallel workers, as already described in Section 5.5. Of the three datasets used in this paper, Netflix and Hugewiki have very small n (20k, 40k, receptive), which prevents cuMF SGD from solving them on multiple GPUs. By comparison, Yahoo! Music can be solved on multiple GPUs, as the dimension of the R is 1M x 625k. We split its R into 8 x 8 blocks and run it with two Pascal GPUs. Figure 17 shows the convergence speed. With 2 Pascal GPUs, cuMF SGD takes 2.5 s to converge to RMSE 22, which is 1.5 times faster than 1 Pascal GPU (3.8 s). The reason behind this sublinear scalability is that the Multi GPU needs synchroniPU MSF to time to iPD."}, {"heading": "6. RELATED WORK", "text": "algorithms. SGD is widely used to solve matrix factorization [1]. SGD can be paralleled to achieve better performance. Of course, ALS is easy to parallelise and can also be used in dense matrix factorization. In parallel, SGD solutions are discussed in multicore [9, 36], multi-nodes [10, 37], MapReduce [27, 38] and parameter servers [39, 40]. SGD solutions are discussed in multicore."}, {"heading": "7. CONCLUSION", "text": "We propose a GPU-based solution by noting that GPUs can provide ample memory bandwidth and have a fast connection within a node. We design workload sharing and schedules to distribute tasks within a GPU and across multiple GPUs without compromising the randomness required by SGD. We also develop highly optimized GPU kernels for customized SGD updates. cuMF SGD 3.1X28.2X runs as fast as modern CPU solutions on 1-64 CPU nodes with just one Maxwell or Pascal GPU. Analyses also show that cuMF SGD is well scaled to multiple GPU in large datasets."}, {"heading": "8. REFERENCES", "text": "[1] Y. Koren, R. Bell, and C. Volinsky, \"Matrix factorizationtechniques for commender systems,\" Computer. [2] \"Recommending items to more than-a-billion-people /. [3] J. Pennington, R. Socher, and C. D. Manning,\" Glove: Global vectors for word representation, \"in EMNLP, 2014. [4] M. Sarwat, Database Management System Support for Collaborative Filtering System Support for Collaborative Recommender Systems, R. Socher, and C. D. Manning. [5] W.-S. Chin, Y. Zhuang, Y.-C. Juan, and C.-J. Lin,\" An almost parallel gradient method for matrix factorization in shared memory systems,. \""}], "references": [{"title": "Matrix factorization techniques for recommender systems", "author": ["Y. Koren", "R. Bell", "C. Volinsky"], "venue": "Computer.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 0}, {"title": "Glove: Global vectors for word representation", "author": ["J. Pennington", "R. Socher", "C.D. Manning"], "venue": "EMNLP, 2014.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2014}, {"title": "Database Management System Support for Collaborative Filtering Recommender Systems", "author": ["M. Sarwat"], "venue": "PhD thesis, UNIVERSITY OF MINNESOTA,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2014}, {"title": "A fast parallel stochastic gradient method for matrix factorization in shared memory systems", "author": ["W.-S. Chin", "Y. Zhuang", "Y.-C. Juan", "C.-J. Lin"], "venue": "ACM Transactions on Intelligent Systems and Technology (TIST), 2015.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2015}, {"title": "Faster and cheaper: Parallelizing large-scale matrix factorization on gpus", "author": ["W. Tan", "L. Cao", "L. Fong"], "venue": "Proceedings of the 25th ACM International Symposium on High-Performance Parallel and Distributed Computing, HPDC \u201916, 2016.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2016}, {"title": "A learning-rate schedule for stochastic gradient methods to matrix factorization", "author": ["W.-S. Chin", "Y. Zhuang", "Y.-C. Juan", "C.-J. Lin"], "venue": "Pacific-Asia Conference on Knowledge Discovery and Data Mining, Springer, 2015.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2015}, {"title": "Distributed matrix completion", "author": ["C. Teflioudi", "F. Makari", "R. Gemulla"], "venue": "2012 IEEE 12th International Conference on Data Mining, IEEE, 2012.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2012}, {"title": "Scalable coordinate descent approaches to parallel matrix factorization for recommender systems", "author": ["H.-F. Yu", "C.-J. Hsieh", "S. Si", "I. Dhillon"], "venue": "2012 IEEE 12th International Conference on Data Mining, IEEE, 2012.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2012}, {"title": "Nomad: Non-locking, stochastic multi-machine algorithm for asynchronous and decentralized matrix completion", "author": ["H. Yun", "H.-F. Yu", "C.-J. Hsieh", "S.V.N. Vishwanathan", "I. Dhillon"], "venue": "Proc. VLDB Endow., 2014.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2014}, {"title": "Hogwild: A lock-free approach to parallelizing stochastic gradient descent", "author": ["B. Recht", "C. Re", "S. Wright", "F. Niu"], "venue": "Advances in Neural Information Processing Systems, pp. 693\u2013701, 2011.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2011}, {"title": "Fast differentially private matrix factorization", "author": ["Z. Liu", "Y.-X. Wang", "A. Smola"], "venue": "Proceedings of the 9th ACM Conference on Recommender Systems, RecSys\u201915, 2015.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2015}, {"title": "Design and evaluation of main memory hash join algorithms for multi-core cpus", "author": ["S. Blanas", "Y. Li", "J.M. Patel"], "venue": "Proceedings of the 2011 ACM SIGMOD International Conference on Management of Data, SIGMOD \u201911, 2011.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2011}, {"title": "Husky: Towards a more efficient and expressive distributed computing framework", "author": ["F. Yang", "J. Li", "J. Cheng"], "venue": "Proceedings of the VLDB Endowment, 2016.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2016}, {"title": "Distributed graphlab: a framework for machine learning and data mining in the cloud", "author": ["Y. Low", "D. Bickson", "J. Gonzalez", "C. Guestrin", "A. Kyrola", "J.M. Hellerstein"], "venue": "Proceedings of the VLDB Endowment, 2012.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2012}, {"title": "Work-efficient parallel skyline computation for the gpu", "author": ["K.S. B\u00f8gh", "S. Chester", "I. Assent"], "venue": "Proc. VLDB Endow., 2015.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}, {"title": "Mega-kv: a case for gpus to maximize the throughput of in-memory key-value stores", "author": ["K. Zhang", "K. Wang", "Y. Yuan", "L. Guo", "R. Lee", "X. Zhang"], "venue": "Proceedings of the VLDB Endowment, 2015.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2015}, {"title": "Concurrent analytical query processing with gpus", "author": ["K. Wang", "K. Zhang", "Y. Yuan", "S. Ma", "R. Lee", "X. Ding", "X. Zhang"], "venue": "Proceedings of the VLDB Endowment, 2014.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2014}, {"title": "Gpu-accelerated string matching for database applications", "author": ["E.A. Sitaridi", "K.A. Ross"], "venue": "The VLDB Journal, 2016.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2016}, {"title": "Generic inverted index on the gpu", "author": ["J. Zhou", "Q. Guo", "H. Jagadish", "W. Luan", "A.K. Tung", "Y. Yang", "Y. Zheng"], "venue": "arXiv preprint arXiv:1603.08390, 2016.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2016}, {"title": "A gpgpu compiler for memory optimization and parallelism management", "author": ["Y. Yang", "P. Xiang", "J. Kong", "H. Zhou"], "venue": " Proceedings of the 31st ACM SIGPLAN Conference on Programming Language Design and Implementation, PLDI \u201910, (New York, NY, USA), pp. 86\u201397, ACM, 2010.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2010}, {"title": "Parallelizing astronomical source extraction on the gpu", "author": ["B. Zhao", "Q. Luo", "C. Wu"], "venue": "eScience (eScience), 2013 IEEE 9th International Conference on, 2013.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2013}, {"title": "Scalable task-parallel sgd on matrix factorization in multicore architectures", "author": ["Y. Nishioka", "K. Taura"], "venue": "Proceedings of the 2015 IEEE International Parallel and Distributed Processing Symposium Workshop, IPDPSW \u201915, (Washington, DC, USA), pp. 1178\u20131184, IEEE Computer Society, 2015.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2015}, {"title": "Large-scale matrix factorization with distributed stochastic gradient descent", "author": ["R. Gemulla", "E. Nijkamp", "P.J. Haas", "Y. Sismanis"], "venue": "Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and data mining, ACM, 2011.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2011}, {"title": "Exploiting simd for complex numerical predicates", "author": ["D. Song", "S. Chen"], "venue": "2016 IEEE 32nd International Conference on Data Engineering Workshops (ICDEW), 2016.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2016}, {"title": "An efficient compiler framework for cache bypassing on gpus", "author": ["X. Xie", "Y. Liang", "G. Sun", "D. Chen"], "venue": "IEEE/ACM International Conference on Computer-Aided Design, 2013.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2013}, {"title": "A GPGPU compiler for memory optimization and parallelism management", "author": ["Y. Yang", "P. Xiang", "J. Kong", "H. Zhou"], "venue": "2010 ACM SIGPLAN Conference on Programming Language Design and Implementation, PLDI \u201910, pp. 86\u201397, 2010.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2010}, {"title": "Enabling efficient intra-warp communication for fourier transforms in a many-core architecture", "author": ["C. del Mundo", "W.-c. Feng"], "venue": "Supercomputing, 2013. Proceedings of the 2013 ACM/IEEE International Conference on, 2013.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2013}, {"title": "Performance upper bound analysis and optimization of sgemm on fermi and kepler gpus", "author": ["J. Lai", "A. Seznec"], "venue": "Proceedings of the 2013 IEEE/ACM International Symposium on Code Generation and Optimization(CGO), 2013.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2013}, {"title": "Optimization principles and application performance evaluation of a multithreaded gpu using cuda", "author": ["S. Ryoo", "C.I. Rodrigues", "S.S. Baghsorkhi", "S.S. Stone", "D.B. Kirk", "W.-m. W. Hwu"], "venue": "Proceedings of the 13th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, PPoPP \u201908, 2008.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2008}, {"title": "Fast coordinate descent methods with variable selection for non-negative matrix factorization", "author": ["C.-J. Hsieh", "I.S. Dhillon"], "venue": "Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and data mining, ACM, 2011.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2011}, {"title": "Fast and robust parallel sgd matrix factorization", "author": ["J. Oh", "W.-S. Han", "H. Yu", "X. Jiang"], "venue": "Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, ACM, 2015.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2015}, {"title": "Distributed matrix completion", "author": ["C. Teflioudi", "F. Makari", "R. Gemulla"], "venue": "IEEE 12th International Conference on Data Mining, IEEE, 2012.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2012}, {"title": "Sparkler: supporting large-scale matrix factorization", "author": ["B. Li", "S. Tata", "Y. Sismanis"], "venue": "Proceedings of the 16th International Conference on Extending Database Technology, ACM, 2013.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2013}, {"title": "Factorbird-a parameter server approach to distributed matrix factorization", "author": ["S. Schelter", "V. Satuluri", "R. Zadeh"], "venue": "arXiv preprint arXiv:1411.0602, 2014.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2014}, {"title": "Exploiting bounded staleness to speed up big data analytics", "author": ["H. Cui", "J. Cipar", "Q. Ho", "J.K. Kim", "S. Lee", "A. Kumar", "J. Wei", "W. Dai", "G.R. Ganger", "P.B. Gibbons"], "venue": "USENIX Annual Technical Conference (USENIX ATC), 2014.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2014}, {"title": "Mllib: Machine learning in apache spark", "author": ["X. Meng", "J. Bradley", "B. Yuvaz", "E. Sparks", "S. Venkataraman", "D. Liu", "J. Freeman", "D. Tsai", "M. Amde", "S. Owen"], "venue": "JMLR, 2016.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2016}, {"title": "Large-scale parallel collaborative filtering for the netflix prize", "author": ["Y. Zhou", "D. Wilkinson", "R. Schreiber", "R. Pan"], "venue": "International Conference on Algorithmic Applications in Management, Springer, 2008.", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2008}, {"title": "Accelerating collaborative filtering using concepts from high performance computing", "author": ["M. Gates", "H. Anzt", "J. Kurzak", "J. Dongarra"], "venue": "Big Data, 2015 IEEE International Conference on, 2015.", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2015}, {"title": "Gpu-accelerated restricted boltzmann machine for collaborative filtering", "author": ["X. Cai", "Z. Xu", "G. Lai", "C. Wu", "X. Lin"], "venue": "International Conference on Algorithms and Architectures for Parallel Processing, Springer, 2012. 12", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2012}, {"title": "Stochastic gradient descent with gpgpu", "author": ["D. Zastrau", "S. Edelkamp"], "venue": "Annual Conference on Artificial Intelligence, Springer, 2012. 13", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2012}], "referenceMentions": [{"referenceID": 0, "context": "Matrix factorization (MF) has been widely used in recommender systems [1] by many companies (i.", "startOffset": 70, "endOffset": 73}, {"referenceID": 1, "context": "It can also be used in topic modeling, word embedding [3], database system [4],", "startOffset": 54, "endOffset": 57}, {"referenceID": 2, "context": "It can also be used in topic modeling, word embedding [3], database system [4],", "startOffset": 75, "endOffset": 78}, {"referenceID": 3, "context": "As previous works show that CGD is prone to reach local optima [5], we do not focus on it in this paper.", "startOffset": 63, "endOffset": 66}, {"referenceID": 0, "context": "ALS is easy to parallelize, and able to deal with implicit feedback such as purchase history [1].", "startOffset": 93, "endOffset": 96}, {"referenceID": 4, "context": "With our previous work already tackled ALS [6], we focus on SGD in this paper.", "startOffset": 43, "endOffset": 46}, {"referenceID": 5, "context": "The algorithmic stream tries to optimize update schemes such as learning rate in gradient descent, in order to reduce the number of epochs (an epoch is a full pass through the training set) needed to converge [7].", "startOffset": 209, "endOffset": 212}, {"referenceID": 4, "context": "The system stream tries to accelerate the computation, in order to run each epoch faster [6, 5, 8, 9, 10, 11].", "startOffset": 89, "endOffset": 109}, {"referenceID": 3, "context": "The system stream tries to accelerate the computation, in order to run each epoch faster [6, 5, 8, 9, 10, 11].", "startOffset": 89, "endOffset": 109}, {"referenceID": 6, "context": "The system stream tries to accelerate the computation, in order to run each epoch faster [6, 5, 8, 9, 10, 11].", "startOffset": 89, "endOffset": 109}, {"referenceID": 7, "context": "The system stream tries to accelerate the computation, in order to run each epoch faster [6, 5, 8, 9, 10, 11].", "startOffset": 89, "endOffset": 109}, {"referenceID": 8, "context": "The system stream tries to accelerate the computation, in order to run each epoch faster [6, 5, 8, 9, 10, 11].", "startOffset": 89, "endOffset": 109}, {"referenceID": 9, "context": "The system stream tries to accelerate the computation, in order to run each epoch faster [6, 5, 8, 9, 10, 11].", "startOffset": 89, "endOffset": 109}, {"referenceID": 3, "context": "State-of-art SGD-based MF solutions are based on either shared-memory multi-threading [5] or distributed systems [10].", "startOffset": 86, "endOffset": 89}, {"referenceID": 8, "context": "State-of-art SGD-based MF solutions are based on either shared-memory multi-threading [5] or distributed systems [10].", "startOffset": 113, "endOffset": 117}, {"referenceID": 3, "context": "Shared-memory CPU systems [5, 12, 13] rely heavily on cache to achieve high memory throughput.", "startOffset": 26, "endOffset": 37}, {"referenceID": 10, "context": "Shared-memory CPU systems [5, 12, 13] rely heavily on cache to achieve high memory throughput.", "startOffset": 26, "endOffset": 37}, {"referenceID": 11, "context": "Shared-memory CPU systems [5, 12, 13] rely heavily on cache to achieve high memory throughput.", "startOffset": 26, "endOffset": 37}, {"referenceID": 3, "context": "To understand this, we evaluate a single-node and shared-memory MF library LIBMF [5] with three data sets (details shown in Section 5).", "startOffset": 81, "endOffset": 84}, {"referenceID": 12, "context": "Distributed systems are frequently used to accelerate timeconsuming applications [14, 15].", "startOffset": 81, "endOffset": 89}, {"referenceID": 13, "context": "Distributed systems are frequently used to accelerate timeconsuming applications [14, 15].", "startOffset": 81, "endOffset": 89}, {"referenceID": 8, "context": "Figure 2(b) evaluates NOMAD [10], a distributed MF system.", "startOffset": 28, "endOffset": 32}, {"referenceID": 17, "context": "Secondly, GPUs do not rely on cache to reduce latency; instead, they rely on thousands of concurrent threads running on hundreds of cores to achieve high throughput [21, 22].", "startOffset": 165, "endOffset": 173}, {"referenceID": 18, "context": "Secondly, GPUs do not rely on cache to reduce latency; instead, they rely on thousands of concurrent threads running on hundreds of cores to achieve high throughput [21, 22].", "startOffset": 165, "endOffset": 173}, {"referenceID": 19, "context": "Due to the architecture distinct, simply mapping CPUs\u2019 algorithms to GPUs will lead to extremely low performance and suboptimal resources usage [24, 25].", "startOffset": 144, "endOffset": 152}, {"referenceID": 20, "context": "Due to the architecture distinct, simply mapping CPUs\u2019 algorithms to GPUs will lead to extremely low performance and suboptimal resources usage [24, 25].", "startOffset": 144, "endOffset": 152}, {"referenceID": 21, "context": "Moreoever, SGD is inherently serial, studies [26] have shown that existing MF solutions do not scale well using merely 30 threads.", "startOffset": 45, "endOffset": 49}, {"referenceID": 9, "context": "Inspired by the lock-free [11] and the block-based [27, 5] approaches, and given the separated CPU/GPU memory space, cuMF SGD adopts a hybrid twolevel execution scheme.", "startOffset": 26, "endOffset": 30}, {"referenceID": 22, "context": "Inspired by the lock-free [11] and the block-based [27, 5] approaches, and given the separated CPU/GPU memory space, cuMF SGD adopts a hybrid twolevel execution scheme.", "startOffset": 51, "endOffset": 58}, {"referenceID": 3, "context": "Inspired by the lock-free [11] and the block-based [27, 5] approaches, and given the separated CPU/GPU memory space, cuMF SGD adopts a hybrid twolevel execution scheme.", "startOffset": 51, "endOffset": 58}, {"referenceID": 3, "context": "Ideally, SGD can be parallelized without losing accuracy, if we update independent samples in parallel, and update dependent samples sequentially [5].", "startOffset": 146, "endOffset": 149}, {"referenceID": 3, "context": "The workload scheduling policies in existing work [5, 10, 26, 11, 27] can be divided into two categories, Hogwild! and matrix-blocking.", "startOffset": 50, "endOffset": 69}, {"referenceID": 8, "context": "The workload scheduling policies in existing work [5, 10, 26, 11, 27] can be divided into two categories, Hogwild! and matrix-blocking.", "startOffset": 50, "endOffset": 69}, {"referenceID": 21, "context": "The workload scheduling policies in existing work [5, 10, 26, 11, 27] can be divided into two categories, Hogwild! and matrix-blocking.", "startOffset": 50, "endOffset": 69}, {"referenceID": 9, "context": "The workload scheduling policies in existing work [5, 10, 26, 11, 27] can be divided into two categories, Hogwild! and matrix-blocking.", "startOffset": 50, "endOffset": 69}, {"referenceID": 22, "context": "The workload scheduling policies in existing work [5, 10, 26, 11, 27] can be divided into two categories, Hogwild! and matrix-blocking.", "startOffset": 50, "endOffset": 69}, {"referenceID": 9, "context": "Hogwild!(Figure 5(a)) is a lock-free approach to parallelize SGD[11].", "startOffset": 64, "endOffset": 68}, {"referenceID": 8, "context": "Matrix-blocking is used by many recent work [10, 5, 26, 27].", "startOffset": 44, "endOffset": 59}, {"referenceID": 3, "context": "Matrix-blocking is used by many recent work [10, 5, 26, 27].", "startOffset": 44, "endOffset": 59}, {"referenceID": 21, "context": "Matrix-blocking is used by many recent work [10, 5, 26, 27].", "startOffset": 44, "endOffset": 59}, {"referenceID": 22, "context": "Matrix-blocking is used by many recent work [10, 5, 26, 27].", "startOffset": 44, "endOffset": 59}, {"referenceID": 21, "context": "This global scheduler has been shown not scalable to many-core architectures [26].", "startOffset": 77, "endOffset": 81}, {"referenceID": 23, "context": "GPUs are SIMD architectures [29], where a thread block is a vector group.", "startOffset": 28, "endOffset": 32}, {"referenceID": 24, "context": "While many GPU applications do not benefit from cache due to cache contention [30], some memory instructions may benefit from cache as the accessed data may be frequently reused in the future (temporal reuse) or by other threads (spatial reuse).", "startOffset": 78, "endOffset": 82}, {"referenceID": 24, "context": "Following the model provided by [30], we observe that the memory load of the rating matrix benefits from cache and use the intrinsic instruction ldg [28] to enable cache-assisted read.", "startOffset": 32, "endOffset": 36}, {"referenceID": 25, "context": "On GPUs, when threads within one warp access the data within one cache line, the access is coalesced to minimize the bandwidth consumption [31].", "startOffset": 139, "endOffset": 143}, {"referenceID": 26, "context": "Warp shuffle instructions [32] are used to compute the dot product p \u00b7 q and broadcast the result.", "startOffset": 26, "endOffset": 30}, {"referenceID": 27, "context": "Register file is an important resource on GPUs [33].", "startOffset": 47, "endOffset": 51}, {"referenceID": 3, "context": "Specifically, we select a representative system LIBMF [5], a shared memory SGD solution to MF.", "startOffset": 54, "endOffset": 57}, {"referenceID": 21, "context": "However, we and others [26] observe that LIBMF faces scalability issues because of the global scheduling table it uses.", "startOffset": 23, "endOffset": 27}, {"referenceID": 21, "context": "Evaluations show that the performance of LIBMF saturates around 30 concurrent workers (CPU threads), which is consistent with the previous study [26].", "startOffset": 145, "endOffset": 149}, {"referenceID": 9, "context": "We propose batch-Hogwild!, a variant of Hogwild! [11] with better cache efficiency.", "startOffset": 49, "endOffset": 53}, {"referenceID": 3, "context": "It is not efficient, however, in terms of data locality [5].", "startOffset": 56, "endOffset": 59}, {"referenceID": 3, "context": "As discussed, existing scheduling schemes [5, 27] impose a global synchronization, where all workers look up a global table to find both row and column coordinates to update.", "startOffset": 42, "endOffset": 49}, {"referenceID": 22, "context": "As discussed, existing scheduling schemes [5, 27] impose a global synchronization, where all workers look up a global table to find both row and column coordinates to update.", "startOffset": 42, "endOffset": 49}, {"referenceID": 3, "context": "There are two main benefits by doing so: (1) reduce the two-dimension look-up table in [5, 27] to an one-dimension array, (2) minimize the workload imbalance problem, as a worker can start the next block earlier compared to waiting for all other workers to finish.", "startOffset": 87, "endOffset": 94}, {"referenceID": 22, "context": "There are two main benefits by doing so: (1) reduce the two-dimension look-up table in [5, 27] to an one-dimension array, (2) minimize the workload imbalance problem, as a worker can start the next block earlier compared to waiting for all other workers to finish.", "startOffset": 87, "endOffset": 94}, {"referenceID": 28, "context": "However, the limited GPU memory capacity [34] prevents cuMF SGD from solving large scale problems.", "startOffset": 41, "endOffset": 45}, {"referenceID": 4, "context": "Nowadays, real-world problems may have 10 samples [6].", "startOffset": 50, "endOffset": 53}, {"referenceID": 21, "context": "We mentioned LIBMF faces scalability issue, as the scheduling overhead increases quickly with the number of workers [26].", "startOffset": 116, "endOffset": 120}, {"referenceID": 5, "context": "\u2022 Compared with state-of-the-art SGD-based approaches on CPUs [7, 10], is cuMF SGD better and why? (Section 5.", "startOffset": 62, "endOffset": 69}, {"referenceID": 8, "context": "\u2022 Compared with state-of-the-art SGD-based approaches on CPUs [7, 10], is cuMF SGD better and why? (Section 5.", "startOffset": 62, "endOffset": 69}, {"referenceID": 4, "context": "3) \u2022 Compared with the ALS-based GPU library cuMF ALS that we published earlier [6], what is the advantage of cuMF SGD? (Section 5.", "startOffset": 80, "endOffset": 83}, {"referenceID": 4, "context": "Instead, we use the parameters adopted by earlier work [6, 10, 5, 7].", "startOffset": 55, "endOffset": 68}, {"referenceID": 8, "context": "Instead, we use the parameters adopted by earlier work [6, 10, 5, 7].", "startOffset": 55, "endOffset": 68}, {"referenceID": 3, "context": "Instead, we use the parameters adopted by earlier work [6, 10, 5, 7].", "startOffset": 55, "endOffset": 68}, {"referenceID": 5, "context": "Instead, we use the parameters adopted by earlier work [6, 10, 5, 7].", "startOffset": 55, "endOffset": 68}, {"referenceID": 8, "context": "[10], where the learning rate st at epoch t is monotonically reduced in the following routine:", "startOffset": 0, "endOffset": 4}, {"referenceID": 3, "context": "\u2022 LIBMF [5].", "startOffset": 8, "endOffset": 11}, {"referenceID": 5, "context": "It leverages SSE instructions and a novel learning rate schedule to speed up the convergence [7].", "startOffset": 93, "endOffset": 96}, {"referenceID": 8, "context": "\u2022 NOMAD [10].", "startOffset": 8, "endOffset": 12}, {"referenceID": 8, "context": "As presented in [10], NOMAD uses 32 nodes for Netflix and Yahoo!Music and 64 HPC nodes for Hugewiki.", "startOffset": 16, "endOffset": 20}, {"referenceID": 4, "context": "Our earlier work cuMF ALS [6] represents the state-ofart ALS-based matrix factorization solution on GPUs.", "startOffset": 26, "endOffset": 29}, {"referenceID": 0, "context": "com/cuMF/ because they serve different purposes: SGD converges fast and easy to do incremental update, while ALS is easy to parallelize and is able to deal with non-sparse rating matrices [1].", "startOffset": 188, "endOffset": 191}, {"referenceID": 3, "context": "It is well-known that SGD parallelization may have subtle implications on convergence [5, 10].", "startOffset": 86, "endOffset": 93}, {"referenceID": 8, "context": "It is well-known that SGD parallelization may have subtle implications on convergence [5, 10].", "startOffset": 86, "endOffset": 93}, {"referenceID": 9, "context": "Given a rating matrix of m\u00d7 n and s parallel workers, convergence is ensured only when the following condition satisfied [11]:", "startOffset": 121, "endOffset": 125}, {"referenceID": 0, "context": "SGD has been widely used to solve matrix factorization [1].", "startOffset": 55, "endOffset": 58}, {"referenceID": 7, "context": "Coordinate descent is another algorithm to solve matrix factorization [9, 35].", "startOffset": 70, "endOffset": 77}, {"referenceID": 29, "context": "Coordinate descent is another algorithm to solve matrix factorization [9, 35].", "startOffset": 70, "endOffset": 77}, {"referenceID": 4, "context": "Our earlier work [6] focuses on ALS algorithm.", "startOffset": 17, "endOffset": 20}, {"referenceID": 3, "context": "Parallel SGD solutions have been discussed in multicore [5, 7, 36], multi-node [10, 37], MapReduce [27, 38] and parameter-servers [39, 40] settings.", "startOffset": 56, "endOffset": 66}, {"referenceID": 5, "context": "Parallel SGD solutions have been discussed in multicore [5, 7, 36], multi-node [10, 37], MapReduce [27, 38] and parameter-servers [39, 40] settings.", "startOffset": 56, "endOffset": 66}, {"referenceID": 30, "context": "Parallel SGD solutions have been discussed in multicore [5, 7, 36], multi-node [10, 37], MapReduce [27, 38] and parameter-servers [39, 40] settings.", "startOffset": 56, "endOffset": 66}, {"referenceID": 8, "context": "Parallel SGD solutions have been discussed in multicore [5, 7, 36], multi-node [10, 37], MapReduce [27, 38] and parameter-servers [39, 40] settings.", "startOffset": 79, "endOffset": 87}, {"referenceID": 31, "context": "Parallel SGD solutions have been discussed in multicore [5, 7, 36], multi-node [10, 37], MapReduce [27, 38] and parameter-servers [39, 40] settings.", "startOffset": 79, "endOffset": 87}, {"referenceID": 22, "context": "Parallel SGD solutions have been discussed in multicore [5, 7, 36], multi-node [10, 37], MapReduce [27, 38] and parameter-servers [39, 40] settings.", "startOffset": 99, "endOffset": 107}, {"referenceID": 32, "context": "Parallel SGD solutions have been discussed in multicore [5, 7, 36], multi-node [10, 37], MapReduce [27, 38] and parameter-servers [39, 40] settings.", "startOffset": 99, "endOffset": 107}, {"referenceID": 33, "context": "Parallel SGD solutions have been discussed in multicore [5, 7, 36], multi-node [10, 37], MapReduce [27, 38] and parameter-servers [39, 40] settings.", "startOffset": 130, "endOffset": 138}, {"referenceID": 34, "context": "Parallel SGD solutions have been discussed in multicore [5, 7, 36], multi-node [10, 37], MapReduce [27, 38] and parameter-servers [39, 40] settings.", "startOffset": 130, "endOffset": 138}, {"referenceID": 9, "context": "Existing works are mostly inspired by Hogwild! [11] that allows lock-free update, or matrix-blocking that partitions to avoid conflicts, or a combination of them.", "startOffset": 47, "endOffset": 51}, {"referenceID": 3, "context": "LIBMF [5, 7] is a representative shared-memory multi-core system.", "startOffset": 6, "endOffset": 12}, {"referenceID": 5, "context": "LIBMF [5, 7] is a representative shared-memory multi-core system.", "startOffset": 6, "endOffset": 12}, {"referenceID": 8, "context": "NOMAD [10] partitions the data on HPC clusters to improve the cache performance.", "startOffset": 6, "endOffset": 10}, {"referenceID": 7, "context": "Parallelization is also used in coordinate descent [9].", "startOffset": 51, "endOffset": 54}, {"referenceID": 3, "context": "However, due to the algorithmic limitation, coordinate descent is prone to reach local optima [5] in the later epochs of training.", "startOffset": 94, "endOffset": 97}, {"referenceID": 35, "context": "Compared with CGD and SGD, ALS is inherently easy to parallel, ALS based parallel solutions are widely discussed [41, 42, 2, 15, 43].", "startOffset": 113, "endOffset": 132}, {"referenceID": 36, "context": "Compared with CGD and SGD, ALS is inherently easy to parallel, ALS based parallel solutions are widely discussed [41, 42, 2, 15, 43].", "startOffset": 113, "endOffset": 132}, {"referenceID": 13, "context": "Compared with CGD and SGD, ALS is inherently easy to parallel, ALS based parallel solutions are widely discussed [41, 42, 2, 15, 43].", "startOffset": 113, "endOffset": 132}, {"referenceID": 37, "context": "Compared with CGD and SGD, ALS is inherently easy to parallel, ALS based parallel solutions are widely discussed [41, 42, 2, 15, 43].", "startOffset": 113, "endOffset": 132}, {"referenceID": 4, "context": "Our earlier work, cuMF ALS [6] focuses on optimizing ALS to matrix factorization on GPUs.", "startOffset": 27, "endOffset": 30}, {"referenceID": 38, "context": "Prior to our work, [44] applies Restricted Boltzmann Machines on GPUs to solve MF.", "startOffset": 19, "endOffset": 23}, {"referenceID": 39, "context": "[45] implements both SGD and ALS on GPU to solve MF.", "startOffset": 0, "endOffset": 4}], "year": 2016, "abstractText": "Matrix factorization (MF) has been widely used in recommender systems, database systems, topic modeling, word embedding and others. Stochastic gradient descent (SGD) is popular in solving MF problems because it can deal with large data sets and is easy to do incremental learning. We observed that SGD for MF is memory bound. Meanwhile, single-node CPU systems with caches perform well only for small data sets; distributed systems have higher aggregated memory bandwidth but suffer from relatively slow network connection. This observation inspires us to accelerate MF by utilizing GPUs\u2019s high memory bandwidth and fast intranode connection. We present cuMF SGD, a CUDA-based SGD solution for large-scale MF problems. On a single GPU, we design two workload scheduling schemes (batch-Hogwild! and wavefront-update) that fully exploit the massive amount of cores. Especially, batch-Hogwild!, a vectorized version of Hogwild!, overcomes the issue of memory discontinuity. We develop highly-optimized kernels for SGD update, leveraging cache, warp-shuffle instructions, half-precision floats, etc. We also design a partition scheme to utilize multiple GPUs while addressing the well-known convergence issue when parallelizing SGD. Evaluations on three data sets with only one Maxwell or Pascal GPU show that cuMF SGD runs 3.1X-28.2X as fast compared with state-of-art CPU solutions on 1-64 CPU nodes. Evaluations also show that cuMF SGD scales well with multiple GPUs on large data sets. Finally, we believe that the lessons learned from building cuMF SGD are applicable to other machine learning algorithms on, e.g., (1) embedding layers in deep learning and (2) bipartite graph.", "creator": "LaTeX with hyperref package"}}}