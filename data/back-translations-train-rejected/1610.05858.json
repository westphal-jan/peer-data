{"id": "1610.05858", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Oct-2016", "title": "Bidirectional LSTM-CRF for Clinical Concept Extraction", "abstract": "Extraction of concepts present in patient clinical records is an essential step in clinical research. The 2010 i2b2/VA Workshop on Natural Language Processing Challenges for clinical records presented concept extraction (CE) task, with aim to identify concepts (such as treatments, tests, problems) and classify them into predefined categories. State-of-the-art CE approaches heavily rely on hand crafted features and domain specific resources which are hard to collect and tune. For this reason, this paper employs bidirectional LSTM with CRF decoding initialized with general purpose off-the-shelf word embeddings for CE. The experimental results achieved on 2010 i2b2/VA reference standard corpora using bidirectional LSTM CRF ranks closely with top ranked systems.", "histories": [["v1", "Wed, 19 Oct 2016 03:59:30 GMT  (14kb)", "http://arxiv.org/abs/1610.05858v1", "This paper \"Bidirectional LSTM-CRF for Clinical Concept Extraction\" is accepted for short paper presentation at Clinical Natural Language Processing Workshop at COLING 2016 Osaka, Japan. December 11, 2016"]], "COMMENTS": "This paper \"Bidirectional LSTM-CRF for Clinical Concept Extraction\" is accepted for short paper presentation at Clinical Natural Language Processing Workshop at COLING 2016 Osaka, Japan. December 11, 2016", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["raghavendra chalapathy", "ehsan zare borzeshi", "massimo piccardi"], "accepted": false, "id": "1610.05858"}, "pdf": {"name": "1610.05858.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["rcha9612@uni.sydney.edu.au", "ezborzeshi@cmcrc.com", "Massimo.Piccardi@uts.edu.au"], "sections": [{"heading": null, "text": "ar Xiv: 161 0.05 858v 1 [cs.C L] 19 Oct 201 6"}, {"heading": "1 Introduction", "text": "In fact, most of them are able to survive on their own if they do not put themselves in a position to survive on their own."}, {"heading": "3 The Proposed Approach", "text": "CE can be formulated as a common segmentation and classification task using a predefined set of classes. Consider, for example, the input sentence in Table 1. The notation follows the widely accepted in / out / begin (IOB) entity representation with, in this case, HCT as the test, 2U PRBC as the treatment. In this essay, we approach the CE task by bidirectional LSTM CRF and therefore provide a brief description below. In a bidirectional LSTM CRF, each word in the input sentence is first assigned to a random real vector of any dimension, i.e., a measurement for the word, identified as x (t), is made by linking the own vector of the word with a window of preceding and following vectors (thew3 (t) = [His, HCT, dropped], \"His's \u2192 xHCT, R's, dropped vectors (T)."}, {"heading": "3.1 Word Embeddings", "text": "Word embeddings are dense vector representations of natural language words that preserve the semantic and syntactical similarities between them. Vector representations can be generated either by counting such as Hellinger-PCA (Lebret and Collobert, 2013), direct predictive models such as Word2Vec, consisting of Skip-gram or Common Bag of Words (CBOW), or glove word embeddings. Glove vector representations capture complex patterns that go beyond word similarity by combining efficient statistics on the use of word contexts and generating a global vector representation for any word."}, {"heading": "3.2 Bidirectional LSTM-CRF Networks", "text": "The LSTM is designed to overcome this limitation by incorporating a gated memory cell to capture extensive dependencies within the data (Hochreiter and Schmidhuber, 1997). In bidirectional LSTM, the network calculates for each given sentence both a left, \u2212 \u2192 h (t), and a right, \u2190 \u2212 h (t), representation of the sentence context on each input, x (t). The final representation is obtained by concatenating it as h (t) = [\u2212 \u2192 h (t); \u2190 \u2212 h (t)]. All of these networks use the h (t) layer as an implicit feature for predicting the entity class: although this model has proven effective in many cases, it is not able to provide a common decoding of the outputs in viterbi-style (e.g., an I group cannot follow a B mark; etc.) Another 2001 modification of the bidirectional STF is called the CRF to add the CRF."}, {"heading": "4 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Datasets", "text": "The 2010 i2b2 / VA Workshop on Natural Language Processing Challenges for Clinical Records presented three tasks, including a concept extraction task focused on extracting medical concepts from patient reports. A total of 394 training reports, 477 test reports, and 877 uncommented reports were identified and published to challenge participants with data usage agreements (Uzuner et al., 2011). However, due to restrictions imposed by the Institutional Review Board (IRB), a portion of this data set is no longer distributed. Table 2 summarizes the basic statistics of the training and test records used in our experiments."}, {"heading": "4.2 Evaluation Methodology", "text": "Our models were blindly evaluated using invisible 2010 i2b2 / VA CE test data using the rigorous evaluation metrics. With this evaluation, the predicted units must accurately correspond to the ground truth units, both in the boundary and in the class. To facilitate the replication of our experimental results, we have used a publicly available library for implementation (i.e. the Theano Neural Network Toolkit (Bergstra et al., 2010), and we publish our code1. The experiments were conducted over a range of hyperparameter values, using the validation set for the selection (Bergstra and Bengio, 2012). Hyper parameters include the number of hidden layer nodes, H-25, 50, 100}, the context window size, s-1, s-3, 5}, and the embedding dimension, d-50, 300, 500, 1000}. Two additional parameters, the learning rates and word rates, the d-1, d-1, d-1, and 1-1-1-1-1-1-1-1-1."}, {"heading": "4.3 Results and Analysis", "text": "Table 3 shows the performance comparison between the used bidirectional LSTM-CRF and the state CRM systems in full significance. As a general note, the bidirectional LSTM-CRF approach has not achieved the same accuracy as the top system, the semi-supervised clinical Markov HMM (de Bruijn et al., 2011). However, our approach has also achieved the second-best rating for each new concept or invisible words that are provided on the basis that the bidirectional LSTM-CRF CE without the use of manually developed functions. As our system learns completely from the data, it is also robust for any new concept or invisible words that are added. In our current experimental setting over 20% of the tokens were either alpha-numeric or abbreviated strings whose Word2Vec or Glove preformed vector embeddings are not available."}], "references": [{"title": "A maximum entropy approach to natural language processing", "author": ["Berger et al.1996] Adam L Berger", "Vincent J Della Pietra", "Stephen A Della Pietra"], "venue": null, "citeRegEx": "Berger et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Berger et al\\.", "year": 1996}, {"title": "Random search for hyper-parameter optimization", "author": ["Bergstra", "Bengio2012] James Bergstra", "Yoshua Bengio"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Bergstra et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Bergstra et al\\.", "year": 2012}, {"title": "Theano: A CPU and GPU math compiler in Python", "author": ["Olivier Breuleux", "Fr\u00e9d\u00e9ric Bastien", "Pascal Lamblin", "Razvan Pascanu", "Guillaume Desjardins", "Joseph Turian", "David Warde-Farley", "Yoshua Bengio"], "venue": "In The 9th Python in Science Conference,", "citeRegEx": "Bergstra et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Bergstra et al\\.", "year": 2010}, {"title": "Cliner: A lightweight tool for clinical named entity recognition", "author": ["Boag et al.2015] William Boag", "Kevin Wacome", "MS Tristan Naumann"], "venue": null, "citeRegEx": "Boag et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Boag et al\\.", "year": 2015}, {"title": "Natural language processing (almost) from scratch", "author": ["Jason Weston", "L\u00e9on Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Collobert et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Machine-learned solutions for three stages of clinical information extraction: the state of the art at i2b2", "author": ["Colin Cherry", "Svetlana Kiritchenko", "Joel Martin", "Xiaodan Zhu"], "venue": "Journal of the American Medical Informatics Association,", "citeRegEx": "Bruijn et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Bruijn et al\\.", "year": 2011}, {"title": "Improving the extraction of clinical concepts from clinical records", "author": ["Fu", "Ananiadou2014] Xiao Fu", "Sophia Ananiadou"], "venue": "Proceedings of BioTxtM14", "citeRegEx": "Fu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Fu et al\\.", "year": 2014}, {"title": "Long short-term memory", "author": ["Hochreiter", "Schmidhuber1997] Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural Computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Text categorization with support vector machines: Learning with many relevant features", "author": ["Thorsten Joachims"], "venue": "In European conference on machine learning,", "citeRegEx": "Joachims.,? \\Q1998\\E", "shortCiteRegEx": "Joachims.", "year": 1998}, {"title": "Enhancing clinical concept extraction with distributional semantics", "author": ["Trevor Cohen", "Stephen Wu", "Graciela Gonzalez"], "venue": "Journal of biomedical informatics,", "citeRegEx": "Jonnalagadda et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Jonnalagadda et al\\.", "year": 2012}, {"title": "System evaluation on a named entity corpus from clinical notes", "author": ["Vinod Kaggal", "James Masanz", "Philip Ogren", "Guergana Savova"], "venue": "In Language Resources and Evaluation Conference,", "citeRegEx": "Kipper.Schuler et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Kipper.Schuler et al\\.", "year": 2008}, {"title": "Conditional random fields: Probabilistic models for segmenting and labeling sequence data", "author": ["Andrew McCallum", "Fernando Pereira"], "venue": "In ICML,", "citeRegEx": "Lafferty et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Lafferty et al\\.", "year": 2001}, {"title": "Neural architectures for named entity recognition", "author": ["Miguel Ballesteros", "Sandeep Subramanian", "Kazuya Kawakami", "Chris Dyer"], "venue": null, "citeRegEx": "Lample et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Lample et al\\.", "year": 2016}, {"title": "Word emdeddings through hellinger pca. arXiv preprint arXiv:1312.5542", "author": ["Lebret", "Collobert2013] R\u00e9mi Lebret", "Ronan Collobert"], "venue": null, "citeRegEx": "Lebret et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Lebret et al\\.", "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean"], "venue": null, "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "A survey of named entity recognition and classification", "author": ["Nadeau", "Sekine2007] David Nadeau", "Satoshi Sekine"], "venue": "Linguisticae Investigationes,", "citeRegEx": "Nadeau et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Nadeau et al\\.", "year": 2007}, {"title": "GloVe: Global vectors for word representation", "author": ["Richard Socher", "Christopher D. Manning"], "venue": "In EMNLP,", "citeRegEx": "Pennington et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "A study of neural word embeddings for named entity recognition in clinical text", "author": ["Wu et al.2015] Yonghui Wu", "Jun Xu", "Min Jiang", "Yaoyun Zhang", "Hua Xu"], "venue": "In AMIA Annual Symposium Proceedings,", "citeRegEx": "Wu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wu et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 10, "context": "Dictionary based systems perform a fast look-up from medical ontologies such as Unified Medical Language System (UMLS) to extract concepts (Kipper-Schuler et al., 2008).", "startOffset": 139, "endOffset": 168}, {"referenceID": 11, "context": "To overcome these limitations various supervised and semi-supervised machine learning (ML) approaches and its variants have been proposed utilizing conditional random fields (CRF), maximum entropy and support vector machines (SVM) models which utilize both textual and contextual information while reducing the dependency on lexicon lookup (Lafferty et al., 2001; Berger et al., 1996; Joachims, 1998).", "startOffset": 340, "endOffset": 400}, {"referenceID": 0, "context": "To overcome these limitations various supervised and semi-supervised machine learning (ML) approaches and its variants have been proposed utilizing conditional random fields (CRF), maximum entropy and support vector machines (SVM) models which utilize both textual and contextual information while reducing the dependency on lexicon lookup (Lafferty et al., 2001; Berger et al., 1996; Joachims, 1998).", "startOffset": 340, "endOffset": 400}, {"referenceID": 8, "context": "To overcome these limitations various supervised and semi-supervised machine learning (ML) approaches and its variants have been proposed utilizing conditional random fields (CRF), maximum entropy and support vector machines (SVM) models which utilize both textual and contextual information while reducing the dependency on lexicon lookup (Lafferty et al., 2001; Berger et al., 1996; Joachims, 1998).", "startOffset": 340, "endOffset": 400}, {"referenceID": 16, "context": "For this reason, this paper employs bidirectional LSTM-CRF intialized with general purpose off-the-shelf neural word embeddings derived from Glove (Pennington et al., 2014) and Word2Vec (Mikolov et al.", "startOffset": 147, "endOffset": 172}, {"referenceID": 14, "context": ", 2014) and Word2Vec (Mikolov et al., 2013) for automatic feature learning thus avoiding time-consuming feature engineering, which deliver system performance comparable to the best submissions from the 2010 i2b2/VA challenge.", "startOffset": 21, "endOffset": 43}, {"referenceID": 3, "context": "Similarly hybrid models obtained by cascading CRF and SVM algorithms along with several pattern matching rules are shown to produce effective results (Boag et al., 2015).", "startOffset": 150, "endOffset": 169}, {"referenceID": 9, "context": "Subsequently (Jonnalagadda et al., 2012) demonstrated that random indexing model with distributional word representations improve clinical concept extraction.", "startOffset": 13, "endOffset": 40}, {"referenceID": 4, "context": "With recent success of incorporating word embeddings derived from the entire English wikipedia in various NER task (Collobert et al., 2011), binarized word embeddings derived from domain specific copora (Eg: Monitoring in Intensive Care (MIMIC) II corpus) has improved performance of CRF based concept extraction system (Wu et al.", "startOffset": 115, "endOffset": 139}, {"referenceID": 17, "context": ", 2011), binarized word embeddings derived from domain specific copora (Eg: Monitoring in Intensive Care (MIMIC) II corpus) has improved performance of CRF based concept extraction system (Wu et al., 2015).", "startOffset": 188, "endOffset": 205}, {"referenceID": 11, "context": "Thus, another modification to the bidirectional LSTM is the addition of a conditional random field (CRF) (Lafferty et al., 2001) as the output layer to provide optimal sequential decoding.", "startOffset": 105, "endOffset": 128}, {"referenceID": 12, "context": "The resulting network is commonly referred to as the bidirectional LSTM-CRF (Lample et al., 2016).", "startOffset": 76, "endOffset": 97}, {"referenceID": 9, "context": "23 distributonal semantics-CRF (Jonnalagadda et al., 2012) 85.", "startOffset": 31, "endOffset": 58}, {"referenceID": 17, "context": "70 binarized neural embedding CRF(Wu et al., 2015) 85.", "startOffset": 33, "endOffset": 50}, {"referenceID": 3, "context": "80 CliNER (Boag et al., 2015) 79.", "startOffset": 10, "endOffset": 29}, {"referenceID": 2, "context": ", the Theano neural network toolkit (Bergstra et al., 2010)) and we publicly release our code1.", "startOffset": 36, "endOffset": 59}, {"referenceID": 14, "context": "A potential way to further improve its performance would be to initialize its training with unsupervised word embeddings such as Word2Vec (Mikolov et al., 2013) and GloVe (Pennington et al.", "startOffset": 138, "endOffset": 160}, {"referenceID": 16, "context": ", 2013) and GloVe (Pennington et al., 2014) trained with domain specific resources such as Monitoring in Intensive Care (MIMIC) II copora.", "startOffset": 18, "endOffset": 43}], "year": 2016, "abstractText": "Extraction of concepts present in patient clinical records is an essential step in clinical research. The 2010 i2b2/VA Workshop on Natural Language Processing Challenges for clinical records presented concept extraction (CE) task, with aim to identify concepts (such as treatments, tests, problems) and classify them into predefined categories. State-of-the-art CE approaches heavily rely on hand crafted features and domain specific resources which are hard to collect and tune. For this reason, this paper employs bidirectional LSTM with CRF decoding initialized with general purpose off-the-shelf word embeddings for CE. The experimental results achieved on 2010 i2b2/VA reference standard corpora using bidirectional LSTM CRF ranks closely with top ranked systems.", "creator": "LaTeX with hyperref package"}}}