{"id": "1610.05984", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Oct-2016", "title": "Particle Swarm Optimization for Generating Interpretable Fuzzy Reinforcement Learning Policies", "abstract": "Fuzzy controllers are known to serve as efficient and interpretable system controllers for continuous state and action spaces. To date these controllers have been constructed by hand, or automatically trained either on expert generated problem specific cost functions or by incorporating detailed knowledge about the optimal control strategy. Both requirements for automatic training processes are not given in the majority of real world reinforcement learning (RL) problems. We introduce a new particle swarm reinforcement learning (PSRL) approach which is capable of constructing fuzzy RL policies solely by training parameters on world models produced from randomly generated samples of the real system. This approach relates self-organizing fuzzy controllers to model-based RL for the first time. PSRL can be used straightforward on any RL problem, which is demonstrated on three standard RL benchmarks, mountain car, cart pole balancing and cart pole swing up. Our experiments yielded high performing and well interpretable fuzzy policies.", "histories": [["v1", "Wed, 19 Oct 2016 12:41:52 GMT  (612kb,D)", "http://arxiv.org/abs/1610.05984v1", null], ["v2", "Fri, 7 Apr 2017 07:22:21 GMT  (651kb,D)", "http://arxiv.org/abs/1610.05984v2", null], ["v3", "Fri, 5 May 2017 09:01:41 GMT  (651kb,D)", "http://arxiv.org/abs/1610.05984v3", null], ["v4", "Thu, 29 Jun 2017 07:13:09 GMT  (650kb,D)", "http://arxiv.org/abs/1610.05984v4", null], ["v5", "Tue, 15 Aug 2017 21:41:03 GMT  (687kb,D)", "http://arxiv.org/abs/1610.05984v5", null]], "reviews": [], "SUBJECTS": "cs.NE cs.AI cs.LG cs.SY", "authors": ["daniel hein", "alexander hentschel", "thomas runkler", "steffen udluft"], "accepted": false, "id": "1610.05984"}, "pdf": {"name": "1610.05984.pdf", "metadata": {"source": "CRF", "title": "Particle Swarm Optimization for Generating Fuzzy Reinforcement Learning Policies", "authors": ["Daniel Hein", "Alexander Hentschel", "Thomas Runkler", "Steffen Udluft"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "This year is the highest in the history of the country."}, {"heading": "2 Model-Based Reinforcement Learning", "text": "In biological learning, an animal interacts with its environment and tries to find action strategies to maximize its perceived accumulated reward."}, {"heading": "3 Fuzzy Rules", "text": "Fuzzy set theory was introduced in 1965 by Zadeh [23]. Based on fuzzy set theory, Mamdani and Assilian [6] have introduced a so-called fuzzy control, specified by a set of linguistic if-then rules whose member functions can be triggered independently and a combined output (calculated by a siutable debuzzification function). In a D input single output system with C rules, a fuzzy rule R (i) can be described: R (i): IF s is m (i) THEN o (i), with i =,.., C, (3), where s-RD denotes the input vector (in our environment state), m (i) is the affiliation to a fuzzy group of the input vector in the premise part, and o (i) is a real number in the subsequent part."}, {"heading": "4 Particle Swarm Optimization", "text": "The Particle Swarm Optimization Algorithm (PSO) is a population-based, non-convex, stochastic optimization problem (U). In general, PSO can operate on any search space that is a limited subspace of a finite vector space. [19] The position of each particle in the PSO swarm represents a potential solution to the problem to be solved. \u2212 The particles fly iteratively through the multidimensional search space, the so-called fitness landscape. After each movement, each particle receives a fitness value for its new position, which is used to update its own velocity vector and the velocity vectors of all particles in a given neighboring city."}, {"heading": "5 Particle Swarm Reinforcement Learning", "text": "The basis for our approach to particle swarm ampling (PSFD) is a data set D, which contains the state transition samples collected by the real system. These samples are represented by tuples (s, a, s), where s \"is the state that follows the state that performs measures a. The data can be generated by applying any (even random) strategies to achieve a better approximate quality. In the second step, we generate world models g\" with input factors (s, a) predicting s \"from data set D. However, it might be convenient to learn the deltas of each state variable and train one model per state variable to achieve a better approximate quality."}, {"heading": "6 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6.1 Mountain Car", "text": "In Mountain Law (MC), an underpowered car must be driven to the top of a hill (see Fig. 2), first driving in the opposite direction to gain enough potential energy. In our implementation, the hilly landscape is calculated as a sin (3\u03c1). The task for the RL agent is to find a sequence of forces at + 1, at + 2,... that drive the car up to the hill, which is achieved by reaching a position beyond [\u2212 1, 1]. At the beginning of each episode, the position of the car is initialized at the interval of [\u2212 \u03c0 / 3; \u03c0 / 6]. The agent perceives a reward of r (s) = {0 if he receives the highest reward in each subsequent step, regardless of the measures applied. When the car reaches the target position, the maximum reward of the car is perceived in each subsequent step."}, {"heading": "6.2 Cart Pole Balancing", "text": "The experiments with the shopping basket (CP) described in the following sections were carried out using a software system called CLS2 (\"clsquare\"). < This software is a freely available RL benchmark system that applies the fourth-order stanchion-cutta method to approximate CP dynamics. The aim of the CP balancing scales is to apply forces to a shopping basket that moves on a one-dimensional track to attach a pole to the shopping basket in an upright position (see Figure 3).The four Markov state variables are the pole angle, the pole angle, the pole angle, the speed of the shopping basket, and the speed of the shopping basket. \u2212 These four variables fully describe the Markov state; no additional information about the system's past behavior is necessary. The RL agent's task is to perform a sequence of force actions at + 1, at + 2."}, {"heading": "6.3 Cart Pole Swing Up", "text": "The CP swing up (CPSU) benchmark is based on the same system dynamics as the CPB benchmark. Unlike the CPB benchmark, neither the position of the basket nor the angle of the pole are limited to a specific region. Consequently, the pole can swing through, which is an important feature of the CPSU. Since the pole angle is initialized at the full interval of [\u2212 \u03c0; \u03c0], it is often necessary for policy to swing the pole from one side to the other several times to gain enough energy to erect the pole and get the highest reward. In the CPSU setting, the policy can apply the actions of \u2212 30 N and + 30 N to the basket. The reward function for the problem is given by r (s) = 0 if it is < 0.5 and vice versa. < 0.5 and vice versa. < 0.5 and \u043c > \u2212 0.5, \u2212 1, otherwise (13), which is similar to the CPS benchmark, but does not include a penalty for failures."}, {"heading": "6.4 Neural network world models", "text": "For our experiments, we created a neural network for each of the state variables. Neural MC networks were sampled with a data set of 100,000 samples (s, a, g (s, a)) with s = (s, a), and the following two neural networks were trained to approximate the MC task."}, {"heading": "6.5 Fuzzy Policy", "text": "With our PSFL approach, we look for the parameterization x for a fuzzy policy built from fuzzy rules. Continuous defucified output of policy o = \u03c0 [x] (s) is mapped to the available actions by calculating aMC = {\u2212 1 if o < 0 1, otherwise, (16) aCPS = {\u2212 10 if o < 0 10, otherwise, (17) aCPSU = {\u2212 30 if o < 0 30, otherwise, (18) for the MC, CPS or CPSU benchmarks."}, {"heading": "7 Results", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "7.1 Mountain Car", "text": "We conducted ten NFQ trainings for the MC benchmark with the setup described in Appendix A. After each NFQ iteration, the latest policy was tested on the world model to calculate an approximation to real performance F-MC. Up to now, the policy that provides the best fitness value has been saved as an intermediate solution. To evaluate the true performance of the NFQ approach, we calculated the true fitness value F \u2212 \u2212 of this intermediate solution using the mathematical MC dynamics g that was used to generate the sample data.For the MC benchmark and \u03b3 = 0.99 a policy that yields F > \u2212 50 can be considered a successful solution to the benchmark problem. Politicians are able to drive the car up the hill from any starting state in less than 200 time steps. Fig. 4 shows the individual results for each NFQ run and the average performance of the technology. Observe that each run successfully produces a policy that is capable of moving the car to the top of the hill."}, {"heading": "7.2 Cart Pole Balancing", "text": "The results presented in Fig. 7 show that it is more difficult to find such a policy with the help of NFQ. Nevertheless, nine out of ten NFQ runs have produced guidelines that yield F > \u2212 1. The average performance after 300 iterations and \u03b3 = 0.99 is \u2212 0.93 \u00b1 0.16. The task for PSRL was to find a parameterization for four fuzzy rules. Here, too, we used a set of 100 particles and an out-of-the-box PSO configuration. Training took place using the sample states S 3 s [\u2212 0.5; 0.5] \u00d7 [\u2212 0.5; 0.5] \u00d7 0 of size 1000. In Fig. 8, the results are evaluated using true CPB dynamics. In each of the ten PSO runs, the swarm convergence to a blurred performance of S51 is displayed in less than a blurred representation of S9 iterations."}, {"heading": "7.3 Cart Pole Swing Up", "text": "While NFQ performed only slightly worse on MC and CPB benchmarks, its performance deteriorated dramatically on the CPSU issue, with average performance stagnating at \u03b3 = 0.99 after 300 iterations at \u2212 92.9 \u00b1 1.2 (see Fig. 10). These results reflect the general problems associated with model-free RL techniques in high-dimensional, continuous states with long time horizons, for a variety of reasons, such as the lack of training samples for problem dimensionality (100,000 samples for five dimensions in our CPSU experiments) and / or the approximate quality of the neural network used, both of which would result in a much higher need for processing resources (memory and computing time)."}, {"heading": "8 Conclusions", "text": "The traditional way of creating self-organizing fuzzy controllers either requires an expert-designed fitness function, where the optimizer finds optimal controller parameters, or relies on the presence of detailed knowledge of the optimal controller policy. Both requirements are rather difficult to meet in the real industrial world. Data collected on the system, as opposed to being controlled by a standard policy, is available in many cases. The PSRL approach presented in this paper is capable of using such data and creating powerful and interpretable fuzzy guidelines for RL problems. At three standard RL benchmarks, we have shown that not only does the performance of PSRL match the performance of the state-of-the-art model-free RL approach, but PSRL has also outperformed NFQ in the CPSU benchmark, which is of higher dimensionality and has a long time horizon."}, {"heading": "Acknowledgment", "text": "The project on which this report is based was funded by the German Federal Ministry of Education and Research under project number 01IB15001. The authors are solely responsible for the content of the report. Dragan Obradovic and Clemens Otte thank them for their insightful discussions and helpful suggestions."}, {"heading": "A Neural Fitted Q Iteration", "text": "Recent years have shown that the NFQ is a model-free RL approach that can either work with stacks of previously collected transition samples or learn successively from real interactions. Transition experiences are collected in quadruples of form (s, a, s, r). The use of samples l of the problem-specific Q function Q: S \u00b7 A \u2192 R, the calculation of the state-action value of the pair sl and al, becomes iterative by the first calculation of target value lk + 1 = r l + l maxa \u2032 Q [xk \u2032 l] (s \u2032 l, a \u2032 l) and the subsequent use of supervised machine learning techniques to minimizexk + 1 ark."}, {"heading": "B Algorithm", "text": "Data: \u2022 N randomly initialized d-dimensional particle positions with xi = yi (eq. (5)) and velocities vi of the particle i, with i = \u2190., N \u2022 fitness function F (eq. (2)) \u2022 inertia factor w and acceleration constants c1 and c2 \u2022 random number generator rand () \u2022 search space limits xmin and xmax \u2022 velocity limits vmin = \u2212 0.1 \u00b7 (eq. (xmin) andvmax = 0.1 \u00b7 (xmax \u2212 xmin) \u2022 swarm topology graph to define the neighborhood NiResult: \u2022 Global best position y \u2212 repetition for each particle i \u2212 dox best position of the particle i (7); y \u00b2 i \u00b2 arg maxz \u00b2 arg = personal position (j | j \u00b2 Ni} F (z); end position vixi \u2212 vixxi \u00b2, for each particle i (eq. (9)."}], "references": [{"title": "Takagi-Sugeno fuzzy controller design via Anfis architecture for inverted pendulum system", "author": ["A.A. Saifizul", "C.A. Azlan", "N.F. Mohd Nasir"], "venue": "Proceedings of International Conference on Man-Machine Systems,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2006}, {"title": "Fundamentals of computational swarm intelligence", "author": ["A.P. Engelbrecht"], "venue": "Wiley,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2005}, {"title": "Reinforcement Learning and Dynamic Programming Using Function Approximation", "author": ["L. Busoniu", "R. Babuska", "B. De Shutter", "D. Ernst"], "venue": "CRC Press,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2010}, {"title": "Interpretability improvements to find the balance interpretability-accuracy in fuzzy modeling: an overview", "author": ["J. Casillas", "O. Cordon", "F. Herrera", "L. Magdalena"], "venue": "Interpretability issues in fuzzy modeling, pages 3\u201322. Springer,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2003}, {"title": "Computational intelligence PC tools", "author": ["R. Eberhart", "P. Simpson", "R. Dobbins"], "venue": "Academic Press Professional, Inc., San Diego, CA, USA,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1996}, {"title": "An experiment in linguistic synthesis with a fuzzy logic controller", "author": ["E.H. Mamdani", "S. Assilian"], "venue": "International Journal of Man-Machine Studies, 7(1):1\u201313,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1975}, {"title": "The application of a fuzzy controller to the control of a multi-degree-freedom robot arm", "author": ["E.M. Scharf", "N.J. Mandve"], "venue": "M. Sugeno, editor, Industrial Application of Fuzzy Control, pages 41\u201362. North-Holland,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1985}, {"title": "Tree-based batch mode reinforcement learning", "author": ["D. Ernst", "P. Geurts", "L. Wehenkel", "L. Littman"], "venue": "Journal of Machine Learning Research, 6:503\u2013556,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2005}, {"title": "Non-linear control for underactuated mechanical systems", "author": ["I. Fantoni", "R. Lozano"], "venue": "Springer,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2002}, {"title": "Neural network ensembles in reinforcement learning", "author": ["S. Fau\u00dfer", "F. Schwenker"], "venue": "Neural Process. Lett., 41(1):55\u201369,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2013}, {"title": "Reducing policy degradation in neurodynamic programming", "author": ["T. Gabel", "M. Riedmiller"], "venue": "ESANN 2006, 14th European Symposium on Artificial Neural Networks, Bruges, Belgium, April 26-28, 2006, Proceedings, pages 653\u2013658,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2006}, {"title": "Stable function approximation in dynamic programming", "author": ["G.J. Gordon"], "venue": "In Machine Learning: Proceedings of the Twelfth International Conference. Morgan Kaufmann,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1995}, {"title": "Reinforcement learning with function approximation converges to a region", "author": ["G.J. Gordon"], "venue": "Advances in Neural Information Processing Systems, pages 1040\u20131046. The MIT Press,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2001}, {"title": "Particle swarm optimization learning fuzzy systems design", "author": ["H.-M. Feng"], "venue": "Third International Conference on Information Technology and Applications, 2005. ICITA 2005, volume 1, pages 363\u2013366. Piscataway, New Jersey: Institute of Electrical and Electronics Engineers,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2005}, {"title": "Self-generation fuzzy modeling systems through hierarchical recursive-based particle swarm optimization", "author": ["H.-M. Feng"], "venue": "Cybernetics and Systems, 36(6):623\u2013639,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2005}, {"title": "Ensembles of neural networks for robust reinforcement learning", "author": ["A. Hans", "S. Udluft"], "venue": "Machine Learning and Applications (ICMLA), 2010 Ninth International Conference on, pages 401\u2013406,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2010}, {"title": "Adaptive-network-based fuzzy inference system", "author": ["J.S. Jang"], "venue": "IEEE Transactions on Systems, Man & Cybernetics, 23(3):665\u2013685,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1993}, {"title": "Particle swarm optimization", "author": ["J. Kennedy", "R.C. Eberhart"], "venue": "Proceedings of the IEEE International Joint Conference on Neural Networks, pages 1942\u20131948,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1995}, {"title": "Particle swarm optimization", "author": ["J. Kennedy", "R.C. Eberhart"], "venue": "Proceedings of the IEEE International Joint Conference on Neural Networks, pages 1942\u20131948,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1995}, {"title": "Stabilization of inverted pendulum using hybrid adaptive neuro fuzzy (anfis) controller", "author": ["A. Kharola", "P. Gupta"], "venue": "Engineering Science Letters, 4:1\u201320,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2014}, {"title": "PSO tuned adaptive neurofuzzy controller for vehicle suspension systems", "author": ["R. Kothandaraman", "L. Ponnusamy"], "venue": "Journal of Advances in Information Technology, 3(1),", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2012}, {"title": "Fuzzy basis functions, universal approximation, and orthogonal least-squares learning", "author": ["L.-X. Wang", "J.M. Mendel"], "venue": "IEEE Transactions on Neural Networks, 3(5):807\u2013814,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 1992}, {"title": "Fuzzy sets", "author": ["L.A. Zadeh"], "venue": "Information and Control, 8:338\u2013353,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 1965}, {"title": "Dynamic output feedback H\u221e control of discretetime fuzzy systems: a fuzzy-basis-dependent lyapunov function approach", "author": ["J. Lam", "S. Zhou"], "venue": "International Journal of Systems Science, 38(1):25\u201337,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2007}, {"title": "Policy search in a space of simple closed-form formulas: towards interpretability of reinforcement learning", "author": ["F. Maes", "R. Fonteneau", "L. Wehenkel", "D. Ernst"], "venue": "Discovery Science, pages 37\u201350,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2012}, {"title": "How to train neural networks", "author": ["R. Neuneier", "H.-G. Zimmermann"], "venue": "G. Montavon, G. Orr, and K.-R. M\u00fcller, editors, Neural Networks: Tricks of the Trade, Second Edition, pages 369\u2013418. Springer,", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2012}, {"title": "Gaussian processes for machine learning (adaptive computation and machine learning)", "author": ["E. Rasmussen", "C.K.I. Williams"], "venue": "MIT Press Ltd,", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2006}, {"title": "Neural fitted Q iteration \u2014 first experiences with a data efficient neural reinforcement learning method", "author": ["M. Riedmiller"], "venue": "Machine Learning: ECML 2005, volume 3720, pages 317\u2013328. Springer,", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2005}, {"title": "Neural reinforcement learning to swing-up and balance a real pole", "author": ["M. Riedmiller"], "venue": "Systems, Man and Cybernetics, 2005 IEEE International Conference on, volume 4, pages 3191\u20133196,", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2005}, {"title": "Reinforcement learning: an introduction", "author": ["R.S. Sutton", "A.G. Barto"], "venue": "A Bradford book,", "citeRegEx": "30", "shortCiteRegEx": null, "year": 1998}, {"title": "Particle swarm optimization based adaptive strategy for tuning of fuzzy logic controller", "author": ["S.B.C. Debnath", "P.C. Shill", "K. Murase"], "venue": "International Journal of Artificial Intelligence & Applications, 4(1):37\u201350,", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2013}, {"title": "Improving optimality of neural rewards regression for data-efficient batch near-optimal policy identification", "author": ["D. Schneega\u00df", "S. Udluft", "T. Martinetz"], "venue": "Proceedings of the International Conference on Artificial Neural Networks,", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2007}, {"title": "Fuzzy self-organizing controller and its application for dynamic processes", "author": ["S. Shao"], "venue": "Fuzzy Sets and Systems, 26:151\u2013164,", "citeRegEx": "33", "shortCiteRegEx": null, "year": 1988}, {"title": "Issues in using function approximation for reinforcement learning", "author": ["S. Thrun", "A. Schwartz"], "venue": "In Proceedings of the Fourth Connectionist Models Summer School. Erlbaum,", "citeRegEx": "34", "shortCiteRegEx": null, "year": 1993}, {"title": "A linguistic self-organizing process controller", "author": ["T.J. Procyk", "E.H. Mamdani"], "venue": "Automatica, 15:15\u201330,", "citeRegEx": "35", "shortCiteRegEx": null, "year": 1979}, {"title": "Design and validation of real time neuro fuzzy controller for stabilization of pendulum-cart system", "author": ["T.O.S Hanafy"], "venue": "Life Science Journal, 8(1):52\u201360,", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2011}, {"title": "Fault detection for uncertain fuzzy systems based on the delta operator approach", "author": ["H. Yang", "X. Li", "Z. Liu", "C. Hua"], "venue": "Circuits, Systems, and Signal Processing, 33(3):733\u2013759,", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2013}, {"title": "Robust fuzzy-scheduling control for nonlinear systems subject to actuator saturation via delta operator approach", "author": ["H. Yang", "X. Li", "Z. Liu", "L. Zhao"], "venue": "Information Sciences, 272:158 \u2013 172,", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2014}, {"title": "Fault-tolerant control for a class of T-S fuzzy systems via delta operator approach", "author": ["H. Yang", "P. Shi", "X. Li", "Z. Li"], "venue": "Signal Process., 98:166\u2013173,", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 29, "context": "In this paper we focus on reinforcement learning (RL) [30] problems in continuous state spaces.", "startOffset": 54, "endOffset": 58}, {"referenceID": 3, "context": "Self-organizing fuzzy controllers are known to serve as efficient and interpretable [4] system controllers in control theory since decades [35, 7, 33].", "startOffset": 84, "endOffset": 87}, {"referenceID": 34, "context": "Self-organizing fuzzy controllers are known to serve as efficient and interpretable [4] system controllers in control theory since decades [35, 7, 33].", "startOffset": 139, "endOffset": 150}, {"referenceID": 6, "context": "Self-organizing fuzzy controllers are known to serve as efficient and interpretable [4] system controllers in control theory since decades [35, 7, 33].", "startOffset": 139, "endOffset": 150}, {"referenceID": 32, "context": "Self-organizing fuzzy controllers are known to serve as efficient and interpretable [4] system controllers in control theory since decades [35, 7, 33].", "startOffset": 139, "endOffset": 150}, {"referenceID": 24, "context": "On the other side, the search for interpretable RL policies is of high academic and industrial interest [25].", "startOffset": 104, "endOffset": 108}, {"referenceID": 17, "context": "In the past years, new optimization algorithms, like particle swarm optimization (PSO) [18, 2], brought self-organizing fuzzy controllers back into the focus of researchers and might extend the scope of usage [14].", "startOffset": 87, "endOffset": 94}, {"referenceID": 1, "context": "In the past years, new optimization algorithms, like particle swarm optimization (PSO) [18, 2], brought self-organizing fuzzy controllers back into the focus of researchers and might extend the scope of usage [14].", "startOffset": 87, "endOffset": 94}, {"referenceID": 13, "context": "In the past years, new optimization algorithms, like particle swarm optimization (PSO) [18, 2], brought self-organizing fuzzy controllers back into the focus of researchers and might extend the scope of usage [14].", "startOffset": 209, "endOffset": 213}, {"referenceID": 16, "context": "In 1993 Jang introduced ANFIS, a fuzzy inference system implemented in the framework of adaptive networks [17].", "startOffset": 106, "endOffset": 110}, {"referenceID": 0, "context": "For instance, the successful application of ANFIS on the cart pole (CP) balancing has been published in [1], [36], and [20].", "startOffset": 104, "endOffset": 107}, {"referenceID": 35, "context": "For instance, the successful application of ANFIS on the cart pole (CP) balancing has been published in [1], [36], and [20].", "startOffset": 109, "endOffset": 113}, {"referenceID": 19, "context": "For instance, the successful application of ANFIS on the cart pole (CP) balancing has been published in [1], [36], and [20].", "startOffset": 119, "endOffset": 123}, {"referenceID": 13, "context": "Feng applied particle swarm optimization (PSO) to generate fuzzy systems for balancing the cart pole system and approximating a nonlinear function [14, 15].", "startOffset": 147, "endOffset": 155}, {"referenceID": 14, "context": "Feng applied particle swarm optimization (PSO) to generate fuzzy systems for balancing the cart pole system and approximating a nonlinear function [14, 15].", "startOffset": 147, "endOffset": 155}, {"referenceID": 30, "context": "optimized parameters of Gaussian membership functions on nonlinear problems and showed that the parameter tuning with PSO is much easier than with conventional methods, since there is no need for derivative knowledge nor complex mathematical equations [31].", "startOffset": 252, "endOffset": 256}, {"referenceID": 20, "context": "applied PSO to tune adaptive neuro fuzzy controllers for a vehicle suspension system [21].", "startOffset": 85, "endOffset": 89}, {"referenceID": 23, "context": "It is widely used for designing fuzzy controllers for non-linear systems [24].", "startOffset": 73, "endOffset": 77}, {"referenceID": 36, "context": "Moreover, fault detection and robustness is also of high interest for fuzzy systems [37, 38, 39].", "startOffset": 84, "endOffset": 96}, {"referenceID": 37, "context": "Moreover, fault detection and robustness is also of high interest for fuzzy systems [37, 38, 39].", "startOffset": 84, "endOffset": 96}, {"referenceID": 38, "context": "Moreover, fault detection and robustness is also of high interest for fuzzy systems [37, 38, 39].", "startOffset": 84, "endOffset": 96}, {"referenceID": 27, "context": "For benchmarking PSRL, we compare our results to the established RL technique neural fitted Q iteration (NFQ) [28, 29].", "startOffset": 110, "endOffset": 118}, {"referenceID": 28, "context": "For benchmarking PSRL, we compare our results to the established RL technique neural fitted Q iteration (NFQ) [28, 29].", "startOffset": 110, "endOffset": 118}, {"referenceID": 29, "context": "For the most common and also most challenging RL problems, an action does not only affect the next reward, but also the subsequent rewards [30].", "startOffset": 139, "endOffset": 143}, {"referenceID": 0, "context": "To incorporate increasing uncertainties when accumulating future rewards, the reward rt+k for k time steps into the future is weighted by \u03b3k, for \u03b3 \u2208 [0, 1] the discount factor.", "startOffset": 150, "endOffset": 156}, {"referenceID": 29, "context": "Furthermore, we follow the common approach to include only a finite number of T \u2265 1 future rewards into the return [30]", "startOffset": 115, "endOffset": 119}, {"referenceID": 2, "context": "Similarly, in model-based RL [3], the real world state transition function g is approximated with a model g\u0303.", "startOffset": 29, "endOffset": 32}, {"referenceID": 26, "context": "physical models or Gaussian process models [27].", "startOffset": 43, "endOffset": 47}, {"referenceID": 29, "context": "To solve RL problems, many techniques for policy learning have been developed in the past, like dynamic programming (DP) [30], neural fitted Q iteration (NFQ) [28, 29] or policy gradient neural rewards regression (PGNRR) [32].", "startOffset": 121, "endOffset": 125}, {"referenceID": 27, "context": "To solve RL problems, many techniques for policy learning have been developed in the past, like dynamic programming (DP) [30], neural fitted Q iteration (NFQ) [28, 29] or policy gradient neural rewards regression (PGNRR) [32].", "startOffset": 159, "endOffset": 167}, {"referenceID": 28, "context": "To solve RL problems, many techniques for policy learning have been developed in the past, like dynamic programming (DP) [30], neural fitted Q iteration (NFQ) [28, 29] or policy gradient neural rewards regression (PGNRR) [32].", "startOffset": 159, "endOffset": 167}, {"referenceID": 31, "context": "To solve RL problems, many techniques for policy learning have been developed in the past, like dynamic programming (DP) [30], neural fitted Q iteration (NFQ) [28, 29] or policy gradient neural rewards regression (PGNRR) [32].", "startOffset": 221, "endOffset": 225}, {"referenceID": 29, "context": "More detailed information about reinforcement learning in general and various policy learning techniques in particular can be found in the book by Sutton and Barto [30].", "startOffset": 164, "endOffset": 168}, {"referenceID": 22, "context": "Fuzzy set theory has been introduced in 1965 by Zadeh [23].", "startOffset": 54, "endOffset": 58}, {"referenceID": 5, "context": "Based on fuzzy set theory Mamdani and Assilian [6] introduced a so-called fuzzy controller specified by a set of linguistic if-then rules, whose membership functions can fire independently from each other and produce a combined output computed by a siutable defuzzification function.", "startOffset": 47, "endOffset": 50}, {"referenceID": 21, "context": "In this paper we apply Gaussian membership functions [22].", "startOffset": 53, "endOffset": 57}, {"referenceID": 18, "context": "Generally, PSO can operate on any search space that is a bounded sub-space of a finite-dimensional vector space [19].", "startOffset": 112, "endOffset": 116}, {"referenceID": 4, "context": "In the experiments presented in Section 6 the ring topology [5] has been used.", "startOffset": 60, "endOffset": 63}, {"referenceID": 8, "context": "\u2208 (\u22121,+1) that prevent the pole from falling over [9].", "startOffset": 50, "endOffset": 53}, {"referenceID": 25, "context": "The network training has been conducted by applying the Vario-Eta algorithm [26] and splitting the data sets into 90, 000 training and 10, 000 validation patterns.", "startOffset": 76, "endOffset": 80}, {"referenceID": 27, "context": "Neural fitted Q iteration (NFQ) is a neural network based RL learning approach published by Riedmiller [28, 29] and is a special realization of the \u2019fitted Q iteration\u2019 algorithm proposed by Ernst et al.", "startOffset": 103, "endOffset": 111}, {"referenceID": 28, "context": "Neural fitted Q iteration (NFQ) is a neural network based RL learning approach published by Riedmiller [28, 29] and is a special realization of the \u2019fitted Q iteration\u2019 algorithm proposed by Ernst et al.", "startOffset": 103, "endOffset": 111}, {"referenceID": 7, "context": "[8].", "startOffset": 0, "endOffset": 3}, {"referenceID": 11, "context": "NFQ belongs to the family of fitted value iteration algorithms [12].", "startOffset": 63, "endOffset": 67}, {"referenceID": 12, "context": "In practice function approximation in RL is not known to converge to a point [13] and is prone to overestimation of utility values [34].", "startOffset": 77, "endOffset": 81}, {"referenceID": 33, "context": "In practice function approximation in RL is not known to converge to a point [13] and is prone to overestimation of utility values [34].", "startOffset": 131, "endOffset": 135}, {"referenceID": 10, "context": "In [11] an RL method that monitors the learning process is presented.", "startOffset": 3, "endOffset": 7}, {"referenceID": 15, "context": "[16] and Fau\u00dfer et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "[10] applied ensembles of neural networks to form a committee of multiple agents and showed that this committee benefits from the diversity on the state-action value estimations.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "Since the performance with NFQ is expected to degrade [11] after time, the current best policy is saved.", "startOffset": 54, "endOffset": 58}], "year": 2017, "abstractText": "Fuzzy controllers are known to serve as efficient and interpretable system controllers for continuous state and action spaces. To date these controllers have been constructed by hand, or automatically trained either on expert generated problem specific cost functions or by incorporating detailed knowledge about the optimal control strategy. Both requirements for automatic training processes are not given in the majority of real world reinforcement learning (RL) problems. We introduce a new particle swarm reinforcement learning (PSRL) approach which is capable of constructing fuzzy RL policies solely by training parameters on world models produced from randomly generated samples of the real system. This approach relates self-organizing fuzzy controllers to model-based RL for the first time. PSRL can be used straightforward on any RL problem, which is demonstrated on three standard RL benchmarks, mountain car, cart pole balancing and cart pole swing up. Our experiments yielded high performing and well interpretable fuzzy policies.", "creator": "LaTeX with hyperref package"}}}