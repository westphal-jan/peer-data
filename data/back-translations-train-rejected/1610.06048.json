{"id": "1610.06048", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Oct-2016", "title": "K-Nearest Neighbor Classification Using Anatomized Data", "abstract": "This paper analyzes k nearest neighbor classification with training data anonymized using anatomy. Anatomy preserves all data values, but introduces uncertainty in the mapping between identifying and sensitive values. We first study the theoretical effect of the anatomized training data on the k nearest neighbor error rate bounds, nearest neighbor convergence rate, and Bayesian error. We then validate the derived bounds empirically. We show that 1) Learning from anatomized data approaches the limits of learning through the unprotected data (although requiring larger training data), and 2) nearest neighbor using anatomized data outperforms nearest neighbor on generalization-based anonymization.", "histories": [["v1", "Wed, 19 Oct 2016 15:00:59 GMT  (113kb)", "http://arxiv.org/abs/1610.06048v1", "Technical Report. arXiv admin note: text overlap with arXiv:1610.05815"]], "COMMENTS": "Technical Report. arXiv admin note: text overlap with arXiv:1610.05815", "reviews": [], "SUBJECTS": "cs.LG cs.CR cs.DB", "authors": ["koray mancuhan", "chris clifton"], "accepted": false, "id": "1610.06048"}, "pdf": {"name": "1610.06048.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Koray Mancuhan", "Chris Clifton"], "emails": ["kmancuha@purdue.edu", "clifton@cs.purdue.edu"], "sections": [{"heading": null, "text": "This year, it will only take one year for an agreement to be reached."}, {"heading": "II. RELATED WORK", "text": "The classification of the closest neighbors using generalized data was investigated by Martin. Nested generalizations and non-nested hyperrectangles were used to generalize the data from which the closest neighbor classifiers were formed [28]. Inan et al. suggested the classifiers of the closest neighbors and supported vector machine classifiers using anonymized training data that fulfill k-anonymity. Taylor approach was used to estimate the Euclidean distance from the anonymized training data [21]. Zhang et al. investigated Na\u0131 \ufffd ve Bayes using partially specified training data [38], proposed a conditional comparative computation algorithm that examined the instance space of attribute value generalization of taxonomies. Agrawal et al. proposed an iterative distribution reconstruction data of 4.5 for the decision algorithm of training distorted."}, {"heading": "III. DEFINITIONS AND NOTATIONS", "text": "In this section, the first four definitions of unprotected data and attribute types that we use are recalled. (Definition 1: A) D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D D: D: D: D: D: D: D: D: D: D D: D: D: D: D: D: D: D: D: D D: D: D: D: D: D: D: D: D: D: D: D: D: D D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D"}, {"heading": "V. BAYESIAN ERROR ON ANATOMIZED TRAINING DATA", "text": "Since it is impossible to know the exact Bayesian errors, many Bayesian error estimation techniques have been proposed [4], [10], [15] and [15]. In this section, the Bayesian error is estimated for the binary classification, which is easier to derive than the k nearest neighbor density estimation approach, Fukunaga will follow [15] and Fukunaga et al. [16] Both approaches show the same behavior with respect to the Bayesian estimate, which makes the discussion general enough for any non-parametric classification method [15]. We give first three axioms and a lemma.Axiom 1: Given the anatomized training data DA and the training data D; let Pi and PAi be the class for labels i = 1 {2}."}, {"heading": "VI. ANATOMIZED 1-NN CONVERGENCE", "text": "We will now discuss the error rate of the anatomized 1-NN classifier if the anatomized training data DA has a limited size Nl. We will then derive the convergence rate from the previous error rate. The discussion will not be generalized here to the anatomized k-NN classification, since we intuitively expect a faster convergence rate than the original 1-NN classification rate. For the number of instances in the training data D, using the anatomized training data DA reduces the variance of each classification error estimation. Therefore, there are fewer possible models to consider for a given sample, which ultimately means a faster convergence to the asymptotic result."}, {"heading": "VII. EXPERIMENTS AND RESULTS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. Preprocessing, Setup and Implementation", "text": "We evaluated the anatomical k-NN classifier with cross-validation on the Adult, Bank Marketing, IPUMS data sets from the UCI collection [5] and on the Fatality (fars) data set from Keel Repository [3].In the adult data set, we predicted the income attribute. Cases with missing values were removed and the features were selected using Weka's Pearson correlation filter (CfsSubsetEval) [34]. After pre-processing, we had 45222 instances with 5 attributes of education, capital return and class attribute income. The other data sets were used without feature selection. In IPUMS, we predicted whether a person is a veteran or not. After removing the N / A and missing values for veteran information, there were 148585 instances with 59 attributes."}, {"heading": "B. Anatomized 1-NN vs Anonymized 1-NN and Original 1-NN", "text": "ieD nlrerueaeaFngn nvo nde nlrerueaeaFngn ni nde nlrlrneaeaeaeFngn ni nde nlrlrheeeeeaeaeFnln ni nde nlrgneeeaeaeFnln ni nde nlrgneaeaeFngn ni rde nlrrrf\u00fc ide nlrrrf\u00fc ni nde nllllrrrgneeaeaeFnlrVnlrteeaeaeeeeeeeeeeeeeFngn nlrf\u00fc ide nlrrrf\u00fc nllllrf\u00fc ide nlrlllllrrf\u00fc"}, {"heading": "D. Convergence Behavior", "text": "We now compare the anatomized 1-NN classifier with the original 1-NN classifier on convergence behavior. We create 5 partitions from the Adult (after preprocessing), Bank Marketing, Fatality, and IPUMS datasets. Each partition is used as test data, and the remaining 4 partitions are used incrementally for training. Our goal is to show how the parameter l in anatomized training data changes the error rates as the training data size is gradually increased. Figure 3 plots the average error rates for the original training data, the anatomized training data with l = 2, the anatomized training data with l = 3, and the theoretical error rate as a function of the training data size. We can practically not know the asymptotic error rate for theoretical error rates. Therefore, we make the following estimate for the theoretical result. For each dataset, we set the RA to the minimum of the error rates in the specific datasets + 0.04 (We then measure the 001 in the datasets)."}, {"heading": "VIII. CONCLUSION", "text": "We show that the asymptotic error margins for anatomized data are the same as for the original data. Perhaps surprisingly, the proposed 1-NN classifier shows faster convergence with the asymptotic error rate than the convergence of the 1-NN classifier using non-anatomized training data. Furthermore, the analysis suggests that Bayean error analysis for each non-parametric classifier using the anatomical training data reduces the variance date of the Bayean error estimate, although it is difficult to define the characteristic of the bias term. Experiments on multiple data sets confirm the theoretical convergence rates. These experiments also show that the proposed k-NN exceeds anatomized data sets or even k-NN on original data."}, {"heading": "ACKNOWLEDGMENT", "text": "This work is supported by \"Anonymous.\" We thank \"Anonymous\" for sharing its implementation to evaluate 1-NN for generalization-based anonymization and \"Anonymous\" for helpful comments throughout the theoretical analysis."}], "references": [{"title": "On the design and quantification of privacy preserving data mining algorithms", "author": ["D. Agrawal", "C.C. Aggarwal"], "venue": "Proceedings of the Twentieth ACM SIGACT-SIGMOD-SIGART Symposium on Principles of Database Systems. Santa Barbara, California: ACM, May 21-23 2001, pp. 247\u2013255. [Online]. Available: http://doi.acm.org/10.1145/375551.375602", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2001}, {"title": "Privacy-preserving data mining", "author": ["R. Agrawal", "R. Srikant"], "venue": "Proceedings of the 2000 ACM SIGMOD Conference on Management of Data. Dallas, TX: ACM, May 14-19 2000, pp. 439\u2013450. [Online]. Available: http://doi.acm.org/10.1145/342009.335438", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2000}, {"title": "Keel data-mining software tool: Data set repository, integration of algorithms and experimental analysis framework", "author": ["J. Alcal\u00e1", "A. Fern\u00e1ndez", "J. Luengo", "J. Derrac", "S. Garc\u0131\u0301a", "L. S\u00e1nchez", "F. Herrera"], "venue": "Journal of Multiple-Valued Logic and Soft Computing, vol. 17, no. 255-287, p. 11, 2010.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2010}, {"title": "Lower bounds for bayes error estimation", "author": ["A. Antos", "L. Devroye", "L. Gy\u00f6rfi"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on, vol. 21, no. 7, pp. 643\u2013645, 1999.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1999}, {"title": "UCI machine learning repository", "author": ["A. Asuncion", "D. Newman"], "venue": "2007. [Online]. Available: http://www.ics.uci.edu/\u223cmlearn/", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2007}, {"title": "kanonymous data mining: A survey", "author": ["V. Ciriani", "S.D.C. di Vimercati", "S. Foresti", "P. Samarati"], "venue": "Privacy-preserving data mining. Springer, 2008, pp. 105\u2013136.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2008}, {"title": "Combining fragmentation and encryption to protect privacy in data storage", "author": ["V. Ciriani", "S.D.C.D. Vimercati", "S. Foresti", "S. Jajodia", "S. Paraboschi", "P. Samarati"], "venue": "ACM Trans. Inf. Syst. Secur., vol. 13, pp. 22:1\u201322:33, July 2010. [Online]. Available: http://doi.acm.org/10.1145/1805974.1805978", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2010}, {"title": "Minimizing minimality and maximizing utility: Analyzing method-based attacks on anonymized data", "author": ["G. Cormode", "N. Li", "T. Li", "D. Srivastava"], "venue": "Proceedings of the VLDB Endowment, vol. 3, no. 1, 2010, pp. 1045\u20131056. [Online]. Available: http://dl.acm.org/citation.cfm?id=1920972", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2010}, {"title": "Nearest neighbor pattern classification", "author": ["T.M. Cover", "P.E. Hart"], "venue": "Information Theory, IEEE Transactions on, vol. 13, no. 1, pp. 21\u201327, 1967.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1967}, {"title": "A probabilistic theory of pattern recognition", "author": ["L. Devroye", "L. Gy\u00f6rfi", "G. Lugosi"], "venue": "Springer Science & Business Media,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2013}, {"title": "Extending loose associations to multiple fragments", "author": ["S.D.C. di Vimercati", "S. Foresti", "S. Jajodia", "G. Livraga", "S. Paraboschi", "P. Samarati"], "venue": "DBSec\u201913, 2013, pp. 1\u201316.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2013}, {"title": "Privacy-preserving decision tree mining based on random substitutions", "author": ["J. Dowd", "S. Xu", "W. Zhang"], "venue": "Emerging Trends in Information and Communication Security. Springer, 2006, pp. 145\u2013159.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2006}, {"title": "Differential privacy", "author": ["C. Dwork"], "venue": "33rd International Colloquium on Automata, Languages and Programming (ICALP 2006), Venice, Italy, Jul. 9-16 2006, pp. 1\u201312. [Online]. Available: http://dx.doi.org/10.1007/11787006 1", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2006}, {"title": "Limiting privacy breaches in privacy preserving data mining", "author": ["A. Evfimievski", "J. Gehrke", "R. Srikant"], "venue": "Proceedings of the 22nd ACM SIGACT-SIGMOD-SIGART Symposium on Principles of Database Systems (PODS 2003), San Diego, CA, Jun. 9-12 2003, pp. 211\u2013222.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2003}, {"title": "Introduction to statistical pattern recognition", "author": ["K. Fukunaga"], "venue": "Academic press,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2013}, {"title": "Bayes error estimation using parzen and k-nn procedures", "author": ["K. Fukunaga", "D.M. Hummels"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on, no. 5, pp. 634\u2013643, 1987.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1987}, {"title": "Bias of nearest neighbor error estimates", "author": ["K. Fukunaga", "D.M. Hummels"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on, no. 1, pp. 103\u2013112, 1987.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1987}, {"title": "Top-down specialization for information and privacy preservation", "author": ["B.C.M. Fung", "K. Wang", "P.S. Yu"], "venue": "Proceedings of the 21st International Conference on Data Engineering, ser. ICDE \u201905. Washington, DC, USA: IEEE Computer Society, 2005, pp. 205\u2013216. [Online]. Available: http://dx.doi.org/10.1109/ICDE.2005.143", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2005}, {"title": "A privacy protection model for patient data with multiple sensitive attributes", "author": ["T. Gal", "Z. Chen", "A. Gangopadhyay"], "venue": "International Journal of Information Security and Privacy, IGI Global, Hershey, PA, vol. 2, no. 3, pp. 28\u201344, 2008.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2008}, {"title": "Permutation anonymization: Improving anatomy for privacy preservation in data publication.", "author": ["X. He", "Y. Xiao", "Y. Li", "Q. Wang", "W. Wang", "B. Shi"], "venue": "PAKDD Workshops, ser. Lecture Notes in Computer Science,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2011}, {"title": "Using anonymized data for classification", "author": ["A. Inan", "M. Kantarcioglu", "E. Bertino"], "venue": "Proceedings of the 2009 IEEE International Conference on Data Engineering, ser. ICDE \u201909. Washington, DC, USA: IEEE Computer Society, 2009, pp. 429\u2013440. [Online]. Available: http://dx.doi.org/10.1109/ICDE.2009.19", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2009}, {"title": "Transforming data to satisfy privacy constraints", "author": ["V.S. Iyengar"], "venue": "Proceedings of the Eighth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, ser. KDD \u201902. New York, NY, USA: ACM, 2002, pp. 279\u2013288. [Online]. Available: http://doi.acm.org/10.1145/775047.775089", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2002}, {"title": "Attacks on privacy and definetti\u2019s theorem", "author": ["D. Kifer"], "venue": "Proceedings of the 2009 ACM SIGMOD International Conference on Management of data. ACM, 2009, pp. 127\u2013138.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2009}, {"title": "t-closeness: Privacy beyond k-anonymity and l-diversity", "author": ["N. Li", "T. Li"], "venue": "Proceedings of the 23nd International Conference on Data Engineering (ICDE \u201907), Istanbul, Turkey, Apr. 16-20 2007. [Online]. Available: http://dx.doi.org/10.1109/ICDE.2007.367856", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2007}, {"title": "On the tradeoff between privacy and utility in data publishing", "author": ["T. Li", "N. Li"], "venue": "Proceedings of the 15th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, Paris, France, June 28 - July 1, 2009, 2009, pp. 517\u2013526. [Online]. Available: http://doi.acm.org/10.1145/1557019.1557079", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2009}, {"title": "Slicing: A new approach for privacy preserving data publishing", "author": ["T. Li", "N. Li", "J. Zhang", "I. Molloy"], "venue": "IEEE Trans. Knowl. Data Eng., vol. 24, no. 3, pp. 561\u2013574, 2012. [Online]. Available: http://doi.ieeecomputersociety.org/10.1109/TKDE.2010.236", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2012}, {"title": "l-diversity: Privacy beyond k-anonymity", "author": ["A. Machanavajjhala", "J. Gehrke", "D. Kifer", "M. Venkitasubramaniam"], "venue": "Proceedings of the 22nd IEEE International Conference on Data Engineering (ICDE 2006), Atlanta Georgia, Apr. 2006. [Online]. Available: http://dx.doi.org/10.1109/ICDE.2006.1", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2006}, {"title": "Instance-based learning : Nearest neighbor with generalization", "author": ["B. Martin"], "venue": "Tech. Rep., 1995.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 1995}, {"title": "Controlled data-swapping techniques for masking public use microdata sets", "author": ["R.A. Moore", "Jr."], "venue": "U.S. Bureau of the Census, Washington, DC., Statistical Research Division Report Series RR 96-04, 1996. [Online]. Available: http://www.census.gov/srd/papers/pdf/rr96-4.pdf", "citeRegEx": "29", "shortCiteRegEx": null, "year": 1996}, {"title": "\u03b4-presence without complete world knowledge", "author": ["M.E. Nergiz", "C. Clifton"], "venue": "IEEE Trans. Knowl. Data Eng., vol. 22, no. 6, pp. 868\u2013883, Jun. 2010. [Online]. Available: http://doi.ieeecomputersociety.org/10.1109/TKDE.2009.125", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2010}, {"title": "Protecting respondent\u2019s privacy in microdata release", "author": ["P. Samarati"], "venue": "IEEE Trans. Knowl. Data Eng., vol. 13, no. 6, pp. 1010\u20131027, Nov./Dec. 2001. [Online]. Available: http://dx.doi.org/10.1109/69.971193", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2001}, {"title": "k-anonymity: a model for protecting privacy", "author": ["L. Sweeney"], "venue": "International Journal on Uncertainty, Fuzziness and Knowledgebased Systems, no. 5, pp. 557\u2013570, 2002. [Online]. Available: http://dx.doi.org/10.1142/S0218488502001648", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2002}, {"title": "Introduction to Data Mining, (First Edition)", "author": ["P.-N. Tan", "M. Steinbach", "V. Kumar"], "venue": null, "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2005}, {"title": "Data Mining: Practical Machine Learning Tools and Techniques with Java Implementations", "author": ["I.H. Witten", "E. Frank"], "venue": null, "citeRegEx": "34", "shortCiteRegEx": "34", "year": 1999}, {"title": "Minimality attack in privacy preserving data publishing", "author": ["R.C.-W. Wong", "A.W.-C. Fu", "K. Wang", "J. Pei"], "venue": "VLDB, 2007, pp. 543\u2013554.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2007}, {"title": "\u03b1, k)-anonymity: An enhanced k-anonymity model for privacy preserving data publishing", "author": ["R.C.-W. Wong", "J. Li", "A.W.-C. Fu", "K. Wang"], "venue": "Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, ser. KDD \u201906. New York, NY, USA: ACM, 2006, pp. 754\u2013759. [Online]. Available: http://doi.acm.org/10.1145/1150402.1150499", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2006}, {"title": "Anatomy: Simple and effective privacy preservation", "author": ["X. Xiao", "Y. Tao"], "venue": "Proceedings of 32nd International Conference on Very Large Data Bases (VLDB 2006), Seoul, Korea, Sep. 12-15 2006. [Online]. Available: http://www.vldb.org/conf/2006/p139-xiao.pdf", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2006}, {"title": "Learning accurate and concise na\u0131\u0308ve bayes classifiers from attribute value taxonomies and data", "author": ["J. Zhang", "D.-K. Kang", "A. Silvescu", "V. Honavar"], "venue": "Knowledge and Information Systems, vol. 9, no. 2, pp. 157\u2013179, 2006.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2006}], "referenceMentions": [{"referenceID": 26, "context": "Many privacy definitions have been proposed based on generalizing/suppressing data (l-diversity [27], k-anonymity [31], [32], t-closeness [24], \u03b4-presence [30], (\u03b1,k)-anonymity [36]).", "startOffset": 96, "endOffset": 100}, {"referenceID": 30, "context": "Many privacy definitions have been proposed based on generalizing/suppressing data (l-diversity [27], k-anonymity [31], [32], t-closeness [24], \u03b4-presence [30], (\u03b1,k)-anonymity [36]).", "startOffset": 114, "endOffset": 118}, {"referenceID": 31, "context": "Many privacy definitions have been proposed based on generalizing/suppressing data (l-diversity [27], k-anonymity [31], [32], t-closeness [24], \u03b4-presence [30], (\u03b1,k)-anonymity [36]).", "startOffset": 120, "endOffset": 124}, {"referenceID": 23, "context": "Many privacy definitions have been proposed based on generalizing/suppressing data (l-diversity [27], k-anonymity [31], [32], t-closeness [24], \u03b4-presence [30], (\u03b1,k)-anonymity [36]).", "startOffset": 138, "endOffset": 142}, {"referenceID": 29, "context": "Many privacy definitions have been proposed based on generalizing/suppressing data (l-diversity [27], k-anonymity [31], [32], t-closeness [24], \u03b4-presence [30], (\u03b1,k)-anonymity [36]).", "startOffset": 155, "endOffset": 159}, {"referenceID": 35, "context": "Many privacy definitions have been proposed based on generalizing/suppressing data (l-diversity [27], k-anonymity [31], [32], t-closeness [24], \u03b4-presence [30], (\u03b1,k)-anonymity [36]).", "startOffset": 177, "endOffset": 181}, {"referenceID": 28, "context": "Other alternatives include value swapping [29], distortion [2], randomization [14], and noise addition (e.", "startOffset": 42, "endOffset": 46}, {"referenceID": 1, "context": "Other alternatives include value swapping [29], distortion [2], randomization [14], and noise addition (e.", "startOffset": 59, "endOffset": 62}, {"referenceID": 13, "context": "Other alternatives include value swapping [29], distortion [2], randomization [14], and noise addition (e.", "startOffset": 78, "endOffset": 82}, {"referenceID": 12, "context": ", differential privacy [13]).", "startOffset": 23, "endOffset": 27}, {"referenceID": 5, "context": "Generalization consists of replacing identifying attribute values with a less specific version [6].", "startOffset": 95, "endOffset": 98}, {"referenceID": 5, "context": "Suppression can be viewed as the ultimate generalization, replacing the identifying value with an \u201cany\u201d value [6].", "startOffset": 110, "endOffset": 113}, {"referenceID": 36, "context": "Xiao and Tao proposed anatomization as a method to enforce l-diversity while preserving specific data values [37].", "startOffset": 109, "endOffset": 113}, {"referenceID": 6, "context": "The more general approach of fragmentation [7] divides a given dataset\u2019s attributes into two sets of attributes (2 partitions) such that an encryption mechanism avoids associations between two different small partitions.", "startOffset": 43, "endOffset": 46}, {"referenceID": 10, "context": "extend fragmentation to multiple partitions [11], and Tamas et al.", "startOffset": 44, "endOffset": 48}, {"referenceID": 18, "context": "propose an extension that deals with multiple sensitive attributes [19].", "startOffset": 67, "endOffset": 71}, {"referenceID": 19, "context": "There is concern that anatomization is vulnerable to several attacks [20], [23], [26].", "startOffset": 69, "endOffset": 73}, {"referenceID": 22, "context": "There is concern that anatomization is vulnerable to several attacks [20], [23], [26].", "startOffset": 75, "endOffset": 79}, {"referenceID": 25, "context": "There is concern that anatomization is vulnerable to several attacks [20], [23], [26].", "startOffset": 81, "endOffset": 85}, {"referenceID": 12, "context": "While this can be an issue, any method that provides meaningful utility fails to provide perfect privacy against a sufficiently strong adversary [13], [25].", "startOffset": 145, "endOffset": 149}, {"referenceID": 24, "context": "While this can be an issue, any method that provides meaningful utility fails to provide perfect privacy against a sufficiently strong adversary [13], [25].", "startOffset": 151, "endOffset": 155}, {"referenceID": 7, "context": "g, minimality [8], [35].", "startOffset": 14, "endOffset": 17}, {"referenceID": 34, "context": "g, minimality [8], [35].", "startOffset": 19, "endOffset": 23}, {"referenceID": 22, "context": "attribute, a task that was found to be #P-complete by Kifer [23].", "startOffset": 60, "endOffset": 64}, {"referenceID": 20, "context": "already gives a practical applications of such a learning scenario [21].", "startOffset": 67, "endOffset": 71}, {"referenceID": 20, "context": "In case of nearest neighbor classifier (1-NN), we also make an additional comparison to generalization based learning scheme [21].", "startOffset": 125, "endOffset": 129}, {"referenceID": 27, "context": "Nested generalization and non-nested hyperrectangles were used to generalize the data from which the nearest neighbor classifiers were trained [28].", "startOffset": 143, "endOffset": 147}, {"referenceID": 20, "context": "Taylor approximation was used to estimate the Euclidean distance from the anonymized training data [21].", "startOffset": 99, "endOffset": 103}, {"referenceID": 37, "context": "studied Na\u0131\u0308ve Bayes using partially specified training data [38], proposing a conditional likehoods computation algorithm exploring the instance space of attribute-value generalization taxonomies.", "startOffset": 61, "endOffset": 65}, {"referenceID": 0, "context": "5 decision tree classifier was trained [1].", "startOffset": 39, "endOffset": 42}, {"referenceID": 21, "context": "5 decision tree classifier was trained from the optimally generalized training data [22].", "startOffset": 84, "endOffset": 88}, {"referenceID": 17, "context": "A new scoring function was proposed for the calculation of decision tree splits from the compressed training data [18].", "startOffset": 114, "endOffset": 118}, {"referenceID": 11, "context": "5 decision tree classifier was learned [12].", "startOffset": 39, "endOffset": 43}, {"referenceID": 31, "context": "Given the former definitions, we will next define the anonymized training data following the definition of kanonymity [32].", "startOffset": 118, "endOffset": 122}, {"referenceID": 31, "context": "Definition 5: A training data D that satisfies the following conditions is said to be anonymized training data Dk [32]:", "startOffset": 114, "endOffset": 118}, {"referenceID": 26, "context": "This requires the definition of groups [27].", "startOffset": 39, "endOffset": 43}, {"referenceID": 36, "context": "that is originally based on l-diverse groups [37].", "startOffset": 45, "endOffset": 49}, {"referenceID": 32, "context": "Although anatomization requires a discrete probability distribution for the sensitive attribute As, such smoothness violation is negligible since the original k-NN classifier is known to fit well on discrete training data [33].", "startOffset": 222, "endOffset": 226}, {"referenceID": 14, "context": "The anatomized k-NN cases where k > 1 and k is even will be ignored, because such cases include the tie between k-nearest neighbors that makes the bounds ambiguous and complicated [15].", "startOffset": 180, "endOffset": 184}, {"referenceID": 8, "context": "The total number of attributes are assumed to be d + 1 (d identifying attributes and 1 sensitive attribute) and all instances are assumed to be in a separable metric space M \u2282 R as in [9], [10], [15].", "startOffset": 184, "endOffset": 187}, {"referenceID": 9, "context": "The total number of attributes are assumed to be d + 1 (d identifying attributes and 1 sensitive attribute) and all instances are assumed to be in a separable metric space M \u2282 R as in [9], [10], [15].", "startOffset": 189, "endOffset": 193}, {"referenceID": 14, "context": "The total number of attributes are assumed to be d + 1 (d identifying attributes and 1 sensitive attribute) and all instances are assumed to be in a separable metric space M \u2282 R as in [9], [10], [15].", "startOffset": 195, "endOffset": 199}, {"referenceID": 14, "context": "When Xj \u223c= X hold for all Xj \u2208 X \u2032 N (k), we denote the error rate by R (X) in Equation 1 [15].", "startOffset": 90, "endOffset": 94}, {"referenceID": 14, "context": "2 computes R(X) [15].", "startOffset": 16, "endOffset": 20}, {"referenceID": 8, "context": "[9].", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "using 1, 2 [9].", "startOffset": 11, "endOffset": 14}, {"referenceID": 14, "context": "We exclude this derivation due to space limitations, but the derivation follows from the original k-NN classifier analysis in [15].", "startOffset": 126, "endOffset": 130}, {"referenceID": 3, "context": "Since it is impossible to know the exact Bayesian error, many Bayesian error estimation techniques were suggested [4], [10], [15].", "startOffset": 114, "endOffset": 117}, {"referenceID": 9, "context": "Since it is impossible to know the exact Bayesian error, many Bayesian error estimation techniques were suggested [4], [10], [15].", "startOffset": 119, "endOffset": 123}, {"referenceID": 14, "context": "Since it is impossible to know the exact Bayesian error, many Bayesian error estimation techniques were suggested [4], [10], [15].", "startOffset": 125, "endOffset": 129}, {"referenceID": 3, "context": "Although such estimation would be very interesting for multilabel classification, the theoretical analysis on unprotected data only covers binary classification [4].", "startOffset": 161, "endOffset": 164}, {"referenceID": 14, "context": "The Parzen density estimation approach, which is easier to derive than the k nearest neighbor density estimation approach, will follow Fukunaga [15] and Fukunaga et al.", "startOffset": 144, "endOffset": 148}, {"referenceID": 15, "context": "[16].", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "Both approaches show the same behavior in terms of the Bayesian estimation that makes the discussion general enough for any non-parametric density based binary classification method [15].", "startOffset": 182, "endOffset": 186}, {"referenceID": 14, "context": "Let P\u0302Ai(X) and P\u0302i(X) be the Parzen density estimations; and K(\u2217) be the kernel function for D with shape matrix A and size/volume parameter r [15].", "startOffset": 144, "endOffset": 148}, {"referenceID": 14, "context": "In 7, the terms other than R stand for the expected estimation error E[\u2206R A] in 8 [15].", "startOffset": 82, "endOffset": 86}, {"referenceID": 14, "context": "From Fukunaga [15], we know that E{\u2206hA(X)} and E{\u2206hA(X)} are expressed in function of the E{P\u0302Ai(X)} and V ar{P\u0302Ai(X)}.", "startOffset": 14, "endOffset": 18}, {"referenceID": 14, "context": "The former equality is the result of using Parzen density estimate [15].", "startOffset": 67, "endOffset": 71}, {"referenceID": 9, "context": "The discussion here won\u2019t be generalized to the anatomized k-NN classifier since the finite size training data performance of k-NN classifiers are not generalized to k > 2 in the pattern recognition literature [10], [15].", "startOffset": 210, "endOffset": 214}, {"referenceID": 14, "context": "The discussion here won\u2019t be generalized to the anatomized k-NN classifier since the finite size training data performance of k-NN classifiers are not generalized to k > 2 in the pattern recognition literature [10], [15].", "startOffset": 216, "endOffset": 220}, {"referenceID": 14, "context": "[15], [17].", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "[15], [17].", "startOffset": 6, "endOffset": 10}, {"referenceID": 14, "context": "The rest of the proof follows Fukunaga [15].", "startOffset": 39, "endOffset": 43}, {"referenceID": 14, "context": "Using 18, expectation with respect to X in 17 (step 3) according to Fukunaga [15] results in 13.", "startOffset": 77, "endOffset": 81}, {"referenceID": 4, "context": "We evaluate the anatomized k-NN classifier using cross validation on the Adult, Bank Marketing, IPUMS datasets from UCI collection [5] and on the Fatality (fars) dataset from Keel repository [3].", "startOffset": 131, "endOffset": 134}, {"referenceID": 2, "context": "We evaluate the anatomized k-NN classifier using cross validation on the Adult, Bank Marketing, IPUMS datasets from UCI collection [5] and on the Fatality (fars) dataset from Keel repository [3].", "startOffset": 191, "endOffset": 194}, {"referenceID": 33, "context": "The instances with missing values were removed and features selected using the Pearson correlation filter (CfsSubsetEval) of Weka [34].", "startOffset": 130, "endOffset": 134}, {"referenceID": 20, "context": "[21]) was used to implement the k-NN classifier [34].", "startOffset": 0, "endOffset": 4}, {"referenceID": 33, "context": "[21]) was used to implement the k-NN classifier [34].", "startOffset": 48, "endOffset": 52}, {"referenceID": 36, "context": "[37].", "startOffset": 0, "endOffset": 4}, {"referenceID": 32, "context": "are provided [33]", "startOffset": 13, "endOffset": 17}, {"referenceID": 20, "context": "\u2019s work considers only the anonymized 1-NN classifier [21].", "startOffset": 54, "endOffset": 58}], "year": 2016, "abstractText": "This paper analyzes k nearest neighbor classification with training data anonymized using anatomy. Anatomy preserves all data values, but introduces uncertainty in the mapping between identifying and sensitive values. We first study the theoretical effect of the anatomized training data on the k nearest neighbor error rate bounds, nearest neighbor convergence rate, and Bayesian error. We then validate the derived bounds empirically. We show that 1) Learning from anatomized data approaches the limits of learning through the unprotected data (although requiring larger training data), and 2) nearest neighbor using anatomized data outperforms nearest neighbor on generalization-based anonymization.", "creator": "LaTeX with hyperref package"}}}