{"id": "1610.06106", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Oct-2016", "title": "Efficiency of active learning for the allocation of workers on crowdsourced classification tasks", "abstract": "Crowdsourcing has been successfully employed in the past as an effective and cheap way to execute classification tasks and has therefore attracted the attention of the research community. However, we still lack a theoretical understanding of how to collect the labels from the crowd in an optimal way. In this paper we focus on the problem of worker allocation and compare two active learning policies proposed in the empirical literature with a uniform allocation of the available budget. To this end we make a thorough mathematical analysis of the problem and derive a new bound on the performance of the system. Furthermore we run extensive simulations in a more realistic scenario and show that our theoretical results hold in practice.", "histories": [["v1", "Wed, 19 Oct 2016 17:03:27 GMT  (181kb,D)", "http://arxiv.org/abs/1610.06106v1", "paper accepted in the CrowdML workshop at NIPS 2016"]], "COMMENTS": "paper accepted in the CrowdML workshop at NIPS 2016", "reviews": [], "SUBJECTS": "cs.HC cs.LG", "authors": ["edoardo manino", "long tran-thanh", "nicholas r jennings"], "accepted": false, "id": "1610.06106"}, "pdf": {"name": "1610.06106.pdf", "metadata": {"source": "CRF", "title": "Efficiency of active learning for the allocation of workers on crowdsourced classification tasks", "authors": ["Edoardo Manino", "Long Tran-Thanh"], "emails": ["em4e15@soton.ac.uk", "ltt08r@ecs.soton.ac.uk", "n.jennings@imperial.ac.uk"], "sections": [{"heading": "1 Introduction", "text": "Crowdsourcing has proven to be an effective way to recruit a temporary workforce from an online community and perform a large number of simple and repetitive data processing tasks. Despite the unreliability of individual members, as a whole, these masses have proved capable of producing valuable solutions that would not have been attainable with traditional methods. In particular, our focus is on classification projects where the employer presents workers with a series of multiple-choice questions and uses their answers to derive the right label of tasks. This model has been successfully used for various purposes including captioning [Sorokin and Forsyth, 2008], voting in complex workflows and filtering [Parameswaran et al., 2012].As the market and demand for these types of solutions expand, there is increasing interest in improving the efficiency of the crowdsourcing process. In the case where workers are rewarded with small payments, as in the famous Amazon Mechanical Platform 1, crowdsourcing could make this goal even more important, as Amazon Crowdsourcing Turkda Crowdsourcing is."}, {"heading": "2 Background", "text": "We are analyzing a setup here in which we must recognize the true parameters of the workers in order to identify them. We also assume that we have a maximum budget of tuning techniques that we can collect from the mass, and individual tuning of the task they are working on is no more possible than by other factors such as fatigue effects or tasks of varying degrees of difficulty. In this model, experts will have a PJ close to one, whereas spammers are associated with a Pj \"1 {2. On the other side of the spectrum, we name opponents of the workers with pj\" 1.2 as their answers are likely to be wrong."}, {"heading": "3 A theoretical analysis of allocation policies", "text": "In this section, we analyze the impact that different labor distribution strategies have on the performance of the system. Specifically, we are interested in calculating the probability that task i will be correctly allocated according to the following guidelines: \u2022 the uniform allocation policy, which assigns the same number of workers to each task [Karger et al., 2011]; \u2022 a greedy policy, which maximizes the expected information gain of each new incoming label [Simpson and Roberts, 2014]; \u2022 a simple rule for selecting uncertainties, which identifies new workers for the task with the least trusted label [Lewis and Gale, 1994, Welinder and Perona, 2010]. To keep our discussion simple, we model our interaction with the set as a result of B different rounds. In each round, a new worker j is available, and we must assign it to a task i of our choice. Once the label \"ij\" is acquired, the system moves to the next round j '1. Furthermore, we assume that this octagon will be released at each parameter 4."}, {"heading": "3.1 Equivalence between different active learning policies", "text": "To do this, we interpret the effect of the next incoming label as a random variable xj. \"logppj {p1.\" pjqq, which modifies the current posterior probability Pp. \"i.\" 1q. \"exppziq\" 1q, and measure the information gain as the Kullback-Leibler divergence between the two distributions: Ipi,. \"xj exppzi.\" q \"1.\" exppziq \"1.\" 4 Since the character of xj a is priori unknown, we must select task i, which represents the maximized equation 4 in expectation, i.e. i \u02da \"argmaxipExj pIpi.\" plik. as an optimal solution for the expected 5-fold equation, i.e. we can select the probable equality 1-fold equation, i.e. i \u02da \"argmaxipExj pIpi,.q."}, {"heading": "3.2 A random walk interpretation of the allocation policies", "text": "From the point of view of a single task, the active learning policy considers the allocation of new workers in short phases q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q qqqqqq qq qqq qq qqqqqq qqqqqqqqq qqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqq"}, {"heading": "3.2.1 The homogeneous crowd case", "text": "An interesting scenario arises when all workers have the same parameter pj \"p, i.e. the distribution of mass is a Dirac delta fP\" \u03b4ppq. In this case, the labels collected from the mass have the same weight, and therefore our aggregation technique simplifies into a majority voting rule. In addition, we can calculate a closed form of the two equations 11 and 12, the only assumption being that ru is an odd integer: Epraq \"'2 exppzBq1' exppzBq '1 \u0445zBp2p' 1q log'p 1 'p \u0445 (13) Pp\u0442 \u0109ui\" \u02da i q \"ru \u00ffr\" rru {2s prp1' pqru'r (14) Interestingly, the active learning policy performs better than a uniform assignment for each value of ru 1 and p P p0, 1 {2q Y p1 {2, 1q, as shown in Figure 2."}, {"heading": "3.2.2 Probability bounds on the heterogeneous case", "text": "In general, we can only have access to the first moments of the distribution fV. If this is the case, we can still have some closed form limits in Eqs 11 and 12. First, we must assume that the distribution fV in r'\u03b3, g's with g '8 or, in other words, that there are no perfect workers with pj \"1 or pj\" 0 in our population. We can then apply the results in Ethier and Khoshnevisan [2002] and derive an upper limit for the expected number of labels needed by active learning policies to achieve e.g.: Epraq \"1EpfV q'p2zb\" \u03b3q e.g. \"0\" 1 \u03c12zb \"1\" zb \"(15), where it is a better solution for the equation, Epical fV q\" 1 \"and can be approximated as follows:\" 2EpfV q VarpfV q (16), which lasts as long as pfV '1. \""}, {"heading": "4 An empirical comparison between policies", "text": "In this section we will study a realistic scenario in which the parameters of the workers are unknown and the workers are allowed to perform several tasks. These two differences in attitude have the following consequences: \u2022 To increase the number of workers, we must rely on the accuracy of their skills, but they cannot neglect the allocation of the workers to the workers."}, {"heading": "5 Conclusions and future works", "text": "In this paper, we addressed the problem of workforce allocation in crowdsourcing classification tasks from a theoretical perspective. First, we examined the characteristics of three allocation strategies proposed in the existing literature. In our analysis, we were able to demonstrate the equivalence between two different active learning strategies and show a new limit to their performance. Second, we compared these adaptive active learning strategies with a uniform allocation and showed that the latter is less efficient in terms of classification accuracy. Finally, we validated our theoretical results through comprehensive empirical analysis. Future work will include improving our limits when the available budget is low, and extending our theoretical analysis to the more realistic case where the skills of the workforce are a priori unknown."}, {"heading": "Acknowledgments", "text": "This research is funded by the ORCHID project of the UK Research Council, grant EP / I011587 / 1"}], "references": [{"title": "Gambler\u2019s Ruin Revisited: The Effects of Skew and Large Jackpots", "author": ["R. Michael Canjar"], "venue": "In Optimal Play: Mathematical Studies of Games and Gambling,", "citeRegEx": "Canjar.,? \\Q2007\\E", "shortCiteRegEx": "Canjar.", "year": 2007}, {"title": "Maximum Likelihood Estimation of Observer Error-Rates Using the EM Algorithm", "author": ["A.P. Dawid", "A.M. Skene"], "venue": "Journal of the Royal Statistical Society Series C Applied Statistics,", "citeRegEx": "Dawid and Skene.,? \\Q1979\\E", "shortCiteRegEx": "Dawid and Skene.", "year": 1979}, {"title": "Bounds on Gambler\u2019s Ruin Probabilities in Terms of Moments", "author": ["S.N. Ethier", "Davar Khoshnevisan"], "venue": "Methodology And Computing In Applied Probability,", "citeRegEx": "Ethier and Khoshnevisan.,? \\Q2002\\E", "shortCiteRegEx": "Ethier and Khoshnevisan.", "year": 2002}, {"title": "Probability Inequalities for Sums of Bounded Random Variables", "author": ["Wassily Hoeffding"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "Hoeffding.,? \\Q1963\\E", "shortCiteRegEx": "Hoeffding.", "year": 1963}, {"title": "Quality management on Amazon Mechanical Turk", "author": ["Panagiotis G. Ipeirotis", "Foster Provost", "Jing Wang"], "venue": "In Proceedings of the ACM SIGKDD Workshop on Human Computation,", "citeRegEx": "Ipeirotis et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Ipeirotis et al\\.", "year": 2010}, {"title": "Budget-Optimal Crowdsourcing using Lowrank Matrix Approximations", "author": ["David R. Karger", "Sewoong Oh", "Devavrat Shah"], "venue": "In Proceedings of the 49th Annual Conference on Communication, Control, and Computing (Allerton),", "citeRegEx": "Karger et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Karger et al\\.", "year": 2011}, {"title": "Kozec. A Rule of Thumb (not Only) for Gamblers", "author": ["S. Andrzej"], "venue": "Stochastic Processes and their Applications,", "citeRegEx": "Andrzej,? \\Q1995\\E", "shortCiteRegEx": "Andrzej", "year": 1995}, {"title": "A Sequential Algorithm for Training Text Classifiers", "author": ["David D Lewis", "William A Gale"], "venue": "In Proceedings of the 17th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval,", "citeRegEx": "Lewis and Gale.,? \\Q1994\\E", "shortCiteRegEx": "Lewis and Gale.", "year": 1994}, {"title": "Variational Inference for Crowdsourcing", "author": ["Qiang Liu", "Jian Peng", "Alexander T Ihler"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Liu et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2012}, {"title": "Optimal Decision Rules in Uncertain Dichotomous Choice Situations", "author": ["Shmuel Nitzan", "Jacob Paroush"], "venue": "International Economic Review,", "citeRegEx": "Nitzan and Paroush.,? \\Q1982\\E", "shortCiteRegEx": "Nitzan and Paroush.", "year": 1982}, {"title": "CrowdScreen: Algorithms for Filtering Data with Humans", "author": ["Aditya G. Parameswaran", "Hector Garcia-Molina", "Hyunjung Park", "Neoklis Polyzotis", "Aditya Ramesh", "Jennifer Widom"], "venue": "In Proceedings of the 2012 ACM SIGMOD International Conference on Management of Data,", "citeRegEx": "Parameswaran et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Parameswaran et al\\.", "year": 2012}, {"title": "Active Learning Literature Survey", "author": ["Burr Settles"], "venue": "Machine Learning,", "citeRegEx": "Settles.,? \\Q2010\\E", "shortCiteRegEx": "Settles.", "year": 2010}, {"title": "Bayesian Methods for Intelligent Task Assignment in Crowdsourcing Systems. In Scalable Decision Making: Uncertainty, Imperfection, Deliberation", "author": ["Edwin Simpson", "Stephen Roberts"], "venue": null, "citeRegEx": "Simpson and Roberts.,? \\Q2014\\E", "shortCiteRegEx": "Simpson and Roberts.", "year": 2014}, {"title": "Online Decision Making in Crowdsourcing Markets: Theoretical Challenges", "author": ["Aleksandrs Slivkins", "Jennifer Wortman Vaughan"], "venue": "ACM SIGecom Exchanges,,", "citeRegEx": "Slivkins and Vaughan.,? \\Q2013\\E", "shortCiteRegEx": "Slivkins and Vaughan.", "year": 2013}, {"title": "Utility Data Annotation with Amazon", "author": ["Alexander Sorokin", "David Forsyth"], "venue": "Mechanical Turk. Urbana,", "citeRegEx": "Sorokin and Forsyth.,? \\Q2008\\E", "shortCiteRegEx": "Sorokin and Forsyth.", "year": 2008}, {"title": "Online Crowdsourcing: Rating Annotators and Obtaining CostEffective Labels", "author": ["Peter Welinder", "Pietro Perona"], "venue": "In Proceedings of the 2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "Welinder and Perona.,? \\Q2010\\E", "shortCiteRegEx": "Welinder and Perona.", "year": 2010}, {"title": "Learning from the Wisdom of Crowds by Minimax Entropy", "author": ["Dengyong Zhou", "John Platt", "Sumit Basu", "Yi Mao"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Zhou et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Zhou et al\\.", "year": 2012}], "referenceMentions": [{"referenceID": 14, "context": "This model has been successfully employed for various purposes including image labeling [Sorokin and Forsyth, 2008], ballot voting in complex workflows and filtering [Parameswaran et al.", "startOffset": 88, "endOffset": 115}, {"referenceID": 10, "context": "This model has been successfully employed for various purposes including image labeling [Sorokin and Forsyth, 2008], ballot voting in complex workflows and filtering [Parameswaran et al., 2012].", "startOffset": 166, "endOffset": 193}, {"referenceID": 4, "context": "One of the main obstacles however is the heterogeneity in the quality of the answers collected from the crowd [Ipeirotis et al., 2010].", "startOffset": 110, "endOffset": 134}, {"referenceID": 9, "context": "When the parameters pj of the workers are known, we can optimally aggregate the workers\u2019 labels according to the following rule [Nitzan and Paroush, 1982]:", "startOffset": 128, "endOffset": 154}, {"referenceID": 1, "context": "The first application of this approach dates back to the expectation-maximisation algorithm of Dawid and Skene [1979] but in more recent years many alternative techniques have been proposed [Karger et al.", "startOffset": 95, "endOffset": 118}, {"referenceID": 1, "context": "The first application of this approach dates back to the expectation-maximisation algorithm of Dawid and Skene [1979] but in more recent years many alternative techniques have been proposed [Karger et al., 2011, Zhou et al., 2012]. In this paper we will use the approximate mean-field algorithm proposed by Liu et al. [2012] which consists in iterating between the following two steps until convergence:", "startOffset": 95, "endOffset": 325}, {"referenceID": 5, "context": "The only exception is the work by Karger et al. [2011], who propose to assign the same number r \u201c B{M of workers to each task and prove that this policy is order-optimal with respect to any other allocation policy that uses only the information available before the crowdsourcing process.", "startOffset": 34, "endOffset": 55}, {"referenceID": 5, "context": "The only exception is the work by Karger et al. [2011], who propose to assign the same number r \u201c B{M of workers to each task and prove that this policy is order-optimal with respect to any other allocation policy that uses only the information available before the crowdsourcing process. In contrast, a number of empirical-oriented works on crowdsourcing advocate the use of other policies that adapt the allocation depending on the data collected during the crowdsourcing process. Specifically Welinder and Perona [2010] propose an algorithm that computes the confidence in the classifications after the acquisition of every new label, and allocates more workers on the most uncertain tasks.", "startOffset": 34, "endOffset": 523}, {"referenceID": 5, "context": "The only exception is the work by Karger et al. [2011], who propose to assign the same number r \u201c B{M of workers to each task and prove that this policy is order-optimal with respect to any other allocation policy that uses only the information available before the crowdsourcing process. In contrast, a number of empirical-oriented works on crowdsourcing advocate the use of other policies that adapt the allocation depending on the data collected during the crowdsourcing process. Specifically Welinder and Perona [2010] propose an algorithm that computes the confidence in the classifications after the acquisition of every new label, and allocates more workers on the most uncertain tasks. Simpson and Roberts [2014] address the same problem from a different perspective and introduce a greedy algorithm to maximise the amount of information collected from the crowd.", "startOffset": 34, "endOffset": 721}, {"referenceID": 11, "context": "Notably these two approaches belong to the family of active learning policies [Settles, 2010], but the authors provide no theoretical guarantee on their performances.", "startOffset": 78, "endOffset": 93}, {"referenceID": 5, "context": "Specifically we are interested in computing the probability Ppp\u0302`qi \u201c `i q of classifying task i correctly under each of the following policies: \u2022 the uniform allocation policy which allocates the same number of workers to each task [Karger et al., 2011]; \u2022 a greedy policy that maximises the expected information gain of each new incoming label [Simpson and Roberts, 2014]; \u2022 a simple uncertainty sampling rule that assigns new workers on the task with the least confident label [Lewis and Gale, 1994, Welinder and Perona, 2010].", "startOffset": 233, "endOffset": 254}, {"referenceID": 12, "context": ", 2011]; \u2022 a greedy policy that maximises the expected information gain of each new incoming label [Simpson and Roberts, 2014]; \u2022 a simple uncertainty sampling rule that assigns new workers on the task with the least confident label [Lewis and Gale, 1994, Welinder and Perona, 2010].", "startOffset": 99, "endOffset": 126}, {"referenceID": 12, "context": "1 Equivalence between different active learning policies We can now adapt the worker allocation policy of Simpson and Roberts [2014] to our setup by choosing the task i that maximises the amount of information collected at each round.", "startOffset": 106, "endOffset": 133}, {"referenceID": 2, "context": "We can then apply the results in Ethier and Khoshnevisan [2002] and derive an upper bound on the expected number of labels required by the active learning policy to reach zB :", "startOffset": 33, "endOffset": 64}, {"referenceID": 3, "context": "It must be noted that Equation 17 is not asymptotically tight, but nevertheless provides a better bound for \u201csmall\u201d values of ru compared to other concentration inequalities [Hoeffding, 1963].", "startOffset": 174, "endOffset": 191}, {"referenceID": 8, "context": "These two differences in the setting have the following consequences: \u2022 in order to aggregate the set of labels collected from the crowd we need to rely on the output of some inference method, which is inherently less reliable than the prediction of the optimal rule by Nitzan and Paroush [1982]; \u2022 in general, having the workers execute multiple tasks is beneficial as it improves the accuracy of the estimation of their skills, however it can also interfere with the allocation policy as we cannot assign the same worker j on a particular task i more than once.", "startOffset": 270, "endOffset": 296}, {"referenceID": 8, "context": "First, we selected the approximate mean-field algorithm by Liu et al. [2012] to learn the workers\u2019 parameter pj and predict the classification of the tasks.", "startOffset": 59, "endOffset": 77}], "year": 2016, "abstractText": "Crowdsourcing has been successfully employed in the past as an effective and cheap way to execute classification tasks and has therefore attracted the attention of the research community. However, we still lack a theoretical understanding of how to collect the labels from the crowd in an optimal way. In this paper we focus on the problem of worker allocation and compare two active learning policies proposed in the empirical literature with a uniform allocation of the available budget. To this end we make a thorough mathematical analysis of the problem and derive a new bound on the performance of the system. Furthermore we run extensive simulations in a more realistic scenario and show that our theoretical results hold in practice.", "creator": "LaTeX with hyperref package"}}}