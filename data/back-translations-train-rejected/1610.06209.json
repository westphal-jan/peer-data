{"id": "1610.06209", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Oct-2016", "title": "Structured adaptive and random spinners for fast machine learning computations", "abstract": "We consider an efficient computational framework for speeding up several machine learning algorithms with almost no loss of accuracy. The proposed framework relies on projections via structured matrices that we call Structured Spinners, which are formed as products of three structured matrix-blocks that incorporate rotations. The approach is highly generic, i.e. i) structured matrices under consideration can either be fully-randomized or learned, ii) our structured family contains as special cases all previously considered structured schemes, iii) the setting extends to the non-linear case where the projections are followed by non-linear functions, and iv) the method finds numerous applications including kernel approximations via random feature maps, dimensionality reduction algorithms, new fast cross-polytope LSH techniques, deep learning, convex optimization algorithms via Newton sketches, quantization with random projection trees, and more. The proposed framework comes with theoretical guarantees characterizing the capacity of the structured model in reference to its unstructured counterpart and is based on a general theoretical principle that we describe in the paper. As a consequence of our theoretical analysis, we provide the first theoretical guarantees for one of the most efficient existing LSH algorithms based on the HD3HD2HD1 structured matrix [Andoni et al., 2015]. The exhaustive experimental evaluation confirms the accuracy and efficiency of structured spinners for a variety of different applications.", "histories": [["v1", "Wed, 19 Oct 2016 20:48:25 GMT  (171kb,D)", "http://arxiv.org/abs/1610.06209v1", "arXiv admin note: substantial text overlap witharXiv:1605.09046"], ["v2", "Fri, 28 Oct 2016 01:25:06 GMT  (171kb,D)", "http://arxiv.org/abs/1610.06209v2", "arXiv admin note: substantial text overlap witharXiv:1605.09046"], ["v3", "Sat, 26 Nov 2016 17:14:56 GMT  (171kb,D)", "http://arxiv.org/abs/1610.06209v3", "arXiv admin note: substantial text overlap witharXiv:1605.09046"]], "COMMENTS": "arXiv admin note: substantial text overlap witharXiv:1605.09046", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["mariusz bojarski", "anna choromanska", "krzysztof choromanski", "francois fagan", "cedric gouy-pailler", "anne morvan", "nourhan sakr", "tamas sarlos", "jamal atif"], "accepted": false, "id": "1610.06209"}, "pdf": {"name": "1610.06209.pdf", "metadata": {"source": "CRF", "title": "Structured adaptive and random spinners for fast machine learning computations", "authors": ["Mariusz Bojarski", "Anna Choromanska", "Krzysztof Choromanski", "Francois Fagan", "Cedric Gouy-Pailler", "Anne Morvan", "Nouri Sakr", "Tamas Sarlos", "Jamal Atif"], "emails": ["mbojarski@nvidia.com", "achoroma@cims.nyu.edu", "kchoro@google.com", "ff2316@columbia.edu", "cedric.gouy-pailler@cea.fr", "anne.morvan@cea.fr", "nts2122@columbia.edu", "stamas@google.com", "jamal.atif@dauphine.fr"], "sections": [{"heading": null, "text": "The proposed framework is based on projections on structured matrices, which we call Structured Spinners, which are formed as the products of three structured matrix blocks containing rotations; the approach is very general, i.e. i) structured matrices that are eligible can be either fully randomized or learned; ii) our structured family includes as special cases all previously considered structured schemes; iii) the setting extends to the nonlinear case where the projections are followed by nonlinear functions; and iv) the method finds numerous applications, including kernel approximations via random feature sketches, dimension reduction algorithms, new fast polytopic LSH techniques, deep learning, convex optimization algorithms via Newton sketches, quantization with random projection trees, and more. The proposed framework comes with theoretical guarantees that describe the ability of the structured part, which is initially structured on the principle of 3H, as it is presented in 3H."}, {"heading": "1 Introduction", "text": "This year, it has reached the stage where it will be able to put itself at the forefront in order to embark on the path to the future."}, {"heading": "2 Related work", "text": "Most of these structured constructions refer to Sparse [Ailon and Chazelle, 2006, Dasgupta and Chazelle], most of these structural constructions refer to Sparse [Ailon and Chazelle, 2006, Dasgupta et al.] or circular matrices [Vyb, 2011, Hinrichs and Vybral, 2011]."}, {"heading": "4 Theoretical results", "text": "We now show that structured spinners can replace their unstructured counterparts in many machine learning algorithms with minimal loss of accuracy.Let AG be a machine learning algorithm applied to a fixed dataset X Rn and parameterized by a set G of matrices G Rm \u00b7 n, in which each G is either learned or Gaussian with independent entries from N (0, 1).Let us also assume that the AG consists of functions f1,... fs, in which each fi applies a certain matrix Gi of G to vectors from any linear space Li of dimensionality. Note that for a fixed dataset X function fi is a function of a random vector qfi = (Gix 1) T,... (Gix di) T-Rdi \u00b7 m, vectors from any linear space Li of dimensionality at most. Note that for a fixed dataset X function fi is a function of a random vector qfi = (Gix di) T,... (Gix di) T-Rdi \u00b7 m, vectors from any linear space Li of dimensionality is replaced by a structural one (x) of the precarious Gx."}, {"heading": "4.1 Random setting", "text": "We need the following definition: Definition 3 A is b-convex, if it is a union of at most b-convex sets.Definition 3 A is b-convex, if it is a union of at most b-convex sets.Definition 3 A, then f-1i (S) is measurable and b-convex is for b-convex, the probability that fi (qfi) belongs to S is close to the probability that f-1 (qf) belongs to S. Theorem 1 (structured random setting) Let A is a randomized algorithm that uses unstructured matrices G and let s, d and fis as at the beginning of the section. Let's replace the unstructured matrix G with one of the structured spinners defined in Section 3 with blocks of m. Then we are large enough for n to have omd (1) and fixed fi with probability psucc 1 (2) at least."}, {"heading": "4.2 Adaptive setting", "text": "The following theorem explains that structured spinners can be used to replace unstructured, fully connected neural network layers that perform dimensionality reduction (like hidden layers in certain autoencoders), provided the input data exhibit a low intrinsic dimensionality. These theoretical results have been confirmed in experiments presented in the next section. Let us use notation from Theorem 1. Theorem 5. Let us consider a matrix M that encodes the connection weights between a layer l0 of size n and a layer l1 of size m in an learned unstructured neural network model. Let us assume that the input to layer vv2 vv2 comes from the d-dimensional space L (although potentially embedded in a much higher dimensional space) Mv2 Mv2 Mv2 Mv2 Mv2 Mv2 Mv2 (though) Mv2 Mv2 Mv2 Mv2 (Mv2) Mv2 Mv2 Mv2 (Mv2) Mv2 Mv2 Mv2 (Mv2) Mv2 Mv2 Mv2 (Mv2) Mv2 Mv2 Mv2 Mv2 (Mv2) Mt2 = M2 (M2) Mn = M2 (Mn) Mn = Mn = (n)"}, {"heading": "5 Experiments", "text": "In this section, we look at a wide range of different applications of structured spinners: site-sensitive hashing, kernel approximations, and finally neural networks. Experiments with Newton sketches are moved to the supplement. Experiments have been performed with Python. In particular, NumPy is associated with a highly optimized BLAS library (Intel MKL). Fast Fourier Transform is performed with numpy.fft and Fast Hadamard Transform is used ffht by [Andoni et al., 2015]. To make a fair comparison, we have set up: OMP NUM THREADS = 1, so that each experiment is performed on a single thread. Each parameter of the structured spinner matrix is calculated in advance, so that only matrix vector products are taken into account."}, {"heading": "5.1 Locality-Sensitive Hashing (LSH)", "text": "In Figure 2, we compare the collision probabilities for the low-dimensional case (n = 256) in which for each interval the collision probability was calculated for 20,000 points. Results are shown for a hash function (averaged over 100 runs). We report results for a random 256 x 64 Gaussian matrix G and five other types of matrices from a family of structured spinners (descending order of parameters): GcircK2K1, GToeplitzD2HD1, Gskew \u2212 circD2HD1, HDg1,..., gnHD2HD1 and HD3HD2HD1, where Ki, GToeplitz and Gskew \u2212 circ are each a Kronecker matrix with discrete entries, Gaussian toeplitzD2HD1 and Gaussian Skew \u2212 circulating matrices. All matrices from the family of structured collision matrices show high collision probabilities with no gausal collisions, no gausal probability of collisions."}, {"heading": "5.2 Kernel approximation", "text": "In the second experiment, we apply the Gaussian and angular kernels with random Fourier characteristics. The Gaussian Random Matrix (with i.i.d. Gaussian entries) can be used to test random Fourier characteristics with a specified \u03c3. This Gaussian Random Matrix is replaced with specific matrices from a family of structured spinners for Gaussian and Sindhwani, 2016. The attribute maps obtained are compared. To test the quality of structured kernel's approximations, we calculate gram matrix reconstruction errors as in [Choromanski and Sindhwani, 2016]: The attribute maps obtained are compared where K, K, respectively, are the exact and approximate gram matrices, as a function of the number of random characteristics. If the number of random characteristics is greater than the data dimensions, we apply the block mechanism."}, {"heading": "5.3 Neural networks", "text": "Finally, we conducted experiments with neural networks with two different network architectures, the first is a fully connected network with two completely connected layers (we call it MLP), where we refer to the size of the hidden layer as h, and the second is a Convolutionary network with the following architecture: \u2022 Folding layer with filter size 5 \u00d7 5, 4 Feature Maps + ReLU + Max Pooling (Region 2 \u00d7 2 and Step 2 \u00d7 2) \u2022 Folding layer with filter size 5 \u00d7 5, 6 Feature maps + ReLU + Max Pooling (Region 2 \u00d7 2 and Step 2 \u00d7 2) \u2022 Fully connected layer (h outputs) + ReLU \u2022 Fully connected layer (10 outputs) \u2022 LogSoftMax.Experiments were performed on the MNIST dataset. In both experiments, we re-parameterized each matrix of weights of fully connected layers with a structured graph matrix."}, {"heading": "5.4.1 Proof of Remark 1", "text": "s Inequality) Let X1,..., Xn be a martingale and take that \u2212 \u03b1i \u2264 \u03b2i for some positive constances \u03b11,..., \u03b1n, \u03b21,..., \u03b2n. Denote X = \u2211 n i = 1Xi. Then the following applies: P [X \u2212 E] | a] \u2264 2e \u2212 a2 \u00b2 n = 1 (\u03b1i + \u03b2i) 2 (4) Proof: Denote by x j an image of xj under transformation HD. Note that the ith dimension of x-j's we is given by the formula: x-ji = hi, 1x j 1 + j, n, where hl, u stands for the lth element of the line HD."}, {"heading": "5.4.3 Proof of Lemma 1", "text": "In this context, it is easy to see that one can be a Gaussian vector (this vector corresponds to the first line of Gcirc). Furthermore, linear figures are defined as: (x0, x1,..., xn \u2212 1) T) = (xn \u2212 i + 1, xi \u2212 1) T, where operations on indices are defined as modulo n. The value of n) and p (n) comes from the fact that matrix HD1 is a matrix (n), p (n) -balanced and from Remark 1. In this context, the sequence (1,..., 2) is discrete and corresponds to the diagonal of D2."}, {"heading": "5.4.4 Proof of Theorem 1", "text": "Let us briefly review the evidence results before presenting them in detail. Challenges to the evidence results for structured matrices arise from the fact that, for each given x-Rn, different dimensions of y = Gstructx are no longer independent (as is the case for the unstructured setting). However, for matrices from the structured spinner family, we can show that it is highly likely that different elements of y correspond to the projections of a given vector r (see Section 3) in directions that are close to orthogonal. The \"near orthogonal\" characteristic is achieved with the use of the Hanson-Wright method, which focuses on square shapes containing vectors of the sub-Gaussian Random variables. If r is Gaussian, then we assume that projections of the Gaussian vector in orthogonal directions are independent, we can conclude that dimensions are independent of y."}, {"heading": "5.4.5 Proof of Theorem 2", "text": "Proof: Suppose fi is a convex function of qfi (if fi is concave, then the proof is completely analogous). For each t \u00b2 R we leave St = {qfi: fi (qfi) \u2264 t} for fi and St = {qf \u00b2 i: f \u00b2 i (qf \u00b2 i) \u2264 t} for f \u00b2 i. From the convexity assumption we can conclude that St is a convex set. Thus, we can apply theorem 1 directly and follow the result with respect to cdf functions. To get the result with respect to the characteristic functions, first note that we have: \u03b2X (t) = \u0445 1 \u2212 1 P cos [tX) > s] ds + i \u0445 1 \u2212 1 P [sin (tX) > s] ds (31) The event {cos (tX) > s} for t 6 = 0 is equivalent to: X \u00b2 I \u00b2 for some families of intervals I. Similar observation applies to the event {sin (tX s) > s."}, {"heading": "5.4.6 Proof of Theorem 3", "text": "Proof: This comes directly from Theorem 1 and Lemma 1."}, {"heading": "5.4.7 Proof of Theorem 4", "text": "The proof: We assume that the structured matrix consists of only one block of m-rows and compare its performance with the unstructured variant of m-rows (the more general case if the structured matrix is obtained by stacking vertically many blocks). Note that they are two-dimensional, where m is the number of rows of the block used in the structured environment. From Theorem 3 we conclude that the probability will be at least pSuccess, where pSuccess, as in the statement of the theorem, is the following attitudes for each convex of m-dimensional setA."}, {"heading": "5.4.9 Proof of Theorem 5", "text": "In this section we show that we derive the vector r-Rk from the above definition, each matrix M-Rm-n-n-B-B-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-"}, {"heading": "5.4.10 Additional experiments", "text": "It's not the way in which we behave in the way in which we behave in the way in which we behave in the way in which we behave in the way in which we behave in the way in which we behave in the way in which we behave in the way in which we behave in the way in which we behave in the way in which we behave in the way in which we behave in the way in which we behave in the way in which we behave in the way in which we behave in the way in which we behave in the way in which we behave in the way in which we behave in the way in which we behave in the way in which we behave in the way in which we behave in the way in which we behave in the way in which we behave in the way in which we behave in the way in which we behave in the way in which we behave in the way in which we behave in the way in which we behave in the way in which we behave in the way in which we behave in the way in which we behave in the way in which we behave in the way in which we behave in the way in which we behave in the way in which we behave in the way in which we behave in the way in which we behave in the way in which we behave in the way in which we behave in the way in which we behave in the way in which we behave in the way in which we behave in the way in which we behave in the way in which we behave in the way in which we behave in the way in which we behave in the way in the way in which we behave in the way in which we behave in the way in the way in the way in which we behave in which we behave in which we behave in the way in the way in which we behave in the way in the way in which we behave in the way in which we behave in the way in which we behave in which we behave in the way in which we behave in the way in the way in which we behave in the way in the way in which we behave in the way in the way in which we behave in which we behave in the way in which we behave in the way in the way in which"}], "references": [{"title": "Approximate nearest neighbors and the fast Johnson-Lindenstrauss transform", "author": ["Ailon", "Chazelle", "N. 2006] Ailon", "B. Chazelle"], "venue": null, "citeRegEx": "Ailon et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Ailon et al\\.", "year": 2006}, {"title": "An almost optimal unrestricted fast JohnsonLindenstrauss transform", "author": ["Ailon", "Liberty", "N. 2011] Ailon", "E. Liberty"], "venue": "In SODA", "citeRegEx": "Ailon et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Ailon et al\\.", "year": 2011}, {"title": "Practical and optimal LSH for angular distance", "author": ["Andoni et al", "A. 2015] Andoni", "P. Indyk", "T. Laarhoven", "I.P. Razenshteyn", "L. Schmidt"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q2015\\E", "shortCiteRegEx": "al. et al\\.", "year": 2015}, {"title": "Bounds on determinants of perturbed diagonal matrices. arXiv:1401.7084", "author": ["Brent et al", "R.P. 2014] Brent", "J.H. Osborn", "W.D. Smith"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q2014\\E", "shortCiteRegEx": "al. et al\\.", "year": 2014}, {"title": "Kernel methods for deep learning", "author": ["Cho", "Saul", "Y. 2009] Cho", "L.K. Saul"], "venue": null, "citeRegEx": "Cho et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2009}, {"title": "Binary embeddings with structured hashed projections", "author": ["Choromanska et al", "A. 2016] Choromanska", "K. Choromanski", "M. Bojarski", "T. Jebara", "S. Kumar", "Y. LeCun"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q2016\\E", "shortCiteRegEx": "al. et al\\.", "year": 2016}, {"title": "Recycling randomness with structure for sublinear time kernel expansions", "author": ["Choromanski", "Sindhwani", "K. 2016] Choromanski", "V. Sindhwani"], "venue": null, "citeRegEx": "Choromanski et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Choromanski et al\\.", "year": 2016}, {"title": "Scalable kernel methods via doubly stochastic gradients", "author": ["Dai et al", "B. 2014] Dai", "B. Xie", "N. He", "Y. Liang", "A. Raj", "Balcan", "M.-F", "L. Song"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q2014\\E", "shortCiteRegEx": "al. et al\\.", "year": 2014}, {"title": "A sparse johnson: Lindenstrauss transform", "author": ["Dasgupta et al", "A. 2010] Dasgupta", "R. Kumar", "T. Sarlos"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q2010\\E", "shortCiteRegEx": "al. et al\\.", "year": 2010}, {"title": "Random projection trees and low dimensional manifolds", "author": ["Dasgupta", "Freund", "S. 2008] Dasgupta", "Y. Freund"], "venue": null, "citeRegEx": "Dasgupta et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Dasgupta et al\\.", "year": 2008}, {"title": "Predicting parameters in deep learning", "author": ["Denil et al", "M. 2013] Denil", "B. Shakibi", "L. Dinh", "M. Ranzato", "N.D. Freitas"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q2013\\E", "shortCiteRegEx": "al. et al\\.", "year": 2013}, {"title": "Random feature mapping with signed circulant matrix projection", "author": ["Feng et al", "C. 2015] Feng", "Q. Hu", "S. Liao"], "venue": "In IJCAI", "citeRegEx": "al. et al\\.,? \\Q2015\\E", "shortCiteRegEx": "al. et al\\.", "year": 2015}, {"title": "Deep learning. Book in preparation for", "author": ["Goodfellow et al", "I. 2016] Goodfellow", "Y. Bengio", "A. Courville"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q2016\\E", "shortCiteRegEx": "al. et al\\.", "year": 2016}, {"title": "Approximate nearest neighbor: Towards removing the curse of dimensionality", "author": ["Har-Peled et al", "S. 2012] Har-Peled", "P. Indyk", "R. Motwani"], "venue": "Theory of Computing,", "citeRegEx": "al. et al\\.,? \\Q2012\\E", "shortCiteRegEx": "al. et al\\.", "year": 2012}, {"title": "Johnson-lindenstrauss lemma for circulant matrices", "author": ["Hinrichs", "Vybral", "A. 2011] Hinrichs", "J. Vybral"], "venue": "Random Struct. Algorithms,", "citeRegEx": "Hinrichs et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Hinrichs et al\\.", "year": 2011}, {"title": "Kernel methods match deep neural networks on timit", "author": ["Huang et al", "2014] Huang", "P.-S", "H. Avron", "T. Sainath", "V. Sindhwani", "B. Ramabhadran"], "venue": "In ICASSP", "citeRegEx": "al. et al\\.,? \\Q2014\\E", "shortCiteRegEx": "al. et al\\.", "year": 2014}, {"title": "Extensions of Lipschitz mappings into a Hilbert space", "author": ["Johnson", "Lindenstrauss", "W. 1984] Johnson", "J. Lindenstrauss"], "venue": "In Conference in modern analysis and probability,", "citeRegEx": "Johnson et al\\.,? \\Q1984\\E", "shortCiteRegEx": "Johnson et al\\.", "year": 1984}, {"title": "Fastfood-computing hilbert space expansions in loglinear time", "author": ["Le et al", "Q. 2013] Le", "T. Sarl\u00f3s", "A. Smola"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q2013\\E", "shortCiteRegEx": "al. et al\\.", "year": 2013}, {"title": "Restructuring of deep neural network acoustic models with singular value decomposition", "author": ["J. Li"], "venue": "In Interspeech", "citeRegEx": "Li,? \\Q2013\\E", "shortCiteRegEx": "Li", "year": 2013}, {"title": "Dense fast random projections and lean Walsh transforms", "author": ["Liberty et al", "E. 2008] Liberty", "N. Ailon", "A. Singer"], "venue": "In RANDOM", "citeRegEx": "al. et al\\.,? \\Q2008\\E", "shortCiteRegEx": "al. et al\\.", "year": 2008}, {"title": "Convolutional kernel networks", "author": ["Mairal et al", "J. 2014] Mairal", "P. Koniusz", "Z. Harchaoui", "C. Schmid"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q2014\\E", "shortCiteRegEx": "al. et al\\.", "year": 2014}, {"title": "Randomized sketches of convex programs with sharp guarantees", "author": ["Pilanci", "Wainwright", "M. 2014] Pilanci", "M.J. Wainwright"], "venue": "In ISIT", "citeRegEx": "Pilanci et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pilanci et al\\.", "year": 2014}, {"title": "Newton sketch: A linear-time optimization algorithm with linear-quadratic convergence. CoRR, abs/1505.02250", "author": ["Pilanci", "Wainwright", "M. 2015] Pilanci", "M.J. Wainwright"], "venue": null, "citeRegEx": "Pilanci et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Pilanci et al\\.", "year": 2015}, {"title": "Random features for large-scale kernel machines", "author": ["Rahimi", "Recht", "A. 2007] Rahimi", "B. Recht"], "venue": null, "citeRegEx": "Rahimi et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Rahimi et al\\.", "year": 2007}, {"title": "Low-rank matrix factorization for deep neural network training with high-dimensional output targets", "author": ["Sainath et al", "T.N. 2013] Sainath", "B. Kingsbury", "V. Sindhwani", "E. Arisoy", "B. Ramabhadran"], "venue": "In ICASSP", "citeRegEx": "al. et al\\.,? \\Q2013\\E", "shortCiteRegEx": "al. et al\\.", "year": 2013}, {"title": "Structured transforms for small-footprint deep learning", "author": ["Sindhwani et al", "V. 2015] Sindhwani", "T.N. Sainath", "S. Kumar"], "venue": "In Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems", "citeRegEx": "al. et al\\.,? \\Q2015\\E", "shortCiteRegEx": "al. et al\\.", "year": 2015}, {"title": "Spherical LSH for approximate nearest neighbor search on unit hypersphere", "author": ["Terasawa", "Tanaka", "K. 2007] Terasawa", "Y. Tanaka"], "venue": "WADS", "citeRegEx": "Terasawa et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Terasawa et al\\.", "year": 2007}, {"title": "Deep fried convnets", "author": ["Yang et al", "Z. 2015] Yang", "M. Moczulski", "M. Denil", "N. de Freitas", "A. Smola", "L. Song", "Z. Wang"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q2015\\E", "shortCiteRegEx": "al. et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 18, "context": ", 2013] introduces low-rank matrix factorization to reduce the size of the fully connected layers at train time, and [Li, 2013] uses low-rank factorizations with SVD after training the full model.", "startOffset": 117, "endOffset": 127}], "year": 2017, "abstractText": "We consider an efficient computational framework for speeding up several machine learning algorithms with almost no loss of accuracy. The proposed framework relies on projections via structured matrices that we call Structured Spinners, which are formed as products of three structured matrix-blocks that incorporate rotations. The approach is highly generic, i.e. i) structured matrices under consideration can either be fully-randomized or learned, ii) our structured family contains as special cases all previously considered structured schemes, iii) the setting extends to the non-linear case where the projections are followed by non-linear functions, and iv) the method finds numerous applications including kernel approximations via random feature maps, dimensionality reduction algorithms, new fast cross-polytope LSH techniques, deep learning, convex optimization algorithms via Newton sketches, quantization with random projection trees, and more. The proposed framework comes with theoretical guarantees characterizing the capacity of the structured model in reference to its unstructured counterpart and is based on a general theoretical principle that we describe in the paper. As a consequence of our theoretical analysis, we provide the first theoretical guarantees for one of the most efficient existing LSH algorithms based on the HD3HD2HD1 structured matrix [Andoni et al., 2015]. The exhaustive exequal contribution perimental evaluation confirms the accuracy and efficiency of structured spinners for a variety of different applications.", "creator": "LaTeX with hyperref package"}}}