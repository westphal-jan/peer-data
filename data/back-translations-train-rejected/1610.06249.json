{"id": "1610.06249", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Oct-2016", "title": "Multilevel Anomaly Detection for Mixed Data", "abstract": "Anomalies are those deviating from the norm. Unsupervised anomaly detection often translates to identifying low density regions. Major problems arise when data is high-dimensional and mixed of discrete and continuous attributes. We propose MIXMAD, which stands for MIXed data Multilevel Anomaly Detection, an ensemble method that estimates the sparse regions across multiple levels of abstraction of mixed data. The hypothesis is for domains where multiple data abstractions exist, a data point may be anomalous with respect to the raw representation or more abstract representations. To this end, our method sequentially constructs an ensemble of Deep Belief Nets (DBNs) with varying depths. Each DBN is an energy-based detector at a predefined abstraction level. At the bottom level of each DBN, there is a Mixed-variate Restricted Boltzmann Machine that models the density of mixed data. Predictions across the ensemble are finally combined via rank aggregation. The proposed MIXMAD is evaluated on high-dimensional realworld datasets of different characteristics. The results demonstrate that for anomaly detection, (a) multilevel abstraction of high-dimensional and mixed data is a sensible strategy, and (b) empirically, MIXMAD is superior to popular unsupervised detection methods for both homogeneous and mixed data.", "histories": [["v1", "Thu, 20 Oct 2016 00:04:55 GMT  (91kb,D)", "http://arxiv.org/abs/1610.06249v1", "9 pages"]], "COMMENTS": "9 pages", "reviews": [], "SUBJECTS": "cs.LG cs.DB", "authors": ["kien do", "truyen tran", "svetha venkatesh"], "accepted": false, "id": "1610.06249"}, "pdf": {"name": "1610.06249.pdf", "metadata": {"source": "CRF", "title": "Multilevel Anomaly Detection for Mixed Data", "authors": ["Kien Do", "Truyen Tran", "Svetha Venkatesh"], "emails": ["dkdo@deakin.edu.au", "truyen.tran@deakin.edu.au", "svetha.venkatesh@deakin.edu.au"], "sections": [{"heading": null, "text": "Unmonitored anomalies often lead to the identification of low-density regions. Important problems arise when data is high-dimensional and mixed with discrete and continuous attributes. We propose MIXMAD, which stands for MIXed Data Multilevel Anomaly Detection, an ensemble method that estimates the sparse regions at multiple levels of abstraction of mixed data. The hypothesis refers to areas where multiple data abstractions exist, a data point can be anomalous in terms of raw representation or more abstract representation. To this end, our method sequentially constructs an interplay of deep belief nets (DBNs) with different depths. Each DBN is an energy-based detector at a predefined abstraction level. At the lowest level of each DBN, there is a mixed-varied Restricted Boltzmann Machine that models the density of mixed data."}, {"heading": "1 Introduction", "text": "This year it is so far that it will be able to use the mentionlcihsrcsrteeS rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rtef\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rtef\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rtef\u00fc the rtef\u00fc the rtef\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf"}, {"heading": "2 Background", "text": "In fact, most of the people who are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance"}, {"heading": "3.1 Prelim: Shallow Model for Mixed Data.", "text": "For subsequent development, let us briefly consider a probabilistic graphical model known as Mixed-Variate Restricted Boltzmann Machines (Mv.RBM) [27] for modelling mixed data. Let x be an input vector of N (mixed-type) elements, and h (0, 1} K be a binary hidden vector, Mv.RBM defines the common distribution as follows: P (x, h) - exp (\u2212 E (x, h))) - where E (x, h) - energy function is broken down as follows: (2) E (x, h) - (N) - (xi) + K - exp = 1 (\u2212 bk) - exp (xi) - hkThe sub-energy functions Ei (xi) and Gik (xi) - exact (xi) - exact (x) - exact (x) - exact (c) - - (exact (-) (- -) (exact (-) (-) (- (-) (-)"}, {"heading": "3.2 Extending Deep Belief Nets for Mixed Data.", "text": "Deep Belief Network (DBN) [17] is a generative data model. It assumes that the data x is generated from hidden binary variables h1, which are derived from higher hidden binary variables h2 etc. Two successive layers in DBN form a Boltzmann Limited Machine (RBM), which models either P (x, h1) at the lowest level or P (hl, hl + 2) at the higher levels. A DBN is usually trained by learning a stack of RBMs in layers. First, an RBM is trained on the input data, its weights are then frozen. The hidden posterior is used to generate inputs for the next RBM, i.e. h \u00b2 P (h1 | x). The process is repeated to the last RBM. This process of freezing the lower weights is aimed at optimizing the variable limit of data probability BBP (x), i.e., the graphs are mainly directed to the upper layer, whereas the DN is mainly directed to the upper layer."}, {"heading": "3.3 Abstracted Anomaly Detection Using Deep Belief Nets.", "text": "Although the stage-by-stage learning process leading to DBN optimizes the lower limit of logP (x), it is still not possible to estimate the limit for density-based anomaly detection. Let L be the number of hidden layers. Existing methods typically use DBNs to (a) learn high-grade features through P (hL | hL \u2212 1) and feed existing anomaly detectors (e.g. [32]); and (b) build a deep autoencoder to then estimate the reconstruction error [7, 14, 25, 26]. Here, we propose an alternative to use DBN (hL | hL \u2212 1) directly for anomaly detection. The idea is to recognize that the RBM at the top of the DBN is working on data abstraction hL, and the previous density PL (hL) can be replaced by the previous density PL (hL)."}, {"heading": "3.4 Multilevel Detection Procedure With DBN Ensemble.", "text": "Remember that our Multilevel Anomaly Detection (MAD) hypothesis is that for areas where multiple data abstractions exist (e.g. in images and videos), an anomaly can be detected on one or more abstract representations. Each level of abstraction would detect abnormality in different ways. Consider, for example, an interior environment in which normal images contain a regular arrangement of furniture. An image of a room with a random arrangement (e.g. a chair in a bed) may appear normal at pixel level and at object class level, but not object context, suggesting the following procedure: Apply multiple levels of abstraction and at each level, estimate of an anomaly value, then combine all valuations. Since free energies in Equation (7) are different at different levels, a direct combination of anomaly values is not possible. A reasonable approach is through ranking, i.e., the energies at each level are suppressed as ranking."}, {"heading": "3.4.2 Separation of Abstraction and Detection.", "text": "Remember that we use RBMs for both abstraction (equivalent (5)) and detection of anomalies (equivalent (7). Note that data abstraction and detection of anomalies have different goals - abstraction typically requires more bits to adequately unravel multiple variation factors [8], while detection requires fewer bits to estimate a ranking value. Figure 2 presents the algorithm for detecting anomalies on multiple levels. It trains an Mv.RBM and (L \u2212 1) DBN's increasing depth - from 2 to L - with linear time complexity in L. They produce L rankings, which are then aggregated with equivalent (9)."}, {"heading": "4 Experiments", "text": "This section reports on experiments and results of MIXMAD on a comprehensive collection of data sets. We first present the cases for individual data types in Section 4.1, and then extend them for mixed data in Section 4.2."}, {"heading": "4.1 Homogeneous Data.", "text": "We use three high-dimensional real datasets with very different features: handwritten numbers (MNIST), Internet displays, and clinical records of birth episodes. \u2022 The MNIST has 60,000 gray images 28 x 28 for training and 10,000 images for testing1. The raw pixels are used as features (784 dimensions). Due to the simple visualization and complex data topology, these are excellent data for testing algorithms for detecting anomalies. We use digits \"8\" as normal and a small percentage (5%) of other digits as outliers. This proves to be a sophisticated digit compared to other digits - see Figure 3 (left) for1http: / / yann.lecun.com / exdb / mnist / failure of pixel-based k-next neighbor. We select 3,000 training images and retain all test sets. \u2022 The second dataset is Internet ads with general hospital anomaly of 69% as hospital injection."}, {"heading": "4.1.1 Models implementation.", "text": "We compare the proposed method with four popular flat, unattended baselines - k-NN, PCA, Gaussian mixture model (GMM) and one-class SVM (OCSVM) [11]. (a) The k-NN uses the mean distance from one test case to the k nearest instances as an outlier score [5]. We set k = 10 at euclidean distance. (b) For PCA, the total energy of \u03b1% is discarded, where \u03b1 is the estimated outlier rate in the training data. (d) The reconstruction error using the remaining eigenvectors is used as an outlier score. (c) The GMMs have four clusters and are regulated to work with high-dimensional data. The negative log probability serves as an outlier score. (d) The OCVSMs have RBF cores with automatic scaling XR XR-R XR-R XR-R XR-R XR-R-cores. We consider RM [baseline XR] XR-XR-XR-XR-XR-XR-XR-XR-XR-XR-XR-XR-XR-XR-XR-XR-XR-XR-XR-XR-XR-XR-XR-XR-XR-XR-XR-XR-XR-XR-XR-XR-XR-XR-XR-XR-XR-XR-XR-XR-XR-XR-XR-XR-XR-XR-XR-XR-XR-XR-XR-XR-XR-XR-XR-XR-XR-XR-XR-XR-XR-XR XR-XR-XR-XR-XR-XR-XR-XR-XR-XR XR-XR-XR-XR-XR-XR-XR-XR-XR-XR-XR-XR-XR-XR-XR-XR-XR-XR-XR-XR"}, {"heading": "4.1.2 Results.", "text": "To see how MIXMAD combines detection of detectors in the overall set, we execute the algorithms: RBM, which combines DBN with 2 layers, and MIXMAD, which combines RBM and DBN results. Fig. 3 shows the determined images of the RBM / DBN / MIXMAD against the classic k-NN. K-NN fails in 15 out of 20 cases, mainly due to the deviation in impact force expected for pixel-based match. RBM and DBN exhibit different errors, which confirms that the anomalies differ between the abstraction levels. Finally, the overall ensemble of RBM / DBN, then MIXMAD significantly improves detection. The error is largely attributable to the high deviation in styles (e.g. 8 with open loops). Table 3 gives the area under the ROC curve (AUC) for all methods and the MIXMAD total data sets between the MIXMAD or MIXM3 layers."}, {"heading": "4.2 Mixed Data.", "text": "To be consistent with previous work [9, 12, 20, 22], we report the results on the basis of the F-scores. The detection performance of the test data is shown in Table 6. Abstraction works well with L = 2, where the performance is generally better than with the flat Mv.RBM. However, when another layer is added, the results are mixed. Indeed, this pattern is not new, as it is similar to what is seen in the literature of DBNs for classification tasks [17, 24]. One possible conjecture is that signals at a certain higher level become too abstract and that the distribution becomes too flat to truly distinguish between low and high density regions."}, {"heading": "5 Discussion and Conclusion", "text": "As evidence for the argument in Section 3.4.2 on the separation of abstraction and detection of RBMs, it can be argued that the sizes of the RBMs that work well on the MNIST are not similar to those commonly found in the literature (e.g. see [17]). Typical numbers of hidden units range from 500 to 1,000 for a good generative model of the digits. However, we observe that 10 to 20 units for detection of RBMs and 50-100 units for abstraction of RBMs work well in our experiments, regardless of the training size. This suggests that the number of bits required for data generation is higher than those required for detection of anomalies. This is plausible, as the accurate data generation model must take into account all factors of variation and a huge number of density modes. On the other hand, the anomaly detection model only needs to be used for the identification of low density regions without taking into account the density of the modes."}, {"heading": "5.1 Conclusion.", "text": "To test the hypothesis, we first proposed the Multi-Level Anomaly Detection (MAD) hypothesis, according to which a data point is anomalous in relation to one or more levels of data abstraction. To test the hypothesis, we introduced MIXMAD, a process for forming a sequence of deep-relief networks, each of which provides a ranking of anomalies. All rankings are then aggregated by a simple p-standard trick. Experiments with both single and mixed data confirmed that (a) the representation of learning data through multi-level abstraction is a reasonable strategy for high-dimensional adjustments; and (b) MIXMAD is a competitive method. However, there is room for improvement. Firstly, the very deep work has not proved very successful. DBNs have proven their usefulness in abstraction, but there are other possibilities [30, 33]. Finally, the simple p standard can be replaced by a more sophisticated method of abstraction and abstraction."}, {"heading": "Acknowledgments", "text": "This work is supported in part by the Telstra-Deakin Centre of Excellence in Big Data and Machine Learning."}], "references": [{"title": "On the surprising behavior of distance metrics in high dimensional space", "author": ["Charu C Aggarwal", "Alexander Hinneburg", "Daniel A Keim"], "venue": "In International Conference on Database Theory,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2001}, {"title": "Theoretical foundations and algorithms for outlier ensembles", "author": ["Charu C Aggarwal", "Saket Sathe"], "venue": "ACM SIGKDD Explorations Newsletter,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2015}, {"title": "Aggregating inconsistent information: ranking and clustering", "author": ["Nir Ailon", "Moses Charikar", "Alantha Newman"], "venue": "Journal of the ACM (JACM),", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2008}, {"title": "Ensemble anomaly detection from multi-resolution trajectory features", "author": ["Shin Ando", "Theerasak Thanomphongphan", "Yoichi Seki", "Einoshin Suzuki"], "venue": "Data Mining and Knowledge Discovery,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2015}, {"title": "Fast outlier detection in high dimensional spaces", "author": ["Fabrizio Angiulli", "Clara Pizzuti"], "venue": "In European Conference on Principles of Data Mining and Knowledge Discovery,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2002}, {"title": "The distribution of clinical phenotypes of preterm birth syndrome: implications for prevention", "author": ["Fernando C Barros", "Aris T Papageorghiou", "Cesar G Victora", "Julia A Noble", "Ruyan Pang", "Jay Iams", "Leila Cheikh Ismail", "Robert L Goldenberg", "Ann Lambert", "Michael S Kramer"], "venue": "JAMA pediatrics,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2015}, {"title": "Deep belief networks for false alarm rejection in forward-looking ground-penetrating radar", "author": ["John Becker", "Timothy C Havens", "Anthony Pinar", "Timothy J Schulz"], "venue": "In SPIE Defense+ Security, pages 94540W\u201394540W. International Society for Optics and Photonics,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2015}, {"title": "Representation learning: A review and new perspectives", "author": ["Yoshua Bengio", "Aaron Courville", "Pascal Vincent"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2013}, {"title": "A practical outlier detection approach for mixed-attribute data", "author": ["Mohamed Bouguessa"], "venue": "Expert Systems with Applications,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2015}, {"title": "On the evaluation of unsupervised outlier detection: measures, datasets, and an empirical study", "author": ["Guilherme O Campos", "Arthur Zimek", "J\u00f6rg Sander", "Ricardo JGB Campello", "Barbora Micenkov\u00e1", "Erich Schubert", "Ira Assent", "Michael E Houle"], "venue": "Data Mining and Knowledge Discovery,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2015}, {"title": "Anomaly detection: A survey", "author": ["Varun Chandola", "Arindam Banerjee", "Vipin Kumar"], "venue": "ACM computing surveys (CSUR),", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2009}, {"title": "Outlier detection on mixed-type data: An energy-based approach", "author": ["Kien Do", "Truyen Tran", "Dinh Phung", "Svetha Venkatesh"], "venue": "International Conference on Advanced Data Mining and Applications (ADMA", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2016}, {"title": "Network anomaly detection with the restricted Boltzmann machine", "author": ["Ugo Fiore", "Francesco Palmieri", "Aniello Castiglione", "Alfredo De Santis"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2013}, {"title": "An intrusion detection model based on deep belief networks", "author": ["Ni Gao", "Ling Gao", "Quanli Gao", "Hai Wang"], "venue": "In Advanced Cloud and Big Data (CBD),", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2014}, {"title": "Loaded: Link-based outlier and anomaly detection in evolving data sets", "author": ["Amol Ghoting", "Matthew Eric Otey", "Srinivasan Parthasarathy"], "venue": "In ICDM,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2004}, {"title": "Training products of experts by minimizing contrastive divergence", "author": ["G.E. Hinton"], "venue": "Neural Computation,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2002}, {"title": "Reducing the dimensionality of data with neural networks", "author": ["G.E. Hinton", "R.R. Salakhutdinov"], "venue": null, "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2006}, {"title": "A fast learning algorithm for deep belief nets", "author": ["Geoffrey E Hinton", "Simon Osindero", "Yee-Whye Teh"], "venue": "Neural computation,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2006}, {"title": "The potential energy of an autoencoder", "author": ["Hanna Kamyshanska", "Roland Memisevic"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2015}, {"title": "Detecting outliers in highdimensional datasets with mixed attributes", "author": ["Anna Koufakou", "Michael Georgiopoulos", "Georgios C Anagnostopoulos"], "venue": "In DMIN,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2008}, {"title": "Discovering anomalies on mixed-type data using a generalized student-t based approach", "author": ["Yen-Cheng Lu", "Feng Chen", "Yating Wang", "Chang- Tien Lu"], "venue": "IEEE Transactions on Knowledge and Data Engineering,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2016}, {"title": "An exact mapping between the variational renormalization group and deep learning", "author": ["Pankaj Mehta", "David J Schwab"], "venue": "arXiv preprint arXiv:1410.3831,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2014}, {"title": "Deep Boltzmann Machines", "author": ["R. Salakhutdinov", "G. Hinton"], "venue": "In Proceedings of 20th AISTATS,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2009}, {"title": "Automated fault detection using deep belief networks for the quality inspection of electromotors", "author": ["Jianwen Sun", "Reto Wyss", "Alexander Steinecker", "Philipp Glocker"], "venue": "tm-Technisches Messen,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2014}, {"title": "Structured denoising autoencoder for fault detection and analysis", "author": ["Takaaki Tagawa", "Yukihiro Tadokoro", "Takehisa Yairi"], "venue": "In ACML,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2014}, {"title": "Mixed-variate restricted Boltzmann machines", "author": ["T. Tran", "D.Q. Phung", "S. Venkatesh"], "venue": "In Proc. of 3rd Asian Conference on Machine Learning (ACML), Taoyuan,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2011}, {"title": "Preterm birth prediction: Deriving stable and interpretable rules from high dimensional data", "author": ["Truyen Tran", "Wei Luo", "Dinh Phung", "Jonathan Morris", "Kristen Rickard", "Svetha Venkatesh"], "venue": "Conference on Machine Learning in Healthcare, LA,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2016}, {"title": "Learning deep representation of multityped objects and tasks", "author": ["Truyen Tran", "Dinh Phung", "Svetha Venkatesh"], "venue": "arXiv preprint arXiv:1603.01359,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2016}, {"title": "Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion", "author": ["P. Vincent", "H. Larochelle", "I. Lajoie", "Y. Bengio", "P.A. Manzagol"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2010}, {"title": "Predicting preterm birth is not elusive: Machine learning paves the way to individual wellness", "author": ["Ilia Vovsha", "Ashwath Rajan", "Ansaf Salleb-Aouissi", "Anita Raja", "Axinia Radeva", "Hatim Diab", "Ashish Tomar", "Ronald Wapner"], "venue": "In 2014 AAAI Spring Symposium Series,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2014}, {"title": "A deep learning approach for detecting malicious JavaScript code", "author": ["Yao Wang", "Wan-dong Cai", "Peng-cheng Wei"], "venue": "Security and Communication Networks,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2016}, {"title": "Deep structured energy based models for anomaly detection", "author": ["Shuangfei Zhai", "Yu Cheng", "Weining Lu", "Zhongfei Zhang"], "venue": "arXiv preprint arXiv:1605.07717,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2016}, {"title": "An effective pattern based outlier detection approach for mixed attribute data", "author": ["Ke Zhang", "Huidong Jin"], "venue": "In Australasian Joint Conference on Artificial Intelligence,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2010}, {"title": "A survey on unsupervised outlier detection in highdimensional numerical data", "author": ["Arthur Zimek", "Erich Schubert", "Hans-Peter Kriegel"], "venue": "Statistical Analysis and Data Mining,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2012}, {"title": "On evaluation of outlier rankings and outlier scores", "author": ["Hans-Peter Kriegel"], "venue": null, "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2012}], "referenceMentions": [{"referenceID": 10, "context": "A disciplined approach is to identify instances lying in low density regions [11].", "startOffset": 77, "endOffset": 81}, {"referenceID": 11, "context": "However, estimating density in high-dimensional and mixed-type settings is difficult [12, 22, 35].", "startOffset": 85, "endOffset": 97}, {"referenceID": 20, "context": "However, estimating density in high-dimensional and mixed-type settings is difficult [12, 22, 35].", "startOffset": 85, "endOffset": 97}, {"referenceID": 33, "context": "However, estimating density in high-dimensional and mixed-type settings is difficult [12, 22, 35].", "startOffset": 85, "endOffset": 97}, {"referenceID": 4, "context": "An alternative is to use distance to k nearest neighbors, assuming that the larger the distance, the less dense the region [5].", "startOffset": 123, "endOffset": 126}, {"referenceID": 0, "context": "But distance is neither well-defined under mixed types nor meaningful in a high-dimensional space [1, 35].", "startOffset": 98, "endOffset": 105}, {"referenceID": 33, "context": "But distance is neither well-defined under mixed types nor meaningful in a high-dimensional space [1, 35].", "startOffset": 98, "endOffset": 105}, {"referenceID": 33, "context": ", see [35] for a recent review) or mixed data (e.", "startOffset": 6, "endOffset": 10}, {"referenceID": 11, "context": ", see [12, 22] for latest attempts).", "startOffset": 6, "endOffset": 14}, {"referenceID": 20, "context": ", see [12, 22] for latest attempts).", "startOffset": 6, "endOffset": 14}, {"referenceID": 25, "context": "To tackle the challenges jointly, we advocate learning data representation through abstraction, a strategy that (a) transforms mixed-data into a homogeneous representation [27], and (b) represents the multilevel structure of data [8].", "startOffset": 172, "endOffset": 176}, {"referenceID": 7, "context": "To tackle the challenges jointly, we advocate learning data representation through abstraction, a strategy that (a) transforms mixed-data into a homogeneous representation [27], and (b) represents the multilevel structure of data [8].", "startOffset": 230, "endOffset": 233}, {"referenceID": 11, "context": "MIXMAD generalizes the recent work in [12] for mixed data by building multiple abstractions of data.", "startOffset": 38, "endOffset": 42}, {"referenceID": 7, "context": "For data abstraction, we leverage recent advances in unsupervised deep learning to abstract the data into multilevel low-dimensional representations [8].", "startOffset": 149, "endOffset": 152}, {"referenceID": 31, "context": "While deep learning has revolutionized supervised prediction [21], its application to unsupervised anomaly detection is very limited [33].", "startOffset": 133, "endOffset": 137}, {"referenceID": 16, "context": "With MIXMAD we build a sequence of Deep Belief Nets (DBNs) [17] of increasing depths.", "startOffset": 59, "endOffset": 63}, {"referenceID": 1, "context": "bears some similarity with the recent ensemble approaches [2, 4], the key difference is MAD relies on multiple data abstractions, not data resampling or random subspaces which are still on the original data level.", "startOffset": 58, "endOffset": 64}, {"referenceID": 3, "context": "bears some similarity with the recent ensemble approaches [2, 4], the key difference is MAD relies on multiple data abstractions, not data resampling or random subspaces which are still on the original data level.", "startOffset": 58, "endOffset": 64}, {"referenceID": 19, "context": "We validate MIXMAD through an extensive set of experiments against well-known shallow baselines, which include the classic methods (PCA, GMM, RBM and one-class SVM), as well as state-of-the-art mixed-type methods (ODMAD [20], BMM [9], GLM-t [22] and Mv.", "startOffset": 220, "endOffset": 224}, {"referenceID": 8, "context": "We validate MIXMAD through an extensive set of experiments against well-known shallow baselines, which include the classic methods (PCA, GMM, RBM and one-class SVM), as well as state-of-the-art mixed-type methods (ODMAD [20], BMM [9], GLM-t [22] and Mv.", "startOffset": 230, "endOffset": 233}, {"referenceID": 20, "context": "We validate MIXMAD through an extensive set of experiments against well-known shallow baselines, which include the classic methods (PCA, GMM, RBM and one-class SVM), as well as state-of-the-art mixed-type methods (ODMAD [20], BMM [9], GLM-t [22] and Mv.", "startOffset": 241, "endOffset": 245}, {"referenceID": 11, "context": "RBM [12]).", "startOffset": 4, "endOffset": 8}, {"referenceID": 0, "context": "High-dimensional data suffers from \u2018curse of dimensionality\u2019 also known more concretely as \u2018distance concentration effect\u2019, irrelevant attributes and redundant attributes, which together cause failure of low-dimensional techniques [1].", "startOffset": 231, "endOffset": 234}, {"referenceID": 33, "context": "Popular anomaly detection approaches targeting high-dimensions include feature selection, dimensionality reduction (such as using PCA) and subspace analysis (readers are referred to [35] for a recent survey and in-depth discussion).", "startOffset": 182, "endOffset": 186}, {"referenceID": 14, "context": "A method called LOADED [15] defines the score on the discrete subspace and combines with a correlation matrix in the continuous subspace.", "startOffset": 23, "endOffset": 27}, {"referenceID": 19, "context": "A related method called ODMAD [20] opts for stage-wise detection in each subspace.", "startOffset": 30, "endOffset": 34}, {"referenceID": 8, "context": "A different strategy is employed in [9], where scores for discrete and continuous spaces are computed separately, then combined using a mixture model.", "startOffset": 36, "endOffset": 39}, {"referenceID": 32, "context": "The work in [34] introduces Pattern-based Anomaly Detection (POD), where a pattern consists of a discrete attribute and all continuous attributes.", "startOffset": 12, "endOffset": 16}, {"referenceID": 11, "context": "Methods with joint distribution of all attributes are introduced recently in [12, 22] using latent variables to link all types together.", "startOffset": 77, "endOffset": 85}, {"referenceID": 20, "context": "Methods with joint distribution of all attributes are introduced recently in [12, 22] using latent variables to link all types together.", "startOffset": 77, "endOffset": 85}, {"referenceID": 11, "context": "We adapt the work in [12] to represent mixed data into a homogeneous form using Mixed-variate Restricted Boltzmann Machines [27].", "startOffset": 21, "endOffset": 25}, {"referenceID": 25, "context": "We adapt the work in [12] to represent mixed data into a homogeneous form using Mixed-variate Restricted Boltzmann Machines [27].", "startOffset": 124, "endOffset": 128}, {"referenceID": 6, "context": "The recent advances of deep networks have inspired some work in anomaly detection [7, 14, 25, 26, 32].", "startOffset": 82, "endOffset": 101}, {"referenceID": 13, "context": "The recent advances of deep networks have inspired some work in anomaly detection [7, 14, 25, 26, 32].", "startOffset": 82, "endOffset": 101}, {"referenceID": 23, "context": "The recent advances of deep networks have inspired some work in anomaly detection [7, 14, 25, 26, 32].", "startOffset": 82, "endOffset": 101}, {"referenceID": 24, "context": "The recent advances of deep networks have inspired some work in anomaly detection [7, 14, 25, 26, 32].", "startOffset": 82, "endOffset": 101}, {"referenceID": 30, "context": "The recent advances of deep networks have inspired some work in anomaly detection [7, 14, 25, 26, 32].", "startOffset": 82, "endOffset": 101}, {"referenceID": 30, "context": "A common strategy is to use unsupervised deep networks to detect features, which are then fed into well-established detection algorithms [32].", "startOffset": 137, "endOffset": 141}, {"referenceID": 6, "context": "Another strategy is to learn a deep autoencoder which maps data to itself, and then use the reconstruction error as anomaly score [7, 14, 25, 26].", "startOffset": 130, "endOffset": 145}, {"referenceID": 13, "context": "Another strategy is to learn a deep autoencoder which maps data to itself, and then use the reconstruction error as anomaly score [7, 14, 25, 26].", "startOffset": 130, "endOffset": 145}, {"referenceID": 23, "context": "Another strategy is to learn a deep autoencoder which maps data to itself, and then use the reconstruction error as anomaly score [7, 14, 25, 26].", "startOffset": 130, "endOffset": 145}, {"referenceID": 24, "context": "Another strategy is to learn a deep autoencoder which maps data to itself, and then use the reconstruction error as anomaly score [7, 14, 25, 26].", "startOffset": 130, "endOffset": 145}, {"referenceID": 18, "context": "A more fundamental problem is that reconstruction error does not reflect data density [19].", "startOffset": 86, "endOffset": 90}, {"referenceID": 11, "context": "A better approach is to use deep networks to estimate the energy directly [12, 13, 33].", "startOffset": 74, "endOffset": 86}, {"referenceID": 12, "context": "A better approach is to use deep networks to estimate the energy directly [12, 13, 33].", "startOffset": 74, "endOffset": 86}, {"referenceID": 31, "context": "A better approach is to use deep networks to estimate the energy directly [12, 13, 33].", "startOffset": 74, "endOffset": 86}, {"referenceID": 25, "context": "RBM) [27] for modelling mixed data.", "startOffset": 5, "endOffset": 9}, {"referenceID": 11, "context": "RBM can be used for outlier detection [12] by noticing that:", "startOffset": 38, "endOffset": 42}, {"referenceID": 15, "context": "i=1 Gik(xi) )) For training, we adopt the standard CD-1 procedure [16].", "startOffset": 66, "endOffset": 70}, {"referenceID": 16, "context": "Deep Belief Network (DBN) [17] is a generative model of data.", "startOffset": 26, "endOffset": 30}, {"referenceID": 17, "context": "This procedure of freezing the lower weights has been shown to optimize the variational bound of the data likelihood logP (x) [18].", "startOffset": 126, "endOffset": 130}, {"referenceID": 30, "context": ", [32]); and (b) build a deep autoencoder then estimate the reconstruction error [7, 14, 25, 26].", "startOffset": 2, "endOffset": 6}, {"referenceID": 6, "context": ", [32]); and (b) build a deep autoencoder then estimate the reconstruction error [7, 14, 25, 26].", "startOffset": 81, "endOffset": 96}, {"referenceID": 13, "context": ", [32]); and (b) build a deep autoencoder then estimate the reconstruction error [7, 14, 25, 26].", "startOffset": 81, "endOffset": 96}, {"referenceID": 23, "context": ", [32]); and (b) build a deep autoencoder then estimate the reconstruction error [7, 14, 25, 26].", "startOffset": 81, "endOffset": 96}, {"referenceID": 24, "context": ", [32]); and (b) build a deep autoencoder then estimate the reconstruction error [7, 14, 25, 26].", "startOffset": 81, "endOffset": 96}, {"referenceID": 2, "context": "One approach to rank aggregation is to find a ranking that minimizes the disagreement with all ranks [3].", "startOffset": 101, "endOffset": 104}, {"referenceID": 7, "context": "Note that data abstraction and anomaly detection have different goals \u2013 abstraction typically requires more bits to adequately disentangle multiple factors of variation [8], whereas detection may require less bits to estimate a rank score.", "startOffset": 169, "endOffset": 172}, {"referenceID": 9, "context": "\u2022 The second dataset is InternetAds with 5% anomaly injection as described in [10].", "startOffset": 78, "endOffset": 82}, {"referenceID": 26, "context": "\u2022 The third dataset consists of birth episodes collected from an urban hospital in Sydney, Australia in the period of 2011\u20132015 [28].", "startOffset": 128, "endOffset": 132}, {"referenceID": 29, "context": "Preterm births are considered anomalous as they have a serious impact on the survival and development of the babies [31].", "startOffset": 116, "endOffset": 120}, {"referenceID": 5, "context": "In general, births occurring within 37 weeks of gestation are considered preterm [6].", "startOffset": 81, "endOffset": 84}, {"referenceID": 0, "context": "All data are normalized into the range [0,1], which is known to work best in [10].", "startOffset": 39, "endOffset": 44}, {"referenceID": 9, "context": "All data are normalized into the range [0,1], which is known to work best in [10].", "startOffset": 77, "endOffset": 81}, {"referenceID": 10, "context": "mixture model (GMM), and one-class SVM (OCSVM) [11].", "startOffset": 47, "endOffset": 51}, {"referenceID": 4, "context": "(a) The k-NN uses the mean distance from a test case to the k nearest instances as outlier score [5].", "startOffset": 97, "endOffset": 100}, {"referenceID": 12, "context": "We also consider RBM [13] as baseline, which is a special case of our method where the number of layers is set to L = 1.", "startOffset": 21, "endOffset": 25}, {"referenceID": 15, "context": "All RBMs are trained using CD-1 [16] with batch size of 64, learning rate of 0.", "startOffset": 32, "endOffset": 36}, {"referenceID": 11, "context": "We use data from [12] where the data statistics are reported in Table 5.", "startOffset": 17, "endOffset": 21}, {"referenceID": 8, "context": "To keep consistent with previous work [9, 12, 20, 22], we report the results using the F-scores.", "startOffset": 38, "endOffset": 53}, {"referenceID": 11, "context": "To keep consistent with previous work [9, 12, 20, 22], we report the results using the F-scores.", "startOffset": 38, "endOffset": 53}, {"referenceID": 19, "context": "To keep consistent with previous work [9, 12, 20, 22], we report the results using the F-scores.", "startOffset": 38, "endOffset": 53}, {"referenceID": 20, "context": "To keep consistent with previous work [9, 12, 20, 22], we report the results using the F-scores.", "startOffset": 38, "endOffset": 53}, {"referenceID": 16, "context": "This pattern is indeed not new as it resembles what can be seen across the literature of DBNs for classification tasks [17, 24].", "startOffset": 119, "endOffset": 127}, {"referenceID": 22, "context": "This pattern is indeed not new as it resembles what can be seen across the literature of DBNs for classification tasks [17, 24].", "startOffset": 119, "endOffset": 127}, {"referenceID": 16, "context": ", see [17]).", "startOffset": 6, "endOffset": 10}, {"referenceID": 21, "context": "There has been an unexpected connection between the construction procedure of DBNs and the variational renomarlization groups in physics [23].", "startOffset": 137, "endOffset": 141}, {"referenceID": 27, "context": "The upper RBMs then integrate all information into coherent representations [29].", "startOffset": 76, "endOffset": 80}, {"referenceID": 8, "context": "BMM [9, 12] \u2013 0.", "startOffset": 4, "endOffset": 11}, {"referenceID": 11, "context": "BMM [9, 12] \u2013 0.", "startOffset": 4, "endOffset": 11}, {"referenceID": 11, "context": "67 ODMAD [12, 20] \u2013 0.", "startOffset": 9, "endOffset": 17}, {"referenceID": 19, "context": "67 ODMAD [12, 20] \u2013 0.", "startOffset": 9, "endOffset": 17}, {"referenceID": 11, "context": "52 GLM-t [12, 22] \u2013 \u2013 \u2013 0.", "startOffset": 9, "endOffset": 17}, {"referenceID": 20, "context": "52 GLM-t [12, 22] \u2013 \u2013 \u2013 0.", "startOffset": 9, "endOffset": 17}, {"referenceID": 11, "context": "RBM [12] 0.", "startOffset": 4, "endOffset": 8}, {"referenceID": 28, "context": "DBNs have demonstrated its usefulness in abstraction, but there exist other possibilities [30, 33].", "startOffset": 90, "endOffset": 98}, {"referenceID": 31, "context": "DBNs have demonstrated its usefulness in abstraction, but there exist other possibilities [30, 33].", "startOffset": 90, "endOffset": 98}, {"referenceID": 34, "context": "Finally, the simple p-norm rank aggregation can be replaced by a more sophisticated method for selecting and building right abstraction levels [36].", "startOffset": 143, "endOffset": 147}], "year": 2016, "abstractText": "Anomalies are those deviating from the norm. Unsupervised anomaly detection often translates to identifying low density regions. Major problems arise when data is high-dimensional and mixed of discrete and continuous attributes. We propose MIXMAD, which stands for MIXed data Multilevel Anomaly Detection, an ensemble method that estimates the sparse regions across multiple levels of abstraction of mixed data. The hypothesis is for domains where multiple data abstractions exist, a data point may be anomalous with respect to the raw representation or more abstract representations. To this end, our method sequentially constructs an ensemble of Deep Belief Nets (DBNs) with varying depths. Each DBN is an energy-based detector at a predefined abstraction level. At the bottom level of each DBN, there is a Mixed-variate Restricted Boltzmann Machine that models the density of mixed data. Predictions across the ensemble are finally combined via rank aggregation. The proposed MIXMAD is evaluated on high-dimensional real-world datasets of different characteristics. The results demonstrate that for anomaly detection, (a) multilevel abstraction of highdimensional and mixed data is a sensible strategy, and (b) empirically, MIXMAD is superior to popular unsupervised detection methods for both homogeneous and mixed data.", "creator": "LaTeX with hyperref package"}}}