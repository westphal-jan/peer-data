{"id": "1610.06251", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Oct-2016", "title": "DeepGraph: Graph Structure Predicts Network Growth", "abstract": "The topological (or graph) structures of real-world networks are known to be predictive of multiple dynamic properties of the networks. Conventionally, a graph structure is represented using an adjacency matrix or a set of hand-crafted structural features. These representations either fail to highlight local and global properties of the graph or suffer from a severe loss of structural information. There lacks an effective graph representation, which hinges the realization of the predictive power of network structures.", "histories": [["v1", "Thu, 20 Oct 2016 00:16:05 GMT  (2925kb,D)", "http://arxiv.org/abs/1610.06251v1", null]], "reviews": [], "SUBJECTS": "cs.SI cs.LG", "authors": ["cheng li", "xiaoxiao guo", "qiaozhu mei"], "accepted": false, "id": "1610.06251"}, "pdf": {"name": "1610.06251.pdf", "metadata": {"source": "CRF", "title": "DeepGraph: Graph Structure Predicts Network Growth", "authors": ["Cheng Li", "Xiaoxiao Guo", "Qiaozhu Mei"], "emails": ["qmei}@umich.edu"], "sections": [{"heading": null, "text": "In this study, we propose to learn the representation of a graph or the topological structure of a network through a deep learning model.This end-to-end prediction model, called DeepGraph, takes the input of the raw adjacence matrix of a real network and predicts the growth of the network.The adjacence matrix is first presented using a graph descriptor based on the thermal nuclear signature, which is then transmitted by a multicolumn, multi-resolution, revolutionary neural network.Extensive experiments on five large collections of real networks show that the proposed prediction model significantly improves the effectiveness of existing methods, including linear or nonlinear regressors that use handmade features, graph cores, and competing deep learning methods.Categories and Subject Descriptors H.4 [Information Systems Applications] GroouseMissionTerms, Graphic Algorithms, Develing,"}, {"heading": "1. INTRODUCTION", "text": "This year, it is only a matter of time before an agreement is reached."}, {"heading": "2. RELATED WORK", "text": "In fact, most of them will be able to play by the rules they have imposed on themselves."}, {"heading": "3. DEEPGRAPH FOR NETWORK GROWTH PREDICTION", "text": "The proposed predictive model, called DeepGraph, combines thermal nuclear signature and deep neural networks. In the following, we describe the two key components of our model, (1) a thermal nuclear-based graphene descriptor and (2) a deep, multi-column, multi-resolution Convolutionary Neural Network, which in turn follows a brief definition of the problem of predicting network growth."}, {"heading": "3.1 Problem Formulation and Notations", "text": "Given a snapshot of a network in the real world at the moment t, we call its graph structure G > > (t) = (V, E), with a series of nodes V and a series of edges E. A node i \u2032 V represents a unit (e.g. an actor in a social network or a paper in a citation network), an edge (i, j).E represents a relationship (e.g. friendship, citation or influence) between node i and node j. An adoption matrix W \u00b2 R | V | V | encodes the topological structure of graph G. In this thesis, we look at the binary adoption matrix (e.g., citations or influences) between node i and node j. An adoption matrix W \u00b2 E and node j. An adoption matrix W \u00b2 E and 0 otherwise.A network property is a function that converts a graph structure G (t) to a property y (t).K \u00b2) The network property could be the number of friends the pre-created Facebook."}, {"heading": "3.2 Heat Kernel Signature based Graph Descriptors", "text": "The motivation in the adoption of heat kernel signature (HKS) is its theoretical proven property in the representation of heat networks: HKS is an intrinsic and informative representation for graphs, which in practice means a special problem of network signature with the same interval characteristic and informativity, if two graphs have the same HKS representation, then they must be isomorphic graphs. Our2In practice, the researchers focus on a special case of network signature with the same interval characteristic, t \"j\" j \u2212 tj \"i\" ti \"C > 0 [20, 35].HKS-based graphhiptor builds on the theoretical properties of HKS and provides further universal representations for graphics with different sizes in the network growth forecast."}, {"heading": "3.3 Deep Graph Descriptor", "text": "In fact, it is as if it were a matter of the kind that has taken place in the United States in recent years. (...) In fact, it is as if most people in the United States are able to move to the United States. (...) It is as if they were going to the United States. (...) It is as if they were going to the United States. (...) It is as if they were going to the United States. (...) It is as if they were going to go to the United States. (...) It is as if they were going to go to the United States. (...) It is as if they were coming to the United States. (...) It is as if they were going to go to the United States. (...) It is as if they wanted to live in the United States. (...) It is as if they wanted to live in the United States. (...) It is as if they wanted to live in the United States. (...) It is as if they wanted to live in the United States."}, {"heading": "3.4 End-to-End Training", "text": "Let McMrConv (., \u03b8) designate the multi-column multi-resolution Convolutionary Neural Network with the parameters \u03b8. The final output of our neural network with a graph Gk is represented as follows: y-k = McMrConv (S (Gk), \u03b8) (7) In the face of a training dataset {(Gk, yk)} Kk = 1, the deep neural network is trained to minimize the average square error. L (\u03b8) = 1 K-K-k = 1 (McMrConv (S (Gk), \u03b8) \u2212 yk) 2 (8) The HKS-based graph descriptor and the deep neural network build DeepGraph, an end-to-end deep architecture for predicting network growth based on the graph structure."}, {"heading": "4. EXPERIMENT SETUP", "text": "We compare our model with existing approaches, including property-based linear or nonlinear regression, graph cores, and alternative deep learning approaches to network growth prediction. We then evaluate variants of our model to give credit to the two key components, the HKS-based graph descriptor and the Deep Neural Network."}, {"heading": "4.1 Data sets", "text": "The five datasets we select include social networks, scientific collaboration networks, information diffusion networks, and entertainment networks. So keep in mind that in any large network where we extract and test the first person networks, it is desirable to have a large number of time-variable networks that are not directly available (e.g., there is a global Facebook network).The statistics of these datasets are presented in Table 1. Please note that due to the diversity of datasets and the varying precision of the available timestamps, it is difficult to apply a uniform timeframe to all datasets. From another perspective, this helps us evaluate the flexibility and generalization of our methods to see if they can be applied to any length and generalization of time."}, {"heading": "4.2 Evaluation Metric", "text": "We use the mean quadratic error (MSE) as a measure of evaluation, which is a frequent choice for regression tasks. Specifically, we refer to the y value as a predictive value and the y value as a basic truth value. The MSE is: MSE = 1n n \u2211 i = 1 (y-i \u2212 yi) 2 (9) As already mentioned, y in the above equation is a scaled version of the original value yo, i.e. y = log2 (y o + 1)."}, {"heading": "4.3 Baseline methods", "text": "We compare DeepGraph with methods derived from two categories: the methods used for network prediction and alternative graph methods. Many structural features have been designed for different network predictions, so most of them are not able to generalize themselves, even though the number of nodes (k = 1) can be generalized. (e.g. the number of closed and open triangles) Other network properties: the length of the shortest path, the density of the nodes (k = 1), the number of edges (k = 2), the number of triads (e.g. the number of triads), the number of triads (n) and the number of commonalities (n).), the number of empty nodes (the number of nodes (n), the number of spaces (n), the number of triads (n), the number of triads (s) and (n)."}, {"heading": "4.5 Variants of DeepGraph", "text": "To give credit to the individual key components of our DeepGraph model, we are also experimenting with some of its variants by feeding our graph descriptor (GD) to a linear regressor (GD-linear), a standard Convolutionary Neural Network (GD-CNN), and a multi-layer perceptron (GD-MLP)."}, {"heading": "5. EXPERIMENT RESULTS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 Overall performance", "text": "We make the following observations. First, the integration of graph descriptors with deep learning, our DeepGraph method significantly outperforms all competing methods, empirically confirming that graph descriptors could retain more information about the network structure than bag-of-substructures, both globally and locally. In contrast, the use of manually designed features can lead to information loss. This shows that we can actually extract more useful characteristics by applying column-by-column and line-by-line convolution over graph descriptors, while DeepGraph can further improve performance by using the semantics of HKS-based graph descriptors. This shows that we can actually extract more useful characteristics by applying column-by-row conversion over graph descriptors. Comparison with GD-linearics, which apply linear bit descriptions at the level of HKS-based descriptors, is very low."}, {"heading": "5.2 Computational Cost of DeepGraph", "text": "The main effort of DeepGraph is the calculation of the HKS-based graph descriptors. We measure empirically the computation time for all data sets on a server with 2.40 GHz CPU and 120G RAM. The graphs in our data sets have a size of up to 5,000 nodes and 200,000 edges, which is sufficient for most network prediction problems [20, 27, 42]. Generating the graph descriptors takes an average of 0.86 hours per data set. In contrast, the strongest baseline, a function-based method, takes an average of 7.9 hours to generate all features, while the strongest graph core, SP, takes almost 5 days."}, {"heading": "5.3 Feature Analysis", "text": "It is fascinating to know whether these learned features fit with the known structural patterns in the network literature, which are characterized by either global or local aspects of the networks, and are able to capture the last hidden layers of the DeepGraph."}, {"heading": "5.4 Error Analysis", "text": "As a compromise, we characterize graphs by a number of simple network properties, such as the number of nodes, edges, and edge densities.First, we want to examine graphs where DeepGraph makes more errors than the baseline, and vice versa. Here, we use the strongest base method, an identification-based method as a reference. The procedure is as follows: Among graphs where DeepGraph has a smaller MSE than the baseline, we select the top 100 with the largest MSE differences between the two methods. For these top graphs, we calculate the average of the above-mentioned properties. Similar procedures are also applied to the basics.The statistics of graphs where either DeepGraph or the baseline is significantly better than the others are higher than the average statistics of the individual data sets. This could result in the distorted distribution of the graphs - a large number of graphs that are larger than the smaller graphs, but are also better performed on the baseline."}, {"heading": "6. CONCLUSION", "text": "We present a novel neural network model that predicts the growth of network characteristics based on its graph structure. This model, DeepGraph, calculates a new representation of the graph structure based on thermal nuclear signatures. A multi-column, multi-resolution neural folding network is designed to further learn the high-level representations and predict network growth in an end-to-end manner. Experiments on large collections of real networks show that DeepGraph methods based on handmade features, graph cores and competing deep learning methods.The superordinate representations learned from DeepGraph correlate well with results and theories in the literature of social networks and show that a deep learning model can automatically detect meaningful and predictable structural patterns in networks.Our study calms the predictive power of network structures and suggests a way to effectively utilize this performance in social networking and show that a deep learning model can automatically detect meaningful and predictable structural patterns in networks."}, {"heading": "7. REFERENCES", "text": "[1] L. Backstrom, D. Huttenlocher, J. Kleinberg, and X. Lan.Group formation in large social networks: membership, growth, and evolution. In Proc. of SIGKDD, 2006. [2] L. Bai, L. Rossi, A. Torsello, and E. R. Hancock. A quantum jensen-shannon graph kernel for unattributed graphs. [3] X. Bai and E. R. Hancock. Heat kernels, manifolds and graph embedding. In Structural, Syntactic, and Statistical Pattern Recognition. Springer, 2004. [4] V. D. Blondel, J.-L. Guillaume, R. Lambiotte, and E. Lefebvre. Fast unfolding of communities in large networks. JSTAT, 2008. [5] K. M. Borgwardt and H.-P. Kriegel. Shortest path kernels on graphs. In Proc. of ICDM, 2005."}], "references": [{"title": "Group formation in large social networks: membership, growth, and evolution", "author": ["L. Backstrom", "D. Huttenlocher", "J. Kleinberg", "X. Lan"], "venue": "In Proc. of SIGKDD,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2006}, {"title": "A quantum jensen\u2013shannon graph kernel for unattributed graphs", "author": ["L. Bai", "L. Rossi", "A. Torsello", "E.R. Hancock"], "venue": "Pattern Recognition,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2015}, {"title": "Heat kernels, manifolds and graph embedding", "author": ["X. Bai", "E.R. Hancock"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2004}, {"title": "Fast unfolding of communities in large networks", "author": ["V.D. Blondel", "J.-L. Guillaume", "R. Lambiotte", "E. Lefebvre"], "venue": "JSTAT,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2008}, {"title": "Shortest-path kernels on graphs", "author": ["K.M. Borgwardt", "H.-P. Kriegel"], "venue": "In Proc. of ICDM,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2005}, {"title": "The network structure of social capital", "author": ["R.S. Burt"], "venue": "Research in organizational behavior,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2000}, {"title": "Does team competition increase pro-social lending? evidence from online microfinance", "author": ["R. Chen", "Y. Chen", "Y. Liu", "Q. Mei"], "venue": "Games and Economic Behavior,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2015}, {"title": "Can cascades be predicted", "author": ["J. Cheng", "L. Adamic", "P.A. Dow", "J.M. Kleinberg", "J. Leskovec"], "venue": "In Proc. of WWW,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2014}, {"title": "Laplacians and the cheeger inequality for directed graphs", "author": ["F. Chung"], "venue": "Annals of Combinatorics,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2005}, {"title": "A Bibliometric and Network Analysis of the field of Computational Linguistics", "author": ["B.G.P.M. Dragomir R. Radev", "Mark Thomas Joseph"], "venue": "JASIST,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2009}, {"title": "Networks, crowds, and markets: Reasoning about a highly connected world", "author": ["D. Easley", "J. Kleinberg"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2010}, {"title": "Heat-passing framework for robust interpretation of data in networks", "author": ["Y. Fang", "M. Sun", "K. Ramani"], "venue": "PloS one,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2015}, {"title": "3d deep shape descriptor", "author": ["Y. Fang", "J. Xie", "G. Dai", "M. Wang", "F. Zhu", "T. Xu", "E. Wong"], "venue": "In Proc. of CVPR,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2015}, {"title": "On graph kernels: Hardness results and efficient alternatives", "author": ["T. G\u00e4rtner", "P. Flach", "S. Wrobel"], "venue": "In Learning Theory and Kernel Machines", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2003}, {"title": "node2vec: Scalable feature learning for networks", "author": ["A. Grover", "J. Leskovec"], "venue": "In Proc. of SIGKDD,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2016}, {"title": "Team assembly mechanisms determine collaboration network structure and team", "author": ["R. Guimera", "B. Uzzi", "J. Spiro", "L.A.N. Amaral"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2005}, {"title": "Kernels for graphs", "author": ["H. Kashima", "K. Tsuda", "A. Inokuchi"], "venue": "Kernel methods in computational biology,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2004}, {"title": "Adam: A method for stochastic optimization", "author": ["D. Kingma", "J. Ba"], "venue": "Proc. of ICLR,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2015}, {"title": "Empirical analysis of an evolving social", "author": ["G. Kossinets", "D.J. Watts"], "venue": "network. science,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2006}, {"title": "Prediction of retweet cascade size over time", "author": ["A. Kupavskii", "L. Ostroumova", "A. Umnov", "S. Usachev", "P. Serdyukov", "G. Gusev", "A. Kustarev"], "venue": "In Proc. of CIKM,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2012}, {"title": "Mining social networks using heat diffusion processes for marketing candidates selection", "author": ["H. Ma", "H. Yang", "M.R. Lyu", "I. King"], "venue": "In Proc. of CIKM,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2008}, {"title": "Online Social Networks: Measurement, Analysis, and Applications to Distributed Information Systems", "author": ["A. Mislove"], "venue": "PhD thesis,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2009}, {"title": "subgraph2vec: Learning distributed representations of rooted sub-graphs from large graphs", "author": ["A. Narayanan", "M. Chandramohan", "L. Chen", "Y. Liu", "S. Saminathan"], "venue": "Workshop on Mining and Learning with Graphs,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2016}, {"title": "Learning convolutional neural networks for graphs. 2016", "author": ["M. Niepert", "M. Ahmed", "K. Kutzkov"], "venue": null, "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2016}, {"title": "Deepwalk: Online learning of social representations", "author": ["B. Perozzi", "R. Al-Rfou", "S. Skiena"], "venue": "In Proc. of SIGKDD,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2014}, {"title": "Graphsig: A scalable approach to mining significant subgraphs in large graph databases", "author": ["S. Ranu", "A.K. Singh"], "venue": "In Proc. of ICDE,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2009}, {"title": "On the interplay between social and topical structure", "author": ["D.M. Romero", "C. Tan", "J. Ugander"], "venue": "Proc. of ICWSM,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2013}, {"title": "gboost: a mathematical programming approach to graph classification and regression", "author": ["H. Saigo", "S. Nowozin", "T. Kadowaki", "T. Kudo", "K. Tsuda"], "venue": "Machine Learning,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2009}, {"title": "Efficient graphlet kernels for large graph comparison", "author": ["N. Shervashidze", "T. Petri", "K. Mehlhorn", "K.M. Borgwardt", "S. Vishwanathan"], "venue": "In International conference on artificial intelligence and statistics,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2009}, {"title": "Weisfeiler-lehman graph kernels", "author": ["N. Shervashidze", "P. Schweitzer", "E.J. Van Leeuwen", "K. Mehlhorn", "K.M. Borgwardt"], "venue": null, "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2011}, {"title": "A concise and provably informative multi-scale signature based on heat diffusion", "author": ["J. Sun", "M. Ovsjanikov", "L. Guibas"], "venue": "In Computer graphics forum,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2009}, {"title": "Pathsim: Meta path-based top-k similarity search in heterogeneous information networks", "author": ["Y. Sun", "J. Han", "X. Yan", "P.S. Yu", "T. Wu"], "venue": "In Proc. of VLDB,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2011}, {"title": "Predicting the popularity of online content", "author": ["G. Szabo", "B.A. Huberman"], "venue": "Communications of the ACM,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2010}, {"title": "Pte: Predictive text embedding through large-scale heterogeneous text networks", "author": ["J. Tang", "M. Qu", "Q. Mei"], "venue": "In Proc. of SIGKDD,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2015}, {"title": "What\u2019s in a hashtag?: content based prediction of the spread of ideas in microblogging communities", "author": ["O. Tsur", "A. Rappoport"], "venue": "In Proc. of WSDM,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2012}, {"title": "Subgraph frequencies: Mapping the empirical and extremal geography of large graph collections", "author": ["J. Ugander", "L. Backstrom", "J. Kleinberg"], "venue": "In Proc. of WWW,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2013}, {"title": "Structural diversity in social contagion", "author": ["J. Ugander", "L. Backstrom", "C. Marlow", "J. Kleinberg"], "venue": "Proc. of the National Academy of Sciences,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2012}, {"title": "On the evolution of user interaction in facebook", "author": ["B. Viswanath", "A. Mislove", "M. Cha", "K.P. Gummadi"], "venue": "In Proc. of WOSN,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2009}, {"title": "Deepshape: Deep learned shape descriptor for 3d shape matching and retrieval", "author": ["J. Xie", "Y. Fang", "F. Zhu", "E. Wong"], "venue": "In Proc. of CVPR,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2015}, {"title": "Deep graph kernels", "author": ["P. Yanardag", "S. Vishwanathan"], "venue": "In Proc. of SIGKDD,", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2015}, {"title": "A structural smoothing framework for robust graph comparison", "author": ["P. Yanardag", "S. Vishwanathan"], "venue": "In NIPS,", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2015}, {"title": "Modeling information diffusion in implicit networks", "author": ["J. Yang", "J. Leskovec"], "venue": "In Proc. of ICDM,", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2010}, {"title": "We know what@ you# tag: does the dual role affect hashtag adoption", "author": ["L. Yang", "T. Sun", "M. Zhang", "Q. Mei"], "venue": "In Proc. of WWW,", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2012}, {"title": "Social influence locality for modeling retweeting behaviors", "author": ["J. Zhang", "B. Liu", "J. Tang", "T. Chen", "J. Li"], "venue": "In IJCAI,", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2013}], "referenceMentions": [{"referenceID": 32, "context": "How to model and predict the dynamic properties of social or information networks has received considerable attentions recently [33, 42, 1, 27, 20, 35, 8].", "startOffset": 128, "endOffset": 154}, {"referenceID": 41, "context": "How to model and predict the dynamic properties of social or information networks has received considerable attentions recently [33, 42, 1, 27, 20, 35, 8].", "startOffset": 128, "endOffset": 154}, {"referenceID": 0, "context": "How to model and predict the dynamic properties of social or information networks has received considerable attentions recently [33, 42, 1, 27, 20, 35, 8].", "startOffset": 128, "endOffset": 154}, {"referenceID": 26, "context": "How to model and predict the dynamic properties of social or information networks has received considerable attentions recently [33, 42, 1, 27, 20, 35, 8].", "startOffset": 128, "endOffset": 154}, {"referenceID": 19, "context": "How to model and predict the dynamic properties of social or information networks has received considerable attentions recently [33, 42, 1, 27, 20, 35, 8].", "startOffset": 128, "endOffset": 154}, {"referenceID": 34, "context": "How to model and predict the dynamic properties of social or information networks has received considerable attentions recently [33, 42, 1, 27, 20, 35, 8].", "startOffset": 128, "endOffset": 154}, {"referenceID": 7, "context": "How to model and predict the dynamic properties of social or information networks has received considerable attentions recently [33, 42, 1, 27, 20, 35, 8].", "startOffset": 128, "endOffset": 154}, {"referenceID": 0, "context": "A function is learned that takes these features as input and outputs a predicted value of the network property in the future [1].", "startOffset": 125, "endOffset": 128}, {"referenceID": 42, "context": "For example, the content of a hashtag is predictive to its diffusion [43] and homophily (e.", "startOffset": 69, "endOffset": 73}, {"referenceID": 6, "context": ", similar demongraphics) is predictive to the growth of social groups [7], but these effects are not generalizable to other networks and other dynamic properties.", "startOffset": 70, "endOffset": 73}, {"referenceID": 10, "context": "For example, open triads with two strong ties are likely to be closed in the near future [11]; dense communities are resistant to novel information and they grow slower than others [16]; nodes spanning structural holes are likely to gain social capital and experience a rapid growth of its prestige and other properties [6].", "startOffset": 89, "endOffset": 93}, {"referenceID": 15, "context": "For example, open triads with two strong ties are likely to be closed in the near future [11]; dense communities are resistant to novel information and they grow slower than others [16]; nodes spanning structural holes are likely to gain social capital and experience a rapid growth of its prestige and other properties [6].", "startOffset": 181, "endOffset": 185}, {"referenceID": 5, "context": "For example, open triads with two strong ties are likely to be closed in the near future [11]; dense communities are resistant to novel information and they grow slower than others [16]; nodes spanning structural holes are likely to gain social capital and experience a rapid growth of its prestige and other properties [6].", "startOffset": 320, "endOffset": 323}, {"referenceID": 30, "context": "We introduce a graph descriptor that is based on the Heat Kernel Signature (HKS) [31], which serves as a universal low-level representation of the topological structures of networks.", "startOffset": 81, "endOffset": 85}, {"referenceID": 12, "context": "HKS has been successfully employed in representing the surface of 3D objects [13, 39].", "startOffset": 77, "endOffset": 85}, {"referenceID": 38, "context": "HKS has been successfully employed in representing the surface of 3D objects [13, 39].", "startOffset": 77, "endOffset": 85}, {"referenceID": 12, "context": "Using a histogram to describe the probability distribution of heat values at a series of time points [13, 39], isomorphic networks (networks with the same topological structure) can be mapped to a unique representation at little loss of structural information.", "startOffset": 101, "endOffset": 109}, {"referenceID": 38, "context": "Using a histogram to describe the probability distribution of heat values at a series of time points [13, 39], isomorphic networks (networks with the same topological structure) can be mapped to a unique representation at little loss of structural information.", "startOffset": 101, "endOffset": 109}, {"referenceID": 32, "context": ", the number of up-votes on Digg stories [33], the number of newly infected nodes in diffusion [42], the growth of a community [1, 27], or the dynamics of a cascade [20, 35, 8].", "startOffset": 41, "endOffset": 45}, {"referenceID": 41, "context": ", the number of up-votes on Digg stories [33], the number of newly infected nodes in diffusion [42], the growth of a community [1, 27], or the dynamics of a cascade [20, 35, 8].", "startOffset": 95, "endOffset": 99}, {"referenceID": 0, "context": ", the number of up-votes on Digg stories [33], the number of newly infected nodes in diffusion [42], the growth of a community [1, 27], or the dynamics of a cascade [20, 35, 8].", "startOffset": 127, "endOffset": 134}, {"referenceID": 26, "context": ", the number of up-votes on Digg stories [33], the number of newly infected nodes in diffusion [42], the growth of a community [1, 27], or the dynamics of a cascade [20, 35, 8].", "startOffset": 127, "endOffset": 134}, {"referenceID": 19, "context": ", the number of up-votes on Digg stories [33], the number of newly infected nodes in diffusion [42], the growth of a community [1, 27], or the dynamics of a cascade [20, 35, 8].", "startOffset": 165, "endOffset": 176}, {"referenceID": 34, "context": ", the number of up-votes on Digg stories [33], the number of newly infected nodes in diffusion [42], the growth of a community [1, 27], or the dynamics of a cascade [20, 35, 8].", "startOffset": 165, "endOffset": 176}, {"referenceID": 7, "context": ", the number of up-votes on Digg stories [33], the number of newly infected nodes in diffusion [42], the growth of a community [1, 27], or the dynamics of a cascade [20, 35, 8].", "startOffset": 165, "endOffset": 176}, {"referenceID": 18, "context": ", triads [19], quads [37], or metapaths [32]).", "startOffset": 9, "endOffset": 13}, {"referenceID": 36, "context": ", triads [19], quads [37], or metapaths [32]).", "startOffset": 21, "endOffset": 25}, {"referenceID": 31, "context": ", triads [19], quads [37], or metapaths [32]).", "startOffset": 40, "endOffset": 44}, {"referenceID": 16, "context": "In graph classification, a myriad of graph kernel methods are proposed which compute pairwise similarities between graphs [17, 2, 30, 29].", "startOffset": 122, "endOffset": 137}, {"referenceID": 1, "context": "In graph classification, a myriad of graph kernel methods are proposed which compute pairwise similarities between graphs [17, 2, 30, 29].", "startOffset": 122, "endOffset": 137}, {"referenceID": 29, "context": "In graph classification, a myriad of graph kernel methods are proposed which compute pairwise similarities between graphs [17, 2, 30, 29].", "startOffset": 122, "endOffset": 137}, {"referenceID": 28, "context": "In graph classification, a myriad of graph kernel methods are proposed which compute pairwise similarities between graphs [17, 2, 30, 29].", "startOffset": 122, "endOffset": 137}, {"referenceID": 28, "context": "For example, graphlets [29, 36] computes the graph similarity based on the distribution of induced, non-isomorphic subgraphs.", "startOffset": 23, "endOffset": 31}, {"referenceID": 35, "context": "For example, graphlets [29, 36] computes the graph similarity based on the distribution of induced, non-isomorphic subgraphs.", "startOffset": 23, "endOffset": 31}, {"referenceID": 27, "context": "Some other graph kernels integrate frequent graph mining into the model training process [28, 26].", "startOffset": 89, "endOffset": 97}, {"referenceID": 25, "context": "Some other graph kernels integrate frequent graph mining into the model training process [28, 26].", "startOffset": 89, "endOffset": 97}, {"referenceID": 33, "context": "Several proposals have been made to learn a low-dimensional vector representation of individual nodes by considering their neighborhood [34, 25, 15].", "startOffset": 136, "endOffset": 148}, {"referenceID": 24, "context": "Several proposals have been made to learn a low-dimensional vector representation of individual nodes by considering their neighborhood [34, 25, 15].", "startOffset": 136, "endOffset": 148}, {"referenceID": 14, "context": "Several proposals have been made to learn a low-dimensional vector representation of individual nodes by considering their neighborhood [34, 25, 15].", "startOffset": 136, "endOffset": 148}, {"referenceID": 39, "context": "Deep learning techniques have also improved graph kernels for graph structure learning[40, 41, 23].", "startOffset": 86, "endOffset": 98}, {"referenceID": 40, "context": "Deep learning techniques have also improved graph kernels for graph structure learning[40, 41, 23].", "startOffset": 86, "endOffset": 98}, {"referenceID": 22, "context": "Deep learning techniques have also improved graph kernels for graph structure learning[40, 41, 23].", "startOffset": 86, "endOffset": 98}, {"referenceID": 23, "context": "[24] applied convolution over receptive fields constructed by sequence of neighboring nodes.", "startOffset": 0, "endOffset": 4}, {"referenceID": 2, "context": "Heat kernels have been studied for the task of graph clustering [3], graph partitioning [12], and modeling social network marketing processes [21].", "startOffset": 64, "endOffset": 67}, {"referenceID": 11, "context": "Heat kernels have been studied for the task of graph clustering [3], graph partitioning [12], and modeling social network marketing processes [21].", "startOffset": 88, "endOffset": 92}, {"referenceID": 20, "context": "Heat kernels have been studied for the task of graph clustering [3], graph partitioning [12], and modeling social network marketing processes [21].", "startOffset": 142, "endOffset": 146}, {"referenceID": 30, "context": "ture has been successfully used to model 3D objects [31, 13, 39], whose surfaces are defined by polygon meshes, a network com-", "startOffset": 52, "endOffset": 64}, {"referenceID": 12, "context": "ture has been successfully used to model 3D objects [31, 13, 39], whose surfaces are defined by polygon meshes, a network com-", "startOffset": 52, "endOffset": 64}, {"referenceID": 38, "context": "ture has been successfully used to model 3D objects [31, 13, 39], whose surfaces are defined by polygon meshes, a network com-", "startOffset": 52, "endOffset": 64}, {"referenceID": 30, "context": "The motivation in adopting Heat Kernel Signature (HKS) is its theoretical proven properties in representing graphs: HKS is an intrinsic and informative representation for graphs [31].", "startOffset": 178, "endOffset": 182}, {"referenceID": 19, "context": "In practice, researchers focus on a specially case of the network growth prediction problem with the equal interval increment constraint, tj \u2212 tj = ti \u2212 ti = C > 0 [20, 35].", "startOffset": 164, "endOffset": 172}, {"referenceID": 34, "context": "In practice, researchers focus on a specially case of the network growth prediction problem with the equal interval increment constraint, tj \u2212 tj = ti \u2212 ti = C > 0 [20, 35].", "startOffset": 164, "endOffset": 172}, {"referenceID": 30, "context": "In computer vision, graphs are stored as meshed networks and heat kernels are computed by finding eigenfunctions of the Laplace-Beltrami operator [31].", "startOffset": 146, "endOffset": 150}, {"referenceID": 30, "context": "Instead, we use eigenfunction expansion of a graph Laplacian [31, 3] to compute the heat kernel for information networks.", "startOffset": 61, "endOffset": 68}, {"referenceID": 2, "context": "Instead, we use eigenfunction expansion of a graph Laplacian [31, 3] to compute the heat kernel for information networks.", "startOffset": 61, "endOffset": 68}, {"referenceID": 8, "context": "There has been studies on how to tackle this problem [9].", "startOffset": 53, "endOffset": 56}, {"referenceID": 30, "context": "These time points are sampled with equal difference after logarithm [31], such that log zn \u2212 log zn\u22121 = log zn+1 \u2212 log zn.", "startOffset": 68, "endOffset": 72}, {"referenceID": 37, "context": "Figure (a) and (b) are subnetworks from Facebook [38].", "startOffset": 49, "endOffset": 53}, {"referenceID": 9, "context": "Figure (c) and (d) are some authors\u2019 collaboration networks built from ACL Anthology [10].", "startOffset": 85, "endOffset": 89}, {"referenceID": 39, "context": "So following [40], from each large network we extract the ego-networks (subgraph consisting of the neighbors", "startOffset": 13, "endOffset": 17}, {"referenceID": 19, "context": "scaled growth scales label y to log2(y + 1) [20, 35].", "startOffset": 44, "endOffset": 52}, {"referenceID": 34, "context": "scaled growth scales label y to log2(y + 1) [20, 35].", "startOffset": 44, "endOffset": 52}, {"referenceID": 39, "context": "We follow the procedure described in [40] to construct ego-nets.", "startOffset": 37, "endOffset": 41}, {"referenceID": 37, "context": "The Facebook data set is collected from the New Orleans networks [38],", "startOffset": 65, "endOffset": 69}, {"referenceID": 21, "context": "As the YouTube [22] data set also describes user friendships, it", "startOffset": 15, "endOffset": 19}, {"referenceID": 9, "context": "The AAN data set [10] is built upon scientific publications from the ACL Anthology, where nodes are authors and edges are collaboration.", "startOffset": 17, "endOffset": 21}, {"referenceID": 43, "context": "The Weibo data set [44] contains a set of Sina Weibo users with their complete following-followee relationships, as well as 300,000 retweeting paths among these users.", "startOffset": 19, "endOffset": 23}, {"referenceID": 19, "context": "Therefore we downsampled 50% graphs of each train/val/test set with zero growth (to the numbers shown in Table 1) and applied a logarithm transformation of the outcome variable (network growth), following [20, 35].", "startOffset": 205, "endOffset": 213}, {"referenceID": 34, "context": "Therefore we downsampled 50% graphs of each train/val/test set with zero growth (to the numbers shown in Table 1) and applied a logarithm transformation of the outcome variable (network growth), following [20, 35].", "startOffset": 205, "endOffset": 213}, {"referenceID": 0, "context": "Many structural features have been designed for various network prediction tasks [1, 27, 35, 8].", "startOffset": 81, "endOffset": 95}, {"referenceID": 26, "context": "Many structural features have been designed for various network prediction tasks [1, 27, 35, 8].", "startOffset": 81, "endOffset": 95}, {"referenceID": 34, "context": "Many structural features have been designed for various network prediction tasks [1, 27, 35, 8].", "startOffset": 81, "endOffset": 95}, {"referenceID": 7, "context": "Many structural features have been designed for various network prediction tasks [1, 27, 35, 8].", "startOffset": 81, "endOffset": 95}, {"referenceID": 36, "context": "com/ Frequencies of k-node substructures (k \u2264 4)[37].", "startOffset": 48, "endOffset": 52}, {"referenceID": 3, "context": "Other network properties: average degree, the length of the shortest path, edge density, the number of leaf nodes (nodes with degree 1), the number of leaf edges, the average closeness of all nodes, clustering coefficient, diameter, and the number of communities obtained by a community detection algorithm [4].", "startOffset": 307, "endOffset": 310}, {"referenceID": 23, "context": "Following [24], we compare with four state-ofthe-art graph kernels: the shortest-path kernel (SP) [5], the random walk kernel (RW) [14], the graphlet count kernel (GK) [29], and the Weisfeiler-Lehman subtree kernel (WL) [30].", "startOffset": 10, "endOffset": 14}, {"referenceID": 4, "context": "Following [24], we compare with four state-ofthe-art graph kernels: the shortest-path kernel (SP) [5], the random walk kernel (RW) [14], the graphlet count kernel (GK) [29], and the Weisfeiler-Lehman subtree kernel (WL) [30].", "startOffset": 98, "endOffset": 101}, {"referenceID": 13, "context": "Following [24], we compare with four state-ofthe-art graph kernels: the shortest-path kernel (SP) [5], the random walk kernel (RW) [14], the graphlet count kernel (GK) [29], and the Weisfeiler-Lehman subtree kernel (WL) [30].", "startOffset": 131, "endOffset": 135}, {"referenceID": 28, "context": "Following [24], we compare with four state-ofthe-art graph kernels: the shortest-path kernel (SP) [5], the random walk kernel (RW) [14], the graphlet count kernel (GK) [29], and the Weisfeiler-Lehman subtree kernel (WL) [30].", "startOffset": 168, "endOffset": 172}, {"referenceID": 29, "context": "Following [24], we compare with four state-ofthe-art graph kernels: the shortest-path kernel (SP) [5], the random walk kernel (RW) [14], the graphlet count kernel (GK) [29], and the Weisfeiler-Lehman subtree kernel (WL) [30].", "startOffset": 220, "endOffset": 224}, {"referenceID": 23, "context": "This exclusion is also observed for the same reason in [24, 41].", "startOffset": 55, "endOffset": 63}, {"referenceID": 40, "context": "This exclusion is also observed for the same reason in [24, 41].", "startOffset": 55, "endOffset": 63}, {"referenceID": 40, "context": "[41] apply smoothing to graph kernels, which extends their method of deep graph kernels [40] by considering structural similarity between sub-structures.", "startOffset": 0, "endOffset": 4}, {"referenceID": 39, "context": "[41] apply smoothing to graph kernels, which extends their method of deep graph kernels [40] by considering structural similarity between sub-structures.", "startOffset": 88, "endOffset": 92}, {"referenceID": 23, "context": "PSCN, which applies convolutional neural networks (CNN) to locally connected regions from graphs [24], achieving better results over graph kernels on some of the classification data sets.", "startOffset": 97, "endOffset": 101}, {"referenceID": 23, "context": "Following [24] for PSCN, the width is set to the average number of nodes, and the receptive field size is chosen between 5 and 10.", "startOffset": 10, "endOffset": 14}, {"referenceID": 17, "context": "Default hyper-parameters of Adam are used [18].", "startOffset": 42, "endOffset": 46}, {"referenceID": 23, "context": "These results are consistent with previous studies [24, 41].", "startOffset": 51, "endOffset": 59}, {"referenceID": 40, "context": "These results are consistent with previous studies [24, 41].", "startOffset": 51, "endOffset": 59}, {"referenceID": 19, "context": "The graphs in our data sets have size as large as 5,000 nodes and 200,000 edges, which is enough for most network prediction problems [20, 27, 42].", "startOffset": 134, "endOffset": 146}, {"referenceID": 26, "context": "The graphs in our data sets have size as large as 5,000 nodes and 200,000 edges, which is enough for most network prediction problems [20, 27, 42].", "startOffset": 134, "endOffset": 146}, {"referenceID": 41, "context": "The graphs in our data sets have size as large as 5,000 nodes and 200,000 edges, which is enough for most network prediction problems [20, 27, 42].", "startOffset": 134, "endOffset": 146}, {"referenceID": 3, "context": "The feature vectors output by the last hidden layer of DeepGraph are fed to t-SNE [4], a dimensionality reduction algorithm for visualizing high-dimensional data sets.", "startOffset": 82, "endOffset": 85}, {"referenceID": 5, "context": "According to social network literature [6], nodes spanning structural holes are likely to gain social capital, promoting the growth of its ego-net.", "startOffset": 39, "endOffset": 42}, {"referenceID": 28, "context": "First, as the number of open and closed triangles are actually features of graphlets [29, 36], we can see that DeepGraph has automatically learned these useful features without human input.", "startOffset": 85, "endOffset": 93}, {"referenceID": 35, "context": "First, as the number of open and closed triangles are actually features of graphlets [29, 36], we can see that DeepGraph has automatically learned these useful features without human input.", "startOffset": 85, "endOffset": 93}], "year": 2016, "abstractText": "The topological (or graph) structures of real-world networks are known to be predictive of multiple dynamic properties of the networks. Conventionally, a graph structure is represented using an adjacency matrix or a set of hand-crafted structural features. These representations either fail to highlight local and global properties of the graph or suffer from a severe loss of structural information. There lacks an effective graph representation, which hinges the realization of the predictive power of network structures. In this study, we propose to learn the represention of a graph, or the topological structure of a network, through a deep learning model. This end-to-end prediction model, named DeepGraph, takes the input of the raw adjacency matrix of a real-world network and outputs a prediction of the growth of the network. The adjacency matrix is first represented using a graph descriptor based on the heat kernel signature, which is then passed through a multicolumn, multi-resolution convolutional neural network. Extensive experiments on five large collections of real-world networks demonstrate that the proposed prediction model significantly improves the effectiveness of existing methods, including linear or nonlinear regressors that use hand-crafted features, graph kernels, and competing deep learning methods.", "creator": "LaTeX with hyperref package"}}}