{"id": "1610.06276", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Oct-2016", "title": "Modeling Scalability of Distributed Machine Learning", "abstract": "Present day machine learning is computationally intensive and processes large amounts of data. It is implemented in a distributed fashion in order to address these scalability issues. The work is parallelized across a number of computing nodes. It is usually hard to estimate in advance how many nodes to use for a particular workload. We propose a simple framework for estimating the scalability of distributed machine learning algorithms. We measure the scalability by means of the speedup an algorithm achieves with more nodes. We propose time complexity models for gradient descent and graphical model inference. We validate our models with experiments on deep learning training and belief propagation. This framework was used to study the scalability of machine learning algorithms in Apache Spark.", "histories": [["v1", "Thu, 20 Oct 2016 03:28:40 GMT  (104kb,D)", "https://arxiv.org/abs/1610.06276v1", "6 pages, 4 figures"], ["v2", "Sat, 25 Mar 2017 02:17:04 GMT  (104kb,D)", "http://arxiv.org/abs/1610.06276v2", "6 pages, 4 figures, appears at ICDE 2017"]], "COMMENTS": "6 pages, 4 figures", "reviews": [], "SUBJECTS": "cs.LG cs.DC", "authors": ["alexander ulanov", "rey simanovsky", "manish marwah"], "accepted": false, "id": "1610.06276"}, "pdf": {"name": "1610.06276.pdf", "metadata": {"source": "CRF", "title": "Modeling Scalability of Distributed Machine Learning", "authors": ["Alexander Ulanov", "Andrey Simanovsky", "Manish Marwah"], "emails": ["alexander.ulanov@hpe.com", "andrey.simanovsky@hpe.com", "manish.marwah@hpe.com"], "sections": [{"heading": null, "text": "This year is the highest in the history of the country."}, {"heading": "II. RELATED WORK", "text": "Most work on performance modelling and resource allocation for machine learning focuses on Hadoop / MapReduce. In addition, they require profile data and use complicated models. In contrast, our method only requires hardware specification. Xiv: 161 0.06 276v 2 [cs.L G] 25 Mar 201 7Gandhi et al. [7] define a fine-grained model for Hadoop / MapReduce. The computational model is approximated by second-order polynomials and would therefore not be accurate if the workload is more time complex, such as deep learning. Herodotou and Babu [8] build models to answer what-if-performance questions, but they require the collection of extensive profile data. The next work to us is Sparks et. al. [9] where they propose a cluster resource allocation estimator."}, {"heading": "III. PROPOSED METHODOLOGY", "text": "There are two components that we model: the machine learning algorithm and the framework for distributed calculations. We assume that the algorithm is implemented with the bulk synchronous parallel (BSP) frame. We define the distributed computing time complexity astcp = c (D) / nwo n is the number of homogeneous computing steps with a synchronization barrier at the end."}, {"heading": "IV. USE CASES", "text": "In this section, we discuss specific machine learning workloads, gradient descent, and graphical model inference. Gradient descent is the most commonly used machine learning optimization algorithm. It is used to train machine learning models for tasks such as ad click rate prediction, image classification, and speech recognition. On the other side of the spectrum of machine learning methods, these are graphical models. Of course, many areas are presented as graphs where nodes or edges contain some parameters that you would like to estimate, the latter being achieved with a certain probable inference algorithm. Graphical models are used in such use cases as calculating the importance of websites, product recommendations, and fraud detection. The important feature of use cases is that they contain large training data. Understanding the scalability of use cases helps the practitioner to select from a variety of configuration options, saving time and money."}, {"heading": "A. Gradient descent", "text": "The course of the cost function is calculated with each iteration, depending on the current parameters of the model and the training set. Then, each parameter is multiplied by the gradient, which is multiplied by a small constant. Iterations are repeated until the parameter values converge. Batch Gradient Descent implies that the gradient is calculated using the entire training parameters of the model and the training set. Stochastic Gradient Descent (SGD) uses a random sample for each iteration. Minibatch SGD uses a random mini-batch of examplers."}, {"heading": "B. Graphical models", "text": "Graphic models [14] combine probabilistic dependence and reasoning with graph theory. Vertexes in the graph represent random variables, while edges denote dependencies. The graph structure provides an elegant representation of the conditional independence relationships between variables. \u00b7 An important task in these models is to derive the state of unknown or hidden variables. While the exact inference is insoluble for large models, approximate methods, such as Gibbs sampling or loopy belief propagation, are commonly used. In our analysis, we look at pairs of Markov Random Field (MRF) models that are generic enough to represent a graphical model. Inference methods are iterative and can be paralleled in different ways. We build a model in which texts are processed in parallel by multiple worker nodes."}, {"heading": "V. EXPERIMENT RESULTS AND VALIDATION", "text": "We conducted a series of experiments to validate the proposed models for the gradient descent and the graphical model conclusion."}, {"heading": "A. Training of deep learning models", "text": "In fact, most of them will be able to play by the rules that they need for their work, and they will not be able to play by the rules that they need for their work."}, {"heading": "B. Belief propagation", "text": "Belief in Reason (BP) is a popular message delivery algorithm for calculating random variables in a graphical model, such as a Markov random field. It provides exact conclusions for trees and approximate conclusions for graphs with cycles (in this case it is called loopy faith propagation).Although loopy faith propagation works in two steps, it is based on the messages of its neighbors, a vertex of updates in practice for many applications [22], such as encryption, malware detection. The loopy faith propagation algorithm works in two steps."}, {"heading": "VI. CONCLUSION AND FUTURE WORK", "text": "We presented a methodology for building performance models of distributed machine learning algorithms, which we applied to two machine learning applications: Gradient Descent and Graphic Model Conclusion. We validated the framework with experiments on neural networks and belief propagation. The results showed close agreement between our models and emperical data. There are several possibilities for future research. One direction is the enrichment of machine learning models. In this regard, we are considering the creation of a model for asynchronous algorithms, such as asynchronous gradient descent [24]. We are also interested in exploring compromises between parallelization and convergence specific to machine learning. For example, parallelization techniques pay off for parallelism with algorithmically slower convergence or convergence to a worse local optimum. Finally, the inclusion of a feedback configuration of paper solutions would almost prevent the adoption of modularity in some of them, as well as the adoption of a paper loop."}, {"heading": "ACKNOWLEDGMENT", "text": "The authors thank Rob Schreiber of Hewlett Packard Labs and Xiangrui Meng of Databricks for inspiring discussions and Carlos Zubieta for his help in the faith propagation experiments."}], "references": [{"title": "An introduction to parallel algorithms", "author": ["J. J\u00e1J\u00e1"], "venue": "Addison-Wesley Reading,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1992}, {"title": "Validity of the single processor approach to achieving large scale computing capabilities", "author": ["G.M. Amdahl"], "venue": "Proceedings of the Spring Joint Computer Conference, ser. AFIPS \u201967 (Spring). New York, NY, USA: ACM, 1967, pp. 483\u2013485.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1967}, {"title": "Reevaluating amdahl\u2019s law", "author": ["J.L. Gustafson"], "venue": "Commun. ACM, vol. 31, no. 5, pp. 532\u2013533, May 1988.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1988}, {"title": "Introduction to parallel processing: algorithms and architectures", "author": ["B. Parhami"], "venue": "Springer Science & Business Media,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2006}, {"title": "Autoscaling for hadoop clusters", "author": ["A. Gandhi", "S. Thota", "P. Dube", "A. Kochut", "L. Zhang"], "venue": "Cloud Engineering (IC2E), 2016 IEEE International Conference on. IEEE, 2016, pp. 109\u2013118.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2016}, {"title": "Profiling, what-if analysis, and costbased optimization of mapreduce programs", "author": ["H. Herodotou", "S. Babu"], "venue": "Proceedings of the VLDB Endowment, vol. 4, no. 11, pp. 1111\u20131122, 2011.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2011}, {"title": "Automating model search for large scale machine learning", "author": ["E.R. Sparks", "A. Talwalkar", "D. Haas", "M.J. Franklin", "M.I. Jordan", "T. Kraska"], "venue": "Proceedings of the Sixth ACM Symposium on Cloud Computing. ACM, 2015, pp. 368\u2013380.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2015}, {"title": "A few bad ideas on the way to the triumph of parallel computing", "author": ["R. Schreiber"], "venue": "Journal of Parallel and Distributed Computing, vol. 74, no. 7, pp. 2544\u20132547, 2014.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2014}, {"title": "Ernest: efficient performance prediction for large-scale advanced analytics", "author": ["S. Venkataraman", "Z. Yang", "M. Franklin", "B. Recht", "I. Stoica"], "venue": "13th USENIX Symposium on Networked Systems Design and Implementation (NSDI 16). USENIX Association, 2016, pp. 363\u2013378.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2016}, {"title": "Distributed training of deep neural networks: theoretical and practical limits of parallel scalability", "author": ["J. Keuper", "F.-J. Preundt"], "venue": "Proceedings of the Workshop on Machine Learning in High Performance Computing Environments. IEEE Press, 2016, pp. 19\u201326.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2016}, {"title": "A bridging model for parallel computation", "author": ["L.G. Valiant"], "venue": "Communications of the ACM, vol. 33, no. 8, pp. 103\u2013111, 1990.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1990}, {"title": "Probabilistic graphical models: principles and techniques", "author": ["D. Koller", "N. Friedman"], "venue": "MIT press,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2009}, {"title": "Deep big simple neural nets excel on handwritten digit recognition", "author": ["C.C. Dan", "C. Cires", "U. Meier"], "venue": "2010.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2010}, {"title": "Rethinking the inception architecture for computer vision", "author": ["C. Szegedy", "V. Vanhoucke", "S. Ioffe", "J. Shlens", "Z. Wojna"], "venue": "arXiv preprint arXiv:1512.00567, 2015.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}, {"title": "Resilient distributed datasets: A fault-tolerant abstraction for in-memory cluster computing", "author": ["M. Zaharia", "M. Chowdhury", "T. Das", "A. Dave", "J. Ma", "M. McCauley", "M.J. Franklin", "S. Shenker", "I. Stoica"], "venue": "Proceedings of the 9th USENIX conference on Networked Systems Design and Implementation. USENIX Association, 2012, pp. 2\u20132.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2012}, {"title": "Revisiting distributed synchronous sgd", "author": ["J. Chen", "R. Monga", "S. Bengio", "R. Jozefowicz"], "venue": "arXiv preprint arXiv:1604.00981, 2016.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2016}, {"title": "Tensorflow: Large-scale machine learning on heterogeneous distributed systems", "author": ["M. Abadi", "A. Agarwal", "P. Barham", "E. Brevdo", "Z. Chen", "C. Citro", "G.S. Corrado", "A. Davis", "J. Dean", "M. Devin"], "venue": "arXiv preprint arXiv:1603.04467, 2016.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2016}, {"title": "Probabilistic reasoning in intelligent systems: networks of plausible inference", "author": ["J. Pearl"], "venue": null, "citeRegEx": "21", "shortCiteRegEx": "21", "year": 1988}, {"title": "Loopy belief propagation for approximate inference: An empirical study", "author": ["K.P. Murphy", "Y. Weiss", "M.I. Jordan"], "venue": "Proceedings of the Fifteenth conference on Uncertainty in artificial intelligence. Morgan Kaufmann Publishers Inc., 1999, pp. 467\u2013475.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 1999}, {"title": "Distributed graphlab: a framework for machine learning and data mining in the cloud", "author": ["Y. Low", "D. Bickson", "J. Gonzalez", "C. Guestrin", "A. Kyrola", "J.M. Hellerstein"], "venue": "Proceedings of the VLDB Endowment, vol. 5, no. 8, pp. 716\u2013727, 2012.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2012}, {"title": "Hogwild: A lock-free approach to parallelizing stochastic gradient descent", "author": ["B. Recht", "C. Re", "S. Wright", "F. Niu"], "venue": "Advances in Neural Information Processing Systems 24, J. Shawe-taylor, R. Zemel, P. Bartlett, F. Pereira, and K. Weinberger, Eds., 2011, pp. 693\u2013701.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2011}], "referenceMentions": [{"referenceID": 0, "context": "Performance models of distributed systems have been thoroughly studied in parallel algorithms community [1].", "startOffset": 104, "endOffset": 107}, {"referenceID": 1, "context": "Amdahl [2] suggested a law that describes how fast problems can be solved using parallel computations.", "startOffset": 7, "endOffset": 10}, {"referenceID": 2, "context": "Gustafson [3] formulated consensus on what happens when", "startOffset": 10, "endOffset": 13}, {"referenceID": 3, "context": "Even though the parallel algorithms community often excluded communication overheads from their models, a computation-communication trade-off was well understood [4].", "startOffset": 162, "endOffset": 165}, {"referenceID": 4, "context": "[7] define a fine-grained model for", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "Herodotou and Babu [8] build models to answer what-if performance questions, however, they require collection of extensive profiling data.", "startOffset": 19, "endOffset": 22}, {"referenceID": 6, "context": "[9] where they propose a cluster resource allocation estimator.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "However, according to Scheiber [10], one could make it decline with increasing n, so that the sequential piece is irrelevant to scaling.", "startOffset": 31, "endOffset": 35}, {"referenceID": 8, "context": "[11].", "startOffset": 0, "endOffset": 4}, {"referenceID": 6, "context": "Their model is similar to [9] with the addition of logarithmic dependency on the number of workers.", "startOffset": 26, "endOffset": 29}, {"referenceID": 9, "context": "Keuper and Pfreundt [12] investigate the scalability of deep learning training and experimentally show that there is a communication bottleneck.", "startOffset": 20, "endOffset": 24}, {"referenceID": 10, "context": "(BSP) framework [13], comprising a series of supersteps.", "startOffset": 16, "endOffset": 20}, {"referenceID": 11, "context": "Graphical models [14] combine probabilistic dependency and reasoning with graph theory.", "startOffset": 17, "endOffset": 21}, {"referenceID": 12, "context": "use is one of the most accurate networks used for MNIST handwritten character recognition [15].", "startOffset": 90, "endOffset": 94}, {"referenceID": 13, "context": "3, a convolutional network for ImageNet image classification challenge [16].", "startOffset": 71, "endOffset": 75}, {"referenceID": 14, "context": "We used the Apache Spark [17] 64-bit implementation of ANN in our experiments.", "startOffset": 25, "endOffset": 29}, {"referenceID": 15, "context": "[19] for speedup vs 50 workers.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "Chen and others conducted experiments with a GPU cluster of nVidia K40 GPU and TensorFlow software [20].", "startOffset": 99, "endOffset": 103}, {"referenceID": 15, "context": "The authors of [19] explored the scalability of synchronous and asynchronous mini-batch SGD in terms of model convergence.", "startOffset": 15, "endOffset": 19}, {"referenceID": 17, "context": "Belief propagation (BP) [21] is a popular message-passing based algorithm for computing marginals of random variables in a graphical model, such as a Markov random field.", "startOffset": 24, "endOffset": 28}, {"referenceID": 18, "context": "Even though loopy belief propagation is an approximate algorithm with no convergence guarantees, it works well in practice for many applications [22] such as", "startOffset": 145, "endOffset": 149}, {"referenceID": 19, "context": "Experimental curves were obtained with our implementation of BP in GraphLab [23], which ran on an HP ProLiant DL980 server with 80 CPU cores at 1.", "startOffset": 76, "endOffset": 80}, {"referenceID": 20, "context": "In that respect, we consider building a model for asynchronous algorithms, such as asynchronous gradient descent [24].", "startOffset": 113, "endOffset": 117}], "year": 2017, "abstractText": "Present day machine learning is computationally intensive and processes large amounts of data. It is implemented in a distributed fashion in order to address these scalability issues. The work is parallelized across a number of computing nodes. It is usually hard to estimate in advance how many nodes to use for a particular workload. We propose a simple framework for estimating the scalability of distributed machine learning algorithms. We measure the scalability by means of the speedup an algorithm achieves with more nodes. We propose time complexity models for gradient descent and graphical model inference. We validate our models with experiments on deep learning training and belief propagation. This framework was used to study the scalability of machine learning algorithms in Apache Spark.", "creator": "LaTeX with hyperref package"}}}