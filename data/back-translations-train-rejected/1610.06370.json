{"id": "1610.06370", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Oct-2016", "title": "Clinical Text Prediction with Numerically Grounded Conditional Language Models", "abstract": "Assisted text input techniques can save time and effort and improve text quality. In this paper, we investigate how grounded and conditional extensions to standard neural language models can bring improvements in the tasks of word prediction and completion. These extensions incorporate a structured knowledge base and numerical values from the text into the context used to predict the next word. Our automated evaluation on a clinical dataset shows extended models significantly outperform standard models. Our best system uses both conditioning and grounding, because of their orthogonal benefits. For word prediction with a list of 5 suggestions, it improves recall from 25.03% to 71.28% and for word completion it improves keystroke savings from 34.35% to 44.81%, where theoretical bound for this dataset is 58.78%. We also perform a qualitative investigation of how models with lower perplexity occasionally fare better at the tasks. We found that at test time numbers have more influence on the document level than on individual word probabilities.", "histories": [["v1", "Thu, 20 Oct 2016 11:48:30 GMT  (502kb,D)", "http://arxiv.org/abs/1610.06370v1", "Accepted at the 7th International Workshop on Health Text Mining and Information Analysis (LOUHI) EMNLP 2016"]], "COMMENTS": "Accepted at the 7th International Workshop on Health Text Mining and Information Analysis (LOUHI) EMNLP 2016", "reviews": [], "SUBJECTS": "cs.CL cs.HC cs.NE", "authors": ["georgios p spithourakis", "steffen e petersen", "sebastian riedel"], "accepted": false, "id": "1610.06370"}, "pdf": {"name": "1610.06370.pdf", "metadata": {"source": "CRF", "title": "Clinical Text Prediction with Numerically Grounded Conditional Language Models", "authors": ["Georgios P. Spithourakis", "Steffen E. Petersen", "William Harvey"], "emails": ["g.spithourakis@cs.ucl.ac.uk", "s.e.petersen@qmul.ac.uk", "s.riedel@cs.ucl.ac.uk"], "sections": [{"heading": "1 Introduction", "text": "It is an assisted data input that aims to save time and effort by reducing the number of keystrokes needed and improving text quality by preventing spelling errors by increasing text generation rates for people with motor or language impairments (Sevenster and Aleksovski, 2010), and has been extended to a range of applications, such as data entry on mobile devices (Dunlop and Crossan, 2000), to increase text generation rates for people with motor or language impairments."}, {"heading": "2 Related Work", "text": "Word completion was a feature of the discharge summary (Chen et al., 2012), brain MRI report (Cannataro et al., 2012) and radiology report (Eng and Eisner, 2004) compilation systems. The goal is clinical document standardization, Sirel (2012) has the ICD10 medical classification codes as a lexical resource, and Lin et al. (2014) built a semi-automatic annotation tool to generate interoperable clinical documentation. Hua et al. (2014) reported a time shortening and 3.9% increase in response accuracy in a data entry task. Gong et al. (2016) found an 87.1% performance for keystroke savings, a 70.5% increase in text generation rate, a significant increase in reporting ringings of text and a significant increase in reporting accuracy, and a significant increase in reporting and a report type."}, {"heading": "3 Methodology", "text": "In this section, we present a solution for the prediction and completion tasks of the word (Section 3.1), and then discuss how language models based on numerical quantities mentioned in the text and / or based on external contexts can be used within our framework (Section 3.2). Finally, we describe our automated evaluation process and different evaluation indicators for the two tasks (Section 3.3)."}, {"heading": "3.1 Word prediction and completion", "text": "In fact, it is such that most people who are able to put themselves into another world, in which they are able to integrate themselves, in which they are able to integrate themselves, in which they are able to integrate themselves, in which they are able to integrate themselves, in which they are able to remain in the world, in which they are able to live, in which they are able to integrate themselves, and in which they are able to integrate themselves, in which they are able to stay in the world, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live in which they live, in which they live, in which they live in which they live, in which they live, in which they live, in which they live, in which they live in which they live, in which they live, in which they live, in which they live, in which they live in which they live, in which they live, in which they live, in which they live in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they are able"}, {"heading": "3.2 Neural language models", "text": "In fact, we will be able to find the aforementioned lcihsrc\u00fcehcs. iDe eerwdnei eerwdnei eerwdnei eerwdnei rf\u00fc ide eerwdnei eerwdnei rf\u00fc ide eerwdnei eerwdnei eerwdnei eerwdne.ndU"}, {"heading": "3.3 Automated evaluation", "text": "We perform an automated assessment for both tasks and all systems by simulating a user typing the character by letter, and the stream of characters comes from a set of completed clinical reports. In practice, we assume that the word from the record is the correct word. For the word completion task, we assume that the user types in the specific key to complete the word as soon as the correct proposal is available. In practice, the two tasks can be tackled simultaneously, for example, a list of suggestions based on a language model, and you can choose to complete the prefix with the word at the top of the list. However, we have decided to decouple the two functions due to their conceptual differences, which require different evaluation metrics. To predict the word, the user has not yet started typing, and they may seek guidance in the system's suggestions for their final decision."}, {"heading": "4 Data", "text": "Our data set includes 16,003 anonymized clinical records from London Chest Hospital. Table 1 summarizes the descriptive statistics of the dataset. Each patient file consists of a text report and accompanying structured KB tuples. The latter describe patient metadata (age and gender) and results of medical tests (e.g. diastolic and systolic end volumes for the left and right ventricles, measured using magnetic resonance imaging).This information was extracted from the hospital's electronic health records and was available to the physician at the time the report was compiled. Overall, the KB describes 20 possible attributes, one of which is categorical (gender) and the rest numerical (age is more integral and test results are evaluated in real terms).On average, 7.7 tuples per record are completed. Numerical tokens make up a large part of the vocabulary (> 40%) and suffer from high vocabulary rates (40% each, in spite of a small percentage of 4.3%)."}, {"heading": "5 Results and discussion", "text": "In this section, we describe the structure of our experiments (Section 5.1) and then present and discuss the evaluation results for the tasks of word prediction (Section 5.2) and word completion (Section 5.3). Finally, we perform a qualitative evaluation (Section 5.4)."}, {"heading": "5.1 Setup", "text": "Our base model LM is a single-layer long-term memory network (LSTM) (Hochreiter and Schmidhuber, 1997) with all latent dimensions (internal matrices, input and output embedded), which is D = 50. We extend this base model with the techniques described in Section 3.2 and derive a model that is bound to the KB (+ c), a model that is numerically grounded (+ g) and a model that is both conditional and grounded (+ c + g). We also experiment with ablations of these models that ignore some information sources at the test date. Specifically, we perform the conditional models without the encoder that ignores the KB (-kb) and grounded models without the numerical representations that ignore the orders of magnitude of the numerical values (-v)."}, {"heading": "5.2 Word prediction", "text": "We show our evaluation results on the test set for the Word prediction task in Table 2. The conditioned model (+ c) achieves double the MRR and quadruples the Recall @ 1 of the base model, although it brings only slight improvements in helplessness. Contrary to intuition, the grounded model (+ g) achieves a more significant perplexity improvement (33%), but lower gains for MRR and Recall @ 1 (85% and 150% improvement, respectively). Contrary to intuition, we find that a model with higher perplexity performs better in a language modeling task. The grounded conditional model (+ c + g) performs best among the systems, with about 5 points of additive improvement across all rating metrics compared to the second best. The benefits of conditioning and grounding appear to be orthogonal to another task. Recall increases with the length of the suggestion list (equivalent to rank).The increase is linear for the metric, but the lower one for each baseline, but the lower one for the baseline."}, {"heading": "5.3 Word completion", "text": "The theoretical limit comes from an ideal system that retrieves the right word after the user has typed the single character. Vocabulary limit is similar, but only makes a suggestion if the right word is in the familiar vocabulary. We extend these limits to the rest of the assessment metrics. The conditioned model (+ c) improves keystroke savings by 25% above the baseline, while halving unnecessary distractions. The grounded model (+ g) achieves minor improvements above the baseline. The grounded conditional model (+ c + g) again performs best among systems, delivering keystroke savings of 44.81%, almost half the theoretical limit, and the lowest number of unnecessary distractions. In this task, the determined conditional model (+ c + g) represents the best performance among systems."}, {"heading": "5.4 Qualitative results", "text": "The results so far show two unexpected situations. First, we observed that a model with poorer perplexity is more likely to target word predictions that model a language. Second, we observed that occasionally a runtime ablation of a conditional or grounded model exceeds its system opposite. We conducted qualitative experiments to investigate these scenarios. We selected a document from the development group and identified a word of interest and numerical values that can influence the choice of the word. In Table 4, we show the document selected and the top suggestions for the word in different systems. Systems do not have access to tokens from the word."}, {"heading": "6 Conclusion", "text": "In this work, we demonstrated how numerically based language models based on an external knowledge base can be used for word prediction and completion tasks. Our experiments with a clinical dataset showed that the two extensions of standard language models have complementary advantages. Our best model uses a combination of conditioning and grounding to improve the memory of the word prediction task from 25.03% to 71.28%. In the task of word completion, it improves keystrokes from 34.35% to 44.81%, with the upper theoretical limit for this dataset being 58.78%. We found that perplexity does not always correlate with system performance in the two downstream tasks. Our ablation experiments and qualitative studies showed that at test points, numbers have a greater impact on the documentation level than on individual word probabilities. Our approach was not based on ontologies or fine-grained data linkages, but would lead to additional improvements in our ability to obtain new information."}, {"heading": "Acknowledgments", "text": "The authors thank the anonymous reviewers for their research, which was supported by the Farr Institute of Health Informatics Research and an Allen Distinguished Investigator Award."}], "references": [{"title": "Type less, find more: fast autocompletion search with a succinct index", "author": ["Bast", "Weber2006] Holger Bast", "Ingmar Weber"], "venue": "In Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval", "citeRegEx": "Bast et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Bast et al\\.", "year": 2006}, {"title": "Predicting sentences using ngram language models", "author": ["Peter Haider", "Tobias Scheffer"], "venue": "In Proceedings of Human Language Technology and Empirical Methods in Natural Language Processing", "citeRegEx": "Bickel et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Bickel et al\\.", "year": 2005}, {"title": "Multimodal distributional semantics", "author": ["Bruni et al.2014] Elia Bruni", "Nam-Khanh Tran", "Marco Baroni"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Bruni et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bruni et al\\.", "year": 2014}, {"title": "Knowledge-based compilation of magnetic resonance diagnosis reports in neuroradiology", "author": ["Orlando Alfieri", "Francesco Fera"], "venue": "In 25th International Symposium on Computer-Based Medical Systems (CBMS). IEEE", "citeRegEx": "Cannataro et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Cannataro et al\\.", "year": 2012}, {"title": "Design and implementation of web-based discharge summary note based on service-oriented architecture", "author": ["Chen et al.2012] Chi-Huang Chen", "Sung-Huai Hsieh", "Yu-Shuan Su", "Kai-Ping Hsu", "Hsiu-Hui Lee", "Feipei Lai"], "venue": "Journal of medical systems,", "citeRegEx": "Chen et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2012}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "author": ["Cho et al.2014] Kyunghyun Cho", "Bart Van Merri\u00ebnboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio"], "venue": null, "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Finding Contradictions in Text", "author": ["Anna N Rafferty", "Christopher D Manning"], "venue": "In Proceedings of ACL", "citeRegEx": "Marneffe et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Marneffe et al\\.", "year": 2008}, {"title": "Predictive text entry methods for mobile phones", "author": ["Dunlop", "Crossan2000] Mark D Dunlop", "Andrew Crossan"], "venue": "Personal Technologies,", "citeRegEx": "Dunlop et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Dunlop et al\\.", "year": 2000}, {"title": "Informatics in radiology (inforad) radiology report entry with automatic phrase completion driven by language", "author": ["Eng", "Eisner2004] John Eng", "Jason M Eisner"], "venue": "modeling. Radiographics,", "citeRegEx": "Eng et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Eng et al\\.", "year": 2004}, {"title": "Testing the efficacy of part-of-speech information in word completion", "author": ["Fazly", "Hirst2003] Afsaneh Fazly", "Graeme Hirst"], "venue": "In Proceedings of the EACL 2003 Workshop on Language Modeling for Text Entry Methods", "citeRegEx": "Fazly et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Fazly et al\\.", "year": 2003}, {"title": "Grounded Language Modeling for Automatic Speech Recognition of Sports Video", "author": ["Fleischman", "Roy2008] Michael Fleischman", "Deb Roy"], "venue": "In Proceedings of ACL", "citeRegEx": "Fleischman et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Fleischman et al\\.", "year": 2008}, {"title": "User-friendly text prediction for translators", "author": ["Foster et al.2002] George Foster", "Philippe Langlais", "Guy Lapalme"], "venue": "In Proceedings of the ACL-02 conference on Empirical methods in natural language", "citeRegEx": "Foster et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Foster et al\\.", "year": 2002}, {"title": "Text prediction systems: a survey", "author": ["Garay-Vitoria", "Julio Abascal"], "venue": "Universal Access in the Information", "citeRegEx": "Garay.Vitoria et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Garay.Vitoria et al\\.", "year": 2006}, {"title": "Contextual lstm (clstm) models for large scale nlp tasks. arXiv:1602.06291", "author": ["Ghosh et al.2016] Shalini Ghosh", "Oriol Vinyals", "Brian Strope", "Scott Roy", "Tom Dean", "Larry Heck"], "venue": null, "citeRegEx": "Ghosh et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Ghosh et al\\.", "year": 2016}, {"title": "Leveraging user\u2019s performance in reporting patient safety events by utilizing text prediction in narrative data entry. Computer methods and programs in biomedicine, 131:181\u2013189", "author": ["Gong et al.2016] Yang Gong", "Lei Hua", "Shen Wang"], "venue": null, "citeRegEx": "Gong et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Gong et al\\.", "year": 2016}, {"title": "Long short-term memory", "author": ["Hochreiter", "Schmidhuber1997] Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Text prediction on structured data entry in healthcare: A two-group randomized usability study measuring the prediction impact on user performance", "author": ["Hua et al.2014] L Hua", "S Wang", "Y Gong"], "venue": "Applied Clinical Informatics,", "citeRegEx": "Hua et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Hua et al\\.", "year": 2014}, {"title": "Multi- and Cross-Modal Semantics Beyond Vision: Grounding in Auditory Perception", "author": ["Kiela", "Clark2015] Douwe Kiela", "Stephen Clark"], "venue": "In Proceedings of EMNLP", "citeRegEx": "Kiela et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kiela et al\\.", "year": 2015}, {"title": "Grounding Semantics in Olfactory Perception", "author": ["Kiela et al.2015] Douwe Kiela", "Luana Bulat", "Stephen Clark"], "venue": "In Proceedings of ACL", "citeRegEx": "Kiela et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kiela et al\\.", "year": 2015}, {"title": "Trans type: Development-evaluation cycles to boost translator\u2019s productivity", "author": ["Langlais", "Lapalme2002] Philippe Langlais", "Guy Lapalme"], "venue": "Machine Translation,", "citeRegEx": "Langlais et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Langlais et al\\.", "year": 2002}, {"title": "Comparison of a semi-automatic annotation tool and a natural language processing application for the generation of clinical statement entries", "author": ["Lin et al.2014] Ching-Heng Lin", "Nai-Yuan Wu", "WeiShao Lai", "Der-Ming Liou"], "venue": "Journal of the American Medical", "citeRegEx": "Lin et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2014}, {"title": "Power of expression in the electronic patient record: structured data or narrative text", "author": ["Robert H Baud", "Pierre Planche"], "venue": "International Journal of Medical Informatics,", "citeRegEx": "Lovis et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Lovis et al\\.", "year": 2000}, {"title": "Modeling semantic containment and exclusion in natural language inference", "author": ["MacCartney", "Manning2008] Bill MacCartney", "Christopher D Manning"], "venue": "In Proceedings of the 22nd International Conference on Computational Linguistics", "citeRegEx": "MacCartney et al\\.,? \\Q2008\\E", "shortCiteRegEx": "MacCartney et al\\.", "year": 2008}, {"title": "Recurrent neural network based language model", "author": ["Martin Karafi\u00e1t", "Luk\u00e1\u0161 Burget", "Jan \u010cernock\u00fd", "Sanjeev Khudanpur"], "venue": "In Interspeech", "citeRegEx": "Mikolov et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2010}, {"title": "Numbers are better than words: Verbal specifications of frequency have no place in medicine", "author": ["Nakao", "Axelrod1983] Michael A Nakao", "Seymour Axelrod"], "venue": null, "citeRegEx": "Nakao et al\\.,? \\Q1983\\E", "shortCiteRegEx": "Nakao et al\\.", "year": 1983}, {"title": "Reasoning about Quantities in Natural Language. Transactions of the Association for Computational Linguistics, 3:1\u201313", "author": ["Roy et al.2015] Subhro Roy", "Tim Vieira", "Dan Roth"], "venue": null, "citeRegEx": "Roy et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Roy et al\\.", "year": 2015}, {"title": "Ask not what textual entailment can do for you.", "author": ["Sammons et al.2010] Mark Sammons", "VG Vydiswaran", "Dan Roth"], "venue": "In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics", "citeRegEx": "Sammons et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Sammons et al\\.", "year": 2010}, {"title": "Snomed ct saves keystrokes: quantifying semantic autocompletion", "author": ["Sevenster", "Zharko Aleksovski"], "venue": "In Proceedings of American Medical Informatics Association (AMIA) Annual Symposium", "citeRegEx": "Sevenster et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Sevenster et al\\.", "year": 2010}, {"title": "Algorithmic and user study of an autocompletion algorithm on a large medical vocabulary", "author": ["Rob van Ommering", "Yuechen Qian"], "venue": "Journal of biomedical informatics,", "citeRegEx": "Sevenster et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Sevenster et al\\.", "year": 2012}, {"title": "Learning Grounded Meaning Representations with Autoencoders", "author": ["Silberer", "Lapata2014] Carina Silberer", "Mirella Lapata"], "venue": "In Proceedings of ACL", "citeRegEx": "Silberer et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Silberer et al\\.", "year": 2014}, {"title": "Dynamic user interfaces for synchronous encoding and linguistic uniforming of textual clinical data", "author": ["Raul Sirel"], "venue": "In Human Language", "citeRegEx": "Sirel.,? \\Q2012\\E", "shortCiteRegEx": "Sirel.", "year": 2012}, {"title": "Numerically grounded language models for semantic error correction", "author": ["Isabelle Augenstein", "Sebastian Riedel"], "venue": "In Proceedings of EMNLP", "citeRegEx": "Spithourakis et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Spithourakis et al\\.", "year": 2016}, {"title": "The roles of experience and domain of expertise in using numerical and verbal probability terms in medical decisions", "author": ["Dani\u00eblle Timmermans"], "venue": "Medical Decision Making,", "citeRegEx": "Timmermans.,? \\Q1994\\E", "shortCiteRegEx": "Timmermans.", "year": 1994}, {"title": "Evaluating word prediction: framing keystroke savings", "author": ["Trnka", "McCoy2008] Keith Trnka", "Kathleen F McCoy"], "venue": "In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics on Human Language Technologies", "citeRegEx": "Trnka et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Trnka et al\\.", "year": 2008}, {"title": "Adaptive language modeling for word prediction", "author": ["Keith Trnka"], "venue": "In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics on Human Language Technologies", "citeRegEx": "Trnka.,? \\Q2008\\E", "shortCiteRegEx": "Trnka.", "year": 2008}, {"title": "Efficient context-sensitive word completion for mobile devices. In Proceedings of the 10th international conference on Human computer interaction with mobile devices and services", "author": ["Van Den Bosch", "Bogers2008] Antal Van Den Bosch", "Toine Bogers"], "venue": null, "citeRegEx": "Bosch et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Bosch et al\\.", "year": 2008}, {"title": "Show and tell: A neural image caption generator", "author": ["Alexander Toshev", "Samy Bengio", "Dumitru Erhan"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition", "citeRegEx": "Vinyals et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Vinyals et al\\.", "year": 2015}, {"title": "Methods to integrate a language model with semantic information for a word prediction component", "author": ["Wandmacher", "Antoine2008] Tonio Wandmacher", "Jean-Yves Antoine"], "venue": "In Proceedings of EMNLP", "citeRegEx": "Wandmacher et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Wandmacher et al\\.", "year": 2008}, {"title": "Describing videos by exploiting temporal structure", "author": ["Yao et al.2015] Li Yao", "Atousa Torabi", "Kyunghyun Cho", "Nicolas Ballas", "Christopher Pal", "Hugo Larochelle", "Aaron Courville"], "venue": "In Proceedings of the IEEE International Conference on Computer Vision", "citeRegEx": "Yao et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Yao et al\\.", "year": 2015}, {"title": "ADADELTA: An Adaptive Learning Rate Method. CoRR, abs/1212.5701", "author": ["Matthew D. Zeiler"], "venue": null, "citeRegEx": "Zeiler.,? \\Q2012\\E", "shortCiteRegEx": "Zeiler.", "year": 2012}], "referenceMentions": [{"referenceID": 28, "context": "It is an assisted data entry function that aims to save time and effort by reducing the number of keystrokes needed and to improve text quality by preventing misspellings, promoting adoption of standard terminologies and allowing for exploration of the vocabulary (Sevenster and Aleksovski, 2010; Sevenster et al., 2012).", "startOffset": 264, "endOffset": 320}, {"referenceID": 11, "context": "Its scope has been extended to a gamut of applications, such as data entry in mobile devices (Dunlop and Crossan, 2000), interactive machine translation (Foster et al., 2002), search term autoar X iv :1 61 0.", "startOffset": 153, "endOffset": 174}, {"referenceID": 3, "context": "completion (Bast and Weber, 2006) and assisted clinical report compilation (Eng and Eisner, 2004; Cannataro et al., 2012).", "startOffset": 75, "endOffset": 121}, {"referenceID": 31, "context": "We address these issues by employing numerically grounded conditional language models (Spithourakis et al., 2016).", "startOffset": 86, "endOffset": 113}, {"referenceID": 4, "context": "Word completion has been a feature of discharge summary (Chen et al., 2012), brain MRI report (Cannataro et al.", "startOffset": 56, "endOffset": 75}, {"referenceID": 3, "context": ", 2012), brain MRI report (Cannataro et al., 2012) and radiology report (Eng and Eisner, 2004) compilation systems.", "startOffset": 26, "endOffset": 50}, {"referenceID": 3, "context": ", 2012), brain MRI report (Cannataro et al., 2012) and radiology report (Eng and Eisner, 2004) compilation systems. Aiming towards clinical document standardisation, Sirel (2012) adopted the ICD10 medical classification codes as a lexical resource and Lin et al.", "startOffset": 27, "endOffset": 179}, {"referenceID": 3, "context": ", 2012), brain MRI report (Cannataro et al., 2012) and radiology report (Eng and Eisner, 2004) compilation systems. Aiming towards clinical document standardisation, Sirel (2012) adopted the ICD10 medical classification codes as a lexical resource and Lin et al. (2014) built a semi-automatic annotation tool to generate entry-level interoperable clinical documents.", "startOffset": 27, "endOffset": 270}, {"referenceID": 14, "context": "Gong et al. (2016) found a performance of 87.", "startOffset": 0, "endOffset": 19}, {"referenceID": 33, "context": "Wandmacher and Antoine (2008) explored methods to integrate n-gram language models with semantic information and Trnka (2008) used topic-adapted language models for word prediction.", "startOffset": 113, "endOffset": 126}, {"referenceID": 13, "context": "More recently, Ghosh et al. (2016) incor-", "startOffset": 15, "endOffset": 35}, {"referenceID": 25, "context": "The motivation to include this information as context to text prediction system is based on the importance of numerical quantities to textual entailment systems (Roy et al., 2015; Sammons et al., 2010; MacCartney and Manning, 2008; De Marneffe et al., 2008).", "startOffset": 161, "endOffset": 257}, {"referenceID": 26, "context": "The motivation to include this information as context to text prediction system is based on the importance of numerical quantities to textual entailment systems (Roy et al., 2015; Sammons et al., 2010; MacCartney and Manning, 2008; De Marneffe et al., 2008).", "startOffset": 161, "endOffset": 257}, {"referenceID": 32, "context": "adjectives and adverbs) has been associated with less precise understanding of frequencies (Nakao and Axelrod, 1983) and probabilities (Timmermans, 1994).", "startOffset": 135, "endOffset": 153}, {"referenceID": 21, "context": "A combination of structured data and free text is deemed more suitable for communicating clinical information (Lovis et al., 2000).", "startOffset": 110, "endOffset": 130}, {"referenceID": 1, "context": "text prediction systems (Bickel et al., 2005; Wandmacher and Antoine, 2008; Trnka, 2008; Ghosh et al., 2016).", "startOffset": 24, "endOffset": 108}, {"referenceID": 34, "context": "text prediction systems (Bickel et al., 2005; Wandmacher and Antoine, 2008; Trnka, 2008; Ghosh et al., 2016).", "startOffset": 24, "endOffset": 108}, {"referenceID": 13, "context": "text prediction systems (Bickel et al., 2005; Wandmacher and Antoine, 2008; Trnka, 2008; Ghosh et al., 2016).", "startOffset": 24, "endOffset": 108}, {"referenceID": 5, "context": "foreign language text for machine translation (Cho et al., 2014), images (Vinyals et al.", "startOffset": 46, "endOffset": 64}, {"referenceID": 36, "context": ", 2014), images (Vinyals et al., 2015; Donahue et al., 2015) and videos (Yao et al.", "startOffset": 16, "endOffset": 60}, {"referenceID": 38, "context": ", 2015) and videos (Yao et al., 2015) for captioning, etc.", "startOffset": 19, "endOffset": 37}, {"referenceID": 2, "context": "Previous work grounds language on vision (Bruni et al., 2014; Silberer and Lapata, 2014), audio (Kiela and Clark, 2015),", "startOffset": 41, "endOffset": 88}, {"referenceID": 17, "context": "video (Fleischman and Roy, 2008) and the olfactory perception (Kiela et al., 2015).", "startOffset": 62, "endOffset": 82}, {"referenceID": 17, "context": "video (Fleischman and Roy, 2008) and the olfactory perception (Kiela et al., 2015). Spithourakis et al. (2016) use numerically grounded language models and language models conditioned on a lexicalised knowledge base for the tasks of semantic error de-", "startOffset": 63, "endOffset": 111}, {"referenceID": 23, "context": "Recurrent neural networks (RNNs) have been successfully used for language modelling (Mikolov et al., 2010).", "startOffset": 84, "endOffset": 106}, {"referenceID": 31, "context": "We use two extensions to the baseline neural LM, described in Spithourakis et al. (2016). A language model can be conditioned on the external context by using an encoder-decoder framework.", "startOffset": 62, "endOffset": 89}, {"referenceID": 39, "context": "Models are trained to minimise a cross-entropy loss, with 20 epochs of back-propagation and gradient descent with adaptive learing rates (AdaDelta) (Zeiler, 2012) and minibatch size set to 64.", "startOffset": 148, "endOffset": 162}, {"referenceID": 1, "context": "precision measurements, respectively, a trade-off is expected between them (Bickel et al., 2005).", "startOffset": 75, "endOffset": 96}, {"referenceID": 16, "context": "In the past, deployment of text prediction systems in clinical settings has lead to measurable gains in productivity (Hua et al., 2014; Gong et al., 2016).", "startOffset": 117, "endOffset": 154}, {"referenceID": 14, "context": "In the past, deployment of text prediction systems in clinical settings has lead to measurable gains in productivity (Hua et al., 2014; Gong et al., 2016).", "startOffset": 117, "endOffset": 154}], "year": 2016, "abstractText": "Assisted text input techniques can save time and effort and improve text quality. In this paper, we investigate how grounded and conditional extensions to standard neural language models can bring improvements in the tasks of word prediction and completion. These extensions incorporate a structured knowledge base and numerical values from the text into the context used to predict the next word. Our automated evaluation on a clinical dataset shows extended models significantly outperform standard models. Our best system uses both conditioning and grounding, because of their orthogonal benefits. For word prediction with a list of 5 suggestions, it improves recall from 25.03% to 71.28% and for word completion it improves keystroke savings from 34.35% to 44.81%, where theoretical bound for this dataset is 58.78%. We also perform a qualitative investigation of how models with lower perplexity occasionally fare better at the tasks. We found that at test time numbers have more influence on the document level than on individual word probabilities.", "creator": "LaTeX with hyperref package"}}}