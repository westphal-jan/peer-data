{"id": "1610.06402", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Oct-2016", "title": "A Growing Long-term Episodic & Semantic Memory", "abstract": "The long-term memory of most connectionist systems lies entirely in the weights of the system. Since the number of weights is typically fixed, this bounds the total amount of knowledge that can be learned and stored. Though this is not normally a problem for a neural network designed for a specific task, such a bound is undesirable for a system that continually learns over an open range of domains. To address this, we describe a lifelong learning system that leverages a fast, though non-differentiable, content-addressable memory which can be exploited to encode both a long history of sequential episodic knowledge and semantic knowledge over many episodes for an unbounded number of domains. This opens the door for investigation into transfer learning, and leveraging prior knowledge that has been learned over a lifetime of experiences to new domains.", "histories": [["v1", "Thu, 20 Oct 2016 13:29:56 GMT  (216kb,D)", "http://arxiv.org/abs/1610.06402v1", "Submission to NIPS workshop on Continual Learning. 4 page extended abstract plus 5 more pages of references, figures, and supplementary material"]], "COMMENTS": "Submission to NIPS workshop on Continual Learning. 4 page extended abstract plus 5 more pages of references, figures, and supplementary material", "reviews": [], "SUBJECTS": "cs.AI cs.LG cs.NE", "authors": ["marc pickett", "rami al-rfou", "louis shao", "chris tar"], "accepted": false, "id": "1610.06402"}, "pdf": {"name": "1610.06402.pdf", "metadata": {"source": "CRF", "title": "A Growing Long-term Episodic & Semantic Memory", "authors": ["Marc Pickett", "Rami Al-Rfou", "Louis Shao", "Chris Tar"], "emails": ["pickett@google.com", "rmyeid@google.com", "overmind@google.com", "ctar@google.com"], "sections": [{"heading": null, "text": "The long-term memory of most connectionist systems lies entirely in the weights of the system. Since the number of weights is typically fixed, this limits the total amount of knowledge that can be learned and stored. Although this is usually not a problem for a neural network designed for a specific task, such a limit is undesirable for a system that continuously learns over an open area. To address this, we describe a lifelong learning system that uses a fast, though undifferentiated, content-addressable memory that can be exploited to encode both a long history of sequential episodic knowledge and semantic knowledge across many episodes for an unlimited number of domains."}, {"heading": "1 Introduction", "text": "Over the course of several decades of experience, a person typically learns a multitude of different domains. A person can learn a new domain and retain their long-standing knowledge of domains they have already learned. However, many neural models experience catastrophic interference when trained in new domains, where new learning overrides or corrupts the previous knowledge of the network. Similarly, if a machine is able to learn a multitude of domains over its \"lifetime,\" it might allow the machine to transfer knowledge between domains and carry a large box of tools when confronted with a new problem. The problem we are addressing is: How can a machine store an unlimited amount of episodic and semantic memory so that it can store knowledge from previous tasks and efficiently retrieve relevant information for new tasks?"}, {"heading": "2 Lifelong Unsupervised Learning", "text": "This year, we will be able to look for a solution that is capable, that we are able to find a solution, that we are able to find a solution, that we are able to find a solution, that we are able to find a solution, that we are able to find a solution, that we are able to find a solution, that we are able to find a solution, that we are able to find a solution, that we are able to find a solution, that we are able to find a solution. \""}, {"heading": "3 Solution Overview: Storing Program Vectors in Long-term Memory", "text": "It is one of the largest hubs in the history of the EWS-EWS-EWS-EWS-EWS-EWS-EWS-EWS-EWS-EWS-EWS-EWS-EWS-EWS-EWS-EWS-EWS-EWS-EWS-EWS-EWS-EWS-EWS-EWS-EWS-EWS-EWS-EWS-EWS-EWS-EWS-EWS-EWS-EWS-EWS-EWS-EWS-EWS-EWS-EWS-EWS-EWS-EWS-EWS-EWS-EWS-EWS-EWS-EWS-EWS-EWS-EWS-EWS-EWS-EWS-EWS-EWS-EWS-EWS-EWS-EWS-EWS-EWS-EWS-EWS-EWS-EWS-EWS-EWS-EWS-EWS-EWS-EWS-EWS-EWS-EWS-EWS-EWS-EWS-EWS-EWS-EWS-EWS-EWS-EWS-EWS-EWS-EWS-EWS-EWS-EWS-EWS-EWS-EWS-EWS-EWS-EWS-EWS-EWS-EWS-EWS-EWS-EWS-EWS-EWS-EWS-EWS-EWS-EWS-EWS-EWS-EWS-EWS-EWS-EWS-EWS-EWS-EWS-EWS-EWS-EWS-EWS-EWS-EWS-E"}, {"heading": "4 Preliminary Results", "text": "We trained the stretcher network by training sequences from different Atari games and varying the number of program vectors the system is allowed to use. So far, the strongest result is that if the number of program vectors corresponds to the number of Atari games, the system tends to use the same program vector for the same domain, i.e. automatically segments the domains without receiving explicit information about which Atari game the tracks come from (see Figure 3 in the Supplementary Material)."}, {"heading": "5 Related work", "text": "Most of these methods work on the basis of static data, which in turn are stored on the basis of semantic knowledge. Other methods are in search of catastrophic interferences (e.g. knowledge of patterns), which are stored only implicitly (e.g. in the data points), both in relation to the instances and to the anchoring of semantic knowledge."}, {"heading": "6 Open Challenges and Future work", "text": "The main contribution of our work is a system for encoding an unlimited amount of episodic and semantic knowledge in an expandable, content-addressing vector memory. This work is still in its infancy and there are many unresolved questions to answer about how a machine can store a lifelong knowledge in such a way that it can be usefully retrieved and transferred to new situations. We share our current approaches to solving some of these problems in supplementary material."}, {"heading": "7 Approaches for Addressing Shortcomings of the Current Approach", "text": "Below we present a selection of the shortcomings of the current approach and what we are doing to address them."}, {"heading": "7.1 Train simple seq2vec classifier that produces best program vector for input vector", "text": "When our current system is given a track, it checks each program vector in its vector memory, one by one, and encodes the tracks using the program vector with the least loss. This linear search is undesirable if there are thousands or millions of program vectors that a continuously learning system could accumulate over its lifetime. Instead, we propose a system that quickly retrieves a small subset of relevant program vectors with a track. We do this by adding a different key vector to each program vector. When we receive an input key, the vector memory retrieves the vectors whose keys are closest to the input key. Then, we train a classifier that takes a track as input and generates a key. This key is then fed into the vector memory, which generates a (small) set of candidate program vectors."}, {"heading": "7.2 Train simple greedy growing number of LTM program vectors", "text": "The simplest case is that we have a batch record. In this case, we gradually add program vectors (by initializing them so that they are close to existing program vectors), and then train with the new vector. (This buffering can be done through the vector memory.) When the buffer is full (which will take longer as the system learns more patterns), the system can add program vectors and train them on the data in the buffer, similar to the batch case, but take the buffer as a stack. The system can also retrieve earlier memory from the vector memory and store it in the buffer along with the data in the training, similar to the complementary model. [24]"}, {"heading": "7.3 Make predictions via instance retrieval (use K-nearest neighbors to transfer knowledge)", "text": "Although the current system is an autoencoder, there are at least two minor modifications that can turn it into a predictive model. The easiest way is to use predictive loss instead of reconstruction loss while we train the stretcher network and program vectors. An alternative approach is more closely related to case-base reasoning. At this point, for each thought vector (with its program vector) we simply use the vector memory to remember the next consistent thought vector in the sequence. If we get a new thought-with-program vector, we retrieve nearby vectors and use the stored consequent vectors to predict the resulting vector for input. This could be a weighted average, or we could simply allow multiple predictions."}, {"heading": "7.4 Smarter parsing: Try multiple parsings", "text": "Our current model does not search for the parsing of the data stream. It simply splits the chunks into sequences of a certain length. The system could encode the stream more compactly if it instead examines multiple windows where parsing is possible. Although there were methods for a differentiated system to learn parsing [7], our first attempts at this will be a simpler discrete search for possible parsing and segmentation, which allows us to easily add further functionality to the parsing process, such as top-down contextual influences of a hierarchical system and classic parsing ideas through dynamic programming and traceability."}, {"heading": "7.5 Train \u201ccontinuation\u201d version, where sequences \u201ccall\u201d next sequence", "text": "When we use the decoder to unroll thought vectors in our current model, we expect to get back only the literal original (short) sequence used in the coding of the thought vector. There are variations of this idea that one can try. Most obvious is predicting or skipping the thought [20], in which we try to predict the following sequence rather than recite the current sequence.Another approach we want to examine is the idea that an unwanted sequence can call other sequences. Instead of just creating literal vectors at the base level, an unwanted sequence can contain a length-two sub-sequences corresponding to a program vector, followed by its argument (i.e., a thought vector).A simple approach to train this would be to create the program vector and thought vector for the last sub-sequence of a long sequence, then an unwanted sequence can be extended to include the previous one (the two short) sub-sequence."}, {"heading": "7.6 Reusable Submodules, Mixture of Experts, and Explaining Away", "text": "A pattern common in many Atari games is a binary \"counter\" in the first 8 bits of memory 5. However, in the current system, each short track is encoded by only one program vector, so each program vector must represent this \"counter\" pattern independently. Although the program vectors implicitly share knowledge through the network, it would be useful if \"expert\" program vectors could specialize in patterns that are used in different areas. An approach to solving this problem is to gradually \"explain\" elements of a trace by greedily applying program vectors that reduce the reconstruction costs the most. For example, if a trace in its first 8 bits has a binary counter pattern, and if we have a program vector Pb specifying itself in that pattern, then we can encode the thought vector created by encoding the trace with the program vector Pb. Ideally, we would generate the subveector in this pattern by matching the specific thought pattern (we would then generate the subdictionary of this particular thought pattern)."}], "references": [{"title": "Instance-based learning algorithms, Machine learning", "author": ["David W Aha", "Dennis Kibler", "Marc K Albert"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1991}, {"title": "Nanoconnectomic upper bound on the variability of synaptic plasticity, Elife", "author": ["Thomas M Bartol Jr.", "Cailey Bromer", "Justin Kinney", "Michael A Chirillo", "Jennifer N Bourne", "Kristen M Harris", "Terrence J Sejnowski"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2015}, {"title": "Learning feed-forward one-shot learners", "author": ["Luca Bertinetto", "Jo\u00e3o F. Henriques", "Jack Valmadre", "Philip H.S. Torr", "Andrea Vedaldi"], "venue": "CoRR abs/1606.05233", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2016}, {"title": "Net2net: Accelerating learning via knowledge transfer, arXiv preprint", "author": ["Tianqi Chen", "Ian Goodfellow", "Jonathon Shlens"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2015}, {"title": "Hierarchical multiscale recurrent neural networks, arXiv preprint", "author": ["Junyoung Chung", "Sungjin Ahn", "Yoshua Bengio"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2016}, {"title": "Adanet: Adaptive structural learning of artificial neural networks, arXiv preprint", "author": ["Corinna Cortes", "Xavi Gonzalvo", "Vitaly Kuznetsov", "Mehryar Mohri", "Scott Yang"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2016}, {"title": "Efficiently implementing episodic memory", "author": ["Nate Derbinsky", "John E Laird"], "venue": "International Conference on Case-Based Reasoning,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2009}, {"title": "Do we have brain to spare", "author": ["David A Drachman"], "venue": "Neurology", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2005}, {"title": "A growing neural gas network learns topologies, Advances in neural information processing systems", "author": ["Bernd Fritzke"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1995}, {"title": "Draw: A recurrent neural network for image generation, arXiv preprint", "author": ["Karol Gregor", "Ivo Danihelka", "Alex Graves", "Danilo Jimenez Rezende", "Daan Wierstra"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2015}, {"title": "Deep residual learning for image recognition, arXiv preprint", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"], "venue": null, "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2015}, {"title": "Chicken, Nonparametric statistical methods", "author": ["Myles Hollander", "Douglas A Wolfe", "Eric"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2013}, {"title": "Sparse Distributed Memory", "author": ["Pentti Kanerva"], "venue": null, "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1988}, {"title": "Ask me anything: Dynamic memory networks for natural language processing, arXiv preprint", "author": ["Ankit Kumar", "Ozan Irsoy", "Jonathan Su", "James Bradbury", "Robert English", "Brian Pierce", "Peter Ondruska", "Ishaan Gulrajani", "Richard Socher"], "venue": null, "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2015}, {"title": "Learning without forgetting", "author": ["Zhizhong Li", "Derek Hoiem"], "venue": "European Conference on Computer Vision, Springer,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2016}, {"title": "Control of memory, active perception, and action in minecraft", "author": ["Junhyuk Oh", "Valliappa Chockalingam", "Satinder Singh", "Honglak Lee"], "venue": null, "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2016}, {"title": "Neocortical neuron number in humans: effect of sex and age", "author": ["Bente Pakkenberg", "Hans J\u00f8rgen G Gundersen"], "venue": "Journal of Comparative Neurology", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 1997}, {"title": "Memory systems for cognitive agents, Proceedings of Human Memory for Artificial Agents Symposium at the Artificial Intelligence and Simulation of Behavior Convention (AISB\u201911)", "author": ["Uma Ramamurthy", "Stan Franklin"], "venue": null, "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2011}, {"title": "On learning to think: Algorithmic information theory for novel combinations of reinforcement learning controllers and recurrent neural world models, arXiv preprint", "author": ["Juergen Schmidhuber"], "venue": null, "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2015}, {"title": "A biologically realistic cleanup memory: Autoassociation in spiking neurons", "author": ["Terrence C Stewart", "Yichuan Tang", "Chris Eliasmith"], "venue": "Cognitive Systems Research", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2011}, {"title": "Memory systems within a cognitive architecture", "author": ["Ron Sun"], "venue": "New Ideas in Psychology", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2012}, {"title": "Sequence to sequence learning with neural networks, Advances in neural information processing", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V Le"], "venue": null, "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2014}, {"title": "Modeling order in neural word embeddings at scale", "author": ["Andrew Trask", "David Gilmore", "Matthew Russell"], "venue": "Proceedings of the 32nd International Conference on Machine Learning (ICML),", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2015}, {"title": "What is episodic memory", "author": ["Endel Tulving"], "venue": "Current Directions in Psychological Science", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 1993}, {"title": "Hashing for similarity search: A survey, arXiv preprint", "author": ["Jingdong Wang", "Heng Tao Shen", "Jingkuan Song", "Jianqiu Ji"], "venue": null, "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2014}], "referenceMentions": [{"referenceID": 23, "context": "This is a looser definition than that used by Tulving and others [33], who require that episodic memory be autobiographical, for example.", "startOffset": 65, "endOffset": 69}, {"referenceID": 16, "context": "This a perfectly reasonable approach, as there is evidence that the total number of neurons in humans actually decreases with age, even accounting for neurogenesis [25].", "startOffset": 164, "endOffset": 168}, {"referenceID": 12, "context": "Various models have been proposed for this type of memory, such as Sparse Distributed Memory [19], Clean-up Memory [29], and approximate nearest neighbor search methods [34].", "startOffset": 93, "endOffset": 97}, {"referenceID": 19, "context": "Various models have been proposed for this type of memory, such as Sparse Distributed Memory [19], Clean-up Memory [29], and approximate nearest neighbor search methods [34].", "startOffset": 115, "endOffset": 119}, {"referenceID": 24, "context": "Various models have been proposed for this type of memory, such as Sparse Distributed Memory [19], Clean-up Memory [29], and approximate nearest neighbor search methods [34].", "startOffset": 169, "endOffset": 173}, {"referenceID": 18, "context": "Following [28], our system is unsupervised, and its goal is to compress its experiences, which is an uninterrupted stream of fixed width vectors.", "startOffset": 10, "endOffset": 14}, {"referenceID": 7, "context": "A healthy adult cortex is estimated to have roughly 20 billion neurons and 150 trillion synapses [10].", "startOffset": 97, "endOffset": 101}, {"referenceID": 1, "context": "7 bits per synapse [2], yielding roughly 700 trillion bits.", "startOffset": 19, "endOffset": 22}, {"referenceID": 22, "context": "By contrast, it is currently rare for artificial neural networks to have more than 100 billion floating point parameters [32], with typical networks having much fewer.", "startOffset": 121, "endOffset": 125}, {"referenceID": 10, "context": "7 million parameters [17].", "startOffset": 21, "endOffset": 25}, {"referenceID": 21, "context": "Our initial approach for compressing the data in this buffer was to train an LSTM Sequence to Sequence auto-encoder [31] to encode subsequences from this longer sequence (we used subsequences of length 7), then commit the 64-element-wide thought vectors for the subsequences to the vector-memory.", "startOffset": 116, "endOffset": 120}, {"referenceID": 2, "context": "To address this, we reduced the dimensionality of the auto-encoders in a manner reminiscent of HyperNetworks [16] and learnets [3].", "startOffset": 127, "endOffset": 130}, {"referenceID": 0, "context": "Nonparametric methods, such as Case-based reasoning [1], K-means and others (see [18] for a survey), have the ability to grow their capacity linearly with the data.", "startOffset": 52, "endOffset": 55}, {"referenceID": 11, "context": "Nonparametric methods, such as Case-based reasoning [1], K-means and others (see [18] for a survey), have the ability to grow their capacity linearly with the data.", "startOffset": 81, "endOffset": 85}, {"referenceID": 14, "context": "For example, Complementary Learning Systems [24] and Learning without Forgetting [22] both interleave training of remembered earlier data with new data.", "startOffset": 81, "endOffset": 85}, {"referenceID": 8, "context": "Other models have since built on these ideas such as growing neural gas [12], Net2Net [6], and AdaNet [8].", "startOffset": 72, "endOffset": 76}, {"referenceID": 3, "context": "Other models have since built on these ideas such as growing neural gas [12], Net2Net [6], and AdaNet [8].", "startOffset": 86, "endOffset": 89}, {"referenceID": 5, "context": "Other models have since built on these ideas such as growing neural gas [12], Net2Net [6], and AdaNet [8].", "startOffset": 102, "endOffset": 105}, {"referenceID": 13, "context": "There have been recent advances in differentiable memory, such as Neural Turing Machines [13], Memory Networks [35, 21], Differentiable Neural Computers [14], and Memory-based Deep Reinforcement Learning [23].", "startOffset": 111, "endOffset": 119}, {"referenceID": 15, "context": "There have been recent advances in differentiable memory, such as Neural Turing Machines [13], Memory Networks [35, 21], Differentiable Neural Computers [14], and Memory-based Deep Reinforcement Learning [23].", "startOffset": 204, "endOffset": 208}, {"referenceID": 6, "context": "Episodic memory has also been a component of many cognitive architectures, such as SOAR [9], LIDA [26], and CLARION [30].", "startOffset": 88, "endOffset": 91}, {"referenceID": 17, "context": "Episodic memory has also been a component of many cognitive architectures, such as SOAR [9], LIDA [26], and CLARION [30].", "startOffset": 98, "endOffset": 102}, {"referenceID": 20, "context": "Episodic memory has also been a component of many cognitive architectures, such as SOAR [9], LIDA [26], and CLARION [30].", "startOffset": 116, "endOffset": 120}, {"referenceID": 6, "context": "Our work was originally influenced specifically by SOAR [9], but extends these by using recent developments in sequence to sequence models to encode sequences as static vectors.", "startOffset": 56, "endOffset": 59}], "year": 2016, "abstractText": "The long-term memory of most connectionist systems lies entirely in the weights of the system. Since the number of weights is typically fixed, this bounds the total amount of knowledge that can be learned and stored. Though this is not normally a problem for a neural network designed for a specific task, such a bound is undesirable for a system that continually learns over an open range of domains. To address this, we describe a lifelong learning system that leverages a fast, though non-differentiable, content-addressable memory which can be exploited to encode both a long history of sequential episodic knowledge and semantic knowledge over many episodes for an unbounded number of domains. This opens the door for investigation into transfer learning, and leveraging prior knowledge that has been learned over a lifetime of experiences to new domains.", "creator": "LaTeX with hyperref package"}}}