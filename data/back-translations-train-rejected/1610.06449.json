{"id": "1610.06449", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Oct-2016", "title": "Exploiting inter-image similarity and ensemble of extreme learners for fixation prediction using deep features", "abstract": "This paper presents a novel fixation prediction and saliency modeling framework based on inter-image similarities and ensemble of Extreme Learning Machines (ELM). The proposed framework is inspired by two observations, 1) the contextual information of a scene along with low-level visual cues modulates attention, 2) the influence of scene memorability on eye movement patterns caused by the resemblance of a scene to a former visual experience. Motivated by such observations, we develop a framework that estimates the saliency of a given image using an ensemble of extreme learners, each trained on an image similar to the input image. That is, after retrieving a set of similar images for a given image, a saliency predictor is learnt from each of the images in the retrieved image set using an ELM, resulting in an ensemble. The saliency of the given image is then measured in terms of the mean of predicted saliency value by the ensemble's members.", "histories": [["v1", "Thu, 20 Oct 2016 14:55:29 GMT  (8900kb,D)", "http://arxiv.org/abs/1610.06449v1", null]], "reviews": [], "SUBJECTS": "cs.CV cs.AI", "authors": ["hamed r -tavakoli", "ali borji", "jorma laaksonen", "esa rahtu"], "accepted": false, "id": "1610.06449"}, "pdf": {"name": "1610.06449.pdf", "metadata": {"source": "CRF", "title": "Exploiting inter-image similarity and ensemble of extreme learners for fixation prediction using deep features", "authors": ["Hamed R.-Tavakolia", "Ali Borji", "Jorma Laaksonen", "Esa Rahtu"], "emails": ["hamed.r-tavakoli@aalto.fi"], "sections": [{"heading": null, "text": "This paper presents a novel framework for fixing and highlighting highlighting, based on similarities between the images and the interaction of Extreme Learning Machines (ELM).The proposed framework is based on two observations: 1) the contextual information of a scene along with low visual cues modulates attention, 2) the influence of the memory of scenes on eye movement patterns caused by the similarity of a scene to a previous visual experience.Motivated by such observations, we develop a framework that evaluates the highlighting of a particular image using an ensemble of extreme learners, each trained on an image similar to the input image. That is, after retrieving a series of similar images for a particular image, an ELM is used to determine a highlighting factor from each of the images in the retrieved image, resulting in an overall ensemble. The prominence of the given image is then measured in terms of the mean value of the predicted highlighting member."}, {"heading": "1. Introduction", "text": "This year, it is only a matter of time before agreement is reached."}, {"heading": "2. Related work", "text": "A widely accepted group of models applies feature integration theory [40] and considers a central interaction of features [45, 2, 46, 47, 48, 49, 50, 51, 52, 53, 54]. However, there are models that take into account the fundamentals of information theory [55, 56, 58, 60] that go beyond the scope of this article and its readers to consult relevant surveys [72, 73, 66, 67, 68], diffusion and random running techniques [69, 70, 71], and so on. Examining the extent of selection modeling goes beyond the scope within which relevant surveys and readers are advised. However, we briefly review some of the most relevant relevant techniques. Learning techniques are a large group of methods that establish a relationship between a feature space and human fixations."}, {"heading": "3. Saliency benefits from inter-image similarity", "text": "The main motivation behind the proposed model is that people may have similar fixation patterns when exposed to similar images. In other words, highlighting between images favors highlighting the highlighting of the highlighting in a similar image. To investigate such a claim, we create a toy problem to determine how well the highlighting map of an image predicts highlighting the highlighting in a similar image. We select a common highlighting database [75] and calculate the core [41] of the scene for each image. Subsequently, the most similar image pairs and the most dissimilar pairs were identified [86], where the value of the prediction with similar pairs is 0.54 and the value of the prediction with different image pairs is better than the predicted highlighting map of the other. The assessment shows that such a prediction scheme produces significantly different (p \u2264 0.05) mixed AUC values for each image pair [86], where the value of the prediction with similar pairs is better than the value of the prediction with different image pairs is 0.54, and the value of the prediction with different image pairs is better than the 0.5, but the results for similar pairs are 0.59."}, {"heading": "4. Saliency Model", "text": "The frame components include: 1) an image feature transformation, 2) a similar image retrieval engine and a scene repository bank, and 3) an interplay of neural highlighting (fixation) predictors. The image feature transformation performs feature extraction and generates a pool of features used by the other units in the system. In a similar image retrieval, the most similar images are found that are stored in the scene bank and correspond to a given image. Subsequently, the predictors extracted from these images are retrieved to facilitate the formation of the ensemble of saliency predictors. Later in this section, we explained the details of the components mentioned."}, {"heading": "4.1. Image feature transform", "text": "The image function transforms several characteristics from the image and passes them on to the other units. Recently, the application of characteristics we have learned from image statistics and deep Convolutionary Neural Networks (CNNs) has increased by leaps and bounds in a wide range of computational visual applications. In this thesis, we use a filter-bank approach to the use of CNNs [87] to highlight the highlighting of highlights. Therefore, we build an image pyramid and calculate the responses of the CNNs across each scale using the architecture of VGG16 [82]. To combine the folding reactions of each scale, we use an upsampling technique and match the characteristics from the last folding layer of each scale to create a feature map. In addition, we calculate the classrooms [43] from deep pipelines, that is, the probability of each of the thousand classes of ImageNet [88] is calculated using the fully connected layers of G16."}, {"heading": "4.2. Similar image retrieval & scene bank", "text": "The similar query unit retrieves the information required to build an ensemble of neural predictors from the scene bank. The scene bank contains a series of images related to the scene representation vector, consisting of class characteristics and the core descriptor, and a neural fixation prediction unit for each image. Given the vector of scene representation of an input image, known as vq, the query method retrieves the most n similar images from the group of scene vectors, V = {v1, \u00b7 \u00b7, vn \u2032}, using euclidean distance, i.e. Disti = Vq \u2212 vi, and then retrieves the neural fixation prediction units corresponding to the nimages with the smallest disti, to form the ensemble of neural fixation predictions to be discussed in Section 4.3.Figure 3 demonstrates the results of the query system. It visualizes an image and its corresponding image at the neural level, whereas the most similar image can be retrieved from the two of the fixing areas, but only at the neural level."}, {"heading": "4.3. Saliency prediction", "text": "We define the highlighting of an image in terms of characteristics and locations, i.e., Sal = p (y | x, m), where y corresponds to the highlighting at pixel level, x corresponds to image characteristics and m is the location. Assuming independence, the highlighting amounts to: Sal = p (y | x) p (y | m). (1) The p (y | x) corresponds to highlighting the highlighting of the highlighting of image characteristics and p (y | m) stands for a spatial prediction. We estimate p (y | x) using an ensemble of neural predictors and p (y | m) we learn from the information about the human gaze. Figure 4 shows the interaction of neural predictors. The interaction of neural predictors consists of several neural units with the same contributions. In the training phase, we train a neural unit for each image in the training group and store it in the scenario bank of the test phase of the same neural units calling several neural units with the same predictors."}, {"heading": "4.3.1. Neural units", "text": "The Neural Saliency Predictor uses randomly weighted single-layer feedback networks (SLFNs) to establish an assignment from feature space to aliency space. The idea of randomly weighted single-layer feedback networks (SLFNs) can be traced back to Gamba Perceptron [91], followed by others such as [92, 93]. In the Neural Saliency Predictor we take over the recent implementation of Extreme Learning Machines (ELM) [94]. ELM theory facilitates the implementation of a neural network architecture so that the hidden layer weights can be randomly selected while the output layer weights are analytically determined [95]. Motivated by better functional approximation properties of ELMs [96, 97] we use them as the primary unit of neural Saliency Prediction L."}, {"heading": "4.3.2. Learning spatial prior", "text": "In order to learn the spatial prior, p (y | m), we adapt a mixture of Gaussian data on the fixation data of the eye. We learn the spatial prior from the visual data of [90], where the number of nuclei corresponds to the number of fixation points. Spatial prior puts more emphasis on the regions that are better accepted by observers. As demonstrated in many papers highlighting spatial prioritization, the spatial prior introduces a center bias effect [98]. The same phenomenon can be observed in Figure 5, which represents the spatial prior. While there are arguments for gaining the advantage of location prioritization, we address the problem by selecting appropriate evaluation metrics and benchmarks."}, {"heading": "5. Experiments", "text": "We conduct several experiments to evaluate the model. Test databases include MIT [75], MIT300 [99] and CAT2000 [100]. The MIT database consists of 1003 images of indoor and outdoor scenes with eye movements of 15 observers. MIT300 consists of 300 natural indoor and outdoor scenes and CAT2000 consists of 4000 images divided into two groups of trains and tests, each with 2000 images in each group. CAT2000 includes 20 image categories, including action, affective, art, black and white, cartoon, fractal, indoor, outdoor, reverse, confused, line drawings, low resolution, loud, object, outdoor, nature, pattern, random, satellite, sketch and social. MIT300 and CAT2000 (test set) do not allow access to the basic truth model to allow a fair comparison."}, {"heading": "5.1. System parameters", "text": "The system parameters are the number of neural units in each ensemble, referred to as n, the number of hidden layers in each unit, L, and the attenuation factor, \u03b1. Furthermore, we learn a smoothing Gaussian core after processing, referred to as \u03c3, which is used to smooth the maps of the model. All parameters except the number of hidden nodes are learned. For each of the ensembles, the number of hidden nodes in each neural unit is fixed and equal to 20. The remaining parameters of the system are optimized in the Toronto database [56]. The tuning cost function minimizes the KL divergence between the maps of the model and the maps of the fixation density. Figure 6 shows the effect of the number of neural units in connection with the value of the attenuation factor on ensemble performance. Based on our observations, a size 10 ensemble is required to achieve an acceptable result."}, {"heading": "5.2. Performance generalization", "text": "To test the generalization of the model, we evaluate its performance using the MIT database [75]. We select the ensemble of deep neural networks (eDN) [79] as the base model based on the use of deep characteristics and SVM classifiers. However, the proposed model uses an ensemble of ELM regression units. We also evaluate several models, including AIM [55], GBVS [69], AWS [101], Judd [75] and FES [51] for comparison with conventional models. To simplify the interpretation of the evaluation, we select a subset of values that complement each other. We use mixed AUC (sAUC, an AUC metric that is robust toward center bias), similarity metric (SIM, a metric that indicates how two distributions resemble each other, 44) and the results of human scaling (sAUC, an AUC metric that is robust toward center bias), metric of similarity (SIM, a metric that shows two metallic consistencies, one with the other)."}, {"heading": "5.3. Benchmark", "text": "We therefore rely on the available benchmarks. We report on the performance of all metrics and published work carried out based on the MIT benchmark. However, in the short term, the focus will be on the most recent high-performance models. Results also include the performance of \"Infinite Human\" and \"Mean One Human\" to indicate how good a model is compared to the position of several people (above the limit) and the average performance of a human. Results at the MIT300 level summarize the performance of the model in which the proposed model is published based on NSS. MIT300 is the largest benchmark size with over 60 models at the time of this writing."}, {"heading": "6. Discussion & conclusion", "text": "We demonstrated the usefulness of the similarity of the scene in predicting conspicuousness, which is motivated by the effect of a scene's familiarity with the eye movements of the observer. However, the idea can easily be extended to the use of the observer's eye movements in task-specific models, in which a model is trained for a specific task and eye movements of experts are included. An expert approach to solving a specific task differs from that of a na\u00efve observer. Therefore, we can consider the coding of experts \"eye movements as an implicit use of expert knowledge that can be useful in scenarios of scene analysis, such as the detection of object-specific anomalies from conspicuity maps in order to reduce the search time.We introduced a conspicuousness model with the motif to exploit the effect of immediate scene memory on human perception. The proposed model uses randomly weighted neural networks as an ensemble architecture."}, {"heading": "Acknowledgement", "text": "Hamed R.-Tavakoli and Jorma Laaksonen were supported by the Finnish Center of Excellence in Computational Inference Research (COIN). The authors thank the MIT benchmark team, in particular Zoya Bylinskii, for their rapid response to benchmark requests."}], "references": [{"title": "A Computational Perspective on Visual Attention", "author": ["J.K. Tsotsos"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2011}, {"title": "Vocus: A visual attention system for object detection and goal-directed search, Ph.D", "author": ["S. Frintrop"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2006}, {"title": "A selective attention-based method for visual pattern recognition with application to handwritten digit recognition and face recognition", "author": ["A. Salah", "E. Alpaydin", "L. Akarun"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2002}, {"title": "Rapid biologically-inspired scene classification using features shared with visual attention", "author": ["C. Siagian", "L. Itti"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2007}, {"title": "Discriminant saliency, the detection of suspicious coincidences, and applications to visual recognition", "author": ["D. Gao", "S. Han", "N. Vasconcelos"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2009}, {"title": "Robust classification of objects, faces, and flowers using natural image statistics", "author": ["C. Kanan", "G. Cottrell"], "venue": "in: CVPR,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2010}, {"title": "A trainable system for object detection", "author": ["C. Papageorgiou", "T. Poggio"], "venue": "Int. J. Comput. Vision", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2000}, {"title": "Hierarchical part-based visual object categorization", "author": ["G. Bouchard", "B. Triggs"], "venue": "in: CVPR,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2005}, {"title": "Modeling the shape of the scene: A holistic representation of the spatial envelope", "author": ["A. Oliva", "A. Torralba"], "venue": "Int. J. Comput. Vision", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2001}, {"title": "Contextual priming for object detection", "author": ["A. Torralba"], "venue": "Int. J. Comput. Vision", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2003}, {"title": "Modeling search for people", "author": ["K. Ehinger", "B. Hidalgo-Sotelo", "A. Torralba", "A. Oliva"], "venue": "scenes, Vis. Cogn", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2009}, {"title": "Attentive object detection using an information theoretic saliency measure", "author": ["G. Fritz", "C. Seifert", "L. Paletta", "H. Bischof"], "venue": "in: WAPCV,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2005}, {"title": "Second-generation image-coding techniques", "author": ["M. Kunt", "A. Ikonomopoulos", "M. Kocher"], "venue": "Proc. IEEE", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1985}, {"title": "Saliency-based multifoveated mpeg compression", "author": ["N. Dhavale", "L. Itti"], "venue": "in: ISSPA,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2003}, {"title": "A novel multiresolution spatiotemporal saliency detection model and its applications in image and video compression", "author": ["C. Guo", "L. Zhang"], "venue": "IEEE Trans. Img. Proc", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2010}, {"title": "Saliency-based discriminant tracking", "author": ["V. Mahadevan", "N. Vasconcelos"], "venue": "in: CVPR,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2009}, {"title": "General object tracking with a component-based target descriptor", "author": ["S. Frintrop"], "venue": "in: ICRA,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2010}, {"title": "Adaptive object tracking by learning background context", "author": ["A. Borji", "S. Frintrop", "D. Sihite", "L. Itti"], "venue": "in: CVPRW,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2012}, {"title": "Active segmentation with fixation", "author": ["A. Mishra", "Y. Aloimonos", "C.L. Fah"], "venue": "in: CVPR,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2009}, {"title": "Saliency cuts: An automatic approach to object segmentation", "author": ["Y. Fu", "J. Cheng", "Z. Li", "H. Lu"], "venue": "in: ICPR,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2008}, {"title": "Salient object detection: From pixels to segments, Image and Vision Comput", "author": ["V. Yanulevskaya", "J. Uijlings", "J.-M. Geusebroek"], "venue": null, "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2013}, {"title": "A generic virtual content insertion system based on visual attention analysis", "author": ["H. Liu", "S. Jiang", "Q. Huang", "C. Xu"], "venue": "in: ACM MM,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2008}, {"title": "Do predictions of visual perception aid design", "author": ["R. Rosenholtz", "A. Dorai", "R. Freeman"], "venue": "ACM Trans. Appl. Percept", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2011}, {"title": "Does where you gaze on an image affect your perception of quality? applying visual attention to image quality", "author": ["A. Ninassi", "O.L. Meur", "P.L. Callet", "D. Barba"], "venue": null, "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2007}, {"title": "Saliency-based image quality assessment criterion", "author": ["Q. Ma", "L. Zhang"], "venue": "in: ICIC,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2008}, {"title": "Spatiotemporal saliency in dynamic scenes", "author": ["V. Mahadevan", "N. Vasconcelos"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2010}, {"title": "Heikkil, Temporal saliency for fast motion detection", "author": ["H.R.-Tavakoli", "J.E. Rahtu"], "venue": "in: ACCV workshops,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2013}, {"title": "Memorability of natural scenes: The role of attention", "author": ["M. Mancas", "O.L. Meur"], "venue": "in: ICIP,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2013}, {"title": "Optimal scanning for faster object detection", "author": ["N. Butko", "J.R. Movellan"], "venue": "in: CVPR,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2009}, {"title": "Performance evaluation of neuromorphic-vision object recognition", "author": ["R. Kasturi", "D. Goldgof", "R. Ekambaram", "R. Sharma", "G. Pratt", "M. Anderson", "M. Peot", "M. Aguilar", "E. Krotkov", "D. Hackett", "D. Khosla", "Y. Chen", "K. Kim", "Y. Ran", "Q. Zheng", "L. Elazary", "R. Voorhies", "D. Parks", "L. Itti"], "venue": null, "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2014}, {"title": "Accurate visual memory for previously attended objects in natural scenes", "author": ["A. Hollingworth", "J.M. Henderson"], "venue": "J. Exp. Psychol. Hum. Percept. Perform", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2002}, {"title": "Initial scene representations facilitate eye movement guidance in visual search", "author": ["M. Castelhano", "J. Henderson"], "venue": "J. Exp. Psychol. Hum. Percept. Perform", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2007}, {"title": "Recognition of visual memory recall processes using eye movement analysis", "author": ["A. Bulling", "D. Roggen"], "venue": "in: UbiComp,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2011}, {"title": "Gelade, A feature-integration theory of attention, Cognitive Psychol", "author": ["G.A.M. Treisman"], "venue": null, "citeRegEx": "40", "shortCiteRegEx": "40", "year": 1980}, {"title": "Contextual guidance of eye movements and attention in real-world scenes: the role of global features in object search", "author": ["A. Torralba", "A. Oliva", "M. Castelhano", "J. Henderson"], "venue": null, "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2006}, {"title": "Efficient object category recognition using classemes", "author": ["L. Torresani", "M. Szummer", "A. Fitzgibbon"], "venue": "in: ECCV,", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2010}, {"title": "A benchmark of computational models of saliency to predict human fixations", "author": ["T. Judd", "F. Durand", "A. Torralba"], "venue": "Massachusetts institute of technology", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2012}, {"title": "A model of saliency-based visual attention for rapid scene analysis", "author": ["L. Itti", "C. Koch", "E. Niebur"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell", "citeRegEx": "45", "shortCiteRegEx": "45", "year": 1998}, {"title": "Biologically motivated vergence control system using human-like selective attention model, Neurocomputing", "author": ["S.-B. Choi", "B.-S. Jung", "S.-W. Ban", "H. Niitsuma", "M. Lee"], "venue": null, "citeRegEx": "46", "shortCiteRegEx": "46", "year": 2006}, {"title": "A coherent computational approach to model bottom-up visual attention", "author": ["O. Le Meur", "P. Le Callet", "D. Barba", "D. Thoreau"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell", "citeRegEx": "47", "shortCiteRegEx": "47", "year": 2006}, {"title": "Saliency estimation using a non-parametric low-level vision", "author": ["N. Murray", "M. Vanrell", "X. Otazu", "C. Parraga"], "venue": null, "citeRegEx": "48", "shortCiteRegEx": "48", "year": 2011}, {"title": "The discriminant center-surround hypothesis for bottom-up saliency", "author": ["D. Gao", "V. Mahadevan", "N. Vasconcelos"], "venue": "in: NIPS,", "citeRegEx": "49", "shortCiteRegEx": "49", "year": 2007}, {"title": "Nonparametric bottom-up saliency detection by self-resemblance", "author": ["H.J. Seo", "P. Milanfar"], "venue": "in: CVPR,", "citeRegEx": "50", "shortCiteRegEx": "50", "year": 2009}, {"title": "Heikkil, Fast and efficient saliency detection using sparse sampling and kernel density estimation", "author": ["H.R.-Tavakoli", "J.E. Rahtu"], "venue": "in: SCIA,", "citeRegEx": "51", "shortCiteRegEx": "51", "year": 2011}, {"title": "Visual saliency by selective contrast, IEEE Transactions on Circuits and Systems for Video Technology", "author": ["Q. Wang", "Y. Yuan", "P. Yan"], "venue": null, "citeRegEx": "53", "shortCiteRegEx": "53", "year": 2013}, {"title": "Computational attention: Towards attentive computers", "author": ["M. Mancas"], "venue": "Ph.D. thesis, CIACO University", "citeRegEx": "57", "shortCiteRegEx": "57", "year": 2007}, {"title": "Dynamic visual attention: searching for coding length increments", "author": ["X. Hou", "L. Zhang"], "venue": "in: NIPS,", "citeRegEx": "58", "shortCiteRegEx": "58", "year": 2008}, {"title": "Visual saliency based on conditional entropy", "author": ["Y. Li", "Y. Zhou", "J. Yan", "Z. Niu", "J. Yang"], "venue": "in: ACCV,", "citeRegEx": "59", "shortCiteRegEx": "59", "year": 2010}, {"title": "Incremental sparse saliency detection", "author": ["Y. Li", "Y. Zhou", "L. Xu", "X. Yang", "J. Yang"], "venue": "in: ICIP,", "citeRegEx": "60", "shortCiteRegEx": "60", "year": 2009}, {"title": "Saliency detection: A spectral residual approach", "author": ["X. Hou", "L. Zhang"], "venue": "in: CVPR,", "citeRegEx": "61", "shortCiteRegEx": "61", "year": 2007}, {"title": "Spatio-temporal saliency detection using phase spectrum of quaternion fourier transform", "author": ["C. Guo", "Q. Ma", "L. Zhang"], "venue": "in: CVPR,", "citeRegEx": "62", "shortCiteRegEx": "62", "year": 2008}, {"title": "Biological plausibility of spectral domain approach for spatiotemporal visual saliency", "author": ["P. Bian", "L. Zhang"], "venue": "in: ICONIP,", "citeRegEx": "63", "shortCiteRegEx": "63", "year": 2008}, {"title": "Visual saliency: a biologically plausible contourlet-like frequency domain approach, Cogn. Neurodyn", "author": ["P. Bian", "L. Zhang"], "venue": null, "citeRegEx": "64", "shortCiteRegEx": "64", "year": 2010}, {"title": "Saliency detection based on frequency and spatial domain analyses", "author": ["J. Li", "M. Levine", "X. An", "H. He"], "venue": "in: BMVC,", "citeRegEx": "65", "shortCiteRegEx": "65", "year": 2011}, {"title": "Visual saliency based on scalespace analysis in the frequency domain", "author": ["J. Li", "M. Levine", "X. An", "X. Xu", "H. He"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell", "citeRegEx": "66", "shortCiteRegEx": "66", "year": 2013}, {"title": "Predicting human gaze using quaternion dct image signature saliency and face detection", "author": ["B. Schauerte", "R. Stiefelhagen"], "venue": "in: WACV,", "citeRegEx": "68", "shortCiteRegEx": "68", "year": 2012}, {"title": "Graph-based visual saliency", "author": ["J. Harel", "C. Koch", "P. Perona"], "venue": "in: NIPS,", "citeRegEx": "69", "shortCiteRegEx": "69", "year": 2007}, {"title": "Random walks on graphs to model saliency in images", "author": ["V. Gopalakrishnan", "Y. Hu", "D. Rajan"], "venue": "in: CVPR,", "citeRegEx": "70", "shortCiteRegEx": "70", "year": 2009}, {"title": "Measuring visual saliency by site entropy rate", "author": ["W. Wang", "Y. Wang", "Q. Huang", "W. Gao"], "venue": "in: CVPR,", "citeRegEx": "71", "shortCiteRegEx": "71", "year": 2010}, {"title": "Computational versus psychophysical bottom-up image saliency: A comparative evaluation study", "author": ["A. Toet"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell", "citeRegEx": "72", "shortCiteRegEx": "72", "year": 2011}, {"title": "State-of-the-art in visual attention modeling", "author": ["A. Borji", "L. Itti"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell", "citeRegEx": "73", "shortCiteRegEx": "73", "year": 2013}, {"title": "Learning to predict where humans look", "author": ["T. Judd", "K. Ehinger", "F. Durand", "A. Torralba"], "venue": "in: ICCV,", "citeRegEx": "75", "shortCiteRegEx": "75", "year": 2009}, {"title": "Saliency detection by multiple-instance learning", "author": ["Q. Wang", "Y. Yuan", "P. Yan", "X. Li"], "venue": "IEEE Trans. Cybern", "citeRegEx": "76", "shortCiteRegEx": "76", "year": 2013}, {"title": "Large-scale optimization of hierarchical features for saliency prediction in natural images", "author": ["E. Vig", "M. Dorr", "D. Cox"], "venue": "in: CVPR,", "citeRegEx": "79", "shortCiteRegEx": "79", "year": 2014}, {"title": "Deep gaze i: Boosting saliency prediction with feature maps trained on imagenet", "author": ["M. K\u00fcmmerer", "L. Theis", "M. Bethge"], "venue": "in: ICLR Workshop,", "citeRegEx": "80", "shortCiteRegEx": "80", "year": 2015}, {"title": "Shallow and deep convolutional networks for saliency prediction", "author": ["J. Pan", "K. McGuinness", "E. Sayrol", "N. O\u2019Connor", "X. Giro-i Nieto"], "venue": null, "citeRegEx": "81", "shortCiteRegEx": "81", "year": 2016}, {"title": "Very deep convolutional networks for largescale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "in: ICLR,", "citeRegEx": "82", "shortCiteRegEx": "82", "year": 2015}, {"title": "Predicting eye fixations using convolutional neural networks", "author": ["N. Liu", "J. Han", "D. Zhang", "S. Wen", "T. Liu"], "venue": "in: CVPR,", "citeRegEx": "83", "shortCiteRegEx": "83", "year": 2015}, {"title": "Salicon: Reducing the semantic gap in saliency prediction by adapting deep neural networks", "author": ["X. Huang", "C. Shen", "X. Boix", "Q. Zhao"], "venue": "in: ICCV,", "citeRegEx": "84", "shortCiteRegEx": "84", "year": 2015}, {"title": "End-to-end saliency mapping via probability distribution prediction", "author": ["S. Jetley", "N. Murray", "E. Vig"], "venue": null, "citeRegEx": "85", "shortCiteRegEx": "85", "year": 2016}, {"title": "Analysis of scores, datasets, and models in visual saliency prediction", "author": ["A. Borji", "H.R.-Tavakoli", "D.N. Sihite", "L. Itti"], "venue": "in: ICCV,", "citeRegEx": "86", "shortCiteRegEx": "86", "year": 2013}, {"title": "Deep filter banks for texture recognition and segmentation", "author": ["M. Cimpoi", "S. Maji", "A. Vedaldi"], "venue": "in: CVPR,", "citeRegEx": "87", "shortCiteRegEx": "87", "year": 2015}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "in: NIPS,", "citeRegEx": "88", "shortCiteRegEx": "88", "year": 2012}, {"title": "Predicting human gaze beyond", "author": ["J. Xu", "M. Jiang", "S. Wang", "M.S. Kankanhalli", "Q. Zhao"], "venue": "pixels, J. Vis", "citeRegEx": "90", "shortCiteRegEx": "90", "year": 2014}, {"title": "Perceptrons: an intorduction to computational geometry", "author": ["M. Minsky", "S. Papert"], "venue": null, "citeRegEx": "91", "shortCiteRegEx": "91", "year": 1969}, {"title": "Feedforward neural networks with random weights", "author": ["W. Schmidt", "M. Kraaijveld", "R. Duin"], "venue": "in: ICPR,", "citeRegEx": "92", "shortCiteRegEx": "92", "year": 1992}, {"title": "Extreme learning machine: a new learning scheme of feedforward neural networks", "author": ["G.-B. Huang", "Q.-Y. Zhu", "C.-K. Siew"], "venue": "in: IJCNN,", "citeRegEx": "94", "shortCiteRegEx": "94", "year": 2004}, {"title": "Extereme learning machine: Theory and applicatons, Neurocomput", "author": ["G.-B. Huang", "Q.-Y. Zhu", "C.-K. Siew"], "venue": null, "citeRegEx": "95", "shortCiteRegEx": "95", "year": 2006}, {"title": "Real-time learning capability of neural netwroks", "author": ["G.-B. Huang", "Q.-Y. Zhu", "C.-K. Siew"], "venue": "IEEE Trans. Neural Netw", "citeRegEx": "96", "shortCiteRegEx": "96", "year": 2006}, {"title": "The prominence of behavioural biases in eye", "author": ["B.W. Tatler", "B.T. Vincent"], "venue": "guidance, Vis. Cogn", "citeRegEx": "98", "shortCiteRegEx": "98", "year": 2009}, {"title": "Mit saliency benchmark (Jul 2016)", "author": ["Z. Bylinskii", "T. Judd", "A. Borji", "L. Itti", "F. Durand", "A. Oliva", "A. Torralba"], "venue": null, "citeRegEx": "99", "shortCiteRegEx": "99", "year": 2016}, {"title": "Cat2000: A large scale fixation dataset for boosting saliency research", "author": ["A. Borji", "L. Itti"], "venue": "in: CVPR workshops,", "citeRegEx": "100", "shortCiteRegEx": "100", "year": 2015}, {"title": "Decorrelation and distinctiveness provide with human-like saliency", "author": ["A. Garcia-Diaz", "X. Fdez-Vidal", "X. Pardo", "R. Dosil"], "venue": "in: ACIVS,", "citeRegEx": "101", "shortCiteRegEx": "101", "year": 2009}, {"title": "A deep multi-level network for saliency prediction", "author": ["M. Cornia", "L. Baraldi", "G. Serra", "R. Cucchiara"], "venue": null, "citeRegEx": "102", "shortCiteRegEx": "102", "year": 2016}, {"title": "Sclaroff, saliency detection: a Boolean map approach", "author": ["S.J. Zhang"], "venue": "in: ICCV,", "citeRegEx": "103", "shortCiteRegEx": "103", "year": 2013}], "referenceMentions": [{"referenceID": 0, "context": "It is part of the computational perspective of visual attention [1], the process of narrowing down the available visual information upon which to focus for enhanced processing.", "startOffset": 64, "endOffset": 67}, {"referenceID": 1, "context": "Computer vision community has been investigating the fixation prediction and saliency modeling extensively because of its wide range of applications, including, recognition [2, 3, 4, 5, 6], detection [7, 8, 9, 10, 11, 12], compression [13, 14, 15, 16], tracking [17, 18, 19, 20], segmentation [21, 22, 23], supperresolution [24], advertisement [25], perceptual designing [26], image quality assessment [27, 28], motion detection and background subtraction [29, 30, 31], scene memorability [32] and visual search [33, 34].", "startOffset": 173, "endOffset": 188}, {"referenceID": 2, "context": "Computer vision community has been investigating the fixation prediction and saliency modeling extensively because of its wide range of applications, including, recognition [2, 3, 4, 5, 6], detection [7, 8, 9, 10, 11, 12], compression [13, 14, 15, 16], tracking [17, 18, 19, 20], segmentation [21, 22, 23], supperresolution [24], advertisement [25], perceptual designing [26], image quality assessment [27, 28], motion detection and background subtraction [29, 30, 31], scene memorability [32] and visual search [33, 34].", "startOffset": 173, "endOffset": 188}, {"referenceID": 3, "context": "Computer vision community has been investigating the fixation prediction and saliency modeling extensively because of its wide range of applications, including, recognition [2, 3, 4, 5, 6], detection [7, 8, 9, 10, 11, 12], compression [13, 14, 15, 16], tracking [17, 18, 19, 20], segmentation [21, 22, 23], supperresolution [24], advertisement [25], perceptual designing [26], image quality assessment [27, 28], motion detection and background subtraction [29, 30, 31], scene memorability [32] and visual search [33, 34].", "startOffset": 173, "endOffset": 188}, {"referenceID": 4, "context": "Computer vision community has been investigating the fixation prediction and saliency modeling extensively because of its wide range of applications, including, recognition [2, 3, 4, 5, 6], detection [7, 8, 9, 10, 11, 12], compression [13, 14, 15, 16], tracking [17, 18, 19, 20], segmentation [21, 22, 23], supperresolution [24], advertisement [25], perceptual designing [26], image quality assessment [27, 28], motion detection and background subtraction [29, 30, 31], scene memorability [32] and visual search [33, 34].", "startOffset": 173, "endOffset": 188}, {"referenceID": 5, "context": "Computer vision community has been investigating the fixation prediction and saliency modeling extensively because of its wide range of applications, including, recognition [2, 3, 4, 5, 6], detection [7, 8, 9, 10, 11, 12], compression [13, 14, 15, 16], tracking [17, 18, 19, 20], segmentation [21, 22, 23], supperresolution [24], advertisement [25], perceptual designing [26], image quality assessment [27, 28], motion detection and background subtraction [29, 30, 31], scene memorability [32] and visual search [33, 34].", "startOffset": 173, "endOffset": 188}, {"referenceID": 6, "context": "Computer vision community has been investigating the fixation prediction and saliency modeling extensively because of its wide range of applications, including, recognition [2, 3, 4, 5, 6], detection [7, 8, 9, 10, 11, 12], compression [13, 14, 15, 16], tracking [17, 18, 19, 20], segmentation [21, 22, 23], supperresolution [24], advertisement [25], perceptual designing [26], image quality assessment [27, 28], motion detection and background subtraction [29, 30, 31], scene memorability [32] and visual search [33, 34].", "startOffset": 200, "endOffset": 221}, {"referenceID": 7, "context": "Computer vision community has been investigating the fixation prediction and saliency modeling extensively because of its wide range of applications, including, recognition [2, 3, 4, 5, 6], detection [7, 8, 9, 10, 11, 12], compression [13, 14, 15, 16], tracking [17, 18, 19, 20], segmentation [21, 22, 23], supperresolution [24], advertisement [25], perceptual designing [26], image quality assessment [27, 28], motion detection and background subtraction [29, 30, 31], scene memorability [32] and visual search [33, 34].", "startOffset": 200, "endOffset": 221}, {"referenceID": 8, "context": "Computer vision community has been investigating the fixation prediction and saliency modeling extensively because of its wide range of applications, including, recognition [2, 3, 4, 5, 6], detection [7, 8, 9, 10, 11, 12], compression [13, 14, 15, 16], tracking [17, 18, 19, 20], segmentation [21, 22, 23], supperresolution [24], advertisement [25], perceptual designing [26], image quality assessment [27, 28], motion detection and background subtraction [29, 30, 31], scene memorability [32] and visual search [33, 34].", "startOffset": 200, "endOffset": 221}, {"referenceID": 9, "context": "Computer vision community has been investigating the fixation prediction and saliency modeling extensively because of its wide range of applications, including, recognition [2, 3, 4, 5, 6], detection [7, 8, 9, 10, 11, 12], compression [13, 14, 15, 16], tracking [17, 18, 19, 20], segmentation [21, 22, 23], supperresolution [24], advertisement [25], perceptual designing [26], image quality assessment [27, 28], motion detection and background subtraction [29, 30, 31], scene memorability [32] and visual search [33, 34].", "startOffset": 200, "endOffset": 221}, {"referenceID": 10, "context": "Computer vision community has been investigating the fixation prediction and saliency modeling extensively because of its wide range of applications, including, recognition [2, 3, 4, 5, 6], detection [7, 8, 9, 10, 11, 12], compression [13, 14, 15, 16], tracking [17, 18, 19, 20], segmentation [21, 22, 23], supperresolution [24], advertisement [25], perceptual designing [26], image quality assessment [27, 28], motion detection and background subtraction [29, 30, 31], scene memorability [32] and visual search [33, 34].", "startOffset": 200, "endOffset": 221}, {"referenceID": 11, "context": "Computer vision community has been investigating the fixation prediction and saliency modeling extensively because of its wide range of applications, including, recognition [2, 3, 4, 5, 6], detection [7, 8, 9, 10, 11, 12], compression [13, 14, 15, 16], tracking [17, 18, 19, 20], segmentation [21, 22, 23], supperresolution [24], advertisement [25], perceptual designing [26], image quality assessment [27, 28], motion detection and background subtraction [29, 30, 31], scene memorability [32] and visual search [33, 34].", "startOffset": 200, "endOffset": 221}, {"referenceID": 12, "context": "Computer vision community has been investigating the fixation prediction and saliency modeling extensively because of its wide range of applications, including, recognition [2, 3, 4, 5, 6], detection [7, 8, 9, 10, 11, 12], compression [13, 14, 15, 16], tracking [17, 18, 19, 20], segmentation [21, 22, 23], supperresolution [24], advertisement [25], perceptual designing [26], image quality assessment [27, 28], motion detection and background subtraction [29, 30, 31], scene memorability [32] and visual search [33, 34].", "startOffset": 235, "endOffset": 251}, {"referenceID": 13, "context": "Computer vision community has been investigating the fixation prediction and saliency modeling extensively because of its wide range of applications, including, recognition [2, 3, 4, 5, 6], detection [7, 8, 9, 10, 11, 12], compression [13, 14, 15, 16], tracking [17, 18, 19, 20], segmentation [21, 22, 23], supperresolution [24], advertisement [25], perceptual designing [26], image quality assessment [27, 28], motion detection and background subtraction [29, 30, 31], scene memorability [32] and visual search [33, 34].", "startOffset": 235, "endOffset": 251}, {"referenceID": 14, "context": "Computer vision community has been investigating the fixation prediction and saliency modeling extensively because of its wide range of applications, including, recognition [2, 3, 4, 5, 6], detection [7, 8, 9, 10, 11, 12], compression [13, 14, 15, 16], tracking [17, 18, 19, 20], segmentation [21, 22, 23], supperresolution [24], advertisement [25], perceptual designing [26], image quality assessment [27, 28], motion detection and background subtraction [29, 30, 31], scene memorability [32] and visual search [33, 34].", "startOffset": 235, "endOffset": 251}, {"referenceID": 15, "context": "Computer vision community has been investigating the fixation prediction and saliency modeling extensively because of its wide range of applications, including, recognition [2, 3, 4, 5, 6], detection [7, 8, 9, 10, 11, 12], compression [13, 14, 15, 16], tracking [17, 18, 19, 20], segmentation [21, 22, 23], supperresolution [24], advertisement [25], perceptual designing [26], image quality assessment [27, 28], motion detection and background subtraction [29, 30, 31], scene memorability [32] and visual search [33, 34].", "startOffset": 262, "endOffset": 278}, {"referenceID": 16, "context": "Computer vision community has been investigating the fixation prediction and saliency modeling extensively because of its wide range of applications, including, recognition [2, 3, 4, 5, 6], detection [7, 8, 9, 10, 11, 12], compression [13, 14, 15, 16], tracking [17, 18, 19, 20], segmentation [21, 22, 23], supperresolution [24], advertisement [25], perceptual designing [26], image quality assessment [27, 28], motion detection and background subtraction [29, 30, 31], scene memorability [32] and visual search [33, 34].", "startOffset": 262, "endOffset": 278}, {"referenceID": 17, "context": "Computer vision community has been investigating the fixation prediction and saliency modeling extensively because of its wide range of applications, including, recognition [2, 3, 4, 5, 6], detection [7, 8, 9, 10, 11, 12], compression [13, 14, 15, 16], tracking [17, 18, 19, 20], segmentation [21, 22, 23], supperresolution [24], advertisement [25], perceptual designing [26], image quality assessment [27, 28], motion detection and background subtraction [29, 30, 31], scene memorability [32] and visual search [33, 34].", "startOffset": 262, "endOffset": 278}, {"referenceID": 18, "context": "Computer vision community has been investigating the fixation prediction and saliency modeling extensively because of its wide range of applications, including, recognition [2, 3, 4, 5, 6], detection [7, 8, 9, 10, 11, 12], compression [13, 14, 15, 16], tracking [17, 18, 19, 20], segmentation [21, 22, 23], supperresolution [24], advertisement [25], perceptual designing [26], image quality assessment [27, 28], motion detection and background subtraction [29, 30, 31], scene memorability [32] and visual search [33, 34].", "startOffset": 293, "endOffset": 305}, {"referenceID": 19, "context": "Computer vision community has been investigating the fixation prediction and saliency modeling extensively because of its wide range of applications, including, recognition [2, 3, 4, 5, 6], detection [7, 8, 9, 10, 11, 12], compression [13, 14, 15, 16], tracking [17, 18, 19, 20], segmentation [21, 22, 23], supperresolution [24], advertisement [25], perceptual designing [26], image quality assessment [27, 28], motion detection and background subtraction [29, 30, 31], scene memorability [32] and visual search [33, 34].", "startOffset": 293, "endOffset": 305}, {"referenceID": 20, "context": "Computer vision community has been investigating the fixation prediction and saliency modeling extensively because of its wide range of applications, including, recognition [2, 3, 4, 5, 6], detection [7, 8, 9, 10, 11, 12], compression [13, 14, 15, 16], tracking [17, 18, 19, 20], segmentation [21, 22, 23], supperresolution [24], advertisement [25], perceptual designing [26], image quality assessment [27, 28], motion detection and background subtraction [29, 30, 31], scene memorability [32] and visual search [33, 34].", "startOffset": 293, "endOffset": 305}, {"referenceID": 21, "context": "Computer vision community has been investigating the fixation prediction and saliency modeling extensively because of its wide range of applications, including, recognition [2, 3, 4, 5, 6], detection [7, 8, 9, 10, 11, 12], compression [13, 14, 15, 16], tracking [17, 18, 19, 20], segmentation [21, 22, 23], supperresolution [24], advertisement [25], perceptual designing [26], image quality assessment [27, 28], motion detection and background subtraction [29, 30, 31], scene memorability [32] and visual search [33, 34].", "startOffset": 344, "endOffset": 348}, {"referenceID": 22, "context": "Computer vision community has been investigating the fixation prediction and saliency modeling extensively because of its wide range of applications, including, recognition [2, 3, 4, 5, 6], detection [7, 8, 9, 10, 11, 12], compression [13, 14, 15, 16], tracking [17, 18, 19, 20], segmentation [21, 22, 23], supperresolution [24], advertisement [25], perceptual designing [26], image quality assessment [27, 28], motion detection and background subtraction [29, 30, 31], scene memorability [32] and visual search [33, 34].", "startOffset": 371, "endOffset": 375}, {"referenceID": 23, "context": "Computer vision community has been investigating the fixation prediction and saliency modeling extensively because of its wide range of applications, including, recognition [2, 3, 4, 5, 6], detection [7, 8, 9, 10, 11, 12], compression [13, 14, 15, 16], tracking [17, 18, 19, 20], segmentation [21, 22, 23], supperresolution [24], advertisement [25], perceptual designing [26], image quality assessment [27, 28], motion detection and background subtraction [29, 30, 31], scene memorability [32] and visual search [33, 34].", "startOffset": 402, "endOffset": 410}, {"referenceID": 24, "context": "Computer vision community has been investigating the fixation prediction and saliency modeling extensively because of its wide range of applications, including, recognition [2, 3, 4, 5, 6], detection [7, 8, 9, 10, 11, 12], compression [13, 14, 15, 16], tracking [17, 18, 19, 20], segmentation [21, 22, 23], supperresolution [24], advertisement [25], perceptual designing [26], image quality assessment [27, 28], motion detection and background subtraction [29, 30, 31], scene memorability [32] and visual search [33, 34].", "startOffset": 402, "endOffset": 410}, {"referenceID": 25, "context": "Computer vision community has been investigating the fixation prediction and saliency modeling extensively because of its wide range of applications, including, recognition [2, 3, 4, 5, 6], detection [7, 8, 9, 10, 11, 12], compression [13, 14, 15, 16], tracking [17, 18, 19, 20], segmentation [21, 22, 23], supperresolution [24], advertisement [25], perceptual designing [26], image quality assessment [27, 28], motion detection and background subtraction [29, 30, 31], scene memorability [32] and visual search [33, 34].", "startOffset": 456, "endOffset": 468}, {"referenceID": 26, "context": "Computer vision community has been investigating the fixation prediction and saliency modeling extensively because of its wide range of applications, including, recognition [2, 3, 4, 5, 6], detection [7, 8, 9, 10, 11, 12], compression [13, 14, 15, 16], tracking [17, 18, 19, 20], segmentation [21, 22, 23], supperresolution [24], advertisement [25], perceptual designing [26], image quality assessment [27, 28], motion detection and background subtraction [29, 30, 31], scene memorability [32] and visual search [33, 34].", "startOffset": 456, "endOffset": 468}, {"referenceID": 27, "context": "Computer vision community has been investigating the fixation prediction and saliency modeling extensively because of its wide range of applications, including, recognition [2, 3, 4, 5, 6], detection [7, 8, 9, 10, 11, 12], compression [13, 14, 15, 16], tracking [17, 18, 19, 20], segmentation [21, 22, 23], supperresolution [24], advertisement [25], perceptual designing [26], image quality assessment [27, 28], motion detection and background subtraction [29, 30, 31], scene memorability [32] and visual search [33, 34].", "startOffset": 489, "endOffset": 493}, {"referenceID": 28, "context": "Computer vision community has been investigating the fixation prediction and saliency modeling extensively because of its wide range of applications, including, recognition [2, 3, 4, 5, 6], detection [7, 8, 9, 10, 11, 12], compression [13, 14, 15, 16], tracking [17, 18, 19, 20], segmentation [21, 22, 23], supperresolution [24], advertisement [25], perceptual designing [26], image quality assessment [27, 28], motion detection and background subtraction [29, 30, 31], scene memorability [32] and visual search [33, 34].", "startOffset": 512, "endOffset": 520}, {"referenceID": 29, "context": "In many of these applications, a saliency map can facilitate the selection of a subset of regions in a scene for elaborate analysis which reduces the computation complexity and improves energy efficiency [35].", "startOffset": 204, "endOffset": 208}, {"referenceID": 30, "context": "It is shown that human relies on the prior knowledge about the scene and long-term memory as crucial components for construction and maintenance of scene representation [36].", "startOffset": 169, "endOffset": 173}, {"referenceID": 31, "context": "In a similar vein, [37] suggests that an abstract visual representation can be retained in memory upon a short exposure to a scene and this representation influences eye movements later.", "startOffset": 19, "endOffset": 23}, {"referenceID": 32, "context": "These findings have been the basis of research for measuring memorability of scenes from pure observer eye movements [39, 32], that is similar images have alike eye movement patterns and statistics.", "startOffset": 117, "endOffset": 125}, {"referenceID": 27, "context": "These findings have been the basis of research for measuring memorability of scenes from pure observer eye movements [39, 32], that is similar images have alike eye movement patterns and statistics.", "startOffset": 117, "endOffset": 125}, {"referenceID": 31, "context": "Inspired by the findings of [37, 36, 38] and scene memorability research, we incorporate the similarity of images as an influencing factor in fixation prediction.", "startOffset": 28, "endOffset": 40}, {"referenceID": 30, "context": "Inspired by the findings of [37, 36, 38] and scene memorability research, we incorporate the similarity of images as an influencing factor in fixation prediction.", "startOffset": 28, "endOffset": 40}, {"referenceID": 33, "context": ") affect saliency formation [40] and contextual information of a scene can modulate the saliency map [41, 42].", "startOffset": 28, "endOffset": 32}, {"referenceID": 34, "context": ") affect saliency formation [40] and contextual information of a scene can modulate the saliency map [41, 42].", "startOffset": 101, "endOffset": 109}, {"referenceID": 34, "context": "Then, we introduce 1) an image similarity metric using gist descriptor [41] and classemes [43], 2) a fixation prediction algorithm, using an ensemble of extreme learning machines, where for a given image, each member of the ensemble is trained with an image similar", "startOffset": 71, "endOffset": 75}, {"referenceID": 35, "context": "Then, we introduce 1) an image similarity metric using gist descriptor [41] and classemes [43], 2) a fixation prediction algorithm, using an ensemble of extreme learning machines, where for a given image, each member of the ensemble is trained with an image similar", "startOffset": 90, "endOffset": 94}, {"referenceID": 36, "context": "We report the performance of the proposed framework on MIT saliency benchmarks [44], both MIT300 and CAT2000 databases, along with evaluations on databases with publicly available ground-truth.", "startOffset": 79, "endOffset": 83}, {"referenceID": 33, "context": "A widely recognized group of models apply the feature integration theory [40] and consider a center-surround interaction of features [45, 2, 46, 47, 48, 49, 50, 51, 52, 53, 54].", "startOffset": 73, "endOffset": 77}, {"referenceID": 37, "context": "A widely recognized group of models apply the feature integration theory [40] and consider a center-surround interaction of features [45, 2, 46, 47, 48, 49, 50, 51, 52, 53, 54].", "startOffset": 133, "endOffset": 176}, {"referenceID": 1, "context": "A widely recognized group of models apply the feature integration theory [40] and consider a center-surround interaction of features [45, 2, 46, 47, 48, 49, 50, 51, 52, 53, 54].", "startOffset": 133, "endOffset": 176}, {"referenceID": 38, "context": "A widely recognized group of models apply the feature integration theory [40] and consider a center-surround interaction of features [45, 2, 46, 47, 48, 49, 50, 51, 52, 53, 54].", "startOffset": 133, "endOffset": 176}, {"referenceID": 39, "context": "A widely recognized group of models apply the feature integration theory [40] and consider a center-surround interaction of features [45, 2, 46, 47, 48, 49, 50, 51, 52, 53, 54].", "startOffset": 133, "endOffset": 176}, {"referenceID": 40, "context": "A widely recognized group of models apply the feature integration theory [40] and consider a center-surround interaction of features [45, 2, 46, 47, 48, 49, 50, 51, 52, 53, 54].", "startOffset": 133, "endOffset": 176}, {"referenceID": 41, "context": "A widely recognized group of models apply the feature integration theory [40] and consider a center-surround interaction of features [45, 2, 46, 47, 48, 49, 50, 51, 52, 53, 54].", "startOffset": 133, "endOffset": 176}, {"referenceID": 42, "context": "A widely recognized group of models apply the feature integration theory [40] and consider a center-surround interaction of features [45, 2, 46, 47, 48, 49, 50, 51, 52, 53, 54].", "startOffset": 133, "endOffset": 176}, {"referenceID": 43, "context": "A widely recognized group of models apply the feature integration theory [40] and consider a center-surround interaction of features [45, 2, 46, 47, 48, 49, 50, 51, 52, 53, 54].", "startOffset": 133, "endOffset": 176}, {"referenceID": 44, "context": "A widely recognized group of models apply the feature integration theory [40] and consider a center-surround interaction of features [45, 2, 46, 47, 48, 49, 50, 51, 52, 53, 54].", "startOffset": 133, "endOffset": 176}, {"referenceID": 45, "context": "There are models which consider the information theoretic foundations [55, 56, 57, 58, 59, 60], frequency domain aspect [61, 62, 16, 63, 64, 65, 66, 67, 68], diffusion and random walk techniques [69, 70, 71], and etc.", "startOffset": 70, "endOffset": 94}, {"referenceID": 46, "context": "There are models which consider the information theoretic foundations [55, 56, 57, 58, 59, 60], frequency domain aspect [61, 62, 16, 63, 64, 65, 66, 67, 68], diffusion and random walk techniques [69, 70, 71], and etc.", "startOffset": 70, "endOffset": 94}, {"referenceID": 47, "context": "There are models which consider the information theoretic foundations [55, 56, 57, 58, 59, 60], frequency domain aspect [61, 62, 16, 63, 64, 65, 66, 67, 68], diffusion and random walk techniques [69, 70, 71], and etc.", "startOffset": 70, "endOffset": 94}, {"referenceID": 48, "context": "There are models which consider the information theoretic foundations [55, 56, 57, 58, 59, 60], frequency domain aspect [61, 62, 16, 63, 64, 65, 66, 67, 68], diffusion and random walk techniques [69, 70, 71], and etc.", "startOffset": 70, "endOffset": 94}, {"referenceID": 49, "context": "There are models which consider the information theoretic foundations [55, 56, 57, 58, 59, 60], frequency domain aspect [61, 62, 16, 63, 64, 65, 66, 67, 68], diffusion and random walk techniques [69, 70, 71], and etc.", "startOffset": 120, "endOffset": 156}, {"referenceID": 50, "context": "There are models which consider the information theoretic foundations [55, 56, 57, 58, 59, 60], frequency domain aspect [61, 62, 16, 63, 64, 65, 66, 67, 68], diffusion and random walk techniques [69, 70, 71], and etc.", "startOffset": 120, "endOffset": 156}, {"referenceID": 14, "context": "There are models which consider the information theoretic foundations [55, 56, 57, 58, 59, 60], frequency domain aspect [61, 62, 16, 63, 64, 65, 66, 67, 68], diffusion and random walk techniques [69, 70, 71], and etc.", "startOffset": 120, "endOffset": 156}, {"referenceID": 51, "context": "There are models which consider the information theoretic foundations [55, 56, 57, 58, 59, 60], frequency domain aspect [61, 62, 16, 63, 64, 65, 66, 67, 68], diffusion and random walk techniques [69, 70, 71], and etc.", "startOffset": 120, "endOffset": 156}, {"referenceID": 52, "context": "There are models which consider the information theoretic foundations [55, 56, 57, 58, 59, 60], frequency domain aspect [61, 62, 16, 63, 64, 65, 66, 67, 68], diffusion and random walk techniques [69, 70, 71], and etc.", "startOffset": 120, "endOffset": 156}, {"referenceID": 53, "context": "There are models which consider the information theoretic foundations [55, 56, 57, 58, 59, 60], frequency domain aspect [61, 62, 16, 63, 64, 65, 66, 67, 68], diffusion and random walk techniques [69, 70, 71], and etc.", "startOffset": 120, "endOffset": 156}, {"referenceID": 54, "context": "There are models which consider the information theoretic foundations [55, 56, 57, 58, 59, 60], frequency domain aspect [61, 62, 16, 63, 64, 65, 66, 67, 68], diffusion and random walk techniques [69, 70, 71], and etc.", "startOffset": 120, "endOffset": 156}, {"referenceID": 55, "context": "There are models which consider the information theoretic foundations [55, 56, 57, 58, 59, 60], frequency domain aspect [61, 62, 16, 63, 64, 65, 66, 67, 68], diffusion and random walk techniques [69, 70, 71], and etc.", "startOffset": 120, "endOffset": 156}, {"referenceID": 56, "context": "There are models which consider the information theoretic foundations [55, 56, 57, 58, 59, 60], frequency domain aspect [61, 62, 16, 63, 64, 65, 66, 67, 68], diffusion and random walk techniques [69, 70, 71], and etc.", "startOffset": 195, "endOffset": 207}, {"referenceID": 57, "context": "There are models which consider the information theoretic foundations [55, 56, 57, 58, 59, 60], frequency domain aspect [61, 62, 16, 63, 64, 65, 66, 67, 68], diffusion and random walk techniques [69, 70, 71], and etc.", "startOffset": 195, "endOffset": 207}, {"referenceID": 58, "context": "There are models which consider the information theoretic foundations [55, 56, 57, 58, 59, 60], frequency domain aspect [61, 62, 16, 63, 64, 65, 66, 67, 68], diffusion and random walk techniques [69, 70, 71], and etc.", "startOffset": 195, "endOffset": 207}, {"referenceID": 59, "context": "Investigating the extent of saliency modeling approaches is beyond the scope of this article and readers are advised to consult relevant surveys [72, 73].", "startOffset": 145, "endOffset": 153}, {"referenceID": 60, "context": "Investigating the extent of saliency modeling approaches is beyond the scope of this article and readers are advised to consult relevant surveys [72, 73].", "startOffset": 145, "endOffset": 153}, {"referenceID": 61, "context": "In [75], a linear SVM classifier is used to establish a relation between three channels of low- (intensity, color, etc), mid- (horizon line)", "startOffset": 3, "endOffset": 7}, {"referenceID": 62, "context": "In a similar vein, [76] employs multiple-instance learning.", "startOffset": 19, "endOffset": 23}, {"referenceID": 63, "context": "Ensembles of Deep Networks (eDN) [79] adopts the neural filters learned during image classification task by deep neural networks and learns a classifier to perform fixation prediction.", "startOffset": 33, "endOffset": 37}, {"referenceID": 61, "context": "eDN can be considered an extension to [75] in which the features are obtained from layers of a deep neural network.", "startOffset": 38, "endOffset": 42}, {"referenceID": 64, "context": "Deep Gaze I [80] utilizes CNNs for the fixation prediction task by treating saliency prediction as point processing.", "startOffset": 12, "endOffset": 16}, {"referenceID": 63, "context": "Despite this model is justified differently than [79] and [75], in practice, it boils down to the same framework.", "startOffset": 49, "endOffset": 53}, {"referenceID": 61, "context": "Despite this model is justified differently than [79] and [75], in practice, it boils down to the same framework.", "startOffset": 58, "endOffset": 62}, {"referenceID": 65, "context": "SalNet [81] is another technique that employs a CNN-based architecture, where the last layer is a deconvolution.", "startOffset": 7, "endOffset": 11}, {"referenceID": 66, "context": "The first convolution layers are initialized by the VGG16 [82] and the deconvolution is learnt by fine-tuning the architecture for fixation prediction.", "startOffset": 58, "endOffset": 62}, {"referenceID": 67, "context": "Multiresolution CNN (Mr-CNN) [83] designs a deep CNN-based technique to discriminate image patches centered on fixations from non-fixated image patches at multiple resolutions.", "startOffset": 29, "endOffset": 33}, {"referenceID": 68, "context": "SALICON [84] develops a model by fine-tuning the convolutional neural network, trained on ImageNet, using saliency evaluation metrics as objective functions.", "startOffset": 8, "endOffset": 12}, {"referenceID": 65, "context": "a probability map in terms of a regression problem [81, 84, 85].", "startOffset": 51, "endOffset": 63}, {"referenceID": 68, "context": "a probability map in terms of a regression problem [81, 84, 85].", "startOffset": 51, "endOffset": 63}, {"referenceID": 69, "context": "a probability map in terms of a regression problem [81, 84, 85].", "startOffset": 51, "endOffset": 63}, {"referenceID": 61, "context": "We choose a common saliency database [75] and computed the gist [41] of the scene for each image.", "startOffset": 37, "endOffset": 41}, {"referenceID": 34, "context": "We choose a common saliency database [75] and computed the gist [41] of the scene for each image.", "startOffset": 64, "endOffset": 68}, {"referenceID": 70, "context": "05) shuffled AUC scores [86] where the score of prediction using similar pairs is 0.", "startOffset": 24, "endOffset": 28}, {"referenceID": 71, "context": "In this work, we adopt a filter-bank approach to the use of CNNs [87] for saliency prediction.", "startOffset": 65, "endOffset": 69}, {"referenceID": 66, "context": "We, thus, build an image pyramid and compute the CNNs\u2019 responses over each scale using the architecture of VGG16 [82] .", "startOffset": 113, "endOffset": 117}, {"referenceID": 35, "context": "Furthermore, we compute the classemes [43] from deep pipeline, that is, the probability of each of the one thousand classes of ImageNet [88] is computed using the fully-connected layers of the VGG16.", "startOffset": 38, "endOffset": 42}, {"referenceID": 72, "context": "Furthermore, we compute the classemes [43] from deep pipeline, that is, the probability of each of the one thousand classes of ImageNet [88] is computed using the fully-connected layers of the VGG16.", "startOffset": 136, "endOffset": 140}, {"referenceID": 8, "context": "The classemes are complemented by the low-level scene representation to make the gist of the scene [9].", "startOffset": 99, "endOffset": 102}, {"referenceID": 34, "context": "The classemes and low-level scene features of [41] build a spatial representation of the outside world that is rich enough to convey the meaning of a scene as envisioned in [89].", "startOffset": 46, "endOffset": 50}, {"referenceID": 61, "context": "The query images are from [75] and the closest match is from [90].", "startOffset": 26, "endOffset": 30}, {"referenceID": 73, "context": "The query images are from [75] and the closest match is from [90].", "startOffset": 61, "endOffset": 65}, {"referenceID": 74, "context": "The idea of randomly-weighted single-hidden-layer feedforward networks (SLFNs) can be traced back to the Gamba perceptron [91] followed by others like [92, 93].", "startOffset": 122, "endOffset": 126}, {"referenceID": 75, "context": "The idea of randomly-weighted single-hidden-layer feedforward networks (SLFNs) can be traced back to the Gamba perceptron [91] followed by others like [92, 93].", "startOffset": 151, "endOffset": 159}, {"referenceID": 76, "context": "In the neural saliency predictor, we adopt the recent implementation of Extreme Learning Machines (ELM) [94].", "startOffset": 104, "endOffset": 108}, {"referenceID": 77, "context": "The theory of ELM facilitates the implementation of a neural network architecture such that the hidden layer weights can be chosen randomly meanwhile the output layer weights are determined analytically [95].", "startOffset": 203, "endOffset": 207}, {"referenceID": 78, "context": "Motivated by better function approximation properties of ELMs [96, 97], we employ them as the primary entity of the neural saliency prediction.", "startOffset": 62, "endOffset": 70}, {"referenceID": 73, "context": "We learn the spatial prior using the gaze data of [90], where the number of kernels corresponds to the number of fixation points.", "startOffset": 50, "endOffset": 54}, {"referenceID": 79, "context": "As demonstrated in many saliency research papers, the spatial prior introduces a center-bias effect [98].", "startOffset": 100, "endOffset": 104}, {"referenceID": 73, "context": "Figure 5: Spatial prior learnt from [90].", "startOffset": 36, "endOffset": 40}, {"referenceID": 61, "context": "The test databases include MIT [75], MIT300 [99], and CAT2000 [100].", "startOffset": 31, "endOffset": 35}, {"referenceID": 80, "context": "The test databases include MIT [75], MIT300 [99], and CAT2000 [100].", "startOffset": 44, "endOffset": 48}, {"referenceID": 81, "context": "The test databases include MIT [75], MIT300 [99], and CAT2000 [100].", "startOffset": 62, "endOffset": 67}, {"referenceID": 73, "context": "The first is trained on the OSIE database [90] and the latter is trained using the training set of CAT2000.", "startOffset": 42, "endOffset": 46}, {"referenceID": 61, "context": "Performance generalization To test the generalization of the model, we evaluate its performance using the MIT database [75].", "startOffset": 119, "endOffset": 123}, {"referenceID": 63, "context": "We choose the ensemble of deep neural networks (eDN) [79] as a baseline model because of the use of deep features and SVM classifiers.", "startOffset": 53, "endOffset": 57}, {"referenceID": 56, "context": "We also evaluate several models including, AIM [55], GBVS [69], AWS [101], Judd [75], and FES [51] for the sake of comparison with traditional models.", "startOffset": 58, "endOffset": 62}, {"referenceID": 82, "context": "We also evaluate several models including, AIM [55], GBVS [69], AWS [101], Judd [75], and FES [51] for the sake of comparison with traditional models.", "startOffset": 68, "endOffset": 73}, {"referenceID": 61, "context": "We also evaluate several models including, AIM [55], GBVS [69], AWS [101], Judd [75], and FES [51] for the sake of comparison with traditional models.", "startOffset": 80, "endOffset": 84}, {"referenceID": 43, "context": "We also evaluate several models including, AIM [55], GBVS [69], AWS [101], Judd [75], and FES [51] for the sake of comparison with traditional models.", "startOffset": 94, "endOffset": 98}, {"referenceID": 36, "context": "We employ shuffled AUC (sAUC, an AUC metric that is robust towards center bias), similarity metric (SIM, a metric indicating how two distributions resemble each other [44]), and normalized scanpath saliency (NSS, a metric to measure consistency with human fixation locations).", "startOffset": 167, "endOffset": 171}, {"referenceID": 70, "context": "NSS and sAUC scores are utilized in [86], which we borrow part of the scores from, and complement them with the SIM score.", "startOffset": 36, "endOffset": 40}, {"referenceID": 63, "context": "Figure 7: Performance generalization: the performance of the proposed model compared to traditional models and eDN [79] as a baseline model.", "startOffset": 115, "endOffset": 119}, {"referenceID": 68, "context": "29 SALICON [84] 0.", "startOffset": 11, "endOffset": 15}, {"referenceID": 69, "context": "12 PDP [85] 0.", "startOffset": 7, "endOffset": 11}, {"referenceID": 83, "context": "05 ML-Net [102] 0.", "startOffset": 10, "endOffset": 15}, {"referenceID": 65, "context": "65 SalNet [81] 0.", "startOffset": 10, "endOffset": 14}, {"referenceID": 84, "context": "51 BMS [103] 0.", "startOffset": 7, "endOffset": 12}, {"referenceID": 67, "context": "41 Mr-CNN [83] 0.", "startOffset": 10, "endOffset": 14}, {"referenceID": 63, "context": "37 eDN [79] 0.", "startOffset": 7, "endOffset": 11}, {"referenceID": 84, "context": "The proposed model, ensembleCAT2k, ranks similarly with BMS [103] at the top of the ranking.", "startOffset": 60, "endOffset": 65}, {"referenceID": 84, "context": "67 BMS [103] 0.", "startOffset": 7, "endOffset": 12}, {"referenceID": 84, "context": "62 FES [103] 0.", "startOffset": 7, "endOffset": 12}, {"referenceID": 61, "context": "54 Judd [75] 0.", "startOffset": 8, "endOffset": 12}, {"referenceID": 63, "context": "30 eDN [79] 0.", "startOffset": 7, "endOffset": 11}], "year": 2016, "abstractText": "This paper presents a novel fixation prediction and saliency modeling framework based on inter-image similarities and ensemble of Extreme Learning Machines (ELM). The proposed framework is inspired by two observations, 1) the contextual information of a scene along with low-level visual cues modulates attention, 2) the influence of scene memorability on eye movement patterns caused by the resemblance of a scene to a former visual experience. Motivated by such observations, we develop a framework that estimates the saliency of a given image using an ensemble of extreme learners, each trained on an image similar to the input image. That is, after retrieving a set of similar images for a given image, a saliency predictor is learnt from each of the images in the retrieved image set using an ELM, resulting in an ensemble. The saliency of the given image is then measured in terms of the mean of predicted saliency value by the ensemble\u2019s members.", "creator": "LaTeX with hyperref package"}}}