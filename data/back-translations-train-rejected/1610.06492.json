{"id": "1610.06492", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Oct-2016", "title": "Utilization of Deep Reinforcement Learning for saccadic-based object visual search", "abstract": "The paper focuses on the problem of learning saccades enabling visual object search. The developed system combines reinforcement learning with a neural network for learning to predict the possible outcomes of its actions. We validated the solution in three types of environment consisting of (pseudo)-randomly generated matrices of digits. The experimental verification is followed by the discussion regarding elements required by systems mimicking the fovea movement and possible further research directions.", "histories": [["v1", "Thu, 20 Oct 2016 16:34:08 GMT  (1814kb,D)", "http://arxiv.org/abs/1610.06492v1", "Paper submitted to special session on Machine Intelligence organized during 23rd International AUTOMATION Conference"]], "COMMENTS": "Paper submitted to special session on Machine Intelligence organized during 23rd International AUTOMATION Conference", "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["tomasz kornuta", "kamil rocki"], "accepted": false, "id": "1610.06492"}, "pdf": {"name": "1610.06492.pdf", "metadata": {"source": "CRF", "title": "Utilization of Deep Reinforcement Learning for saccadic-based object visual search", "authors": ["Tomasz Kornuta", "Kamil Rocki"], "emails": ["tkornut@us.ibm.com", "kmrocki@us.ibm.com"], "sections": [{"heading": null, "text": "Keywords: visual search, saccades, q-learning, neural networks"}, {"heading": "1 Introduction", "text": "Humans do not observe a scene in a passive, fixed, steady manner. Instead, their eyes move around it, actively locating and analyzing interesting parts of the scene, constantly building their mental, three-dimensional model. These rapid jerky movements of the eyeball are known as saccades and serve to see by redirecting the fovea along with its visual axis to a new region. (b) (c) (d) Fig. 1. The desired behavior of a system that realizes Saccadian principles: (a) image classification / semantic description of the scene, (b) visual object search, (c) an exemplary labyrinth of digits (20x20) with indicated: target (red cross), current agent pose (white circle) and saccadic paths (green line) along with (d) the current agent observation system (7x7) ar Xiv: 161 0.06 cv 2ct [2ct] interest."}, {"heading": "2 Related works", "text": "Reinforcement Learning (RL), as a general method of injecting goal and learning goal-oriented behaviors, has had several successful applications in various fields. For example, since robots have effectors and receptors that allow them to interact with their environment, RL was originally at the core of a vaist of successful robot applications [2], including such challenging tasks as learning helicopter maneuvers [3] or optimizing a humanoid robot passage [4]. Since Reinforcement Learning is based at its core on the idea of finding measures that are optimal for a given state, pure RL-based systems have problems with modeling a huge number of states (or more generally, with high-dimensional inputs). Therefore, humans began to combine RL with neural networks (NNs), using the latter as (state) approximators. Such a combination has a long history."}, {"heading": "3 Saccadic-based visual search", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Deep Reinforcement Learning primer", "text": "Reinforcement Learning aims at learning strategies that control the actions of an agent who interacts with an unknown environment [13]. Such an environment is often formalized as a Markov decision-making process (MDP), which is described by a quadruple (S, A, P, R). In each step of time, t, while the actor who is in a state in which he is in an S, selects and executes an action. Subsequently, the actor receives a reward rt + 1 (R) in the next step and finds himself in a new state. P represents the state transitional model of the environment. The goal of the actor is to maximize the expected discounted reward that he executes by giving k = 0 \u03b3krt + k + 1, indicating the discounts. Our approach is based on Q-Learning [14], a temporary difference (TD) learning method for estimating state action values called Q-values by actualizing the current Wst-Q."}, {"heading": "3.2 System for learning saccadic-based visual search", "text": "The data flow diagram of our system (actor implementing visual search) is shown in Fig. 2. There are two main components of the system: actor, responsible for interacting with the environment, and learner, responsible for learning from the experiences gained. Actor and learner share the parameters neural network and experience memory. The general principle of the functioning of the actor is as follows: The current state of the environment si (consisting of a single observation, in our case an image field) is passed in iteration i to the neural network, which predicts the Q values obtained by Qi after performing four possible types of action: A = {N, E, S, W}, i.e. N (north), E (south), S (south) and W (west). This allows the actor to decide which action to perform next (an epsilonous action selection) and which action to perform, leading to the transition to the next state of the environment."}, {"heading": "4 Experimental verification", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Partially observable environment: the maze of digits", "text": "The experimental verification of our solution was carried out in a controlled environment known as \"a labyrinth of digits.\" Such a labyrinth is simply a matrix of digits (integers) from 0 to 9, with 9 indicating the target we want to find or reach. These labyrinth can be presented as simplified, single-channel images where the goal is to find a specific characteristic area of the image. In Fig. 1c, the color of the cell indicates the corresponding digit, the red cross on a white field indicates the target, while the current position of the agent is represented by a white circle. The current agent path is marked with green lines connecting the successive grid cells, with the brightness indicating the \"age\" of a particular step - the darker the color the older the step. Since it is assumed that the system can observe only part of the environment (Fig. 1d), which forms the problem as a partially observable Markov decision process (MDP), the three objects have been completely reformulated with its structure (the three)."}, {"heading": "4.2 Random mazes of digits", "text": "First, we analyzed convergence in the vicinity of completely random labyrinths of digits. In each experiment, we created a single labyrinth, while we placed the agent in a different, random position at the beginning of each episode. Since these labyrinth types do not have a clear structure or tendency indicating how close the agent's target is, the main objective of these experiments was to check how good the solution is in storing the actions to be performed in different sections of the environment, and whether the system can learn correctly (approximate Q values in different states). In Fig. 4 and Fig. 5, we presented the exemplary results for labyrinths of size 10x10 and 20x20, and observation windows of size 3x3 and 5x5, respectively. The \"current\" value plotted represents the ratio between the optimal (i.e. shortest) path from the initial agent position to the target and the length of a saccadic path, based on the number of steps in each case, versus the average path of the number of steps."}, {"heading": "4.3 Mazes of digits forming a circle", "text": "Natural images are not random - the majority of our environment has some kind of structure (forests consist of trees, there are beaches near oceans and seas, houses are surrounded by roads that lead to them, etc.) For this reason, we conducted a series of experiments with random labyrinths that have some kind of underlying structure. Firstly, we created labyrinths with values that decrease with the growth of the distance to the target. Furthermore, unlike previous experiments in this case, we placed the agent in a newly generated, completely random (i.e. unique) labyrinth. The results obtained (fig. 6) indicate that the system was able to generalize across different labyrinths and learn to follow the color trend."}, {"heading": "4.4 Mazes of digits forming a path", "text": "Next, we validated the system on the random labyrinths with another underlying structure, i.e., digits that formed a pathway to the target. Experiments were conducted in a similar setup to the experiments with random lattice worlds, i.e. in each episode a completely new labyrinth was generated and the agent started from a completely different, random position. Again, the system was able to learn to follow this kind of structural tendency, but the variation in current values is much higher than before. This is because in many cases the system had to take additional steps, first to find the path and then to begin to follow it. For example, although the sacadian paths in Fig. 7a and Fig. 7a seem perfectly reasonable, the resulting \"current\" values were 0.56 and 0.76, respectively."}, {"heading": "5 Discussion of results and future works", "text": "There are three essential elements of the Saccadic system that enable people to carry out such a search, namely: the parallel recognition of potential destinations, the integration of information about fixations, and the selection of the next fixation location. In this paper, we have focused on the problem of Saccadic visual object search, narrowing the scope of our research to the third element. The solution developed, which combines amplification of learning with a neural network, was able to learn in a way that was randomly generated, creating invisible environments."}], "references": [{"title": "Eye movements during perception of complex objects", "author": ["A.L. Yarbus"], "venue": "Eye movement and vision. Plenum Press, New York", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1967}, {"title": "Reinforcement learning in robotics: A survey", "author": ["J. Kober", "J.A. Bagnell", "J. Peters"], "venue": "The International Journal of Robotics Research", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2013}, {"title": "An application of reinforcement learning to aerobatic helicopter flight", "author": ["P. Abbeel", "A. Coates", "M. Quigley", "A.Y. Ng"], "venue": "Advances in neural information processing systems 19", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2007}, {"title": "Reinforcement learning with experience replay for model-free humanoid walking optimization", "author": ["P. Wawrzy\u0144ski"], "venue": "International Journal of Humanoid Robotics 11(03)", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2014}, {"title": "Temporal difference learning and td-gammon", "author": ["G. Tesauro"], "venue": "Communications of the ACM 38(3)", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1995}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "Advances in neural information processing systems.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2012}, {"title": "Deep learning", "author": ["Y. LeCun", "Y. Bengio", "G. Hinton"], "venue": "Nature 521(7553)", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2015}, {"title": "Evolving large-scale neural networks for vision-based reinforcement learning", "author": ["J. Koutn\u00edk", "G. Cuccu", "J. Schmidhuber", "F. Gomez"], "venue": "Proceedings of the 15th annual conference on Genetic and evolutionary computation, ACM", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2013}, {"title": "Human-level control through deep reinforcement learning", "author": ["V. Mnih", "K. Kavukcuoglu", "D. Silver", "A.A. Rusu", "J. Veness", "M.G. Bellemare", "A. Graves", "M. Riedmiller", "A.K. Fidjeland", "G Ostrovski"], "venue": "Nature 518(7540)", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2015}, {"title": "Mastering the game of go with deep neural networks and tree search", "author": ["D. Silver", "A. Huang", "C.J. Maddison", "A. Guez", "L. Sifre", "G. Van Den Driessche", "J. Schrittwieser", "I. Antonoglou", "V. Panneershelvam", "M Lanctot"], "venue": "Nature 529(7587)", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2016}, {"title": "End-to-end training of deep visuomotor policies", "author": ["S. Levine", "C. Finn", "T. Darrell", "P. Abbeel"], "venue": "Journal of Machine Learning Research 17(39)", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2016}, {"title": "Deep reinforcement learning for robotic manipulation", "author": ["S. Gu", "E. Holly", "T. Lillicrap", "S. Levine"], "venue": "arXiv preprint arXiv:1610.00633", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2016}, {"title": "Reinforcement learning: An introduction", "author": ["R.S. Sutton", "A.G. Barto"], "venue": "MIT press Cambridge", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1998}, {"title": "Learning from delayed rewards", "author": ["Watkins", "C.J.C.H."], "venue": "PhD thesis, University of Cambridge England", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1989}, {"title": "Optimal eye movement strategies in visual search", "author": ["J. Najemnik", "W.S. Geisler"], "venue": "Nature 434(7031)", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2005}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural computation 9(8)", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1997}, {"title": "A simple way to initialize recurrent networks of rectified linear units", "author": ["Q.V. Le", "N. Jaitly", "G.E. Hinton"], "venue": "arXiv preprint arXiv:1504.00941", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2015}, {"title": "Recurrent models of visual attention", "author": ["V. Mnih", "N. Heess", "A Graves"], "venue": "Advances in Neural Information Processing Systems.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2014}, {"title": "Neural turing machines", "author": ["A. Graves", "G. Wayne", "I. Danihelka"], "venue": "arXiv preprint arXiv:1410.5401", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2014}, {"title": "Surprisal-driven feedback in recurrent networks", "author": ["K.M. Rocki"], "venue": "arXiv preprint arXiv:1608.06027", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 0, "context": "Human eyes fixate mainly on certain elements of the scene that carry or might carry essential or usefull information, whereas saccadic movements depend not only on the objects present in the scene, but also on the task the observer has to achieve [1].", "startOffset": 247, "endOffset": 250}, {"referenceID": 1, "context": "For example, as robots possess effectors and receptors enabling them to interact with their environment, RL was the core of a vaist of successfull robotic aplications [2], including such challenging tasks as learning of aerobatic helicopter maneuvers [3] or optimization of a humanoid robot gait [4].", "startOffset": 167, "endOffset": 170}, {"referenceID": 2, "context": "For example, as robots possess effectors and receptors enabling them to interact with their environment, RL was the core of a vaist of successfull robotic aplications [2], including such challenging tasks as learning of aerobatic helicopter maneuvers [3] or optimization of a humanoid robot gait [4].", "startOffset": 251, "endOffset": 254}, {"referenceID": 3, "context": "For example, as robots possess effectors and receptors enabling them to interact with their environment, RL was the core of a vaist of successfull robotic aplications [2], including such challenging tasks as learning of aerobatic helicopter maneuvers [3] or optimization of a humanoid robot gait [4].", "startOffset": 296, "endOffset": 299}, {"referenceID": 4, "context": "One of the very first examples is TD-Gammon [5], where weights of a neural net were updated according to a learning rule being a form of temporaldifference (TD) learning.", "startOffset": 44, "endOffset": 47}, {"referenceID": 5, "context": "Recent progress in the end-to-end training of multi-layer neural networks [6] (and in the so called Deep Learning [7]) once again attracted the attention of researchers to neural nets.", "startOffset": 74, "endOffset": 77}, {"referenceID": 6, "context": "Recent progress in the end-to-end training of multi-layer neural networks [6] (and in the so called Deep Learning [7]) once again attracted the attention of researchers to neural nets.", "startOffset": 114, "endOffset": 117}, {"referenceID": 7, "context": "In [8] authors presented a system combining neural networks with evolutionary programming and reinforcement learning, that was able to learn policies in an end-to-end manner straight from images, enabling it to drive a virtual racing car in a TORCS simulator.", "startOffset": 3, "endOffset": 6}, {"referenceID": 8, "context": "such a combination is DQN (Deep-Q-Network) [9].", "startOffset": 43, "endOffset": 46}, {"referenceID": 9, "context": "Yet another application of CNN combined with RL (and additionally supplemented with Monte Carlo Tree Search) is AlphaGo [10], a system the managed to beat both the European (Fan Hui) and World (Lee Se-dol) Go Champions.", "startOffset": 120, "endOffset": 124}, {"referenceID": 10, "context": "[11] presented a setup consisting of a battery of 14 robotic manipulators learning simultaneously to grasp different objects and moreover, exchanging gained experience by sharing the policy network, whereas in [12] the authors used a similar setup for learning to robustly open doors.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[11] presented a setup consisting of a battery of 14 robotic manipulators learning simultaneously to grasp different objects and moreover, exchanging gained experience by sharing the policy network, whereas in [12] the authors used a similar setup for learning to robustly open doors.", "startOffset": 210, "endOffset": 214}, {"referenceID": 12, "context": "Reinforcement Learning aims at learning policies controlling actions of an agent interacting with an unknown environment [13].", "startOffset": 121, "endOffset": 125}, {"referenceID": 13, "context": "Our approach is based on Q-learning [14], a Temporal Difference (TD) learning method for estimation of state action values called Q-values by updating the current estimate of Q(st, at) (Qt in short) towards the reward Rt and estimated utility of the resulting state st+1:", "startOffset": 36, "endOffset": 40}, {"referenceID": 8, "context": "In our system we have decided to combine Reinforcement Learning with a (multi-layer) neural network and use the latter for approximation of Q-values, similarly to DQN [9].", "startOffset": 167, "endOffset": 170}, {"referenceID": 14, "context": "According to [15], there are three major elements of saccadic system that enable the humans to perform visual search so well, namely: parallel detection (responsible for finding potential target locations), integration of information accross fixations, and the selection of the next fixation location.", "startOffset": 13, "endOffset": 17}, {"referenceID": 15, "context": "One possible solution is to use Recurrent Neural Networks (RNNs) such as Long-Short Term Memory (LSTM) [16].", "startOffset": 103, "endOffset": 107}, {"referenceID": 16, "context": "For example, in [17] the authors successfully used RNNs for classification of MNIST digits represented as a sequence of pixels feed to the network one pixel at a time.", "startOffset": 16, "endOffset": 20}, {"referenceID": 17, "context": "in [18] the authors used RNN emitting the location of the next image patch to be analysed.", "startOffset": 3, "endOffset": 7}, {"referenceID": 18, "context": "More effective analysis concerns also the avoidance of already visited places \u2013 one possible solution is to use Neural Turing Machine (NTM) [19], i.", "startOffset": 140, "endOffset": 144}, {"referenceID": 17, "context": "in [18] the RNN was feed (aside of multi-scale image patch called glimpse) with a position encoded be a simple NN called location network.", "startOffset": 3, "endOffset": 7}, {"referenceID": 19, "context": "The recent results on utilization of surprisal for learning of long sequences [20] prove that it is a good idea.", "startOffset": 78, "endOffset": 82}], "year": 2016, "abstractText": "The paper focuses on the problem of learning saccades enabling visual object search. The developed system combines reinforcement learning with a neural network for learning to predict the possible outcomes of its actions. We validated the solution in three types of environment consisting of (pseudo)-randomly generated matrices of digits. The experimental verification is followed by the discussion regarding elements required by systems mimicking the fovea movement and possible further research directions.", "creator": "LaTeX with hyperref package"}}}