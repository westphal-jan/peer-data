{"id": "1610.06510", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Oct-2016", "title": "Learning variable length units for SMT between related languages via Byte Pair Encoding", "abstract": "We explore the use of segments learnt using Byte Pair Encoding (referred to as BPE units) as basic units for statistical machine translation between related languages and compare it with orthographic syllables, which are currently the best performing basic units for this translation task. BPE identifies the most frequent character sequences as basic units, while orthographic syllables are linguistically motivated pseudo-syllables. We show that BPE units outperform orthographic syllables as units of translation, showing up to 11% increase in BLEU scores. In addition, BPE can be applied to any writing system, while orthographic syllables can be used only for languages whose writing systems use vowel representations. We show that BPE units outperform word and morpheme level units for translation involving languages like Urdu, Japanese whose writing systems do not use vowels (either completely or partially). Across many language pairs, spanning multiple language families and types of writing systems, we show that translation with BPE segments outperforms orthographic syllables, especially for morphologically rich languages.", "histories": [["v1", "Thu, 20 Oct 2016 17:32:32 GMT  (124kb,D)", "http://arxiv.org/abs/1610.06510v1", "A earlier version of this paper is under review at EACL 2107. (10 pages, 2 figures, 9 tables)"], ["v2", "Mon, 12 Jun 2017 17:22:45 GMT  (127kb,D)", "http://arxiv.org/abs/1610.06510v2", "Under review, 10 pages"], ["v3", "Thu, 20 Jul 2017 21:33:00 GMT  (46kb,D)", "http://arxiv.org/abs/1610.06510v3", "Accepted at First Workshop on Subword and Character LEvel Models in NLP (SCLeM) to be held at EMNLP 2017"]], "COMMENTS": "A earlier version of this paper is under review at EACL 2107. (10 pages, 2 figures, 9 tables)", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["anoop kunchukuttan", "pushpak bhattacharyya"], "accepted": false, "id": "1610.06510"}, "pdf": {"name": "1610.06510.pdf", "metadata": {"source": "CRF", "title": "Learning variable length units for SMT between related languages via Byte Pair Encoding", "authors": ["Anoop Kunchukuttan", "Pushpak Bhattacharyya"], "emails": ["anoopk@cse.iitb.ac.in", "pb@cse.iitb.ac.in"], "sections": [{"heading": "1 Introduction", "text": "In fact, most of them are able to survive on their own, and they see themselves able to survive on their own."}, {"heading": "2 Related Work", "text": "There are two broad approaches that have been researched in literature for translation between related languages to take advantage of lexical similarity between source and target languages. The first is the use of transliteration of source words into target languages, which can be done by transliterating the untranslated words in a post-processing step (Nakov and Tiedemann, 2012; Kunchukuttan et al., 2014), a technique commonly used for dealing with named entities in SMT. The disadvantage is that transliteration candidates cannot be punctuated together with other features used in the SMT system. This constraint can be overcome by integrating the transliterature module into the decoder (Durrani et al, 2010), so that the translation candidates cannot be punctuated and baptized."}, {"heading": "3 Byte Pair Encoding as basic unit", "text": "Byte Pair Encoding is a data compression algorithm that was first adapted for Neural Machine Translation by Sennrich et al. (2016) to learn a limited vocabulary for an almost open vocabulary translation. The essential idea is to identify the most common strings to be added to the initial vocabulary. The initial vocabulary consists only of the characters in the text. An iterative greedy algorithm is used to add a new vocabulary in each iteration. In each iteration, the most common bigram (based on the current vocabulary) in the monolingual corpus is added to the vocabulary (the merge operation). With the updated vocabulary, the text is re-encoded and this process is repeated for a predetermined number of merge operations. The number of merge operations is the only hyperparameter for the system that needs to be represented. Figure 1 shows a sepseudo-graded, we only the word is pseudo-dosised."}, {"heading": "4 Experimental Setup", "text": "We trained translation systems on the basis of the following basic units: letter, morphem, word, orthographic syllable and BPE unit. In this section, we summarize the languages and writing systems selected for our experiments, the data sets used and the experimental configuration of our translation systems. Translation systems were evaluated using BLEU (Papineni et al., 2002)."}, {"heading": "4.1 Languages and writing systems", "text": "Our experiments covered a variety of languages: 16 language pairs, 17 languages and 10 writing systems. Table 1 summarizes the most important aspects of the languages involved in the experiment.The selected languages comprise 4 large language families (6 large subgroups: Indo-Aryan, Slavic and Germanic belong to the larger Indo-European language family).The languages exhibit a number of differences in word order and morphological complexity. Classification of Japanese and Korean into the Altaic family is discussed, but various lexical and grammatical similarities are indisputable, either due to genetic or cognate kinship (Robbeets, 2005; Vovin, 2010).However, the source of lexical similarity is immaterial to the current work. To achieve a better classification, we use the name Altaic to indicate the relationship between Japanese and Korean."}, {"heading": "4.2 Datasets", "text": "The parallel corpora of the Indo-Aryan and Dravidian languages are taken from the multilingual corpus of the Indian Language Corpora Initiative (ILCI) (Jha, 2012). Parallel corpora for other couples were obtained from the OpenSubtitles2016 section of the OPUS Corpora Collection (Tiedemann, 2009b). Language models for vocabulary systems were trained on the landing page of the training corpora plus additional monolingual corpora from various sources (see Table 3 for details). We used only the target-language side of the parallel corpora for signs, morphemes, OS and BPEunit LMs."}, {"heading": "4.3 System details", "text": "Phrase-based SMT systems were trained with the Moses system (Koehn et al., 2007), Growdiag-final-and heuristic for phrase extraction, and batch MIRA (Cherry and Foster, 2012) for tuning (standard parameters). We trained 5-gram LMs with Kneser-Ney smoothing for word and morpheme level models, and 10-gram LMs for character, OS, and BPE level models. We used unmonitored morphological segments trained with morfessor (Virpioja et al., 2013) to obtain morpheme representations. We used unattended morphology segments (trained with morfessor (Virpioja et al., 2013) and orthographic silogy rules from the Indian NLP library (Virpioja et al., 2013)."}, {"heading": "5 Results and Analysis", "text": "This section describes and analyzes the results of various experiments. It examines a comparison of the BPE with other units across languages and writing systems, the choice of number of merge operations, as well as the effects of domain change and the size of training data. Initial results will also be published using a common bilingual BPE model."}, {"heading": "5.1 Comparison of BPE with other units", "text": "Table 4 shows the translation accuracy of all language pairs as part of experiments for different translation units. The number of BPE merge operations was chosen so that the resulting vocabulary size would correspond to the vocabulary size of the orthographic syllable with encoded corpus. Since we do not have orthographic syllabification2https: / / github.com / rsennrich / subword-nmt 3http: / / nlp.ist.i.kyoto-u.ac.jp / EN / index.php? JUMAN 4https: / bitbucket.org / eunjeon / mecab-kofor urd, kor, jpn, we have selected the merge operations as follows: (\u2020) For Urd, the number of merge operations based on OS vocabularies was selected, as back-and forth-visual units registers of the same language are registers of the same language, kor, jpn, compared to PE (for the same conversion units are better than the average units of the conversion units)."}, {"heading": "5.2 Applicability to different writing systems", "text": "Alphabetic and abugida writing systems, on the other hand, fall into this category. However, logographic writing systems (Japanese kanji, Chinese) and abjad writing systems (Arabic, Hebrew, Syrian, etc.) do not represent vowels. To be more precise, abjad writing systems, depending on language, pragmatics and conventions, represent some / all vowels. Syllabic writing systems such as the Korean hangul do not explicitly represent vowels, as the basic unit (the syllable) implicitly represents vowels. Themajor's advantage of byte-pair coding is the independence of the writing system and our results show that BPE-coded units are useful for translations with abjad (Urdu uses an advanced Arabic writing system), logographic (Japanese kanji) and syllabic (Korean hangul) writing systems. Language pairs that include urine give an average of 12% improvement over the word level and an 18% improvement over the average."}, {"heading": "5.3 Choosing the number of BPE merge operations", "text": "The above-mentioned results for BPE units do not examine the optimal values of the number of merge operations. This is the only hyper parameter that needs to be selected for BPE. We experimented with a number of merge operations in the range of 1000 to 4000, and the translation results for these are in Table 5. Selecting the optimal value of merge operations resulted in a modest, average increase of 1.6% and a maximum increase in the translation accuracy of Bmatch across different language pairs. We also experimented with a higher number of merge operations for some language pairs, but there seemed to be no benefit in a higher number of merge operations. Compared with the number of merge operations reported by Sennrich et al. (2016) in a more general environment for NMT (60k), the number of merge operations for translation between related languages with limited parallel corpora is much lower. We must not forget that their goal was to use a more comprehensive correlation to the number of MPUs, but it was more possible for them to use a more parocular E."}, {"heading": "5.4 Robustness to Domain Change", "text": "Kunchukuttan and Bhattacharyya (2016) have shown that models based on orthographic syl-Bestprev JB1k JB2k JB3k JB4k ben-hin E (33.46) 33.54 33.23 33.54 33.35 pan-hin E (72.51) 72.41 72.35 72.13 72.04 kok-mar B1k (23.84) 24.01 10.76 23.8 23.86 times-hin B1k (21.23) 21.22 21.12 21.06 Istanbul-mac B2k (22.27) 8.72 tel-times B3k (9.12) 8.47 8.92 back-times B1k (10.96) 11.19 11.09 11.1 10.96 times-back B1k (21.23) 8.82 8.74 tel-times B3k (9.12) 8.72 tel-times B3k (9.12) 8.47 8.84 8.89 8.92 back-times B1k (10.96 times B1k (21.23) 11.96 times B1k (21.23) 22.96 B22k (22.22) 26.279 26.26 (22.58) 26.26 B311"}, {"heading": "5.5 Effect of training data size", "text": "For different training set sizes, we trained SMT systems with different display units (Figure 2 shows the learning curves for two language pairs).BPE level models are better than OS, morphem and word level in a number of dataset sizes. Especially when the training data is very small, the OS level models perform significantly better than the word and morpheme level models. The BPE level model is also better than OS level model when it comes to using more training data in the case of Malhin."}, {"heading": "5.6 Joint bilingual training of BPE models", "text": "We experimented with a joint training of BPE models on source and target language corpora, as proposed by Sennrich et al. (2016) The idea is to acquire a coding that is consistent across source and target languages and thus facilitates alignment. If source and target use the same font system, a common model is created by learning BPE via chained source and target language corpus. If the font systems are different, we transliterate one corpus into another by single-character mapping. This is possible between Indian fonts, but this scheme cannot be applied between Urdu and Indian fonts, as well as between Korean Hangul and Japanese Kanji fonts. Table 7 shows the results of the common BPE model for language pairs in which such a model is built. This performance of this model is roughly equivalent to the most powerful monolingual BPE model."}, {"heading": "6 Why is BPE unit better than other units?", "text": "The improved performance of the BPE units compared to the word and morph levels is easy to explain: with a limited vocabulary, they address the problem of data economy. But the models of the unigram level also have a limited vocabulary, but they do not improve translation performance except for very nearby languages. This is because they can effectively acquire character mappings that are sufficient for transliteration to some extent. But, the translation of cognates, morphological affinities, etc. requires a larger context. BPE and OS units - which offer more context - surpass unigrams.In order to understand why BPE performs better than OS, we need to understand an important desirable feature of the segmentation scheme: segmentation should result in a model with good prediction capacity (for example, in terms of language modeling).In fact, segmentation systems such as Wordpieces (Schuster and Nakajmaum, Excita) are to be explained."}, {"heading": "7 Conclusion & Future Work", "text": "We have shown that the BPE unit, a translation unit motivated by data compression and information theory, can outperform all previously proposed translation units, including the most powerful orthographic syllables, in the task of translating between related languages when limited parallel corpus is available. Furthermore, BPE encoding is system independent and can therefore be applied to any language. Experimental results on a large number of language pairs extending across different language families and writing systems support the advantages of BPE encoding for this task. We also show that BPE units are more robust when it comes to translation changes, and perform better in extremely low-data scenarios and morphologically rich languages. Initial experiments to build common BPE models across source and target languages have revealed minor shortcomings. Compared to the application of BPE in NMT, only a small number of merger operations are suitable for learning optimum vocabulary between languages."}, {"heading": "Acknowledgments", "text": "We thank the Technology Development for Indian Languages (TDIL) Programme and the Department of Electronics & Information Technology, Govt. of India for their support."}], "references": [{"title": "Statistical machine translation between related languages", "author": ["Pushpak Bhattacharyya", "Mitesh Khapra", "Anoop Kunchukuttan."], "venue": "NAACL Tutorials.", "citeRegEx": "Bhattacharyya et al\\.,? 2016", "shortCiteRegEx": "Bhattacharyya et al\\.", "year": 2016}, {"title": "HindEnCorp \u2013 Hindi-English and Hindi-only Corpus for Machine Translation", "author": ["Ond\u0159ej Bojar", "Vojt\u011bch Diatka", "Pavel Rychl\u00fd", "Pavel Stra\u0148\u00e1k", "V\u0131\u0301t Suchomel", "Ale\u0161 Tamchyna", "Daniel Zeman"], "venue": "In Proceedings of the 9th International Conference", "citeRegEx": "Bojar et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bojar et al\\.", "year": 2014}, {"title": "Batch tuning strategies for statistical machine translation", "author": ["Colin Cherry", "George Foster."], "venue": "Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies.", "citeRegEx": "Cherry and Foster.,? 2012", "shortCiteRegEx": "Cherry and Foster.", "year": 2012}, {"title": "Variablelength word encodings for neural translation models", "author": ["Rohan Chitnis", "John DeNero."], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing.", "citeRegEx": "Chitnis and DeNero.,? 2015", "shortCiteRegEx": "Chitnis and DeNero.", "year": 2015}, {"title": "A character-level decoder without explicit segmentation for neural machine translation", "author": ["Junyoung Chung", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "ACL.", "citeRegEx": "Chung et al\\.,? 2016", "shortCiteRegEx": "Chung et al\\.", "year": 2016}, {"title": "Urdu word segmentation", "author": ["Nadir Durrani", "Sarmad Hussain."], "venue": "Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics.", "citeRegEx": "Durrani and Hussain.,? 2010", "shortCiteRegEx": "Durrani and Hussain.", "year": 2010}, {"title": "Hindi-to-Urdu machine translation through transliteration", "author": ["Nadir Durrani", "Hassan Sajjad", "Alexander Fraser", "Helmut Schmid."], "venue": "Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics.", "citeRegEx": "Durrani et al\\.,? 2010", "shortCiteRegEx": "Durrani et al\\.", "year": 2010}, {"title": "A new algorithm for data compression", "author": ["Philip Gage."], "venue": "C Users J.", "citeRegEx": "Gage.,? 1994", "shortCiteRegEx": "Gage.", "year": 1994}, {"title": "Urdu monolingual corpus", "author": ["Bushra Jawaid", "Amir Kamran", "Ond\u0159ej Bojar."], "venue": "LINDAT/CLARIN digital library at the Institute of Formal and Applied Linguistics, Charles University in Prague.", "citeRegEx": "Jawaid et al\\.,? 2014", "shortCiteRegEx": "Jawaid et al\\.", "year": 2014}, {"title": "The TDIL program and the Indian Language Corpora Initiative", "author": ["Girish Nath Jha."], "venue": "Language Resources and Evaluation Conference.", "citeRegEx": "Jha.,? 2012", "shortCiteRegEx": "Jha.", "year": 2012}, {"title": "Moses: Open source toolkit for Statistical Machine Translation", "author": ["Philipp Koehn", "Hieu Hoang", "Alexandra Birch", "Chris Callison-Burch", "Marcello Federico", "Nicola Bertoldi", "Brooke Cowan", "Wade Shen", "Christine Moran", "Richard Zens"], "venue": null, "citeRegEx": "Koehn et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Koehn et al\\.", "year": 2007}, {"title": "Orthographic syllable as basic unit for smt between related languages", "author": ["Anoop Kunchukuttan", "Pushpak Bhattacharyya."], "venue": "Empirical Methods in Natural Language Processing.", "citeRegEx": "Kunchukuttan and Bhattacharyya.,? 2016", "shortCiteRegEx": "Kunchukuttan and Bhattacharyya.", "year": 2016}, {"title": "The IIT Bombay SMT System for ICON 2014 Tools contest", "author": ["Anoop Kunchukuttan", "Ratish Pudupully", "Rajen Chatterjee", "Abhijit Mishra", "Pushpak Bhattacharyya."], "venue": "NLP Tools Contest at ICON 2014.", "citeRegEx": "Kunchukuttan et al\\.,? 2014", "shortCiteRegEx": "Kunchukuttan et al\\.", "year": 2014}, {"title": "Automatic evaluation and uniform filter cascades for inducing n-best translation lexicons", "author": ["I Dan Melamed."], "venue": "Third Workshop on Very Large Corpora.", "citeRegEx": "Melamed.,? 1995", "shortCiteRegEx": "Melamed.", "year": 1995}, {"title": "Combining word-level and character-level models for machine translation between closely-related languages", "author": ["Preslav Nakov", "J\u00f6rg Tiedemann."], "venue": "Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Short", "citeRegEx": "Nakov and Tiedemann.,? 2012", "shortCiteRegEx": "Nakov and Tiedemann.", "year": 2012}, {"title": "BLEU: a method for automatic evaluation of machine translation", "author": ["Kishore Papineni", "Salim Roukos", "Todd Ward", "WeiJing Zhu."], "venue": "Association for Computational Linguistics.", "citeRegEx": "Papineni et al\\.,? 2002", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "Corpus portal for search in monolingual corpora", "author": ["Uwe Quasthoff", "Matthias Richter", "Christian Biemann."], "venue": "Proceedings of the fifth international conference on language resources and evaluation.", "citeRegEx": "Quasthoff et al\\.,? 2006", "shortCiteRegEx": "Quasthoff et al\\.", "year": 2006}, {"title": "Morphological Processing for English-Tamil Statistical Machine Translation", "author": ["Loganathan Ramasamy", "Ond\u0159ej Bojar", "Zden\u011bk \u017dabokrtsk\u00fd."], "venue": "Proceedings of the Workshop on Machine Translation and Parsing in Indian Languages.", "citeRegEx": "Ramasamy et al\\.,? 2012", "shortCiteRegEx": "Ramasamy et al\\.", "year": 2012}, {"title": "Is Japanese Related to Korean, Tungusic, Mongolic and Turkic? Otto Harrassowitz Verlag", "author": ["Martine Irma"], "venue": "Robbeets", "citeRegEx": "Irma,? \\Q2005\\E", "shortCiteRegEx": "Irma", "year": 2005}, {"title": "Japanese and korean voice search", "author": ["Mike Schuster", "Kaisuke Nakajima."], "venue": "2012 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 5149\u20135152. IEEE.", "citeRegEx": "Schuster and Nakajima.,? 2012", "shortCiteRegEx": "Schuster and Nakajima.", "year": 2012}, {"title": "Neural machine translation of rare words with subword units", "author": ["Rico Sennrich", "Barry Haddow", "Alexandra Birch."], "venue": "ACL.", "citeRegEx": "Sennrich et al\\.,? 2016", "shortCiteRegEx": "Sennrich et al\\.", "year": 2016}, {"title": "South Asian Languages: A Syntactic Typology", "author": ["Karumuri Subbarao."], "venue": "Cambridge University Press.", "citeRegEx": "Subbarao.,? 2012", "shortCiteRegEx": "Subbarao.", "year": 2012}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V Le."], "venue": "Advances in neural information processing systems.", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Analyzing the use of character-level translation with sparse and noisy datasets", "author": ["J\u00f6rg Tiedemann", "Preslav Nakov."], "venue": "RANLP.", "citeRegEx": "Tiedemann and Nakov.,? 2013", "shortCiteRegEx": "Tiedemann and Nakov.", "year": 2013}, {"title": "Character-based PBSMT for closely related languages", "author": ["J\u00f6rg Tiedemann."], "venue": "Proceedings of the 13th Conference of the European Association for Machine Translation.", "citeRegEx": "Tiedemann.,? 2009a", "shortCiteRegEx": "Tiedemann.", "year": 2009}, {"title": "News from opus-a collection of multilingual parallel corpora with tools and interfaces", "author": ["J\u00f6rg Tiedemann."], "venue": "Recent advances in natural language processing.", "citeRegEx": "Tiedemann.,? 2009b", "shortCiteRegEx": "Tiedemann.", "year": 2009}, {"title": "Character-based pivot translation for under-resourced languages and domains", "author": ["J\u00f6rg Tiedemann."], "venue": "EACL.", "citeRegEx": "Tiedemann.,? 2012", "shortCiteRegEx": "Tiedemann.", "year": 2012}, {"title": "Can we translate letters", "author": ["David Vilar", "Jan-T Peter", "Hermann Ney"], "venue": "In Proceedings of the Second Workshop on Statistical Machine Translation", "citeRegEx": "Vilar et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Vilar et al\\.", "year": 2007}, {"title": "Morfessor 2.0: Python implementation and extensions for morfessor baseline", "author": ["Sami Virpioja", "Peter Smit", "Stig-Arne Gr\u00f6nroos", "Mikko Kurimo"], "venue": null, "citeRegEx": "Virpioja et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Virpioja et al\\.", "year": 2013}, {"title": "Korea-Japonica: A ReEvaluation of a Common Genetic Origin", "author": ["Alexander Vovin."], "venue": "University of Hawaii Press.", "citeRegEx": "Vovin.,? 2010", "shortCiteRegEx": "Vovin.", "year": 2010}, {"title": "Source language adaptation for resource-poor machine translation", "author": ["Pidong Wang", "Preslav Nakov", "Hwee Tou Ng."], "venue": "Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Lan-", "citeRegEx": "Wang et al\\.,? 2012", "shortCiteRegEx": "Wang et al\\.", "year": 2012}, {"title": "Google\u2019s Neural Machine Translation System: Bridging the Gap between Human and Machine Translation", "author": ["Y. Wu", "M. Schuster", "Z. Chen", "Q.V. Le", "M. Norouzi."], "venue": "ArXiv e-prints.", "citeRegEx": "Wu et al\\.,? 2016", "shortCiteRegEx": "Wu et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 0, "context": "Related languages are those that exhibit lexical and structural similarities on account of sharing a common ancestry or being in prolonged contact for a long period of time (Bhattacharyya et al., 2016).", "startOffset": 173, "endOffset": 201}, {"referenceID": 11, "context": "\u2022 We show that BPE units outperform orthographic syllable units (Kunchukuttan and Bhattacharyya, 2016), the best performing basic unit for translation between related languages, resulting in up to 11% improvement in translation accuracy.", "startOffset": 64, "endOffset": 102}, {"referenceID": 14, "context": "This can done by transliterating the untranslated words in a post-processing step (Nakov and Tiedemann, 2012; Kunchukuttan et al., 2014), a technique generally used for handling named entities in SMT.", "startOffset": 82, "endOffset": 136}, {"referenceID": 12, "context": "This can done by transliterating the untranslated words in a post-processing step (Nakov and Tiedemann, 2012; Kunchukuttan et al., 2014), a technique generally used for handling named entities in SMT.", "startOffset": 82, "endOffset": 136}, {"referenceID": 6, "context": "This limitation can be overcome by integrating the transliteration module into the decoder (Durrani et al., 2010), so", "startOffset": 91, "endOffset": 113}, {"referenceID": 27, "context": "has been explored for very closely related languages like Bulgarian-Macedonian, IndonesianMalay, Spanish-Catalan with modest success (Vilar et al., 2007; Tiedemann, 2009a; Tiedemann and Nakov, 2013).", "startOffset": 133, "endOffset": 198}, {"referenceID": 24, "context": "has been explored for very closely related languages like Bulgarian-Macedonian, IndonesianMalay, Spanish-Catalan with modest success (Vilar et al., 2007; Tiedemann, 2009a; Tiedemann and Nakov, 2013).", "startOffset": 133, "endOffset": 198}, {"referenceID": 23, "context": "has been explored for very closely related languages like Bulgarian-Macedonian, IndonesianMalay, Spanish-Catalan with modest success (Vilar et al., 2007; Tiedemann, 2009a; Tiedemann and Nakov, 2013).", "startOffset": 133, "endOffset": 198}, {"referenceID": 26, "context": "vides very little context for learning translation models (Tiedemann, 2012).", "startOffset": 58, "endOffset": 75}, {"referenceID": 23, "context": "The use of character n-gram units to address this limitation leads to data sparsity for higher order n-grams and provides little benefit (Tiedemann and Nakov, 2013).", "startOffset": 137, "endOffset": 164}, {"referenceID": 11, "context": "Recently, Kunchukuttan and Bhattacharyya (2016) proposed orthographic syllables (OS), a linguistically-motivated variablelength unit.", "startOffset": 10, "endOffset": 48}, {"referenceID": 20, "context": "Source: Sennrich et al. (2016)", "startOffset": 8, "endOffset": 31}, {"referenceID": 22, "context": "(Sutskever et al., 2014).", "startOffset": 0, "endOffset": 24}, {"referenceID": 7, "context": "It is in this context that Byte Pair Encoding, a data compression method (Gage, 1994), was adapted to learn sub-word units for NMT (Sennrich et al.", "startOffset": 73, "endOffset": 85}, {"referenceID": 20, "context": "It is in this context that Byte Pair Encoding, a data compression method (Gage, 1994), was adapted to learn sub-word units for NMT (Sennrich et al., 2016).", "startOffset": 131, "endOffset": 154}, {"referenceID": 4, "context": "ter (Chung et al., 2016), Huffman encoding based units (Chitnis and DeNero, 2015), word pieces (Schuster and Nakajima, 2012; Wu et al.", "startOffset": 4, "endOffset": 24}, {"referenceID": 3, "context": ", 2016), Huffman encoding based units (Chitnis and DeNero, 2015), word pieces (Schuster and Nakajima, 2012; Wu et al.", "startOffset": 38, "endOffset": 64}, {"referenceID": 19, "context": ", 2016), Huffman encoding based units (Chitnis and DeNero, 2015), word pieces (Schuster and Nakajima, 2012; Wu et al., 2016).", "startOffset": 78, "endOffset": 124}, {"referenceID": 31, "context": ", 2016), Huffman encoding based units (Chitnis and DeNero, 2015), word pieces (Schuster and Nakajima, 2012; Wu et al., 2016).", "startOffset": 78, "endOffset": 124}, {"referenceID": 20, "context": "Byte Pair Encoding is a data compression algorithm which was first adapted for Neural Machine Translation by Sennrich et al. (2016) as a way to learn a limited vocabulary for near open vocabulary translation.", "startOffset": 109, "endOffset": 132}, {"referenceID": 20, "context": "The only way we differ from Sennrich et al. (2016) is in terms of the representation of the encoded words in the corpus.", "startOffset": 28, "endOffset": 51}, {"referenceID": 15, "context": "Evaluation of the translation systems was done using BLEU (Papineni et al., 2002).", "startOffset": 58, "endOffset": 81}, {"referenceID": 29, "context": "tionship (Robbeets, 2005; Vovin, 2010).", "startOffset": 9, "endOffset": 38}, {"referenceID": 13, "context": "Table 4 shows an indication of the lexical similarity between them in terms of the Longest Common Subsequence Ratio (LCSR) (Melamed, 1995) between the parallel training sentences at character level (shown only for language pairs where the writing systems are the same or can be easily mapped to do the LCSR computation).", "startOffset": 123, "endOffset": 138}, {"referenceID": 21, "context": "On the other end of the spectrum, some language pairs like Hindi and Malayalam belong to different language families, but show many lexical and grammatical similarities due to contact for a long time (Subbarao, 2012).", "startOffset": 200, "endOffset": 216}, {"referenceID": 1, "context": "hin (Bojar et al., 2014) 10M urd (Jawaid et al.", "startOffset": 4, "endOffset": 24}, {"referenceID": 8, "context": ", 2014) 10M urd (Jawaid et al., 2014) 5M tam (Ramasamy et al.", "startOffset": 16, "endOffset": 37}, {"referenceID": 17, "context": ", 2014) 5M tam (Ramasamy et al., 2012) 1M mar (news websites) 1.", "startOffset": 15, "endOffset": 38}, {"referenceID": 16, "context": "8M mal (Quasthoff et al., 2006) 200K swe (OpenSubtitles2016) 2.", "startOffset": 7, "endOffset": 31}, {"referenceID": 25, "context": "4M mac (Tiedemann, 2009b) 680K ind (Tiedemann, 2009b) 640K", "startOffset": 7, "endOffset": 25}, {"referenceID": 25, "context": "4M mac (Tiedemann, 2009b) 680K ind (Tiedemann, 2009b) 640K", "startOffset": 35, "endOffset": 53}, {"referenceID": 5, "context": "For instance, Urdu word segmentation can be very inconsistent (Durrani and Hussain, 2010) and generally short vowels are not denoted.", "startOffset": 62, "endOffset": 89}, {"referenceID": 9, "context": "tive (ILCI) corpus (Jha, 2012).", "startOffset": 19, "endOffset": 30}, {"referenceID": 25, "context": "Parallel corpora for other pairs were obtained from the OpenSubtitles2016 section of the OPUS corpus collection (Tiedemann, 2009b).", "startOffset": 112, "endOffset": 130}, {"referenceID": 10, "context": "Phrase-based SMT systems were trained using the Moses system (Koehn et al., 2007), with the growdiag-final-and heuristic for extracting phrases, and Batch MIRA (Cherry and Foster, 2012) for tuning (default parameters).", "startOffset": 61, "endOffset": 81}, {"referenceID": 2, "context": ", 2007), with the growdiag-final-and heuristic for extracting phrases, and Batch MIRA (Cherry and Foster, 2012) for tuning (default parameters).", "startOffset": 86, "endOffset": 111}, {"referenceID": 28, "context": "We used unsupervised morphological segmenters trained with Morfessor (Virpioja et al., 2013) for obtaining morpheme representations.", "startOffset": 69, "endOffset": 92}, {"referenceID": 28, "context": "We used unsupervised morphsegmenters (trained using Morfessor (Virpioja et al., 2013)) and orthographic syllabification rules from the Indic NLP Library1.", "startOffset": 62, "endOffset": 85}, {"referenceID": 20, "context": "of merge operations reported by Sennrich et al. (2016) in a more general setting for NMT (60k), the number of merge operations is far less for translation between related languages with limited parallel corpora.", "startOffset": 32, "endOffset": 55}, {"referenceID": 20, "context": "We experimented with jointly training BPE models over source and target language corpora as suggested by Sennrich et al. (2016). The idea is to Src-Tgt W M Bmatch O C", "startOffset": 105, "endOffset": 128}, {"referenceID": 19, "context": "In fact, segmentation schemes like wordpieces (Schuster and Nakajima, 2012) explicity try to maximize this goal.", "startOffset": 46, "endOffset": 75}, {"referenceID": 26, "context": "pivot based MT, domain adaptation (Tiedemann, 2012) and translation between a lingua franca and related languages (Wang et al.", "startOffset": 34, "endOffset": 51}, {"referenceID": 30, "context": "pivot based MT, domain adaptation (Tiedemann, 2012) and translation between a lingua franca and related languages (Wang et al., 2012) can be revisited with these basic units.", "startOffset": 114, "endOffset": 133}], "year": 2016, "abstractText": "We explore the use of segments learnt using Byte Pair Encoding (referred to as BPE units) as basic units for statistical machine translation between related languages and compare it with orthographic syllables, which are currently the best performing basic units for this translation task. BPE identifies the most frequent character sequences as basic units, while orthographic syllables are linguistically motivated pseudo-syllables. We show that BPE units outperform orthographic syllables as units of translation, showing up to 11% increase in BLEU scores. In addition, BPE can be applied to any writing system, while orthographic syllables can be used only for languages whose writing systems use vowel representations. We show that BPE units outperform word and morpheme level units for translation involving languages like Urdu, Japanese whose writing systems do not use vowels (either completely or partially). Across many language pairs, spanning multiple language families and types of writing systems, we show that translation with BPE segments outperforms orthographic syllables, especially for morphologically rich languages.", "creator": "TeX"}}}