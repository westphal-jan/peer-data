{"id": "1610.06540", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Oct-2016", "title": "Jointly Learning to Align and Convert Graphemes to Phonemes with Neural Attention Models", "abstract": "We propose an attention-enabled encoder-decoder model for the problem of grapheme-to-phoneme conversion. Most previous work has tackled the problem via joint sequence models that require explicit alignments for training. In contrast, the attention-enabled encoder-decoder model allows for jointly learning to align and convert characters to phonemes. We explore different types of attention models, including global and local attention, and our best models achieve state-of-the-art results on three standard data sets (CMUDict, Pronlex, and NetTalk).", "histories": [["v1", "Thu, 20 Oct 2016 19:00:48 GMT  (70kb,D)", "http://arxiv.org/abs/1610.06540v1", "Accepted in SLT 2016"]], "COMMENTS": "Accepted in SLT 2016", "reviews": [], "SUBJECTS": "cs.CL cs.AI", "authors": ["shubham toshniwal", "karen livescu"], "accepted": false, "id": "1610.06540"}, "pdf": {"name": "1610.06540.pdf", "metadata": {"source": "CRF", "title": "JOINTLY LEARNING TO ALIGN AND CONVERT GRAPHEMES TO PHONEMES WITH NEURAL ATTENTION MODELS", "authors": ["Shubham Toshniwal", "Karen Livescu"], "emails": ["klivescu}@ttic.edu"], "sections": [{"heading": null, "text": "In fact, most of them are able to understand themselves. (...) Most of them are able to identify themselves. (...) Most of them are not able to identify themselves. (...) Most of them are not able to identify themselves. (...) Most of them are able to identify themselves. (...) Most of them are not able to identify themselves. (...) Most of them are able to identify themselves. (...) Most of them are able to identify themselves. (...) Most of them are able to identify themselves. (...) Most of them are not able to identify themselves. (...) Most of them are not able to identify themselves. (...) Most of them are able to identify themselves. (...)"}, {"heading": "3.1. Encoder-decoder models", "text": "We briefly describe the encoder decoder (\"sequence-to-sequence\") approach previously proposed by [13]. An encoder decoder model comprises an encoder read in the input sequence (graph) and a decoder that generates the output sequence (phoneme).A typical encoder decoder model is shown in Figure 1. In our model, the encoder is a bidirectional long-term memory (BiLSTM); we use a bidirectional network to capture the context on both sides of each graph. The encoder takes as an input sequence the graph sequence represented as a sequence of the vectors x \u2212 (x1, \u00b7 \u00b7 \u00b7 \u00b7, xTg) by multiplying the most uniform vectors representing the input characters."}, {"heading": "3.2. Global Attention", "text": "One of the important enhancements to encoder decoder models is the use of attention mechanisms to adjust the context vector c for each output label prediction [7]. Instead of using only the context vector as the initial state for the decoder LSTM, we use a different context vector ct for each decoder time step, where ct is a linear combination of all the hidden states of the encoder. Choosing the output state for the decoder LSTM is now less important; we simply use the last hidden state of the encoder backward LSTM. The ability to pay attention to different encoder states when decoding each output label means that the attention mechanism can be viewed as a soft alignment between the input (graph) sequence and the output (phoneme) sequence. We use the attention mechanism suggested by [16], the WD + vector is given time (VHi = VHi)."}, {"heading": "3.3. Local Attention", "text": "The global attention mechanism takes into account all encoder states when predicting each decoder output. However, it can be argued that the G2P problem is inherently local in nature; that is, each output typically depends only on a small window of input markers. Therefore, it should be sufficient to limit attention to an appropriate window of encoder states to predict each phoneme. Local attention models work by first finding an aligned position pt in each time step t, which is an index in the input sequence, and then looking at a context window [pt \u2212 D, pt + D] of the indices around it. In our experiments, we find D = 3 to function well. Context vector model ct is then calculated in a similar manner to that described in the global attention model, but taking into account only the encoder states that fall within the context window. We consider two variants of local attention based on 3.17 local alignment [the below]."}, {"heading": "3.4. Input Feeding", "text": "One of the problems with attention models is that at each decoder step, the attention model decides what to pay attention to regardless of the decisions made in other time steps. Therefore, there is no explicit global restriction to ensure that the entire source sequence is \"covered,\" i.e. that all input is taken into account at one point or another. An indirect way to solve this potential problem is to link the previous attention-weighted context vector to the input for the next decoder time step, as in [17]. This \"feeding\" of earlier attention makes the model more aware of the attention decisions made in earlier time steps. We treat the choice of input feeding as the hyperparameter of our models."}, {"heading": "4.1. Data", "text": "To directly compare with previous results, we use three standard datasets that are typically used to evaluate G2P models, namely CMUDict, Pronlex, and NetTalk. For all three datasets, we use the same set of experiments as in [1], which we briefly describe hereinafter. 4The CMUDict dataset is divided into a 106,837-word training set and a 12,000-word test set. As in previous work, we sample 2,670 words from the training set to create a development set for hyperparameter tuning.The Pronlex dataset includes 83,182 words in the training set, 2,400 words in the development set, and 4,800 words in the test set. The development and test sets are divided into 3 categories. We report only the total results and not categories for pronlex.The NetTalk dataset is compared to the other two sets of test sets, however, we generate a small set of 1,951 words in the training set and only 4,951 words in the final."}, {"heading": "4.2. Evaluation", "text": "We evaluate performance using the standard measures of the word error rate (WHO) and the phoneme error rate (PER), which are given in percent. PER corresponds to the Levenshtein distance of the predicted phoneme sequence from the basic truth divided by the total number of phonemes in the basic truth. WHO corresponds to the total number of words in which there is at least one telephone error divided by the total number of words. As in previous works, for words with multiple basic truths, we select the basic truth that gives the lowest PER."}, {"heading": "4.3. Training", "text": "We use Minibatch stochastic gradient descent (SGD) together with Adam [18] with a minibatch size of 256. We use an initial learning rate of 0.001 and reduce this learning rate by a multiplicative factor of 0.8 whenever the WHO for development data does not decrease at the end of an epoch. We train the model for 100 epochs, but update / save it at the end of an epoch only if it decreases the WHO for development data. To prevent an overhaul, we insert: (a) a fail layer [19] between each pair of successive layers of stacked LSTMs and (b) use scheduled scan [20], with a linear decay, on the decoder side. We adjust the failure probability across the range."}, {"heading": "4.4. Inference", "text": "We use a greedy decoder (bar size = 1) to decrypt the phoneme sequence during the inference. That is, at each decoding time step we consider the output phone to be the Argmax of the Softmax output of the decoder in that time frame. We have not made any reliable gains by using bar search with a bar size greater than 1."}, {"heading": "4.5. Results", "text": "The main results of our study are those of Yao and Zweig with an aligned, deep bidirectional LSTM. Our best attention models outperform all previous models in terms of PER. In terms of attention, the model outperforms all previous models. In terms of attention, it outperforms all others."}, {"heading": "4.5.1. Ablation analysis for CMUDict", "text": "To measure the contributions of the various components of our attention models, we performed an ablation analysis for the global attention model, which was evaluated on the basis of CMUDict development data. Results are presented in Table 2. As shown in the table, eliminating the input leads to a very low performance drop. The use of regularization in the form of dropout and scheduled sampling also provides a slight boost. The importance of attention is reflected in an almost 1% absolute performance drop when the input is removed. As we will see later, this thrust is more pronounced over longer periods of time. Furthermore, we tested the effect of bidirectionality by comparing it to a \"reverse unidirectional\" LSTM, which drops the bidirectionality and returns the input in reverse order. Interestingly, the reverse unidirectional non-attention model is better than using the bidirectional counterpart of LSTM, which could be due to our combination or STM."}, {"heading": "5.1. Error Comparison Based on Word Length", "text": "We compare the errors of the global attention model and the noattention model depending on the word length. To this end, we categorize the word length into 4 categories: short (length \u2264 6), medium (length \u0432 {7, 8}), long (length \u0438 {9, 10}), very long (length \u2265 11). Figure 2 provides the results of this comparison. Two essential observations can be derived from the diagram: (i) Both models make fewer errors with longer words; this is somewhat counterintuitive, but perhaps because the longer a word is, the more possibilities the model has to recognize certain important global characteristics such as its origin. (ii) The global attention model has an even greater advantage over the model of not paying attention with longer words. The second observation is quite intuitive: the non-attention model is forced to present all information about a word with a single hidden vector, which, of course, fails with longer sequences."}, {"heading": "5.2. Phoneme Error Comparison Between Attention Models", "text": "We are interested in analyzing the distribution of phoneme errors across words, that is, how often are word errors due to very small telephone errors, and how often are they due to a catastrophic failure? We look at this question by analyzing all word errors made by global and local p attention models. We categorize the number of small and medium errors per word PER into 4 categories: small (\u2264 10% of phones are wrong), medium (10-20% wrong), large (20-30%), and very large (\u2265 30%). We find that the number of small and medium errors per word PER is comparable for the two types of attention models. However, the local p makes much larger and very large errors. Predictably, the local p attention model depends heavily on the predicted alignment position; if this prediction is wrong, this can lead to a cascading of error effects and make recovery of the model more difficult."}, {"heading": "5.3. Error Classification", "text": "The low PER of the global attention model means that even most of the false predictions are still very close to the basic truth. We analyze the cases where the prediction is still very far from the basic truth in order to understand the type of phenomena that still need to be addressed. We look at the following four most common error categories, which are also presented in the table: \u2022 Foreign Origin Names can be very challenging, and at the same time quite common."}, {"heading": "5.4. Phoneme Embedding Visualization", "text": "Most of the model parameters have no clear meaning, but the learned telephone embeddings should be meaningful; for example, similar phones with similar embeddings should be assigned and vice versa. To confirm this, we have in Figure 3 a two-dimensional visualization generated with t-SNE [24], the telephone embeddings learned from global attention models. [The embeddings for the most part behave as expected.] The phones are broadly clustered by type classes: vowels, diphthongs, fricatives and stops each have their own part of the room (with some intermingling of stops and fricatives), and diphthongs form a narrow subgroup within the vowel region. Voiced / voiceless pairs of stops and fricatives are closely connected, and vowels are placed in reasonable places corresponding to their height / frontness."}], "references": [{"title": "Conditional and joint models for graphemeto-phoneme conversion", "author": ["Stanley F. Chen"], "venue": "Eurospeech, 2003.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2003}, {"title": "Joint-sequence models for Grapheme-to-Phoneme Conversion", "author": ["Maximilian Bisani", "Hermann Ney"], "venue": "Speech Commun., vol. 50, no. 5, pp. 434\u2013451, May 2008.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2008}, {"title": "Applying Many-to-Many Alignments and Hidden Markov Models to Letter-to-Phoneme Conversion", "author": ["Sittichai Jiampojamarn", "Grzegorz Kondrak", "Tarek Sherif"], "venue": "NAACL-HLT, 2007.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2007}, {"title": "Long Short-Term Memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural Comput., vol. 9, no. 8, pp. 1735\u20131780, Nov. 1997.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1997}, {"title": "Grapheme-to-phoneme conversion using Long Short- Term Memory Recurrent Neural Networks", "author": ["Kanishka Rao", "Fuchun Peng", "Hasim Sak", "Fran\u00e7oise Beaufays"], "venue": "ICASSP, 2015.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2015}, {"title": "Sequence-to-Sequence Neural Net Models for Grapheme-to-Phoneme Conversion", "author": ["Kaisheng Yao", "Geoffrey Zweig"], "venue": "Interspeech, 2015.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2015}, {"title": "Neural Machine Translation by Jointly Learning to Align and Translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio"], "venue": "CoRR, vol. abs/1409.0473, 2014.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2014}, {"title": "Listen, Attend and Spell", "author": ["William Chan", "Navdeep Jaitly", "Quoc V. Le", "Oriol Vinyals"], "venue": "CoRR, vol. abs/1508.01211, 2015.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "Show, Attend and Tell: Neural Image Caption Generation with Visual Attention", "author": ["Kelvin Xu", "Jimmy Ba", "Ryan Kiros", "Kyunghyun Cho", "Aaron Courville", "Ruslan Salakhudinov", "Rich Zemel", "Yoshua Bengio"], "venue": "ICML, 2015.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2015}, {"title": "Bi-directional conversion between graphemes and phonemes using a joint n-gram model", "author": ["Lucian Galescu", "James F. Allen"], "venue": "4th ITRW on Speech Synthesis, 2001.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2001}, {"title": "Letter-to-Sound Pronunciation Prediction using Conditional Random Fields", "author": ["Dong Wang", "Simon King"], "venue": "IEEE Signal Process. Lett., vol. 18, no. 2, pp. 122\u2013125, 2011.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2011}, {"title": "Connectionist temporal classification: Labelling unsegmented sequence data with recurrent neural networks", "author": ["Alex Graves", "Santiago Fern\u00e1ndez", "Faustino Gomez"], "venue": "ICML, 2006.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2006}, {"title": "Sequence to Sequence Learning with Neural Networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V. Le"], "venue": "NIPS, 2014.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2014}, {"title": "Generating Sequences with Recurrent Neural Networks", "author": ["Alex Graves"], "venue": "CoRR, vol. abs/1308.0850, 2013.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2013}, {"title": "Recurrent Neural Network Regularization", "author": ["Wojciech Zaremba", "Ilya Sutskever", "Oriol Vinyals"], "venue": "CoRR, vol. abs/1409.2329, 2014.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2014}, {"title": "Grammar as a Foreign Language", "author": ["Oriol Vinyals", "Lukasz Kaiser", "Terry Koo", "Slav Petrov", "Ilya Sutskever", "Geoffrey E. Hinton"], "venue": "NIPS, 2015.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}, {"title": "Effective Approaches to Attention-based Neural Machine Translation", "author": ["Thang Luong", "Hieu Pham", "Christopher D. Manning"], "venue": "EMNLP, 2015.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2015}, {"title": "Adaptive Subgradient Methods for Online Learning and Stochastic Optimization", "author": ["John Duchi", "Elad Hazan", "Yoram Singer"], "venue": "JMLR, vol. 12, pp. 2121\u20132159, July 2011.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2011}, {"title": "Dropout: a simple way to prevent neural networks from overfitting", "author": ["Nitish Srivastava", "Geoffrey E Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov"], "venue": "JMLR, vol. 15, no. 1, pp. 1929\u20131958, 2014.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1929}, {"title": "Scheduled Sampling for Sequence Prediction with Recurrent Neural Networks", "author": ["Samy Bengio", "Oriol Vinyals", "Navdeep Jaitly", "Noam Shazeer"], "venue": "CoRR, vol. abs/1506.03099, 2015.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2015}, {"title": "TensorFlow: Large-scale machine learning on heterogeneous systems", "author": ["Mart\u0131\u0301n Abadi", "Ashish Agarwal", "Paul Barham", "Eugene Brevdo", "Zhifeng Chen", "Craig Citro", "Greg S. Corrado", "Andy Davis", "Jeffrey Dean", "Matthieu Devin", "Sanjay Ghemawat", "Ian Goodfellow", "Andrew Harp", "Geoffrey Irving", "Michael Isard", "Yangqing Jia", "Rafal Jozefowicz", "Lukasz Kaiser", "Manjunath Kudlur", "Josh Levenberg", "Dan Man\u00e9", "Rajat Monga", "Sherry Moore", "Derek Murray", "Chris Olah", "Mike Schuster", "Jonathon Shlens", "Benoit Steiner", "Ilya Sutskever", "Kunal Talwar", "Paul Tucker", "Vincent Vanhoucke", "Vijay Vasudevan", "Fernanda Vi\u00e9gas", "Oriol Vinyals", "Pete Warden", "Martin Wattenberg", "Martin Wicke", "Yuan Yu", "Xiaoqiang Zheng"], "venue": "2015.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation", "author": ["Kyunghyun Cho", "Bart van Merrienboer", "\u00c7aglar G\u00fcl\u00e7ehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio"], "venue": "EMNLP, 2014.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2014}, {"title": "Coverage-based neural machine translation", "author": ["Zhaopeng Tu", "Zhengdong Lu", "Yang Liu", "Xiaohua Liu", "Hang Li"], "venue": "CoRR, vol. abs/1601.04811, 2016.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2016}, {"title": "Visualizing data using t-SNE", "author": ["Laurens van der Maaten", "Geoffrey Hinton"], "venue": "JMLR, vol. 9, no. Nov, pp. 2579\u20132605, 2008.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2008}, {"title": "G2P conversion of proper names using word origin information", "author": ["Sonjia Waxmonsky", "Sravana Reddy"], "venue": "Stroudsburg, PA, USA, NAACL-HLT, 2012, pp. 367\u2013371.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2012}], "referenceMentions": [{"referenceID": 0, "context": "A typical approach to G2P involves using joint sequence models, where the alignment is provided via some external aligner [1, 2, 3].", "startOffset": 122, "endOffset": 131}, {"referenceID": 1, "context": "A typical approach to G2P involves using joint sequence models, where the alignment is provided via some external aligner [1, 2, 3].", "startOffset": 122, "endOffset": 131}, {"referenceID": 2, "context": "A typical approach to G2P involves using joint sequence models, where the alignment is provided via some external aligner [1, 2, 3].", "startOffset": 122, "endOffset": 131}, {"referenceID": 3, "context": "Specifically, long short-term memory (LSTM) networks [4] have recently been explored [5, 6].", "startOffset": 53, "endOffset": 56}, {"referenceID": 4, "context": "Specifically, long short-term memory (LSTM) networks [4] have recently been explored [5, 6].", "startOffset": 85, "endOffset": 91}, {"referenceID": 5, "context": "Specifically, long short-term memory (LSTM) networks [4] have recently been explored [5, 6].", "startOffset": 85, "endOffset": 91}, {"referenceID": 5, "context": "When used in an encoder-decoder approach, as in [6], they in principle require no alignment between the input (grapheme", "startOffset": 48, "endOffset": 51}, {"referenceID": 6, "context": "In this paper we explore an extension of encoder-decoder networks based on an attention mechanism, which has proven useful in other sequence prediction tasks [7, 8, 9].", "startOffset": 158, "endOffset": 167}, {"referenceID": 7, "context": "In this paper we explore an extension of encoder-decoder networks based on an attention mechanism, which has proven useful in other sequence prediction tasks [7, 8, 9].", "startOffset": 158, "endOffset": 167}, {"referenceID": 8, "context": "In this paper we explore an extension of encoder-decoder networks based on an attention mechanism, which has proven useful in other sequence prediction tasks [7, 8, 9].", "startOffset": 158, "endOffset": 167}, {"referenceID": 0, "context": "Most previous work on learning G2P breaks the problem into two steps: (i) align the grapheme and phoneme sequences (ii) train a conditional or joint maximum entropy model on the aligned data [1, 2, 3, 10].", "startOffset": 191, "endOffset": 204}, {"referenceID": 1, "context": "Most previous work on learning G2P breaks the problem into two steps: (i) align the grapheme and phoneme sequences (ii) train a conditional or joint maximum entropy model on the aligned data [1, 2, 3, 10].", "startOffset": 191, "endOffset": 204}, {"referenceID": 2, "context": "Most previous work on learning G2P breaks the problem into two steps: (i) align the grapheme and phoneme sequences (ii) train a conditional or joint maximum entropy model on the aligned data [1, 2, 3, 10].", "startOffset": 191, "endOffset": 204}, {"referenceID": 9, "context": "Most previous work on learning G2P breaks the problem into two steps: (i) align the grapheme and phoneme sequences (ii) train a conditional or joint maximum entropy model on the aligned data [1, 2, 3, 10].", "startOffset": 191, "endOffset": 204}, {"referenceID": 0, "context": "The factored terms in the distribution can be modeled using a maximum entropy classifier [1], or the full product can be modeled as a conditional random field (CRF) [11].", "startOffset": 89, "endOffset": 92}, {"referenceID": 10, "context": "The factored terms in the distribution can be modeled using a maximum entropy classifier [1], or the full product can be modeled as a conditional random field (CRF) [11].", "startOffset": 165, "endOffset": 169}, {"referenceID": 1, "context": "The graphone sequence can be modeled via n-gram models [2, 10] or maximum entropy models [1].", "startOffset": 55, "endOffset": 62}, {"referenceID": 9, "context": "The graphone sequence can be modeled via n-gram models [2, 10] or maximum entropy models [1].", "startOffset": 55, "endOffset": 62}, {"referenceID": 0, "context": "The graphone sequence can be modeled via n-gram models [2, 10] or maximum entropy models [1].", "startOffset": 89, "endOffset": 92}, {"referenceID": 4, "context": "[5] use bidirectional LSTMs with a connectionist temporal classification (CTC) layer [12] which doesn\u2019t require the data to be aligned.", "startOffset": 0, "endOffset": 3}, {"referenceID": 11, "context": "[5] use bidirectional LSTMs with a connectionist temporal classification (CTC) layer [12] which doesn\u2019t require the data to be aligned.", "startOffset": 85, "endOffset": 89}, {"referenceID": 5, "context": "Yao and Zweig [6] explore both an encoder-decoder architechture [13] and an input-output", "startOffset": 14, "endOffset": 17}, {"referenceID": 12, "context": "Yao and Zweig [6] explore both an encoder-decoder architechture [13] and an input-output", "startOffset": 64, "endOffset": 68}, {"referenceID": 6, "context": "for machine translation [7] (though a precursor of this model was the windowing approach of Graves [14]), which has since been applied to a variety of tasks including speech recognition [8] and image caption generation [9].", "startOffset": 24, "endOffset": 27}, {"referenceID": 13, "context": "for machine translation [7] (though a precursor of this model was the windowing approach of Graves [14]), which has since been applied to a variety of tasks including speech recognition [8] and image caption generation [9].", "startOffset": 99, "endOffset": 103}, {"referenceID": 7, "context": "for machine translation [7] (though a precursor of this model was the windowing approach of Graves [14]), which has since been applied to a variety of tasks including speech recognition [8] and image caption generation [9].", "startOffset": 186, "endOffset": 189}, {"referenceID": 8, "context": "for machine translation [7] (though a precursor of this model was the windowing approach of Graves [14]), which has since been applied to a variety of tasks including speech recognition [8] and image caption generation [9].", "startOffset": 219, "endOffset": 222}, {"referenceID": 12, "context": "We briefly describe the encoder-decoder (\u201csequence-to-sequence\u201d) approach, as proposed by [13].", "startOffset": 90, "endOffset": 94}, {"referenceID": 6, "context": "One of the important extensions of encoder-decoder models is the use of attention mechanism to adapt the context vector c for every output label prediction [7].", "startOffset": 156, "endOffset": 159}, {"referenceID": 15, "context": "We use the attention mechanism proposed by [16], where the context vector ct at time t is given by:", "startOffset": 43, "endOffset": 47}, {"referenceID": 14, "context": "[15].", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "We consider two variants of local attention, based on [17], described below.", "startOffset": 54, "endOffset": 58}, {"referenceID": 16, "context": "One indirect way of addressing this potential problem is by concatenating the previous attention-weighted context vector with the input for the next decoder time step, as done in [17].", "startOffset": 179, "endOffset": 183}, {"referenceID": 0, "context": "For all three data sets, we use the same experimental setup as in [1], which we briefly describe below.", "startOffset": 66, "endOffset": 69}, {"referenceID": 17, "context": "We use minibatch stochastic gradient descent (SGD) together with Adam [18] using a minibatch size of 256.", "startOffset": 70, "endOffset": 74}, {"referenceID": 18, "context": "To prevent overfitting we: (a) introduce a dropout [19] layer between every pair of consecutive layers of the stacked LSTMs, and (b) use scheduled sampling [20], with a linear decay, on the decoder side.", "startOffset": 51, "endOffset": 55}, {"referenceID": 19, "context": "To prevent overfitting we: (a) introduce a dropout [19] layer between every pair of consecutive layers of the stacked LSTMs, and (b) use scheduled sampling [20], with a linear decay, on the decoder side.", "startOffset": 156, "endOffset": 160}, {"referenceID": 20, "context": "All of the models are implemented using TensorFlow [21].", "startOffset": 51, "endOffset": 55}, {"referenceID": 5, "context": "CMUDict BiDir LSTM + Alignment [6] 5.", "startOffset": 31, "endOffset": 34}, {"referenceID": 4, "context": "55 DBLSTM-CTC [5] 25.", "startOffset": 14, "endOffset": 17}, {"referenceID": 4, "context": "8 DBLSTM-CTC + 5-gram model [5] 21.", "startOffset": 28, "endOffset": 31}, {"referenceID": 5, "context": "Pronlex BiDir LSTM + Alignment [6] 6.", "startOffset": 31, "endOffset": 34}, {"referenceID": 5, "context": "NetTalk BiDir LSTM + Alignment [6] 7.", "startOffset": 31, "endOffset": 34}, {"referenceID": 5, "context": "For all three data sets, the best prior results to our knowledge with a single model are those of Yao and Zweig [6] with an alignment-based deep bidirectional LSTM.", "startOffset": 112, "endOffset": 115}, {"referenceID": 4, "context": "For CMUDict, a better WER was obtained by [5] by ensembling their CTC bidirectional LSTM with an alignment-based 5-gram model, but no corresponding PER was reported.", "startOffset": 42, "endOffset": 45}, {"referenceID": 16, "context": "Although the assumption of monotonic alignment turns out to be too simplistic for other tasks, such as machine translation [17], it is a reasonable choice for G2P.", "startOffset": 123, "endOffset": 127}, {"referenceID": 21, "context": "The importance of LSTM units can be gauged by the performance drop when they are replaced by Gated Recurrent Units (GRUs) [22] in a reverse unidirectional global attention model.", "startOffset": 122, "endOffset": 126}, {"referenceID": 22, "context": "This problem has also been observed in neural machine translation, and there has been some recent work to address it [23].", "startOffset": 117, "endOffset": 121}, {"referenceID": 23, "context": "In order to confirm this, we plot in Figure 3 a two-dimensional visualization, generated with t-SNE [24], of the phone embeddings learned by the global attention model.", "startOffset": 100, "endOffset": 104}, {"referenceID": 24, "context": "This issue has been studied in some prior work [25], but has not been explicitly addressed in current state-of-the-art methods.", "startOffset": 47, "endOffset": 51}], "year": 2016, "abstractText": "We propose an attention-enabled encoder-decoder model for the problem of grapheme-to-phoneme conversion. Most previous work has tackled the problem via joint sequence models that require explicit alignments for training. In contrast, the attention-enabled encoder-decoder model allows for jointly learning to align and convert characters to phonemes. We explore different types of attention models, including global and local attention, and our best models achieve state-of-the-art results on three standard data sets (CMUDict, Pronlex, and NetTalk).", "creator": "LaTeX with hyperref package"}}}