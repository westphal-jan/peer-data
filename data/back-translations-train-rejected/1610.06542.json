{"id": "1610.06542", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Oct-2016", "title": "Lexicons and Minimum Risk Training for Neural Machine Translation: NAIST-CMU at WAT2016", "abstract": "This year, the Nara Institute of Science and Technology (NAIST)/Carnegie Mellon University (CMU) submission to the Japanese-English translation track of the 2016 Workshop on Asian Translation was based on attentional neural machine translation (NMT) models. In addition to the standard NMT model, we make a number of improvements, most notably the use of discrete translation lexicons to improve probability estimates, and the use of minimum risk training to optimize the MT system for BLEU score. As a result, our system achieved the highest translation evaluation scores for the task.", "histories": [["v1", "Thu, 20 Oct 2016 19:10:09 GMT  (19kb)", "http://arxiv.org/abs/1610.06542v1", "To Appear in the Workshop on Asian Translation (WAT). arXiv admin note: text overlap witharXiv:1606.02006"]], "COMMENTS": "To Appear in the Workshop on Asian Translation (WAT). arXiv admin note: text overlap witharXiv:1606.02006", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["graham neubig"], "accepted": false, "id": "1610.06542"}, "pdf": {"name": "1610.06542.pdf", "metadata": {"source": "CRF", "title": "Lexicons and Minimum Risk Training for Neural Machine Translation: NAIST-CMU at WAT2016", "authors": ["Graham Neubig"], "emails": ["gneubig@cs.cmu.edu"], "sections": [{"heading": null, "text": "ar Xiv: 161 0.06 542v 1 [cs.C L] 20 Oct 201 6"}, {"heading": "1 Introduction", "text": "Neural machine translation (NMT; (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014)), the creation of translation models using neural networks, has rapidly achieved state-of-the-art results in a number of translation tasks (Luong and Manning, 2015; Sennrich et al., 2016a). In this paper, we describe NMT systems for the Japanese-English scientific translation task of the Workshop on Asian Translation (WAT) 2016 (Nakazawa et al., 2016a), focusing in particular on two: First, we follow the recent work of Arthur et al. (2016) by including discrete translation lexicons to improve the probability estimates of the neural translation model (\u00a7 3); second, we include English courses (Shen et al., 2016) to optimize the results of these translation tasks (5 in total)."}, {"heading": "2 Baseline Neural Machine Translation Model", "text": "Our basic translation model is the attention model implemented in the Lamtram toolkit (Neubig, 2015), which is a combination of the models of Bahdanau et al. (2015) and Luong et al. (2015) that we considered to be effective."}, {"heading": "2.1 Model Structure", "text": "Our model generates a model of the target sentence E = e | 1 given source sentence F = f | F | 1. These words belong to the source vocabulary Vf, and the target vocabulary Ve \u2212 \u2212 \u2212 respectively. NMT performs this translation by performing the conditional probability pm (ei | F, e \u2212 1) of the target word ei \u2212 1 and the preceding target words ei \u2212 11, using the context < F, ei \u2212 1 > as afixed-width vector i, and calculating the probability calculation as follows: pm (ei, e \u2212 1) = softmax (Ws8 + bs), (1) where Ws and bs are respectively weight matrix and bias parameters. The exact variety of the NMT model depends on how we use it as input, and as mentioned above."}, {"heading": "2.2 Parameter Optimization", "text": "If we define all the parameters in this model as \u03b8, we can train the model by reducing the negative log probability of the training data sets. Specifically, we use the ADAM Optimizer (Kingma and Ba, 2014) with an initial learning rate of 0.001. Minibatches of 2048 words are generated by sorting sentences in descending order and grouping sentences sequentially, adding sentences to the next sentence, thereby exceeding the minibatch size of 2048 words. [1] Gradations are truncated so that their standard is no higher than 5. Training may run, with the probability of the development set being periodically reviewed (all 250k sentences processed), and the model that achieves the best probability on the development set being stored. As soon as no improvements have been observed on the development set, training is started at a pre-set learning rate of 0.00005% and the learning process is completed at 0.005%."}, {"heading": "2.3 Search", "text": "In order to find the best-rated translation, we perform a bar search with a bar size of 5 at the test time. At each step of the bar search, the hypothesis remaining in the bar is stored with the best score ending with the end of sentence symbol. At the point where the hypothesis with the highest score in the bar has a probability of less than or equal to the best end of sentence hypothesis, the search is terminated and the best end of sentence hypothesis is output as a translation. As NMT models often tend to shorter sentences, we add an optional \"word penalty\" \u03bb, where the probability of each hypothesis is multiplied for comparison with other hypotheses of varying lengths."}, {"heading": "3 Incorporating Discrete Lexicons", "text": "The first modification that we adapt to the basic model is the inclusion of discrete lexicon to improve translation probabilities, according to Arthur et al. (2016). The motivation behind this method is twofold: editing low-frequency words could improve the translation probability of these words. (Dyer et al., 2013), and by applying the neural MT system with these efficiently trained alignments, it is easier to learn models that achieve good results. The model starts with lexical translation probabilities pl (e) for individual words obtained through traditional word alignment methods. These probabilities must first be converted into a form that can be used together with pm (ei)."}, {"heading": "4 Minimum Risk Training", "text": "The second improvement we are making to our model is the use of a risk training with minimal risk. As mentioned in Section 2.2, our base model optimizes the model parameters according to the maximum probability of the training data. However, there is a discrepancy between the evaluation of our systems using translation accuracy (such as BLEU (Papineni et al., 2002) and this maximum probability of objectification. To eliminate this discrepancy, we use the method of Shen et al. (2016) to optimize our systems directly using the BLEU score. Specifically, we define the following loss function using the model parameters \u03b8 for a single training set pair < F, E > LF, E (\u03b8) = E \u2032 err (E \u2032) P (E \u2032 | F; disruption), which is summed up as all potential translations E \u2032 in the target language."}, {"heading": "5 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 Experimental Setup", "text": "To generate data to train the model, we use the uppermost 2M sets of the Japanese-English training corpus of ASPEC (Nakazawa et al., 2016b) provided by the task. Japanese size of the corpus is symbolized by KyTea (Neubig et al., 2011) and the English side by the tokenizer of the Travatar toolkit (Neubig, 2013). Japanese is further normalized so that all full-width Roman characters and digits are normalized to half-width."}, {"heading": "5.2 Experimental Results", "text": "In Figure 1 we show results for different settings in terms of attention, the use of lexicon, training criteria and word penalties. In addition, we calculate the total set of 6 models, using the average probability assigned to each of the models to determine the probability of the next word at the test date. We can read a number of observations from the results in the table. Use of lexicon: Comparing (1) with (2-4), we can see that the use of lexicon generally tends to have an advantage, especially when the \"parameter\" is set to a small value. Type of attention: Comparing (2-4) with (5-7), we can see that attention to several layers was more effective than the use of the point product. Use of word penalties: Comparing the first and second columns of the results results results results in a large increase in accuracy across the whole line when using a word."}, {"heading": "5.3 Manual Evaluation Results", "text": "The most probability-trained ensemble system with a word penalty of 0.8 (the lower middle system in Table 1) was submitted for manual evaluation; the system was evaluated according to the official WAT metric \"HUMAN\" (Nakazawa et al., 2016a), which consists of pairs of comparisons with a phrase-based system, in which the rated system receives + 1 for each win, -1 for each tie, 0 for each loss, these values are averaged over all rated sentences, then the value is multiplied by 100. This system achieved a manual evaluation of 47.50, which was slightly higher than the other systems involved in the task. Furthermore, while the complete results of the minimally risk-based ensemble were not ready in time for the manual evaluation phase, a provisional system similar to the minimally high-risk versions of the first four systems (1) - (4) in Table 1 (its BLEU / RIBES scores were comparable to those of the small system, although they received a grade of 425) was submitted for manual evaluation."}, {"heading": "6 Conclusion", "text": "In this paper, we described the NAIST-CMU system for the Japanese-English task at WAT, which achieved the most accurate results on this language pair. In particular, the inclusion of discrete translation lexicons and minimal risk training proved useful to achieve these results. Acknowledgements: This work was supported by JSPS KAKENHI grant number 25730136."}], "references": [{"title": "Incorporating discrete translation lexicons into neural machine translation", "author": ["Arthur et al.2016] Philip Arthur", "Graham Neubig", "Satoshi Nakamura"], "venue": "In Proc. EMNLP", "citeRegEx": "Arthur et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Arthur et al\\.", "year": 2016}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Kyunghyun Cho", "Yoshua Bengio"], "venue": "In Proc. ICLR", "citeRegEx": "Bahdanau et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Neural probabilistic language models", "author": ["Bengio et al.2006] Yoshua Bengio", "Holger Schwenk", "Jean-S\u00e9bastien Sen\u00e9cal", "Fr\u00e9deric Morin", "Jean-Luc Gauvain"], "venue": "In Innovations in Machine Learning,", "citeRegEx": "Bengio et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2006}, {"title": "A simple, fast, and effective reparameterization of ibm model 2", "author": ["Dyer et al.2013] Chris Dyer", "Victor Chahuneau", "Noah A. Smith"], "venue": "In Proc. NAACL,", "citeRegEx": "Dyer et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Dyer et al\\.", "year": 2013}, {"title": "LSTM: A search space odyssey", "author": ["Greff et al.2015] Klaus Greff", "Rupesh Kumar Srivastava", "Jan Koutn\u0131\u0301k", "Bas R. Steunebrink", "J\u00fcrgen Schmidhuber"], "venue": null, "citeRegEx": "Greff et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Greff et al\\.", "year": 2015}, {"title": "Long short-term memory", "author": ["Hochreiter", "Schmidhuber1997] Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Recurrent continuous translation models", "author": ["Kalchbrenner", "Blunsom2013] Nal Kalchbrenner", "Phil Blunsom"], "venue": "In Proc. EMNLP,", "citeRegEx": "Kalchbrenner et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2013}, {"title": "Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980", "author": ["Kingma", "Ba2014] Diederik Kingma", "Jimmy Ba"], "venue": null, "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Orange: a method for evaluating automatic evaluation metrics for machine translation", "author": ["Lin", "Och2004] Chin-Yew Lin", "Franz Josef Och"], "venue": "In Proc. COLING,", "citeRegEx": "Lin et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2004}, {"title": "Stanford neural machine translation systems for spoken language domains", "author": ["Luong", "Manning2015] Minh-Thang Luong", "Christopher D Manning"], "venue": "In Proc. IWSLT", "citeRegEx": "Luong et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Effective approaches to attention-based neural machine translation", "author": ["Luong et al.2015] Thang Luong", "Hieu Pham", "Christopher D. Manning"], "venue": "In Proc. EMNLP,", "citeRegEx": "Luong et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Pointwise prediction for robust, adaptable Japanese morphological analysis", "author": ["Neubig et al.2011] Graham Neubig", "Yosuke Nakata", "Shinsuke Mori"], "venue": "In Proc. ACL,", "citeRegEx": "Neubig et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Neubig et al\\.", "year": 2011}, {"title": "Travatar: A forest-to-string machine translation engine based on tree transducers", "author": ["Graham Neubig"], "venue": "In Proc. ACL Demo Track,", "citeRegEx": "Neubig.,? \\Q2013\\E", "shortCiteRegEx": "Neubig.", "year": 2013}, {"title": "lamtram: A toolkit for language and translation modeling using neural networks. http://www.github.com/neubig/lamtram", "author": ["Graham Neubig"], "venue": null, "citeRegEx": "Neubig.,? \\Q2015\\E", "shortCiteRegEx": "Neubig.", "year": 2015}, {"title": "BLEU: a method for automatic evaluation of machine translation", "author": ["Salim Roukos", "Todd Ward", "Wei-Jing Zhu"], "venue": "In Proc. ACL,", "citeRegEx": "Papineni et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "2016a. Edinburgh neural machine translation systems for WMT16", "author": ["Barry Haddow", "Alexandra Birch"], "venue": "In Proc. WMT,", "citeRegEx": "Sennrich et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Sennrich et al\\.", "year": 2016}, {"title": "Neural machine translation of rare words with subword units", "author": ["Barry Haddow", "Alexandra Birch"], "venue": "In Proc. ACL,", "citeRegEx": "Sennrich et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Sennrich et al\\.", "year": 2016}, {"title": "Minimum risk training for neural machine translation", "author": ["Shen et al.2016] Shiqi Shen", "Yong Cheng", "Zhongjun He", "Wei He", "Hua Wu", "Maosong Sun", "Yang Liu"], "venue": "In Proc. ACL,", "citeRegEx": "Shen et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Shen et al\\.", "year": 2016}, {"title": "Sequence to sequence learning with neural networks", "author": ["Oriol Vinyals", "Quoc VV Le"], "venue": "In Proc. NIPS,", "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 18, "context": "1 Introduction Neural machine translation (NMT; (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014)), creation of translation models using neural networks, has quickly achieved state-of-the-art results on a number of translation tasks (Luong and Manning, 2015; Sennrich et al.", "startOffset": 48, "endOffset": 104}, {"referenceID": 1, "context": "The systems are built using attentional neural networks (Bahdanau et al., 2015; Luong et al., 2015), with a number of improvements (\u00a72).", "startOffset": 56, "endOffset": 99}, {"referenceID": 9, "context": "The systems are built using attentional neural networks (Bahdanau et al., 2015; Luong et al., 2015), with a number of improvements (\u00a72).", "startOffset": 56, "endOffset": 99}, {"referenceID": 17, "context": "Second, we incorporate minimum-risk training (Shen et al., 2016) to optimize the parameters of the model to improve translation accuracy (\u00a74).", "startOffset": 45, "endOffset": 64}, {"referenceID": 0, "context": "First, we follow the recent work of Arthur et al. (2016) in incorporating discrete translation lexicons to improve the probability estimates of the neural translation model (\u00a73).", "startOffset": 36, "endOffset": 57}, {"referenceID": 13, "context": "2 Baseline Neural Machine Translation Model Our baseline translation model is the attentional model implemented in the lamtram toolkit (Neubig, 2015), which is a combination of the models of Bahdanau et al.", "startOffset": 135, "endOffset": 149}, {"referenceID": 1, "context": "2 Baseline Neural Machine Translation Model Our baseline translation model is the attentional model implemented in the lamtram toolkit (Neubig, 2015), which is a combination of the models of Bahdanau et al. (2015) and Luong et al.", "startOffset": 191, "endOffset": 214}, {"referenceID": 1, "context": "2 Baseline Neural Machine Translation Model Our baseline translation model is the attentional model implemented in the lamtram toolkit (Neubig, 2015), which is a combination of the models of Bahdanau et al. (2015) and Luong et al. (2015) that we found to be effective.", "startOffset": 191, "endOffset": 238}, {"referenceID": 2, "context": "Here the embed(\u00b7) function maps the words into a representation (Bengio et al., 2006), and enc(\u00b7) is long short term memory (LSTM) neural network (Hochreiter and Schmidhuber, 1997) with forget gates set to one minus the value of the input gate (Greff et al.", "startOffset": 64, "endOffset": 85}, {"referenceID": 4, "context": ", 2006), and enc(\u00b7) is long short term memory (LSTM) neural network (Hochreiter and Schmidhuber, 1997) with forget gates set to one minus the value of the input gate (Greff et al., 2015).", "startOffset": 166, "endOffset": 186}, {"referenceID": 9, "context": "In our systems, we test two similarity functions, the dot product (Luong et al., 2015) sim(hi, rj) := h \u22ba i rj (4) and the multi-layered perceptron (Bahdanau et al.", "startOffset": 66, "endOffset": 86}, {"referenceID": 1, "context": ", 2015) sim(hi, rj) := h \u22ba i rj (4) and the multi-layered perceptron (Bahdanau et al., 2015)", "startOffset": 69, "endOffset": 92}, {"referenceID": 18, "context": "The motivation behind this method is twofold: Handling low-frequency words: Neural machine translation systems tend to have trouble translating low-frequency words (Sutskever et al., 2014), so incorporating translation lexicons with good coverage of content words could improve translation accuracy of these words.", "startOffset": 164, "endOffset": 188}, {"referenceID": 3, "context": "Training speed: Training the alignments needed for discrete lexicons can be done efficiently (Dyer et al., 2013), and by seeding the neural MT system with these efficiently trained alignments it is easier to learn models that achieve good results more quickly.", "startOffset": 93, "endOffset": 112}, {"referenceID": 0, "context": "3 Incorporating Discrete Lexicons The first modification that we make to the base model is incorporating discrete lexicons to improve translation probabilities, according to the method of Arthur et al. (2016). The motivation behind this method is twofold: Handling low-frequency words: Neural machine translation systems tend to have trouble translating low-frequency words (Sutskever et al.", "startOffset": 188, "endOffset": 209}, {"referenceID": 14, "context": "However, there is a disconnect between the evaluation of our systems using translation accuracy (such as BLEU (Papineni et al., 2002)) and this maximum likelihood objective.", "startOffset": 110, "endOffset": 133}, {"referenceID": 14, "context": "However, there is a disconnect between the evaluation of our systems using translation accuracy (such as BLEU (Papineni et al., 2002)) and this maximum likelihood objective. To remove this disconnect, we use the method of Shen et al. (2016) to optimize our systems directly using BLEU score.", "startOffset": 111, "endOffset": 241}, {"referenceID": 11, "context": "The Japanese size of the corpus is tokenized using KyTea (Neubig et al., 2011), and the English side is tokenized with the tokenizer provided with the Travatar toolkit (Neubig, 2013).", "startOffset": 57, "endOffset": 78}, {"referenceID": 12, "context": ", 2011), and the English side is tokenized with the tokenizer provided with the Travatar toolkit (Neubig, 2013).", "startOffset": 97, "endOffset": 111}], "year": 2016, "abstractText": "This year, the Nara Institute of Science and Technology (NAIST)/Carnegie Mellon University (CMU) submission to the Japanese-English translation track of the 2016 Workshop on Asian Translation was based on attentional neural machine translation (NMT) models. In addition to the standard NMT model, we make a number of improvements, most notably the use of discrete translation lexicons to improve probability estimates, and the use of minimum risk training to optimize the MT system for BLEU score. As a result, our system achieved the highest translation evaluation scores for the task.", "creator": "LaTeX with hyperref package"}}}