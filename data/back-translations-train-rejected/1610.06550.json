{"id": "1610.06550", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Oct-2016", "title": "Neural Machine Translation with Characters and Hierarchical Encoding", "abstract": "Most existing Neural Machine Translation models use groups of characters or whole words as their unit of input and output. We propose a model with a hierarchical char2word encoder, that takes individual characters both as input and output. We first argue that this hierarchical representation of the character encoder reduces computational complexity, and show that it improves translation performance. Secondly, by qualitatively studying attention plots from the decoder we find that the model learns to compress common words into a single embedding whereas rare words, such as names and places, are represented character by character.", "histories": [["v1", "Thu, 20 Oct 2016 19:33:02 GMT  (592kb,D)", "http://arxiv.org/abs/1610.06550v1", "8 pages, 7 figures"]], "COMMENTS": "8 pages, 7 figures", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["alexander rosenberg johansen", "jonas meinertz hansen", "elias khazen obeid", "casper kaae s{\\o}nderby", "ole winther"], "accepted": false, "id": "1610.06550"}, "pdf": {"name": "1610.06550.pdf", "metadata": {"source": "CRF", "title": "NEURAL MACHINE TRANSLATION WITH CHARACTERS AND HIERARCHICAL ENCODING", "authors": ["Alexander Rosenberg Johansen", "Jonas Meinertz Hansen", "Elias Khazen Obeid", "Casper Kaae S\u00f8nderby", "Ole Winther"], "emails": [], "sections": [{"heading": "1. INTRODUCTION", "text": "In fact, most people who stay in the city go to another world in order to live where they are: in a world in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they, in which they live, in which they, in which they, in which they live, in which they, in which they live, in which they, in which they, in which they live."}, {"heading": "2. RELATED WORK", "text": "Other approaches to circumvent the increase in sequence lengths while reducing the dictionary size have been suggested: First, byte-pair encoding (BPE) [Sennrich et al., 2015], where the dictionary is a combination of the most common characters. In practice, this means that most words are encoded with fewer bits than with less common words. Second, a hybrid model [Luong and Manning, 2016] where a word encoder decoder consults a character encoder when confronted with foreign words. Third, pre-trained word-based encoder decoder models with character inputs used for word embedding [Ling et al., 2015] have shown that similar results can be achieved to word-based approaches. Last mention may be the character decoder with BPE."}, {"heading": "3. MATERIALS AND METHODS", "text": "First we will give a brief description of the Neural Machine Translation model, then we will explain in detail our proposed architecture for character and word segmentation."}, {"heading": "3.1. Neural Machine Translation", "text": "From a probabilistic perspective, translation can be defined by maximizing the conditional probability of arg maxy p (y | x), where y1, y2,.. yTy is the target sequence and x1, x2,.. xTx is the source sequence.The conditional probability p (y | x) is modelled by an encoder-decoder model, in which the encoder and decoder are modelled by separate recursive neural networks (RNNs) and the entire model is trained end-to-end on a large parallel corpus.The model uses memory-based RNN variants as they allow the modelling of longer dependencies in the sequences [Cho et al., 2014b, Hochreiter and Schmidhuber, 1997].The encoder part (input RNN) calculates a number of hidden representations, h1, h2, hTx, based on the input part (xt)."}, {"heading": "3.1.1. Attention", "text": "As described in section 1, the attention mechanism can calculate a new context vector for each time step by combining the hidden representations of the encoder and the previous hidden state st \u2212 1 of the decoder = Tx \u2211 j = 1 atjhj (6), calculating the weight parameter atj of each comment hj asatj = exp (etj) \u2211 Tx k exp (etk) (7) and having the etj = a (st \u2212 1, hj) (8), where atj and etj represent the meaning of the previous decoder state \u2212 1. The attention function, a, is a nonlinear, possibly multi-layered neural network. The encoder decoder and the attention model are trained together to minimize the loss function."}, {"heading": "4. OUR MODEL", "text": "We propose two models: the char-to-char NMT model and the char2word-to-char NMT model. Both models are based on the encoder-decoder model with attention as defined in Section 3.1 and Section 3.1.1. In the following we will give specific model definitions. The char encoderOur character-level encoder (referred to as the char encoder) is based on a bidirectional RNN (1). The encoder function, ht, in Equation (1) becomesht = [Ext, \u2212 ht \u2212 1) hb (Ext, -) hb (Ext, - ht \u2212 ht \u2212)."}, {"heading": "4.3.1. Attention mechanism", "text": "The attention model a (defined in Equation (14)) is used to calculate the context ct for the time step t used by the decoder to perform variable length attention. Attention function, a, has been parameterized asa (st \u2212 1, hj) = v T a tanh (Wast \u2212 1 + Uahj + ba), (14) where Wa-Rms \u00b7 ms, Ua-Rms \u00b7 mh, va-Rmh, ba-Rmh, ms the number of hidden units in the decoder mh is the number of hidden units in the encoder. Since Uahj does not depend on t, we can calculate it in advance for optimization purposes."}, {"heading": "4.3.2. Output function", "text": "The output of the decoder g (yt \u2212 1, st, ct) is based on a linear combination of the current hidden state in the decoder st, followed by a Softmax function.g (yt \u2212 1, st, ct) = exp (Wyst + by) \u2211 K = 1 exp (Wysi + by), (15) where Wy-RK \u00b7 ms is the set of output classes. We use the same decoder taking into account both the character and the Char2word-to-character model shown in Figure 1 and Figure 3, the main difference being that our Figure 1 model has significantly fewer units to service."}, {"heading": "5. EXPERIMENTS", "text": "All models were evaluated using the BLEU-Score1 [Papineni et al., 2002]."}, {"heading": "5.1. Data and Preprocessing", "text": "We trained our models on two different sets of language pairs from WMT '15: En-De (4.5 M) and De-En (4.5 M). For validation we used newstest2013 and for testing newstest2014 and newstest2015. Data pre-processing is identical to Chung et al. [2016] for En-De and De-Enwith a set sentence length of 250 characters instead of 50 BPE units. In short, we normalize punctuation and tokenization using Mosesscripts2. We exclude all samples where the source sentence exceeds 250 characters and the target sentence exceeds 500 characters. Source and target languages have separate dictionaries with 300 common characters each. Characters that are not in the dictionary are replaced by an unknown token."}, {"heading": "5.2. Training details", "text": "The hyperparameters of the model are in Tables 1 and 2. For the RNN functions in the encoder and decoder, we use gated recurrent units (GRU) [Cho et al., 2014b]. For training, we use backpropagation with stochastic gradient descent with the Adam optimizer [Kingma and Ba, 2014] with a learning rate of \u03b1 = 0.001. For L2 regulation, we use \u03bb = 1 \u00d7 10 \u2212 6. To stabilize the training and avoid exploding gradients, the norms of the gradients are truncated to a threshold of 1 before updating the parameters. All models are implemented with TensorFlow [Abadi et al., 2016] and the code and details of the setup are available on GitHub3."}, {"heading": "5.2.1. Batch details", "text": "When training with stacks, all sequences must be padded to match the longest sequence in the stack, and recurring layers must perform the full set of calculations for all samples and all time frames, which can lead to a lot of wasted resources [Hannun et al., 2014] (see Figure 4). Training translation models is complicated by the fact that while source and target sentences may be correlated, they may have different lengths, and it is necessary to consider both when building stacks to make the most of computing power and RAM. To get around this problem, we start each era by mixing all samples in the dataset and sorting them by source and target set using a stable sorting algorithm."}, {"heading": "5.3. Results", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.3.1. Quantitative", "text": "The quantitative results of our models are in Table 3. Note that the char2word-to-char model outperforms the char-to-charmodel in all datasets (average performance increase of 1.28 BLEU), which may indicate that either hierarchical word-like representations on the encoder, or simply the fact that the encoder was significantly smaller, are helpful when using a character encoder in NMT."}, {"heading": "5.3.2. Qualitative", "text": "The representation of the weights of atj (defined in Equation (14)) is popular in NMT research, as these give an indication of where the model found relevant information during decoding. We have diagrams of our character-to-chart and Char2word-to-sign models in Figures 6 and 7. The more intense the blue color, the higher the values of atj at this point. Note that each column corresponds to the decoding of a single unit, which results in each column being summed up to 1. Interestingly, the attention diagrams from character to character accompanying each character indicate that words that are normally considered as out-of-dictionary (see Lisette Verhaig in Figure 6) are translated letter by letter, whereas common words at the end / beginning of each word 4 are treated to be used as individual words. This observation could explain why the use of hierarchical type of hybrid coding works by hybrid models (2016) and PE-based on the same hybrid mode of coding (2016)."}, {"heading": "6. CONCLUSION", "text": "We have carefully proposed a purely character-based encoder decoder model that uses hierarchical encoding. We note that using our newly proposed character encoding mechanism, hierarchical encoding improves the BLEU score by an average of 1.28, compared to models that use a standard character encoder.Qualitatively, we note that the attention of a character model without hierarchical encoding learns to create hierarchical representations even without being explicitly prompted to switch between word and character embeddings for common and rare words. This observation is consistent with recent research on byte-pair encoding and hybrid word-character models, as these models use words such as embeddings for common words and undersigned characters or characters for rare words. In addition, we qualitatively determine that our hierarchical encoding finds lexemes in the source set, when they become available at the end of larger encoding models, as well as current ones with much larger encoding."}], "references": [{"title": "Tensorflow: Large-scale machine learning on heterogeneous distributed systems", "author": ["ter", "Jonathon Shlens", "Benoit Steiner", "Ilya Sutskever", "Kunal Talwar", "Paul A. Tucker", "Vincent Vanhoucke", "Vijay Vasudevan", "Fernanda B. Vi\u00e9gas", "Oriol Vinyals", "Pete Warden", "Martin Wattenberg", "Martin Wicke", "Yuan Yu", "Xiaoqiang Zheng"], "venue": "CoRR, abs/1603.04467,", "citeRegEx": "ter et al\\.,? \\Q2016\\E", "shortCiteRegEx": "ter et al\\.", "year": 2016}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio"], "venue": "CoRR, abs/1409.0473,", "citeRegEx": "Bahdanau et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "On the properties of neural machine translation: Encoder-decoder approaches", "author": ["KyungHyun Cho", "Bart van Merrienboer", "Dzmitry Bahdanau", "Yoshua Bengio"], "venue": "CoRR, abs/1409.1259,", "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Learning phrase representations using RNN encoderdecoder for statistical machine translation", "author": ["Kyunghyun Cho", "Bart van Merrienboer", "\u00c7aglar G\u00fcl\u00e7ehre", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio"], "venue": "CoRR, abs/1406.1078,", "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "A character-level decoder without explicit segmentation for neural machine translation", "author": ["Junyoung Chung", "Kyunghyun Cho", "Yoshua Bengio"], "venue": "CoRR, abs/1603.06147,", "citeRegEx": "Chung et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Chung et al\\.", "year": 2016}, {"title": "Deep speech: Scaling up end-to-end speech recognition", "author": ["Awni Y. Hannun", "Carl Case", "Jared Casper", "Bryan Catanzaro", "Greg Diamos", "Erich Elsen", "Ryan Prenger", "Sanjeev Satheesh", "Shubho Sengupta", "Adam Coates", "Andrew Y. Ng"], "venue": "CoRR, abs/1412.5567,", "citeRegEx": "Hannun et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Hannun et al\\.", "year": 2014}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural Comput.,", "citeRegEx": "Hochreiter and Schmidhuber.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "On using very large target vocabulary for neural machine translation", "author": ["S\u00e9bastien Jean", "Kyunghyun Cho", "Roland Memisevic", "Yoshua Bengio"], "venue": "CoRR, abs/1412.2007,", "citeRegEx": "Jean et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Jean et al\\.", "year": 2014}, {"title": "Recurrent continuous translation models", "author": ["Nal Kalchbrenner", "Phil Blunsom"], "venue": "In EMNLP,", "citeRegEx": "Kalchbrenner and Blunsom.,? \\Q2013\\E", "shortCiteRegEx": "Kalchbrenner and Blunsom.", "year": 2013}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik P. Kingma", "Jimmy Ba"], "venue": "CoRR, abs/1412.6980,", "citeRegEx": "Kingma and Ba.,? \\Q2014\\E", "shortCiteRegEx": "Kingma and Ba.", "year": 2014}, {"title": "Ask me anything: Dynamic memory networks for natural language processing", "author": ["Ankit Kumar", "Ozan Irsoy", "Jonathan Su", "James Bradbury", "Robert English", "Brian Pierce", "Peter Ondruska", "Ishaan Gulrajani", "Richard Socher"], "venue": "CoRR, abs/1506.07285,", "citeRegEx": "Kumar et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kumar et al\\.", "year": 2015}, {"title": "Character-based neural machine translation", "author": ["Wang Ling", "Isabel Trancoso", "Chris Dyer", "Alan W. Black"], "venue": "CoRR, abs/1511.04586,", "citeRegEx": "Ling et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ling et al\\.", "year": 2015}, {"title": "Achieving open vocabulary neural machine translation with hybrid word-character models", "author": ["Minh-Thang Luong", "Christopher D. Manning"], "venue": "CoRR, abs/1604.00788,", "citeRegEx": "Luong and Manning.,? \\Q2016\\E", "shortCiteRegEx": "Luong and Manning.", "year": 2016}, {"title": "Addressing the rare word problem in neural machine translation", "author": ["Thang Luong", "Ilya Sutskever", "Quoc V. Le", "Oriol Vinyals", "Wojciech Zaremba"], "venue": "CoRR, abs/1410.8206,", "citeRegEx": "Luong et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Luong et al\\.", "year": 2014}, {"title": "Bleu: A method for automatic evaluation of machine translation", "author": ["Kishore Papineni", "Salim Roukos", "Todd Ward", "Wei-Jing Zhu"], "venue": "In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics,", "citeRegEx": "Papineni et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "Bidirectional recurrent neural networks", "author": ["M. Schuster", "K.K. Paliwal"], "venue": "Trans. Sig. Proc.,", "citeRegEx": "Schuster and Paliwal.,? \\Q1997\\E", "shortCiteRegEx": "Schuster and Paliwal.", "year": 1997}, {"title": "Japanese and korean voice search", "author": ["Mike Schuster", "Kaisuke Nakajima"], "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "Schuster and Nakajima.,? \\Q2012\\E", "shortCiteRegEx": "Schuster and Nakajima.", "year": 2012}, {"title": "Neural machine translation of rare words with subword units", "author": ["Rico Sennrich", "Barry Haddow", "Alexandra Birch"], "venue": "CoRR, abs/1508.07909,", "citeRegEx": "Sennrich et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sennrich et al\\.", "year": 2015}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V. Le"], "venue": "CoRR, abs/1409.3215,", "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Google\u2019s neural machine translation system: Bridging the gap between human and machine translation", "author": ["Yonghui Wu", "Mike Schuster", "Zhifeng Chen", "Quoc V Le", "Mohammad Norouzi", "Wolfgang Macherey", "Maxim Krikun", "Yuan Cao", "Qin Gao", "Klaus Macherey"], "venue": "arXiv preprint arXiv:1609.08144,", "citeRegEx": "Wu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Wu et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 1, "context": "Early NMT models introduced by Cho et al. [2014a], Kalchbrenner and Blunsom [2013], Sutskever et al.", "startOffset": 31, "endOffset": 50}, {"referenceID": 1, "context": "Early NMT models introduced by Cho et al. [2014a], Kalchbrenner and Blunsom [2013], Sutskever et al.", "startOffset": 31, "endOffset": 83}, {"referenceID": 1, "context": "Early NMT models introduced by Cho et al. [2014a], Kalchbrenner and Blunsom [2013], Sutskever et al. [2014] are based on the encoder-decoder network architecture.", "startOffset": 31, "endOffset": 108}, {"referenceID": 1, "context": "To overcome this shortcoming, the attention mechanism proposed by Bahdanau et al. [2014] assists the decoder by learning to selectively attend to parts of the input sequence, which it deems most relevant for generating the next element in the output sequence and effectively reducing the distance from encoding to decoding.", "startOffset": 66, "endOffset": 89}, {"referenceID": 12, "context": "Further, when all words are represented as separate entities, the model has to learn how every word is related to one-another, which can be challenging for rare words even if they are obvious variations of more frequent words [Luong and Manning, 2016].", "startOffset": 226, "endOffset": 251}, {"referenceID": 17, "context": "Other approaches to circumventing the increase in sequence lengths while reducing the dictionary size have been proposed: First, byte-pair Encoding (BPE) [Sennrich et al., 2015], currently holding state-of-the-art in the WMT\u201914 English-to-French and English-to-German [Wu et al.", "startOffset": 154, "endOffset": 177}, {"referenceID": 19, "context": ", 2015], currently holding state-of-the-art in the WMT\u201914 English-to-French and English-to-German [Wu et al., 2016], where the dictionary is a combination of the most common characters.", "startOffset": 98, "endOffset": 115}, {"referenceID": 12, "context": "Secondly, a hybrid model [Luong and Manning, 2016] where a word encoder-decoder consults a character encoder-decoder when confronted with out-of-dictionary words.", "startOffset": 25, "endOffset": 50}, {"referenceID": 11, "context": "Thirdly, pretrained word-based encoder-decoder models with character input used for creating word embeddings [Ling et al., 2015] have been shown to achieve similar results to word-based approaches.", "startOffset": 109, "endOffset": 128}, {"referenceID": 4, "context": "As a last mention, character decoder with BPE encoder has shown to be end-to-end trained successfully [Chung et al., 2016].", "startOffset": 102, "endOffset": 122}, {"referenceID": 10, "context": "The RNN encoderdecoder with attention approach is used not only within machine translation, but can be regarded as a general architecture to extract information from a sequence and answer some type of question related to it [Kumar et al., 2015].", "startOffset": 224, "endOffset": 244}, {"referenceID": 4, "context": "As a last mention, character decoder with BPE encoder has shown to be end-to-end trained successfully [Chung et al., 2016]. Wu et al. [2016] provides a good summary and large-scale demonstration of many of the techniques that are known to work well for NMT and RNNs in general.", "startOffset": 103, "endOffset": 141}, {"referenceID": 15, "context": "Our character-level encoder (referred to as the char encoder) is built upon a bi-directional RNN [Schuster and Paliwal, 1997].", "startOffset": 97, "endOffset": 125}, {"referenceID": 1, "context": "The decoder uses a RNN and the attention mechanism [Bahdanau et al., 2014] when decoding each character.", "startOffset": 51, "endOffset": 74}, {"referenceID": 14, "context": "All models were evaluated using the BLEU score1 [Papineni et al., 2002].", "startOffset": 48, "endOffset": 71}, {"referenceID": 4, "context": "The data preprocessing applies is identical to Chung et al. [2016] on En-De and De-Enwith the source sentence length set to 250 characters instead of 50 BPE units.", "startOffset": 47, "endOffset": 67}, {"referenceID": 9, "context": "For training we use back-propagation with stochastic-gradient descent using the Adam optimiser [Kingma and Ba, 2014] with a learning rate of \u03b1 = 0.", "startOffset": 95, "endOffset": 116}, {"referenceID": 5, "context": "When training with batches, all sequences must be padded to match the longest sequence in the batch, and the recurrent layers must do the full set of computations for all samples and all timesteps, which can result in a lot of wasted resources [Hannun et al., 2014] (see figure 4).", "startOffset": 244, "endOffset": 265}, {"referenceID": 11, "context": "BPE based models and the hybrid word-char model by Luong and Manning [2016] effectively works in the same manner, when translating common words BPE- and hybrid word-char models will work on a word level, whereas with rare words the BPE will work with sub-parts of the word (maybe even characters) and the hybrid approach will use character representations.", "startOffset": 51, "endOffset": 76}, {"referenceID": 4, "context": "The attention plot seems very similar to the BPE-toChar plot proposed by Chung et al. [2016]. This might indicate that it is possible to imitate lexeme (word) based models using smaller dictionaries and preserving relationship between words.", "startOffset": 73, "endOffset": 93}], "year": 2016, "abstractText": "Most existing Neural Machine Translation models use groups of characters or whole words as their unit of input and output. We propose a model with a hierarchical char2word encoder, that takes individual characters both as input and output. We first argue that this hierarchical representation of the character encoder reduces computational complexity, and show that it improves translation performance. Secondly, by qualitatively studying attention plots from the decoder we find that the model learns to compress common words into a single embedding whereas rare words, such as names and places, are represented character by character.", "creator": "LaTeX with hyperref package"}}}