{"id": "1610.06602", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Oct-2016", "title": "Iterative Refinement for Machine Translation", "abstract": "Existing machine translation decoding algorithms generate translations in a strictly monotonic fashion and never revisit previous decisions. As a result, earlier mistakes cannot be corrected at a later stage. In this paper, we present a translation scheme that starts from an initial guess and then makes iterative improvements that may revisit previous decisions. We parameterize our model as a convolutional neural network that predicts discrete substitutions to an existing translation based on an attention mechanism over both the source sentence as well as the current translation output. By making less than one modification per sentence, we improve the output of a phrase-based translation system by up to 0.4 BLEU on WMT15 German-English translation.", "histories": [["v1", "Thu, 20 Oct 2016 20:54:07 GMT  (398kb,D)", "https://arxiv.org/abs/1610.06602v1", null], ["v2", "Wed, 26 Oct 2016 16:23:30 GMT  (398kb,D)", "http://arxiv.org/abs/1610.06602v2", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["roman novak", "michael auli", "david grangier"], "accepted": false, "id": "1610.06602"}, "pdf": {"name": "1610.06602.pdf", "metadata": {"source": "CRF", "title": "Iterative Refinement for Machine Translation", "authors": ["Roman Novak", "Michael Auli", "David Grangier"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "In fact, it is such that the translators are able to translate the translation by translating the translation into another language. The translation into another language, into the language, into the language, into the language, into the language, into the language, into the language, into the language, into the language, into the language, into the language, into the language, into the language, into the language, into the language, into the language, into the language, into the language, into the language, into the language, into the language, into the language, into the language, into the language, into the language, into the language, into the language, into the language, into the language, into the language, into the language, into the language, into the language, into the language, into the language, into the language, into the language, into the language, into the language, into the language, into the language, into the language, into the language, into the language, into the language, into the language, into the language, into the language, into the language, into the language, into the language, into the language, into the language, into the language, into the language, into the language, into the language, into the language, into the language, into the language, into the language, into the language, into the language, into the language, into the language, into the language, into the language, into the language, into the language, into the language, into the language, into the language, into the language, into the language, into the language, into the language, into the language, into the language, into the language, into the language, into the language, into the language, into the language, into the language, into the language, into the language, into the language, into the language, into the language, into the language, into the language, into the language, into the language, in the language, into the language, into the language, into the language, in the language, in the language, into the language, in the language, into the language, in the language, in the language, in the language of the language, in the language, in the language of the language, in the language, in the language, in the language, in the language of the language, in the language, in the language, in the language, in the language of the language, in the language, in the"}, {"heading": "2 Detecting Errors", "text": "Before correcting errors that we make with X =. \"X.\" Our task is to anticipate the errors in an existing translation system. \"We,\" says the author, \"can only recognize the errors in the translation from which we start and which were produced by a phrase-based translation (\u00a7 6.1), and\" yref, \"such as the reference translation.\" The sentences are vectors of indices that indicate an original vocabulary X or a target vocabulary Y. \"For an example, x = (x1).X.\" X. \"We have the vocabulary vocabulary vocabulary Y or a target vocabulary vocabulary vocabulary vocabulary vocabulary vocabulary vocabulary vocabulary.\" vocabulary vocabulary vocabulary vocabulary vocabulary vocabulary vocabulary vocabulary \"vocabulary vocabulary vocabulary vocabulary vocabulary\" vocabulary vocabulary vocabulary vocabulary."}, {"heading": "3 Attention-based Model", "text": "We introduce a model to predict modifications to a translation that can be trained on bilingual texts. In section 5 we discuss strategies for applying this model iteratively to its own output in order to improve a translation.Our model F takes a source sentence x and a target sentence y as input and outputs a distribution across the vocabulary for each target position, F (x, y), the probability that the word j is at position i and the target context y (y1,., yi \u2212 1, y) i, j estimates P (yi = j | x, y \u2212 i), the probability that the word j is at position i and the target context y \u2212 i = (y1,., yi \u2212 1, y |) the surrounding attention i. In other words, we learn a non-causal language model (Bengio et al, 2003) based on the source x.Architecture.We rely on a revolutionary model with fixed attention to the source space."}, {"heading": "4 Dual Attention Model", "text": "This means that we can use the whole assumption, including the middle word, compared to the assumption that we have to remove the middle word. In the training period, the dual attention model takes 3 inputs, that is, the assumption and the reference. In the test period, the reference input is replaced by the assumption. Specifically, the modelFdual (x, yg, yref) is taken in relation to the assumption. [0] yref | Y | estimates P (yiref | x, yg ref) for each position i in the reference set. The model is based on the single attention model by having two attention functions with different parameters."}, {"heading": "5 Iterative Refinement", "text": "The models in \u00a7 3 and \u00a7 4 suggest word substitutions for each position in the candidate translation yg taking into account the current context. The application of a single substitution changes the context of the surrounding words and requires an update of the model predictions. We therefore perform several rounds of substitutions. In each round, the model calculates its predictions, then our refinement strategy selects a substitution and executes it, unless the strategy decides that it can no longer improve the target sentence. This means that the refinement process should be able to (i) prioritize the proposed substitutions and (ii) decide to stop the iterative process. We determine the best treatment for each position i in yg by selecting the word with the highest probability estimate: yipred = arg max j, YF (x, yg) i, j. Then we calculate a trust value in this prediction s (yg, ypred) i, possibly taking into account the presupposition for the current processing of the same word."}, {"heading": "6 Experiments & Results", "text": "We first describe our experimental setup and then discuss our results."}, {"heading": "6.1 Experimental Setup", "text": "This year it has come to the point that it will be able to erenen.n"}, {"heading": "6.2 Results", "text": "This year it is more than ever before."}, {"heading": "7 Conclusion and Future Work", "text": "We present a simple iterative decoding scheme for machine translation, motivated by the inability of existing models to rethink erroneous decoding decisions of the past. Our models improve initial decoding by simple word substitutions over several rounds. In each round, the model has access to the source as well as the results of the previous round, which is a complete translation of the source. This is different from existing decoding algorithms that make predictions based on a limited partial translation and are unable to revise previous erroneous decoding decisions.Our results increase translation accuracy by up to 0.4 BLEU for the German-English translation of WMT15 and modify only 0.6 words per sentence. In our experimental setup, we start with the output of a phrase-based translation system, but our model is generic enough to deal with arbitrary guess translations (we are likely to see several future beginnings of a different edition of the translation experiment here)."}, {"heading": "Acknowledgments", "text": "We would like to thank Marc'Aurelio Ranzato and Sumit Chopra for the helpful conversations in connection with this work."}, {"heading": "A Examples", "text": "It is not the first time that the EU Commission has taken such a step."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "Proc. of ICLR. Association for Computational Linguistics, May.", "citeRegEx": "Bahdanau et al\\.,? 2015", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "A neural probabilistic language model", "author": ["Yoshua Bengio", "R\u00e9jean Ducharme", "Pascal Vincent", "Christian Janvin."], "venue": "J. Mach. Learn. Res., 3:1137\u20131155, March.", "citeRegEx": "Bengio et al\\.,? 2003", "shortCiteRegEx": "Bengio et al\\.", "year": 2003}, {"title": "Confidence estimation for machine translation", "author": ["John Blatz", "Erin Fitzgerald", "George F. Foster", "Simona Gandrabur", "Cyril Goutte", "Alex Kulesza", "Alberto Sanch\u0131\u0301s", "Nicola Ueffing"], "venue": "In Proc. of COLING", "citeRegEx": "Blatz et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Blatz et al\\.", "year": 2004}, {"title": "Findings of the 2016 conference on machine translation", "author": ["Post", "Raphael Rubino", "Carolina Scarton", "Lucia Specia", "Marco Turchi", "Karin M. Verspoor", "Marcos Zampieri."], "venue": "WMT.", "citeRegEx": "Post et al\\.,? 2016", "shortCiteRegEx": "Post et al\\.", "year": 2016}, {"title": "Torch7: A matlab-like environment for machine learning", "author": ["R. Collobert", "K. Kavukcuoglu", "C. Farabet."], "venue": "BigLearn, NIPS Workshop.", "citeRegEx": "Collobert et al\\.,? 2011", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "What\u2019s in a translation rule", "author": ["Michel Galley", "Mark Hopkins", "Kevin Knight", "Daniel Marcu"], "venue": null, "citeRegEx": "Galley et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Galley et al\\.", "year": 2004}, {"title": "Forest-based algorithms in natural language processing", "author": ["Liang Huang."], "venue": "Ph.D. thesis, University of Pennsylvania.", "citeRegEx": "Huang.,? 2008", "shortCiteRegEx": "Huang.", "year": 2008}, {"title": "Statistical Phrase-Based Translation", "author": ["Philipp Koehn", "Franz Josef Och", "Daniel Marcu."], "venue": "pages 127\u2013133, Edmonton, Canada, May.", "citeRegEx": "Koehn et al\\.,? 2003", "shortCiteRegEx": "Koehn et al\\.", "year": 2003}, {"title": "Word embeddings through hellinger pca", "author": ["R\u00e9mi Lebret", "Ronan Collobert."], "venue": "14th Conference of the European Chapter of the Association for Computational Linguistics.", "citeRegEx": "Lebret and Collobert.,? 2014", "shortCiteRegEx": "Lebret and Collobert.", "year": 2014}, {"title": "Neural network-based word alignment through score aggregation", "author": ["Joel Legrand", "Michael Auli", "Ronan Collobert."], "venue": "Proceedings of WMT.", "citeRegEx": "Legrand et al\\.,? 2016", "shortCiteRegEx": "Legrand et al\\.", "year": 2016}, {"title": "Effective approaches to attention-based neural machine translation", "author": ["Thang Luong", "Hieu Pham", "Christopher D. Manning."], "venue": "Llus Mrquez, Chris Callison-Burch, Jian Su, Daniele Pighin, and Yuval Marton, editors, EMNLP, pages 1412\u20131421. The", "citeRegEx": "Luong et al\\.,? 2015", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Bleu: A method for automatic evaluation of machine translation", "author": ["Kishore Papineni", "Salim Roukos", "Todd Ward", "WeiJing Zhu."], "venue": "Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, ACL \u201902, pages 311\u2013318,", "citeRegEx": "Papineni et al\\.,? 2002", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "A neural attention model for sentence summarization", "author": ["Alexander M Rush", "Sumit Chopra", "Jason Weston."], "venue": "Proc. of EMNLP.", "citeRegEx": "Rush et al\\.,? 2015", "shortCiteRegEx": "Rush et al\\.", "year": 2015}, {"title": "Word lattices for multi-source translation", "author": ["Josh Schroeder", "Trevor Cohn", "Philipp Koehn."], "venue": "Proc. of EACL.", "citeRegEx": "Schroeder et al\\.,? 2009", "shortCiteRegEx": "Schroeder et al\\.", "year": 2009}, {"title": "Statistical phrase-based post-editing", "author": ["Michel Simard", "Cyril Goutte", "Pierre Isabelle."], "venue": "Proc. of NAACL.", "citeRegEx": "Simard et al\\.,? 2007", "shortCiteRegEx": "Simard et al\\.", "year": 2007}, {"title": "Wordlevel confidence estimation for machine translation", "author": ["Nicola Ueffing", "Hermann Ney."], "venue": "Computational Linguistics, 33:9\u201340.", "citeRegEx": "Ueffing and Ney.,? 2007", "shortCiteRegEx": "Ueffing and Ney.", "year": 2007}], "referenceMentions": [{"referenceID": 7, "context": "Existing decoding schemes for translation generate outputs either left-to-right, such as for phrasebased or neural translation models, or bottom-up as in syntactic models (Koehn et al., 2003; Galley et al., 2004; Bahdanau et al., 2015).", "startOffset": 171, "endOffset": 235}, {"referenceID": 5, "context": "Existing decoding schemes for translation generate outputs either left-to-right, such as for phrasebased or neural translation models, or bottom-up as in syntactic models (Koehn et al., 2003; Galley et al., 2004; Bahdanau et al., 2015).", "startOffset": 171, "endOffset": 235}, {"referenceID": 0, "context": "Existing decoding schemes for translation generate outputs either left-to-right, such as for phrasebased or neural translation models, or bottom-up as in syntactic models (Koehn et al., 2003; Galley et al., 2004; Bahdanau et al., 2015).", "startOffset": 171, "endOffset": 235}, {"referenceID": 6, "context": "A beam of size 50 contains fewer than six binary decisions, all of which frequently share the same prefix (Huang, 2008).", "startOffset": 106, "endOffset": 119}, {"referenceID": 11, "context": "4 BLEU (Papineni et al., 2002) by making on average only 0.", "startOffset": 7, "endOffset": 30}, {"referenceID": 14, "context": "Our approach differs from automatic postediting since it does not require post-edited text which is a scarce resource (Simard et al., 2007; Bojar et al., 2016).", "startOffset": 118, "endOffset": 159}, {"referenceID": 9, "context": "We use an architecture similar to the word alignment model of Legrand et al. (2016). The source and the target sequences are embedded via a lookup table that replace each word type with a learned vector.", "startOffset": 62, "endOffset": 84}, {"referenceID": 2, "context": "The task of predicting mistakes is not easy as previously shown in confidence estimation (Blatz et al., 2004; Ueffing and Ney, 2007).", "startOffset": 89, "endOffset": 132}, {"referenceID": 15, "context": "The task of predicting mistakes is not easy as previously shown in confidence estimation (Blatz et al., 2004; Ueffing and Ney, 2007).", "startOffset": 89, "endOffset": 132}, {"referenceID": 1, "context": "In other words, we learn a non-causal language model (Bengio et al., 2003) which is also conditioned on the source x.", "startOffset": 53, "endOffset": 74}, {"referenceID": 10, "context": "These weights correspond to dot-product attention scores (Luong et al., 2015; Rush et al., 2015).", "startOffset": 57, "endOffset": 96}, {"referenceID": 12, "context": "These weights correspond to dot-product attention scores (Luong et al., 2015; Rush et al., 2015).", "startOffset": 57, "endOffset": 96}, {"referenceID": 0, "context": "Similar to maximum likelihood training for left-to-right translation systems (Bahdanau et al., 2015), the model is therefore not exposed to the same type of context in training (reference contexts from yref) and testing (guess contexts from yg).", "startOffset": 77, "endOffset": 100}, {"referenceID": 0, "context": "Similar to maximum likelihood training for left-to-right translation systems (Bahdanau et al., 2015), the model is therefore not exposed to the same type of context in training (reference contexts from yref) and testing (guess contexts from yg). Discussion. Our model is similar to the attention-based translation approach of Bahdanau et al. (2015). In addition to using convolutions, the main difference is that we have access to both left and right target context y\u2212i|k since we start from an initial guess translation.", "startOffset": 78, "endOffset": 349}, {"referenceID": 4, "context": "All models were implemented in Torch (Collobert et al., 2011) and trained with stochastic gradient descent to minimize the cross-entropy loss.", "startOffset": 37, "endOffset": 61}, {"referenceID": 0, "context": "In this setup, we replaced dot-product attention with MLP attention (Bahdanau et al., 2015) as it performed better on the validation set.", "startOffset": 68, "endOffset": 91}, {"referenceID": 8, "context": "All weights were initialized randomly apart from the word embedding layers, which were precomputed with Hellinger Principal Component Analysis (Lebret and Collobert, 2014) applied to the bilingual co-occurrence matrix constructed on the training set.", "startOffset": 143, "endOffset": 171}, {"referenceID": 13, "context": "Finally, the dual-attention model in \u00a74 may present a good starting point for neural multi-source translation (Schroeder et al., 2009).", "startOffset": 110, "endOffset": 134}], "year": 2016, "abstractText": "Existing machine translation decoding algorithms generate translations in a strictly monotonic fashion and never revisit previous decisions. As a result, earlier mistakes cannot be corrected at a later stage. In this paper, we present a translation scheme that starts from an initial guess and then makes iterative improvements that may revisit previous decisions. We parameterize our model as a convolutional neural network that predicts discrete substitutions to an existing translation based on an attention mechanism over both the source sentence as well as the current translation output. By making less than one modification per sentence, we improve the output of a phrase-based translation system by up to 0.4 BLEU on WMT15 German-English translation.", "creator": "TeX"}}}