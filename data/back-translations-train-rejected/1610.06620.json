{"id": "1610.06620", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Oct-2016", "title": "Proposing Plausible Answers for Open-ended Visual Question Answering", "abstract": "Answering open-ended questions is an essential capability for any intelligent agent. One of the most interesting recent open-ended question answering challenges is Visual Question Answering (VQA) which attempts to evaluate a system's visual understanding through its answers to natural language questions about images. There exist many approaches to VQA, the majority of which do not exhibit deeper semantic understanding of the candidate answers they produce. We study the importance of generating plausible answers to a given question by introducing the novel task of `Answer Proposal': for a given open-ended question, a system should generate a ranked list of candidate answers informed by the semantics of the question. We experiment with various models including a neural generative model as well as a semantic graph matching one. We provide both intrinsic and extrinsic evaluations for the task of Answer Proposal, showing that our best model learns to propose plausible answers with a high recall and performs competitively with some other solutions to VQA.", "histories": [["v1", "Thu, 20 Oct 2016 22:01:36 GMT  (3332kb,D)", "https://arxiv.org/abs/1610.06620v1", null], ["v2", "Mon, 24 Oct 2016 00:12:29 GMT  (3332kb,D)", "http://arxiv.org/abs/1610.06620v2", null]], "reviews": [], "SUBJECTS": "cs.CL cs.AI cs.CV", "authors": ["omid bakhshandeh", "trung bui", "zhe lin", "walter chang"], "accepted": false, "id": "1610.06620"}, "pdf": {"name": "1610.06620.pdf", "metadata": {"source": "CRF", "title": "Proposing Plausible Answers for Open-ended Visual Question Answering", "authors": ["Omid Bakhshandeh", "Trung Bui", "Zhe Lin", "Walter Chang"], "emails": ["omidb@cs.rochester.edu", "bui@adobe.com", "zlin@adobe.com", "wachang@adobe.com"], "sections": [{"heading": "1 Introduction", "text": "In fact, the fact is that most of them are able to survive themselves without being able to survive themselves, and that they are able to survive themselves, \"he said in an interview with the\" New York Times. \""}, {"heading": "2 The Task of Answer Proposal", "text": "We define the task of the \"Suggested Answer\" as follows: Definition 2.1. Considering question Q, create a list of all plausible answers, P, ranked according to their previous probabilities. For example, the goal of a system is to generate list P so that the actual correct answer appears higher in the ranking. Therefore, we define the intrinsic evaluation of the task as the following. Definition 2.2. Faced with a list of M triplets from questions, answers, and the plausible list of answers, such as (qi, ai, pi), we define Recall @ N as: Recall @ N = 1 toM IiN MIiN = 1, if ai triplets from questions, answers, and the plausible list of answers, such as (qi, ai, pi) Recall @ N as: Recall @ N = such an evaluation of the task, which is ibable in the order of the use of N variables."}, {"heading": "3 Approach for Tackling VQA", "text": "Our approach to coping with the VQA question consists of two main modules: the answer suggestion generator and a simple binary classifier. The answer suggestion module takes up the question and then generates a list of plausible suggestions answers (p) using the semantic characteristics of the question (q). Then each element from the suggestion list (p) is fed into the deep binary classifier, which then predicts the probability that the three-fold (q, i, p) is correct. Figure 2 shows this pipeline. In section 4 we will introduce various models for the answer suggestion modular. \"Deep Binary ClassifiedThis module classifies whether a given quantum source classifier (q, i, p) is correct. Our deep binary classifier is a simple multi-layer perceptron, which works as follows: It takes in the linked quantum ecale units, the fictitious quantum units, the fictitious quantum units, which represent the"}, {"heading": "4 Answer Proposal Models", "text": "In this section, we present different models for creating answer suggestion lists. We essentially develop two classes of approaches: generative and retrival.2We also experimented with the use of a recursive neural network to encode the question, which led to poorer results (Jabri et al., 2016; Zhou et al., 2015)."}, {"heading": "4.1 Generative Model", "text": "The generative model is an encoder-decoder Recurrent Neural Network (RNN) architecture (Sutskever et al., 2014; Cho et al., 2014) that generates the answer proposal conditioned on the question. The encoder RNN processes the question and the decoder 3 generates the proposed answer one token at a time until it meets the EOS token. Each question is encoded in a state vector of size 512, which is then set as the initial recursive state of the decoder. We adjust the model parameters to the total set, setting the number of layers to 2. The model is trained end-to-end using Stochastic Gradient Descent with early stop."}, {"heading": "4.2 Retrieval Models", "text": "In fact, most of them are able to determine for themselves what they want to do and what they want to do."}, {"heading": "5 Experiments", "text": "In this section, we summarize our experiments on intrinsic and extrinsic assessments of different response suggestion models. For all experiments, we use the trainval2014 data set from COCO with the same train10Our semantic graph matching model does not need to move to further phases of the YES / NO-QUESTION, since the ultimate plausible answer set is {yes, no}.11Linguistic knowledge of the semantic core role of verbs enables us to make an informed decision about deleting the nodes. And Val set split as at Zhou et al. (2015) with 339,482 training sessions and 30,377 test cases. As a test set, we use the test2015 standard blind set. In addition to the models described in Section 4, we include the W2V + Sem model in the experiments. Considering the rankings from semantics and the W2V model, the W2V list model simply generates a W2S-S2 agem model."}, {"heading": "5.1 Intrinsic Evaluation", "text": "This year, it will be able to leave the country to save it."}, {"heading": "5.2 Extrinsic Evaluation on VQA", "text": "We evaluate our approach to coping with VQA (described in Section 3 and illustrated in Figure 2) using various response suggestion modules. We train all models on trainval2014-train. Table 2 shows the final predicted answers to a few sample questions. Table 6 shows the test results on the 2015 test standard. In this table we also include three state-of-the-art models (including the current leader on Ranking 14), which is shown in Section 6. Toprank is a baseline that naively predicts that the answer will be the highest placed answer in the W2V + Sem suggestion list. It is interesting to see that this model can actually predict yes / no with an accuracy of 70.7%, which shows the bias of the test data set. As the results show, W2V + Sem is our most powerful system, which also reflects its higher memory. Although our model (which uses a very simple classification module) predicts some of the most modern models that overlaps with the best responses from the QA, it overlaps the weakest ones in the answers."}, {"heading": "5.3 Revisiting Mutliple-choice VQA", "text": "A recent paper (Jabri et al., 2016) that examines the distortions of the VQA dataset along with the best performing systems shows that their simple approach to binary classification (a multi-layer perceptron that can best exploit the distortions of the VQA dataset (question, picture, answer). They propose two explanations for this observation: (1) the best performing systems are those that can best exploit the distortions in the VQA dataset (2). (2) The current VQA models are all too short in modeling the problem and reach the same upper limit in accuracy. Our response suggestion model allows us to shed more light on this matter. We trained the same binary classifier on multiple-choice-choice questions that reach the accuracy of the Test2015 defaults. Then, we exchange the multiple-choice list with our pre-selection system to see Q249 to partially answer the multiplier."}, {"heading": "6 Related Work", "text": "This year, it is only a matter of time before an agreement is reached."}, {"heading": "7 Conclusion", "text": "We have introduced the novel task of proposing plausible answers to a given open question, whereby a system should generate a ranking of plausible answers to a question. We use the VQA task as a multimodal test framework for training and testing of response suggestion models. We provide various response suggestion models, ranging from vector-based to deep semantic models. We show that our most powerful model, which combines our two call models, achieves a high recall rate. We also show that our semantic graph matching approach produces truly plausible answers, as opposed to the state-of-the-art models. However, our complete VQA model outperforms some other solutions for VQA, which is weaker than the current most powerful systems. We assume that answering questions with the condition of generating only plausible answers can be more difficult than just answering questions. Our next step is to have better use of external suggestion models, mainly from knowledge sources, although QA may be experimented with by others."}], "references": [{"title": "Deep semantic analysis of text", "author": ["James F. Allen", "Mary Swift", "Will de Beaumont."], "venue": "Proceedings of the 2008 Conference on Semantics in Text Processing, STEP \u201908, pages 343\u2013354, Stroudsburg, PA, USA. Association for Computational Linguistics.", "citeRegEx": "Allen et al\\.,? 2008", "shortCiteRegEx": "Allen et al\\.", "year": 2008}, {"title": "Spice: Semantic propositional image caption evaluation", "author": ["Peter Anderson", "Basura Fernando", "Mark Johnson", "Stephen Gould."], "venue": "ECCV.", "citeRegEx": "Anderson et al\\.,? 2016", "shortCiteRegEx": "Anderson et al\\.", "year": 2016}, {"title": "Learning to compose neural networks for question answering", "author": ["Jacob Andreas", "Marcus Rohrbach", "Trevor Darrell", "Dan Klein."], "venue": "NAACL.", "citeRegEx": "Andreas et al\\.,? 2016", "shortCiteRegEx": "Andreas et al\\.", "year": 2016}, {"title": "VQA: Visual question answering", "author": ["Stanislaw Antol", "Aishwarya Agrawal", "Jiasen Lu", "Margaret Mitchell", "Dhruv Batra", "C. Lawrence Zitnick", "Devi Parikh."], "venue": "International Conference on Computer Vision (ICCV).", "citeRegEx": "Antol et al\\.,? 2015", "shortCiteRegEx": "Antol et al\\.", "year": 2015}, {"title": "Abstract meaning representation for sembanking", "author": ["Laura Banarescu", "Claire Bonial", "Shu Cai", "Madalina Georgescu", "Kira Griffitt", "Ulf Hermjakob", "Kevin Knight", "Philipp Koehn", "Martha Palmer", "Nathan Schneider."], "venue": "Proceedings of the 7th Linguistic", "citeRegEx": "Banarescu et al\\.,? 2013", "shortCiteRegEx": "Banarescu et al\\.", "year": 2013}, {"title": "Wide-coverage semantic analysis with boxer", "author": ["Johan Bos."], "venue": "Johan Bos and Rodolfo Delmonte, editors, Semantics in Text Processing. STEP 2008 Conference Proceedings, Research in Computational Semantics, pages 277\u2013286. College Publications.", "citeRegEx": "Bos.,? 2008", "shortCiteRegEx": "Bos.", "year": 2008}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "author": ["Kyunghyun Cho", "Bart Van Merri\u00ebnboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio."], "venue": "arXiv preprint", "citeRegEx": "Cho et al\\.,? 2014", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Human attention in visual question answering: Do humans and deep networks look at the same regions? CoRR, abs/1606.03556", "author": ["Abhishek Das", "Harsh Agrawal", "C. Lawrence Zitnick", "Devi Parikh", "Dhruv Batra"], "venue": null, "citeRegEx": "Das et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Das et al\\.", "year": 2016}, {"title": "Multimodal compact bilinear pooling for visual question answering and visual grounding", "author": ["Akira Fukui", "Dong Huk Park", "Daylen Yang", "Anna Rohrbach", "Trevor Darrell", "Marcus Rohrbach."], "venue": "Proceedings of the 2016 Conference on Empirical Methods", "citeRegEx": "Fukui et al\\.,? 2016", "shortCiteRegEx": "Fukui et al\\.", "year": 2016}, {"title": "Deep residual learning for image recognition", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun."], "venue": "CVPR.", "citeRegEx": "He et al\\.,? 2016", "shortCiteRegEx": "He et al\\.", "year": 2016}, {"title": "Revisiting visual question answering baselines", "author": ["Allan Jabri", "Armand Joulin", "Laurens van der Maaten."], "venue": "CoRR, abs/1606.08390.", "citeRegEx": "Jabri et al\\.,? 2016", "shortCiteRegEx": "Jabri et al\\.", "year": 2016}, {"title": "Skip-thought vectors", "author": ["Ryan Kiros", "Yukun Zhu", "Ruslan Salakhutdinov", "Richard S Zemel", "Antonio Torralba", "Raquel Urtasun", "Sanja Fidler."], "venue": "NIPS.", "citeRegEx": "Kiros et al\\.,? 2015", "shortCiteRegEx": "Kiros et al\\.", "year": 2015}, {"title": "Hierarchical question-image co-attention for visual question answering", "author": ["Jiasen Lu", "Jianwei Yang", "Dhruv Batra", "Devi Parikh."], "venue": "CoRR, abs/1606.00061.", "citeRegEx": "Lu et al\\.,? 2016", "shortCiteRegEx": "Lu et al\\.", "year": 2016}, {"title": "Learning to answer questions from image using convolutional neural network", "author": ["Lin Ma", "Zhengdong Lu", "Hang Li."], "venue": "AAAI, pages 3567\u20133573. AAAI Press.", "citeRegEx": "Ma et al\\.,? 2016", "shortCiteRegEx": "Ma et al\\.", "year": 2016}, {"title": "A multiworld approach to question answering about realworld scenes based on uncertain input", "author": ["Mateusz Malinowski", "Mario Fritz."], "venue": "Advances in Neural Information Processing Systems 27, pages 1682\u20131690.", "citeRegEx": "Malinowski and Fritz.,? 2014", "shortCiteRegEx": "Malinowski and Fritz.", "year": 2014}, {"title": "The Stanford CoreNLP natural language processing toolkit", "author": ["Christopher D. Manning", "Mihai Surdeanu", "John Bauer", "Jenny Finkel", "Steven J. Bethard", "David McClosky."], "venue": "Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics:", "citeRegEx": "Manning et al\\.,? 2014", "shortCiteRegEx": "Manning et al\\.", "year": 2014}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Gregory S. Corrado", "Jeffrey Dean."], "venue": "Advances in Neural Information Processing Systems 26: 27th Annual Conference on Neu-", "citeRegEx": "Mikolov et al\\.,? 2013", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Wordnet: A lexical database for english", "author": ["George A. Miller."], "venue": "Commun. ACM, 38(11):39\u201341, November.", "citeRegEx": "Miller.,? 1995", "shortCiteRegEx": "Miller.", "year": 1995}, {"title": "A framework for representing knowledge", "author": ["Marvin Minsky."], "venue": "Technical report, Cambridge, MA, USA.", "citeRegEx": "Minsky.,? 1974", "shortCiteRegEx": "Minsky.", "year": 1974}, {"title": "BLEU: A method for automatic evaluation of machine translation", "author": ["Kishore Papineni", "Salim Roukos", "Todd Ward", "WeiJing Zhu."], "venue": "Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, ACL \u201902, pages 311\u2013318, Strouds-", "citeRegEx": "Papineni et al\\.,? 2002", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "Question answering about images using visual semantic embeddings", "author": ["Mengye Ren", "Ryan Kiros", "Richard Zemel."], "venue": "Deep Learning Workshop, ICML 2015.", "citeRegEx": "Ren et al\\.,? 2015", "shortCiteRegEx": "Ren et al\\.", "year": 2015}, {"title": "Learning to localize little landmarks", "author": ["Kevin J. Shih", "Saurabh Singh", "Derek Hoiem."], "venue": "Computer Vision and Pattern Recognition.", "citeRegEx": "Shih et al\\.,? 2016", "shortCiteRegEx": "Shih et al\\.", "year": 2016}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V. Le."], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Jointly modeling deep video and compositional text to bridge vision and language in a unified framework", "author": ["R. Xu", "C. Xiong", "W. Chen", "J.J. Corso."], "venue": "Proceedings of AAAI Conference on Artificial Intelligence.", "citeRegEx": "Xu et al\\.,? 2015", "shortCiteRegEx": "Xu et al\\.", "year": 2015}, {"title": "Stacked attention networks for image question answering", "author": ["Zichao Yang", "Xiaodong He", "Jianfeng Gao", "Li Deng", "Alexander J. Smola."], "venue": "CoRR, abs/1511.02274.", "citeRegEx": "Yang et al\\.,? 2015", "shortCiteRegEx": "Yang et al\\.", "year": 2015}, {"title": "Simple baseline for visual question answering", "author": ["Bolei Zhou", "Yuandong Tian", "Sainbayar Sukhbaatar", "Arthur Szlam", "Rob Fergus."], "venue": "CoRR, abs/1512.02167.", "citeRegEx": "Zhou et al\\.,? 2015", "shortCiteRegEx": "Zhou et al\\.", "year": 2015}, {"title": "Visual7w: Grounded question answering in images", "author": ["Yuke Zhu", "Oliver Groth", "Michael S. Bernstein", "Li Fei-Fei."], "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR). IEEE, IEEE.", "citeRegEx": "Zhu et al\\.,? 2016", "shortCiteRegEx": "Zhu et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 18, "context": "As Minsky (1974) points out, a question also includes suggestions and recommendations about its set of answers (assignment proposal).", "startOffset": 3, "endOffset": 17}, {"referenceID": 3, "context": "Visual Question Answering (VQA) (Antol et al., 2015; Ren et al., 2015) is one of the most interesting multimodal ar X iv :1 61 0.", "startOffset": 32, "endOffset": 70}, {"referenceID": 20, "context": "Visual Question Answering (VQA) (Antol et al., 2015; Ren et al., 2015) is one of the most interesting multimodal ar X iv :1 61 0.", "startOffset": 32, "endOffset": 70}, {"referenceID": 3, "context": "The Answer row of this table shows the top answers from one of the state-of-the-art systems (hereinafter, SOTAQA)1 (Antol et al., 2015).", "startOffset": 115, "endOffset": 135}, {"referenceID": 9, "context": "Our image features are 2,048-dimensional vectors computed using the penultimate layer of the state-of-the-art convolutional neural network for image recognition, Resnet101 (He et al., 2016).", "startOffset": 172, "endOffset": 189}, {"referenceID": 16, "context": "Both the question and plausible answer are represented using 300-dimensional average Word2Vec word embeddings (Mikolov et al., 2013) which is a bag-of-words (BOW) model2.", "startOffset": 110, "endOffset": 132}, {"referenceID": 10, "context": "We also experimented with using a Recurrent Neural Network for encoding the question, which yielded worse results (Jabri et al., 2016; Zhou et al., 2015).", "startOffset": 114, "endOffset": 153}, {"referenceID": 25, "context": "We also experimented with using a Recurrent Neural Network for encoding the question, which yielded worse results (Jabri et al., 2016; Zhou et al., 2015).", "startOffset": 114, "endOffset": 153}, {"referenceID": 22, "context": "The generative model is an encoder-decoder Recurrent Neural Network (RNN) architecture (Sutskever et al., 2014; Cho et al., 2014), which generates the answer proposal being conditioned on the question.", "startOffset": 87, "endOffset": 129}, {"referenceID": 6, "context": "The generative model is an encoder-decoder Recurrent Neural Network (RNN) architecture (Sutskever et al., 2014; Cho et al., 2014), which generates the answer proposal being conditioned on the question.", "startOffset": 87, "endOffset": 129}, {"referenceID": 19, "context": "BLEU is the widely used Machine Translation (MT) metric (Papineni et al., 2002) which", "startOffset": 56, "endOffset": 79}, {"referenceID": 11, "context": "We also tried other sentence-level embedding models, such as the Skip-thoughts model (Kiros et al., 2015), all of which performed weaker than Word2Vec in capturing generic textual similarity scores a hypothesis against a gold reference by computing the geometric mean of precision scores for different n-grams.", "startOffset": 85, "endOffset": 105}, {"referenceID": 16, "context": "We use Word2Vec (Mikolov et al., 2013) as a sentence-level vector representation where we average the word-level embeddings to obtain the sentence-level vector.", "startOffset": 16, "endOffset": 38}, {"referenceID": 4, "context": "A broadcoverage semantic parser (Banarescu et al., 2013; Bos, 2008; Allen et al., 2008) operates at the generic natural language level, mapping surface level words into their underlying meaning representation.", "startOffset": 32, "endOffset": 87}, {"referenceID": 5, "context": "A broadcoverage semantic parser (Banarescu et al., 2013; Bos, 2008; Allen et al., 2008) operates at the generic natural language level, mapping surface level words into their underlying meaning representation.", "startOffset": 32, "endOffset": 87}, {"referenceID": 0, "context": "A broadcoverage semantic parser (Banarescu et al., 2013; Bos, 2008; Allen et al., 2008) operates at the generic natural language level, mapping surface level words into their underlying meaning representation.", "startOffset": 32, "endOffset": 87}, {"referenceID": 15, "context": ", CoreNLP (Manning et al., 2014)) very often fail at parsing questions altogether, mainly confusing copular and auxiliary constructions.", "startOffset": 10, "endOffset": 32}, {"referenceID": 0, "context": "Here we use the TRIPS6 (Allen et al., 2008) broad-coverage semantic parser which produces the state-of-the-art logical form from natural text (Allen et al.", "startOffset": 23, "endOffset": 43}, {"referenceID": 0, "context": ", 2008) broad-coverage semantic parser which produces the state-of-the-art logical form from natural text (Allen et al., 2008).", "startOffset": 106, "endOffset": 126}, {"referenceID": 17, "context": "What is not shown in this graph is that words are also sense disambiguated according to WordNet (Miller, 1995) senses.", "startOffset": 96, "endOffset": 110}, {"referenceID": 25, "context": "and val set split as with Zhou et al. (2015), containing 339,482 training and 30,377 test instances.", "startOffset": 26, "endOffset": 45}, {"referenceID": 10, "context": "A recent work (Jabri et al., 2016) which studies the biases of the VQA dataset along with the best performing systems shows that their simple binary classification approach (a multilayer perceptron, which takes in triplet of (question,image,answer)) outperforms many of the other complex systems.", "startOffset": 14, "endOffset": 34}, {"referenceID": 3, "context": "VQA is one of the most interesting recent challenges, mainly facilitated by the release of the VQA dataset (Antol et al., 2015), the Toronto COCO-QA (CQA) dataset (Ren et al.", "startOffset": 107, "endOffset": 127}, {"referenceID": 20, "context": ", 2015), the Toronto COCO-QA (CQA) dataset (Ren et al., 2015), and the Visual7W dataset (Zhu et al.", "startOffset": 43, "endOffset": 61}, {"referenceID": 26, "context": ", 2015), and the Visual7W dataset (Zhu et al., 2016).", "startOffset": 34, "endOffset": 52}, {"referenceID": 26, "context": "Visual7W (Zhu et al., 2016) is another recent dataset, which establishes a grounding link between a textual answer and the regions of the image.", "startOffset": 9, "endOffset": 27}, {"referenceID": 12, "context": "For this classification there are various neural network architectures combining complex attention mechanisms and memory networks (Lu et al., 2016; Yang et al., 2015).", "startOffset": 130, "endOffset": 166}, {"referenceID": 24, "context": "For this classification there are various neural network architectures combining complex attention mechanisms and memory networks (Lu et al., 2016; Yang et al., 2015).", "startOffset": 130, "endOffset": 166}, {"referenceID": 26, "context": "(Zhu et al., 2016) use deep convolutional features for representing images and averaging word embeddings as question features.", "startOffset": 0, "endOffset": 18}, {"referenceID": 13, "context": "Another work (Ma et al., 2016) uses a one-dimensional convolutional network instead of an LSTM encoder for getting the question-level embedding from word-level embeddings.", "startOffset": 13, "endOffset": 30}, {"referenceID": 25, "context": "Another model (iBOWImg) (Zhou et al., 2015), is a bag-of-words baseline which concatenates the word features from the question and convolutional features from the image to predict the answer, which shows results competitive with many recent more complex approaches using recurrent neural networks (LSTMImg).", "startOffset": 24, "endOffset": 43}, {"referenceID": 2, "context": "The Dependency Neural Module Network (D-NMN) approach (Andreas et al., 2016) performs dynamic image processing via a compositional network which dynamically restructures it-", "startOffset": 54, "endOffset": 76}, {"referenceID": 12, "context": "Another recent work (Lu et al., 2016) introduced coattention of image and question, where they jointly learn a hierarchical attention mechanism based on parsing the question and the image.", "startOffset": 20, "endOffset": 37}, {"referenceID": 7, "context": "Recently it has been shown that the attentions generated by neural attention mechanisms are either negatively correlated with where a human looks in the image or if they have positive correlation it is worse than taskindependent saliency (Das et al., 2016).", "startOffset": 238, "endOffset": 256}, {"referenceID": 10, "context": "Furthermore, many simpler classification approaches (Jabri et al., 2016) are shown to outperform the complex attention architectures that are expected to perform some complex reasoning.", "startOffset": 52, "endOffset": 72}, {"referenceID": 10, "context": "This brings up questions regarding the effectiveness of the current complex approaches to VQA and further reveals the biases of the VQA multiple-choice question set (Jabri et al., 2016).", "startOffset": 165, "endOffset": 185}, {"referenceID": 7, "context": "Fukui et al. (2016) use Multimodal Compact Bilinear pooling (MCB) for combining multi-modal (textual and visual) information.", "startOffset": 0, "endOffset": 20}, {"referenceID": 21, "context": "(Shih et al., 2016) and Jabri et al.", "startOffset": 0, "endOffset": 19}, {"referenceID": 10, "context": "(Jabri et al., 2016) which also take the answer as an input variable to a classifier that then assigns a probability to the (question, image, answer) triplet as a whole.", "startOffset": 0, "endOffset": 20}, {"referenceID": 1, "context": "The impact of semantics as opposed to surface ngram wording of textual content has also been studied in SPICE captioning evaluation (Anderson et al., 2016).", "startOffset": 132, "endOffset": 155}, {"referenceID": 23, "context": "We also note the work of (Xu et al., 2015) that while focused on caption generation and retrieval tasks for video using a joint language and vision model, proposed a compositional semantics language model that enforced semantic compatibility between essential concepts, similar to our goal of using question semantics to constrain our answer proposals.", "startOffset": 25, "endOffset": 42}], "year": 2016, "abstractText": "Answering open-ended questions is an essential capability for any intelligent agent. One of the most interesting recent open-ended question answering challenges is Visual Question Answering (VQA) which attempts to evaluate a system\u2019s visual understanding through its answers to natural language questions about images. There exist many approaches to VQA, the majority of which do not exhibit deeper semantic understanding of the candidate answers they produce. We study the importance of generating plausible answers to a given question by introducing the novel task of \u2018Answer Proposal\u2019: for a given open-ended question, a system should generate a ranked list of candidate answers informed by the semantics of the question. We experiment with various models including a neural generative model as well as a semantic graph matching one. We provide both intrinsic and extrinsic evaluations for the task of Answer Proposal, showing that our best model learns to propose plausible answers with a high recall and performs competitively with some other solutions to VQA.", "creator": "LaTeX with hyperref package"}}}