{"id": "1610.06781", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Oct-2016", "title": "Modular Deep Q Networks for Sim-to-real Transfer of Visuo-motor Policies", "abstract": "In this paper we describe a deep network architecture that maps visual input to control actions for a robotic planar reaching task with 100% reliability in real-world trials. Our network is trained in simulation and fine-tuned with a limited number of real-world images. The policy search is guided by a kinematics-based controller (K-GPS), which works more effectively and efficiently than $\\varepsilon$-Greedy. A critical insight in our system is the need to introduce a bottleneck in the network between the perception and control networks, and to initially train these networks independently.", "histories": [["v1", "Fri, 21 Oct 2016 13:36:25 GMT  (7705kb,D)", "http://arxiv.org/abs/1610.06781v1", "Under review for the IEEE Robotics and Automation Letters (RA-L) with the option for the IEEE International Conference on Robotics and Automation (ICRA) 2017 (submitted on 10 Sep, 2016)"], ["v2", "Wed, 1 Mar 2017 09:59:51 GMT  (6020kb,D)", "http://arxiv.org/abs/1610.06781v2", "Under review for the IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) 2017 (submitted on 1 Mar, 2017)"], ["v3", "Mon, 17 Jul 2017 09:59:35 GMT  (2949kb,D)", "http://arxiv.org/abs/1610.06781v3", "Under review for CoRL 2017 (submitted on 28 June, 2017)"]], "COMMENTS": "Under review for the IEEE Robotics and Automation Letters (RA-L) with the option for the IEEE International Conference on Robotics and Automation (ICRA) 2017 (submitted on 10 Sep, 2016)", "reviews": [], "SUBJECTS": "cs.RO cs.AI cs.CV cs.LG cs.SY", "authors": ["fangyi zhang", "j\\\"urgen leitner", "michael milford", "peter corke"], "accepted": false, "id": "1610.06781"}, "pdf": {"name": "1610.06781.pdf", "metadata": {"source": "CRF", "title": "Vision-Based Reaching Using Modular Deep Networks: from Simulation to the Real World", "authors": ["Fangyi Zhang", "J\u00fcrgen Leitner", "Ben Upcroft", "Peter Corke"], "emails": ["fangyi.zhang@hdr.qut.edu.au"], "sections": [{"heading": null, "text": "I. INTRODUCTIONSensor-based robotic tasks such as visual achievement are now implemented by man-written code. Typically, a perception system will identify the target in a local or global coordinate system, and the robot motion planner and joint control system will ensure that the required collision-free motion is performed. Understanding complex visual scenes has traditionally been a tough problem, but in recent years, deep learning techniques have led to significant improvements in robustness and performance [1]. Motion planning is usually implemented using classic algorithmic approaches, and joint control is implemented using classic control techniques. Inspired by the success of deep learning in computer vision, some pioneers have used deep neural networks to perform robotic tasks based directly on image data, such as object capture and bottle closure screwing [2], [4], [5]. However, most existing works did not achieve a good performance, but by adding large-scale data sets to the collection or perfect performance."}, {"heading": "II. RELATED WORK", "text": "In fact, it is such that it is a matter of a way in which people are able to determine themselves what they want and what they want. (...) It is such that people are able to determine themselves what they want. (...) It is such that people are able to determine themselves what they want. (...) It is such that people are able to determine what they want. (...) It is such that people are able to determine themselves what they want. (...) It is such that people are able to determine themselves what they want. (...) It is such that (...)... (...) \"(...)\" (...) \"(...)\" (...) \"(...)\" (...) \"(...) (...)\" (...) (...) (...) (...) (...) (...) (...) () (...) () (...) () ()"}, {"heading": "III. PROBLEM FORMULATION", "text": "A canonical problem in robotics is to reach the object with which one can interact. This goal achievement task is defined as controlling a robot arm so that its end effector reaches a specific target in the operating room. An n-DOF robot movement is represented in the joint or configuration room C-Sn; joint angles define the robot position q-C. A controller then reduces the error between the current target and the uninvolved target q-C. Consider the case when the robot has only one visual perception. In this case, the target T is represented by its image coordinates (Tx, Ty). The setup consists of a Baxter robot aiming to reach an arbitrarily placed blue target in a 2D vertical plane, with three hinges of its left arm (Fig. 1C). The robot perception is limited to raw pixel images of an external monocular circle, which is used as a single-colored camera."}, {"heading": "IV. METHODOLOGY", "text": "Due to the fragile perception, a DQN trained in simulation is not naively transferred to the real world. [7] In order to adjust perception cost-effectively, we propose a modular deep-learning architecture that can only learn how to reach robots using visual input, inspired by the DQN system of the Deepmind."}, {"heading": "A. Modular neural networks", "text": "The advantage of a separate perception module is that a small number of real samples is sufficient to optimize it for a real setup after the simulation training.The original Deepmind DQN uses three convolution (conv) layers, followed by two fully connected (FC) layers (fig. 2 (left)).The intuition is that the conv part focuses on perception, i.e., extracts the relevant information from the visual input, and the FC part performs the robot control.We verify this by dividing the network into perception and control modules. To create a useful interface, we propose a bottleneck layer (BN) that represents the minimum description of the scene - a combination of the robot state q and the target location (fig. 2 (right).Based on the physical representation contained in the movement, we can assess performance independently."}, {"heading": "B. Perception module", "text": "(The perception module learns how to extract the scene configuration from raw pixel images.) Architecture: Compared to the original DQN, instead of 4 consecutive images, we use a single grayscale image as input, sufficient to enable planar achievement of step position control where no time information is required. Before entering into the perception module, images from the simulator (RGB, 160 x 210) are converted to grayscale and scaled to 84 x 84. To train the module on the basis of our physical intuition in the bottleneck, a FC layer is added to the map that exhibits high-dimensional features from Conv3 to scene configuration. The network is initialized with random weights, apart from the first conv layer. Conv1 is initialized with the weights from a GoogLeNet [17] previously trained on the ImageNet dataset [18]. A grayscale conversion, based on RGB is necessary."}, {"heading": "C. Control module", "text": "The control module decides what actions can be taken based on the current state of the scenes.1 Q > Q = Q (Q) architecture: The control module has 3 FC levels, with 400 and 300 units in the two hidden levels (FC _ c1 and FC _ c2), as shown in Fig. 2 (right) Input to the control algorithm 1: DQN with K-GPS 1 Initialize replay memory D 2 Initialize Q function Q (a) with random weights 3 for iteration = 1, K do 4, if the previous attempt then 5 Start of a new study: 6 Randomly generates two arm poses q and q \u00b2 7 Get the end-effector position (Tx, Ty) for the pose q function 8 Use (Tx, Ty) as the target in this trial; and q as the beginning pose9 end 10 With probability emerge an action at byargmin a)."}, {"heading": "V. EXPERIMENTS AND RESULTS", "text": "We first investigated the accuracy of perception modules trained under different conditions; then we evaluated the performance of control modules and analyzed the benefits of using K-GPS; finally, we tested the performance of a network composed of perception and control modules in a real end-to-end attainment."}, {"heading": "A. Perception training and fine-tuning", "text": "Real perception was obviously trained by the simulator and labelled with different percentages of real images. It is pleasing that the percentage of real or simulated images is so high that 1418 images were used in a minibatch for weight updating, not that in a training setup. In real world training, images were generated by the simulator and labelled with different percentages of real images. In training or fine-tuning with real images, 1418 images with their real scenes were used. Real images were collected in the real world (Fig. 1C), with different arm positions distributed in the common space. For convenience, instead of using a real target, a blue circle was drawn on each real target using the simulator."}, {"heading": "B. Control training", "text": "1) \u03b5-Greedy VS K-GPS: To understand the benefits of using K-GPS for Q-Learning, we trained 6 control modules for planar accomplishment of tasks with different degrees of freedom (1, 2 and 3 DoF). To understand the benefits of using K-GPS for Q-Learning, we trained 6 control modules for planar accomplishment of tasks with different degrees of freedom (1, 2 and 3 DoF). In training, we used a learning rate between 0.1 and 0.01, and a minibatch of 64. The probability decreased from 1 to 0.1 within 1 million training steps for 1 DoF, 2 million steps for 2 DoF, and 3 million steps for 3 DoF, and 3 million steps for 3 DoF. \u03b5 and K-GPS used the same settings for principles. Figure 5 shows the learning curves for all 6 modules, indicating the success rate of a module after a certain number of training steps."}, {"heading": "C. End-to-end reaching", "text": "In order to test the performance of a network consisting of perception and control modules in the real world, we performed an evaluation for end-to-end achievement, where inputs were raw pixel images and outputs were actions. For comparison, we evaluated two networks: one was composed by SIM and C (EE1); the other was composed by FT75 and C (EE2). Each network was trained purely in simulation; FT75 achieved the best performance in the real world. Similar to the settings in Section V-B, performance was evaluated based on measurements of success rate and error distance d. Each network was evaluated with 20 real-world achievement tests. As mentioned in Section V-B, we used virtual targets for more convenient quantitative acquisition of outcomes. The results are shown in Table V, where d\u00b5 and d\u03c3 are the mean and standard deviations of error distances."}, {"heading": "VI. CONCLUSION AND DISCUSSION", "text": "The proposed modular, deep network architecture is a viable and fast method of transmission, which can be used to divide a task into a number of sub-tasks, making it comprehensible to learn and transfer complex skills; the physical intuition in the goal achievement task that one part of the network performs visual perception while the other part exercises control has been verified by the introduction of a bottleneck layer. This bottleneck represents the scene configuration that is the robot position and target position; a kinematics-based guided policy search (K-GPS) has also been introduced, which is more effective and efficient than a robot that achieves planar tasks; the KGPS method assumes that we already have enough knowledge of the task we need to learn and had a model for it; it can be applied to some other robotic applications where kinematics models are available but are not suitable for some demanding tasks where time is running short."}], "references": [{"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "Advances in Neural Information Processing Systems (NIPS), 2012, pp. 1097\u20131105.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2012}, {"title": "End-to-end training of deep visuomotor policies", "author": ["S. Levine", "C. Finn", "T. Darrell", "P. Abbeel"], "venue": "Journal of Machine Learning Research, vol. 17, no. 39, pp. 1\u201340, 2016.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2016}, {"title": "Learning hand-eye coordination for robotic grasping with deep learning and large-scale data collection", "author": ["S. Levine", "P.P. Sampedro", "A. Krizhevsky", "D. Quillen"], "venue": "International Symposium on Experimental Robotics (ISER), 2016.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2016}, {"title": "Supersizing self-supervision: Learning to grasp from 50k tries and 700 robot hours", "author": ["L. Pinto", "A. Gupta"], "venue": "IEEE International Conference on Robotics and Automation (ICRA), 2016, pp. 3406\u2013 3413.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2016}, {"title": "Deepmpc: Learning deep latent features for model predictive control", "author": ["I. Lenz", "R. Knepper", "A. Saxena"], "venue": "Robotics: Science and Systems (RSS), 2015.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2015}, {"title": "Human-level control through deep reinforcement learning", "author": ["V. Mnih", "K. Kavukcuoglu", "D. Silver", "A.A. Rusu", "J. Veness", "M.G. Bellemare", "A. Graves", "M. Riedmiller", "A.K. Fidjeland", "G. Ostrovski"], "venue": "Nature, vol. 518, no. 7540, pp. 529\u2013533, 2015.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2015}, {"title": "Towards vision-based deep reinforcement learning for robotic motion control", "author": ["F. Zhang", "J. Leitner", "M. Milford", "B. Upcroft", "P. Corke"], "venue": "Australasian Conference on Robotics and Automation (ACRA), 2015.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2015}, {"title": "Reinforcement learning: An introduction", "author": ["A.G. Barto"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1998}, {"title": "A deep-network solution towards modelless obstacle avoidance", "author": ["L. Tai", "S. Li", "M. Liu"], "venue": "IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2016.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2016}, {"title": "The Cross-Entropy Method: A Unified Approach to Combinatorial Optimization, Monte-Carlo Simulation, and Machine Learning", "author": ["R.Y. Rubinstein", "D.P. Kroese"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2004}, {"title": "Transfer learning", "author": ["L. Torrey", "J. Shavlik"], "venue": "Handbook of Research on Machine Learning Applications and Trends: Algorithms, Methods, and Techniques, E. S. Olivas, J. D. M. Guerrero, M. M. Sober, J. R. M. Benedito, and A. J. S. Lopez, Eds. Hershey, PA, USA: IGI Global, 2009, ch. 11, pp. 242\u2013264.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2009}, {"title": "A survey on transfer learning", "author": ["S.J. Pan", "Q. Yang"], "venue": "IEEE Transactions on Knowledge and Data Engineering, vol. 22, no. 10, pp. 1345\u20131359, 2010.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2010}, {"title": "Transfer learning for reinforcement learning domains: A survey", "author": ["M.E. Taylor", "P. Stone"], "venue": "The Journal of Machine Learning Research, vol. 10, pp. 1633\u20131685, 2009.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2009}, {"title": "Progressive neural networks", "author": ["A.A. Rusu", "N.C. Rabinowitz", "G. Desjardins", "H. Soyer", "J. Kirkpatrick", "K. Kavukcuoglu", "R. Pascanu", "R. Hadsell"], "venue": "arXiv preprint arXiv:1606.04671, 2016.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2016}, {"title": "A similarity-based approach to skill transfer", "author": ["T. Fitzgerald", "A. Goel", "A. Thomaz"], "venue": "Women in Robotics Workshop at Robotics: Science and Systems Conference (RSS), 2015.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2015}, {"title": "Towards adapting deep visuomotor representations from simulated to real environments", "author": ["E. Tzeng", "C. Devin", "J. Hoffman", "C. Finn", "X. Peng", "S. Levine", "K. Saenko", "T. Darrell"], "venue": "arXiv preprint arXiv:1511.07111, 2015.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}, {"title": "Going deeper with convolutions", "author": ["C. Szegedy", "W. Liu", "Y. Jia", "P. Sermanet", "S. Reed", "D. Anguelov", "D. Erhan", "V. Vanhoucke", "A. Rabinovich"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2015, pp. 1\u20139.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2015}, {"title": "Imagenet: A large-scale hierarchical image database", "author": ["J. Deng", "W. Dong", "R. Socher", "L.-J. Li", "K. Li", "L. Fei-Fei"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2009, pp. 248\u2013 255.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2009}, {"title": "Learning representations by backpropagating errors", "author": ["D.E. Rumelhart", "G.E. Hinton", "R.J. Williams"], "venue": "Nature, vol. 323, pp. 533\u2013 536, 1986.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1986}, {"title": "Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude", "author": ["T. Tieleman", "G. Hinton"], "venue": "COURSERA: Neural Networks for Machine Learning, vol. 4, no. 2, 2012.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2012}], "referenceMentions": [{"referenceID": 0, "context": "Understanding complex visual scenes has traditionally been a hard problem but in recent years deep learning techniques have led to significant improvements in robustness and performance [1].", "startOffset": 186, "endOffset": 189}, {"referenceID": 1, "context": "Inspired by the success of deep learning in computer vision, some pioneers have used deep neural networks to perform robotic tasks based directly on image data, such as object grasping and bottle cap screwing [2], [3], [4], [5].", "startOffset": 209, "endOffset": 212}, {"referenceID": 2, "context": "Inspired by the success of deep learning in computer vision, some pioneers have used deep neural networks to perform robotic tasks based directly on image data, such as object grasping and bottle cap screwing [2], [3], [4], [5].", "startOffset": 214, "endOffset": 217}, {"referenceID": 3, "context": "Inspired by the success of deep learning in computer vision, some pioneers have used deep neural networks to perform robotic tasks based directly on image data, such as object grasping and bottle cap screwing [2], [3], [4], [5].", "startOffset": 219, "endOffset": 222}, {"referenceID": 4, "context": "Inspired by the success of deep learning in computer vision, some pioneers have used deep neural networks to perform robotic tasks based directly on image data, such as object grasping and bottle cap screwing [2], [3], [4], [5].", "startOffset": 224, "endOffset": 227}, {"referenceID": 5, "context": "Our work is inspired by the Deepmind\u2019s Deep Q-Network approach [6], which showed that a deep learnt system was able to take images and directly synthesise control actions for Atari video games, albeit all in simulation.", "startOffset": 63, "endOffset": 66}, {"referenceID": 6, "context": "While this result is an important and exciting breakthrough we have found that it does not transfer directly to robotics application in a naive way, as the trained deep network\u2019s perception is quite brittle when dealing with real cameras observing real scenes [7].", "startOffset": 260, "endOffset": 263}, {"referenceID": 5, "context": "Especially, the Atari game playing DeepQ-Network (DQN) [6] has inspired robotics researchers to look at deep learning for tasks that require a close interaction ar X iv :1 61 0.", "startOffset": 55, "endOffset": 58}, {"referenceID": 7, "context": "This is done by using Q-learning [8], a form of reinforcement learning, which aims to maximise the future expected reward given a current state s and a policy \u03c0:", "startOffset": 33, "endOffset": 36}, {"referenceID": 0, "context": "This is possible by using convolutional network layers to extract the relevant features [1].", "startOffset": 88, "endOffset": 91}, {"referenceID": 6, "context": "Through the simulator, a DQN agent learned to control the 3-joint arm to reach towards a blue-circle target [7].", "startOffset": 108, "endOffset": 111}, {"referenceID": 8, "context": "A convolutional neural network (CNN) was trained to cope with obstacle avoidance problem for mobile robots [9].", "startOffset": 107, "endOffset": 110}, {"referenceID": 1, "context": "A CNN based policy representation architecture (deep visuomotor policies) and its guided policy search (GPS) method were introduced [2].", "startOffset": 132, "endOffset": 135}, {"referenceID": 2, "context": "By using a large-scale dataset with 800,000 grasp attempts, a CNN was trained to predict the success probability of a sequence of motions aiming at grasping on a 7 DoF robotic manipulator with a 2-finger gripper [3].", "startOffset": 212, "endOffset": 215}, {"referenceID": 9, "context": "simple derivative-free optimization algorithm (CEM [10]) the grasping system achieved a success rate of 80%.", "startOffset": 51, "endOffset": 55}, {"referenceID": 3, "context": "Another example of deep learning for grasping is self-supervised grasping learning in the real world where force sensors were used to autonomously label samples [4].", "startOffset": 161, "endOffset": 164}, {"referenceID": 4, "context": ", by combining RNN and model predictive control (DeepMPC) [5].", "startOffset": 58, "endOffset": 61}, {"referenceID": 10, "context": ", transfer knowledge learned in one or more sources tasks to a related target task) [11].", "startOffset": 84, "endOffset": 88}, {"referenceID": 11, "context": "Many TL methods have been introduced in the areas of data mining [12] and reinforcement learning [13].", "startOffset": 65, "endOffset": 69}, {"referenceID": 12, "context": "Many TL methods have been introduced in the areas of data mining [12] and reinforcement learning [13].", "startOffset": 97, "endOffset": 101}, {"referenceID": 13, "context": "Progressive neural networks were developed for both leveraging transfer and avoiding catastrophic forgetting to enable the learning of complex sequences of tasks, whose effectiveness has been validated on a wide variety of reinforcement learning tasks such as Atari and 3D maze games [14].", "startOffset": 284, "endOffset": 288}, {"referenceID": 14, "context": "Most methods need manually designed task mapping information, such as a similarity-based approach to skill transfer for robots [15].", "startOffset": 127, "endOffset": 131}, {"referenceID": 1, "context": "To reduce the number of real-world images for pose CNN training in GPS [2], a method of adapting visual representations from simulated to real environments was proposed.", "startOffset": 71, "endOffset": 74}, {"referenceID": 15, "context": "2% in a \u201chook loop\u201d task, with 10 times less real-world images [16].", "startOffset": 63, "endOffset": 67}, {"referenceID": 6, "context": "Previously it was shown that a DQN trained in simulation had a poor performance in real-world reaching, due to the brittleness of the transferred perception [7].", "startOffset": 157, "endOffset": 160}, {"referenceID": 6, "context": "A DQN trained in simulation does not transfer naively to real-world reaching, due to the brittle perception [7].", "startOffset": 108, "endOffset": 111}, {"referenceID": 16, "context": "Conv1 is initialized with the weights from a GoogLeNet [17] pre-trained on the ImageNet dataset [18].", "startOffset": 55, "endOffset": 59}, {"referenceID": 17, "context": "Conv1 is initialized with the weights from a GoogLeNet [17] pre-trained on the ImageNet dataset [18].", "startOffset": 96, "endOffset": 100}, {"referenceID": 18, "context": "In the training, the weights are updated through backpropagation [19] using the quadratic cost function", "startOffset": 65, "endOffset": 69}, {"referenceID": 5, "context": "The implementation is based on Deepmind\u2019s DQN, where a replay memory and target Q network are used to stabilize the training [6].", "startOffset": 125, "endOffset": 128}, {"referenceID": 19, "context": "RmsProp [20] was also adopted.", "startOffset": 8, "endOffset": 12}, {"referenceID": 1, "context": "An extra end-to-end fine-tuning might also be helpful to improve the performance [2].", "startOffset": 81, "endOffset": 84}], "year": 2016, "abstractText": "In this paper we describe a deep network architecture that maps visual input to control actions for a robotic planar reaching task with 100% reliability in real-world trials. Our network is trained in simulation and fine-tuned with a limited number of real-world images. The policy search is guided by a kinematics-based controller (K-GPS), which works more effectively and efficiently than \u03b5-Greedy. A critical insight in our system is the need to introduce a bottleneck in the network between the perception and control networks, and to initially train these networks independently.", "creator": "LaTeX with hyperref package"}}}