{"id": "1610.06918", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Oct-2016", "title": "Learning to Protect Communications with Adversarial Neural Cryptography", "abstract": "We ask whether neural networks can learn to use secret keys to protect information from other neural networks. Specifically, we focus on ensuring confidentiality properties in a multiagent system, and we specify those properties in terms of an adversary. Thus, a system may consist of neural networks named Alice and Bob, and we aim to limit what a third neural network named Eve learns from eavesdropping on the communication between Alice and Bob. We do not prescribe specific cryptographic algorithms to these neural networks; instead, we train end-to-end, adversarially. We demonstrate that the neural networks can learn how to perform forms of encryption and decryption, and also how to apply these operations selectively in order to meet confidentiality goals.", "histories": [["v1", "Fri, 21 Oct 2016 19:58:29 GMT  (181kb,D)", "http://arxiv.org/abs/1610.06918v1", "15 pages"]], "COMMENTS": "15 pages", "reviews": [], "SUBJECTS": "cs.CR cs.LG", "authors": ["mart\\'in abadi", "david g", "ersen"], "accepted": false, "id": "1610.06918"}, "pdf": {"name": "1610.06918.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["David G. Andersen"], "emails": [], "sections": [{"heading": "1 INTRODUCTION", "text": "In fact, most of them are able to determine for themselves what they want and what they want."}, {"heading": "2 LEARNING SYMMETRIC ENCRYPTION", "text": "This section describes how to protect the confidentiality of plaintext using common keys. It describes the organization of the system we are considering and the goals of the participants in this system. It also explains the training of these participants, defines their architecture and presents experiments."}, {"heading": "2.1 SYSTEM ORGANIZATION", "text": "A classic security scenario involves three parties: Alice, Bob and Eve. Typically, Alice and Bob at least want to communicate securely, and Eve wants to eavesdrop on their communications, so the desired security feature is secrecy (not integrity), and the opponent is a \"passive attacker\" who can intercept Bob Williams, but otherwise is quite limited: he cannot initiate sessions, inject messages, or modify messages in transit. We start with a particularly simple instance of this scenario, shown in Figure 1, in which Alice wants to send a single confidential message P to Bob. Message P is an input to Alice. When Alice processes this input, she produces an output C. (\"P\" stands for \"plaintext\" and \"C\" stands for \"ciphertext.\") Both Bob and Eve receive C, process it and try to recover P. We represent what they calculate from PBob and PEve, respectively. Alice and Bob have an advantage over Eva: they share a secret key as an additional K."}, {"heading": "2.2 OBJECTIVES", "text": "s goal is simple: to accurately reconstruct P (in other words, to minimize the error between P and PEve). Alice and Bob want to communicate clearly (to minimize the error between P and PBob), but also to hide their communication from Eve. Bob notes that, in accordance with modern cryptographic definitions (e.g. Goldwasser & Micali, 1984), it is not a goal for Eve to distinguish C from an accidental value drawn from any distribution. Eva's goals are in contrast to those of the GANs adversaries. On the other hand, one could try to formulate Eve's goal in terms of distinguishing the digits from two different digits."}, {"heading": "2.3 TRAINING REFINEMENTS", "text": "Our training method is based on SGD. In practice, as in working on GANs, however, our training method cuts a few corners and includes a few improvements in terms of the high-level description of the objectives of Section 2.2. We present these refinements next and give further details in Section 2.5.First, the training is based on estimated values calculated via \"minibatches\" of hundreds or thousands of examples, rather than on expected values via a distribution. We do not calculate the \"optimal Eva\" for a given value of \u03b8A, but they approach simply by alternating the training of Eve with that of Alice and Bob. Intuitively, the training can proceed something like this: Alice initially produces ciphertext that neither Bob nor Eve for a few steps, Alice and Bob may discover a way to communicate that allows to decipher and Bob to decipher."}, {"heading": "2.4 NEURAL NETWORK ARCHITECTURE", "text": "The Alice, Bob and Eve Architecture Since we want to explore whether a general neural network can learn to communicate safely, rather than develop a particular method, we aimed to create a neural network architecture that is sufficient to learn mixing functions such as XOR, but that does not strongly encode the shape of a particular algorithm. To this end, we chose the following \"Mix & Transform\" architecture, which has a first fully connected (FC) layer, in which the number of outputs equals the number of inputs, the plaintext and key bits are fed into this layer. Since each output bit can be a linear combination of all input bits, this layer allows for mixing between the key and the plaintext bits - but does not prescribe. Specifically, this layer can permutate the bits. The FC layer is followed by a sequence of revolutionary layers that produce output of a size suitable for plaintext or numerical text."}, {"heading": "2.5 EXPERIMENTS", "text": "In fact, it is so that most people are able to survive themselves, and that they are able to survive themselves, and that they are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are not able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are not able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are not able to survive themselves. (...) Most of them are not able to survive themselves. (...)"}, {"heading": "3 LEARNING WHAT TO PROTECT", "text": "Building on the results of Section 2, we examine selective protection. In other words, we look at the question of whether neural networks can learn what information needs to be protected, given the confidentiality objectives described in relation to an adversary. In the simplest case, a plaintext can have several components; if we want to prevent the adversary from seeing one of the components, it may be enough to encrypt it. More generally, we may want to publish some values that correlate with the plaintext, but prevent the adversary from calculating other values. In such scenarios, selective protection of information while maximizing benefit is both challenging and interesting."}, {"heading": "3.1 THE PROBLEM", "text": "To test these ideas and demonstrate the viability of selective protection, we are focusing on a concrete experiment. We are constructing an artificial data set consisting of tuples of four values, < A, B, C, D >. We aim to build and train a system that outputs two predictions of D, since the first three values are defined as inputs: a \"true\" prediction of D (i.e. the most accurate possible estimate given by D < A, B, C >), and a \"data-holding\" estimate of D, which is defined as the best possible estimate of D, which does not include information about the value of C. The system organization is essentially that of Figure 1, with Alice and Bob sharing a key, but Alice receives < B, C > and produces D-public in addition to a ciphertext; both Bob and Eve have an AC-Cess to Alice's outputs; Bob uses it for an improved estimate of D while Eve tries to recover."}, {"heading": "3.2 NEURAL NETWORK ARCHITECTURE AND TRAINING GOALS", "text": "In the experiments that follow, we use an advanced version of the neural network architecture of Section 2.4. Inputs are first fed into a new FC layer (12 inputs - eight key bits and four values - and 12 outputs); the outputs of this first layer are fed into a network with the architecture of Section 2.4. Intuitively, we chose this advanced layer because it should be able to predict D of A, B, and C, as well as make a prediction related to C; and the architecture of Section 2.4 is sufficient to encrypt each output of the first layer under the key."}, {"heading": "3.3 RESULTS", "text": "To understand whether the system is learning to hide information correctly, we train a separate evaluator, which we call \"Blind Eve,\" who is aware of the distribution of C. Blind Eve tries to guess C by relying only on this basic information, while the real Eve also knows the intermediate results (D-public) and ciphertext. If Eve's reconstruction error becomes identical to Blind Eve's, we know that Eve does not successfully extract information from the public estimate and ciphertext. Figure 4 shows the results of the opposing training to hide C. The red \"public error line\" indicates the average error of D-public vs. D. The green \"Bob error line\" indicates Bob's average reconstruction error, which decreases over time until Bob makes the best possible prediction of D as he enters it. The public error becomes low, but is slightly higher than Bob's. The blue dots show Eva's average error improvement over Eve's error over Eve's 0.2% (compared to Eve's 0.2)."}, {"heading": "4 CONCLUSION", "text": "In this paper, we show that neural networks can learn to protect communication. In this context, we do not need to prescribe specific cryptographic algorithms or indicate methods of applying these algorithms: they are only based on a confidentiality specification represented by the training objectives. In this context, we model attackers through neural networks; alternative models may be activated through enhanced learning. Cryptography has more to offer than encryption. In this sense, further work could consider other tasks, such as steganography, pseudorandome number generation, or integrity testing. Finally, neural networks can be useful not only for cryptographic protection, but also for attacks."}, {"heading": "ACKNOWLEDGMENTS", "text": "We thank Samy Bengio, Laura Downs, U'lfar Erlingsson, Jakob Foerster, Nando de Freitas, Ian Goodfellow, Geoff Hinton, Chris Olah, Ananth Raghunathan and Luke Vilnis for their discussions on this topic."}, {"heading": "A LEARNING ASYMMETRIC ENCRYPTION", "text": "In fact, most of them will be able to play by the rules they have imposed on themselves, and they will be able to play by the rules they have imposed on themselves."}, {"heading": "B BACKGROUND ON NEURAL NETWORKS", "text": "Most of this work assumes only a few basic concepts in the field of machine learning and neural networks, as provided by general introductions (e.g. LeCun et al. (2015). The following is a brief review.Neural networks are specifications of parameterized functions, typically constructed from a sequence of reasonably modular building blocks. For example, input to Alice is a vector of bits that represents the concatenation of the key and the plaintex. This vector (x) is entered into a \"fully connected\" layer consisting of a matrix consisting of (by A) and a vector addition (by b): Ax + B. The result of this operation is then transferred into a nonlinear function, sometimes referred to as the \"activation function,\" such as the sigmoid function, or the hyperbolic tangent functions."}], "references": [{"title": "TensorFlow: Large-scale machine learning on heterogeneous distributed systems", "author": ["Talwar", "Paul A. Tucker", "Vincent Vanhoucke", "Vijay Vasudevan", "Fernanda B. Vi\u00e9gas", "Oriol Vinyals", "Pete Warden", "Martin Wattenberg", "Martin Wicke", "Yuan Yu", "Xiaoqiang Zheng"], "venue": "CoRR, abs/1603.04467,", "citeRegEx": "Talwar et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Talwar et al\\.", "year": 2016}, {"title": "TensorFlow: A system for large-scale machine learning", "author": ["Mart\u0131\u0301n Abadi", "Paul Barham", "Jianmin Chen", "Zhifeng Chen", "Andy Davis", "Jeffrey Dean", "Matthieu Devin", "Sanjay Ghemawat", "Geoffrey Irving", "Michael Isard", "Manjunath Kudlur", "Josh Levenberg", "Rajat Monga", "Sherry Moore", "Derek Gordon Murray", "Benoit Steiner", "Paul A. Tucker", "Vijay Vasudevan", "Pete Warden", "Martin Wicke", "Yuan Yu", "Xiaoqiang Zhang"], "venue": "CoRR, abs/1605.08695,", "citeRegEx": "Abadi et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Abadi et al\\.", "year": 2016}, {"title": "On the (im)possibility of obfuscating programs", "author": ["Boaz Barak", "Oded Goldreich", "Russell Impagliazzo", "Steven Rudich", "Amit Sahai", "Salil Vadhan", "Ke Yang"], "venue": "J. ACM,", "citeRegEx": "Barak et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Barak et al\\.", "year": 2012}, {"title": "Fully automated analysis of padding-based encryption in the computational model", "author": ["Gilles Barthe", "Juan Manuel Crespo", "Benjamin Gr\u00e9goire", "C\u00e9sar Kunz", "Yassine Lakhnech", "Benedikt Schmidt", "Santiago Zanella-B\u00e9guelin"], "venue": "In Proceedings of the 2013 ACM SIGSAC conference on Computer  communications security,", "citeRegEx": "Barthe et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Barthe et al\\.", "year": 2013}, {"title": "InfoGAN: Interpretable representation learning by information maximizing generative adversarial nets", "author": ["Xi Chen", "Yan Duan", "Rein Houthooft", "John Schulman", "Ilya Sutskever", "Pieter Abbeel"], "venue": "CoRR, abs/1606.03657,", "citeRegEx": "Chen et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2016}, {"title": "Deep generative image models using a Laplacian pyramid of adversarial networks", "author": ["Emily L. Denton", "Soumith Chintala", "Arthur Szlam", "Robert Fergus"], "venue": "CoRR, abs/1506.05751,", "citeRegEx": "Denton et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Denton et al\\.", "year": 2015}, {"title": "Applied neuro-cryptography", "author": ["S\u00e9bastien Dourlens"], "venue": null, "citeRegEx": "Dourlens.,? \\Q1996\\E", "shortCiteRegEx": "Dourlens.", "year": 1996}, {"title": "Censoring representations with an adversary", "author": ["Harrison Edwards", "Amos J. Storkey"], "venue": "CoRR, abs/1511.05897,", "citeRegEx": "Edwards and Storkey.,? \\Q2015\\E", "shortCiteRegEx": "Edwards and Storkey.", "year": 2015}, {"title": "Learning to communicate to solve riddles with deep distributed recurrent Q-networks", "author": ["Jakob N. Foerster", "Yannis M. Assael", "Nando de Freitas", "Shimon Whiteson"], "venue": "CoRR, abs/1602.02672,", "citeRegEx": "Foerster et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Foerster et al\\.", "year": 2016}, {"title": "Learning to communicate with deep multi-agent reinforcement learning", "author": ["Jakob N. Foerster", "Yannis M. Assael", "Nando de Freitas", "Shimon Whiteson"], "venue": "CoRR, abs/1605.06676,", "citeRegEx": "Foerster et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Foerster et al\\.", "year": 2016}, {"title": "Cryptonets: Applying neural networks to encrypted data with high throughput and accuracy", "author": ["Ran Gilad-Bachrach", "Nathan Dowlin", "Kim Laine", "Kristin E. Lauter", "Michael Naehrig", "John Wernsing"], "venue": "Proceedings of the 33nd International Conference on Machine Learning,", "citeRegEx": "Gilad.Bachrach et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Gilad.Bachrach et al\\.", "year": 2016}, {"title": "Probabilistic encryption", "author": ["Shafi Goldwasser", "Silvio Micali"], "venue": "J. Comput. Syst. Sci.,", "citeRegEx": "Goldwasser and Micali.,? \\Q1984\\E", "shortCiteRegEx": "Goldwasser and Micali.", "year": 1984}, {"title": "Generative adversarial nets", "author": ["Ian J. Goodfellow", "Jean Pouget-Abadie", "Mehdi Mirza", "Bing Xu", "David Warde-Farley", "Sherjil Ozair", "Aaron C. Courville", "Yoshua Bengio"], "venue": "Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems", "citeRegEx": "Goodfellow et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2014}, {"title": "Explaining and harnessing adversarial examples", "author": ["Ian J. Goodfellow", "Jonathon Shlens", "Christian Szegedy"], "venue": "CoRR, abs/1412.6572,", "citeRegEx": "Goodfellow et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2014}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik P. Kingma", "Jimmy Ba"], "venue": "CoRR, abs/1412.6980,", "citeRegEx": "Kingma and Ba.,? \\Q2014\\E", "shortCiteRegEx": "Kingma and Ba.", "year": 2014}, {"title": "Analysis of neural cryptography", "author": ["Alexander Klimov", "Anton Mityagin", "Adi Shamir"], "venue": "New Zealand, December", "citeRegEx": "Klimov et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Klimov et al\\.", "year": 2002}, {"title": "The variational fair autoencoder", "author": ["Christos Louizos", "Kevin Swersky", "Yujia Li", "Max Welling", "Richard S. Zemel"], "venue": "CoRR, abs/1511.00830,", "citeRegEx": "Louizos et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Louizos et al\\.", "year": 2015}, {"title": "Neural programmer: Inducing latent programs with gradient descent", "author": ["Arvind Neelakantan", "Quoc V. Le", "Ilya Sutskever"], "venue": "CoRR, abs/1511.04834,", "citeRegEx": "Neelakantan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Neelakantan et al\\.", "year": 2015}, {"title": "f-GAN: Training generative neural samplers using variational divergence minimization", "author": ["Sebastian Nowozin", "Botond Cseke", "Ryota Tomioka"], "venue": "CoRR, abs/1606.00709,", "citeRegEx": "Nowozin et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Nowozin et al\\.", "year": 2016}, {"title": "Characterization and computation of local nash equilibria in continuous games", "author": ["Lillian J Ratliff", "Samuel A Burden", "S Shankar Sastry"], "venue": "In Communication, Control, and Computing (Allerton),", "citeRegEx": "Ratliff et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Ratliff et al\\.", "year": 2013}, {"title": "Neural synchronization and cryptography", "author": ["Andreas Ruttor"], "venue": "PhD thesis, Julius Maximilian University of Wu\u0308rzburg,", "citeRegEx": "Ruttor.,? \\Q2006\\E", "shortCiteRegEx": "Ruttor.", "year": 2006}, {"title": "Improved techniques for training GANs", "author": ["Tim Salimans", "Ian Goodfellow", "Wojciech Zaremba", "Vicki Cheung", "Alec Radford", "Xi Chen"], "venue": "CoRR, abs/1606.03498,", "citeRegEx": "Salimans et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Salimans et al\\.", "year": 2016}, {"title": "Learning multiagent communication with backpropagation", "author": ["Sainbayar Sukhbaatar", "Arthur Szlam", "Rob Fergus"], "venue": "CoRR, abs/1605.07736,", "citeRegEx": "Sukhbaatar et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Sukhbaatar et al\\.", "year": 2016}, {"title": "Intriguing properties of neural networks", "author": ["Christian Szegedy", "Wojciech Zaremba", "Ilya Sutskever", "Joan Bruna", "Dumitru Erhan", "Ian J. Goodfellow", "Rob Fergus"], "venue": "CoRR, abs/1312.6199,", "citeRegEx": "Szegedy et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Szegedy et al\\.", "year": 2013}, {"title": "Simple statistical gradient-following algorithms for connectionist reinforcement learning", "author": ["Ronald J. Williams"], "venue": "In Machine Learning,", "citeRegEx": "Williams.,? \\Q1992\\E", "shortCiteRegEx": "Williams.", "year": 1992}, {"title": "Crypto-nets: Neural networks over encrypted data", "author": ["Pengtao Xie", "Misha Bilenko", "Tom Finley", "Ran Gilad-Bachrach", "Kristin E. Lauter", "Michael Naehrig"], "venue": "CoRR, abs/1412.6181,", "citeRegEx": "Xie et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Xie et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 22, "context": ", (Foerster et al., 2016a;b; Sukhbaatar et al., 2016)).", "startOffset": 2, "endOffset": 53}, {"referenceID": 23, "context": "They arise, in particular, in work on adversarial examples (Szegedy et al., 2013; Goodfellow et al., 2014b) and on generative adversarial networks (GANs) (Goodfellow et al.", "startOffset": 59, "endOffset": 107}, {"referenceID": 3, "context": "In this respect, our work resembles recent research on automatic synthesis of cryptosystems, with tools such as ZooCrypt (Barthe et al., 2013), and contrasts with most of the literature, where hand-crafted cryptosystems are the norm.", "startOffset": 121, "endOffset": 142}, {"referenceID": 16, "context": "In this respect, the present work follows the recent research on fair representations (Edwards & Storkey, 2015; Louizos et al., 2015), which can hide or remove sensitive information, but goes beyond that work by allowing for the possibility of decryption, which supports richer dataflow structures.", "startOffset": 86, "endOffset": 133}, {"referenceID": 25, "context": "In particular, homomorphic encryption enables inference on encrypted data (Xie et al., 2014; Gilad-Bachrach et al., 2016).", "startOffset": 74, "endOffset": 121}, {"referenceID": 10, "context": "In particular, homomorphic encryption enables inference on encrypted data (Xie et al., 2014; Gilad-Bachrach et al., 2016).", "startOffset": 74, "endOffset": 121}, {"referenceID": 17, "context": ", (Neelakantan et al., 2015))\u2014into neural networks remains a fascinating problem.", "startOffset": 2, "endOffset": 28}, {"referenceID": 20, "context": "Prior work at the intersection of machine learning and cryptography has focused on the generation and establishment of cryptographic keys (Ruttor, 2006; Kinzel & Kanter, 2002), and on corresponding attacks (Klimov et al.", "startOffset": 138, "endOffset": 175}, {"referenceID": 15, "context": "Prior work at the intersection of machine learning and cryptography has focused on the generation and establishment of cryptographic keys (Ruttor, 2006; Kinzel & Kanter, 2002), and on corresponding attacks (Klimov et al., 2002).", "startOffset": 206, "endOffset": 227}, {"referenceID": 5, "context": ", (Goodfellow et al., 2014a; Denton et al., 2015; Salimans et al., 2016; Nowozin et al., 2016; Chen et al., 2016; Ganin et al., 2015)).", "startOffset": 2, "endOffset": 133}, {"referenceID": 21, "context": ", (Goodfellow et al., 2014a; Denton et al., 2015; Salimans et al., 2016; Nowozin et al., 2016; Chen et al., 2016; Ganin et al., 2015)).", "startOffset": 2, "endOffset": 133}, {"referenceID": 18, "context": ", (Goodfellow et al., 2014a; Denton et al., 2015; Salimans et al., 2016; Nowozin et al., 2016; Chen et al., 2016; Ganin et al., 2015)).", "startOffset": 2, "endOffset": 133}, {"referenceID": 4, "context": ", (Goodfellow et al., 2014a; Denton et al., 2015; Salimans et al., 2016; Nowozin et al., 2016; Chen et al., 2016; Ganin et al., 2015)).", "startOffset": 2, "endOffset": 133}, {"referenceID": 24, "context": "We have explored alternatives (based on Williams\u2019 REINFORCE algorithm (Williams, 1992) or on Foerster et al.", "startOffset": 70, "endOffset": 86}, {"referenceID": 6, "context": "Given these objectives, instead of training each of Alice and Bob separately to implement some known cryptosystem (Dourlens, 1996), we train Alice and Bob jointly to communicate successfully and to defeat Eve without a pre-specified notion of what cryptosystem they may discover for this", "startOffset": 114, "endOffset": 130}, {"referenceID": 19, "context": ", (Ratliff et al., 2013)) which it might be possible to apply in our setting.", "startOffset": 2, "endOffset": 24}, {"referenceID": 21, "context": "Although training with an adversary is often unstable (Salimans et al., 2016), we suspect that some additional engineering of the neural network and its training may be able to increase this overall success rate.", "startOffset": 54, "endOffset": 77}], "year": 2016, "abstractText": "We ask whether neural networks can learn to use secret keys to protect information from other neural networks. Specifically, we focus on ensuring confidentiality properties in a multiagent system, and we specify those properties in terms of an adversary. Thus, a system may consist of neural networks named Alice and Bob, and we aim to limit what a third neural network named Eve learns from eavesdropping on the communication between Alice and Bob. We do not prescribe specific cryptographic algorithms to these neural networks; instead, we train end-to-end, adversarially. We demonstrate that the neural networks can learn how to perform forms of encryption and decryption, and also how to apply these operations selectively in order to meet confidentiality goals.", "creator": "LaTeX with hyperref package"}}}