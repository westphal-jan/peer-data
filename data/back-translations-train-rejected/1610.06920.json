{"id": "1610.06920", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Oct-2016", "title": "Bit-pragmatic Deep Neural Network Computing", "abstract": "We quantify a source of ineffectual computations when processing the multiplications of the convolutional layers in Deep Neural Networks (DNNs) and propose Pragmatic (PRA), an architecture that exploits it improving performance and energy efficiency. The source of these ineffectual computations is best understood in the context of conventional multipliers which generate internally multiple terms, that is, products of the multiplicand and powers of two, which added together produce the final product [1]. At runtime, many of these terms are zero as they are generated when the multiplicand is combined with the zero-bits of the multiplicator. While conventional bit-parallel multipliers calculate all terms in parallel to reduce individual product latency, PRA calculates only the non-zero terms using a) on-the-fly conversion of the multiplicator representation into an explicit list of powers of two, and b) hybrid bit-parallel multplicand/bit-serial multiplicator processing units. PRA exploits two sources of ineffectual computations: 1) the aforementioned zero product terms which are the result of the lack of explicitness in the multiplicator representation, and 2) the excess in the representation precision used for both multiplicants and multiplicators, e.g., [2]. Measurements demonstrate that for the convolutional layers, a straightforward variant of PRA improves performance by 2.6x over the DaDiaNao (DaDN) accelerator [3] and by 1.4x over STR [4]. Similarly, PRA improves energy efficiency by 28% and 10% on average compared to DaDN and STR. An improved cross lane synchronication scheme boosts performance improvements to 3.1x over DaDN. Finally, Pragmatic benefits persist even with an 8-bit quantized representation [5].", "histories": [["v1", "Thu, 20 Oct 2016 22:16:05 GMT  (1108kb,D)", "http://arxiv.org/abs/1610.06920v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.AI cs.AR cs.CV", "authors": ["j albericio", "p judd", "a delm\\'as", "s sharify", "a moshovos"], "accepted": false, "id": "1610.06920"}, "pdf": {"name": "1610.06920.pdf", "metadata": {"source": "CRF", "title": "Bit-Pragmatic Deep Neural Network Computing", "authors": ["Jorge Albericio", "Patrick Judd", "Alberto Delm\u00e1s", "Sayeh Sharify", "Andreas Moshovos"], "emails": ["moshovos}@ece.utoronto.ca"], "sections": [{"heading": null, "text": "This year, the time has come for such an event to occur only once."}, {"heading": "II. MOTIVATION", "text": "Suppose that a p-bit parallel multiplier with a simple implementation of the shift and add algorithm, in which n \u00b7 s is calculated as \u2211 p i = 0 ni \u00b7 (s i), where ni is the ith-bit of n. The multiplier calculates p-terms, a product of s and a bit of n respectively, and adds them to achieve the end result. Terms and their sum can be calculated simultaneously to reduce latency [1].In such a hardware arrangement, there are two sources of ineffective compressions resulting from: 1) an excess of precision (EoP) and 2) lack of explicitness (LoE).Figure 1 shows an example of these sources being represented with a bit-parallel multiplier using an 8-bit unsigned fixed-point number with 4 fractional bits and 4 integer bits, resulting in the null bits of the constituent bits of our constitutional bits being represented with only two (five) bits, with only five of them."}, {"heading": "A. Essential Neuron Bit Content", "text": "Table V reports on the essential bit content of the neural stream of state-of-the-art DNNs for two commonly used fixed-length representations: 1) 16-bit fixed point of DaDianNao [3], 2) 8-bit quantified by Tensorflow [5]. The essential bit content is the average number of non-zero bits, the 1. Two measurements are presented per representation: for all neuron values (\"All\") and for non-zero neurons (\"NZ\") as accelerators that can skip zero neurons for fixed-point representations, were recently proposed [10], [11]. Taking all neurons into account, the essential bit content is no more than 12.7% and 38.4% for the fixed-point or quantified representations, respectively. The measurements correspond to the neuron values that follow a normal distribution centered at 0, and are then explicitly filtered by a linear rectifier unit (ReLU)."}, {"heading": "B. Pragmatic\u2019s Potential", "text": "This year it is so far that it will only take one year to move on to the next round."}, {"heading": "IV. BACKGROUND", "text": "This paper presents Pragmatic as a modification of the state-of-the-art DaDianNao accelerator. Accordingly, this section provides the necessary background information: Section IV-A examines the functioning of wavelengths, and Section IV-B gives an overview of DaDN and how it processes wavelengths."}, {"heading": "A. Convolutional Layer Computation", "text": "A conventional layer processes and produces neuron arrays, i.e. 3D arrays of real numbers. The layer applies N filters, which are also real numbers. Output neuron arrays are Ox-Oy-N, which have their depth of the filter number. Each filter corresponds to a desired characteristic and the goal of the layer is to determine where in the input neuron arrays these characteristics appear."}, {"heading": "B. Baseline System: DaDianNao", "text": "In this context, it should be noted that this is not a purely formal decision, but a purely formal one."}, {"heading": "V. Pragmatic", "text": "In this section, the pragmatic architecture is presented, in section V-A the processing approach of PRA and in section V-B its organization. In sections V-D and V-E, two optimizations are presented, each improving the range and performance. For the sake of simplicity, the description of specific values for different design parameters starts so that the performance of PRA in the worst case is consistent with the DaDN configuration of section IV-B."}, {"heading": "A. Approach", "text": "This year, it will be able to put itself at the top of the group."}, {"heading": "B. Tile Organization and Operation", "text": "Figure 5b shows the pragmatic tile architecture, which includes an array of 16 x 16 = 256 pragmatic inner product units (PIPs). PIP (i, j) processes a neuron effect from the ith window and its corresponding synapse from the j-th filter. Specifically, all PIPs along the i-th row receive the same synapse stone that belongs to the i-th filter, and all PIPs along the j-th column receive an impact from each neuron stone that belongs to the j-th window. The necessary neuron effects are read from the NBin, where they are placed by the dispatcher and the off-set generator units as section V-C. Each cycle NBin sends 256 influences 16 per window track. All PIPs in one column receive the same 16 influences that correspond to the neurons of a single window."}, {"heading": "C. Dispatcher and Oneffset Generators", "text": "In fact, it is a very rare disease, which is a very rare disease, which is usually a very rare disease."}, {"heading": "E. Per-Column Neuron Lane Synchronization", "text": "In fact, if we are able to hide, if we are able to hide, we will be able to see ourselves, \"he said."}, {"heading": "F. The Role of Software", "text": "PRA provides an additional dimension where hardware and software can attempt to further increase performance and energy efficiency, namely controlling the essential value content of neurons. This work examines a software-based approach that uses the precision requirements of each layer to zero a number of prefix and suffix bits at the output of each layer. Using the profiling method of Judd et al. [2], software communicates the precision required by each layer as metadata. The hardware trims the output neurons before writing them to AND gates and precisely derived bitmasks in NM."}, {"heading": "VI. EVALUATION", "text": "The performance, space and energy efficiency of Pragmatic is compared with DaDN [3] and Stripes [4], two state-of-the-art DNN accelerators. DaDN is the fastest bit-parallel accelerator ever proposed, processing all neurons regardless of their values, and STR improves DaDN by taking advantage of the precision requirements per layer of DNNs. Cnvlutin improves DaDN by skipping most neurons with zero value [11], but stripes has proven to be more powerful than DaDN. The rest of this section is broken down as follows: Section VI-A presents the experimental methodology. Section VI-B and VI-C analyze the PRA design space taking into account one- and two-stage shift configurations and column synchronization. Section VI-D reports energy efficiency for the best configuration. Section VI-E analyzes the contribution of the software provided to precision."}, {"heading": "A. Methodology", "text": "To estimate performance and area, all designs were synthesized using the Synopsis Design Compiler [15] for a TSMC 65nm library; NBI and NBout SRAM buffers were modeled using CACTI [16]; eDRAM area and energy were modeled using Destiny [17]; compared to STR, the numerical representation requirements per layer specified in Table II were determined using the methodology of Judd et al. [4]; all PRA configurations studied use the precision specified in Section V-F. Section VI-E analyzes the impact of this information on total performance; all performance measurements refer only to the revolutionary layers, which account for more than 92% of the total time of execution in DN-3 layers."}, {"heading": "B. Single- and 2-stage Shifting", "text": "This section evaluates the single-step displacement of the PRA configuration of sections V-A-V-B, and the two-step displacement of sections V-D. Section VI-B1 reports performance, while Section VI-B2 reports area and performance. In this section, all PRA systems are labeled with the number of bits used to operate the first stage; synapse shifts, such as the synapse shifts of the \"2-bit bars,\" or PRA2B, can be moved to four-bit positions."}, {"heading": "C. Per-column synchronization", "text": "1) Power:: Figure 10 shows the performance of PRA2b with column synchronization and as a function of the number of SSRs according to section V-E. of Stripes (first bar of each group) and Pragmatic (rest of bars) relative to DaDN. Configuration PRAxR2b refers to a configuration with x SSRs. Even PRA1R2b increases the performance to an average of 3.1 x near the 3.45 x that is ideally possible with PRA \u221e R2b. 2) Area and Power:: Table IV indicates the area per unit as well as the area and power per chip. PRA1R2b, which offers the most performance advantages, increases the chip area by only 1.35 x and the power by only 2.19 x compared to DaDN."}, {"heading": "D. Energy Efficiency", "text": "Figure 11 shows the energy efficiency of different configurations of Pragmatic. Energy efficiency or simply efficiency for a system NEW compared to BASE is defined as the EBASE / ENEW ratio of the energy required by BASE to calculate all folding layers over that of NEW. For the selected networks, STR is 16% more efficient than DaDN. PRAsingle (PRA4b) is more efficient than doubling, resulting in a circuit that is 5% less efficient than DaDN. PRA2b reduces this energy consumption while maintaining performance and yields an efficiency of 28%. PRA1R2b yields the best efficiency at 48% over DaDN."}, {"heading": "E. The Impact of Software", "text": "All PRA configurations studied to date used software per layer accuracy to reduce essential bit content. PRA does not require this precision to operate. Table V shows which fraction of the performance benefits are attributable to software guidance for PRA1R2b, the best configuration studied. Results show that: 1) PRA would outperform other architectures even without software guidance, and 2) software guidance improves performance by an average of 19%, which is in line with Section II's estimate for the ideal PRA (from 10% to 8%)."}, {"heading": "F. Quantization", "text": "Figure 12 reports on the performance of DaDN- and PRA-configurations using the 8-bit quantified representation used in Tensorflow [5], [18]. This quantification uses 8-bit to set arbitrary minimum and maximum limits per level for the neurons and synapses separately, and maps the 256 available 8-bit values linearly into the resulting interval. This representation has greater flexibility and better utilization than the reduced precision approach of stripes, as the range does not have to be symmetrical and the limits do not have to be two-dimensional, while easy multiplication of the values is still possible. Limits are set at the maximum and minimum neuron values for each layer, and the quantification uses the recommended rounding mode. Figure 12 reports on the performance relative to DaDN for PRAsingle, PRA2b, PRA1b, and PRA-R13.5."}, {"heading": "VII. RELATED WORK", "text": "Acceleration of deep learning is an active area of research and has produced numerous proposals for hardware acceleration. DaDianNao (DaDN) is the de facto standard for high-performance DNN acceleration [3]. In the interest of space, this section limits attention to methods that are either directly related to DaDN or that follow a value-based approach to DNN acceleration, since pragmatists fall into this category of accelerators. Value-based accelerators use the properties of the values processed to improve performance or energy beyond what is possible by using compression structures alone. Cnvlutin [11] and Stripes [19] are such accelerators that have already been discussed and compared in this working process. PuDianNao is a hardware accelerator that supports seven machine learning algorithms, including DNNs [20]."}, {"heading": "VIII. CONCLUSION", "text": "To the best of our knowledge, Pragmatic is the first DNN accelerator to exploit not only the precision requirements per layer of DNNs, but also the essential bit information content of the neuron values. While this work was aimed at powerful implementations, Pragmatic's core approach should be applicable to other hardware accelerators."}], "references": [{"title": "A suggestion for a fast multiplier", "author": ["C.S. Wallace"], "venue": "IEEE Trans. Electronic Computers, vol. 13, no. 1, pp. 14\u201317, 1964.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1964}, {"title": "Reduced-Precision Strategies for Bounded Memory in Deep Neural Nets, arXiv:1511.05236v4 [cs.LG", "author": ["P. Judd", "J. Albericio", "T. Hetherington", "T. Aamodt", "N.E. Jerger", "R. Urtasun", "A. Moshovos"], "venue": "arXiv.org, 2015.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2015}, {"title": "Dadiannao: A machine-learning supercomputer", "author": ["Y. Chen", "T. Luo", "S. Liu", "S. Zhang", "L. He", "J. Wang", "L. Li", "T. Chen", "Z. Xu", "N. Sun", "O. Temam"], "venue": "Microarchitecture (MICRO), 2014 47th Annual IEEE/ACM International Symposium on, pp. 609\u2013622, Dec 2014.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2014}, {"title": "Stripes: Bit-serial Deep Neural Network Computing", "author": ["P. Judd", "J. Albericio", "A. Moshovos"], "venue": "Computer Architecture Letters, 2016.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2016}, {"title": "Rich feature hierarchies for accurate object detection and semantic segmentation", "author": ["R.B. Girshick", "J. Donahue", "T. Darrell", "J. Malik"], "venue": "CoRR, vol. abs/1311.2524, 2013.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2013}, {"title": "Deep speech: Scaling up end-to-end speech recognition", "author": ["A.Y. Hannun", "C. Case", "J. Casper", "B.C. Catanzaro", "G. Diamos", "E. Elsen", "R. Prenger", "S. Satheesh", "S. Sengupta", "A. Coates", "A.Y. Ng"], "venue": "CoRR, vol. abs/1412.5567, 2014.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2014}, {"title": "Dark silicon and the end of multicore scaling", "author": ["H. Esmaeilzadeh", "E. Blem", "R. St. Amant", "K. Sankaralingam", "D. Burger"], "venue": "Proceedings of the 38th Annual International Symposium on Computer Architecture, ISCA \u201911, (New York, NY, USA), pp. 365\u2013376, ACM, 2011.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2011}, {"title": "Bit-serial multipliers and squarers", "author": ["P. Ienne", "M.A. Viredaz"], "venue": "IEEE Transactions on Computers, vol. 43, no. 12, pp. 1445\u20131450, 1994.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1994}, {"title": "EIE: Efficient Inference Engine on Compressed Deep Neural Network", "author": ["S. Han", "X. Liu", "H. Mao", "J. Pu", "A. Pedram", "M.A. Horowitz", "W.J. Dally"], "venue": "arXiv:1602.01528 [cs], Feb. 2016. arXiv: 1602.01528.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2016}, {"title": "Cnvlutin: Ineffectual-neuron-free deep neural network computing", "author": ["J. Albericio", "P. Judd", "T. Hetherington", "T. Aamodt", "N.E. Jerger", "A. Moshovos"], "venue": "2016 IEEE/ACM International Conference on Computer Architecture (ISCA), 2016.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2016}, {"title": "Rectified linear units improve restricted boltzmann machines", "author": ["V. Nair", "G.E. Hinton"], "venue": "Proceedings of the 27th International Conference on Machine Learning (ICML-10), pp. 807\u2013814, 2010.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2010}, {"title": "Eyeriss: An Energy-Efficient Reconfigurable Accelerator for Deep Convolutional Neural Networks", "author": ["Chen", "Yu-Hsin", "Krishna", "Tushar", "Emer", "Joel", "Sze", "Vivienne"], "venue": "IEEE International Solid-State Circuits Conference, ISSCC 2016, Digest of Technical Papers, pp. 262\u2013263, 2016.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2016}, {"title": "Minerva: Enabling lowpower, highly-accurate deep neural network accelerators", "author": ["B. Reagen", "P. Whatmough", "R. Adolf", "S. Rama", "H. Lee", "S.K. Lee", "J.M. Hernndez-Lobato", "G.-Y. Wei", "D. Brooks"], "venue": "International Symposium on Computer Architecture, 2016.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2016}, {"title": "Destiny: A tool for modeling emerging 3d nvm and edram caches", "author": ["M. Poremba", "S. Mittal", "D. Li", "J. Vetter", "Y. Xie"], "venue": "Design, Automation Test in Europe Conference Exhibition (DATE), 2015, pp. 1543\u20131546, March 2015.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2015}, {"title": "Stripes: Bit-serial Deep Neural Network Computing", "author": ["P. Judd", "J. Albericio", "T. Hetherington", "T. Aamodt", "A. Moshovos"], "venue": "Proceedings of the 49th Annual IEEE/ACM International Symposium on Microarchitecture, MICRO-49, 2016.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2016}, {"title": "PuDianNao: A Polyvalent Machine Learning Accelerator", "author": ["D. Liu", "T. Chen", "S. Liu", "J. Zhou", "S. Zhou", "O. Teman", "X. Feng", "X. Zhou", "Y. Chen"], "venue": "Proceedings of the Twentieth International Conference on Architectural Support for Programming Languages and Operating Systems, ASPLOS \u201915, (New York, NY, USA), pp. 369\u2013381, ACM, 2015. PuDianNao.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2015}, {"title": "ShiDianNao: Shifting vision processing closer to the sensor", "author": ["Z. Du", "R. Fasthuber", "T. Chen", "P. Ienne", "L. Li", "T. Luo", "X. Feng", "Y. Chen", "O. Temam"], "venue": "2015 ACM/IEEE 42nd Annual International Symposium on Computer Architecture (ISCA), pp. 92\u2013104, June 2015. ShiDianNao.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2015}, {"title": "Cambricon: An instruction set architecture for neural networks", "author": ["S. Liu", "Z. Du", "J. Tao", "D. Han", "T. Luo", "Y. Xie", "Y. Chen", "T. Chen"], "venue": "2016 IEEE/ACM International Conference on Computer Architecture (ISCA), 2016.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2016}, {"title": "Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding", "author": ["S. Han", "H. Mao", "W.J. Dally"], "venue": "arXiv:1510.00149 [cs], Oct. 2015. arXiv: 1510.00149.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2015}, {"title": "X1000 real-time phoneme recognition VLSI using feed-forward deep neural networks", "author": ["J. Kim", "K. Hwang", "W. Sung"], "venue": "2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 7510\u20137514, May 2014.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2014}, {"title": "Dynamically exploiting narrow width operands to improve processor power and performance", "author": ["D. Brooks", "M. Martonosi"], "venue": "Proceedings of the 5th International Symposium on High Performance Computer Architecture, HPCA \u201999, (Washington, DC, USA), pp. 13\u2013, IEEE Computer Society, 1999.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 1999}, {"title": "Dynamic Bit-Width Adaptation in DCT: An Approach to Trade Off Image Quality and Computation Energy", "author": ["J. Park", "J.H. Choi", "K. Roy"], "venue": "IEEE Transactions on Very Large Scale Integration (VLSI) Systems, vol. 18, pp. 787\u2013793, May 2010.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2010}], "referenceMentions": [{"referenceID": 0, "context": "The source of these ineffectual computations is best understood in the context of conventional multipliers which generate internally multiple terms, that is, products of the multiplicand and powers of two, which added together produce the final product [1].", "startOffset": 253, "endOffset": 256}, {"referenceID": 1, "context": ", [2].", "startOffset": 2, "endOffset": 5}, {"referenceID": 2, "context": "6x over the DaDiaNao (DaDN) accelerator [3] and by 1.", "startOffset": 40, "endOffset": 43}, {"referenceID": 3, "context": "4x over STR [4].", "startOffset": 12, "endOffset": 15}, {"referenceID": 4, "context": "Deep neural networks (DNNs) have become the state-ofthe-art technique in many recognition tasks such as object [6] and speech recognition [7].", "startOffset": 111, "endOffset": 114}, {"referenceID": 5, "context": "Deep neural networks (DNNs) have become the state-ofthe-art technique in many recognition tasks such as object [6] and speech recognition [7].", "startOffset": 138, "endOffset": 141}, {"referenceID": 2, "context": "Yet, the need for even more sophisticated DNNs demands even higher performance and energy efficiency motivating special purpose architectures such as the state-of-the-art DaDianNao (DaDN) [3].", "startOffset": 188, "endOffset": 191}, {"referenceID": 6, "context": "With power limiting modern high-performance designs, achieving better energy efficiency is essential can enable further advances [8].", "startOffset": 129, "endOffset": 132}, {"referenceID": 2, "context": "DNNs comprise a pipeline of layers where more than 92% of the processing time is spent in convolutional layers [3], which this work targets.", "startOffset": 111, "endOffset": 114}, {"referenceID": 2, "context": "DNN hardware typically uses either 16-bit fixed-point [3] or quantized 8-bit numbers [5] and bit-parallel compute units.", "startOffset": 54, "endOffset": 57}, {"referenceID": 1, "context": "Since the actual precision requirements vary considerably across DNN layers [2], typical DNN hardware ends up processing an excess of bits when processing these inner products [4].", "startOffset": 76, "endOffset": 79}, {"referenceID": 3, "context": "Since the actual precision requirements vary considerably across DNN layers [2], typical DNN hardware ends up processing an excess of bits when processing these inner products [4].", "startOffset": 176, "endOffset": 179}, {"referenceID": 7, "context": "Recent work, Stripes (STR) uses serial-parallel multiplication [9] to avoid processing these zero prefix and suffix bits [4] yielding performance and energy benefits.", "startOffset": 63, "endOffset": 66}, {"referenceID": 3, "context": "Recent work, Stripes (STR) uses serial-parallel multiplication [9] to avoid processing these zero prefix and suffix bits [4] yielding performance and energy benefits.", "startOffset": 121, "endOffset": 124}, {"referenceID": 0, "context": "The terms and their sum can be calculated concurrently to reduce latency [1].", "startOffset": 73, "endOffset": 76}, {"referenceID": 2, "context": "Table V reports the essential bit content of the neuron stream of state-of-the-art DNNs for two commonly used fixed length representations: 1) 16-bit fixed-point of DaDianNao [3], 2) 8-bit quantized of Tensorflow [5].", "startOffset": 175, "endOffset": 178}, {"referenceID": 8, "context": "Two measurements are presented per representation: over all neuron values (\u201cAll\u201d), and over the non-zero neurons (\u201cNZ\u201d) as accelerators that can skip zero neurons for fixed-point representations have been recently proposed [10], [11].", "startOffset": 223, "endOffset": 227}, {"referenceID": 9, "context": "Two measurements are presented per representation: over all neuron values (\u201cAll\u201d), and over the non-zero neurons (\u201cNZ\u201d) as accelerators that can skip zero neurons for fixed-point representations have been recently proposed [10], [11].", "startOffset": 229, "endOffset": 233}, {"referenceID": 10, "context": "The measurements are consistent with the neuron values following a normal distribution centered at 0, and then being filtered by a rectifier linear unit (ReLU) function [12].", "startOffset": 169, "endOffset": 173}, {"referenceID": 3, "context": "Stripes [4], tackles the excess of precision, exploiting the variability in numerical precision DNNs requirements to increase performance by processing the neurons bit-serially.", "startOffset": 8, "endOffset": 11}, {"referenceID": 2, "context": "16-bit Fixed-Point Representation: The following computing engines are considered: 1) baseline representative of DaDN using 16-bit fixed-point bit-parallel units [3], 2) a hypothetical enhanced baseline ZN, that can skip all zero valued neurons,", "startOffset": 162, "endOffset": 165}, {"referenceID": 9, "context": "3) Cnvlutin (CVN) a practical design that can skip zero value neurons for all but the first layer [11], 4) STR that avoids EoP (see Table II, Section VI-A) [4], 5) an ideal, softwaretransparent PRA, PRA-fp16 that processes only the essential neuron bits, and 6) an ideal PRA, PRA-red, where software communicates in advance how many prefix and suffix bits can be zeroed out after each layer (see Section V-F).", "startOffset": 98, "endOffset": 102}, {"referenceID": 3, "context": "3) Cnvlutin (CVN) a practical design that can skip zero value neurons for all but the first layer [11], 4) STR that avoids EoP (see Table II, Section VI-A) [4], 5) an ideal, softwaretransparent PRA, PRA-fp16 that processes only the essential neuron bits, and 6) an ideal PRA, PRA-red, where software communicates in advance how many prefix and suffix bits can be zeroed out after each layer (see Section V-F).", "startOffset": 156, "endOffset": 159}, {"referenceID": 8, "context": "In summary, this section corroborated past observations that: a) many neuron values are zero [10], [11], [13], [14], and b) only close to a half of the computations performed traditionally is needed if numerical precision is properly adjusted [4].", "startOffset": 93, "endOffset": 97}, {"referenceID": 9, "context": "In summary, this section corroborated past observations that: a) many neuron values are zero [10], [11], [13], [14], and b) only close to a half of the computations performed traditionally is needed if numerical precision is properly adjusted [4].", "startOffset": 99, "endOffset": 103}, {"referenceID": 11, "context": "In summary, this section corroborated past observations that: a) many neuron values are zero [10], [11], [13], [14], and b) only close to a half of the computations performed traditionally is needed if numerical precision is properly adjusted [4].", "startOffset": 105, "endOffset": 109}, {"referenceID": 12, "context": "In summary, this section corroborated past observations that: a) many neuron values are zero [10], [11], [13], [14], and b) only close to a half of the computations performed traditionally is needed if numerical precision is properly adjusted [4].", "startOffset": 111, "endOffset": 115}, {"referenceID": 3, "context": "In summary, this section corroborated past observations that: a) many neuron values are zero [10], [11], [13], [14], and b) only close to a half of the computations performed traditionally is needed if numerical precision is properly adjusted [4].", "startOffset": 243, "endOffset": 246}, {"referenceID": 3, "context": "b) Bit-serial unit with equivalent throughput (Stripes[4]).", "startOffset": 54, "endOffset": 57}, {"referenceID": 2, "context": "[3].", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "This allows PRA to reuse STR\u2019s approach for fetching the next pallet from the single-ported NM [4].", "startOffset": 95, "endOffset": 98}, {"referenceID": 3, "context": "PRA uses the same dispatcher design as STR [4].", "startOffset": 43, "endOffset": 46}, {"referenceID": 1, "context": ", [2], software communicates the precisions needed by each layer as meta-data.", "startOffset": 2, "endOffset": 5}, {"referenceID": 2, "context": "The performance, area and energy efficiency of Pragmatic is compared against DaDN [3] and Stripes [4], two stateof-the-art DNN accelerators.", "startOffset": 82, "endOffset": 85}, {"referenceID": 3, "context": "The performance, area and energy efficiency of Pragmatic is compared against DaDN [3] and Stripes [4], two stateof-the-art DNN accelerators.", "startOffset": 98, "endOffset": 101}, {"referenceID": 9, "context": "Cnvlutin improves upon DaDN by skipping most zero-valued neurons [11], however, Stripes has been shown to outperform it.", "startOffset": 65, "endOffset": 69}, {"referenceID": 13, "context": "The eDRAM area and energy were modelled with Destiny [17].", "startOffset": 53, "endOffset": 57}, {"referenceID": 3, "context": "[4].", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "All performance measurements are for the convolutional layers only which account for more than 92% of the overall execution time in DaDN [3].", "startOffset": 137, "endOffset": 140}, {"referenceID": 2, "context": "DaDianNao (DaDN) is the de facto standard for high-performance DNN acceleration [3].", "startOffset": 80, "endOffset": 83}, {"referenceID": 9, "context": "Cnvlutin [11] and Stripes [4][19] are such accelerators and they have been already discussed and compared against in this work.", "startOffset": 9, "endOffset": 13}, {"referenceID": 3, "context": "Cnvlutin [11] and Stripes [4][19] are such accelerators and they have been already discussed and compared against in this work.", "startOffset": 26, "endOffset": 29}, {"referenceID": 14, "context": "Cnvlutin [11] and Stripes [4][19] are such accelerators and they have been already discussed and compared against in this work.", "startOffset": 29, "endOffset": 33}, {"referenceID": 15, "context": "PuDianNao is a hardware accelerator that supports seven machine learning algorithms including DNNs [20].", "startOffset": 99, "endOffset": 103}, {"referenceID": 16, "context": "ShiDianNao is a camera-integrated low power accelerator that exploits integration to reduce communication overheads and to further improve energy efficiency [21].", "startOffset": 157, "endOffset": 161}, {"referenceID": 17, "context": "Cambricon is the first instruction set architecture for Deep Learning [22].", "startOffset": 70, "endOffset": 74}, {"referenceID": 12, "context": "Minerva is a highly automated software and hardware co-design approach targeting ultra low-voltage, highly-efficient DNN accelerators [14].", "startOffset": 134, "endOffset": 138}, {"referenceID": 11, "context": "Eyeriss is a low power, real-time DNN accelerator that exploits zero valued neurons for memory compression and energy reduction [13].", "startOffset": 128, "endOffset": 132}, {"referenceID": 8, "context": "The Efficient Inference Engine (EIE) exploits efficient neuron and synapse representations and pruning to greatly reduce communication costs, to improve energy efficiency and to boost performance by avoiding certain ineffectual computations [10][23].", "startOffset": 241, "endOffset": 245}, {"referenceID": 18, "context": "The Efficient Inference Engine (EIE) exploits efficient neuron and synapse representations and pruning to greatly reduce communication costs, to improve energy efficiency and to boost performance by avoiding certain ineffectual computations [10][23].", "startOffset": 245, "endOffset": 249}, {"referenceID": 19, "context": "Profiling has been used to determine the precision requirements of a neural network for a hardwired implementation [24].", "startOffset": 115, "endOffset": 119}, {"referenceID": 20, "context": "[25] exploit the prefix bits due to EoP to turn off parts of the datapath improving energy.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "[26], use a similar approach to trade off image quality for improved energy efficiency.", "startOffset": 0, "endOffset": 4}], "year": 2016, "abstractText": "We quantify a source of ineffectual computations when processing the multiplications of the convolutional layers in Deep Neural Networks (DNNs) and propose Pragmatic (PRA), an architecture that exploits it improving performance and energy efficiency. The source of these ineffectual computations is best understood in the context of conventional multipliers which generate internally multiple terms, that is, products of the multiplicand and powers of two, which added together produce the final product [1]. At runtime, many of these terms are zero as they are generated when the multiplicand is combined with the zero-bits of the multiplicator. While conventional bit-parallel multipliers calculate all terms in parallel to reduce individual product latency, PRA calculates only the non-zero terms using a) on-thefly conversion of the multiplicator representation into an explicit list of powers of two, and b) bit-parallel multplicand/bit-serial multiplicator processing units. PRA exploits two sources of ineffectual computations: 1) the aforementioned zero product terms which are the result of the lack of explicitness in the multiplicator representation, and 2) the excess in the representation precision used for both multiplicants and multiplicators, e.g., [2]. Measurements demonstrate that for the convolutional layers, a straightforward variant of PRA improves performance by 2.6x over the DaDiaNao (DaDN) accelerator [3] and by 1.4x over STR [4]. Similarly, PRA improves energy efficiency by 28% and 10% on average compared to DaDN and STR. An improved cross lane synchronization scheme boosts performance improvements to 3.1x over DaDN. Finally, Pragmatic benefits persist even with an 8-bit quantized representation [5].", "creator": "LaTeX with hyperref package"}}}