{"id": "1610.07149", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Oct-2016", "title": "Two are Better than One: An Ensemble of Retrieval- and Generation-Based Dialog Systems", "abstract": "Open-domain human-computer conversation has attracted much attention in the field of NLP. Contrary to rule- or template-based domain-specific dialog systems, open-domain conversation usually requires data-driven approaches, which can be roughly divided into two categories: retrieval-based and generation-based systems. Retrieval systems search a user-issued utterance (called a query) in a large database, and return a reply that best matches the query. Generative approaches, typically based on recurrent neural networks (RNNs), can synthesize new replies, but they suffer from the problem of generating short, meaningless utterances. In this paper, we propose a novel ensemble of retrieval-based and generation-based dialog systems in the open domain. In our approach, the retrieved candidate, in addition to the original query, is fed to an RNN-based reply generator, so that the neural model is aware of more information. The generated reply is then fed back as a new candidate for post-reranking. Experimental results show that such ensemble outperforms each single part of it by a large margin.", "histories": [["v1", "Sun, 23 Oct 2016 11:22:40 GMT  (614kb,D)", "http://arxiv.org/abs/1610.07149v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["yiping song", "rui yan", "xiang li", "dongyan zhao", "ming zhang"], "accepted": false, "id": "1610.07149"}, "pdf": {"name": "1610.07149.pdf", "metadata": {"source": "CRF", "title": "Two are Better than One: An Ensemble of Retrieval- and Generation-Based Dialog Systems", "authors": ["Yiping Song", "Rui Yan", "Xiang Li", "Dongyan Zhao", "Ming Zhang"], "emails": ["}@pku.edu.cn"], "sections": [{"heading": "1 Introduction", "text": "In fact, most people are able to decide for themselves what they want and what they want."}, {"heading": "2 The Proposed Model Ensemble", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Overview", "text": "Figure 1 describes the overall framework of our proposed set of query and generative dialog systems. It essentially consists of the following components. \u2022 When a user sends a query expression q, our approach uses a modern information query system to search for a query-response pair < q *, r * > that best matches the query q issued by the user. The corresponding r * is retrieved as a candidate's response. \u2022 Then, a biseq2seq model takes the original query q and the retrieved answer r * as input, transforming each sequence into a fixed-size vector. These two vectors are linked and linearly transformed as the initial state of the decoder, generating a new expression r + as a further response from the candidate. \u2022 Finally, we use a ringanchor (which is part of the query system) to describe either r * or r + as the ultimate response to the original query each in the remainder of this component."}, {"heading": "2.2 Retrieval-Based Dialog System", "text": "'We have a system where we behave in the way we behave in the way we behave in the way we behave in the way we behave in the way we behave in the way we behave in the way we behave in the way we behave in the way we behave in the way we behave in the way we behave in the way we behave in the way we behave in the way we behave in the way we behave in the way we behave in the way we behave in the way we behave in the way we behave in the way we behave in the way we behave in the way we behave in the way we behave in the way we behave in the way we behave in the way we behave. '"}, {"heading": "2.4 Post-Reranking", "text": "Now that we have both a retrieved candidate response r * and a generated response r +, we select one as the q-r gatekeeper's final answer in the call-based dialog system (as described in previous sections and not repeated here). Using manually constructed functions, this step can either eliminate meaningless short answers that are unfortunately generated by biseq2seq, or less relevant responses that are given by the call-up system. We call this a post-anchor in our model ensemble."}, {"heading": "2.5 Training", "text": "We train each component individually because the retrieval part is not fully learnable.In the retrieval system, we use the classifier's trust as relevance points.The training set consists of 10k samples, which are either contained in the original human-human expression pairs or generated by negative samples.We have tried to collect binary labels from a crowd sourcing platform that indicate whether a query is relevant to another query and whether it is relevant to a particular answer.We find that the use of crowd sourcing labels performs better than the original negative sample.For biseq2seq, we use human-human expression pairs < q, r > as data samples. A retrieved candidate r is also provided as input when we train the neural network.Standard entropy loss of all the words in the response is used as a training object.For a certain test, the T = T = 1, where the length of the response is T = 1."}, {"heading": "3 Evaluation", "text": "In this section, we evaluate our model set for the Chinese (language) human-computer conversation. First, we describe the data sets and settings. Then, we compare our approach with strong baselines."}, {"heading": "3.1 Experimental Setup", "text": "Typically, a very large database of query-reply pairs is a prerequisite for a successful retrieval-based conversation system based on Baiq, since the answer must appear in the database. However, for RNN-based sequence generators, it is time-consuming to train with such a large data set; the performance of RNN can also be satisfactory when we have several million samples.5 To construct a database for retrieving information, we gathered human utterances from massive online forums, microblogs, and communities to answer questions, such as Sina Weibo, 3 Baidu Zhidao, 4, and Baidu Tieba.5 We filtered short and meaningless answers such as. \"and\" Errr. \"In total, the database contains 7 million query-reply pairs for retrieval. For the generational part, we constructed another dataset from various resources on public websites with 1,606,741 query-reply pairs."}, {"heading": "3.2 Competing Methods", "text": "We compare our model set with each individual component and provide a thorough ablation test. Below are the competing methods listed in our experiments. \u2022 Retrieval. A state-of-the-art dialog system that is part of our model set; it is also a strong baseline due to its comprehensive human engineering. \u2022 seq2seq. An encoder encoder framework [21] that was first introduced in [18] for dialog systems. \u2022 biseq2seq. Another component of our approach, adapted from [30], which is essentially a seq2seq model that is extended by a retrieved response. \u2022 Rerank (Retrieval, seq2seq). Post-retrieval between a retrieved candidate and a retrieved candidate generated by seq2seq. \u2022 Rerank (Retrieval, biseq2seq). This is the complete proposed model set. All baselines have been trained in the same way as if our model is fully matched and matched."}, {"heading": "3.3 Overall Performance", "text": "We evaluated our approach in terms of both subjective and objective evaluation. \u2022 Human evaluation, while time-consuming and labor-consuming, is the ultimate goal of open domain conversation systems. We asked three trained volunteers to comment on the results using a common protocol known as pointed annotations [10, 12, 18]. In other words, annotators were asked to point either \"0\" (bad), \"1\" (borderline), or \"2\" (good) to a pair of queries and answers. Subjective evaluation was performed in a strictly random and blind manner to exclude human bias. \u2022 We used BLEU-2, BLEU-4 as an automatic evaluation. While [11] it continues to be aggressively argued that no existing automatic metrics are suitable for open domain dialogues, they show a slight positive correlation between BLEU-2 and human evaluation."}, {"heading": "3.4 Analysis and Discussion", "text": "This year it is as far as ever."}, {"heading": "3.5 Case Study", "text": "Table 4 shows two examples of our ensemble and its \"base\" models. We see that biseq2seq is actually influenced by the retrieved candidates. Unlike traditional seq2seq, the output of biseq2seq also includes several substantive words in the retrieved answers (e.g. crush), making the statements more meaningful."}, {"heading": "4 Related Work", "text": "In the early years, researchers mainly focused on domain-specific dialog systems, such as collecting answers to information-based questions [2], film information [1], and human tutoring [5]. Typically, a pre-constructed ontology defines a finite set of slots and values, such as cuisine, location, and price range in a food dialog system; during human-computer interaction, a government tracker fills plausible values for each slot from user input and recommends the restaurant that best meets the user's needs [13, 24, 26]. In the open field, however, such slot-filling approaches would likely fail due to the diversity of topics and natural language expressions. [6] Information techniques are used to search for related queries and answers. [7] and [28] both flat handcrafted features and deep neural networks are used for mapping. [10] A random walkie-talkie algorithm is used to suggest candidates."}, {"heading": "5 Conclusion and Future Work", "text": "In this paper, we propose a novel interplay of call-based and generation-based open-domain dialog systems. In the call-off section, a suitable candidate is searched for, which is fed to an RNN-based biseq2seq response generator together with the original query. As a new candidate, the generated expression is fed back to the call-back system for post-reranking. Experimental results show that our team far exceeds its underlying call-back system and generation system. Furthermore, the ablation test shows that both biseq2seq and post-reranking mechanisms play an important role in the call-back system. Our research also points to several promising approaches for future work, such as the development of new mechanisms for combining call-back and generative dialog systems and the inclusion of other data-driven approaches to human-computer conversations."}], "references": [{"title": "MIMIC: An adaptive mixed initiative spoken dialogue system for information queries", "author": ["Jennifer Chu-Carroll"], "venue": "In Proc. Conf. Applied Natural Language Processing,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2000}, {"title": "Trains-95: Towards a mixed-initiative planning assistant", "author": ["G. Ferguson", "J. Allen", "B. Miller"], "venue": "In AIPS,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1996}, {"title": "Measuring nominal scale agreement among many raters", "author": ["Joseph L Fleiss"], "venue": "Psychological Bulletin,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1971}, {"title": "Engagement driven topic selection for an informationgiving agent", "author": ["Nadine Glas", "Ken Prepin", "Catherine Pelachaud"], "venue": "In Proc. Workshop on the Semantics and Pragmatics of Dialogue,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2015}, {"title": "AutoTutor: An intelligent tutoring system with mixedinitiative dialogue", "author": ["A. Graesser", "P. Chipman", "B. Haynes", "A. Olney"], "venue": "IEEE Trans. Education,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2005}, {"title": "Cobot in LambdaMOO: A social statistics agent", "author": ["Charles Lee Isbell", "Michael Kearns", "Dave Kormann", "Satinder Singh", "Peter Stone"], "venue": "In AAAI,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2000}, {"title": "An information retrieval approach to short text conversation", "author": ["Zongcheng Ji", "Zhengdong Lu", "Hang Li"], "venue": "arXiv preprint arXiv:1408.6988,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2014}, {"title": "A diversity-promoting objective function for neural conversation models", "author": ["Jiwei Li", "Michel Galley", "Chris Brockett", "Jianfeng Gao", "Bill Dolan"], "venue": "In NAACL-HLT,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2016}, {"title": "A hierarchical neural autoencoder for paragraphs and documents", "author": ["Jiwei Li", "Thang Luong", "Dan Jurafsky"], "venue": "In ACL-IJCNLP,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2015}, {"title": "StalemateBreaker: A proactive content-introducing approach to automatic human-computer conversation", "author": ["Xiang Li", "Lili Mou", "Rui Yan", "Ming Zhang"], "venue": "In IJCAI,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2016}, {"title": "How NOT to evaluate your dialogue system: An empirical study of unsupervised evaluation metrics for dialogue response generation", "author": ["Chia-Wei Liu", "Ryan Lowe", "Iulian V Serban", "Michael Noseworthy", "Laurent Charlin", "Joelle Pineau"], "venue": "In EMNLP (to appear),", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2016}, {"title": "Sequence to backward and forward sequences: A content-introducing approach to generative short-text conversation", "author": ["Lili Mou", "Yiping Song", "Rui Yan", "Ge Li", "Lu Zhang", "Zhi Jin"], "venue": "arXiv preprint arXiv:1607.00970,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2016}, {"title": "Multi-domain dialog state tracking using recurrent neural networks", "author": ["Nikola Mrk\u0161i\u0107", "Diarmuid \u00d3 S\u00e9aghdha", "Blaise Thomson", "Milica Gasic", "Pei-Hao Su", "David Vandyke", "Tsung-Hsien Wen", "Steve Young"], "venue": "In ACL-IJCNLP,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2015}, {"title": "Data-driven response generation in social media", "author": ["Alan Ritter", "Colin Cherry", "William B Dolan"], "venue": "In EMNLP,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2011}, {"title": "A neural attention model for abstractive sentence summarization", "author": ["Alexander M. Rush", "Sumit Chopra", "Jason Weston"], "venue": "In EMNLP,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2015}, {"title": "Building endto-end dialogue systems using generative hierarchical neural network models", "author": ["Iulian V Serban", "Alessandro Sordoni", "Yoshua Bengio", "Aaron Courville", "Joelle Pineau"], "venue": "In AAAI,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2016}, {"title": "A hierarchical latent variable encoder-decoder model for generating dialogues", "author": ["Iulian Vlad Serban", "Alessandro Sordoni", "Ryan Lowe", "Laurent Charlin", "Joelle Pineau", "Aaron Courville", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1605.06069,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2016}, {"title": "Neural responding machine for short-text conversation", "author": ["Lifeng Shang", "Zhengdong Lu", "Hang Li"], "venue": "In ACL-IJCNLP,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2015}, {"title": "A neural network approach to context-sensitive generation of conversational responses", "author": ["Alessandro Sordoni", "Michel Galley", "Michael Auli", "Chris Brockett", "Yangfeng Ji", "Margaret Mitchell", "Jian- Yun Nie", "Jianfeng Gao", "Bill Dolan"], "venue": "In NAACL-HLT,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2015}, {"title": "Continuously learning neural dialogue management", "author": ["Pei-Hao Su", "Milica Gasic", "Nikola Mrksic", "Lina Rojas-Barahona", "Stefan Ultes", "David Vandyke", "Tsung- Hsien Wen", "Steve Young"], "venue": "arXiv preprint arXiv:1606.02689,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2016}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V Le"], "venue": "In NIPS, pages 3104\u20133112,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2014}, {"title": "A neural conversational model", "author": ["Oriol Vinyals", "Quoc Le"], "venue": "arXiv preprint arXiv:1506.05869,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2015}, {"title": "Stochastic language generation in dialogue using recurrent neural networks with convolutional sentence reranking", "author": ["Tsung-Hsien Wen", "Milica Gasic", "Dongho Kim", "Nikola Mrksic", "Pei-Hao Su", "David Vandyke", "Steve Young"], "venue": "In SIGDIAL,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2015}, {"title": "A network-based end-to-end trainable task-oriented dialogue system", "author": ["Tsung-Hsien Wen", "Milica Gasic", "Nikola Mrksic", "Lina M Rojas-Barahona", "Pei-Hao Su", "Stefan Ultes", "David Vandyke", "Steve Young"], "venue": "arXiv preprint arXiv:1604.04562,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2016}, {"title": "The dialog state tracking challenge", "author": ["Jason Williams", "Antoine Raux", "Deepak Ramachandran", "Alan Black"], "venue": "In SIGDIAL,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2013}, {"title": "Web-style ranking and SLU combination for dialog state tracking", "author": ["Jason D Williams"], "venue": "In SIGDIAL,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2014}, {"title": "Topic augmented neural response generation with a joint attention mechanism", "author": ["Chen Xing", "Wei Wu", "Yu Wu", "Jie Liu", "Yalou Huang", "Ming Zhou", "Wei-Ying Ma"], "venue": "arXiv preprint arXiv:1606.08340,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2016}, {"title": "Learning to respond with deep neural networks for retrieval-based human-computer conversation system", "author": ["Rui Yan", "Yiping Song", "Hua Wu"], "venue": "In SIGIR,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2016}, {"title": "AdaDelta: An adaptive learning rate method", "author": ["Matthew D Zeiler"], "venue": "arXiv preprint arXiv:1212.5701,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2012}, {"title": "Multi-source neural translation", "author": ["Barret Zoph", "Kevin Knight"], "venue": "In NAACL-ACL,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2016}], "referenceMentions": [{"referenceID": 1, "context": "Automatic dialog/conversation systems have served humans for a long time in various fields, ranging from train routing [2] to museum guiding [4].", "startOffset": 119, "endOffset": 122}, {"referenceID": 3, "context": "Automatic dialog/conversation systems have served humans for a long time in various fields, ranging from train routing [2] to museum guiding [4].", "startOffset": 141, "endOffset": 144}, {"referenceID": 22, "context": "In the above scenarios, the dialogs are domain-specific, and a typical approach to such in-domain systems is by human engineering, for example, using manually constructed ontologies [23], natural language templates [20], and even predefined dialog states [25].", "startOffset": 182, "endOffset": 186}, {"referenceID": 19, "context": "In the above scenarios, the dialogs are domain-specific, and a typical approach to such in-domain systems is by human engineering, for example, using manually constructed ontologies [23], natural language templates [20], and even predefined dialog states [25].", "startOffset": 215, "endOffset": 219}, {"referenceID": 24, "context": "In the above scenarios, the dialogs are domain-specific, and a typical approach to such in-domain systems is by human engineering, for example, using manually constructed ontologies [23], natural language templates [20], and even predefined dialog states [25].", "startOffset": 255, "endOffset": 259}, {"referenceID": 5, "context": "Recently, researchers have paid increasing attention to open-domain, chatbot-style human-computer conversation, because of its important commercial applications, and because it tackles the real challenges of natural language understanding and generation [6, 16, 18].", "startOffset": 254, "endOffset": 265}, {"referenceID": 15, "context": "Recently, researchers have paid increasing attention to open-domain, chatbot-style human-computer conversation, because of its important commercial applications, and because it tackles the real challenges of natural language understanding and generation [6, 16, 18].", "startOffset": 254, "endOffset": 265}, {"referenceID": 17, "context": "Recently, researchers have paid increasing attention to open-domain, chatbot-style human-computer conversation, because of its important commercial applications, and because it tackles the real challenges of natural language understanding and generation [6, 16, 18].", "startOffset": 254, "endOffset": 265}, {"referenceID": 5, "context": "When a user issues an utterance (called a query), retrieval systems search for a most similar query in a massive database (which consists of large numbers of query-reply pairs), and respond to the user with the corresponding reply [6,7].", "startOffset": 231, "endOffset": 236}, {"referenceID": 6, "context": "When a user issues an utterance (called a query), retrieval systems search for a most similar query in a massive database (which consists of large numbers of query-reply pairs), and respond to the user with the corresponding reply [6,7].", "startOffset": 231, "endOffset": 236}, {"referenceID": 15, "context": "Generative dialog systems, on the other hand, can synthesize a new sentence as the reply by language models [16, 18, 19].", "startOffset": 108, "endOffset": 120}, {"referenceID": 17, "context": "Generative dialog systems, on the other hand, can synthesize a new sentence as the reply by language models [16, 18, 19].", "startOffset": 108, "endOffset": 120}, {"referenceID": 18, "context": "Generative dialog systems, on the other hand, can synthesize a new sentence as the reply by language models [16, 18, 19].", "startOffset": 108, "endOffset": 120}, {"referenceID": 7, "context": "Despite these, RNN also has its own weakness when applied to dialog systems: the generated sentence tends to be short, universal, and meaningless, for example, \u201cI don\u2019t know\u201d [8] or \u201csomething\u201d [16].", "startOffset": 175, "endOffset": 178}, {"referenceID": 15, "context": "Despite these, RNN also has its own weakness when applied to dialog systems: the generated sentence tends to be short, universal, and meaningless, for example, \u201cI don\u2019t know\u201d [8] or \u201csomething\u201d [16].", "startOffset": 194, "endOffset": 198}, {"referenceID": 29, "context": "The query, along with the candidate reply, is then fed to an utterance generator based on the \u201cbi-sequence to sequence\u201d (biseq2seq) model [30].", "startOffset": 138, "endOffset": 142}, {"referenceID": 5, "context": "2 Retrieval-Based Dialog System Information retrieval is among prevailing techniques for open-domain, chatbot-style human-computer conversation [6, 7].", "startOffset": 144, "endOffset": 150}, {"referenceID": 6, "context": "2 Retrieval-Based Dialog System Information retrieval is among prevailing techniques for open-domain, chatbot-style human-computer conversation [6, 7].", "startOffset": 144, "endOffset": 150}, {"referenceID": 20, "context": "With recurrent neural networks (RNNs) as the encoder and decoder, such architecture is also known as a seq2seq model, which has wide applications in neural machine translation [21], abstractive summarization [15], etc.", "startOffset": 176, "endOffset": 180}, {"referenceID": 14, "context": "With recurrent neural networks (RNNs) as the encoder and decoder, such architecture is also known as a seq2seq model, which has wide applications in neural machine translation [21], abstractive summarization [15], etc.", "startOffset": 208, "endOffset": 212}, {"referenceID": 11, "context": "[12] suggests that, in open-domain conversation systems, the query does not carry sufficient information for the reply; that the seq2seq model thus tends to generate short and meaningless sentences with little substance.", "startOffset": 0, "endOffset": 4}, {"referenceID": 29, "context": "To address this problem, we adopt a biseq2seq model, which is proposed in [30] for multi-source machine translation.", "startOffset": 74, "endOffset": 78}, {"referenceID": 28, "context": "We adopt mini-batched AdaDelta [29] for optimization.", "startOffset": 31, "endOffset": 35}, {"referenceID": 8, "context": "The biseq2seq then degrades to an utterance autoencoder [9].", "startOffset": 56, "endOffset": 59}, {"referenceID": 2, "context": "2824 [3], std = 0.", "startOffset": 5, "endOffset": 8}, {"referenceID": 17, "context": "To train our neural models, we followed [18] for hyperparameter settings.", "startOffset": 40, "endOffset": 44}, {"referenceID": 20, "context": "An encoder-encoder framework [21], first introduced in [18] for dialog systems.", "startOffset": 29, "endOffset": 33}, {"referenceID": 17, "context": "An encoder-encoder framework [21], first introduced in [18] for dialog systems.", "startOffset": 55, "endOffset": 59}, {"referenceID": 29, "context": "Another component in our approach, adapted from [30], which is essentially a seq2seq model extended with a retrieved reply.", "startOffset": 48, "endOffset": 52}, {"referenceID": 9, "context": "We asked three educated volunteers to annotate the results using a common protocol known as pointwise annotation [10, 12, 18].", "startOffset": 113, "endOffset": 125}, {"referenceID": 11, "context": "We asked three educated volunteers to annotate the results using a common protocol known as pointwise annotation [10, 12, 18].", "startOffset": 113, "endOffset": 125}, {"referenceID": 17, "context": "We asked three educated volunteers to annotate the results using a common protocol known as pointwise annotation [10, 12, 18].", "startOffset": 113, "endOffset": 125}, {"referenceID": 10, "context": "While [11] further aggressively argues that no existing automatic metric is appropriate for open-domain dialogs, they show a slight positive correlation between BLEU-2 and human evaluation in non-technical Twitter domain, which is similar to our scenario.", "startOffset": 6, "endOffset": 10}, {"referenceID": 7, "context": "BLEUs are also used in [8] for model comparison and in [12] for model selection.", "startOffset": 23, "endOffset": 26}, {"referenceID": 11, "context": "BLEUs are also used in [8] for model comparison and in [12] for model selection.", "startOffset": 55, "endOffset": 59}, {"referenceID": 17, "context": "The result is not consistent with [18], where their RNNs are slightly better than retrieval-based methods.", "startOffset": 34, "endOffset": 38}, {"referenceID": 16, "context": "Entropy is used in [17] and [12] to measure the serendipity of generated utterances.", "startOffset": 19, "endOffset": 23}, {"referenceID": 11, "context": "Entropy is used in [17] and [12] to measure the serendipity of generated utterances.", "startOffset": 28, "endOffset": 32}, {"referenceID": 1, "context": ", train routing [2], movie information [1], and human tutoring [5].", "startOffset": 16, "endOffset": 19}, {"referenceID": 0, "context": ", train routing [2], movie information [1], and human tutoring [5].", "startOffset": 39, "endOffset": 42}, {"referenceID": 4, "context": ", train routing [2], movie information [1], and human tutoring [5].", "startOffset": 63, "endOffset": 66}, {"referenceID": 12, "context": "Typically, a pre-constructed ontology defines a finite set of slots and values, for example, cuisine, location, and price range in a food service dialog system; during human-computer interaction, a state tracker fills plausible values to each slot from user input, and recommend the restaurant that best meets the user\u2019s requirement [13, 24, 26].", "startOffset": 333, "endOffset": 345}, {"referenceID": 23, "context": "Typically, a pre-constructed ontology defines a finite set of slots and values, for example, cuisine, location, and price range in a food service dialog system; during human-computer interaction, a state tracker fills plausible values to each slot from user input, and recommend the restaurant that best meets the user\u2019s requirement [13, 24, 26].", "startOffset": 333, "endOffset": 345}, {"referenceID": 25, "context": "Typically, a pre-constructed ontology defines a finite set of slots and values, for example, cuisine, location, and price range in a food service dialog system; during human-computer interaction, a state tracker fills plausible values to each slot from user input, and recommend the restaurant that best meets the user\u2019s requirement [13, 24, 26].", "startOffset": 333, "endOffset": 345}, {"referenceID": 5, "context": "[6] applies information retrieval techniques to search for related queries and replies.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[7] and [28] use both shallow hand-crafted features and deep neural networks for matching.", "startOffset": 0, "endOffset": 3}, {"referenceID": 27, "context": "[7] and [28] use both shallow hand-crafted features and deep neural networks for matching.", "startOffset": 8, "endOffset": 12}, {"referenceID": 9, "context": "[10] proposes a random walk-style algorithm to rank candidate replies.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[14] formulates query-reply transformation as a phrase-based machine translation.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "Since the last year, the renewed prosperity of neural networks witnesses an emerging trend in using RNN for dialog systems [16, 18, 19, 22].", "startOffset": 123, "endOffset": 139}, {"referenceID": 17, "context": "Since the last year, the renewed prosperity of neural networks witnesses an emerging trend in using RNN for dialog systems [16, 18, 19, 22].", "startOffset": 123, "endOffset": 139}, {"referenceID": 18, "context": "Since the last year, the renewed prosperity of neural networks witnesses an emerging trend in using RNN for dialog systems [16, 18, 19, 22].", "startOffset": 123, "endOffset": 139}, {"referenceID": 21, "context": "Since the last year, the renewed prosperity of neural networks witnesses an emerging trend in using RNN for dialog systems [16, 18, 19, 22].", "startOffset": 123, "endOffset": 139}, {"referenceID": 7, "context": "[8] proposes a mutual information objective in contrast to the conventional maximum likelihood criterion.", "startOffset": 0, "endOffset": 3}, {"referenceID": 11, "context": "[12] and [27] introduce additional content (either the most mutually informative word or topic information) to the reply generator.", "startOffset": 0, "endOffset": 4}, {"referenceID": 26, "context": "[12] and [27] introduce additional content (either the most mutually informative word or topic information) to the reply generator.", "startOffset": 9, "endOffset": 13}, {"referenceID": 16, "context": "[17] applies a variational encoder to capture query information as a distribution, from which a random vector is sampled for reply generation.", "startOffset": 0, "endOffset": 4}], "year": 2016, "abstractText": "Open-domain human-computer conversation has attracted much attention in the field of NLP. Contrary to ruleor template-based domain-specific dialog systems, open-domain conversation usually requires datadriven approaches, which can be roughly divided into two categories: retrieval-based and generation-based systems. Retrieval systems search a user-issued utterance (called a query) in a large database, and return a reply that best matches the query. Generative approaches, typically based on recurrent neural networks (RNNs), can synthesize new replies, but they suffer from the problem of generating short, meaningless utterances. In this paper, we propose a novel ensemble of retrieval-based and generation-based dialog systems in the open domain. In our approach, the retrieved candidate, in addition to the original query, is fed to an RNN-based reply generator, so that the neural model is aware of more information. The generated reply is then fed back as a new candidate for post-reranking. Experimental results show that such ensemble outperforms each single part of it by a large margin.", "creator": "LaTeX with hyperref package"}}}