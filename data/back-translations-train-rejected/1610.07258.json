{"id": "1610.07258", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Oct-2016", "title": "Representation Learning with Deconvolution for Multivariate Time Series Classification and Visualization", "abstract": "We propose a new model based on the deconvolutional networks and SAX discretization to learn the representation for multivariate time series. Deconvolutional networks fully exploit the advantage the powerful expressiveness of deep neural networks in the manner of unsupervised learning. We design a network structure specifically to capture the cross-channel correlation with deconvolution, forcing the pooling operation to perform the dimension reduction along each position in the individual channel. Discretization based on Symbolic Aggregate Approximation is applied on the feature vectors to further extract the bag of features. We show how this representation and bag of features helps on classification. A full comparison with the sequence distance based approach is provided to demonstrate the effectiveness of our approach on the standard datasets. We further build the Markov matrix from the discretized representation from the deconvolution to visualize the time series as complex networks, which show more class-specific statistical properties and clear structures with respect to different labels.", "histories": [["v1", "Mon, 24 Oct 2016 01:53:12 GMT  (1052kb,D)", "https://arxiv.org/abs/1610.07258v1", "Submitted to NeuroComputing. arXiv admin note: text overlap witharXiv:1505.04366by other authors"], ["v2", "Wed, 26 Oct 2016 00:17:45 GMT  (1054kb,D)", "http://arxiv.org/abs/1610.07258v2", "Submitted to NeuroComputing. arXiv admin note: text overlap witharXiv:1505.04366by other authors"], ["v3", "Sat, 26 Nov 2016 21:02:49 GMT  (1054kb,D)", "http://arxiv.org/abs/1610.07258v3", "arXiv admin note: text overlap witharXiv:1505.04366by other authors"]], "COMMENTS": "Submitted to NeuroComputing. arXiv admin note: text overlap witharXiv:1505.04366by other authors", "reviews": [], "SUBJECTS": "cs.LG cs.NE", "authors": ["zhiguang wang", "wei song", "lu liu", "fan zhang", "junxiao xue", "yangdong ye", "ming fan", "mingliang xu"], "accepted": false, "id": "1610.07258"}, "pdf": {"name": "1610.07258.pdf", "metadata": {"source": "CRF", "title": "Representation Learning with Deconvolution for Multivariate Time Series Classification and Visualization", "authors": ["Wei Songa", "Zhiguang Wangb", "Lu Liub", "Fan Zhangc", "Junxiao Xued", "Yangdong Yea", "Ming Fana", "MingLiang Xud"], "emails": ["iewsong@zzu.edu.cn"], "sections": [{"heading": null, "text": "We propose a new model based on deconvolutionary networks and SAX discretization to learn the representation of multivariate time series. Deconvolutionary networks take full advantage of the powerful expressivity of deep neural networks in the manner of unsupervised learning. We design a network structure that specifically captures the cross-channel correlation with deconvolution and forces the pooling operation to perform dimension reduction along each position within each channel. Discretization based on symbolic aggregate approximation is applied to the trait vectors to further extract the trait bags. We show how this representation and trait bags help in classification. A complete comparison with the sequence distance-based approach shows the effectiveness of our approach based on standard datasets. We build the Markov matrix from the discredited representation of devolution to further visualize the labels as distinct time series structures, and the distinct time series structures."}, {"heading": "1. Introduction", "text": "In recent years, the number of unemployed has tripled compared to previous years, and the number of unemployed has tripled in recent years."}, {"heading": "2. Background and Related Work", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1. Deep Neural Networks", "text": "A successful deep-learning architecture used in computer monitoring is Convolutionary Neural Networks (CNN) [13]. CNNs use translational invariance by extracting traits through receptive fields [14] and learning by weight distribution, which becomes the state-of-the-art approach to various image recognition and computer monitoring tasks [15]. The most exciting advances come from research into unmonitored learning algorithms for generative models, such as Deep Belief Networks (DBN) and denoized auto-encoders (DA) [16, 17]. Many deep generative models are developed based on energy-based models or auto-encoders. Temporal auto-encoding is integrated with Restrict Boltzmann machines (RBMs) to improve generative models."}, {"heading": "2.2. Discretization and Visualization for Time Series", "text": "The discrediting of time series is generally used in a symbolic approximation-based approach. Aligned Cluster Analysis (ACA) is introduced as an uncontrolled method to cluster the temporal patterns of human motion data [27]. It is an extension of the kernel k means clustering, but does require computing power. Persist is an uncontrolled discrediting method that then leads to a maximization of the persistence measurement of each symbol [28]. Piecewise Aggregate Approximation (PAA) methods are proposed by Keogh [29] to reduce the dimensionality of time series, which are then upgraded to symbolic aggregates."}, {"heading": "3. Representation Learning Using Deconvolutional Networks", "text": "Deconvolutionary networks have the same mathematical form as revolutionary networks. The difference is that deconvolutionary networks contain the \"reverse\" operation of convolution and pooling for reconstruction."}, {"heading": "3.1. Deconvolution", "text": "In contrast, devolutionary layers associate a single input activation with multiple outputs (Figure 2). The output of the devolutionary layer is an enlarged and dense feature map. In practice, we trim the boundary of the extended feature map to keep the size of the output map identical to that of the previous unpooling layer. The filters learned in deconvolutionary layers actually match the bases to reconstruct the same form of the input map, so, similar to the evolution network, a hierarchical structure of deconvolutionary layers is used to capture different shape details. Low-level filters tend to capture detailed / fine-grained features, while filters in higher layers tend to capture more abstract features."}, {"heading": "3.2. Unpooling", "text": "Pooling in a folding network abstracts activations in a receptive field with a single representative value to achieve robustness to noise and translation. Although it facilitates classification by maintaining only robust activations in the upper layers, spatial information within a receptive field is lost during pooling. Such loss of information can be critical for precise feature learning required for reconstruction and classification. Unbundling layers in the deconvolution network performs the reverse process of pooling and reconstructs the original size of activations as shown in Figure 3. To perform the unpooling process, we capture the locations of the maximum activations selected during pooling in the transposed variables used to return each activation to its original pooled location. This unpooling strategy is particularly useful to reconstruct the structure of the input object."}, {"heading": "3.3. Deconvolution for Multivariate Time Series", "text": "Unlike simple common deconvolution and pooling, which are both performed with square cores, our algorithm generates feature maps using a deep deconvolution network across the channel, but pooling along each individual channel. Dense, elemental deconvolution is obtained through successive operations of deconvolution, deconvolution, and rectification. Figure 4 illustrates the exemplary network structure layer by layer, which is helpful to understand the internal operations of our deconvolution network. We can observe that deconvolutions are applied with multiple 3 \u00d7 3 filters to capture both the temporal and cross-channel correlation. Lower layers tend to capture the rough configuration of the short-term signals (e.g. location and frequency), while more complex patterns are detected in higher layers, while generative deconstellations and deconfiguration form different roles for each construction of the individual features that play an effective role within the unit."}, {"heading": "4. Visualization and Classification", "text": "To illustrate the learned representation that we study and understand, we select the discretization and transformation of the final encoding in the hidden layers of the deconvolutionary networks into a Markov matrix, thus visualizing them as complex networks [33]. As in Figure 5, a time series X is divided into Q = 10 quantiles, with each quantile Qi being assigned a point in quantile q j. Repeated transitions between quantiles lead to arcs in the network with greater weights, therefore the connection is represented by thicker lines, where the weight of the arc is given by the probability that a point in the quantile Qi is followed by a point in quantile q. Repeated transitions between quantities lead to time arcs in the network with greater weights, hence the discretization is originally based on quantile Qi."}, {"heading": "5. Experiments and Results", "text": "This section describes the settings and results of deconvolution learning and then analyzes and evaluates the proposed representation in classification and visualization tasks. We mainly use two standard datasets, which are widely used in multivariate time series 2 literature. The ECG dataset contains 200 samples with two channels, of which 133 samples are normal and 67 samples abnormal. The length of an MTS sample is between 39 and 153. The wafer datasets contain 1194 samples. 1067 samples are normal and 127 samples are abnormal. The length of a sample is between 104 and 198. Pre-processing of each dataset is done by standardizing and realigning all signals with the maximum length. All missing values are filled in by 0. Table 1 shows the statistical summary of each dataset."}, {"heading": "5.1. Representation Learning with Deconvolution", "text": "Figure 4 summarizes the detailed configuration of the proposed network. Our network has a symmetrical configuration of folding and deconvolution centered around the output of the 2nd layer of folding. Input and output layers correspond to the input signals and their corresponding reconstruction. We use ReLU as activation function. The network is provided by Adadelta with the learning rate 0.1 and \u03c1 = 0.95 3.2http: / / www.cs.cmu.edu / economic bobski / 3Codes at https: / / github.com / cauchyturing / Deconv SAXFigure 6 and 7 show the reconstructions through our deconvolutionary networks. While the filters contracted by the deconvolution capture both the temporal and cross-channel information, our network is able to generate the exact reconstruction of the multivariate time series, guaranteeing the expressivity of the learned representations."}, {"heading": "5.2. Classification", "text": "For classification, we feed both the learned representation vector and the bag of SAX words into a linear SVM. Note that we only use training data to train representation with deconvolutionary networks, and then test the test representation with a single run. SAX parameters, the number of symbols and the convergence of words are selected using Leave-One-Out validation. [34] After discrediting and symbolism, the dictionary is converted through a sliding window of length and converting."}, {"heading": "5.3. Visualization and Statistical Analysis", "text": "Figure 8 has shown how the vector representation is reflected as a time series. To fully understand the representation learned through deconvolution and the effect of discretization by SAX, we flatten and discredit each function card (which is fed to the classifier as input) and visualize it as complex networks to verify further statistical characteristics. In terms of the number of discretization containers (or alphabet size in our SAX settings), we use Q = 60 for the ECG dataset and Q = 120 for the wafer dataset. We use the hierarchical force-directed algorithm as the network layout [35]. As shown in Figure 9 and 10, the demos on both datasets show different network structures. For ECGs, the normal sample tends to have a round layout, while the abnormal sample always has a narrow and winding date structure."}, {"heading": "6. Conclusion and Future Work", "text": "We propose a new model based on deconvolutionary networks and SAX discretization to learn the representation of multivariate time series. Deconvolutionary networks take full advantage of the powerful expressivity of deep neural networks in the manner of unsupervised learning. We design a network structure that specifically captures the cross-channel correlation with deconvolution and forces the pooling operation to perform dimension reduction along each position in each channel. SAX discretization is applied to the trait vectors to further extract the trait bag. We show how this representation and the trait bag help with classification. A full comparison with the sequence distance-based approach is offered to demonstrate the effectiveness of our approach. We continue to build the Markov matrix from the discretized representation to visualize the time series as complex networks that exhibit more statistical properties and class-specific structures in relation to different ones."}, {"heading": "7. Acknowledgment", "text": "This work is supported by the Natural Science Foundation of China (NSFC) under grant number 61472370, 61170223; the National Key Technologies Research and Development Programme of China under grant number 2013BAH23F01; Henan Province Ministry of Education under grant number 13A520453; Henan Province Ministry of Science and Technology under grant number 142300410229."}], "references": [{"title": "Correlation based dynamic time warping of multivariate time series", "author": ["Z. Bank\u00f3", "J. Abonyi"], "venue": "Expert Systems with Applications 39 (17) ", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2012}, {"title": "Classification of multivariate time series using locality preserving projections", "author": ["X. Weng", "J. Shen"], "venue": "Knowledge-Based Systems 21 (7) ", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2008}, {"title": "Pattern recognition in time series", "author": ["J B.K.D.D. Lin", "S Williamson"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2012}, {"title": "Early classification on multivariate time series", "author": ["G. He", "Y. Duan", "R. Peng", "X. Jing", "T. Qian", "L. Wang"], "venue": "Neurocomputing 149 ", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning a symbolic representation for multivariate time series classification", "author": ["M.G. Baydogan", "G. Runger"], "venue": "Data Mining and Knowledge Discovery ", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2014}, {"title": "T", "author": ["Z. Wang"], "venue": "Oates, Pooling sax-bop approaches with boosting to classify multivariate synchronous physiological time series data., in: FLAIRS Conference", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2015}, {"title": "Time series classification using multi-channels deep convolutional neural networks", "author": ["Y. Zheng", "Q. Liu", "E. Chen", "Y. Ge", "J.L. Zhao"], "venue": "in: International Conference on Web-Age Information Management, Springer", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2014}, {"title": "A symbolic representation of time series", "author": ["J. Lin", "E. Keogh", "S. Lonardi", "B. Chiu"], "venue": "with implications for streaming algorithms, in: Proceedings of the 8th ACM SIGMOD workshop on Research issues in data mining and knowledge discovery, ACM", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2003}, {"title": "An improvement of symbolic aggregate approximation distance measure for time series", "author": ["Y. Sun", "J. Li", "J. Liu", "B. Sun", "C. Chow"], "venue": "Neurocomputing 138 ", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning deep architectures for ai", "author": ["Y. Bengio"], "venue": "Foundations and trends R  \u00a9 in Machine Learning 2 (1) ", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2009}, {"title": "Deep learning: Methods and applications", "author": ["L. Deng", "D. Yu"], "venue": "Tech. Rep. MSR-TR-2014-21 ", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2014}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE 86 (11) ", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1998}, {"title": "Receptive fields", "author": ["D.H. Hubel", "T.N. Wiesel"], "venue": "binocular interaction and functional architecture in the cat\u2019s visual cortex, The Journal of physiology 160 (1) ", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1962}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "in: Advances in neural information processing systems", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2012}, {"title": "A fast learning algorithm for deep belief nets", "author": ["G. Hinton", "S. Osindero", "Y.-W. Teh"], "venue": "Neural computation 18 (7) ", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2006}, {"title": "Extracting and composing robust features with denoising autoencoders", "author": ["P. Vincent", "H. Larochelle", "Y. Bengio", "P.-A. Manzagol"], "venue": "in: Proceedings of the 25th international conference on Machine learning, ACM", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2008}, {"title": "Training energy-based models for time-series imputation", "author": ["P. Brakel", "D. Stroobandt", "B. Schrauwen"], "venue": "The Journal of Machine Learning Research 14 (1) ", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2013}, {"title": "Generalized denoising autoencoders as generative models", "author": ["Y. Bengio", "L. Yao", "G. Alain", "P. Vincent"], "venue": "in: Advances in Neural Information Processing Systems", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2013}, {"title": "Why does unsupervised pre-training help deep learning", "author": ["D. Erhan", "Y. Bengio", "A. Courville", "P.-A. Manzagol", "P. Vincent", "S. Bengio"], "venue": "The Journal of Machine Learning Research 11 ", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2010}, {"title": "Encouraging orthogonality between weight vectors in pretrained deep neural networks", "author": ["K. Grzegorczyk", "M. Kurdziel", "P.I. W\u00f3jcik"], "venue": "Neurocomputing 202 ", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2016}, {"title": "Deconvolutional networks", "author": ["M.D. Zeiler", "D. Krishnan", "G.W. Taylor", "R. Fergus"], "venue": "in: Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference on, IEEE", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2010}, {"title": "Tiled convolutional neural networks", "author": ["J. Ngiam", "Z. Chen", "D. Chia", "P.W. Koh", "Q.V. Le", "A.Y. Ng"], "venue": "in: Advances in Neural Information Processing Systems", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2010}, {"title": "An efficient and effective convolutional auto-encoder extreme learning machine network for 3d feature learning", "author": ["Y. Wang", "Z. Xie", "K. Xu", "Y. Dou", "Y. Lei"], "venue": "Neurocomputing 174 ", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2016}, {"title": "Aligned cluster analysis for temporal segmentation of human motion", "author": ["F. Zhou", "F. Torre", "J.K. Hodgins"], "venue": "in: Automatic Face & Gesture Recognition, 2008. 8th IEEE International Conference on, IEEE", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2008}, {"title": "Finding persisting states for knowledge discovery in time series", "author": ["F. M\u00f6rchen", "A. Ultsch"], "venue": "in: From Data and Information Analysis to Knowledge Engineering, Springer", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2006}, {"title": "Dimensionality reduction for fast similarity search in large time series databases", "author": ["E. Keogh", "K. Chakrabarti", "M. Pazzani", "S. Mehrotra"], "venue": "Knowl- 7 Wafer Abnormal  Normal Figure 10: Visualization of the complex network generated from the discretized deconvolutional features on the Wafer dataset. edge and information Systems 3 (3) ", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2001}, {"title": "Recurrence networksa novel paradigm for nonlinear time series analysis", "author": ["R.V. Donner", "Y. Zou", "J.F. Donges", "N. Marwan", "J. Kurths"], "venue": "New Journal of Physics 12 (3) ", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2010}, {"title": "Recurrence-based time series analysis by means of complex network methods", "author": ["R.V. Donner", "M. Small", "J.F. Donges", "N. Marwan", "Y. Zou", "R. Xiang", "J. Kurths"], "venue": "International Journal of Bifurcation and Chaos 21 (04) ", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2011}, {"title": "Time series classification using compression distance of recurrence plots", "author": ["D.F. Silva", "V. Souza", "M. De", "G.E. Batista"], "venue": "in: Data Mining (ICDM), 2013 IEEE 13th International Conference on, IEEE", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2013}, {"title": "Duality between time series and networks", "author": ["A.S. Campanharo", "M.I. Sirer", "R.D. Malmgren", "F.M. Ramos", "L.A.N. Amaral"], "venue": "PloS one 6 (8) ", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2011}, {"title": "Practical bayesian optimization of machine learning algorithms", "author": ["J. Snoek", "H. Larochelle", "R.P. Adams"], "venue": "in: Advances in neural information processing systems", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2012}, {"title": "Efficient", "author": ["Y. Hu"], "venue": "high-quality force-directed graph drawing, Mathematica Journal 10 (1) ", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2005}, {"title": "Fast unfolding of communities in large networks", "author": ["V.D. Blondel", "J.-L. Guillaume", "R. Lambiotte", "E. Lefebvre"], "venue": "Journal of statistical mechanics: theory and experiment 2008 (10) ", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2008}, {"title": "Google\u2019s PageRank and beyond: The science of search engine rankings", "author": ["A.N. Langville", "C.D. Meyer"], "venue": "Princeton University Press", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2011}], "referenceMentions": [{"referenceID": 0, "context": "Multivariate time series data is not only characterized by individual attributes, but also by the relationships between the attributes [1].", "startOffset": 135, "endOffset": 138}, {"referenceID": 1, "context": "Such information is not captured by the similarity between the individual sequences [2].", "startOffset": 84, "endOffset": 87}, {"referenceID": 2, "context": "(ERP) and Time Warping Edit Distance (TWED) are summarized and tested on several benchmark dataset [3].", "startOffset": 99, "endOffset": 102}, {"referenceID": 3, "context": "Mining core feature for early classification (MCFEC) along the sequence is proposed to capture the shapelets in each channel independently [4].", "startOffset": 139, "endOffset": 142}, {"referenceID": 4, "context": "SMTS builds a tree learner with two ensembles to learn the segmentations and a high-dimensional codebook [5].", "startOffset": 105, "endOffset": 108}, {"referenceID": 4, "context": "Inspired by recent advances in feature learning for image classification, several feature-based approaches are proposed like [5, 6].", "startOffset": 125, "endOffset": 131}, {"referenceID": 5, "context": "Inspired by recent advances in feature learning for image classification, several feature-based approaches are proposed like [5, 6].", "startOffset": 125, "endOffset": 131}, {"referenceID": 6, "context": "However, the feature learning approach are only limited on the scenario of supervised learning and few comparison towards distance-based learning approaches (like [7]).", "startOffset": 163, "endOffset": 166}, {"referenceID": 5, "context": "The method described in [6] is simple but not fully automated, instead they still need to design the weighting scheme manually.", "startOffset": 24, "endOffset": 27}, {"referenceID": 7, "context": "Inspired by the discretization approaches like Symbolic Aggregate Approximation (SAX) with its variations [8, 9, 6] and Markov matrix [10], we further show how this representation helps on classification and visualization tasks.", "startOffset": 106, "endOffset": 115}, {"referenceID": 8, "context": "Inspired by the discretization approaches like Symbolic Aggregate Approximation (SAX) with its variations [8, 9, 6] and Markov matrix [10], we further show how this representation helps on classification and visualization tasks.", "startOffset": 106, "endOffset": 115}, {"referenceID": 5, "context": "Inspired by the discretization approaches like Symbolic Aggregate Approximation (SAX) with its variations [8, 9, 6] and Markov matrix [10], we further show how this representation helps on classification and visualization tasks.", "startOffset": 106, "endOffset": 115}, {"referenceID": 9, "context": "Since 2006, the techniques developed from deep neural networks (or, deep learning) have greatly impacted natural language processing, speech recognition and computer vision research [11, 12].", "startOffset": 182, "endOffset": 190}, {"referenceID": 10, "context": "Since 2006, the techniques developed from deep neural networks (or, deep learning) have greatly impacted natural language processing, speech recognition and computer vision research [11, 12].", "startOffset": 182, "endOffset": 190}, {"referenceID": 11, "context": "One successful deep learning architecture used in computer vision is convolutional neural networks (CNN) [13].", "startOffset": 105, "endOffset": 109}, {"referenceID": 12, "context": "CNNs exploit translational invariance by extracting features through receptive fields [14] and learning with weight sharing, becoming the state-of-the-art approach in various image recognition and computer vision tasks [15].", "startOffset": 86, "endOffset": 90}, {"referenceID": 13, "context": "CNNs exploit translational invariance by extracting features through receptive fields [14] and learning with weight sharing, becoming the state-of-the-art approach in various image recognition and computer vision tasks [15].", "startOffset": 219, "endOffset": 223}, {"referenceID": 14, "context": "Most exciting advance comes from the exploration of unsupervised learning algorithms for generative models, such as Deep Belief Networks (DBN) and Denoised Auto-encoders (DA) [16, 17].", "startOffset": 175, "endOffset": 183}, {"referenceID": 15, "context": "Most exciting advance comes from the exploration of unsupervised learning algorithms for generative models, such as Deep Belief Networks (DBN) and Denoised Auto-encoders (DA) [16, 17].", "startOffset": 175, "endOffset": 183}, {"referenceID": 16, "context": "A training strategy inspired by recent work on optimizationbased learning is proposed to train complex neural networks for imputation tasks [19].", "startOffset": 140, "endOffset": 144}, {"referenceID": 17, "context": "A generalized Denoised Auto-encoder extends the theoretical framework and is applied to Deep Generative Stochastic Networks (DGSN) [20, 21].", "startOffset": 131, "endOffset": 139}, {"referenceID": 18, "context": "However, since unsupervised pretraining has been shown to improve performance in both fully supervised tasks and weakly supervised tasks [22, 23], deconvolution and Topographic Independent Component Analysis (TICA) are integrated as unsupervised pretraining approaches to learn more diverse features with complex invariance [24, 25, 26].", "startOffset": 137, "endOffset": 145}, {"referenceID": 19, "context": "However, since unsupervised pretraining has been shown to improve performance in both fully supervised tasks and weakly supervised tasks [22, 23], deconvolution and Topographic Independent Component Analysis (TICA) are integrated as unsupervised pretraining approaches to learn more diverse features with complex invariance [24, 25, 26].", "startOffset": 137, "endOffset": 145}, {"referenceID": 20, "context": "However, since unsupervised pretraining has been shown to improve performance in both fully supervised tasks and weakly supervised tasks [22, 23], deconvolution and Topographic Independent Component Analysis (TICA) are integrated as unsupervised pretraining approaches to learn more diverse features with complex invariance [24, 25, 26].", "startOffset": 324, "endOffset": 336}, {"referenceID": 21, "context": "However, since unsupervised pretraining has been shown to improve performance in both fully supervised tasks and weakly supervised tasks [22, 23], deconvolution and Topographic Independent Component Analysis (TICA) are integrated as unsupervised pretraining approaches to learn more diverse features with complex invariance [24, 25, 26].", "startOffset": 324, "endOffset": 336}, {"referenceID": 22, "context": "However, since unsupervised pretraining has been shown to improve performance in both fully supervised tasks and weakly supervised tasks [22, 23], deconvolution and Topographic Independent Component Analysis (TICA) are integrated as unsupervised pretraining approaches to learn more diverse features with complex invariance [24, 25, 26].", "startOffset": 324, "endOffset": 336}, {"referenceID": 23, "context": "Aligned Cluster Analysis (ACA) is introduced as an unsupervised method to cluster the temporal patterns of human motion data [27].", "startOffset": 125, "endOffset": 129}, {"referenceID": 24, "context": "Persist is an unsupervised discretization methods to maximize the persistence measurement of each symbol [28].", "startOffset": 105, "endOffset": 109}, {"referenceID": 25, "context": "Piecewise Aggregate Approximation (PAA) methods is proposed by Keogh [29] to reduce the dimensionality of time series, which is then upgraded to Symbolic Aggregate Approximation (SAX) [8].", "startOffset": 69, "endOffset": 73}, {"referenceID": 7, "context": "Piecewise Aggregate Approximation (PAA) methods is proposed by Keogh [29] to reduce the dimensionality of time series, which is then upgraded to Symbolic Aggregate Approximation (SAX) [8].", "startOffset": 184, "endOffset": 187}, {"referenceID": 26, "context": "Recurrence Networks were proposed to analyze the structural properties of time series from complex systems [30, 31].", "startOffset": 107, "endOffset": 115}, {"referenceID": 27, "context": "Recurrence Networks were proposed to analyze the structural properties of time series from complex systems [30, 31].", "startOffset": 107, "endOffset": 115}, {"referenceID": 28, "context": "extended the recurrence plot paradigm for time series classification using compression distance [32].", "startOffset": 96, "endOffset": 100}, {"referenceID": 29, "context": "Another way to build a weighted adjacency matrix is extracting transition dynamics from the first order Markov matrix [33].", "startOffset": 118, "endOffset": 122}, {"referenceID": 29, "context": "we simply build the Markov Matrix to visualize the topology of the formed complex networks as given by [33].", "startOffset": 103, "endOffset": 107}, {"referenceID": 29, "context": "To visualize the learned representation to inspect and understand, we choose to discretize and convert the final encoding in the hidden layers of the deconvolutional networks to a Markov Matrix, hence visualizing them as complex networks [33].", "startOffset": 238, "endOffset": 242}, {"referenceID": 30, "context": "The parameters of SAX, window length n, number of symbols w and alphabet size a is selected using Leave-One-Out cross validation in the training set with Bayesian optimization [34].", "startOffset": 176, "endOffset": 180}, {"referenceID": 2, "context": "We compared our model with several best methods for multivariate time series classification in recent literatures including Dynamic Time Warping (DTW), Edit Distance on Real sequence (EDR), Edit distance with Real Penalty (ERP)[3], STMS [5] and MCFEC [4] and Pooling SAX [6].", "startOffset": 227, "endOffset": 230}, {"referenceID": 4, "context": "We compared our model with several best methods for multivariate time series classification in recent literatures including Dynamic Time Warping (DTW), Edit Distance on Real sequence (EDR), Edit distance with Real Penalty (ERP)[3], STMS [5] and MCFEC [4] and Pooling SAX [6].", "startOffset": 237, "endOffset": 240}, {"referenceID": 3, "context": "We compared our model with several best methods for multivariate time series classification in recent literatures including Dynamic Time Warping (DTW), Edit Distance on Real sequence (EDR), Edit distance with Real Penalty (ERP)[3], STMS [5] and MCFEC [4] and Pooling SAX [6].", "startOffset": 251, "endOffset": 254}, {"referenceID": 5, "context": "We compared our model with several best methods for multivariate time series classification in recent literatures including Dynamic Time Warping (DTW), Edit Distance on Real sequence (EDR), Edit distance with Real Penalty (ERP)[3], STMS [5] and MCFEC [4] and Pooling SAX [6].", "startOffset": 271, "endOffset": 274}, {"referenceID": 31, "context": "We use the hierarchical force-directed algorithm as the network layout [35].", "startOffset": 71, "endOffset": 75}, {"referenceID": 32, "context": "Table 3 summarizes four statistics of all the complex networks generated from the deconvolutional representations: average degree, modularity class [36], Pagerank index [37] and average path length.", "startOffset": 148, "endOffset": 152}, {"referenceID": 33, "context": "Table 3 summarizes four statistics of all the complex networks generated from the deconvolutional representations: average degree, modularity class [36], Pagerank index [37] and average path length.", "startOffset": 169, "endOffset": 173}], "year": 2016, "abstractText": "We propose a new model based on the deconvolutional networks and SAX discretization to learn the representation for multivariate time series. Deconvolutional networks fully exploit the advantage the powerful expressiveness of deep neural networks in the manner of unsupervised learning. We design a network structure specifically to capture the cross-channel correlation with deconvolution, forcing the pooling operation to perform the dimension reduction along each position in the individual channel. Discretization based on Symbolic Aggregate Approximation is applied on the feature vectors to further extract the bag of features. We show how this representation and bag of features helps on classification. A full comparison with the sequence distance based approach is provided to demonstrate the effectiveness of our approach on the standard datasets. We further build the Markov matrix from the discretized representation from the deconvolution to visualize the time series as complex networks, which show more class-specific statistical properties and clear structures with respect to different labels.", "creator": "LaTeX with hyperref package"}}}