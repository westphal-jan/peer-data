{"id": "1610.07272", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Oct-2016", "title": "Bridging Neural Machine Translation and Bilingual Dictionaries", "abstract": "Neural Machine Translation (NMT) has become the new state-of-the-art in several language pairs. However, it remains a challenging problem how to integrate NMT with a bilingual dictionary which mainly contains words rarely or never seen in the bilingual training data. In this paper, we propose two methods to bridge NMT and the bilingual dictionaries. The core idea behind is to design novel models that transform the bilingual dictionaries into adequate sentence pairs, so that NMT can distil latent bilingual mappings from the ample and repetitive phenomena. One method leverages a mixed word/character model and the other attempts at synthesizing parallel sentences guaranteeing massive occurrence of the translation lexicon. Extensive experiments demonstrate that the proposed methods can remarkably improve the translation quality, and most of the rare words in the test sentences can obtain correct translations if they are covered by the dictionary.", "histories": [["v1", "Mon, 24 Oct 2016 03:39:22 GMT  (1466kb,D)", "http://arxiv.org/abs/1610.07272v1", "10 pages, 2 figures"]], "COMMENTS": "10 pages, 2 figures", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["jiajun zhang", "chengqing zong"], "accepted": false, "id": "1610.07272"}, "pdf": {"name": "1610.07272.pdf", "metadata": {"source": "CRF", "title": "Bridging Neural Machine Translation and Bilingual Dictionaries", "authors": ["Jiajun Zhang", "Chengqing Zong"], "emails": ["jjzhang@nlpr.ia.ac.cn", "cqzong@nlpr.ia.ac.cn"], "sections": [{"heading": null, "text": "Neural Machine Translation (NMT) is the new state of the art in multiple language pairs. However, integrating NMT with a bilingual dictionary that mainly contains words that rarely or never appear in the bilingual training data remains a difficult problem. In this essay, we propose two methods for bridging NMT and the bilingual dictionaries. The basic idea behind this is to design novel models that transform the bilingual dictionaries into adequate sentence pairs so that NMT can distil latent bilingual mappings from the abundant and repetitive phenomena. One method uses a mixed word-sign model and the other attempts to synthesize parallel sentences, guaranteeing a massive appearance of the translation lexicon. Extensive experiments show that the proposed methods can significantly improve translation quality and that most of the rare words in the test sentences receive correct translations when covered by the dictionary."}, {"heading": "1 Introduction", "text": "This year, more than ever before in the history of the country in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country and in which it is a country."}, {"heading": "2 Neural Machine Translation", "text": "Our framework, which bridges NMT and the discrete bilingual dictionaries, can be applied to any neural machine translation model (< < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < <"}, {"heading": "3 Incorporating Bilingual Dictionaries", "text": "The word translation pairs in bilingual dictionaries are difficult to use in neural machine translation, mainly because they are rarely or never seen in the parallel training corpus. We are trying to build a bridge between NMT and bilingual dictionaries. We believe that the bridge is a data transformation that can turn rare or invisible word translation pairs into frequent ones, and provide NMT with adequate information to learn latent translation mappings. In this paper, we propose two methods for performing data transformations at the character or word level."}, {"heading": "3.1 Mixed Word/Character Model", "text": "Faced with a bilingual dictionary Dic = {(Dic (i) x > >, Dic (i) y)} Ii = 1, we focus on translation lexicons (Dicx, Dicy) when Dicx is a rare or unknown word in the bilingual corpus DB. We first introduce data transformation using the character-based method. We all know that words consist of characters and most characters occur frequently, although the word is never seen. Popularly, this idea is used to use open vocabulary NMT (Ling et al., 2015; Costa-Jussa and Fonollosa, 2016; Chung et al., 2016). Translation assignments of characters are much easier for NMT to learn than word translations. However, given a string of a word in the source language, NMT cannot be guaranteed that the generated string Oltleeds will result in a valid word in the target language, < and we would prefer a few words < < < a few words in the source language."}, {"heading": "3.2 Pseudo Sentence Pair Synthesis Model", "text": "This year, as never before in the history of a country in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is not a country, in which it is not a country, but in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which is a country, in which is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which is a country, in which it is a country, in which it is a country, in which it is a country in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which is a country, in which is a country, in which it is a country, in which is a country"}, {"heading": "4 Experimental Settings", "text": "In this section, we describe the data sets, data preprocessing, training and evaluation details, and all the translation methods we compare in the experiments."}, {"heading": "4.1 Dataset", "text": "Our bilingual training data Db contains 630K2 pairs of sentences (each sentence length is limited to 50 words) extracted from the LDC corpora3. For validation, we select the data set NIST 2003 (MT03). For testing, we use the data sets NIST 2004 (MT04), NIST 2005 (MT05), NIST 2006 (MT06) and NIST 2006 (MT08). The test sentences remain in their original length. As for the source monolingual data Dsm, we collect about 100 million Chinese sentences, of which about 40% are provided by Sogou and the rest is collected by searching for words in the bilingual data on the Web. We use two bilingual dictionaries: one is from the LDC (LDC2002L27) and the other is collected manually by us. The combined dictionary Dic contains a total of 86,252 translation lexicons."}, {"heading": "4.2 Data Preprocessing", "text": "The English sentences are tokenized using the tokenizer script from the Moses decoder 5. We limit the vocabulary in both Chinese and English with a fre-2Without using very extensive data, it is relatively easy to assess the effectiveness of the bilingual dictionaries. 3LDC2000T50, LDC2002T01, LDC2002E18, LDC2003E07, LDC2003E14, LDC2003T17, LDC2004T07.4http: / nlp.stanford.edu / software / segmenter.shtml 5http: / / www.statmt.org / moses / quency threshold and we select uc = 10 for Chinese and ue = 8 for English, which turns out that the words Dic15 and Ve | = 30514 for Chinese and Db appear in these rare translation sets based on Dixic /."}, {"heading": "4.3 Training and Evaluation Details", "text": "We build the described models using the Zoph RNN6 toolkit, written in C + + / CUDA, which provides efficient training across multiple GPUs. In the NMT architecture, as illustrated in Fig. 2, the encoder includes two stacked LSTM layers, followed by a global attention layer, and the decoder also contains two stacked LSTM layers followed by the Softmax layer. The embedding dimension of the word and the size of the hidden layers are all up to 1000. Each NMT model is trained on the GPU K80 using stochastic gradient-worthy algorithm AdaGrad (Duchi et al., 2011). We use a mini-stack size of B = 128 and we perform a total of 20 iterations for all data sets. Training time for each model ranges from 2 days to 4 days. At the test date, we use the beam search with EU beam size 10. We use the metric sheet translation (2002)."}, {"heading": "4.4 Translation Methods", "text": "In the experiments, we compare our method with the conventional SMT model and the baseline-based NMT model. We list all translation methods as follows: \u2022 Moses: It is the ultramodern, phrase-based SMT system (Koehn et al., 2007). We use its default configuration and train a 4-gram language model on the target of bilingual training data. \u2022 Zoph RNN: It is the basic attention-based NMT system (Luong et al., 2015a; Zoph et al., 2016) using two stacked LSTM layers for both the encoder and the decoder. 6https: / github.com / isi-nlp-pseudo-pseudo-bilingual pseudo-pseudo-pseudo-mixed-dic: It is our NMT system that integrates the bilingual pseudo-dictionaries by using the bilingual pseudo-bilingual pseudo-bilingual pseudo-pseudo-bilingual pseudo-pseudo-pseudo-bilingual pseudo-pseudo-pseudo-pseudo-pseudo-bilingual RNN \u2022 Zoph RNN-mixed-dic: It is our NMT system that integrates the bilingual pseudo-pseudo-pseudo-dictionals for both the bilingual pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-bilingual pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-bilingual pseudo-pseudo-pseudo-pseudo-system."}, {"heading": "5 Translation Results and Analysis", "text": "To assess the quality of translation, we try to identify the following three questions: 1) Could the attention-based NMT SMT used exceed even less than 1 million sentence pairs? 2) Which model is more effective for integrating bilingual dictionaries: mixed word-character model or pseudo-sentence pair synthesis data? 3) Can the two suggested methods further improve translation performance?"}, {"heading": "5.1 NMT vs. SMT", "text": "Comparing the first two lines in Table 1, it is very obvious that the attention-based NMT system Zoph RNN significantly outperforms the phrase-based SMT system Moses at only 630K bilingual Sino-English sentence pairs, the gap can be up to 6.36 absolute BLEU points for MT04, and the average improvement is up to 4.43 BLEU points (32.98 vs. 28.55), in line with the results published in (Wu et al., 2016; Junczys-Dowmunt et al., 2016), which have conducted experiments on tens of millions or even more parallel sentence pairs. Furthermore, our experiments show that NMT can be much better even if we have less than 1 million sentence pairs."}, {"heading": "5.2 The Effect of The Mixed W/C Model", "text": "The two lines (3-4 in Table 1) present the BLEU values when applying the mixed word / character model in NMT. We can see that this model significantly improves the translation quality compared to the base-oriented NMT, although the idea behind it is very simple. Specifically, the Zoph RNN-mixed system, which is trained only on the bittext Db, fails to achieve an average improvement of more than 1.0 BLEU points (34.19 vs 32.98) above the base line Zoph RNN. It shows that the mixed word / character model can alleviate the OOV translation problem in some ex-tents. For example, the number 31.3 is an OOV word in Chinese. The mixed model translates this word into < B > 3 < M > 1 < M > 3 and it is correct that the OOV model is correct."}, {"heading": "5.3 The Effect of Data Synthesis Model", "text": "The eight lines (5-12) in Table 1 show the translation performance of the pseudo sentence pair synthesis model. We can analyze the results from three perspectives: 1) the effect of the self-learning method for using the source-side monolingual data; 2) the effect of the bilingual dictionary; and 3) the effect of the pseudo sentence pair number. Results in the odd lines (lines with Zoph RNN pseudo) show that the synthesized parallel sentence pairs using source-side monolingual data can significantly improve the baseline NMT Zoph RNN, and the average improvement can be up to 1.62 BLEU points (34.60 vs. 32.98). This finding is also reported by Cheng et al. (2016b) and Zhang and Zong (2016). After we have studied Zoph RNN pseudo-pseudo-pseudo-pseudo-dictionals with bilingual dictionaries, we can achieve 41 points in comparison, if we can achieve an average improvement to 23.2 pairs = the three points."}, {"heading": "5.4 Mixed W/C Model vs. Data Synthesis Model", "text": "If we compare the results between the mixed model and the data synthesis model (Zoph RNN-mixeddic vs. Zoph RNN-pseudo-dic) in Table 1, we can easily see that the data synthesis model is much better for integrating bilingual dictionaries in NMT. Zoph RNN-pseudo-dic can significantly exceed Zoph RNN-mixed-dic by an average improvement of up to 1.69 BLEU points (36.39 vs. 34.70). Through in-depth analysis, we find that most rare or invisible words in test sets of Zoph RNN-pseudo-dic can be translated well if they are covered by the bilingual dictionary. Table 3 reports the hit rate of bilingual dictionaries. 0.71 indicates that 2010 (2831 \u00d7 0.71) words in test sets can be well translated."}, {"heading": "6 Related Work", "text": "The fact is that we are able to put ourselves at the top and that we are able to put ourselves at the top, \"he said."}, {"heading": "7 Conclusions and Future Work", "text": "In this paper, we have presented two models for bridging neural machine translation and the bilingual dictionaries in which translation dictionaries are rarely or never seen in the bilingual training data. Our proposed methods focus on data transmission mechanisms that ensure the massive and repetitive occurrence of the translation dictionary. The mixed-word character model solves this problem by re-labeling the OOV words with string, while our data synthesis model constructs appropriate pseudo-sentence pairs for each translation dictionary. Extensive experiments show that the data synthesis model significantly outperforms the mixed-word / character model and performs the combined method best. All proposed methods achieve promising improvements over the base model NMT. Furthermore, we note that more than 70% of the rare or invisible words in test sentences can obtain correct translations as long as they are covered by the bilingual dictionary."}], "references": [{"title": "Incorporating discrete translation lexicons into neural machine translation", "author": ["Arthur et al.2016] Philip Arthur", "Graham Neubig", "Satoshi Nakamura"], "venue": "arXiv preprint arXiv:1606.02006", "citeRegEx": "Arthur et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Arthur et al\\.", "year": 2016}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Kyunghyun Cho", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1409.0473", "citeRegEx": "Bahdanau et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "2016a. Agreement-based joint training for bidirectional attention-based neural machine translation", "author": ["Cheng et al.2016a] Yong Cheng", "Shiqi Shen", "Zhongjun He", "Wei He", "Hua Wu", "Maosong Sun", "Yang Liu"], "venue": "In Proceedings of AAAI 2016", "citeRegEx": "Cheng et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Cheng et al\\.", "year": 2016}, {"title": "Semi-supervised learning for neural machine translation", "author": ["Cheng et al.2016b] Yong Cheng", "Wei Xu", "Zhongjun He", "Wei He", "Hua Wu", "Maosong Sun", "Yang Liu"], "venue": "In Proceedings of ACL 2016", "citeRegEx": "Cheng et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Cheng et al\\.", "year": 2016}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical", "author": ["Bart Van Merri\u00ebnboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio"], "venue": null, "citeRegEx": "Merri\u00ebnboer et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Merri\u00ebnboer et al\\.", "year": 2014}, {"title": "A character-level decoder without explicit segmentation for neural machine translation", "author": ["Chung et al.2016] Junyoung Chung", "Kyunghyun Cho", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1603.06147", "citeRegEx": "Chung et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Chung et al\\.", "year": 2016}, {"title": "Incorporating structural alignment biases into an attentional neural translation model", "author": ["Cohn et al.2016] Trevor Cohn", "Cong Duy Vu Hoang", "Ekaterina Vymolova", "Kaisheng Yao", "Chris Dyer", "Gholamreza Haffari"], "venue": "Proceedings of NAACL 2016", "citeRegEx": "Cohn et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Cohn et al\\.", "year": 2016}, {"title": "Characterbased neural machine translation", "author": ["Costa-Juss\u00e0", "Jos\u00e9 AR Fonollosa"], "venue": "arXiv preprint arXiv:1603.00810", "citeRegEx": "Costa.Juss\u00e0 et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Costa.Juss\u00e0 et al\\.", "year": 2016}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["Duchi et al.2011] John Duchi", "Elad Hazan", "Yoram Singer"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Duchi et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Duchi et al\\.", "year": 2011}, {"title": "Implicit distortion and fertility models for attention-based encoder-decoder nmt model", "author": ["Feng et al.2016] Shi Feng", "Shujie Liu", "Mu Li", "Ming Zhou"], "venue": "arXiv preprint arXiv:1601.03317", "citeRegEx": "Feng et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Feng et al\\.", "year": 2016}, {"title": "On using monolingual corpora in neural machine translation", "author": ["Orhan Firat", "Kelvin Xu", "Kyunghyun Cho", "Loic Barrault", "HueiChi Lin", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio"], "venue": null, "citeRegEx": "Gulcehre et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gulcehre et al\\.", "year": 2015}, {"title": "Long short-term memory", "author": ["Hochreiter", "Schmidhuber1997] Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "On using very large target vocabulary for neural machine translation", "author": ["Jean et al.2015] Sebastien Jean", "Kyunghyun Cho", "Roland Memisevic", "Yoshua Bengio"], "venue": "In Proceedings of ACL", "citeRegEx": "Jean et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Jean et al\\.", "year": 2015}, {"title": "Is neural machine translation ready for deployment? a case study on 30 translation directions", "author": ["Tomasz Dwojak", "Hieu Hoang"], "venue": "arXiv preprint arXiv:1610.01108", "citeRegEx": "JunczysDowmunt et al\\.,? \\Q2016\\E", "shortCiteRegEx": "JunczysDowmunt et al\\.", "year": 2016}, {"title": "Recurrent continuous translation models", "author": ["Kalchbrenner", "Blunsom2013] Nal Kalchbrenner", "Phil Blunsom"], "venue": "In Proceedings of EMNLP", "citeRegEx": "Kalchbrenner et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2013}, {"title": "Moses: Open source toolkit for statistical machine", "author": ["Koehn et al.2007] Philipp Koehn", "Hieu Hoang", "Alexandra Birch", "Chris Callison-Burch", "Marcello Federico", "Nicola Bertoldi", "Brooke Cowan", "Wade Shen", "Christine Moran", "Richard Zens"], "venue": null, "citeRegEx": "Koehn et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Koehn et al\\.", "year": 2007}, {"title": "Fully character-level neural machine translation without explicit segmentation", "author": ["Lee et al.2016] Jason Lee", "Kyunghyun Cho", "Thomas Hofmann"], "venue": "arXiv preprint arXiv:1610.03017", "citeRegEx": "Lee et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Lee et al\\.", "year": 2016}, {"title": "Towards zero unknown word in neural machine translation", "author": ["Li et al.2016] Xiaoqing Li", "Jiajun Zhang", "Chengqing Zong"], "venue": "In Proceedings of IJCAI 2016", "citeRegEx": "Li et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Li et al\\.", "year": 2016}, {"title": "Characterbased neural machine translation", "author": ["Ling et al.2015] Wang Ling", "Isabel Trancoso", "Chris Dyer", "Alan W Black"], "venue": "arXiv preprint arXiv:1511.04586", "citeRegEx": "Ling et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ling et al\\.", "year": 2015}, {"title": "Neural machine translation with supervised attention", "author": ["Liu et al.2016] Lemao Liu", "Masao Utiyama", "Andrew Finch", "Eiichiro Sumita"], "venue": "arXiv preprint arXiv:1609.04186", "citeRegEx": "Liu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2016}, {"title": "Multi-task sequence to sequence learning", "author": ["Quoc V Le", "Ilya Sutskever", "Oriol Vinyals", "Lukasz Kaiser"], "venue": "arXiv preprint arXiv:1511.06114", "citeRegEx": "Luong et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Effective approaches to attention-based neural machine translation", "author": ["Hieu Pham", "Christopher D Manning"], "venue": "In Proceedings of EMNLP", "citeRegEx": "Luong et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Addressing the rare word problem in neural machine translation", "author": ["Ilya Sutskever", "Quoc V Le", "Oriol Vinyals", "Wojciech Zaremba"], "venue": "In Proceedings of ACL", "citeRegEx": "Luong et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Interactive attention for neural machine translation", "author": ["Meng et al.2016] Fandong Meng", "Zhengdong Lu", "Hang Li", "Qun Liu"], "venue": "arXiv preprint arXiv:1610.05011", "citeRegEx": "Meng et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Meng et al\\.", "year": 2016}, {"title": "2016a. A coverage embedding model for neural machine translation", "author": ["Mi et al.2016a] Haitao Mi", "Baskaran Sankaran", "Zhiguo Wang", "Abe Ittycheriah"], "venue": "In Proceedings of EMNLP 2016", "citeRegEx": "Mi et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Mi et al\\.", "year": 2016}, {"title": "Supervised attentions for neural machine translation", "author": ["Mi et al.2016b] Haitao Mi", "Zhiguo Wang", "Niyu Ge", "Abe Ittycheriah"], "venue": "In Proceedings of EMNLP 2016", "citeRegEx": "Mi et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Mi et al\\.", "year": 2016}, {"title": "2016c. Vocabulary manipulation for large vocabulary neural machine translation", "author": ["Mi et al.2016c] Haitao Mi", "Zhiguo Wang", "Abe Ittycheriah"], "venue": "In Proceedings of ACL 2016", "citeRegEx": "Mi et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Mi et al\\.", "year": 2016}, {"title": "Bleu: a method for automatic evaluation of machine translation", "author": ["Salim Roukos", "Todd Ward", "Wei-Jing Zhu"], "venue": "In Proceedings of ACL", "citeRegEx": "Papineni et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "Improving neural machine translation models with monolingual data", "author": ["Barry Haddow", "Alexandra Birch"], "venue": "arXiv preprint arXiv:1511.06709", "citeRegEx": "Sennrich et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sennrich et al\\.", "year": 2015}, {"title": "Neural machine translation of rare words with subword units. arXiv preprint arXiv:1508.07909", "author": ["Barry Haddow", "Alexandra Birch"], "venue": null, "citeRegEx": "Sennrich et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sennrich et al\\.", "year": 2015}, {"title": "Minimum risk training for neural machine translation", "author": ["Shen et al.2016] Shiqi Shen", "Yong Cheng", "Zhongjun He", "Wei He", "Hua Wu", "Maosong Sun", "Yang Liu"], "venue": "In Proceedings of ACL 2016", "citeRegEx": "Shen et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Shen et al\\.", "year": 2016}, {"title": "Sequence to sequence learning with neural networks", "author": ["Oriol Vinyals", "Quoc VV Le"], "venue": "In Proceedings of NIPS", "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Coverage-based neural machine translation", "author": ["Tu et al.2016] Zhaopeng Tu", "Zhengdong Lu", "Yang Liu", "Xiaohua Liu", "Hang Li"], "venue": "In Proceedings of ACL 2016", "citeRegEx": "Tu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Tu et al\\.", "year": 2016}, {"title": "Domain adaptation for statistical machine translation with domain dictionary and monolingual corpora", "author": ["Wu et al.2008] Hua Wu", "Haifeng Wang", "Chengqing Zong"], "venue": "In Proceedings of COLING", "citeRegEx": "Wu et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Wu et al\\.", "year": 2008}, {"title": "Maximum entropy based phrase reordering model for statistical machine translation", "author": ["Xiong et al.2006] Deyi Xiong", "Qun Liu", "Shouxun Lin"], "venue": "In Proceedings of ACL-COLING,", "citeRegEx": "Xiong et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Xiong et al\\.", "year": 2006}, {"title": "Exploiting source-side monolingual data in neural machine translation", "author": ["Zhang", "Zong2016] Jiajun Zhang", "Chengqing Zong"], "venue": "In Proceedings of EMNLP", "citeRegEx": "Zhang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2016}, {"title": "Multi-source neural translation", "author": ["Zoph et al.2016] Barret Zoph", "Deniz Yuret", "Jonathan May", "Kevin Knight"], "venue": "In Proceedings of NAACL", "citeRegEx": "Zoph et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Zoph et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 31, "context": "Due to its superior ability in modelling the end-to-end translation process, neural machine translation (NMT), recently proposed by (Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Sutskever et al., 2014), has become the novel paradigm and achieved the new state-of-the-art translation performance for several language pairs, such as English-to-French, English-to-German and Chinese-to-English (Sutskever et al.", "startOffset": 132, "endOffset": 206}, {"referenceID": 31, "context": ", 2014), has become the novel paradigm and achieved the new state-of-the-art translation performance for several language pairs, such as English-to-French, English-to-German and Chinese-to-English (Sutskever et al., 2014; Bahdanau et al., 2014; Luong et al., 2015b; Sennrich et al., 2015b; Wu et al., 2016).", "startOffset": 197, "endOffset": 306}, {"referenceID": 1, "context": ", 2014), has become the novel paradigm and achieved the new state-of-the-art translation performance for several language pairs, such as English-to-French, English-to-German and Chinese-to-English (Sutskever et al., 2014; Bahdanau et al., 2014; Luong et al., 2015b; Sennrich et al., 2015b; Wu et al., 2016).", "startOffset": 197, "endOffset": 306}, {"referenceID": 0, "context": "Recently, Arthur et al. (2016) attempt at incorporating discrete translation lexicons into NMT.", "startOffset": 10, "endOffset": 31}, {"referenceID": 33, "context": "Our first method extends the mixed word/character model proposed by Wu et al. (2016) to re-label the rare words in both of the dictionary and training data with character sequences in which characters are now frequent and the character translation mappings can be learnt by NMT.", "startOffset": 68, "endOffset": 85}, {"referenceID": 1, "context": ", 2014), the authors choose C = hmTx , while Bahdanau et al. (2014) use different context ci at different time step and the conditional probability will become:", "startOffset": 45, "endOffset": 68}, {"referenceID": 18, "context": "This idea is popularly used to deploy open vocabulary NMT (Ling et al., 2015; Costa-Juss\u00e0 and Fonollosa, 2016; Chung et al., 2016).", "startOffset": 58, "endOffset": 130}, {"referenceID": 5, "context": "This idea is popularly used to deploy open vocabulary NMT (Ling et al., 2015; Costa-Juss\u00e0 and Fonollosa, 2016; Chung et al., 2016).", "startOffset": 58, "endOffset": 130}, {"referenceID": 33, "context": "Therefore, we prefer the framework mixing the words and characters, which is employed by Wu et al. (2016) to handle OOV words.", "startOffset": 89, "endOffset": 106}, {"referenceID": 15, "context": "phrase-based SMT (Koehn et al., 2007; Xiong et al., 2006)) is easy to integrate bilingual dictionaries (Wu et al.", "startOffset": 17, "endOffset": 57}, {"referenceID": 34, "context": "phrase-based SMT (Koehn et al., 2007; Xiong et al., 2006)) is easy to integrate bilingual dictionaries (Wu et al.", "startOffset": 17, "endOffset": 57}, {"referenceID": 33, "context": ", 2006)) is easy to integrate bilingual dictionaries (Wu et al., 2008) as long as we consider the translation lexicons of bilingual dictionaries as phrasal translation rules.", "startOffset": 53, "endOffset": 70}, {"referenceID": 33, "context": "Following (Wu et al., 2008), we first merge the bilingual sentence corpus Db with the bilingual dictionaries Dic, and employ the phrase-based SMT to train an SMT system called PBMT (line 1 in Algorithm 1).", "startOffset": 10, "endOffset": 27}, {"referenceID": 2, "context": "(2015b), Cheng et al. (2016b) and Zhang and Zong (2016) observe from large-scale experiments that the synthesized bilingual data using self-learning framework can substantially improve NMT performance.", "startOffset": 9, "endOffset": 30}, {"referenceID": 2, "context": "(2015b), Cheng et al. (2016b) and Zhang and Zong (2016) observe from large-scale experiments that the synthesized bilingual data using self-learning framework can substantially improve NMT performance.", "startOffset": 9, "endOffset": 56}, {"referenceID": 8, "context": "Each NMT model is trained on GPU K80 using stochastic gradient decent algorithm AdaGrad (Duchi et al., 2011).", "startOffset": 88, "endOffset": 108}, {"referenceID": 27, "context": "We use case-insensitive 4-gram BLEU score as the automatic metric (Papineni et al., 2002) for translation quality evaluation.", "startOffset": 66, "endOffset": 89}, {"referenceID": 15, "context": "\u2022 Moses: It is the state-of-the-art phrase-based SMT system (Koehn et al., 2007).", "startOffset": 60, "endOffset": 80}, {"referenceID": 36, "context": "\u2022 Zoph RNN: It is the baseline attention-based NMT system (Luong et al., 2015a; Zoph et al., 2016) using two stacked LSTM layers for both of the encoder and the decoder.", "startOffset": 58, "endOffset": 98}, {"referenceID": 2, "context": "This finding is also reported by Cheng et al. (2016b) and Zhang and Zong (2016).", "startOffset": 33, "endOffset": 54}, {"referenceID": 2, "context": "This finding is also reported by Cheng et al. (2016b) and Zhang and Zong (2016).", "startOffset": 33, "endOffset": 80}, {"referenceID": 6, "context": "Most of the existing methods mainly focus on designing better attention models (Luong et al., 2015b; Cheng et al., 2016a; Cohn et al., 2016; Feng et al., 2016; Liu et al., 2016; Meng et al., 2016; Mi et al., 2016a; Mi et al., 2016b; Tu et al., 2016), better objective functions for BLEU evaluation (Shen et al.", "startOffset": 79, "endOffset": 249}, {"referenceID": 9, "context": "Most of the existing methods mainly focus on designing better attention models (Luong et al., 2015b; Cheng et al., 2016a; Cohn et al., 2016; Feng et al., 2016; Liu et al., 2016; Meng et al., 2016; Mi et al., 2016a; Mi et al., 2016b; Tu et al., 2016), better objective functions for BLEU evaluation (Shen et al.", "startOffset": 79, "endOffset": 249}, {"referenceID": 19, "context": "Most of the existing methods mainly focus on designing better attention models (Luong et al., 2015b; Cheng et al., 2016a; Cohn et al., 2016; Feng et al., 2016; Liu et al., 2016; Meng et al., 2016; Mi et al., 2016a; Mi et al., 2016b; Tu et al., 2016), better objective functions for BLEU evaluation (Shen et al.", "startOffset": 79, "endOffset": 249}, {"referenceID": 23, "context": "Most of the existing methods mainly focus on designing better attention models (Luong et al., 2015b; Cheng et al., 2016a; Cohn et al., 2016; Feng et al., 2016; Liu et al., 2016; Meng et al., 2016; Mi et al., 2016a; Mi et al., 2016b; Tu et al., 2016), better objective functions for BLEU evaluation (Shen et al.", "startOffset": 79, "endOffset": 249}, {"referenceID": 32, "context": "Most of the existing methods mainly focus on designing better attention models (Luong et al., 2015b; Cheng et al., 2016a; Cohn et al., 2016; Feng et al., 2016; Liu et al., 2016; Meng et al., 2016; Mi et al., 2016a; Mi et al., 2016b; Tu et al., 2016), better objective functions for BLEU evaluation (Shen et al.", "startOffset": 79, "endOffset": 249}, {"referenceID": 30, "context": ", 2016), better objective functions for BLEU evaluation (Shen et al., 2016), better strategies for handling open vocabulary (Ling et al.", "startOffset": 56, "endOffset": 75}, {"referenceID": 18, "context": ", 2016), better strategies for handling open vocabulary (Ling et al., 2015; Luong et al., 2015c; Jean et al., 2015; Sennrich et al., 2015b; CostaJuss\u00e0 and Fonollosa, 2016; Lee et al., 2016; Li et al., 2016; Mi et al., 2016c; Wu et al., 2016) and exploiting large-scale monolingual data (Gulcehre et al.", "startOffset": 56, "endOffset": 241}, {"referenceID": 12, "context": ", 2016), better strategies for handling open vocabulary (Ling et al., 2015; Luong et al., 2015c; Jean et al., 2015; Sennrich et al., 2015b; CostaJuss\u00e0 and Fonollosa, 2016; Lee et al., 2016; Li et al., 2016; Mi et al., 2016c; Wu et al., 2016) and exploiting large-scale monolingual data (Gulcehre et al.", "startOffset": 56, "endOffset": 241}, {"referenceID": 16, "context": ", 2016), better strategies for handling open vocabulary (Ling et al., 2015; Luong et al., 2015c; Jean et al., 2015; Sennrich et al., 2015b; CostaJuss\u00e0 and Fonollosa, 2016; Lee et al., 2016; Li et al., 2016; Mi et al., 2016c; Wu et al., 2016) and exploiting large-scale monolingual data (Gulcehre et al.", "startOffset": 56, "endOffset": 241}, {"referenceID": 17, "context": ", 2016), better strategies for handling open vocabulary (Ling et al., 2015; Luong et al., 2015c; Jean et al., 2015; Sennrich et al., 2015b; CostaJuss\u00e0 and Fonollosa, 2016; Lee et al., 2016; Li et al., 2016; Mi et al., 2016c; Wu et al., 2016) and exploiting large-scale monolingual data (Gulcehre et al.", "startOffset": 56, "endOffset": 241}, {"referenceID": 10, "context": ", 2016) and exploiting large-scale monolingual data (Gulcehre et al., 2015; Sennrich et al., 2015a; Cheng et al., 2016b; Zhang and Zong, 2016).", "startOffset": 52, "endOffset": 142}, {"referenceID": 2, "context": "Cheng et al. (2016b) and Zhang and Zong (2016) also investigate the effect of the synthesized parallel sentences.", "startOffset": 0, "endOffset": 21}, {"referenceID": 2, "context": "Cheng et al. (2016b) and Zhang and Zong (2016) also investigate the effect of the synthesized parallel sentences.", "startOffset": 0, "endOffset": 47}, {"referenceID": 0, "context": "Very recently, Arthur et al. (2016) try to use discrete translation lexicons in NMT.", "startOffset": 15, "endOffset": 36}], "year": 2016, "abstractText": "Neural Machine Translation (NMT) has become the new state-of-the-art in several language pairs. However, it remains a challenging problem how to integrate NMT with a bilingual dictionary which mainly contains words rarely or never seen in the bilingual training data. In this paper, we propose two methods to bridge NMT and the bilingual dictionaries. The core idea behind is to design novel models that transform the bilingual dictionaries into adequate sentence pairs, so that NMT can distil latent bilingual mappings from the ample and repetitive phenomena. One method leverages a mixed word/character model and the other attempts at synthesizing parallel sentences guaranteeing massive occurrence of the translation lexicon. Extensive experiments demonstrate that the proposed methods can remarkably improve the translation quality, and most of the rare words in the test sentences can obtain correct translations if they are covered by the dictionary.", "creator": "LaTeX with hyperref package"}}}