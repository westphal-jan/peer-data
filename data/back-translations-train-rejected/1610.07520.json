{"id": "1610.07520", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Oct-2016", "title": "Nonlinear Adaptive Algorithms on Rank-One Tensor Models", "abstract": "This work proposes a low complexity nonlinearity model and develops adaptive algorithms over it. The model is based on the decomposable---or rank-one, in tensor language---Volterra kernels. It may also be described as a product of FIR filters, which explains its low-complexity. The rank-one model is also interesting because it comes from a well-posed problem in approximation theory. The paper uses such model in an estimation theory context to develop an exact gradient-type algorithm, from which adaptive algorithms such as the least mean squares (LMS) filter and its data-reuse version---the TRUE-LMS---are derived. Stability and convergence issues are addressed. The algorithms are then tested in simulations, which show its good performance when compared to other nonlinear processing algorithms in the literature.", "histories": [["v1", "Mon, 24 Oct 2016 18:12:18 GMT  (884kb,D)", "http://arxiv.org/abs/1610.07520v1", null]], "reviews": [], "SUBJECTS": "cs.SY cs.LG", "authors": ["felipe c pinheiro", "cassio g lopes"], "accepted": false, "id": "1610.07520"}, "pdf": {"name": "1610.07520.pdf", "metadata": {"source": "CRF", "title": "Nonlinear Adaptive Algorithms on Rank-One Tensor Models", "authors": ["Felipe C. Pinheiro", "Cassio G. Lopes"], "emails": [], "sections": [{"heading": null, "text": "This year, it has come to the point that it has never come as far as it has this year."}, {"heading": "A. Notation", "text": "The notation of the paper follows that of [22] and introduces a new notation if necessary. Conventions are summarized as follows: 161 0.07 520v 1 [cs.S Y] 24 Oct 201 62 \u2022 Scalars and vectors are represented by lowercase letters. (E.g. x and a.) \u2022 Time-changing vectors are indexed as xi, while time-changing scalars are represented as a (i). The time variable is always i or j. \u2022 Matrices and constants are represented as uppercase letters. (E.g. R and A.) \u2022 Tensors are represented as calligraphic letters. (E.g. W and X.) \u2022 Time-changing matrices and tensors follow the vector convention. \u2022 Tensors have a matrix representation by nature. (E.g. if they are interpreted as matrices, the operations involved are interpreted as matrix equivalents. (E.g. the tensor product is interpreted as a general product, e.g. an index)."}, {"heading": "II. TENSORS", "text": "This year, it is only a matter of time before an agreement is reached."}, {"heading": "A. Rank-one approximation", "text": "It is, of course, possible to extend standards and internal products of vectors to corresponding standards of tensors. The details of how they are calculated are set out in Appendix A. These terms allow the assumption of approximation problems. For example, it is possible to pose the problem of rank R approxation3: With a tensor T it is possible to find a tensor X that solves the problem X-X, s. rankX \u2264 R. (6) This problem has the potential to reduce the number of parameters necessary to represent the tensor, but it is generally not well positioned and may not have a solution [21]. A case in which this always has a solution is with order 2 tensors - that is matrices. But for general order K tensors is the case in which it is always guaranteed that it has a solution if R = 1, the so-called rank-one approximation. This fact follows from the next proposal sentence: X-D = therefore the most probable problem is X-1."}, {"heading": "III. THE VOLTERRA SERIES", "text": "The Volterra series is treated as a universal approximator for nonlinear systems. It can be described as a polynomial representation of the system (1), closely related to a Taylor series representation. Explicitly, a nonlinear system with input signal u (i) and output y (i), its Volterra series representation is the sumy (i) = y0 + y1 (i) + y2 (i) + y3 (i) + \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 (i) is called the order K homogeneous component of the system and is defined by byyk (i) = i1,..., ikHk (i1),."}, {"heading": "A. Decomposable Model", "text": "Consider the volterra kernel tensor HK of order K and assume that it is a decomposable tensor, that there are vectors w1,.., wK, and CM, such as HK = w1 \u00b7 wK. (15) A kernel like this fulfills the decomposable model - also called the Simple Multilinear Model (SML) [20]. This model allows for large increases in computational complexity, as it can be seen by performing the following calculations: yK (i) = u ki Hk = (ui ki Hk) (ui K times) (w1 \u00b7 \u00b7 wK) = M i1,..., iK = 0 u (i \u2212 i1) \u00b7 iK (i \u2212 iK) w1 (i1) \u00b7 wK (iK)."}, {"heading": "IV. ESTIMATION THEORY OF THE DECOMPOSABLE MODEL", "text": "Imagine you would rank the best order of K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K-K"}, {"heading": "V. THE STEEPEST DESCENT ALGORITHM", "text": "The steepest descendant algorithm can be used to find the minimum of (18), which is based on the recursionwi = wi \u2212 1 \u2212 \u00b5 [\u03c3J (wi \u2212 1)] \u043c. (35) With respect to the individual vectors, this can be rewritten as asws, i = ws, i \u2212 1 + \u00b5W (s) \u0432 [RduK \u2212 RuKW]. (36) Due to the nonlinear nature of the iteration process \u2212 m, there are some problems with the initializations of the parameters. Firstly, not all of them should be initialized at zero, as (36) shows that this would result in the parameters remaining at zero \u2212 m throughout the iteration process. Another bad initialization is to set all of them to the same initial values. Recursion shows that this would result in them being adjusted exactly the same way, never being able to deviate from each other."}, {"heading": "VI. ADAPTIVE ALGORITHMS", "text": "Since it is impractical to use the steepest descent algorithm, an adaptive solution is found that implements an approximate recursion (36) by calculating a real-time approximation to the parameters (19)."}, {"heading": "A. Rectangular Window Approximation", "text": "One possible approximation is to use a rectangular window to estimate the correlation parameters - that is, the last L samples of the signals are used in an average to calculate the approximations. (41) This allows the calculation of an estimate of the block gradient (23): J-ws = [\u2212 R-uKd + W-uK] W (s) = 1L-i-j-duK. (41) This allows the calculation of an estimate of the block gradient (23): J-ws = [\u2212 R-uKd + W-K] W (s) = 1L-i-i-j-j-j] u Kj W (s). (42) Equation (42) can be conveniently rewritten as follows."}, {"heading": "B. LMS Algorithm", "text": "If L = 1, equation (46) becomes equivalent to \"J = \u2212 e (i) \u0445 (yi ui), (49), where e (i), d (i) \u2212 u i W. Separating them for each ws results in the LMS equation, i = ws, i \u2212 1 + \u00b5e (i) ys (i) \u0445 u \u0445 i. (50) This is a recursion of the smallest medium-sized squares (LMS)."}, {"heading": "C. True-LMS Algorithm", "text": "Using a general value of L, the recursion of the TRUE-LMS (sometimes referred to as Dat Reuse LMS) is derived: wi = wi \u2212 1 + \u00b5 L T \u0445 i ei. (51) This can be further simplified by separating each ws, i = ws, i \u2212 1 + \u00b5L i \u2211 j = i \u2212 L + 1 [d (j) \u2212 u Kj Wi \u2212 1] ys (j) \u0445 u \u0445 j = ws, i \u2212 1 + \u00b5 L (ys, i Ui) \u0445 ei, (52) where, i, ys (i)... ys (i \u2212 L + 1) (L + 1) (L \u00b7 K), Ui \u2212 L + 1 (L \u00b7 M) (53) and \u0438 denotes serial scalar multiplication.0 500 1000 1500 2000 2500 3000 3500 4000 4000 4500 5000 \u2212 30 \u2212 20 \u2212 100 Iteration dBMean Square Error (estimated) (a) (the arrows point to 1,000 realisations."}, {"heading": "D. Stabilization of the Algorithms", "text": "It is well known that non-linearity in the update equations can lead to a probability of deviation that is not zero, due to the fact that the algorithms are not globally asymptotically stable. An example of this phenomenon is in Fig. 3. The large \"jumps\" in MSE curve 3a show that the algorithm is currently entering an area of instability. If the parameters are far enough away from the stability region, the algorithm would deviate into infinity, as shown in Fig. 3b. For the effective application of these algorithms, some normalization may be necessary. In the case of the LMS, normalization can be carried out by conditionally modifying the update equation as in (54)."}, {"heading": "E. Step Bounds", "text": "For a first description of the algorithms to be completed, there must be a method to select the parameters of the algorithms. < < < / p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p >"}, {"heading": "F. Computational Complexity", "text": "Table I shows an efficient implementation of the TRUELMS algorithm, along with the required number of operations in each step of the process. The following auxiliary variables are used: 8Table II and the implementation of the LMS are shown. Some optimizations regarding the TRUE-LMS are possible, making it more efficient than one would expect by using L = 1 in the previous table."}, {"heading": "VII. SIMULATIONS", "text": "In all of these experiments, the input vector ui was chosen to have a delay line structure as inui = [u (i) u (i \u2212 1)... u (i \u2212 M + 1)], (63) with u (i) an i.i.d. signal sampled from the normal distribution N (0, 1), the desired signal follows a \"system identification model\" with d (i) = u Ki Wo + v (i), (64) where the plant to be identified is and v (i) an i.i.d. signal sampled from the distribution N (0, \u03c32v) independently of u (i). All adaptive algorithms simulated here are stabilized versions 54 and 55. They are hereinafter referred to as SML."}, {"heading": "A. Decomposable plants", "text": "The simulations in this section use a decomposable plant, which represents the best case scenario for the SML algorithms = 1,000 were repeated, and were performed against the algorithms Volterra-LMS and Wiener-LMS [27]. The Volterra algorithm is based on (10) and the Wiener is a modified version, in which the 0 1000 2000 3000 4000 5000 \u2212 40 \u2212 20 \u2212 100 iteration dBExcess Mean Square Error (estimated) VolterraWienerSML (a) K = 2 and \u03c32v = 10 \u2212 3.0 1000 2000 4000 \u2212 40 \u2212 100 \u2212 100 iteration dBExcess Mean Square Error (estimated) SMLVolterraWiener (b) K = 2 \u2212 6, but with a different plant.The resulting regressor is statistically orthonormal. Both are linear-in-the parameters algorithms, in contrast to the SML inputs, which were repeated simulations not.1,000 = 1,000."}, {"heading": "B. Stability", "text": "As already mentioned in Section VI-D, non-linearity in algorithm recursion can lead to non-global stability. The following experiments use the same decomposable plant as in Fig. 5a and are intended to show the stability of algorithms with different step sizes. This experiment was carried out by a total of 10,000 realizations and the number of divergent realizations was counted. Table III shows the results for the LMS and the TRUE-LMS (L = 4 and L = 8), depending on the step size. It is possible to notice the stability of the algorithms for steps smaller than \u00b50. Furthermore, the greater the expected value of L, the more stable they become. Additionally, the step-bound \u00b50 may be too conservative for the TRUE-LMS, but this depends in a complex way on the value of L."}, {"heading": "C. Adaptive algorithms as an approximation of the steepest descent", "text": "Following on from the layout in Fig. 5a, this section intends to show how the adaptive algorithms can be considered as an approximation to the steepest descendant algorithm on (36), provided the step width is small enough. Here, the curves of the TRUE-LMS are not shown, as they would almost overlap the curves of the LMS, as in Fig. 6. The curves of the adaptive algorithms form an ensemble average of 10,000 realizations and are present Fig. 8. If the step width is small, as in Fig. 8a, the curves are almost superimposed, and when they are increased, they begin to take different paths. Another noteworthy effect is the increase in the minimum MSE in the adaptive algorithm. 10"}, {"heading": "D. A family of non-decomposable plants", "text": "This section examines the decomposition hypothesis. A volterra nucleus given by a bivariate normal probability function was chosen as the family of test plants, because the correlation coefficient \u03c1 proves to be a good measure of the decomposition of the plant, where \u03c1 = is the perfect decomposition and \u03c1 = 1 is the worst-case scenario. Expressed explicitly, the plant is given by where (i, j) = \u03b1 exp [\u2212 (i \u2212 1) 2 + (i \u2212 1) 2 + 2\u03c1 (i \u2212 1) (j \u2212 1) 18 (1 \u2212 \u03c12)], (66) where \u03b1 is a normalization parameter. A typical plant is shown in Fig. 7. The tests were performed with \u03c32v = 10 \u2212 3 and by varying the practical parameter between 0 and 1 in steps of 0.1. The results of the tests are shown in Fig. 7b. As predicted, the best results were associated with the lowest heat of the lowest and lowest energy system with this highest."}, {"heading": "E. SML versus algorithms in the literature", "text": "The algorithm was tested against several others from the literature, with K = 2. The tested were: \u2022 The power filter (PF) [24] am (11) with D = 1, which uses only the diagonal elements of the Volterra series. 22 coefficients. \u2022 The simplified Volterra filter (SV) [7], [8], the truncated diagonal model in (11), with D = 3.. 60 coefficients. \u2022 The economical interpolated Volterra (IV) [9], [10], which estimates only some entries of the Volterra kernel and interpolates the others. 66 coefficients. \u2022 The complete Volterra model (V) am (10). 231 coefficients. \u2022 The SML-LMS algorithm on (50), [10], which estimates only some entries of the Volterra kernel and interpolates the others. The plant to be identified is a smooth coefficient, as in [10] the parameters most parallel to SML. Here are the results on SML = SML."}, {"heading": "VIII. CHAOTIC BEHAVIOR", "text": "A final experiment concerns the formation of a chaotic behavior of the algorithm for certain values of the step size. Let's take a scalar model with M = 1 and K = 2. In this simple case, the algorithm adjusts two parameters. Let's also assume the model (i) = 100u (i) 2, (67), where u (i) = 1 is a constant signal. The bifurcation diagram of the LMS algorithm, parameterized by \u00b5, is indicated by Figure 10. It is possible to detect a well-behaved convergence region in 0 < \u00b5 < 0.01, as predicted by step boundaries. From then on, the convergence begins to oscillate and begins to experience a period duplication until a point becomes completely chaotic. Starting from 0.016, the parameter begins to jump from positive to negative regions. A typical orbit in this region will be shown in Figure 11.When the algorithm for 2.5 starts to appear monotically, this part begins to diversify."}, {"heading": "IX. CONCLUSION", "text": "This paper presents a low complexity of non-linear filters based on an approximation of the Volterra 11, motivated both by the exponential reduction in complexity and by the positioning of the Volterra series. Description of this model is accompanied by an estimation problem resulting in a continuous descent."}, {"heading": "APPENDIX A NORMS AND INNER PRODUCTS OF TENSORS", "text": "Let (V1, < \u00b7, \u00b7 > 1), (V2, < \u00b7, \u00b7 > 2),.. (UK, < \u00b7, \u00b7 > K) be complex inner product spaces. The natural way to induce an inner product to the tensor product V1 V2 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 UK is to define the inner product on the decomposable tensors as < v1 v2 v2 and extend beyond the sesquilinearity for general tensors, the linear combinations of decomposable ones.This inner product induces a '2 standard in the usual way as \"T\" < T > procompilable tensors, the linear combinations of decomposable ones.This inner product induces a' 2 standard in the usual way as \"T\" norm. \""}], "references": [{"title": "Limitations of handsfree acoustic echo cancellers due to nonlinear loudspeaker distortion and enclosure vibration effects", "author": ["A.N. Birkett", "R.A. Goubran"], "venue": "Applications of Signal Processing to Audio and Acoustics, 1995., IEEE ASSP Workshop on, Oct 1995, pp. 103\u2013106.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1995}, {"title": "Modulation and detection for data transmission on the telephone channel", "author": ["R.W. Lucky"], "venue": "New Directions in Signal Processing in Communication and Control, 1975.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1975}, {"title": "Adaptive linearization of a loudspeaker", "author": ["F.X.Y. Gao", "W.M. Snelgrove"], "venue": "Acoustics, Speech, and Signal Processing, 1991. ICASSP- 91., 1991 International Conference on, Apr 1991, pp. 3589\u20133592 vol.5.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1991}, {"title": "New nonlinear adaptive fir digital filter for broadband noise cancellation", "author": ["H.K. Kwan", "Q.P. Li"], "venue": "IEEE Transactions on Circuits and Systems II: Analog and Digital Signal Processing, vol. 41, no. 5, pp. 355\u2013360, May 1994.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1994}, {"title": "Adaptive polynomial filters", "author": ["V. Mathews"], "venue": "Signal Processing Magazine, IEEE, vol. 8, no. 3, pp. 10\u201326, July 1991.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1991}, {"title": "Low-complexity nonlinear adaptive filters for acoustic echo cancellation in gsm handset receivers", "author": ["A. Fermo", "A. Carini", "G.L. Sicuranza"], "venue": "European Transactions on Telecommunications, vol. 14, no. 2, pp. 161\u2013169, 2003. [Online]. Available: http://dx.doi.org/10.1002/ett.908", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2003}, {"title": "Simplified Volterra filters for acoustic echo cancellation in gsm receivers", "author": ["\u2014\u2014"], "venue": "Signal Processing Conference, 2000 10th European, Sept 2000, pp. 1\u20134.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2000}, {"title": "A fully lms adaptive interpolated Volterra structure", "author": ["E. Batista", "O.J. Tobias", "R. Seara"], "venue": "Acoustics, Speech and Signal Processing, 2008. ICASSP 2008. IEEE International Conference on, March 2008, pp. 3613\u20133616.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2008}, {"title": "A sparse-interpolated scheme for implementing adaptive Volterra filters", "author": ["E. Batista", "O. Tobias", "R. Seara"], "venue": "Signal Processing, IEEE Transactions on, vol. 58, no. 4, pp. 2022\u20132035, April 2010.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2022}, {"title": "Adaptive parallelcascade truncated Volterra filters", "author": ["T.M. Panicker", "V.J. Mathews", "G.L. Sicuranza"], "venue": "IEEE Transactions on Signal Processing, vol. 46, no. 10, pp. 2664\u20132673, Oct 1998.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1998}, {"title": "Nonlinear system modeling and identification using volterra-parafac models", "author": ["G. Favier", "A.Y. Kibangou", "T. Bouilloc"], "venue": "International Journal of Adaptive Control and Signal Processing, vol. 26, no. 1, pp. 30\u201353, 2012.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2012}, {"title": "Multilinear Algebra, ser. Universitext", "author": ["W. Greub"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2012}, {"title": "Low rank estimation of higher order statistics", "author": ["T. Andre", "R. Nowak", "B. Van Veen"], "venue": "Acoustics, Speech, and Signal Processing, 1996. ICASSP-96. Conference Proceedings., 1996 IEEE International Conference on, vol. 5, May 1996, pp. 3026\u2013308a vol. 5.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1996}, {"title": "Tensor product basis approximations for volterra filters", "author": ["R. Nowak", "B. Van Veen"], "venue": "Signal Processing, IEEE Transactions on, vol. 44, no. 1, pp. 36\u201350, Jan 1996.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1996}, {"title": "Tensors: a brief introduction", "author": ["P. Comon"], "venue": "IEEE Signal Processing Magazine, vol. 31, no. 3, pp. 44\u201353, 2014.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2014}, {"title": "A finite algorithm to compute rank-1 tensor approximations", "author": ["A.P. da Silva", "P. Comon", "A.L.F. de Almeida"], "venue": "IEEE Signal Processing Letters, vol. 23, no. 7, pp. 959\u2013963, July 2016.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2016}, {"title": "Gradient-based approaches to learn tensor products", "author": ["M. Rupp", "S. Schwarz"], "venue": "Signal Processing Conference (EUSIPCO), 2015 23rd European, Aug 2015, pp. 2486\u20132490.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2015}, {"title": "A tensor lms algorithm", "author": ["\u2014\u2014"], "venue": "2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), April 2015, pp. 3347\u20133351.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2015}, {"title": "Newton-like nonlinear adaptive filters via simple multilinear functionals", "author": ["F.C. Pinheiro", "C. Lopes"], "venue": "2016 24th European Signal Processing Conference (EUSIPCO) (EUSIPCO 2016), Budapest, Hungary, Aug. 2016, pp. 1588\u20131592.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2016}, {"title": "Tensor rank and the ill-posedness of the best low-rank approximation problem", "author": ["V. de Silva", "L.-H. Lim"], "venue": "SIAM Journal on Matrix Analysis and Applications, vol. 30, no. 3, pp. 1084\u20131127, 2008. [Online]. Available: http://dx.doi.org/10.1137/06066518X", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2008}, {"title": "Adaptive Filters", "author": ["A.H. Sayed"], "venue": null, "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2008}, {"title": "Kronecker products and matrix calculus in system theory", "author": ["J. Brewer"], "venue": "Circuits and Systems, IEEE Transactions on, vol. 25, no. 9, pp. 772\u2013781, Sep 1978.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 1978}, {"title": "Nonlinear acoustic echo cancellation using adaptive orthogonalized power filters", "author": ["F. Kuech", "A. Mitnacht", "W. Kellermann"], "venue": "Acoustics, Speech, and Signal Processing, 2005. Proceedings. (ICASSP \u201905). IEEE International Conference on, vol. 3, March 2005, pp. iii/105\u2013iii/108 Vol. 3.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2005}, {"title": "Probability of divergence for the least-mean fourth algorithm", "author": ["V.H. Nascimento", "J.C.M. Bermudez"], "venue": "IEEE Transactions on Signal Processing, vol. 54, no. 4, pp. 1376\u20131385, April 2006.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2006}, {"title": "Convergence analysis of lms filters with uncorrelated gaussian data", "author": ["A. Feuer", "E. Weinstein"], "venue": "IEEE Transactions on Acoustics, Speech, and Signal Processing, vol. 33, no. 1, pp. 222\u2013230, Feb 1985.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 1985}, {"title": "Adaptive Nonlinear System Indentification: The Volterra and Wiener Model Approaches", "author": ["T. Ogunfunmi"], "venue": null, "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2006}, {"title": "There\u2019s plenty of room at the bottom: Incremental combinations of sign-error lms filters", "author": ["L.F.O. Chamon", "C.G. Lopes"], "venue": "2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), May 2014, pp. 7248\u20137252.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 0, "context": "A few situations that may ask for their use are echo cancellation [1], equalization of communication systems [2], acoustics [3], or broadband noise canceling [4].", "startOffset": 66, "endOffset": 69}, {"referenceID": 1, "context": "A few situations that may ask for their use are echo cancellation [1], equalization of communication systems [2], acoustics [3], or broadband noise canceling [4].", "startOffset": 109, "endOffset": 112}, {"referenceID": 2, "context": "A few situations that may ask for their use are echo cancellation [1], equalization of communication systems [2], acoustics [3], or broadband noise canceling [4].", "startOffset": 124, "endOffset": 127}, {"referenceID": 3, "context": "A few situations that may ask for their use are echo cancellation [1], equalization of communication systems [2], acoustics [3], or broadband noise canceling [4].", "startOffset": 158, "endOffset": 161}, {"referenceID": 4, "context": "This complexity may be high even for modern computers, as is the case of the Volterra series [5], [6] and its exponentially increasing complexity.", "startOffset": 93, "endOffset": 96}, {"referenceID": 5, "context": "Examples of these approaches are the filters with truncated diagonals [7], [8], which works with the central coefficients of the model, which are usually the most significant ones.", "startOffset": 70, "endOffset": 73}, {"referenceID": 6, "context": "Examples of these approaches are the filters with truncated diagonals [7], [8], which works with the central coefficients of the model, which are usually the most significant ones.", "startOffset": 75, "endOffset": 78}, {"referenceID": 7, "context": "Another kind of model is based in interpolation techniques [9], [10], in a way that the algorithms work with only a few coefficients while interpolating the others.", "startOffset": 59, "endOffset": 62}, {"referenceID": 8, "context": "Another kind of model is based in interpolation techniques [9], [10], in a way that the algorithms work with only a few coefficients while interpolating the others.", "startOffset": 64, "endOffset": 68}, {"referenceID": 9, "context": "This is the case with cascade structures [11].", "startOffset": 41, "endOffset": 45}, {"referenceID": 10, "context": "An example is ones based in tensor decomposition[12].", "startOffset": 48, "endOffset": 52}, {"referenceID": 11, "context": "Tensors are also a bridge between the multilinear [13] world\u2014of multilinear functions, or functions of many variables that are linear in each one of them\u2014and the linear world.", "startOffset": 50, "endOffset": 54}, {"referenceID": 12, "context": "Not surprisingly, tensors have indeed been used in nonlinear signal processing-related problem for quite some time [14], [15].", "startOffset": 115, "endOffset": 119}, {"referenceID": 13, "context": "Not surprisingly, tensors have indeed been used in nonlinear signal processing-related problem for quite some time [14], [15].", "startOffset": 121, "endOffset": 125}, {"referenceID": 14, "context": "The tensor representation, and specially tensor rank decomposition [16] allows for a dramatic decrease in representational complexity of the model and, should this be exploited in the Volterra series, many low-complexity algorithms may be obtained.", "startOffset": 67, "endOffset": 71}, {"referenceID": 15, "context": "Of particular importance is the rank-one approximation [17].", "startOffset": 55, "endOffset": 59}, {"referenceID": 16, "context": "This concept has seem some success in approximating linear responses [18], [19] with certain structures, and the possible gains to nonlinear signal processing are even more pronounced\u2014exponential reductions in complexity can be achieved [20].", "startOffset": 69, "endOffset": 73}, {"referenceID": 17, "context": "This concept has seem some success in approximating linear responses [18], [19] with certain structures, and the possible gains to nonlinear signal processing are even more pronounced\u2014exponential reductions in complexity can be achieved [20].", "startOffset": 75, "endOffset": 79}, {"referenceID": 18, "context": "This concept has seem some success in approximating linear responses [18], [19] with certain structures, and the possible gains to nonlinear signal processing are even more pronounced\u2014exponential reductions in complexity can be achieved [20].", "startOffset": 237, "endOffset": 241}, {"referenceID": 19, "context": "The other theoretical advantage of rank-one approximations is their well-posedness, which may not be the case for general low-rank decompositions [21].", "startOffset": 146, "endOffset": 150}, {"referenceID": 20, "context": "Notation The notation of the paper follows the one on [22], while introducing new notation when necessary.", "startOffset": 54, "endOffset": 58}, {"referenceID": 21, "context": "the tensor product \u2297 is to be interpreted as the Kronecker product [23].", "startOffset": 67, "endOffset": 71}, {"referenceID": 14, "context": "[16] Just like vectors can alternatively characterized by the concept of a vector space, tensors can be abstractly defined as objects of an algebraic tensor product [13].", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[16] Just like vectors can alternatively characterized by the concept of a vector space, tensors can be abstractly defined as objects of an algebraic tensor product [13].", "startOffset": 165, "endOffset": 169}, {"referenceID": 21, "context": "In computational terms, the tensor product can be implemented as a Kronecker product [23].", "startOffset": 85, "endOffset": 89}, {"referenceID": 19, "context": "This problem has the potential to decrease the number of parameters necessary to represent the tensor, but it is, in general, not well posed and may not have a solution [21].", "startOffset": 169, "endOffset": 173}, {"referenceID": 19, "context": "Proof given by [21] in Proposition 4.", "startOffset": 15, "endOffset": 19}, {"referenceID": 5, "context": "For example, for second order kernels, one possible approach is to truncate some of the diagonals of H2(i, j) [7], [8], which allows the the computation to be written as", "startOffset": 110, "endOffset": 113}, {"referenceID": 6, "context": "For example, for second order kernels, one possible approach is to truncate some of the diagonals of H2(i, j) [7], [8], which allows the the computation to be written as", "startOffset": 115, "endOffset": 118}, {"referenceID": 22, "context": "When D = 1, this uses only the main diagonal, with M parameters, resulting in a model called the Power Filter [24].", "startOffset": 110, "endOffset": 114}, {"referenceID": 10, "context": "This representation has been used, for symmetric tensors, in [12], to reduce the representational complexity of the series.", "startOffset": 61, "endOffset": 65}, {"referenceID": 18, "context": "A kernel like this will be said to satisfy the decomposable model\u2014also called the Simple Multilinear Model (SML) [20].", "startOffset": 113, "endOffset": 117}, {"referenceID": 20, "context": "(28) The other terms from (18) involve only the conjugates of the entries of w, so their Wirtinger derivatives become zero [22].", "startOffset": 123, "endOffset": 127}, {"referenceID": 20, "context": "When doing this to the linear estimation problem, the algorithm obtained is the classical LMS [22].", "startOffset": 94, "endOffset": 98}, {"referenceID": 23, "context": "Stabilization of the Algorithms It has been known that nonlinearities in the update equations may lead to a nonzero probability of divergence for the algorithm, no matter how small is the step-size [25], whenever the probability distribution of the signals has infinite support.", "startOffset": 198, "endOffset": 202}, {"referenceID": 17, "context": "This result is similar to the one obtained in [19].", "startOffset": 46, "endOffset": 50}, {"referenceID": 24, "context": "The classical LMS has a bound given by 0 < \u03bc < 2/(3 trRu) that guarantees convergence in the MSE [26].", "startOffset": 97, "endOffset": 101}, {"referenceID": 20, "context": "[22] For the adaptive algorithms, since yi is not a stationary signal, this is still not an universal bound.", "startOffset": 0, "endOffset": 4}, {"referenceID": 25, "context": "They were put to run against the Volterra-LMS and Wiener-LMS algorithms [27].", "startOffset": 72, "endOffset": 76}, {"referenceID": 22, "context": "The ones tested were: \u2022 The Power Filter (PF) [24] on (11) with D = 1, which uses only the diagonal elements of the Volterra series.", "startOffset": 46, "endOffset": 50}, {"referenceID": 5, "context": "\u2022 The Simplified Volterra Filter (SV) [7], [8], the truncated diagonals model in (11), with D = 3.", "startOffset": 38, "endOffset": 41}, {"referenceID": 6, "context": "\u2022 The Simplified Volterra Filter (SV) [7], [8], the truncated diagonals model in (11), with D = 3.", "startOffset": 43, "endOffset": 46}, {"referenceID": 7, "context": "\u2022 The Sparse Interpolated Volterra (IV) [9], [10], which estimates only a few entries of the Volterra kernel and interpolates the others.", "startOffset": 40, "endOffset": 43}, {"referenceID": 8, "context": "\u2022 The Sparse Interpolated Volterra (IV) [9], [10], which estimates only a few entries of the Volterra kernel and interpolates the others.", "startOffset": 45, "endOffset": 49}, {"referenceID": 8, "context": "The plant to be identified is a smooth one, as in [10].", "startOffset": 50, "endOffset": 54}, {"referenceID": 9, "context": "9b shows a simulation against the Parallel Cascade Filter [11] (CF \u2013 65 coefficients), a model that can be represented as a product of exactly two Volterra filters, and the SML (20 coefficients).", "startOffset": 58, "endOffset": 62}, {"referenceID": 18, "context": "The solution to the estimation problem can be further studied by the used of Newton\u2019s method [20].", "startOffset": 93, "endOffset": 97}, {"referenceID": 26, "context": "[28] The same should apply in the algorithms developed here.", "startOffset": 0, "endOffset": 4}], "year": 2016, "abstractText": "This work proposes a low complexity nonlinearity model and develops adaptive algorithms over it. The model is based on the decomposable\u2014or rank-one, in tensor language\u2014 Volterra kernels. It may also be described as a product of FIR filters, which explains its low-complexity. The rank-one model is also interesting because it comes from a well-posed problem in approximation theory. The paper uses such model in an estimation theory context to develop an exact gradienttype algorithm, from which adaptive algorithms such as the least mean squares (LMS) filter and its data-reuse version\u2014the TRUE-LMS\u2014are derived. Stability and convergence issues are addressed. The algorithms are then tested in simulations, which show its good performance when compared to other nonlinear processing algorithms in the literature.", "creator": "LaTeX with hyperref package"}}}