{"id": "1610.07651", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Oct-2016", "title": "UTD-CRSS Systems for 2016 NIST Speaker Recognition Evaluation", "abstract": "This document briefly describes the systems submitted by the Center for Robust Speech Systems (CRSS) from The University of Texas at Dallas (UTD) to the 2016 National Institute of Standards and Technology (NIST) Speaker Recognition Evaluation (SRE). We developed several UBM and DNN i-Vector based speaker recognition systems with different data sets and feature representations. Given that the emphasis of the NIST SRE 2016 is on language mismatch between training and enrollment/test data, so-called domain mismatch, in our system development we focused on: (1) using unlabeled in-domain data for centralizing data to alleviate the domain mismatch problem, (2) finding the best data set for training LDA/PLDA, (3) using newly proposed dimension reduction technique incorporating unlabeled in-domain data before PLDA training, (4) unsupervised speaker clustering of unlabeled data and using them alone or with previous SREs for PLDA training, (5) score calibration using only unlabeled data and combination of unlabeled and development (Dev) data as separate experiments.", "histories": [["v1", "Mon, 24 Oct 2016 21:05:05 GMT  (19kb)", "http://arxiv.org/abs/1610.07651v1", "5 pages"]], "COMMENTS": "5 pages", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["chunlei zhang", "fahimeh bahmaninezhad", "shivesh ranjan", "chengzhu yu", "navid shokouhi", "john h l hansen"], "accepted": false, "id": "1610.07651"}, "pdf": {"name": "1610.07651.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Chunlei Zhang", "Fahimeh Bahmaninezhad", "Shivesh Ranjan", "Chengzhu Yu"], "emails": ["chunlei.zhang@utdallas.edu", "fahimeh.bahmaninezhad@utdallas.edu", "john.hansen@utdallas.edu"], "sections": [{"heading": null, "text": "ar Xiv: 161 0.07 651v 1 [cs.C L] 24 Oct 201 6This document briefly describes the systems transmitted by the Center for Robust Speech Systems (CRSS) of the University of Texas at Dallas (UTD) to the 2016 National Institute of Standards and Technology (NIST) Speaker Recognition Evaluation (SRE). We developed several UBM- and DNN-i-Vector-based speaker recognition systems with different data sets and feature representations. Given that the NIST SRE 2016 focuses on the linguistic mismatch between training and enrollment / test data, the so-called domain mismatch, our system development focused on: (1) the use of unlabeled domain data to centralize the domain mismatch problem, (2) the determination of the best dataset for training LDA / PLDA / PLDA / PLDA, (3) the use of newly proposed dimension reduction data (PLDA-excluding the domain identification data) prior to PLDA-4 use."}, {"heading": "1. INTRODUCTION", "text": "The main task for the NIST's loudspeaker detection evaluations is to detect speakers, i.e., to determine whether or not a particular target speaker is speaking during a particular language segment. Compared to previous SRE challenges, there are some differences: (1) target speaker data is not distributed in advance as in SRE12, (2) a fixed condition is introduced, (3) greater variability of duration is introduced in the test data, (4) the linguistic mismatch between training (mainly English) and enrollment / test data. All these new features make this SRE very difficult, especially with limited labeled data in the specified condition [1]. This report introduces how CRSS systems address the problem. The entire report is organized as follows: Sec.2 describes several base systems that focus on the overview of the front-end level, including both data sets and functional representations. Sec.3 introduces several core techniques, including a cluster of speakers we have applied, including the RES16."}, {"heading": "2. CRSS BASELINES", "text": "We have developed four base systems in this SRE, all of which are i-Vector-based systems, but with different acoustic models, i.e. UBM or different DNN models [2, 3]. In the backend, we mainly use LDA / SVDA to reduce the dimension of i-vectors and PLDA to calculate probability values. Table 1 summarizes the number of speakers, language segments used for UBM training, the total variability matrix (TV), LDA / SVDA and PLDA models, and the statistics for the dev set provided by NIST for system development."}, {"heading": "2.1. CRSS1: UBM i-Vector", "text": "This system is mainly a modified version of Kaldi (sre10 / v1). 60-dimensional feature vectors for each frame are adopted here, including 20-dimensional MFCC features, appended with \u2206 + \u2206 \u2206. Unspoken portions of the utterances are removed using energy-based voice activity recognition (VAD). 2048 mixtures UBM and total variability (TV) matrix, SRE2004, 2005, 2006, 2008 Telephone data from SRE 2010, switchboard II Phase 2,3 and switchboard Cellular Part1 and Part2 (SWB) and Fisher English are used for the training. Next, 600-dimensional i-vectors are extracted and their dimensions are reduced to 580 with LDA. For the training of LDA / PLDA, only SRE 04-08 and switchboard Cellular Part1 and Part2 (SWB) and Fisher English are used. In addition, 600-dimensional vectors are reduced with LDA and their dimensions."}, {"heading": "2.2. CRSS2: SWB DNN i-Vector", "text": "We developed a DNN i-Vector system based on Kaldi (swbd / s5 & sre10 / v2).The DNN acoustic model is used to calculate the soft alignments for i-Vector extraction.The DNN architecture has 6 fully connected hidden layers with 1024 nodes for each layer.The crossentropy lens function is used to estimate the rear probabilities of 3178 senons.The ASR corpus we used for training the DNN acoustic model is switchboard. The 11-frame context of the 39-dimensional MFCC function is projected into 40-dimensional for each utterance using fMLLR transformation.The reason why we are using the fMLLR function here is that we are using loudspeaker moralization, which we expect to see more detail in the following matrix-long TV training]."}, {"heading": "2.3. CRSS3: UBM i-Vector", "text": "An alternative UBM i-vector system has also been adopted by Kaldi (sre10 / v1). In this system, feature vectors contain 20 MFCCs attached with (\u2206 + \u2206 \u2206) coefficients. Window length and layer size are 25-ms and 10-ms, respectively. In addition, we have performed a ceptral mean normalization with 3-sec sliding windows. Next, non-linguistic frames with energy-based speech activity detection are discarded. However, 2048 mixtures with complete covariance UBM and total variability matrix have been trained using data from SRE2004, 2005, 2006, 2008 and Switchboard II Phase 2,3 and Switchboard Cellular Part1 and Part2. On the back, after extracting i-vectors, the global mean is calculated from minor and significant unlabeled data."}, {"heading": "2.4. CRSS4: Fisher English DNN i-Vector", "text": "The last baseline is a DNN i-Vector system with Kaldi (sre10 / v2) based on the multi-splice time delay DNN (TDNN) [5]. TDNN is trained with only a small portion of Fisher English data (1239 expressions). The feature vectors contain 40 dimensional f-bank features. TDNN has six layers; the hidden layers have an input dimension of 350 and an output dimension of 3500. The Softmax output layer calculates posteriors for 3859 triphon states. Further details on the TDNN structure and training sequence can be found in [5]. After the TDNN training, 20 MFCCs with attached coefficients (total 60 MFCCs) are used for training TV matrix. Next, 600-dim i-vectors are extracted. After the i-Vector extraction, we apply strategies similar to PLDA (briefly described in the LDA / CRDA section above)."}, {"heading": "3. CORE COMPONENTS IN SYSTEM DEVELOPMENT", "text": "In the solid state of SRE 2016, we have an enormous amount of out-of-domain data, i.e. previous SREs, SWB, Fisher English, etc. Only a small amount of in-domain data is available (without speaker labels), making it very difficult to work with these so-called domain mismatches. In SRE 2016, NIST provided blank training data containing two subsets, i.e. blank minor and blank major. The blank minor data set contains 200 expressions, while the major data set contains 2272 expressions. The Minor data set has two languages for system development purposes, while the major data set contains two different languages corresponding to the final evaluation. To address this issue, several techniques are proposed in this evaluation."}, {"heading": "3.1. Speaker clustering of unlabeled data", "text": "There are several stages in which we can use the unlabeled data, such as LDA / PLDA training and calibration. First, it is very intuitive to perform a speaker clustering of the unlabeled data and then generate an \"estimated\" speaker label for each utterance, similar to the method we used in the 2015 NIST LRE iVector Challenge [6]. With these labels, we integrate the indomain information from unlabeled data to train LDA and PLDA. In fact, this simple operation in the experiment improved the LDA / PLDA base performance for the development set.In practice, we train gender identification using previous SRE data before performing a speaker clustering, and then apply a simple K-mean algorithm to gender-dependent subsets, and finally, we bundle these two subsets together. In the experiment, we found that this can provide more precise clustering and benefit for the following PLDA training and calibration."}, {"heading": "3.2. Discriminant analysis via support vectors (SVDA)", "text": "Support Vector Discrimination Analysis (SVDA) is a variant of SVDA that uses only support vectors to calculate between and within class covariance matrices. In contrast to LDA, SVDA covers class boundaries and performs well on small sample size problems (i.e., when the dimensionality is greater than the sample size).The idea of using support vectors with discriminant analysis has already been introduced in [7], which have brought about significant improvements over LDA. Additionally, the effectiveness of SVDA in iVector / PLDA speaker recognition for NIST SRE2010 may have been previously introduced in [8] for both long and short duration of test statements. Specifically, LDA defines class separation strategies toward A as, \u03bb = ATSbAATSwA, (1) where Sb and Sw are properties between class and within class covariance matrices."}, {"heading": "3.3. Unlabeled data PLDA", "text": "To fully explore the information from the unlabeled data in the domain, we conducted an interesting experiment using only the unlabeled data in the domain to train PLDA (SRE04-08, however, is used to train LDA) using the \"estimated\" labels from the Speaker Clustering. Surprisingly, PLDA achieved 20.5% EER for 200 i-vectors in the secondary language in the Dev experiment (using i-vectors for CRSS1 baseline), which is not so bad. However, if we add more data (i.e. 75 + 300 estimated speakers of 2472 i-vectors in secondary and main languages) for PLDA training, the performance deteriorates from 20.5% to 26.3%. This observation suggests that extra-domain language data is not helpful to train a discriminatory classifier, as from the point of view of dev-enrollment test data is still outside the language data / domain."}, {"heading": "3.4. Calibration and fusion", "text": "The CRSS calibration and fusion system is mainly based on the BOSARIS toolkit [9]. The PAV algorithm is used to create a calibration transformation matrix. We used two sets of data for calibration. The first consists of Dev data, we use all the Dev test information provided by NIST for system development to train the calibration system. As in this SRE Dev and the evaluation set have completely different languages, there is no guarantee that the calibration for the final evaluation set will work well. To this end, we have created a new test list to calibrate evaluation results, and we used unlabeled data with estimated loudspeaker labels. We believe that the distribution of the unlabeled data will be closer to the evaluation. After calibration, we have merged our subsystems for final transmission. For the logistic fusion, we used a simple merger system."}, {"heading": "4. CRSS SUB-SYSTEMS", "text": "We developed 7 subsystems out of 4 CRSS baselines that used SREs to train SVDA, LDA and PLDA. In addition, as described above, we developed 4 subsystems with only unlabeled data PLDA idea. The details of each system about the data and techniques they use are listed in Table 2 and Table 3. Specifically, subsystems 8, 9, 10, 11 have the same configuration as subsystems 3, 4, 6, 2; however, only unlabeled data is used to train PLDA."}, {"heading": "5. CRSS SUBMISSIONS", "text": "In the final submission, we tried different system combinations as well as different calibration strategies. As the primary submission, we submitted a merger of 1-7 subsystems with Dev + unlabeled data for calibration. To test our hypothesis that the unlabeled PLDA idea will be beneficial for the evaluation set, we submitted this as a contrasting submission, all of which make our final submissions to SRE 2016."}, {"heading": "6. PERFORMANCE OF CRSS SUB-SYSTEMS ON SRE 2016 DEV DATA", "text": "Tables 5, 6 and 7 show the same error rate (EER), minimum Cprimary (min-Cprimary) and actual Cprimary (act-Cprimary) costs for individual systems and fusion systems using NIST scoring software. These results are evaluated on Dev set."}, {"heading": "7. COMPUTATIONAL RESOURCES", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "7.1. CPU cluster", "text": "The speaker recognition system is implemented on our in-house high-performance Dell computing cluster, which runs the Rocks 6.0 (Mamba) Linux distribution. It consists of eight 6C Intel Xeon 2.67 GHz CPUs, four 10C Intel Xeon 2.40 GHz CPUs and 18 quad-core Intel Xeon 2.33 GHz CPUs, making a total of 408 processors. All of the internal RAM on the cluster exceeds 1 TB. All of our data, including audio files, features, statistics, etc. is stored in a directly connected 30TB of Dell PowerVault MD1000 memory."}, {"heading": "7.2. GPU machines", "text": "For DNN training on SWB data, a GeForce GTX TITAN Black graphics card with 6144 MB RAM is used. For DNN training on Fisher English, we used a 12 GB Tesla K40."}, {"heading": "7.3. CPU execution time", "text": "We tested the scoring process of the systems with a 2.67 GHz CPU and 32 GB RAM. We selected a 3-minute utterance (exact duration of 181.45 seconds) and calculated the time required for functional extraction (20-dimensional MFCC), speech activity detection (Kaldi SRE10 / v1 default), extraction of zero and first order statistics, and 600-dimensional i-Vector. The time required for this process chain is 37.58, which is calculated by averaging the elapsed time from three independent passes. Scoring an utterance with our PLDA model takes an average of less than 0.1 seconds. The training of the models depends on how many expressions are scheduled for training. As the BM- and TV-matrices are trained offline, the speaker login only needs to extract the corresponding i-vectors, which provides the time required for multiple speakers."}, {"heading": "8. ACKNOWLEDGEMENT", "text": "We would like to thank Dr. Kong Aik Lee for providing the blank test list, organizing I4U meetings and other members of the I4U group for sharing many ideas and insights during the I4U meetings. We would like to thank Qian Zhang, Abhinav Misra, Dr. Finnian Kelly and other CRSS colleagues for their many insights and helpful discussions in developing the systems. We would like to thank Dr. Gang Liu (CRSS graduate) from Alibaba Group for providing the fusion system."}, {"heading": "9. REFERENCES", "text": "[1] \"NIST 2016 Speaker recognition evaluation plan,\" https: / / www.nist.gov / sites / sleeper / default / files / documents / itl / iad / mig / SRE16 Eval Plan V1-0.pdf, 2016. [2] Y. Lei, N. Scheffer, L. Ferrer, and M. McLaren, \"A new scheme for speaker recognition using a phonetically-aware deep neural network,\" in 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2014, pp. IEEE-1695- 1699. [3] S. O. Sadjadi, S. Ganapathy, and J. Pelecanos \"The ibm 2016 speaker recognition system,\" arXiv pre print arXiv: 1602.07291, 2016, H.I. Ygorithor, S. O."}], "references": [{"title": "A novel scheme for speaker recognition using a phonetically-aware deep neural network", "author": ["Y. Lei", "N. Scheffer", "L. Ferrer", "M. McLaren"], "venue": "2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2014, pp. 1695\u2013 1699.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2014}, {"title": "The ibm 2016 speaker recognition system", "author": ["S.O. Sadjadi", "S. Ganapathy", "J. Pelecanos"], "venue": "arXiv preprint arXiv:1602.07291, 2016.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2016}, {"title": "Msr identity toolbox v1. 0: A matlab toolbox for speaker-recognition research", "author": ["S.O. Sadjadi", "M. Slaney", "L. Heck"], "venue": "Speech and Language Processing Technical Committee Newsletter, vol. 1, no. 4, 2013.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2013}, {"title": "Time delay deep neural network-based universal background models for speaker recognition", "author": ["D. Snyder", "D. Garcia-Romero", "D. Povey"], "venue": "2015 IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU). IEEE, 2015, pp. 92\u2013 97.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2015}, {"title": "Utd-crss system for the nist 2015 language recognition i-vector machine learning challenge", "author": ["C. Yu", "C. Zhang", "S. Ranjan", "Q. Zhang", "A. Misra", "F. Kelly", "J.H.L. Hansen"], "venue": "2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2016, pp. 5835\u20135839.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2016}, {"title": "Discriminant analysis via support vectors", "author": ["S. Gu", "Y. Tan", "X. He"], "venue": "Neurocomputing, vol. 73, no. 10, pp. 1669\u20131675, 2010.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2010}, {"title": "i-vector/PLDA speaker recognition using support vectors with discriminant analysis", "author": ["F. Bahmaninezhad", "J.H.L. Hanesn"], "venue": "(submitted to) IEEE ICASSP, 2017.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2017}, {"title": "The bosaris toolkit: Theory, algorithms and code for surviving the new dcf", "author": ["N. Br\u00fcmmer", "E. de Villiers"], "venue": "arXiv preprint arXiv:1304.2865, 2013.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2013}], "referenceMentions": [{"referenceID": 0, "context": ", UBM or different DNN models [2, 3].", "startOffset": 30, "endOffset": 36}, {"referenceID": 1, "context": ", UBM or different DNN models [2, 3].", "startOffset": 30, "endOffset": 36}, {"referenceID": 1, "context": "The reason we apply fMLLR feature here is that, by speaker normalization, we expect to acquire more accurate phonetic alignment in the following TV matrix training, see more details in [3].", "startOffset": 185, "endOffset": 188}, {"referenceID": 2, "context": "For back-end, mostly MSR [4] toolkit has been adopted.", "startOffset": 25, "endOffset": 28}, {"referenceID": 3, "context": "The last baseline is a DNN i-Vector system using Kaldi (sre10/v2) that is based on the multisplice time delay DNN (TDNN) [5].", "startOffset": 121, "endOffset": 124}, {"referenceID": 3, "context": "More details on the TDNN structure and training procedure are provided in [5].", "startOffset": 74, "endOffset": 77}, {"referenceID": 4, "context": "First, it is very intuitive to do a speaker clustering of the unlabeled data, and then generate an \u201cestimated\u201d speaker label for each utterance, similar with the method that we used in 2015 NIST LRE iVector challenge[6].", "startOffset": 216, "endOffset": 219}, {"referenceID": 5, "context": "The idea of using support vectors with discriminant analysis has been previously introduced in [7] which made significant improvement over LDA.", "startOffset": 95, "endOffset": 98}, {"referenceID": 6, "context": "In addition, the effectiveness of SVDA in iVector/PLDA speaker recognition for NIST SRE2010 is studied in [8] previously for both long and short duration test utterances.", "startOffset": 106, "endOffset": 109}, {"referenceID": 6, "context": "More details on the advantages and properties of SVDA are provided in [8].", "startOffset": 70, "endOffset": 73}, {"referenceID": 7, "context": "The CRSS calibration and fusion system is mainly based on the BOSARIS toolkit [9].", "startOffset": 78, "endOffset": 81}], "year": 2016, "abstractText": "This document briefly describes the systems submitted by the Center for Robust Speech Systems (CRSS) from The University of Texas at Dallas (UTD) to the 2016 National Institute of Standards and Technology (NIST) Speaker Recognition Evaluation (SRE). We developed several UBM and DNN i-Vector based speaker recognition systems with different data sets and feature representations. Given that the emphasis of the NIST SRE 2016 is on language mismatch between training and enrollment/test data, so-called domain mismatch, in our system development we focused on: (1) using unlabeled indomain data for centralizing data to alleviate the domain mismatch problem, (2) finding the best data set for training LDA/PLDA, (3) using newly proposed dimension reduction technique incorporating unlabeled in-domain data before PLDA training, (4) unsupervised speaker clustering of unlabeled data and using them alone or with previous SREs for PLDA training, (5) score calibration using only unlabeled data and combination of unlabeled and development (Dev) data as separate experiments.", "creator": "LaTeX with hyperref package"}}}