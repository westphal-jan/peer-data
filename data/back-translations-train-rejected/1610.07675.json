{"id": "1610.07675", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Oct-2016", "title": "Surprisal-Driven Zoneout", "abstract": "We propose a novel method of regularization for recurrent neural networks called suprisal-driven zoneout. In this method, states \\textit{zoneout} (maintain their previous value rather than updating), when the \\textit{suprisal} (discrepancy between the last state's prediction and target) is small. Thus regularization is adaptive and input-driven on a per-neuron basis. We demonstrate the effectiveness of this idea by achieving state-of-the-art bits per character of 1.32 on the Hutter Prize Wikipedia dataset, significantly reducing the gap to the best known highly-engineered compression methods.", "histories": [["v1", "Mon, 24 Oct 2016 22:38:52 GMT  (1234kb,D)", "http://arxiv.org/abs/1610.07675v1", "Submitted to Continual Learning and Deep Networks Workshop NIPS 2016"], ["v2", "Fri, 28 Oct 2016 19:55:16 GMT  (1234kb,D)", "http://arxiv.org/abs/1610.07675v2", "Submitted to Continual Learning and Deep Networks Workshop NIPS 2016"], ["v3", "Mon, 31 Oct 2016 15:18:11 GMT  (1234kb,D)", "http://arxiv.org/abs/1610.07675v3", "Submitted to Continual Learning and Deep Networks Workshop NIPS 2016"], ["v4", "Thu, 3 Nov 2016 17:09:23 GMT  (1234kb,D)", "http://arxiv.org/abs/1610.07675v4", "Submitted to Continual Learning and Deep Networks Workshop NIPS 2016"], ["v5", "Thu, 24 Nov 2016 06:40:26 GMT  (1242kb,D)", "http://arxiv.org/abs/1610.07675v5", "To be published at the Continual Learning and Deep Networks Workshop NIPS 2016"], ["v6", "Tue, 13 Dec 2016 23:32:24 GMT  (1242kb,D)", "http://arxiv.org/abs/1610.07675v6", "Published at the Continual Learning and Deep Networks Workshop; NIPS 2016"]], "COMMENTS": "Submitted to Continual Learning and Deep Networks Workshop NIPS 2016", "reviews": [], "SUBJECTS": "cs.LG cs.AI cs.NE", "authors": ["kamil rocki", "tomasz kornuta", "tegan maharaj"], "accepted": false, "id": "1610.07675"}, "pdf": {"name": "1610.07675.pdf", "metadata": {"source": "META", "title": "Suprisal-Driven Zoneout", "authors": ["Suprisal-Driven Zoneout", "Kamil Rocki", "Tomasz Kornuta"], "emails": ["KMROCKI@US.IBM.COM", "TKORNUT@US.IBM.COM"], "sections": [{"heading": "1. Introduction", "text": "This year is the highest in the history of the country."}, {"heading": "2. The model", "text": "We used the surprise feedback matrix LSTM (Rocki, 2016b) \u0445, which tracks past predictions p and compares them with current observations x to calculate surprises: st = \u2212 \u2211 i log pt \u2212 1 xt (2,1) Next, we calculate the gate activations: ft = \u03c3 (Wf \u00b7 xt + Uf \u00b7 ht \u2212 1 + Vf \u00b7 st + bf) (2,2) it = \u03c3 (Wi \u00b7 xt + Ui \u00b7 ht \u2212 1 + Vi \u00b7 st + bi) (2,3) ot = \u03c3 (Where \u00b7 xt + Uo \u00b7 ht \u2212 1 + Vo \u00b7 st + bo) (2,4) ut = tanh (Wu \u00b7 xt + Uu \u00b7 ht \u2212 1 + Vu \u00b7 st + bu) (2,5) The zoneout rate is adaptive; it is a function of st, \u0442 is a threshold value added for numerical stability (Wy is an h \u2192 y connection matrix: zt = max."}, {"heading": "3. Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1. Datasets", "text": "Hutter Prize Wikipedia Hutter Prize Wikipedia (also known as enwik8) dataset (Hutter, 2012) This corpus contains 205 symbols, including XML markups and special characters. Linux This dataset contains about 603MB raw Linux 4.7 kernel source code \u2020"}, {"heading": "3.2. Methodology", "text": "In both cases, the first 90% of each corpus was used for training, the next 5% for validation, and the last 5% for reporting test accuracy. In each iteration sequence with a length of 10,000 was randomly selected. The learning algorithm used was Adadelta with a learning rate of 0.001. Weights were initialized using the so-called Xavier initialization (Glorot and Bengio, 2010). Sequence length for BPTT was 100 and batch size 128. In all experiments, only a layer of 4,000 LSTM cells was used. States were transmitted over the entire sequence of 10,000 emulations of the complete BPTT. Forget bias was initially set to 1. Other parameters were set to zero. The algorithm was written in C + + and CUDA 8 and ran for up to 2 weeks on the GTX Titan GPU."}, {"heading": "3.3. Results and discussion", "text": "We observed significant improvements in enwik8 (Table 3.1) and Linux (Table 3.2) datasets. Our hypothesis is that this is due to the presence of memorable tags and multinestedness. Patterns such as < timestamps > or long periods of space can be represented by a single impulse in this approach and split completely until the end of the pattern. Without adaptive zoneout, this would have to be completely controlled by gates. Figure 3.2 shows that the dynamic span of memory cells is greater when adaptive zoneout is used. In addition, we show that activations using adaptive zoneout are indeed more economical than without what supports our intuition about the inner workings of the network. < A particularly interesting observation is the fact that adaptive zoneout structures are used."}, {"heading": "4. Summary", "text": "The proposed surprisingly controlled zoneout solution seemed to be a flexible mechanism for controlling the activation of a particular cell. Empirically, this method performs extremely well with enwik8 and Linux datasets and represents by far a new state of the art."}, {"heading": "5. Further work", "text": "We would like to study variations of a suprisal zoneout solution on both the state and the cell, and the effects of suprisal feedback on the gating mechanisms of the LSTM in more detail. Another interesting direction to follow is the connection with a sparse coding - a suprisal zoneout solution only reveals the cell contents of the LSTM more sparsely over time, potentially leading to a more effective use of the information."}, {"heading": "Acknowledgements", "text": "This work was supported in part by the Defense Advanced Research Projects Agency (DARPA)."}], "references": [{"title": "Possible principles underlying the transformations of sensory messages", "author": ["H.B. Barlow"], "venue": null, "citeRegEx": "Barlow.,? \\Q1961\\E", "shortCiteRegEx": "Barlow.", "year": 1961}, {"title": "Hierarchical multiscale recurrent neural networks", "author": ["J. Chung", "S. Ahn", "Y. Bengio"], "venue": "arXiv preprint arXiv:1609.01704,", "citeRegEx": "Chung et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Chung et al\\.", "year": 2016}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["X. Glorot", "Y. Bengio"], "venue": "Proceedings of the International Conference on Artificial Intelligence and Statistics (AISTATS???10). Society for Artificial Intelligence and Statistics,", "citeRegEx": "Glorot and Bengio.,? \\Q2010\\E", "shortCiteRegEx": "Glorot and Bengio.", "year": 2010}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural Comput.,", "citeRegEx": "Hochreiter and Schmidhuber.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Grid long short-term memory", "author": ["N. Kalchbrenner", "I. Danihelka", "A. Graves"], "venue": "CoRR, abs/1507.01526,", "citeRegEx": "Kalchbrenner et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2015}, {"title": "Zoneout: Regularizing rnns by randomly preserving hidden activations", "author": ["D. Krueger", "T. Maharaj", "J. Kram\u00e1r", "M. Pezeshki", "N. Ballas", "N.R. Ke", "A. Goyal", "Y. Bengio", "H. Larochelle", "A.C. Courville", "C. Pal"], "venue": "CoRR, abs/1606.01305,", "citeRegEx": "Krueger et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Krueger et al\\.", "year": 2016}, {"title": "Recurrent memory array structures", "author": ["K. Rocki"], "venue": "arXiv preprint arXiv:1607.03085,", "citeRegEx": "Rocki.,? \\Q2016\\E", "shortCiteRegEx": "Rocki.", "year": 2016}, {"title": "Surprisal-driven feedback in recurrent networks", "author": ["K.M. Rocki"], "venue": "arXiv preprint arXiv:1608.06027,", "citeRegEx": "Rocki.,? \\Q2016\\E", "shortCiteRegEx": "Rocki.", "year": 2016}, {"title": "A formal theory of inductive inference", "author": ["R.J. Solomonoff"], "venue": "i. Information and control,", "citeRegEx": "Solomonoff.,? \\Q1964\\E", "shortCiteRegEx": "Solomonoff.", "year": 1964}, {"title": "On multiplicative integration with recurrent neural networks. CoRR, abs/1606.06630, 2016", "author": ["Y. Wu", "S. Zhang", "Y. Zhang", "Y. Bengio", "R. Salakhutdinov"], "venue": "URL http: //arxiv.org/abs/1606.06630", "citeRegEx": "Wu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Wu et al\\.", "year": 2016}, {"title": "Recurrent highway networks, 2016", "author": ["J.G. Zilly", "R.K. Srivastava", "J. Koutn?k", "J. Schmidhuber"], "venue": null, "citeRegEx": "Zilly et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Zilly et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 3, "context": "The Long-Short Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997) architecture, a type of RNN, has proven to be exceptionally well suited for learning longterm dependencies, and is very widely used to model sequence data.", "startOffset": 34, "endOffset": 68}, {"referenceID": 8, "context": "It has been proven (Solomonoff, 1964) that the most genN o r t h e r n I r e l a n d ] ] = = E x t e r n a l l i n k s = = * Input Input clock Surprisal Operation probability N or th ern Ireland]] = = xternal links == * Memory E", "startOffset": 19, "endOffset": 37}, {"referenceID": 0, "context": "According to Redundancy-Reducing Hypothesis (Barlow, 1961) neurons within the brain can code messages using different number of impulses.", "startOffset": 44, "endOffset": 58}, {"referenceID": 5, "context": "One of the successful, recently proposed solutions, is zoneout (Krueger et al., 2016; Rocki, 2016a).", "startOffset": 63, "endOffset": 99}, {"referenceID": 2, "context": "Weights were initialized using the so-called Xavier initialization (Glorot and Bengio, 2010).", "startOffset": 67, "endOffset": 92}, {"referenceID": 4, "context": "58 Grid LSTM (Kalchbrenner et al., 2015) 1.", "startOffset": 13, "endOffset": 40}, {"referenceID": 9, "context": "45 MI-LSTM (Wu et al., 2016) 1.", "startOffset": 11, "endOffset": 28}, {"referenceID": 10, "context": "44 RHN (Zilly et al., 2016) 1.", "startOffset": 7, "endOffset": 27}, {"referenceID": 1, "context": "40 HM-LSTM (Chung et al., 2016) 1.", "startOffset": 11, "endOffset": 31}, {"referenceID": 1, "context": "A similar approach to the same problem is called Hierarchical Multiscale Recurrent Neural Networks (Chung et al., 2016).", "startOffset": 99, "endOffset": 119}], "year": 2017, "abstractText": "We propose a novel method of regularization for recurrent neural networks called suprisal-driven zoneout. In this method, states zoneout (maintain their previous value rather than updating), when the suprisal (discrepancy between the last state\u2019s prediction and target) is small. Thus regularization is adaptive and input-driven on a per-neuron basis. We demonstrate the effectiveness of this idea by achieving state-of-the-art bits per character of 1.32 on the Hutter Prize Wikipedia dataset, significantly reducing the gap to the best known highly-engineered compression methods.", "creator": "LaTeX with hyperref package"}}}