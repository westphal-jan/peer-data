{"id": "1610.07686", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Oct-2016", "title": "Co-Occuring Directions Sketching for Approximate Matrix Multiply", "abstract": "We introduce co-occurring directions sketching, a deterministic algorithm for approximate matrix product (AMM), in the streaming model. We show that co-occuring directions achieves a better error bound for AMM than other randomized and deterministic approaches for AMM. Co-occurring directions gives a $1 + \\epsilon$ -approximation of the optimal low rank approximation of a matrix product. Empirically our algorithm outperforms competing methods for AMM, for a small sketch size. We validate empirically our theoretical findings and algorithms", "histories": [["v1", "Tue, 25 Oct 2016 00:01:33 GMT  (429kb)", "http://arxiv.org/abs/1610.07686v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["youssef mroueh", "etienne marcheret", "vaibhava goel"], "accepted": false, "id": "1610.07686"}, "pdf": {"name": "1610.07686.pdf", "metadata": {"source": "CRF", "title": "Co-Occuring Directions Sketching for Approximate Matrix Multiply", "authors": ["Youssef Mroueh", "Etienne Marcheret", "Vaibhava Goel"], "emails": [], "sections": [{"heading": null, "text": "ar Xiv: 161 0.07 686v 1 [cs.L G] 25 Oct 201 6We introduce in the flow model the outlining of simultaneous directions, a deterministic algorithm for approximate matrix products (AMM). We show that simultaneous directions achieve a better margin of error for AMM than other randomized and deterministic approaches for AMM. Simultaneous directions yield a (1 + \u03b5) approximation to the optimal approximation of a low-order matrix product. Empirically, our algorithm outperforms competing methods for AMM at a small sketch size. We empirically validate our theoretical results and algorithms."}, {"heading": "1 Introduction", "text": "The enormous and continuously growing amount of multimodal data presents some challenges in terms of collecting and handling this data. (...) Multimodal datasets are often considered multimodal large matrices that describe the same content with different modalities. (...) The product of large multimodal matrices is of practical interest as it exploits the correlation between different modalities. (...) Methods such as Partial Least Squares (PLS), which truncated singular value decomposition of a matrix product.The data streaming paradigm takes a single pass over the data and a small memory footprint, resulting in a space / accuracy tradeoff. (...) Multimodal data can occupy a large amount of memory or be generated sequentially."}, {"heading": "2 Sketching from Covariance to Correlation", "text": "In this section we check the covariance of the sketch."}, {"heading": "2.2.1 Main Results", "text": "In the following we give our main results about the approximation error of the simultaneous direction in AMM (theorem 2) and in the k \u2212 th marginal approximation of a matrix product (theorem 3). Proofs are provided in section 3.Theorem 2 (AMM). Outputting the simultaneous statements (algorithm 2) gives a correlation diagram (BX, BY) of (X, Y), for \u2264 min (mx, my) time and requires a distance of O (mx + my +). Theorem 3 (Low Rank Product Approximation) Lasse (BX, BY) the output of algorithm 2 in O (mx + my +) time and requires a space of O (mx + my +)."}, {"heading": "2.2.2 Discussion of Main Results", "text": "It is easy to see that the looming statements for a product of the spectral standards of X indicate a Y as in Equation (2). Leave sr (X) = X-X-X-X-X as in Equation (2). Leave sr (X) = X-X-X-X-X as in Equation (2) the stable rank of X. It is easy to see that the common statements for the product of X (X) sr (X) sr (Y) have an error rate of X."}, {"heading": "2.2.3 Running Time Analysis and Parralelization.", "text": "We compare the space and runtime of our sketch with a na\u00efve implementation of the correlation sketch. 1) Naive correlation sketch: In the if statement of algorithm 2, we calculate the thin svd SVD (BXB Y) = [U, \u03a3, V], BX / 2, which is O (nmxmy), so we do not need a gain in terms of raw force. 2) Co-occuring Directions: Algorithm 2 avoids the calculation of BXB Y by using the QR decomposition of BX and BY. The space required is O (mxmy), so no gain in terms of raw force. 2) Cetting: Algorithm 2 avoids the calculation of BXB Y by using the QR decomposition of BX and BY."}, {"heading": "3 Proofs", "text": "In this section we give evidence for our main results: Proof 1 (Proof of Theorem 2) By construction we have: CxC's (QxU) (QyV's) (QyV's) (Qx's) Q's = Qx (RxR's) Q's (QxRx's) (QyRy's) Therefore, the algorithm calculates a form of R-SVD of BXB'Y followed by a shrinkage of the correlation matrix. Let Bix, B's y s, C i y's (BX's, C's, BY's, Cy's, Cy's after executing the main."}, {"heading": "4 Previous Work on Approximate Matrix Multilply", "text": "We list here a catalog of baselines for AMM: Brute Force. We keep a running correlation C \u2190 C + XiY'i. We perform a thin svd at the end of the stream. Space O (mxmy), runtime: O (nmxmy) + O (mxmy), the cost of the sketch update and the thin svd sampling [DKM06]. We define a distribution over [n], pi = Xi Yi S, where S = ni = 1 Xi Yi. Form BX and BY by taking iid samples (slit indexes) using pi s. In the streaming model, since S is not known, we use independent column patterns. Therefore, the space required is O (mx + my), the runtime is O (mx)."}, {"heading": "5 Experiments", "text": "We are committed to the fact that we will be able in the second half of the year in the second half of the year in the second half of the year in the third half of the year in the second half of the year in the third half of the year in the third half of the year in the second half of the year in the third half of the year in the third half of the year in the third half of the year in the third half of the year in the third half of the year in the third half of the year in the third half of the year in the third half of the third half of the year in the third half of the third half of the year in the second half of the third half of the year in the second half of the third half of the year in the third half of the third half of the year in the third half of the third half of the year."}, {"heading": "6 Conclusion", "text": "In this paper, we introduced a deterministic sketching algorithm for AMM, which we called co-occurring directions. We showed its error limits (in spectral standard) for AMM and the low approximation of a product. We showed empirically that co-occurring directions exceed deterministic and randomized baselines in the streaming model. In fact, co-occurring direction exhibits the best error / space compromise among known baselines with errors specified in the spectral standard in the streaming model. Two questions remain open: First, whether guarantees of theorem 2 can be improved similar to the improved guarantees for frequent directions given [GLPW15]. This would establish an explicit link between the sketch length, the low structure of the matrix product XY and / or the low structure of the individual matrices. Second, whether the robustness of co-occurring directions can be improved by using shrinkage operators such as P14."}, {"heading": "A Low Rank product Approximation", "text": "B MS-Coco Timing Experiments"}], "references": [{"title": "Approximate matrix multiplication with application to linear embeddings", "author": ["Michail Vlachos Anastasios T. Kyrillidis", "Anastasios Zouzias"], "venue": "Corr,", "citeRegEx": "ATKZ14", "shortCiteRegEx": null, "year": 2014}, {"title": "Optimal approximate matrix product in terms of stable rank", "author": ["Michael B. Cohen", "Jelani Nelson", "David P. Woodruff"], "venue": "CoRR,", "citeRegEx": "CNW15", "shortCiteRegEx": null, "year": 2015}, {"title": "Low rank approximation and regression in input sparsity time", "author": ["Kenneth L. Clarkson", "David P. Woodruff"], "venue": "STOC,", "citeRegEx": "CW13", "shortCiteRegEx": null, "year": 2013}, {"title": "Co-clustering documents and words using bipartite spectral graph partitioning", "author": ["Inderjit S. Dhillon"], "venue": "KDD,", "citeRegEx": "Dhi01", "shortCiteRegEx": null, "year": 2001}, {"title": "Fast monte carlo algorithms for matrices i: Approximating matrix multiplication", "author": ["Petros Drineas", "Ravi Kannan", "Michael W. Mahoney"], "venue": "SIAM J. Comput.,", "citeRegEx": "DKM06", "shortCiteRegEx": null, "year": 2006}, {"title": "Phillips", "author": ["Mina Ghashami", "Amey Desai", "Jeff M"], "venue": "Improved Practical Matrix Sketching with Guarantees.", "citeRegEx": "GDP14", "shortCiteRegEx": null, "year": 2014}, {"title": "Frequent directions : Simple and deterministic matrix sketching", "author": ["Mina Ghashami", "Edo Liberty", "Jeff M. Phillips", "David P. Woodruff"], "venue": "CoRR,", "citeRegEx": "GLPW15", "shortCiteRegEx": null, "year": 2015}, {"title": "Relations between two sets of variates", "author": ["Harold Hotteling"], "venue": "Biometrika,", "citeRegEx": "Hot36", "shortCiteRegEx": null, "year": 1936}, {"title": "Deep residual learning for image recognition", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"], "venue": "CVPR,", "citeRegEx": "HZRS16", "shortCiteRegEx": null, "year": 2016}, {"title": "In KDD", "author": ["Edo Liberty. Simple", "deterministic matrix sketching"], "venue": "ACM,", "citeRegEx": "Lib13", "shortCiteRegEx": null, "year": 2013}, {"title": "Finding repeated elements", "author": ["J. Misra", "David Gries"], "venue": "Science of Computer Programming,", "citeRegEx": "MG82", "shortCiteRegEx": null, "year": 1982}, {"title": "Multimodal retrieval with asymmetrically weighted CCA and hierarchical kernel sentence embedding", "author": ["Youssef Mroueh", "Etienne Marcheret", "Vaibhava Goel"], "venue": "ArXiv,", "citeRegEx": "MMG16", "shortCiteRegEx": null, "year": 2016}, {"title": "Low rank matrix-valued chernoff bounds and approximate matrix multiplication", "author": ["Avner Magen", "Anastasios Zouzias"], "venue": "SODA,", "citeRegEx": "MZ11", "shortCiteRegEx": null, "year": 2011}, {"title": "Improved approximation algorithms for large matrices via random projections", "author": ["Tamas Sarlos"], "venue": null, "citeRegEx": "Sarlos.,? \\Q2006\\E", "shortCiteRegEx": "Sarlos.", "year": 2006}, {"title": "A survey of partial least squares (pls) methods", "author": ["Jacob A. Wegelin"], "venue": "with emphasis on the two-block case. Technical report,", "citeRegEx": "Weg00", "shortCiteRegEx": null, "year": 2000}, {"title": "Comput", "author": ["David P. Woodruff. Sketching as a tool for numerical linear algebra. Found. Trends Theor"], "venue": "Sci.,", "citeRegEx": "Woo14", "shortCiteRegEx": null, "year": 2014}, {"title": "Frequent direction algorithms for approximate matrix multiplication with applications in CCA", "author": ["Qiaomin Ye", "Luo Luo", "Zhihua Zhang"], "venue": "IJCAI,", "citeRegEx": "YLZ16", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 14, "context": "Methods such as Partial Least Squares (PLS) [Weg00], Canonical Correlation Analysis (CCA)[Hot36], Spectral Co-Clustering [Dhi01], exploit the low rank structure of the correlation matrix to mine the hidden joint factors, by computing the truncated singular value decomposition of a matrix product.", "startOffset": 44, "endOffset": 51}, {"referenceID": 7, "context": "Methods such as Partial Least Squares (PLS) [Weg00], Canonical Correlation Analysis (CCA)[Hot36], Spectral Co-Clustering [Dhi01], exploit the low rank structure of the correlation matrix to mine the hidden joint factors, by computing the truncated singular value decomposition of a matrix product.", "startOffset": 89, "endOffset": 96}, {"referenceID": 3, "context": "Methods such as Partial Least Squares (PLS) [Weg00], Canonical Correlation Analysis (CCA)[Hot36], Spectral Co-Clustering [Dhi01], exploit the low rank structure of the correlation matrix to mine the hidden joint factors, by computing the truncated singular value decomposition of a matrix product.", "startOffset": 121, "endOffset": 128}, {"referenceID": 15, "context": "Approximate Matrix Multiplication (AMM), is gaining an increasing interest in streaming applications (See the recent monograph [Woo14] for more details ).", "startOffset": 127, "endOffset": 134}, {"referenceID": 4, "context": "Randomized approaches for AMM were pioneered by the work of [DKM06].", "startOffset": 60, "endOffset": 67}, {"referenceID": 4, "context": "The approach of [DKM06] is based on the sampling of l columns of X and Y .", "startOffset": 16, "endOffset": 23}, {"referenceID": 4, "context": "[DKM06] shows that by choosing an appropriate sampling matrix \u03a0 \u2208 R, we obtain a Frobenius error guarantee (\u2016.", "startOffset": 0, "endOffset": 7}, {"referenceID": 2, "context": "\u2016) , such as JL embeddings or efficient subspace embeddings [Sar06, MZ11, ATKZ14, CNW15] that can be applied to any type of matrices X in input sparisty time [CW13].", "startOffset": 158, "endOffset": 164}, {"referenceID": 1, "context": "[CNW15] showed that using a subspace embedding \u03a0 \u2208 R we have with a probability 1\u2212 \u03b4:", "startOffset": 0, "endOffset": 7}, {"referenceID": 10, "context": "in [Lib13, GLPW15], drawing the connection between covariance matrix sketching, and the classic problem of estimation of frequent items [MG82].", "startOffset": 136, "endOffset": 142}, {"referenceID": 16, "context": "Based on this observation, [YLZ16] proposed to use the frequent directions algorithm of [Lib13] to perform AMM in a deterministic way, we refer to this approach as FDAMM.", "startOffset": 27, "endOffset": 34}, {"referenceID": 9, "context": "Based on this observation, [YLZ16] proposed to use the frequent directions algorithm of [Lib13] to perform AMM in a deterministic way, we refer to this approach as FDAMM.", "startOffset": 88, "endOffset": 95}, {"referenceID": 16, "context": "FD-AMM [YLZ16] outputs BX , BY such that", "startOffset": 7, "endOffset": 14}, {"referenceID": 9, "context": "2 Sketching from Covariance to Correlation In this section we review covariance sketching with the frequent directions algorithm of [Lib13] and state its theoretical guarantees [Lib13, GLPW15].", "startOffset": 132, "endOffset": 139}, {"referenceID": 9, "context": "Frequent directions algorithm introduced in [Lib13] (Algorithm 1) achieves this goal.", "startOffset": 44, "endOffset": 51}, {"referenceID": 9, "context": "Theorem 1 ([Lib13]) DX the output of algorithm 1 satisfies:", "startOffset": 11, "endOffset": 18}, {"referenceID": 9, "context": "For X = Y co-occuring directions reduces to frequent directions of [Lib13], and Theorem 2 recovers Theorem 1 of [Lib13].", "startOffset": 67, "endOffset": 74}, {"referenceID": 9, "context": "For X = Y co-occuring directions reduces to frequent directions of [Lib13], and Theorem 2 recovers Theorem 1 of [Lib13].", "startOffset": 112, "endOffset": 119}, {"referenceID": 6, "context": "Stronger bounds for frequent directions were given in [GLPW15] where the bound in Equation (4) is improved, for l > 2k, for any k:", "startOffset": 54, "endOffset": 62}, {"referenceID": 16, "context": "Hence by defining Z = [X ;Y ] \u2208 R (mx+my)\u00d7n and applying frequent directions to Z (FD-AMM [YLZ16]), we obtain BX , BY satisfying:", "startOffset": 90, "endOffset": 97}, {"referenceID": 16, "context": "A sharper analysis for co-occuring directions remains an open question, but the following discussion of Theorem 3 will shed some light on the advantages of co-occuring directions on FD-AMM [YLZ16].", "startOffset": 189, "endOffset": 196}, {"referenceID": 9, "context": "Similarly to the frequent directions [Lib13], co-occuring directions algorithm is simply parallelizable.", "startOffset": 37, "endOffset": 44}, {"referenceID": 4, "context": "Sampling [DKM06].", "startOffset": 9, "endOffset": 16}, {"referenceID": 2, "context": "Hashing [CW13].", "startOffset": 8, "endOffset": 14}, {"referenceID": 16, "context": "FD-AMM [YLZ16].", "startOffset": 7, "endOffset": 14}, {"referenceID": 6, "context": "We consider X \u2208 R mx\u00d7n and Y \u2208 Ry, generated using a non-noisy low rank model [GLPW15] as follows: X = VxSxU \u22a4 x , where Ux \u2208 Rx , (Ux)i,j \u223c N (0, 1), Sx \u2208 Rxx is a diagonal matrix with (Sx)jj = 1\u2212(j\u22121)/kx, and Vx \u2208 R mx\u00d7kx is such that V \u22a4 x Vx = Ikx .", "startOffset": 78, "endOffset": 86}, {"referenceID": 8, "context": "For visual features we use the residual CNN Resnet101, [HZRS16].", "startOffset": 55, "endOffset": 63}, {"referenceID": 11, "context": "ding HSKE of [MMG16] that results in a feature vector of dimension my = 3000.", "startOffset": 13, "endOffset": 20}, {"referenceID": 6, "context": "First, whether guarantees of Theorem 2 can be improved akin to the improved guarantees for frequent directions given [GLPW15].", "startOffset": 117, "endOffset": 125}], "year": 2016, "abstractText": "We introduce co-occurring directions sketching, a deterministic algorithm for approximate matrix product (AMM), in the streaming model. We show that co-occuring directions achieves a better error bound for AMM than other randomized and deterministic approaches for AMM. Co-occurring directions gives a (1 + \u03b5)-approximation of the optimal low rank approximation of a matrix product. Empirically our algorithm outperforms competing methods for AMM, for a small sketch size. We validate empirically our theoretical findings and algorithms.", "creator": "LaTeX with hyperref package"}}}