{"id": "1610.07722", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Oct-2016", "title": "Sparse Hierarchical Tucker Factorization and its Application to Healthcare", "abstract": "We propose a new tensor factorization method, called the Sparse Hierarchical-Tucker (Sparse H-Tucker), for sparse and high-order data tensors. Sparse H-Tucker is inspired by its namesake, the classical Hierarchical Tucker method, which aims to compute a tree-structured factorization of an input data set that may be readily interpreted by a domain expert. However, Sparse H-Tucker uses a nested sampling technique to overcome a key scalability problem in Hierarchical Tucker, which is the creation of an unwieldy intermediate dense core tensor; the result of our approach is a faster, more space-efficient, and more accurate method. We extensively test our method on a real healthcare dataset, which is collected from 30K patients and results in an 18th order sparse data tensor. Unlike competing methods, Sparse H-Tucker can analyze the full data set on a single multi-threaded machine. It can also do so more accurately and in less time than the state-of-the-art: on a 12th order subset of the input data, Sparse H-Tucker is 18x more accurate and 7.5x faster than a previously state-of-the-art method. Even for analyzing low order tensors (e.g., 4-order), our method requires close to an order of magnitude less time and over two orders of magnitude less memory, as compared to traditional tensor factorization methods such as CP and Tucker. Moreover, we observe that Sparse H-Tucker scales nearly linearly in the number of non-zero tensor elements. The resulting model also provides an interpretable disease hierarchy, which is confirmed by a clinical expert.", "histories": [["v1", "Tue, 25 Oct 2016 04:08:11 GMT  (2907kb,D)", "http://arxiv.org/abs/1610.07722v1", "This is an extended version of a paper presented at the 15th IEEE International Conference on Data Mining (ICDM 2015)"]], "COMMENTS": "This is an extended version of a paper presented at the 15th IEEE International Conference on Data Mining (ICDM 2015)", "reviews": [], "SUBJECTS": "cs.LG cs.NA", "authors": ["ioakeim perros", "robert chen", "richard vuduc", "jimeng sun"], "accepted": false, "id": "1610.07722"}, "pdf": {"name": "1610.07722.pdf", "metadata": {"source": "CRF", "title": "Sparse Hierarchical Tucker Factorization and its Application to Healthcare", "authors": ["Ioakeim Perros", "Robert Chen", "Richard Vuduc", "Jimeng Sun"], "emails": [], "sections": [{"heading": null, "text": "Unlike competing methods, Sparse H-Tucker can analyze the entire dataset on a single multi-strand machine, more accurately and in less time than the state of the art: for a subset of 12th order input data, Sparse H-Tucker is 18 times more accurate and 7.5 times faster than a previously state-of-the-art method. Even for the analysis of low order tensors (e.g. 4-order), our method requires almost an order of magnitude less time and over two orders of magnitude less memory than conventional tensor factoring methods such as CP and Tucker. Furthermore, we observe that Sparse H-Tucker scales almost linearly in the number of unequal tensor elements, and the resulting model also provides an interpretable disease hierarchy that is confirmed by a clinical expert."}, {"heading": "1 Introduction", "text": "In fact, the fact is that most of them are able to survive themselves, and that they are able to survive themselves, \"he said in an interview with the\" New York Times, \"which dealt with the question of how this situation could have come about.\" I don't think it will come to that, \"he said in an interview with the\" New York Times, \"\" but I believe it will come to that as it will. \""}, {"heading": "2 Background", "text": "This section presents the necessary definitions and precursor operations of matrix and tensor operations. Table 1 lists the notations used throughout the essay."}, {"heading": "2.1 Matrix factorizations", "text": "The Eckhart-Young theorem for the Singular Value Decomposition (SVD) [16] for UsuccessV = svd (A), where A-Rm \u00b7 n defines that if k < r = rank (A) and Ak = \u2211 k = 1 \u03c3iuiv T i, then: minrank (B) = k | | A-B | | 2 = | A-Ak | | 2 = \u03c3k + 1. Instead of using the singular vectors in SVD, the CUR decomposition [30] uses representative columns C and R to approximate the input matrix. The relative error guarantees of this method [14] depend on the notion of the lever score sampling2. The lever values for each j = 1,... n column of A are: \u03c0j = 1 / k \u00b2 samprix = amprix \u00b2, where vj = right singular vector of A = 1."}, {"heading": "2.2 Tensor operations and factorizations", "text": "A fiber is a vector that is extracted from a tensor by specifying all modes except one and a disc. I: = I1 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 Id and the index set of each mode is I\u00b5: = {1,.., n\u00b5},.Matricization (or transformation, unfolding), logically reorganizing the tensors into other forms without changing the values themselves. Leave the index set I (\u00b5): = I1 \u00b7 \u00b7 \u00b7 I\u00b5 \u2212 1 \u00b7 I\u00b5 \u00b7 \u00b7 \u00b7 \u00b7 Id mode."}, {"heading": "2.3 Tensor networks", "text": "A tensor network diagram, or simply a tensor network, hereinafter, provides an intuitive and precise graphical notation for displaying tensors and operations on tensors [9, 10]. A scalar, vector, matrix, or tensor is represented by the \"Balland Stick\" symbol that appears in Figure 1, where each circle denotes the object and each edge denotes an order or mode. Noted circles indicate a particular structure such as they are sparse. Where an open edge represents a mode, a closed edge represents a contraction along the given edge. The contraction of two tensors, A-RI1 and B-RJ1, results in another tensor representing another tensor."}, {"heading": "3 Sparse Hierarchical Tucker", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Model", "text": "Our proposed target model is called Sparse Hierarchical Tucker (Sparse H-Tucker). An example of this model in tensor network notation appears in Figure 2. In Sparse H-Tucker, tensor modes are recursively split, resulting in a binary tree that we call dimension tree and designate with TI. Each node of this tree contains a subset of t-1,..., d) of modes and is either a leaf and singleton t = \u00b5 or the union of its two disjointed successors t1, t2: t = t1, t2. Each tree node is associated with an output factor of the model. We designate these output factors by, (Bt) t-I (TI) t-kt1 \u00d7 kt2, (Ut) t-L (TI) t nodes. The tensors Bt are referred to as transfer types corresponding to the inner nodes, I (TI) kt2, which are associated with the structure (TI)."}, {"heading": "3.2 Sparse H-Tucker factorization algorithm", "text": "The proposed approaches are divided into two phases: phase 1, where we behave in a way, how we behave in a way, how we behave in a way, how we behave in a way, how we behave in a way, how we behave in a way, how we behave in a way, how we behave in a way, how we behave in a way, how we behave in a way, how we behave in a way, how we behave in a way, how we behave in a way, how we behave in a way, how we behave in a way, how we behave in a way, how we behave in a way, how we behave in a way, how we behave in a way, and how we behave in a way, and how we behave in a way, and how we behave in a way, and a way, and a way, and a way, and a way, and a way, and a way, and a way, and a way, and a way, and a way, and a way, and a way, and a way, and a way, and a way, and a way, and a way, and a way, and a way, and a way, and a way, and a way, and a way, and a way, a way, and a way, a way, and a way, and a way, a way, and a way, and a way, and a way, and a way, and a way, and a way, and a way, and a way, and a way, and a way, and a way, and a way, and a way, and a way, and a way, and a way, and a way, and a way, and a way, and a way, and a way, and a way, and a way, and a way, a way, a way, and a way, and a way, and a way, and a way, a way, and a way, a way, and a way, and a way, a way, and a way, a way, and a way, a way, and a way, a way, and a way, a way, and a way, a way, and a way, a way, a way, a way, and a way, a way, and a way, a way,"}, {"heading": "3.3 Tensor approximation via the model\u2019s factors", "text": "In the following we will describe how to approximate the input tensor using the Sparse HTucker model. (Ut) i = kt1 = 1 (Bt) i, j, l (Ut1) j (Ut2) l) (3) where {t1, t2} = s (t), Bt (kt1) \u00b7 R (It1It2) \u00b7 kt (Ut1) \u00b7 kt1, and Ut2 (t2). This process is used for all inner nodes in a bottom-up mode.3A remark about the algorithm 2 is that only for the root node the successors (i.e.) Ut1 \u00b7 kt1 \u00b7 kt1 \u00b7 kt1 (tr)."}, {"heading": "3.4 Nested sampling", "text": "This year it has come to the point where it will be able to take the lead, \"he said in an interview with the German Press Agency.\" We have no idea what it will be like, \"he said,\" but we have no idea what it will be like. \""}, {"heading": "4 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Setup", "text": "Our experiments were conducted on a server running the Red Hat Enterprise 6.6 operating system with 64 AMD Opteron processors (1.4 GHz) and 512 GB of RAM. To promote reproducible and usable research, we used Matlab R2015a as a programming framework and Matlab Tensor Toolbox v2.6 [2] to support tensor operations. To promote reproducible and usable research, our code is openly available. Methods in comparison are: \u2022 Sparse H-Tucker (Sequential): Sparse H-Tucker implementation with sequential execution in Phase 2. \u2022 Sparse H-Tucker (Parallel): Sparse H-Tucker implementation with parallel execution in Phase 2. \u2022 H-Tucker: Hierarchical Tucker implementation in htucker Toolbox [27] 6; \u2022 CP-ALS: Tensor Toolbox [2] Implementation and Olecker-Tulebox: HOTI]."}, {"heading": "4.2 Experiments on real healthcare data", "text": "This year it is more than ever before."}, {"heading": "4.3 Disease phenotyping case study", "text": "This year, it is so far that it will be able to retaliate, \"he said.\" We have never lost so much time, \"he said.\" We are not yet as far as we would like, \"he said."}, {"heading": "5 Conclusion", "text": "In this paper, we propose a scalable method of tensor factorization, which has been developed specifically for data analysis. Our experiments on real health data have established the accuracy and scalability of our approach. Also, its application to the problem of phenotyping diseases has confirmed its usefulness for health analysis and confirmed the correctness of our way of interpreting the resulting factors. This work is the first to use tensor network formalism in practice for unattended learning processes in data mining applications. We would like to emphasize the fact that Sparse H-Tucker is not limited to health care applications; health care is only the focus of current work and the application to more datasets and domains than future work. Furthermore, it is that we are not limited to the proposed method and the obvious benefits that we have even in the case of low tensors as we have experimentally verified them."}], "references": [{"title": "Matlab tensor toolbox version 2.6", "author": ["B.W. Bader", "T.G. Kolda"], "venue": "Available online,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2015}, {"title": "Fast evaluation of near-field boundary integrals using tensor approximations", "author": ["J. Ballani"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2012}, {"title": "Tree adaptive approximation in the hierarchical tensor format", "author": ["J. Ballani", "L. Grasedyck"], "venue": "SIAM Journal on Scientific Computing, 36(4):A1415\u2013 A1431", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2014}, {"title": "Black box approximation of tensors in hierarchical tucker format", "author": ["J. Ballani", "L. Grasedyck", "M. Kluge"], "venue": "Linear Algebra and its Applications, 438(2):639 \u2013 657", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2013}, {"title": "Analysis of individual differences in multidimensional scaling via an n-way generalization of eckart-young decomposition", "author": ["J.D. Carroll", "J.-J. Chang"], "venue": "Psychometrika, 35(3):283\u2013319", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1970}, {"title": "W", "author": ["S.R. Chinnamsetty", "M. Espig", "B.N. Khoromskij"], "venue": "Hackbusch, and H.- J. Flad. Tensor product approximation with optimal rank in quantum chemistry. The Journal of chemical physics, 127(8):084110", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2007}, {"title": "Tensor decompositions: a new concept in brain data analysis", "author": ["A. Cichocki"], "venue": "arXiv preprint arXiv:1305.0395,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2013}, {"title": "Era of big data processing: A new approach via tensor networks and tensor decompositions", "author": ["A. Cichocki"], "venue": "arXiv preprint arXiv:1403.2048", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2014}, {"title": "Tensor networks for big data analytics and large-scale optimization problems", "author": ["A. Cichocki"], "venue": "CoRR, abs/1407.3124", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2014}, {"title": "A multilinear singular value decomposition", "author": ["L. De Lathauwer", "B. De Moor", "J. Vandewalle"], "venue": "SIAM journal on Matrix Analysis and Applications, 21(4):1253\u20131278", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2000}, {"title": "On the best rank-1 and rank-(r 1", "author": ["L. De Lathauwer", "B. De Moor", "J. Vandewalle"], "venue": "r 2,..., rn) approximation of higher-order tensors. SIAM Journal on Matrix Analysis and Applications, 21(4):1324\u20131342", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2000}, {"title": "Fast monte carlo algorithms for matrices iii: Computing a compressed approximate matrix decomposition", "author": ["P. Drineas", "R. Kannan", "M.W. Mahoney"], "venue": "SIAM Journal on Computing, 36(1):184\u2013206", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2006}, {"title": "Relative-error cur matrix decompositions", "author": ["P. Drineas", "M.W. Mahoney", "S. Muthukrishnan"], "venue": "SIAM Journal on Matrix Analysis and Applications, 30(2):844\u2013881", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2008}, {"title": "Fast dtt: a near linear algorithm for decomposing a tensor into factor tensors", "author": ["X. Fang", "R. Pan"], "venue": "Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 967\u2013976. ACM", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2014}, {"title": "Matrix computations", "author": ["G.H. Golub", "C.F. Van Loan"], "venue": "volume 3. JHU Press", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2012}, {"title": "Hierarchical singular value decomposition of tensors", "author": ["L. Grasedyck"], "venue": "SIAM J. Matrix Anal. Appl.,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2010}, {"title": "A literature survey of low-rank tensor approximation techniques", "author": ["L. Grasedyck", "D. Kressner", "C. Tobler"], "venue": "GAMM-Mitteilungen, 36(1):53\u201378", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2013}, {"title": "Foundations of the parafac procedure: Models and conditions for an", "author": ["R.A. Harshman"], "venue": "explanatory\u201d multi-modal factor analysis", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1970}, {"title": "Limestone: High-throughput candidate phenotype generation via tensor factorization", "author": ["J.C. Ho", "J. Ghosh", "S.R. Steinhubl", "W.F. Stewart", "J.C. Denny", "B.A. Malin", "J. Sun"], "venue": "Journal of biomedical informatics, 52:199\u2013 211", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2014}, {"title": "Marble: high-throughput phenotyping from electronic health records via sparse nonnegative tensor factorization", "author": ["J.C. Ho", "J. Ghosh", "J. Sun"], "venue": "Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 115\u2013124. ACM", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2014}, {"title": "Fema: flexible evolutionary multi-faceted analysis for dynamic behavioral pattern discovery", "author": ["M. Jiang", "P. Cui", "F. Wang", "X. Xu", "W. Zhu", "S. Yang"], "venue": "Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 1186\u20131195. ACM", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2014}, {"title": "Gigatensor: Scaling tensor analysis up by 100 times - algorithms and discoveries", "author": ["U. Kang", "E. Papalexakis", "A. Harpale", "C. Faloutsos"], "venue": "Proceedings of the 18th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD \u201912, pages 316\u2013324, New York, NY, USA", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2012}, {"title": "Direct solution of the chemical master equation using quantized tensor trains", "author": ["V. Kazeev", "M. Khammash", "M. Nip", "C. Schwab"], "venue": "PLoS computational biology, 10(3):e1003359", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2014}, {"title": "Structured data-sparse approximation to high order tensors arising from the deterministic boltzmann equation", "author": ["B. Khoromskij"], "venue": "Mathematics of computation, 76(259):1291\u20131315", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2007}, {"title": "Tensor decompositions and applications", "author": ["T. Kolda", "B. Bader"], "venue": "SIAM Review, 51(3):455\u2013500", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2009}, {"title": "Algorithm 941: htucker\u2014a matlab toolbox for tensors in hierarchical tucker format", "author": ["D. Kressner", "C. Tobler"], "venue": "ACM Transactions on Mathematical Software (TOMS), 40(3):22", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2014}, {"title": "Challenges in multimodal data fusion", "author": ["D. Lahat", "T. Adaly", "C. Jutten"], "venue": "Signal Processing Conference (EUSIPCO), 2013 Proceedings of the 22nd European, pages 101\u2013105. IEEE", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2014}, {"title": "Multiway array decomposition analysis of eegs in alzheimer\u2019s disease", "author": ["C.-F.V. Latchoumane", "F.-B. Vialatte", "J. Sol\u00e9-Casals", "M. Maurice", "S.R. Wimalaratna", "N. Hudson", "J. Jeong", "A. Cichocki"], "venue": "Journal of neuroscience methods, 207(1):41\u201350", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2012}, {"title": "Cur matrix decompositions for improved data analysis", "author": ["M.W. Mahoney", "P. Drineas"], "venue": "Proceedings of the National Academy of Sciences, 106(3):697\u2013 702", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2009}, {"title": "Tensor-cur decompositions for tensor-based data", "author": ["M.W. Mahoney", "M. Maggioni", "P. Drineas"], "venue": "Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD \u201906, pages 327\u2013336, New York, NY, USA", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2006}, {"title": "W", "author": ["Y. Matsubara", "Y. Sakurai"], "venue": "G. van Panhuis, and C. Faloutsos. Funnel: Automatic mining of spatially coevolving epidemics. In Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD \u201914, pages 105\u2013114, New York, NY, USA", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2014}, {"title": "Advances on tensor network theory: symmetries", "author": ["R. Orus"], "venue": "fermions, entanglement, and holography. The European Physical Journal B, 87(11)", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2014}, {"title": "Tensor-train decomposition", "author": ["I. Oseledets"], "venue": "SIAM Journal on Scientific Computing, 33(5):2295\u20132317", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2011}, {"title": "Parcube: Sparse parallelizable tensor decompositions", "author": ["E.E. Papalexakis", "C. Faloutsos", "N.D. Sidiropoulos"], "venue": "P. A. Flach, T. D. Bie, and N. Cristianini, editors, ECML/PKDD (1), volume 7523 of Lecture Notes in Computer Science, pages 521\u2013536. Springer", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2012}, {"title": "Sparse hierarchical tucker factorization and its application to healthcare", "author": ["I. Perros", "R. Chen", "R. Vuduc", "J. Sun"], "venue": "Data Mining (ICDM), 2015 IEEE International Conference on, pages 943\u2013948. IEEE", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2015}, {"title": "Pairwise interaction tensor factorization for personalized tag recommendation", "author": ["S. Rendle", "L. Schmidt-Thieme"], "venue": "Proceedings of the third ACM international conference on Web search and data mining, pages 81\u201390. ACM", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2010}, {"title": "Multiparameter intelligent monitoring in intensive care II (MIMIC-II): a public-access intensive care unit database", "author": ["M. Saeed"], "venue": "Critical care medicine,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2011}, {"title": "Beyond streams and graphs: Dynamic tensor analysis", "author": ["J. Sun", "D. Tao", "C. Faloutsos"], "venue": "Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD \u201906, pages 374\u2013383, New York, NY, USA", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2006}, {"title": "Mach: Fast randomized tensor decompositions", "author": ["C.E. Tsourakakis"], "venue": "SDM, pages 689\u2013700. SIAM", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2010}, {"title": "Some mathematical notes on three-mode factor analysis", "author": ["L.R. Tucker"], "venue": "Psychometrika, 31(3):279\u2013311", "citeRegEx": "41", "shortCiteRegEx": null, "year": 1966}, {"title": "Rubik: Knowledge guided tensor factorization and completion for health data analytics", "author": ["Y. Wang", "R. Chen", "J. Ghosh", "J.C. Denny", "A. Kho", "Y. Chen", "B.A. Malin", "J. Sun"], "venue": "Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD \u201915, pages 1265\u20131274, New York, NY, USA", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 26, "context": "Analyzing multi-modal data arises in data mining due to the abundance of information available that describes the same data objects [28].", "startOffset": 132, "endOffset": 136}, {"referenceID": 27, "context": "We are motivated to study tensor methods because they are recognized as one of the most promising approaches for mining multi-modal data, with proof-ofconcept demonstrations in a broad variety of application domains, such as neuroscience [29, 8], epidemics [32], human behavior modeling [22], natural language", "startOffset": 238, "endOffset": 245}, {"referenceID": 6, "context": "We are motivated to study tensor methods because they are recognized as one of the most promising approaches for mining multi-modal data, with proof-ofconcept demonstrations in a broad variety of application domains, such as neuroscience [29, 8], epidemics [32], human behavior modeling [22], natural language", "startOffset": 238, "endOffset": 245}, {"referenceID": 30, "context": "We are motivated to study tensor methods because they are recognized as one of the most promising approaches for mining multi-modal data, with proof-ofconcept demonstrations in a broad variety of application domains, such as neuroscience [29, 8], epidemics [32], human behavior modeling [22], natural language", "startOffset": 257, "endOffset": 261}, {"referenceID": 20, "context": "We are motivated to study tensor methods because they are recognized as one of the most promising approaches for mining multi-modal data, with proof-ofconcept demonstrations in a broad variety of application domains, such as neuroscience [29, 8], epidemics [32], human behavior modeling [22], natural language", "startOffset": 287, "endOffset": 291}, {"referenceID": 21, "context": "processing [23], social network analysis [35], network intrusion detection [39], and healthcare analytics [21, 20, 42], to name just a few.", "startOffset": 11, "endOffset": 15}, {"referenceID": 33, "context": "processing [23], social network analysis [35], network intrusion detection [39], and healthcare analytics [21, 20, 42], to name just a few.", "startOffset": 41, "endOffset": 45}, {"referenceID": 37, "context": "processing [23], social network analysis [35], network intrusion detection [39], and healthcare analytics [21, 20, 42], to name just a few.", "startOffset": 75, "endOffset": 79}, {"referenceID": 19, "context": "processing [23], social network analysis [35], network intrusion detection [39], and healthcare analytics [21, 20, 42], to name just a few.", "startOffset": 106, "endOffset": 118}, {"referenceID": 18, "context": "processing [23], social network analysis [35], network intrusion detection [39], and healthcare analytics [21, 20, 42], to name just a few.", "startOffset": 106, "endOffset": 118}, {"referenceID": 40, "context": "processing [23], social network analysis [35], network intrusion detection [39], and healthcare analytics [21, 20, 42], to name just a few.", "startOffset": 106, "endOffset": 118}, {"referenceID": 24, "context": "In data analysis, each dimension is referred to as a mode, order, or way [26].", "startOffset": 73, "endOffset": 77}, {"referenceID": 17, "context": "Examples of well-known tensor decomposition methods include CP (CANDECOMP-PARAFAC) and Tucker methods [19, 6, 41, 11].", "startOffset": 102, "endOffset": 117}, {"referenceID": 4, "context": "Examples of well-known tensor decomposition methods include CP (CANDECOMP-PARAFAC) and Tucker methods [19, 6, 41, 11].", "startOffset": 102, "endOffset": 117}, {"referenceID": 39, "context": "Examples of well-known tensor decomposition methods include CP (CANDECOMP-PARAFAC) and Tucker methods [19, 6, 41, 11].", "startOffset": 102, "endOffset": 117}, {"referenceID": 9, "context": "Examples of well-known tensor decomposition methods include CP (CANDECOMP-PARAFAC) and Tucker methods [19, 6, 41, 11].", "startOffset": 102, "endOffset": 117}, {"referenceID": 16, "context": "Then, the dense core has size r, which in this case is nearly 9 Petabytes, assuming 8 bytes per (double-precision floating-point) value [18].", "startOffset": 136, "endOffset": 140}, {"referenceID": 35, "context": ", 3 orders [37, 15]).", "startOffset": 11, "endOffset": 19}, {"referenceID": 13, "context": ", 3 orders [37, 15]).", "startOffset": 11, "endOffset": 19}, {"referenceID": 13, "context": "models the interactions of each one of the three modes with the two others through a tensor, the horizontal slices of which are further decomposed into two lowrank matrices [15].", "startOffset": 173, "endOffset": 177}, {"referenceID": 5, "context": "In order to tackle this limitation as well, we adopt a recently proposed tensor formalism called tensor networks, originally developed for applications in quantum chemistry and physics [7, 24, 25].", "startOffset": 185, "endOffset": 196}, {"referenceID": 22, "context": "In order to tackle this limitation as well, we adopt a recently proposed tensor formalism called tensor networks, originally developed for applications in quantum chemistry and physics [7, 24, 25].", "startOffset": 185, "endOffset": 196}, {"referenceID": 23, "context": "In order to tackle this limitation as well, we adopt a recently proposed tensor formalism called tensor networks, originally developed for applications in quantum chemistry and physics [7, 24, 25].", "startOffset": 185, "endOffset": 196}, {"referenceID": 32, "context": "Besides their simple and intuitive graphical representations, tensor networks also provide a set of computational strategies to approximate a high-order dense tensor by an interconnected graph of low-order tensors (typically, 2nd and 3rd order tensors) [34, 17, 33, 9].", "startOffset": 253, "endOffset": 268}, {"referenceID": 15, "context": "Besides their simple and intuitive graphical representations, tensor networks also provide a set of computational strategies to approximate a high-order dense tensor by an interconnected graph of low-order tensors (typically, 2nd and 3rd order tensors) [34, 17, 33, 9].", "startOffset": 253, "endOffset": 268}, {"referenceID": 31, "context": "Besides their simple and intuitive graphical representations, tensor networks also provide a set of computational strategies to approximate a high-order dense tensor by an interconnected graph of low-order tensors (typically, 2nd and 3rd order tensors) [34, 17, 33, 9].", "startOffset": 253, "endOffset": 268}, {"referenceID": 7, "context": "Besides their simple and intuitive graphical representations, tensor networks also provide a set of computational strategies to approximate a high-order dense tensor by an interconnected graph of low-order tensors (typically, 2nd and 3rd order tensors) [34, 17, 33, 9].", "startOffset": 253, "endOffset": 268}, {"referenceID": 7, "context": "1The intuition behind and potential applications of tensor networks in data processing appear in recent surveys by Cichocki [9, 10].", "startOffset": 124, "endOffset": 131}, {"referenceID": 8, "context": "1The intuition behind and potential applications of tensor networks in data processing appear in recent surveys by Cichocki [9, 10].", "startOffset": 124, "endOffset": 131}, {"referenceID": 34, "context": "ICDM 2015 [36].", "startOffset": 10, "endOffset": 14}, {"referenceID": 34, "context": "In addition to the ones of [36], our contributions in this extended version can be summarized as follows:", "startOffset": 27, "endOffset": 31}, {"referenceID": 14, "context": "The Eckhart-Young Theorem for the Singular Value Decomposition (SVD) [16] for U\u03a3V = svd(A), where A \u2208 Rm\u00d7n defines that if k < r = rank(A) and Ak = \u2211k i=1 \u03c3iuiv T i , then: min rank(B)=k ||A \u2212 B||2 = ||A \u2212 Ak||2 = \u03c3k+1.", "startOffset": 69, "endOffset": 73}, {"referenceID": 28, "context": "stead of using the singular vectors in SVD, the CUR decomposition [30] uses representative columns C and rows R to approximate the input matrix.", "startOffset": 66, "endOffset": 70}, {"referenceID": 12, "context": "The relative-error guarantees of this method [14] depend on the notion of leverage score sampling.", "startOffset": 45, "endOffset": 49}, {"referenceID": 11, "context": "based on the row/column norms) are known to give much coarser additive error estimates [13].", "startOffset": 87, "endOffset": 91}, {"referenceID": 29, "context": "Tensor versions of CUR approximation are given in [31, 40], but cannot handle high-order tensors due to cost limitations faced, similar to the ones of Tucker.", "startOffset": 50, "endOffset": 58}, {"referenceID": 38, "context": "Tensor versions of CUR approximation are given in [31, 40], but cannot handle high-order tensors due to cost limitations faced, similar to the ones of Tucker.", "startOffset": 50, "endOffset": 58}, {"referenceID": 24, "context": "A fiber is a vector extracted from a tensor by fixing all modes but one and a slice is a matrix extracted from a tensor by fixing all modes but two [26].", "startOffset": 148, "endOffset": 152}, {"referenceID": 17, "context": "The factorization of a tensor into a sum of component rank-one tensors is called the CP/PARAFAC [19, 6] factorization.", "startOffset": 96, "endOffset": 103}, {"referenceID": 4, "context": "The factorization of a tensor into a sum of component rank-one tensors is called the CP/PARAFAC [19, 6] factorization.", "startOffset": 96, "endOffset": 103}, {"referenceID": 17, "context": "The most popular factorization method approximating the above model is the CP-Alternating Least Squares (ALS) [19, 6, 26], which optimizes iteratively over each one of the output matrices by fixing all others.", "startOffset": 110, "endOffset": 121}, {"referenceID": 4, "context": "The most popular factorization method approximating the above model is the CP-Alternating Least Squares (ALS) [19, 6, 26], which optimizes iteratively over each one of the output matrices by fixing all others.", "startOffset": 110, "endOffset": 121}, {"referenceID": 24, "context": "The most popular factorization method approximating the above model is the CP-Alternating Least Squares (ALS) [19, 6, 26], which optimizes iteratively over each one of the output matrices by fixing all others.", "startOffset": 110, "endOffset": 121}, {"referenceID": 39, "context": "The Tucker format is given by the following form [41, 11]: A = (U1, .", "startOffset": 49, "endOffset": 57}, {"referenceID": 9, "context": "The Tucker format is given by the following form [41, 11]: A = (U1, .", "startOffset": 49, "endOffset": 57}, {"referenceID": 9, "context": "If the core tensor is computed in the above way and each U\u03bc contains the leading k\u03bc left singular vectors of A(\u03bc), the factorization of tensor A is called the higher-order SVD (HOSVD) [11, 26].", "startOffset": 184, "endOffset": 192}, {"referenceID": 24, "context": "If the core tensor is computed in the above way and each U\u03bc contains the leading k\u03bc left singular vectors of A(\u03bc), the factorization of tensor A is called the higher-order SVD (HOSVD) [11, 26].", "startOffset": 184, "endOffset": 192}, {"referenceID": 10, "context": "HOSVD is considered as a good initialization to the higher-order orthogonal iteration (HOOI) [12], which is also an ALS-type algorithm, being the most popular way to approximate the Tucker format in real world applications.", "startOffset": 93, "endOffset": 97}, {"referenceID": 7, "context": "A tensor network diagram, or just tensor network hereafter, provides an intuitive and concise graphical notation for representing tensors and operations on tensors [9, 10].", "startOffset": 164, "endOffset": 171}, {"referenceID": 8, "context": "A tensor network diagram, or just tensor network hereafter, provides an intuitive and concise graphical notation for representing tensors and operations on tensors [9, 10].", "startOffset": 164, "endOffset": 171}, {"referenceID": 15, "context": "Hierarchical Tucker and its Limitations One popular model of the tensor network family that shares structural similarities with our proposed model is the Hierarchical Tucker (H-Tucker in short) presented in [17].", "startOffset": 207, "endOffset": 211}, {"referenceID": 15, "context": "Intuitively, the HTucker factorization algorithm proposed in [17] first decomposes the input tensor into the Tucker format through the HOSVD and then recursively factorizes the output tensor of this process.", "startOffset": 61, "endOffset": 65}, {"referenceID": 3, "context": "Another factorization scheme that is based on H-Tucker and is similar to ours was proposed in the tensor community by Ballani et al [5, 3].", "startOffset": 132, "endOffset": 138}, {"referenceID": 1, "context": "Another factorization scheme that is based on H-Tucker and is similar to ours was proposed in the tensor community by Ballani et al [5, 3].", "startOffset": 132, "endOffset": 138}, {"referenceID": 15, "context": "Our proposed model\u2019s tree structure is like that of the H-Tucker model [17].", "startOffset": 71, "endOffset": 75}, {"referenceID": 15, "context": "The equations that govern the reconstruction of our model also apply in the H-Tucker model [17], where Equation 3 reflects a property called nestedness; we will use the same terminology hereafter.", "startOffset": 91, "endOffset": 95}, {"referenceID": 28, "context": "Our approach is to form the factors approximating A through the CUR decomposition based on leverage score sampling [30].", "startOffset": 115, "endOffset": 119}, {"referenceID": 28, "context": "More specifically, we follow the same sampling strategy as in [30], by retrieving O(k log k/ ) rows or columns for each required approximation, where k is the rank of SVD, which is set to a small value (k = 5).", "startOffset": 62, "endOffset": 66}, {"referenceID": 0, "context": "6 [2] in order to support tensor operations.", "startOffset": 2, "endOffset": 5}, {"referenceID": 25, "context": "\u2022 H-Tucker: Hierarchical Tucker implementation provided in htucker toolbox [27]; \u2022 CP-ALS: Tensor Toolbox [2] implementation; and \u2022 Tucker-ALS: Tensor Toolbox [2] implementation of HOOI.", "startOffset": 75, "endOffset": 79}, {"referenceID": 0, "context": "\u2022 H-Tucker: Hierarchical Tucker implementation provided in htucker toolbox [27]; \u2022 CP-ALS: Tensor Toolbox [2] implementation; and \u2022 Tucker-ALS: Tensor Toolbox [2] implementation of HOOI.", "startOffset": 106, "endOffset": 109}, {"referenceID": 0, "context": "\u2022 H-Tucker: Hierarchical Tucker implementation provided in htucker toolbox [27]; \u2022 CP-ALS: Tensor Toolbox [2] implementation; and \u2022 Tucker-ALS: Tensor Toolbox [2] implementation of HOOI.", "startOffset": 159, "endOffset": 162}, {"referenceID": 36, "context": "The dataset is called MIMIC-II and can be found in [38] .", "startOffset": 51, "endOffset": 55}, {"referenceID": 25, "context": "Phase 2) H-Tucker [27] CP-ALS [2] Tucker-ALS [2]", "startOffset": 18, "endOffset": 22}, {"referenceID": 0, "context": "Phase 2) H-Tucker [27] CP-ALS [2] Tucker-ALS [2]", "startOffset": 30, "endOffset": 33}, {"referenceID": 0, "context": "Phase 2) H-Tucker [27] CP-ALS [2] Tucker-ALS [2]", "startOffset": 45, "endOffset": 48}, {"referenceID": 25, "context": "Phase 2) H-Tucker [27] CP-ALS [2] Tucker-ALS [2]", "startOffset": 18, "endOffset": 22}, {"referenceID": 0, "context": "Phase 2) H-Tucker [27] CP-ALS [2] Tucker-ALS [2]", "startOffset": 30, "endOffset": 33}, {"referenceID": 0, "context": "Phase 2) H-Tucker [27] CP-ALS [2] Tucker-ALS [2]", "startOffset": 45, "endOffset": 48}, {"referenceID": 2, "context": "The tree construction for the H-Tucker model is attempted by the recent work in [4].", "startOffset": 80, "endOffset": 83}], "year": 2016, "abstractText": "We propose a new tensor factorization method, called the Sparse Hierarchical Tucker (Sparse H-Tucker), for sparse and high-order data tensors. Sparse H-Tucker is inspired by its namesake, the classical Hierarchical Tucker method, which aims to compute a tree-structured factorization of an input data set that may be readily interpreted by a domain expert. However, Sparse H-Tucker uses a nested sampling technique to overcome a key scalability problem in Hierarchical Tucker, which is the creation of an unwieldy intermediate dense core tensor; the result of our approach is a faster, more space-efficient, and more accurate method. We extensively test our method on a real healthcare dataset, which is collected from 30K patients and results in an 18th order sparse data tensor. Unlike competing methods, Sparse H-Tucker can analyze the full data set on a single multi-threaded machine. It can also do so more accurately and in less time than the state-of-the-art: on a 12th order subset of the input data, Sparse H-Tucker is 18\u00d7 more accurate and 7.5\u00d7 faster than a previously state-of-the-art method. Even for analyzing low order tensors (e.g., 4-order), our method requires close to an order of magnitude less time and over two orders of magnitude less memory, as compared to traditional tensor factorization methods such as CP and Tucker. Moreover, we observe that Sparse H-Tucker scales nearly linearly in the number of non-zero tensor elements. The resulting model also provides an interpretable disease hierarchy, which is confirmed by a clinical expert.", "creator": "LaTeX with hyperref package"}}}