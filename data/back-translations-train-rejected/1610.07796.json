{"id": "1610.07796", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Oct-2016", "title": "Still not there? Comparing Traditional Sequence-to-Sequence Models to Encoder-Decoder Neural Networks on Monotone String Translation Tasks", "abstract": "We analyze the performance of encoder-decoder neural models and compare them with well-known established methods. The latter represent different classes of traditional approaches that are applied to the monotone sequence-to-sequence tasks OCR post-correction, spelling correction, grapheme-to-phoneme conversion, and lemmatization. Such tasks are of practical relevance for various higher-level research fields including digital humanities, automatic text correction, and speech recognition. We investigate how well generic deep-learning approaches adapt to these tasks, and how they perform in comparison with established and more specialized methods, including our own adaptation of pruned CRFs.", "histories": [["v1", "Tue, 25 Oct 2016 09:14:05 GMT  (830kb)", "https://arxiv.org/abs/1610.07796v1", "Accepted for publication at COLING 2016. See also:this https URL&amp;tx_bibtex_pi1%5Bpub_id%5D=TUD-CS-2016-1450"], ["v2", "Wed, 26 Oct 2016 13:05:39 GMT  (734kb)", "http://arxiv.org/abs/1610.07796v2", "Accepted for publication at COLING 2016. See also:this https URL&amp;tx_bibtex_pi1%5Bpub_id%5D=TUD-CS-2016-1450 Version 2: corrected spelling of third author"]], "COMMENTS": "Accepted for publication at COLING 2016. See also:this https URL&amp;tx_bibtex_pi1%5Bpub_id%5D=TUD-CS-2016-1450", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["carsten schnober", "steffen eger", "erik-l\\^an do dinh", "iryna gurevych"], "accepted": false, "id": "1610.07796"}, "pdf": {"name": "1610.07796.pdf", "metadata": {"source": "CRF", "title": "Still not there? Comparing Traditional Sequence-to-Sequence Models to Encoder-Decoder Neural Networks on Monotone String Translation Tasks", "authors": ["Carsten Schnober", "Steffen Eger"], "emails": [], "sections": [{"heading": null, "text": "ar Xiv: 161 0.07 796v 2 [cs.C L] 26 Oct 201 6"}, {"heading": "1 Introduction", "text": "However, we have shown that these models answer the various NLP tasks including machine translation (Cho et al., 2014), conversation modeling (Vinyals and Le, 2015), questions (Yin et al., 2016), and, more generally, language correction (Schmaltz et al., 2016). We have noticed that given the huge interest currently surrounding neural architectures, current research is somewhat overenthusiastic about the performance of encoder-decoder tasks."}, {"heading": "2 Task Description", "text": "Depending on various factors, including paper, and a sequence of symbols defined by ~ x. Therefore, a set of lengths s is referred to as ~ x = x1. xs. Really evaluated vectors are referred to as bold letters, x.Spellchecker is the problem of converting an \"erroneous\" input sequence ~ x into a corrected version ~ y. In terms of errors committed by people (typos), spellchecker often deals with errors based on the keyboard adjacency of characters and grapho-phonemic mismatches (e.g. emergency, wuld \u2192 would).OCR post-correction can be considered a special case of spellchecker. OCR (optical character recognition) is the process of digitalizing printed texts that are commonly used to make text data from the pre-electronic age digitally available."}, {"heading": "3 Data", "text": "These data sets reflect the different Seq2Seq tasks we want to examine, the data has been digitized and OCR errors have been corrected manually, the corpus contains 19,024 pages, 17,186 of which are in Swiss German, and 1,838 are in French. We have 88,302 unique misrecognized words along with their manually corrected counterparts, and for our experiments we used randomly selected 72K entries to train and test our models for additional 9K entries, and we report the results for each model trained on a reduced training (10K entries).Twitter Typo Corpus3: We use a corpus of 39,172 spelling errors that we extracted from their respective tweets."}, {"heading": "4 Model Description", "text": "In this section, we briefly describe neural models of encoders and decoders, truncated CRFs, and our three basic lines."}, {"heading": "4.1 Encoder-Decoder Neural Models", "text": "We compare three variants of encoder decoder models: the \"classic\" variant and two modifications of hidden cell units: \u2022 enc-dec: encoder decoder models using recursive neural networks (RNs) for Seq2Seq tasks were developed by Cho et al. (2014) and Sutskever et al. (2014) The encoder reads an input ~ x and 3Twitter type corpus: http: / / luululu.com / tweet / generates a vector representation e. The encoder predicts the output ~ y in a time step based on e. The probability for each output symbol yt hence depends on e and all previously generated output symbols: p y | e = T \u2032 t = 1 p (y1 \u00b7 \u00b7 yt \u2212 1), where T \u2212 1 is the length of the output sequence."}, {"heading": "4.2 Pruned Conditional Random Fields", "text": "Conditional Random Fields (CRFs) were introduced by Lafferty et al. (2001) and were an important workhorse for many sequence labeling tasks such as part-of-speech tagging and named entity recognition during the 2000s. Unfortunately, training and decoding time are polynomically dependent on the tagset size and exponentially depend on the order of the CRF. Here, the order refers to the dependencies on the label side. This makes higher CRFs impractical for large training data sizes, which is why practically only first-order (linear chain) CRFs to the Recently.Mu \ufffd ller et al. (2013) introduced pruned CRFs (PCRFs) that use the CRF objective function with coarse to fine decoding degree (Charniak and Johnson, 2005). PCRFs require much shorter runtime and are therefore able to execute higher orders."}, {"heading": "4.3 Further Baseline Systems", "text": "Considering the similarity of G2P conversion, spelling correction and lemmatization in terms of their innate monotonicity (Eger et al., 2016; Nicolai et al., 2015; Eger, 2015), we are examining three additional approaches for all our data sets that originally applied to G2P conversion. Sequitur (Bisani et al., 2008) is a \"common\" model for Seq2Seq in the sense of the classical distinction between common and discriminatory models. Its core architecture is a model of \"common n-grams,\" also referred to in the original publication as \"graphs\" (i.e. pairs of substrings of the ~ x and ~ y sequences).DirecTL + (Jiampojamarn et al al al al al al., 2010) is a discriminatory model for monotonous Seq2Seqs models that integrates common n-gram-specific features."}, {"heading": "5 Results and Analysis", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 Model Performances", "text": "This year, more than ever before in the history of the country in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is not a country, but in which it is a country, a country, a city and a country."}, {"heading": "5.2 Training Time", "text": "Another potentially limiting factor for the applicability of a model in real-world scenarios, especially for large datasets, is training time. Under all circumstances, weighted finite-state transducers (phonetisaurus) are trained orders of magnitude faster than any other approaches. Training times range from just 6 seconds for the smallest training set to 247 seconds for the full text + mountain training set (72K entries).In comparison, training times for the encoder decoder models range from 2 to 80 hours (excluding GPUs) for 30 epochs, depending on the size of the networks and training data. In addition, there is no discernible difference between either of the three encoder decoder variations. Training time increases roughly linearly with the number of layers, the size of layers and training data. All of these factors add up, meaning that the number of layers and the size of the decoder decoder decoder sequence data roughly doubles with the size of the training layers, and the number of training layers increases with the size of the training area."}, {"heading": "5.3 Error Analysis", "text": "We have divided three of our test sets (Text + Berg, Twitter and Combilex) by entering string lengths and evaluated PCRF-Seq2Seq and encoder decoder neural models for these subsets of test data. As shown in Figures 2 and 3, we observe a consistent trend: PCRF-Seq2Seq behaves relatively robustly with input strings of varying lengths, while the performance of encoder decoder models decreases more drastically as sequences become longer, especially those without attention mechanism. In shorter sequences, we observe that standard encoder decoder models have their attention-based counterparts, as well as PCRF-Seq2Seq, both in the Twitter spell-correction task (Figure 3) and in G2P conversion, very long codes-code models (as opposed to their rather low performance with full sets)."}, {"heading": "6 Conclusions", "text": "The universality of neural networks makes them attractive for a wide range of possible tasks. As part of this work, we have applied encoder decoder neural models to monotonous Seq2Seq tasks. We have shown that in some cases they may be comparable to more specialized models, but are not (yet) consistently able to outperform established approaches and sometimes still significantly below them. Furthermore, the advantage that feature engineering and hyperparameter optimization are unnecessary in the traditional sense is notoriously replaced by the search for optimal neural network topology.At first glance, our analysis of string lengths is consistent with those reported by Bahdanau et al. (2014) They find that - for the field of machine translation - the attention mechanism leads to improvements over the standard encoder decoder decoder models. We also observe these positive effects on our tasks, where the attention mechanism reduces the performance of the encoder standardized encoder models over long periods of time."}, {"heading": "Acknowledgements", "text": "This work was supported by the Volkswagen Foundation as part of the Lichtenberg Professorship Programme under grant number I / 82806 and by the German Institute for Human Development (DIPF) as part of the graduate programme \"Knowledge Discovery in Scientific Literature\" (KDSL)."}], "references": [{"title": "Semi-supervised learning of morphological paradigms and lexicons", "author": ["Malin Ahlberg", "Markus Forsberg", "Mans Hulden."], "venue": "Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, Gothenburg, Sweden 26\u201330 April 2014, pages 569\u2013578.", "citeRegEx": "Ahlberg et al\\.,? 2014", "shortCiteRegEx": "Ahlberg et al\\.", "year": 2014}, {"title": "Neural Machine Translation by Jointly Learning to Align and Translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "arXiv:1409.0473 [cs, stat], September.", "citeRegEx": "Bahdanau et al\\.,? 2014", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Joint-Sequence Models for Grapheme-to-Phoneme Conversion", "author": ["Maximilian Bisani", "Hermann Ney."], "venue": "Speech Communication, 50(5):434\u2013451, May.", "citeRegEx": "Bisani and Ney.,? 2008", "shortCiteRegEx": "Bisani and Ney.", "year": 2008}, {"title": "An Improved Error Model for Noisy Channel Spelling Correction", "author": ["Eric Brill", "Robert C. Moore."], "venue": "Proceedings of ACL \u201900, pages 286\u2013293, Stroudsburg, PA, USA. Association for Computational Linguistics.", "citeRegEx": "Brill and Moore.,? 2000", "shortCiteRegEx": "Brill and Moore.", "year": 2000}, {"title": "Coarse-to-Fine n-Best Parsing and MaxEnt Discriminative Reranking", "author": ["Eugene Charniak", "Mark Johnson."], "venue": "Proceedings of ACL \u201905, pages 173\u2013180, Ann Arbor, MI, USA. Association for Computational Linguistics.", "citeRegEx": "Charniak and Johnson.,? 2005", "shortCiteRegEx": "Charniak and Johnson.", "year": 2005}, {"title": "Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation", "author": ["Kyunghyun Cho", "Bart van Merrienboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio."], "venue": "arXiv:1406.1078 [cs, stat], June.", "citeRegEx": "Cho et al\\.,? 2014", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Normalizing Tweets with Edit Scripts and Recurrent Neural Embeddings", "author": ["Grzegorz Chrupa\u0142a."], "venue": "Proceedings of ACL \u201914, pages 680\u2013686, Baltimore, MD, USA. Association for Computational Linguistics.", "citeRegEx": "Chrupa\u0142a.,? 2014", "shortCiteRegEx": "Chrupa\u0142a.", "year": 2014}, {"title": "Natural Language Processing (Almost) from Scratch", "author": ["Ronan Collobert", "Jason Weston", "L\u00e9on Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa."], "venue": "The Journal of Machine Learning Research, 12:2493\u2013 2537, February.", "citeRegEx": "Collobert et al\\.,? 2011", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Spelling Correction as an Iterative Process that Exploits the Collective Knowledge of Web Users", "author": ["Silviu Cucerzan", "Eric Brill."], "venue": "Proceedings of EMNLP \u201904, pages 293\u2013300, Barcelona, Spain. Association for Computational Linguistics.", "citeRegEx": "Cucerzan and Brill.,? 2004", "shortCiteRegEx": "Cucerzan and Brill.", "year": 2004}, {"title": "Supervised learning of complete morphological paradigms", "author": ["Greg Durrett", "John DeNero."], "venue": "Proceedings of NAACL-HLT \u201913, pages 1185\u20131195, Atlanta, GA, USA. Association for Computational Linguistics.", "citeRegEx": "Durrett and DeNero.,? 2013", "shortCiteRegEx": "Durrett and DeNero.", "year": 2013}, {"title": "A Comparison of Four Character-Level String-toString Translation Models for (OCR) Spelling Error Correction", "author": ["Steffen Eger", "Tim vor der Br\u00fcck", "Alexander Mehler."], "venue": "The Prague Bulletin of Mathematical Linguistics, 105(1):77\u201399, April.", "citeRegEx": "Eger et al\\.,? 2016", "shortCiteRegEx": "Eger et al\\.", "year": 2016}, {"title": "Designing and comparing g2p-type lemmatizers for a morphology-rich language", "author": ["Steffen Eger."], "venue": "Systems and Frameworks for Computational Morphology - Fourth International Workshop, SFCM 2015, Stuttgart, Germany, September 17-18, 2015, Proceedings, pages 27\u201340.", "citeRegEx": "Eger.,? 2015", "shortCiteRegEx": "Eger.", "year": 2015}, {"title": "Generalized Character-Level Spelling Error Correction", "author": ["Noura Farra", "Nadi Tomeh", "Alla Rozovskaya", "Nizar Habash."], "venue": "Proceedings of ACL \u201914, pages 161\u2013167, Baltimore, MD, USA. Association for Computational Linguistics.", "citeRegEx": "Farra et al\\.,? 2014", "shortCiteRegEx": "Farra et al\\.", "year": 2014}, {"title": "Morphological Inflection Generation Using Character Sequence to Sequence Learning", "author": ["Manaal Faruqui", "Yulia Tsvetkov", "Graham Neubig", "Chris Dyer."], "venue": "Proceedings of NAACL-HLT \u201916, pages 634\u2013643, San Diego, CA, USA. Association for Computational Linguistics.", "citeRegEx": "Faruqui et al\\.,? 2016", "shortCiteRegEx": "Faruqui et al\\.", "year": 2016}, {"title": "Incorporating copying mechanism in sequence-tosequence learning", "author": ["Jiatao Gu", "Zhengdong Lu", "Hang Li", "Victor O.K. Li."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016, August 7-12, 2016, Berlin, Germany, Volume 1: Long Papers.", "citeRegEx": "Gu et al\\.,? 2016", "shortCiteRegEx": "Gu et al\\.", "year": 2016}, {"title": "Improved Iterative Correction for Distant Spelling Errors", "author": ["Sergey Gubanov", "Irina Galinskaya", "Alexey Baytin."], "venue": "Proceedings of ACL \u201914, pages 168\u2013173, Baltimore, MD, USA. Association for Computational Linguistics.", "citeRegEx": "Gubanov et al\\.,? 2014", "shortCiteRegEx": "Gubanov et al\\.", "year": 2014}, {"title": "Integrating Joint n-gram Features into a Discriminative Training Framework", "author": ["Sittichai Jiampojamarn", "Colin Cherry", "Grzegorz Kondrak."], "venue": "Proceedings of NAACL-HLT \u201910, pages 697\u2013700, Los Angeles, CA, USA. Association for Computational Linguistics.", "citeRegEx": "Jiampojamarn et al\\.,? 2010", "shortCiteRegEx": "Jiampojamarn et al\\.", "year": 2010}, {"title": "The CMU Arctic speech databases", "author": ["John Kominek", "Alan W Black."], "venue": "Fifth ISCA Workshop on Speech Synthesis, pages 223\u2013224, Pittsburgh, PA, USA.", "citeRegEx": "Kominek and Black.,? 2004", "shortCiteRegEx": "Kominek and Black.", "year": 2004}, {"title": "Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data", "author": ["John Lafferty", "Andrew McCallum", "Fernando Pereira."], "venue": "Proceedings of ICML \u201901, pages 282\u2013289, San Francisco, CA, USA. Morgan Kaufmann Publishers Inc.", "citeRegEx": "Lafferty et al\\.,? 2001", "shortCiteRegEx": "Lafferty et al\\.", "year": 2001}, {"title": "Neural Network Recognition of Spelling Errors", "author": ["Mark Lewellen."], "venue": "Proceedings of ACL-COLING \u201998, pages 1490\u20131492, Montr\u00e9al, Qu\u00e9bec, Canada. Association for Computational Linguistics.", "citeRegEx": "Lewellen.,? 1998", "shortCiteRegEx": "Lewellen.", "year": 1998}, {"title": "Effective Approaches to Attention-based Neural Machine Translation", "author": ["Thang Luong", "Hieu Pham", "Christopher D. Manning."], "venue": "Proceedings of EMNLP \u201915, pages 1412\u20131421, Lisbon, Portugal. Association for Computational Linguistics.", "citeRegEx": "Luong et al\\.,? 2015", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Efficient Higher-Order CRFs for Morphological Tagging", "author": ["Thomas M\u00fcller", "Helmut Schmid", "Hinrich Sch\u00fctze."], "venue": "Proceedings of EMNLP \u201913, pages 322\u2013332, Seattle, WA, USA. Association for Computational Linguistics.", "citeRegEx": "M\u00fcller et al\\.,? 2013", "shortCiteRegEx": "M\u00fcller et al\\.", "year": 2013}, {"title": "Inflection generation as discriminative string transduction", "author": ["Garrett Nicolai", "Colin Cherry", "Grzegorz Kondrak."], "venue": "Proceedings of NAACL-HLT \u201915, pages 922\u2013931, Denver, CO, USA. Association for Computational Linguistics.", "citeRegEx": "Nicolai et al\\.,? 2015", "shortCiteRegEx": "Nicolai et al\\.", "year": 2015}, {"title": "WFST-Based Grapheme-to-Phoneme Conversion: Open Source tools for Alignment, Model-Building and Decoding", "author": ["Josef R. Novak", "Nobuaki Minematsu", "Keikichi Hirose."], "venue": "Proceedings of the 10th International Workshop on Finite State Methods and Natural Language Processing, pages 45\u201349, Donostia, Spain. Association for Computational Linguistics.", "citeRegEx": "Novak et al\\.,? 2012", "shortCiteRegEx": "Novak et al\\.", "year": 2012}, {"title": "A Discriminative Candidate Generator for String Transformations", "author": ["Naoaki Okazaki", "Yoshimasa Tsuruoka", "Sophia Ananiadou", "Jun\u2019ichi Tsujii"], "venue": "In Proceedings of EMNLP", "citeRegEx": "Okazaki et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Okazaki et al\\.", "year": 2008}, {"title": "A Deep Graphical Model for Spelling Correction", "author": ["Stephan Raaijmakers."], "venue": "Proceedings of the 25th Benelux Conference on Artificial Intelligence, pages 160\u2013167, Delft, Netherlands. Delft University of Technology.", "citeRegEx": "Raaijmakers.,? 2013", "shortCiteRegEx": "Raaijmakers.", "year": 2013}, {"title": "Grapheme-to-Phoneme Conversion Using Long Short-Term Memory Recurrent Neural Networks", "author": ["Kanishka Rao", "Fuchun Peng", "Hasim Sak", "Fran\u00e7oise Beaufays."], "venue": "Proceedings of ICASSP \u201915, pages 4225\u2013 4229, South Brisbane, QLD, Australia. Institute of Electrical and Electronics Engineers.", "citeRegEx": "Rao et al\\.,? 2015", "shortCiteRegEx": "Rao et al\\.", "year": 2015}, {"title": "Unsupervised Profiling of OCRed Historical Documents", "author": ["Ulrich Reffle", "Christoph Ringlstetter."], "venue": "Pattern Recognition, 46(5):1346\u20131357, May.", "citeRegEx": "Reffle and Ringlstetter.,? 2013", "shortCiteRegEx": "Reffle and Ringlstetter.", "year": 2013}, {"title": "On OCR Ground Truths and OCR Post-correction Gold Standards, Tools and Formats", "author": ["Martin Reynaert."], "venue": "Proceedings of DATeCH \u201914, pages 159\u2013166, New York, NY, USA. ACM.", "citeRegEx": "Reynaert.,? 2014", "shortCiteRegEx": "Reynaert.", "year": 2014}, {"title": "Robust LTS rules with the Combilex speech technology lexicon", "author": ["Korin Richmond", "Robert A.J. Clark", "Susan Fitt."], "venue": "Proceedings of INTERSPEECH \u201909, pages 1295\u20131298, Brighton, UK. ISCA.", "citeRegEx": "Richmond et al\\.,? 2009", "shortCiteRegEx": "Richmond et al\\.", "year": 2009}, {"title": "Sentence-Level Grammatical Error Identification as Sequence-to-Sequence Correction", "author": ["Allen Schmaltz", "Yoon Kim", "Alexander M Rush", "Stuart M Shieber."], "venue": "arXiv preprint arXiv:1604.04677.", "citeRegEx": "Schmaltz et al\\.,? 2016", "shortCiteRegEx": "Schmaltz et al\\.", "year": 2016}, {"title": "Substring-Based Transliteration", "author": ["Tarek Sherif", "Grzegorz Kondrak."], "venue": "Proceedings of ACL \u201907, pages 944\u2013951, Prague, Czech Republic. Association for Computational Linguistics.", "citeRegEx": "Sherif and Kondrak.,? 2007", "shortCiteRegEx": "Sherif and Kondrak.", "year": 2007}, {"title": "OCR of Historical Printings of Latin Texts: Problems, Prospects, Progress", "author": ["Uwe Springmann", "Dietmar Najock", "Hermann Morgenroth", "Helmut Schmid", "Annette Gotscharek", "Florian Fink."], "venue": "Proceedings of the First International Conference on Digital Access to Textual Cultural Heritage, pages 71\u201375, New York, NY, USA. ACM.", "citeRegEx": "Springmann et al\\.,? 2014", "shortCiteRegEx": "Springmann et al\\.", "year": 2014}, {"title": "Sequence to Sequence Learning with Neural Networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc VV Le."], "venue": "Proceedings of Neural Information Processing Systems 2014, pages 3104\u20133112, Montr\u00e9al, Qu\u00e9bec, Canada. Curran Associates, Inc.", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Pronunciation modeling for improved spelling correction", "author": ["Kristina Toutanova", "Robert C. Moore."], "venue": "Proceedings of ACL \u201902, pages 144\u2013151, Philadelphia, PA, USA. Association for Computational Linguistics.", "citeRegEx": "Toutanova and Moore.,? 2002", "shortCiteRegEx": "Toutanova and Moore.", "year": 2002}, {"title": "A Neural Conversational Model", "author": ["Oriol Vinyals", "Quoc Le."], "venue": "Proceedings of the 31st International Conference on Machine Learning, JMLR: W&CP, volume 37, Lille, France.", "citeRegEx": "Vinyals and Le.,? 2015", "shortCiteRegEx": "Vinyals and Le.", "year": 2015}, {"title": "Is it time to switch to Word Embedding and Recurrent Neural Networks for Spoken Language Understanding", "author": ["Vedran Vukotic", "Christian Raymond", "Guillaume Gravier"], "venue": null, "citeRegEx": "Vukotic et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Vukotic et al\\.", "year": 2015}, {"title": "Neural Language Correction with Character-Based Attention", "author": ["Ziang Xie", "Anand Avati", "Naveen Arivazhagan", "Dan Jurafsky", "Andrew Y Ng."], "venue": "arXiv preprint arXiv:1603.09727.", "citeRegEx": "Xie et al\\.,? 2016", "shortCiteRegEx": "Xie et al\\.", "year": 2016}, {"title": "A Probabilistic Approach to String Transformation", "author": ["Gu Xu", "Hang Li", "Ming Zhang", "Ziqi Wang."], "venue": "IEEE Transactions on Knowledge and Data Engineering, 26(5):1063\u20131075, May.", "citeRegEx": "Xu et al\\.,? 2014", "shortCiteRegEx": "Xu et al\\.", "year": 2014}, {"title": "Sequence-to-Sequence Neural Net Models for Grapheme-to-Phoneme Conversion", "author": ["Kaisheng Yao", "Geoffrey Zweig."], "venue": "Proceedings of INTERSPEECH \u201915, pages 3330\u20133334, Dresden, Germany. ISCA.", "citeRegEx": "Yao and Zweig.,? 2015", "shortCiteRegEx": "Yao and Zweig.", "year": 2015}, {"title": "Attention-Based Convolutional Neural Network for Machine Comprehension", "author": ["Wenpeng Yin", "Sebastian Ebert", "Hinrich Sch\u00fctze."], "venue": "Proceedings of the Workshop on Human-Computer Question Answering, pages 15\u201321, San Diego, CA, USA. Association for Computational Linguistics.", "citeRegEx": "Yin et al\\.,? 2016", "shortCiteRegEx": "Yin et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 33, "context": "Encoder-decoder neural models (Sutskever et al., 2014) are a generic deep-learning approach to sequence-to-sequence translation (Seq2Seq) tasks.", "startOffset": 30, "endOffset": 54}, {"referenceID": 5, "context": "These models have shown to achieve state-ofthe-art or at least highly competitive results for various NLP tasks including machine translation (Cho et al., 2014), conversation modeling (Vinyals and Le, 2015), question answering (Yin et al.", "startOffset": 142, "endOffset": 160}, {"referenceID": 35, "context": ", 2014), conversation modeling (Vinyals and Le, 2015), question answering (Yin et al.", "startOffset": 31, "endOffset": 53}, {"referenceID": 40, "context": ", 2014), conversation modeling (Vinyals and Le, 2015), question answering (Yin et al., 2016), and, more generally, language correction (Schmaltz et al.", "startOffset": 74, "endOffset": 92}, {"referenceID": 30, "context": ", 2016), and, more generally, language correction (Schmaltz et al., 2016; Xie et al., 2016).", "startOffset": 50, "endOffset": 91}, {"referenceID": 37, "context": ", 2016), and, more generally, language correction (Schmaltz et al., 2016; Xie et al., 2016).", "startOffset": 50, "endOffset": 91}, {"referenceID": 17, "context": "(2015) achieves an extremely low error rate on the CMUdict dataset (Kominek and Black, 2004), the neural architecture itself has a mediocre performance and only outperforms traditional models in combination with a weighted finite state transducer.", "startOffset": 67, "endOffset": 92}, {"referenceID": 39, "context": "Monotone Seq2Seq tasks such as morphological analysis/lemmatization, grapheme-to-phoneme conversion (G2P) (Yao and Zweig, 2015; Rao et al., 2015), transliteration (Sherif and Kondrak, 2007), and spelling correction (Brill and Moore, 2000) have been fundamental problem classes in natural language processing (NLP) ever since the origins of the field.", "startOffset": 106, "endOffset": 145}, {"referenceID": 26, "context": "Monotone Seq2Seq tasks such as morphological analysis/lemmatization, grapheme-to-phoneme conversion (G2P) (Yao and Zweig, 2015; Rao et al., 2015), transliteration (Sherif and Kondrak, 2007), and spelling correction (Brill and Moore, 2000) have been fundamental problem classes in natural language processing (NLP) ever since the origins of the field.", "startOffset": 106, "endOffset": 145}, {"referenceID": 31, "context": ", 2015), transliteration (Sherif and Kondrak, 2007), and spelling correction (Brill and Moore, 2000) have been fundamental problem classes in natural language processing (NLP) ever since the origins of the field.", "startOffset": 25, "endOffset": 51}, {"referenceID": 3, "context": ", 2015), transliteration (Sherif and Kondrak, 2007), and spelling correction (Brill and Moore, 2000) have been fundamental problem classes in natural language processing (NLP) ever since the origins of the field.", "startOffset": 77, "endOffset": 100}, {"referenceID": 1, "context": "We compare three variants of encoder-decoder models \u2014 including attention-based models (Bahdanau et al., 2014; Luong et al., 2015) and the model proposed by Faruqui et al.", "startOffset": 87, "endOffset": 130}, {"referenceID": 20, "context": "We compare three variants of encoder-decoder models \u2014 including attention-based models (Bahdanau et al., 2014; Luong et al., 2015) and the model proposed by Faruqui et al.", "startOffset": 87, "endOffset": 130}, {"referenceID": 3, "context": "These models have shown to achieve state-ofthe-art or at least highly competitive results for various NLP tasks including machine translation (Cho et al., 2014), conversation modeling (Vinyals and Le, 2015), question answering (Yin et al., 2016), and, more generally, language correction (Schmaltz et al., 2016; Xie et al., 2016). We have noticed that, given the enormous interest currently surrounding neural architectures, recent research appears to somewhat over-enthusiastically praise the performance of encoder-decoder approaches for Seq2Seq tasks. For example, while the encoder-decoder G2P model by Rao et al. (2015) achieves an extremely low error rate on the CMUdict dataset (Kominek and Black, 2004), the neural architecture itself has a mediocre performance and only outperforms traditional models in combination with a weighted finite state transducer.", "startOffset": 143, "endOffset": 625}, {"referenceID": 3, "context": "These models have shown to achieve state-ofthe-art or at least highly competitive results for various NLP tasks including machine translation (Cho et al., 2014), conversation modeling (Vinyals and Le, 2015), question answering (Yin et al., 2016), and, more generally, language correction (Schmaltz et al., 2016; Xie et al., 2016). We have noticed that, given the enormous interest currently surrounding neural architectures, recent research appears to somewhat over-enthusiastically praise the performance of encoder-decoder approaches for Seq2Seq tasks. For example, while the encoder-decoder G2P model by Rao et al. (2015) achieves an extremely low error rate on the CMUdict dataset (Kominek and Black, 2004), the neural architecture itself has a mediocre performance and only outperforms traditional models in combination with a weighted finite state transducer. Similarly, Faruqui et al. (2016) report on \u201cpar or better\u201d performance of their inflection generation neural architecture.", "startOffset": 143, "endOffset": 899}, {"referenceID": 1, "context": "We compare three variants of encoder-decoder models \u2014 including attention-based models (Bahdanau et al., 2014; Luong et al., 2015) and the model proposed by Faruqui et al. (2016) \u2014 to three very", "startOffset": 88, "endOffset": 179}, {"referenceID": 2, "context": "well-established baselines for monotone Seq2Seq, namely Sequitur (Bisani and Ney, 2008), DirecTL+ (Jiampojamarn et al.", "startOffset": 65, "endOffset": 87}, {"referenceID": 16, "context": "well-established baselines for monotone Seq2Seq, namely Sequitur (Bisani and Ney, 2008), DirecTL+ (Jiampojamarn et al., 2010), and Phonetisaurus (Novak et al.", "startOffset": 98, "endOffset": 125}, {"referenceID": 23, "context": ", 2010), and Phonetisaurus (Novak et al., 2012).", "startOffset": 27, "endOffset": 47}, {"referenceID": 21, "context": "For that purpose, we have adapted higher-order pruned conditional random fields (PCRFs) (M\u00fcller et al., 2013; Lafferty et al., 2001) to handle generic monotone Seq2Seq tasks.", "startOffset": 88, "endOffset": 132}, {"referenceID": 18, "context": "For that purpose, we have adapted higher-order pruned conditional random fields (PCRFs) (M\u00fcller et al., 2013; Lafferty et al., 2001) to handle generic monotone Seq2Seq tasks.", "startOffset": 88, "endOffset": 132}, {"referenceID": 32, "context": "OCR (optical character recognition) is the process of digitizing printed texts automatically, often applied to make text data from the pre-electronic age digitally available (Springmann et al., 2014).", "startOffset": 174, "endOffset": 199}, {"referenceID": 28, "context": "Depending on various factors including paper and scan quality, typeface, and OCR engine, OCR error rate can be extraordinarily high (Reynaert, 2014).", "startOffset": 132, "endOffset": 148}, {"referenceID": 3, "context": "Previous works in OCR post-correction apply noisy-channel models (Brill and Moore, 2000) and various extensions (Toutanova and Moore, 2002; Cucerzan and Brill, 2004; Gubanov et al.", "startOffset": 65, "endOffset": 88}, {"referenceID": 34, "context": "Previous works in OCR post-correction apply noisy-channel models (Brill and Moore, 2000) and various extensions (Toutanova and Moore, 2002; Cucerzan and Brill, 2004; Gubanov et al., 2014), generic string-to-string substitution models (Xu et al.", "startOffset": 112, "endOffset": 187}, {"referenceID": 8, "context": "Previous works in OCR post-correction apply noisy-channel models (Brill and Moore, 2000) and various extensions (Toutanova and Moore, 2002; Cucerzan and Brill, 2004; Gubanov et al., 2014), generic string-to-string substitution models (Xu et al.", "startOffset": 112, "endOffset": 187}, {"referenceID": 15, "context": "Previous works in OCR post-correction apply noisy-channel models (Brill and Moore, 2000) and various extensions (Toutanova and Moore, 2002; Cucerzan and Brill, 2004; Gubanov et al., 2014), generic string-to-string substitution models (Xu et al.", "startOffset": 112, "endOffset": 187}, {"referenceID": 38, "context": ", 2014), generic string-to-string substitution models (Xu et al., 2014), discriminative models (Okazaki et al.", "startOffset": 54, "endOffset": 71}, {"referenceID": 24, "context": ", 2014), discriminative models (Okazaki et al., 2008; Farra et al., 2014), and user-interactive approaches (Reffle and Ringlstetter, 2013).", "startOffset": 31, "endOffset": 73}, {"referenceID": 12, "context": ", 2014), discriminative models (Okazaki et al., 2008; Farra et al., 2014), and user-interactive approaches (Reffle and Ringlstetter, 2013).", "startOffset": 31, "endOffset": 73}, {"referenceID": 27, "context": ", 2014), and user-interactive approaches (Reffle and Ringlstetter, 2013).", "startOffset": 41, "endOffset": 72}, {"referenceID": 25, "context": "Neural network designs including auto-encoders (Raaijmakers, 2013) and recurrent neural networks (Chrupa\u0142a, 2014) were also investigated in previous works.", "startOffset": 47, "endOffset": 66}, {"referenceID": 6, "context": "Neural network designs including auto-encoders (Raaijmakers, 2013) and recurrent neural networks (Chrupa\u0142a, 2014) were also investigated in previous works.", "startOffset": 97, "endOffset": 113}, {"referenceID": 9, "context": "The task can be seen as the inverse to inflection generation (Durrett and DeNero, 2013; Ahlberg et al., 2014; Nicolai et al., 2015; Faruqui et al., 2016), where an inflected form is generated from a lemma plus an inflection tag.", "startOffset": 61, "endOffset": 153}, {"referenceID": 0, "context": "The task can be seen as the inverse to inflection generation (Durrett and DeNero, 2013; Ahlberg et al., 2014; Nicolai et al., 2015; Faruqui et al., 2016), where an inflected form is generated from a lemma plus an inflection tag.", "startOffset": 61, "endOffset": 153}, {"referenceID": 22, "context": "The task can be seen as the inverse to inflection generation (Durrett and DeNero, 2013; Ahlberg et al., 2014; Nicolai et al., 2015; Faruqui et al., 2016), where an inflected form is generated from a lemma plus an inflection tag.", "startOffset": 61, "endOffset": 153}, {"referenceID": 13, "context": "The task can be seen as the inverse to inflection generation (Durrett and DeNero, 2013; Ahlberg et al., 2014; Nicolai et al., 2015; Faruqui et al., 2016), where an inflected form is generated from a lemma plus an inflection tag.", "startOffset": 61, "endOffset": 153}, {"referenceID": 29, "context": "The Combilex data set (Richmond et al., 2009) provides mappings from English graphemes to phonetic representations (Table 1).", "startOffset": 22, "endOffset": 45}, {"referenceID": 9, "context": "For lemmatization, we use the Wiktionary Morphology Dataset (Durrett and DeNero, 2013).", "startOffset": 60, "endOffset": 86}, {"referenceID": 8, "context": "For lemmatization, we use the Wiktionary Morphology Dataset (Durrett and DeNero, 2013). The data set contains inflected forms for different languages and parts of speech, corresponding lemmas, and detailed inflection information, including mood, case, and tense. We conduct experiments on the German and Finnish verb datasets and further reduce the size of the latter by considering present tense indicative verb forms in active voice only. Note that our results are not comparable to the ones presented by Durrett and DeNero (2013), Ahlberg et al.", "startOffset": 61, "endOffset": 533}, {"referenceID": 0, "context": "Note that our results are not comparable to the ones presented by Durrett and DeNero (2013), Ahlberg et al. (2014), Nicolai et al.", "startOffset": 93, "endOffset": 115}, {"referenceID": 0, "context": "Note that our results are not comparable to the ones presented by Durrett and DeNero (2013), Ahlberg et al. (2014), Nicolai et al. (2015), and Faruqui et al.", "startOffset": 93, "endOffset": 138}, {"referenceID": 0, "context": "Note that our results are not comparable to the ones presented by Durrett and DeNero (2013), Ahlberg et al. (2014), Nicolai et al. (2015), and Faruqui et al. (2016) because we focus on lemmatization, not inflection generation, as mentioned.", "startOffset": 93, "endOffset": 165}, {"referenceID": 0, "context": "Note that our results are not comparable to the ones presented by Durrett and DeNero (2013), Ahlberg et al. (2014), Nicolai et al. (2015), and Faruqui et al. (2016) because we focus on lemmatization, not inflection generation, as mentioned. We do so because this produces less overhead \u2014 e.g., Faruqui et al. (2016) train 27 different systems for German verbs, one for each inflection type \u2014 and there is a priori not much difference in whether we transform an inflected form to a lemma or vice versa.", "startOffset": 93, "endOffset": 316}, {"referenceID": 5, "context": "\u2022 enc-dec: Encoder-decoder models using recurrent neural networks (RNNs) for Seq2Seq tasks were introduced by Cho et al. (2014) and Sutskever et al.", "startOffset": 110, "endOffset": 128}, {"referenceID": 5, "context": "\u2022 enc-dec: Encoder-decoder models using recurrent neural networks (RNNs) for Seq2Seq tasks were introduced by Cho et al. (2014) and Sutskever et al. (2014). The encoder reads an input ~x and", "startOffset": 110, "endOffset": 156}, {"referenceID": 1, "context": "\u2022 attn-enc-dec: We explore the attention-based encoder-decoder model proposed by Bahdanau et al. (2014) (Figure 1).", "startOffset": 81, "endOffset": 104}, {"referenceID": 1, "context": "Illustration from Bahdanau et al. (2014).", "startOffset": 18, "endOffset": 41}, {"referenceID": 13, "context": "\u2022 morph-trans: Faruqui et al. (2016) present a new encoder-decoder model designed for morphological inflection, proposing to feed the input sequence directly into the decoder.", "startOffset": 15, "endOffset": 37}, {"referenceID": 13, "context": "\u2022 morph-trans: Faruqui et al. (2016) present a new encoder-decoder model designed for morphological inflection, proposing to feed the input sequence directly into the decoder. This approach is motivated by the observation that input and output are usually very similar in problems such as morphological inflection. Similar ideas have been proposed in Gu et al. (2016) in their so-called \u201cCopyNet\u201d encoder-decoder model (which they apply to text summarization) that allows for portions of the input sequence to be simply copied to the output sequence, without modifications.", "startOffset": 15, "endOffset": 368}, {"referenceID": 13, "context": "For the tested neural models, we follow the same overall approach as Faruqui et al. (2016): we perform decoding and evaluation of the test data using an ensemble of k = 5 independently trained models in order to deal with the non-convex nature of the optimization problem of neural networks and the risk of", "startOffset": 69, "endOffset": 91}, {"referenceID": 7, "context": "running into a local optimum (Collobert et al., 2011).", "startOffset": 29, "endOffset": 53}, {"referenceID": 4, "context": "(2013) introduced pruned CRFs (PCRFs) that approximate the CRF objective function using coarse-to-fine decoding (Charniak and Johnson, 2005).", "startOffset": 112, "endOffset": 140}, {"referenceID": 17, "context": "Conditional random fields (CRFs) were introduced by Lafferty et al. (2001) and have been a major workhorse for many sequence labeling tasks such as part-of-speech tagging and named entity recognition during the 2000s.", "startOffset": 52, "endOffset": 75}, {"referenceID": 17, "context": "Conditional random fields (CRFs) were introduced by Lafferty et al. (2001) and have been a major workhorse for many sequence labeling tasks such as part-of-speech tagging and named entity recognition during the 2000s. Unfortunately, training and decoding time depend polynomially on the tag set size and exponentially on the order of the CRF. Here, order refers to the dependencies on the label side. This makes higher-order CRFs impractical for large training data sizes, which is the reason why virtually only first-order (linear chain) CRFs were used until recently. M\u00fcller et al. (2013) introduced pruned CRFs (PCRFs) that approximate the CRF objective function using coarse-to-fine decoding (Charniak and Johnson, 2005).", "startOffset": 52, "endOffset": 591}, {"referenceID": 4, "context": "(2013) introduced pruned CRFs (PCRFs) that approximate the CRF objective function using coarse-to-fine decoding (Charniak and Johnson, 2005). PCRFs require much shorter runtime and are thus able to make use of higher orders. Higher orders, in turn, have been shown to be highly beneficial for coarse and fine-grained part-of-speech tagging, outperforming first-order models. For our tasks, we have adapted the implementation from M\u00fcller et al. (2013) \u2014 originally designed for sequence labeling \u2014 to general monotone Seq2Seq tasks.", "startOffset": 113, "endOffset": 451}, {"referenceID": 16, "context": "More sophisticated \u2018joint\u2019 approaches (Jiampojamarn et al., 2010) are considerably more computationally expensive.", "startOffset": 38, "endOffset": 65}, {"referenceID": 10, "context": "Considering the similarity of G2P conversion, spelling correction, and lemmatization with regard to their innate monotonicity (Eger et al., 2016; Nicolai et al., 2015; Eger, 2015), we explore for all our datasets three further approaches that were originally designed for G2P conversion.", "startOffset": 126, "endOffset": 179}, {"referenceID": 22, "context": "Considering the similarity of G2P conversion, spelling correction, and lemmatization with regard to their innate monotonicity (Eger et al., 2016; Nicolai et al., 2015; Eger, 2015), we explore for all our datasets three further approaches that were originally designed for G2P conversion.", "startOffset": 126, "endOffset": 179}, {"referenceID": 11, "context": "Considering the similarity of G2P conversion, spelling correction, and lemmatization with regard to their innate monotonicity (Eger et al., 2016; Nicolai et al., 2015; Eger, 2015), we explore for all our datasets three further approaches that were originally designed for G2P conversion.", "startOffset": 126, "endOffset": 179}, {"referenceID": 2, "context": "Sequitur (Bisani and Ney, 2008) is a \u2018joint\u2019 model for Seq2Seq in the sense of the classic distinction between joint and discriminative models.", "startOffset": 9, "endOffset": 31}, {"referenceID": 16, "context": "DirecTL+ (Jiampojamarn et al., 2010) is a discriminative model for monotone Seq2Seq that integrates joint n-gram features.", "startOffset": 9, "endOffset": 36}, {"referenceID": 23, "context": "Phonetisaurus (Novak et al., 2012) implements a weighted finite state transducer (WFST) to align input and output tokens.", "startOffset": 14, "endOffset": 34}, {"referenceID": 13, "context": "Finnish vowel harmony makes a vowel control other vowels in the word, potentially across multiple syllables; see Faruqui et al. (2016) for more detailed explanation.", "startOffset": 113, "endOffset": 135}, {"referenceID": 1, "context": "At first sight, our analyses based on string lengths are in line with those reported by Bahdanau et al. (2014). They state that \u2014 for the field of machine translation \u2014 the attention mechanism leads to improvements over the standard encoder-decoder model on longer sentences.", "startOffset": 88, "endOffset": 111}, {"referenceID": 1, "context": "At first sight, our analyses based on string lengths are in line with those reported by Bahdanau et al. (2014). They state that \u2014 for the field of machine translation \u2014 the attention mechanism leads to improvements over the standard encoder-decoder model on longer sentences. We also observe this positive impact for our tasks, where the attention-based mechanism alleviates the drastic performance drop of the standard encoder-decoder models on long sequences to some extent. At the same time, we see that very performance drop persisting \u2014 CRFs still outperform encoder-decoder models on long sequences, even when employing attention-mechanisms. As described in Section 5.3, neural models are only able to successfully compete when more complex phenomena occur, on which traditional models fail. Nevertheless, previous works such as Vukotic et al. (2015) also indicate that even in more complex sequence labeling tasks such as spoken language understanding, neural networks are not guaranteed to outperform CRFs.", "startOffset": 88, "endOffset": 857}, {"referenceID": 1, "context": "At first sight, our analyses based on string lengths are in line with those reported by Bahdanau et al. (2014). They state that \u2014 for the field of machine translation \u2014 the attention mechanism leads to improvements over the standard encoder-decoder model on longer sentences. We also observe this positive impact for our tasks, where the attention-based mechanism alleviates the drastic performance drop of the standard encoder-decoder models on long sequences to some extent. At the same time, we see that very performance drop persisting \u2014 CRFs still outperform encoder-decoder models on long sequences, even when employing attention-mechanisms. As described in Section 5.3, neural models are only able to successfully compete when more complex phenomena occur, on which traditional models fail. Nevertheless, previous works such as Vukotic et al. (2015) also indicate that even in more complex sequence labeling tasks such as spoken language understanding, neural networks are not guaranteed to outperform CRFs. The task-specific extensions to the encoder-decoder proposed by Faruqui et al. (2016) have been shown to produce mostly bad results in our settings.", "startOffset": 88, "endOffset": 1101}, {"referenceID": 1, "context": "At first sight, our analyses based on string lengths are in line with those reported by Bahdanau et al. (2014). They state that \u2014 for the field of machine translation \u2014 the attention mechanism leads to improvements over the standard encoder-decoder model on longer sentences. We also observe this positive impact for our tasks, where the attention-based mechanism alleviates the drastic performance drop of the standard encoder-decoder models on long sequences to some extent. At the same time, we see that very performance drop persisting \u2014 CRFs still outperform encoder-decoder models on long sequences, even when employing attention-mechanisms. As described in Section 5.3, neural models are only able to successfully compete when more complex phenomena occur, on which traditional models fail. Nevertheless, previous works such as Vukotic et al. (2015) also indicate that even in more complex sequence labeling tasks such as spoken language understanding, neural networks are not guaranteed to outperform CRFs. The task-specific extensions to the encoder-decoder proposed by Faruqui et al. (2016) have been shown to produce mostly bad results in our settings. This is particularly surprising for the OCR data, for which input and output sequences are usually very similar, so that we had expected that re-feeding the input to the decoder should be equally beneficial in that domain. As discussed, one explanation might be that OCR, or spelling correction generally, putatively exhibits few long-range dependencies. This might explain why the morph-trans approach works quite well and competitive in morphological analysis tasks, as re-confirmed in our experiments. Thus, long-range dependencies might actually be a more crucial aspect for the performance of the model presented by Faruqui et al. (2016) than the similarity between input and output sequence.", "startOffset": 88, "endOffset": 1807}], "year": 2016, "abstractText": "We analyze the performance of encoder-decoder neural models and compare them with wellknown established methods. The latter represent different classes of traditional approaches that are applied to the monotone sequence-to-sequence tasks OCR post-correction, spelling correction, grapheme-to-phoneme conversion, and lemmatization. Such tasks are of practical relevance for various higher-level research fields including digital humanities, automatic text correction, and speech recognition. We investigate how well generic deep-learning approaches adapt to these tasks, and how they perform in comparison with established and more specialized methods, including our own adaptation of pruned CRFs.", "creator": "dvips(k) 5.991 Copyright 2011 Radical Eye Software"}}}