{"id": "1610.07809", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Oct-2016", "title": "How Document Pre-processing affects Keyphrase Extraction Performance", "abstract": "The SemEval-2010 benchmark dataset has brought renewed attention to the task of automatic keyphrase extraction. This dataset is made up of scientific articles that were automatically converted from PDF format to plain text and thus require careful preprocessing so that irrevelant spans of text do not negatively affect keyphrase extraction performance. In previous work, a wide range of document preprocessing techniques were described but their impact on the overall performance of keyphrase extraction models is still unexplored. Here, we re-assess the performance of several keyphrase extraction models and measure their robustness against increasingly sophisticated levels of document preprocessing.", "histories": [["v1", "Tue, 25 Oct 2016 09:59:13 GMT  (28kb)", "http://arxiv.org/abs/1610.07809v1", "Accepted at the COLING 2016 Workshop on Noisy User-generated Text (WNUT)"]], "COMMENTS": "Accepted at the COLING 2016 Workshop on Noisy User-generated Text (WNUT)", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["florian boudin", "hugo mougard", "damien cram"], "accepted": false, "id": "1610.07809"}, "pdf": {"name": "1610.07809.pdf", "metadata": {"source": "CRF", "title": "How Document Pre-processing affects Keyphrase Extraction Performance", "authors": ["Florian Boudin"], "emails": ["firstname.lastname@univ-nantes.fr"], "sections": [{"heading": null, "text": "ar Xiv: 161 0.07 809v 1 [cs.C L] 25 Oct 201 6The SemEval-2010 benchmark dataset has once again focused attention on the task of automatic keyphrase extraction, which consists of scientific articles that have been automatically converted from PDF to plaintext and therefore require careful pre-processing so that unclear passages of text do not adversely affect keyphrase extraction. Previous work has described a wide range of document pre-processing techniques, but their impact on the overall performance of keyphrase extraction models is still unexplored."}, {"heading": "1 Introduction", "text": "In recent years, we have seen an increase in interest in automatic keyphrase extraction, thanks to the availability of SemEval 2010 benchmark data sets (Kim et al., 2010), which consist of documents (scientific articles) that have been automatically converted from PDF format to plain text. As a result, most documents contain irrelevant pieces of text (e.g. confused sentences, equations, footnotes) that require special handling so as not to impede the performance of keyphrase extraction systems. In previous work, these are removed in the pre-processing stage, but with a variety of techniques ranging from simple heuristics (Wang and Li, 2010; Treeratpituk et al., 2010) to complex documents that restore logical structure capture in richly formatted documents (Nguyen and Luong, 2010)."}, {"heading": "2 Dataset and Preprocessing", "text": "The SemEval 2010 benchmark dataset (Kim et al., 2010) is composed of 244 scientific articles collected by the ACM Digital Library (conference and seminar papers).The input papers range from 6 to 8 pages and have therefore been converted from a PDF format to plain text using a standard selection of words from the shelf Tool3.The only pre-processing that is used is a systematic decoding in line 4 and the removal of keyphrases assigned by the author. Scientific articles were selected from four different research areas defined in the ACM classification, and were equally distributed in training (144 articles) and testing (100 articles).Gold standard keyphrases are composed of keyphrases assigned to both authors, collected from the original PDF files and reader-associated keyphrases. Long documents such as the ones used in the SemEval 2010 benchmark dataset systems are difficult to handle due to the large number of keyphrases that are eligible for keyphrases (i.e.)."}, {"heading": "3 Keyphrase Extraction Models", "text": "We have newly implemented five keyphrase extraction models: the first two keyphrases are commonly used as baselines, the third is a resource-efficient, uncontrolled graph-based ranking, and the last two were among the top-performing systems in the SemEval 2010 keyphrase extraction task (Kim et al., 2010). We point out that two of the systems are monitored and rely on training to build their classification models. The number of documents calculated by task organizers is also applied to training to enable more robust matching models. The various keyphrase extraction models are briefly described below: TF \u00d7 IDF: We implement the TF \u00d7 IDF-gram n-gram based on the task description. We use 1, 3-grams as keyphrase candidates and film these shorter than 3 characters containing words that are only punctuation markers or a keyphrase 1999 or a character (Kelongal, 1)."}, {"heading": "4 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Experimental settings", "text": "We follow the evaluation method used in the SemEval 2010 competition and rate the performance of each model with respect to f-measure (F) at the top N-keyphrases18. We use the combination of author and reader-assigned keyphrases as reference keyphrases. Extracted keyphrases and reference keyphrases are constrained to reduce the number of mismatches."}, {"heading": "4.2 Results", "text": "Overall, we observe a significant increase in the performance of all Level 3 models, which confirms that document pre-processing plays an important role in pre-processing performance, and the difference in the f-score between models, measured by the standard deviation \u03c31, gradually decreases as the level of pre-processing increases. This result reinforces the assumption made in this paper that the performance discrepancy between models is partly a function of the effectiveness of document pre-processing. Slightly surprisingly, the order of the two best models at Level 3 reverses, showing that rankings are strongly influenced by the pre-processing phase, despite the widespread lack of detail and analysis affecting it in explanatory papers. We also point out that the most powerful model, namely KP-Miner, is not supervised, which affects the results of the (Hasan and Ng, 2014) pre-processing phase results, suggesting that recent uncontrolled approaches compete with their counterparts."}, {"heading": "4.3 Reproducibility", "text": "The ability to reproduce experimental results is a central aspect of the scientific method. While we assessed the importance of the pre-processing phase for five approaches, we found that several results were not reproducible, as shown in Table 5.We note that trends in baselines and high-level systems are contrary: compared to the published results, our reproduction of peak systems is below average and our reproduction of baselines above average. We suspect that this is due to a difference in hyperparameter setting, including the one implicating the pre-processing phase. We also find that competitors have strong incentives to optimize hyperparameters correctly, achieve a high ranking and get more publicity for their work, while competition organizers may have the opposite incentive: A too strong baseline cannot be considered as a baseline. We also find that the gap between baselines and peak systems in this level-upgraded pre-processing stage reduces the importance of raw anchoring, which in turn is much smaller to interpret the common task."}, {"heading": "5 Additional experiments", "text": "In the previous sections, we provided empirical evidence that document pre-processing strongly influences the outcome of keyword extraction models, which raises the question of whether further improvements can be achieved through more aggressive pre-processing. To answer this question, we go a step further than filtering content and shorten the input text from pre-processed Level 3 documents using an unattended summary technique, selecting only the most content-intensive sentences from the remaining contents. To achieve this, sentences are filtered using TextRank (Mihalcea and Tarau, 2004) and the less informative sentences determined by their TextRank values, which are normalized by their length in words. However, finding the optimal subset of sentences from already shortened documents is not a trivial task as the number of discarded sentences decreases."}, {"heading": "6 Conclusion", "text": "In this study, we re-evaluated the performance of multiple keyphrase extraction models and showed that performance variations between models are partly a function of the effectiveness of document pre-processing. Our results also suggest that monitored keyphrase extraction models are more robust than loud inputs. Given our findings, we recommend that future work use joint pre-processing to evaluate the interest of keyphrase extraction approaches. Therefore, we make the four levels of pre-processing used in this study available to the community."}], "references": [{"title": "Reducing over-generation errors for automatic keyphrase extraction using integer linear programming", "author": ["Florian Boudin"], "venue": "In Proceedings of the ACL 2015 Workshop on Novel Computational Approaches to Keyphrase Extraction,", "citeRegEx": "Boudin.,? \\Q2015\\E", "shortCiteRegEx": "Boudin.", "year": 2015}, {"title": "pke: an open source python-based keyphrase extraction toolkit", "author": ["Florian Boudin"], "venue": "In Proceedings of COLING 2016: System Demonstrations,", "citeRegEx": "Boudin.,? \\Q2016\\E", "shortCiteRegEx": "Boudin.", "year": 2016}, {"title": "Topicrank: Graph-based topic ranking for keyphrase extraction", "author": ["Florian Boudin", "B\u00e9atrice Daille"], "venue": "In Proceedings of the Sixth International Joint Conference on Natural Language Processing,", "citeRegEx": "Bougouin et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bougouin et al\\.", "year": 2013}, {"title": "Dfki keywe: Ranking keyphrases extracted from scientific articles", "author": ["Eichler", "Neumann2010] Kathrin Eichler", "G\u00fcnter Neumann"], "venue": "In Proceedings of the 5th International Workshop on Semantic Evaluation,", "citeRegEx": "Eichler et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Eichler et al\\.", "year": 2010}, {"title": "Kp-miner: Participation in semeval", "author": ["El-Beltagy", "Rafea2010] Samhaa R. El-Beltagy", "Ahmed Rafea"], "venue": null, "citeRegEx": "El.Beltagy et al\\.,? \\Q2010\\E", "shortCiteRegEx": "El.Beltagy et al\\.", "year": 2010}, {"title": "Automatic keyphrase extraction: A survey of the state of the art", "author": ["Hasan", "Ng2014] Kazi Saidul Hasan", "Vincent Ng"], "venue": "In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),", "citeRegEx": "Hasan et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Hasan et al\\.", "year": 2014}, {"title": "Logical structure recovery in scholarly articles with rich document features", "author": ["Kan et al.2010] Min-Yen Kan", "Minh-Thang Luong", "Thuy Dung Nguyen"], "venue": "Int. J. Digit. Library Syst.,", "citeRegEx": "Kan et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Kan et al\\.", "year": 2010}, {"title": "Re-examining automatic keyphrase extraction approaches in scientific articles", "author": ["Kim", "Kan2009] Su Nam Kim", "Min-Yen Kan"], "venue": "In Proceedings of the Workshop on Multiword Expressions: Identification, Interpretation, Disambiguation and Applications,", "citeRegEx": "Kim et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Kim et al\\.", "year": 2009}, {"title": "Semeval-2010 task 5 : Automatic keyphrase extraction from scientific articles", "author": ["Kim et al.2010] Su Nam Kim", "Olena Medelyan", "Min-Yen Kan", "Timothy Baldwin"], "venue": "In Proceedings of the 5th International Workshop on Semantic Evaluation,", "citeRegEx": "Kim et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Kim et al\\.", "year": 2010}, {"title": "Humb: Automatic key term extraction from scientific articles in grobid", "author": ["Lopez", "Romary2010] Patrice Lopez", "Laurent Romary"], "venue": "In Proceedings of the 5th International Workshop on Semantic Evaluation,", "citeRegEx": "Lopez et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Lopez et al\\.", "year": 2010}, {"title": "The stanford corenlp natural language processing toolkit", "author": ["Mihai Surdeanu", "John Bauer", "Jenny Finkel", "Steven Bethard", "David McClosky"], "venue": "In Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations,", "citeRegEx": "Manning et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Manning et al\\.", "year": 2014}, {"title": "Textrank: Bringing order into texts", "author": ["Mihalcea", "Tarau2004] Rada Mihalcea", "Paul Tarau"], "venue": "Proceedings of EMNLP", "citeRegEx": "Mihalcea et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Mihalcea et al\\.", "year": 2004}, {"title": "Wingnus: Keyphrase extraction utilizing document logical structure", "author": ["Nguyen", "Luong2010] Thuy Dung Nguyen", "Minh-Thang Luong"], "venue": "In Proceedings of the 5th International Workshop on Semantic Evaluation,", "citeRegEx": "Nguyen et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Nguyen et al\\.", "year": 2010}, {"title": "Seerlab: A system for extracting keyphrases from scholarly documents", "author": ["Pradeep Teregowda", "Jian Huang", "C. Lee Giles"], "venue": "In Proceedings of the 5th International Workshop on Semantic Evaluation,", "citeRegEx": "Treeratpituk et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Treeratpituk et al\\.", "year": 2010}, {"title": "Sjtultlab: Chunk based method for keyphrase extraction", "author": ["Wang", "Li2010] Letian Wang", "Fang Li"], "venue": "In Proceedings of the 5th International Workshop on Semantic Evaluation,", "citeRegEx": "Wang et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2010}, {"title": "How preprocessing affects unsupervised keyphrase extraction", "author": ["Wang et al.2014] Rui Wang", "Wei Liu", "Chris Mcdonald"], "venue": "In Proceedings of the 15th International Conference on Computational Linguistics and Intelligent Text Processing - Volume 8403,", "citeRegEx": "Wang et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2014}, {"title": "Kea: Practical automatic keyphrase extraction", "author": ["Witten et al.1999] Ian H. Witten", "Gordon W. Paynter", "Eibe Frank", "Carl Gutwin", "Craig G. Nevill-Manning"], "venue": "In Proceedings of the Fourth ACM Conference on Digital Libraries, DL", "citeRegEx": "Witten et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Witten et al\\.", "year": 1999}, {"title": "Uvt: The uvt term extraction system in the keyphrase extraction task", "author": ["Kalliopi Zervanou"], "venue": "In Proceedings of the 5th International Workshop on Semantic Evaluation,", "citeRegEx": "Zervanou.,? \\Q2010\\E", "shortCiteRegEx": "Zervanou.", "year": 2010}], "referenceMentions": [{"referenceID": 8, "context": "Recent years have seen a surge of interest in automatic keyphrase extraction, thanks to the availability of the SemEval-2010 benchmark dataset (Kim et al., 2010).", "startOffset": 143, "endOffset": 161}, {"referenceID": 13, "context": "In previous work, these are usually removed at the preprocessing step, but using a variety of techniques ranging from simple heuristics (Wang and Li, 2010; Treeratpituk et al., 2010; Zervanou, 2010) to sophisticated document logical structure detection on richly-formatted documents recovered from Google Scholar (Nguyen and Luong, 2010).", "startOffset": 136, "endOffset": 198}, {"referenceID": 17, "context": "In previous work, these are usually removed at the preprocessing step, but using a variety of techniques ranging from simple heuristics (Wang and Li, 2010; Treeratpituk et al., 2010; Zervanou, 2010) to sophisticated document logical structure detection on richly-formatted documents recovered from Google Scholar (Nguyen and Luong, 2010).", "startOffset": 136, "endOffset": 198}, {"referenceID": 1, "context": "\u2022 We release both a new version of the SemEval-2010 dataset1 with preprocessed documents and our implementation of the state-of-the-art keyphrase extraction models2 using the pke toolkit (Boudin, 2016) for use by the community.", "startOffset": 187, "endOffset": 201}, {"referenceID": 8, "context": "The SemEval-2010 benchmark dataset (Kim et al., 2010) is composed of 244 scientific articles collected from the ACM Digital Library (conference and workshop papers).", "startOffset": 35, "endOffset": 53}, {"referenceID": 10, "context": "Level 1: We process each input file with the Stanford CoreNLP suite (Manning et al., 2014)5.", "startOffset": 68, "endOffset": 90}, {"referenceID": 6, "context": "We then extract the enriched7 textual content of the PDF files using an Optical Character Recognition (OCR) system8, and perform document logical structure detection using ParsCit (Kan et al., 2010)9.", "startOffset": 180, "endOffset": 198}, {"referenceID": 13, "context": "Level 3: As pointed out by (Treeratpituk et al., 2010; Nguyen and Luong, 2010; Wang and Li, 2010; Eichler and Neumann, 2010; El-Beltagy and Rafea, 2010), considering only the keyphrase dense parts of the scientific articles allows to improve keyphrase extraction performance.", "startOffset": 27, "endOffset": 152}, {"referenceID": 8, "context": "We re-implemented five keyphrase extraction models : the first two are commonly used as baselines, the third is a resource-lean unsupervised graph-based ranking approach, and the last two were among the top performing systems in the SemEval-2010 keyphrase extraction task (Kim et al., 2010).", "startOffset": 272, "endOffset": 290}, {"referenceID": 16, "context": "Kea (Witten et al., 1999): keyphrase candidates are 1, 2, 3-grams that do not begin or end with a stopword14.", "startOffset": 4, "endOffset": 25}, {"referenceID": 2, "context": "TopicRank (Bougouin et al., 2013): keyphrase candidates are the longest sequences of adjacent nouns and adjectives.", "startOffset": 10, "endOffset": 33}, {"referenceID": 15, "context": "For details on how candidate selection methods affect keyphrase extraction, please refer to (Wang et al., 2014).", "startOffset": 92, "endOffset": 111}, {"referenceID": 0, "context": "Yet, recent work has shown that up to 12% of the overall error made by state-of-the-art keyphrase extraction systems were due to redundancy (Hasan and Ng, 2014; Boudin, 2015).", "startOffset": 140, "endOffset": 174}, {"referenceID": 2, "context": "Table 5: Difference in f-score between our re-implementation and the original scores reported in (Hasan and Ng, 2014; Bougouin et al., 2013).", "startOffset": 97, "endOffset": 140}], "year": 2016, "abstractText": "The SemEval-2010 benchmark dataset has brought renewed attention to the task of automatic keyphrase extraction. This dataset is made up of scientific articles that were automatically converted from PDF format to plain text and thus require careful preprocessing so that irrevelant spans of text do not negatively affect keyphrase extraction performance. In previous work, a wide range of document preprocessing techniques were described but their impact on the overall performance of keyphrase extraction models is still unexplored. Here, we re-assess the performance of several keyphrase extraction models and measure their robustness against increasingly sophisticated levels of document preprocessing.", "creator": "LaTeX with hyperref package"}}}