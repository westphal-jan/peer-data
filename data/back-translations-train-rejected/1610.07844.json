{"id": "1610.07844", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Oct-2016", "title": "Improving historical spelling normalization with bi-directional LSTMs and multi-task learning", "abstract": "Natural-language processing of historical documents is complicated by the abundance of variant spellings and lack of annotated data. A common approach is to normalize the spelling of historical words to modern forms. We explore the suitability of a deep neural network architecture for this task, particularly a deep bi-LSTM network applied on a character level. Our model compares well to previously established normalization algorithms when evaluated on a diverse set of texts from Early New High German. We show that multi-task learning with additional normalization data can improve our model's performance further.", "histories": [["v1", "Tue, 25 Oct 2016 12:30:26 GMT  (25kb)", "http://arxiv.org/abs/1610.07844v1", "Accepted to COLING 2016"]], "COMMENTS": "Accepted to COLING 2016", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["marcel bollmann", "anders s{\\o}gaard"], "accepted": false, "id": "1610.07844"}, "pdf": {"name": "1610.07844.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["bollmann@linguistics.rub.de", "soegaard@hum.ku.dk"], "sections": [{"heading": null, "text": "ar Xiv: 161 0.07 844v 1 [cs.C L] 25 Oct 201 6The processing of historical documents in natural language is complicated by the abundance of different spellings and the absence of annotated data. A common approach is to normalize the spelling of historical words into modern forms. We are investigating the suitability of a deep neural network architecture for this task, in particular a deep bi-LSTM network at the character level. Our model compares well with previously established normalization algorithms when evaluated on a variety of texts from early New High German. We show that multi-task learning with additional normalization data can further improve the performance of our model."}, {"heading": "1 Introduction", "text": "Interest in the computerized processing of historical documents is on the rise, as evidenced by the growing field of digital humanities and the increasing number of digitally available resources of historical data. Spelling normatification, i.e. the mapping of historical spelling variants to standardized / modernized forms, is often used as a pre-processing step to enable the use of existing tools for the respective modern target language (Piotrowski, 2012). Training data for supervised learning of spelling normalization is usually scarce in the historical realm. In addition, dialectical influences and even individual preferences of an author can have a huge impact on the spelling properties of a particular text, meaning that even training data from other companies of the same language and time span cannot always be reliably used. Algorithms have often been developed with this fact in mind, for example by being based on a phonetic, graphic or semantic similarity."}, {"heading": "2 Datasets", "text": "We use a total of 44 texts from the Anselm corpus (Dipper and Schultz-Balluff, 2013) of early New High Germany. [1] The corpus is a collection of several manuscripts and prints of the same core text, a religious treatise. Although the texts are semi-parallel and have a certain vocabulary in common, they were written in different periods (between the 14th and 16th centuries) as well as in different dialectical regions and have quite different spelling properties. Thus, the New German word Frau \"Frau\" can be written as fraw / vraw (I), frawe (N2), frauwe (St), frau \ufffd we (B2), frow (Stu), vrowe (Ka), vorwe (Sa) or vrouwe (B), among others. [2] All texts in the Anselm corpus are manually provided with gold-standard normalizations that follow the guidelines described in Krasselt."}, {"heading": "2.1 Conversion to labeled character sequences", "text": "Normalization is commented on at a word level; to rephrase the problem as a character-based sequence marking task, we need to align historical word forms and their normalizations at a character level. Ideally, we want these alignments to be linguistically plausible, i.e., that characters most likely match each other (e.g. historical j and modern i, as in jn - him \"him\") whenever possible. The Levenshtein algorithm (Levenshtein, 1966) can be used to produce alignments that preferably align identical letters but are ambiguous when multiple alignments exist with the same Levenshtein distance. Therefore, we use iterated Levenshtein distance alignment (Wieling et al., 2009), which uses sign-related information about aligned segments to estimate statistical dependence, and prefers alignments of letters that frequently occur within the given string of characters."}, {"heading": "3 Model", "text": "Our model architecture consists of: (i) an embedding layer for the input characters; (ii) a stack of bi-directional long-term storage units (bi-LSTMs); and (iii) a final dense layer with a softmax activation at 1https: / / www.linguistics.rub.de / anselm / 2Abbreviations in brackets refer to individual texts that use the same internal IDs found in the Anselm corpus. We have not used pre-formed embedding; the embedding is randomly initialized and learned as part of the regular network training model; the embedding layer forms a hot input vector (representing historical characters) to condense vectors."}, {"heading": "3.1 Multi-task learning setup", "text": "In Multi-Task Learning (MTL), the performance of a model on a given task is improved by additional training on one or more auxiliary tasks (Caruana, 1993). For our Bi-LSTM model, this means that all layers of the model are divided between the tasks, except the final prediction level, which remains separated for the main and auxiliary tasks. Thus, errors in an auxiliary task propagated backwards through the network also have an effect on the prediction of the main task, which helps to regulate the weights of the network and prevent matching. Multi-task learning with (deep) neural network architectures has been shown to be effective for a variety of NLP tasks, such as part-of-speech tagging, chunking, the so-called entity recognition (Collobert et al., 2011); sentence compression with (Kleret al., 2016), or the translation of a given task (in our normal)."}, {"heading": "3.2 Hyperparameters", "text": "We left aside one of the texts (B) from the Anselm corpus to test different hyperparameter configurations. On this text, we obtained the best results with a dimensionality of 128 for the embedding and bi-LSTM layers, using a dropout of 0.1 and training the model for 30 iterations, which were then used for all subsequent experiments."}, {"heading": "3.3 Other models used for comparison", "text": "For comparison, we also train and evaluate using the Norma tool described by Bollmann (2012), since it was originally developed for the Anselm corpus and its implementation is publicly available.3 In fact, however, Norma consists of a combination of three different normalization methods, one of which is a simple wordlist mapping of historical symbols to normalized shapes. Since this wordlist mapping is conceptually very simple and could easily be added to our (or any other) normalization method, we exclude it for comparison and use only Norma's remaining two algorithms (which we call the norma \u043a). Moreover, since we present the problem as a sequence marking task, we compare our results with a simple sequence marking model using conditional random fields (CRF). The CRF model receives the same input / output sequences as our Bi-LSTM model (cf. Sec. 2.1) and uses the two preceding characters and the following wording form as additional octaves (the historical octave)."}, {"heading": "4 Evaluation", "text": "We evaluate our model separately for each text in our dataset. Of each text, we use 1,000 tokens as a rating set, set another 1,000 tokens aside as a development set (which was not used at the time), and train on the remaining tokens (between 2,000 and 11,000 depending on the text). Both CRF and our Bi-LSTM model get their input as strings (as described in paragraph 2.1), while Norma requires full words as input, while we randomly select the text we currently rate as our main task from all Anselm texts and consider each text to be its own task. Effectively, we learn a common model of all Anselm texts with common parameters but different prediction stories, while we consider the text we currently rate as our main task, and the others as auxiliary tasks. The MTL setup applies only to our Bi-LSTM model, as the auxiliary task is that we compare the normalization of this data with the other training data (where we could compare the other methods)."}, {"heading": "4.1 Word accuracy", "text": "Evaluation of the choice of words can be found in Table 1."}, {"heading": "4.2 Effect of training set size", "text": "s rank correlation coefficient (\u03c1) between the size of the training sets and the normalization accuracy for each column in Table 1. We do not find a significant correlation for the CRF and Bi-LSTM models (| \u03c1 | < 0.25), although there usually seems to be a moderate inverse correlation for the Norma results (\u03c1 \u2212 0.48 on Norma \"S\"). The reasons for this are outside the scope of this paper, considering how much training data is needed to effectively train a model is particularly relevant for historical spelling normalization, as training data in this area can be very sparse."}, {"heading": "4.3 Multi-task learning with grapheme-to-phoneme mappings", "text": "It is conceivable to use tasks other than historical spelling as an auxiliary task in a multi-task learning setup, as it can be considered a similar form of character-based sequence transmission. We used the German part of the lexical database CELEX (Baayen et al., 1995) for our data set, in particular the database of phonetic transcriptions of German word forms. In total, the database contains 365,530 word forms with transcriptions in DISC format that assign a character to each unique phonological segment (including affricates and diphthongs). Thus, for example, the word virgin is represented as \"jUN-frB,\" whereby we randomly sampled 4,000 tokens from this data set for our experiment and used the same algorithm as for the historical data to convert a slightly magnified 0.1 ppM deviation into a 1 ppM representation."}, {"heading": "5 Related Work", "text": "Various methods have been proposed to normalize spelling in historical texts; for an overview, see Piotrowski (2012). Many approaches use distance calculations or some form of rule rewriting at character level, but require either manual work on the rules (Baron and Rayson, 2008) or an encyclopaedic resource to filter their results (Bollmann, 2012; Porta et al., 2013). A more recent approach is the use of character-based statistical machine translation (Pettersson et al., 2013; Sa'nchez-Mart\u00ed nez et al., 2013; Scherrer and Erjavec, 2013). Unlike our approach to sequence marking, these methods do not require fixed character alignment between word forms, but it is not clear whether this is actually an advantage. Our point is that a comparative evaluation between these methods and other approaches has not yet been performed. Azawi et al. (2013) represent the only other approach that we are aware of using neural networks, but that they are more significant on our historical networks as well."}, {"heading": "6 Conclusion and Future Work", "text": "We presented an approach to historical spelling normalization using bidirectional short-term memory networks and showed that it exceeds a CRF baseline and Bollmann's Norma tool (2012) for almost all texts in our dataset, a diverse corpus of early New High German, although it uses a relatively small amount of training data (about 2,000 to 11,000 tokens) and does not use a lexical resource (as Norma does). We also showed that multi-task learning with additional normalization data can improve accuracy with Bi-LSTMs, while adding the same data to the training set of Norma and CRF on average does not help and can even be harmful. Many improvements to this approach are conceivable. Character-based statistical machine translation has been successfully applied to spell normalization (see Sec. 5), but we are not aware of any experiments with neural machine translation, so the task (Cho) could always be similar to this one (data architecture) in 2014."}, {"heading": "Acknowledgments", "text": "Marcel Bollmann was supported by the German Research Foundation (DFG), grant DI 1558 / 4."}], "references": [{"title": "Using comparable collections of historical texts for building a diachronic dictionary for spelling normalization", "author": ["Marilisa Amoia", "Jose Manuel Martinez."], "venue": "Proceedings of the 7th Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities, pages 84\u201389, Sofia, Bulgaria.", "citeRegEx": "Amoia and Martinez.,? 2013", "shortCiteRegEx": "Amoia and Martinez.", "year": 2013}, {"title": "Normalizing historical orthography for OCR historical documents using LSTM", "author": ["Mayce Al Azawi", "Muhammad Zeshan Afzal", "Thomas M. Breuel."], "venue": "Proceedings of the 2nd International Workshop on Historical Document Imaging and Processing, pages 80\u201385. ACM.", "citeRegEx": "Azawi et al\\.,? 2013", "shortCiteRegEx": "Azawi et al\\.", "year": 2013}, {"title": "The CELEX lexical database (Release 2) (CD-ROM)", "author": ["R. Harald Baayen", "Richard Piepenbrock", "L\u00e9on Gulikers."], "venue": "Linguistic Data Consortium, University of Pennsylvania, Philadelphia, PA.", "citeRegEx": "Baayen et al\\.,? 1995", "shortCiteRegEx": "Baayen et al\\.", "year": 1995}, {"title": "VARD 2: A tool for dealing with spelling variation in historical corpora", "author": ["Alistair Baron", "Paul Rayson."], "venue": "Proceedings of the Postgraduate Conference in Corpus Linguistics.", "citeRegEx": "Baron and Rayson.,? 2008", "shortCiteRegEx": "Baron and Rayson.", "year": 2008}, {"title": "Semi-)automatic normalization of historical texts using distance measures and the Norma tool", "author": ["Marcel Bollmann."], "venue": "Proceedings of the Second Workshop on Annotation of Corpora for Research in the Humanities (ACRH-2), Lisbon, Portugal.", "citeRegEx": "Bollmann.,? 2012", "shortCiteRegEx": "Bollmann.", "year": 2012}, {"title": "Multitask learning: A knowledge-based source of inductive bias", "author": ["Rich Caruana."], "venue": "Proceedings of the 10th International Conference on Machine Learning (ICML), pages 41\u201348.", "citeRegEx": "Caruana.,? 1993", "shortCiteRegEx": "Caruana.", "year": 1993}, {"title": "On the properties of neural machine translation: Encoder\u2013decoder approaches", "author": ["Kyunghyun Cho", "Bart van Merrienboer", "Dzmitry Bahdanau", "Yoshua Bengio."], "venue": "Proceedings of the Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation (SSST-8), pages 103\u2013111, Doha, Qatar.", "citeRegEx": "Cho et al\\.,? 2014", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Natural language processing (almost) from scratch", "author": ["Ronan Collobert", "Jason Weston", "L\u00e9on Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa."], "venue": "The Journal of Machine Learning Research, 12:2493\u20132537.", "citeRegEx": "Collobert et al\\.,? 2011", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "The Anselm corpus: Methods and perspectives of a parallel aligned corpus", "author": ["Stefanie Dipper", "Simone Schultz-Balluff."], "venue": "Proceedings of the NODALIDA Workshop on Computational Historical Linguistics.", "citeRegEx": "Dipper and Schultz.Balluff.,? 2013", "shortCiteRegEx": "Dipper and Schultz.Balluff.", "year": 2013}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "Neural Computation, 9(8):1735\u2013 1780.", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "More than words: using token context to improve canonicalization of historical German", "author": ["Bryan Jurish."], "venue": "Journal for Language Technology and Computational Linguistics, 25(1):23\u201339.", "citeRegEx": "Jurish.,? 2010", "shortCiteRegEx": "Jurish.", "year": 2010}, {"title": "Improving sentence compression by learning to predict gaze", "author": ["Sigrid Klerke", "Yoav Goldberg", "Anders S\u00f8gaard."], "venue": "Proceedings of NAACL-HLT 2016, pages 1528\u20131533, San Diego, CA.", "citeRegEx": "Klerke et al\\.,? 2016", "shortCiteRegEx": "Klerke et al\\.", "year": 2016}, {"title": "Guidelines for normalizing historical German texts", "author": ["Julia Krasselt", "Marcel Bollmann", "Stefanie Dipper", "Florian Petran."], "venue": "Bochumer Linguistische Arbeitsberichte, 15.", "citeRegEx": "Krasselt et al\\.,? 2015", "shortCiteRegEx": "Krasselt et al\\.", "year": 2015}, {"title": "Binary codes capable of correcting deletions, insertions, and reversals", "author": ["Vladimir I. Levenshtein."], "venue": "Soviet Physics Doklady, 10(8):707\u2013710.", "citeRegEx": "Levenshtein.,? 1966", "shortCiteRegEx": "Levenshtein.", "year": 1966}, {"title": "Multi-task sequence to sequence learning", "author": ["Minh-Thang Luong", "Quoc V. Le", "Ilya Sutskever", "Oriol Vinyals", "Lukasz Kaiser."], "venue": "arXiv:1511.06114v4.", "citeRegEx": "Luong et al\\.,? 2016", "shortCiteRegEx": "Luong et al\\.", "year": 2016}, {"title": "CRFsuite: a fast implementation of conditional random fields (CRFs)", "author": ["Naoaki Okazaki."], "venue": "http://www.chokkan.org/software/crfsuite/.", "citeRegEx": "Okazaki.,? 2007", "shortCiteRegEx": "Okazaki.", "year": 2007}, {"title": "An SMT approach to automatic annotation of historical text", "author": ["Eva Pettersson", "Be\u00e1ta Megyesi", "J\u00f6rg Tiedemann."], "venue": "Proceedings of the NODALIDA Workshop on Computational Historical Linguistics, Oslo, Norway.", "citeRegEx": "Pettersson et al\\.,? 2013", "shortCiteRegEx": "Pettersson et al\\.", "year": 2013}, {"title": "Natural Language Processing for Historical Texts", "author": ["Michael Piotrowski."], "venue": "Number 17 in Synthesis Lectures on Human Language Technologies. Morgan & Claypool, San Rafael, CA.", "citeRegEx": "Piotrowski.,? 2012", "shortCiteRegEx": "Piotrowski.", "year": 2012}, {"title": "Edit transducers for spelling variation in Old Spanish", "author": ["Jordi Porta", "Jos\u00e9-Luis Sancho", "Javier G\u00f3mez."], "venue": "Proceedings of the NODALIDA Workshop on Computational Historical Linguistics, Oslo, Norway.", "citeRegEx": "Porta et al\\.,? 2013", "shortCiteRegEx": "Porta et al\\.", "year": 2013}, {"title": "Modernizing historical Slovene words with character-based SMT", "author": ["Yves Scherrer", "Toma\u017e Erjavec."], "venue": "Proceedings of the 4th Biennial Workshop on Balto-Slavic Natural Language Processing, Sofia, Bulgaria.", "citeRegEx": "Scherrer and Erjavec.,? 2013", "shortCiteRegEx": "Scherrer and Erjavec.", "year": 2013}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V. Le."], "venue": "Advances in Neural Information Processing Systems (NIPS 2014), number 27, pages 3104\u20133112.", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "An open diachronic corpus of historical Spanish: annotation criteria and automatic modernisation of spelling", "author": ["Felipe S\u00e1nchez-Mart\u0131\u0301nez", "Isabel Mart\u0131\u0301nez-Sempere", "Xavier Ivars-Ribes", "Rafael C. Carrasco"], "venue": null, "citeRegEx": "S\u00e1nchez.Mart\u0131\u0301nez et al\\.,? \\Q2013\\E", "shortCiteRegEx": "S\u00e1nchez.Mart\u0131\u0301nez et al\\.", "year": 2013}, {"title": "Evaluating the pairwise string alignment of pronunciations", "author": ["Martijn Wieling", "Jelena Proki\u0107", "John Nerbonne."], "venue": "Proceedings of the EACL 2009 Workshop on Language Technology and Resources for Cultural Heritage, Social Sciences, Humanities, and Education (LaTeCH \u2013 SHELT&R 2009), pages 26\u201334, Athens, Greece.", "citeRegEx": "Wieling et al\\.,? 2009", "shortCiteRegEx": "Wieling et al\\.", "year": 2009}], "referenceMentions": [{"referenceID": 17, "context": "the mapping of historical spelling variants to standardized/modernized forms, is often employed as a pre-processing step to allow the utilization of existing tools for the respective modern target language (Piotrowski, 2012).", "startOffset": 206, "endOffset": 224}, {"referenceID": 10, "context": "by being based on some form of phonetic, graphematic, or semantic similarity measure (Jurish, 2010; Bollmann, 2012; Amoia and Martinez, 2013).", "startOffset": 85, "endOffset": 141}, {"referenceID": 4, "context": "by being based on some form of phonetic, graphematic, or semantic similarity measure (Jurish, 2010; Bollmann, 2012; Amoia and Martinez, 2013).", "startOffset": 85, "endOffset": 141}, {"referenceID": 0, "context": "by being based on some form of phonetic, graphematic, or semantic similarity measure (Jurish, 2010; Bollmann, 2012; Amoia and Martinez, 2013).", "startOffset": 85, "endOffset": 141}, {"referenceID": 4, "context": "We show that this model outperforms both the existing normalization tool Norma (Bollmann, 2012) and a CRF-based tagger when evaluated on a diverse dataset from Early New High German.", "startOffset": 79, "endOffset": 95}, {"referenceID": 8, "context": "We use a total of 44 texts from the Anselm corpus (Dipper and Schultz-Balluff, 2013) of Early New High German.", "startOffset": 50, "endOffset": 84}, {"referenceID": 8, "context": "We use a total of 44 texts from the Anselm corpus (Dipper and Schultz-Balluff, 2013) of Early New High German.1 The corpus is a collection of several manuscripts and prints of the same core text, a religious treatise. Although the texts are semi-parallel and share some vocabulary, they were written in different time periods (between the 14th and 16th century) as well as different dialectal regions, and show quite diverse spelling characteristics. For example, the modern German word Frau \u2018woman\u2019 can be spelled as fraw/vraw (Me), frawe (N2), frauwe (St), fra\u00fcwe (B2), frow (Stu), vrowe (Ka), vorwe (Sa), or vrouwe (B), among others.2 All texts in the Anselm corpus are manually annotated with gold-standard normalizations following guidelines described in Krasselt et al. (2015). For our experiments, we excluded texts from the corpus that are shorter than 4,000 tokens, as well as a few texts for which annotations were not yet available at the time of writing (mostly Low German and Dutch versions).", "startOffset": 51, "endOffset": 783}, {"referenceID": 13, "context": "The Levenshtein algorithm (Levenshtein, 1966) can be used to produce alignments that preferably align identical characters, but is ambiguous when multiple alignments with the same Levenshtein distance exist.", "startOffset": 26, "endOffset": 45}, {"referenceID": 22, "context": "We therefore use iterated Levenshtein distance alignment (Wieling et al., 2009), which uses pointwise mutual information on aligned segments to estimate statistical dependence, and favors alignments of characters that tend to cooccur often within the dataset.", "startOffset": 57, "endOffset": 79}, {"referenceID": 9, "context": "LSTMs (Hochreiter and Schmidhuber, 1997) are a form of recurrent neural network (RNN) designed to better learn long-term dependencies, and have proven advantageous to plain RNNs on many tasks.", "startOffset": 6, "endOffset": 40}, {"referenceID": 5, "context": "In multi-task learning (MTL), the performance of a model on a given task is improved by additionally training it on one or more auxiliary tasks (Caruana, 1993).", "startOffset": 144, "endOffset": 159}, {"referenceID": 7, "context": "Multi-task learning with (deep) neural network architectures was shown to be effective for a variety of NLP tasks, such as part-of-speech tagging, chunking, named entity recognition (Collobert et al., 2011); sentence compression (Klerke et al.", "startOffset": 182, "endOffset": 206}, {"referenceID": 11, "context": ", 2011); sentence compression (Klerke et al., 2016); or machine translation (Luong et al.", "startOffset": 30, "endOffset": 51}, {"referenceID": 14, "context": ", 2016); or machine translation (Luong et al., 2016).", "startOffset": 32, "endOffset": 52}, {"referenceID": 15, "context": "Implementation was done with CRFsuite (Okazaki, 2007) using the averaged perceptron algorithm for training.", "startOffset": 38, "endOffset": 53}, {"referenceID": 4, "context": "For comparison, we also train and evaluate with the Norma tool described by Bollmann (2012), since it was originally developed for the Anselm corpus and the implementation is publicly available.", "startOffset": 76, "endOffset": 92}, {"referenceID": 2, "context": "For our dataset, we used the German part of the CELEX lexical database (Baayen et al., 1995), particularly the database of phonetic transcriptions of German wordforms.", "startOffset": 71, "endOffset": 92}, {"referenceID": 3, "context": "Many approaches use edit distance calculations or some form of character-level rewrite rules, but require either hand-crafting of the rules (Baron and Rayson, 2008) or a lexical resource to filter their output (Bollmann, 2012; Porta et al.", "startOffset": 140, "endOffset": 164}, {"referenceID": 4, "context": "Many approaches use edit distance calculations or some form of character-level rewrite rules, but require either hand-crafting of the rules (Baron and Rayson, 2008) or a lexical resource to filter their output (Bollmann, 2012; Porta et al., 2013).", "startOffset": 210, "endOffset": 246}, {"referenceID": 18, "context": "Many approaches use edit distance calculations or some form of character-level rewrite rules, but require either hand-crafting of the rules (Baron and Rayson, 2008) or a lexical resource to filter their output (Bollmann, 2012; Porta et al., 2013).", "startOffset": 210, "endOffset": 246}, {"referenceID": 16, "context": "A newer approach is the application of character-based statistical machine translation (Pettersson et al., 2013; S\u00e1nchez-Mart\u0131\u0301nez et al., 2013; Scherrer and Erjavec, 2013).", "startOffset": 87, "endOffset": 172}, {"referenceID": 21, "context": "A newer approach is the application of character-based statistical machine translation (Pettersson et al., 2013; S\u00e1nchez-Mart\u0131\u0301nez et al., 2013; Scherrer and Erjavec, 2013).", "startOffset": 87, "endOffset": 172}, {"referenceID": 19, "context": "A newer approach is the application of character-based statistical machine translation (Pettersson et al., 2013; S\u00e1nchez-Mart\u0131\u0301nez et al., 2013; Scherrer and Erjavec, 2013).", "startOffset": 87, "endOffset": 172}, {"referenceID": 13, "context": "Various methods have been proposed to perform spelling normalization on historical texts; for an overview, see Piotrowski (2012). Many approaches use edit distance calculations or some form of character-level rewrite rules, but require either hand-crafting of the rules (Baron and Rayson, 2008) or a lexical resource to filter their output (Bollmann, 2012; Porta et al.", "startOffset": 111, "endOffset": 129}, {"referenceID": 1, "context": "Azawi et al. (2013) present the only other approach we are aware of that applies neural networks to normalization of historical data.", "startOffset": 0, "endOffset": 20}, {"referenceID": 4, "context": "We presented an approach to historical spelling normalization using bi-directional long short-term memory networks and showed that it outperforms a CRF baseline and the Norma tool by Bollmann (2012) for almost all of the texts in our dataset, a diverse corpus of Early New High German, despite using a relatively low amount of training data (about 2,000 to 11,000 tokens) and not making use of a lexical resource (like Norma does).", "startOffset": 183, "endOffset": 199}, {"referenceID": 6, "context": "5), but we are not aware of any experiments with neural machine translation (Cho et al., 2014) on this domain.", "startOffset": 76, "endOffset": 94}, {"referenceID": 6, "context": "5), but we are not aware of any experiments with neural machine translation (Cho et al., 2014) on this domain. Using an encoder\u2013decoder architecture, e.g. similar to Sutskever et al. (2014), would remove the need for an explicit character alignment (cf.", "startOffset": 77, "endOffset": 190}], "year": 2016, "abstractText": "Natural-language processing of historical documents is complicated by the abundance of variant spellings and lack of annotated data. A common approach is to normalize the spelling of historical words to modern forms. We explore the suitability of a deep neural network architecture for this task, particularly a deep bi-LSTM network applied on a character level. Our model compares well to previously established normalization algorithms when evaluated on a diverse set of texts from Early New High German. We show that multi-task learning with additional normalization data can improve our model\u2019s performance further.", "creator": "dvips(k) 5.991 Copyright 2011 Radical Eye Software"}}}