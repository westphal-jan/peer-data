{"id": "1610.07883", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Oct-2016", "title": "Generalization Bounds for Weighted Automata", "abstract": "This paper studies the problem of learning weighted automata from a finite labeled training sample. We consider several general families of weighted automata defined in terms of three different measures: the norm of an automaton's weights, the norm of the function computed by an automaton, or the norm of the corresponding Hankel matrix. We present new data-dependent generalization guarantees for learning weighted automata expressed in terms of the Rademacher complexity of these families. We further present upper bounds on these Rademacher complexities, which reveal key new data-dependent terms related to the complexity of learning weighted automata.", "histories": [["v1", "Tue, 25 Oct 2016 14:10:11 GMT  (40kb,D)", "http://arxiv.org/abs/1610.07883v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.FL", "authors": ["borja balle", "mehryar mohri"], "accepted": false, "id": "1610.07883"}, "pdf": {"name": "1610.07883.pdf", "metadata": {"source": "CRF", "title": "Generalization Bounds for Weighted Automata", "authors": ["B. Balle", "M. Mohri"], "emails": ["b.deballepigem@lancaster.ac.uk"], "sections": [{"heading": "1 Introduction", "text": "This year, the time has come for such a process to take place, in which the question is to what extent such a process has taken place."}, {"heading": "2 Preliminaries", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Weighted Automata, Rational Functions, and Hankel Matrices", "text": "Let us specify the empty string and the quantity of all finite strings over the alphabet. < F = 1 \u00b7 F = 1 \u00b7 F = 1 \u00b7 F = 1 \u00b7 F = 1 \u00b7 F = 1 \u00b7 F = 1 \u00b7 F = 0 \u00b7 F = 0 \u00b7 F = 0 \u00b7 F = 0 \u00b7 F = 0 \u00b7 F = 0 \u00b7 F = 0 \u00b7 F = 0 \u00b7 F = 0 \u00b7 F = 0 \u00b7 F = 0 \u00b7 F = 0 \u00b7 F = 0 \u00b7 F = 0 \u00b7 F = 0 \u00b7 F = 0 \u00b7 F = 0 \u00b7 F = 0 \u00b7 F = 0 \u00b7 F = 0 \u00b7 F = 0 \u00b7 F = 0 \u00b7 F = 0 \u00b7 F = 0 \u00b7 F = 0 \u00b7 F = 0 \u00b7 F = 0 \u00b7 F = 0 \u00b7 F = 0 \u00b7 F = 0 \u00b7 F = 0 \u00b7 F = 0 \u00b7 F = 0 \u00b7 F = 0 \u00b7 F = 0 \u00b7 F = 0 \u00b7 F = 0 \u00b7 F = 0 \u00b7 F = 0 \u00b7 F = 0 \u00b7 F = 0 \u00b7 F = 0 \u00b7 F = 0 \u00b7 F = 0 \u00b7 F = 0 \u00b7 F = 0 \u00b7 F = 0 \u00b7 F = 0 \u00b7 F = 0 \u00b7 F = 0 \u00b7 F = 0 \u00b7 F = 0 \u00b7 F = 0 \u00b7 F = 0 \u00b7 F = 0 \u00b7 F = 0 \u00b7 F = 0 \u00b7 F = 0 \u00b7 F = 0 \u00b7 F = 0 \u00b7 F = 0 \u00b7 F = 0 \u00b7 0 \u00b7 F = 0 \u00b7 F = 0 \u00b7 F = 0 \u00b7 F = 0 \u00b7 F = 0 \u00b7 F = 0 \u00b7 F = 0 \u00b7 F = 0 \u00b7 F = 0 \u00b7 F = 0 \u00b7 F = 0 \u00b7 F = 0 \u00b7 0 \u00b7 F = 0 \u00b7 F = 0 \u00b7 F = 0 = 0 \u00b7 F = 0 = 0 \u00b7 F = 0 \u00b7 F = 0 = 0 = 0 \u00b7 F = 0 = 0 \u00b7 F = 0 = 0 = 0 \u00b7 F = 0 = 0 \u00b7 F = 0 = 0 \u00b7 F = 0 = 0 = 0 \u00b7 F = 0 = 0 \u00b7 F = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 \u00b7 F = 0 = 0 = 0 = 0 = 0 = 0 = 0 = 0 = F = 0 = 0 = 0 = 0 = 0 = 0 = F = 0 = 0 = 0 = 0 = 0 = 0 = 0 = F = 0 = 0 = 0 = F = 0 = 0 ="}, {"heading": "2.2 Learning Scenario", "text": "Let Z be a measurable subset of R. Let F be a subset of the functional family mapped from X to Y, with Y R, according to some unknown distribution D over V \u00b7 R.Let F be a subset of the functional family mapped from X to Y, with Y R, and let \": Y \u00b7 Z \u2192 R + be a loss function representing the divergence between the prediction y \u00b2 Y formed by a function in F and the target mark z \u00b2 Z. The goal of the learner is to derive a labeled learning sample S = (x1, z1),. (xm, zm) of size m to choose a function f \u00b2 F with small expected loss, the isLD (f) = E (x).Our goal is to derive learning guarantees for wide families of automata or rational functions used as a hypothesis for learning algorithms."}, {"heading": "3 Classes of Rational Functions", "text": "In this section, we present several classes of Rational Functions, each of which is defined using a different method to measure the complexity of Rational Functions. The first is based on the weights of an explicit EFA representation, while the other two are based on intrinsic quantities associated with the function: the norm of function and the norm of the corresponding Hankel matrix when viewed as a linear operator on a specific Hilbert space. These three aspects measure different aspects of the complexity of a rational function, and each of them offers clear advantages in analyzing learning with WFAs. The Rademacher complexity of each of these classes is analyzed in Sections 4, 5 and 6.3.1. The class An, p, r We begin by looking at the case of Rational Function through a fixed EFA representation. Our learning limits would then, of course, depend on the number of states and the weights of EFA representations."}, {"heading": "3.1.1 Examples", "text": "First, we consider the class of deterministic finite automata (DFA). A DFA can be represented by an EFA, where: \u03b1 is the indicator vector of the starting state; the entries of \u03b2 are values in {0, 1} that indicate whether a state accepts or rejects; and in a transition from state i to state i, we have that the ith line of Aa is either the all-zero vector if there is no transition from the ith state marked with one or an indicator vector marked with one on the jth position when a transition from state i to state j. Therefore, a DFA A = < \u03b1, {Aa} > satisfies a transition from the ith state marked with one or an indicator vector marked with one on the jth position. Another important class of EFA contained in An, 1,1 is that of the probabilistic ability (PFA)."}, {"heading": "3.2.1 Examples and Membership Testing", "text": "If we are a PFA member, then the function of the FA is a challenge and we have the opportunity to do what we do. \"-\" We. \"-\" We. \"-\" We. \"-\" We. \"-\" We. \"-\" We. \"-\" We. \"-\" We. \"-\" We. \"-\" We. \"-\" We. \"-\" We. \"-\" We. \"-\" We. \"-\" We. \"-\" We. \"-\" We. \"-\" We. \"-\" We. \"-\" We. \"-\" We. \"-\" We. \"-\" We. \"-\" We. \"-\" We. \"-\" We. \"-\" We. \"-\" We. \"-\""}, {"heading": "4.1 Proof of Theorem 2", "text": "Let's start the proof by referring to several well-known facts and definitions that relate to the coverage of numbers (see e.g. [24]). Let's set a number of vectors and S = (x1,.., xm). We say that V has a (\"1, 2\") coverage for S with respect to An, p, r, if for each A, p, r, there are some V that require such coverage. (S) We say that V has a (\"1, 2\") coverage for S with respect to An, p, r, if for each A, p, r, there are some V that have such coverage of V. (S)."}, {"heading": "7 Distribution-Dependent Rademacher Complex-", "text": "The limits for the Rademacher complexity of R1, r and H1, r are given above by two important distributional parameters (Cm = ES), which reflect the effects of distribution D on the complexity of learning these classes of rational functions. We begin by conveniently rewriting these classes. Let's rewrite E = {ex: D = x x x x), which rewrite the class of all indicators on the Rademacher complexities Rm (R1, r) and Rm (H1, r). Let's rewrite E = {ex: D = x x), the class of all indicators on ex (R1, r) and ex (H1, r)."}, {"heading": "8 Learning and Sample Complexity Bounds", "text": "In cases where we have different limits for the empirical and expected complexity of Rademacher, we will discuss in the next section what the open problems are related to obtaining efficient algorithms to solve these optimization problems. Proof of these theorems is a simple combination of the Rademacher complexity with well-known generalization limits. Do you leave D a probability distribution over the distribution of probability?"}, {"heading": "9 Conclusion", "text": "We have three ways to analyze the complexity of WFA's and their functionalities."}], "references": [{"title": "On the computational complexity of approximating distributions by probabilistic automata", "author": ["Naoki Abe", "Manfred K Warmuth"], "venue": "Machine Learning,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1992}, {"title": "Statistical modeling for unit selection in speech synthesis", "author": ["Cyril Allauzen", "Mehryar Mohri", "Michael Riley"], "venue": "In Proceedings of ACL,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2004}, {"title": "Sequence kernels for predicting protein essentiality", "author": ["Cyril Allauzen", "Mehryar Mohri", "Ameet Talwalkar"], "venue": "In Proceedings of ICML,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2008}, {"title": "Formal analysis of online algorithms", "author": ["Benjamin Aminof", "Orna Kupferman", "Robby Lampert"], "venue": "In Proceedings of ATVA,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2011}, {"title": "Model checking linear-time properties of probabilistic systems", "author": ["C. Baier", "M. Gr\u00f6\u00dfer", "F. Ciesinski"], "venue": "In Handbook of Weighted automata. Springer,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2009}, {"title": "Grammatical inference as a principal component analysis problem", "author": ["R. Bailly", "F. Denis", "L. Ralaivola"], "venue": "In ICML,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2009}, {"title": "Absolute convergence of rational series is semi-decidable", "author": ["Rapha\u00ebl Bailly", "Fran\u00e7ois Denis"], "venue": "Inf. Comput.,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2011}, {"title": "Spectral learning of weighted automata: A forward-backward perspective", "author": ["B. Balle", "X. Carreras", "F.M. Luque", "A. Quattoni"], "venue": "Machine Learning,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2014}, {"title": "Methods of moments for learning stochastic languages: Unified presentation and empirical comparison", "author": ["B. Balle", "W.L. Hamilton", "J. Pineau"], "venue": "In ICML,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2014}, {"title": "Spectral learning of general weighted automata via constrained matrix completion", "author": ["Borja Balle", "Mehryar Mohri"], "venue": "In NIPS,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2012}, {"title": "Learning weighted automata", "author": ["Borja Balle", "Mehryar Mohri"], "venue": "In CAI,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2015}, {"title": "A canonical form for weighted automata and applications to approximate minimization", "author": ["Borja Balle", "Prakash Panangaden", "Doina Precup"], "venue": "In Logic in Computer Science (LICS),", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2015}, {"title": "Rademacher and gaussian complexities: Risk bounds and structural results", "author": ["Peter L Bartlett", "Shahar Mendelson"], "venue": "In COLT,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2001}, {"title": "Rational Series and Their Languages", "author": ["Jean Berstel", "Christophe Reutenauer"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1988}, {"title": "Noncommutative rational series with applications", "author": ["Jean Berstel", "Christophe Reutenauer"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2011}, {"title": "Closing the learning-planning loop with predictive state representations", "author": ["B. Boots", "S. Siddiqi", "G. Gordon"], "venue": "In RSS,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2009}, {"title": "The OCRopus open source OCR system", "author": ["Thomas M. Breuel"], "venue": "In Proceedings of IS&T/SPIE,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2008}, {"title": "Realizations by stochastic finite automata", "author": ["Jack W. Carlyle", "Azaria Paz"], "venue": "J. Comput. Syst. Sci.,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1971}, {"title": "Pre-reduction graph products: Hardnesses of properly learning dfas and approximating edp on dags", "author": ["P. Chalermsook", "B. Laekhanukit", "D. Nanongkai"], "venue": "In Proceedings of FOCS,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2014}, {"title": "Partially distribution-free learning of regular languages from positive samples", "author": ["Alexander Clark", "Franck Thollard"], "venue": "In Proceedings of the 20th international conference on Computational Linguistics,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2004}, {"title": "Rational kernels: Theory and algorithms", "author": ["Corinna Cortes", "Patrick Haffner", "Mehryar Mohri"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2004}, {"title": "Lp distance and equivalence of probabilistic automata", "author": ["Corinna Cortes", "Mehryar Mohri", "Ashish Rastogi"], "venue": "International Journal of Foundations of Computer Science,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2007}, {"title": "Hierarchical phrase-based translation with weighted finite-state transducers and shallow-n grammars", "author": ["A. de Gispert", "G. Iglesias", "G. Blackwood", "E.R. Banga", "W. Byrne"], "venue": "Computational Linguistics,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2010}, {"title": "Combinatorial methods in density estimation", "author": ["Luc Devroye", "G\u00e1bor Lugosi"], "venue": null, "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2001}, {"title": "Handbook of weighted automata", "author": ["Manfred Droste", "Werner Kuich", "Heiko Vogler", "editors"], "venue": "EATCS Monographs on Theoretical Computer Science. Springer,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2009}, {"title": "Uniform central limit theorems, volume 23", "author": ["Richard M Dudley"], "venue": null, "citeRegEx": "26", "shortCiteRegEx": "26", "year": 1999}, {"title": "Biological Sequence Analysis: Probabilistic Models of Proteins and Nucleic Acids", "author": ["Richard Durbin", "Sean R. Eddy", "Anders Krogh", "Graeme J. Mitchison"], "venue": null, "citeRegEx": "27", "shortCiteRegEx": "27", "year": 1998}, {"title": "Automata, Languages and Machines, volume A", "author": ["Samuel Eilenberg"], "venue": null, "citeRegEx": "28", "shortCiteRegEx": "28", "year": 1974}, {"title": "Matrices de Hankel", "author": ["M. Fliess"], "venue": "Journal de Mathe\u0301matiques Pures et Applique\u0301es,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 1974}, {"title": "Modelling sparse dynamical systems with compressed predictive state representations", "author": ["W.L. Hamilton", "M.M. Fard", "J. Pineau"], "venue": "In ICML,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2013}, {"title": "A spectral algorithm for learning hidden Markov models", "author": ["D. Hsu", "S.M. Kakade", "T. Zhang"], "venue": "In COLT,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2009}, {"title": "Image compression using weighted finite automata", "author": ["Karel Culik II", "Jarkko Kari"], "venue": "Computers & Graphics,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 1993}, {"title": "Vc-dimensions of finite automata and commutative finite automata with k letters and n states", "author": ["Yoshiyasu Ishigami", "Sei\u2019ichi Tani"], "venue": "Discrete Applied Mathematics,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 1997}, {"title": "Regular models of phonological rule systems", "author": ["Ronald M. Kaplan", "Martin Kay"], "venue": "Computational Linguistics,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 1994}, {"title": "The replace operator", "author": ["Lauri Karttunen"], "venue": "In Proceedings of ACL,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 1995}, {"title": "Cryptographic limitations on learning boolean formulae and finite automata", "author": ["Michael J. Kearns", "Leslie G. Valiant"], "venue": "Journal of ACM,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 1994}, {"title": "Rademacher processes and bounding the risk of function learning", "author": ["Vladimir Koltchinskii", "Dmitry Panchenko"], "venue": "In High Dimensional Probability II,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2000}, {"title": "Semirings, Automata, Languages. Number 5 in EATCS Monographs on Theoretical Computer Science", "author": ["Werner Kuich", "Arto Salomaa"], "venue": null, "citeRegEx": "38", "shortCiteRegEx": "38", "year": 1986}, {"title": "Low-rank spectral learning with weighted loss functions", "author": ["A. Kulesza", "N. Jiang", "S. Singh"], "venue": "In AISTATS,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2015}, {"title": "Low-Rank Spectral Learning", "author": ["Alex Kulesza", "N Raj Rao", "Satinder Singh"], "venue": "In AISTATS,", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2014}, {"title": "Some applications of concentration inequalities to statistics", "author": ["Pascal Massart"], "venue": "Annales de la Faculte\u0301 des Sciences de Toulouse,", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2000}, {"title": "A trace inequality of John von Neumann", "author": ["L. Mirsky"], "venue": "Monatshefte fr Mathematik,", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 1975}, {"title": "Finite-state transducers in language and speech processing", "author": ["Mehryar Mohri"], "venue": "Computational Linguistics,", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 1997}, {"title": "Weighted automata algorithms. In Handbook of Weighted Automata, Monographs in Theoretical Computer Science, pages 213\u2013254", "author": ["Mehryar Mohri"], "venue": null, "citeRegEx": "45", "shortCiteRegEx": "45", "year": 2009}, {"title": "Weighted automata in text and speech processing", "author": ["Mehryar Mohri", "Fernando Pereira", "Michael Riley"], "venue": "In Proceedings of ECAI-96 Workshop on Extended finite state models of language,", "citeRegEx": "46", "shortCiteRegEx": "46", "year": 1996}, {"title": "Dynamic compilation of weighted context-free grammars", "author": ["Mehryar Mohri", "Fernando C.N. Pereira"], "venue": "In Proceedings of COLING-ACL,", "citeRegEx": "47", "shortCiteRegEx": "47", "year": 1998}, {"title": "Speech recognition with weighted finite-state transducers", "author": ["Mehryar Mohri", "Fernando C.N. Pereira", "Michael Riley"], "venue": "In Handbook on Speech Processing and Speech Comm. Springer,", "citeRegEx": "48", "shortCiteRegEx": "48", "year": 2008}, {"title": "Foundations of machine learning", "author": ["Mehryar Mohri", "Afshin Rostamizadeh", "Ameet Talwalkar"], "venue": "MIT press,", "citeRegEx": "49", "shortCiteRegEx": "49", "year": 2012}, {"title": "An efficient compiler for weighted rewrite rules", "author": ["Mehryar Mohri", "Richard Sproat"], "venue": "In Proceedings of ACL,", "citeRegEx": "50", "shortCiteRegEx": "50", "year": 1996}, {"title": "Speech recognition by composition of weighted finite automata. In Finite-State Language Processing", "author": ["Fernando Pereira", "Michael Riley"], "venue": null, "citeRegEx": "51", "shortCiteRegEx": "51", "year": 1997}, {"title": "The minimum consistent DFA problem cannot be approximated within any polynomial", "author": ["Leonard Pitt", "Manfred K. Warmuth"], "venue": "Journal of the ACM,", "citeRegEx": "52", "shortCiteRegEx": "52", "year": 1993}, {"title": "Automata-Theoretic Aspects of Formal Power Series", "author": ["Arto Salomaa", "Matti Soittola"], "venue": null, "citeRegEx": "53", "shortCiteRegEx": "53", "year": 1978}, {"title": "A finite-state architecture for tokenization and graphemeto-phoneme conversion in multilingual text analysis", "author": ["Richard Sproat"], "venue": "In Proceedings of the ACL SIGDAT Workshop", "citeRegEx": "54", "shortCiteRegEx": "54", "year": 1995}, {"title": "Finite Automata: Behavior and Synthesis", "author": ["B Trakhtenbrot", "Y Barzdin"], "venue": null, "citeRegEx": "55", "shortCiteRegEx": "55", "year": 1973}, {"title": "An introduction to matrix concentration inequalities", "author": ["Joel A. Tropp"], "venue": "Foundations and Trends R  \u00a9 in Machine Learning,", "citeRegEx": "56", "shortCiteRegEx": "56", "year": 2015}, {"title": "Lectures in Geometrical Functional Analysis", "author": ["Roman Vershynin"], "venue": null, "citeRegEx": "57", "shortCiteRegEx": "57", "year": 2009}], "referenceMentions": [{"referenceID": 27, "context": "The mathematical theory behind WFAs, that of rational power series, has been extensively studied in the past [28, 53, 38, 14] and has been more recently the topic of a dedicated handbook [25].", "startOffset": 109, "endOffset": 125}, {"referenceID": 51, "context": "The mathematical theory behind WFAs, that of rational power series, has been extensively studied in the past [28, 53, 38, 14] and has been more recently the topic of a dedicated handbook [25].", "startOffset": 109, "endOffset": 125}, {"referenceID": 37, "context": "The mathematical theory behind WFAs, that of rational power series, has been extensively studied in the past [28, 53, 38, 14] and has been more recently the topic of a dedicated handbook [25].", "startOffset": 109, "endOffset": 125}, {"referenceID": 13, "context": "The mathematical theory behind WFAs, that of rational power series, has been extensively studied in the past [28, 53, 38, 14] and has been more recently the topic of a dedicated handbook [25].", "startOffset": 109, "endOffset": 125}, {"referenceID": 24, "context": "The mathematical theory behind WFAs, that of rational power series, has been extensively studied in the past [28, 53, 38, 14] and has been more recently the topic of a dedicated handbook [25].", "startOffset": 187, "endOffset": 191}, {"referenceID": 31, "context": "WFAs are widely used in modern applications, perhaps most prominently in image processing and speech recognition where the terminology of weighted automata seems to have been first introduced and made popular [32, 46, 51, 44, 48], in several other speech processing applications such as speech synthesis [54, 2], in phonological and morphological rule compilation [34, 35, 50], in parsing [47], machine translation [23], bioinformatics [27, 3], sequence modeling and prediction [21], formal verification and model checking [5, 4], in optical character recognition [17], and in many other areas.", "startOffset": 209, "endOffset": 229}, {"referenceID": 44, "context": "WFAs are widely used in modern applications, perhaps most prominently in image processing and speech recognition where the terminology of weighted automata seems to have been first introduced and made popular [32, 46, 51, 44, 48], in several other speech processing applications such as speech synthesis [54, 2], in phonological and morphological rule compilation [34, 35, 50], in parsing [47], machine translation [23], bioinformatics [27, 3], sequence modeling and prediction [21], formal verification and model checking [5, 4], in optical character recognition [17], and in many other areas.", "startOffset": 209, "endOffset": 229}, {"referenceID": 49, "context": "WFAs are widely used in modern applications, perhaps most prominently in image processing and speech recognition where the terminology of weighted automata seems to have been first introduced and made popular [32, 46, 51, 44, 48], in several other speech processing applications such as speech synthesis [54, 2], in phonological and morphological rule compilation [34, 35, 50], in parsing [47], machine translation [23], bioinformatics [27, 3], sequence modeling and prediction [21], formal verification and model checking [5, 4], in optical character recognition [17], and in many other areas.", "startOffset": 209, "endOffset": 229}, {"referenceID": 42, "context": "WFAs are widely used in modern applications, perhaps most prominently in image processing and speech recognition where the terminology of weighted automata seems to have been first introduced and made popular [32, 46, 51, 44, 48], in several other speech processing applications such as speech synthesis [54, 2], in phonological and morphological rule compilation [34, 35, 50], in parsing [47], machine translation [23], bioinformatics [27, 3], sequence modeling and prediction [21], formal verification and model checking [5, 4], in optical character recognition [17], and in many other areas.", "startOffset": 209, "endOffset": 229}, {"referenceID": 46, "context": "WFAs are widely used in modern applications, perhaps most prominently in image processing and speech recognition where the terminology of weighted automata seems to have been first introduced and made popular [32, 46, 51, 44, 48], in several other speech processing applications such as speech synthesis [54, 2], in phonological and morphological rule compilation [34, 35, 50], in parsing [47], machine translation [23], bioinformatics [27, 3], sequence modeling and prediction [21], formal verification and model checking [5, 4], in optical character recognition [17], and in many other areas.", "startOffset": 209, "endOffset": 229}, {"referenceID": 52, "context": "WFAs are widely used in modern applications, perhaps most prominently in image processing and speech recognition where the terminology of weighted automata seems to have been first introduced and made popular [32, 46, 51, 44, 48], in several other speech processing applications such as speech synthesis [54, 2], in phonological and morphological rule compilation [34, 35, 50], in parsing [47], machine translation [23], bioinformatics [27, 3], sequence modeling and prediction [21], formal verification and model checking [5, 4], in optical character recognition [17], and in many other areas.", "startOffset": 304, "endOffset": 311}, {"referenceID": 1, "context": "WFAs are widely used in modern applications, perhaps most prominently in image processing and speech recognition where the terminology of weighted automata seems to have been first introduced and made popular [32, 46, 51, 44, 48], in several other speech processing applications such as speech synthesis [54, 2], in phonological and morphological rule compilation [34, 35, 50], in parsing [47], machine translation [23], bioinformatics [27, 3], sequence modeling and prediction [21], formal verification and model checking [5, 4], in optical character recognition [17], and in many other areas.", "startOffset": 304, "endOffset": 311}, {"referenceID": 33, "context": "WFAs are widely used in modern applications, perhaps most prominently in image processing and speech recognition where the terminology of weighted automata seems to have been first introduced and made popular [32, 46, 51, 44, 48], in several other speech processing applications such as speech synthesis [54, 2], in phonological and morphological rule compilation [34, 35, 50], in parsing [47], machine translation [23], bioinformatics [27, 3], sequence modeling and prediction [21], formal verification and model checking [5, 4], in optical character recognition [17], and in many other areas.", "startOffset": 364, "endOffset": 376}, {"referenceID": 34, "context": "WFAs are widely used in modern applications, perhaps most prominently in image processing and speech recognition where the terminology of weighted automata seems to have been first introduced and made popular [32, 46, 51, 44, 48], in several other speech processing applications such as speech synthesis [54, 2], in phonological and morphological rule compilation [34, 35, 50], in parsing [47], machine translation [23], bioinformatics [27, 3], sequence modeling and prediction [21], formal verification and model checking [5, 4], in optical character recognition [17], and in many other areas.", "startOffset": 364, "endOffset": 376}, {"referenceID": 48, "context": "WFAs are widely used in modern applications, perhaps most prominently in image processing and speech recognition where the terminology of weighted automata seems to have been first introduced and made popular [32, 46, 51, 44, 48], in several other speech processing applications such as speech synthesis [54, 2], in phonological and morphological rule compilation [34, 35, 50], in parsing [47], machine translation [23], bioinformatics [27, 3], sequence modeling and prediction [21], formal verification and model checking [5, 4], in optical character recognition [17], and in many other areas.", "startOffset": 364, "endOffset": 376}, {"referenceID": 45, "context": "WFAs are widely used in modern applications, perhaps most prominently in image processing and speech recognition where the terminology of weighted automata seems to have been first introduced and made popular [32, 46, 51, 44, 48], in several other speech processing applications such as speech synthesis [54, 2], in phonological and morphological rule compilation [34, 35, 50], in parsing [47], machine translation [23], bioinformatics [27, 3], sequence modeling and prediction [21], formal verification and model checking [5, 4], in optical character recognition [17], and in many other areas.", "startOffset": 389, "endOffset": 393}, {"referenceID": 22, "context": "WFAs are widely used in modern applications, perhaps most prominently in image processing and speech recognition where the terminology of weighted automata seems to have been first introduced and made popular [32, 46, 51, 44, 48], in several other speech processing applications such as speech synthesis [54, 2], in phonological and morphological rule compilation [34, 35, 50], in parsing [47], machine translation [23], bioinformatics [27, 3], sequence modeling and prediction [21], formal verification and model checking [5, 4], in optical character recognition [17], and in many other areas.", "startOffset": 415, "endOffset": 419}, {"referenceID": 26, "context": "WFAs are widely used in modern applications, perhaps most prominently in image processing and speech recognition where the terminology of weighted automata seems to have been first introduced and made popular [32, 46, 51, 44, 48], in several other speech processing applications such as speech synthesis [54, 2], in phonological and morphological rule compilation [34, 35, 50], in parsing [47], machine translation [23], bioinformatics [27, 3], sequence modeling and prediction [21], formal verification and model checking [5, 4], in optical character recognition [17], and in many other areas.", "startOffset": 436, "endOffset": 443}, {"referenceID": 2, "context": "WFAs are widely used in modern applications, perhaps most prominently in image processing and speech recognition where the terminology of weighted automata seems to have been first introduced and made popular [32, 46, 51, 44, 48], in several other speech processing applications such as speech synthesis [54, 2], in phonological and morphological rule compilation [34, 35, 50], in parsing [47], machine translation [23], bioinformatics [27, 3], sequence modeling and prediction [21], formal verification and model checking [5, 4], in optical character recognition [17], and in many other areas.", "startOffset": 436, "endOffset": 443}, {"referenceID": 20, "context": "WFAs are widely used in modern applications, perhaps most prominently in image processing and speech recognition where the terminology of weighted automata seems to have been first introduced and made popular [32, 46, 51, 44, 48], in several other speech processing applications such as speech synthesis [54, 2], in phonological and morphological rule compilation [34, 35, 50], in parsing [47], machine translation [23], bioinformatics [27, 3], sequence modeling and prediction [21], formal verification and model checking [5, 4], in optical character recognition [17], and in many other areas.", "startOffset": 478, "endOffset": 482}, {"referenceID": 4, "context": "WFAs are widely used in modern applications, perhaps most prominently in image processing and speech recognition where the terminology of weighted automata seems to have been first introduced and made popular [32, 46, 51, 44, 48], in several other speech processing applications such as speech synthesis [54, 2], in phonological and morphological rule compilation [34, 35, 50], in parsing [47], machine translation [23], bioinformatics [27, 3], sequence modeling and prediction [21], formal verification and model checking [5, 4], in optical character recognition [17], and in many other areas.", "startOffset": 523, "endOffset": 529}, {"referenceID": 3, "context": "WFAs are widely used in modern applications, perhaps most prominently in image processing and speech recognition where the terminology of weighted automata seems to have been first introduced and made popular [32, 46, 51, 44, 48], in several other speech processing applications such as speech synthesis [54, 2], in phonological and morphological rule compilation [34, 35, 50], in parsing [47], machine translation [23], bioinformatics [27, 3], sequence modeling and prediction [21], formal verification and model checking [5, 4], in optical character recognition [17], and in many other areas.", "startOffset": 523, "endOffset": 529}, {"referenceID": 16, "context": "WFAs are widely used in modern applications, perhaps most prominently in image processing and speech recognition where the terminology of weighted automata seems to have been first introduced and made popular [32, 46, 51, 44, 48], in several other speech processing applications such as speech synthesis [54, 2], in phonological and morphological rule compilation [34, 35, 50], in parsing [47], machine translation [23], bioinformatics [27, 3], sequence modeling and prediction [21], formal verification and model checking [5, 4], in optical character recognition [17], and in many other areas.", "startOffset": 564, "endOffset": 568}, {"referenceID": 30, "context": "The recent developments in spectral learning [31, 6] have triggered a renewed interest in the use of WFAs in machine learning, with several recent successes in \u2217Corresponding author: b.", "startOffset": 45, "endOffset": 52}, {"referenceID": 5, "context": "The recent developments in spectral learning [31, 6] have triggered a renewed interest in the use of WFAs in machine learning, with several recent successes in \u2217Corresponding author: b.", "startOffset": 45, "endOffset": 52}, {"referenceID": 7, "context": "natural language processing [8, 9] and reinforcement learning [16, 30].", "startOffset": 28, "endOffset": 34}, {"referenceID": 8, "context": "natural language processing [8, 9] and reinforcement learning [16, 30].", "startOffset": 28, "endOffset": 34}, {"referenceID": 15, "context": "natural language processing [8, 9] and reinforcement learning [16, 30].", "startOffset": 62, "endOffset": 70}, {"referenceID": 29, "context": "natural language processing [8, 9] and reinforcement learning [16, 30].", "startOffset": 62, "endOffset": 70}, {"referenceID": 30, "context": "The interest in spectral learning algorithms for WFAs is driven by the many appealing theoretical properties of such algorithms, which include their polynomial-time complexity, the absence of local minima, statistical consistency, and finite sample bounds \u00e0 la PAC [31].", "startOffset": 265, "endOffset": 269}, {"referenceID": 10, "context": "See [11] for a recent survey of algorithms for learning WFAs with a discussion of the different assumptions and learning models.", "startOffset": 4, "endOffset": 8}, {"referenceID": 9, "context": "For spectral learning of WFAs, an algorithm-dependent agnostic generalization bound was proven in [10] using a stability argument.", "startOffset": 98, "endOffset": 102}, {"referenceID": 9, "context": "However, while [10] proposed a broad family of algorithms for learning WFAs parametrized by several choices of loss functions and regularizations, their bounds hold only for one particular algorithm within this family.", "startOffset": 15, "endOffset": 19}, {"referenceID": 9, "context": "In this paper, we start the systematic development of algorithm-independent generalization bounds for learning with WFAs, which apply to all the algorithms proposed in [10], as well as to others using WFAs as their hypothesis class.", "startOffset": 168, "endOffset": 172}, {"referenceID": 36, "context": "The use of Rademacher complexity to derive generalization bounds is standard [37] (see also [13] and [49]).", "startOffset": 77, "endOffset": 81}, {"referenceID": 12, "context": "The use of Rademacher complexity to derive generalization bounds is standard [37] (see also [13] and [49]).", "startOffset": 92, "endOffset": 96}, {"referenceID": 47, "context": "The use of Rademacher complexity to derive generalization bounds is standard [37] (see also [13] and [49]).", "startOffset": 101, "endOffset": 105}, {"referenceID": 47, "context": "see [49] and references therein).", "startOffset": 4, "endOffset": 8}, {"referenceID": 32, "context": "The VC-dimension of deterministic finite automata (DFAs) with n states over an alphabet of size k was shown by [33] to be in O(kn log n).", "startOffset": 111, "endOffset": 115}, {"referenceID": 0, "context": "For probabilistic finite automata (PFAs), it was shown by [1] that, in an agnostic setting, a sample of size \u00d5(kT n/\u03b5) is sufficient to learn a PFA with n states and k symbols whose log-loss error is at most \u03b5 away from the optimal one in the class when the error is measured on all strings of length T .", "startOffset": 58, "endOffset": 61}, {"referenceID": 39, "context": "Another recent line of work, which aims to provide guarantees for spectral learning of WFAs in the non-realizable setting, is the so-called low-rank spectral learning approach [40].", "startOffset": 176, "endOffset": 180}, {"referenceID": 38, "context": "This has led to interesting upper bounds on the approximation error between minimal WFAs of different sizes [39].", "startOffset": 108, "endOffset": 112}, {"referenceID": 11, "context": "See [12] for a polynomial-time algorithm for computing these approximations.", "startOffset": 4, "endOffset": 8}, {"referenceID": 27, "context": "Note that for the purpose of this paper we only consider weighted automata over the familiar field of real numbers with standard addition and multiplication (see [28, 53, 15, 38, 45] for more general definitions of WFAs over arbitrary semirings).", "startOffset": 162, "endOffset": 182}, {"referenceID": 51, "context": "Note that for the purpose of this paper we only consider weighted automata over the familiar field of real numbers with standard addition and multiplication (see [28, 53, 15, 38, 45] for more general definitions of WFAs over arbitrary semirings).", "startOffset": 162, "endOffset": 182}, {"referenceID": 14, "context": "Note that for the purpose of this paper we only consider weighted automata over the familiar field of real numbers with standard addition and multiplication (see [28, 53, 15, 38, 45] for more general definitions of WFAs over arbitrary semirings).", "startOffset": 162, "endOffset": 182}, {"referenceID": 37, "context": "Note that for the purpose of this paper we only consider weighted automata over the familiar field of real numbers with standard addition and multiplication (see [28, 53, 15, 38, 45] for more general definitions of WFAs over arbitrary semirings).", "startOffset": 162, "endOffset": 182}, {"referenceID": 43, "context": "Note that for the purpose of this paper we only consider weighted automata over the familiar field of real numbers with standard addition and multiplication (see [28, 53, 15, 38, 45] for more general definitions of WFAs over arbitrary semirings).", "startOffset": 162, "endOffset": 182}, {"referenceID": 51, "context": "Functions mapping strings to real numbers can also be viewed as non-commutative formal power series, which often helps deriving rigorous proofs in formal language theory [53, 15, 38].", "startOffset": 170, "endOffset": 182}, {"referenceID": 14, "context": "Functions mapping strings to real numbers can also be viewed as non-commutative formal power series, which often helps deriving rigorous proofs in formal language theory [53, 15, 38].", "startOffset": 170, "endOffset": 182}, {"referenceID": 37, "context": "Functions mapping strings to real numbers can also be viewed as non-commutative formal power series, which often helps deriving rigorous proofs in formal language theory [53, 15, 38].", "startOffset": 170, "endOffset": 182}, {"referenceID": 28, "context": "By the theorem of Fliess [29] (see also [18] and [15]), Hf has finite rank n if and only if f is rational and there exists a WFA A with n states computing f , that is, rank(f) = rank(Hf ).", "startOffset": 25, "endOffset": 29}, {"referenceID": 17, "context": "By the theorem of Fliess [29] (see also [18] and [15]), Hf has finite rank n if and only if f is rational and there exists a WFA A with n states computing f , that is, rank(f) = rank(Hf ).", "startOffset": 40, "endOffset": 44}, {"referenceID": 14, "context": "By the theorem of Fliess [29] (see also [18] and [15]), Hf has finite rank n if and only if f is rational and there exists a WFA A with n states computing f , that is, rank(f) = rank(Hf ).", "startOffset": 49, "endOffset": 53}, {"referenceID": 36, "context": "The Rademacher complexity of a hypothesis class can be used to derive generalization bounds for a variety of learning tasks [37, 13, 49].", "startOffset": 124, "endOffset": 136}, {"referenceID": 12, "context": "The Rademacher complexity of a hypothesis class can be used to derive generalization bounds for a variety of learning tasks [37, 13, 49].", "startOffset": 124, "endOffset": 136}, {"referenceID": 47, "context": "The Rademacher complexity of a hypothesis class can be used to derive generalization bounds for a variety of learning tasks [37, 13, 49].", "startOffset": 124, "endOffset": 136}, {"referenceID": 6, "context": "Membership in R1,r was shown to be semi-decidable in [7].", "startOffset": 53, "endOffset": 56}, {"referenceID": 21, "context": "On the other hand, membership in R2,r can be decided in polynomial time [22].", "startOffset": 72, "endOffset": 76}, {"referenceID": 11, "context": "The following result follows from [12] and gives a useful condition for the boundedness of Hf .", "startOffset": 34, "endOffset": 38}, {"referenceID": 21, "context": "Since membership in R2 is efficiently testable [22], a polynomial time algorithm from [12] can be used to compute \u2016f\u2016H,p and thus test membership in Hp,r.", "startOffset": 47, "endOffset": 51}, {"referenceID": 11, "context": "Since membership in R2 is efficiently testable [22], a polynomial time algorithm from [12] can be used to compute \u2016f\u2016H,p and thus test membership in Hp,r.", "startOffset": 86, "endOffset": 90}, {"referenceID": 23, "context": "[24]).", "startOffset": 0, "endOffset": 4}, {"referenceID": 55, "context": "3 in [57]).", "startOffset": 5, "endOffset": 9}, {"referenceID": 9, "context": "The second bound was proven in [10].", "startOffset": 31, "endOffset": 35}, {"referenceID": 40, "context": "Lemma 7 (Massart [42]).", "startOffset": 17, "endOffset": 21}, {"referenceID": 47, "context": "The lower bound is obtained using Khintchine\u2013Kahane\u2019s inequality (see appendix of [49]):", "startOffset": 82, "endOffset": 86}, {"referenceID": 41, "context": "Then, by von Neumann\u2019s trace inequality [43] and H\u00f6lder\u2019s inequality, the following holds:", "startOffset": 40, "endOffset": 44}, {"referenceID": 54, "context": "To bound the Rademacher complexity of Hp,r in the case p = 1 we will need the following moment bound for the operator norm of a random matrix from [56].", "startOffset": 147, "endOffset": 151}, {"referenceID": 54, "context": "2 [56]).", "startOffset": 2, "endOffset": 6}, {"referenceID": 25, "context": "where in the last inequality we used that the VC-dimension of E is 1, in which case Dudley\u2019s chaining method [26] yields Rm(E) \u2264 C \u221a 1/m for some universal constant C > 0.", "startOffset": 109, "endOffset": 113}, {"referenceID": 47, "context": "The proofs of these theorems are a straightforward combination of the bounds on the Rademacher complexity with well-known generalization bounds [49].", "startOffset": 144, "endOffset": 148}, {"referenceID": 30, "context": "Another important feature of our bounds for the classes Hp,r is that they depend on spectral properties of Hankel matrices, which are commonly used in spectral learning algorithms for WFAs [31, 10].", "startOffset": 189, "endOffset": 197}, {"referenceID": 9, "context": "Another important feature of our bounds for the classes Hp,r is that they depend on spectral properties of Hankel matrices, which are commonly used in spectral learning algorithms for WFAs [31, 10].", "startOffset": 189, "endOffset": 197}, {"referenceID": 7, "context": "This is a problem of practical relevance when working with large amounts of data which require balancing trade-offs between computation and accuracy [8].", "startOffset": 149, "endOffset": 152}, {"referenceID": 50, "context": "Nonetheless, the computational complexity of learning from such a sample might be hard, since we know this is the case for DFAs and PFAs [52, 36, 19].", "startOffset": 137, "endOffset": 149}, {"referenceID": 35, "context": "Nonetheless, the computational complexity of learning from such a sample might be hard, since we know this is the case for DFAs and PFAs [52, 36, 19].", "startOffset": 137, "endOffset": 149}, {"referenceID": 18, "context": "Nonetheless, the computational complexity of learning from such a sample might be hard, since we know this is the case for DFAs and PFAs [52, 36, 19].", "startOffset": 137, "endOffset": 149}, {"referenceID": 19, "context": "[20] show DFAs are learnable from positive data generated by \u201ceasy\u201d distributions, and [55] showed that exact learning can be done efficiently when the sample contains short witnesses distinguishing every pair of states).", "startOffset": 0, "endOffset": 4}, {"referenceID": 53, "context": "[20] show DFAs are learnable from positive data generated by \u201ceasy\u201d distributions, and [55] showed that exact learning can be done efficiently when the sample contains short witnesses distinguishing every pair of states).", "startOffset": 87, "endOffset": 91}, {"referenceID": 30, "context": "For PFAs, spectral methods show that polynomial learnability is possible if a new parameter related to spectral properties of the Hankel matrix is added to the complexity [31].", "startOffset": 171, "endOffset": 175}, {"referenceID": 9, "context": "In [10], we proposed an efficient algorithm for", "startOffset": 3, "endOffset": 7}], "year": 2016, "abstractText": "This paper studies the problem of learning weighted automata from a finite labeled training sample. We consider several general families of weighted automata defined in terms of three different measures: the norm of an automaton\u2019s weights, the norm of the function computed by an automaton, or the norm of the corresponding Hankel matrix. We present new data-dependent generalization guarantees for learning weighted automata expressed in terms of the Rademacher complexity of these families. We further present upper bounds on these Rademacher complexities, which reveal key new data-dependent terms related to the complexity of learning weighted automata.", "creator": "TeX"}}}