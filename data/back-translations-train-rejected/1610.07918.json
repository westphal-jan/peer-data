{"id": "1610.07918", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Oct-2016", "title": "Sequence Segmentation Using Joint RNN and Structured Prediction Models", "abstract": "We describe and analyze a simple and effective algorithm for sequence segmentation applied to speech processing tasks. We propose a neural architecture that is composed of two modules trained jointly: a recurrent neural network (RNN) module and a structured prediction model. The RNN outputs are considered as feature functions to the structured model. The overall model is trained with a structured loss function which can be designed to the given segmentation task. We demonstrate the effectiveness of our method by applying it to two simple tasks commonly used in phonetic studies: word segmentation and voice onset time segmentation. Results sug- gest the proposed model is superior to previous methods, ob- taining state-of-the-art results on the tested datasets.", "histories": [["v1", "Tue, 25 Oct 2016 15:21:25 GMT  (138kb,D)", "http://arxiv.org/abs/1610.07918v1", "under review"]], "COMMENTS": "under review", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["yossi adi", "joseph keshet", "emily cibelli", "matthew goldrick"], "accepted": false, "id": "1610.07918"}, "pdf": {"name": "1610.07918.pdf", "metadata": {"source": "CRF", "title": "SEQUENCE SEGMENTATION USING JOINT RNN AND STRUCTURED PREDICTION MODELS", "authors": ["Yossi Adi", "Joseph Keshet", "Emily Cibelli", "Matthew Goldrick"], "emails": [], "sections": [{"heading": null, "text": "It is an important task for many speech and audio applications such as loudspeaker diarization, phonology research, speech synthesis and automatic speech recognition (ASR).Segmentation models can be used as a pre-process step to purify the data (e.g. removing non-speech regions such as music or noise to reduce ASR errors).They can also be used as tools in clinical language - or as theoretically focused phonetic studies that use acoustic properties as a dependent metric. We have a key feature to distinguish between spoken and voiceless consonants in languages [3], which is important in both ASR [4], clinical studies and theoretical studies."}, {"heading": "3.1. Structured Prediction", "text": "We consider the following prediction rule with w-Rd, so that y-Rd is a good approximation of the true name of x-Rd, as follows: (1) y-Rd (x-Rd) = argmax y-Rd (x-Rd, y-Rd). According to the structured prediction framework, we assume that there is an unknown probability distribution over pairs (x-Rd, y-Rd) where y-Rd is the desired power (or reference power) for the input x-Rd. Both x-Rd and y-Rd are normally structured objects such as sequences, trees, etc. Our goal is to set w in such a way that the expected cost or risk is minimized, w-Rd = argmin w E (x-Rd, y-Rd). (y-Rd, w-Rd)]. (2) This objective function is difficult to minimize as the cost distribution is unknown."}, {"heading": "3.2. Recurrent Neural Networks as Feature Functions", "text": "RNN is a deep network architecture that can model the behavior of dynamic time sequences using an internal state that can be thought of as memory (18, 19). RNN provides the ability to predict the current frame label based on previous frames. Bidirectional RNN is a model composed of two RNNs: the first is a standard RNN, while the second reads the input backwards. Such a model can predict the current frame based on past and future frames. By using the RNN outputs, we can jointly train the structured and network models.Recall our prediction rule in Eq. (1): Note that the prediction rule in Eq. (x, y) can be considered as a model."}, {"heading": "4.1. Word Segmentation", "text": "In the problem of word segmentation, we obtain a speech expression containing a single word; our goal is to predict its beginning and end times. The ability to determine these timings is crucial for phonetic studies measuring the speaker characteristics (e.g. response time [23]) or as a pre-processing step for other phonetic analysis tools [11, 10, 9, 8, 24]."}, {"heading": "4.1.1. Dataset", "text": "Our data set comes from a laboratory study by Fink and Goldrick [23]. Native English speakers were shown a set of 90 images, with some participants generating the name of the image (e.g. \"cat,\" \"chair\"), while others performing a semantic classification task (e.g. \"natural,\" \"man-made\"), excluding productions other than the intended reaction or disfluence. Recordings were randomly assigned to two transcriptors commenting on the beginning and offset of each word. we analyze a subset of recordings, including data from 60 participants, evenly distributed across the tasks."}, {"heading": "4.1.2. Results", "text": "We compare our model to an RNN trained using the Negative Log Liklihood (NLL). The NLL model makes a binary decision in each frame to predict whether or not language activity is present. Remember, our goal is to find the beginning and end times of the word; in this task, the RNN leaves us with a distribution over all possible influences. To take this into account, we apply a smoothing algorithm and find the most likely time pair. We trained the DeepSegmentor model using the structured loss function as described in (6), called Combined Duration (CD) loss. The motivation for using this function is based on differences in the manual annotations, which are common and both by human error and objective difficulties in placing the 1All models were implemented with Torch7 toolkit [21, 22] limits."}, {"heading": "4.2. VOT Segmentation", "text": "As already mentioned in the introduction, it is used in theoretical and clinical studies as well as in ASR tasks. In this problem, the input is a speech expression containing a single stop consonant, and the output is the VOT insertion and offset time. We have compared our model with two other methods for VOT measurement: First, with the AutoVOT algorithm [9]. This algorithm follows the structured predictive approach of the linear classifier with handmade features and feature functions. The second algorithm is the DeepVOT algorithm [11]. This algorithm uses RNNNNs with NLL as a loss function, so it predicts for each image whether it is related to the VOT or not."}, {"heading": "4.2.1. Datasets", "text": "We use two different sets of data. The first, PGWORDS, comes from a laboratory study by Paterson and Goldrick [6]. American English Monolinguals and Brazilian Portuguese (L1) -English Bilingual (24 participants each) gave a set of 144 images, excluding productions other than the intended label, as well as those with code switching or disfluences; the second set, BB, consists of spontaneous speech from the 2008 season of Big Brother UK, a British reality TV show [27, 9], recorded from 4 speakers in the \"diary room,\" an acoustically clean environment. VOTs were measured manually by two transcripts. 4.2.Results - PGWORDS For the PWORDS data, we use two layers of bi-directional LSTMs with dropout."}, {"heading": "4.2.3. Results \u2014 BB", "text": "We have experience with bidirectional LSTMs as well as with LSTM, but only forward LSTM works better on this dataset. We use (6) as our loss function. We use the same functions as in [9, 11], in total we have 51 functions per frame. We optimize the networks using AdaGrad optimization. All parameters have been tuned to a dedicated development. Table 3 summarizes the results using the loss function as in [9]. It is worth noting that we have the same behavior on this dataset as in relation to the DeepVOT preforms better then the DeepSegmentor in hight tolerance values.Future work will explore timing sequence of length greater than 2 - for instance, in phonic segmentation, where the sequence varies across training examples."}], "references": [{"title": "Transcribing radio news", "author": ["Francis Kubala", "Tasos Anastasakos", "Hubert Jin", "Long Nguyen", "Richard Schwartz"], "venue": "ICSLP, 1996, vol. 2, pp. 598\u2013601.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1996}, {"title": "Audio segmentation for speech recognition using segment features", "author": ["David Rybach", "Christian Gollan", "Ralf Schluter", "Hermann Ney"], "venue": "ICASSP, 2009, pp. 4197\u2013 4200.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2009}, {"title": "A cross-language study of voicing in initial stops: acoustical measurements", "author": ["L. Lisker", "A. Abramson"], "venue": "Word, vol. 20, pp. 384\u2013422, 1964.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1964}, {"title": "Automatic voice onset time detection for unvoiced stops (/p/,/t/,/k/) with application to accent classification", "author": ["J.H.L. Hansen", "S.S. Gray", "W. Kim"], "venue": "Speech Commun., vol. 52, pp. 777\u2013789, 2010.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2010}, {"title": "Voice onset time in aphasia, apraxia of speech and dysarthria: a review", "author": ["P. Auzou", "C. Ozsancak", "R.J. Morris", "M. Jan", "F. Eustache", "D. Hannequin"], "venue": "Clin. Linguist. Phonet., vol. 14, pp. 131\u2013150, 2000.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2000}, {"title": "Interactions in Bilingual Speech Processing", "author": ["Nattalia Paterson"], "venue": "Ph.D. thesis, Northwestern University,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2011}, {"title": "Automatic phonetic segmentation", "author": ["Doroteo Torre Toledano", "Luis A Hern\u00e1ndez G\u00f3mez", "Luis Villarrubia Grande"], "venue": "IEEE transactions on speech and audio processing, vol. 11, no. 6, pp. 617\u2013625, 2003.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2003}, {"title": "A large margin algorithm for speech-tophoneme and music-to-score alignment", "author": ["Joseph Keshet", "Shai Shalev-Shwartz", "Yoram Singer", "Dan Chazan"], "venue": "IEEE Transactions on Audio, Speech, and Language Processing, vol. 15, no. 8, pp. 2373\u20132382, 2007.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2007}, {"title": "Automatic measurement of voice onset time using discriminative structured predictiona)", "author": ["Morgan Sonderegger", "Joseph Keshet"], "venue": "JASA, vol. 132, no. 6, pp. 3965\u20133979, 2012.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2012}, {"title": "Vowel duration measurement using deep neural networks", "author": ["Yossi Adi", "Joseph Keshet", "Matthew Goldrick"], "venue": "MLSP, 2015, pp. 1\u20136.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "Automatic measurement of voice onset time and prevoicing using recurrent neural networks", "author": ["Yossi Adi", "Joseph Keshet", "Olga Dmitrieva", "Matt Goldrick"], "venue": ".", "citeRegEx": "11", "shortCiteRegEx": null, "year": 0}, {"title": "Neural conditional random fields", "author": ["Trinh Do", "Thierry Arti"], "venue": "AISTATS, 2010, pp. 177\u2013184.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2010}, {"title": "Conditional random fields as recurrent neural networks", "author": ["Shuai Zheng", "Sadeep Jayasumana", "Bernardino Romera- Paredes", "Vibhav Vineet", "Zhizhong Su", "Dalong Du", "Chang Huang", "Philip HS Torr"], "venue": "ICCV, 2015, pp. 1529\u20131537.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning deep structured models", "author": ["Liang-Chieh Chen", "Alexander G Schwing", "Alan L Yuille", "Raquel Urtasun"], "venue": "ICML, 2015.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2015}, {"title": "Simple and accurate dependency parsing using bidirectional lstm feature representations", "author": ["Eliyahu Kiperwasser", "Yoav Goldberg"], "venue": "arXiv preprint, 2016.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2016}, {"title": "Neural architectures for named entity recognition", "author": ["Guillaume Lample", "Miguel Ballesteros", "Sandeep Subramanian", "Kazuya Kawakami", "Chris Dyer"], "venue": "arXiv preprint, 2016.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2016}, {"title": "Large margin methods for structured and interdependent output variables", "author": ["Ioannis Tsochantaridis", "Thorsten Joachims", "Thomas Hofmann", "Yasemin Altun"], "venue": "JMLR, 2005, pp. 1453\u20131484.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2005}, {"title": "Distributed representations, simple recurrent networks, and grammatical structure", "author": ["Jeffrey L. Elman"], "venue": "Machine learning, vol. 7, no. 2-3, pp. 195\u2013225, 1991.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1991}, {"title": "Speech recognition with deep recurrent neural networks", "author": ["Alan Graves", "Abdel-rahman Mohamed", "Geoffrey Hinton"], "venue": "ICASSP, 2013, pp. 6645\u20136649.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2013}, {"title": "Learning representations by backpropagating errors", "author": ["David E Rumelhart", "Geoffrey E Hinton", "Ronald J Williams"], "venue": "Cognitive modeling, vol. 5, no. 3, pp. 1, 1988.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 1988}, {"title": "Torch7: A matlab-like environment for machine learning", "author": ["Ronan Collobert", "Koray Kavukcuoglu", "Cl\u00e9ment Farabet"], "venue": "BigLearn, NIPS Workshop, 2011, number EPFL-CONF-192376.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2011}, {"title": "rnn: Recurrent library for torch", "author": ["Nicholas L\u00e9onard", "Sagar Waghmare", "Yang Wang"], "venue": "arXiv preprint, 2015.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2015}, {"title": "The Role of Domain-General Executive Functions, Conceptualization, and Articulation during Spoken Word Production, Ph.D", "author": ["Angela Fink"], "venue": null, "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2016}, {"title": "Fave (forced alignment and vowel extraction)", "author": ["Ingrid Rosenfelder", "Josef Fruehwald", "Keelan Evanini", "Scott Seyfarth", "Kyle Gorman", "Hilary Prichard", "Jiahong Yuan"], "venue": "Program suite v1.2.2 10.5281/zenodo.22281, 2014.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2014}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors", "author": ["Geoffrey E. Hinton", "Nitish Srivastava", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov"], "venue": "CoRR, 2012.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2012}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["John Duchi", "Elad Hazan", "Yoram Singer"], "venue": "JMLR, vol. 12, pp. 2121\u20132159, 2011.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2011}, {"title": "Longitudinal phonetic variation in a closed system", "author": ["Max Bane", "Peter Graff", "Morgan Sonderegger"], "venue": "Proc. CLS, vol. 46, pp. 43\u201358, 2010.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2010}], "referenceMentions": [{"referenceID": 0, "context": ", removing non-speech regions such as music or noise to reduce ASR error [1, 2]).", "startOffset": 73, "endOffset": 79}, {"referenceID": 1, "context": ", removing non-speech regions such as music or noise to reduce ASR error [1, 2]).", "startOffset": 73, "endOffset": 79}, {"referenceID": 2, "context": "For example, voice onset time, a key feature distinguishing voiced and voiceless consonants across languages [3], is important both in ASR [4], clinical [5], and theoretical studies [6].", "startOffset": 109, "endOffset": 112}, {"referenceID": 3, "context": "For example, voice onset time, a key feature distinguishing voiced and voiceless consonants across languages [3], is important both in ASR [4], clinical [5], and theoretical studies [6].", "startOffset": 139, "endOffset": 142}, {"referenceID": 4, "context": "For example, voice onset time, a key feature distinguishing voiced and voiceless consonants across languages [3], is important both in ASR [4], clinical [5], and theoretical studies [6].", "startOffset": 153, "endOffset": 156}, {"referenceID": 5, "context": "For example, voice onset time, a key feature distinguishing voiced and voiceless consonants across languages [3], is important both in ASR [4], clinical [5], and theoretical studies [6].", "startOffset": 182, "endOffset": 185}, {"referenceID": 6, "context": "Previous work on speech sequence segmentation focuses on generative models such as hidden Markov models (see for example [7] and the references therein); on discriminative methods [2, 8, 9]; or on deep learning [10, 11].", "startOffset": 121, "endOffset": 124}, {"referenceID": 1, "context": "Previous work on speech sequence segmentation focuses on generative models such as hidden Markov models (see for example [7] and the references therein); on discriminative methods [2, 8, 9]; or on deep learning [10, 11].", "startOffset": 180, "endOffset": 189}, {"referenceID": 7, "context": "Previous work on speech sequence segmentation focuses on generative models such as hidden Markov models (see for example [7] and the references therein); on discriminative methods [2, 8, 9]; or on deep learning [10, 11].", "startOffset": 180, "endOffset": 189}, {"referenceID": 8, "context": "Previous work on speech sequence segmentation focuses on generative models such as hidden Markov models (see for example [7] and the references therein); on discriminative methods [2, 8, 9]; or on deep learning [10, 11].", "startOffset": 180, "endOffset": 189}, {"referenceID": 9, "context": "Previous work on speech sequence segmentation focuses on generative models such as hidden Markov models (see for example [7] and the references therein); on discriminative methods [2, 8, 9]; or on deep learning [10, 11].", "startOffset": 211, "endOffset": 219}, {"referenceID": 10, "context": "Previous work on speech sequence segmentation focuses on generative models such as hidden Markov models (see for example [7] and the references therein); on discriminative methods [2, 8, 9]; or on deep learning [10, 11].", "startOffset": 211, "endOffset": 219}, {"referenceID": 11, "context": "Inspired by the recent work on combined deep network and structured prediction models [12, 13, 14, 15, 16], we would like to further improve performance on speech sequence segmentation and propose a new efficient joint deep", "startOffset": 86, "endOffset": 106}, {"referenceID": 12, "context": "Inspired by the recent work on combined deep network and structured prediction models [12, 13, 14, 15, 16], we would like to further improve performance on speech sequence segmentation and propose a new efficient joint deep", "startOffset": 86, "endOffset": 106}, {"referenceID": 13, "context": "Inspired by the recent work on combined deep network and structured prediction models [12, 13, 14, 15, 16], we would like to further improve performance on speech sequence segmentation and propose a new efficient joint deep", "startOffset": 86, "endOffset": 106}, {"referenceID": 14, "context": "Inspired by the recent work on combined deep network and structured prediction models [12, 13, 14, 15, 16], we would like to further improve performance on speech sequence segmentation and propose a new efficient joint deep", "startOffset": 86, "endOffset": 106}, {"referenceID": 15, "context": "Inspired by the recent work on combined deep network and structured prediction models [12, 13, 14, 15, 16], we would like to further improve performance on speech sequence segmentation and propose a new efficient joint deep", "startOffset": 86, "endOffset": 106}, {"referenceID": 16, "context": "In this work the surrogate loss function is the structural hinge loss [17] defined as", "startOffset": 70, "endOffset": 74}, {"referenceID": 17, "context": "RNN is a deep network architecture that can model the behavior of dynamic temporal sequences using an internal state which can be thought of as memory [18, 19].", "startOffset": 151, "endOffset": 159}, {"referenceID": 18, "context": "RNN is a deep network architecture that can model the behavior of dynamic temporal sequences using an internal state which can be thought of as memory [18, 19].", "startOffset": 151, "endOffset": 159}, {"referenceID": 19, "context": "In order to optimize the network parameters using the backpropagation algorithm [20], we must find the outer derivative of each layer with respect to the model parameters and inputs.", "startOffset": 80, "endOffset": 84}, {"referenceID": 22, "context": "response time [23]) or as a preprocessing step for other phonetic analysis tools [11, 10, 9, 8, 24].", "startOffset": 14, "endOffset": 18}, {"referenceID": 10, "context": "response time [23]) or as a preprocessing step for other phonetic analysis tools [11, 10, 9, 8, 24].", "startOffset": 81, "endOffset": 99}, {"referenceID": 9, "context": "response time [23]) or as a preprocessing step for other phonetic analysis tools [11, 10, 9, 8, 24].", "startOffset": 81, "endOffset": 99}, {"referenceID": 8, "context": "response time [23]) or as a preprocessing step for other phonetic analysis tools [11, 10, 9, 8, 24].", "startOffset": 81, "endOffset": 99}, {"referenceID": 7, "context": "response time [23]) or as a preprocessing step for other phonetic analysis tools [11, 10, 9, 8, 24].", "startOffset": 81, "endOffset": 99}, {"referenceID": 23, "context": "response time [23]) or as a preprocessing step for other phonetic analysis tools [11, 10, 9, 8, 24].", "startOffset": 81, "endOffset": 99}, {"referenceID": 22, "context": "Our dataset comes from a laboratory study by Fink and Goldrick [23].", "startOffset": 63, "endOffset": 67}, {"referenceID": 20, "context": "1All models were implemented using Torch7 toolkit [21, 22] boundaries.", "startOffset": 50, "endOffset": 58}, {"referenceID": 21, "context": "1All models were implemented using Torch7 toolkit [21, 22] boundaries.", "startOffset": 50, "endOffset": 58}, {"referenceID": 24, "context": "We use two layers of bidirectional LSTMs for the DeepSegmentor model with dropout [25] after each recurrent layer.", "startOffset": 82, "endOffset": 86}, {"referenceID": 25, "context": "We optimize the networks using AdaGrad [26].", "startOffset": 39, "endOffset": 43}, {"referenceID": 8, "context": "First is the AutoVOT algorithm [9].", "startOffset": 31, "endOffset": 34}, {"referenceID": 10, "context": "The second algorithm is the DeepVOT algorithm [11].", "startOffset": 46, "endOffset": 50}, {"referenceID": 5, "context": "The first one, PGWORDS, is from a laboratory study by Paterson and Goldrick [6].", "startOffset": 76, "endOffset": 79}, {"referenceID": 26, "context": "The second dataset, BB, consists of spontaneous speech from the 2008 season of Big Brother UK, a British reality television show [27, 9].", "startOffset": 129, "endOffset": 136}, {"referenceID": 8, "context": "The second dataset, BB, consists of spontaneous speech from the 2008 season of Big Brother UK, a British reality television show [27, 9].", "startOffset": 129, "endOffset": 136}, {"referenceID": 8, "context": "The input features are the same as in [9, 11]; overall we have 63 features per frame.", "startOffset": 38, "endOffset": 45}, {"referenceID": 10, "context": "The input features are the same as in [9, 11]; overall we have 63 features per frame.", "startOffset": 38, "endOffset": 45}, {"referenceID": 8, "context": "Table 2 summarizes the results using the same loss function as in [9].", "startOffset": 66, "endOffset": 69}, {"referenceID": 8, "context": "We use the same features as in [9, 11], overall we have 51 features per frame.", "startOffset": 31, "endOffset": 38}, {"referenceID": 10, "context": "We use the same features as in [9, 11], overall we have 51 features per frame.", "startOffset": 31, "endOffset": 38}, {"referenceID": 8, "context": "Table 3 summarize the results using the loss function as in [9].", "startOffset": 60, "endOffset": 63}], "year": 2016, "abstractText": "We describe and analyze a simple and effective algorithm for sequence segmentation applied to speech processing tasks. We propose a neural architecture that is composed of two modules trained jointly: a recurrent neural network (RNN) module and a structured prediction model. The RNN outputs are considered as feature functions to the structured model. The overall model is trained with a structured loss function which can be designed to the given segmentation task. We demonstrate the effectiveness of our method by applying it to two simple tasks commonly used in phonetic studies: word segmentation and voice onset time segmentation. Results suggest the proposed model is superior to previous methods, obtaining state-of-the-art results on the tested datasets.", "creator": "LaTeX with hyperref package"}}}