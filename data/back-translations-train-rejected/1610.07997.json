{"id": "1610.07997", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Oct-2016", "title": "Artificial Intelligence Safety and Cybersecurity: a Timeline of AI Failures", "abstract": "In this work, we present and analyze reported failures of artificially intelligent systems and extrapolate our analysis to future AIs. We suggest that both the frequency and the seriousness of future AI failures will steadily increase. AI Safety can be improved based on ideas developed by cybersecurity experts. For narrow AIs safety failures are at the same, moderate, level of criticality as in cybersecurity, however for general AI, failures have a fundamentally different impact. A single failure of a superintelligent system may cause a catastrophic event without a chance for recovery. The goal of cybersecurity is to reduce the number of successful attacks on the system; the goal of AI Safety is to make sure zero attacks succeed in bypassing the safety mechanisms. Unfortunately, such a level of performance is unachievable. Every security system will eventually fail; there is no such thing as a 100% secure system.", "histories": [["v1", "Tue, 25 Oct 2016 18:14:24 GMT  (274kb)", "http://arxiv.org/abs/1610.07997v1", null]], "reviews": [], "SUBJECTS": "cs.AI cs.CY", "authors": ["roman v yampolskiy", "m s spellchecker"], "accepted": false, "id": "1610.07997"}, "pdf": {"name": "1610.07997.pdf", "metadata": {"source": "CRF", "title": "Artificial Intelligence Safety and Cybersecurity: a Timeline of AI Failures", "authors": ["Roman V. Yampolskiy"], "emails": ["roman.yampolskiy@louisville.edu", "msspell@microsoft.com"], "sections": [{"heading": null, "text": "In this paper, we present and analyze reported failures of artificially intelligent systems and extrapolate our analysis of future AI. We suggest that both the frequency and severity of future AI failures will steadily increase. AI security can be improved on the basis of ideas developed by cybersecurity experts. In narrow AI security bugs, the level of criticism is at the same moderate level as in cybersecurity, but for general AI bugs have fundamentally different effects. A single failure of a super-intelligent system can cause a catastrophic event with no chance of recovery. The aim of cybersecurity is to reduce the number of successful attacks on the system; the aim of AI security is to ensure that zero attacks succeed in circumventing the security mechanisms. Unfortunately, such a level of performance is unattainable. Every security system will eventually fail; there is no 100% secure system.Keywords: AI security, cybersecurity, failure, superintelligence."}, {"heading": "1. Introduction", "text": "In fact, it is so that most people are able to determine for themselves what they want and what they want. (...) In fact, it is so that they are able to determine for themselves. (...) It is so that they do not want it. (...) It is so that they do not want it. (...) It is so that they do not want it. (...) \"It is so that they do not want it. (...)\" \"(...)\" (...) \"(...)\" (...) \"(...)\" (...) \"(...\") \"(...\" () \"(...\") \"(...\") \"(...)\" () \"(...\") (... \"()\" () \"(...\") (... \"() () () () () () () () () () () () () () () () () () ()) () () () () () () () () () ()) () () () () () () () () () ()) () () () () () () () ()) () () () () () ()) () () () ()) () () () () ()) () () () () ()) () () () () () ()) () () () () () () () () () () () () () () () () () () () () ().) () () () () () () () () () () () () () () () () () () () () () () ()."}, {"heading": "2. AI Failures", "text": "In fact, most of them are able to reform themselves."}, {"heading": "3. AI Safety and Security", "text": "In fact, most of them will be able to play by the rules they have shown in the past, and they will be able to play by the rules they have shown in the past."}, {"heading": "4. Cybersecurity vs. AI Safety", "text": "This year, it has come to the point that it has never come as far as it has this year."}, {"heading": "5. Conclusions", "text": "In fact, we will be able to manoeuvre ourselves into a situation in which we are able to do things that we do not want to do."}, {"heading": "Acknowledgements", "text": "The author is grateful to Elon Musk and the Future of Life Institute as well as Jaan Tallinn and Effective Altruism Ventures for partially funding his work. In particular, the author is grateful to Yana Feygin and S\u00f8ren Elverlin for proofreading a draft of this work. He is also grateful to his Facebook and Twitter contacts for providing examples of AI bugs."}], "references": [{"title": "Taxonomy of Pathways to Dangerous Artificial Intelligence", "author": ["R.V. Yampolskiy"], "venue": "Workshops at the Thirtieth AAAI Conference on Artificial Intelligence, 2016.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2016}, {"title": "Unethical Research: How to Create a Malevolent Artificial Intelligence", "author": ["F. Pistono", "R.V. Yampolskiy"], "venue": "presented at the 25th International Joint Conference on Artificial Intelligence (IJCAI-16). Ethics for Artificial Intelligence Workshop (AI-Ethics-2016), New York, NY, July 9, 2016.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2016}, {"title": "Responses to Catastrophic AGI Risk: A Survey", "author": ["K. Sotala", "R.V. Yampolskiy"], "venue": "Physica Scripta, vol. 90, 2015.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2015}, {"title": "Artificial intelligence as a positive and negative factor in global risk", "author": ["E. Yudkowsky"], "venue": "Global catastrophic risks, vol. 1, p. 303, 2008.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2008}, {"title": "Learning to Drive a Bicycle Using Reinforcement Learning and Shaping", "author": ["J. Randl\u00f8v", "P. Alstr\u00f8m"], "venue": "ICML, 1998, pp. 463-471.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1998}, {"title": "The first level of Super Mario Bros. is easy with lexicographic orderings and time travel", "author": ["VII T.M."], "venue": "The Association for Computational Heresy (SIGBOVIK) 2013, 2013.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2013}, {"title": "Policy invariance under reward transformations: Theory and application to reward shaping", "author": ["A.Y. Ng", "D. Harada", "S. Russell"], "venue": "ICML, 1999, pp. 278-287.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1999}, {"title": "Unethical Research: How to Create a Malevolent Artificial Intelligence", "author": ["F. Pistono", "R.V. Yampolskiy"], "venue": "arXiv preprint arXiv:1605.02817, 2016.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2016}, {"title": "Autonomous Weapons and Operational Risk", "author": ["P. Scharre"], "venue": "presented at the Center for a New American Society, Washington DC, 2016.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2016}, {"title": "Intriguing properties of neural networks", "author": ["C. Szegedy", "W. Zaremba", "I. Sutskever", "J. Bruna", "D. Erhan", "I. Goodfellow"], "venue": "arXiv preprint arXiv:1312.6199, 2013.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2013}, {"title": "Artificial Intelligence Safety Engineering: Why Machine Ethics is a Wrong Approach", "author": ["R.V. Yampolskiy"], "venue": "presented at the Philosophy and Theory of Artificial Intelligence (PT- AI2011), Thessaloniki, Greece, October 3-4, 2011.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2011}, {"title": "Safety Engineering for Artificial General Intelligence", "author": ["R.V. Yampolskiy", "J. Fox"], "venue": "Topoi. Special Issue on Machine Ethics & the Ethics of Building Intelligent Machines, 2012.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2012}, {"title": "Roman Yampolskiy on AI Safety Engineering", "author": ["L. Muehlhauser", "R. Yampolskiy"], "venue": "presented at the Machine Intelligence Research Institute, Available at: http://intelligence.org/2013/07/15/roman-interview/ July 15, 2013.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2013}, {"title": "Artificial intelligence safety engineering: Why machine ethics is a wrong approach", "author": ["R.V. Yampolskiy"], "venue": "Philosophy and Theory of Artificial Intelligence, ed: Springer Berlin Heidelberg, 2013, pp. 389-396.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2013}, {"title": "AI safety engineering through introduction of selfreference into felicific calculus via artificial pain and pleasure", "author": ["A.M. Majot", "R.V. Yampolskiy"], "venue": "IEEE International Symposium on Ethics in Science, Technology and Engineering, Chicago, IL, May 23-24, 2014, pp. 1-6.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2014}, {"title": "Artificial Superintelligence: a Futuristic Approach", "author": ["R.V. Yampolskiy"], "venue": "Chapman and Hall/CRC,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2015}, {"title": "The nature, importance, and difficulty of machine ethics", "author": ["J.H. Moor"], "venue": "IEEE intelligent systems, vol. 21, pp. 18-21, 2006.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2006}, {"title": "Creating friendly AI 1.0: The analysis and design of benevolent goal architectures", "author": ["E. Yudkowsky"], "venue": "Singularity Institute for Artificial Intelligence, San Francisco, CA, June, vol. 15, 2001.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2001}, {"title": "Concrete Problems in AI Safety", "author": ["D. Amodei", "C. Olah", "J. Steinhardt", "P. Christiano", "J. Schulman", "D. Man\u00e9"], "venue": "arXiv preprint arXiv:1606.06565, 2016.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2016}, {"title": "Leakproofing the Singularity Artificial Intelligence Confinement Problem", "author": ["R. Yampolskiy"], "venue": "Journal of Consciousness Studies, vol. 19, pp. 1-2, 2012.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2012}, {"title": "The AGI Containment Problem", "author": ["J. Babcock", "J. Kramar", "R. Yampolskiy"], "venue": "arXiv preprint arXiv:1604.00545, 2016.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2016}, {"title": "The AGI Containment Problem", "author": ["J. Babcock", "J. Kramar", "R. Yampolskiy"], "venue": "The Ninth Conference on Artificial General Intelligence (AGI2015), 2016.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2015}, {"title": "Security Solutions for Intelligent and Complex Systems", "author": ["S. Armstrong", "R.V. Yampolskiy"], "venue": "Security Solutions for Hyperconnectivity and the Internet of Things, ed: IGI Global, 2016, pp. 37-88.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2016}, {"title": "Reducibility Among Combinatorial Problems", "author": ["R.M. Karp"], "venue": "Complexity of Computer Computations, R. E. Miller and J. W. Thatcher, Eds., ed New York: Plenum, 1972, pp. 85- 103.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 1972}, {"title": "Turing Test as a Defining Feature of AI-Completeness", "author": ["R. Yampolskiy"], "venue": "Artificial Intelligence, Evolutionary Computing and Metaheuristics. vol. 427, X.-S. Yang, Ed., ed: Springer Berlin Heidelberg, 2013, pp. 3-17.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2013}, {"title": "AI-Complete, AI-Hard, or AI-Easy\u2013Classification of Problems in AI", "author": ["R.V. Yampolskiy"], "venue": "The 23rd Midwest Artificial Intelligence and Cognitive Science Conference, Cincinnati, OH, USA, 2012.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2012}, {"title": "Efficiency Theory: a Unifying Theory for Information, Computation and Intelligence", "author": ["R.V. Yampolskiy"], "venue": "Journal of Discrete Mathematical Sciences & Cryptography, vol. 16(4-5), pp. 259-277, 2013.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2013}, {"title": "AI-Complete CAPTCHAs as Zero Knowledge Proofs of Access to an Artificially Intelligent System", "author": ["R.V. Yampolskiy"], "venue": "ISRN Artificial Intelligence, vol. 271878, 2011.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2011}, {"title": "On the Origin of Samples: Attribution of Output to a Particular Algorithm", "author": ["R.V. Yampolskiy"], "venue": "arXiv preprint arXiv:1608.06172, 2016.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2016}, {"title": "Defining Human Values for Value Learners", "author": ["K. Sotala"], "venue": "2nd International Workshop on AI, Ethics and Society, AAAI-2016, 2016.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2016}, {"title": "Learning what to value", "author": ["D. Dewey"], "venue": "Artificial General Intelligence, pp. 309-314, 2011.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2011}, {"title": "Aligning superintelligence with human interests: A technical research agenda", "author": ["N. Soares", "B. Fallenstein"], "venue": "Machine Intelligence Research Institute (MIRI) technical report, vol. 8, 2014.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2014}, {"title": "Verifier Theory and Unverifiability", "author": ["R.V. Yampolskiy"], "venue": "arXiv preprint arXiv:1609.00331, 2016.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2016}, {"title": "The superintelligent will: Motivation and instrumental rationality in advanced artificial agents", "author": ["N. Bostrom"], "venue": "Minds and Machines, vol. 22, pp. 71-85, 2012.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2012}, {"title": "Welcome to Less Wrong! (5th thread, March 2013) \" presented at the Less Wrong", "author": ["R. Yampolskiy"], "venue": "Available at: http://lesswrong.com/lw/h3p/welcome_to_less_wrong_5th_thread_march_2013,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2013}], "referenceMentions": [{"referenceID": 0, "context": "In a recent publication, we proposed a Taxonomy of Pathways to Dangerous AI [1], which was motivated as follows: \u201cIn order to properly handle a potentially dangerous artificially intelligent", "startOffset": 76, "endOffset": 79}, {"referenceID": 1, "context": "A follow up paper [2] explored how a Malevolent AI could be constructed and why it is important to study and understand malicious intelligent software.", "startOffset": 18, "endOffset": 21}, {"referenceID": 2, "context": "In the domain of AI Safety Engineering, hundreds of papers [3] have been published on different proposals geared at the creation of a safe machine, yet nothing else has been published on how to design a malevolent machine.", "startOffset": 59, "endOffset": 62}, {"referenceID": 3, "context": "A frequently cited example is a computer vision system which was supposed to classify pictures of tanks but instead learned to distinguish backgrounds of such images [4].", "startOffset": 166, "endOffset": 169}, {"referenceID": 4, "context": "Other examples include problems caused by poorly-designed utility functions rewarding only partially desirable behaviors of agents, such as riding a bicycle in circles around the target [5], pausing a game to avoid losing [6], or repeatedly touching a soccer ball to get credit for possession [7].", "startOffset": 186, "endOffset": 189}, {"referenceID": 5, "context": "Other examples include problems caused by poorly-designed utility functions rewarding only partially desirable behaviors of agents, such as riding a bicycle in circles around the target [5], pausing a game to avoid losing [6], or repeatedly touching a soccer ball to get credit for possession [7].", "startOffset": 222, "endOffset": 225}, {"referenceID": 6, "context": "Other examples include problems caused by poorly-designed utility functions rewarding only partially desirable behaviors of agents, such as riding a bicycle in circles around the target [5], pausing a game to avoid losing [6], or repeatedly touching a soccer ball to get credit for possession [7].", "startOffset": 293, "endOffset": 296}, {"referenceID": 0, "context": "During the performance phase, the system may succumb to a number of possible causes [1, 8, 9] all leading to an AI Failure.", "startOffset": 84, "endOffset": 93}, {"referenceID": 7, "context": "During the performance phase, the system may succumb to a number of possible causes [1, 8, 9] all leading to an AI Failure.", "startOffset": 84, "endOffset": 93}, {"referenceID": 8, "context": "During the performance phase, the system may succumb to a number of possible causes [1, 8, 9] all leading to an AI Failure.", "startOffset": 84, "endOffset": 93}, {"referenceID": 9, "context": "2013 Object recognition neural networks saw phantom objects in particular noise images [10].", "startOffset": 87, "endOffset": 91}, {"referenceID": 10, "context": "He formally presented his ideas on AI Safety at a peer-reviewed conference in 2011 [11], with", "startOffset": 83, "endOffset": 87}, {"referenceID": 11, "context": "subsequent publications on the topic in 2012 [12], 2013 [13, 14], 2014 [15], 2015 [16], 2016 [1, 8].", "startOffset": 45, "endOffset": 49}, {"referenceID": 12, "context": "subsequent publications on the topic in 2012 [12], 2013 [13, 14], 2014 [15], 2015 [16], 2016 [1, 8].", "startOffset": 56, "endOffset": 64}, {"referenceID": 13, "context": "subsequent publications on the topic in 2012 [12], 2013 [13, 14], 2014 [15], 2015 [16], 2016 [1, 8].", "startOffset": 56, "endOffset": 64}, {"referenceID": 14, "context": "subsequent publications on the topic in 2012 [12], 2013 [13, 14], 2014 [15], 2015 [16], 2016 [1, 8].", "startOffset": 71, "endOffset": 75}, {"referenceID": 15, "context": "subsequent publications on the topic in 2012 [12], 2013 [13, 14], 2014 [15], 2015 [16], 2016 [1, 8].", "startOffset": 82, "endOffset": 86}, {"referenceID": 0, "context": "subsequent publications on the topic in 2012 [12], 2013 [13, 14], 2014 [15], 2015 [16], 2016 [1, 8].", "startOffset": 93, "endOffset": 99}, {"referenceID": 7, "context": "subsequent publications on the topic in 2012 [12], 2013 [13, 14], 2014 [15], 2015 [16], 2016 [1, 8].", "startOffset": 93, "endOffset": 99}, {"referenceID": 16, "context": "Before that the most common names for the relevant concepts were \u201cMachine Ethics\u201d [17] or \u201cFriendly AI\u201d [18].", "startOffset": 82, "endOffset": 86}, {"referenceID": 17, "context": "Before that the most common names for the relevant concepts were \u201cMachine Ethics\u201d [17] or \u201cFriendly AI\u201d [18].", "startOffset": 104, "endOffset": 108}, {"referenceID": 18, "context": "Today the term \u201cAI Safety\u201d appears to be the accepted name for the field used by a majority of top researchers [19].", "startOffset": 111, "endOffset": 115}, {"referenceID": 19, "context": "Attempts have been made to introduce techniques which were first developed by cybersecurity experts to secure software systems to this new domain of securing intelligent machines [20-23].", "startOffset": 179, "endOffset": 186}, {"referenceID": 20, "context": "Attempts have been made to introduce techniques which were first developed by cybersecurity experts to secure software systems to this new domain of securing intelligent machines [20-23].", "startOffset": 179, "endOffset": 186}, {"referenceID": 21, "context": "Attempts have been made to introduce techniques which were first developed by cybersecurity experts to secure software systems to this new domain of securing intelligent machines [20-23].", "startOffset": 179, "endOffset": 186}, {"referenceID": 22, "context": "Attempts have been made to introduce techniques which were first developed by cybersecurity experts to secure software systems to this new domain of securing intelligent machines [20-23].", "startOffset": 179, "endOffset": 186}, {"referenceID": 23, "context": "In theoretical computer science, a common way of isolating the essence of a difficult problem is via the method of reduction to another, sometimes better analyzed, problem [24-26].", "startOffset": 172, "endOffset": 179}, {"referenceID": 24, "context": "In theoretical computer science, a common way of isolating the essence of a difficult problem is via the method of reduction to another, sometimes better analyzed, problem [24-26].", "startOffset": 172, "endOffset": 179}, {"referenceID": 25, "context": "In theoretical computer science, a common way of isolating the essence of a difficult problem is via the method of reduction to another, sometimes better analyzed, problem [24-26].", "startOffset": 172, "endOffset": 179}, {"referenceID": 26, "context": "If such a reduction is a possibility and is computationally efficient [27], such a reduction implies that if the", "startOffset": 70, "endOffset": 74}, {"referenceID": 24, "context": "Formally such a reduction can be done via restricted Turing Test in the domain of safety in a manner identical to how AI-Completeness of a problem could be established [25, 28].", "startOffset": 168, "endOffset": 176}, {"referenceID": 27, "context": "Formally such a reduction can be done via restricted Turing Test in the domain of safety in a manner identical to how AI-Completeness of a problem could be established [25, 28].", "startOffset": 168, "endOffset": 176}, {"referenceID": 28, "context": "Even with advanced genetic engineering [29], the best we can hope for is some additional reduction in how", "startOffset": 39, "endOffset": 43}, {"referenceID": 29, "context": "Despite being trivial examples of a solution to the Value Learning Problem [30-32], human beings are anything but safe, bringing into question our current hope that solving VLP will get us to Safe AI.", "startOffset": 75, "endOffset": 82}, {"referenceID": 30, "context": "Despite being trivial examples of a solution to the Value Learning Problem [30-32], human beings are anything but safe, bringing into question our current hope that solving VLP will get us to Safe AI.", "startOffset": 75, "endOffset": 82}, {"referenceID": 31, "context": "Despite being trivial examples of a solution to the Value Learning Problem [30-32], human beings are anything but safe, bringing into question our current hope that solving VLP will get us to Safe AI.", "startOffset": 75, "endOffset": 82}, {"referenceID": 32, "context": "Perhaps, friendly AI research is exactly what will teach us how to do that, but we think fundamental limits on verifiability [33] will prevent any such proof.", "startOffset": 125, "endOffset": 129}, {"referenceID": 33, "context": "[34], the only actual example of intelligence we have is likely to give up its pre-programmed friendliness via rational de-biasing if exposed to certain new data.", "startOffset": 0, "endOffset": 4}, {"referenceID": 34, "context": "Most likely we will see something predicted by Kurzweil (merger of machines and people) [35].", "startOffset": 88, "endOffset": 92}], "year": 2016, "abstractText": "In this work, we present and analyze reported failures of artificially intelligent systems and extrapolate our analysis to future AIs. We suggest that both the frequency and the seriousness of future AI failures will steadily increase. AI Safety can be improved based on ideas developed by cybersecurity experts. For narrow AIs safety failures are at the same, moderate, level of criticality as in cybersecurity, however for general AI, failures have a fundamentally different impact. A single failure of a superintelligent system may cause a catastrophic event without a chance for recovery. The goal of cybersecurity is to reduce the number of successful attacks on the system; the goal of AI Safety is to make sure zero attacks succeed in bypassing the safety mechanisms. Unfortunately, such a level of performance is unachievable. Every security system will eventually fail; there is no such thing as a 100% secure system.", "creator": "Microsoft\u00ae Word 2013"}}}