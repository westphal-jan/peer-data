{"id": "1610.08078", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Oct-2016", "title": "Dis-S2V: Discourse Informed Sen2Vec", "abstract": "Vector representation of sentences is important for many text processing tasks that involve clustering, classifying, or ranking sentences. Recently, distributed representation of sentences learned by neural models from unlabeled data has been shown to outperform the traditional bag-of-words representation. However, most of these learning methods consider only the content of a sentence and disregard the relations among sentences in a discourse by and large.", "histories": [["v1", "Tue, 25 Oct 2016 20:19:35 GMT  (2195kb,D)", "http://arxiv.org/abs/1610.08078v1", null]], "reviews": [], "SUBJECTS": "cs.CL cs.IR", "authors": ["tanay kumar saha", "shafiq joty", "naeemul hassan", "mohammad al hasan"], "accepted": false, "id": "1610.08078"}, "pdf": {"name": "1610.08078.pdf", "metadata": {"source": "CRF", "title": "Dis-S2V: Discourse Informed Sen2Vec", "authors": ["Tanay Kumar Saha", "Shafiq Joty", "Naeemul Hassan", "Mohammad Al Hasan"], "emails": ["tksaha@iupui.edu}", "alhasan@iupui.edu}", "{sjoty@qf.org.qa}", "{nhassan@olemiss.edu}", "permissions@acm.org."], "sections": [{"heading": null, "text": "In this paper, we propose a series of novel models for learning latent representations of sentences (Sen2Vec) that take into account both the content of a sentence and the relationships between sentences. We first represent the relationships between sentences and a language network, and then use the network to induce contextual information into the content-based Sen2Vec models. Two different approaches are introduced to utilize the information in the network. Our first approach retrofits (already trained) Sen2Vec vectors with respect to the network in two different ways: (i) utilizing the neighborhood relations of a node, and (ii) using a stochastic sampling method that is more flexible in sampling neighbors of a node. The second approach uses a regulator to encode the information in the network into the existing Sen2Vec model. Experimental results show that our proposed models exceed existing methods in three basic information system tasks that demonstrate the effectiveness of our approach."}, {"heading": "1. INTRODUCTION AND MOTIVATION", "text": "This year, the time has come for us to be able to try to find a solution that we are able to find, that we are able to find a solution."}, {"heading": "2.1 Discourse Graph Formulation", "text": "Let D = {v1, v2, \u00b7 \u00b7 \u00b7, vn} be the records in our dataset that form the nodes in the discourse graph. Edge weights w (vi, vj) are calculated by measuring the similarity between the corresponding records \u03c3 (vi, vj). Here \u03c3 denotes a similarity metric (e.g. cosine, jaccard).In the structure of the network we distinguish between two types of edges: (i) edges within a document, i.e. edges between records of the same document, and (ii) edges between documents, i.e. edges between records of different documents. Depending on whether they are edges within a document or between documents, different thresholds can be set."}, {"heading": "2.2 Content-based Model (S2V)", "text": "We use the Sen2Vec model proposed in [12] as the base model, which is formed exclusively on the basis of the content of the sentences. This approach concatenates the vectors learned through the two models shown in Figure 1: (a) a distributed memory model (DM) and (b) a distributed word packet (DBOW). In the DM model, each sentence in dataset D is concatenated by a d-dimensional vector in a common reference matrix S-Rn-d. Likewise, each word in the vocabulary (BOW model) is averaged by a d-dimensional vector (BOW vector) in another common reference matrix S-Rn-d. For an input set v = (v1, v2 \u00b7 vm), the corresponding sentence vector from S and the corresponding word vector from Lare (BOW vector) to predict the next word in a context. Let's leave the text on the respective word vectors in their respective maps S and vectors in L."}, {"heading": "2.3 Network-based Model (N2V)", "text": "??????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????"}, {"heading": "2.4 Dis-S2V by Retrofitting", "text": "We explore the general idea of retrofitting [4] to integrate information from both the content and the neighborhood of a node (or a sentence) into a common learning box. Let us redesign this vector so that the revised vector (v) also resembles the previous vector method (v) and (ii) is similar to the vectors of its neighboring nodes (u). To achieve this, we define the following objective function to minimize vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-"}, {"heading": "2.5 Dis-S2V by Regularization", "text": "Instead of retrofitting the vectors from the content-based model on the discourse graph as a post-processing step, we can integrate the neighborhood information directly into the objective function of the content-based model as a regulator and learn the dis-S2V vectors in a single step. We define the following objective to minimize it: J (v) = \u2211 V [L (v) + \u03b2 \u2211 (u, v) E Wu, v | \u03c6 (u) \u2212 \u03c6 (v) | 2] (14), where the first component L (v) refers to the negative log probability loss of the content-based model described in Section 2.2, i.e., equation 1 for DM and equation 3 for DBOW. The second component is the graph that can be the regulator with \u03b2 as the regularizing strength. In our experiments, we use the vector-vector model as our content-based model."}, {"heading": "3. EXPERIMENTAL SETTINGS", "text": "In this section, we describe our experimental setup. In particular, we explain the tasks, data sets, metrics and test protocols we use to demonstrate the effectiveness of our models."}, {"heading": "3.1 Task Description and Datasets", "text": "In fact, most of them are able to go in search of a solution worthy of their name, most of them are able to go in search of a solution, most of them are able to go in search of a solution, most of them are able to go in search of a solution, most of them are able to go in search of a solution, most of them are able to go in search of a solution, most of them are able to find a solution for themselves, most of them are able to find a solution for themselves, most of them are able to find a solution for themselves."}, {"heading": "3.2 Discourse Network Statistics", "text": "To construct our discourse graph, we use the vectors we learned from S2V (paragraph 2.2). Each sentence can be connected to any set of data sets. However, we limit the connections in order to make the graph reasonably economical for computing power, while ensuring that the graph is informative enough. We achieve this by imposing two types of constraints based on the similarity values. First, we limit the edges by setting thresholds for intra- and cross-document connections; sentences in a document are connected only if their cosmic similarity is above the threshold within a document; similarly, sentences within documents are connected only if their similarity is above the threshold within a document. We use 0.5 or 0.8 for cross-document and intra- and cross-document thresholds. Second, we trim the network further by keeping only the top 20 similar neighbors for each node. Table 3 shows the basic statistics of the resulting data sets for all of our discourses."}, {"heading": "3.3 The Extractive Summarizer", "text": "In order to obtain the summary sentences of a document, we first create a weighted graph in which the nodes represent the sentences of the document and the edges represent the cosinal similarity between the learned representations (using one of the models in section 2) of the two corresponding sentences. To keep the graph thin, we avoid edges weighing less than 0.10. We run the PageRank algorithm [18] on the graph to determine the rank of each sentence in the document, extracting the key sentences as a summary of the document. The dumping factor in PageRank is set to 0.85."}, {"heading": "3.4 Evaluation Metrics", "text": "Topic Classification: We use Precision (P), Recall (R), Accuracy (Acc), F1 Measure (F1) and Cohen's Kappa (\u0445) as evaluation variables for the classification task. Topic Clustering: To measure cluster performance, we use various measurements such as Uniformity Value (H), Completeness Value (C), V Measure [20] (V) and Adjusted Mutual Information (AMI). The idea of homogeneity is that the class distribution within each cluster should be shifted to a single class. Completeness value determines whether all members of a given class are assigned to the same cluster. The harmonious mean of these two measurements is the Vmeasure. AMI measures the correspondence of two mappings, in our case clustering and class distribution. It is normalized against chance. All these measurements are summarized by [0, 1]. Higher scores mean a better summary."}, {"heading": "3.5 Experiment Protocols", "text": "Model variants: In Table 4, we briefly describe our core models and their variants. For detailed discussions, please see the sections below. Model settings: All models except the (iterative) retrofitted (i.e. IT-w, IT-uw) were trained with stochastic gradient descent (SGD), whereby the gradient is achieved by backpropagation. We frequently used subsampling words and negative sampling in the classification layer, as described in [15], which cause significant speed increases in training. Table 5 shows the hyperparameters of our models and the values we have set for these hyperparameters. For each data set, we randomly selected 20% documents from the set to form a reserved validation set on which we tune these parameters. To find the best parameter values, we optimize F1 for classification, and ROUGE-1 for summarizing the validation."}, {"heading": "4. RESULTS", "text": "In the tables, we divide our models into 4 blocks; see also Table 4 for a brief description of these blocks. Block I shows results for the S2V base model described in Section 2.2, which is a purely content-based model; the S2V model concatenates the vectors learned through DM and DBOW models; the concatenated model performed better than individual models; we show the relative performance of all other models in relation to this S2V base model; Block II presents the results for N2V and N2V-i described in Section 2.3; N2V-i initializes the vectors with the representation learned from the content-based model S2V; and Block III presents the results of \"Dis-S2V by Retrofitting\" models described in Section 2.4."}, {"heading": "4.1 Classification Results", "text": "The results in Table 7 show a significant improvement in the performance of our models in the 8-class classification task compared to the baseline, S2V. For the Reuters-21578 data set, REGuw from Block IV becomes the best performer and improves metrically by 3.60%, 3.80%, 3.40%, 3.42% and 4.32% compared to the baseline in R, P, Acc and Calvin. The interrater (\u2212 \u2212) improves by more than 4%, which is a good indicator that REG-uw is more consistent with the gold label assessor. With 20 newsgroups (740%, 3.42% and 4.32%), REG-w becomes the best performer in all metrics. A closer observation will show that REG-w is also competitive with REG-uw in Reuters-21578 data sets. This is intuitive, as these models use information from both the content and from the discourse network, since the NV model only cuts off from the 2V."}, {"heading": "4.2 Clustering Results", "text": "The cluster results in Table 8 clearly show that all of our models significantly exceed the baseline, and the models in Block IV perform significantly better in all metrics than the models in Block III. The Adjusted Mutual Information Value (AMI) is greater than 0.40 for all of our models, suggesting that our models are better than the random cluster assignments. An important observation is that N2V-i performs best with a relative improvement of about 13% compared to the baseline. Note that N2V-i incorporates information from pure content representation into the initialization of its model, which is then updated based on the network environment. N2V, which only uses the network environment, also performs better than the pure content baseline. This indicates that the neighborhood information is quite beneficial for clustering, whereas these promising results once again point to the effectiveness of our representation model versus the pure content models indicating cluster formation, and the use of only three of the two categories, V2V is one of three cluster illustrations out of three."}, {"heading": "4.3 Ranking Results", "text": "Table 9 shows that N2V-r, REG-84, REG-uw, REG-w, DICTREG-uw and DICTREG-w consistently outshine the basic model S2V across all summary tasks. For DUC 2001 in 10-page summary, N2V-r wins by 2.21% over S2V, while for 100-page summary, N2V-i wins by 5.16% over S2V. For DUC 2002, REG-w and REG-uw perform better than all other models. For length 10, REG-w improves over S2V by 2.5%, whereas REG-uw improves over length by 3.26% over length 100. Summarized models in Block IV and N2V-r, i.e. models that use the information content of the discourse network in the learning phase, achieve 486 discourse-w + S2V by 2.5%, whereas REG-uw improves over length by 3.26%."}, {"heading": "5. RELATED WORK", "text": "Our work is closely related to two different branches of representative learning research: (i) the distributed representation of text units (e.g. words, paragraphs) and (2) the latent representation of nodes in a network."}, {"heading": "5.1 Representation for Textual Units", "text": "The Word2Vec model [14] for distributed representation of words is very popular in word processing, whereas our model, due to its simple architecture, is also highly scalable in practice. [12] Extended Word2Vec to learn how to represent words and documents. It maps each sentence to a unique ID and learns the representation for the sentence from the context of words in the sentence - either by predicting the entire context or by predicting a word in the given context. In our work, we extend this model to integrate relationships between sentences in the form of a discourse graph. We do this with a graph smoothing regulator in the original objective function or by retrofitting the original vectors on the discourse graph. More recently, we propose methods to incorporate semantic knowledge into the word representation models."}, {"heading": "5.2 Representation for Nodes in Networks", "text": "DeepWalk [19], LINE [25], and node2vec [5] are some of the recent advances in learning representation of nodes in a network and are closely related to our work. These studies show promising results compared to linear and non-linear factorization techniques such as PCA [10], Iso-Map [26], and LLE [21]. They are also related in the sense that they all use discriminatory models to learn representation. node2vec uses a controlled random path to generate the neighborhood (i.e. the context) of a node from the graph. DeepWalk generates contexts using truncated random walk. Both Line and node2vec use alias sampling to generate contexts. node2vec uses a controlled random path to initially combine width and depth adequately with network-based content."}, {"heading": "6. CONCLUSION AND FUTURE WORK", "text": "In this paper, we have proposed a series of novel models for vector representation of sentences that take into account not only the content of a sentence, but also relationships between sentences. The relationships between sentences are encoded in a discourse graph that is then used to build discourse-oriented sentence representation models. We have explored two different ways to include discourse information: (i) by retrofitting the vectors we learned from a content-based model on the discourse graph, and (ii) by regulating the content-based model by a graph smoothing factor. We evaluated our models based on three tasks of classification, clustering, and ranking. Our results show that our models exceed the purely content-based baseline, and that the regulated models perform well in all three tasks, whereas the retrofitting models are good only for classification and clustering tasks. In the future, we would like to expand our models to represent groups in social networks and use pre-tasks to directly evaluate groups (the difference between a social network and a social network)."}, {"heading": "7. REFERENCES", "text": "[1] C. F. Baker, C. J. Fillmore, and J. B. Lowe. Theberkeley framenet project. [2] In Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics-Volume 1, pp. 86-90, 1998. [2] Y. Bengio, R. Ducharme, P. Vincent, and C. Janvin. [3] Y. Bengio and D. R. Radev. Lexrank: Graph-based lexical centrality as salience in text marization. J. Artif. Int."}], "references": [{"title": "The berkeley framenet project", "author": ["C.F. Baker", "C.J. Fillmore", "J.B. Lowe"], "venue": "Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics-Volume 1, pages 86\u201390", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1998}, {"title": "A neural probabilistic language model", "author": ["Y. Bengio", "R. Ducharme", "P. Vincent", "C. Janvin"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2003}, {"title": "Lexrank: Graph-based lexical centrality as salience in text summarization", "author": ["G. Erkan", "D.R. Radev"], "venue": "J. Artif. Int. Res.,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2004}, {"title": "Retrofitting word vectors to semantic lexicons", "author": ["M. Faruqui", "J. Dodge", "S.K. Jauhar", "C. Dyer", "E. Hovy", "N.A. Smith"], "venue": "Proc. of NAACL", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2015}, {"title": "node2vec: Scalable feature learning for networks", "author": ["A. Grover", "J. Leskovec"], "venue": "Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 855\u2013864", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2016}, {"title": "Learning distributed representations of sentences from unlabelled data", "author": ["F. Hill", "K. Cho", "A. Korhonen"], "venue": "In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2016}, {"title": "Learning distributed representations of sentences from unlabelled data", "author": ["F. Hill", "K. Cho", "A. Korhonen"], "venue": "arXiv preprint arXiv:1602.03483", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2016}, {"title": "Coherence and coreference", "author": ["J.R. Hobbs"], "venue": "Cognitive science, 3(1):67\u201390", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1979}, {"title": "Training linear svms in linear time", "author": ["T. Joachims"], "venue": "Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD \u201906, pages 217\u2013226", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2006}, {"title": "Principal component analysis", "author": ["I. Jolliffe"], "venue": "Wiley Online Library", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2002}, {"title": "Skip-thought vectors", "author": ["R. Kiros", "Y. Zhu", "R.R. Salakhutdinov", "R. Zemel", "R. Urtasun", "A. Torralba", "S. Fidler"], "venue": "Advances in neural information processing systems, pages 3294\u20133302", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "Distributed representations of sentences and documents", "author": ["Q.V. Le", "T. Mikolov"], "venue": "ICML, volume 14, pages 1188\u20131196", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2014}, {"title": "Rouge: A package for automatic evaluation of summaries", "author": ["C.-Y. Lin"], "venue": "Text summarization branches out: Proceedings of the ACL-04 workshop, volume 8. Barcelona, Spain", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2004}, {"title": "Efficient estimation of word representations in vector space", "author": ["T. Mikolov", "K. Chen", "G. Corrado", "J. Dean"], "venue": "arXiv preprint arXiv:1301.3781", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["T. Mikolov", "I. Sutskever", "K. Chen", "G. Corrado", "J. Dean"], "venue": "Proceedings of the 26th International Conference on Neural Information Processing Systems, NIPS\u201913, pages 3111\u20133119", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2013}, {"title": "Wordnet: a lexical database for english", "author": ["G.A. Miller"], "venue": "Communications of the ACM, 38(11):39\u201341", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1995}, {"title": "Automatic  summarization", "author": ["A. Nenkova", "K. McKeown"], "venue": "Foundations and Trends\u00c2\u0151 in Information Retrieval, 5(2\u00e2\u0102\u015e3):103\u2013233", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2011}, {"title": "The pagerank citation ranking: bringing order to the web", "author": ["L. Page", "S. Brin", "R. Motwani", "T. Winograd"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1999}, {"title": "Deepwalk: Online learning of social representations", "author": ["B. Perozzi", "R. Al-Rfou", "S. Skiena"], "venue": "Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 701\u2013710. ACM", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2014}, {"title": "V-measure: A conditional entropy-based external cluster evaluation measure", "author": ["A. Rosenberg", "J. Hirschberg"], "venue": "EMNLP-CoNLL, volume 7, pages 410\u2013420", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2007}, {"title": "Nonlinear dimensionality reduction by locally linear embedding", "author": ["S.T. Roweis", "L.K. Saul"], "venue": "Science, 290(5500):2323\u20132326", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2000}, {"title": "Discourse processing", "author": ["M. Stede"], "venue": "Morgan & Claypool Publishers", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2011}, {"title": "New regularized algorithms for transductive learning", "author": ["P.P. Talukdar", "K. Crammer"], "venue": "Proceedings of the European Conference on Machine Learning and Knowledge Discovery in Databases: Part II, ECML PKDD \u201909, pages 442\u2013457. Springer-Verlag", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2009}, {"title": "Pte: Predictive text embedding through large-scale heterogeneous text networks", "author": ["J. Tang", "M. Qu", "Q. Mei"], "venue": "Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 1165\u20131174. ACM", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2015}, {"title": "Line: Large-scale information network embedding", "author": ["J. Tang", "M. Qu", "M. Wang", "M. Zhang", "J. Yan", "Q. Mei"], "venue": "Proceedings of the 24th International Conference on World Wide Web, pages 1067\u20131077. ACM", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2015}, {"title": "A global geometric framework for nonlinear dimensionality reduction", "author": ["J.B. Tenenbaum", "V. De Silva", "J.C. Langford"], "venue": "science, 290(5500):2319\u20132323", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2000}, {"title": "Rc-net: A general framework for incorporating knowledge into word representations", "author": ["C. Xu", "Y. Bai", "J. Bian", "B. Gao", "G. Wang", "X. Liu", "T.-Y. Liu"], "venue": "Proceedings of the 23rd ACM International Conference on Conference on Information and Knowledge Management, pages 1219\u20131228. ACM", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2014}, {"title": "Improving lexical embeddings with semantic knowledge", "author": ["M. Yu", "M. Dredze"], "venue": "In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2014}], "referenceMentions": [{"referenceID": 2, "context": "Similarly, for the task of ranking sentences based on their importance in the text using a ranking model like LexRank [3] or SVMRank [9], one needs to first represent the sentences with fixed-length vectors.", "startOffset": 118, "endOffset": 121}, {"referenceID": 8, "context": "Similarly, for the task of ranking sentences based on their importance in the text using a ranking model like LexRank [3] or SVMRank [9], one needs to first represent the sentences with fixed-length vectors.", "startOffset": 133, "endOffset": 136}, {"referenceID": 11, "context": "Recently, distributed representations, in the form of dense real-valued vectors, learned by neural network models from unlabeled data, has been shown to outperform the traditional bag-of-words representation [12].", "startOffset": 208, "endOffset": 212}, {"referenceID": 14, "context": "Distributed representations encode the semantics of linguistic units and yield better generalization [15, 2].", "startOffset": 101, "endOffset": 108}, {"referenceID": 1, "context": "Distributed representations encode the semantics of linguistic units and yield better generalization [15, 2].", "startOffset": 101, "endOffset": 108}, {"referenceID": 11, "context": "However, most existing methods to devise distributed representation for sentences consider only the content of a sentence, and disregard relations between sentences in a text by and large [12, 6].", "startOffset": 188, "endOffset": 195}, {"referenceID": 5, "context": "However, most existing methods to devise distributed representation for sentences consider only the content of a sentence, and disregard relations between sentences in a text by and large [12, 6].", "startOffset": 188, "endOffset": 195}, {"referenceID": 7, "context": ", elaboration, contrast) to express the meaning as a whole [8].", "startOffset": 59, "endOffset": 62}, {"referenceID": 21, "context": ", sentences are also topically related [22].", "startOffset": 39, "endOffset": 43}, {"referenceID": 15, "context": ", synonymy, hypernymy, hyponymy) encoded in semantic lexicons like WordNet [16] or Framenet [1] can improve the quality of word vectors that are trained solely on unlabeled data [27, 4, 28].", "startOffset": 75, "endOffset": 79}, {"referenceID": 0, "context": ", synonymy, hypernymy, hyponymy) encoded in semantic lexicons like WordNet [16] or Framenet [1] can improve the quality of word vectors that are trained solely on unlabeled data [27, 4, 28].", "startOffset": 92, "endOffset": 95}, {"referenceID": 26, "context": ", synonymy, hypernymy, hyponymy) encoded in semantic lexicons like WordNet [16] or Framenet [1] can improve the quality of word vectors that are trained solely on unlabeled data [27, 4, 28].", "startOffset": 178, "endOffset": 189}, {"referenceID": 3, "context": ", synonymy, hypernymy, hyponymy) encoded in semantic lexicons like WordNet [16] or Framenet [1] can improve the quality of word vectors that are trained solely on unlabeled data [27, 4, 28].", "startOffset": 178, "endOffset": 189}, {"referenceID": 27, "context": ", synonymy, hypernymy, hyponymy) encoded in semantic lexicons like WordNet [16] or Framenet [1] can improve the quality of word vectors that are trained solely on unlabeled data [27, 4, 28].", "startOffset": 178, "endOffset": 189}, {"referenceID": 4, "context": "Our choice of network to represent inter-sentence relations is due to the facts that networks provide flexible ways to represent relations between any pair of sentences, and recent advances in learning distributed representations for nodes and edges in networks have shown promising results [5, 25, 24, 19].", "startOffset": 291, "endOffset": 306}, {"referenceID": 24, "context": "Our choice of network to represent inter-sentence relations is due to the facts that networks provide flexible ways to represent relations between any pair of sentences, and recent advances in learning distributed representations for nodes and edges in networks have shown promising results [5, 25, 24, 19].", "startOffset": 291, "endOffset": 306}, {"referenceID": 23, "context": "Our choice of network to represent inter-sentence relations is due to the facts that networks provide flexible ways to represent relations between any pair of sentences, and recent advances in learning distributed representations for nodes and edges in networks have shown promising results [5, 25, 24, 19].", "startOffset": 291, "endOffset": 306}, {"referenceID": 18, "context": "Our choice of network to represent inter-sentence relations is due to the facts that networks provide flexible ways to represent relations between any pair of sentences, and recent advances in learning distributed representations for nodes and edges in networks have shown promising results [5, 25, 24, 19].", "startOffset": 291, "endOffset": 306}, {"referenceID": 11, "context": ", the Sen2Vec model proposed in [12].", "startOffset": 32, "endOffset": 36}, {"referenceID": 22, "context": "The retrofitting is performed in two different ways: (i) by using an efficient iterative algorithm [23, 4] that incorporates adjacency relations in 1-hop neighborhood of a node, and (ii) by training a discriminative model that seeks to preserve local neighborhoods of nodes, and in such case, the neighborhood is sampled by a flexible stochastic sampling method [5].", "startOffset": 99, "endOffset": 106}, {"referenceID": 3, "context": "The retrofitting is performed in two different ways: (i) by using an efficient iterative algorithm [23, 4] that incorporates adjacency relations in 1-hop neighborhood of a node, and (ii) by training a discriminative model that seeks to preserve local neighborhoods of nodes, and in such case, the neighborhood is sampled by a flexible stochastic sampling method [5].", "startOffset": 99, "endOffset": 106}, {"referenceID": 4, "context": "The retrofitting is performed in two different ways: (i) by using an efficient iterative algorithm [23, 4] that incorporates adjacency relations in 1-hop neighborhood of a node, and (ii) by training a discriminative model that seeks to preserve local neighborhoods of nodes, and in such case, the neighborhood is sampled by a flexible stochastic sampling method [5].", "startOffset": 362, "endOffset": 365}, {"referenceID": 5, "context": ", sentiment classification, paraphrase identification) and sentence-pair similarity computation task [6, 12].", "startOffset": 101, "endOffset": 108}, {"referenceID": 11, "context": ", sentiment classification, paraphrase identification) and sentence-pair similarity computation task [6, 12].", "startOffset": 101, "endOffset": 108}, {"referenceID": 11, "context": "We use the Sen2Vec model proposed in [12] as our baseline model, which is trained solely based on the contents of the sentences.", "startOffset": 37, "endOffset": 41}, {"referenceID": 4, "context": "We use the recently proposed Node2Vec method [5].", "startOffset": 45, "endOffset": 48}, {"referenceID": 13, "context": "It uses the skipgram model of Word2Vec [14] with the intuition that nodes within a graph context (or neighborhood) should have similar representations.", "startOffset": 39, "endOffset": 43}, {"referenceID": 3, "context": "We explore the general idea of retrofitting [4] to incorporate information from both the content and the neighborhood of a node (or a sentence) in a joint learning framework.", "startOffset": 44, "endOffset": 47}, {"referenceID": 22, "context": "The quadratic cost in Equation 9 is convex in \u03c6, and has a closed form solution [23].", "startOffset": 80, "endOffset": 84}, {"referenceID": 22, "context": "Formally, the random walk view has the following iterative update [23]:", "startOffset": 66, "endOffset": 70}, {"referenceID": 24, "context": "However, preserving only local structures may not be sufficient for many applications [25].", "startOffset": 86, "endOffset": 90}, {"referenceID": 15, "context": ", WordNet [16], Framenet [1]) can improve the quality of word vectors that are trained solely on the sentences [27, 4, 28].", "startOffset": 10, "endOffset": 14}, {"referenceID": 0, "context": ", WordNet [16], Framenet [1]) can improve the quality of word vectors that are trained solely on the sentences [27, 4, 28].", "startOffset": 25, "endOffset": 28}, {"referenceID": 26, "context": ", WordNet [16], Framenet [1]) can improve the quality of word vectors that are trained solely on the sentences [27, 4, 28].", "startOffset": 111, "endOffset": 122}, {"referenceID": 3, "context": ", WordNet [16], Framenet [1]) can improve the quality of word vectors that are trained solely on the sentences [27, 4, 28].", "startOffset": 111, "endOffset": 122}, {"referenceID": 27, "context": ", WordNet [16], Framenet [1]) can improve the quality of word vectors that are trained solely on the sentences [27, 4, 28].", "startOffset": 111, "endOffset": 122}, {"referenceID": 26, "context": "Following previous work [27, 28], we first convert any kind of semantic lexicon into a network, where semantically similar words defined by the lexicon become neighbors.", "startOffset": 24, "endOffset": 32}, {"referenceID": 27, "context": "Following previous work [27, 28], we first convert any kind of semantic lexicon into a network, where semantically similar words defined by the lexicon become neighbors.", "startOffset": 24, "endOffset": 32}, {"referenceID": 16, "context": "For classification (or clustering), we measure the performance of our models in classifying (or grouping) sentences based on their topics, whereas in Ranking, we investigate our model\u2019s performance in ranking the most central topical sentences by evaluating it on an extractive summarization task [17].", "startOffset": 297, "endOffset": 301}, {"referenceID": 2, "context": "To minimize this noise, we use an extractive (unsupervised) summarizer, LexRank [3], to select the top P% sentences as representatives of the document and label them with the same topic label as the document; see Sec.", "startOffset": 80, "endOffset": 83}, {"referenceID": 2, "context": "We use the graph-based LexRank algorithm [3] to rank the sentences in a document based on their importance.", "startOffset": 41, "endOffset": 44}, {"referenceID": 17, "context": "We run the PageRank algorithm [18] on the graph to determine the rank of each sentence in the document, and thereby extract the key sentences as summary of that document.", "startOffset": 30, "endOffset": 34}, {"referenceID": 19, "context": "Topic Clustering: For measuring clustering performance, we use various measures such as homogeneity score (H), completeness score (C), V-measure [20] (V), and adjusted mutual information (AMI) score.", "startOffset": 145, "endOffset": 149}, {"referenceID": 7, "context": "S2V Sen2Vec k [8, 10, 12] N2V-r N2V retrofit \u03b2 [0.", "startOffset": 14, "endOffset": 25}, {"referenceID": 9, "context": "S2V Sen2Vec k [8, 10, 12] N2V-r N2V retrofit \u03b2 [0.", "startOffset": 14, "endOffset": 25}, {"referenceID": 11, "context": "S2V Sen2Vec k [8, 10, 12] N2V-r N2V retrofit \u03b2 [0.", "startOffset": 14, "endOffset": 25}, {"referenceID": 0, "context": "All these measures are bounded by [0, 1].", "startOffset": 34, "endOffset": 40}, {"referenceID": 12, "context": "Summarization: We use the widely used automatic evaluation metric ROUGE [13] to evaluate the system-generated summaries.", "startOffset": 72, "endOffset": 76}, {"referenceID": 12, "context": ", n = 1) has been shown to correlate well with human judgments for short summaries [13].", "startOffset": 83, "endOffset": 87}, {"referenceID": 14, "context": "We used subsampling of frequent words and negative sampling in the classification layer as described in [15], which give significant speed-ups in training.", "startOffset": 104, "endOffset": 108}, {"referenceID": 13, "context": "The Word2Vec model [14] to learn distributed representation of words is very popular for text processing tasks.", "startOffset": 19, "endOffset": 23}, {"referenceID": 11, "context": "[12] extended Word2Vec to learn the representation for sentences and documents.", "startOffset": 0, "endOffset": 4}, {"referenceID": 26, "context": "Recently, [27, 4, 28] propose methods to incorporate semantic knowledge into the word representation models.", "startOffset": 10, "endOffset": 21}, {"referenceID": 3, "context": "Recently, [27, 4, 28] propose methods to incorporate semantic knowledge into the word representation models.", "startOffset": 10, "endOffset": 21}, {"referenceID": 27, "context": "Recently, [27, 4, 28] propose methods to incorporate semantic knowledge into the word representation models.", "startOffset": 10, "endOffset": 21}, {"referenceID": 10, "context": "Skip-Thought [11] and Fast-Sent [7] are the most recently proposed sentence representation methods.", "startOffset": 13, "endOffset": 17}, {"referenceID": 6, "context": "Skip-Thought [11] and Fast-Sent [7] are the most recently proposed sentence representation methods.", "startOffset": 32, "endOffset": 35}, {"referenceID": 6, "context": "SkipThought is computationally very expensive [7].", "startOffset": 46, "endOffset": 49}, {"referenceID": 18, "context": "DeepWalk [19], LINE [25] and node2vec [5] are some of the recent advancements on learning representation for nodes in a network, and are closely related to our work.", "startOffset": 9, "endOffset": 13}, {"referenceID": 24, "context": "DeepWalk [19], LINE [25] and node2vec [5] are some of the recent advancements on learning representation for nodes in a network, and are closely related to our work.", "startOffset": 20, "endOffset": 24}, {"referenceID": 4, "context": "DeepWalk [19], LINE [25] and node2vec [5] are some of the recent advancements on learning representation for nodes in a network, and are closely related to our work.", "startOffset": 38, "endOffset": 41}, {"referenceID": 9, "context": "These studies show promising results over linear and non-linear matrix factorization techniques, such as PCA [10], Iso-Map [26], and LLE [21].", "startOffset": 109, "endOffset": 113}, {"referenceID": 25, "context": "These studies show promising results over linear and non-linear matrix factorization techniques, such as PCA [10], Iso-Map [26], and LLE [21].", "startOffset": 123, "endOffset": 127}, {"referenceID": 20, "context": "These studies show promising results over linear and non-linear matrix factorization techniques, such as PCA [10], Iso-Map [26], and LLE [21].", "startOffset": 137, "endOffset": 141}], "year": 2016, "abstractText": "Vector representation of sentences is important for many text processing tasks that involve clustering, classifying, or ranking sentences. Recently, distributed representation of sentences learned by neural models from unlabeled data has been shown to outperform the traditional bag-of-words representation. However, most of these learning methods consider only the content of a sentence and disregard the relations among sentences in a discourse by and large. In this paper, we propose a series of novel models for learning latent representations of sentences (Sen2Vec) that consider the content of a sentence as well as inter-sentence relations. We first represent the inter-sentence relations with a language network and then use the network to induce contextual information into the content-based Sen2Vec models. Two different approaches are introduced to exploit the information in the network. Our first approach retrofits (already trained) Sen2Vec vectors with respect to the network in two different ways: (i) using the adjacency relations of a node, and (ii) using a stochastic sampling method which is more flexible in sampling neighbors of a node. The second approach uses a regularizer to encode the information in the network into the existing Sen2Vec model. Experimental results show that our proposed models outperform existing methods in three fundamental information system tasks demonstrating the effectiveness of our approach. The models leverage the computational power of multi-core CPUs to achieve fine-grained computational efficiency. We make our code publicly available upon acceptance.", "creator": "LaTeX with hyperref package"}}}