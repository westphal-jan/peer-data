{"id": "1610.08095", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Oct-2016", "title": "Modeling Ambiguity, Subjectivity, and Diverging Viewpoints in Opinion Question Answering Systems", "abstract": "Product review websites provide an incredible lens into the wide variety of opinions and experiences of different people, and play a critical role in helping users discover products that match their personal needs and preferences. To help address questions that can't easily be answered by reading others' reviews, some review websites also allow users to pose questions to the community via a question-answering (QA) system. As one would expect, just as opinions diverge among different reviewers, answers to such questions may also be subjective, opinionated, and divergent. This means that answering such questions automatically is quite different from traditional QA tasks, where it is assumed that a single `correct' answer is available. While recent work introduced the idea of question-answering using product reviews, it did not account for two aspects that we consider in this paper: (1) Questions have multiple, often divergent, answers, and this full spectrum of answers should somehow be used to train the system; and (2) What makes a `good' answer depends on the asker and the answerer, and these factors should be incorporated in order for the system to be more personalized. Here we build a new QA dataset with 800 thousand questions---and over 3.1 million answers---and show that explicitly accounting for personalization and ambiguity leads both to quantitatively better answers, but also a more nuanced view of the range of supporting, but subjective, opinions.", "histories": [["v1", "Tue, 25 Oct 2016 21:08:15 GMT  (242kb,D)", "http://arxiv.org/abs/1610.08095v1", "10 pages, accepted by ICDM'2016"]], "COMMENTS": "10 pages, accepted by ICDM'2016", "reviews": [], "SUBJECTS": "cs.IR cs.CL", "authors": ["mengting wan", "julian mcauley"], "accepted": false, "id": "1610.08095"}, "pdf": {"name": "1610.08095.pdf", "metadata": {"source": "CRF", "title": "Modeling Ambiguity, Subjectivity, and Diverging Viewpoints in Opinion Question Answering Systems", "authors": ["Mengting Wan", "Julian McAuley"], "emails": ["m5wan@eng.ucsd.edu", "jmcauley@eng.ucsd.edu"], "sections": [{"heading": null, "text": "In fact, most of them will be able to abide by the rules that they have imposed on themselves, and they will be able to understand the rules that they have imposed on themselves."}, {"heading": "A. Ambiguity and Subjectivity in Opinion QA Systems", "text": "Answering this new view of question-answering is a challenge and requires the development of new techniques to identify different, possibly conflicting, labels1 Note that even binary questions can still be subjective, so that both \"yes\" and \"no\" answers are possible within a monitored framework. We identify two main perspectives from which ambiguity and subjectivity can be examined in productively rated QA systems: \u2022 Multiple answers. We note that in previous studies there was only one bottom-truth answer for each question. However, in real-world QA systems there are often multiple answers available. We find that this is true for both binary and open-ended questions. If multiple answers are available, they often describe different aspects of the questions or different personal experiences. By including multiple answers at the training point, we expect that the relevant assessments accessed by the system at the test point contain subjective answers to subjective assessments."}, {"heading": "B. Contributions", "text": "To our knowledge, our study is the first to systematically model ambiguity and subjectivity in QA systems. We provide a new data set consisting of 135 thousand products from Amazon, 808 thousand questions, 3 million answers and 11 million values.2 By modelling ambiguity in product-related questions2, this study builds bridges not only between QA systems and ratings, but also between opinion research and the2Data and code available on the first author's website. Idea of \"Learning from Crowds.\" For both binary and open questions, we are successfully developing a model to deal with multiple (and possibly contradictory) answers and integrate subjective features that predict binary question labels and present the user with a relevant score list. Quantitatively, we show that modeling ambiguity and subjectivity leads to significant performance gains in terms of the accuracy of our question-answer system."}, {"heading": "II. BACKGROUND", "text": "In this study, we build on the mix of expert frameworks (MoE) previously used by [3]. We extend this approach by modelling ambiguity and subjectivity from the perspective of responses and reviews. Prior to the introduction of the full model, we present standard measures of relevance and the mix of expert frameworks (MoE) as background knowledge. The basic notation used in this paper is contained in Table I."}, {"heading": "A. Standard Relevance Measures", "text": "We first describe two types of similarity measures for relevance assessment in the context of our QA problem as follows: (1) Okapi BM25: One of the standard relevance assessment measures for information gathering, Okapi BM25 is a \"tf-idf\" -based ranking function that has been successfully applied to a number of problems, including QA tasks [5], [6]. Specifically, the standard BM25 measure asbm25 (q, r) = n \u2211 i = 1 idf (qi, r) \u00b7 f (qi, r) \u00b7 (k1 + 1) f (qi, r) + k1 \u00b7 (1 \u2212 b + b \u00b7 r | r | avgrl) is defined, (1) where qi, i = 1,.. n keywords in q, f (qi, r) \u00b7 f (PLL) \u00b7 the frequency of qi in r, | r | is the length of the check r and Cavgrl the average length of all considerations."}, {"heading": "B. Mixtures of Experts", "text": "The mixing of experts (MoE) [8] is a supervised learning approach, the q = q q = q q = q q q (q = q = q = q = q = q = q = q (q = q = q = q = q = q = q = q (q = q = q = q = q = q q = q = q q = q = q q = q = q = q (q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q (q = q = q = q = q = q = q = q q = q = q = q = q = q = q (q = q = q = q = q = q q = q q = q = q = q = q = q q = q = q = q = q = q = q = q) (q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q"}, {"heading": "C. Relevance and Prediction with Text-Only Features", "text": "As described above, for a binary question, the probability associated with a positive (i.e. \"q\" = > q \") identifier P (yq = 1 | Xq) (pq in short) can be modelled using a MoE framework in which each rating is considered a weak classifier. If only one identifier is included in the training process for a question, we can train by maximizing the following log probability: L = logP (Y | X) = q [yq log pq + (1 \u2212 yq) log (1 \u2212 pq))) (8), which includes all parameters and models pq as in (4). A number of attributes can be applied to define the\" relevance \"(vq, r) and\" prediction \"(wq, r) functions that we use in [3], text attributes only to define similarity and bilinear models."}, {"heading": "III. AMBIGUITY AND SUBJECTIVITY IN BINARY QUESTIONS", "text": "So far, we have followed the basic approach of [3] by assuming only textual attributes and a single name (answer) for each question. However, as we note in our data (see Section V), answers in real quality of opinion systems exhibit significant ambiguities, even for binary questions. However, previous studies only considered a single answer to each question. In this section, we will develop additional mechanisms that allow us to model ambiguity and subjectivity, and in particular deal with training data with multiple (and possibly contradictory) answers."}, {"heading": "A. Modeling Ambiguity: Learning with Multiple Labels.", "text": "Note that in the previous log probability expression (8), only a label q = q = q = q q = q q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q = q ="}, {"heading": "IV. MODELING OPEN-ENDED QUESTIONS", "text": "Although our KL-MoE and EM-MoE frameworks can model ambiguity in binary questions and take into account simple characteristics that encode subjectivity, we still need to develop methods to consider ambiguity in open questions. We are no longer concerned with the divergence between yes / no answers, but rather with the idea that there is a pool of answers to each question that should be considered more valid than alternatives. As with binary questions, these open questions can be subjective and often there are multiple answers in our data. The difference is that it is difficult for us to automatically assess whether these answers are consistent or not. Therefore, we strive to generate candidate responses that cover the spectrum of answers on the ground as much as possible. First, we give a little more detail about the basic framework with a single open answer, which we have briefly described in Section II-B2. Then we simply expand this framework to include multiple open answers and subjective information."}, {"heading": "A. Basic Framework: Learning with a Single Answer.", "text": "The answer to this question is a set of non-answers that are randomly selected from all the answers. In other words, a good system is one that can determine exactly what the real answer is. In practice, we maximize a smooth answer to this question in the form of the log-livelihood model. In practice, we can maximize a smooth answer to this question in the form of the log-livelihood model. A good system is one that can determine exactly what the real answer is. In practice, we can find a smooth answer to this question in the form of the log-livelihood model."}, {"heading": "V. DATASET AND EXPLORATORY ANALYSIS", "text": "In [3], the authors collected questions / answers from Amazon.com, including a single answer (the most popular) to each question. We collected all related URLs in that data set, and continued to search through all available answers to each question (duplicates were discarded, as well as questions that have been removed since the original data set was collected from Amazon). For each product, we also have the corresponding ratings. Ultimately, we received about 808 thousand questions with 3 million answers to 135 thousand products in 8 major categories. For these products, we have a total of 11 million reviews. For detailed information, see Table II. In practice, we break review paragraphs down into sentences, so that each sentence in our MoE framework is treated like a single \"expert.\" We used Stanford's CoreNLP [11] library to split reviews into sentences, word marks, etc."}, {"heading": "A. Obtaining Ground-Truth labels for Binary Questions", "text": "In the [3] dataset, a thousand questions were manually labeled as \"binary\" or \"open-ended.\" As in [3], we used an approach developed by Google to determine whether a question is binary, using a set of simple grammatical rules. Under our labeled data, this approach achieved 97% accuracy and 82% callback in that manually labeled dataset. [5] We then developed a simple logistic regression model to label observed answers to these binary questions. The characteristics we applied are the frequency of each unigram plus the question of whether the first word is \"yes\" or whether it is \"no\" (which is often the case in practice). Note that, as we want to investigate the ambiguity that arises from the question itself, not from any error in our machine labels, that the first word is \"yes\" or whether it is \"no\" (which is common in practice)."}, {"heading": "B. Exploratory Analysis", "text": "After constructing classifiers to label our training data with high reliability, the next step is to determine whether there really are conflicts between multiple answers to binary questions; the distribution of ambiguous (i.e. both yes and no questions) versus non-ambiguous binary questions is illustrated in Figure 2a, which shows that we are willing to sacrifice some callbacks for accuracy, since a low recall simply means discarding some examples from our data set, as opposed to training on mislabeled instances. Do we have a share of binary questions that can confidently be classified as \"ambiguous\"? A real example of such a question is shown in Figure 1. One might expect the answer to such a question to be a definite \"yes\" or \"no, as it is a (seemingly objective) question about compatibility; however, the answers prove contradictory as different users focus on different aspects of the product."}, {"heading": "VI. EXPERIMENTS", "text": "We evaluate our proposed methods for binary questions and open questions using a large dataset of questions, answers and ratings from Amazon. For binary questions, we evaluate the model's ability to make correct predictions. For open questions, we evaluate the model's ability to distinguish \"true\" answers from alternatives. Since our primary goal is to address ambiguity and subjectivity, we focus on evaluating our model's ability to use multiple labels / answers and the effect of features derived from subjective information."}, {"heading": "A. Binary Questions", "text": "This is just a random comparison against methods that are able to predict only a single label, it is difficult to judge which predictions of the system are the most \"correct\" in the event of a conflict. So, for the evaluation, we build two sets of tests consisting of decreasing ambiguous questions. We then hope that by modelling ambiguity and personalization during the training, our system will be more reliable, even for unique questions. We build up two sets of assessments as follows: \u2022 Silver Standard Ground Truth. Here, we simply consider majority voting under ambiguous answers as the \"true\" label (questions with attachments are discarded). \u2022 Gold Standard Ground Truth. We ignore all questions with contradictory labels."}, {"heading": "B. Open-ended Questions", "text": "Following our previous method of distinguishing between binary and open questions, Figure 2b leaves us with a total of 698,618 open questions (85% of all questions) in our data set. We record the distribution of answers to each open question and find that most of these questions have more than one answer.1) Evaluation Methodology: Our goal is to investigate whether using multiple answers during the training period can provide us with more accurate results, as defined in the AUC of (23). In practice, we randomly sample an alternative non-answer from the pool of all answers for each answer a. If we specify the probability of output that the answer to question q is preferred to a non-answer, p q, a > a.Then the AUC measurement is defined as asAUC o = 1 | Q | E measurement performed during the training period."}, {"heading": "VII. RELATED WORK", "text": "There are several previous studies that consider the problem of answering opinion questions [1] - [3], [13] - [17] where questions are subjective and traditional QA approaches may not be as effective as in factual questions. Yu and Hatzivassiloglou [18] first proposed a series of assessments to separate opinions and facts and identify the polarities of opinion sets. Ku et al. [19] applied a two-tiered framework to classify questions and estimated question types and polarities in order to filter irrelevant sentences. Li et al. [16] proposed a graph-based approach that considered sentences as nodes and weighted edges by similarity of sentences; by constructing such a chart, they were able to apply an \"Opinion PageRank\" model to investigate different relationships."}, {"heading": "VIII. CONCLUSION AND FUTURE DIRECTIONS", "text": "In this study, we systematically developed a set of methods for modeling ambiguity and subjectivity in product-based response systems for opinion questions. We proposed an EM-like expert framework for binary questions that can successfully incorporate multiple loud labels and subjective information. Results suggest that this type of framework is consistently superior to traditional frameworks that use a single label. In open-ended questions, we also found that including multiple answers during training improves the model's ability to identify correct answers at test times.Recognition. This work is supported by NSF-IIS-1636879 and donations from Adobe, Symantec and NVIDIA."}], "references": [{"title": "AQA: aspect-based opinion question answering", "author": ["S. Moghaddam", "M. Ester"], "venue": "ICDMW, 2011.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2011}, {"title": "Answering opinion questions on products by exploiting hierarchical organization of consumer reviews", "author": ["J. Yu", "Z.-J. Zha", "T.-S. Chua"], "venue": "EMNLP-CoNLL, 2012.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2012}, {"title": "Addressing complex and subjective product-related queries with customer reviews", "author": ["J. McAuley", "A. Yang"], "venue": "WWW, 2016.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2016}, {"title": "Summarization of yes/no questions using a feature function model", "author": ["J. He", "D. Dai"], "venue": "JMLR, 2011.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2011}, {"title": "A probabilistic model of information retrieval: development and comparative experiments", "author": ["K. Jones", "S. Walker", "S. Robertson"], "venue": "Information Processing & Management, 2000.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2000}, {"title": "Rouge: A package for automatic evaluation of summaries", "author": ["C.-Y. Lin"], "venue": "ACL Workshop on Text Summarization Branches Out, 2004.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2004}, {"title": "Hierarchical mixtures of experts and the em algorithm", "author": ["M.I. Jordan", "R.A. Jacobs"], "venue": "Neural computation, 1994.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1994}, {"title": "Maximum likelihood from incomplete data via the EM algorithm", "author": ["A.P. Dempster", "N.M. Laird", "D.B. Rubin"], "venue": "Journal of the royal statistical society, 1977.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1977}, {"title": "The Stanford CoreNLP natural language processing toolkit", "author": ["C.D. Manning", "M. Surdeanu", "J. Bauer", "J. Finkel", "S.J. Bethard", "D. McClosky"], "venue": "ACL System Demo, 2014.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2014}, {"title": "Summarization of yes/no questions using a feature function model", "author": ["J. He", "D. Dai"], "venue": "ACML, 2011.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2011}, {"title": "Going beyond traditional QA systems: challenges and keys in opinion question answering", "author": ["A. Balahur", "E. Boldrini", "A. Montoyo", "P. Mart\u0131\u0301nez-Barco"], "venue": "COLING, 2010.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2010}, {"title": "Mart\u0131\u0301nez-Barco, \u201cOpinion question answering: Towards a unified approach.", "author": ["A. Balahur", "E. Boldrini", "A. Montoyo"], "venue": "in ECAI,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2010}, {"title": "Opinion and generic question answering systems: a performance analysis", "author": ["A. Balahur", "E. Boldrini", "A. Montoyo", "P. Martinez- Barco"], "venue": "ACL-IJCNLP, 2009.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2009}, {"title": "Answering opinion questions with random walks on graphs", "author": ["F. Li", "Y. Tang", "M. Huang", "X. Zhu"], "venue": "ACL-IJCNLP, 2009.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2009}, {"title": "Multi-perspective question answering using the OpQA corpus", "author": ["V. Stoyanov", "C. Cardie", "J. Wiebe"], "venue": "HLT/EMNLP, 2005.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2005}, {"title": "Towards answering opinion questions: Separating facts from opinions and identifying the polarity of opinion sentences", "author": ["H. Yu", "V. Hatzivassiloglou"], "venue": "EMNLP, 2003.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2003}, {"title": "Question analysis and answer passage retrieval for opinion question answering systems.", "author": ["L.-W. Ku", "Y.-T. Liang", "H.-H. Chen"], "venue": "ROCLING,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2007}, {"title": "Mining and summarizing customer reviews", "author": ["M. Hu", "B. Liu"], "venue": "SIGKDD, 2004.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2004}, {"title": "Opinion searching in multiproduct reviews", "author": ["J. Liu", "G. Wu", "J. Yao"], "venue": "CIT, 2006.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2006}, {"title": "Hidden factors and hidden topics: understanding rating dimensions with review text", "author": ["J. McAuley", "J. Leskovec"], "venue": "RecSys, 2013.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2013}, {"title": "Latent aspect rating analysis on review text data: a rating regression approach", "author": ["H. Wang", "Y. Lu", "C. Zhai"], "venue": "SIGKDD, 2010.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2010}, {"title": "Learning from crowds", "author": ["V. Raykar", "S. Yu", "L. Zhao", "G.H. Valadez", "C. Florin", "L. Bogoni", "L. Moy"], "venue": "JMLR, 2010.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2010}], "referenceMentions": [{"referenceID": 0, "context": "To build systems capable of leveraging such information, a series of methods [1]\u2013[3] for product-related opinion quesar X iv :1 61 0.", "startOffset": 77, "endOffset": 80}, {"referenceID": 2, "context": "To build systems capable of leveraging such information, a series of methods [1]\u2013[3] for product-related opinion quesar X iv :1 61 0.", "startOffset": 81, "endOffset": 84}, {"referenceID": 0, "context": "Many of these approaches develop information retrieval systems where traditional text similarity measures are explored and text fragments are filtered based on question types or the attributes of the product that users refer to in their questions [1], [2].", "startOffset": 247, "endOffset": 250}, {"referenceID": 1, "context": "Many of these approaches develop information retrieval systems where traditional text similarity measures are explored and text fragments are filtered based on question types or the attributes of the product that users refer to in their questions [1], [2].", "startOffset": 252, "endOffset": 255}, {"referenceID": 2, "context": "Recently, a supervised approach, Mixtures of Opinions for Question Answering (MoQA) [3], was developed for opinion QA systems using product reviews.", "startOffset": 84, "endOffset": 87}, {"referenceID": 3, "context": ", to build a labeled dataset) in a supervised setting using a binary classifier [4].", "startOffset": 80, "endOffset": 83}, {"referenceID": 2, "context": "In this study, we build upon the mixture of experts (MoE) framework as used previously by [3].", "startOffset": 90, "endOffset": 93}, {"referenceID": 4, "context": "1) Okapi BM25: One of the standard relevance ranking measures for information retrieval, Okapi BM25 is a bag-ofwords \u2018tf-idf\u2019-based ranking function that has been successfully applied in a number of problems including QA tasks [5], [6].", "startOffset": 232, "endOffset": 235}, {"referenceID": 5, "context": "2) Rouge-L: Next we consider another similarity measure, Rouge-L [7], which is a Longest Common Subsequence (LCS) based statistic.", "startOffset": 65, "endOffset": 68}, {"referenceID": 6, "context": "Mixtures of experts (MoE) [8] is a supervised learning approach that smoothly combines the outputs of several \u2018weak\u2019 classifiers in order to generate predictions.", "startOffset": 26, "endOffset": 29}, {"referenceID": 2, "context": "Previously in [3], only text features were used to define pairwise similarity measures and bilinear models.", "startOffset": 14, "endOffset": 17}, {"referenceID": 2, "context": "Following [3], we include BM25 [5] and RougeL [7] measures in s(q, r).", "startOffset": 10, "endOffset": 13}, {"referenceID": 5, "context": "Following [3], we include BM25 [5] and RougeL [7] measures in s(q, r).", "startOffset": 46, "endOffset": 49}, {"referenceID": 2, "context": "So far, we have followed the basic approach of [3], assuming text-only features, and a single label (answer) associated with each question.", "startOffset": 47, "endOffset": 50}, {"referenceID": 7, "context": "However, we can apply the EM Algorithm [10] to optimize it by estimating the label yq and the parameters \u0398 iteratively.", "startOffset": 39, "endOffset": 43}, {"referenceID": 2, "context": "In [3], the authors collected Q/A data from Amazon.", "startOffset": 3, "endOffset": 6}, {"referenceID": 8, "context": "We used the Stanford CoreNLP [11] library to split reviews into sentences, handle word tokenization, etc.", "startOffset": 29, "endOffset": 33}, {"referenceID": 2, "context": "In the dataset from [3], one thousand questions have been manually labeled as \u2018binary\u2019 or \u2018open-ended.", "startOffset": 20, "endOffset": 23}, {"referenceID": 2, "context": "As in [3], we applied an approach developed by Google [12] to determine whether a question is binary, using a series of simple grammatical rules.", "startOffset": 6, "endOffset": 9}, {"referenceID": 9, "context": "As in [3], we applied an approach developed by Google [12] to determine whether a question is binary, using a series of simple grammatical rules.", "startOffset": 54, "endOffset": 58}, {"referenceID": 2, "context": "This gave us zero error on the held-out manually labeled data from [3].", "startOffset": 67, "endOffset": 70}, {"referenceID": 2, "context": "This is a state-of-the-art method for opinion QA from [3].", "startOffset": 54, "endOffset": 57}, {"referenceID": 2, "context": "This is the method from [3].", "startOffset": 24, "endOffset": 27}, {"referenceID": 0, "context": "There are several previous studies considering the problem of opinion question answering [1]\u2013[3], [13]\u2013[17], where questions are subjective and traditional QA approaches may not be as effective as they have been for factual questions.", "startOffset": 89, "endOffset": 92}, {"referenceID": 2, "context": "There are several previous studies considering the problem of opinion question answering [1]\u2013[3], [13]\u2013[17], where questions are subjective and traditional QA approaches may not be as effective as they have been for factual questions.", "startOffset": 93, "endOffset": 96}, {"referenceID": 10, "context": "There are several previous studies considering the problem of opinion question answering [1]\u2013[3], [13]\u2013[17], where questions are subjective and traditional QA approaches may not be as effective as they have been for factual questions.", "startOffset": 98, "endOffset": 102}, {"referenceID": 14, "context": "There are several previous studies considering the problem of opinion question answering [1]\u2013[3], [13]\u2013[17], where questions are subjective and traditional QA approaches may not be as effective as they have been for factual questions.", "startOffset": 103, "endOffset": 107}, {"referenceID": 15, "context": "Yu and Hatzivassiloglou [18] first proposed a series approaches", "startOffset": 24, "endOffset": 28}, {"referenceID": 16, "context": "[19] applied a two-layer framework to classify questions and estimated question types and polarities to filter irrelevant sentences.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[16] proposed a graph-based approach that regarded sentences as nodes and weighted edges by sentences similarity; by constructing such a graph, they could apply an \u2018Opinion PageRank\u2019 model and an \u2018Opinion HITS\u2019 model to explore different relations.", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": ", addressing product-related questions with reviews, an aspect-based approach was proposed where aspect-rating data were applied [1].", "startOffset": 129, "endOffset": 132}, {"referenceID": 1, "context": "[2], a new model was developed to generate appropriate answers for opinion questions by exploiting the hierarchical organization of consumer reviews.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "Most recently, a supervised learning approach, MoQA, was proposed for the product-related opinion QA problem, where a mixture of experts model was applied and each review was regarded as an expert [3].", "startOffset": 197, "endOffset": 200}, {"referenceID": 17, "context": "A number of opinion mining studies focus on opinion summarization [20], and opinion retrieval and search in review text [21].", "startOffset": 66, "endOffset": 70}, {"referenceID": 18, "context": "A number of opinion mining studies focus on opinion summarization [20], and opinion retrieval and search in review text [21].", "startOffset": 120, "endOffset": 124}, {"referenceID": 19, "context": "In addition, review text can be used to improve recommender systems by modeling different aspects related to customers\u2019 opinions [22], [23].", "startOffset": 129, "endOffset": 133}, {"referenceID": 20, "context": "In addition, review text can be used to improve recommender systems by modeling different aspects related to customers\u2019 opinions [22], [23].", "startOffset": 135, "endOffset": 139}, {"referenceID": 21, "context": "The major technique of modeling ambiguity with multiple labels in this study is inspired by approaches for resolving noisy labels in crowdsourcing tasks [24].", "startOffset": 153, "endOffset": 157}], "year": 2016, "abstractText": "Product review websites provide an incredible lens into the wide variety of opinions and experiences of different people, and play a critical role in helping users discover products that match their personal needs and preferences. To help address questions that can\u2019t easily be answered by reading others\u2019 reviews, some review websites also allow users to pose questions to the community via a question-answering (QA) system. As one would expect, just as opinions diverge among different reviewers, answers to such questions may also be subjective, opinionated, and divergent. This means that answering such questions automatically is quite different from traditional QA tasks, where it is assumed that a single \u2018correct\u2019 answer is available. While recent work introduced the idea of question-answering using product reviews, it did not account for two aspects that we consider in this paper: (1) Questions have multiple, often divergent, answers, and this full spectrum of answers should somehow be used to train the system; and (2) What makes a \u2018good\u2019 answer depends on the asker and the answerer, and these factors should be incorporated in order for the system to be more personalized. Here we build a new QA dataset with 800 thousand questions\u2014and over 3.1 million answers\u2014and show that explicitly accounting for personalization and ambiguity leads both to quantitatively better answers, but also a more nuanced view of the range of supporting, but subjective, opinions.", "creator": "LaTeX with hyperref package"}}}