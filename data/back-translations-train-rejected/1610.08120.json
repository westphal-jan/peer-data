{"id": "1610.08120", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Oct-2016", "title": "Image Segmentation for Fruit Detection and Yield Estimation in Apple Orchards", "abstract": "Ground vehicles equipped with monocular vision systems are a valuable source of high resolution image data for precision agriculture applications in orchards. This paper presents an image processing framework for fruit detection and counting using orchard image data. A general purpose image segmentation approach is used, including two feature learning algorithms; multi-scale Multi-Layered Perceptrons (MLP) and Convolutional Neural Networks (CNN). These networks were extended by including contextual information about how the image data was captured (metadata), which correlates with some of the appearance variations and/or class distributions observed in the data. The pixel-wise fruit segmentation output is processed using the Watershed Segmentation (WS) and Circular Hough Transform (CHT) algorithms to detect and count individual fruits. Experiments were conducted in a commercial apple orchard near Melbourne, Australia. The results show an improvement in fruit segmentation performance with the inclusion of metadata on the previously benchmarked MLP network. We extend this work with CNNs, bringing agrovision closer to the state-of-the-art in computer vision, where although metadata had negligible influence, the best pixel-wise F1-score of $0.791$ was achieved. The WS algorithm produced the best apple detection and counting results, with a detection F1-score of $0.858$. As a final step, image fruit counts were accumulated over multiple rows at the orchard and compared against the post-harvest fruit counts that were obtained from a grading and counting machine. The count estimates using CNN and WS resulted in the best performance for this dataset, with a squared correlation coefficient of $r^2=0.826$.", "histories": [["v1", "Tue, 25 Oct 2016 23:38:02 GMT  (8916kb,D)", "http://arxiv.org/abs/1610.08120v1", "This paper is the initial version of the manuscript submitted to The Journal of Field Robotics in May 2016. Following reviews and revisions, the paper has been accepted for publication. The reviewed version includes extended comparison between the different classification frameworks and a more in-depth literature review"]], "COMMENTS": "This paper is the initial version of the manuscript submitted to The Journal of Field Robotics in May 2016. Following reviews and revisions, the paper has been accepted for publication. The reviewed version includes extended comparison between the different classification frameworks and a more in-depth literature review", "reviews": [], "SUBJECTS": "cs.RO cs.CV cs.LG", "authors": ["suchet bargoti", "james underwood"], "accepted": false, "id": "1610.08120"}, "pdf": {"name": "1610.08120.pdf", "metadata": {"source": "CRF", "title": "Image Segmentation for Fruit Detection and Yield Estimation in Apple Orchards", "authors": ["Suchet Bargoti", "James P. Underwood"], "emails": ["@acfr.usyd.edu.au"], "sections": [{"heading": null, "text": "Ground vehicles equipped with monocular image processing systems are a valuable source of high-resolution image data for precise agricultural applications in orchards. This paper presents an image processing framework for identifying and counting crops based on orchard image data. A general purpose image segmentation approach is used, including two feature learning algorithms; multi-scale Multi-Layered Perceptrons (MLP) and Convolutional Neural Networks (CNN). These networks have been enhanced with contextual information on how image data was acquired (metadata) that correlates with some of the appearance variations and / or class distributions observed in the data. Pixel-by-pixel fruit segmentation output is performed using the Watershed Segmentation (WS) and Circular Hough Transform (CHT) algorithms for detecting and counting individual crops. Experiments were conducted in a commercial apple plantation near Melbourne, Australia."}, {"heading": "1 Introduction", "text": "In fact, most of them are able to survive on their own without being able to afford it."}, {"heading": "2 Related Work", "text": "This is an essential point that is used in relation to current image processing in the general computer vision community (Kapach et al., 2014). In this section we discuss the main approaches used in agrovision, while challenging them against the state-of-the-art techniques of computer vision. Image processing at Orchards spans a wide variety of fruits such as grapes., 2014; Font et al., 2015), Mangos (Chhabra et al., 2012; Payne et al., 2014), Apples (Linker et al., 2013; Stajko et al., 2009; Li i al."}, {"heading": "3 Image Segmentation", "text": "In this paper, we present several image segmentation architectures for the binary classification of fruit tree image data into fruit / non-crop classes, including a multi-scale MLP and a state-of-the-art deep CNN architecture. We then extend these neural network architectures by including Orchard metadata. All network training is performed at the pixel level, but conclusions are drawn on the overall picture, resulting in a dense likely output of fruit and non-crop classes, which can be used to obtain a binary fruit mask and then perform fruit recognition or yield estimation as detailed in Section 6."}, {"heading": "3.1 Multi-scale Multi-Layered Perceptron", "text": "Given the success of image segmentation in Hung et al. (2013) for different crop species, the reference segmentation architecture in this paper is based on a multi-dimensional multi-layered perceptron (which we call ms-MLP), which takes as input a contextual window around single pixels from the raw RGB image, with the windows sampled at different image scales. (The data is propagated through multiple fully connected layers and the output of the classifier) is a probability of a given pixel belonging to the fruit / non-fruit class. (The multi-dimensional patch representation of each pixel provides scale inventory for classification and allows us to capture local variations at different scales, such as edges between fruit and leaves and between trees and the skyline, while the input dimensions low.A provide three layers of the MLP architecture in Figure 3."}, {"heading": "3.2 Convolution Neural Networks", "text": "Like MLP, the network calculates the probability that a pixel is a fruit or a non-fruit, using the image intensities of a square window with the pixel itself as input, but this is due to the CNNs ability to split smaller filters in each layer, minimizing the number of model parameters. A typical CNN consists of a sequence of revolutionary, max-pooling and fully connected layers, as in Figure 4. Each revolutionary layer performs a 2D folding of its input cards with a square filter, minimizing the number of model parameters. A typical CNN consists of a sequence of revolutionary, max-pooling and fully connected layers, as in Figure 4. Each convolution layer performs a 2D folding of its input cards with a square filter."}, {"heading": "3.3 Adding Metadata", "text": "Metadata corresponding to individual pixels can be integrated into image segmentation architectures by appending the information to one of the fully connected layers. We can then link any set of metadata for a given input instance Ii, j, k, d n i, j, k, n, where N is the set of different meta-parameters, e.g. sun position, tree species, etc. The different metadata can then be linked together. Di, j, k = U sDi, j, k + b s 1) (6) In ms-MLP, propagation into the first hidden layer is then given by: H1 = S s = 1 \u03c3 (W s1 I s, j, k + U sDi, j, k + b s 1) (6), being the amount of weights learned for each hidden layer used for scale-independent metadata input."}, {"heading": "3.4 Scene Inference", "text": "In practice, this can be done densely over sliding windows for both the ms-MLP and CNN architecture to obtain a segmented image, but this approach is highly inefficient. Instead, a mathematically efficient approach is to redesign the learned models as fully revolutionary operations that allow for fast, dense prediction over arbitrarily large inputs. For the ms-MLP, we transform the first layer weights into patch-wise cores that are sampled at different scales via the test images."}, {"heading": "4 Experimental Setup", "text": "Image segmentation architectures for the orchard were tested in an apple orchard in Victoria, Australia (Figure 1). Data were collected over a 0.5 hectare block housing a modern V-grid structure of Gu \ufffd ttingen, where the plants are planted over pairs of trellis arranged into a V-shape. These structures consist of support poles and wires along the rows, providing better support for tree limbs, allowing more sunlight for fruit and easier harvesting. There were various apple varieties on the trees, including Kanzi and Pink Lady. Fruit color ranged from bright red to a blend of pink-green and all-green as shown in Figure 5. The experiments were conducted a week before the harvest (March 2013), with apple diameters ranging from 70 to 76 mm."}, {"heading": "4.1 Data Capturing", "text": "The Shrimp Test Platform is a ground vehicle built at the Australian Centre for Field Robotics at the University of Sydney to investigate perception (Figure 1) and, in addition to a number of sensor types, is equipped with a Spherical Digital Video Camera Point Grey Ladybug3, which includes six 2MP cameras designed to capture a full 360 \u00b0 panorama. To capture the dataset, the vehicle was remotely controlled between 15 rows of orchards at 1.5 to 2 ms \u2212 1. The driver followed the vehicle and guided it manually along the centerline of rows arranged at a distance of 4 m. Images of the size 1232 x 1616 were captured at 5 Hz by the camera facing the trellis structure, and the camera's field of view was crucial in its selection as it was able to capture the 4 m tall trees from a distance of 2 m, as shown in Figure 1. Data was collected under natural lighting conditions, and specific daytime angles, and sunlight conditions were not specifically selected to reflect the weather conditions (Figure 1)."}, {"heading": "4.2 Image Dataset", "text": "In total, the image dataset consists of over 8,000 images, each with 1232 x 1616 pixels. As it would be impractical to label so much data manually, a subset was collected by random sub-sampling, each image was split into 32 sub-images with 308 x 202 pixels, and a fixed number of sub-images from each line were randomly scanned to maximize the diversity of data. In total, 1,100 images were collected and manually labeled with binary pixel labels for the fruit and non-fruit classes. Dividing the image into smaller sections facilitates the manual labeling process and results in greater spatial variation within the dataset. Examples of the reduced dataset and associated pixel-wise labels are shown in Figure 6. Apples in the images varied in size from 25 to 50 pixels in diameter, which is attributed to the distance to the image platform and viewing angle."}, {"heading": "4.3 Segmentation Architecture", "text": "For the training of the MLP a similar architecture as in Hung et al. (2015) was used. Individual instances / patches of the size [8 x 8 x 3] were randomly sampled via screens. (1 x 2 x 4 x 8 x 8 cm) However, a network was also built to initiate the first layer of the filters. (1 x 8 x 8 cm) An approach was applied that refers to the approach of the individual countries. (1 x 8 x 8 cm) A network was built in which the individual layers of the individual layers are divided. (2 x 8 x 8 cm) A layer was formed. (2 x 8 x 8 cm) A layer was formed. (2 x 8 x 8 cm) A layer was formed. (2 x 8 x 8 x 8 cm)"}, {"heading": "5 Segmentation Results", "text": "To evaluate image segmentation performance, the labeled dataset (1100 images) was randomly split into an 80 \u2212 10 \u2212 10 split of training, validation, and test images. Single-scale and multi-scale patches were sampled (between classes) from the set of training images to train the CNN and ms-MLP architecture2. In this section, we evaluate the performance of the various architectures using the fruit F1 score, which was randomly evaluated from the test image set. The class threshold for the optimal F1 score was evaluated via the sampled instances from the validation set. In all tests, mean and standard deviation classification results were obtained over 10 iterations, while the 80 \u2212 10 \u2212 10 split was mixed. In the last part of this section, the patch-based models were used for the entire image reference, with qualitative segmentation results presented over images from the test set."}, {"heading": "5.1 Multi-scale Multi-Layered Perceptron", "text": "For comparison, the original architecture of (Hung et al., 2015), a two-tiered MLP with 200 hidden units, was trained with 200,000 training instances, which we call ms-MLP-2. Various metadata sources were then added to this configuration, and the MLP was retrained each time. Training of each MLP network until convergence via the validation rate took about 5 minutes on a GPU-enabled desktop. Classification results are shown in Table 1. There is a marked improvement in classification results by including all metadata that increase the F1 score most by 6.2% (F1: 0.683 \u2192 0.725). On its own basis, it was determined that pi (the height within the original image) are the most important individual metadata that do not represent relevant metadata during interpretation, which is most empirically related to the appearance variations in the data."}, {"heading": "5.1.1 Optimal ms-MLP Architecture", "text": "We range from Hung et al. (2015) across different combinations of depth and width for the MLP architecture, a grid search was performed over networks with a depth of 2-5 layers and a width of 200 to 1000 units per layer. Optimal fruit classification performance was achieved with a 3-layer MLP, with each hidden layer containing 200 hidden units. We refer to this network as ms-MLP-3 * and it is in Figure 3. As before, metadata within this network was added to the input layer. During the optimization phase, we experimented with adding the metadata to deeper layers instead of the input layer and its distribution by independent hidden layers, before merging with the image data as in Rao et al. (2014), however, the best results were obtained when the raw metadata was added along with the input data on multiple layers. Segment results are presented in Table 2."}, {"heading": "5.1.2 Varying Training Size", "text": "As mentioned in Section 1, a classifier is rather invariant compared to intra-class variations due to high model complexity and sufficient training data. The training size for patch-based segmentation architectures is the number of pixels (neighborhood parts) extracted from the set of labeled images for training. A larger number of training instances would typically lead to better classification performance, but leads to a linear increase in algorithm runtime during training. Using the ms-MLP-3 * configuration, we evaluate the impact of training size on classification outcomes, with and without the inclusion of metadata. The classification F1 score (shown in Figure 8) increases as we sample more instances from the training set and achieve convergence by 500,000 training instances. Theoretically, a total of 56 x 106 patches can be sampled from the metadata, i.e. intersecting individual instances from the training instances can be marginal."}, {"heading": "5.2 Convolutional Neural Network", "text": "For the optimal CNN configuration, we searched for different depths, widths, and input sizes while monitoring the F1 score over the pre-set validation set. We tested a series of input sizes covering a {32 \u00d7 32}, {48 \u00d7 48}, {64 \u00d7 64} region around a pixel. For the Constitutional layers, we tested a depth of 2-3 layers, each containing a convolution, activation, and pooling operation, followed by two fully connected layers, as usually used in computer vision literature. The optimal architecture was found through a rough grid search and is referred to in this paper as CNN * (illustrated in Figure 4). The first convolutionary layer filtered the 48 x 48 x 48 x 3 entered image fields with 64 cores of size 7 x 7 x 3 and followed by a ReLU activation and a max pooling operation with a spacing of 2 x 2 pixels."}, {"heading": "5.3 Whole Image Segmentation", "text": "To obtain input for consistent crop recognition and yield estimation, and to qualitatively analyze the results of segmentation, the trained ms-MLP and CNN architectures were used to segment entire images using the techniques discussed in Section 3.4. The image conclusion was performed using the ms-MLP-3 * architecture with and without metadata, and with the CNN * architecture without metadata. Inclusion of metadata in the CNN architecture was not tested here due to the minimal classification gains. The resulting binary mask for some images is shown in columns 3 and 4 of Figure 9, with and without metadata."}, {"heading": "6 Fruit Detection and Yield Prediction", "text": "The results of the above image segmentation methods are class probability maps or binary masks that indicate the position of individual fruit pixels in the images. In this section, we present methods to translate these into agronomically meaningful information, such as the recognition of individual fruit, fruit count and fruit yield maps. The segmentation architectures we focus on are ms-MLP-3 * (no metadata), ms-MLP-3 * with metadata and CNN * (no metadata).The metadata configuration of the CNN network is not being further tested as it has resulted in minimal improvements in segmentation outcomes. In addition, we evaluate the recognition and yield estimation performance of the fruit against different basic truths and compare the effects of different segmentation architectures on higher agricultural tasks."}, {"heading": "6.1 Fruit Detection", "text": "This year it is so far that it will be able to eren.ndnei the aforementioned lcsrteeSe"}, {"heading": "6.2 Yield estimation", "text": "However, even with perfect segmentation, followed by an appropriate detection algorithm, the precise image distribution and recording of yields cannot directly indicate the actual number of apples due to suspicions. Rather, we assume that there is a constant ratio of visible fruits to hidden fruits, which allows us to vary yields on the farm or make yield estimates that include some calibration data. In this case, the grower has to produce the calibration data by counting and weighing the offspring independently of each other. The data collected ranges from 15 lines, from 3,000 to 12,000 apples per year."}, {"heading": "7 Discussion", "text": "In fact, it's like most people who are able to determine for themselves what they want and what they want to do have to take matters into their own hands, \"he said.\" But it's not like people are able to do the things they need to do. \"He added,\" I don't think people are able to do the things they need to do. \"He added,\" I don't think people are able to do the things they need to do, the things they need to do, the things they need to do, the things they need to do, the things they need to do, the things they do, the things they do, the things they do, the things they do, the things they don't do, the things they do, the things they do, the things they do, the things they do, the things they do, the things they do, the things they do, the things they do, the things they do, the things they don't do, the things they do, the things they do, the things they do, the things they do, the things they do, the things they don't do, the things they do, the things they do, the things they do, the things they do, the things they do, the things they don't do, the things they do, the things they do, the things they do, the things they do, the things they do, the things they do, the things they do, the things they don't do, the things, the things they do, the things they do, the things they do, the things they do, the things they do, the things they do, the things they do, the things they do, the things they do, the things they do, the things they do, the things they don't do, the things, the things they do, the things, the things they do, the things they do, the things they do, the things they do, the things, the things they do, the things they do, the things, the things they do, the things they do, the things they do, the things, the things they do, they do, the things, the things, they do, the things, they do, the things, they do, the things, the things, they don't do, the things, they don't do, the things, the things, the things, they do, the things, the things, the things, the"}, {"heading": "7.1 Image Processing Errors", "text": "The most common image segmentation errors were in regions with poor image quality due to adverse illumination conditions (e.g. solar flares and underexposure in the shade) or in ambiguous image regions (e.g. closed apples between green leaves); in addition, there were errors and inconsistencies within human-identified data where fruit was either overlooked or background crops (in an adjacent row) were inconsistently labeled, limiting the performance and evaluation measures of the segmentation and recognition algorithms; the first two lines in Figure 13 are examples of discrepancies in soil truth seen by differences in pixel and fruit labeling; the primary causes of detection errors were poor image segmentation, where fruit appear in clusters, and duplication of fruit regions; the WS and CHT detection algorithms are efficient in splitting fruit cultures, which are located in two, but not in three, clusters."}, {"heading": "7.2 Lessons Learned", "text": "In fact, most people are able to decide for themselves what they want and what they want."}, {"heading": "8 Conclusion", "text": "This paper presented an image processing framework for crop detection and yield estimation in an orchard. However, general purpose learning algorithms were used for image segmentation, followed by the detection and counting of individual fruits. Image segmentation was performed using a multi-scale multi-layered perceptron (ms-MLP) and a Convolutional Neural Network (CNN) to explicitly capture relationships between meta-parameters and object classes to be learned. Pixel-by-pixel image segmentation was evaluated using watershed segmentation (WS) and the Circular Hough Transform (CHT) algorithms to capture the relationships between meta-parameters and the object classes to be learned.The various stages of the pipeline were evaluated using image data collected over a 0.5 hectare apple and fruit tree block with a monocular camera mounted on an unmanned ground vehicle."}, {"heading": "Acknowledgments", "text": "This work is supported by the Australian Centre for Field Robotics at the University of Sydney and Horticulture Australia Limited through the AH11009 Autonomous Perception System for Horticulture Tree Crops Project. Thanks to Calvin Hung for his guidance on the MLP architecture and Kevin Sander for his support during field trials in his apple orchard. More information and videos at: http: / / sydney.edu.au / acfr / agriculture"}], "references": [{"title": "Color vision system for estimating citrus yield in real-time", "author": ["P. Annamalai", "W.S. Lee", "T.F. Burks"], "venue": "ASAE Annual International Meeting.", "citeRegEx": "Annamalai et al\\.,? 2004", "shortCiteRegEx": "Annamalai et al\\.", "year": 2004}, {"title": "Size invariant circle detection", "author": ["T.J. Atherton", "D.J. Kerbyson"], "venue": "Image and Vision Computing, 17(11):795\u2013803.", "citeRegEx": "Atherton and Kerbyson,? 1999", "shortCiteRegEx": "Atherton and Kerbyson", "year": 1999}, {"title": "Utilising Metadata to Aid Image Classification in Orchards", "author": ["S. Bargoti", "J. Underwood"], "venue": "IEEE International Conference on Intelligent Robots and Systems (IROS), Workshop on Alternative Sensing for Robot Perception (WASRoP).", "citeRegEx": "Bargoti and Underwood,? 2015", "shortCiteRegEx": "Bargoti and Underwood", "year": 2015}, {"title": "Image Classification with Orchard Metadata", "author": ["S. Bargoti", "J. Underwood"], "venue": "IEEE International Conference on Robotics and Automation (ICRA) (Accepted).", "citeRegEx": "Bargoti and Underwood,? 2016", "shortCiteRegEx": "Bargoti and Underwood", "year": 2016}, {"title": "A Pipeline for Trunk Detection in Trellis Structured Apple Orchards", "author": ["S. Bargoti", "J.P. Underwood", "J.I. Nieto", "S. Sukkarieh"], "venue": "Journal of Field Robotics, 32(8):1075\u20131094.", "citeRegEx": "Bargoti et al\\.,? 2015", "shortCiteRegEx": "Bargoti et al\\.", "year": 2015}, {"title": "Multi-spectral SIFT for scene category recognition", "author": ["M. Brown", "S. Susstrunk"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 177\u2013184.", "citeRegEx": "Brown and Susstrunk,? 2011", "shortCiteRegEx": "Brown and Susstrunk", "year": 2011}, {"title": "Convolutional Patch Networks with Spatial Prior for Road Detection and Urban Scene Understanding", "author": ["Brust", "C.-a.", "S. Sickert", "M. Simon", "E. Rodner", "J. Denzler"], "venue": "International Conference on Computer Vision Theory and Applications (VISAPP), pages 510\u2013517.", "citeRegEx": "Brust et al\\.,? 2015", "shortCiteRegEx": "Brust et al\\.", "year": 2015}, {"title": "Automated Detection of Fully and Partially Riped Mango by Machine Vision", "author": ["M. Chhabra", "A. Gupta", "P. Mehrotra", "S. Reel"], "venue": "Proceedings of the International Conference on Soft Computing for Problem Solving (SocProS), volume 131 of Advances in Intelligent and Soft Computing, pages 153\u2013164. Springer India.", "citeRegEx": "Chhabra et al\\.,? 2012", "shortCiteRegEx": "Chhabra et al\\.", "year": 2012}, {"title": "Deep Neural Networks Segment Neuronal Membranes in Electron Microscopy Images", "author": ["D. Ciresan", "A. Giusti", "L.M. Gambardella", "J. Schmidhuber"], "venue": "Advances in Neural Information Processing Systems 25, pages 2843\u20132851. Curran Associates, Inc.", "citeRegEx": "Ciresan et al\\.,? 2012", "shortCiteRegEx": "Ciresan et al\\.", "year": 2012}, {"title": "Why Does Unsupervised Pre-training Help Deep Learning", "author": ["D. Erhan", "Y. Bengio", "A. Courville", "Manzagol", "P.-A", "P. Vincent", "S. Bengio"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Erhan et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Erhan et al\\.", "year": 2010}, {"title": "Learning Hierarchical Features for Scene Labeling", "author": ["C. Farabet", "C. Couprie", "L. Najman", "Y. LeCun"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, 35(8):1915\u20131929.", "citeRegEx": "Farabet et al\\.,? 2013", "shortCiteRegEx": "Farabet et al\\.", "year": 2013}, {"title": "Vineyard Yield Estimation Based on the Analysis of High Resolution Images Obtained with Artificial Illumination at Night", "author": ["D. Font", "M. Tresanchez", "D. Mart\u0301\u0131nez", "J. Moreno", "E. Clotet", "J. Pala\u0107\u0131n"], "venue": null, "citeRegEx": "Font et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Font et al\\.", "year": 2015}, {"title": "N-Fields: Neural Network Nearest Neighbor Fields for Image Transforms", "author": ["Y. Ganin", "V.S. Lempitsky"], "venue": "CoRR, abs/1406.6.", "citeRegEx": "Ganin and Lempitsky,? 2014", "shortCiteRegEx": "Ganin and Lempitsky", "year": 2014}, {"title": "Pylearn2: a machine learning research library", "author": ["I. Goodfellow", "D. Warde-Farley"], "venue": "arXiv preprint arXiv:1308.4214, pages 1\u20139.", "citeRegEx": "Goodfellow and Warde.Farley,? 2013", "shortCiteRegEx": "Goodfellow and Warde.Farley", "year": 2013}, {"title": "Orchard fruit segmentation using multi-spectral feature learning", "author": ["C. Hung", "J. Nieto", "Z. Taylor", "J. Underwood", "S. Sukkarieh"], "venue": "IEEE International Conference on Intelligent Robots and Systems (IROS), pages 5314\u20135320.", "citeRegEx": "Hung et al\\.,? 2013", "shortCiteRegEx": "Hung et al\\.", "year": 2013}, {"title": "A Feature Learning Based Approach for Automated Fruit Yield Estimation", "author": ["C. Hung", "J. Underwood", "J. Nieto", "S. Sukkarieh"], "venue": "Field and Service Robotics (FSR), pages 485\u2013498. Springer.", "citeRegEx": "Hung et al\\.,? 2015", "shortCiteRegEx": "Hung et al\\.", "year": 2015}, {"title": "Automatic recognition vision system guided for apple harvesting robot", "author": ["W. Ji", "D. Zhao", "F. Cheng", "B. Xu", "Y. Zhang", "J. Wang"], "venue": "Computers & Electrical Engineering, 38(5):1186\u20131195.", "citeRegEx": "Ji et al\\.,? 2012", "shortCiteRegEx": "Ji et al\\.", "year": 2012}, {"title": "A survey of computer vision methods for locating fruit on trees", "author": ["A.R. Jimenez", "R. Ceres", "J.L. Pons"], "venue": "Transactions of the ASAE-American Society of Agricultural Engineers, 43(6):1911\u20131920.", "citeRegEx": "Jimenez et al\\.,? 2000", "shortCiteRegEx": "Jimenez et al\\.", "year": 2000}, {"title": "Computer Vision for Fruit Harvesting Robots - state of the Art and Challenges Ahead", "author": ["K. Kapach", "E. Barnea", "R. Mairon", "Y. Edan", "O. Ben-Shahar"], "venue": "International Journal of Computational Vision and Robotics, 3(1-2):4\u201334.", "citeRegEx": "Kapach et al\\.,? 2012", "shortCiteRegEx": "Kapach et al\\.", "year": 2012}, {"title": "A Novel Red Apple Detection Algorithm Based on AdaBoost Learning", "author": ["D. Kim", "H. Choi", "J. Choi", "S.J. Yoo", "D. Han"], "venue": "IEIE Transactions on Smart Processing & Computing, 4(4):265\u2013271.", "citeRegEx": "Kim et al\\.,? 2015", "shortCiteRegEx": "Kim et al\\.", "year": 2015}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "Advances in neural information processing systems, pages 1097\u20131105.", "citeRegEx": "Krizhevsky et al\\.,? 2012", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Immature peach detection in colour images acquired in natural illumination conditions using statistical classifiers and neural network", "author": ["F. Kurtulmus", "W. Lee", "A. Vardar"], "venue": "Precision Agriculture, 15(1):57\u201379.", "citeRegEx": "Kurtulmus et al\\.,? 2014", "shortCiteRegEx": "Kurtulmus et al\\.", "year": 2014}, {"title": "Study on citrus fruit image data separability by segmentation methods", "author": ["P. Li", "Lee", "S.-h.", "Hsu", "H.-Y."], "venue": "Procedia Engineering, 23:408\u2013416.", "citeRegEx": "Li et al\\.,? 2011", "shortCiteRegEx": "Li et al\\.", "year": 2011}, {"title": "Determination of the number of green apples in RGB images recorded in orchards", "author": ["R. Linker", "O. Cohen", "A. Naor"], "venue": "Computers and Electronics in Agriculture, 81:45\u201357.", "citeRegEx": "Linker et al\\.,? 2012", "shortCiteRegEx": "Linker et al\\.", "year": 2012}, {"title": "Automatic grape bunch detection in vineyards for precise yield estimation", "author": ["S. Liu", "M. Whitty", "S. Cossell"], "venue": null, "citeRegEx": "Liu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2015}, {"title": "Fully Convolutional Networks for Semantic Segmentation", "author": ["J. Long", "E. Shelhamer", "T. Darrell"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 3431\u2013 3440.", "citeRegEx": "Long et al\\.,? 2015", "shortCiteRegEx": "Long et al\\.", "year": 2015}, {"title": "Deep learning via Hessian-free optimization", "author": ["J. Martens"], "venue": "Proceedings of the 27th International Conference on Machine Learning (ICML), pages 735\u2013742.", "citeRegEx": "Martens,? 2010", "shortCiteRegEx": "Martens", "year": 2010}, {"title": "Fruit detection, tracking, and 3D reconstruction for crop mapping and yield estimation", "author": ["J. Moonrinta", "S. Chaivivatrakul", "M.N. Dailey", "M. Ekpanyapong"], "venue": "International Conference on Control Automation Robotics & Vision (ICARCV), pages 1181\u20131186.", "citeRegEx": "Moonrinta et al\\.,? 2010", "shortCiteRegEx": "Moonrinta et al\\.", "year": 2010}, {"title": "Toward automatic phenotyping of developing embryos from videos", "author": ["F. Ning", "D. Delhomme", "Y. LeCun", "F. Piano", "L. Bottou", "P.E. Barbano"], "venue": "IEEE Transactions on Image Processing, 14(9):1360\u2013 1371.", "citeRegEx": "Ning et al\\.,? 2005", "shortCiteRegEx": "Ning et al\\.", "year": 2005}, {"title": "Automated visual yield estimation in vineyards", "author": ["S. Nuske", "K. Wilshusen", "S. Achar", "L. Yoder", "S. Singh"], "venue": "Journal of Field Robotics, 31(5):837\u2013860.", "citeRegEx": "Nuske et al\\.,? 2014", "shortCiteRegEx": "Nuske et al\\.", "year": 2014}, {"title": "Machine vision in estimation of fruit crop yield", "author": ["A. Payne", "K. Walsh"], "venue": "Plant Image Analysis: Fundamentals and Applications, chapter 16, pages 329\u2013374. CRC Press.", "citeRegEx": "Payne and Walsh,? 2014", "shortCiteRegEx": "Payne and Walsh", "year": 2014}, {"title": "Estimating mango crop yield using image analysis using fruit at \u2019stone hardening\u2019 stage and night time imaging", "author": ["Payne", "K. Walsh", "P. Subedi", "D. Jarvis"], "venue": "Computers and Electronics in Agriculture,", "citeRegEx": "Payne et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Payne et al\\.", "year": 2014}, {"title": "Recurrent Convolutional Neural Networks for Scene Parsing", "author": ["P.H.O. Pinheiro", "R. Collobert"], "venue": "CoRR, abs/1306.2.", "citeRegEx": "Pinheiro and Collobert,? 2013", "shortCiteRegEx": "Pinheiro and Collobert", "year": 2013}, {"title": "Identification of fruit and branch in natural scenes for citrus harvesting robot using machine vision and support vector machine", "author": ["L. Qiang", "C. Jianrong", "L. Bin", "D. Lie", "Z. Yajing"], "venue": "International Journal of Agricultural and Biological Engineering, 7(2):115\u2013121.", "citeRegEx": "Qiang et al\\.,? 2014", "shortCiteRegEx": "Qiang et al\\.", "year": 2014}, {"title": "Multimodal learning for autonomous underwater vehicles from visual and bathymetric data", "author": ["D. Rao", "M. De Deuge", "N. Nourani-Vatani", "B. Douillard", "S.B. Williams", "O. Pizarro"], "venue": "IEEE International Conference on Robotics and Automation (ICRA), pages 3819\u20133825.", "citeRegEx": "Rao et al\\.,? 2014", "shortCiteRegEx": "Rao et al\\.", "year": 2014}, {"title": "Citrus fruit identification and size determination using machine vision and ultrasonic sensors", "author": ["M. Regunathan", "W.S. Lee"], "venue": "ASAE annual international meeting.", "citeRegEx": "Regunathan and Lee,? 2005", "shortCiteRegEx": "Regunathan and Lee", "year": 2005}, {"title": "The watershed transform: Definitions, algorithms and parallelization strategies", "author": ["Roerdink", "J.B.T.M.", "A. Meijster"], "venue": "Fundamenta informaticae, 41(1-2):187\u2013228.", "citeRegEx": "Roerdink et al\\.,? 2000", "shortCiteRegEx": "Roerdink et al\\.", "year": 2000}, {"title": "On Visual Detection of Highly-occluded Objects for Harvesting Automation in Horticulture", "author": ["I. Sa", "C. McCool", "C. Lehnert", "T. Perez"], "venue": "IEEE International Conference on Robotics and Automation (ICRA). ICRA.", "citeRegEx": "Sa et al\\.,? 2015", "shortCiteRegEx": "Sa et al\\.", "year": 2015}, {"title": "Identification and determination of the number of immature green citrus fruit in a canopy under different ambient light conditions", "author": ["S. Sengupta", "W.S. Lee"], "venue": "Biosystems Engineering, 117:51\u201361.", "citeRegEx": "Sengupta and Lee,? 2014", "shortCiteRegEx": "Sengupta and Lee", "year": 2014}, {"title": "Overfeat: Integrated recognition, localization and detection using convolutional networks", "author": ["P. Sermanet", "D. Eigen", "X. Zhang", "M. Mathieu", "R. Fergus", "Y. LeCun"], "venue": "arXiv preprint arXiv:1312.6229.", "citeRegEx": "Sermanet et al\\.,? 2013", "shortCiteRegEx": "Sermanet et al\\.", "year": 2013}, {"title": "Identification of red apples in field environment with over the row machine vision system", "author": ["A. Silwal", "A. Gongal", "M. Karkee"], "venue": "Agricultural Engineering International: CIGR Journal, 16(4):66\u201375.", "citeRegEx": "Silwal et al\\.,? 2014", "shortCiteRegEx": "Silwal et al\\.", "year": 2014}, {"title": "Automatic fruit recognition and counting from multiple images", "author": ["Y. Song", "Glasbey", "C. a", "G.W. Horgan", "G. Polder", "Dieleman", "J. a", "van der Heijden", "G.W. a. M"], "venue": "Biosystems Engineering,", "citeRegEx": "Song et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Song et al\\.", "year": 2014}, {"title": "Modelling apple fruit yield using image analysis for fruit colour, shape and texture", "author": ["D. Stajnko", "J. Rakun", "M.M. Blanke"], "venue": "European Journal of Horticultural Science, 74(6):260\u2013267.", "citeRegEx": "Stajnko et al\\.,? 2009", "shortCiteRegEx": "Stajnko et al\\.", "year": 2009}, {"title": "Superparsing", "author": ["J. Tighe", "S. Lazebnik"], "venue": "International Journal of Computer Vision, 101(2):329\u2013349.", "citeRegEx": "Tighe and Lazebnik,? 2013", "shortCiteRegEx": "Tighe and Lazebnik", "year": 2013}, {"title": "Automated Crop Yield Estimation for Apple Orchards", "author": ["Q. Wang", "S. Nuske", "M. Bergerman", "S. Singh"], "venue": "Experimental Robotics, volume 88 of Springer Tracts in Advanced Robotics, pages 745\u2013758. Springer International Publishing.", "citeRegEx": "Wang et al\\.,? 2013", "shortCiteRegEx": "Wang et al\\.", "year": 2013}, {"title": "Towards a generalized colour image segmentation for kiwifruit detection", "author": ["P. Wijethunga", "S. Samarasinghe", "D. Kulasiri", "I. Woodhead"], "venue": "International Conference Image and Vision Computing New Zealand (IVCNZ), pages 62\u201366.", "citeRegEx": "Wijethunga et al\\.,? 2009", "shortCiteRegEx": "Wijethunga et al\\.", "year": 2009}, {"title": "On Plant Detection of Intact Tomato Fruits Using Image Analysis and Machine Learning Methods", "author": ["K. Yamamoto", "W. Guo", "Y. Yoshioka", "S. Ninomiya"], "venue": "Sensors, 14(7):12191\u201312206.", "citeRegEx": "Yamamoto et al\\.,? 2014", "shortCiteRegEx": "Yamamoto et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 31, "context": "Typically, prior works utilise hand engineered features to encode visual attributes that discriminate fruits from non-fruit regions (Payne et al., 2014; Wang et al., 2013; Nuske et al., 2014).", "startOffset": 132, "endOffset": 191}, {"referenceID": 44, "context": "Typically, prior works utilise hand engineered features to encode visual attributes that discriminate fruits from non-fruit regions (Payne et al., 2014; Wang et al., 2013; Nuske et al., 2014).", "startOffset": 132, "endOffset": 191}, {"referenceID": 29, "context": "Typically, prior works utilise hand engineered features to encode visual attributes that discriminate fruits from non-fruit regions (Payne et al., 2014; Wang et al., 2013; Nuske et al., 2014).", "startOffset": 132, "endOffset": 191}, {"referenceID": 14, "context": "In contrast, supervised feature learning approaches can be used to automatically learn transformations that capture the data distribution, enabling their use with different datasets (Hung et al., 2013).", "startOffset": 182, "endOffset": 201}, {"referenceID": 20, "context": "Such approaches have high model complexity and utilise extensive training exemplars to model the variations in the data (Krizhevsky et al., 2012).", "startOffset": 120, "endOffset": 145}, {"referenceID": 40, "context": "For example, Tighe and Lazebnik (2013) and Brust et al.", "startOffset": 13, "endOffset": 39}, {"referenceID": 4, "context": "For example, Tighe and Lazebnik (2013) and Brust et al. (2015) specify a spatial prior over the labelled classes to aid image segmentation of public image datasets such as LabelMeFacade and SIFT Flow.", "startOffset": 43, "endOffset": 63}, {"referenceID": 3, "context": "This paper extends from our previous work with evaluation using new segmentation architectures including different configurations of the previously used MLP architecture (Bargoti and Underwood, 2016) and the more widely used CNNs.", "startOffset": 170, "endOffset": 199}, {"referenceID": 14, "context": "\u2022 Image fruit segmentation analysis using the previously benchmarked multi-scale Multi-Layered Perceptron (Hung et al., 2013; Bargoti and Underwood, 2016) and the state-of-the-art Convolutional Neural Networks.", "startOffset": 106, "endOffset": 154}, {"referenceID": 3, "context": "\u2022 Image fruit segmentation analysis using the previously benchmarked multi-scale Multi-Layered Perceptron (Hung et al., 2013; Bargoti and Underwood, 2016) and the state-of-the-art Convolutional Neural Networks.", "startOffset": 106, "endOffset": 154}, {"referenceID": 18, "context": "Computer vision in agriculture, agrovision (Kapach et al., 2012), has been explored in multiple literature studies for the purposes of fruit detection and yield estimation (Jimenez et al.", "startOffset": 43, "endOffset": 64}, {"referenceID": 17, "context": ", 2012), has been explored in multiple literature studies for the purposes of fruit detection and yield estimation (Jimenez et al., 2000; Payne and Walsh, 2014; Kapach et al., 2012).", "startOffset": 115, "endOffset": 181}, {"referenceID": 30, "context": ", 2012), has been explored in multiple literature studies for the purposes of fruit detection and yield estimation (Jimenez et al., 2000; Payne and Walsh, 2014; Kapach et al., 2012).", "startOffset": 115, "endOffset": 181}, {"referenceID": 18, "context": ", 2012), has been explored in multiple literature studies for the purposes of fruit detection and yield estimation (Jimenez et al., 2000; Payne and Walsh, 2014; Kapach et al., 2012).", "startOffset": 115, "endOffset": 181}, {"referenceID": 18, "context": "Agrovision literature is typically data specific, designed for the task at hand, and can often be very heuristic when compared against the most recent work in the general computer vision community (Kapach et al., 2012).", "startOffset": 197, "endOffset": 218}, {"referenceID": 29, "context": "Image processing at orchards spans a large variety of fruits such as grapes (Nuske et al., 2014; Font et al., 2015), mangoes (Chhabra et al.", "startOffset": 76, "endOffset": 115}, {"referenceID": 11, "context": "Image processing at orchards spans a large variety of fruits such as grapes (Nuske et al., 2014; Font et al., 2015), mangoes (Chhabra et al.", "startOffset": 76, "endOffset": 115}, {"referenceID": 7, "context": ", 2015), mangoes (Chhabra et al., 2012; Payne et al., 2014), apples (Linker et al.", "startOffset": 17, "endOffset": 59}, {"referenceID": 31, "context": ", 2015), mangoes (Chhabra et al., 2012; Payne et al., 2014), apples (Linker et al.", "startOffset": 17, "endOffset": 59}, {"referenceID": 23, "context": ", 2014), apples (Linker et al., 2012; Wang et al., 2013; Stajnko et al., 2009; Silwal et al., 2014; Ji et al., 2012; Kim et al., 2015; Hung et al., 2015), citrus (Li et al.", "startOffset": 16, "endOffset": 153}, {"referenceID": 44, "context": ", 2014), apples (Linker et al., 2012; Wang et al., 2013; Stajnko et al., 2009; Silwal et al., 2014; Ji et al., 2012; Kim et al., 2015; Hung et al., 2015), citrus (Li et al.", "startOffset": 16, "endOffset": 153}, {"referenceID": 42, "context": ", 2014), apples (Linker et al., 2012; Wang et al., 2013; Stajnko et al., 2009; Silwal et al., 2014; Ji et al., 2012; Kim et al., 2015; Hung et al., 2015), citrus (Li et al.", "startOffset": 16, "endOffset": 153}, {"referenceID": 40, "context": ", 2014), apples (Linker et al., 2012; Wang et al., 2013; Stajnko et al., 2009; Silwal et al., 2014; Ji et al., 2012; Kim et al., 2015; Hung et al., 2015), citrus (Li et al.", "startOffset": 16, "endOffset": 153}, {"referenceID": 16, "context": ", 2014), apples (Linker et al., 2012; Wang et al., 2013; Stajnko et al., 2009; Silwal et al., 2014; Ji et al., 2012; Kim et al., 2015; Hung et al., 2015), citrus (Li et al.", "startOffset": 16, "endOffset": 153}, {"referenceID": 19, "context": ", 2014), apples (Linker et al., 2012; Wang et al., 2013; Stajnko et al., 2009; Silwal et al., 2014; Ji et al., 2012; Kim et al., 2015; Hung et al., 2015), citrus (Li et al.", "startOffset": 16, "endOffset": 153}, {"referenceID": 15, "context": ", 2014), apples (Linker et al., 2012; Wang et al., 2013; Stajnko et al., 2009; Silwal et al., 2014; Ji et al., 2012; Kim et al., 2015; Hung et al., 2015), citrus (Li et al.", "startOffset": 16, "endOffset": 153}, {"referenceID": 22, "context": ", 2015), citrus (Li et al., 2011; Annamalai et al., 2004; Sengupta and Lee, 2014; Regunathan and Lee, 2005; Qiang et al., 2014), kiwifruit (Wijethunga et al.", "startOffset": 16, "endOffset": 127}, {"referenceID": 0, "context": ", 2015), citrus (Li et al., 2011; Annamalai et al., 2004; Sengupta and Lee, 2014; Regunathan and Lee, 2005; Qiang et al., 2014), kiwifruit (Wijethunga et al.", "startOffset": 16, "endOffset": 127}, {"referenceID": 38, "context": ", 2015), citrus (Li et al., 2011; Annamalai et al., 2004; Sengupta and Lee, 2014; Regunathan and Lee, 2005; Qiang et al., 2014), kiwifruit (Wijethunga et al.", "startOffset": 16, "endOffset": 127}, {"referenceID": 35, "context": ", 2015), citrus (Li et al., 2011; Annamalai et al., 2004; Sengupta and Lee, 2014; Regunathan and Lee, 2005; Qiang et al., 2014), kiwifruit (Wijethunga et al.", "startOffset": 16, "endOffset": 127}, {"referenceID": 33, "context": ", 2015), citrus (Li et al., 2011; Annamalai et al., 2004; Sengupta and Lee, 2014; Regunathan and Lee, 2005; Qiang et al., 2014), kiwifruit (Wijethunga et al.", "startOffset": 16, "endOffset": 127}, {"referenceID": 45, "context": ", 2014), kiwifruit (Wijethunga et al., 2009) and peaches (Kurtulmus et al.", "startOffset": 19, "endOffset": 44}, {"referenceID": 21, "context": ", 2009) and peaches (Kurtulmus et al., 2014).", "startOffset": 20, "endOffset": 44}, {"referenceID": 28, "context": "For example, Nuske et al. (2014) exploits radial symmetries in the specular reflection of the individual berries to extract key-points, which are then classified as berries or not-berries.", "startOffset": 13, "endOffset": 33}, {"referenceID": 28, "context": "For example, Nuske et al. (2014) exploits radial symmetries in the specular reflection of the individual berries to extract key-points, which are then classified as berries or not-berries. Using key-points allows distinct grape-berries to be identified, which is important to extract measurements that are invariant to the stage of the berry development. To detect citrus fruits, Sengupta and Lee (2014) first use Circular Hough Transforms (CHT) to extract key-points.", "startOffset": 13, "endOffset": 404}, {"referenceID": 24, "context": "Alternatively, Liu et al. (2015) and Song et al.", "startOffset": 15, "endOffset": 33}, {"referenceID": 24, "context": "Alternatively, Liu et al. (2015) and Song et al. (2014)", "startOffset": 15, "endOffset": 56}, {"referenceID": 31, "context": "Image segmentation on the other hand returns a rich likelihood map of the fruits, onto which a threshold can be applied to obtain a binary fruit mask detailing regions of the image containing fruit (Payne et al., 2014; Linker et al., 2012; Yamamoto et al., 2014; Sa et al., 2015).", "startOffset": 198, "endOffset": 279}, {"referenceID": 23, "context": "Image segmentation on the other hand returns a rich likelihood map of the fruits, onto which a threshold can be applied to obtain a binary fruit mask detailing regions of the image containing fruit (Payne et al., 2014; Linker et al., 2012; Yamamoto et al., 2014; Sa et al., 2015).", "startOffset": 198, "endOffset": 279}, {"referenceID": 46, "context": "Image segmentation on the other hand returns a rich likelihood map of the fruits, onto which a threshold can be applied to obtain a binary fruit mask detailing regions of the image containing fruit (Payne et al., 2014; Linker et al., 2012; Yamamoto et al., 2014; Sa et al., 2015).", "startOffset": 198, "endOffset": 279}, {"referenceID": 37, "context": "Image segmentation on the other hand returns a rich likelihood map of the fruits, onto which a threshold can be applied to obtain a binary fruit mask detailing regions of the image containing fruit (Payne et al., 2014; Linker et al., 2012; Yamamoto et al., 2014; Sa et al., 2015).", "startOffset": 198, "endOffset": 279}, {"referenceID": 23, "context": ", 2014; Linker et al., 2012; Yamamoto et al., 2014; Sa et al., 2015). Payne et al. (2014) designs a set of heuristic measures based on local colours and textures to classify individual pixels as mangoes or nonmangoes.", "startOffset": 8, "endOffset": 90}, {"referenceID": 23, "context": ", 2014; Linker et al., 2012; Yamamoto et al., 2014; Sa et al., 2015). Payne et al. (2014) designs a set of heuristic measures based on local colours and textures to classify individual pixels as mangoes or nonmangoes. Blob extraction was done on the resultant binary mask to identify individual mangoes. Linker et al. (2012) incorporates further post-processing for apple detection where individual blobs are expanded, segmented and combined to manage occluded fruits and fruit clusters.", "startOffset": 8, "endOffset": 325}, {"referenceID": 23, "context": ", 2014; Linker et al., 2012; Yamamoto et al., 2014; Sa et al., 2015). Payne et al. (2014) designs a set of heuristic measures based on local colours and textures to classify individual pixels as mangoes or nonmangoes. Blob extraction was done on the resultant binary mask to identify individual mangoes. Linker et al. (2012) incorporates further post-processing for apple detection where individual blobs are expanded, segmented and combined to manage occluded fruits and fruit clusters. Stajnko et al. (2009) instead uses shape analysis and template matching to extract circular apples from the segmented image.", "startOffset": 8, "endOffset": 510}, {"referenceID": 23, "context": ", 2014; Linker et al., 2012; Yamamoto et al., 2014; Sa et al., 2015). Payne et al. (2014) designs a set of heuristic measures based on local colours and textures to classify individual pixels as mangoes or nonmangoes. Blob extraction was done on the resultant binary mask to identify individual mangoes. Linker et al. (2012) incorporates further post-processing for apple detection where individual blobs are expanded, segmented and combined to manage occluded fruits and fruit clusters. Stajnko et al. (2009) instead uses shape analysis and template matching to extract circular apples from the segmented image. Yamamoto et al. (2014) implements a second classification component on tomato blobs extracted via image segmentation to remove any background detections.", "startOffset": 8, "endOffset": 636}, {"referenceID": 30, "context": "Typically, orchard image data is subject to highly variable illumination conditions, shadowing effects, fruits/crops of different shapes and sizes, captured over different seasons, etc (Payne and Walsh, 2014), which makes classification a challenging task.", "startOffset": 185, "endOffset": 208}, {"referenceID": 28, "context": "Typically, orchard image data is subject to highly variable illumination conditions, shadowing effects, fruits/crops of different shapes and sizes, captured over different seasons, etc (Payne and Walsh, 2014), which makes classification a challenging task. To simplify and minimise the variations in the data, one can enforce constraints on the environment or the data gathering operation. For example, pepper detection in Song et al. (2014) is conducted in a greenhouse with controlled illumination conditions.", "startOffset": 186, "endOffset": 442}, {"referenceID": 28, "context": "Equivalently, in Nuske et al. (2014); Payne et al.", "startOffset": 17, "endOffset": 37}, {"referenceID": 28, "context": "Equivalently, in Nuske et al. (2014); Payne et al. (2014); Font et al.", "startOffset": 17, "endOffset": 58}, {"referenceID": 11, "context": "(2014); Font et al. (2015) the data is captured at night using strobes, which significantly restricts the illumination variance.", "startOffset": 8, "endOffset": 27}, {"referenceID": 18, "context": "Although the methods stated above have produced promising performance over the respective fruits/datasets, they are distinct and ad hoc, seldom replicated, and often disconnected from progress in the general computer vision literature (Kapach et al., 2012).", "startOffset": 235, "endOffset": 256}, {"referenceID": 14, "context": "A general purpose adaptive feature learning algorithm is therefore desirable, as proposed in (Hung et al., 2013).", "startOffset": 93, "endOffset": 112}, {"referenceID": 14, "context": "Additionally, the same architecture has been used for different datasets such as almonds (Hung et al., 2013), apples (Hung et al.", "startOffset": 89, "endOffset": 108}, {"referenceID": 15, "context": ", 2013), apples (Hung et al., 2015; Bargoti and Underwood, 2015, 2016) and tree trunks (Bargoti et al.", "startOffset": 16, "endOffset": 70}, {"referenceID": 4, "context": ", 2015; Bargoti and Underwood, 2015, 2016) and tree trunks (Bargoti et al., 2015) without any changes to the image segmentation pipeline.", "startOffset": 59, "endOffset": 81}, {"referenceID": 20, "context": "With the advancements of parallel computing using GPUs, deeper neural network architectures, which host a significantly larger number of model parameters, are showing potential in capturing large variability in data (Krizhevsky et al., 2012).", "startOffset": 216, "endOffset": 241}, {"referenceID": 18, "context": "With the advancements of parallel computing using GPUs, deeper neural network architectures, which host a significantly larger number of model parameters, are showing potential in capturing large variability in data (Krizhevsky et al., 2012). For example, Ning et al. (2005); Ciresan et al.", "startOffset": 217, "endOffset": 275}, {"referenceID": 8, "context": "(2005); Ciresan et al. (2012); Pinheiro and Collobert (2013); Ganin and Lempitsky (2014) use multi-layered Convolutional Neural", "startOffset": 8, "endOffset": 30}, {"referenceID": 8, "context": "(2005); Ciresan et al. (2012); Pinheiro and Collobert (2013); Ganin and Lempitsky (2014) use multi-layered Convolutional Neural", "startOffset": 8, "endOffset": 61}, {"referenceID": 8, "context": "(2005); Ciresan et al. (2012); Pinheiro and Collobert (2013); Ganin and Lempitsky (2014) use multi-layered Convolutional Neural", "startOffset": 8, "endOffset": 89}, {"referenceID": 6, "context": "Brust et al. (2015) perform road image segmentation while incorporating the pixel position, to help the classifier learn that road pixels are pre-dominantly found near the bottom half of images.", "startOffset": 0, "endOffset": 20}, {"referenceID": 14, "context": "In this paper we evaluate the performance of a previously benchmarked multi-scale Multi-Layered Perceptron (MLP) architecture for image segmentation in agrovision (Hung et al., 2013) including an extension with metadata, which we have shown improves performance significantly in Bargoti and Underwood (2015, 2016).", "startOffset": 163, "endOffset": 182}, {"referenceID": 2, "context": ", 2013) including an extension with metadata, which we have shown improves performance significantly in Bargoti and Underwood (2015, 2016). We further extend this study with comparison against state-of-the-art CNNs, with and without the addition of metadata, showing improved pixel classification performance. We evaluate the utility of improved image segmentation towards fruit detection and yield estimation. With this we also shorten the gap between image processing techniques used in agrovision to the current work in computer vision literature, which is a limitation addressed in the literature survey conducted in Kapach et al. (2012).", "startOffset": 104, "endOffset": 642}, {"referenceID": 14, "context": "Given the success of the image segmentation framework in Hung et al. (2013) for different fruit types, the reference segmentation architecture in this paper is based on a multi-scale Multi-Layered Perceptron (which we denote as ms-MLP).", "startOffset": 57, "endOffset": 76}, {"referenceID": 10, "context": "As done in Farabet et al. (2013), input patches from each scale are treated independently.", "startOffset": 11, "endOffset": 33}, {"referenceID": 9, "context": "It has been shown in literature (and through our own experimentations) that unsupervised pre-training boosts classification performance for fully connected networks as they learn generalised features, which are useful for initialising the supervised training (Erhan et al., 2010).", "startOffset": 259, "endOffset": 279}, {"referenceID": 9, "context": "It has been shown in literature (and through our own experimentations) that unsupervised pre-training boosts classification performance for fully connected networks as they learn generalised features, which are useful for initialising the supervised training (Erhan et al., 2010). Each set of first layer filters W s 1 \u2200s \u2208 {1, . . . , S} are therefore pre-learnt using a held out dataset using a De-noising Auto encoder (DAE) with a sparsity penalty (see Hung et al. (2013) for details).", "startOffset": 260, "endOffset": 475}, {"referenceID": 9, "context": "It has been shown in literature (and through our own experimentations) that unsupervised pre-training boosts classification performance for fully connected networks as they learn generalised features, which are useful for initialising the supervised training (Erhan et al., 2010). Each set of first layer filters W s 1 \u2200s \u2208 {1, . . . , S} are therefore pre-learnt using a held out dataset using a De-noising Auto encoder (DAE) with a sparsity penalty (see Hung et al. (2013) for details). The learnt weights consist of a combination of edge and colour filters. The deeper layers are initialised by using the sparse initialisation scheme proposed in Martens (2010).", "startOffset": 260, "endOffset": 664}, {"referenceID": 20, "context": "This is followed by a non-linear activation function using the Rectified Linear Unit (ReLU) and a max-pooling sub-sampling layer as used in Krizhevsky et al. (2012). The output of each block is then:", "startOffset": 140, "endOffset": 165}, {"referenceID": 10, "context": "Some work with CNNs (Farabet et al., 2013; Ning et al., 2005) upscale the label output by the sub-sampling factor to get to the input image size.", "startOffset": 20, "endOffset": 61}, {"referenceID": 28, "context": "Some work with CNNs (Farabet et al., 2013; Ning et al., 2005) upscale the label output by the sub-sampling factor to get to the input image size.", "startOffset": 20, "endOffset": 61}, {"referenceID": 25, "context": "These are then interlaced such that the fine resolution predictions correspond to the pixels at the centres of their receptive fields (Long et al., 2015; Pinheiro and Collobert, 2013; Sermanet et al., 2013).", "startOffset": 134, "endOffset": 206}, {"referenceID": 32, "context": "These are then interlaced such that the fine resolution predictions correspond to the pixels at the centres of their receptive fields (Long et al., 2015; Pinheiro and Collobert, 2013; Sermanet et al., 2013).", "startOffset": 134, "endOffset": 206}, {"referenceID": 39, "context": "These are then interlaced such that the fine resolution predictions correspond to the pixels at the centres of their receptive fields (Long et al., 2015; Pinheiro and Collobert, 2013; Sermanet et al., 2013).", "startOffset": 134, "endOffset": 206}, {"referenceID": 5, "context": "A separate natural scene dataset (Brown and Susstrunk, 2011) was used to initialise the first layer filters with a DAE.", "startOffset": 33, "endOffset": 60}, {"referenceID": 8, "context": "For the CNN, lacking equivalent implementation in agrovision, we build a network around previous work done for patch wise classification over electron microscopy images (Ciresan et al., 2012) and urban scenes Brust et al.", "startOffset": 169, "endOffset": 191}, {"referenceID": 11, "context": "For training the ms-MLP, a similar architecture to the one presented in Hung et al. (2015) was used.", "startOffset": 72, "endOffset": 91}, {"referenceID": 5, "context": "A separate natural scene dataset (Brown and Susstrunk, 2011) was used to initialise the first layer filters with a DAE. ZCA whitening was used for pre-processing. For the CNN, lacking equivalent implementation in agrovision, we build a network around previous work done for patch wise classification over electron microscopy images (Ciresan et al., 2012) and urban scenes Brust et al. (2015). Pixel centred, single scale patches are extracted from the labelled dataset, while ensuring that they are large enough to identify/contain the fruits.", "startOffset": 34, "endOffset": 392}, {"referenceID": 13, "context": "The architectures were developed in Python using the open-source deep learning library, Pylearn2 (Goodfellow and Warde-Farley, 2013).", "startOffset": 97, "endOffset": 132}, {"referenceID": 34, "context": "In the latter, image height is discretised into a fixed number of bins with a single active unit denoting the pixel height (as done in Rao et al. (2014)).", "startOffset": 135, "endOffset": 153}, {"referenceID": 15, "context": "For baseline comparison, the original architecture from (Hung et al., 2015), a 2-layer MLP with 200 hidden units was trained with 200, 000 training instances, which we denote as ms-MLP-2.", "startOffset": 56, "endOffset": 75}, {"referenceID": 14, "context": "Table 1: Pixel-wise fruit classification results with the original ms-MLP architecture as presented in Hung et al. (2015). Combinations of metadata are added to the network, and the corresponding classification results listed as the absolute F1-score and as difference from the default method.", "startOffset": 103, "endOffset": 122}, {"referenceID": 14, "context": "We extend from Hung et al. (2015) by searching over different combinations of depth and width for the MLP architecture.", "startOffset": 15, "endOffset": 34}, {"referenceID": 14, "context": "We extend from Hung et al. (2015) by searching over different combinations of depth and width for the MLP architecture. A grid search was performed over networks with a depth of 2-5 layers and with varying width of 200 to 1000 units per layer. The optimal fruit classification performance was obtained with a 3-layer MLP with each hidden layer containing 200 hidden units. We denote this network as ms-MLP-3* and it is illustrated in Figure 3. As before, metadata were then added within this network at the input layer. During the optimisation phase we experimented with adding the metadata to deeper layers instead of the input layer and propagating it through independent hidden layers before merging with the image data as done in Rao et al. (2014). However, the best results were obtained when the raw metadata were added alongside the input multi-scale image data.", "startOffset": 15, "endOffset": 752}, {"referenceID": 14, "context": "The networks used are msMLP-2 from Hung et al. (2015) and ms-MLP-3*, optimised over width and depth.", "startOffset": 35, "endOffset": 54}, {"referenceID": 1, "context": "For this, we implement two different detection techniques, the Watershed Segmentation (WS) algorithm (Roerdink and Meijster, 2000) and the Circular Hough Transform (CHT) algorithm (Atherton and Kerbyson, 1999).", "startOffset": 180, "endOffset": 209}, {"referenceID": 2, "context": "This information can be extracted from the pixel-labelled data by doing fruit detection using the approaches mentioned above (as done in Bargoti and Underwood (2016)).", "startOffset": 137, "endOffset": 166}, {"referenceID": 14, "context": "Similar to Hung et al. (2015), yield estimation accuracy was evaluated using the r-squared correlation coefficient between the estimated row yield and the true counts as shown on the left in Figure 11.", "startOffset": 11, "endOffset": 30}, {"referenceID": 12, "context": "656 reported in Hung et al. (2015), evaluated over a single iteration using the ms-MLP-2 network and the CHT detection framework.", "startOffset": 16, "endOffset": 35}, {"referenceID": 2, "context": "As reported in Bargoti and Underwood (2016), using the ms-MLP-3* architecture with metadata, the linear fit with WS detection increased to 0.", "startOffset": 15, "endOffset": 44}, {"referenceID": 12, "context": "Operating under natural illumination conditions, the previously proposed ms-MLP network by Hung et al. (2015) resulted in a fruit segmentation F1-score of 0.", "startOffset": 91, "endOffset": 110}, {"referenceID": 2, "context": "751 with the inclusion of metadata, as previously shown in Bargoti and Underwood (2016). With the state-of-the-art CNN approach, we obtain the highest F1-score of 0.", "startOffset": 59, "endOffset": 88}, {"referenceID": 44, "context": "However, such large cluster occurrences were rare due to standard thinning operations employed in this orchard block, which are commonly used on orchards to optimise the quality of fruit (Wang et al., 2013).", "startOffset": 187, "endOffset": 206}, {"referenceID": 23, "context": "Although outside the scope of this paper, the detection results could be further improved with more specific shape based detection approaches like the ones used in (Linker et al., 2012), where fruits are defined as arc segments rather than whole circles.", "startOffset": 164, "endOffset": 185}, {"referenceID": 43, "context": "To accurately accumulate fruit counts between frames, multiple viewpoint fruit registration would be required as done in Wang et al. (2013) and Moonrinta et al.", "startOffset": 121, "endOffset": 140}, {"referenceID": 27, "context": "(2013) and Moonrinta et al. (2010), which use stereo camera configurations to localise each fruit in 3D space.", "startOffset": 11, "endOffset": 35}, {"referenceID": 31, "context": "A more controlled operation could involve night time imagery with strobes as done in Payne et al. (2014), however, as mentioned in Section 2, daytime operations are more desirable for integration with the current farming practices.", "startOffset": 85, "endOffset": 105}, {"referenceID": 12, "context": "Supervised image segmentation requires training examples, provided as pixel-wise labels in this paper, denoting fruit and non-fruit regions (the same as the ones used in Hung et al. (2015) and Bargoti and Underwood (2016)).", "startOffset": 170, "endOffset": 189}, {"referenceID": 2, "context": "(2015) and Bargoti and Underwood (2016)).", "startOffset": 11, "endOffset": 40}, {"referenceID": 2, "context": "Following our previous work in Bargoti and Underwood (2016), the metadata yielded a boost in performance with the ms-MLP network.", "startOffset": 31, "endOffset": 60}], "year": 2016, "abstractText": "Ground vehicles equipped with monocular vision systems are a valuable source of high resolution image data for precision agriculture applications in orchards. This paper presents an image processing framework for fruit detection and counting using orchard image data. A general purpose image segmentation approach is used, including two feature learning algorithms; multi-scale Multi-Layered Perceptrons (MLP) and Convolutional Neural Networks (CNN). These networks were extended by including contextual information about how the image data was captured (metadata), which correlates with some of the appearance variations and/or class distributions observed in the data. The pixel-wise fruit segmentation output is processed using the Watershed Segmentation (WS) and Circular Hough Transform (CHT) algorithms to detect and count individual fruits. Experiments were conducted in a commercial apple orchard near Melbourne, Australia. The results show an improvement in fruit segmentation performance with the inclusion of metadata on the previously benchmarked MLP network. We extend this work with CNNs, bringing agrovision closer to the state-of-the-art in computer vision, where although metadata had negligible influence, the best pixel-wise F1-score of 0.791 was achieved. The WS algorithm produced the best apple detection and counting results, with a detection F1-score of 0.858. As a final step, image fruit counts were accumulated over multiple rows at the orchard and compared against the post-harvest fruit counts that were obtained from a grading and counting machine. The count estimates using CNN and WS resulted in the best performance for this dataset, with a squared correlation coefficient of r = 0.826.", "creator": "LaTeX with hyperref package"}}}