{"id": "1610.08431", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-Oct-2016", "title": "Broad Context Language Modeling as Reading Comprehension", "abstract": "Progress in text understanding has been driven by the availability of large datasets that test particular capabilities, like recent datasets for assessing reading comprehension. We focus here on the LAMBADA dataset, a word prediction task requiring broader context than the immediate sentence. We view the LAMBADA task as a reading comprehension problem and apply off-the-shelf comprehension models based on neural networks. Though these models are constrained to choose a word from the context, they improve the state of the art on LAMBADA from 7.3% to 45.4%. We analyze 100 instances, finding that neural network readers perform well in cases that involve selecting a name from the context based on dialogue or discourse cues but struggle when coreference resolution or external knowledge is needed.", "histories": [["v1", "Wed, 26 Oct 2016 17:25:38 GMT  (18kb)", "https://arxiv.org/abs/1610.08431v1", null], ["v2", "Mon, 19 Dec 2016 18:54:44 GMT  (21kb)", "http://arxiv.org/abs/1610.08431v2", null], ["v3", "Thu, 16 Feb 2017 21:33:30 GMT  (22kb)", "http://arxiv.org/abs/1610.08431v3", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["zewei chu", "hai wang", "kevin gimpel", "david mcallester"], "accepted": false, "id": "1610.08431"}, "pdf": {"name": "1610.08431.pdf", "metadata": {"source": "CRF", "title": "Broad Context Language Modeling as Reading Comprehension", "authors": ["Zewei Chu", "Hai Wang", "Kevin Gimpel", "David McAllester"], "emails": ["zeweichu@uchicago.edu,", "haiwang@ttic.edu", "kgimpel@ttic.edu", "mcallester@ttic.edu"], "sections": [{"heading": null, "text": "ar Xiv: 161 0.08 431v 3 [cs.C L] February 16, 2017Progress in text comprehension has been driven by large datasets that test certain skills, such as the most recent datasets for reading comprehension (Hermann et al., 2015). We focus here on the LAMBADA dataset (Paperno et al., 2016), a word prediction task that requires a broader context than the immediate sentence. We consider LAMBADA to be a reading comprehension problem and apply understanding models based on neural networks. Although these models are limited to selecting a word from the context, they improve the state of the art in relation to LAMBADA from 7.3% to 49%. We analyze 100 cases and find that readers of neural networks perform well in cases where it is a matter of selecting a name from the context based on dialogue or discourse references, but struggle when reference resolution or external performance is required."}, {"heading": "1 Introduction", "text": "The LAMBADA dataset (Paperno et al., 2016) was designed by identifying word prediction tasks that require a broad context; each instance is pulled from the BookCorpus (Zhu et al., 2015) and consists of a passage of several sentences in which the task is to predict the last word of the last sentence; the instances are manually filtered to find cases that are advisable to humans given the larger context, but not if only the last sentence has been removed; the effort of this manual filtering has limited the dataset to only about 10,000 instances that are considered development and test data; the training data are taken as books in the corpus, not from which the evaluation passages were extracted; Paperno et al. (2016) provide baseline results with popular language models and neural network architectures; all achieve zero percent accuracy achieved by the randomly chosen passage."}, {"heading": "2 Methods", "text": "We now describe the models we use for the LAMBADA task (Section 2.1), as well as our procedure for building data sets (Section 2.2)."}, {"heading": "2.1 Neural Readers", "text": "The question we asked ourselves is whether we should focus the reader's attention on the question and passage in order to select an answer from the words in the passage. We use d for the context word sequence we use for the question (with a blank to fill), and V for the candidate list. We describe the neural readers in terms of three components: 1. Embedding and encoding: Every word in dand q is converted into a V-dimensional vocabulary function. We describe the neural readers in terms of three components: 1. Embedding and encoding: Every word in dand q is converted into a V-dimensional vector function."}, {"heading": "2.2 Training Data Construction", "text": "Each LAMBADA instance is divided into a context (average 4.6 sentences) and a target sentence, and the last word of the target sentence is the target word to be predicted. The LAMBADA dataset consists of the sentences Development (DEV) and Test (TEST); Paperno et al. (2016) also provides a control dataset (CONTROL), an unfiltered sample of instances from the BookCorpus. We construct a new training dataset from the BookCorpus. We limit it to instances that contain the target word in the context. This decision is obvious because we use neural readers who assume that the answer is contained in the passage. We also ensure that the context contains at least 50 words and 4 or 5 sentences, and we require that the target sentences have more than 10 words.Some neural readers need a candidate word list to choose from. We list all words in the context as candidate answers, with the exception of punctuation."}, {"heading": "3 Experiments", "text": "We use the Stanford Reader (Chen et al., 2016), our modified Stanford Reader (Eq. 1), the Attention Sum (AS) Reader (Kadlec et al., 2016), and the Gated Attention (GA) Reader (Dhingra et al., 2016). We also add the simple features of Wang et al. (2016) to the AS and GA Readers. Characteristics are associated with the word embeddings in the context, including: whether the word appears in the target sentence, the frequency of the word in the context, the position of the first occurrence of the word in the context as a percentage of the context length, and whether the text surrounding the word that encloses the blank in the target sentence matches. For the last feature, we consider the match with the left word, as the blank is not the last word in the target sentence."}, {"heading": "4 Results", "text": "Table 1 shows our results. We report accuracy on the totality of TEST and CONTROL (\"all\"), as well as separately on the CONTROL page, where the target word is in context (\"context\"). The first part of the table shows results from Paperno et al. (2016). We then show our baselines, which select a word from the context. Choosing the most common word yields a surprisingly high accuracy of 11.7%, which is better than all the results from Paperno et al. Our language models perform comparably, with the n-gram + cache model performing best. By forcing language models to select a word from the context, the accuracy on TEST is much higher than the analog models from Paperno et al., although the accuracy on CONTROL. We then show results with the neural readers, which show that they give much higher accuracy on TEST than any other methods. The GA reader with the simple additional functions provided by Wang et al (2016) delivers the highest accuracy."}, {"heading": "4.1 Manual Analysis", "text": "In fact, most people who are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to hurry, to move, to hurry, to hurry, to hurry, to move, to move, to move, to move, to move, to move, to move, to hurry, to hurry, to hurry, to hurry, to hurry, to hurry, to hurry, to hurry, to hurry, to hurry, to hurry, to hurry, to hurry, to hurry, to hurry, to hurry, to hurry, to hurry, to hurry, to hurry, to hurry, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to, to move, to move, to move, to move, to move, to move, to move, to move, to, to move, to move, to move, to, to move, to move, to move, to move, to move, to move, to move,"}, {"heading": "4.2 Discussion", "text": "At CONTROL, while our readers outperform our other baselines, they are outperformed by the language models of the baselines of Paperno et al. This suggests that while we have improved the state of the art at LAMBADAby more than 40% in absolute terms, we have not solved the general language modeling problem; there is not a single model that does well at both TEST and CONTROL. Our 36% estimate of human performance at CONTROL shows the difficulty of the general problem and shows a 14% gap between the best language model and human accuracy. A natural question is whether the use of neural readers is a good direction for this task, as they fail in the 17% of instances that do not have the target word in context. Moreover, this subset of LAMBADA may indeed represent the most interesting and challenging phenomena. Some neural readers, such as the Stanford reader, can easily be used to predict target words that do not appear in the context."}, {"heading": "5 Conclusion", "text": "We designed a new training set for LAMBADA and used it to train neural readers to improve the state of the art from 7.3% to 49%. We also provided results with several other strong baselines and included a manual evaluation to better understand the phenomena tested by the task. We hope that other researchers will look for models and training systems that perform well on both LAMBADA and CONTROL at the same time, with the aim of solving the general problem of speech modeling."}, {"heading": "Acknowledgments", "text": "We thank Denis Paperno for answering our questions about the LAMBADA dataset and we thank NVIDIA Corporation for donating GPUs used in this research."}], "references": [{"title": "Learning phrase representations using RNN encoder-decoder for statistical machine translation", "author": ["Kyunghyun Cho", "Bart van Merrienboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio."], "venue": "Proc. of", "citeRegEx": "Cho et al\\.,? 2014", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Gated-attention readers for text comprehension", "author": ["Bhuwan Dhingra", "Hanxiao Liu", "WilliamW. Cohen", "Ruslan Salakhutdinov."], "venue": "arXiv preprint.", "citeRegEx": "Dhingra et al\\.,? 2016", "shortCiteRegEx": "Dhingra et al\\.", "year": 2016}, {"title": "Teachingmachines to read and comprehend", "author": ["Karl Moritz Hermann", "Tom Koisk", "Edward Grefenstette", "Lasse Espeholt", "Will Kay", "Mustafa Suleyman", "Phil Blunsom."], "venue": "Proc. of NIPS.", "citeRegEx": "Hermann et al\\.,? 2015", "shortCiteRegEx": "Hermann et al\\.", "year": 2015}, {"title": "The Goldilocks principle: Reading children\u2019s books with explicit memory representations", "author": ["Felix Hill", "Antoine Bordes", "Sumit Chopra", "Jason Weston."], "venue": "Proc. of ICLR.", "citeRegEx": "Hill et al\\.,? 2016", "shortCiteRegEx": "Hill et al\\.", "year": 2016}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "Neural Computation, 9(8).", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Text understanding with the attention sum reader network", "author": ["Rudolf Kadlec", "Martin Schmid", "Ondrej Bajgar", "Jan Kleindienst."], "venue": "Proc. of ACL.", "citeRegEx": "Kadlec et al\\.,? 2016", "shortCiteRegEx": "Kadlec et al\\.", "year": 2016}, {"title": "Who did What: A large-scale person-centered cloze dataset", "author": ["Takeshi Onishi", "Hai Wang", "Mohit Bansal", "Kevin Gimpel", "David McAllester."], "venue": "Proc. of EMNLP.", "citeRegEx": "Onishi et al\\.,? 2016", "shortCiteRegEx": "Onishi et al\\.", "year": 2016}, {"title": "The LAMBADA dataset: Word prediction requiring a broad discourse context", "author": ["Denis Paperno", "Germn Kruszewski", "Angeliki Lazaridou", "Quan Ngoc Pham", "Raffaella Bernardi", "Sandro Pezzelle", "Marco Baroni", "Gemma Boleda", "Raquel Fernndez"], "venue": null, "citeRegEx": "Paperno et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Paperno et al\\.", "year": 2016}, {"title": "MCTest: A challenge dataset for the open-domainmachine comprehension of text", "author": ["Matthew Richardson", "Christopher JC Burges", "Erin Renshaw."], "venue": "Proc. of EMNLP.", "citeRegEx": "Richardson et al\\.,? 2013", "shortCiteRegEx": "Richardson et al\\.", "year": 2013}, {"title": "SRILM-an extensible language modeling toolkit", "author": ["Andreas Stolcke."], "venue": "Proc. of Interspeech.", "citeRegEx": "Stolcke.,? 2002", "shortCiteRegEx": "Stolcke.", "year": 2002}, {"title": "Emergent logical structure in vector representations of neural readers", "author": ["Hai Wang", "Takeshi Onishi", "Kevin Gimpel", "David McAllester."], "venue": "arXiv preprint.", "citeRegEx": "Wang et al\\.,? 2016", "shortCiteRegEx": "Wang et al\\.", "year": 2016}, {"title": "Aligning books and movies: Towards story-like visual explanations by watching movies and reading books", "author": ["Yukun Zhu", "Ryan Kiros", "Richard Zemel", "Ruslan Salakhutdinov", "Raquel Urtasun", "Antonio Torralba", "Sanja Fidler."], "venue": "Proc. of ICCV.", "citeRegEx": "Zhu et al\\.,? 2015", "shortCiteRegEx": "Zhu et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 2, "context": "Progress in text understanding has been driven by large datasets that test particular capabilities, like recent datasets for reading comprehension (Hermann et al., 2015).", "startOffset": 147, "endOffset": 169}, {"referenceID": 7, "context": "We focus here on the LAMBADA dataset (Paperno et al., 2016), a word prediction task requiring broader context than the immediate sentence.", "startOffset": 37, "endOffset": 59}, {"referenceID": 7, "context": "The LAMBADA dataset (Paperno et al., 2016) was designed by identifying word prediction tasks that require broad context.", "startOffset": 20, "endOffset": 42}, {"referenceID": 11, "context": "from the BookCorpus (Zhu et al., 2015) and consists of a passage of several sentences where the task is to predict the last word of the last sen-", "startOffset": 20, "endOffset": 38}, {"referenceID": 4, "context": "Paperno et al. (2016) provide baseline results with popular language models and neural network architectures; all achieve zero percent accuracy.", "startOffset": 0, "endOffset": 22}, {"referenceID": 2, "context": "Second, we treat the problem as a reading comprehension task similar to the CNN/Daily Mail datasets introduced by Hermann et al. (2015), the Children\u2019s Book Test (CBT) of Hill et al.", "startOffset": 114, "endOffset": 136}, {"referenceID": 2, "context": "Second, we treat the problem as a reading comprehension task similar to the CNN/Daily Mail datasets introduced by Hermann et al. (2015), the Children\u2019s Book Test (CBT) of Hill et al. (2016), and the Who-did-What dataset of Onishi et al.", "startOffset": 114, "endOffset": 190}, {"referenceID": 2, "context": "Second, we treat the problem as a reading comprehension task similar to the CNN/Daily Mail datasets introduced by Hermann et al. (2015), the Children\u2019s Book Test (CBT) of Hill et al. (2016), and the Who-did-What dataset of Onishi et al. (2016). We show that standard models for reading comprehension, trained on our automatically generated training set, improve the state of the art on the LAMBADA test set from 7.", "startOffset": 114, "endOffset": 244}, {"referenceID": 5, "context": ", 2016), Attention Sum (Kadlec et al., 2016), and GatedAttention (Dhingra et al.", "startOffset": 23, "endOffset": 44}, {"referenceID": 1, "context": ", 2016), and GatedAttention (Dhingra et al., 2016) Readers.", "startOffset": 28, "endOffset": 50}, {"referenceID": 0, "context": "The RNNs use either gated recurrent units (Cho et al., 2014) or long short-term memory (Hochreiter and Schmidhuber, 1997).", "startOffset": 42, "endOffset": 60}, {"referenceID": 4, "context": ", 2014) or long short-term memory (Hochreiter and Schmidhuber, 1997).", "startOffset": 34, "endOffset": 68}, {"referenceID": 1, "context": "The GatedAttention Reader uses a richer attention architecture (Dhingra et al., 2016); space does not permit a detailed description.", "startOffset": 63, "endOffset": 85}, {"referenceID": 7, "context": "The LAMBADA dataset consists of development (DEV) and test (TEST) sets; Paperno et al. (2016) also provide", "startOffset": 72, "endOffset": 94}, {"referenceID": 5, "context": "1), the Attention Sum (AS) Reader (Kadlec et al., 2016), and the Gated-Attention (GA) Reader (Dhingra et al.", "startOffset": 34, "endOffset": 55}, {"referenceID": 1, "context": ", 2016), and the Gated-Attention (GA) Reader (Dhingra et al., 2016).", "startOffset": 45, "endOffset": 67}, {"referenceID": 1, "context": ", 2016), and the Gated-Attention (GA) Reader (Dhingra et al., 2016). We also add the simple features fromWang et al. (2016) to the AS and GA Readers.", "startOffset": 46, "endOffset": 124}, {"referenceID": 7, "context": "We evaluate several other baseline systems inspired by those of Paperno et al. (2016), but we fo-", "startOffset": 64, "endOffset": 86}, {"referenceID": 8, "context": "txt We use the stopword list from Richardson et al. (2013). Method TEST CONTROL", "startOffset": 34, "endOffset": 59}, {"referenceID": 7, "context": "all all context Baselines (Paperno et al., 2016) Random in context 1.", "startOffset": 26, "endOffset": 48}, {"referenceID": 7, "context": "The first section is from Paperno et al. (2016). \u2217Estimated from 100 randomly-sampled DEV instances.", "startOffset": 26, "endOffset": 48}, {"referenceID": 9, "context": "We use the SRILM toolkit (Stolcke, 2002) to estimate a 4-gram model with modified KneserNey smoothing on the combination of TRAIN and VAL.", "startOffset": 25, "endOffset": 40}, {"referenceID": 7, "context": "The first part of the table shows results from Paperno et al. (2016). We then show our baselines that choose a word from the context.", "startOffset": 47, "endOffset": 69}, {"referenceID": 10, "context": "The GA Reader with the simple additional features (Wang et al., 2016) yields the highest accuracy, reaching 49.", "startOffset": 50, "endOffset": 69}, {"referenceID": 10, "context": "benefits of anonymization (Wang et al., 2016).", "startOffset": 26, "endOffset": 45}, {"referenceID": 7, "context": "Even though two annotators were able to correctly answer all LAMBADA instances during dataset construction (Paperno et al., 2016), our results give an estimate of how often a third would agree.", "startOffset": 107, "endOffset": 129}, {"referenceID": 6, "context": "This suggests that our simple method of dataset creation could be used to create additional training or evaluation sets for challenging language modeling problems like LAMBADA, perhaps by combining it with baseline suppression (Onishi et al., 2016).", "startOffset": 227, "endOffset": 248}], "year": 2017, "abstractText": "Progress in text understanding has been driven by large datasets that test particular capabilities, like recent datasets for reading comprehension (Hermann et al., 2015). We focus here on the LAMBADA dataset (Paperno et al., 2016), a word prediction task requiring broader context than the immediate sentence. We view LAMBADA as a reading comprehension problem and apply comprehension models based on neural networks. Though these models are constrained to choose a word from the context, they improve the state of the art on LAMBADA from 7.3% to 49%. We analyze 100 instances, finding that neural network readers perform well in cases that involve selecting a name from the context based on dialogue or discourse cues but struggle when coreference resolution or external knowledge is needed.", "creator": "dvips(k) 5.996 Copyright 2016 Radical Eye Software"}}}