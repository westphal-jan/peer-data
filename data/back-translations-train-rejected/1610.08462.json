{"id": "1610.08462", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-Oct-2016", "title": "Distraction-Based Neural Networks for Document Summarization", "abstract": "Distributed representation learned with neural networks has recently shown to be effective in modeling natural languages at fine granularities such as words, phrases, and even sentences. Whether and how such an approach can be extended to help model larger spans of text, e.g., documents, is intriguing, and further investigation would still be desirable. This paper aims to enhance neural network models for such a purpose. A typical problem of document-level modeling is automatic summarization, which aims to model documents in order to generate summaries. In this paper, we propose neural models to train computers not just to pay attention to specific regions and content of input documents with attention models, but also distract them to traverse between different content of a document so as to better grasp the overall meaning for summarization. Without engineering any features, we train the models on two large datasets. The models achieve the state-of-the-art performance, and they significantly benefit from the distraction modeling, particularly when input documents are long.", "histories": [["v1", "Wed, 26 Oct 2016 18:57:00 GMT  (89kb,D)", "http://arxiv.org/abs/1610.08462v1", "Published in IJCAI-2016: the 25th International Joint Conference on Artificial Intelligence"]], "COMMENTS": "Published in IJCAI-2016: the 25th International Joint Conference on Artificial Intelligence", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["qian chen", "xiaodan zhu", "zhenhua ling", "si wei", "hui jiang"], "accepted": false, "id": "1610.08462"}, "pdf": {"name": "1610.08462.pdf", "metadata": {"source": "CRF", "title": "Distraction-Based Neural Networks for Document Summarization", "authors": ["Qian Chen", "Xiaodan Zhu", "Zhenhua Ling", "Si Wei", "Hui Jiang"], "emails": ["cq1231@mail.ustc.edu.cn,", "zhu2048@gmail.com,", "zhling@ustc.edu.cn,", "siwei@iflytek.com,", "hj@cse.yorku.ca"], "sections": [{"heading": "1 Introduction", "text": "In fact, it is such that most of them will be able to move into a different world, in which they are able to move, in which they are able to move, in which they move, in which they move, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live."}, {"heading": "2 Related work", "text": "Distributed representation Distributed representation has proven effective in modeling fine text granularities as discussed above. Many recent work has also attempted to model longer stretches of text using neural networks [Li et al., 2015; Hu et al., 2015; Lin et al., 2015; Wang et al., 2015; Hermann et al., 2015], including research that includes document-level information for speech modeling [Wang and Cho, 2015; Lin et al., 2015] and answers the questions [Hermann et al., 2015] by grasping input documents with attention-based models. More relevant to us is the work of [Li et al., 2015] taught distributed representation for short documents with an average length of about a hundred word marks, although the goal is not a summary. Document summary is usually longer than this, and document summary may be more necessary if documents are long. In this paper we propose neural models for summaries."}, {"heading": "3 Our approach", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Overview", "text": "We base our model on the general encoder decoder framework [Sutskever et al., 2011; Sutskever et al., 2014; Cho et al., 2014], which has recently proven effective in various tasks. This is a general sequence-to-sequence modeling framework in which the coding part can be devoted to modeling the input documents and the decoder to generate output. We believe that the control layer that helps navigate the input documents to optimize the generation goals would be important, and we will focus on the control level in this paper and improve its meaningfulness. specifically for the summary, as opposed to many recent works that focus more attention on grasping the local context or match (e.g. machine translation and typesetting compression), we force our models to traverse between different contents of a document in order to avoid a general concentration on the same region or better to grasp the content for a composition."}, {"heading": "3.2 GRU-based encoding and decoding", "text": "In fact, most people are able to determine for themselves how they have behaved."}, {"heading": "3.3 The control layers", "text": "A key problem is how these two components are associated with each other. In sentence-level modeling, such as machine translation and speech recognition, the attention model is often used to capture the local context and correspondence between input and output texts. In translation documents, it is shown that it is very useful to adapt the words of the target language (the language being translated) to the corresponding source words and their contexts. Attention can be seen as a kind of cognitive control. In document modeling, we take a general position on this control layer and represent distraction modeling to track the contents of a long document, and we will show that it significantly improves summarized performance. In general, the control layer allows a complicated check on input. In this section, we describe the controls that take both attention and distraction into account."}, {"heading": "4 Experiment set-up", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Data", "text": "We are experimenting with our summary models on two publicly available corporations with different document lengths and in different languages: a CNN news collection [Hermann et al., 2015] and a Chinese corpus recently made available in [Hu et al., 2015], both of which are large sets of data suitable for the formation of neural models, and which, as discussed above, use a large number of parameters to match the potentially complicated summary process in which input documents are presented, summaries are created, and interacted between them. CNN data [Hermann et al., 2015] includes a man-made summary for each news article. The data set collected in was made available at GitHub.1 The data was obtained using the tools of Stanford CoreNLP [Manning et al., 2014] for tokenization and judgment recognition in real life; all capital information is retained."}, {"heading": "4.2 Training details", "text": "We used Mini-Batch stochastic gradient descent (SGD) to optimize log probability, and Adadelta [Lines, 2012] to automatically adjust the learning rate of parameters (= 10 \u2212 6 and \u03c1 = 0.95).For the CNN dataset, training was performed with mixed mini-batches of size 64 by length sorting. We limited our vocabulary to the top 25,000 most common words. Other words were replaced by the token < UNK >, as discussed earlier in the thesis. Based on the validation data, we set the embedding dimension to 120, the vector length in hidden layers to 500 for uni-GRU and 600 for bi-GRU. Between each sentence, a token for end of sentence was inserted, and at the end of the document, a token was added that set the end of the document to 120, the bar size of the decoder to 5. For the LCSTS data, a mini-batch-256 was found."}, {"heading": "5 Experimental results", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 Results on the CNN dataset", "text": "Overall Performance Our results on the CNN dataset are presented in Table 2. We used rouge values [Lin, 2004] to measure performance. Since the summary lengths are not preset to be equal, we report on F1 Rouge. The upper part of the table contains the basic results of a series of typical summary algorithms that we have listed in the table as Luhn [Luhn, 1958], Edmundson [Edmundson, 1969], LSA [Steinberger and Jezek, 2004], Lex rank [Erkan and Radev, 2004], Text rank [Mihalcea and Tarau, 2004], Sumbasic [Vanderwende et al., 2007], and KL sum [Haghighi and Vanderwende, 2009]. These basic results are presented in the open source tool SUMY3.The results in the lower half of the table show that the bi-GRU encoder performs better than the uniGRU encoder."}, {"heading": "5.2 Results on the LCSTS dataset", "text": "We are experimenting with the proposed model for the public LCSTS corpus. The baseline is the best result available in [Hu et al., 2015] 4. Our modified uni-GRU achieves a slight improvement over the reported results. The Bi-GRU-based model achieves better performance and confirms the usefulness of bidirectional models for summary as well as that our implementation is state-of-the-art and serves as a very strong baseline in the CNN dataset discussed above. Note that since the input length of LCSTS is much shorter than the CNN documents, each containing about 100 words and about 6-8 sentences, we show that distraction does not improve performance, but in contrast, when documents are longer, their benefits are significant and achieve the greatest improvement as previously discussed. This suggests that the effectiveness of distraction modeling in the summary of more4We thank the authors of Hu et al, 2015 for sharing the results of their latest models."}, {"heading": "6 Conclusions and future work", "text": "We propose to train neural document summary models not only to link specific regions of input documents to attention models, but also to redirect models to other contents in order to better understand the general meaning of input documents. Without technical features, we train models on two large sets of data. Models reach the state of the art and they benefit significantly from distraction modeling, especially if the input documents are long. We also examine several newer technologies for summary and show that they also contribute to improving summary performance. Even when applied to models that have already used these technologies, the distraction models can significantly improve performance. From a more general point of view, enriching the meaningfulness of the control layers that link the input coding layer to the output coding layer could be important to address the shortcomings of the current models. We plan to do more work in this direction."}, {"heading": "Acknowledgments", "text": "The first and third authors of this paper were partially supported by the Science and Technology Development of Anhui Province, China (Grants No. 2014z02006), the Fundamental Research Funds for the Central Universities (Grants No. WK2350000001) and the Strategic Priority Research Program of the Chinese Academy of Sciences (Grants No. XDB02070006)."}], "references": [{"title": "CoRR", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio. Neural machine translation by jointly learning to align", "translate"], "venue": "abs/1409.0473,", "citeRegEx": "Bahdanau et al.. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Theano: a cpu and gpu math expression compiler", "author": ["J Bergstra", "O Breuleux", "F Bastien", "P Lamblin", "R Pascanu", "G Desjardins", "J Turian", "D Warde-Farley", "Y Bengio"], "venue": "SciPy, volume 4, page 3. Austin, TX", "citeRegEx": "Bergstra et al.. 2010", "shortCiteRegEx": null, "year": 2010}, {"title": "Revisiting word embedding for contrasting meaning", "author": ["Zhigang Chen", "Wei Lin", "Qian Chen", "Si Wei", "Hui Jiang", "Xiaodan Zhu"], "venue": "Proceedings of ACL,", "citeRegEx": "Chen et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Enhancing and combining sequential and tree lstm for natural language inference", "author": ["Qian Chen", "Xiaodan Zhu", "Zhenhua Ling", "Si Wei", "Hui Jiang"], "venue": "arXiv:1609.06038v1,", "citeRegEx": "Chen et al.. 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "author": ["K. Cho", "B. Van Merri\u00ebnboer", "C. Gulcehre", "D. Bahdanau", "F. Bougares", "H. Schwenk", "Y. Bengio"], "venue": "EMNLP", "citeRegEx": "Cho et al.. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Natural language processing (almost) from scratch", "author": ["R. Collobert", "J. Weston", "L. Bottou", "M. Karlen", "K. Kavukcuoglu", "P. Kuksa"], "venue": "JMLR, 12:2493\u20132537", "citeRegEx": "Collobert et al.. 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "A survey on automatic text summarization", "author": ["Das", "Martins", "2007] Dipanjan Das", "Andre Martins"], "venue": null, "citeRegEx": "Das et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Das et al\\.", "year": 2007}, {"title": "JACM", "author": ["Harold P Edmundson. New methods in automatic extracting"], "venue": "16(2):264\u2013285,", "citeRegEx": "Edmundson. 1969", "shortCiteRegEx": null, "year": 1969}, {"title": "Lexrank: Graph-based lexical centrality as salience in text summarization", "author": ["G\u00fcnes Erkan", "Dragomir R Radev"], "venue": "JAIR, pages 457\u2013479,", "citeRegEx": "Erkan and Radev. 2004", "shortCiteRegEx": null, "year": 2004}, {"title": "Exploring content models for multi-document summarization", "author": ["Aria Haghighi", "Lucy Vanderwende"], "venue": "NAACL,", "citeRegEx": "Haghighi and Vanderwende. 2009", "shortCiteRegEx": null, "year": 2009}, {"title": "Teaching machines to read and comprehend", "author": ["K. Hermann", "T. Kocisky", "E. Grefenstette", "L. Espeholt", "W. Kay", "M. Suleyman", "P. Blunsom"], "venue": "NIPS", "citeRegEx": "Hermann et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Neural computation", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber. Long short-term memory"], "venue": "9(8):1735\u20131780,", "citeRegEx": "Hochreiter and Schmidhuber. 1997", "shortCiteRegEx": null, "year": 1997}, {"title": "Lcsts: A large scale chinese short text summarization dataset", "author": ["Baotian Hu", "Qingcai Chen", "Fangze Zhu"], "venue": "EMNLP,", "citeRegEx": "Hu et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep recursive neural networks for compositionality in language", "author": ["Irsoy", "Cardie", "Ozan Irsoy", "Claire Cardie"], "venue": "In NIPS,", "citeRegEx": "Irsoy et al\\.,? \\Q2096\\E", "shortCiteRegEx": "Irsoy et al\\.", "year": 2096}, {"title": "On using very large target vocabulary for neural machine translation", "author": ["S\u00e9bastien Jean", "KyungHyun Cho", "Roland Memisevic", "Yoshua Bengio"], "venue": "ACL,", "citeRegEx": "Jean et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "ACL", "author": ["Nal Kalchbrenner", "Edward Grefenstette", "Phil Blunsom. A convolutional neural network for modelling sentences"], "venue": "June", "citeRegEx": "Kalchbrenner et al.. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "A hierarchical neural autoencoder for paragraphs and documents", "author": ["Jiwei Li", "Minh-Thang Luong", "Dan Jurafsky"], "venue": "ACL,", "citeRegEx": "Li et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Hierarchical recurrent neural network for document modeling", "author": ["R. Lin", "S. Liu", "M. Yang", "M. Li", "M. Zhou", "S. Li"], "venue": "EMNLP", "citeRegEx": "Lin et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "CoRR", "author": ["Konstantin Lopyrev. Generating news headlines with recurrent neural networks"], "venue": "abs/1512.01712,", "citeRegEx": "Lopyrev. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "IBM Journal of research and development", "author": ["Hans Peter Luhn. The automatic creation of literature abstracts"], "venue": "2(2):159\u2013 165,", "citeRegEx": "Luhn. 1958", "shortCiteRegEx": null, "year": 1958}, {"title": "Effective approaches to attention-based neural machine translation", "author": ["Thang Luong", "Hieu Pham", "Christopher D. Manning"], "venue": "EMNLP,", "citeRegEx": "Luong et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Benjamins Pub", "author": ["J Inderjeet Mani. Automatic Summarization."], "venue": "Co., Amsterdam,", "citeRegEx": "Mani. 2001", "shortCiteRegEx": null, "year": 2001}, {"title": "The stanford corenlp natural language processing toolkit", "author": ["C Manning", "M Surdeanu", "J Bauer", "J Finkel", "S Bethard", "D McClosky"], "venue": "ACL", "citeRegEx": "Manning et al.. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Textrank: Bringing order into text", "author": ["Rada Mihalcea", "Paul Tarau"], "venue": "EMNLP,", "citeRegEx": "Mihalcea and Tarau. 2004", "shortCiteRegEx": null, "year": 2004}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["T. Mikolov", "I. Sutskever", "K. Chen", "G. Corrado", "J. Dean"], "venue": "NIPS", "citeRegEx": "Mikolov et al.. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "A survey of text summarization techniques", "author": ["Ani Nenkova", "Kathleen McKeown"], "venue": "Springer,", "citeRegEx": "Nenkova and McKeown. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "A neural attention model for abstractive sentence summarization", "author": ["Alexander M. Rush", "Sumit Chopra", "Jason Weston"], "venue": "EMNLP,", "citeRegEx": "Rush et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Semantic compositionality through recursive matrixvector spaces", "author": ["R. Socher", "B. Huval", "C. Manning", "A. Ng"], "venue": "EMNLP", "citeRegEx": "Socher et al.. 2012", "shortCiteRegEx": null, "year": 2012}, {"title": "Using latent semantic analysis in text summarization and summary evaluation", "author": ["J. Steinberger", "K. Jezek"], "venue": "ISIM, pages 93\u2013100", "citeRegEx": "Steinberger and Jezek. 2004", "shortCiteRegEx": null, "year": 2004}, {"title": "In ICML", "author": ["Ilya Sutskever", "James Martens", "Geoffrey Hinton. Generating text with recurrent neural networks"], "venue": "pages 1017\u20131024,", "citeRegEx": "Sutskever et al.. 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "In NIPS", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc VV Le. Sequence to sequence learning with neural networks"], "venue": "pages 3104\u20133112,", "citeRegEx": "Sutskever et al.. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Improved Semantic Representations From TreeStructured Long Short-Term Memory Networks", "author": ["Kai Sheng Tai", "Richard Socher", "Christopher Manning"], "venue": "ACL,", "citeRegEx": "Tai et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "CoRR", "author": ["Zhaopeng Tu", "Zhengdong Lu", "Yang Liu", "Xiaohua Liu", "Hang Li. Coverage-based neural machine translation"], "venue": "abs/1601.04811,", "citeRegEx": "Tu et al.. 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Beyond sumbasic: Task-focused summarization with sentence simplification and lexical expansion", "author": ["L. Vanderwende", "H. Suzuki", "C. Brockett", "A. Nenkova"], "venue": "IP&M, 43(6):1606\u20131618", "citeRegEx": "Vanderwende et al.. 2007", "shortCiteRegEx": null, "year": 2007}, {"title": "CoRR", "author": ["Tian Wang", "Kyunghyun Cho. Largercontext language modelling"], "venue": "abs/1511.03729,", "citeRegEx": "Wang and Cho. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "In ACL 2014 Student Research Workshop", "author": ["W. Yin", "H. Sch\u00fctze. An exploration of embeddings for generalized phrases"], "venue": "pages 41\u201347, June", "citeRegEx": "Yin and Sch\u00fctze. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Adadelta: An adaptive learning rate method", "author": ["Matthew D. Zeiler"], "venue": "CoRR, abs/1212.5701,", "citeRegEx": "Zeiler. 2012", "shortCiteRegEx": null, "year": 2012}, {"title": "acoustic and spoken-language features on spontaneous conversation summarization", "author": ["Xiaodan Zhu", "Gerald Penn. Comparing the roles of textual"], "venue": "NAACL,", "citeRegEx": "Zhu and Penn. 2006", "shortCiteRegEx": null, "year": 2006}, {"title": "Summarizing multiple spoken documents: Finding evidence from untranscribed audio", "author": ["Xiaodan Zhu", "Gerald Penn", "Frank Rudzicz"], "venue": "ACL,", "citeRegEx": "Zhu et al.. 2010", "shortCiteRegEx": null, "year": 2010}, {"title": "An empirical study on the effect of negation words on sentiment", "author": ["Xiaodan Zhu", "Hongyu Guo", "Saif Mohammad", "Svetlana Kiritchenko"], "venue": "Proceedings of the 52th Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "Zhu et al.. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "In Proceedings of Joint Conference on Lexical and Computational Semantics", "author": ["Xiaodan Zhu", "Hongyu Guo", "Parinaz Sobhani. Neural networks for integrating compositional", "noncompositional sentiment in sentiment composition"], "venue": "June", "citeRegEx": "Zhu et al.. 2015a", "shortCiteRegEx": null, "year": 2015}, {"title": "Long Short-Term Memory over Recursive Structures", "author": ["Xiaodan Zhu", "Parinaz Sobhani", "Hongyu Guo"], "venue": "Proceedings of International Conference on Machine Learning,", "citeRegEx": "Zhu et al.. 2015b", "shortCiteRegEx": null, "year": 2015}, {"title": "Dag-structured long short-term memory for semantic compositionality", "author": ["Xiaodan Zhu", "Parinaz Sobhani", "Hongyu Guo"], "venue": "Proceedings of the Meeting of the North American Chapter of the Association for Computational Linguistics (NAACL),", "citeRegEx": "Zhu et al.. 2016", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 5, "context": "Distributed representation learned with neural networks has recently shown to be effective in modeling fine granularities of text, including words [Collobert et al., 2011; Mikolov et al., 2013; Chen et al., 2015], phrases [Yin and Sch\u00fctze, 2014; Zhu et al.", "startOffset": 147, "endOffset": 212}, {"referenceID": 24, "context": "Distributed representation learned with neural networks has recently shown to be effective in modeling fine granularities of text, including words [Collobert et al., 2011; Mikolov et al., 2013; Chen et al., 2015], phrases [Yin and Sch\u00fctze, 2014; Zhu et al.", "startOffset": 147, "endOffset": 212}, {"referenceID": 2, "context": "Distributed representation learned with neural networks has recently shown to be effective in modeling fine granularities of text, including words [Collobert et al., 2011; Mikolov et al., 2013; Chen et al., 2015], phrases [Yin and Sch\u00fctze, 2014; Zhu et al.", "startOffset": 147, "endOffset": 212}, {"referenceID": 35, "context": ", 2015], phrases [Yin and Sch\u00fctze, 2014; Zhu et al., 2014], and arguably sentences [Socher et al.", "startOffset": 17, "endOffset": 58}, {"referenceID": 39, "context": ", 2015], phrases [Yin and Sch\u00fctze, 2014; Zhu et al., 2014], and arguably sentences [Socher et al.", "startOffset": 17, "endOffset": 58}, {"referenceID": 27, "context": ", 2014], and arguably sentences [Socher et al., 2012; Irsoy and Cardie, ; Kalchbrenner et al., 2014; Tai et al., 2015; Zhu et al., 2015b; Zhu et al., 2015a; Chen et al., 2016; Zhu et al., 2016].", "startOffset": 32, "endOffset": 193}, {"referenceID": 15, "context": ", 2014], and arguably sentences [Socher et al., 2012; Irsoy and Cardie, ; Kalchbrenner et al., 2014; Tai et al., 2015; Zhu et al., 2015b; Zhu et al., 2015a; Chen et al., 2016; Zhu et al., 2016].", "startOffset": 32, "endOffset": 193}, {"referenceID": 31, "context": ", 2014], and arguably sentences [Socher et al., 2012; Irsoy and Cardie, ; Kalchbrenner et al., 2014; Tai et al., 2015; Zhu et al., 2015b; Zhu et al., 2015a; Chen et al., 2016; Zhu et al., 2016].", "startOffset": 32, "endOffset": 193}, {"referenceID": 41, "context": ", 2014], and arguably sentences [Socher et al., 2012; Irsoy and Cardie, ; Kalchbrenner et al., 2014; Tai et al., 2015; Zhu et al., 2015b; Zhu et al., 2015a; Chen et al., 2016; Zhu et al., 2016].", "startOffset": 32, "endOffset": 193}, {"referenceID": 40, "context": ", 2014], and arguably sentences [Socher et al., 2012; Irsoy and Cardie, ; Kalchbrenner et al., 2014; Tai et al., 2015; Zhu et al., 2015b; Zhu et al., 2015a; Chen et al., 2016; Zhu et al., 2016].", "startOffset": 32, "endOffset": 193}, {"referenceID": 3, "context": ", 2014], and arguably sentences [Socher et al., 2012; Irsoy and Cardie, ; Kalchbrenner et al., 2014; Tai et al., 2015; Zhu et al., 2015b; Zhu et al., 2015a; Chen et al., 2016; Zhu et al., 2016].", "startOffset": 32, "endOffset": 193}, {"referenceID": 42, "context": ", 2014], and arguably sentences [Socher et al., 2012; Irsoy and Cardie, ; Kalchbrenner et al., 2014; Tai et al., 2015; Zhu et al., 2015b; Zhu et al., 2015a; Chen et al., 2016; Zhu et al., 2016].", "startOffset": 32, "endOffset": 193}, {"referenceID": 21, "context": "A typical problem of documentlevel modeling is automatic summarization [Mani, 2001; Das and Martins, 2007; Nenkova and McKeown, 2013], in which computers generate summaries for documents, based on their shallow or deep understanding of the documents.", "startOffset": 71, "endOffset": 133}, {"referenceID": 25, "context": "A typical problem of documentlevel modeling is automatic summarization [Mani, 2001; Das and Martins, 2007; Nenkova and McKeown, 2013], in which computers generate summaries for documents, based on their shallow or deep understanding of the documents.", "startOffset": 71, "endOffset": 133}, {"referenceID": 12, "context": "This work explores the former direction and utilizes relatively large datasets [Hu et al., 2015; Hermann et al., 2015] to train neural summarization models.", "startOffset": 79, "endOffset": 118}, {"referenceID": 10, "context": "This work explores the former direction and utilizes relatively large datasets [Hu et al., 2015; Hermann et al., 2015] to train neural summarization models.", "startOffset": 79, "endOffset": 118}, {"referenceID": 0, "context": "Additional modeling such as soft or hard attention has been applied to retrospect subsequences or even words in input text to remedy the limits, which has shown to improve performances of different tasks such as those discussed in [Bahdanau et al., 2014; Luong et al., 2015; Rush et al., 2015] among others.", "startOffset": 231, "endOffset": 293}, {"referenceID": 20, "context": "Additional modeling such as soft or hard attention has been applied to retrospect subsequences or even words in input text to remedy the limits, which has shown to improve performances of different tasks such as those discussed in [Bahdanau et al., 2014; Luong et al., 2015; Rush et al., 2015] among others.", "startOffset": 231, "endOffset": 293}, {"referenceID": 26, "context": "Additional modeling such as soft or hard attention has been applied to retrospect subsequences or even words in input text to remedy the limits, which has shown to improve performances of different tasks such as those discussed in [Bahdanau et al., 2014; Luong et al., 2015; Rush et al., 2015] among others.", "startOffset": 231, "endOffset": 293}, {"referenceID": 16, "context": "Much recent work has also attempted to model longer spans of text with neural networks [Li et al., 2015; Hu et al., 2015; Lin et al., 2015; Wang and Cho, 2015; Hermann et al., 2015].", "startOffset": 87, "endOffset": 181}, {"referenceID": 12, "context": "Much recent work has also attempted to model longer spans of text with neural networks [Li et al., 2015; Hu et al., 2015; Lin et al., 2015; Wang and Cho, 2015; Hermann et al., 2015].", "startOffset": 87, "endOffset": 181}, {"referenceID": 17, "context": "Much recent work has also attempted to model longer spans of text with neural networks [Li et al., 2015; Hu et al., 2015; Lin et al., 2015; Wang and Cho, 2015; Hermann et al., 2015].", "startOffset": 87, "endOffset": 181}, {"referenceID": 34, "context": "Much recent work has also attempted to model longer spans of text with neural networks [Li et al., 2015; Hu et al., 2015; Lin et al., 2015; Wang and Cho, 2015; Hermann et al., 2015].", "startOffset": 87, "endOffset": 181}, {"referenceID": 10, "context": "Much recent work has also attempted to model longer spans of text with neural networks [Li et al., 2015; Hu et al., 2015; Lin et al., 2015; Wang and Cho, 2015; Hermann et al., 2015].", "startOffset": 87, "endOffset": 181}, {"referenceID": 34, "context": "This includes research that incorporates document-level information for language modeling [Wang and Cho, 2015; Lin et al., 2015] and that answers questions [Hermann et al.", "startOffset": 90, "endOffset": 128}, {"referenceID": 17, "context": "This includes research that incorporates document-level information for language modeling [Wang and Cho, 2015; Lin et al., 2015] and that answers questions [Hermann et al.", "startOffset": 90, "endOffset": 128}, {"referenceID": 10, "context": ", 2015] and that answers questions [Hermann et al., 2015] by comprehending input documents with attention-based models.", "startOffset": 35, "endOffset": 57}, {"referenceID": 16, "context": "More relevant to ours, the work of [Li et al., 2015] learned distributed representation for short documents with the averaged length of about a hundred word tokens, although the objective is not summarization.", "startOffset": 35, "endOffset": 52}, {"referenceID": 21, "context": "Neural summarization models Automatic summarization has been intensively studied for both text [Mani, 2001; Das and Martins, 2007; Nenkova and McKeown, 2013] and speech [Zhu and Penn, 2006; Zhu et al.", "startOffset": 95, "endOffset": 157}, {"referenceID": 25, "context": "Neural summarization models Automatic summarization has been intensively studied for both text [Mani, 2001; Das and Martins, 2007; Nenkova and McKeown, 2013] and speech [Zhu and Penn, 2006; Zhu et al.", "startOffset": 95, "endOffset": 157}, {"referenceID": 37, "context": "Neural summarization models Automatic summarization has been intensively studied for both text [Mani, 2001; Das and Martins, 2007; Nenkova and McKeown, 2013] and speech [Zhu and Penn, 2006; Zhu et al., 2010].", "startOffset": 169, "endOffset": 207}, {"referenceID": 38, "context": "Neural summarization models Automatic summarization has been intensively studied for both text [Mani, 2001; Das and Martins, 2007; Nenkova and McKeown, 2013] and speech [Zhu and Penn, 2006; Zhu et al., 2010].", "startOffset": 169, "endOffset": 207}, {"referenceID": 26, "context": "Recent neural summarization models include the recent efforts of [Rush et al., 2015; Lopyrev, 2015; Hu et al., 2015].", "startOffset": 65, "endOffset": 116}, {"referenceID": 18, "context": "Recent neural summarization models include the recent efforts of [Rush et al., 2015; Lopyrev, 2015; Hu et al., 2015].", "startOffset": 65, "endOffset": 116}, {"referenceID": 12, "context": "Recent neural summarization models include the recent efforts of [Rush et al., 2015; Lopyrev, 2015; Hu et al., 2015].", "startOffset": 65, "endOffset": 116}, {"referenceID": 26, "context": "The research performed in [Rush et al., 2015] focuses on neural models for sentence compression and rewriting, but not full document summarization.", "startOffset": 26, "endOffset": 45}, {"referenceID": 18, "context": "The work of [Lopyrev, 2015] leverages neural networks to generate news headline, where input documents are limited to 50 word tokens, and the work of [Hu et al.", "startOffset": 12, "endOffset": 27}, {"referenceID": 12, "context": "The work of [Lopyrev, 2015] leverages neural networks to generate news headline, where input documents are limited to 50 word tokens, and the work of [Hu et al., 2015] also deals with short texts (up to dozens of word tokens), in which summarization problems such as content redundancy is less prominent and attention-based models seem to be sufficient.", "startOffset": 150, "endOffset": 167}, {"referenceID": 12, "context": "Note that our improvement is achieved over the model that has already outperformed the attention-based model reported in [Hu et al., 2015] on short documents.", "startOffset": 121, "endOffset": 138}, {"referenceID": 29, "context": "We base our model on the general encoder-decoder framework [Sutskever et al., 2011; Sutskever et al., 2014; Cho et al., 2014] that has shown to be effective recently on different tasks.", "startOffset": 59, "endOffset": 125}, {"referenceID": 30, "context": "We base our model on the general encoder-decoder framework [Sutskever et al., 2011; Sutskever et al., 2014; Cho et al., 2014] that has shown to be effective recently on different tasks.", "startOffset": 59, "endOffset": 125}, {"referenceID": 4, "context": "We base our model on the general encoder-decoder framework [Sutskever et al., 2011; Sutskever et al., 2014; Cho et al., 2014] that has shown to be effective recently on different tasks.", "startOffset": 59, "endOffset": 125}, {"referenceID": 11, "context": "The recent literature shows long-short term memory (LSTM) [Hochreiter and Schmidhuber, 1997; Sutskever et al., 2014] and gated recurrent units (GRU) [Bahdanau et al.", "startOffset": 58, "endOffset": 116}, {"referenceID": 30, "context": "The recent literature shows long-short term memory (LSTM) [Hochreiter and Schmidhuber, 1997; Sutskever et al., 2014] and gated recurrent units (GRU) [Bahdanau et al.", "startOffset": 58, "endOffset": 116}, {"referenceID": 0, "context": ", 2014] and gated recurrent units (GRU) [Bahdanau et al., 2014] are both good architectures.", "startOffset": 40, "endOffset": 63}, {"referenceID": 0, "context": "The bottom part of Figure 1 shows the encoder intuitively, while for more details, readers can refer to [Bahdanau et al., 2014] for further discussion.", "startOffset": 104, "endOffset": 127}, {"referenceID": 20, "context": "Two-layer hidden output We first extended the recent twolevel hidden output model [Luong et al., 2015] to our summarization models.", "startOffset": 82, "endOffset": 102}, {"referenceID": 32, "context": "The model in [Tu et al., 2016] also uses history attention weights, but we use history here to force distraction in order to avoid redundancy, which is not a concern in the machine translation task.", "startOffset": 13, "endOffset": 30}, {"referenceID": 14, "context": "Unknown word replacement for summarization We borrowed the unknown word replacement [Jean et al., 2015] from machine translation to our summarization models and found it improved the performance when summarizing long documents.", "startOffset": 84, "endOffset": 103}, {"referenceID": 10, "context": "We experiment with our summarization models on two publicly available corpora with different document lengths and in different languages: a CNN news collection [Hermann et al., 2015] and a Chinese corpus made available more recently in [Hu et al.", "startOffset": 160, "endOffset": 182}, {"referenceID": 12, "context": ", 2015] and a Chinese corpus made available more recently in [Hu et al., 2015].", "startOffset": 61, "endOffset": 78}, {"referenceID": 10, "context": "CNN data The CNN data [Hermann et al., 2015] have a human-generated real-life summary for each news article.", "startOffset": 22, "endOffset": 44}, {"referenceID": 22, "context": "1 The data was preprocessed with the Stanford CoreNLP tools [Manning et al., 2014] for tokenization and sentenceboundary detection; all capital information is kept.", "startOffset": 60, "endOffset": 82}, {"referenceID": 12, "context": "LCSTS data The second corpus is LCSTS, which is a Chinese corpus made available more recently in [Hu et al., 2015].", "startOffset": 97, "endOffset": 114}, {"referenceID": 12, "context": "We used the original training/testing split mentioned in [Hu et al., 2015], but additionally randomly sampled a small part of the training data as our validation set.", "startOffset": 57, "endOffset": 74}, {"referenceID": 36, "context": "We used mini-batch stochastic gradient descent (SGD) to optimize log-likelihood, and Adadelta [Zeiler, 2012] to automatically adapt the learning rate of parameters ( = 10\u22126 and \u03c1 = 0.", "startOffset": 94, "endOffset": 108}, {"referenceID": 12, "context": "com/deepmind/rc-data Same as in [Hu et al., 2015], we used characters rather than words as our tokens.", "startOffset": 32, "endOffset": 49}, {"referenceID": 1, "context": "Our implementation uses python and is based on the Theano library [Bergstra et al., 2010].", "startOffset": 66, "endOffset": 89}, {"referenceID": 19, "context": "The upper part of the table includes the baseline results of a number of typical summarization algorithms, which we listed in the table as Luhn [Luhn, 1958], Edmundson [Edmundson, 1969], LSA [Steinberger and Jezek, 2004], Lex-rank [Erkan and Radev, 2004], Text-rank [Mihalcea and Tarau, 2004], Sumbasic [Vanderwende et al.", "startOffset": 144, "endOffset": 156}, {"referenceID": 7, "context": "The upper part of the table includes the baseline results of a number of typical summarization algorithms, which we listed in the table as Luhn [Luhn, 1958], Edmundson [Edmundson, 1969], LSA [Steinberger and Jezek, 2004], Lex-rank [Erkan and Radev, 2004], Text-rank [Mihalcea and Tarau, 2004], Sumbasic [Vanderwende et al.", "startOffset": 168, "endOffset": 185}, {"referenceID": 28, "context": "The upper part of the table includes the baseline results of a number of typical summarization algorithms, which we listed in the table as Luhn [Luhn, 1958], Edmundson [Edmundson, 1969], LSA [Steinberger and Jezek, 2004], Lex-rank [Erkan and Radev, 2004], Text-rank [Mihalcea and Tarau, 2004], Sumbasic [Vanderwende et al.", "startOffset": 191, "endOffset": 220}, {"referenceID": 8, "context": "The upper part of the table includes the baseline results of a number of typical summarization algorithms, which we listed in the table as Luhn [Luhn, 1958], Edmundson [Edmundson, 1969], LSA [Steinberger and Jezek, 2004], Lex-rank [Erkan and Radev, 2004], Text-rank [Mihalcea and Tarau, 2004], Sumbasic [Vanderwende et al.", "startOffset": 231, "endOffset": 254}, {"referenceID": 23, "context": "The upper part of the table includes the baseline results of a number of typical summarization algorithms, which we listed in the table as Luhn [Luhn, 1958], Edmundson [Edmundson, 1969], LSA [Steinberger and Jezek, 2004], Lex-rank [Erkan and Radev, 2004], Text-rank [Mihalcea and Tarau, 2004], Sumbasic [Vanderwende et al.", "startOffset": 266, "endOffset": 292}, {"referenceID": 33, "context": "The upper part of the table includes the baseline results of a number of typical summarization algorithms, which we listed in the table as Luhn [Luhn, 1958], Edmundson [Edmundson, 1969], LSA [Steinberger and Jezek, 2004], Lex-rank [Erkan and Radev, 2004], Text-rank [Mihalcea and Tarau, 2004], Sumbasic [Vanderwende et al., 2007], and KL-sum [Haghighi and Vanderwende, 2009].", "startOffset": 303, "endOffset": 329}, {"referenceID": 9, "context": ", 2007], and KL-sum [Haghighi and Vanderwende, 2009].", "startOffset": 20, "endOffset": 52}, {"referenceID": 12, "context": "The baseline is the best result reported in [Hu et al., 2015]4.", "startOffset": 44, "endOffset": 61}, {"referenceID": 12, "context": "We thank the authors of [Hu et al., 2015] for generously sharing us the latest output of their models, which achieves a better performance than the results reported in [Hu et al.", "startOffset": 24, "endOffset": 41}, {"referenceID": 12, "context": ", 2015] for generously sharing us the latest output of their models, which achieves a better performance than the results reported in [Hu et al., 2015].", "startOffset": 134, "endOffset": 151}, {"referenceID": 12, "context": "[Hu et al., 2015] 29.", "startOffset": 0, "endOffset": 17}], "year": 2016, "abstractText": "Distributed representation learned with neural networks has recently shown to be effective in modeling natural languages at fine granularities such as words, phrases, and even sentences. Whether and how such an approach can be extended to help model larger spans of text, e.g., documents, is intriguing, and further investigation would still be desirable. This paper aims to enhance neural network models for such a purpose. A typical problem of document-level modeling is automatic summarization, which aims to model documents in order to generate summaries. In this paper, we propose neural models to train computers not just to pay attention to specific regions and content of input documents with attention models, but also distract them to traverse between different content of a document so as to better grasp the overall meaning for summarization. Without engineering any features, we train the models on two large datasets. The models achieve the state-of-the-art performance, and they significantly benefit from the distraction modeling, particularly when input documents are long.", "creator": "LaTeX with hyperref package"}}}