{"id": "1610.08500", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-Oct-2016", "title": "Synthesis of Shared Control Protocols with Provable Safety and Performance Guarantees", "abstract": "We formalize synthesis of shared control protocols with correctness guarantees for temporal logic specifications. More specifically, we introduce a modeling formalism in which both a human and an autonomy protocol can issue commands to a robot towards performing a certain task. These commands are blended into a joint input to the robot. The autonomy protocol is synthesized using an abstraction of possible human commands accounting for randomness in decisions caused by factors such as fatigue or incomprehensibility of the problem at hand. The synthesis is designed to ensure that the resulting robot behavior satisfies given safety and performance specifications, e.g., in temporal logic. Our solution is based on nonlinear programming and we address the inherent scalability issue by presenting alternative methods. We assess the feasibility and the scalability of the approach by an experimental evaluation.", "histories": [["v1", "Wed, 26 Oct 2016 19:49:09 GMT  (1170kb,D)", "http://arxiv.org/abs/1610.08500v1", null]], "reviews": [], "SUBJECTS": "cs.RO cs.AI cs.LG", "authors": ["nils jansen", "murat cubuktepe", "ufuk topcu"], "accepted": false, "id": "1610.08500"}, "pdf": {"name": "1610.08500.pdf", "metadata": {"source": "CRF", "title": "Synthesis of Shared Control Protocols with Provable Safety and Performance Guarantees", "authors": ["Nils Jansen", "Murat Cubuktepe", "Ufuk Topcu"], "emails": ["njansen@utexas.edu"], "sections": [{"heading": null, "text": "This year, the time has come for the EU Commission to bring the EU Council Presidency to the negotiating table in order to strengthen the EU Council Presidency."}, {"heading": "II. SHARED CONTROL", "text": "We assume that the underlying strategies are consistent with the underlying strategy of DP - in addition, we have a set of specifications, a formal model of robot behavior, and a blending function. In detail, a robot is there to take care of a particular task. For example, it will move to a certain milestone. This task is subject to certain performance and safety considerations, such as it is not safe to take the shortest route because there are too many obstacles. These considerations are expressed by a set of specifications 1,., n. The possible behaviors of the robot within an environment are given by a Markov decision-making process (MDP) Mr. Having MDPs leads to decisions of certain actions and to randomness in the environment: A chosen path could cause a high probability to achieve the goal, while with a low probability, the robot could slip and thus fail to complete the task."}, {"heading": "III. PRELIMINARIES", "text": "1) Models: A probability distribution over a finite or countable set X is a function \u00b5: X \u2192 [0,1] R with a finite or infinite set of options (1). The set of all distributions on X is defined by Distr (X). Definition 1 (MDP): A Markov decision process (MDP) M = (S, sI, A, P) is a tuple with a series of states S, a unique initial state sI S, a finite set A of actions and a (partial) probable transitional function P: S \u00b7 A \u2192 Distr (S). MDPs operate by means of non-deterministic decisions of actions on each state, whose successors are then likely to be determined in relation to the associated probability distribution. The enabled actions on states S are determined by A (s) = {4)."}, {"heading": "IV. SYNTHESIZING SHARED CONTROL PROTOCOLS", "text": "In this section, we will describe our formal approach to synthesizing a common control protocol in the presence of randomization, starting with formalizing the concepts of mixing and disrupting the strategy, then formulating the general problem and showing that the solution to the synthesis problem is correct. Example 2: Consider Figure 3, where a space to navigate is abstracted into a grid. We will use this as our running example. A wheelchair like the one in [11] must be steered from the lower left corner of the grid to the exit in the upper right corner of the grid. There is also an autonomous robotic vacuum cleaner that moves in space; the goal is for the wheelchair to reach the exit without crashing into the vacuum cleaner. We now assume that the vacuum cleaner moves according to probabilities previously determined from evidence; these probabilities are unknown or incomprehensible to the human user."}, {"heading": "A. Strategy blending", "text": "Considering two strategies, they have to be integrated into a new strategy that favours the decisions of one or the other in each state of the MDP. In our setting, the human strategy, in view of an arbitrary mixing function (8), intuitively reflects the confidence of the autonomy protocol in its ability to help in relation to the goals of the human user. Furthermore, factors that are probably unknown or incomprehensible to humans, such as safety or performance optimization, are also reflected by such a function. In other words, the mixing function should assign low confidence to the possible actions of the user when he cannot be trusted to make the right decisions. Example 2: In cells of the grid, where the wheelchair could collide with the vacuum cleaner with a very high probability, it makes sense to assign a high confidence in the decisions of the autonomy protocol."}, {"heading": "B. Perturbation of strategies", "text": "As already mentioned, we want to ensure that the mixed strategy deviates minimally from the human strategy. In order to measure such a deviation now, we introduce the concept of disruption, which - on a level of complexity theory - has been investigated, for example, in [5]. At this point, we present an additive disorder for a (randomized) strategy that increases or reduces the probability of action options in such a way that a well-defined distribution across actions is maintained. Definition 5 (Strategy Disorder): In view of MDP M and strategy degradation SchedM, an (additive) disorder is a function \u03b4: S \u00d7 A \u2192 [\u2212 1,1] with the probability that it extends to all actions."}, {"heading": "C. Design of the autonomy protocol", "text": "For the formal problem, we are given a blending function b, specifications 1,. n, MDP Mr, and human strategy \u03c3h, Mr. We assume that \u03c3h does not meet all specifications, i. e, \u03c3h 6 | = 1,. n. The autonomy protocol represents the autonomous strategy a a SchedMr. According to b, the strategies a and \u03c3h are superimposed in the strategy ha, see definition 4, i. e, \u03c3ha (s, \u03b1) = b (s) \u00b7 \u03c3a (s, \u03b1) + (s) b (s) \u00b7 b (s) \u00b7 p h (s). The shared control synthesis problem is to design the autonomy protocol so that it is for the mixed strategy ha | = 1,. n, while the minimally deviating strategy h."}, {"heading": "D. Additional specifications", "text": "Let us now explain how the NLP can be extended for further specifications. Suppose that, in addition to \"P\" \u2264 \"(\u2666 T), an additional accessibility property\" P \"=\" P \"=\" T \"(\u2666 T\") is specified with \"T\" = \"6.\" We add another set of probability variables \"p\" s \"for each state s\" S \"; (2) is copied for\" p \"sI\" and \"\u03bb\"; (3) is copied for all states s \"T\" T \"and (8) is copied for all p\" s, calculating the probability of reaching T \"below\" ha \"for all states. To handle an expected cost property, we use variables\" r \"to which are assigned the expected cost of reaching G for all s\" S. \"We add the following equations: rsI\" s \"G\" (9), \"s\" s \"G\" s. \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"(\" 0), \"s.\" (\")"}, {"heading": "E. Generalized blending", "text": "If the problem is not feasible for the given blending function, the Autonomy Protocol can optionally try to calculate a new function b: S \u2192 [0,1] for which the modified problem is feasible. We call this procedure generalized blending. The idea is that the calculation of this function gives the protocol designer insight into where more trust in autonomy must be placed, or vice versa, where man cannot be trusted to meet the given specifications. Calculation of this new function is achieved by almost the same NLP as for a fixed blending function, while variables must be added for each state. Definition of the new blending function is b (s) = bs. We replace Equation 7 with previous strategies."}, {"heading": "V. COMPUTATIONALLY TRACTABLE APPROACH", "text": "The non-linear programming approach presented in the previous section provides a rigorous method of solving the shared control synthesis problem and serves as a mathematically precise definition of the problem. However, NLPs are known to have strict limitations on scalability and suffer from numerical instabilities; the crucial point for an efficient solution is to bypass the expensive calculation of optimal randomized strategies and reduce the number of variables. We propose a heuristic solution that makes it possible to use linear programming (LP) while ensuring soundness.We use a technique called model repair. Intuitively, a flawed model is modified to meet certain specifications, especially in the face of a Markov chain denying a specification violated by D is an automated method that transforms it into a new MC D."}, {"heading": "VI. CASE STUDY AND EXPERIMENTS", "text": "Defining a formal synthesis approach to the shared control scenario requires a predictive assessment of a human user's intentions. As explained in the previous chapter, we address inherent uncertainties by applying a randomized strategy on possible actions to be taken, discussing how such strategies can be obtained, and reporting on benchmark results."}, {"heading": "A. Experimental setting", "text": "Our setting is the wheelchair scenario from Example 2 within an interactive python environment. The size of the grid is variable and any number of stationary and randomly moved obstacles (the vacuum cleaner) can be defined. An agent (the wheelchair) is moved according to predefined (randomized) strategies or interactively by a human user. From this scenario, an MDP is created with states corresponding to the position of the agent and the obstacles. Actions induce changes in the position of the agent. The safety specification ensures that the agent reaches a target cell without a certain probability of hitting an obstacle, either in the form of a worst-case analysis for any strategy or specifically for a strategy. The entire tool chain integrates the simulation environment with the approaches described in the previous sections. We use the probability model checker PRISM [15] for verification, either in the form of a worst-case analysis for any possible strategy or concrete strategy."}, {"heading": "B. Data collection", "text": "We ask five participants to perform tests in the vicinity with the aim of moving the agent into a target cell without him being in the same cell as the moving obstacle. From the data obtained from each participant, an individual, randomized human strategy \u03c3h can be derived for that participant using Maximum Entropy Inverse Reinforcement Learning (MEIRL) [22]. For example, reverse reinforcement learning has also been used in [14] to collect data on human behavior in a common control scenario (but without formal guarantees) or in [18] to distinguish human intentions in relation to various tasks. In our setting, each sample is a specific command from the participant, while we must assume that the command is actually carried out with the intention to safely reach the specification of meeting a target cell. For the resulting strategy, the probability of a possible deviation from the actual intention in relation to the number of samples conducted by Hoeffdings may be limited."}, {"heading": "C. Experiments", "text": "The workflow of the experiments is shown in Figure 4. First, we will discuss sample data for a particular participant with an 8 x 8 grid with a moving obstacle, which induces an MDP of 2304 states. In synthesis, we will use the model repair procedure, as explained in Section V, because the approach is based on NLP, practicable only for very small examples. We will design the blending function as follows: In states where human strategy induces a high probability of crash, we place low trust in humans and vice versa. With this function, the autonomous strategy \u03c3a is created and transferred (along with the function) back into the environment. Note that the blended strategy \u03c3ah is ensured to meet the specification, see Lemma 1. We leave the same participant as before test runs, but this time we mix the human commands with the (randomized) commands of the autonomous strategy. Then the actual action of the agent is determined stochastically."}, {"heading": "VII. CONCLUSION", "text": "We introduced a formal approach to the synthesis of autonomy protocols in a common control framework with guarantees for quantitative safety and performance specifications. Data-based experiments demonstrated the practical applicability of our approach. Future work will focus on experiments in robotic scenarios and further scalability improvement."}], "references": [{"title": "Apprenticeship learning via inverse reinforcement learning", "author": ["Pieter Abbeel", "Andrew Y Ng"], "venue": "In Proceedings of the twenty-first international conference on Machine learning,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2004}, {"title": "Principles of Model Checking", "author": ["Christel Baier", "Joost-Pieter Katoen"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2008}, {"title": "Model repair for probabilistic systems", "author": ["Ezio Bartocci", "Radu Grosu", "Panagiotis Katsaros", "CR Ramakrishnan", "Scott A Smolka"], "venue": "In TACAS,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2011}, {"title": "Large-scale nonlinear programming using IPOPT: An integrating framework for enterprisewide dynamic optimization", "author": ["Lorenz T. Biegler", "Victor M. Zavala"], "venue": "Computers & Chemical Engineering,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2009}, {"title": "Perturbation analysis in verification of discrete-time Markov chains", "author": ["Taolue Chen", "Yuan Feng", "David S. Rosenblum", "Guoxin Su"], "venue": "In CONCUR,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2014}, {"title": "Model repair for Markov decision processes. In TASE, pages 85\u201392", "author": ["Taolue Chen", "Ernst Moritz Hahn", "Tingting Han", "Marta Kwiatkowska", "Hongyang Qu", "Lijun Zhang"], "venue": "IEEE CS,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2013}, {"title": "Formalizing assistive teleoperation", "author": ["Anca D. Dragan", "Siddhartha S. Srinivasa"], "venue": "In Robotics: Science and Systems,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2012}, {"title": "A policy-blending formalism for shared control", "author": ["Anca D. Dragan", "Siddhartha S. Srinivasa"], "venue": "I. J. Robotic Res.,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2013}, {"title": "Multi-objective model checking of Markov decision processes", "author": ["Kousha Etessami", "Marta Z. Kwiatkowska", "Moshe Y. Vardi", "Mihalis Yannakakis"], "venue": "Logical Methods in Computer Science,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2008}, {"title": "Synthesis of shared autonomy policies with temporal logic specifications", "author": ["Jie Fu", "Ufuk Topcu"], "venue": "IEEE Trans. Automation Science and Engineering,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2016}, {"title": "A brain-actuated wheelchair: Asynchronous and non-invasive brain-computer interfaces for continuous control of robots", "author": ["F. Gal\u00e1n", "M. Nuttin", "E. Lew", "P.W. Ferrez", "G. Vanacker", "J. Philips", "J. del R. Mill\u00e1n"], "venue": "Clinical Neurophysiology,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2016}, {"title": "Shared control of a robot using eeg-based feedback signals. In Proceedings of the 2Nd Workshop on Machine Learning for Interactive Systems: Bridging the Gap Between Perception, Action and Communication, MLIS", "author": ["I\u00f1aki Iturrate", "Jason Omedes", "Luis Montesano"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2013}, {"title": "Shared autonomy via hindsight optimization", "author": ["Shervin Javdani", "J Andrew Bagnell", "Siddhartha Srinivasa"], "venue": "In Proceedings of Robotics: Science and Systems,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2015}, {"title": "PRISM 4.0: Verification of probabilistic real-time systems", "author": ["Marta Kwiatkowska", "Gethin Norman", "David Parker"], "venue": "In CAV,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2011}, {"title": "Algorithms for inverse reinforcement learning", "author": ["Andrew Y Ng", "Stuart J Russell"], "venue": "In Icml,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2000}, {"title": "A greedy approach for the efficient repair of stochastic models", "author": ["Shashank Pathak", "Erika \u00c1brah\u00e1m", "Nils Jansen", "Armando Tacchella", "Joost-Pieter Katoen"], "venue": "In NFM,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2015}, {"title": "Modular inverse reinforcement learning for visuomotor behavior", "author": ["Constantin A Rothkopf", "Dana H Ballard"], "venue": "Biological cybernetics,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2013}, {"title": "Assistive planning in complex, dynamic environments: a probabilistic approach", "author": ["Pete Trautman"], "venue": "CoRR, abs/1506.06784,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2015}, {"title": "A unified approach to 3 basic challenges in shared autonomy", "author": ["Pete Trautman"], "venue": "CoRR, abs/1508.01545,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2015}, {"title": "Modeling purposeful adaptive behavior with the principle of maximum causal entropy", "author": ["Brian D Ziebart"], "venue": null, "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2010}, {"title": "Maximum entropy inverse reinforcement learning", "author": ["Brian D Ziebart", "Andrew L Maas", "J Andrew Bagnell", "Anind K Dey"], "venue": null, "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2008}], "referenceMentions": [{"referenceID": 10, "context": "Such scenarios are for instance found in remotely operated semi-autonomous wheelchairs [11].", "startOffset": 87, "endOffset": 91}, {"referenceID": 6, "context": "Earlier work discusses shared control from different perspectives [7], [8], [20], [19], [13], [10], however, formal correctness in the sense of ensuring safety or optimizing performance has not been considered.", "startOffset": 66, "endOffset": 69}, {"referenceID": 7, "context": "Earlier work discusses shared control from different perspectives [7], [8], [20], [19], [13], [10], however, formal correctness in the sense of ensuring safety or optimizing performance has not been considered.", "startOffset": 71, "endOffset": 74}, {"referenceID": 18, "context": "Earlier work discusses shared control from different perspectives [7], [8], [20], [19], [13], [10], however, formal correctness in the sense of ensuring safety or optimizing performance has not been considered.", "startOffset": 76, "endOffset": 80}, {"referenceID": 17, "context": "Earlier work discusses shared control from different perspectives [7], [8], [20], [19], [13], [10], however, formal correctness in the sense of ensuring safety or optimizing performance has not been considered.", "startOffset": 82, "endOffset": 86}, {"referenceID": 11, "context": "Earlier work discusses shared control from different perspectives [7], [8], [20], [19], [13], [10], however, formal correctness in the sense of ensuring safety or optimizing performance has not been considered.", "startOffset": 88, "endOffset": 92}, {"referenceID": 9, "context": "Earlier work discusses shared control from different perspectives [7], [8], [20], [19], [13], [10], however, formal correctness in the sense of ensuring safety or optimizing performance has not been considered.", "startOffset": 94, "endOffset": 98}, {"referenceID": 7, "context": "As in [8], the blending puts weight on either the human\u2019s or the autonomy protocol\u2019s choices depending on factors such as the confidence of the human or the level of information the autonomy protocol has at its disposal.", "startOffset": 6, "endOffset": 9}, {"referenceID": 10, "context": "typical shared control scenario based on a wheelchair [11] where a human user and an autonomy protocol share the control responsibility.", "startOffset": 54, "endOffset": 58}, {"referenceID": 14, "context": "Having a human user solving a task, we compute strategies from the obtained data using inverse reinforcement learning [16], [1].", "startOffset": 118, "endOffset": 122}, {"referenceID": 0, "context": "Having a human user solving a task, we compute strategies from the obtained data using inverse reinforcement learning [16], [1].", "startOffset": 124, "endOffset": 127}, {"referenceID": 7, "context": "First, Dragan and Srinivasa discussed strategy blending for shared control in [8], [7].", "startOffset": 78, "endOffset": 81}, {"referenceID": 6, "context": "First, Dragan and Srinivasa discussed strategy blending for shared control in [8], [7].", "startOffset": 83, "endOffset": 86}, {"referenceID": 11, "context": "presented shared control using feedback based on electroencephalography (a method to record electrical activity of the brain) [13], where a robot is partly controlled via error signals from a brain-computer interface.", "startOffset": 126, "endOffset": 130}, {"referenceID": 17, "context": "In [19], Trautman proposes to treat shared control broadly as a random process where different components are modeled by their joint probability distributions.", "startOffset": 3, "endOffset": 7}, {"referenceID": 9, "context": "Finally, in [10] a synthesis method switches authority between a human operator and the autonomy such that satisfaction of linear temporal logic constraints can be ensured.", "startOffset": 12, "endOffset": 16}, {"referenceID": 0, "context": "1) Models: A probability distribution over a finite or countably infinite set X is a function \u03bc : X\u2192 [0,1]\u2286R with \u2211x\u2208X \u03bc(x) = \u03bc(X) = 1.", "startOffset": 101, "endOffset": 106}, {"referenceID": 1, "context": "The unique probability measure Pr (\u03a0) for a set \u03a0 of paths of MC D can be defined by the usual cylinder set construction, the expected cost of a set \u03a0 of paths is denoted by EC (\u03a0), see [2] for details.", "startOffset": 186, "endOffset": 189}, {"referenceID": 1, "context": "For practical reasons, we restrict ourselves to memoryless strategies, again refer to [2] for details.", "startOffset": 86, "endOffset": 89}, {"referenceID": 0, "context": "2) Specifications: A quantitative reachability property P\u2264\u03bb (\u2666T ) with upper probability threshold \u03bb \u2208 [0,1]\u2286Q and target set T \u2286 S constrains the probability to reach T from sI in M to be at most \u03bb .", "startOffset": 103, "endOffset": 108}, {"referenceID": 8, "context": ",\u03c6n, can be formally verified for an MDP using multi-objective model checking [9].", "startOffset": 78, "endOffset": 81}, {"referenceID": 10, "context": "A wheelchair as in [11] is to be steered from the lower left corner of the grid to the exit on the upper right corner of the grid.", "startOffset": 19, "endOffset": 23}, {"referenceID": 7, "context": "In [8] it is argued that blending intuitively reflects the confidence in how good the autonomy protocol is able to assist with respect to the human user\u2019s goals.", "startOffset": 3, "endOffset": 6}, {"referenceID": 17, "context": "In [19], additional notions of blending are discussed.", "startOffset": 3, "endOffset": 7}, {"referenceID": 0, "context": "Definition 4 (Linear blending): Given an MDP Mr = (S,sI ,A,P), two strategies \u03c3h,\u03c3a \u2208 Schedr , and a blending function b : S\u2192 [0,1], the blended strategy \u03c3ha \u2208 Schedr for all states s \u2208 S, and actions \u03b1 \u2208 A is", "startOffset": 126, "endOffset": 131}, {"referenceID": 4, "context": "To now measure such a deviation, we introduce the concept of perturbation which was\u2014on a complexity theoretic level\u2014for instance investigated in [5].", "startOffset": 145, "endOffset": 148}, {"referenceID": 0, "context": "We introduce the following specific set Var of variables: \u2022 \u03c3 s,\u03b1 a ,\u03c3 s,\u03b1 ha \u2208 [0,1] for each s \u2208 S and \u03b1 \u2208 A define the autonomous strategy \u03c3a and the blended strategy \u03c3ha.", "startOffset": 80, "endOffset": 85}, {"referenceID": 0, "context": "\u2022 ps \u2208 [0,1] for each s \u2208 S are assigned the probability of reaching T \u2286 S from state s under strategy \u03c3ha.", "startOffset": 7, "endOffset": 12}, {"referenceID": 0, "context": "If the problem is not feasible for the given blending function, optionally the autonomy protocol can try to compute a new function b : S\u2192 [0,1] for which the altered problem is feasible.", "startOffset": 138, "endOffset": 143}, {"referenceID": 0, "context": "A satisfying assignment for the resulting nonlinear program induces a suitable blending function b : S\u2192 [0,1] in addition to the strategies.", "startOffset": 104, "endOffset": 109}, {"referenceID": 3, "context": "The results were obtained using the NLP solver IPOPT [4].", "startOffset": 53, "endOffset": 56}, {"referenceID": 2, "context": "In [3], the first approach to automatically repair an MC model was presented as an NLP.", "startOffset": 3, "endOffset": 6}, {"referenceID": 5, "context": "Simulation-based algorithms were investigated in [6].", "startOffset": 49, "endOffset": 52}, {"referenceID": 15, "context": "A heuristic but very scalable technique called local repair was proposed in [17].", "startOffset": 76, "endOffset": 80}, {"referenceID": 0, "context": "The safety specification ensures that the agent reaches a target cell without crashing into an obstacle with a certain high probability \u03bb \u2208 [0,1], formally P\u2265\u03bb (\u00accrash U target).", "startOffset": 140, "endOffset": 145}, {"referenceID": 13, "context": "We use the probabilistic model checker PRISM [15] for verification,", "startOffset": 45, "endOffset": 49}, {"referenceID": 3, "context": "We use the NLP solver IPOPT [4] and the LP solver Gurobi [12].", "startOffset": 28, "endOffset": 31}, {"referenceID": 15, "context": "the greedy method from [17] into our framework augmented by side constraints ensuring well-defined strategies.", "startOffset": 23, "endOffset": 27}, {"referenceID": 20, "context": "domized human strategy \u03c3h for this participant can be obtained via Maximum Entropy Inverse Reinforcement Learning (MEIRL) [22].", "startOffset": 122, "endOffset": 126}, {"referenceID": 12, "context": "Inverse reinforcement learning has\u2014for instance\u2014also been used in [14] to collect data about human behavior in a shared control scenario (though without any", "startOffset": 66, "endOffset": 70}, {"referenceID": 16, "context": "formal guarantees) or in [18] to distinguish human intents Process data via MEIRL Shared control synthesis", "startOffset": 25, "endOffset": 29}, {"referenceID": 19, "context": "For the resulting strategy, the probability of a possible deviation from the actual intend can be bounded with respect to the number of samples using Hoeffding\u2019s inequality, see [21] for details.", "startOffset": 178, "endOffset": 182}, {"referenceID": 13, "context": "Thus, the encoding in the PRISM-language [15]", "startOffset": 41, "endOffset": 45}], "year": 2016, "abstractText": "We formalize synthesis of shared control protocols with correctness guarantees for temporal logic specifications. More specifically, we introduce a modeling formalism in which both a human and an autonomy protocol can issue commands to a robot towards performing a certain task. These commands are blended into a joint input to the robot. The autonomy protocol is synthesized using an abstraction of possible human commands accounting for randomness in decisions caused by factors such as fatigue or incomprehensibility of the problem at hand. The synthesis is designed to ensure that the resulting robot behavior satisfies given safety and performance specifications, e.g., in temporal logic. Our solution is based on nonlinear programming and we address the inherent scalability issue by presenting alternative methods. We assess the feasibility and the scalability of the approach by an experimental evaluation.", "creator": "LaTeX with hyperref package"}}}