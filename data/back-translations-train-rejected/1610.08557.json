{"id": "1610.08557", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-Oct-2016", "title": "Knowledge-Based Biomedical Word Sense Disambiguation with Neural Concept Embeddings", "abstract": "Biomedical word sense disambiguation (WSD) is an important intermediate task in many natural language processing applications such as named entity recognition, syntactic parsing, and relation extraction. In this paper, we employ knowledge-based approaches that also exploit recent advances in neural word/concept embeddings to improve over the state-of-the-art in biomedical WSD using the MSH WSD dataset as the test set. Our methods involve distant supervision - we do not use any hand-labeled examples for WSD to build our prediction models; however, we employ an existing well known named entity recognition and concept mapping program, MetaMap, to obtain our concept vectors. Over the MSH WSD dataset, our linear time (in terms of numbers of senses and words in the test instance) method achieves an accuracy of 92.24% which is an absolute 3% improvement over the best known results obtained via unsupervised or knowledge-based means. A more expensive approach that we developed relies on a nearest neighbor framework and achieves an accuracy of 94.34%. Employing dense vector representations learned from unlabeled free text has been shown to benefit many language processing tasks recently and our efforts show that biomedical WSD is no exception to this trend. For a complex and rapidly evolving domain such as biomedicine, building labeled datasets for larger sets of ambiguous terms may be impractical. Here we demonstrate that distant supervision that leverages recent advances in representation learning can rival supervised approaches in biomedical WSD.", "histories": [["v1", "Wed, 26 Oct 2016 21:49:15 GMT  (20kb)", "https://arxiv.org/abs/1610.08557v1", null], ["v2", "Sun, 4 Dec 2016 00:57:16 GMT  (17kb)", "http://arxiv.org/abs/1610.08557v2", null], ["v3", "Mon, 27 Feb 2017 20:38:45 GMT  (156kb,D)", "http://arxiv.org/abs/1610.08557v3", null], ["v4", "Wed, 28 Jun 2017 02:13:13 GMT  (206kb,D)", "http://arxiv.org/abs/1610.08557v4", "8 pages"], ["v5", "Sat, 30 Sep 2017 01:01:50 GMT  (260kb,D)", "http://arxiv.org/abs/1610.08557v5", "8 pages, accepted to appear in proceedings of IEEE BIBE 2017"]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["a k m sabbir", "antonio jimeno yepes", "ramakanth kavuluru"], "accepted": false, "id": "1610.08557"}, "pdf": {"name": "1610.08557.pdf", "metadata": {"source": "CRF", "title": "Knowledge-Based Biomedical Word Sense Disambiguation with Neural Concept Embeddings", "authors": ["AKM Sabbir", "Antonio Jimeno-Yepes", "Ramakanth Kavuluru"], "emails": ["akm.sabbir@uky.edu", "antonio.jimeno@gmail.com", "rvkavu2@uky.edu"], "sections": [{"heading": null, "text": "I. INTRODUCTIONBiomedical Natural Language Processing (NLP), which goes beyond simple word processing, is becoming increasingly indispensable for deriving values and insights from vast amounts of unstructured data generated in the form of scientific articles [3] - [5], clinical narratives [6] and health-related social media posts [9] - [11]. The specific components, including the named entity recognition programs, syntactic parsers and relations, form the backbone of many high-level information and knowledge applications. For most components in an NLP application pipeline, there is a clear snowball effect of errors in one component, leading to more errors in other subsequent components and the final results of the application."}, {"heading": "II. BACKGROUND AND RELATED WORK", "text": "For a thorough overview of approaches to WSD, we refer readers to the Navigli survey [13], which mainly suggests three categories - supervised, knowledge-based and unsupervised approaches. Supervised approaches to WSD [14], [15] use a labeled dataset along with interesting lexical / syntactical features derived from the context around the term to create machine-learned models that predict the correct sense in invisible test contexts. Knowledge-based approaches [1], [16] do not use a corpus, but rely exclusively on thesauric or sense inventories such as WordNet and the Unified Medical Language System (UMLS), which contain short definitions of different senses and corresponding synonyms. Unsupervised approaches may use topic-based modeling methods [17] to disambiguate if the senses are known in the preambiguity of time."}, {"heading": "A. WSD in Biomedicine", "text": "This year is the highest in the history of the country."}, {"heading": "B. Neural Embeddings for WSD", "text": "Neural word representations have shown that they capture both semantic and syntactical information, and some newer approaches learn word vectors [30] - [32] (as elements of Rd, where d is the dimension) in an unattended manner from textual corpora. These dense word vectors avoid the conciseness problems inherent in the so-called most uniform word representations 1. Chen et al. [34] adapted neural word embeddings to calculate different embeddings (of the same word), and performed competitively on the SemEval 2007 WSD dataset [35]. An ambiguity is achieved by selecting the sense that maximizes the cosmic similarity of the corresponding meaning vector with the context vector for an ambiguous term. Recently, Iacobacci et al. [36] have evaluated and demonstrated the superiority of neural word embeddings as features in monitored WD data sets on the SD Semval."}, {"heading": "III. OUR APPROACH", "text": "There are 203 ambiguous terms in the MSH WSD dataset [1] with a total of 424 unique CUIs (from the UMLS), each of which has a unique meaning. Thus, the dataset has an average of 424 / 203 = 2.08 senses. There are a total of 38,495 test instances of contexts (a few sentences), each of which leads to a unique representation (typically the size of the vocabulary), which leads to further problems with similarity calculations, a phenomenon often referred to as the curse of dimensionality [33, chapter 1.4] of the 203 ambiguous terms along with the correct sense (CUI). Apart from being the largest biomedical WSD dataset, it also contains a richer set of ambiguities, including 106 ambiguous abbreviations, 88 ambiguous noun phrases, and 9 which are combinations of the two. Due to these features, the NSD researchers are directly encouraging these approaches to use their data."}, {"heading": "A. Neural Word and Concept Embeddings", "text": "We ran Google's well-known word2vec [32] word embedding program (the Skip-gram model) on over 20 million biomedical citations (titles and abstracts) from PubMed to obtain word vector representations with a word window size of ten words and dimensionality d = 300, with all other parameters set to the default settings. To learn concepts or CUI vectors of the same dimensionality, we curated a dataset of five million randomly selected citations (published between 1998 and 2014) for this subset of PubMed. For this subset of PubMed, we launched MetaMap [29] with its WSD option to obtain unique CUIs for potentially ambiguous terms2. MetaMap passed the text two adjacent non-stop words at a time to capture as many CUIs as possible. Next, we treated these sequences of CUIs in each version of the CUI as a textual derivative, although we did not consider the result of this 4th component useful."}, {"heading": "B. WSD with Word/Concept Embeddings and KnowledgeBased Approaches", "text": "It is also a WSD option based on concept profiles generated by different concepts in biomedical literature."}, {"heading": "C. WSD with Weak Supervision", "text": "In fact, it is as if most of them are able to abide by the rules. (...) It is as if they were able to abide by the rules. (...) It is as if they were able to abide by the rules. (...) It is as if they were able to abide by the rules. (...) It is as if they were able to abide by the rules. (...) It is as if they were able to abide by the rules. (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...).). (...). (...). (...).). (...). (...). (...). (...). (...).). (...). (...).). (...). (...). (...). (...). (...).). (...).). (...). (...). (...).). (...). (...). (...).). (...). (...). (...).). (...). (...). (...).). (...). (...). (...). (...). (...). (...). (...). (...).). (...). (...). (...). (...). (...). (...). (...).).). (...). (...). (...). (...). (...). (...).).). (...)."}, {"heading": "IV. RESULTS AND DISCUSSION", "text": "In fact, most of them are able to go in search of a solution that has its origins in the past."}, {"heading": "V. CONCLUSION", "text": "This year it is more than ever before."}, {"heading": "ACKNOWLEDGMENTS", "text": "Our work is primarily supported by the National Library of Medicine through the R21LM012274 grant, the National Center for Advancing Translational Sciences through the UL1TR001998 grant, and the Kentucky Lung Cancer Research Program through the PO2 41514000040001 grant. Content is the sole responsibility of the authors and does not necessarily reflect the official opinion of the NIH."}], "references": [{"title": "Exploiting MeSH indexing in MEDLINE to generate a data set for word sense disambiguation", "author": ["A. Jimeno-Yepes", "B.T. McInnes", "A.R. Aronson"], "venue": "BMC bioinformatics, vol. 12, no. 223, 2011.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2011}, {"title": "Knowledge based word-concept model estimation and refinement for biomedical text mining", "author": ["A. Jimeno-Yepes", "R. Berlanga"], "venue": "Journal of biomedical informatics, vol. 53, pp. 300\u2013307, 2015.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2015}, {"title": "Bridging semantics and syntax with graph algorithm \u2013 state-of-the-art of extracting biomedical relations", "author": ["Y. Luo", "\u00d6. Uzuner", "P. Szolovits"], "venue": "Briefings in bioinformatics, p. bbw001, 2016.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2016}, {"title": "Context-driven automatic subgraph creation for literature-based discovery", "author": ["D. Cameron", "R. Kavuluru", "T.C. Rindflesch", "A.P. Sheth", "K. Thirunarayan", "O. Bodenreider"], "venue": "Journal of biomedical informatics, vol. 54, pp. 141\u2013157, 2015.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2015}, {"title": "An up-to-date knowledge-based literature search and exploration framework for focused bioscience domains", "author": ["R. Kavuluru", "C. Thomas", "A.P. Sheth", "V. Chan", "W. Wang", "A. Smith", "A. Soto", "A. Walters"], "venue": "Proc. of the 2nd ACM SIGHIT Health Informatics Symposium. ACM, 2012, pp. 275\u2013284.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2012}, {"title": "Mayo clinical text analysis and knowledge extraction system (cTAKES)", "author": ["G.K. Savova", "J.J. Masanz", "P.V. Ogren", "J. Zheng", "S. Sohn", "K.K. Schuler", "C.G. Chute"], "venue": "Journal of the American Medical Informatics Association, vol. 17, no. 5, pp. 507\u2013513, 2010.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2010}, {"title": "2010 i2b2/VA challenge on concepts, assertions, and relations in clinical text", "author": ["\u00d6. Uzuner", "B.R. South", "S. Shen", "S.L. DuVall"], "venue": "Journal of the American Medical Informatics Association, vol. 18, no. 5, pp. 552\u2013556, 2011.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2011}, {"title": "An empirical evaluation of supervised learning approaches in assigning diagnosis codes to electronic medical records", "author": ["R. Kavuluru", "A. Rios", "Y. Lu"], "venue": "Artificial intelligence in medicine, vol. 65, no. 2, pp. 155\u2013166, 2015.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "Utilizing social media data for pharmacovigilance: a review", "author": ["A. Sarker", "R. Ginn", "A. Nikfarjam", "K. O\u2019Connor", "K. Smith", "S. Jayaraman", "T. Upadhaya", "G. Gonzalez"], "venue": "Journal of biomedical informatics, vol. 54, pp. 202\u2013212, 2015.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2015}, {"title": "Toward automated e-cigarette surveillance: Spotting e-cigarette proponents on Twitter", "author": ["R. Kavuluru", "A. Sabbir"], "venue": "Journal of biomedical informatics, vol. 61, pp. 19\u201326, 2016.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2016}, {"title": "Social media mining for public health monitoring and surveillance.", "author": ["M. Paul", "A. Sarker", "J. Brownstein", "A. Nikfarjam", "M. Scotch", "K. Smith", "G. Gonzalez"], "venue": "Pacific Symposium on Biocomputing.,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2015}, {"title": "Word sense disambiguation improves information retrieval", "author": ["Z. Zhong", "H.T. Ng"], "venue": "Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume 1. Association for Computational Linguistics, 2012, pp. 273\u2013282.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2012}, {"title": "Word sense disambiguation: A survey", "author": ["R. Navigli"], "venue": "ACM Computing Surveys (CSUR), vol. 41, no. 2, p. 10, 2009.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2009}, {"title": "It makes sense: A wide-coverage word sense disambiguation system for free text", "author": ["Z. Zhong", "H.T. Ng"], "venue": "Proceedings of the ACL 2010 System Demonstrations. Association for Computational Linguistics, 2010, pp. 78\u201383.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2010}, {"title": "Disambiguation of biomedical text using diverse sources of information", "author": ["M. Stevenson", "Y. Guo", "R. Gaizauskas", "D. Martinez"], "venue": "BMC bioinformatics, vol. 9, no. 11, 2008.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2008}, {"title": "Evaluating measures of semantic similarity and relatedness to disambiguate terms in biomedical text", "author": ["B.T. McInnes", "T. Pedersen"], "venue": "Journal of biomedical informatics, vol. 46, no. 6, pp. 1116\u20131124, 2013.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2013}, {"title": "Link-topic model for biomedical abbreviation disambiguation", "author": ["S. Kim", "J. Yoon"], "venue": "Journal of biomedical informatics, vol. 53, pp. 367\u2013 380, 2015.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2015}, {"title": "A sense-topic model for word sense induction with unsupervised data enrichment", "author": ["J. Wang", "M. Bansal", "K. Gimpel", "B.D. Ziebart", "T.Y. Clement"], "venue": "Transactions of the Association for Computational Linguistics, vol. 3, pp. 59\u201371, 2015.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2015}, {"title": "Word sense disambiguation in the biomedical domain: an overview", "author": ["M.J. Schuemie", "J.A. Kors", "B. Mons"], "venue": "Journal of Computational Biology, vol. 12, no. 5, pp. 554\u2013565, 2005.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2005}, {"title": "Developing a test collection for biomedical word sense disambiguation.", "author": ["M. Weeber", "J.G. Mork", "A.R. Aronson"], "venue": "Proceedings of the AMIA Symposium. American Medical Informatics Association,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2001}, {"title": "Abbreviation and acronym disambiguation in clinical discourse", "author": ["S. Pakhomov", "T. Pedersen", "C.G. Chute"], "venue": "AMIA Annual Symposium Proceedings, vol. 2005. American Medical Informatics Association, 2005, p. 589.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2005}, {"title": "Combining corpus-derived sense profiles with estimated frequency information to disambiguate clinical abbreviations", "author": ["H. Xu", "P.D. Stetson", "C. Friedman"], "venue": "AMIA Annual Symposium Proceedings, vol. 2012. American Medical Informatics Association, 2012, p. 1004.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2012}, {"title": "A multi-aspect comparison study of supervised word sense disambiguation", "author": ["H. Liu", "V. Teller", "C. Friedman"], "venue": "Journal of the American Medical Informatics Association, vol. 11, no. 4, pp. 320\u2013331, 2004.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2004}, {"title": "Word sense disambiguation across two domains: biomedical literature and clinical notes", "author": ["G.K. Savova", "A.R. Coden", "I.L. Sominsky", "R. Johnson", "P.V. Ogren", "P.C. De Groen", "C.G. Chute"], "venue": "Journal of biomedical informatics, vol. 41, no. 6, pp. 1088\u20131100, 2008.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2008}, {"title": "Hyperdimensional computing approach to word sense disambiguation", "author": ["B.-T. Berster", "J.C. Goodwin", "T. Cohen"], "venue": "AMIA Annual Symposium Proceedings. American Medical Informatics Association, 2012, pp. 1129\u20131138.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2012}, {"title": "Word sense disambiguation in the clinical domain: a comparison of knowledge-rich and knowledge-poor unsupervised methods", "author": ["R. Chasin", "A. Rumshisky", "O. Uzuner", "P. Szolovits"], "venue": "Journal of the American Medical Informatics Association, vol. 21, no. 5, pp. 842\u2013849, 2014.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2014}, {"title": "Clinical word sense disambiguation with interactive search and classification", "author": ["Y. Wang", "K. Zheng", "H. Xu", "Q. Mei"], "venue": "AMIA Annual Symposium Proceedings. American Medical Informatics Association, 2016, pp. 2062\u20132071.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2016}, {"title": "An overview of MetaMap: historical perspective and recent advances", "author": ["A.R. Aronson", "F.-M. Lang"], "venue": "Journal of the American Medical Informatics Association, vol. 17, no. 3, pp. 229\u2013236, 2010.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2010}, {"title": "A neural probabilistic language model", "author": ["Y. Bengio", "R. Ducharme", "P. Vincent", "C. Janvin"], "venue": "The Journal of Machine Learning Research, vol. 3, pp. 1137\u20131155, 2003.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2003}, {"title": "A unified architecture for natural language processing: Deep neural networks with multitask learning", "author": ["R. Collobert", "J. Weston"], "venue": "Proceedings of the 25th international conference on Machine learning. ACM, 2008, pp. 160\u2013167.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2008}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["T. Mikolov", "I. Sutskever", "K. Chen", "G.S. Corrado", "J. Dean"], "venue": "Advances in Neural Information Processing Systems, 2013, pp. 3111\u20133119.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2013}, {"title": "Pattern recognition and machine", "author": ["C.M. Bishop"], "venue": null, "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2006}, {"title": "A unified model for word sense representation and disambiguation", "author": ["X. Chen", "Z. Liu", "M. Sun"], "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing. ACL, 2014, pp. 1025\u20131035.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2014}, {"title": "Semeval-2007 task 07: Coarse-grained english all-words task", "author": ["R. Navigli", "K.C. Litkowski", "O. Hargraves"], "venue": "Proceedings of the 4th International Workshop on Semantic Evaluations. Association for Computational Linguistics, 2007, pp. 30\u201335.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2007}, {"title": "Embeddings for word sense disambiguation: An evaluation study", "author": ["I. Iacobacci", "M.T. Pilehvar", "R. Navigli"], "venue": "Proceedings of the  54th Annual Meeting of the Association for Computational Linguistics. ACL, 2016, pp. 897\u2013907.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2016}, {"title": "Corpus domain effects on distributional semantic modeling of medical terms", "author": ["S.V. Pakhomov", "G. Finley", "R. McEwan", "Y. Wang", "G.B. Melton"], "venue": "Bioinformatics, p. In Press, 2016.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2016}, {"title": "Integration of UMLS and Medline in unsupervised word sense disambiguation", "author": ["A. Jimeno-Yepes", "A.R. Aronson"], "venue": "2012 AAAI fall symposium series, 2012, pp. 26\u201331.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2012}, {"title": "Elementary Linear Algebra", "author": ["R. Larson", "D.C. Falvo"], "venue": null, "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2008}, {"title": "Convolutional neural networks for biomedical text classification: application in indexing biomedical articles", "author": ["A. Rios", "R. Kavuluru"], "venue": "Proceedings of the 6th ACM Conference on Bioinformatics, Computational Biology and Health Informatics. ACM, 2015, pp. 258\u2013267.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2015}, {"title": "Locality-sensitive hashing for finding nearest neighbors [lecture notes", "author": ["M. Slaney", "M. Casey"], "venue": "IEEE Signal Processing Magazine, vol. 25, no. 2, pp. 128\u2013131, 2008.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2008}, {"title": "Word embeddings and recurrent neural networks based on long-short term memory nodes in supervised biomedical word sense disambiguation", "author": ["A. Jimeno-Yepes"], "venue": "arXiv preprint arXiv:1604.02506, 2016.", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2016}, {"title": "Distributed representations of sentences and documents", "author": ["Q. Le", "T. Mikolov"], "venue": "Proceedings of the 31st International Conference on Machine Learning (ICML-14), 2014, pp. 1188\u20131196.", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2014}, {"title": "Supervised Sequence Labelling with Recurrent Neural Networks, ser", "author": ["A. Graves"], "venue": "Studies in Computational Intelligence. Springer,", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2012}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural computation, vol. 9, no. 8, pp. 1735\u20131780, 1997.", "citeRegEx": "45", "shortCiteRegEx": null, "year": 1997}], "referenceMentions": [{"referenceID": 0, "context": "In this paper, we employ knowledgebased approaches that also exploit recent advances in neural word/concept embeddings to improve over the state-of-the-art in biomedical WSD using the public MSH WSD dataset [1] as the test set.", "startOffset": 207, "endOffset": 210}, {"referenceID": 1, "context": "24% which is a 3% improvement over the best known results [2] obtained via unsupervised means.", "startOffset": 58, "endOffset": 61}, {"referenceID": 2, "context": "Biomedical natural language processing (NLP) that goes beyond simple text processing is increasingly becoming indispensable to derive value and insights from vast amounts of unstructured data generated in the form of scientific articles [3]\u2013[5], clinical narratives [6]\u2013[8] and health related social media posts [9]\u2013[11].", "startOffset": 237, "endOffset": 240}, {"referenceID": 4, "context": "Biomedical natural language processing (NLP) that goes beyond simple text processing is increasingly becoming indispensable to derive value and insights from vast amounts of unstructured data generated in the form of scientific articles [3]\u2013[5], clinical narratives [6]\u2013[8] and health related social media posts [9]\u2013[11].", "startOffset": 241, "endOffset": 244}, {"referenceID": 5, "context": "Biomedical natural language processing (NLP) that goes beyond simple text processing is increasingly becoming indispensable to derive value and insights from vast amounts of unstructured data generated in the form of scientific articles [3]\u2013[5], clinical narratives [6]\u2013[8] and health related social media posts [9]\u2013[11].", "startOffset": 266, "endOffset": 269}, {"referenceID": 7, "context": "Biomedical natural language processing (NLP) that goes beyond simple text processing is increasingly becoming indispensable to derive value and insights from vast amounts of unstructured data generated in the form of scientific articles [3]\u2013[5], clinical narratives [6]\u2013[8] and health related social media posts [9]\u2013[11].", "startOffset": 270, "endOffset": 273}, {"referenceID": 8, "context": "Biomedical natural language processing (NLP) that goes beyond simple text processing is increasingly becoming indispensable to derive value and insights from vast amounts of unstructured data generated in the form of scientific articles [3]\u2013[5], clinical narratives [6]\u2013[8] and health related social media posts [9]\u2013[11].", "startOffset": 312, "endOffset": 315}, {"referenceID": 10, "context": "Biomedical natural language processing (NLP) that goes beyond simple text processing is increasingly becoming indispensable to derive value and insights from vast amounts of unstructured data generated in the form of scientific articles [3]\u2013[5], clinical narratives [6]\u2013[8] and health related social media posts [9]\u2013[11].", "startOffset": 316, "endOffset": 320}, {"referenceID": 11, "context": "Recent research also shows that resolving ambiguities provides performance gains in information retrieval and search system design [12].", "startOffset": 131, "endOffset": 135}, {"referenceID": 0, "context": "In this effort, we employ knowledge-based methods, neural concept and word vectors learned through unsupervised deep learning approaches, and a straightforward nearest neighbor approach to achieve new state-of-the-art results over a public gold standard dataset [1] in biomedical word sense disambiguation (WSD).", "startOffset": 262, "endOffset": 265}, {"referenceID": 12, "context": "For a thorough overview of approaches to WSD, we direct the readers to the survey by Navigli [13], which suggests mainly three categories \u2013 supervised, knowledgebased, and unsupervised approaches.", "startOffset": 93, "endOffset": 97}, {"referenceID": 13, "context": "Supervised approaches for WSD [14], [15] use a labeled dataset along with interesting lexical/syntactic features derived from the context around the term to build machine learned models that predict the correct sense in unseen test contexts.", "startOffset": 30, "endOffset": 34}, {"referenceID": 14, "context": "Supervised approaches for WSD [14], [15] use a labeled dataset along with interesting lexical/syntactic features derived from the context around the term to build machine learned models that predict the correct sense in unseen test contexts.", "startOffset": 36, "endOffset": 40}, {"referenceID": 0, "context": "Knowledge-based approaches [1], [16] do not use any corpus but solely rely on thesauri or sense inventories such as WordNet and the Unified Medical Language System (UMLS) that contain brief definitions of different senses and corresponding synonyms.", "startOffset": 27, "endOffset": 30}, {"referenceID": 15, "context": "Knowledge-based approaches [1], [16] do not use any corpus but solely rely on thesauri or sense inventories such as WordNet and the Unified Medical Language System (UMLS) that contain brief definitions of different senses and corresponding synonyms.", "startOffset": 32, "endOffset": 36}, {"referenceID": 16, "context": "Unsupervised approaches may employ topic modeling [17] based methods to disambiguate when the senses are known ahead of time.", "startOffset": 50, "endOffset": 54}, {"referenceID": 17, "context": "Some unsupervised approaches [18] are often referred to as performing word sense discrimination or ar X iv :1 61 0.", "startOffset": 29, "endOffset": 33}, {"referenceID": 18, "context": "[20] present a nice survey of approaches and efforts in biomedical WSD until 2005 including the wellknown NLM WSD dataset [21], which has 50 ambiguous terms with 5000 test instances.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "[20] present a nice survey of approaches and efforts in biomedical WSD until 2005 including the wellknown NLM WSD dataset [21], which has 50 ambiguous terms with 5000 test instances.", "startOffset": 122, "endOffset": 126}, {"referenceID": 20, "context": "Disambiguation efforts were also focused on a small set of 10\u201315 ambiguous abbreviations [22], [23] using combinations of supervised and unsupervised approaches.", "startOffset": 89, "endOffset": 93}, {"referenceID": 21, "context": "Disambiguation efforts were also focused on a small set of 10\u201315 ambiguous abbreviations [22], [23] using combinations of supervised and unsupervised approaches.", "startOffset": 95, "endOffset": 99}, {"referenceID": 22, "context": "More recent approaches [24], [25] used supervised models including Naive Bayes, SVMs, logistic regressors, decision lists with a variety of features using both subsets of the NLM WSD dataset and other smaller datasets.", "startOffset": 23, "endOffset": 27}, {"referenceID": 23, "context": "More recent approaches [24], [25] used supervised models including Naive Bayes, SVMs, logistic regressors, decision lists with a variety of features using both subsets of the NLM WSD dataset and other smaller datasets.", "startOffset": 29, "endOffset": 33}, {"referenceID": 24, "context": "[26] encoded senses, contexts, and ambiguous terms using random indexing and conducted supervised ten-fold cross validation experiments on the NLM WSD dataset using the binary splatter code method.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "McInnes and Pedersen [16] used the network structure of the UMLS (specifically the hypernymic trees) and concept definitions to devise concept relatedness measures which are in turn used for WSD for the MSH WSD dataset.", "startOffset": 21, "endOffset": 25}, {"referenceID": 25, "context": "[27] demonstrated the application of topic modeling for a clinical WSD dataset of 50 ambiguous terms curated from the Mayo Clinic [25].", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "[27] demonstrated the application of topic modeling for a clinical WSD dataset of 50 ambiguous terms curated from the Mayo Clinic [25].", "startOffset": 130, "endOffset": 134}, {"referenceID": 26, "context": "[28] used an active learning strategy to involve domain experts in an interactive supervised machine learning framework for biomedical WSD.", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "Among all the datasets available, the MSH WSD that we use in our current effort is the largest publicly available dataset [1] for biomedical WSD (more in Section III) and also has the least skewed distribution (the average percentage of majority sense is 54% [28]).", "startOffset": 122, "endOffset": 125}, {"referenceID": 26, "context": "Among all the datasets available, the MSH WSD that we use in our current effort is the largest publicly available dataset [1] for biomedical WSD (more in Section III) and also has the least skewed distribution (the average percentage of majority sense is 54% [28]).", "startOffset": 259, "endOffset": 263}, {"referenceID": 1, "context": "In a recent approach Jimeno-Yepes and Berlanga [2] used a hybrid approach that combined a knowledge-based component that exploits the UMLS definitions and synonyms for different concepts with unlabeled biomedical narratives (from Medline/PubMed) to derive word-concept probability estimates P (w|c) for any word w and UMLS concept c.", "startOffset": 47, "endOffset": 50}, {"referenceID": 0, "context": "1% on the MSH WSD dataset [1].", "startOffset": 26, "endOffset": 29}, {"referenceID": 27, "context": "Our methods can be classified as weakly supervised given we employ the well-known biomedical concept mapping tool MetaMap [29] to generate concept vectors and employ them in combination with the knowledge-based method from Jimeno-Yepes and Berlanga [2].", "startOffset": 122, "endOffset": 126}, {"referenceID": 1, "context": "Our methods can be classified as weakly supervised given we employ the well-known biomedical concept mapping tool MetaMap [29] to generate concept vectors and employ them in combination with the knowledge-based method from Jimeno-Yepes and Berlanga [2].", "startOffset": 249, "endOffset": 252}, {"referenceID": 28, "context": "Neural word representations have been shown to capture both semantic and syntactic information and a few recent approaches learn word vectors [30]\u2013[32] (as elements of R, where d is the dimension) in an unsupervised fashion from textual corpora.", "startOffset": 142, "endOffset": 146}, {"referenceID": 30, "context": "Neural word representations have been shown to capture both semantic and syntactic information and a few recent approaches learn word vectors [30]\u2013[32] (as elements of R, where d is the dimension) in an unsupervised fashion from textual corpora.", "startOffset": 147, "endOffset": 151}, {"referenceID": 32, "context": "[34] adapted neural word embeddings to compute different sense embeddings (of the same word) and showed competitive performance on the SemEval 2007 WSD dataset [35].", "startOffset": 0, "endOffset": 4}, {"referenceID": 33, "context": "[34] adapted neural word embeddings to compute different sense embeddings (of the same word) and showed competitive performance on the SemEval 2007 WSD dataset [35].", "startOffset": 160, "endOffset": 164}, {"referenceID": 34, "context": "[36] evaluated and demonstrated the superiority of neural word embeddings as features in supervised WSD models on the same SemEval dataset.", "startOffset": 0, "endOffset": 4}, {"referenceID": 35, "context": "[37] use word embeddings (without corpus enhanced concept embeddings) for the MSH WSD dataset but only report 77% accuracy although the central aim of their paper is not limited to WSD.", "startOffset": 0, "endOffset": 4}, {"referenceID": 32, "context": "[34] to directly learn sense vectors using a pure distributional semantics framework that does not rely on word vectors.", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "There are 203 ambiguous terms in the MSH WSD dataset [1] with a total of 424 unique CUIs (from the UMLS), each of which is a unique sense.", "startOffset": 53, "endOffset": 56}, {"referenceID": 30, "context": "We ran the well-known word2vec [32] word embedding program (the skip-gram model) from Google on over 20 million biomedical citations (titles and abstracts) from PubMed to obtain word vector representations with a word window size of ten words and dimensionality d = 300 with all other parameters set to the default settings.", "startOffset": 31, "endOffset": 35}, {"referenceID": 27, "context": "For this subset of PubMed, we ran MetaMap [29] with its WSD option turned on so we obtain unique CUIs for potential ambiguous terms2.", "startOffset": 42, "endOffset": 46}, {"referenceID": 1, "context": "This is because we use, as a component, the CUI definition based information via the word-probability estimate based approach [2] outlined earlier.", "startOffset": 126, "endOffset": 129}, {"referenceID": 1, "context": "As will see in Section IV, this intuition appears to work as well as other state-of-the-art approaches [2].", "startOffset": 103, "endOffset": 106}, {"referenceID": 36, "context": "It also has a WSD option which is based on concept profiles generated through words co-occurring with different concepts in biomedical literature [38].", "startOffset": 146, "endOffset": 150}, {"referenceID": 1, "context": "4) Our final approach uses a probabilistic model developed in an earlier effort by Jimeno-Yepes and Berlanga [2] (as outlined in Section II-A and elaborated in the Appendix) that selects the c that maximizes P (T |c).", "startOffset": 109, "endOffset": 112}, {"referenceID": 1, "context": "26% which is slightly better than the current best result [2] achieved through unsupervised and knowledge-based approaches.", "startOffset": 58, "endOffset": 61}, {"referenceID": 15, "context": "The usage of accuracy as the evaluation metric is inline with a few prior efforts on biomedical WSD [16], [22], [24], [37] and is justified [27] given the notions of precision and recall are equivalent to it in this scenario.", "startOffset": 100, "endOffset": 104}, {"referenceID": 20, "context": "The usage of accuracy as the evaluation metric is inline with a few prior efforts on biomedical WSD [16], [22], [24], [37] and is justified [27] given the notions of precision and recall are equivalent to it in this scenario.", "startOffset": 106, "endOffset": 110}, {"referenceID": 22, "context": "The usage of accuracy as the evaluation metric is inline with a few prior efforts on biomedical WSD [16], [22], [24], [37] and is justified [27] given the notions of precision and recall are equivalent to it in this scenario.", "startOffset": 112, "endOffset": 116}, {"referenceID": 35, "context": "The usage of accuracy as the evaluation metric is inline with a few prior efforts on biomedical WSD [16], [22], [24], [37] and is justified [27] given the notions of precision and recall are equivalent to it in this scenario.", "startOffset": 118, "endOffset": 122}, {"referenceID": 25, "context": "The usage of accuracy as the evaluation metric is inline with a few prior efforts on biomedical WSD [16], [22], [24], [37] and is justified [27] given the notions of precision and recall are equivalent to it in this scenario.", "startOffset": 140, "endOffset": 144}, {"referenceID": 1, "context": "Jimeno-Yepes and Berlanga [2] 89.", "startOffset": 26, "endOffset": 29}, {"referenceID": 1, "context": "Combining f , f, and [2] (f ) 92.", "startOffset": 21, "endOffset": 24}, {"referenceID": 1, "context": "our ensemble method that combines our word/concept vector approach with the knowledge-based method by Jimeno-Yepes and Berlanga [2].", "startOffset": 128, "endOffset": 131}, {"referenceID": 38, "context": "Given our prior experiences in convolutional neural networks (CNNs) in biomedical text classification [40] that proved superior over traditional linear classifiers such as support vector machines and logistic regression models, we built 203 multiclass CNN models, one for each ambiguous term based on this weakly supervised dataset.", "startOffset": 102, "endOffset": 106}, {"referenceID": 39, "context": "Alternative approaches such as locality sensitive hashing [41] that address the dimensionality problems without having to compute cosine similarities may be helpful to alleviate the situation.", "startOffset": 58, "endOffset": 62}, {"referenceID": 0, "context": "Our results rival performances achieved by supervised approaches \u2013 the best published supervised result achieves 93% macro accuracy over ten fold cross validation experiments on the MSH WSD dataset with the Naive Bayes model [1].", "startOffset": 225, "endOffset": 228}, {"referenceID": 40, "context": "Based on additional ten fold cross validation experiments with support vector machines that use neural word vector features, Jimeno-Yepes [42] was able to achieve close to 96% macro accuracy.", "startOffset": 138, "endOffset": 142}, {"referenceID": 41, "context": "To this end, one option is to directly model paragraphs as fixed size vectors using a word2vec style unsupervised learning architecture as demonstrated by Le and Mikolov [43] where paragraph vectors are learned along with word vectors.", "startOffset": 170, "endOffset": 174}, {"referenceID": 0, "context": "The weighted averages of the words and contextual CUIs can then be compared separately with the candidate concept vectors from C(w) to generate two different scores \u2208 [0, 1] whose sum can form the final score to select the correct sense.", "startOffset": 167, "endOffset": 173}, {"referenceID": 41, "context": "\u2022 Both the paragraph vector approach [43] and the weighted averaging approach discussed earlier do not explicitly model word order when composing test context words to form fixed size vectors that better capture the semantics of the full context.", "startOffset": 37, "endOffset": 41}, {"referenceID": 43, "context": "Recurrent neural networks (RNNs [44, Chapter 3]), especially with long short-term memory units [45], are a more suitable alternative for such scenarios but would need training data to set the parameters of the recurrent layer.", "startOffset": 95, "endOffset": 99}], "year": 2017, "abstractText": "Biomedical word sense disambiguation (WSD) is an important intermediate task in many natural language processing applications such as named entity recognition, syntactic parsing, and relation extraction. In this paper, we employ knowledgebased approaches that also exploit recent advances in neural word/concept embeddings to improve over the state-of-the-art in biomedical WSD using the public MSH WSD dataset [1] as the test set. Our methods involve weak supervision \u2013 we do not use any hand-labeled examples for WSD to build our prediction models; however, we employ an existing concept mapping program, MetaMap, to obtain our concept vectors. Over the MSH WSD dataset, our linear time (in terms of numbers of senses and words in the test instance) method achieves an accuracy of 92.24% which is a 3% improvement over the best known results [2] obtained via unsupervised means. A more expensive approach that we developed relies on a nearest neighbor framework and achieves accuracy of 94.34%, essentially cutting the error rate in half. Employing dense vector representations learned from unlabeled free text has been shown to benefit many language processing tasks recently and our efforts show that biomedical WSD is no exception to this trend. For a complex and rapidly evolving domain such as biomedicine, building labeled datasets for larger sets of ambiguous terms may be impractical. Here, we show that weak supervision that leverages recent advances in representation learning can rival supervised approaches in biomedical WSD. However, external knowledge bases (here sense inventories) play a key role in the improvements achieved.", "creator": "LaTeX with hyperref package"}}}