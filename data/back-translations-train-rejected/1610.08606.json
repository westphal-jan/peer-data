{"id": "1610.08606", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Oct-2016", "title": "Fast Low-rank Shared Dictionary Learning for Image Classification", "abstract": "Despite the fact that different objects possess distinct class-specific features, they also usually share common patterns. This observation has been exploited partially in a recently proposed dictionary learning framework by separating the particularity and the commonality (COPAR). Inspired by this, we propose a novel method to explicitly and simultaneously learn a set of common patterns as well as class-specific features for classification with more intuitive constraints. Our dictionary learning framework is hence characterized by both a shared dictionary and particular (class-specific) dictionaries. For the shared dictionary, we enforce a low-rank constraint, i.e. claim that its spanning subspace should have low dimension and the coefficients corresponding to this dictionary should be similar. For the particular dictionaries, we impose on them the well-known constraints stated in the Fisher discrimination dictionary learning (FDDL). Further, we develop new fast and accurate algorithms to solve the subproblems in the learning step, accelerating its convergence. The said algorithms could also be applied to FDDL and its extensions. The efficiencies of these algorithms are theoretically and experimentally verified by comparing their complexities and running time with those of other well-known dictionary learning methods. Experimental results on widely used image datasets establish the advantages of our method over state-of-the-art dictionary learning methods.", "histories": [["v1", "Thu, 27 Oct 2016 03:58:17 GMT  (992kb,D)", "http://arxiv.org/abs/1610.08606v1", "First submission to TIP"], ["v2", "Thu, 6 Jul 2017 15:57:15 GMT  (1311kb,D)", "http://arxiv.org/abs/1610.08606v2", "Accepted version"], ["v3", "Sun, 16 Jul 2017 02:39:50 GMT  (1741kb,D)", "http://arxiv.org/abs/1610.08606v3", "Accepted version"]], "COMMENTS": "First submission to TIP", "reviews": [], "SUBJECTS": "cs.CV cs.AI", "authors": ["tiep vu", "vishal monga"], "accepted": false, "id": "1610.08606"}, "pdf": {"name": "1610.08606.pdf", "metadata": {"source": "CRF", "title": "Fast Low-rank Shared Dictionary Learning for Image Classification", "authors": ["Tiep Huu Vu", "Vishal Monga"], "emails": ["thv102@psu.edu)."], "sections": [{"heading": null, "text": "Index terms - sparse encoding, dictionary learning, low-rank models, shared features, object classification.I. INTRODUCTIONSparse representations have emerged as a powerful tool for a range of signal processing applications, including compressed scanning [1], signal discrimination, sparse signal recovery [2], image processing-based samples [3], image segmentation [4], and more recently, signal classification. In such representations, most signals can be expressed by a linear combination of a few bases taken from a \"dictionary.\" Based on this theory, an economical representation-based classification (SRC) [5] was initially developed for robust face recognition. SRC was adapted to numerous signal / image classification problems ranging from medical image classification [6] - [8], hyperspectral image classification [9] - [13] radar classification [13], radar classification [13]."}, {"heading": "A. Closely Related work and Motivation", "text": "The assumption made by most discriminatory dictionary methods, i.e. non-overlapping subdomains, is unrealistic in practice. Often, objects from different classes share some common characteristics, such as background in scene classification. This problem has been partly addressed by recent efforts, namely DLSI [27], COPAR [28] and CSDL [29]. However, DLSI does not explicitly learn common characteristics, as they are still hidden in the sub-dictionaries. COPAR and CSDL explicitly learn a common dictionary D0, but suffer from the following disadvantages. First, we claim that the subspace spanned by the columns of the common dictionary must have a low rank. Otherwise, class-specific characteristics are represented by the common dictionary."}, {"heading": "B. Contributions", "text": "This year, it has reached the point where it will be able to retaliate."}, {"heading": "II. DISCRIMINATIVE DICTIONARY LEARNING FRAMEWORK", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. Notation", "text": "Besides the notation mentioned in the introduction, D0 is the common dictionary, I am the identity matrix with the dimension derived from the context. For c = 1,.., C; i = 0, 1,.., C, assume that Yc-Rd \u00b7 nc and Y-Rd \u00b7 n with N = 0 nc; Di-Rd \u00b7 ki, D-Rd \u00b7 K with K = 0 kc; and X-RK \u00b7 n. Denote Xi the sparse coefficient of Y on n the sparse coefficient of Yc on D, Xic the sparse coefficient of Yc = 0 kc; and X-RK \u00b7 n. Denote X on Di, the sparse coefficient of Y on n."}, {"heading": "B. Closely related work: Fisher discrimination dictionary learning (FDDL)", "text": "In particular, the discriminatory dictionary Dand learns the sparse coefficient matrix X based on the minimization of the following cost function: JY (D, X) = 12 fY (D, X) + 0 fY (D, X) + 2 g (X), (3) where fY (D, X) = C \u2211 c = 1rYc (D, Xc) is discriminatory fidelity with: rYc (D, Xc) = 1 (D, Xc) = 2 g (X), where fY (D, X) = 2 g (X, X) is discriminatory fidelity with: rYc (D, Xc) = 2 g (D, Xc) = 1 Yc \u2212 DXc, 2F + 2 F + 1 Yc \u2212 DcXcc, 2F + 2 F = c = c, DjXjc, 2F, g (X) = 1 c (X, Mc \u2212 Mc, 2F) + 2 is discriminatory coefficient based on Fisher and the coefficient of 1 and lmul."}, {"heading": "C. Proposed Low-rank shared dictionary learning (LRSDL)", "text": "In the presence of the shared dictionary, Yc is expected to be well represented by the cooperation of the special dictionary Dc and the shared dictionary D0. Specifically, the discriminatory term of loyalty fY (D, X) in (3) can be extended to fY (D, X) = shared contribution C = 1 rYc (D, Xc) with rYc (D, Xc), defining the discriminatory term of loyalty fY (D, X) = shared contribution C = 1 rYc (D, Xc) with rYc (D, Xc). Note that since r-Yc (D, Xc) = rYc (D, Xc) with Yc \u2212 D0X0c \u2212 shared contribution 2F (see Figure 2b), we have: fY (D, X) = shared contribution JC (D, X) = shared contribution 2F."}, {"heading": "D. Efficient solutions for optimization problems", "text": "Before we delve into minimizing the LRSDL lens function in (6), we first introduce efficient algorithms to minimize the FDDL lens function in (3).1) Efficient FDDL Dictionary Update: Remember that in [24] the dictionary update step is divided into C sub-problems, each of which updates one class-specific dictionary Dc while fixing others. This process is repeated until convergence. This approach is not only very time-consuming, but also inaccurate. We will do this in a small example in Section IV-B. We refer this initial FDDL dictionary update to O-FDDL-D. Specifically, if we fix X in Equation 3, we propose an efficient algorithm for updating the dictionary, which is called E-FDDL-D, where the entire dictionary D is optimized when X is fixed, which significantly reduces the computing costs.Specifically, if we fix X in the equation (min), the problem is blessed (9) by (T = D)."}, {"heading": "2) Efficient FDDL sparse coefficient update (E-FDDL-X):", "text": "If D (D) is solid, we become X (DTD) -M (DTD) -M (DTD) -M (DTD) -M (DDL) -M (M1 M2 M2 M2 M2 M2) -M (DTD) -M (X) -M (X) -M (X) -M (X) -M (X) -M (X) -M (X) -M (X) -M (X) -M (X) -M (X) -M (X) -M (X) -M (X) -M (M1 M2 M2.. Mc) -M (13) -M (we) -M (M) -M (DTD) + 2I) -M (DTY M2 M2 M2 M2."}, {"heading": "E. Efficient solutions for other dictionary learning methods", "text": "We also propose here another efficient algorithm for updating the dictionary to two other well-known dictionary learning methods: DLSI [27] and COPAR [28]. The cost function J1 (D, X) in DLSI is defined as: C \u2211 c = 1 (| Yc \u2212 DcXc \u00b2 2F + \u03bb \u00b2 Xc \u00b2 1 + \u03b72C \u00b2 j = 1, j \u2212 6 = c \u00b2 DTj \u00b2 2F) (28) Each class-specific dictionary Dc is updated by solving and solving other problems: Dc = arg min Dc \u00b2 Yc \u00b2 2F \u00b2 2F \u00b2 \u00b2 \u00b2 ADc \u00b2 F, (29) with A = [D1, Dc + 1,.., DC] T. The original solution to this problem, called O-FDDL-D, updates each column dc \u00b2, j \u00b2 Dc \u00b2 Dc \u00b2."}, {"heading": "III. COMPLEXITY ANALYSIS", "text": "We compare the computational complexity for the efficient algorithms and their corresponding original algorithms. We also evaluate the overall complexity of the proposed LRSDL and competing dictionary learning methods: DLSI [27], COPAR [28] and FDDL [24]. The complexity for each algorithm is estimated as the (approximate) number of multiplications required for an iteration (sparse code update and dictionary update).For simplicity's sake, we assume: i) the number of training samples, the number of dictionary bases in each class (and the divided class) are the same, meaning: nc = n, ki = k. ii) The number of bases in each dictionary is comparable to the number of training samples per class and much less than the signal dimension, i.e. k = n d. iii) Each iterative materation algorithm requires q iterations to converge."}, {"heading": "A. Online Dictionary Learning (ODL)", "text": "We start with the well-known online dictionary learning [32], whose cost function is: J (D, X) = 12-Y-DX-2F + \u03bb-X-1. (37), where Y-Rd \u00b7 n, D-Rd \u00b7 k, X-Rk \u00b7 n. Most dictionary learning methods find their solutions by alternatively solving one variable and fixing others. There are two partial problems: 1) Update X (ODL-X): When the dictionary D is fixed, the sparse coefficient X is updated by solving the problem: X = arg min X1 2-Y \u2212 DX-DX-X (38) using FISTA [31]. In each q iteration, the most arithmetic task is to calculate DTDX \u2212 qDTY \u2212 qDTY, where DTY and DTY are precompiled with the complexities k2d and kdn."}, {"heading": "B. Dictionary learning with structured incoherence (DLSI)", "text": "The cost function J1 (D, X) of the DLSI is defined as follows: (28).1) Update X (DLSI-X): In each iteration, the algorithm solves C sub-problems: Xc = arg min Xc-Yc \u2212 DcXc-2F + \u03bb Xc-1. (41) with Yc-Rd-n, Dc-Rd-k and Xc-Rk-n. Based on (39), the complexity of the update X (C sub-problems): Ck (kd + dn + qkn). (42) 2) Original update D (O-DLSI-D): For the update of the D-q-Qi matrix version, each sub-dictionary Sc version has to be solved via (29). The most important step in the algorithm is the compilation of D (O-DLSI-D) - Q3 matrix version."}, {"heading": "C. Separating the particularity and the commonality dictionary learning (COPAR)", "text": "1) Cost function: COPAR [28] is another learning method of the dictionary, which also takes into account the common dictionary (but without the limitation of the lower rank). By using the same notation as in LRSDL, we can describe the cost function of COPAR in the following form: 1 2 f1 (Y, D, X) + 3 (Y, II, Xc) + 3 (Yc, D, Xc) and r1 (Yc, Xc) is defined as: 1 (Y, DXc, 2F + 2) DCTi Dc, 2F \u2212 DcX0c \u2212 Dcc, X, X) = 1 (Y, J, II, Xc) and r1 (Yc, D, Xc, Xc \u2212 2 \u2212 C \u2212 4)."}, {"heading": "D. Fisher discrimination dictionary learning (FDDL)", "text": "1) Original Update X (O-FDDL-X): Based on the results reported in DFDL [7], the complexity of O-FDDL-X is approximately C2kn (d + qCk) + C3dk2 = C2k (dn + qCkn + Cdk).2) Efficient Update X (E-FDDL-X): Based on Section II-D2, the complexity of E-FDDL-X is mainly derived from Equation (14). Let us remember this function M (\u2022), which does not require much computing power. Calculation of M and Mc can also be neglected, since any required calculation of one column, all other columns are the same. Then, the total complexity of the E-FDDL-X algorithm is approximately: (Ck) d (Ck) d (Ck) d (Ck) M (DTD + 2) + (Cn)."}, {"heading": "Dc = argmin", "text": "The complexity of updating Dc k, C 1, is: qdk2 + (C2 + 1) T complexity: d (Cn) k + dnk, F = 2 (Xc) (Xc) T complexity k (Cn) k.When d k, C 1, complexity of updating Dc: qdk2 + (C2 + 1) T complexity: d (Cn) k + dnk, F = 2 (Xc) (Xc) T complexity k (Cn) k.When d k, C 1, complexity of updating Dc: qdk2 + (C2 + 1) dkn (Ck2n) qdk2 + C2dkn (46) Then the complexity of O-FDDL-D qdk (C2 + 4) Cdk-D (E-D) 2K complexity (E-M)."}, {"heading": "E. LRSDL", "text": "1) Update X, X0: From (23), (25) and (27), in each iteration of updating X (II), we need to compute: (M (DTD) + 2\u03bb2I) X \u2212 M (DTY) + + \u03bb2 (M \u2212 2M) \u2212 M (DTD0X0), and (2DT0 D0 + \u04212I) X \u2212 2DT0 Y + DT0 DM (X \u2212 2M0.Therefore, the complexity of LRSDL-X is: (CK) d (CK) DTD + (Ck) d (Cn) DTDDDDDDDDDDDDDDDDDDD0 DDDDDDDDD0 DDDDDDDD2K k k 2k (Cn)."}, {"heading": "F. Summary", "text": "Table I and Table II show the final complexity analysis of each proposed efficient algorithm and its original equivalents. Table II compares LRSDL with other state-of-the-art methods. We choose a typical set of parameters with 100 classes, 20 training samples per class, 10 bases per sub-dictionary and shared dictionary, data dimension 500 and 50 iterations for each iterative method. Specifically: C = 100, n = 20, k = 10, q = 50, d = 500. We also assume that in (50) q2 = 50. Table I shows that all three proposed efficient algorithms require less calculation than original versions with the most significant improvements to accelerate DLSI-D. Table II shows an interesting fact. LRSDL is mathematically the cheapest compared to other original dictionary learning algorithms, and only E-FDDL has lower complexity, which is to be expected, since the FDDL cost function is a special feature of the first LRDL-learning algorithm."}, {"heading": "IV. EXPERIMENTAL RESULTS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. Comparing methods and datasets", "text": "This year, it is so far that it is only a matter of time before it is ready, until it is ready."}, {"heading": "D. Effect of the shared dictionary sizes on overall accuracy", "text": "We are conducting an experiment to examine the effects of the common dictionary size on the overall results of the classification of two dictionary methods: COPAR [28] and LRSDL in the AR gender dataset. In this experiment, 40 images of each class are used for training. The number of common dictionary bases varies between 10 and 80. In LRSDL, since there is a regularization parameter linked to the low term (see Equation (6)), we continue to consider three values of \u03b7 = 0, i.e. no low-level constraint, \u03b7 = 0.01 and \u03b7 = 0.1 for two different focal points. The results are shown in Figure 8.We observe that the performance of COPAR strongly depends on the choice of k0 and its results deteriorate as the size of the common dictionary increases, because when k0 is large, COPAR tends to include class-specific features in the common dictionary."}, {"heading": "E. Overall Classification Accuracy", "text": "Table III shows the overall results of the classification of different methods across all the data sets presented. It is obvious that in most cases, two dictionary learning methods with common characteristics (COPAR [28] and our proposed LRSDL) perform better than others with all five highest values contained in our proposed LRSDL."}, {"heading": "F. Performance vs. size of training set", "text": "To understand the training dependence of the different techniques, we present a comparison of the overall accuracy of the classification as a function of the size of the training sets of the different methods. Figure 9 shows the overall accuracy of the classification for all the above data sets, which correspond to different scenarios. It is easy to see that LRSDL shows the most graceful decline when the training is reduced. In addition, LRSDL also shows high performance, even with low training in AR facial and AR sex data sets."}, {"heading": "V. DISCUSSION AND CONCLUSION", "text": "In this paper, our primary contribution is the development of a discriminatory dictionary learning framework by introducing a common dictionary with two key limitations. First, the common dictionary must have a lower rank. Second, the sparse coefficients corresponding to the common dictionary are subject to a similarity limitation. In conjunction with the discriminatory model proposed in [24], [25], this leads to a more flexible model in which common characteristics are excluded from classification. An important advantage of this model is the robustness of the framework in terms of size (k0) and regulation parameter (\u03b7) of the common dictionary. Compared to state-of-the-art algorithms developed specifically for these tasks, our LRSDL approach offers better classification performance on average. In Section II-D and II-E we discuss the efficient algorithms for FDDL [25], DLSI [27] and then apply them flexibly to more complex models."}, {"heading": "A. Proof of Lemma 1", "text": "Let's wc. (0, 1) K is a binary vector whose j-th element is one if and only if the j-th columns from D belong to Dc, and Wc = diag (wc). We observe that DcXci = DWcXi. We can paraphrase fY, X (D) as: \"Y - DX - 2F + C.\" (XXCXc - 2F + XX.J - 6 = c - DjX. (XXT + C - 2F) = \"Y - DX - 2F - C = 1.\" Let's leave that. (\u2212 2CX.T - X.)."}, {"heading": "B. Proof of Lemma 2", "text": "We have to prove two parts: For the gradient of f \u2212 \u2212 \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212 n \u2212"}, {"heading": "C. Proof of Lemma 3", "text": "If Y, D, X are specified, we have: JY, D, X (D0, X 0) = 1 2 2 2 2 Y \u2212 D0X0 \u2212 DX, 2F + 1 2 2 2 3 Yc \u2212 DcXcc and Y = [Y] 1 Y + 1 1 2... Y \u00b2 C], we can rewrite (55) as: JY, D, X (D0, X 0) = 1 2 Y \u2212 D0X0, 2F + 1 2 Y \u2212 D0X0, 2F + 1 2 Y \u2212 D0X0, 2F + + 1 1 1 1, DY + 1, DY + 1, DY + 2, DY + 2, DY + 2, DY + 2, DY + 2, DY + 2, DY + 2, DY + 2, DY + 2, DY + 2, DY + 2, DY + 2, DY, DY, DY, DY, DY, DY + 2, DY + 2, DY + 2, DY + 2, DY + 2, DY + 2, DY + 2, DY, DY + 2, DY + 2, DY, DY + 2, DY + 2, DY, DY + 2, DY, DY + 2, DY, DY + 2, DY, DY, DY, DY + 2, DY, DY, DY + 2, DY, DY, DY, DY + 2, DY + 2, DY, DY, DY + 2, DY, DY, DY, DY + 2, DY, DY + 2, DY, DY, DY, DY + 2, DY + 2, DY + 2, DY, DY, DY, DY, DY + 2, DY, DX, DX, DX \u2212 DX \u2212 DX \u2212 D0X0 \u2212 DX, DX, DX \u2212 D0X0, DX, DX, DX, DX, DX + 1, DX + 1, DX, DX + 1, D0X0, D0X0, DX, D"}], "references": [{"title": "Compressed sensing,", "author": ["D.L. Donoho"], "venue": "IEEE Transactions on information theory,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2006}, {"title": "Spratling, \u201cImage segmentation using a sparse coding model of cortical area v1,", "author": ["W. M"], "venue": "IEEE Trans. on Image Processing,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2013}, {"title": "Robust face recognition via sparse representation,", "author": ["J. Wright", "A. Yang", "A. Ganesh", "S. Sastry", "Y. Ma"], "venue": "IEEE Trans. on Pattern Analysis and Machine Int.,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2009}, {"title": "DFDL: Discriminative feature-oriented dictionary learning for histopathological image classification,", "author": ["T.H. Vu", "H.S. Mousavi", "V. Monga", "U. Rao", "G. Rao"], "venue": "Proc. IEEE International Symposium on Biomedical Imaging,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2015}, {"title": "Histopathological image classification using discriminative featureoriented dictionary learning,", "author": ["T.H. Vu", "H.S. Mousavi", "V. Monga", "U. Rao", "G. Rao"], "venue": "IEEE Transactions on Medical Imaging,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2016}, {"title": "Task-driven dictionary learning for hyperspectral image classification with structured sparsity constraints,", "author": ["X. Sun", "N.M. Nasrabadi", "T.D. Tran"], "venue": "IEEE Transactions on Geoscience and Remote Sensing,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2015}, {"title": "Structured priors for sparse-representation-based hyperspectral image classification,", "author": ["X. Sun", "Q. Qu", "N.M. Nasrabadi", "T.D. Tran"], "venue": "Geoscience and Remote Sensing Letters, IEEE,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2014}, {"title": "Hyperspectral image classification via kernel sparse representation,", "author": ["Y. Chen", "N.M. Nasrabadi", "T.D. Tran"], "venue": "IEEE Transactions on Geoscience and Remote Sensing,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2013}, {"title": "Multi-view automatic target recognition using joint sparse representation,", "author": ["H. Zhang", "N.M. Nasrabadi", "Y. Zhang", "T.S. Huang"], "venue": "IEEE Trans. on Aerospace and Electronic Systems,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2012}, {"title": "An image recapture detection algorithm based on learning dictionaries of edge profiles,", "author": ["T. Thongkamwitoon", "H. Muammar", "P.-L. Dragotti"], "venue": "IEEE Transactions on Information Forensics and Security,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2015}, {"title": "Adaptive sparse representations for video anomaly detection,", "author": ["X. Mo", "V. Monga", "R. Bala", "Z. Fan"], "venue": "Circuits and Systems for Video Technology, IEEE Transactions on,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2014}, {"title": "Multi-task image classification via collaborative, hierarchical spikeand-slab priors,", "author": ["H.S. Mousavi", "U. Srinivas", "V. Monga", "Y. Suo", "M. Dao", "T. Tran"], "venue": "in Proc. IEEE Conf. on Image Processing,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2014}, {"title": "Structured sparse priors for image classification,", "author": ["U. Srinivas", "Y. Suo", "M. Dao", "V. Monga", "T.D. Tran"], "venue": "Image Processing, IEEE Transactions on,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2015}, {"title": "Jointstructured-sparsity-based classification for multiple-measurement transient acoustic signals,", "author": ["H. Zhang", "Y. Zhang", "N.M. Nasrabadi", "T.S. Huang"], "venue": "Systems, Man, and Cybernetics, Part B: Cybernetics, IEEE Transactions on,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2012}, {"title": "Structured sparse representation with low-rank interference,", "author": ["M. Dao", "Y. Suo", "S.P. Chin", "T.D. Tran"], "venue": "in 2014 48th Asilomar Conf. on Signals, Systems and Computers", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2014}, {"title": "Discriminative K-SVD for dictionary learning in face recognition,", "author": ["Q. Zhang", "B. Li"], "venue": "in Proc. IEEE Conf. Computer Vision Pattern Recognition", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2010}, {"title": "Learning a discriminative dictionary for sparse coding via label consistent K-SVD,", "author": ["Z. Jiang", "Z. Lin", "L.S. Davis"], "venue": "in Proc. IEEE Conf. Computer Vision Pattern Recognition,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2011}, {"title": "Fisher discrimination dictionary learning for sparse representation,", "author": ["M. Yang", "L. Zhang", "X. Feng", "D. Zhang"], "venue": "in Proc. IEEE International Conference on Computer Vision,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2011}, {"title": "Sparse representation based fisher discrimination dictionary learning for image classification,", "author": ["M. Yang", "L. Zhang", "X. Feng", "D. Zhang"], "venue": "Int. Journal of Computer Vision, vol. 109,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2014}, {"title": "Classification and clustering via dictionary learning with structured incoherence and shared features,", "author": ["I. Ramirez", "P. Sprechmann", "G. Sapiro"], "venue": "IEEE Conf. on Comp. Vision and Pattern Recog. IEEE,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2010}, {"title": "A dictionary learning approach for classification: separating the particularity and the commonality,", "author": ["S. Kong", "D. Wang"], "venue": null, "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2012}, {"title": "Learning category-specific dictionary and shared dictionary for fine-grained image categorization,", "author": ["S. Gao", "I.W.-H. Tsang", "Y. Ma"], "venue": "IEEE Trans. on Image Processing,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2014}, {"title": "Learning a low-rank shared dictionary for object classification,", "author": ["T.H. Vu", "V. Monga"], "venue": "IEEE International Conference on Image Processing,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2016}, {"title": "A fast iterative shrinkage-thresholding algorithm for linear inverse problems,", "author": ["A. Beck", "M. Teboulle"], "venue": "SIAM Journal on Imaging Sciences,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2009}, {"title": "Online learning for matrix factorization and sparse coding,", "author": ["J. Mairal", "F. Bach", "J. Ponce", "G. Sapiro"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2010}, {"title": "Distributed optimization and statistical learning via the alternating direction method of multipliers,", "author": ["S. Boyd", "N. Parikh", "E. Chu", "B. Peleato", "J. Eckstein"], "venue": "Foundations and Trends R  \u00a9 in Machine Learning,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2011}, {"title": "Guaranteed minimum-rank solutions of linear matrix equations via nuclear norm minimization,", "author": ["B. Recht", "M. Fazel", "P.A. Parrilo"], "venue": "SIAM review,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2010}, {"title": "A singular value thresholding algorithm for matrix completion,", "author": ["J.-F. Cai", "E.J. Cand\u00e8s", "Z. Shen"], "venue": "SIAM Journal on Optimization,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2010}, {"title": "From few to many: Illumination cone models for face recognition under variable lighting and pose,", "author": ["A. Georghiades", "P. Belhumeur", "D. Kriegman"], "venue": "IEEE Trans. on Pattern Analysis and Machine Int.,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2001}, {"title": "The AR face database,", "author": ["A. Martinez", "R. Benavente"], "venue": "CVC Technical Report,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 1998}, {"title": "A visual vocabulary for flower classification,", "author": ["M.-E. Nilsback", "A. Zisserman"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2006}, {"title": "Learning generative visual models from few training examples: An incremental bayesian approach tested on 101 object categories,", "author": ["L. Fei-Fei", "R. Fergus", "P. Perona"], "venue": "Computer Vision and Image Understanding,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2007}, {"title": "A study of cross-validation and bootstrap for accuracy estimation and model selection.", "author": ["R. Kohavi"], "venue": null, "citeRegEx": "40", "shortCiteRegEx": "40", "year": 1995}, {"title": "Effective use of frequent itemset mining for image classification,", "author": ["B. Fernando", "E. Fromont", "T. Tuytelaars"], "venue": "Vision\u2013ECCV", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2012}, {"title": "Linear spatial pyramid matching using sparse coding for image classification,", "author": ["J. Yang", "K. Yu", "Y. Gong", "T. Huang"], "venue": "IEEE Conf. on Comp. Vision and Pattern Recog. IEEE,", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2009}, {"title": "Latent dictionary learning for sparse representation based classification,", "author": ["M. Yang", "D. Dai", "L. Shen", "L. Gool"], "venue": "in Proc. IEEE Conf. Computer Vision Pattern Recognition,", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2014}, {"title": "A hierarchical-structured dictionary learning for image classification,", "author": ["J. Yoon", "J. Choi", "C.D. Yoo"], "venue": "in Proc. IEEE Conf. on Image Processing,", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2014}], "referenceMentions": [{"referenceID": 0, "context": "Applications include compressed sensing [1], signal denoising, sparse signal recovery [2], image inpainting [3], image segmentation [4], and more recently, signal classification.", "startOffset": 40, "endOffset": 43}, {"referenceID": 1, "context": "Applications include compressed sensing [1], signal denoising, sparse signal recovery [2], image inpainting [3], image segmentation [4], and more recently, signal classification.", "startOffset": 132, "endOffset": 135}, {"referenceID": 2, "context": "Based on this theory, a sparse representation-based classifier (SRC) [5] was initially developed for robust face recognition.", "startOffset": 69, "endOffset": 72}, {"referenceID": 3, "context": "Thereafter, SRC was adapted to numerous signal/image classification problems, ranging from medical image classification [6]\u2013[8], hyperspectral image classification [9]\u2013[11], synthetic aperture radar (SAR) image classification [12], recaptured image recognition [13], video anomaly detection [14], and several others [15]\u2013[20].", "startOffset": 120, "endOffset": 123}, {"referenceID": 5, "context": "Thereafter, SRC was adapted to numerous signal/image classification problems, ranging from medical image classification [6]\u2013[8], hyperspectral image classification [9]\u2013[11], synthetic aperture radar (SAR) image classification [12], recaptured image recognition [13], video anomaly detection [14], and several others [15]\u2013[20].", "startOffset": 164, "endOffset": 167}, {"referenceID": 7, "context": "Thereafter, SRC was adapted to numerous signal/image classification problems, ranging from medical image classification [6]\u2013[8], hyperspectral image classification [9]\u2013[11], synthetic aperture radar (SAR) image classification [12], recaptured image recognition [13], video anomaly detection [14], and several others [15]\u2013[20].", "startOffset": 168, "endOffset": 172}, {"referenceID": 8, "context": "Thereafter, SRC was adapted to numerous signal/image classification problems, ranging from medical image classification [6]\u2013[8], hyperspectral image classification [9]\u2013[11], synthetic aperture radar (SAR) image classification [12], recaptured image recognition [13], video anomaly detection [14], and several others [15]\u2013[20].", "startOffset": 226, "endOffset": 230}, {"referenceID": 9, "context": "Thereafter, SRC was adapted to numerous signal/image classification problems, ranging from medical image classification [6]\u2013[8], hyperspectral image classification [9]\u2013[11], synthetic aperture radar (SAR) image classification [12], recaptured image recognition [13], video anomaly detection [14], and several others [15]\u2013[20].", "startOffset": 261, "endOffset": 265}, {"referenceID": 10, "context": "Thereafter, SRC was adapted to numerous signal/image classification problems, ranging from medical image classification [6]\u2013[8], hyperspectral image classification [9]\u2013[11], synthetic aperture radar (SAR) image classification [12], recaptured image recognition [13], video anomaly detection [14], and several others [15]\u2013[20].", "startOffset": 291, "endOffset": 295}, {"referenceID": 11, "context": "Thereafter, SRC was adapted to numerous signal/image classification problems, ranging from medical image classification [6]\u2013[8], hyperspectral image classification [9]\u2013[11], synthetic aperture radar (SAR) image classification [12], recaptured image recognition [13], video anomaly detection [14], and several others [15]\u2013[20].", "startOffset": 316, "endOffset": 320}, {"referenceID": 15, "context": "Based on the K-SVD [3] model for general sparse representations, Discriminative K-SVD (D-KSVD) [21] and Label-Consistent K-SVD (LC-KSVD) [22], [23] learn the discriminative dictionaries by encouraging a projection of sparse codes X to be close to a sparse matrix with all non-zeros being one while satisfying a block diagonal structure as in Figure 1.", "startOffset": 95, "endOffset": 99}, {"referenceID": 16, "context": "Based on the K-SVD [3] model for general sparse representations, Discriminative K-SVD (D-KSVD) [21] and Label-Consistent K-SVD (LC-KSVD) [22], [23] learn the discriminative dictionaries by encouraging a projection of sparse codes X to be close to a sparse matrix with all non-zeros being one while satisfying a block diagonal structure as in Figure 1.", "startOffset": 137, "endOffset": 141}, {"referenceID": 3, "context": "[6], [7] with DFDL and Yang et al.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[6], [7] with DFDL and Yang et al.", "startOffset": 5, "endOffset": 8}, {"referenceID": 17, "context": "[24], [25] with FDDL apply Fisher-based ideas on dictionaries and sparse coefficients, respectively.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "[24], [25] with FDDL apply Fisher-based ideas on dictionaries and sparse coefficients, respectively.", "startOffset": 6, "endOffset": 10}, {"referenceID": 19, "context": "This problem has been partially addressed by recent efforts, namely DLSI [27], COPAR [28] and CSDL [29].", "startOffset": 73, "endOffset": 77}, {"referenceID": 20, "context": "This problem has been partially addressed by recent efforts, namely DLSI [27], COPAR [28] and CSDL [29].", "startOffset": 85, "endOffset": 89}, {"referenceID": 21, "context": "This problem has been partially addressed by recent efforts, namely DLSI [27], COPAR [28] and CSDL [29].", "startOffset": 99, "endOffset": 103}, {"referenceID": 17, "context": "Our framework is basically a generalized version of the well-known FDDL [24], [25] with the additional capability of capturing shared features, resulting in better performance.", "startOffset": 72, "endOffset": 76}, {"referenceID": 18, "context": "Our framework is basically a generalized version of the well-known FDDL [24], [25] with the additional capability of capturing shared features, resulting in better performance.", "startOffset": 78, "endOffset": 82}, {"referenceID": 18, "context": "In this paper, we aim to mitigate these drawbacks by proposing efficient and accurate algorithms which allows to directly solve X and D in two fundamental discriminative dictionary learning methods: FDDL [25] and DLSI [27].", "startOffset": 204, "endOffset": 208}, {"referenceID": 19, "context": "In this paper, we aim to mitigate these drawbacks by proposing efficient and accurate algorithms which allows to directly solve X and D in two fundamental discriminative dictionary learning methods: FDDL [25] and DLSI [27].", "startOffset": 218, "endOffset": 222}, {"referenceID": 20, "context": "These algorithms can also be applied to speed-up our proposed LRSDL, COPAR [28], DLR [26] and other related works.", "startOffset": 75, "endOffset": 79}, {"referenceID": 22, "context": "Our framework simultaneously learns each class-dictionary 1The preliminary version of this work was presented in IEEE International Conference on Image Processing, 2016 [30].", "startOffset": 169, "endOffset": 173}, {"referenceID": 18, "context": "We present three effective algorithms for dictionary learning: i) sparse coefficient update in FDDL [25] by using FISTA [31].", "startOffset": 100, "endOffset": 104}, {"referenceID": 23, "context": "We present three effective algorithms for dictionary learning: i) sparse coefficient update in FDDL [25] by using FISTA [31].", "startOffset": 120, "endOffset": 124}, {"referenceID": 18, "context": "ii) Dictionary update in FDDL [25] by a simple ODL [32] procedure using M(\u2022) and another lemma.", "startOffset": 30, "endOffset": 34}, {"referenceID": 24, "context": "ii) Dictionary update in FDDL [25] by a simple ODL [32] procedure using M(\u2022) and another lemma.", "startOffset": 51, "endOffset": 55}, {"referenceID": 19, "context": "iii) Dictionary update in DLSI [27] by a simple ADMM [33] procedure which requires only one matrix inversion instead of several matrix inversions as originally proposed in [27].", "startOffset": 31, "endOffset": 35}, {"referenceID": 25, "context": "iii) Dictionary update in DLSI [27] by a simple ADMM [33] procedure which requires only one matrix inversion instead of several matrix inversions as originally proposed in [27].", "startOffset": 53, "endOffset": 57}, {"referenceID": 19, "context": "iii) Dictionary update in DLSI [27] by a simple ADMM [33] procedure which requires only one matrix inversion instead of several matrix inversions as originally proposed in [27].", "startOffset": 172, "endOffset": 176}, {"referenceID": 2, "context": "The toolbox includes implementations of SRC [5], ODL [32], LC-KSVD [23]2, efficient DLSI [27], efficient COPAR [28], efficient FDDL [25], DLR [26] and the proposed LRSDL.", "startOffset": 44, "endOffset": 47}, {"referenceID": 24, "context": "The toolbox includes implementations of SRC [5], ODL [32], LC-KSVD [23]2, efficient DLSI [27], efficient COPAR [28], efficient FDDL [25], DLR [26] and the proposed LRSDL.", "startOffset": 53, "endOffset": 57}, {"referenceID": 19, "context": "The toolbox includes implementations of SRC [5], ODL [32], LC-KSVD [23]2, efficient DLSI [27], efficient COPAR [28], efficient FDDL [25], DLR [26] and the proposed LRSDL.", "startOffset": 89, "endOffset": 93}, {"referenceID": 20, "context": "The toolbox includes implementations of SRC [5], ODL [32], LC-KSVD [23]2, efficient DLSI [27], efficient COPAR [28], efficient FDDL [25], DLR [26] and the proposed LRSDL.", "startOffset": 111, "endOffset": 115}, {"referenceID": 18, "context": "The toolbox includes implementations of SRC [5], ODL [32], LC-KSVD [23]2, efficient DLSI [27], efficient COPAR [28], efficient FDDL [25], DLR [26] and the proposed LRSDL.", "startOffset": 132, "endOffset": 136}, {"referenceID": 23, "context": "We also recall here the FISTA algorithm [31] for solving the family of problems:", "startOffset": 40, "endOffset": 44}, {"referenceID": 17, "context": "FDDL [24] has been used broadly as a technique for exploiting both structured dictionary and learning discriminative coefficient.", "startOffset": 5, "endOffset": 9}, {"referenceID": 26, "context": "For the shared dictionary, as stated in the Introduction, we constrain rank(D0) to be small by using the nuclear norm \u2016D0\u2016\u2217 which is its convex relaxation [34].", "startOffset": 155, "endOffset": 159}, {"referenceID": 0, "context": "arg min 1\u2264c\u2264C (w\u2016y \u2212Dcx\u20162 + (1\u2212 w)\u2016x\u2212mc\u20162), (8) where w \u2208 [0, 1] is a preset weight for balancing the contribution of the two terms.", "startOffset": 58, "endOffset": 64}, {"referenceID": 17, "context": "1) Efficient FDDL dictionary update: Recall that in [24], the dictionary update step is divided into C subproblems, each updates one class-specific dictionary Dc while others fixed.", "startOffset": 52, "endOffset": 56}, {"referenceID": 24, "context": "The problem (10) can be solved effectively by Online Dictionary Learning (ODL) method [32].", "startOffset": 86, "endOffset": 90}, {"referenceID": 23, "context": "The problem (11) has the form of equation (2), and can hence be solved by FISTA [31].", "startOffset": 80, "endOffset": 84}, {"referenceID": 25, "context": "Based on the Lemma 3, D0 can be updated by solving: D0 = arg min D0 trace(FD0 D0)\u2212 2trace(ED0 ) + \u03b7\u2016D0\u2016\u2217 where: E = V(X) ; F = X(X) (17) using the ADMM [33] method and the singular value thresholding algorithm [35].", "startOffset": 152, "endOffset": 156}, {"referenceID": 27, "context": "Based on the Lemma 3, D0 can be updated by solving: D0 = arg min D0 trace(FD0 D0)\u2212 2trace(ED0 ) + \u03b7\u2016D0\u2016\u2217 where: E = V(X) ; F = X(X) (17) using the ADMM [33] method and the singular value thresholding algorithm [35].", "startOffset": 210, "endOffset": 214}, {"referenceID": 27, "context": "Z =D\u03b7/\u03c1(Dc + U), (20) U =U + D0 \u2212 Z, (21) where D is the shrinkage thresholding operator [35].", "startOffset": 89, "endOffset": 93}, {"referenceID": 24, "context": "The optimization problem (18) can be solved by ODL [32].", "startOffset": 51, "endOffset": 55}, {"referenceID": 22, "context": "4) LRSDL sparse coefficients update (LRSDL-X): In our preliminary work [30], we proposed a method for effectively solving X and X alternatively, now we combine both problems into one and find X by solving the following optimization problem:", "startOffset": 71, "endOffset": 75}, {"referenceID": 23, "context": "We again solve this problem using FISTA [31] with the gradient of h(X):", "startOffset": 40, "endOffset": 44}, {"referenceID": 19, "context": "We also propose here another efficient algorithm for updating dictionary in two other well-known dictionary learning methods: DLSI [27] and COPAR [28].", "startOffset": 131, "endOffset": 135}, {"referenceID": 20, "context": "We also propose here another efficient algorithm for updating dictionary in two other well-known dictionary learning methods: DLSI [27] and COPAR [28].", "startOffset": 146, "endOffset": 150}, {"referenceID": 25, "context": "We propose one ADMM [33] procedure to update Dc which requires only one matrix inversion, which will be referred as EDLSI-D.", "startOffset": 20, "endOffset": 24}, {"referenceID": 19, "context": "Later in this paper, we will both theoretically and experimentally show that E-DLSI-D is much more efficient than O-DLSI-D [27].", "startOffset": 123, "endOffset": 127}, {"referenceID": 20, "context": "Note that this algorithm can be beneficial for two subproblems of updating the common dictionary and the particular dictionary in COPAR [28] as well.", "startOffset": 136, "endOffset": 140}, {"referenceID": 19, "context": "We also evaluate the total complexity of the proposed LRSDL and competing dictionary learning methods: DLSI [27], COPAR [28] and FDDL [24].", "startOffset": 108, "endOffset": 112}, {"referenceID": 20, "context": "We also evaluate the total complexity of the proposed LRSDL and competing dictionary learning methods: DLSI [27], COPAR [28] and FDDL [24].", "startOffset": 120, "endOffset": 124}, {"referenceID": 17, "context": "We also evaluate the total complexity of the proposed LRSDL and competing dictionary learning methods: DLSI [27], COPAR [28] and FDDL [24].", "startOffset": 134, "endOffset": 138}, {"referenceID": 24, "context": "Online Dictionary Learning (ODL) We start with the well-known Online Dictionary Learning [32] whose cost function is:", "startOffset": 89, "endOffset": 93}, {"referenceID": 23, "context": "X = arg min X 1 2 \u2016Y \u2212DX\u2016F + \u03bb\u2016X\u20161 (38) using FISTA [31].", "startOffset": 52, "endOffset": 56}, {"referenceID": 19, "context": "DLSI [27] proposed a method to encourage the independence between bases of different classes by minimizing coherence between cross-class bases.", "startOffset": 5, "endOffset": 9}, {"referenceID": 20, "context": "1) Cost function: COPAR [28] is another dictionary learning method which also considers the shared dictionary (but without the low-rank constraint).", "startOffset": 24, "endOffset": 28}, {"referenceID": 20, "context": "2) Update X (COPAR-X): In sparse coefficient update step, COPAR [28] solve Xc one by one via one l1-norm regularization problem:", "startOffset": 64, "endOffset": 68}, {"referenceID": 20, "context": "1 of [28]).", "startOffset": 5, "endOffset": 9}, {"referenceID": 4, "context": "1) Original update X (O-FDDL-X): Based on results reported in DFDL [7], the complexity of O-FDDL-X is roughly Ckn(d+ qCk) + Cdk = Ck(dn+ qCkn+ Cdk).", "startOffset": 67, "endOffset": 70}, {"referenceID": 28, "context": "Comparing methods and datasets We present the experimental results of applying these methods to five diverse datasets: the Extended YaleB face dataset [36], the AR face dataset [37], the AR gender dataset, the Oxford Flower dataset [38], and one multi-class object category dataset \u2013 the Caltech 101 [39].", "startOffset": 151, "endOffset": 155}, {"referenceID": 29, "context": "Comparing methods and datasets We present the experimental results of applying these methods to five diverse datasets: the Extended YaleB face dataset [36], the AR face dataset [37], the AR gender dataset, the Oxford Flower dataset [38], and one multi-class object category dataset \u2013 the Caltech 101 [39].", "startOffset": 177, "endOffset": 181}, {"referenceID": 30, "context": "Comparing methods and datasets We present the experimental results of applying these methods to five diverse datasets: the Extended YaleB face dataset [36], the AR face dataset [37], the AR gender dataset, the Oxford Flower dataset [38], and one multi-class object category dataset \u2013 the Caltech 101 [39].", "startOffset": 232, "endOffset": 236}, {"referenceID": 31, "context": "Comparing methods and datasets We present the experimental results of applying these methods to five diverse datasets: the Extended YaleB face dataset [36], the AR face dataset [37], the AR gender dataset, the Oxford Flower dataset [38], and one multi-class object category dataset \u2013 the Caltech 101 [39].", "startOffset": 300, "endOffset": 304}, {"referenceID": 2, "context": "We compare our results with those using SRC [5] and other state-of-the-art dictionary learning methods: LC-KSVD [23], DLSI [27], FDDL [25], COPAR [28], and DLR [26].", "startOffset": 44, "endOffset": 47}, {"referenceID": 19, "context": "We compare our results with those using SRC [5] and other state-of-the-art dictionary learning methods: LC-KSVD [23], DLSI [27], FDDL [25], COPAR [28], and DLR [26].", "startOffset": 123, "endOffset": 127}, {"referenceID": 18, "context": "We compare our results with those using SRC [5] and other state-of-the-art dictionary learning methods: LC-KSVD [23], DLSI [27], FDDL [25], COPAR [28], and DLR [26].", "startOffset": 134, "endOffset": 138}, {"referenceID": 20, "context": "We compare our results with those using SRC [5] and other state-of-the-art dictionary learning methods: LC-KSVD [23], DLSI [27], FDDL [25], COPAR [28], and DLR [26].", "startOffset": 146, "endOffset": 150}, {"referenceID": 32, "context": "Regularization parameters in all methods are chosen using cross-validation [40].", "startOffset": 75, "endOffset": 79}, {"referenceID": 15, "context": "As in [21], the dimension of a random-face feature in the Extended YaleB is d = 504, while the dimension in AR face is d = 540.", "startOffset": 6, "endOffset": 10}, {"referenceID": 18, "context": "For feature extraction, based on the impressive results presented in [25], we choose the Frequent Local Histogram feature extractor [41] to obtain feature vectors of dimension 10,000.", "startOffset": 69, "endOffset": 73}, {"referenceID": 33, "context": "For feature extraction, based on the impressive results presented in [25], we choose the Frequent Local Histogram feature extractor [41] to obtain feature vectors of dimension 10,000.", "startOffset": 132, "endOffset": 136}, {"referenceID": 34, "context": "We then extract the sparse coding spatial pyramid matching (ScSPM) feature [42], which is the concatenation of vectors pooled from words of the extracted DSIFT descriptor.", "startOffset": 75, "endOffset": 79}, {"referenceID": 19, "context": "Figure 7c) show sample learned bases using DLSI [27] where shared features are still hidden in class-specific bases.", "startOffset": 48, "endOffset": 52}, {"referenceID": 20, "context": "Effect of the shared dictionary sizes on overall accuracy We perform an experiment to study the effect of the shared dictionary size on the overall classification results of two dictionary methods: COPAR [28] and LRSDL in the AR gender dataset.", "startOffset": 204, "endOffset": 208}, {"referenceID": 20, "context": "It is evident that in most cases, two dictionary learning methods with shared features (COPAR [28] and our proposed LRSDL) outperform others with all five highest values presenting in our proposed LRSDL.", "startOffset": 94, "endOffset": 98}, {"referenceID": 2, "context": "YaleB AR AR gender Oxford Flower Caltech 101 SRC [5] 97.", "startOffset": 49, "endOffset": 52}, {"referenceID": 19, "context": "60 DLSI [27] 96.", "startOffset": 8, "endOffset": 12}, {"referenceID": 18, "context": "67 FDDL [25] 97.", "startOffset": 8, "endOffset": 12}, {"referenceID": 20, "context": "26 COPAR [28] 98.", "startOffset": 9, "endOffset": 13}, {"referenceID": 17, "context": "In conjunction with discriminative model as proposed in [24], [25], this leads to a more flexible model where shared features are excluded before doing classification.", "startOffset": 56, "endOffset": 60}, {"referenceID": 18, "context": "In conjunction with discriminative model as proposed in [24], [25], this leads to a more flexible model where shared features are excluded before doing classification.", "startOffset": 62, "endOffset": 66}, {"referenceID": 18, "context": "In Section II-D and II-E, we discuss the efficient algorithms for FDDL [25], DLSI [27], then flexibly apply them into more sophisticated models.", "startOffset": 71, "endOffset": 75}, {"referenceID": 19, "context": "In Section II-D and II-E, we discuss the efficient algorithms for FDDL [25], DLSI [27], then flexibly apply them into more sophisticated models.", "startOffset": 82, "endOffset": 86}, {"referenceID": 35, "context": "Very recently, researchers have begun to address this issue [43], [44].", "startOffset": 60, "endOffset": 64}, {"referenceID": 36, "context": "Very recently, researchers have begun to address this issue [43], [44].", "startOffset": 66, "endOffset": 70}], "year": 2016, "abstractText": "Despite the fact that different objects possess distinct class-specific features, they also usually share common patterns. This observation has been exploited partially in a recently proposed dictionary learning framework by separating the particularity and the commonality (COPAR). Inspired by this, we propose a novel method to explicitly and simultaneously learn a set of common patterns as well as class-specific features for classification with more intuitive constraints. Our dictionary learning framework is hence characterized by both a shared dictionary and particular (class-specific) dictionaries. For the shared dictionary, we enforce a low-rank constraint, i.e. claim that its spanning subspace should have low dimension and the coefficients corresponding to this dictionary should be similar. For the particular dictionaries, we impose on them the well-known constraints stated in the Fisher discrimination dictionary learning (FDDL). Further, we develop new fast and accurate algorithms to solve the subproblems in the learning step, accelerating its convergence. The said algorithms could also be applied to FDDL and its extensions. The efficiencies of these algorithms are theoretically and experimentally verified by comparing their complexities and running time with those of other well-known dictionary learning methods. Experimental results on widely used image datasets establish the advantages of our method over state-of-the-art dictionary learning methods.", "creator": "LaTeX with hyperref package"}}}