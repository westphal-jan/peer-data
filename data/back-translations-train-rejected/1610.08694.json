{"id": "1610.08694", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Oct-2016", "title": "CogALex-V Shared Task: LexNET - Integrated Path-based and Distributional Method for the Identification of Semantic Relations", "abstract": "We present a submission to the CogALex 2016 shared task on the corpus-based identification of semantic relations, using LexNET (Shwartz and Dagan, 2016), an integrated path-based and distributional method for semantic relation classification. The reported results in the shared task bring this submission to the third place on subtask 1 (word relatedness), and the first place on subtask 2 (semantic relation classification), demonstrating the utility of integrating the complementary path-based and distributional information sources in recognizing semantic relatedness. Combined with a common similarity measure, LexNET performs fairly good on the word relatedness task (subtask 1). The relatively lower performance of LexNET and the various other systems on subtask 2, however, confirms the difficulty of the semantic relation classification task, and stresses the need to develop additional methods for this task.", "histories": [["v1", "Thu, 27 Oct 2016 10:49:00 GMT  (103kb)", "http://arxiv.org/abs/1610.08694v1", "5 pages, accepted to the 5th Workshop on Cognitive Aspects of the Lexicon (CogALex-V), in COLING 2016"], ["v2", "Sun, 30 Oct 2016 09:42:41 GMT  (105kb)", "http://arxiv.org/abs/1610.08694v2", "5 pages, accepted to the 5th Workshop on Cognitive Aspects of the Lexicon (CogALex-V), in COLING 2016"], ["v3", "Tue, 1 Nov 2016 10:10:01 GMT  (105kb)", "http://arxiv.org/abs/1610.08694v3", "5 pages, accepted to the 5th Workshop on Cognitive Aspects of the Lexicon (CogALex-V), in COLING 2016"]], "COMMENTS": "5 pages, accepted to the 5th Workshop on Cognitive Aspects of the Lexicon (CogALex-V), in COLING 2016", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["vered shwartz", "ido dagan"], "accepted": false, "id": "1610.08694"}, "pdf": {"name": "1610.08694.pdf", "metadata": {"source": "CRF", "title": "CogALex-V Shared Task: LexNET - Integrated Path-based and Distributional Method for the Identification of Semantic Relations", "authors": ["Vered Shwartz", "Ido Dagan"], "emails": ["vered1986@gmail.com", "dagan@cs.biu.ac.il"], "sections": [{"heading": null, "text": "ar Xiv: 161 0.08 694v 1 [cs.C L] 27 October 201 6We present a template for the CogALex 2016 Shared Task for corpus-based identification of semantic relationships using LexNET (Shwartz and Dagan, 2016), an integrated path-based and distribution-based method for semantic relationship classification. The reported results in the Shared Task place this template in third place in Subtask 1 (word-related) and first place in Subtask 2 (semantic relationship classification), demonstrating the benefits of integrating complementary path-based and distributed information sources in the recognition of semantic relationships. Combined with a common similarity measurement, LexNET performs reasonably well in the Word Relation Table (Subtask 1). However, the relatively low performance of LexNET and the various other systems in Subtask 2 confirms the difficulty of the setic relationship classification task and stresses the need to develop additional methods for this task."}, {"heading": "1 Introduction", "text": "Discovering whether words are semantically related and identifying the specific semantic relationship that exists between them is a key component in many NLP applications, such as answering questions and recognizing textual relationships (Dagan et al., 2013). Automated methods of semantic kinship are often corpus-based and rely mainly on the distribution representation of each word. However, the CogALex task based on the corpus-based identification of semantic relationships consists of two subtasks. In the first task, the system must identify for a word pair whether the words are semantically related or not (e.g. True: (dog, cat), false: (dog, fruit). In the second task, the goal is to determine the specific semantic relationship that applies to a particular pair, if any (PART OF: (tail, cat), HYPER: (cat, animal)."}, {"heading": "2 Background", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Word Relatedness", "text": "The detection of word affinities is typically addressed by distribution methods, i.e., to determine the extent to which x and y are related, a vector similarity / distance measure is applied to their distribution representations: sim (~ vwx, ~ vwy), which is a simple application of the distribution hypothesis (Harris, 1954), according to which related words occur in similar contexts and therefore exhibit similar vector representations. Most commonly, the vector cosine is used as a similarity measure (Turney et al., 2010) There are many other measurements, including, but not limited to, euclidean distance, the KL divergence (Cover and Thomas, 2012), Jaccard's coefficient (Salton and McGill, 1986) and more recently the rank of neighbors (Hare et al., 2009; Lapesa et Evert, 2013) and APSyn (Santus et al, 2012), in order to relate this problem to a classification or a non-binary problem."}, {"heading": "2.2 Semantic Relation Classification", "text": "Most corpus-based methods classify the relationship between a pair of words x and y based on the distribution representation of each word (Baroni et al., 2012; Roller et al., 2014; Fu et al., 2014; Weeds et al., 2014). Previous methods used the dependence paths linking the presence of x and y in the corpus as a keyword for the relationship between words (Hearst, 1992; Snow et al., 2004; Nakashole et al., 2012). Recently, Shwartz and Dagan (2016) introduced LexNET, a method that integrates both path-based and distribution-based information for semantic relation classification that exceeded individual approaches in several common datasets (Baroni and Lenci, 2011; Necs valescus, 2015; Santal et, 2015)."}, {"heading": "3 System Description", "text": "We use LexNET (Shwartz and Dagan, 2016), an integrated path-based and distribution-based method for semantic relation classification. In LexNET, a word pair (x, y) is represented as a feature vector consisting of both distribution-based and path-based properties: ~ vxy = [~ vwx, ~ vpaths (x, y), ~ vwy], where ~ vwx and ~ vwy are word embeddings x and y providing their distribution-based representation, and ~ vpaths (x, y) are the average embeddings vector of all dependency paths linking x and y in the corpus. Dependency paths are embedded using an LSTM (Riders and Schmidhuber, 1997), as described in Shwartz et al. (2016) This vector is then embedded in a neural network that gives the class distribution ~ vc, ~ soxy number, and the ~ point relationship (~ Mxi = 1xi) (1 tzxi). This vector is then embedded in a neural network that gives the class distribution ~ vxi, and the point number ~ soxy = 1xy (Mxi = 1xy)."}, {"heading": "3.1 A Note About Word Relatedness", "text": "While path-based approaches are commonly used to classify semantic relationships (Hearst, 1992; Snow et al., 2004; Nakashole et al., 2012; Necs ulescu et al., 2015), they are never used for word relationships that are considered a \"classic\" task for distribution methods. We argue that path-based information can also improve the performance of tasks related to word relationships (see Section 4.1). We train LexNET to distinguish between two classes: CONNECTIONS and UNCONNECTIONS, and combine them with the common cosine similarity to solve subproblem 1."}, {"heading": "4 Experimental Settings", "text": "The organizers of the split task provided a data set extracted from EVALution 1.0 (Santus et al., 2015), which was divided into training and test sets. As commanded, we trained and optimized our method on the training set and evaluated it once on the test set. To optimize the hyperparameters, we divided the training set into 90% turn and 10% validation sets. Since the data set only contains 318 different words (in the x slot), we performed the split so that the move and validation contained unique x-words.2LexNET has several tunable hyperparameters. As the underlying corpus, we used the Wikipedia corpus from Shwartz and Dagan (2016). Similar to Shwartz et al. (2016), we initialized the network's word embedding with the 50-dimensional pre-trained word embedding (Pennington et al, 2014), which were trained on Wikipedia."}, {"heading": "4.1 Subtask 1: Word Relatedness", "text": "We calculated the cosine similarity for each (x, y) pair in the dataset and normalized it to the range [0, 1]. We evaluated each (x, y) pair by combining the LexNET value for the RELATED class and the cosine similarity value: Rel (x, y) = wC \u00b7 cos (~ vwx, y) + wL \u00b7 c [RELATED] (4), where wC, wL are the weights assigned to the cosine similarities."}, {"heading": "4.2 Subtask 2: Semantic Relation Classification", "text": "The traction set of the subtask is highly unbalanced compared to random instances (about 10 times more than any other relationship), and the training of each monitored method results in overadjustment to the random class. Therefore, we trained the model only on the basis of related classes (except for RANDOM pairs), for which the classes are more balanced. During the follow-up period, we used the model from subtask 1 to assign a random value to each pair, Rel (x, y), and calculated the class distribution using the model from subtask 2, only for pairs corresponding to this score.Finally, we applied a correction that for a word pair (x, y) the difference in results between the top assessment classes is small (< 0.2), and one of the classes is SYN, then it is classified only as SYN if the number of paths is less than 3. This is due to the fact that synonyms are difficult to recognize in subdivisions."}, {"heading": "5 Results and Analysis", "text": "In addition to the two baselines provided by the shared task organizers (plurality and randomness), we also report the results of our baselines, which are detailed in Section 4. The majority of baselines classify all instances as UNREALTED (Subtask 1) or RANDOM (Subtask 2). Because these labels are excluded from the averaged F1 computations, these baseline performance metrics are all zero. Subtask 1: Word Relatedness Cos achieves pretty good performance (F1 = 0.747), and LexNET + Cos slightly improves on them. To better understand LexNET's contribution, we examined pairs that were correctly classified by LexNET + Cos while they were incorrectly classified by Cos. Of the 57 pairs that were really negative in LexNET and false positive in Cos, only one we judged as somehow related."}, {"heading": "6 Conclusion", "text": "Our system is based on LexNET (Shwartz and Dagan, 2016), an integrated path-based and distribution-based method for semantic relationship classification. LexNET was the most powerful system in Subtask 2, demonstrating the benefits of integrating complementary path-based and distribution-based information sources in the recognition of semantic relationships. We demonstrated that Subtask 1 (word kinship) achieves reasonable performance with cosmic similarity and is slightly improved in combination with LexNET, especially when the relationship between the words is not prototypical. However, the performance in Subtask 2 was relatively low for all systems participating in the shared task, including LexNET. This shows the difficulty of the classification task of semantic relationships and highlights the need to develop additional and improved methods for this task."}, {"heading": "Acknowledgments", "text": "This work was partially supported by an Intel ICRI-CI scholarship, the Israel Science Foundation scholarship 880 / 12, and the German Research Foundation through the German-Israeli Project Cooperation (DIP, scholarship DA 1600 / 1-1). 5When the random class is included in the averaged F1 score, the results are: P = 0.780, R = 0.786, F1 = 0.781."}], "references": [{"title": "How we blessed distributional semantic evaluation", "author": ["Baroni", "Lenci2011] Marco Baroni", "Alessandro Lenci"], "venue": "In GEMS Workshop on GEometrical Models of Natural Language Semantics", "citeRegEx": "Baroni et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Baroni et al\\.", "year": 2011}, {"title": "Entailment above the word level in distributional semantics", "author": ["Baroni et al.2012] Marco Baroni", "Raffaella Bernardi", "Ngoc-Quynh Do", "Chung-chieh Shan"], "venue": null, "citeRegEx": "Baroni et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Baroni et al\\.", "year": 2012}, {"title": "Elements of information theory", "author": ["Cover", "Thomas2012] Thomas M Cover", "Joy A Thomas"], "venue": null, "citeRegEx": "Cover et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Cover et al\\.", "year": 2012}, {"title": "Recognizing textual entailment", "author": ["Dagan et al.2013] Ido Dagan", "Dan Roth", "Mark Sammons"], "venue": null, "citeRegEx": "Dagan et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Dagan et al\\.", "year": 2013}, {"title": "Learning semantic hierarchies via word embeddings", "author": ["Fu et al.2014] Ruiji Fu", "Jiang Guo", "Bing Qin", "Wanxiang Che", "Haifeng Wang", "Ting Liu"], "venue": null, "citeRegEx": "Fu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Fu et al\\.", "year": 2014}, {"title": "Activating event knowledge", "author": ["Hare et al.2009] Mary Hare", "Michael Jones", "Caroline Thomson", "Sarah Kelly", "Ken McRae"], "venue": null, "citeRegEx": "Hare et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Hare et al\\.", "year": 2009}, {"title": "Automatic acquisition of hyponyms from large text corpora", "author": ["Marti A Hearst"], "venue": null, "citeRegEx": "Hearst.,? \\Q1992\\E", "shortCiteRegEx": "Hearst.", "year": 1992}, {"title": "Long short-term memory", "author": ["Hochreiter", "Schmidhuber1997] Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Evaluating neighbor rank and distance measures as predictors of semantic priming", "author": ["Lapesa", "Evert2013] Gabriella Lapesa", "Stefan Evert"], "venue": "In ACL Workshop on Cognitive Modeling and Computational Linguistics (CMCL", "citeRegEx": "Lapesa et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Lapesa et al\\.", "year": 2013}, {"title": "Measures of distributional similarity", "author": ["Lillian Lee"], "venue": null, "citeRegEx": "Lee.,? \\Q1999\\E", "shortCiteRegEx": "Lee.", "year": 1999}, {"title": "Dependency-based word embeddings", "author": ["Levy", "Goldberg2014] Omer Levy", "Yoav Goldberg"], "venue": null, "citeRegEx": "Levy et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Levy et al\\.", "year": 2014}, {"title": "Do supervised distributional methods really learn lexical inference relations", "author": ["Levy et al.2015] Omer Levy", "Steffen Remus", "Chris Biemann", "Ido Dagan"], "venue": null, "citeRegEx": "Levy et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Levy et al\\.", "year": 2015}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Ilya Sutskever", "Kai Chen", "Gregory S Corrado", "Jeffrey Dean"], "venue": null, "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Patty: a taxonomy of relational patterns with semantic types", "author": ["Gerhard Weikum", "Fabian Suchanek"], "venue": "In EMNLP and CoNLL", "citeRegEx": "Nakashole et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Nakashole et al\\.", "year": 2012}, {"title": "Reading between the lines: Overcoming data sparsity for accurate classification of lexical relationships. *SEM", "author": ["N\u00faria Bel", "Sara Mendes", "David Jurgens", "Roberto Navigli"], "venue": null, "citeRegEx": "Nec\u015fulescu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Nec\u015fulescu et al\\.", "year": 2015}, {"title": "Glove: Global vectors for word representation", "author": ["Richard Socher", "Christopher D. Manning"], "venue": null, "citeRegEx": "Pennington et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Inclusive yet selective: Supervised distributional hypernymy detection", "author": ["Katrin Erk", "Gemma Boleda"], "venue": "In COLING", "citeRegEx": "Roller et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Roller et al\\.", "year": 2014}, {"title": "Introduction to modern information retrieval", "author": ["Salton", "McGill1986] Gerard Salton", "Michael J McGill"], "venue": null, "citeRegEx": "Salton et al\\.,? \\Q1986\\E", "shortCiteRegEx": "Salton et al\\.", "year": 1986}, {"title": "Evalution 1.0: an evolving semantic dataset for training and evaluation of distributional semantic models", "author": ["Santus et al.2015] Enrico Santus", "Frances Yung", "Alessandro Lenci", "Chu-Ren Huang"], "venue": null, "citeRegEx": "Santus et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Santus et al\\.", "year": 2015}, {"title": "2016a. Testing apsyn against vector cosine on similarity estimation. PACLIC", "author": ["Emmanuele Chersoni", "Alessandro Lenci", "Chu-Ren Huang", "Philippe Blache"], "venue": null, "citeRegEx": "Santus et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Santus et al\\.", "year": 2016}, {"title": "2016b. Nine features in a random forest to learn taxonomical semantic relations", "author": ["Alessandro Lenci", "Tin-Shing Chiu", "Qin Lu", "Chu-Ren Huang"], "venue": null, "citeRegEx": "Santus et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Santus et al\\.", "year": 2016}, {"title": "Path-based vs. distributional information in recognizing lexical semantic relations", "author": ["Shwartz", "Dagan2016] Vered Shwartz", "Ido Dagan"], "venue": "Proceedings of the 5th Workshop on Cognitive Aspects of the Lexicon (CogALex-V),", "citeRegEx": "Shwartz et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Shwartz et al\\.", "year": 2016}, {"title": "Improving hypernymy detection with an integrated path-based and distributional method", "author": ["Yoav Goldberg", "Ido Dagan"], "venue": null, "citeRegEx": "Shwartz et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Shwartz et al\\.", "year": 2016}, {"title": "Learning syntactic patterns for automatic hypernym discovery", "author": ["Snow et al.2004] Rion Snow", "Daniel Jurafsky", "Andrew Y Ng"], "venue": null, "citeRegEx": "Snow et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Snow et al\\.", "year": 2004}, {"title": "From frequency to meaning: Vector space models of semantics. JAIR", "author": ["Patrick Pantel"], "venue": null, "citeRegEx": "Turney and Pantel,? \\Q2010\\E", "shortCiteRegEx": "Turney and Pantel", "year": 2010}, {"title": "Learning to distinguish hypernyms and co-hyponyms", "author": ["Weeds et al.2014] Julie Weeds", "Daoud Clarke", "Jeremy Reffin", "David Weir", "Bill Keller"], "venue": "In COLING", "citeRegEx": "Weeds et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Weeds et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 3, "context": "Discovering whether words are semantically related and identifying the specific semantic relation that holds between them is a key component in many NLP applications, such as question answering and recognizing textual entailment (Dagan et al., 2013).", "startOffset": 229, "endOffset": 249}, {"referenceID": 1, "context": "The conflict between the mediocre performance and the recent success of distributional methods on several other common datasets for semantic relation classification (Baroni et al., 2012; Weeds et al., 2014; Roller et al., 2014) could be explained by the strict evaluation setup in this subtask, which is supposed to demonstrate more closely real-world application.", "startOffset": 165, "endOffset": 227}, {"referenceID": 25, "context": "The conflict between the mediocre performance and the recent success of distributional methods on several other common datasets for semantic relation classification (Baroni et al., 2012; Weeds et al., 2014; Roller et al., 2014) could be explained by the strict evaluation setup in this subtask, which is supposed to demonstrate more closely real-world application.", "startOffset": 165, "endOffset": 227}, {"referenceID": 16, "context": "The conflict between the mediocre performance and the recent success of distributional methods on several other common datasets for semantic relation classification (Baroni et al., 2012; Weeds et al., 2014; Roller et al., 2014) could be explained by the strict evaluation setup in this subtask, which is supposed to demonstrate more closely real-world application.", "startOffset": 165, "endOffset": 227}, {"referenceID": 5, "context": "Many other measures exist, including but not limited to Euclidean distance, KL divergence (Cover and Thomas, 2012), Jaccard\u2019s coefficient (Salton and McGill, 1986), and more recently neighbor rank (Hare et al., 2009; Lapesa and Evert, 2013) and APSyn (Santus et al.", "startOffset": 197, "endOffset": 240}, {"referenceID": 1, "context": "Most corpus-based methods classify the relation between a pair of words x and y based on the distributional representation of each word (Baroni et al., 2012; Roller et al., 2014; Fu et al., 2014; Weeds et al., 2014).", "startOffset": 136, "endOffset": 215}, {"referenceID": 16, "context": "Most corpus-based methods classify the relation between a pair of words x and y based on the distributional representation of each word (Baroni et al., 2012; Roller et al., 2014; Fu et al., 2014; Weeds et al., 2014).", "startOffset": 136, "endOffset": 215}, {"referenceID": 4, "context": "Most corpus-based methods classify the relation between a pair of words x and y based on the distributional representation of each word (Baroni et al., 2012; Roller et al., 2014; Fu et al., 2014; Weeds et al., 2014).", "startOffset": 136, "endOffset": 215}, {"referenceID": 25, "context": "Most corpus-based methods classify the relation between a pair of words x and y based on the distributional representation of each word (Baroni et al., 2012; Roller et al., 2014; Fu et al., 2014; Weeds et al., 2014).", "startOffset": 136, "endOffset": 215}, {"referenceID": 6, "context": "Earlier methods utilized the dependency paths that connect the co-occurrences of x and y in the corpus as a cue to the relation between the words (Hearst, 1992; Snow et al., 2004; Nakashole et al., 2012).", "startOffset": 146, "endOffset": 203}, {"referenceID": 23, "context": "Earlier methods utilized the dependency paths that connect the co-occurrences of x and y in the corpus as a cue to the relation between the words (Hearst, 1992; Snow et al., 2004; Nakashole et al., 2012).", "startOffset": 146, "endOffset": 203}, {"referenceID": 13, "context": "Earlier methods utilized the dependency paths that connect the co-occurrences of x and y in the corpus as a cue to the relation between the words (Hearst, 1992; Snow et al., 2004; Nakashole et al., 2012).", "startOffset": 146, "endOffset": 203}, {"referenceID": 14, "context": "Recently, Shwartz and Dagan (2016) presented LexNET, a method that integrates both path-based and distributional information for semantic relation classification, which outperformed individual approaches on several common datasets (Baroni and Lenci, 2011; Nec\u015fulescu et al., 2015; Santus et al., 2015; Santus et al., 2016b).", "startOffset": 231, "endOffset": 323}, {"referenceID": 18, "context": "Recently, Shwartz and Dagan (2016) presented LexNET, a method that integrates both path-based and distributional information for semantic relation classification, which outperformed individual approaches on several common datasets (Baroni and Lenci, 2011; Nec\u015fulescu et al., 2015; Santus et al., 2015; Santus et al., 2016b).", "startOffset": 231, "endOffset": 323}, {"referenceID": 0, "context": "Most corpus-based methods classify the relation between a pair of words x and y based on the distributional representation of each word (Baroni et al., 2012; Roller et al., 2014; Fu et al., 2014; Weeds et al., 2014). Earlier methods utilized the dependency paths that connect the co-occurrences of x and y in the corpus as a cue to the relation between the words (Hearst, 1992; Snow et al., 2004; Nakashole et al., 2012). Recently, Shwartz and Dagan (2016) presented LexNET, a method that integrates both path-based and distributional information for semantic relation classification, which outperformed individual approaches on several common datasets (Baroni and Lenci, 2011; Nec\u015fulescu et al.", "startOffset": 137, "endOffset": 457}, {"referenceID": 21, "context": "Dependency paths are embedded using a LSTM (Hochreiter and Schmidhuber, 1997), as described in Shwartz et al. (2016). This vector is then fed into a neural network that outputs the class distribution ~c, and the pair is classified to the relation with the highest score r:", "startOffset": 95, "endOffset": 117}, {"referenceID": 9, "context": "See Lee (1999) for an extensive list of such measures.", "startOffset": 4, "endOffset": 15}, {"referenceID": 6, "context": "1 A Note About Word Relatedness While path-based approaches have been commonly used for semantic relation classification (Hearst, 1992; Snow et al., 2004; Nakashole et al., 2012; Nec\u015fulescu et al., 2015), they are never used for word relatedness, which is considered a \u201cclassical\u201d task for distributional methods.", "startOffset": 121, "endOffset": 203}, {"referenceID": 23, "context": "1 A Note About Word Relatedness While path-based approaches have been commonly used for semantic relation classification (Hearst, 1992; Snow et al., 2004; Nakashole et al., 2012; Nec\u015fulescu et al., 2015), they are never used for word relatedness, which is considered a \u201cclassical\u201d task for distributional methods.", "startOffset": 121, "endOffset": 203}, {"referenceID": 13, "context": "1 A Note About Word Relatedness While path-based approaches have been commonly used for semantic relation classification (Hearst, 1992; Snow et al., 2004; Nakashole et al., 2012; Nec\u015fulescu et al., 2015), they are never used for word relatedness, which is considered a \u201cclassical\u201d task for distributional methods.", "startOffset": 121, "endOffset": 203}, {"referenceID": 14, "context": "1 A Note About Word Relatedness While path-based approaches have been commonly used for semantic relation classification (Hearst, 1992; Snow et al., 2004; Nakashole et al., 2012; Nec\u015fulescu et al., 2015), they are never used for word relatedness, which is considered a \u201cclassical\u201d task for distributional methods.", "startOffset": 121, "endOffset": 203}, {"referenceID": 18, "context": "0 (Santus et al., 2015), which was split into training and test sets.", "startOffset": 2, "endOffset": 23}, {"referenceID": 15, "context": "(2016), we initialized the network\u2019s word embeddings with the 50-dimensional pre-trained GloVe word embeddings (Pennington et al., 2014), trained on Wikipedia.", "startOffset": 111, "endOffset": 136}, {"referenceID": 17, "context": "0 (Santus et al., 2015), which was split into training and test sets. As instructed, we trained and tuned our method on the training set, and evaluated it once on the test set. To tune the hyper-parameters, we split the training set to 90% train and 10% validation sets. Since the dataset contains only 318 different words (in the x slot), we performed the split such that the train and the validation contain distinct x words.2 LexNET has several tunable hyper-parameters. As an underlying corpus, we used the Wikipedia corpus from Shwartz and Dagan (2016). Similarly to Shwartz et al.", "startOffset": 3, "endOffset": 558}, {"referenceID": 17, "context": "0 (Santus et al., 2015), which was split into training and test sets. As instructed, we trained and tuned our method on the training set, and evaluated it once on the test set. To tune the hyper-parameters, we split the training set to 90% train and 10% validation sets. Since the dataset contains only 318 different words (in the x slot), we performed the split such that the train and the validation contain distinct x words.2 LexNET has several tunable hyper-parameters. As an underlying corpus, we used the Wikipedia corpus from Shwartz and Dagan (2016). Similarly to Shwartz et al. (2016), we initialized the network\u2019s word embeddings with the 50-dimensional pre-trained GloVe word embeddings (Pennington et al.", "startOffset": 3, "endOffset": 594}, {"referenceID": 15, "context": "(2016), we initialized the network\u2019s word embeddings with the 50-dimensional pre-trained GloVe word embeddings (Pennington et al., 2014), trained on Wikipedia. We fixed this hyper-parameter due to computational limitations with higher-dimensional embeddings. For each subtask, we tuned LexNET\u2019s hyper-parameters on the validation set: the number of hidden layers (0 or 1), the number of training epochs, and the word dropout rate (see Shwartz et al. (2016) for technical details).", "startOffset": 112, "endOffset": 457}, {"referenceID": 11, "context": "A random split yielded perfect results on the validation set, which were due to lexical memorization (Levy et al., 2015).", "startOffset": 101, "endOffset": 120}, {"referenceID": 12, "context": "word2vec (300 dimensions, SGNS, trained on GoogleNews) (Mikolov et al., 2013), GloVe (50-300 dimensions, trained on Wikipedia) (Pennington et al.", "startOffset": 55, "endOffset": 77}, {"referenceID": 15, "context": ", 2013), GloVe (50-300 dimensions, trained on Wikipedia) (Pennington et al., 2014), and dependency-based embeddings (Levy and Goldberg, 2014).", "startOffset": 57, "endOffset": 82}, {"referenceID": 1, "context": "We experimented with several combination methods (concatenation (Baroni et al., 2012), difference (Fu et al.", "startOffset": 64, "endOffset": 85}, {"referenceID": 4, "context": ", 2012), difference (Fu et al., 2014; Weeds et al., 2014), and ASYM (Roller et al.", "startOffset": 20, "endOffset": 57}, {"referenceID": 25, "context": ", 2012), difference (Fu et al., 2014; Weeds et al., 2014), and ASYM (Roller et al.", "startOffset": 20, "endOffset": 57}, {"referenceID": 16, "context": ", 2014), and ASYM (Roller et al., 2014)), regularization factors, and pre-trained word embeddings (Mikolov et al.", "startOffset": 18, "endOffset": 39}, {"referenceID": 12, "context": ", 2014)), regularization factors, and pre-trained word embeddings (Mikolov et al., 2013; Pennington et al., 2014; Levy and Goldberg, 2014).", "startOffset": 66, "endOffset": 138}, {"referenceID": 15, "context": ", 2014)), regularization factors, and pre-trained word embeddings (Mikolov et al., 2013; Pennington et al., 2014; Levy and Goldberg, 2014).", "startOffset": 66, "endOffset": 138}, {"referenceID": 15, "context": "To compare LexNET\u2019s performance on the validation set with other methods\u2019 performances, we adapted the distributional baseline employed by Shwartz et al. (2016) and Shwartz and Dagan (2016), where a classifier is trained on the combination of x and y\u2019s word embeddings.", "startOffset": 139, "endOffset": 161}, {"referenceID": 15, "context": "To compare LexNET\u2019s performance on the validation set with other methods\u2019 performances, we adapted the distributional baseline employed by Shwartz et al. (2016) and Shwartz and Dagan (2016), where a classifier is trained on the combination of x and y\u2019s word embeddings.", "startOffset": 139, "endOffset": 190}, {"referenceID": 1, "context": "Subtask 2: Semantic Relation Classification We note that the overall results on this task are low, in contrast to the success of several methods on common datasets (Baroni et al., 2012; Weeds et al., 2014; Roller et al., 2014; Shwartz and Dagan, 2016).", "startOffset": 164, "endOffset": 251}, {"referenceID": 25, "context": "Subtask 2: Semantic Relation Classification We note that the overall results on this task are low, in contrast to the success of several methods on common datasets (Baroni et al., 2012; Weeds et al., 2014; Roller et al., 2014; Shwartz and Dagan, 2016).", "startOffset": 164, "endOffset": 251}, {"referenceID": 16, "context": "Subtask 2: Semantic Relation Classification We note that the overall results on this task are low, in contrast to the success of several methods on common datasets (Baroni et al., 2012; Weeds et al., 2014; Roller et al., 2014; Shwartz and Dagan, 2016).", "startOffset": 164, "endOffset": 251}, {"referenceID": 11, "context": "5 Additionally, the dataset is lexically split, disabling lexical memorization (Levy et al., 2015).", "startOffset": 79, "endOffset": 98}, {"referenceID": 21, "context": "Moreover, synonyms were also sometimes mistaken with hypernyms, as the difference between the two relations is often subtle (Shwartz et al., 2016).", "startOffset": 124, "endOffset": 146}], "year": 2017, "abstractText": "We present a submission to the CogALex 2016 shared task on the corpus-based identification of semantic relations, using LexNET (Shwartz and Dagan, 2016), an integrated path-based and distributional method for semantic relation classification. The reported results in the shared task bring this submission to the third place on subtask 1 (word relatedness), and the first place on subtask 2 (semantic relation classification), demonstrating the utility of integrating the complementary path-based and distributional information sources in recognizing semantic relatedness. Combined with a common similarity measure, LexNET performs fairly good on the word relatedness task (subtask 1). The relatively lower performance of LexNET and the various other systems on subtask 2, however, confirms the difficulty of the semantic relation classification task, and stresses the need to develop additional methods for this task.", "creator": "LaTeX with hyperref package"}}}