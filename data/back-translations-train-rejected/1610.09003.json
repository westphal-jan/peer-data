{"id": "1610.09003", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Oct-2016", "title": "Cross-Modal Scene Networks", "abstract": "People can recognize scenes across many different modalities beyond natural images. In this paper, we investigate how to learn cross-modal scene representations that transfer across modalities. To study this problem, we introduce a new cross-modal scene dataset. While convolutional neural networks can categorize scenes well, they also learn an intermediate representation not aligned across modalities, which is undesirable for cross-modal transfer applications. We present methods to regularize cross-modal convolutional neural networks so that they have a shared representation that is agnostic of the modality. Our experiments suggest that our scene representation can help transfer representations across modalities for retrieval. Moreover, our visualizations suggest that units emerge in the shared representation that tend to activate on consistent concepts independently of the modality.", "histories": [["v1", "Thu, 27 Oct 2016 20:24:36 GMT  (4689kb,D)", "http://arxiv.org/abs/1610.09003v1", "See more atthis http URLarXiv admin note: text overlap witharXiv:1607.07295"]], "COMMENTS": "See more atthis http URLarXiv admin note: text overlap witharXiv:1607.07295", "reviews": [], "SUBJECTS": "cs.CV cs.LG cs.MM", "authors": ["yusuf aytar", "lluis castrejon", "carl vondrick", "hamed pirsiavash", "antonio torralba"], "accepted": false, "id": "1610.09003"}, "pdf": {"name": "1610.09003.pdf", "metadata": {"source": "CRF", "title": "Cross-Modal Scene Networks", "authors": ["Yusuf Aytar", "Lluis Castrejon", "Carl Vondrick", "Hamed Pirsiavash", "Antonio Torralba"], "emails": [], "sections": [{"heading": null, "text": "Index Terms - Modal Perception, Domain Adaptation, Scene Understanding"}, {"heading": "1 INTRODUCTION", "text": "Most people have the ability to perceive a concept in a modality, but represent it independently of the modality. This cross-modal ability allows people to perform some important abstraction tasks, such as learning in different modalities (cartoons, stories) and applying it in the real world. Unfortunately, visual representations do not yet have this modal ability. Standard approaches typically learn a separate representation for each modality that works well when it is operated within the same modality. However, the learned representations are not cross-modal, making modal transfer difficult. Two modalities are strongly aligned when we have paired data and correspondence at the object level for two images from each modality. In contrast, weak alignment is when we have only unpaired data and a rough global label that is shared in both images."}, {"heading": "Y Aytar, C Vondrick, A Torralba are with Massachusetts Institute of Technology, 77 Massachusetts Ave, Cambridge, MA 02139 USA.", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "L Castrejon is with the Department of Computer Science, University of Toronto, Ontario, Canada.", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "H Pirsiavash is with the University of Maryland Baltimore County, 1000 Hilltop Cir, ITE 342, Baltimore, MD 21250 USA", "text": "This year, we have reached a point where it can only take one year to reach an agreement."}, {"heading": "2 RELATED WORK", "text": "In fact, most people who live in the US are able to determine for themselves what they want and what they want. In fact, it is so that they are able to determine for themselves what they want and what they want. In fact, it is so that they do not do it."}, {"heading": "3 CROSS-MODAL PLACES DATASET", "text": "In fact, most people are able to move to another world in which they are in the position in which they find themselves."}, {"heading": "4 CROSS-MODAL SCENE REPRESENTATION", "text": "In this section, we describe our approach to learning intermodal scene representations. Our goal is to learn a highly coordinated representation for the different modalities in CMPlaces. Specifically, we want to learn a representation in 4, which different scene parts or concepts are represented independently of the modality. This task is difficult in part because our training data is labeled only with scene labels, rather than having one-to-one matches, which means that our approach must learn a strong alignment from poorly aligned data."}, {"heading": "4.1 Cross-Modal Scene Networks", "text": "The main modifications we introduce are that a) we have a network for each modality and b) we impose higher levels that are shared across all modalities; the motivation is to have early layers specialize in modality-specific features (such as edges in natural images, shapes in line drawings, or phrases in text), while higher layers are to capture higher-level concepts (such as objects) in a representation that is independent of modality. We show this network topology in Figure 3 with model-specific layers (white) and split layers (red); the model-specific layers each generate a convolution-specific feature map (pool5) that is then fed into the common layers (fc6 and fc7); for visual modalities we use the same revolutionary network architectures (Figure 3a) but different weights across modalities."}, {"heading": "4.2 Method A: Modality Tuning", "text": "The conventional approach to finetuning is to replace the last layer of the network with a new layer for the target task. Intuition behind fine-tuning is that the earlier layers can be shared across all vision tasks (which might otherwise be difficult without learning large amounts of data in the target task), while the later layers can specialize in the target task. We propose a modification of the fine-tuning process for the cross-modality. Instead of replacing the last layers of the network (which are task-specific), we can resort to the ModalityShared Across All ModalitiesSceneImage (a) ImagesShared Across All ModalitiesSpecialized Across All ModaliesSpecific to TextScene Skip Thought Vector (b) Description Fig. 3: Scene Networks."}, {"heading": "4.3 Method B: Statistical Regularization", "text": "Our approach is based on [14], [1] transferring statistical properties via object recognition tasks. Here, we will instead use statistical properties of activations via modalities.Let xn and yn be a training image and the scene label respectively, which we use to learn the network activations h: min w; w) to point to the hidden activations for the specified input xn, and z (xn; w) is the output of the network. During learning, we add a regularization term about hidden activations h: min w; n L (z; w), yn), i-Ri \u00b7 Ri (hi xn; w), where the first term L is the standard softmax objective and the second term R is a activations.2 The meaning of this regularization is controlled by the hyperparameters."}, {"heading": "4.4 Method C: Joint Method", "text": "The two proposed methods (A and B) work according to complementary principles and can be applied jointly during the learning of the networks. We combine both methods by first defining the common layers for a certain number of iterations, then defrosting the weights of the common layers, but now we train with the regulation of method B to encourage activations to be statistically similar across modalities and to avoid overadjustment to a particular modality."}, {"heading": "4.5 Implementation Details", "text": "We implemented our network models with the help of Caffe [20]. Both methods are based on the model described in the first half of the 20th century."}, {"heading": "5 EXPERIMENTAL RESULTS", "text": "Our goal in this paper is to learn a cross-modality representation. We show four main results that assess how well our methods solve this problem: First, we perform a cross-modal query of semantic content; second, we analyze the network's ability to detect scene categories that are outside of modality; third, we show visualizations of the learned representations that provide a qualitative measure of how this alignment is achieved; and finally, we show that we can reconstruct natural images from other modalities by using the characteristics of the aligned representation as a qualitative measure by which semantics is preserved in our cross-modal representation."}, {"heading": "5.1 Cross-Modal Retrieval", "text": "In this experiment, we test the performance of our models to retrieve content that reflects the same scene in terms of modalities. Our hypothesis is that if our representation is strongly aligned with each other, the closest neighbors in that shared representation are semantically connected and similar scenes are retrieved. We assume that the validation of each modality is based on the different levels of our intermodal representation. Then, for each type of modality, we will perform a query and calculate the cosmic distance to the individual content in the other modalities. We evaluate the documents according to the distances and calculate the Average Precision (AP) when we retrieve them, and report the APs obtained for cross-modalities."}, {"heading": "5.2 Zero-Shot Recognition and Retrieval", "text": "In fact, most of them are able to put themselves in a different world, in which they move, in which they move, in which they move, in which they move, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they, in which they live, in which they, in which they live, in which they, in which they, in which they live, in which they, in which they live, in which they live."}, {"heading": "5.3 Hidden Unit Visualizations", "text": "For visual data, we use a visualization similar to [45]. For textual descriptions, we calculate the paragraphs that maximum activate each filter, and then use tf-idf attributes to determine the most frequently relevant words in these paragraphs. Figure 6 shows how the same concept can be recognized across modalities, without explicitly referring to 9Fig. 7: t-SNE Embedding of Cross-Modal Representation: We visualize the embedding for fc7 of representations from different networks using t-SNE."}, {"heading": "5.4 Analyzing Modality Invariance", "text": "Using examples from the validation theorem, Figure 7 shows a two-dimensional embedding of the representation from our networks using t-SNE [27]. For this purpose, 1,000 samples from each modality are randomly selected and t-SNE of the fc7 characteristics is calculated. Subsequently, we color each point according to the modality. Visualization shows that the base network (without any cross-modal regularization) clearly separates the representation according to modality, which is undesirable. Statistical regularization provides a certain inventory of modality, except for text. While our representation is not completely invariant to modality, the visualization indicates that the complete approach is better at discarding modality than information on baseline."}, {"heading": "5.5 Feature Reconstructions", "text": "We use the reconstruction approach of [7] out-of-the-box, but we train the network based on our characteristics. We use pool5 characteristics as opposed to fc7 characteristics because the amount of compression of the input image in the latter results in poorer reconstructions. If concepts in our representation are aligned correctly, our hypothesis is that the reconstruction network will learn to produce images that capture the statistics of the data in the output modality while realigning modalities in similar spatial locations."}, {"heading": "6 CONCLUSION", "text": "People are able to use knowledge and experience regardless of the modality in which they perceive it, and a10Fig. 8: Inverting features across modalities: We visualize some of the images generated by our inverting network using real images. Top row: Reconstructions from real images. These preserve most of the details of the original image, but are blurred due to the low dimensionality of the pool5 representation. Second row: Reconstructions from line drawings in which the network adds colors to the reconstructions while preserving the original scene composition. Third row: Inversions from the spatial text modality. Reconstructions are less detailed, but retain roughly the location, shape and colors of the different parts of the input scene. Fourth row: Inversions from the clip art modality; and inversions from the natural image for line drawing. Similar capabilities in machines would allow several important applications in terms of retrieval and recognition. In this paper, we proposed a modal approach to learn from Google's support for intersection."}], "references": [{"title": "Part level transfer regularization for enhancing exemplar svms", "author": ["Y. Aytar", "A. Zisserman"], "venue": "In Computer Vision and Image Understanding,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2015}, {"title": "Predicting deep zero-shot convolutional neural networks using textual descriptions", "author": ["J. Ba", "K. Swersky", "S. Fidler", "R. Salakhutdinov"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2015}, {"title": "Cross-generalization: Learning novel classes from a single example by feature replacement", "author": ["E. Bart", "S. Ullman"], "venue": "In Computer Vision and Pattern Recognition,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2005}, {"title": "Curriculum learning", "author": ["Y. Bengio", "J. Louradour", "R. Collobert", "J. Weston"], "venue": "In ICML,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2009}, {"title": "Signature verification using a siamese time delay neural network", "author": ["J. Bromley", "J.W. Bentz", "L. Bottou", "I. Guyon", "Y. LeCun", "C. Moore", "E. S\u00e4ckinger", "R. Shah"], "venue": "International Journal of Pattern Recognition and Artificial Intelligence,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1993}, {"title": "Decaf: A deep convolutional activation feature for generic visual recognition", "author": ["J. Donahue", "Y. Jia", "O. Vinyals", "J. Hoffman", "N. Zhang", "E. Tzeng", "T. Darrell"], "venue": "arXiv preprint arXiv:1310.1531,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2013}, {"title": "Inverting convolutional networks with convolutional networks. arXiv", "author": ["A. Dosovitskiy", "T. Brox"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2015}, {"title": "Sketchbased image retrieval: Benchmark and bag-of-features descriptors", "author": ["M. Eitz", "K. Hildebrand", "T. Boubekeur", "M. Alexa"], "venue": "TVCG,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2011}, {"title": "Write a classifier: Zeroshot learning using purely textual descriptions", "author": ["M. Elhoseiny", "B. Saleh", "A. Elgammal"], "venue": "In ICCV,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2013}, {"title": "One-shot learning of object categories", "author": ["L. Fei-Fei", "R. Fergus", "P. Perona"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2006}, {"title": "Object classification from a single example utilizing class relevance metrics", "author": ["M. Fink"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2005}, {"title": "Predicting object dynamics in scenes", "author": ["D.F. Fouhey", "C.L. Zitnick"], "venue": "In Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2014}, {"title": "Devise: A deep visual-semantic embedding model", "author": ["A. Frome", "G.S. Corrado", "J. Shlens", "S. Bengio", "J. Dean", "T. Mikolov"], "venue": "In NIPS,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2013}, {"title": "What makes a good detector?\u2013 structured priors for learning from few examples", "author": ["T. Gao", "M. Stark", "D. Koller"], "venue": "In Computer Vision\u2013ECCV", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2012}, {"title": "Rich feature hierarchies for accurate object detection and semantic segmentation", "author": ["R. Girshick", "J. Donahue", "T. Darrell", "J. Malik"], "venue": "In CVPR,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2014}, {"title": "Domain adaptation for object recognition: An unsupervised approach", "author": ["R. Gopalan", "R. Li", "R. Chellappa"], "venue": "In ICCV,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2011}, {"title": "Canonical correlation analysis: An overview with application to learning methods", "author": ["D.R. Hardoon", "S. Szedmak", "J. Shawe-Taylor"], "venue": "Neural computation,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2004}, {"title": "Geometric context from a single image", "author": ["D. Hoiem", "A.A. Efros", "M. Hebert"], "venue": "In Computer Vision,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2005}, {"title": "Learning cross-modality similarity for multinomial data", "author": ["Y. Jia", "M. Salzmann", "T. Darrell"], "venue": "In ICCV,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2011}, {"title": "Caffe: Convolutional architecture for fast feature embedding", "author": ["Y. Jia", "E. Shelhamer", "J. Donahue", "S. Karayev", "J. Long", "R. Girshick", "S. Guadarrama", "T. Darrell"], "venue": "In Proceedings of the ACM International Conference on Multimedia,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2014}, {"title": "Undoing the damage of dataset bias", "author": ["A. Khosla", "T. Zhou", "T. Malisiewicz", "A.A. Efros", "A. Torralba"], "venue": "In ECCV,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2012}, {"title": "Unifying visualsemantic embeddings with multimodal neural language models", "author": ["R. Kiros", "R. Salakhutdinov", "R.S. Zemel"], "venue": null, "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2014}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "In NIPS,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2012}, {"title": "Learning to detect unseen object classes by between-class attribute transfer", "author": ["C.H. Lampert", "H. Nickisch", "S. Harmeling"], "venue": "In CVPR,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2009}, {"title": "Learning transferable features with deep adaptation networks", "author": ["M. Long", "J. Wang"], "venue": "arXiv preprint arXiv:1502.02791,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2015}, {"title": "Visualizing data using t-sne", "author": ["L. v. d. Maaten", "G. Hinton"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2008}, {"title": "Contextual models for object detection using boosted random fields", "author": ["K. Murphy", "W. Freeman"], "venue": null, "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2004}, {"title": "Multimodal deep learning", "author": ["J. Ngiam", "A. Khosla", "M. Kim", "J. Nam", "H. Lee", "A.Y. Ng"], "venue": "In Proceedings of the 28th international conference on machine learning", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2011}, {"title": "Zero-shot learning by convex combination of semantic embeddings", "author": ["M. Norouzi", "T. Mikolov", "S. Bengio", "Y. Singer", "J. Shlens", "A. Frome", "G.S. Corrado", "J. Dean"], "venue": null, "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2013}, {"title": "Visually indicated sounds", "author": ["A. Owens", "P. Isola", "J. McDermott", "A. Torralba", "E.H. Adelson", "W.T. Freeman"], "venue": "arXiv preprint arXiv:1512.08512,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2015}, {"title": "Zeroshot learning with semantic output codes", "author": ["M. Palatucci", "D. Pomerleau", "G.E. Hinton", "T.M. Mitchell"], "venue": "In NIPS,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2009}, {"title": "Multi-label cross-modal retrieval", "author": ["V. Ranjan", "N. Rasiwasia", "C. Jawahar"], "venue": "In Proceedings of the IEEE International Conference on Computer Vision,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2015}, {"title": "A new approach to cross-modal multimedia retrieval", "author": ["N. Rasiwasia", "J. Costa Pereira", "E. Coviello", "G. Doyle", "G.R. Lanckriet", "R. Levy", "N. Vasconcelos"], "venue": "In ICM,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2010}, {"title": "Cluster canonical correlation analysis", "author": ["N. Rasiwasia", "D. Mahajan", "V. Mahadevan", "G. Aggarwal"], "venue": "In AISTATS,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2014}, {"title": "Adapting visual category models to new domains", "author": ["K. Saenko", "B. Kulis", "M. Fritz", "T. Darrell"], "venue": "In ECCV,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2010}, {"title": "Zero-shot learning through cross-modal transfer", "author": ["R. Socher", "M. Ganjoo", "C.D. Manning", "A. Ng"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2013}, {"title": "Unbiased look at dataset bias", "author": ["A. Torralba", "A. Efros"], "venue": "In CVPR,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2011}, {"title": "Simultaneous deep transfer across domains and tasks", "author": ["E. Tzeng", "J. Hoffman", "T. Darrell", "K. Saenko"], "venue": "In Proceedings of the IEEE International Conference on Computer Vision,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2015}, {"title": "Hoggles: Visualizing object detection features", "author": ["C. Vondrick", "A. Khosla", "T. Malisiewicz", "A. Torralba"], "venue": "In Proceedings of the IEEE International Conference on Computer Vision,", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2013}, {"title": "Learning visual biases from human imagination", "author": ["C. Vondrick", "H. Pirsiavash", "A. Oliva", "A. Torralba"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2015}, {"title": "Sketch-based 3d shape retrieval using convolutional neural networks", "author": ["F. Wang", "L. Kang", "Y. Li"], "venue": null, "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2015}, {"title": "Sun database: Large-scale scene recognition from abbey to zoo", "author": ["J. Xiao", "J. Hays", "K. Ehinger", "A. Oliva", "A. Torralba"], "venue": "In CVPR,", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2010}, {"title": "Show, attend and tell: Neural image caption generation with visual attention", "author": ["K. Xu", "J. Ba", "R. Kiros", "A. Courville", "R. Salakhutdinov", "R. Zemel", "Y. Bengio"], "venue": "arXiv preprint arXiv:1502.03044,", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2015}, {"title": "Object detectors emerge in deep scene cnns", "author": ["B. Zhou", "A. Khosla", "A. Lapedriza", "A. Oliva", "A. Torralba"], "venue": null, "citeRegEx": "45", "shortCiteRegEx": "45", "year": 2014}, {"title": "Learning deep features for scene recognition using places database", "author": ["B. Zhou", "A. Lapedriza", "J. Xiao", "A. Torralba", "A. Oliva"], "venue": "In NIPS,", "citeRegEx": "46", "shortCiteRegEx": "46", "year": 2014}, {"title": "Bringing semantics into focus using visual abstraction", "author": ["C.L. Zitnick", "D. Parikh"], "venue": "In CVPR,", "citeRegEx": "47", "shortCiteRegEx": "47", "year": 2013}], "referenceMentions": [{"referenceID": 34, "context": "Our approach builds on a foundation of domain adaptation [36], [16] and multi-modal learning [13], [30], [37] methods in computer vision.", "startOffset": 57, "endOffset": 61}, {"referenceID": 15, "context": "Our approach builds on a foundation of domain adaptation [36], [16] and multi-modal learning [13], [30], [37] methods in computer vision.", "startOffset": 63, "endOffset": 67}, {"referenceID": 12, "context": "Our approach builds on a foundation of domain adaptation [36], [16] and multi-modal learning [13], [30], [37] methods in computer vision.", "startOffset": 93, "endOffset": 97}, {"referenceID": 28, "context": "Our approach builds on a foundation of domain adaptation [36], [16] and multi-modal learning [13], [30], [37] methods in computer vision.", "startOffset": 99, "endOffset": 103}, {"referenceID": 35, "context": "Our approach builds on a foundation of domain adaptation [36], [16] and multi-modal learning [13], [30], [37] methods in computer vision.", "startOffset": 105, "endOffset": 109}, {"referenceID": 34, "context": "[36] proposes a method for domain adaptation using metric learning.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "In [16] this approach is extended to work on unsupervised settings where one does not have access to target data labels, while [39] uses deep CNNs instead.", "startOffset": 3, "endOffset": 7}, {"referenceID": 37, "context": "In [16] this approach is extended to work on unsupervised settings where one does not have access to target data labels, while [39] uses deep CNNs instead.", "startOffset": 127, "endOffset": 131}, {"referenceID": 36, "context": "[38] shows the biases inherent in common vision datasets and [21] proposes models that remain invariant to them.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "[38] shows the biases inherent in common vision datasets and [21] proposes models that remain invariant to them.", "startOffset": 61, "endOffset": 65}, {"referenceID": 24, "context": "[26] learns an aligned representation for domain adaptation using CNNs and the MMD metric.", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "One-Shot/Zero-Shot Learning: One-shot learning techniques [10] have been developed to learn classifiers from a single or a few examples, mostly by reusing classifier parameters [11], using contextual information [28], [18] or sharing part detectors [3].", "startOffset": 58, "endOffset": 62}, {"referenceID": 10, "context": "One-Shot/Zero-Shot Learning: One-shot learning techniques [10] have been developed to learn classifiers from a single or a few examples, mostly by reusing classifier parameters [11], using contextual information [28], [18] or sharing part detectors [3].", "startOffset": 177, "endOffset": 181}, {"referenceID": 26, "context": "One-Shot/Zero-Shot Learning: One-shot learning techniques [10] have been developed to learn classifiers from a single or a few examples, mostly by reusing classifier parameters [11], using contextual information [28], [18] or sharing part detectors [3].", "startOffset": 212, "endOffset": 216}, {"referenceID": 17, "context": "One-Shot/Zero-Shot Learning: One-shot learning techniques [10] have been developed to learn classifiers from a single or a few examples, mostly by reusing classifier parameters [11], using contextual information [28], [18] or sharing part detectors [3].", "startOffset": 218, "endOffset": 222}, {"referenceID": 2, "context": "One-Shot/Zero-Shot Learning: One-shot learning techniques [10] have been developed to learn classifiers from a single or a few examples, mostly by reusing classifier parameters [11], using contextual information [28], [18] or sharing part detectors [3].", "startOffset": 249, "endOffset": 252}, {"referenceID": 23, "context": "In a similar fashion, zero-shot learning [25], [32], [9], [2], [41] addresses the problem of learning new classifiers without training examples in a given domain, e.", "startOffset": 41, "endOffset": 45}, {"referenceID": 30, "context": "In a similar fashion, zero-shot learning [25], [32], [9], [2], [41] addresses the problem of learning new classifiers without training examples in a given domain, e.", "startOffset": 47, "endOffset": 51}, {"referenceID": 8, "context": "In a similar fashion, zero-shot learning [25], [32], [9], [2], [41] addresses the problem of learning new classifiers without training examples in a given domain, e.", "startOffset": 53, "endOffset": 56}, {"referenceID": 1, "context": "In a similar fashion, zero-shot learning [25], [32], [9], [2], [41] addresses the problem of learning new classifiers without training examples in a given domain, e.", "startOffset": 58, "endOffset": 61}, {"referenceID": 39, "context": "In a similar fashion, zero-shot learning [25], [32], [9], [2], [41] addresses the problem of learning new classifiers without training examples in a given domain, e.", "startOffset": 63, "endOffset": 67}, {"referenceID": 7, "context": "[8] focuses on recovering semantically related natural images to a given sketch query and [42] uses query sketches to recover 3D shapes.", "startOffset": 0, "endOffset": 3}, {"referenceID": 40, "context": "[8] focuses on recovering semantically related natural images to a given sketch query and [42] uses query sketches to recover 3D shapes.", "startOffset": 90, "endOffset": 94}, {"referenceID": 18, "context": "[19] uses an MRF of topic models to retrieve images using text, while [34] models the correlations between visual SIFT features and text hidden topic models to retrieve media across both domains.", "startOffset": 0, "endOffset": 4}, {"referenceID": 32, "context": "[19] uses an MRF of topic models to retrieve images using text, while [34] models the correlations between visual SIFT features and text hidden topic models to retrieve media across both domains.", "startOffset": 70, "endOffset": 74}, {"referenceID": 16, "context": "CCA [17] and variants [35] are commonly employed methods in cross-modal content retrieval.", "startOffset": 4, "endOffset": 8}, {"referenceID": 33, "context": "CCA [17] and variants [35] are commonly employed methods in cross-modal content retrieval.", "startOffset": 22, "endOffset": 26}, {"referenceID": 12, "context": "[13], [30] learn a semantic embedding that joins representations from a CNN trained on ImageNet and distributed word representations.", "startOffset": 0, "endOffset": 4}, {"referenceID": 28, "context": "[13], [30] learn a semantic embedding that joins representations from a CNN trained on ImageNet and distributed word representations.", "startOffset": 6, "endOffset": 10}, {"referenceID": 21, "context": "[22], [44] extend them to include a decoder that maps common representations to captions.", "startOffset": 0, "endOffset": 4}, {"referenceID": 42, "context": "[22], [44] extend them to include a decoder that maps common representations to captions.", "startOffset": 6, "endOffset": 10}, {"referenceID": 35, "context": "[37] maps visual features to a word semantic embedding.", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": "Another group of works incorporate sound as another modality [29], [31].", "startOffset": 61, "endOffset": 65}, {"referenceID": 29, "context": "Another group of works incorporate sound as another modality [29], [31].", "startOffset": 67, "endOffset": 71}, {"referenceID": 45, "context": "Learning from Visual Abstraction: [47] introduced clipart images for visual abstraction.", "startOffset": 34, "endOffset": 38}, {"referenceID": 11, "context": "[12] learns dynamics and [48] learns sentence phrases in this abstract world and transfer them to natural images.", "startOffset": 0, "endOffset": 4}, {"referenceID": 43, "context": "Above, we also show masks of inputs that activate specific units the most [45].", "startOffset": 74, "endOffset": 78}, {"referenceID": 44, "context": "We use the same list of 205 scene categories as Places [46], which is one of the largest scene datasets available today.", "startOffset": 55, "endOffset": 59}, {"referenceID": 44, "context": "For each modality we select 10 random examples in each of the 205 categories for the validation set and the rest for the training set, except for natural images for which we employ the training and validation splits from [46] containing 2.", "startOffset": 221, "endOffset": 225}, {"referenceID": 44, "context": "Natural Images: We use images from the Places 205 Database [46] to form the natural images modality.", "startOffset": 59, "endOffset": 63}, {"referenceID": 45, "context": "This dataset complements other cartoon datasets [47], but focuses on scenes.", "startOffset": 48, "endOffset": 52}, {"referenceID": 41, "context": "We automatically construct this dataset using images from SUN [43] and its annotated objects.", "startOffset": 62, "endOffset": 66}, {"referenceID": 22, "context": "We extend single-modality classification networks [24] in order to handle multiple modalities.", "startOffset": 50, "endOffset": 54}, {"referenceID": 4, "context": "Note that, in contrast to siamese networks [5], our architecture allows learning alignments without paired data.", "startOffset": 43, "endOffset": 46}, {"referenceID": 5, "context": "Our first approach is inspired by finetuning, which is a popular method for transfer learning with deep architectures [6], [15], [46].", "startOffset": 118, "endOffset": 121}, {"referenceID": 14, "context": "Our first approach is inspired by finetuning, which is a popular method for transfer learning with deep architectures [6], [15], [46].", "startOffset": 123, "endOffset": 127}, {"referenceID": 44, "context": "Our first approach is inspired by finetuning, which is a popular method for transfer learning with deep architectures [6], [15], [46].", "startOffset": 129, "endOffset": 133}, {"referenceID": 44, "context": "a) For pixel based modalities, we use a CNN based off [46] to produce pool5.", "startOffset": 54, "endOffset": 58}, {"referenceID": 43, "context": "Places is a reasonable representation to start with because [45] shows that high-level concepts (objects) emerge in the later layers.", "startOffset": 60, "endOffset": 64}, {"referenceID": 3, "context": "Our approach is a form of curriculum learning [4].", "startOffset": 46, "endOffset": 49}, {"referenceID": 13, "context": "Our approach builds upon [14], [1] who transfer statistical properties across object detection tasks.", "startOffset": 25, "endOffset": 29}, {"referenceID": 0, "context": "Our approach builds upon [14], [1] who transfer statistical properties across object detection tasks.", "startOffset": 31, "endOffset": 34}, {"referenceID": 19, "context": "We implemented our network models using Caffe [20].", "startOffset": 46, "endOffset": 50}, {"referenceID": 22, "context": "Both our methods build on top of the model described in [24], with the modification that the activations from layers pool5 onwards are shared across modalities, and layers before are modal-specific.", "startOffset": 56, "endOffset": 60}, {"referenceID": 44, "context": "For each model we have a separate CNN initialized using the weights of Places-CNN [46].", "startOffset": 82, "endOffset": 86}, {"referenceID": 22, "context": "The CNNs follow the AlexNet [24] architecture and are initialized with the weights of Places-CNN.", "startOffset": 28, "endOffset": 32}, {"referenceID": 33, "context": "Cluster CCA [35] works for class-level alignments, but the formulation is intended for only two modalities.", "startOffset": 12, "endOffset": 16}, {"referenceID": 16, "context": "On the other hand, Generalized CCA [17] does work for multiple modalities but still requires sample-level alignments.", "startOffset": 35, "endOffset": 39}, {"referenceID": 31, "context": "Concurrent work with ours extends CCA to multi-label settings [33].", "startOffset": 62, "endOffset": 66}, {"referenceID": 43, "context": "6: Visualizing Unit Activations: We visualize pool5 in our cross-modal representation above by finding masks of images/descriptions that activate a specific unit the most [45].", "startOffset": 171, "endOffset": 175}, {"referenceID": 43, "context": "For visual data, we use a visualization similar to [45].", "startOffset": 51, "endOffset": 55}, {"referenceID": 25, "context": "7: t-SNE Embedding of Cross-Modal Representation: We visualize the embedding for fc7 of representations from different networks using t-SNE [27].", "startOffset": 140, "endOffset": 144}, {"referenceID": 25, "context": "Using examples from the validation set, Figure 7 shows a two-dimensional embedding of the representation from our networks using t-SNE [27].", "startOffset": 135, "endOffset": 139}, {"referenceID": 38, "context": "The motivation is to gain some visual understanding of which concepts are preserved across modalities and which information is discarded [40].", "startOffset": 137, "endOffset": 141}, {"referenceID": 6, "context": "We use the reconstruction approach from [7] out-of-the-box, but we train the network using our features.", "startOffset": 40, "endOffset": 43}, {"referenceID": 6, "context": "We refer readers to [7] for full details.", "startOffset": 20, "endOffset": 23}, {"referenceID": 6, "context": "However, our reconstructions have similar quality to those in [7] when reconstructing from pool5 features within a modality.", "startOffset": 62, "endOffset": 65}], "year": 2016, "abstractText": "People can recognize scenes across many different modalities beyond natural images. In this paper, we investigate how to learn cross-modal scene representations that transfer across modalities. To study this problem, we introduce a new cross-modal scene dataset. While convolutional neural networks can categorize scenes well, they also learn an intermediate representation not aligned across modalities, which is undesirable for cross-modal transfer applications. We present methods to regularize cross-modal convolutional neural networks so that they have a shared representation that is agnostic of the modality. Our experiments suggest that our scene representation can help transfer representations across modalities for retrieval. Moreover, our visualizations suggest that units emerge in the shared representation that tend to activate on consistent concepts independently of the modality.", "creator": "LaTeX with hyperref package"}}}