{"id": "1610.09091", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Oct-2016", "title": "Representation Learning Models for Entity Search", "abstract": "We focus on the problem of learning distributed representations for entity search queries, named entities, and their short descriptions. With our representation learning models, the entity search query, named entity and description can be represented as low-dimensional vectors. Our goal is to develop a simple but effective model that can make the distributed representations of query related entities similar to the query in the vector space. Hence, we propose three kinds of learning strategies, and the difference between them mainly lies in how to deal with the relationship between an entity and its description. We analyze the strengths and weaknesses of each learning strategy and validate our methods on public datasets which contain four kinds of named entities, i.e., movies, TV shows, restaurants and celebrities. The experimental results indicate that our proposed methods can adapt to different types of entity search queries, and outperform the current state-of-the-art methods based on keyword matching and vanilla word2vec models. Besides, the proposed methods can be trained fast and be easily extended to other similar tasks.", "histories": [["v1", "Fri, 28 Oct 2016 06:33:33 GMT  (1954kb)", "http://arxiv.org/abs/1610.09091v1", "submitted to WWW2017"], ["v2", "Tue, 20 Dec 2016 02:19:01 GMT  (0kb,I)", "http://arxiv.org/abs/1610.09091v2", "This paper has been withdrawn by the author because the proposed model need to be re-evaluate"], ["v3", "Sun, 15 Jan 2017 13:57:23 GMT  (0kb,I)", "http://arxiv.org/abs/1610.09091v3", "This paper has been withdrawn by the author because the proposed model need to be re-evaluate"]], "COMMENTS": "submitted to WWW2017", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["shijia e", "yang xiang", "mohan zhang"], "accepted": false, "id": "1610.09091"}, "pdf": {"name": "1610.09091.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Yang Xiang", "Mohan Zhang"], "emails": ["436_eshijia@tongji.edu.cn", "shxiangyang@tongji.edu.cn", "1631600@tongji.edu.cn"], "sections": [{"heading": null, "text": "ar Xiv: 161 0.09 091v 1 [cs.C L] 28 Oct 201 6CCS Concepts \u2022 Information Systems \u2192 Ranking learning; Query presentation; Language models; Answering questions; Keywords Representation learning; Entity search; Language models, Entity embedding"}, {"heading": "1. INTRODUCTION", "text": "In fact, most of them are able to determine for themselves what they want and what they want."}, {"heading": "2. RELATED WORK", "text": "Our work relates to the Neural Network Language Model (NNLM) and Q & A response selection methods, the core of which is the use of distributed word representation, and has lately been applied to several Natural Language Processing (NLP) tasks. Also, the idea of distributed word representation can be generalized to sentences, paragraphs, or even documents [9]. [8] Compares different word embedding models with different tasks. It is a good guide for word embedding training. From this work, we know that the corpus domain is indispensable for generating meaningful word embedding for a given task. [10] and [3] suggest a CNN-based method with word embedding to solve the problem of short text classification."}, {"heading": "2.1 NNLM", "text": "A language model for neural networks predicts the probability distribution of the next word using several previous words [1]. For a training sample (w1, w2,..., wk) in the corpus, the goal of the model is to maximize the log probability of p (wk | w1, w2,..., wk \u2212 1) (1), where wk (the t word in the input sequence) is the target word we have to predict. Figure 1 shows the basic structure of the NNLM. In this model, the previous words are collectively referred to as context to the word wk, and the model concatenates the embedding of the context as input. The output softmax layer consists of N units, where N is the vocabulary size of the corpus, and the model attempts to predict the target word wk with the highest probability. The challenge of the basic model is that the computational overhead, in contrast to training & word hierarchy, is a W, we place the W in the context, and W is the contrasting word."}, {"heading": "2.2 Answer Selection", "text": "In addition to the distributed representation of questions and answers, it is also important to provide a measurement to measure the degree of agreement of the Q & A pairs. [16] The general framework for the solution is shown in Figure 2. [6] It is a framework based on CNN. Questions and Answers share the same CNN layers to represent the characteristics. It is also attempted to measure several common similarities such as cosmic similarity. [16] Considers CNN's shortcomings and uses the LSTM to model the Q & A pairs. LSTM is essentially a recursive neural network (RNN), and the learned characteristics can effectively maintain the word order to further improve the overall performance of the model."}, {"heading": "3. ENTITY SEARCH MODELS", "text": "In this section, we present our representative learning model for entity search tasks, which is inspired by distributed word representations and deep neural network response selection. We learn distributed representations for entity search queries, candidates and their descriptions in a low-dimensional vector space. In contrast to the typical answer selection task presented by [6] in the research field of questions and answers, we use various embedding strategies and take into account the short-text character of entities and descriptions. In addition, we consider the entity description as a bridge between the named entity and the search query. It is worth noting that the proposed model can still perform well if the entity description is missing."}, {"heading": "3.1 Problem Definition", "text": "Definition 1. Entity search query. The entity search query reflects the query intention. Let q = (v1, v2,.., vi) denote the entity search query, where v is a single word in the vocabulary list V. Let e = (v1, v2,.., vj) denote the embedding of a named entity. j is the sequence length by word segmentation. Definition 2. Named entity. A named entity is something that exists in itself. Let e = (v1, v2,...) the word sequence of a named entity. j is the sequence length. The reason why a named entity is considered as a sequence of words, not just an independent entity, is that some named entities have a certain description."}, {"heading": "3.2 Model Architecture", "text": "This year, it has come to the point that it has never come as far as it has this year."}, {"heading": "4. EXPERIMENTS", "text": "In this section we describe various experiments on the evalu algorithm 1 Translation with Description EmbeddingInput: All q in training set and their corresponding (e +, d +) and (e \u2212, d \u2212). margin m, embeddings dim n. 1: v \u2190 uniform (\u2212 0.05, 0.05) for each v \u00b2 V 2: repeat 3: qbatch \u2190 (q1, q2,..., qb size) / / sample a mini-batch of size b size 4: e + batch \u2190 (e + 1, e + 2,., e + b size) 5: d + batch \u2190 (d + 1, d + 2,.., d + b size) 6: Trainbatch Vegas 7: for all q \u00b2 qbatch do 8: e \u2212 batch \u2190 (e \u2212 1, e \u2212 2,."}, {"heading": "4.1 Datasets", "text": "The datasets used in our experiments come from Baidu Cup \"16. It consists of four types of information that are not able to find information (as in Table 1).tvShow. In this dataset list is the number of data collected in terms of the way they are selected in terms of the way they are picked in terms of the way they are picked in terms of the way they are picked in terms of the way they are picked in terms of the way they are picked in terms of the way they are picked in terms of the way they are picked in terms of the way they are picked in terms of the way they are picked in terms of the way they are picked in terms of the way they are picked in terms of the way they are picked in terms of the way they are picked in terms of the way they are picked in terms of the way they are picked out in terms of the way they are picked in terms of the way they are picked in terms of the way they are picked in terms of the way they are picked out in terms of the way they are picked in terms of the way they are picked in terms of the way they are picked out in terms of the way in terms of the way they are picked out in terms of the way they in terms of the way they are picked out in terms of the way they in terms of the way they in terms of the way they are picked out in terms of the way they in terms of the way they in terms of the way they are picked out in terms of the way in terms of the way they are picked out in terms of the way they in terms of the way they in terms of the way they in terms of the way they are picked out in terms of the way in terms of the way they in terms of the way they in terms of the way they are picked out in terms of the manner in terms of the way they in terms of the way they are picked out in terms of the manner in terms of the way they in terms of the way they are picked out in terms of the way they in terms of the way they in terms of the way they in terms of the way they in terms of the manner in the manner in the manner in the manner in the manner in the manner in the manner in the manner in the way they are picked out in the manner in the manner in the manner in the manner in the manner in the manner in the manner in the manner in the manner in the manner in the manner"}, {"heading": "4.3 Results", "text": "This year, it has reached the stage where it will be able to take the lead."}, {"heading": "5. CONCLUSIONS", "text": "With three embedding strategies, our proposed models can be adapted to different types of entity query tasks. Experimental results on four different sets of data confirm the superiority of the proposed models, and we also give an analysis of how to select a suitable optimizer to train the representation models.The core idea of our models is to consider the relationships between named entities, entity descriptions and entity queries in low-dimensional vector space.We also pay attention to the short text properties of the same. The overall framework in this essay is language-independent and can easily be generalized to other similar tasks. There are many potential future research directions for this work. A significant trend is the combination with more external data sources, such as knowledge base, to obtain more entity characteristics and thus improve the overall performance of the models.Our work is an excellent guide to the use of entity learning methods to solve the entity search."}, {"heading": "6. ACKNOWLEDGMENTS", "text": "This work was supported by the National Basic Research Programme of China (2014CB340404), the National Natural Science Foundation of China (71571136), the Shanghai City Science and Technology Commission (16JC1403000) and the Shanghai Municipal Science and Technology Research Project (14511108002). We thank Jianting Chen for useful discussions."}, {"heading": "7. REFERENCES", "text": "[1] Y. Bengio, R. Ducharme, P. Vincent, and C. Jauvin.A neural probabilistic language model. journal of machine learning research, 3 (Feb): 1137-1155, 2003. [2] R. Collobert and J. Weston. A unified architecture for natural language processing: Deep neural networks with multitask learning. In Proceedings of the 25th International Conference on Machine learning, pp. 160-167. ACM, 2008. [3] C. N. dos Santos and M. Gatti. Deep convolutional neural networks for sentiment analysis of short texts. In COLING, pp. 69-78, 2014. [4] J. Duchi, E. Hazan, and Y. Xi. Singer. Adaptive subgradient methods for online learning and stochastic optimization. Journal of Machine Learning Research, 12 (Jul): 2121-2159, 2011. [5] M. Fan, Q. Zhou, E. Chang, and T. Zheng."}], "references": [{"title": "A neural probabilistic language model", "author": ["Y. Bengio", "R. Ducharme", "P. Vincent", "C. Jauvin"], "venue": "journal of machine learning research,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2003}, {"title": "A unified architecture for natural language processing: Deep neural networks with multitask learning", "author": ["R. Collobert", "J. Weston"], "venue": "In Proceedings of the 25th international conference on Machine learning,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2008}, {"title": "Deep convolutional neural networks for sentiment analysis of short texts", "author": ["C.N. dos Santos", "M. Gatti"], "venue": "In COLING,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2014}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["J. Duchi", "E. Hazan", "Y. Singer"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2011}, {"title": "Transition-based knowledge graph embedding with relational mapping properties", "author": ["M. Fan", "Q. Zhou", "E. Chang", "T.F. Zheng"], "venue": "In Proceedings of the 28th Pacific Asia Conference on Language,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2014}, {"title": "Applying deep learning to answer selection: A study and an open task", "author": ["M. Feng", "B. Xiang", "M.R. Glass", "L. Wang", "B. Zhou"], "venue": "In 2015 IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU),", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2015}, {"title": "Adam: A method for stochastic optimization", "author": ["D. Kingma", "J. Ba"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2014}, {"title": "How to generate a good word embedding", "author": ["S. Lai", "K. Liu", "S. He", "J. Zhao"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2015}, {"title": "Distributed representations of sentences and documents", "author": ["Q.V. Le", "T. Mikolov"], "venue": "In ICML,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2014}, {"title": "Sequential short-text classification with recurrent and convolutional neural networks", "author": ["J.Y. Lee", "F. Dernoncourt"], "venue": "arXiv preprint arXiv:1603.03827,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2016}, {"title": "Ranking friendly result composition for xml keyword search", "author": ["Z. Liu", "Y. Cai", "Y. Shan", "Y. Chen"], "venue": "In International Conference on Conceptual Modeling,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2015}, {"title": "Efficient estimation of word representations in vector space", "author": ["T. Mikolov", "K. Chen", "G. Corrado", "J. Dean"], "venue": "arXiv preprint arXiv:1301.3781,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2013}, {"title": "A fast and simple algorithm for training neural probabilistic language models", "author": ["A. Mnih", "Y.W. Teh"], "venue": "arXiv preprint arXiv:1206.6426,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2012}, {"title": "Hierarchical probabilistic neural network language model", "author": ["F. Morin", "Y. Bengio"], "venue": "In Aistats,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2005}, {"title": "Dropout: a simple way to prevent neural networks from overfitting", "author": ["N. Srivastava", "G.E. Hinton", "A. Krizhevsky", "I. Sutskever", "R. Salakhutdinov"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1929}, {"title": "Lstm-based deep learning models for non-factoid answer selection", "author": ["M. Tan", "B. Xiang", "B. Zhou"], "venue": "arXiv preprint arXiv:1511.04108,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2015}, {"title": "Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude", "author": ["T. Tieleman", "G. Hinton"], "venue": "COURSERA: Neural Networks for Machine Learning,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2012}, {"title": "Clustering user queries of a search engine", "author": ["J.-R. Wen", "J.-Y. Nie", "H.-J. Zhang"], "venue": "In Proceedings of the 10th international conference on World Wide Web,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2001}, {"title": "Beyond query: Interactive user intention understanding", "author": ["Y. Yang", "J. Tang"], "venue": "In Data Mining (ICDM),", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2015}, {"title": "Adadelta: an adaptive learning rate method", "author": ["M.D. Zeiler"], "venue": "arXiv preprint arXiv:1212.5701,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2012}], "referenceMentions": [{"referenceID": 18, "context": "In the field of the search engine, how to make the search system understand the user intentions behind the user queries is a crucial question [19].", "startOffset": 142, "endOffset": 146}, {"referenceID": 17, "context": "An intelligent search system should meet either precise or vague requirement from users [18].", "startOffset": 88, "endOffset": 92}, {"referenceID": 0, "context": "The probability distribution of observing a word depends on some fixed number of surrounding words with neural language models (NLM) [1].", "startOffset": 133, "endOffset": 136}, {"referenceID": 11, "context": "The performance of the proposed framework is better than the traditional keyword matching and the vanilla word embedding method [12].", "startOffset": 128, "endOffset": 132}, {"referenceID": 5, "context": "Our work is most related to [6], and it learns the vector representations of questions and answers", "startOffset": 28, "endOffset": 31}, {"referenceID": 6, "context": "We optimized the models using adaptive moment estimation (Adam) [7] which is an enhanced variant of stochastic gradient descent.", "startOffset": 64, "endOffset": 67}, {"referenceID": 8, "context": "Also, the idea of distributed word representations can be generalized to model sentences, paragraphs or even documents [9].", "startOffset": 119, "endOffset": 122}, {"referenceID": 7, "context": "[8] compares various word embedding models on different tasks.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "[10] and [3] propose a CNN based method with word embeddings to solve the short text classification task.", "startOffset": 0, "endOffset": 4}, {"referenceID": 2, "context": "[10] and [3] propose a CNN based method with word embeddings to solve the short text classification task.", "startOffset": 9, "endOffset": 12}, {"referenceID": 5, "context": "[6] and [16] design a few architectures of DNNs using CNN and longshort term memory networks (LSTM) to solve the problem.", "startOffset": 0, "endOffset": 3}, {"referenceID": 15, "context": "[6] and [16] design a few architectures of DNNs using CNN and longshort term memory networks (LSTM) to solve the problem.", "startOffset": 8, "endOffset": 12}, {"referenceID": 0, "context": "A neural networks language model predicts the probability distribution of the next word utilizing several previous words [1].", "startOffset": 121, "endOffset": 124}, {"referenceID": 13, "context": "Hence, [14] and [13] use hierarchical softmax and noise contrastive estimation respectively to help reduce the training duration.", "startOffset": 7, "endOffset": 11}, {"referenceID": 12, "context": "Hence, [14] and [13] use hierarchical softmax and noise contrastive estimation respectively to help reduce the training duration.", "startOffset": 16, "endOffset": 20}, {"referenceID": 1, "context": "[2] proposed a model known as the C&W model where the central word in a sequence is the target word, and the surrounding words are put together into a context.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[6] presents a framework based on CNN.", "startOffset": 0, "endOffset": 3}, {"referenceID": 15, "context": "[16] considers the shortcomings of CNN, and adopts the LSTM to model the Q&A pairs.", "startOffset": 0, "endOffset": 4}, {"referenceID": 5, "context": "Unlike the typical answer selection task presented by [6] in Q&A research area, we exploit various embedding strategies and consider the short text character of entities and descriptions.", "startOffset": 54, "endOffset": 57}, {"referenceID": 14, "context": "The dropout layer is a regularization technique for reducing overfitting by preventing complex coadaptations on training data [15].", "startOffset": 126, "endOffset": 130}, {"referenceID": 4, "context": "Another important reason we use cosine similarity is that the inner product facilitates the derivation of gradients [5].", "startOffset": 116, "endOffset": 119}, {"referenceID": 10, "context": "Baseline 1 is based on the keyword matching method (KWM) used in many search systems in practice, such as [11].", "startOffset": 106, "endOffset": 110}, {"referenceID": 11, "context": "Baseline 2 utilizes vanilla word embeddings (W2V) trained by word2vec [12], i.", "startOffset": 70, "endOffset": 74}, {"referenceID": 3, "context": "The performance of vanilla SGD [4] is poor.", "startOffset": 31, "endOffset": 34}, {"referenceID": 3, "context": "Adagrad [4] adapts the learning rate to the parameters, performing larger updates for infrequent and smaller updates for frequent parameters, and Figure 4: Training loss vs epochs with various opti-", "startOffset": 8, "endOffset": 11}, {"referenceID": 19, "context": "Adadelta [20] and RMSprop [17] are both extensions of Adagrad, and they seek to reduce the aggressive, monotonically decreasing learning rate.", "startOffset": 9, "endOffset": 13}, {"referenceID": 16, "context": "Adadelta [20] and RMSprop [17] are both extensions of Adagrad, and they seek to reduce the aggressive, monotonically decreasing learning rate.", "startOffset": 26, "endOffset": 30}, {"referenceID": 6, "context": "Adam [7] also calculates adaptive learning rates for each parameter.", "startOffset": 5, "endOffset": 8}], "year": 2016, "abstractText": "We focus on the problem of learning distributed representations for entity search queries, named entities, and their short descriptions. With our representation learning models, the entity search query, named entity and description can be represented as low-dimensional vectors. Our goal is to develop a simple but effective model that can make the distributed representations of query related entities similar to the query in the vector space. Hence, we propose three kinds of learning strategies, and the difference between them mainly lies in how to deal with the relationship between an entity and its description. We analyze the strengths and weaknesses of each learning strategy and validate our methods on public datasets which contain four kinds of named entities, i.e., movies, TV shows, restaurants and celebrities. The experimental results indicate that our proposed methods can adapt to different types of entity search queries, and outperform the current state-of-the-art methods based on keyword matching and vanilla word2vec models. Besides, the proposed methods can be trained fast and be easily extended to other similar tasks.", "creator": "LaTeX with hyperref package"}}}