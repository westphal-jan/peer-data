{"id": "1610.09158", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Oct-2016", "title": "Towards a continuous modeling of natural language domains", "abstract": "Humans continuously adapt their style and language to a variety of domains. However, a reliable definition of `domain' has eluded researchers thus far. Additionally, the notion of discrete domains stands in contrast to the multiplicity of heterogeneous domains that humans navigate, many of which overlap. In order to better understand the change and variation of human language, we draw on research in domain adaptation and extend the notion of discrete domains to the continuous spectrum. We propose representation learning-based models that can adapt to continuous domains and detail how these can be used to investigate variation in language. To this end, we propose to use dialogue modeling as a test bed due to its proximity to language modeling and its social component.", "histories": [["v1", "Fri, 28 Oct 2016 10:26:40 GMT  (3086kb,D)", "http://arxiv.org/abs/1610.09158v1", "5 pages, 3 figures, published in Uphill Battles in Language Processing workshop, EMNLP 2016"]], "COMMENTS": "5 pages, 3 figures, published in Uphill Battles in Language Processing workshop, EMNLP 2016", "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["sebastian ruder", "parsa ghaffari", "john g breslin"], "accepted": false, "id": "1610.09158"}, "pdf": {"name": "1610.09158.pdf", "metadata": {"source": "CRF", "title": "Towards a continuous modeling of natural language domains", "authors": ["Sebastian Ruder", "Parsa Ghaffari", "John G. Breslin"], "emails": ["sebastian.ruder@insight-centre.org", "john.breslin@insight-centre.org", "sebastian@aylien.com", "parsa@aylien.com"], "sections": [{"heading": "1 Introduction", "text": "The concept of the domain permeates natural language and human interaction: people continually vary their language according to context, in writing, dialogue, and language. However, the concept of the domain is poorly defined, with conflicting definitions aimed at grasping the essence of what constitutes a domain. In semantics, a domain is considered a \"specific area of cultural emphasis\" (Oppenheimer, 2006) that involves a certain terminology, such as a certain sport. In sociolinguistics, a domain consists of a group of related social situations, such as all human activities that take place at home. In discourse, a domain is a \"cognitive construct (that) created in response to a number of factors\" (Douglas, 2004) and includes a variety of registers. Finally, in the context of transfer learning, a domain is defined that consists of a trait consisting of a trait, space X, and a marginal probability distribution (X)."}, {"heading": "2 Continuous domains vs. mixtures of discrete domains", "text": "In domain adaptation, a novel target domain is traditionally considered discrete and independent of the source domain (Blitzer et al., 2006). Other research uses blends to model the target domain on the basis of a single (thumb \u0301 III and Marcu, 2006) or multiple discrete source domains (Mansour, 2009). We argue that modelling a novel domain as a mixture of existing domains is insufficient given three factors. First, the diversity of human language makes it impossible to confine yourself to a limited number of source domains, all of which are modeled, as illustrated by the diversity of the web, which contains billions of heterogeneous web pages; the Yahoo! Directory1 Directory1 famously contained thousands of handcrafted categories in an attempt to separate them."}, {"heading": "3 Dialogue modeling as a test bed for investigating domains", "text": "Since a domain requires a social component and relies on context, we suggest using dialog modeling as a test bed to gain a more nuanced understanding of how language differs from domain to domain. Dialogue modeling can be viewed as a prototypical task in the processing of natural language, similar to language modeling, and should therefore show variations in the underlying language. It allows us to observe the effects of different strategies for modeling language variations across domains on a downstream task while they remain unattended by nature. In a similar way, it has been shown that the linguistic patterns of individual users in online communities adapt to those of the community we have data in (Niederhoffer and Pennebaker, 2002; Levitan et al., 2011)."}, {"heading": "4 Representing continuous domains", "text": "In fact, it is such that it is a matter of a way in which people move in a country in which they are able to live and live in a world in which they are able to live and live, in a world in which they are able to move, in which they move, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they live, in which they are able to live, in which they live, in which they live, in which they live, in which they live in which they live, in which they live, in which they live, in which they live in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they live, in which they live, in which they live, in which they live, in which they are able to live, in a country, in a country, in which they are able to live, in a country, in which they are able to live, in which they are able to live, in which they are able to live in"}, {"heading": "5 Investigating language change", "text": "By its very nature, a continuous conception of domains lends itself to a diachronic linguistic study. By looking at the representations generated by the model over different time steps, one gets insights into the change of language in a community or another area. Likewise, the observation of how a user adapts his style to different users and communities reveals insights into the language of these units. Domain mixture models use various measures of domain similarity to determine how similar the languages of two domains are, such as Renyi divergence (Van Asch and Daelemans, 2010), KullbackLeibler (KL) divergence, Jensen-Shannon divergence and vector similarity metrics (Plank and vanNoord, 2011), and task-specific metrics (Zhou et al., 2016). While word distributions have traditionally been used to compare domains, embedding domains offers a variety of ways to evaluate the learned spatial representations."}, {"heading": "6 Evaluation", "text": "Our evaluation consists of three parts to evaluate the learned representations, the model and the variation of language itself. First, because our models produce new representations for each subspace, we can compare a snapshot of the representation of a domain after each n time step to show the course of its changes. Second, because we conduct dialog modeling experiments, we can easily make gold data available for evaluation in the form of the actual answer. At the same time, we can build a model based on edited data from a certain period of time, adjust it to a stream of future conversations, and evaluate its performance with BLEU or some other metric that might be more appropriate to show the variation of language. At the same time, human evaluations will show whether the generated responses are true to the target area. Finally, the learned representations will allow us to examine the variations in language. Ideally, we want to traverse the multiplicity and observe how language changes as we move from one domain to another, similar to Radford 2016)."}, {"heading": "7 Conclusion", "text": "We have proposed an idea of continuous areas of natural language, along with dialogue models as a test bed. We have presented a representation of continuous areas and detailed how this representation can feed into representative learning-based models. Finally, we have outlined how these models can be used to investigate changes and variations in language. While our models allow us to shed light on how language changes, models that can adapt to continuous change are key to the personalization and reality of the struggle with an ever-changing world."}], "references": [{"title": "Domain Adaptation with Structural Correspondence Learning", "author": ["John Blitzer", "Ryan McDonald", "Fernando Pereira."], "venue": "EMNLP \u201906 Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing, (July):120\u2013128.", "citeRegEx": "Blitzer et al\\.,? 2006", "shortCiteRegEx": "Blitzer et al\\.", "year": 2006}, {"title": "Domain Separation Networks", "author": ["Konstantinos Bousmalis", "George Trigeorgis", "Nathan Silberman", "Dilip Krishnan", "Dumitru Erhan."], "venue": "NIPS.", "citeRegEx": "Bousmalis et al\\.,? 2016", "shortCiteRegEx": "Bousmalis et al\\.", "year": 2016}, {"title": "InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets", "author": ["Xi Chen", "Yan Duan", "Rein Houthooft", "John Schulman", "Ilya Sutskever", "Pieter Abbeel."], "venue": "arXiv preprint arXiv:1606.03657.", "citeRegEx": "Chen et al\\.,? 2016", "shortCiteRegEx": "Chen et al\\.", "year": 2016}, {"title": "No Country for Old Members : User Lifecycle and Linguistic Change in Online Communities", "author": ["Cristian Danescu-Niculescu-Mizil", "Robert West", "Dan Jurafsky", "Christopher Potts."], "venue": "Proceedings of the 22nd international conference on World Wide Web, pages", "citeRegEx": "Danescu.Niculescu.Mizil et al\\.,? 2013", "shortCiteRegEx": "Danescu.Niculescu.Mizil et al\\.", "year": 2013}, {"title": "Domain Adaptation for Statistical Classifiers", "author": ["Hal Daum\u00e9 III", "Daniel Marcu."], "venue": "Journal of Artificial Intelligence Research, 26:101\u2013126.", "citeRegEx": "III and Marcu.,? 2006", "shortCiteRegEx": "III and Marcu.", "year": 2006}, {"title": "Frustratingly Easy Domain Adaptation", "author": ["Hal Daum\u00e9 III."], "venue": "Association for Computational Linguistic (ACL)s, (June):256\u2013263.", "citeRegEx": "III.,? 2007", "shortCiteRegEx": "III.", "year": 2007}, {"title": "Discourse Domains: The Cognitive Context of Speaking", "author": ["Dan Douglas."], "venue": "Diana Boxer and Andrew D. Cohen, editors, Studying Speaking to Inform Second Language Learning. Multilingual Matters.", "citeRegEx": "Douglas.,? 2004", "shortCiteRegEx": "Douglas.", "year": 2004}, {"title": "Improving Vector Space Word Representations Using Multilingual Correlation", "author": ["Manaal Faruqui", "Chris Dyer."], "venue": "Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 462 \u2013 471.", "citeRegEx": "Faruqui and Dyer.,? 2014", "shortCiteRegEx": "Faruqui and Dyer.", "year": 2014}, {"title": "Unsupervised Visual Domain Adaptation Using Subspace Alignment", "author": ["Basura Fernando", "Amaury Habrard", "Marc Sebban", "Tinne Tuytelaars", "K U Leuven", "Laboratoire Hubert", "Curien Umr", "Benoit Lauras."], "venue": "Proceedings of the IEEE International Conference on", "citeRegEx": "Fernando et al\\.,? 2013", "shortCiteRegEx": "Fernando et al\\.", "year": 2013}, {"title": "Geodesic Flow Kernel for Unsupervised Domain Adaptation", "author": ["Boqing Gong", "Yuan Shi", "Fei Sha", "Kristen Grauman."], "venue": "2012 IEEE Conference on Computer Vision and Pattern Recognition (CVPR).", "citeRegEx": "Gong et al\\.,? 2012", "shortCiteRegEx": "Gong et al\\.", "year": 2012}, {"title": "Continuous manifold based adaptation for evolving visual domains", "author": ["Judy Hoffman", "Trevor Darrell", "Kate Saenko."], "venue": "Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, pages 867\u2013874.", "citeRegEx": "Hoffman et al\\.,? 2014", "shortCiteRegEx": "Hoffman et al\\.", "year": 2014}, {"title": "Auto-Encoding Variational Bayes", "author": ["Diederik P Kingma", "Max Welling."], "venue": "arXiv preprint arXiv:1312.6114, (Ml):1\u201314.", "citeRegEx": "Kingma and Welling.,? 2013", "shortCiteRegEx": "Kingma and Welling.", "year": 2013}, {"title": "Entrainment in Speech Preceding Backchannels", "author": ["Rivka Levitan", "Agustn Gravano", "Julia Hirschberg."], "venue": "Annual Meeting of the Association for Computational Linguistics (ACL/HLT), pages 113\u2013117.", "citeRegEx": "Levitan et al\\.,? 2011", "shortCiteRegEx": "Levitan et al\\.", "year": 2011}, {"title": "Domain Adaptation with Multiple Sources", "author": ["Yishay Mansour."], "venue": "NIPS, pages 1\u20138.", "citeRegEx": "Mansour.,? 2009", "shortCiteRegEx": "Mansour.", "year": 2009}, {"title": "Automatic domain adaptation for parsing", "author": ["David McClosky", "Eugene Charniak", "Mark Johnson."], "venue": "Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 28\u201336.", "citeRegEx": "McClosky et al\\.,? 2010", "shortCiteRegEx": "McClosky et al\\.", "year": 2010}, {"title": "Bilingual Word Embeddings from Parallel and Non-parallel Corpora for Cross-Language Text Classification", "author": ["Aditya Mogadala", "Achim Rettinger."], "venue": "NAACL, pages 692\u2013702.", "citeRegEx": "Mogadala and Rettinger.,? 2016", "shortCiteRegEx": "Mogadala and Rettinger.", "year": 2016}, {"title": "Language use as a reflection of socialization in online communities", "author": ["Dong Nguyen", "Carolyn P. Ros\u00e9."], "venue": "Proceedings of the Workshop on Languages in . . . , (June):76\u201385.", "citeRegEx": "Nguyen and Ros\u00e9.,? 2011", "shortCiteRegEx": "Nguyen and Ros\u00e9.", "year": 2011}, {"title": "Linguistic Style Matching in Social Interaction", "author": ["K.G. Niederhoffer", "J.W. Pennebaker."], "venue": "Journal of Language and Social Psychology, 21(4):337\u2013360.", "citeRegEx": "Niederhoffer and Pennebaker.,? 2002", "shortCiteRegEx": "Niederhoffer and Pennebaker.", "year": 2002}, {"title": "The Anthropology of Language: An Introduction to Linguistic Anthropology", "author": ["Harriet J. Oppenheimer."], "venue": "Wadsworth, Belmont (Canada).", "citeRegEx": "Oppenheimer.,? 2006", "shortCiteRegEx": "Oppenheimer.", "year": 2006}, {"title": "A survey on transfer learning", "author": ["Sinno Jialin Pan", "Qiang Yang."], "venue": "IEEE Transactions on Knowledge and Data Engineering, 22(10):1345\u20131359.", "citeRegEx": "Pan and Yang.,? 2010", "shortCiteRegEx": "Pan and Yang.", "year": 2010}, {"title": "Effective Measures of Domain Similarity for Parsing", "author": ["Barbara Plank", "Gertjan van Noord."], "venue": "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, 1:1566\u20131576.", "citeRegEx": "Plank and Noord.,? 2011", "shortCiteRegEx": "Plank and Noord.", "year": 2011}, {"title": "Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks", "author": ["Alec Radford", "Luke Metz", "Soumith Chintala."], "venue": "ICLR, pages 1\u201315.", "citeRegEx": "Radford et al\\.,? 2016", "shortCiteRegEx": "Radford et al\\.", "year": 2016}, {"title": "Progressive Neural Networks", "author": ["Andrei A Rusu", "Neil C Rabinowitz", "Guillaume Desjardins", "Hubert Soyer", "James Kirkpatrick", "Koray Kavukcuoglu", "Razvan Pascanu", "Raia Hadsell", "Google Deepmind."], "venue": "arXiv preprint arXiv:1606.04671.", "citeRegEx": "Rusu et al\\.,? 2016", "shortCiteRegEx": "Rusu et al\\.", "year": 2016}, {"title": "Using Domain Similarity for Performance Estimation", "author": ["Vincent Van Asch", "Walter Daelemans."], "venue": "Computational Linguistics, (July):31\u201336.", "citeRegEx": "Asch and Daelemans.,? 2010", "shortCiteRegEx": "Asch and Daelemans.", "year": 2010}, {"title": "Bi-Transferring Deep Neural Networks for Domain Adaptation", "author": ["Guangyou Zhou", "Zhiwen Xie", "Jimmy Xiangji Huang", "Tingting He."], "venue": "ACL, pages 322\u2013332.", "citeRegEx": "Zhou et al\\.,? 2016", "shortCiteRegEx": "Zhou et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 18, "context": "In semantics, a domain is considered a \u201cspecific area of cultural emphasis\u201d (Oppenheimer, 2006) that entails a particular terminology, e.", "startOffset": 76, "endOffset": 95}, {"referenceID": 6, "context": "In discourse a domain is a \u201ccognitive construct (that is) created in response to a number of factors\u201d (Douglas, 2004) and includes a variety of registers.", "startOffset": 102, "endOffset": 117}, {"referenceID": 19, "context": ", xn} and xi is the ith feature vector (Pan and Yang, 2010).", "startOffset": 39, "endOffset": 59}, {"referenceID": 14, "context": "from a continuous underlying process that is reflected in many facets of natural language: The web contains an exponentially growing amount of data, where each document \u201cis potentially its own domain\u201d (McClosky et al., 2010); a second-language learner adapts their style as their command of the language improves; language changes with time and with locality; even the WSJ section of the Penn Treebank \u2013 often treated as a single domain \u2013 contains different types of documents, such as news, lists of stock prices, etc.", "startOffset": 201, "endOffset": 224}, {"referenceID": 0, "context": "In domain adaptation, a novel target domain is traditionally assumed to be discrete and independent of the source domain (Blitzer et al., 2006).", "startOffset": 121, "endOffset": 143}, {"referenceID": 13, "context": "Other research uses mixtures to model the target domain based on a single (Daum\u00e9 III and Marcu, 2006) or multiple discrete source domains (Mansour, 2009).", "startOffset": 138, "endOffset": 153}, {"referenceID": 16, "context": "Similarly, it has been shown that the linguistic patterns of individual users in online communities adapt to match those of the community they participate in (Nguyen and Ros\u00e9, 2011; DanescuNiculescu-Mizil et al., 2013).", "startOffset": 158, "endOffset": 218}, {"referenceID": 2, "context": "Similar to Chen et al. (2016), we would like to learn representations that allow us to disentangle factors that are normally intertwined, such as style and genre, and that will allow us to gain more insight about the variation in language.", "startOffset": 11, "endOffset": 30}, {"referenceID": 24, "context": "In line with past research (Daum\u00e9 III, 2007; Zhou et al., 2016), we assume that every domain has an inherent low-dimensional structure, which allows its", "startOffset": 27, "endOffset": 63}, {"referenceID": 8, "context": "In computer vision, methods such as Subspace Alignment (Fernando et al., 2013) or the Geodesic Flow Kernel (Gong et al.", "startOffset": 55, "endOffset": 78}, {"referenceID": 9, "context": ", 2013) or the Geodesic Flow Kernel (Gong et al., 2012) have been used to find such transformations A and B.", "startOffset": 36, "endOffset": 55}, {"referenceID": 7, "context": "Similarly, in natural language processing, CCA (Faruqui and Dyer, 2014) and Procrustes analysis (Mogadala and Rettinger, 2016) have been used to align subspaces pertaining to different languages.", "startOffset": 47, "endOffset": 71}, {"referenceID": 15, "context": "Similarly, in natural language processing, CCA (Faruqui and Dyer, 2014) and Procrustes analysis (Mogadala and Rettinger, 2016) have been used to align subspaces pertaining to different languages.", "startOffset": 96, "endOffset": 126}, {"referenceID": 1, "context": "Many recent approaches using autoencoders (Bousmalis et al., 2016; Zhou et al., 2016) learn such a transformation between discrete domains.", "startOffset": 42, "endOffset": 85}, {"referenceID": 24, "context": "Many recent approaches using autoencoders (Bousmalis et al., 2016; Zhou et al., 2016) learn such a transformation between discrete domains.", "startOffset": 42, "endOffset": 85}, {"referenceID": 22, "context": "network architectures that are immune to forgetting, such as the recently proposed Progressive Neural Networks (Rusu et al., 2016) are appealing for continuous domain adaptation.", "startOffset": 111, "endOffset": 130}, {"referenceID": 24, "context": "Domain mixture models use various domain similarity measures to determine how similar the languages of two domains are, such as Renyi divergence (Van Asch and Daelemans, 2010), KullbackLeibler (KL) divergence, Jensen-Shannon divergence, and vector similarity metrics (Plank and van Noord, 2011), as well as task-specific measures (Zhou et al., 2016).", "startOffset": 330, "endOffset": 349}, {"referenceID": 11, "context": "For this, cosine similarity as used for comparing word embeddings or KL divergence as used in the Variational Autoencoder (Kingma and Welling, 2013) are a natural fit.", "startOffset": 122, "endOffset": 148}, {"referenceID": 21, "context": "Ideally, we would like to walk the manifold and observe how language changes as we move from one domain to the other, similarly to (Radford et al., 2016).", "startOffset": 131, "endOffset": 153}], "year": 2016, "abstractText": "Humans continuously adapt their style and language to a variety of domains. However, a reliable definition of \u2018domain\u2019 has eluded researchers thus far. Additionally, the notion of discrete domains stands in contrast to the multiplicity of heterogeneous domains that humans navigate, many of which overlap. In order to better understand the change and variation of human language, we draw on research in domain adaptation and extend the notion of discrete domains to the continuous spectrum. We propose representation learningbased models that can adapt to continuous domains and detail how these can be used to investigate variation in language. To this end, we propose to use dialogue modeling as a test bed due to its proximity to language modeling and its social component.", "creator": "LaTeX with hyperref package"}}}