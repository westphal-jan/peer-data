{"id": "1610.09333", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Oct-2016", "title": "Word Embeddings for the Construction Domain", "abstract": "We introduce word vectors for the construction domain. Our vectors were obtained by running word2vec on an 11M-word corpus that we created from scratch by leveraging freely-accessible online sources of construction-related text. We first explore the embedding space and show that our vectors capture meaningful construction-specific concepts. We then evaluate the performance of our vectors against that of ones trained on a 100B-word corpus (Google News) within the framework of an injury report classification task. Without any parameter tuning, our embeddings give competitive results, and outperform the Google News vectors in many cases. Using a keyword-based compression of the reports also leads to a significant speed-up with only a limited loss in performance. We release our corpus and the data set we created for the classification task as publicly available, in the hope that they will be used by future studies for benchmarking and building on our work.", "histories": [["v1", "Fri, 28 Oct 2016 18:15:08 GMT  (913kb,D)", "http://arxiv.org/abs/1610.09333v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["antoine j -p tixier", "michalis vazirgiannis", "matthew r hallowell"], "accepted": false, "id": "1610.09333"}, "pdf": {"name": "1610.09333.pdf", "metadata": {"source": "META", "title": "Word Embeddings for the Construction Domain", "authors": ["Antoine J.-P. Tixier", "Michalis Vazirgiannis", "Matthew R. Hallowell"], "emails": ["ANTOINE.TIXIER-1@COLORADO.EDU", "MVAZIRG@LIX.POLYTECHNIQUE.FR", "MATTHEW.HALLOWELL@COLORADO.EDU"], "sections": [{"heading": "1. Introduction", "text": "In construction, as in many other industries, larger and larger volumes of digital nature texts are made available, suppressing the need for efficient processing of this text. However, recent approaches to text introduced in the construction industry do not take advantage of the latest advances in natural language processing (NLP). For example, the functions of the extraction system of (Tixier et al., 2016a) are based on manually written rules and man-made dictionaries, while (Chokor et al., 2014; Yu and Hsu, 2013; Caldas and Soibelman, 2003) are all based on traditional vector space models and term frequencies - Inverse Document Frequency (TF-IDF) weighting scheme (Salton and Buckley)."}, {"heading": "2. Word Embeddings", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1. Limitations of the vector space model", "text": "Traditionally, text is represented with the vector space model (Salton and Buckley, 1988). Within this framework, each unique term (i.e. unique, bigram, etc., up to a certain order) in the universe of documents is considered an independent dimension of space and encoded as a so-called \"one-of-words\" vector. In this discrete space, documents are presented as sparse vectors in which the entries are usually binary (1 if the word is present in the document, otherwise 0), the occurrence counts (\"bag-of-words\" approach) or TF-IDF weight.This approach is limited because it considers terms as independent units. Therefore, semantic (meaning) and syntactic (grammar) term dependencies are completely overlooked. For example, the word hammer can be considered a vector [0, 0, 1,..., 0, 0, 0, 0, 0, 0, 0] and the word tool as a vector."}, {"heading": "2.2. Distributed word representations", "text": "The two limitations mentioned have motivated the use of distributed word representations, also known as word embeddings or word vectors (Bengio et al., 2003). \u2212 As shown in Figure 1 \u2212 \u2212 Word embeddings form each word in the vocabulary into a real, valuable vector in a dense, continuous space. \u2212 The m features encode concepts that are shared by all words. \u2212 \u2212 Typically \u2212 V is within the [105, 106] interval, m absorbs values within [100, 500]. Word embeddings: distributed word replication \u2212 smoothed (discrete to continuous) \u2212 densification (sparse to dense) \u2212 combating the curse of dimensionality with: \u2212 Similar words end in feature space close together \u2022 Each unique word is mapped to a point in a continuous m-dimen ional space \u2212 Typical, V 106, < < < < < <;"}, {"heading": "2.3. word2vec", "text": "The model of (Mikolov et al., 2013a), also known as word2vec, is based exactly on the Distributional Hypothesis (Harris, 1954), which can be roughly summarized as \"we will know a word from the company that holds it,\" and is simply presented in Figure 2. \u2022 Key word of word2vec: better performance not by using a more complex model (i.e., with more layers), but by enabling a simpler (simpler) model that can be trained on much larger amounts of data. \u2022 Key words for learning vectors: - CBOW: predicting goal out of context, what follows) - Skip-gram: predicting context from the target context, compared to Bengio et al: - no hidden layer (leading to 1000X speedup) - projection layer is divided (not just the weight matrix) - context: words from the history of Google ve2vec."}, {"heading": "3. Word Vectors Creation", "text": "The main goal at this stage was to collect as much construction-related text as possible. In fact, word2vec requires large amounts of text to achieve good embedding (the more the better, with decreasing yield).Although the size of our input text was important, we had to refer this text to the construction industry as much as possible. Below, sorted by size, we present the various publicly available resources that we used to create our construction-specific corpus."}, {"heading": "3.1. Wikipedia", "text": "A first and obvious big source of text was the English Wikipedia. We selected all pages related to the building category and all their children and grandchildren and came up with a final list of 12,256 pages 3. Then we used Wikipedia's Special: Export tool4 to download all these pages as XML files. All the text corresponding to the contents of these files was extracted and yielded a corpus of 6,383,953 words (403,763 unique words).1https: / / code.google.com / archive / p / word2vec / 2https: / / radimrehurek.com / gensim / models / word2vec.html 3https: / / github.com / Tixierae / WECD / blob / master / list _ wikipedia _ page.txt 4https: / / de.wikipedia.org / wiki / Special: Export"}, {"heading": "3.2. ELCOSH", "text": "The Electronic Library for Occupational Safety and Health in Construction (ELCOSH) also proved to be a valuable source of construction-related text. We scraped the pages related to handouts (245 documents), toolbox talks (179), research reports (166) and training materials (102). Whenever text was not directly available on the website, we analyzed the linked PDF documents, resulting in a corpus of 2,074,769 words (56,070 unique words)."}, {"heading": "3.3. OSHA", "text": "In addition, we extracted text from the OSHA website as follows: IMIS. We queried the Integrated Management Information System (IMIS) Accident Search Tool 6 for reports of NAICS codes 236, 237 and 238, which correspond to the construction, civil engineering and specialist trade categories and were associated with 2,691, 2,430 and 9,780 injury reports at the time of the study. Overall, the 14,901 reports provided a corpus of 1,497,056 words (25,382 unique words)."}, {"heading": "3.4. CPWR", "text": "Publications. The Center for Construction Research and Training, also known as CPWR, provides a list of research findings and articles on the publication page of its website8. Specifically, we analyzed the research reports on the Design for Safety, Accident Data Analysis, Health Hazards, Safety Hazards, and Hispanic Workers pages. We also analyzed the PDF documents linked to in Key Findings from Research, resulting in a total of 162 documents, resulting in a corpus of 416,150 words (26,095 unique words). Workbook. We also analyzed the workbook of Day Laborers \"Health and Safety Workbook9. This 453-page document yielded a 68,776 word (6,679 unique words) corpus.5http: / / www.elcosh.org / index.php 6https: / www.osha.gov / pls / publications / imis / accidentsearch. html 7https: / www.go.gov / index.SC / STx.php / Publications"}, {"heading": "3.5. NIOSH FACE", "text": "We found another source of text specific to the construction sector in the form of accident reports from the Fatality Assessment and Control Evaluation (FACE) program of the National Institute for Occupational Safety and Health (NIOSH).10 The text from the 249 reports that were assigned to the construction category (at the time of the study) was compiled. Whenever the websites contained links to PDF documents, these documents were also evaluated, resulting in a corpus of 381,969 words (13,770 unique words)."}, {"heading": "3.6. USACE", "text": "Finally, we analyzed the US Army Corps of Engineers (USACE) manual, which mandates health and safety requirements for all Corps of Engineers operations, a 977-page document containing 185,449 words (11,621 unique words)."}, {"heading": "3.7. Aggregation and learning", "text": "After final cleaning, we received a total corpus of 11,043,511 words (456,402 unique words, 70MB in size), which was divided into 55,495 200-word chunks before being handed over to the Gensim. To enable future studies to build on our work, we make our corpus and the various subcorpus previously freely available for download. To generate our word embeddings, we used the Skip-gram architecture, which was reported to perform better with small corpus (Mikolov et al., 2013b), with default parameter values: context size 10, m = 300, a sampling triangle of 10 \u2212 5 for high-frequency words, a negative sampling value of 3 and 10 training periods. In addition, words that occurred less than five times in the corpus were not embedded in standard English verses and user-defined stopwords 12."}, {"heading": "4. Exploration of the Embedding Space", "text": "In fact, we will be able to go in search of a solution that will enable us, that will enable us, that will enable us to find a solution that will enable us, that will enable us to put ourselves in a position, that will enable us to put ourselves in a position, that will enable us to put ourselves in a position, that will enable us to put ourselves in a position, that will enable us to put ourselves in a position, that will enable us to put ourselves in a position that we are in."}, {"heading": "5. Application to Injury Report Classification", "text": "In addition, we wanted to quantify the quality of our word vectors in a real application. To achieve this, we designed a classification task for injury reports. In the following, we describe the creation of our data set, the experimental setup and the results."}, {"heading": "5.1. Data set creation", "text": "In fact, it is such that it will be able to be able to be able to be able to be able to be able to be able to be able to be able to be able. (7) It is able to be able to be able. (7) It is able to be able to be able. (7) It is able to be able to be able. (7) It is able to be able to be able. (7) It is able to be able to be able. (7) It is able to be able to be able. (7) It is able to be able to be able. (7) It is able to be able to be able. (7)"}, {"heading": "5.3. Word Mover\u2019s Distance", "text": "In fact, it is the case that most of them are able to survive themselves if they are not able to save themselves."}, {"heading": "5.4. Experimental set-up", "text": "This year, it has reached the stage where it will be able to take the lead."}, {"heading": "5.5. Results", "text": "In fact, most people are able to determine for themselves what they want and what they want. It's not that they want it, but it's that they don't want it."}, {"heading": "6. Conclusion and Next steps", "text": "We presented one of the earliest applications of word embedding in the construction sector. In addition to releasing one of the largest publicly available collections of shell texts to date (11M words, 450K unique words), and a novel data collection for injury reports, we demonstrated through several examples and a case study that the use of word embedding in the construction industry is very promising and has many potential applications. By allowing more flexible, semi-monitored classifications in categories, they were able to better predict and prevent injuries (Tixier et al., 2016d) or to model and simulate more accurate safety risks (Tixier et al.). We were able to improve our embedding in several categories."}], "references": [{"title": "A neural probabilistic language model", "author": ["Bengio", "Yoshua", "Ducharme", "R\u00e9jean", "Vincent", "Pascal", "Jauvin", "Christian"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Bengio et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2003}, {"title": "Automating hierarchical document classification for construction management information systems", "author": ["Caldas", "Carlos H", "Soibelman", "Lucio"], "venue": "Automation in Construction,", "citeRegEx": "Caldas et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Caldas et al\\.", "year": 2003}, {"title": "Analyzing Arizona OSHA Injury Reports Using Unsupervised Machine Learning", "author": ["Chokor", "Abbas", "Naganathan", "Hariharan", "Chong", "Wai K", "El Asmar", "Mounir"], "venue": "Procedia Engineering,", "citeRegEx": "Chokor et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Chokor et al\\.", "year": 2016}, {"title": "Nearest neighbor pattern classification", "author": ["Cover", "Thomas", "Hart", "Peter"], "venue": "IEEE transactions on information theory,", "citeRegEx": "Cover et al\\.,? \\Q1967\\E", "shortCiteRegEx": "Cover et al\\.", "year": 1967}, {"title": "Query Expansion with Locally-Trained Word Embeddings", "author": ["Diaz", "Fernando", "Mitra", "Bhaskar", "Craswell", "Nick"], "venue": "arXiv preprint", "citeRegEx": "Diaz et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Diaz et al\\.", "year": 2016}, {"title": "Estimation of probabilities from sparse data for the language model component of a speech recognizer", "author": ["Katz", "Slava"], "venue": "IEEE transactions on acoustics, speech, and signal processing,", "citeRegEx": "Katz and Slava.,? \\Q1987\\E", "shortCiteRegEx": "Katz and Slava.", "year": 1987}, {"title": "From word embeddings to document distances", "author": ["Kusner", "Matt J", "Sun", "Yu", "Kolkin", "Nicholas", "Weinberger", "Kilian Q"], "venue": "In Proceedings of the 32nd International Conference on Machine Learning (ICML 2015)", "citeRegEx": "Kusner et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kusner et al\\.", "year": 2015}, {"title": "Efficient estimation of word representations in vector space", "author": ["Mikolov", "Tomas", "Chen", "Kai", "Corrado", "Greg", "Dean", "Jeffrey"], "venue": "arXiv preprint arXiv:1301.3781,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Exploiting similarities among languages for machine translation", "author": ["Mikolov", "Tomas", "Le", "Quoc V", "Sutskever", "Ilya"], "venue": "arXiv preprint arXiv:1309.4168,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Linguistic Regularities in Continuous Space Word Representations", "author": ["Mikolov", "Tomas", "Yih", "Wen-tau", "Zweig", "Geoffrey"], "venue": "In Proceedings of HLT-NAACL", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Mikolov", "Tomas", "Sutskever", "Ilya", "Chen", "Kai", "Corrado", "Greg S", "Dean", "Jeff"], "venue": "Advances in neural information processing systems,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "A Language and Environment for Statistical Computing", "author": ["R R Core Team"], "venue": "R Foundation for Statistical Computing,", "citeRegEx": "Team.,? \\Q2015\\E", "shortCiteRegEx": "Team.", "year": 2015}, {"title": "Software Framework for Topic Modelling with Large Corpora", "author": ["Radim \u0158eh\u016f\u0159ek", "Petr Sojka"], "venue": "In Proceedings of the LREC 2010 Workshop on New Challenges for NLP Frameworks ,", "citeRegEx": "\u0158eh\u016f\u0159ek and Sojka.,? \\Q2010\\E", "shortCiteRegEx": "\u0158eh\u016f\u0159ek and Sojka.", "year": 2010}, {"title": "word2vec parameter learning explained", "author": ["Rong", "Xin"], "venue": "arXiv preprint arXiv:1411.2738,", "citeRegEx": "Rong and Xin.,? \\Q2014\\E", "shortCiteRegEx": "Rong and Xin.", "year": 2014}, {"title": "The earth mover\u2019s distance as a metric for image retrieval. International journal of computer vision", "author": ["Rubner", "Yossi", "Tomasi", "Carlo", "Guibas", "Leonidas J"], "venue": null, "citeRegEx": "Rubner et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Rubner et al\\.", "year": 2000}, {"title": "Term-weighting approaches in automatic text retrieval", "author": ["Salton", "Gerard", "Buckley", "Christopher"], "venue": "Information processing & management,", "citeRegEx": "Salton et al\\.,? \\Q1988\\E", "shortCiteRegEx": "Salton et al\\.", "year": 1988}, {"title": "A Graph Degeneracy-based Approach to Keyword Extraction", "author": ["Tixier", "Antoine J.-P", "Malliaros", "Fragkiskos. D", "Vazirgiannis", "Michalis"], "venue": "In Proceedings of EMNLP,", "citeRegEx": "Tixier et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Tixier et al\\.", "year": 2016}, {"title": "GoWvis: a web application for Graph-ofWords-based text visualization and summarization", "author": ["Tixier", "Antoine J.-P", "Skianis", "Konstantinos", "Vazirgiannis", "Michalis"], "venue": "In Proceedings of ACL ,", "citeRegEx": "Tixier et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Tixier et al\\.", "year": 2016}, {"title": "Application of machine learning to construction injury prediction", "author": ["Tixier", "Antoine J.-P", "Hallowell", "Matthew R", "Rajagopalan", "Balaji", "Bowman", "Dean"], "venue": "Automation in Construction,", "citeRegEx": "Tixier et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Tixier et al\\.", "year": 2016}, {"title": "Construction Safety Risk Modeling and Simulation", "author": ["Tixier", "Antoine J.-P", "Hallowell", "Matthew R", "Rajagopalan", "Balaji"], "venue": "arXiv preprint arXiv:1609.07912,", "citeRegEx": "Tixier et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Tixier et al\\.", "year": 2016}, {"title": "Predicting construction cost overruns using text mining, numerical data and ensemble classifiers", "author": ["Williams", "Trefor P", "Gong", "Jie"], "venue": "Automation in Construction,", "citeRegEx": "Williams et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Williams et al\\.", "year": 2014}, {"title": "Content-based text mining technique for retrieval of CAD documents", "author": ["Yu", "Wen-der", "Hsu", "Jia-yang"], "venue": "Automation in Construction,", "citeRegEx": "Yu et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Yu et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 2, "context": ", 2016a) relies on manually written rules and humanbuilt dictionaries of keywords, while (Chokor et al., 2016; Williams and Gong, 2014; Yu and Hsu, 2013; Caldas and Soibelman, 2003) are all based on the traditional vector space model and Term Frequency - Inverse Document Frequency (TF-IDF) weighting scheme (Salton and Buckley, 1988).", "startOffset": 89, "endOffset": 181}, {"referenceID": 6, "context": ", 2013b), or document classification (Kusner et al., 2015).", "startOffset": 37, "endOffset": 58}, {"referenceID": 0, "context": "The two aforementioned limitations have motivated the use of distributed representations of words, also known as word embeddings or word vectors (Bengio et al., 2003).", "startOffset": 145, "endOffset": 166}, {"referenceID": 0, "context": "\u2022 Compared to Bengio et al.\u2019s (2003) NNLM: - no hidden layer (leads to 1000X speedup) - projection layer is shared (not just the weight matrix) - context: words from both history & future: \u201cYou shall know a word by the company it keeps\u201d (John R.", "startOffset": 14, "endOffset": 37}, {"referenceID": 12, "context": "In this study, we used the popular gensim Python implementation2 (\u0158eh\u016f\u0159ek and Sojka, 2010).", "startOffset": 65, "endOffset": 90}, {"referenceID": 6, "context": "For this purpose we used the recently introduced Word Mover\u2019s Distance (Kusner et al., 2015), or WMD.", "startOffset": 71, "endOffset": 92}, {"referenceID": 14, "context": "The WMD is a simple adaptation of the Earth Mover\u2019s Distance (Rubner et al., 2000), a well-known metric in the Computer Vision field, to NLP.", "startOffset": 61, "endOffset": 82}, {"referenceID": 6, "context": "Furthermore, (Kusner et al., 2015) represent a document ~ pk \u2208 R|V | as a normalized vector of word counts in the vector space model (normalized bag-of-words representation), with elements:", "startOffset": 13, "endOffset": 34}, {"referenceID": 6, "context": "This tends to corroborate (Kusner et al., 2015; Mikolov et al., 2013a) who observed that using more data is superior than using relevant data when training embeddings.", "startOffset": 26, "endOffset": 70}, {"referenceID": 4, "context": "These results are in accordance with (Diaz et al., 2016).", "startOffset": 37, "endOffset": 56}], "year": 2016, "abstractText": "We introduce word vectors for the construction domain. Our vectors were obtained by running word2vec on an 11M-word corpus that we created from scratch by leveraging freely-accessible online sources of construction-related text. We first explore the embedding space and show that our vectors capture meaningful constructionspecific concepts. We then evaluate the performance of our vectors against that of ones trained on a 100B-word corpus (Google News) within the framework of an injury report classification task. Without any parameter tuning, our embeddings give competitive results, and outperform the Google News vectors in many cases. Using a keyword-based compression of the reports also leads to a significant speed-up with only a limited loss in performance. We release our corpus and the data set we created for the classification task as publicly available, in the hope that they will be used by future studies for benchmarking and building on our work.", "creator": "LaTeX with hyperref package"}}}