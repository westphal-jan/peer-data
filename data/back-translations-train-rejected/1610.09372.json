{"id": "1610.09372", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-Oct-2016", "title": "Multi-agent projective simulation: A starting point", "abstract": "We develop a two-defender (Alice and Bob) invasion game using the method of projective simulation. The agent, say Alice, encounters attack' symbols coming from the right attacker where she can learn to prevent. However, some of these signs are invisible for her. Instead, she perceives some other signs that are related to Bob's task. We elaborate an example in which an agent perceives an equal portion of percepts from both attackers. Alice can choose to concentrate on her job, though she loses some attacks. Alternatively, she can have some collaboration with Bob to get and give help. It is concluded that the maximum blocking efficiency in concentration is just the minimum blocking efficiency in collaboration. Furthermore, Alice has a choice to select two different forgetting factors for her task or helping. Therefore, she can choose between herself and the other. As the main result, we conclude that if Alice selects to be selfish, she probably earns more blocking in her task and also, higher efficiency in collective blocking, regardless of Bob's selection. In addition, it turns out that when the selection of both partners is selfishness, it is the highest justice on sharing individual efficiency and it is a maximum in collective blocking efficiency too. Finally, we propose some other questions that can be tracked regarding the present study.", "histories": [["v1", "Sat, 29 Oct 2016 10:35:53 GMT  (1360kb)", "http://arxiv.org/abs/1610.09372v1", "17 pages, 11 figures"], ["v2", "Sun, 4 Jun 2017 16:28:34 GMT  (1036kb)", "http://arxiv.org/abs/1610.09372v2", "22 pages, 13 figures"]], "COMMENTS": "17 pages, 11 figures", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["rasoul kheiri"], "accepted": false, "id": "1610.09372"}, "pdf": {"name": "1610.09372.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["R. Kheiri"], "emails": ["r.kheiry@ph.iut.ac.ir"], "sections": [{"heading": null, "text": "We develop an invasion game with two defenders (Alice and Bob) using the method of projective simulation. The agent, say Alice, encounters attack symbols of the right attacker, where she can learn to prevent them. However, some of these signs are invisible to her. Instead, she perceives some other signs related to Bob's task. We elaborate an example in which an agent perceives an equal share of the perceptions of both attackers. Alice can choose to focus on her work, although she loses some attacks. Alternatively, she can collaborate with Bob to get help and give. Therefore, we conclude that the maximum blocking efficiency in cooperation is only the minimal blocking efficiency in cooperation. Furthermore, Alice has the choice between two different oblivion factors for her task or help. Therefore, she can choose between herself and the other. As a main result, we come to the conclusion that if she selfishly chooses Bob, the blocking efficiency is likely to be more selective efficiency in her task, even if it deserving a higher efficiency in both areas."}, {"heading": "1- Introduction", "text": "In the article \"Projection for Artificial Intelligence,\" there are two attackers (two isolated areas). Alice encounters some perceptions and her goal is to select efficient actions in new conditions, namely through a random passage into her episodic memory. Alice thinks of previously rewarded or unrewarded actions that build new transitional opportunities between her memory clips themselves, where the random passage takes place. The authors apply the formulation of the projective simulation for a defender in an invasion game and check the speed of learning, maximum blocking efficiency, etc. Then, they investigate the composition in which learning itself can be altered with a change in clips or even with the creation of new clips. In the current proposal, we try to expand the context of projective information that might have between two defenders named Alice and Bob in an invasion we have a collaboration between them."}, {"heading": "2- A specific example", "text": "Suppose a problem where Alice receives half of all characters from A1 (= 0.5) in blue color and she can also see half of all characters coming from A2 in red color. () The situation is exactly the same for Bob. () We derive the blocking efficiency by creating a predefined matrix for both scenarios of concentration () () () () () () () () () ()) () ()) () ()) ()) ()) () ()) () () ()) () () ()) (()) and () () () () ()) ()) () \"()\" () \"()\" () \"()\" () \"()\" () \"() () (\") () (\")\" () () (\") () (\") () () (\") () () ()) (\" () () () ()) ()) (\"()) () ()) () ()) ()) (\") () ()) () ()) ()) () ()) ()) () ()) ()) ()) ()) (\"()) ()) () ()) () ()) () ())) (\" () \")."}, {"heading": "Results", "text": "In the following, we compare the blocking efficiency of Alice in the USA in a variety of scenarios with the maximum effectiveness of Alice in the USA without using her for this purpose. In the following, we compare the collective blocking efficiency of Alice in the USA with that of Bob in the USA, in the USA, in the USA, in Europe, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in Canada, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA"}, {"heading": "3- Conclusion", "text": "We studied a model of the projected simulation for a two-person invasion, in which the defenders (Alice and Bob) could choose between solitude and a variety of collaborations. In our model, each of them encountered a certain proportion of the characters (blue eyes) blocking each other while the other people remained invisible. We worked out an example in which each partner received an equal portion of blue and red color."}, {"heading": "4- Some other possible questions", "text": "In this context, it should be noted that this project is a project, which is primarily a project."}], "references": [{"title": "Projective simulation for artificial intelligence", "author": ["H.J. Briegel", "G. De las Cuevas"], "venue": "Sci. Rep", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2012}], "referenceMentions": [{"referenceID": 0, "context": "1- Introduction In the article \u201cprojective simulation for artificial intelligence\u201d [1] to put it briefly, there is an agent that encounters some percepts and its aim is learning to select efficient actions in new conditions via a random walk in its episodic memory.", "startOffset": 83, "endOffset": 86}, {"referenceID": 0, "context": "4- We have modified the effect of forgetting factor (\u03b3) in rewarding actions slightly differently from the referenced paper [1].", "startOffset": 124, "endOffset": 127}, {"referenceID": 0, "context": "paper [1], rewarding actions can not be forgotten completely, even in the case of \u03b3 = 1.", "startOffset": 6, "endOffset": 9}, {"referenceID": 0, "context": "Reference [1] Briegel, H.", "startOffset": 10, "endOffset": 13}], "year": 2016, "abstractText": "We develop a two-defender (Alice and Bob) invasion game using the method of projective simulation. The agent, say Alice, encounters attack\u2019 symbols coming from the right attacker where she can learn to prevent. However, some of these signs are invisible for her. Instead, she perceives some other signs that are related to Bob\u2019s task. We elaborate an example in which an agent perceives an equal portion of percepts from both attackers. Alice can choose to concentrate on her job, though she loses some attacks. Alternatively, she can have some collaboration with Bob to get and give help. It is concluded that the maximum blocking efficiency in concentration is just the minimum blocking efficiency in collaboration. Furthermore, Alice has a choice to select two different forgetting factors for her task or helping. Therefore, she can choose between herself and the other. As the main result, we conclude that if Alice selects to be selfish, she probably earns more blocking in her task and also, higher efficiency in collective blocking, regardless of Bob\u2019s selection. In addition, it turns out that when the selection of both partners is selfishness, it is the highest justice on sharing individual efficiency and it is a maximum in collective blocking efficiency too. Finally, we propose some other questions that can be tracked regarding the present study. 1Introduction In the article \u201cprojective simulation for artificial intelligence\u201d [1] to put it briefly, there is an agent that encounters some percepts and its aim is learning to select efficient actions in new conditions via a random walk in its episodic memory. The agent thinks of previously rewarded or nonrewarded actions, which build new transition probabilities between its memory clips of episodic memory where the random walk takes place. The authors apply the formulation of the projective simulation for a defender in an invasion game and check the speed of learning, maximum blocking efficiency, etc. Then, they investigate learning with the composition in which the memory itself can be changed with a change in clips or even with the creation of new clips. In the current proposal, we try to expand the context of projective simulation to include interchangeable information between two defenders named Alice and Bob in an invasion game where we might have collaboration between them in a situation in which they are not completely isolated. Consider, for example, a situation like that illustrated in figure 1, where there are two attackers (A1, A2) and two defenders (D1= Alice, D2= Bob), with their precept spaces S1 and S2 for D1 and D2, respectively. Here, the condition is that the defenders are isolated in their action space, but are not isolated in their percept spaces. We divide the signs coming from each attacker to two portions, blue and red portions. If there is not any intersection between blue and red percepts, we can write A1 signs = blue signs of A1 + red signs of A1, A2 signs = blue signs of A2 + red signs of A2. The main applied constraint is that one portion of the signs of A1 is invisible for Alice, say red signs of A1; instead, one portion of the signs of A2 is visible for her (red signs of A2). The situation could be the same for Bob too. Thus, For Alice (Bob), the blue percepts directly come from A1 (A2), where the defender can learn them and accomplish right actions for them. However, the blue percepts are less than what Alice needs to cover all attacks of A1. It means that A1 (A2) sends some signs, but not all, in blue for the right defender Alice (Bob) and sends the rest of the signs to Bob(Alice) in red. Hence, the red percepts of Alice (Bob) come from D2 (D1), where the defender does not need them for her (his) actions. However, Alice can learn the red signs for Bob and sends green signs for him and vice versa (see figure 1). Therefore, the percepts-action space for both of them is as S1 \u2261 S2: {\u21d2,\u21d0,\u21d2,\u21d0,\u21d2,\u21d0}, A1 \u2261 A2: {+,\u2212,\u21d2,\u21d0}. Here we choose not to distinguish between green actions and green percepts. In a generic situation, there is an open question related to such choices; what is the maximum reduction we could have for (S\u00d7 A) space related to a given situation? (Especially, reduction of actions would be more important so that Alice could do fewer actions and increase her dilation time (D_max) dramatically). In the other words, can we map a situation to another one with a smaller h matrix? Nevertheless, we have a choice to assign a separate episodic memory and engine for the implementation of green actions that would divide the (S\u00d7 A) space to two subspaces. Figure 1: A model of two attacker-defender invasion game. Suppose that Alice sees a given fraction (\u03b1) of all signs coming from A1, which are blue percepts, and she can also see some red percepts coming from A2. Alice has a choice to ignore the red signs by focusing on blue ones and learns her job to reach the maximum blocking efficiency in rmax = \u03b1/1 if her forgetting factor \u03b3 is zero. In this way, she will lose to perform actions for the fraction of 1 \u2212 \u03b1 attacks of A1. As another choice, on the other hand, she can have some collaboration with Bob. Each of them can analyzes the red signs separately and interchange green signs that contain indirect information about the reds. Then, eventually, Alice can make use of green signs coming from Bob for action against the attacker A1. However, in collaboration, the speed of learning is decreased. Since Bob himself is learning the reds, the green signs coming from Bob are not the same as the-reds perceived before. Therefore, there needs some more time for Alice to do the right actions against the green percepts. In addition, the deliberation time is increased in collaboration. Because the percept space for each partner is increased, there can be some dilation in deliberation time raised by expanding h_matrix. Moreover, more importantly, D_max is decreased directly, because Alice needs to carry out more actions (\u03b1 \u2212 1 more actions at least, if she has a separate engine for the green signs) at a given time, in comparison with the time when she concentrates on the blues. Even for the problem without considering deliberation time, there can be serious questions like a comparison between maximum blocking efficiencies related to different forgetting factors for Alice and Bob, etc. We are about to answer such questions in the following example. 2A specific example Suppose a problem in which Alice receives half of all signs come from A1 (\u03b1 = 0.5) in blue and she can also see half of all the signs coming from A2 in red. The situation is just the same for Bob. We derive the blocking efficiency by assigning a given h matrix for both scenarios of concentration (that is ignoring the red percepts) and collaboration. Concentration h matrix. If Alice ignores the reds and does action just for the blue percepts, then one can write h = ( h(\u21d2,\u2212) h(\u21d2,+) h(\u21d0,\u2212) h(\u21d0,+) 0 0 0 0 ) , P(s) = 1 4 \u27f6 rn = 1 4 [P(\u2212| \u21d0) + P(+| \u21d2)], where the rows are percepts and the columns are actions. We neglect writing the superscript of n for the conditional probabilities P(a|s) for ease. Also, in this example, Alice and Bob are in the same situation and we neglect writing the subscript for distinguishing them. In this scenario, the maximum blocking efficiency for Alice (or Bob) becomes rmax = 0.5/1. Nevertheless, the concentration h matrix might be different (for example, see section 4, proposition number 4). Collaboration h matrix. One can easily expand the concentration matrix to the collaboration matrix by considering the whole percept-action space (S \u00d7 A), including the reds and the greens as", "creator": "Microsoft\u00ae Word 2013"}}}