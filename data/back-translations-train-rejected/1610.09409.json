{"id": "1610.09409", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Oct-2016", "title": "Probabilistic Model Checking for Complex Cognitive Tasks -- A case study in human-robot interaction", "abstract": "This paper proposes to use probabilistic model checking to synthesize optimal robot policies in multi-tasking autonomous systems that are subject to human-robot interaction. Given the convincing empirical evidence that human behavior can be related to reinforcement models, we take as input a well-studied Q-table model of the human behavior for flexible scenarios. We first describe an automated procedure to distill a Markov decision process (MDP) for the human in an arbitrary but fixed scenario. The distinctive issue is that -- in contrast to existing models -- under-specification of the human behavior is included. Probabilistic model checking is used to predict the human's behavior. Finally, the MDP model is extended with a robot model. Optimal robot policies are synthesized by analyzing the resulting two-player stochastic game. Experimental results with a prototypical implementation using PRISM show promising results.", "histories": [["v1", "Fri, 28 Oct 2016 21:37:14 GMT  (2429kb,D)", "http://arxiv.org/abs/1610.09409v1", null]], "reviews": [], "SUBJECTS": "cs.AI cs.RO", "authors": ["sebastian junges", "nils jansen", "joost-pieter katoen", "ufuk topcu"], "accepted": false, "id": "1610.09409"}, "pdf": {"name": "1610.09409.pdf", "metadata": {"source": "CRF", "title": "Probabilistic Model Checking for Complex Cognitive Tasks", "authors": ["Sebastian Junges", "Nils Jansen", "Joost-Pieter Katoen", "Ufuk Topcu"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "The question of how people can be observed in dynamic, possibly mixed-reality environments enabled us to encode such data into behavioural models. Furthermore, enhanced learning [1] (RL) describes sufficiently quantitative aspects of human behaviour when it comes to solving complex tasks in realistic environments [2,3]. RL includes algorithms that address the optimal control problem through learning, i.e., an agent - the human - learns how to solve a task based on repeated interaction with an environment. So-called Q tables store quantitative information about possible human decisions. We consider this information, which describes human behaviour. Consider the vision-motor setting from [4] in Fig. 1. A person walks down a sidewalk and is asked to devote himself to three modular tasks: avoiding obstacles (purple), approaching targets (blue) or following grey (grey)."}, {"heading": "2 Preliminaries", "text": "In this section we give a brief introduction to models, specifications and our notations; for details we refer to [18, chap. 10].Definition 1 (Probabilistic models).A stochastic game (SG) is a tuple M = (S, sI, Act, P) with a finite sentence S of states, so that S = S \u0432] S2, an initial state sI \u0441S, a finite act of actions and a transitional function P: S \u00d7 Act \u00d7 S \u2192 [0, 1] and [s \u2032 S P (s, \u03b1, s \u2032), an initial state sI \u0445S, a finite act of actions, and a transitional function P: S \u00d7 Act \u00d7 S \u2192 [0, 1] and [S, \u03b1, s \u2032 s \u2032 s \u2032), an initial state S (s), an initial state S (S), a beginner type (S), a beginner type (S), a beginner type (S)."}, {"heading": "3 Description of the cognitive model", "text": "This section describes the scenario of the cognitive model, which is used as a case study. It also defines a formalization that can be applied to similar case studies, providing the basis for obtaining the underlying representation as MDP and - involving robotic movements - as SG.General Scenario. We consider a scenario in which a human agent walks across a sidewalk and encounters objects such as obstacles and garbage, see Fig. 1. Humans are given three modular objectives: While following a sidewalk (represented as a line) to get to the other side, they should avoid going into obstacles and aiming to collect and collect garbage. The line to be followed is abstracted as a series of waypoints. Waypoints and objects such as obstacles and garbage are designated as characteristics of the scenario. As obstacles can be crossed, garbage can be collected and waypoints can be visited, these characteristics are either present or disappeared. For analytical purposes, we mark certain regions of the environment as a target area, and assume that all of the scenarios are originally present in a scenario, and that all of them are a general one."}, {"heading": "3.1 Formal model", "text": "Let us describe how man interacts with his environment. The environment consists of a two-dimensional grid and its characteristics, with the characteristics having a type in Tp = {Fruit, Litt, Wpt}. Definition 2 (Environment). An environment Env = (Loc, Feat) consists of a finite series of places Loc withLoc = (x, y). A feature f = (posh) posp = (posh) posp = (posh) posp consists of a type and a (feature-) location.Features are divided according to their type: Featus FeatWpt, so that FeatWpt, so that Featp {tp} \u00b7 Loc. Consider our running example in Fig. 3 (b) The environment is called Env = (LoatWpt, at) at FeatWpt, so that FeatWpt {tp} \u00b7 Loc."}, {"heading": "3.2 MDP model of the human behaviour", "text": "In view of the above formal description, we are now ready to construct the MDP for human behavior. State space is given by the number of situations; the initial state is given by the starting point of the human being and the assumption that initially all the characteristics are present. As already mentioned, we have two possible sources of non-determinism: (1) sub-specification of the model: The closest relevant characteristic is not unique. (2) Insufficient reliance on some entries in the Q table, for example, when the amount of data does not allow us to draw conclusions. Here, we consider only the first source, the latter is an extension with slightly more non-determinism due to a sub-specification. The resolution of the non-singleton motion vectors is modeled as an action of the environment. Definition 5. The MDP M = (S, sI, Act, P) reflects human behavior from the moment of sensation."}, {"heading": "3.3 SG model of human-robot interaction", "text": "In order to obtain a model that models human behavior and allows us to synthesize a plan for the robot, we need to consider a unified model. Naturally, since we want to select actions for the robot, the robot is modeled as (potentially unlikely) MDP. Note that the non-determinism of the robot is controllable, while the non-determinism of the human model is uncontrollable. This, of course, results in a stochastic two-player game. As a design choice, we let robot and human move alternately (typically alternately). Although this abstraction is no different from synchronous movements per se, it means that we can use individual actions to determine the movements. 5 In combination with the robot, this prevents the distinction between robot and human performance: in this case, coding the reward in the state space would be our last resort. The robot model."}, {"heading": "4 Experiments", "text": "In fact, most of them are able to move to another world, to move to another world, to move to another world, to move to another world, to move to another world, to move to another world, to move to another world, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move."}, {"heading": "5 Conclusion and discussion", "text": "In fact, most of them are able to survive on their own."}, {"heading": "A The size of the encoding", "text": "We argue that the coding is cubic. To describe the transition probabilities, it is sufficient to consider only under what circumstances a feature is not near (s, o): this is exactly if there is another characteristic of type f (o) s.t. dh (f) < dh (f). For each action [fObst, fLitt, fWpt] \"FeatObst\" \u00b7 FeatLitt \u00b7 FeatWpt and each human position we have as a precondition the avoidance of the robot as a separate goal, then the transition coding for each command in the human module must be specified separately for each possible location of the robot."}, {"heading": "B Experimental Performance", "text": "Table 2 (a) indicates the grid size, the number of obstacles, boundaries and waypoints, the number of commands, the number of states, choices and branches of the underlying model, the number of decision diagram nodes (with the number of terminal nodes in brackets), and the time to parse and create the model, either explicitly or via the symbolic engine. A \u2212 indicates that the number could not be reached within 20 hours. The high number of terminal mtbdd nodes is a major challenge for dd-based engines, while the explicit engine cannot cope with the large number of commands. Table 2 (b) provides the same information for the models, including a robot; note that here are the symbolic numbers obtained by converting the model into an MDP. Please note that the decision diagrams are comparatively small: this is due to the regularity of the changing movements."}], "references": [{"title": "Reinforcement Learning: An Introduction", "author": ["R.S. Sutton", "A.G. Barto"], "venue": "MIT Press", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1998}, {"title": "Adaptive critics and the basal ganglia", "author": ["A.G. Barto"], "venue": "Models of Information Processing in the Basal Ganglia. MIT Press", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1995}, {"title": "The computational neurobiology of learning and reward", "author": ["N.D. Daw", "K. Doya"], "venue": "Current Opinion in Neurobiology 16(2)", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2006}, {"title": "Modular inverse reinforcement learning for visuomotor behavior", "author": ["C.A. Rothkopf", "D.H. Ballard"], "venue": "Biological Cybernetics 107(4)", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2013}, {"title": "Algorithms for inverse reinforcement learning", "author": ["A.Y. Ng", "S.J. Russell"], "venue": "Proc. of ICML, Morgan Kaufmann", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2000}, {"title": "The probabilistic model checking landscape", "author": ["J.P. Katoen"], "venue": "Proc. of LICS.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2016}, {"title": "Probabilistic verification for cognitive models", "author": ["S. Junges", "N. Jansen", "J.P. Katoen", "U. Topcu"], "venue": "Proc. of Cross-Disciplinary Challenges for Autonomous Systems (CDCAS). AAAI Fall Symposium", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2016}, {"title": "Markov Decision Processes: Discrete Stochastic Dynamic Programming", "author": ["M.L. Puterman"], "venue": "John Wiley and Sons", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1994}, {"title": "Prism 4.0: Verification of probabilistic real-time systems", "author": ["M. Kwiatkowska", "G. Norman", "D. Parker"], "venue": "Proc. of CAV. Volume 6806 of LNCS,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2011}, {"title": "iscasMc: A web-based probabilistic model checker", "author": ["E.M. Hahn", "Y. Li", "S. Schewe", "A. Turrini", "L. Zhang"], "venue": "Proc. of FM. Volume 8442 of LNCS, Springer", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2014}, {"title": "The complexity of stochastic games", "author": ["A. Condon"], "venue": "Inf. Comput. 96(2)", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1992}, {"title": "PRISM-Games 2.0: A tool for multiobjective strategy synthesis for stochastic games", "author": ["M. Kwiatkowska", "D. Parker", "C. Wiltsche"], "venue": "Proc. of TACAS. Volume 9636 of LNCS,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2016}, {"title": "Credit assignment in multiple goal embodied visuomotor behavior", "author": ["C.A. Rothkopf", "D.H. Ballard"], "venue": "Frontiers in Psychology 1", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2010}, {"title": "Control of gaze while walking: task structure, reward, and uncertainty", "author": ["M.H. Tong", "O. Zohar", "M.M. Hayhoe"], "venue": "Journal of Vision", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2016}, {"title": "Implementation of Symbolic Model Checking for Probabilistic Systems", "author": ["D. Parker"], "venue": "PhD thesis, University of Birmingham", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2002}, {"title": "Symbolic model checking for probabilistic processes", "author": ["C. Baier", "E.M. Clarke", "V. Hartonas-Garmhausen", "M.Z. Kwiatkowska", "M. Ryan"], "venue": "Proc. of ICALP. Volume 1256 of LNCS", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1997}, {"title": "PRISM-games: A model checker for stochastic multi-player games", "author": ["T. Chen", "V. Forejt", "M. Kwiatkowska", "D. Parker", "A. Simaitis"], "venue": "Proc. of TACAS. Volume 7795 of LNCS, Springer", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2013}, {"title": "Principles of Model Checking", "author": ["C. Baier", "J.P. Katoen"], "venue": "The MIT Press", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2008}, {"title": "Automatic verification of probabilistic concurrent finite-state programs", "author": ["M.Y. Vardi"], "venue": "Proc. of FOCS, IEEE CS", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1985}, {"title": "Computing behavioral distances, compositionally", "author": ["G. Bacci", "G. Bacci", "K.G. Larsen", "R. Mardare"], "venue": "Proc. of MFCS. Volume 8087 of LNCS, Springer", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2013}, {"title": "Probabilistic reachability for parametric Markov models", "author": ["E.M. Hahn", "H. Hermanns", "L. Zhang"], "venue": "Software Tools for Technology Transfer 13(1)", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2010}, {"title": "Prophesy: A probabilistic parameter synthesis tool", "author": ["C. Dehnert", "S. Junges", "N. Jansen", "F. Corzilius", "M. Volk", "H. Bruintjes", "J.P. Katoen", "E. Abraham"], "venue": "Proc. of CAV. Volume 9206", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2015}, {"title": "Probabilistic model checking modulo theories", "author": ["B. Wachter", "L. Zhang", "H. Hermanns"], "venue": "Proc. of QEST, IEEE CS", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2007}, {"title": "Verification of Markov decision processes using learning algorithms", "author": ["T. Br\u00e1zdil", "K. Chatterjee", "M. Chmelik", "V. Forejt", "J. Kret\u0301\u0131nsk\u00fd", "M.Z. Kwiatkowska", "D. Parker", "M. Ujma"], "venue": "Proc. of ATVA. Volume 8837 of LNCS, Springer", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 0, "context": "Moreover, reinforcement learning [1] (RL) sufficiently describes quantitative aspects of human behaviour when solving complicated tasks in realistic environments [2,3].", "startOffset": 33, "endOffset": 36}, {"referenceID": 1, "context": "Moreover, reinforcement learning [1] (RL) sufficiently describes quantitative aspects of human behaviour when solving complicated tasks in realistic environments [2,3].", "startOffset": 162, "endOffset": 167}, {"referenceID": 2, "context": "Moreover, reinforcement learning [1] (RL) sufficiently describes quantitative aspects of human behaviour when solving complicated tasks in realistic environments [2,3].", "startOffset": 162, "endOffset": 167}, {"referenceID": 3, "context": "Consider the visio-motor setting from [4] in Fig.", "startOffset": 38, "endOffset": 41}, {"referenceID": 3, "context": "This picture is taken from [4] with permission from Ballard.", "startOffset": 27, "endOffset": 30}, {"referenceID": 4, "context": "To build an accurate and general model of observed human behaviour for different tasks, inverse reinforcement learning (IRL) [5] assigns weights describing preferences over these tasks.", "startOffset": 125, "endOffset": 128}, {"referenceID": 3, "context": "A large class of human behavioural models is covered by a set of Q\u2013tables together with weights obtained by the methods in [4].", "startOffset": 123, "endOffset": 126}, {"referenceID": 5, "context": "This paper proposes probabilistic model checking [6] to analyse human behaviour models described by weighted Q\u2013tables3 For an arbitrary concrete scenario, a Markov decision process (MDP) [8] is generated automatically, see Fig.", "startOffset": 49, "endOffset": 52}, {"referenceID": 7, "context": "This paper proposes probabilistic model checking [6] to analyse human behaviour models described by weighted Q\u2013tables3 For an arbitrary concrete scenario, a Markov decision process (MDP) [8] is generated automatically, see Fig.", "startOffset": 187, "endOffset": 190}, {"referenceID": 8, "context": "We assess the performance of the human for the scenario as well as properties of the human model itself by employing MDP model checking as supported by PRISM [9], StORM, and iscasMc [10].", "startOffset": 158, "endOffset": 161}, {"referenceID": 9, "context": "We assess the performance of the human for the scenario as well as properties of the human model itself by employing MDP model checking as supported by PRISM [9], StORM, and iscasMc [10].", "startOffset": 182, "endOffset": 186}, {"referenceID": 10, "context": "The joint human-robot interaction model is a stochastic two-player game (SG) [11].", "startOffset": 77, "endOffset": 81}, {"referenceID": 11, "context": "We synthesise optimal policies for the robot under the human behaviour using SG model-checking with PRISM-Games [12].", "startOffset": 112, "endOffset": 116}, {"referenceID": 6, "context": "3 A high-level conceptual view in the form of an extended abstract is given in [7].", "startOffset": 79, "endOffset": 82}, {"referenceID": 3, "context": "input data from [4] observations of human behaviour", "startOffset": 16, "endOffset": 19}, {"referenceID": 3, "context": "with loc = {(x, y) | x 2 [0, 4] y 2 [0, 5]}, and", "startOffset": 25, "endOffset": 31}, {"referenceID": 4, "context": "with loc = {(x, y) | x 2 [0, 4] y 2 [0, 5]}, and", "startOffset": 36, "endOffset": 42}, {"referenceID": 6, "context": "\u21b5h 2 Orient = {i \u00b7 14\u21e1 | i 2 [0, 7]}.", "startOffset": 29, "endOffset": 35}, {"referenceID": 12, "context": "for visio-motor ta ks [13,4,14].", "startOffset": 22, "endOffset": 31}, {"referenceID": 3, "context": "for visio-motor ta ks [13,4,14].", "startOffset": 22, "endOffset": 31}, {"referenceID": 13, "context": "for visio-motor ta ks [13,4,14].", "startOffset": 22, "endOffset": 31}, {"referenceID": 14, "context": "6\u00b7107 states, its generation\u2014in absence of a symbolic e gine [15,16] for SGs\u2014takes over twenty hours; analysing maxmi re chability pr babili ies akes three hours.", "startOffset": 61, "endOffset": 68}, {"referenceID": 15, "context": "6\u00b7107 states, its generation\u2014in absence of a symbolic e gine [15,16] for SGs\u2014takes over twenty hours; analysing maxmi re chability pr babili ies akes three hours.", "startOffset": 61, "endOffset": 68}, {"referenceID": 16, "context": "The SGs have a noticeably more complex structure than benchmarks in [17] and offer new challenges to probabilistic verification.", "startOffset": 68, "endOffset": 72}, {"referenceID": 0, "context": "A stochastic game (SG) is a tuple M = (S, sI ,Act ,P) with a finite set S of states such that S = S\u25e6]S2, an initial state sI \u2208 S, a finite set Act of actions, and a transition function P : S \u00d7Act \u00d7 S \u2192 [0, 1] and \u2211s\u2032\u2208S P(s, \u03b1, s\u2032) \u2208 {0, 1} \u2200s \u2208 S, a \u2208 Act.", "startOffset": 202, "endOffset": 208}, {"referenceID": 18, "context": "Nondeterministic choices of actions in SGs and MDPs are resolved schedulers ; here it suffices to consider memoryless deterministic schedulers [19].", "startOffset": 143, "endOffset": 147}, {"referenceID": 0, "context": "A reachability property asserts that a set T \u2286 S of target states is to be reached from the initial state with probability at most \u03bb \u2208 [0, 1], denoted P\u2264\u03bb(\u2666T ).", "startOffset": 135, "endOffset": 141}, {"referenceID": 7, "context": "Verification can be performed by computing maximal (or minimal) probabilities or expected rewards to reach target states using standard techniques, such as linear programming, value iteration, or policy iteration [8].", "startOffset": 213, "endOffset": 216}, {"referenceID": 3, "context": "(a) From [4] (with permission) Wpt Litt Obst inith Goal", "startOffset": 9, "endOffset": 12}, {"referenceID": 3, "context": "The environment is given as Env = (Loc,Feat) with Loc = {(x, y) | x \u2208 [0, 4] y \u2208 [0, 4]}, and", "startOffset": 70, "endOffset": 76}, {"referenceID": 3, "context": "The environment is given as Env = (Loc,Feat) with Loc = {(x, y) | x \u2208 [0, 4] y \u2208 [0, 4]}, and", "startOffset": 81, "endOffset": 87}, {"referenceID": 6, "context": "\u03b1h \u2208 Orient = {i \u00b7 1 4\u03c0 | i \u2208 [0, 7]}.", "startOffset": 30, "endOffset": 36}, {"referenceID": 3, "context": "We adopt the assumption from [4] that for each objective o, only the closest feature of type f(o) is relevant for the behaviour with respect to o.", "startOffset": 29, "endOffset": 32}, {"referenceID": 3, "context": "We assume that we have Q-tables as in [4].", "startOffset": 38, "endOffset": 41}, {"referenceID": 3, "context": "Alike to [4], we translate the set of closest relevant features", "startOffset": 9, "endOffset": 12}, {"referenceID": 0, "context": "[0, 1] (1, 2] (2, 3] .", "startOffset": 0, "endOffset": 6}, {"referenceID": 0, "context": "[0, 1] (1, 2] (2, 3] .", "startOffset": 0, "endOffset": 6}, {"referenceID": 0, "context": "[0, 1] (1, 2] (2, 3] .", "startOffset": 0, "endOffset": 6}, {"referenceID": 3, "context": "obtained by IRL as in [4].", "startOffset": 22, "endOffset": 25}, {"referenceID": 0, "context": "The stochastic behaviour of the human is obtained by translating any vector x 6= \u2212\u221e for (F,x) \u2208 V (s) for some situation s to a distribution over movements, by means of a softmax -function [1]: R\u221e \u2192 [0, 1]n \u2013 which attributes most, but not all probability to the maximum, hence the name.", "startOffset": 189, "endOffset": 192}, {"referenceID": 0, "context": "The stochastic behaviour of the human is obtained by translating any vector x 6= \u2212\u221e for (F,x) \u2208 V (s) for some situation s to a distribution over movements, by means of a softmax -function [1]: R\u221e \u2192 [0, 1]n \u2013 which attributes most, but not all probability to the maximum, hence the name.", "startOffset": 199, "endOffset": 205}, {"referenceID": 11, "context": "0beta3 [12].", "startOffset": 7, "endOffset": 11}, {"referenceID": 7, "context": "We support two options for encoding the objective-reward: (1) Rescale the rewards to state-action rewards (preserving for expected reward measures [8]).", "startOffset": 147, "endOffset": 150}, {"referenceID": 3, "context": "Following a line and giving a penalty for any diverging move (as in [4]) would provide a notion of progress.", "startOffset": 68, "endOffset": 71}, {"referenceID": 13, "context": "[14].", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "It would be interesting to regularise the model on the learning side, or use techniques like -bisimulation [20].", "startOffset": 107, "endOffset": 111}, {"referenceID": 20, "context": "Sensitivity analysis over Q-table values would be of interest but current parameter synthesis techniques based on rational functions [21,22] cannot cope with exponentials.", "startOffset": 133, "endOffset": 140}, {"referenceID": 21, "context": "Sensitivity analysis over Q-table values would be of interest but current parameter synthesis techniques based on rational functions [21,22] cannot cope with exponentials.", "startOffset": 133, "endOffset": 140}, {"referenceID": 22, "context": "For future work, we would also like to investigate automatic abstraction techniques as in [23] and restrict exploration of the model as in [24], as well as sensitivity analysis and/or model repair, potentially based on techniques presented in [22] or those in [25].", "startOffset": 90, "endOffset": 94}, {"referenceID": 23, "context": "For future work, we would also like to investigate automatic abstraction techniques as in [23] and restrict exploration of the model as in [24], as well as sensitivity analysis and/or model repair, potentially based on techniques presented in [22] or those in [25].", "startOffset": 139, "endOffset": 143}, {"referenceID": 21, "context": "For future work, we would also like to investigate automatic abstraction techniques as in [23] and restrict exploration of the model as in [24], as well as sensitivity analysis and/or model repair, potentially based on techniques presented in [22] or those in [25].", "startOffset": 243, "endOffset": 247}], "year": 2016, "abstractText": "This paper proposes to use probabilistic model checking to synthesize optimal robot policies in multi-tasking autonomous systems that are subject to human-robot interaction. Given the convincing empirical evidence that human behavior can be related to reinforcement models, we take as input a well-studied Q-table model of the human behavior for flexible scenarios. We first describe an automated procedure to distill a Markov decision process (MDP) for the human in an arbitrary but fixed scenario. The distinctive issue is that \u2013 in contrast to existing models \u2013 under-specification of the human behavior is included. Probabilistic model checking is used to predict the human\u2019s behavior. Finally, the MDP model is extended with a robot model. Optimal robot policies are synthesized by analyzing the resulting two-player stochastic game. Experimental results with a prototypical implementation using PRISM show promising results.", "creator": "LaTeX with hyperref package"}}}