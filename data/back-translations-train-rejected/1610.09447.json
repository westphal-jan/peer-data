{"id": "1610.09447", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-Oct-2016", "title": "Asynchronous Stochastic Block Coordinate Descent with Variance Reduction", "abstract": "In the big data era, both of the sample size and dimension could be huge at the same time. Asynchronous parallel technology was recently proposed to handle the big data. Specifically, asynchronous stochastic gradient descent algorithms were recently proposed to scale the sample size, and asynchronous stochastic coordinate descent algorithms were proposed to scale the dimension. However, a few existing asynchronous parallel algorithms can scale well in sample size and dimension simultaneously. In this paper, we focus on a composite objective function consists of a smooth convex function f and a separable convex function g. We propose an asynchronous doubly stochastic proximal optimization algorithm with variance reduction (AsyDSPOVR) to scale well with the sample size and dimension simultaneously. We prove that AsyDSPOVR achieves a linear convergence rate when the function f is with the optimal strong convexity property, and a sublinear rate when f is with the general convexity.", "histories": [["v1", "Sat, 29 Oct 2016 03:39:16 GMT  (17kb)", "https://arxiv.org/abs/1610.09447v1", null], ["v2", "Mon, 7 Nov 2016 20:05:31 GMT  (17kb)", "http://arxiv.org/abs/1610.09447v2", null], ["v3", "Mon, 14 Nov 2016 02:14:36 GMT  (19kb)", "http://arxiv.org/abs/1610.09447v3", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["bin gu", "zhouyuan huo", "heng huang"], "accepted": false, "id": "1610.09447"}, "pdf": {"name": "1610.09447.pdf", "metadata": {"source": "CRF", "title": "Asynchronous Stochastic Block Coordinate Descent with Variance Reduction", "authors": ["Bin Gu", "Zhouyuan Huo", "Heng Huang"], "emails": ["jsgubin@gmail.com", "zhouyuan.huo@mavs.uta.edu", "heng@uta.edu"], "sections": [{"heading": null, "text": "In 2015 and 2015, the following methods were proposed: SGD algorithms with different types of acceleration technologies (Zhang, 2004; Johnson and Zhang, 2013; Schmidt et al., 2013; Ghadimi and Lan, 2013; Reddi et al., 2015) were proposed to process large-format convex algorithms with different types of acceleration technologies (Zhang, 2004); stochastic coordinate descent (SCD) algorithms (Taka, 2014; Zhao et al., 2014; Lu and Xiao, 2015) and stochastic dual coordination ascent algorithms (Shalev-Shwartz, 2014) were also proposed."}], "references": [{"title": "Revisiting asynchronous linear solvers: Provable convergence rate through randomization", "author": ["Haim Avron", "Alex Druinsky", "Anshul Gupta"], "venue": "Journal of the ACM (JACM),", "citeRegEx": "Avron et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Avron et al\\.", "year": 2015}, {"title": "Block coordinate descent algorithms for large-scale sparse multiclass classification", "author": ["Mathieu Blondel", "Kazuhiro Seki", "Kuniaki Uehara"], "venue": "Machine learning,", "citeRegEx": "Blondel et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Blondel et al\\.", "year": 2013}, {"title": "Generalization of the cauchy-schwarz inequality", "author": ["DK Callebaut"], "venue": "Journal of mathematical analysis and applications,", "citeRegEx": "Callebaut.,? \\Q1965\\E", "shortCiteRegEx": "Callebaut.", "year": 1965}, {"title": "Asynchronous stochastic convex optimization: the noise is in the noise and sgd don\u2019t care", "author": ["Sorathan Chaturapruek", "John C Duchi", "Christopher R\u00e9"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Chaturapruek et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chaturapruek et al\\.", "year": 2015}, {"title": "Stochastic first-and zeroth-order methods for nonconvex stochastic programming", "author": ["Saeed Ghadimi", "Guanghui Lan"], "venue": "SIAM Journal on Optimization,", "citeRegEx": "Ghadimi and Lan.,? \\Q2013\\E", "shortCiteRegEx": "Ghadimi and Lan.", "year": 2013}, {"title": "Passcode: Parallel asynchronous stochastic dual co-ordinate descent", "author": ["Cho-Jui Hsieh", "Hsiang-Fu Yu", "Inderjit S Dhillon"], "venue": "arXiv preprint,", "citeRegEx": "Hsieh et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hsieh et al\\.", "year": 2015}, {"title": "Asynchronous stochastic gradient descent with variance reduction for non-convex optimization", "author": ["Zhouyuan Huo", "Heng Huang"], "venue": "arXiv preprint arXiv:1604.03584,", "citeRegEx": "Huo and Huang.,? \\Q2016\\E", "shortCiteRegEx": "Huo and Huang.", "year": 2016}, {"title": "Distributed asynchronous dual-free stochastic dual coordinate ascent", "author": ["Zhouyuan Huo", "Heng Huang"], "venue": "arXiv preprint arXiv:1605.09066,", "citeRegEx": "Huo and Huang.,? \\Q2016\\E", "shortCiteRegEx": "Huo and Huang.", "year": 2016}, {"title": "Decoupled asynchronous proximal stochastic gradient descent with variance reduction", "author": ["Zhouyuan Huo", "Bin Gu", "Heng Huang"], "venue": "arXiv preprint arXiv:1609.06804,", "citeRegEx": "Huo et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Huo et al\\.", "year": 2016}, {"title": "Accelerating stochastic gradient descent using predictive variance reduction", "author": ["Rie Johnson", "Tong Zhang"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Johnson and Zhang.,? \\Q2013\\E", "shortCiteRegEx": "Johnson and Zhang.", "year": 2013}, {"title": "Asynchronous parallel stochastic gradient for nonconvex optimization", "author": ["Xiangru Lian", "Yijun Huang", "Yuncheng Li", "Ji Liu"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Lian et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lian et al\\.", "year": 2015}, {"title": "A comprehensive linear speedup analysis for asynchronous stochastic parallel optimization from zerothorder to first-order", "author": ["Xiangru Lian", "Huan Zhang", "Cho-Jui Hsieh", "Yijun Huang", "Ji Liu"], "venue": "arXiv preprint arXiv:1606.00498,", "citeRegEx": "Lian et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Lian et al\\.", "year": 2016}, {"title": "An accelerated proximal coordinate gradient method", "author": ["Qihang Lin", "Zhaosong Lu", "Lin Xiao"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Lin et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2014}, {"title": "Asynchronous stochastic coordinate descent: Parallelism and convergence properties", "author": ["Ji Liu", "Stephen J Wright"], "venue": "SIAM Journal on Optimization,", "citeRegEx": "Liu and Wright.,? \\Q2015\\E", "shortCiteRegEx": "Liu and Wright.", "year": 2015}, {"title": "An asynchronous parallel stochastic coordinate descent algorithm", "author": ["Ji Liu", "Stephen J Wright", "Christopher R\u00e9", "Victor Bittorf", "Srikrishna Sridhar"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Liu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2015}, {"title": "On the complexity analysis of randomized block-coordinate descent methods", "author": ["Zhaosong Lu", "Lin Xiao"], "venue": "Mathematical Programming,", "citeRegEx": "Lu and Xiao.,? \\Q2015\\E", "shortCiteRegEx": "Lu and Xiao.", "year": 2015}, {"title": "Perturbed iterate analysis for asynchronous stochastic optimization", "author": ["Horia Mania", "Xinghao Pan", "Dimitris Papailiopoulos", "Benjamin Recht", "Kannan Ramchandran", "Michael I Jordan"], "venue": "arXiv preprint arXiv:1507.06970,", "citeRegEx": "Mania et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mania et al\\.", "year": 2015}, {"title": "Stochastic proximal gradient descent with acceleration techniques", "author": ["Atsushi Nitanda"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Nitanda.,? \\Q2014\\E", "shortCiteRegEx": "Nitanda.", "year": 2014}, {"title": "Hogwild: A lock-free approach to parallelizing stochastic gradient descent", "author": ["Benjamin Recht", "Christopher Re", "Stephen Wright", "Feng Niu"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Recht et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Recht et al\\.", "year": 2011}, {"title": "On variance reduction in stochastic gradient descent and its asynchronous variants", "author": ["Sashank J Reddi", "Ahmed Hefny", "Suvrit Sra", "Barnabas Poczos", "Alex J Smola"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Reddi et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Reddi et al\\.", "year": 2015}, {"title": "Parallel coordinate descent methods for big data optimization", "author": ["Peter Richt\u00e1rik", "Martin Tak\u00e1\u010d"], "venue": "Mathematical Programming,", "citeRegEx": "Richt\u00e1rik and Tak\u00e1\u010d.,? \\Q2016\\E", "shortCiteRegEx": "Richt\u00e1rik and Tak\u00e1\u010d.", "year": 2016}, {"title": "The group-lasso for generalized linear models: uniqueness of solutions and efficient algorithms", "author": ["Volker Roth", "Bernd Fischer"], "venue": "In Proceedings of the 25th international conference on Machine learning,", "citeRegEx": "Roth and Fischer.,? \\Q2008\\E", "shortCiteRegEx": "Roth and Fischer.", "year": 2008}, {"title": "Nonlinear Optimization. Number 13 in Nonlinear optimization", "author": ["Andrzej P Ruszczy\u0144ski"], "venue": null, "citeRegEx": "Ruszczy\u0144ski.,? \\Q2006\\E", "shortCiteRegEx": "Ruszczy\u0144ski.", "year": 2006}, {"title": "Minimizing finite sums with the stochastic average gradient", "author": ["Mark Schmidt", "Nicolas Le Roux", "Francis Bach"], "venue": "arXiv preprint arXiv:1309.2388,", "citeRegEx": "Schmidt et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Schmidt et al\\.", "year": 2013}, {"title": "SDCA without duality, regularization, and individual convexity", "author": ["Shai Shalev-Shwartz"], "venue": "In Proceedings of the 33nd International Conference on Machine Learning,", "citeRegEx": "Shalev.Shwartz.,? \\Q2016\\E", "shortCiteRegEx": "Shalev.Shwartz.", "year": 2016}, {"title": "Accelerated proximal stochastic dual coordinate ascent for regularized loss minimization", "author": ["Shai Shalev-Shwartz", "Tong Zhang"], "venue": "In ICML,", "citeRegEx": "Shalev.Shwartz and Zhang.,? \\Q2014\\E", "shortCiteRegEx": "Shalev.Shwartz and Zhang.", "year": 2014}, {"title": "Randomized coordinate descent methods for big data optimization", "author": ["Martin Tak\u00e1c"], "venue": null, "citeRegEx": "Tak\u00e1c.,? \\Q2014\\E", "shortCiteRegEx": "Tak\u00e1c.", "year": 2014}, {"title": "Regression shrinkage and selection via the lasso", "author": ["Robert Tibshirani"], "venue": "Journal of the Royal Statistical Society. Series B (Methodological),", "citeRegEx": "Tibshirani.,? \\Q1996\\E", "shortCiteRegEx": "Tibshirani.", "year": 1996}, {"title": "A proximal stochastic gradient method with progressive variance reduction", "author": ["Lin Xiao", "Tong Zhang"], "venue": "SIAM Journal on Optimization,", "citeRegEx": "Xiao and Zhang.,? \\Q2014\\E", "shortCiteRegEx": "Xiao and Zhang.", "year": 2014}, {"title": "Solving large scale linear prediction problems using stochastic gradient descent algorithms", "author": ["Tong Zhang"], "venue": "In Proceedings of the twenty-first international conference on Machine learning,", "citeRegEx": "Zhang.,? \\Q2004\\E", "shortCiteRegEx": "Zhang.", "year": 2004}, {"title": "Fast asynchronous parallel stochastic gradient descent: A lock-free approach with convergence guarantee", "author": ["Shen-Yi Zhao", "Wu-Jun Li"], "venue": "In Thirtieth AAAI Conference on Artificial Intelligence,", "citeRegEx": "Zhao and Li.,? \\Q2016\\E", "shortCiteRegEx": "Zhao and Li.", "year": 2016}, {"title": "Accelerated mini-batch randomized block coordinate descent method", "author": ["Tuo Zhao", "Mo Yu", "Yiming Wang", "Raman Arora", "Han Liu"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Zhao et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zhao et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 29, "context": "Specifically, stochastic gradient descent (SGD) algorithms with various kinds of acceleration technologies (Zhang, 2004; Johnson and Zhang, 2013; Schmidt et al., 2013; Ghadimi and Lan, 2013; Reddi et al., 2015) were proposed to processing large scale smooth convex or nonconvex problems.", "startOffset": 107, "endOffset": 210}, {"referenceID": 9, "context": "Specifically, stochastic gradient descent (SGD) algorithms with various kinds of acceleration technologies (Zhang, 2004; Johnson and Zhang, 2013; Schmidt et al., 2013; Ghadimi and Lan, 2013; Reddi et al., 2015) were proposed to processing large scale smooth convex or nonconvex problems.", "startOffset": 107, "endOffset": 210}, {"referenceID": 23, "context": "Specifically, stochastic gradient descent (SGD) algorithms with various kinds of acceleration technologies (Zhang, 2004; Johnson and Zhang, 2013; Schmidt et al., 2013; Ghadimi and Lan, 2013; Reddi et al., 2015) were proposed to processing large scale smooth convex or nonconvex problems.", "startOffset": 107, "endOffset": 210}, {"referenceID": 4, "context": "Specifically, stochastic gradient descent (SGD) algorithms with various kinds of acceleration technologies (Zhang, 2004; Johnson and Zhang, 2013; Schmidt et al., 2013; Ghadimi and Lan, 2013; Reddi et al., 2015) were proposed to processing large scale smooth convex or nonconvex problems.", "startOffset": 107, "endOffset": 210}, {"referenceID": 19, "context": "Specifically, stochastic gradient descent (SGD) algorithms with various kinds of acceleration technologies (Zhang, 2004; Johnson and Zhang, 2013; Schmidt et al., 2013; Ghadimi and Lan, 2013; Reddi et al., 2015) were proposed to processing large scale smooth convex or nonconvex problems.", "startOffset": 107, "endOffset": 210}, {"referenceID": 26, "context": "Also, stochastic coordinate descent (SCD) algorithms (Tak\u00e1c, 2014; Zhao et al., 2014; Lu and Xiao, 2015) and stochastic dual coordinate ascent algorithms (Shalev-Shwartz and Zhang, 2014; Shalev-Shwartz, 2016) were proposed.", "startOffset": 53, "endOffset": 104}, {"referenceID": 31, "context": "Also, stochastic coordinate descent (SCD) algorithms (Tak\u00e1c, 2014; Zhao et al., 2014; Lu and Xiao, 2015) and stochastic dual coordinate ascent algorithms (Shalev-Shwartz and Zhang, 2014; Shalev-Shwartz, 2016) were proposed.", "startOffset": 53, "endOffset": 104}, {"referenceID": 15, "context": "Also, stochastic coordinate descent (SCD) algorithms (Tak\u00e1c, 2014; Zhao et al., 2014; Lu and Xiao, 2015) and stochastic dual coordinate ascent algorithms (Shalev-Shwartz and Zhang, 2014; Shalev-Shwartz, 2016) were proposed.", "startOffset": 53, "endOffset": 104}, {"referenceID": 25, "context": ", 2014; Lu and Xiao, 2015) and stochastic dual coordinate ascent algorithms (Shalev-Shwartz and Zhang, 2014; Shalev-Shwartz, 2016) were proposed.", "startOffset": 76, "endOffset": 130}, {"referenceID": 24, "context": ", 2014; Lu and Xiao, 2015) and stochastic dual coordinate ascent algorithms (Shalev-Shwartz and Zhang, 2014; Shalev-Shwartz, 2016) were proposed.", "startOffset": 76, "endOffset": 130}, {"referenceID": 28, "context": "For non-smoothing problems, the corresponding proximal algorithms were proposed (Xiao and Zhang, 2014; Lin et al., 2014; Nitanda, 2014).", "startOffset": 80, "endOffset": 135}, {"referenceID": 12, "context": "For non-smoothing problems, the corresponding proximal algorithms were proposed (Xiao and Zhang, 2014; Lin et al., 2014; Nitanda, 2014).", "startOffset": 80, "endOffset": 135}, {"referenceID": 17, "context": "For non-smoothing problems, the corresponding proximal algorithms were proposed (Xiao and Zhang, 2014; Lin et al., 2014; Nitanda, 2014).", "startOffset": 80, "endOffset": 135}, {"referenceID": 20, "context": "To scale up the stochastic optimization algorithms, asynchronous parallel implementations (Richt\u00e1rik and Tak\u00e1\u010d, 2016; Chaturapruek et al., 2015; Liu et al., 2015; Recht et al., 2011; Reddi et al., 2015; Lian et al., 2016; Liu and Wright, 2015; Zhao and Li, 2016; Huo and Huang, 2016a; Lian et al., 2015; Avron et al., 2015; Hsieh et al., 2015; Mania et al., 2015; Huo et al., 2016; Huo and Huang, 2016b) have been proposed recently, and received huge successes.", "startOffset": 90, "endOffset": 403}, {"referenceID": 3, "context": "To scale up the stochastic optimization algorithms, asynchronous parallel implementations (Richt\u00e1rik and Tak\u00e1\u010d, 2016; Chaturapruek et al., 2015; Liu et al., 2015; Recht et al., 2011; Reddi et al., 2015; Lian et al., 2016; Liu and Wright, 2015; Zhao and Li, 2016; Huo and Huang, 2016a; Lian et al., 2015; Avron et al., 2015; Hsieh et al., 2015; Mania et al., 2015; Huo et al., 2016; Huo and Huang, 2016b) have been proposed recently, and received huge successes.", "startOffset": 90, "endOffset": 403}, {"referenceID": 14, "context": "To scale up the stochastic optimization algorithms, asynchronous parallel implementations (Richt\u00e1rik and Tak\u00e1\u010d, 2016; Chaturapruek et al., 2015; Liu et al., 2015; Recht et al., 2011; Reddi et al., 2015; Lian et al., 2016; Liu and Wright, 2015; Zhao and Li, 2016; Huo and Huang, 2016a; Lian et al., 2015; Avron et al., 2015; Hsieh et al., 2015; Mania et al., 2015; Huo et al., 2016; Huo and Huang, 2016b) have been proposed recently, and received huge successes.", "startOffset": 90, "endOffset": 403}, {"referenceID": 18, "context": "To scale up the stochastic optimization algorithms, asynchronous parallel implementations (Richt\u00e1rik and Tak\u00e1\u010d, 2016; Chaturapruek et al., 2015; Liu et al., 2015; Recht et al., 2011; Reddi et al., 2015; Lian et al., 2016; Liu and Wright, 2015; Zhao and Li, 2016; Huo and Huang, 2016a; Lian et al., 2015; Avron et al., 2015; Hsieh et al., 2015; Mania et al., 2015; Huo et al., 2016; Huo and Huang, 2016b) have been proposed recently, and received huge successes.", "startOffset": 90, "endOffset": 403}, {"referenceID": 19, "context": "To scale up the stochastic optimization algorithms, asynchronous parallel implementations (Richt\u00e1rik and Tak\u00e1\u010d, 2016; Chaturapruek et al., 2015; Liu et al., 2015; Recht et al., 2011; Reddi et al., 2015; Lian et al., 2016; Liu and Wright, 2015; Zhao and Li, 2016; Huo and Huang, 2016a; Lian et al., 2015; Avron et al., 2015; Hsieh et al., 2015; Mania et al., 2015; Huo et al., 2016; Huo and Huang, 2016b) have been proposed recently, and received huge successes.", "startOffset": 90, "endOffset": 403}, {"referenceID": 11, "context": "To scale up the stochastic optimization algorithms, asynchronous parallel implementations (Richt\u00e1rik and Tak\u00e1\u010d, 2016; Chaturapruek et al., 2015; Liu et al., 2015; Recht et al., 2011; Reddi et al., 2015; Lian et al., 2016; Liu and Wright, 2015; Zhao and Li, 2016; Huo and Huang, 2016a; Lian et al., 2015; Avron et al., 2015; Hsieh et al., 2015; Mania et al., 2015; Huo et al., 2016; Huo and Huang, 2016b) have been proposed recently, and received huge successes.", "startOffset": 90, "endOffset": 403}, {"referenceID": 13, "context": "To scale up the stochastic optimization algorithms, asynchronous parallel implementations (Richt\u00e1rik and Tak\u00e1\u010d, 2016; Chaturapruek et al., 2015; Liu et al., 2015; Recht et al., 2011; Reddi et al., 2015; Lian et al., 2016; Liu and Wright, 2015; Zhao and Li, 2016; Huo and Huang, 2016a; Lian et al., 2015; Avron et al., 2015; Hsieh et al., 2015; Mania et al., 2015; Huo et al., 2016; Huo and Huang, 2016b) have been proposed recently, and received huge successes.", "startOffset": 90, "endOffset": 403}, {"referenceID": 30, "context": "To scale up the stochastic optimization algorithms, asynchronous parallel implementations (Richt\u00e1rik and Tak\u00e1\u010d, 2016; Chaturapruek et al., 2015; Liu et al., 2015; Recht et al., 2011; Reddi et al., 2015; Lian et al., 2016; Liu and Wright, 2015; Zhao and Li, 2016; Huo and Huang, 2016a; Lian et al., 2015; Avron et al., 2015; Hsieh et al., 2015; Mania et al., 2015; Huo et al., 2016; Huo and Huang, 2016b) have been proposed recently, and received huge successes.", "startOffset": 90, "endOffset": 403}, {"referenceID": 10, "context": "To scale up the stochastic optimization algorithms, asynchronous parallel implementations (Richt\u00e1rik and Tak\u00e1\u010d, 2016; Chaturapruek et al., 2015; Liu et al., 2015; Recht et al., 2011; Reddi et al., 2015; Lian et al., 2016; Liu and Wright, 2015; Zhao and Li, 2016; Huo and Huang, 2016a; Lian et al., 2015; Avron et al., 2015; Hsieh et al., 2015; Mania et al., 2015; Huo et al., 2016; Huo and Huang, 2016b) have been proposed recently, and received huge successes.", "startOffset": 90, "endOffset": 403}, {"referenceID": 0, "context": "To scale up the stochastic optimization algorithms, asynchronous parallel implementations (Richt\u00e1rik and Tak\u00e1\u010d, 2016; Chaturapruek et al., 2015; Liu et al., 2015; Recht et al., 2011; Reddi et al., 2015; Lian et al., 2016; Liu and Wright, 2015; Zhao and Li, 2016; Huo and Huang, 2016a; Lian et al., 2015; Avron et al., 2015; Hsieh et al., 2015; Mania et al., 2015; Huo et al., 2016; Huo and Huang, 2016b) have been proposed recently, and received huge successes.", "startOffset": 90, "endOffset": 403}, {"referenceID": 5, "context": "To scale up the stochastic optimization algorithms, asynchronous parallel implementations (Richt\u00e1rik and Tak\u00e1\u010d, 2016; Chaturapruek et al., 2015; Liu et al., 2015; Recht et al., 2011; Reddi et al., 2015; Lian et al., 2016; Liu and Wright, 2015; Zhao and Li, 2016; Huo and Huang, 2016a; Lian et al., 2015; Avron et al., 2015; Hsieh et al., 2015; Mania et al., 2015; Huo et al., 2016; Huo and Huang, 2016b) have been proposed recently, and received huge successes.", "startOffset": 90, "endOffset": 403}, {"referenceID": 16, "context": "To scale up the stochastic optimization algorithms, asynchronous parallel implementations (Richt\u00e1rik and Tak\u00e1\u010d, 2016; Chaturapruek et al., 2015; Liu et al., 2015; Recht et al., 2011; Reddi et al., 2015; Lian et al., 2016; Liu and Wright, 2015; Zhao and Li, 2016; Huo and Huang, 2016a; Lian et al., 2015; Avron et al., 2015; Hsieh et al., 2015; Mania et al., 2015; Huo et al., 2016; Huo and Huang, 2016b) have been proposed recently, and received huge successes.", "startOffset": 90, "endOffset": 403}, {"referenceID": 8, "context": "To scale up the stochastic optimization algorithms, asynchronous parallel implementations (Richt\u00e1rik and Tak\u00e1\u010d, 2016; Chaturapruek et al., 2015; Liu et al., 2015; Recht et al., 2011; Reddi et al., 2015; Lian et al., 2016; Liu and Wright, 2015; Zhao and Li, 2016; Huo and Huang, 2016a; Lian et al., 2015; Avron et al., 2015; Hsieh et al., 2015; Mania et al., 2015; Huo et al., 2016; Huo and Huang, 2016b) have been proposed recently, and received huge successes.", "startOffset": 90, "endOffset": 403}, {"referenceID": 23, "context": "For example, Zhao and Li (2016) proposed an asynchronous parallel algorithm for SVRG and proved the linear convergence.", "startOffset": 13, "endOffset": 32}, {"referenceID": 8, "context": "Liu and Wright (2015) proposed an asynchronous parallel algorithm for SCD and proved the linear convergence.", "startOffset": 0, "endOffset": 22}, {"referenceID": 0, "context": "Avron et al. (2015) proposed an asynchronous parallel algorithm for SCD to solve a linear system.", "startOffset": 0, "endOffset": 20}, {"referenceID": 0, "context": "Avron et al. (2015) proposed an asynchronous parallel algorithm for SCD to solve a linear system. Mania et al. (2015) proposed a perturbed iterate framework to analyze the asynchronous parallel algorithms of SGD, SCD and sparse SVRG.", "startOffset": 0, "endOffset": 118}, {"referenceID": 0, "context": "Avron et al. (2015) proposed an asynchronous parallel algorithm for SCD to solve a linear system. Mania et al. (2015) proposed a perturbed iterate framework to analyze the asynchronous parallel algorithms of SGD, SCD and sparse SVRG. Huo and Huang (2016a) proposed an asynchronous SGD algorithm with variance reduction on non-convex optimization problems and proved the convergence.", "startOffset": 0, "endOffset": 256}, {"referenceID": 0, "context": "Avron et al. (2015) proposed an asynchronous parallel algorithm for SCD to solve a linear system. Mania et al. (2015) proposed a perturbed iterate framework to analyze the asynchronous parallel algorithms of SGD, SCD and sparse SVRG. Huo and Huang (2016a) proposed an asynchronous SGD algorithm with variance reduction on non-convex optimization problems and proved the convergence. Lian et al. (2016) proposed an asynchronous stochastic optimization algorithm with zeroth order and proved the convergence.", "startOffset": 0, "endOffset": 402}, {"referenceID": 27, "context": "The formulation (1) covers many machine learning and computer vision problems, for example, Lasso (Tibshirani, 1996), group Lasso (Roth and Fischer, 2008), sparse multiclass classification (Blondel et al.", "startOffset": 98, "endOffset": 116}, {"referenceID": 21, "context": "The formulation (1) covers many machine learning and computer vision problems, for example, Lasso (Tibshirani, 1996), group Lasso (Roth and Fischer, 2008), sparse multiclass classification (Blondel et al.", "startOffset": 130, "endOffset": 154}, {"referenceID": 1, "context": "The formulation (1) covers many machine learning and computer vision problems, for example, Lasso (Tibshirani, 1996), group Lasso (Roth and Fischer, 2008), sparse multiclass classification (Blondel et al., 2013), and so on.", "startOffset": 189, "endOffset": 211}, {"referenceID": 13, "context": "Thus, the formulation in (Liu and Wright, 2015) is a special case of (1).", "startOffset": 25, "endOffset": 47}, {"referenceID": 13, "context": "Each iteration in (Liu and Wright, 2015) only modifies a single component of x which is an atomic operation in the parallel system with shared memory.", "startOffset": 18, "endOffset": 40}, {"referenceID": 13, "context": "However, we need to modify a block coordinate Gj of x with lockfree for each iteration, which are more complicated in the asynchronous parallel analysis than the atomic updating in (Liu and Wright, 2015).", "startOffset": 181, "endOffset": 203}, {"referenceID": 1, "context": "The formulation (1) covers many machine learning and computer vision problems, for example, Lasso (Tibshirani, 1996), group Lasso (Roth and Fischer, 2008), sparse multiclass classification (Blondel et al., 2013), and so on. Note that, Liu and Wright (2015) consider the formulation (1) with the constraints that |Gj | = 1 for all j.", "startOffset": 190, "endOffset": 257}, {"referenceID": 13, "context": "And several examples of optimally strongly convex functions that are not strongly convex are provided in (Liu and Wright, 2015).", "startOffset": 105, "endOffset": 127}, {"referenceID": 13, "context": "As mentioned in Liu and Wright (2015), the condition of optimal strong convexity is significantly weaker than the normal strong convexity condition.", "startOffset": 16, "endOffset": 38}, {"referenceID": 13, "context": "Convergence Analysis In this section, we follow the analysis of (Liu and Wright, 2015) and prove the convergence rate of AsyDSCDVR (Theorem 8).", "startOffset": 64, "endOffset": 86}, {"referenceID": 16, "context": "Same as mentioned in (Mania et al., 2015), there might not be an actual time the idea ones exist in the shared memory, except the first and last iterates for each outer loop.", "startOffset": 21, "endOffset": 41}, {"referenceID": 22, "context": "x\\Gj(t) = (x s t )\\Gj(t) If xt+1 is the solution of (18), the solution of optimization problem (19) is also x s t+1 according to the subdifferential version of Karush-Kuhn-Tucker (KKT) conditions (Ruszczy\u0144ski., 2006).", "startOffset": 196, "endOffset": 216}, {"referenceID": 13, "context": "8) in Liu and Wright (2015), we have \u2016xt\u22121 \u2212 xt\u2016 \u2212 \u2016xt \u2212 xt+1\u2016 \u2264 2\u2016xt\u22121 \u2212 xt\u2016\u2016xt \u2212 xt+1 \u2212 xt\u22121 + xt\u2016 (21) The second part in the right half side of (21) is bound as follows if B = {it} and J(t) = {j(t)}.", "startOffset": 6, "endOffset": 28}, {"referenceID": 13, "context": "7 of Liu and Wright (2015), the sixth inequality comes from \u2016xt \u2212 x\u0303s\u2016 = \u2016 \u2211t\u22121 t\u2032=0\u2206 s t\u2032\u2016 \u2264 \u2211t\u22121 t=0 \u2016\u2206t\u2032\u2016.", "startOffset": 5, "endOffset": 27}, {"referenceID": 13, "context": "13 in (Liu and Wright, 2015).", "startOffset": 6, "endOffset": 28}, {"referenceID": 2, "context": "where the first inequality uses the Cauchy-Schwarz inequality (Callebaut, 1965), the third inequality uses (3) and (4), the sixth inequality uses (29).", "startOffset": 62, "endOffset": 79}, {"referenceID": 13, "context": "28) of Liu and Wright (2015). E (F (x)\u2212 F ) \u2265 Lmaxl 2(l\u03b3 + Lmax) S(x) (45)", "startOffset": 7, "endOffset": 29}], "year": 2016, "abstractText": "Asynchronous parallel implementations for stochastic optimization have received huge successes in theory and practice recently. Asynchronous implementations with lock-free are more efficient than the one with writing or reading lock. In this paper, we focus on a composite objective function consisting of a smooth convex function f and a block separable convex function, which widely exists in machine learning and computer vision. We propose an asynchronous stochastic block coordinate descent algorithm with the accelerated technology of variance reduction (AsySBCDVR), which are with lock-free in the implementation and analysis. AsySBCDVR is particularly important because it can scale well with the sample size and dimension simultaneously. We prove that AsySBCDVR achieves a linear convergence rate when the function f is with the optimal strong convexity property, and a sublinear rate when f is with the general convexity. More importantly, a near-linear speedup on a parallel system with shared memory can be obtained.", "creator": "LaTeX with hyperref package"}}}