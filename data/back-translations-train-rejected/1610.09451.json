{"id": "1610.09451", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-Oct-2016", "title": "KeystoneML: Optimizing Pipelines for Large-Scale Advanced Analytics", "abstract": "Modern advanced analytics applications make use of machine learning techniques and contain multiple steps of domain-specific and general-purpose processing with high resource requirements. We present KeystoneML, a system that captures and optimizes the end-to-end large-scale machine learning applications for high-throughput training in a distributed environment with a high-level API. This approach offers increased ease of use and higher performance over existing systems for large scale learning. We demonstrate the effectiveness of KeystoneML in achieving high quality statistical accuracy and scalable training using real world datasets in several domains. By optimizing execution KeystoneML achieves up to 15x training throughput over unoptimized execution on a real image classification application.", "histories": [["v1", "Sat, 29 Oct 2016 04:21:24 GMT  (653kb,D)", "http://arxiv.org/abs/1610.09451v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.DC", "authors": ["evan r sparks", "shivaram venkataraman", "tomer kaftan", "michael j franklin", "benjamin recht"], "accepted": false, "id": "1610.09451"}, "pdf": {"name": "1610.09451.pdf", "metadata": {"source": "CRF", "title": "KeystoneML: Optimizing Pipelines for Large-Scale Advanced Analytics", "authors": ["Evan R. Sparks", "Shivaram Venkataraman", "Tomer Kaftan", "Michael Franklin", "Benjamin Recht"], "emails": ["sparks@cs.berkeley.edu", "shivaram@cs.berkeley.edu", "tomerk11@cs.berkeley.edu", "franklin@cs.berkeley.edu", "brecht@cs.berkeley.edu"], "sections": [{"heading": "1 Introduction", "text": "In this year, it is time for us to get to work, to travel around the world, to travel around the world, to travel around the world, to travel around the world, to travel around the world, to travel around the world, to travel around the world, to save the world, to travel around the world, to travel around the world, to travel around the world, to see the world, to save the world, to save the world, to save the world, to save the world, to save the world, to save the world, to save the world, to save the world, to save the world, to save the world, to save the world, to save the world, to save the world, to save the world, to save the world, to save the world, to save the world, to save the world, to the world, to the world, to the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world."}, {"heading": "2 Pipeline Construction and Core APIs", "text": "In this section, we introduce the KeystoneML API, which expresses end-to-end ML pipelines. Each pipeline consists of multiple concatenated operators. For example, Figure 2 shows the KeystoneML source code for a complete text classification pipeline. Next, we describe the building blocks of our API."}, {"heading": "2.1 Logical ML Operators", "text": "This well-defined environment enables important optimizations. However, ML applications lack such abstraction, and practitioners typically work on imperative libraries. [57] In contrast, we propose a design in which high-level ML operators (such as PCA), convex optimization routines, and multidimensional arrays are used as logical building blocks. [57] In contrast, we are able to perform high-level ML operations."}, {"heading": "2.2 Pipeline Construction", "text": "The concatenation methods are summarized in Figure 4. In addition to linear concatenation of nodes using andThen, KeystoneML's API allows for pipeline branching when a developer calls and then returns a new pipeline object. By calling andThen multiple times within the same pipeline, they can create multiple pipelines that branch off. Developers join the output of multiple pipelines using collect. Redundancy is eliminated by a subexpression optimization detailed in Section 4. We find that these APIs are sufficient for a number of ML applications (Section 5.1), but expect them to expand over time."}, {"heading": "2.3 Pipeline Execution", "text": "KeystoneML is designed to run with large, distributed datasets on raw material clusters. Our high-level API and optimizers can be run with any distributed data flow machine. KeystoneML's execution flow is illustrated in Figure 1. First, developers define pipelines using the KeystoneML APIs described above. As these APIs are called, KeystoneML gradually builds an operator DAG for the pipeline. An example operator DAG for image classification is shown in Figure 5. Once a pipeline is applied to some data, this DAG is optimized with a series of optimizations described below - we call this phase optimization time. Once the application is optimized, the DAG is first run at depth and operators are executed individually, with nodes to pipeline breakers (i.e., stimulators) packed into the same job - this phase is runtime optimization.This is the full optimization process about the application."}, {"heading": "3 Operator-Level Optimization", "text": "This year, it is as far as ever in the history of the city, where it is as far as never before."}, {"heading": "4 Whole-Pipeline Optimization", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Execution Subsampling", "text": "Operator optimization in KeystoneML requires the acquisition of statistics on input data at each stage of the pipeline. For example, a text marker operator can map a string into a 10,000-dimensional sparse feature vector. Without input statistics (e.g. vector sparseness) after identification, a downstream operator cannot make an optimization decision. Thus, data statistics (As) are determined by first estimating the size of the original input data set (in records) and optimizing the first operator in the pipeline with statistics from a sample of input data. Then, the optimized operator is executed at the sample and the subsequent operators are optimized. This process continues until all nodes are optimized. Along the way, we form a pipeline profile that not only contains the information needed to form As in each step, but also information on the execution time and memory consumption of each operator at the automated sample we use to form the optimization sample at the bottom."}, {"heading": "4.2 Common Sub-expression Elimination", "text": "It is common for training data or the output of feature levels to be used in several stages of a pipeline. As a concrete example, in a text classification pipeline, we could first symbolize training data, then identify the 100,000 most common bigrams in a text corpus, combine the data into a binary vector that indicates the presence of each bigram, and then train a classifier on the same training data. Therefore, we need the bigrams of each document for both the calculation of the most common features and the training of the classifier. KeystoneML identifies and fuses such common sub-expressions to enable the re-use of the calculation."}, {"heading": "4.3 Automatic Materialization", "text": "We have described a formulation for the materialization problem in the pipelines, which automatically selects a set of intermediate objects to identify the possibilities for reuse, eliminating redundant compilation. Next, we describe the formulation for the materialization problem in the pipelines and propose to materialize a set of intermediary objects to speed up the execution of ML pipelines.Given the depth of the first execution model and the deterministic free nature of the KeystoneML operators that are visited several times during execution."}, {"heading": "5 Evaluation", "text": "In order to evaluate the effectiveness of KeystoneML, we will examine its ability to efficiently support large ML applications in three areas. We will also compare KeystoneML with other systems for large ML applications and show how our high-level operators and optimizations can improve performance. Afterwards, we will break down the end-to-end benefits of the optimizations discussed above. Finally, we will evaluate the scalability and performance of the system and show that KeystoneML is well scalable by enabling the development of scalable, composable components. Implementation: We will implement KeystoneML on Apache Spark, a cluster computing engine that has been shown to have good scalability and performance for many iterative ML algorithms [44]. In KeystoneML, we have added an additional cache management layer that is aware of the numerous Spark jobs that comprise a pipeline, KeystoneML and non-implemented Spark ML in the Spark library."}, {"heading": "5.1 End-to-End ML Applications", "text": "In fact, most of us are in a position to put ourselves in another world, in which we put ourselves in another world, in which we put ourselves in another world, in which we do not find ourselves, in which we do not find ourselves, in which we do not find ourselves, in which we do not find ourselves, in which we do not find ourselves, in which we do not find ourselves, in which we do not find ourselves, in which we do not find ourselves, in which we do not find ourselves, in which we do not find ourselves, in which we do not find ourselves, in which we do not find ourselves, in which we do not find ourselves."}, {"heading": "5.2 KeystoneML vs. Other Systems", "text": "This year, it has reached the stage where it will be able to take the lead in order to achieve the objectives I have mentioned."}, {"heading": "5.3 Optimization Levels", "text": "To understand the importance of optimizations per operator and per pipeline, as described in Sections 3 and 4, we compare three different levels of optimization: a default non-optimized configuration (None), a configuration that uses only optimizations per operator and per pipeline (Pipe Only), and a configuration that includes optimizations at the operator level and across the pipeline (KeystoneML). Comparing these levels with a tiered-level breakdown of time requirements on the VOC, Amazon, and TIMIT pipelines is shown in Figure 9. For the Amazon pipeline, optimizations of the entire pipeline improve performance by 7 \u00d7, but operator optimizations do not help because the Amazon pipeline uses CoreNLP optimizers that do not have statistical optimizations, and the standard L-BFGS solution proves optimal."}, {"heading": "5.4 Automatic Materialization Strategies", "text": "This year, it has come to the point that it will only be a matter of time before it is ready, until it is ready."}, {"heading": "5.5 Scalability", "text": "As discussed in previous sections, Keystone ML's API design encourages the construction of scalable operators, but some estimators such as linear solvers require coordination [18] between workers to calculate correct results. In Figure 12, we show the scaling properties of 8 to 128 nodes of text, image, and kernel SVM pipelines on the Amazon, ImageNet (with 16k features), and TIMIT datasets (with 65k features), respectively. ImageNet's pipeline has near-perfect horizontal scalability of up to 128 nodes, while the Amazon and TIMIT pipelines scale up to 64 nodes. To understand why the Amazon and TIMIT pipelines do not scale linearly to 128 nodes, we continue to analyze the timing at each stage. We see that each pipeline is dominated by a different part of its calculation."}, {"heading": "6 Related Work", "text": "This year, it is only a matter of time before there is a result in which there is a result."}, {"heading": "7 Future Work and Conclusion", "text": "KeystoneML represents a significant first step toward achieving easy-to-use, robust, and efficient end-to-end ML on a large scale. We plan to explore pipeline optimizations such as rearranging nodes to reduce data transmission, and also how to integrate hyperparameter tuning [56] into the system. Existing KeystoneML operator APIs are synchronized, and our existing pipelines are acyclic. In the future, we plan to explore how algorithms such as asynchronous SGD [36] or Microsoft feedback can be integrated with the robustness and scalability that KeystoneML provides. We have presented the design of KeystoneML, a system that enables the development of end-to-end ML pipelines. By capturing the end-to-end application of Amazon Blue Blue Blue, KeystoneML can be optimized for both operator and full-level execution."}], "references": [{"title": "et al", "author": ["M. Abadi", "A. Agarwal", "P. Barham", "E. Brevdo"], "venue": "TensorFlow: Large-scale machine learning on heterogeneous systems", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2015}, {"title": "et al", "author": ["S. Abu-El-Haija", "N. Kothari", "J. Lee", "P. Natsev"], "venue": "YouTube-8M: A Large-Scale Video Classification Benchmark. arXiv preprint arXiv: 1609.08675", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2016}, {"title": "Caffe con Troll: Shallow Ideas to Speed Up Deep Learning", "author": ["F. Abuzaid", "S. Hadjis", "C. Zhang", "C. R\u00e9"], "venue": "CoRR abs/1504.04343", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2015}, {"title": "Avoiding Communication in Dense Linear Algebra", "author": ["G.M. Ballard"], "venue": "PhD thesis, University of California, Berkeley", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2013}, {"title": "A study of replacement algorithms for a virtual-storage computer", "author": ["L.A. Belady"], "venue": "IBM Systems journal, 5(2):78\u2013 101", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1966}, {"title": "et al", "author": ["M.R. Berthold", "N. Cebron", "F. Dill", "T.R. Gabriel"], "venue": "KNIME: The Konstanz information miner. In Data analysis, machine learning and applications, pages 319\u2013326. Springer", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2008}, {"title": "Parallel and distributed computation: numerical methods", "author": ["D.P. Bertsekas", "J.N. Tsitsiklis"], "venue": "Prentice-Hall, Inc.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1989}, {"title": "et al", "author": ["Z. Cai", "Z.J. Gao", "S. Luo", "L.L. Perez"], "venue": "A comparison of platforms for implementing and running very large scale machine learning algorithms. In SIGMOD 2014, pages 1371\u20131382", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2014}, {"title": "The devil is in the details: an evaluation of recent feature encoding methods", "author": ["K. Chatfield", "V. Lempitsky", "A. Vedaldi", "A. Zisserman"], "venue": "British Machine Vision Conference", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2011}, {"title": "AutoAdmin \u2019What-if\u2019 Index Analysis Utility", "author": ["S. Chaudhuri", "V.R. Narasayya"], "venue": "SIGMOD", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1998}, {"title": "Revisiting distributed synchronous sgd", "author": ["J. Chen", "R. Monga", "S. Bengio", "R. Jozefowicz"], "venue": "arXiv preprint arxiv:1604.00981", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2016}, {"title": "Large-scale l-bfgs using mapreduce", "author": ["W. Chen", "Z. Wang", "J. Zhou"], "venue": "NIPS, pages 1332\u20131340", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2014}, {"title": "Materialized Views", "author": ["R. Chirkova", "J. Yang"], "venue": "Foundations and Trends in Databases", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2012}, {"title": "Learning Feature Representations with K-Means", "author": ["A. Coates", "A.Y. Ng"], "venue": "In Neural Networks: Tricks of the Trade", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2012}, {"title": "Tupleware: Distributed machine learning on small clusters", "author": ["A. Crotty", "A. Galakatos", "T. Kraska"], "venue": "IEEE Data Eng. Bull, 37(3)", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2014}, {"title": "Communication-optimal parallel and sequential QR and LU factorizations", "author": ["J. Demmel", "L. Grigori", "M. Hoemmen", "J. Langou"], "venue": "SIAM Journal on Scientific Computing, 34(1):A206\u2013A239", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2012}, {"title": "ReStore: reusing results of MapReduce jobs", "author": ["I. Elghandour", "A. Aboulnaga"], "venue": "PVLDB", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2012}, {"title": "Towards a unified architecture for in-rdbms analytics", "author": ["X. Feng", "A. Kumar", "B. Recht", "C. R\u00e9"], "venue": "SIGMOD", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2012}, {"title": "et al", "author": ["A. Ghoting", "R. Krishnamurthy", "E. Pednault", "B. Reinwald"], "venue": "SystemML: Declarative machine learning on MapReduce. In ICDE, pages 231\u2013242. IEEE", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2011}, {"title": "The unreasonable effectiveness of data", "author": ["A. Halevy", "P. Norvig", "F. Pereira"], "venue": "Intelligent Systems, IEEE, 24(2):8\u2013 12", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2009}, {"title": "Finding Structure with Randomness: Probabilistic Algorithms for Constructing Approximate Matrix Decompositions", "author": ["N. Halko", "P.G. Martinsson", "J.A. Tropp"], "venue": "SIAM Review", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2011}, {"title": "Implementing Data Cubes Efficiently", "author": ["V. Harinarayan", "A. Rajaraman", "J.D. Ullman"], "venue": "SIGMOD, pages 205\u2013 216", "citeRegEx": "25", "shortCiteRegEx": null, "year": 1996}, {"title": "Query execution techniques for caching expensive methods", "author": ["J.M. Hellerstein", "J.F. Naughton"], "venue": "SIGMOD", "citeRegEx": "26", "shortCiteRegEx": null, "year": 1997}, {"title": "et al", "author": ["J.M. Hellerstein", "C. R\u00e9", "F. Schoppmann", "D.Z. Wang"], "venue": "The MADlib analytics library: or MAD skills, the SQL. PVLDB, 5(12):1700\u20131711", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2012}, {"title": "et al", "author": ["C.-W. Hsu", "C.-C. Chang", "C.-J. Lin"], "venue": "A practical guide to support vector classification. https://goo. gl/m68USr", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2003}, {"title": "Kernel methods match deep neural networks on timit", "author": ["P.-S. Huang", "H. Avron", "T.N. Sainath", "V. Sindhwani", "B. Ramabhadran"], "venue": "ICASSP, pages 205\u2013209. IEEE", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2014}, {"title": "Flare prediction using photospheric and coronal image data", "author": ["E. Jonas", "V. Shankar", "M. Bobra", "B. Recht"], "venue": "AGU Fall Meeting", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2016}, {"title": "The Click modular router", "author": ["E. Kohler", "R. Morris", "B. Chen", "J. Jannotti", "M.F. Kaashoek"], "venue": "ACM Transactions on Computer Systems (TOCS)", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2000}, {"title": "et al", "author": ["T. Kraska", "A. Talwalkar", "J.C. Duchi", "R. Griffith"], "venue": "MLbase: A Distributed Machine-learning System. CIDR", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2013}, {"title": "Convolutional Deep Belief Networks on CIFAR-10", "author": ["A. Krizhevsky", "G. Hinton"], "venue": "Unpublished manuscript", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2010}, {"title": "and A", "author": ["J. Langford", "L. Li"], "venue": "Strehl. Vowpal wabbit online learning project", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2007}, {"title": "LLVM: A Compilation Framework for Lifelong Program Analysis & Transformation", "author": ["C. Lattner", "V. Adve"], "venue": "CGO", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2004}, {"title": "et al", "author": ["M. Li", "D.G. Andersen", "J.W. Park", "A.J. Smola"], "venue": "Scaling Distributed Machine Learning with the Parameter Server. OSDI", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2014}, {"title": "et al", "author": ["Y. Low", "D. Bickson", "J. Gonzalez", "C. Guestrin"], "venue": "Distributed graphlab: a framework for machine learning and data mining in the cloud. PVLDB, 5(8):716\u2013727", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2012}, {"title": "Object recognition from local scale-invariant features", "author": ["D.G. Lowe"], "venue": "ICCV, volume 2, pages 1150\u20131157. IEEE", "citeRegEx": "38", "shortCiteRegEx": null, "year": 1999}, {"title": "Optimization", "author": ["C. Manning", "D. Klein"], "venue": "maxent models, and conditional estimation without magic. In HLT- NAACL, Tutorial Vol 5", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2003}, {"title": "et al", "author": ["C.D. Manning", "M. Surdeanu", "J. Bauer", "J. Finkel"], "venue": "The Stanford CoreNLP natural language processing toolkit. In ACL", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2014}, {"title": "Fast Training of Convolutional Networks through FFTs", "author": ["M. Mathieu", "M. Henaff", "Y. LeCun"], "venue": "ICLR", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2014}, {"title": "Inferring networks of substitutable and complementary products", "author": ["J. McAuley", "R. Pandey", "J. Leskovec"], "venue": "KDD, pages 785\u2013794", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2015}, {"title": "ML Pipelines: A New High-Level API for MLlib", "author": ["X. Meng", "J. Bradley", "E. Sparks", "S. Venkataraman"], "venue": "https: //goo.gl/pluhq0", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2015}, {"title": "et al", "author": ["X. Meng", "J.K. Bradley", "B. Yavuz", "E.R. Sparks"], "venue": "MLlib: Machine Learning in Apache Spark. CoRR, abs/1505.06807", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2015}, {"title": "et al", "author": ["F. Pedregosa", "G. Varoquaux", "A. Gramfort"], "venue": "Scikitlearn: Machine learning in Python. JMLR, 12:2825\u2013 2830", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2011}, {"title": "History-aware query optimization with materialized intermediate views", "author": ["L. Perez", "C. Jermaine"], "venue": "In ICDE,", "citeRegEx": "46", "shortCiteRegEx": "46", "year": 2014}, {"title": "Scalable I/O-bound Parallel Incremental Gradient Descent for Big Data Analytics in GLADE", "author": ["C. Qin", "F. Rusu"], "venue": "DanaC", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2013}, {"title": "Memory versus randomization in on-line algorithms", "author": ["P. Raghavan", "M. Snir"], "venue": "International Colloquium on Automata, Languages, and Programming, pages 687\u2013703. Springer", "citeRegEx": "48", "shortCiteRegEx": null, "year": 1989}, {"title": "Random features for large-scale kernel machines", "author": ["A. Rahimi", "B. Recht"], "venue": "NIPS, pages 1177\u20131184", "citeRegEx": "49", "shortCiteRegEx": null, "year": 2007}, {"title": "Hogwild!: A lockfree approach to parallelizing stochastic gradient descent", "author": ["B. Recht", "C. Re", "S. Wright", "F. Niu"], "venue": "NIPS, pages 693\u2013701", "citeRegEx": "50", "shortCiteRegEx": null, "year": 2011}, {"title": "Improved fisher vector for large scale image classification. http://image-net.org/challenges/ LSVRC/2010/ILSVRC2010_XRCE.pdf", "author": ["J. Sanchez", "F. Perronnin", "T. Mensink"], "venue": null, "citeRegEx": "51", "shortCiteRegEx": "51", "year": 2010}, {"title": "Image classification with the fisher vector: Theory and practice", "author": ["J. S\u00e1nchez", "F. Perronnin", "T. Mensink", "J. Verbeek"], "venue": "International journal of computer vision, 105(3):222\u2013245", "citeRegEx": "52", "shortCiteRegEx": null, "year": 2013}, {"title": "Machine learning: The high interest credit card of technical debt", "author": ["D. Sculley", "G. Holt", "D. Golovin", "E. Davydov", "T. Phillips", "D. Ebner", "V. Chaudhary", "M. Young"], "venue": "SE4ML: Software Engineering for Machine Learning ", "citeRegEx": "53", "shortCiteRegEx": null, "year": 2014}, {"title": "Access path selection in a relational database management system", "author": ["P.G. Selinger", "M.M. Astrahan", "D.D. Chamberlin", "R.A. Lorie", "T.G. Price"], "venue": null, "citeRegEx": "54", "shortCiteRegEx": "54", "year": 1979}, {"title": "High-performance kernel machines with implicit distributed optimization and randomization", "author": ["V. Sindhwani", "H. Avron"], "venue": "CoRR, abs/1409.0940", "citeRegEx": "55", "shortCiteRegEx": null, "year": 2014}, {"title": "et al", "author": ["E.R. Sparks", "A. Talwalkar", "D. Haas", "M.J. Franklin"], "venue": "Automating model search for large scale machine learning. In SoCC \u201915", "citeRegEx": "56", "shortCiteRegEx": null, "year": 2015}, {"title": "Requirements for science data bases and scidb", "author": ["M. Stonebraker", "J. Becla", "D.J. DeWitt", "K.-T. Lim", "D. Maier", "O. Ratzesberger", "S.B. Zdonik"], "venue": "CIDR, volume 7, pages 173\u2013184", "citeRegEx": "57", "shortCiteRegEx": null, "year": 2009}, {"title": "et al", "author": ["C. Szegedy", "V. Vanhoucke"], "venue": "Rethinking the inception architecture for computer vision. arXiv preprint arXiv:1512.00567", "citeRegEx": "58", "shortCiteRegEx": null, "year": 2015}, {"title": "Roofline: An Insightful Visual Performance Model for Multicore Architectures", "author": ["S. Williams", "A. Waterman", "D. Patterson"], "venue": "CACM", "citeRegEx": "59", "shortCiteRegEx": null, "year": 2009}, {"title": "Materialization optimizations for feature selection workloads", "author": ["C. Zhang", "A. Kumar", "C. R\u00e9"], "venue": "SIGMOD", "citeRegEx": "60", "shortCiteRegEx": null, "year": 2014}, {"title": "Dimmwitted: A study of mainmemory statistical analytics", "author": ["C. Zhang", "C. R\u00e9"], "venue": "PVLDB, 7(12):1283\u20131294", "citeRegEx": "61", "shortCiteRegEx": null, "year": 2014}, {"title": "RIOT: I/O-Efficient Numerical Computing without SQL", "author": ["Y. Zhang", "H. Herodotou", "J. Yang"], "venue": "CIDR", "citeRegEx": "62", "shortCiteRegEx": null, "year": 2009}, {"title": "Recommending Materialized Views and Indexes with IBM DB2 Design Advisor", "author": ["D.C. Zilio", "C. Zuzarte", "G.M. Lohman", "H. Pirahesh"], "venue": "ICAC", "citeRegEx": "63", "shortCiteRegEx": "63", "year": 2004}], "referenceMentions": [{"referenceID": 35, "context": "Today\u2019s advanced analytics applications increasingly use machine learning (ML) as a core technique in areas ranging from business intelligence to recommendation to natural language processing [39] and speech recognition [29].", "startOffset": 192, "endOffset": 196}, {"referenceID": 25, "context": "Today\u2019s advanced analytics applications increasingly use machine learning (ML) as a core technique in areas ranging from business intelligence to recommendation to natural language processing [39] and speech recognition [29].", "startOffset": 220, "endOffset": 224}, {"referenceID": 48, "context": "Practitioners build complex, multi-stage pipelines involving feature extraction, dimensionality reduction, data transformations, and training supervised learning models to achieve high accuracy [52].", "startOffset": 194, "endOffset": 198}, {"referenceID": 30, "context": "To assemble such pipelines, developers typically piece together domain specific libraries1 for feature extraction and general purpose numerical optimization packages [34, 44] for supervised learning.", "startOffset": 166, "endOffset": 174}, {"referenceID": 40, "context": "To assemble such pipelines, developers typically piece together domain specific libraries1 for feature extraction and general purpose numerical optimization packages [34, 44] for supervised learning.", "startOffset": 166, "endOffset": 174}, {"referenceID": 49, "context": "This is often a cumbersome and error-prone process [53].", "startOffset": 51, "endOffset": 55}, {"referenceID": 19, "context": "Further, these pipelines need to be completely re-engineered when the training data or features grow by an order of magnitude\u2013 often the difference between an application that provides good statistical accuracy and one that does not [23].", "startOffset": 233, "endOffset": 237}, {"referenceID": 23, "context": "While existing efforts in the data management community [27, 21, 44] and in the broader machine learning systems community [34, 45, 3] have built systems to address some of these problems, each of them misses the mark on at least one of the points above.", "startOffset": 56, "endOffset": 68}, {"referenceID": 18, "context": "While existing efforts in the data management community [27, 21, 44] and in the broader machine learning systems community [34, 45, 3] have built systems to address some of these problems, each of them misses the mark on at least one of the points above.", "startOffset": 56, "endOffset": 68}, {"referenceID": 40, "context": "While existing efforts in the data management community [27, 21, 44] and in the broader machine learning systems community [34, 45, 3] have built systems to address some of these problems, each of them misses the mark on at least one of the points above.", "startOffset": 56, "endOffset": 68}, {"referenceID": 30, "context": "While existing efforts in the data management community [27, 21, 44] and in the broader machine learning systems community [34, 45, 3] have built systems to address some of these problems, each of them misses the mark on at least one of the points above.", "startOffset": 123, "endOffset": 134}, {"referenceID": 41, "context": "While existing efforts in the data management community [27, 21, 44] and in the broader machine learning systems community [34, 45, 3] have built systems to address some of these problems, each of them misses the mark on at least one of the points above.", "startOffset": 123, "endOffset": 134}, {"referenceID": 0, "context": "While existing efforts in the data management community [27, 21, 44] and in the broader machine learning systems community [34, 45, 3] have built systems to address some of these problems, each of them misses the mark on at least one of the points above.", "startOffset": 123, "endOffset": 134}, {"referenceID": 57, "context": "While a significant body of recent work has focused on high performance algorithms [61, 50],", "startOffset": 83, "endOffset": 91}, {"referenceID": 46, "context": "While a significant body of recent work has focused on high performance algorithms [61, 50],", "startOffset": 83, "endOffset": 91}, {"referenceID": 14, "context": "and scalable implementations [17, 44] for model training, they do not capture the featurization process or the logical intent of the workflow.", "startOffset": 29, "endOffset": 37}, {"referenceID": 40, "context": "and scalable implementations [17, 44] for model training, they do not capture the featurization process or the logical intent of the workflow.", "startOffset": 29, "endOffset": 37}, {"referenceID": 28, "context": "To optimize ML pipelines, database query optimization provides a natural motivation for the core design of such a system [32].", "startOffset": 121, "endOffset": 125}, {"referenceID": 46, "context": "Second, many ML operators provide only approximate answers to their inputs [50].", "startOffset": 75, "endOffset": 79}, {"referenceID": 18, "context": "Finally, the system should be aware of the computationvs-communication tradeoffs inherent in distributed processing of ML workloads [21, 34] and choose appropriate execution strategies in this regime.", "startOffset": 132, "endOffset": 140}, {"referenceID": 30, "context": "Finally, the system should be aware of the computationvs-communication tradeoffs inherent in distributed processing of ML workloads [21, 34] and choose appropriate execution strategies in this regime.", "startOffset": 132, "endOffset": 140}, {"referenceID": 48, "context": "Using an image classification pipeline on over 1M images [52], we show that KeystoneML provides linear performance scalability across various cluster sizes, and statistical performance comparable to recent results [11, 52].", "startOffset": 57, "endOffset": 61}, {"referenceID": 8, "context": "Using an image classification pipeline on over 1M images [52], we show that KeystoneML provides linear performance scalability across various cluster sizes, and statistical performance comparable to recent results [11, 52].", "startOffset": 214, "endOffset": 222}, {"referenceID": 48, "context": "Using an image classification pipeline on over 1M images [52], we show that KeystoneML provides linear performance scalability across various cluster sizes, and statistical performance comparable to recent results [11, 52].", "startOffset": 214, "endOffset": 222}, {"referenceID": 26, "context": "KeystoneML is open source software2 and is being used in scientific applications in solar physics [30] and genomics [2]", "startOffset": 98, "endOffset": 102}, {"referenceID": 18, "context": "using linear algebra operators such as matrix multiplication [21], convex optimization routines [20] or multidimensional arrays as logical building blocks [57].", "startOffset": 61, "endOffset": 65}, {"referenceID": 17, "context": "using linear algebra operators such as matrix multiplication [21], convex optimization routines [20] or multidimensional arrays as logical building blocks [57].", "startOffset": 96, "endOffset": 100}, {"referenceID": 53, "context": "using linear algebra operators such as matrix multiplication [21], convex optimization routines [20] or multidimensional arrays as logical building blocks [57].", "startOffset": 155, "endOffset": 159}, {"referenceID": 18, "context": "But this makes designing an optimizer more challenging because unlike relational operators or linear algebra [21], the set of operators in KeystoneML is not closed.", "startOffset": 109, "endOffset": 113}, {"referenceID": 55, "context": "The functions, cexec, and ccoord are developer-defined operator-specific functions (defined as part of the operator CostModel) that describe execution and coordination costs in terms of the longest critical path in the execution graph of the individual operators [59], e.", "startOffset": 263, "endOffset": 267}, {"referenceID": 3, "context": "Such functions are also used in the analysis of parallel algorithms [6] and are well known for common linear algebra based operators.", "startOffset": 68, "endOffset": 71}, {"referenceID": 15, "context": "\u2022 Exact solvers [18] that compute closed form solutions to the least squares loss and return an X to extremely high precision.", "startOffset": 16, "endOffset": 20}, {"referenceID": 6, "context": "\u2022 Block solvers that partition the features into a set of blocks and use second-order Jacobi or GaussSeidel [9] updates to converge to the right solution.", "startOffset": 108, "endOffset": 111}, {"referenceID": 46, "context": "\u2022 Gradient based methods like SGD [50] or LBFGS [14] which perform iterative updates using the gradient and converge to a globally optimal solution.", "startOffset": 34, "endOffset": 38}, {"referenceID": 11, "context": "\u2022 Gradient based methods like SGD [50] or LBFGS [14] which perform iterative updates using the gradient and converge to a globally optimal solution.", "startOffset": 48, "endOffset": 52}, {"referenceID": 20, "context": "cated SVD [24].", "startOffset": 10, "endOffset": 14}, {"referenceID": 2, "context": "a matrix-vector product scheme when convolutions are separable, using BLAS matrix-matrix multiplication [5], or via a Fast Fourier Transform (FFT) [41].", "startOffset": 104, "endOffset": 107}, {"referenceID": 37, "context": "a matrix-vector product scheme when convolutions are separable, using BLAS matrix-matrix multiplication [5], or via a Fast Fourier Transform (FFT) [41].", "startOffset": 147, "endOffset": 151}, {"referenceID": 12, "context": "Cache management and automatic selection of materialized views are important optimizations used by database management systems [15] and they have been studied in the context of analytical query systems [63, 25], and feature selection [60].", "startOffset": 127, "endOffset": 131}, {"referenceID": 59, "context": "Cache management and automatic selection of materialized views are important optimizations used by database management systems [15] and they have been studied in the context of analytical query systems [63, 25], and feature selection [60].", "startOffset": 202, "endOffset": 210}, {"referenceID": 21, "context": "Cache management and automatic selection of materialized views are important optimizations used by database management systems [15] and they have been studied in the context of analytical query systems [63, 25], and feature selection [60].", "startOffset": 202, "endOffset": 210}, {"referenceID": 56, "context": "Cache management and automatic selection of materialized views are important optimizations used by database management systems [15] and they have been studied in the context of analytical query systems [63, 25], and feature selection [60].", "startOffset": 234, "endOffset": 238}, {"referenceID": 4, "context": "It is tempting to reach for classical results [7, 48] in the optimal paging literature to identify an optimal or near-optimal schedule for this problem.", "startOffset": 46, "endOffset": 53}, {"referenceID": 44, "context": "It is tempting to reach for classical results [7, 48] in the optimal paging literature to identify an optimal or near-optimal schedule for this problem.", "startOffset": 46, "endOffset": 53}, {"referenceID": 40, "context": "Implementation: We implement KeystoneML on top of Apache Spark, a cluster computing engine that has been shown to have good scalability and performance for many iterative ML algorithms [44].", "startOffset": 185, "endOffset": 189}, {"referenceID": 30, "context": "We use OpenBLAS for numerical operations and Vowpal Wabbit [34] v8.", "startOffset": 59, "endOffset": 63}, {"referenceID": 18, "context": "0 and SystemML [21] v0.", "startOffset": 15, "endOffset": 19}, {"referenceID": 36, "context": "Combined with libraries like CoreNLP [40], KeystoneML allows for scalable implementations of many text classification pipelines such as the one shown in Figure 2.", "startOffset": 37, "endOffset": 41}, {"referenceID": 35, "context": "We evaluated a text classification pipeline based on [39] on the Amazon Reviews dataset of 65m product reviews [42] with 100k sparse features.", "startOffset": 53, "endOffset": 57}, {"referenceID": 38, "context": "We evaluated a text classification pipeline based on [39] on the Amazon Reviews dataset of 65m product reviews [42] with 100k sparse features.", "startOffset": 111, "endOffset": 115}, {"referenceID": 30, "context": "We find that KeystoneML matches the statistical performance of a Vowpal Wabbit [34] pipeline when run on identical resources", "startOffset": 79, "endOffset": 83}, {"referenceID": 24, "context": "Often their performance has been shown to be much better than simpler generalized linear models [28].", "startOffset": 96, "endOffset": 100}, {"referenceID": 45, "context": "Kernel evaluations can be efficiently approximated using random feature transformations [49, 55] and pipelines are a natural way to specify such transformations.", "startOffset": 88, "endOffset": 96}, {"referenceID": 51, "context": "Kernel evaluations can be efficiently approximated using random feature transformations [49, 55] and pipelines are a natural way to specify such transformations.", "startOffset": 88, "endOffset": 96}, {"referenceID": 51, "context": "By contrast, a 256 node IBM Blue Gene machine with 16 cores per machine takes around 120 minutes [55].", "startOffset": 97, "endOffset": 101}, {"referenceID": 48, "context": "convolutions or spatially-pooled fisher vectors [52], can be used to generate training features.", "startOffset": 48, "endOffset": 52}, {"referenceID": 34, "context": "KeystoneML makes it easy to modularize the pipeline to use efficient implementations of image processing operators like SIFT [38] and Fisher Vectors [52, 11].", "startOffset": 125, "endOffset": 129}, {"referenceID": 48, "context": "KeystoneML makes it easy to modularize the pipeline to use efficient implementations of image processing operators like SIFT [38] and Fisher Vectors [52, 11].", "startOffset": 149, "endOffset": 157}, {"referenceID": 8, "context": "KeystoneML makes it easy to modularize the pipeline to use efficient implementations of image processing operators like SIFT [38] and Fisher Vectors [52, 11].", "startOffset": 149, "endOffset": 157}, {"referenceID": 29, "context": "Many of the same operators we consider here are necessary components of \u201cdeep-learning\u201d pipelines [33] which typically train neural networks via stochastic gradient descent and backpropagation.", "startOffset": 98, "endOffset": 102}, {"referenceID": 8, "context": "Using the VOC dataset, we implement the pipeline described in [11].", "startOffset": 62, "endOffset": 66}, {"referenceID": 47, "context": "The original pipeline takes four days [51] to run using a highly specialized codebase on a 16-core maDataset KeystoneML Reported Accuracy Time (m) Accuracy Time (m)", "startOffset": 38, "endOffset": 42}, {"referenceID": 35, "context": "Amazon [39] 91.", "startOffset": 7, "endOffset": 11}, {"referenceID": 25, "context": "3 TIMIT [29] 66.", "startOffset": 8, "endOffset": 12}, {"referenceID": 48, "context": "33% 120 ImageNet [52]3 67.", "startOffset": 17, "endOffset": 21}, {"referenceID": 8, "context": "58% 5760 VOC 2007 [11] 57.", "startOffset": 18, "endOffset": 22}, {"referenceID": 30, "context": "We compare runtimes for the KeystoneML solver with both a specialized system, Vowpal Wabbit [34], built to estimate linear models, and SystemML [21], a general purpose ML system, which optimizes the implementation of linear algebra operators used in specific algorithms (e.", "startOffset": 92, "endOffset": 96}, {"referenceID": 18, "context": "We compare runtimes for the KeystoneML solver with both a specialized system, Vowpal Wabbit [34], built to estimate linear models, and SystemML [21], a general purpose ML system, which optimizes the implementation of linear algebra operators used in specific algorithms (e.", "startOffset": 144, "endOffset": 148}, {"referenceID": 0, "context": "TensorFlow is a newly open-sourced ML system developed by Google [3].", "startOffset": 65, "endOffset": 68}, {"referenceID": 10, "context": "8 and adapt a multi-GPU example [1] to a distributed setting in a procedure similar to [13].", "startOffset": 87, "endOffset": 91}, {"referenceID": 29, "context": "Specifically, TensorFlow implements a model similar to the one presented in [33], while in KeystoneML we implement a version of the model similar to [16].", "startOffset": 76, "endOffset": 80}, {"referenceID": 13, "context": "Specifically, TensorFlow implements a model similar to the one presented in [33], while in KeystoneML we implement a version of the model similar to [16].", "startOffset": 149, "endOffset": 153}, {"referenceID": 54, "context": "While TensorFlow has better scalability on some model architectures [58], it is not scalable for other architectures.", "startOffset": 68, "endOffset": 72}, {"referenceID": 1, "context": "Finally, a recent benchmark dataset from YouTube [4] describes learning pipelines involving featurization with a neural network [58] followed by a logistic regression model or SVM.", "startOffset": 49, "endOffset": 52}, {"referenceID": 54, "context": "Finally, a recent benchmark dataset from YouTube [4] describes learning pipelines involving featurization with a neural network [58] followed by a logistic regression model or SVM.", "startOffset": 128, "endOffset": 132}, {"referenceID": 15, "context": "However, some estimators like linear solvers need coordination [18] among workers to compute correct results.", "startOffset": 63, "endOffset": 67}, {"referenceID": 15, "context": "Scaling linear solvers is known to require coordination [18], which leads directly to sub-linear scalability of the whole pipeline.", "startOffset": 56, "endOffset": 60}, {"referenceID": 41, "context": "The importance of feature engineering has led to tools like scikit-learn [45] and KNIME [8] adding support for featurization for small datasets.", "startOffset": 73, "endOffset": 77}, {"referenceID": 5, "context": "The importance of feature engineering has led to tools like scikit-learn [45] and KNIME [8] adding support for featurization for small datasets.", "startOffset": 88, "endOffset": 91}, {"referenceID": 7, "context": "Further, existing libraries for large scale ML [10] like Vowpal Wabbit [34], GraphLab [37], MLlib [44], RIOT [62], DimmWitted [61] focus on efficient implementations of learning algorithms like regression, classification and linear alge-", "startOffset": 47, "endOffset": 51}, {"referenceID": 30, "context": "Further, existing libraries for large scale ML [10] like Vowpal Wabbit [34], GraphLab [37], MLlib [44], RIOT [62], DimmWitted [61] focus on efficient implementations of learning algorithms like regression, classification and linear alge-", "startOffset": 71, "endOffset": 75}, {"referenceID": 33, "context": "Further, existing libraries for large scale ML [10] like Vowpal Wabbit [34], GraphLab [37], MLlib [44], RIOT [62], DimmWitted [61] focus on efficient implementations of learning algorithms like regression, classification and linear alge-", "startOffset": 86, "endOffset": 90}, {"referenceID": 40, "context": "Further, existing libraries for large scale ML [10] like Vowpal Wabbit [34], GraphLab [37], MLlib [44], RIOT [62], DimmWitted [61] focus on efficient implementations of learning algorithms like regression, classification and linear alge-", "startOffset": 98, "endOffset": 102}, {"referenceID": 58, "context": "Further, existing libraries for large scale ML [10] like Vowpal Wabbit [34], GraphLab [37], MLlib [44], RIOT [62], DimmWitted [61] focus on efficient implementations of learning algorithms like regression, classification and linear alge-", "startOffset": 109, "endOffset": 113}, {"referenceID": 57, "context": "Further, existing libraries for large scale ML [10] like Vowpal Wabbit [34], GraphLab [37], MLlib [44], RIOT [62], DimmWitted [61] focus on efficient implementations of learning algorithms like regression, classification and linear alge-", "startOffset": 126, "endOffset": 130}, {"referenceID": 32, "context": "Work in Parameter Servers [36] has studied how to share model updates.", "startOffset": 26, "endOffset": 30}, {"referenceID": 18, "context": "Closely related to KeystoneML is SystemML [21] which also uses an optimization based approach to determine the physical execution strategy of ML algorithms.", "startOffset": 42, "endOffset": 46}, {"referenceID": 56, "context": "Other work [60, 5] has looked at optimizing caching strategies and operator selection in the regime of feature selection and feature generation workloads.", "startOffset": 11, "endOffset": 18}, {"referenceID": 2, "context": "Other work [60, 5] has looked at optimizing caching strategies and operator selection in the regime of feature selection and feature generation workloads.", "startOffset": 11, "endOffset": 18}, {"referenceID": 0, "context": "Developed concurrently to KeystoneML is TensorFlow [3].", "startOffset": 51, "endOffset": 54}, {"referenceID": 17, "context": "Projects such as Bismarck [20], MADLib [27], and GLADE [47] have proposed techniques to integrate ML algorithms inside database engines.", "startOffset": 26, "endOffset": 30}, {"referenceID": 23, "context": "Projects such as Bismarck [20], MADLib [27], and GLADE [47] have proposed techniques to integrate ML algorithms inside database engines.", "startOffset": 39, "endOffset": 43}, {"referenceID": 43, "context": "Projects such as Bismarck [20], MADLib [27], and GLADE [47] have proposed techniques to integrate ML algorithms inside database engines.", "startOffset": 55, "endOffset": 59}, {"referenceID": 39, "context": "Finally, Spark ML [43] represents an early design of a similar high-level API for machine learning.", "startOffset": 18, "endOffset": 22}, {"referenceID": 31, "context": "Finally, the concept of using a high-level programming model has been explored in a number of other contexts, including compilers [35] and networking [31].", "startOffset": 130, "endOffset": 134}, {"referenceID": 27, "context": "Finally, the concept of using a high-level programming model has been explored in a number of other contexts, including compilers [35] and networking [31].", "startOffset": 150, "endOffset": 154}, {"referenceID": 50, "context": "Even the earliest relational query optimizers [54] used multiple physical implementations of equivalent logical operators, and like many relational optimizers, the KeystoneML optimizer is cost-based.", "startOffset": 46, "endOffset": 50}, {"referenceID": 12, "context": "The caching strategy employed by KeystoneML can be viewed as a form of view selection for materialized view maintenance over queries with expensive userdefined functions [15, 26], we focus on materialization for intra-query optimization, as opposed to inter-query optimization [25, 12, 63, 19, 46].", "startOffset": 170, "endOffset": 178}, {"referenceID": 22, "context": "The caching strategy employed by KeystoneML can be viewed as a form of view selection for materialized view maintenance over queries with expensive userdefined functions [15, 26], we focus on materialization for intra-query optimization, as opposed to inter-query optimization [25, 12, 63, 19, 46].", "startOffset": 170, "endOffset": 178}, {"referenceID": 21, "context": "The caching strategy employed by KeystoneML can be viewed as a form of view selection for materialized view maintenance over queries with expensive userdefined functions [15, 26], we focus on materialization for intra-query optimization, as opposed to inter-query optimization [25, 12, 63, 19, 46].", "startOffset": 277, "endOffset": 297}, {"referenceID": 9, "context": "The caching strategy employed by KeystoneML can be viewed as a form of view selection for materialized view maintenance over queries with expensive userdefined functions [15, 26], we focus on materialization for intra-query optimization, as opposed to inter-query optimization [25, 12, 63, 19, 46].", "startOffset": 277, "endOffset": 297}, {"referenceID": 59, "context": "The caching strategy employed by KeystoneML can be viewed as a form of view selection for materialized view maintenance over queries with expensive userdefined functions [15, 26], we focus on materialization for intra-query optimization, as opposed to inter-query optimization [25, 12, 63, 19, 46].", "startOffset": 277, "endOffset": 297}, {"referenceID": 16, "context": "The caching strategy employed by KeystoneML can be viewed as a form of view selection for materialized view maintenance over queries with expensive userdefined functions [15, 26], we focus on materialization for intra-query optimization, as opposed to inter-query optimization [25, 12, 63, 19, 46].", "startOffset": 277, "endOffset": 297}, {"referenceID": 42, "context": "The caching strategy employed by KeystoneML can be viewed as a form of view selection for materialized view maintenance over queries with expensive userdefined functions [15, 26], we focus on materialization for intra-query optimization, as opposed to inter-query optimization [25, 12, 63, 19, 46].", "startOffset": 277, "endOffset": 297}, {"referenceID": 52, "context": "We plan to investigate pipeline optimizations like node reordering to reduce data transfers and also look at how hyperparameter tuning [56] can be integrated into the system.", "startOffset": 135, "endOffset": 139}, {"referenceID": 32, "context": "In the future we plan to study how algorithms like asynchronous SGD [36] or back-propagation can be integrated with the robustness and scalability that KeystoneML provides.", "startOffset": 68, "endOffset": 72}], "year": 2016, "abstractText": "Modern advanced analytics applications make use of machine learning techniques and contain multiple steps of domain-specific and general-purpose processing with high resource requirements. We present KeystoneML, a system that captures and optimizes the end-to-end largescale machine learning applications for high-throughput training in a distributed environment with a high-level API. This approach offers increased ease of use and higher performance over existing systems for large scale learning. We demonstrate the effectiveness of KeystoneML in achieving high quality statistical accuracy and scalable training using real world datasets in several domains. By optimizing execution KeystoneML achieves up to 15\u00d7 training throughput over unoptimized execution on a real image classification application.", "creator": "LaTeX with hyperref package"}}}