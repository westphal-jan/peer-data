{"id": "1610.09565", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-Oct-2016", "title": "Sequence-to-sequence neural network models for transliteration", "abstract": "Transliteration is a key component of machine translation systems and software internationalization. This paper demonstrates that neural sequence-to-sequence models obtain state of the art or close to state of the art results on existing datasets. In an effort to make machine transliteration accessible, we open source a new Arabic to English transliteration dataset and our trained models.", "histories": [["v1", "Sat, 29 Oct 2016 19:21:19 GMT  (15kb,D)", "http://arxiv.org/abs/1610.09565v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["mihaela rosca", "thomas breuel"], "accepted": false, "id": "1610.09565"}, "pdf": {"name": "1610.09565.pdf", "metadata": {"source": "CRF", "title": "Sequence-to-sequence neural network models for transliteration", "authors": ["Mihaela Rosca", "Thomas Breuel"], "emails": ["mihaelacr@google.com", "tmb@google.com"], "sections": [{"heading": "1 Introduction", "text": "Transliteration - the conversion of proper names from one orthographic system to another - is an important task in multilingual word processing, useful in applications such as online mapping and as a component of machine translation systems. Transliteration is determined by the capture of historical accidents, conventions and statistical regularities: many language pairs have adopted different rules for transliteration in the past, and many transliterations depend on the origin of a word. These characteristics make it desirable to search for high-quality automated machine learning solutions for machine transliteration. Such models assume that character sequences in source orthography correspond to predictable character sequences, possibly depending on the context. Some models also assume that such correspondence is influenced by phonetic information."}, {"heading": "2 Models and datasets", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Epsilon Insertion", "text": "Epsilon insertion (EI) (Azawi et al., 2013) is a simple technique that allows sequence-to-sequence models to generate strings of varying lengths from an input string. Epsilon insertion replaces the original problem of transliteration with a similar problem where the source string was modified by inserting epsilons (which we will present as \"\" \"), transliteration is done by an LSTM (possibly bidirectional and deep), and output is aligned using CTC. \u2022 Source string: KLSTM1, an open source C + + library."}, {"heading": "2.2 Attentional Sequence-to-Sequence Models", "text": "Attention sequence-to-sequence models (Bahdanau et al., 2014) (Seq2Seq) work with an encoder RNN to learn representations of the input sequence, and a decoder RNN to generate the output sequence from the hidden representations created by the encoder. The attention sequence mechanism allows the decoder to focus on different parts of the input sequence for each step in the output sequence, and can be considered an analogy of the alignment mechanism used in traditional statistical translation models. Sequence sequence models do not have the implicit monotonicity assumption that unidirectional CTC models do, so they are more flexible in terms of rearranging the input data. This is critical for machine translation, but less important for transliteration, where the tone sequence is preserved from the source to the target. For sequence sequence models of a 2014 sequence sequence sequence sequence and the STM sequence experiments, and the use of the GRAM for the bidirectional submission."}, {"heading": "2.3 Datasets", "text": "The records we use are described in Table 2.3. For the transliteration from Arabic to English, we are introducing a new corpus extracted from Wikipedia: First, we have created a bilingual record full of names from titles of Arabic and English articles referring to the same person; second, we have used it to learn alignments between parts of names in order to create the final record. As no direction-specific information was used in the data collection, the data can be used for both English and English transliteration."}, {"heading": "3 Experimental results", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Training and parameters", "text": "For all experiments, we used 10% of the data for testing, 10% of the remaining data for evaluation and the rest for training. All networks were trained by gradient descent with impulse. To avoid exploding RNN gradients (Pascanu et al., 2012), EI models used a batch size of 1, gradient section norm of 9 and 3 epsilons. In training EI models, we randomly varied the learning rate (10 \u2212 5 to 0.1), pulse rate (0.5 to 0.99) and the number of hidden units (100 to 1000). For sequence-to-sequence models, we varied the following hyperparameters: learning rate (10 \u2212 5 to 10), pulse rate (0.5 to 0.99), batch size (1 to 50), gradient section norm (1 to 10), and number of hidden units (50 to 1000). For both models, we trained 1000 networks with different hyperparameter values, but a fixed number of best-selected and best-performed intervals within each of the predetermined evaluation and each of the selected ones."}, {"heading": "3.2 Results", "text": "Our results are described in Tables 2, 3 and 4. Table 3.2 compares our results with models trained on the same datasets. For the sake of completeness, we report other transliteration results, although they are not directly comparable to our work, since they used different datasets. Using statistical phonetic machine translation (Finch and Sumita, 2008), we obtain a 31% CER for transliteration from English to Japanese. (De-3http: / / www.speech.cs.cmu.edu / cgi-bin / cmudict4https: / / github.com / eob / english-Japanese-transliteration / 5https: / / github.com / googlei18n / transliterationselaers et al., 2009) use deep neural networks for Arabic-English transliteration, giving a CER of 22.7%, while traditional approaches combined with a single-layer Percepon achieved 11.1% for the same task in 2007 (Friday)."}, {"heading": "3.3 Error analysis", "text": "The most common mistake that both models examined make in this task is the confusion of vowels in the output. This is to be expected, since Arabic has fewer vowels than English and often short vowels are not written. Confusing \"p\" with \"b\" is another common mistake, due to the lack of a proper sound for the English \"p\" in Arabic."}, {"heading": "4 Discussion", "text": "In fact, most of them will be able to feel as if they are able to survive on their own."}, {"heading": "5 Acknowledgments", "text": "We would like to thank Andy Staudacher, Lara Scheidegger and Vincent Vanhoucke for their support in this work."}], "references": [{"title": "Tensorflow: Large-scale machine learning on heterogeneous distributed systems", "author": ["Abadi et al.2016] Mart\u0131n Abadi", "Ashish Agarwal", "Paul Barham", "Eugene Brevdo", "Zhifeng Chen", "Craig Citro", "Greg S Corrado", "Andy Davis", "Jeffrey Dean", "Matthieu Devin"], "venue": null, "citeRegEx": "Abadi et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Abadi et al\\.", "year": 2016}, {"title": "Electro-cortical manifestations of common vs. proper name processing during reading", "author": ["Mirella Manfredi", "Alice Mado Proverbio"], "venue": "Brain and language,", "citeRegEx": "Adorni et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Adorni et al\\.", "year": 2014}, {"title": "Transliteration by sequence labeling with lattice encodings and reranking", "author": ["Ammar et al.2012] Waleed Ammar", "Chris Dyer", "Noah A Smith"], "venue": "In Proceedings of the 4th Named Entity Workshop,", "citeRegEx": "Ammar et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Ammar et al\\.", "year": 2012}, {"title": "Normalizing historical orthography for ocr historical documents using lstm", "author": ["Azawi et al.2013] Mayce Al Azawi", "Muhammad Zeshan Afzal", "Thomas M Breuel"], "venue": "In Proceedings of the 2nd International Workshop on Historical Document Imaging and Process-", "citeRegEx": "Azawi et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Azawi et al\\.", "year": 2013}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Kyunghyun Cho", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1409.0473", "citeRegEx": "Bahdanau et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Listen, attend and spell", "author": ["Chan et al.2015] William Chan", "Navdeep Jaitly", "Quoc V Le", "Oriol Vinyals"], "venue": "arXiv preprint arXiv:1508.01211", "citeRegEx": "Chan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chan et al\\.", "year": 2015}, {"title": "A deep learning approach to machine transliteration", "author": ["Sa\u0161a Hasan", "Oliver Bender", "Hermann Ney"], "venue": "In Proceedings of the Fourth Workshop on Statistical Machine Translation,", "citeRegEx": "Deselaers et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Deselaers et al\\.", "year": 2009}, {"title": "Phrase-based machine transliteration", "author": ["Finch", "Sumita2008] Andrew Finch", "Eiichiro Sumita"], "venue": "In Proceedings of the Workshop on Technologies and Corpora for Asia-Pacific Speech Translation (TCAST),", "citeRegEx": "Finch et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Finch et al\\.", "year": 2008}, {"title": "A sequence alignment model based on the averaged perceptron", "author": ["Shahram Khadivi"], "venue": null, "citeRegEx": "Freitag and Khadivi,? \\Q2007\\E", "shortCiteRegEx": "Freitag and Khadivi", "year": 2007}, {"title": "Statistical transliterationalledfor cross language information retrieval using hmm alignment model and crf", "author": ["Ganesh et al.2008] Surya Ganesh", "Sree Harsha", "Prasad Pingali", "Vasudeva Verma"], "venue": "In Proceedings of the 2nd Workshop on Cross Lingual Infor-", "citeRegEx": "Ganesh et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Ganesh et al\\.", "year": 2008}, {"title": "Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks", "author": ["Graves et al.2006] Alex Graves", "Santiago Fern\u00e1ndez", "Faustino Gomez", "J\u00fcrgen Schmidhuber"], "venue": "In Proceedings of the 23rd international conference on", "citeRegEx": "Graves et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2006}, {"title": "Adaptive computation time for recurrent neural networks. arXiv preprint arXiv:1603.08983", "author": ["Alex Graves"], "venue": null, "citeRegEx": "Graves.,? \\Q2016\\E", "shortCiteRegEx": "Graves.", "year": 2016}, {"title": "Named entity transcription with pair ngram models", "author": ["Jansche", "Sproat2009] Martin Jansche", "Richard Sproat"], "venue": "In Proceedings of the 2009 Named Entities Workshop: Shared Task on Transliteration,", "citeRegEx": "Jansche et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Jansche et al\\.", "year": 2009}, {"title": "On the difficulty of training recurrent neural networks. arXiv preprint arXiv:1211.5063", "author": ["Tomas Mikolov", "Yoshua Bengio"], "venue": null, "citeRegEx": "Pascanu et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Pascanu et al\\.", "year": 2012}, {"title": "Grapheme-tophoneme conversion using long short-term memory recurrent neural networks", "author": ["Rao et al.2015] Kanishka Rao", "Fuchun Peng", "Hasim Sak", "Fran\u00e7oise Beaufays"], "venue": "In Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "Rao et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Rao et al\\.", "year": 2015}, {"title": "Sequence to sequence learning with neural networks. In Advances in neural information processing systems, pages 3104\u20133112", "author": ["Oriol Vinyals", "Quoc V Le"], "venue": null, "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Sequence-to-sequence neural net models for grapheme-to-phoneme conversion", "author": ["Yao", "Zweig2015] Kaisheng Yao", "Geoffrey Zweig"], "venue": "arXiv preprint arXiv:1506.00196", "citeRegEx": "Yao et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Yao et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 2, "context": "Typical examples of such models are (Ammar et al., 2012; Ganesh et al., 2008); transliteration in such a system is based on an alignment step, followed by a CRF model that performs local string rewriting.", "startOffset": 36, "endOffset": 77}, {"referenceID": 9, "context": "Typical examples of such models are (Ammar et al., 2012; Ganesh et al., 2008); transliteration in such a system is based on an alignment step, followed by a CRF model that performs local string rewriting.", "startOffset": 36, "endOffset": 77}, {"referenceID": 14, "context": "The closest approaches to the transliteration methods described in this paper are probably found in (Rao et al., 2015), using a bidirectional LSTM models together with input delays for grapheme to phoneme conversion (which can be viewed as a kind of \u201ctransliteration\u201d from English to IPA) and (Yao and Zweig, 2015), using attentionless sequence-sequence models for the same task.", "startOffset": 100, "endOffset": 118}, {"referenceID": 10, "context": "The first model is based on epsilon insertions and CTC (Graves et al., 2006) alignment, the second model is an attentional sequence-to-sequence model commonly used in end-to-end machine translation (Bahdanau et al.", "startOffset": 55, "endOffset": 76}, {"referenceID": 4, "context": ", 2006) alignment, the second model is an attentional sequence-to-sequence model commonly used in end-to-end machine translation (Bahdanau et al., 2014).", "startOffset": 129, "endOffset": 152}, {"referenceID": 3, "context": "Epsilon insertion (EI) (Azawi et al., 2013) is a simple technique for allowing sequence-to-sequence models to produce strings of different lengths from an input string.", "startOffset": 23, "endOffset": 43}, {"referenceID": 4, "context": "Attentional sequence-to-sequence models (Bahdanau et al., 2014) (Seq2Seq) work by using an encoder RNN to learn representations of the input sequence and a decoder RNN to produce the output sequence from the hidden representations the encoder created.", "startOffset": 40, "endOffset": 63}, {"referenceID": 15, "context": "As recommended in (Sutskever et al., 2014), we feed the input sequence to the encoder in reverse order.", "startOffset": 18, "endOffset": 42}, {"referenceID": 0, "context": "transliteration bedding layer provided by TensorFlow (Abadi et al., 2016) .", "startOffset": 53, "endOffset": 73}, {"referenceID": 13, "context": "We used gradient clipping to avoid exploding RNN gradients (Pascanu et al., 2012).", "startOffset": 59, "endOffset": 81}, {"referenceID": 11, "context": "A way is by exploring other recurrent network architectures, such as adaptive computation time networks (Graves, 2016) or classifier combination via boosting or other methods (Rao et al.", "startOffset": 104, "endOffset": 118}, {"referenceID": 14, "context": "A way is by exploring other recurrent network architectures, such as adaptive computation time networks (Graves, 2016) or classifier combination via boosting or other methods (Rao et al., 2015).", "startOffset": 175, "endOffset": 193}, {"referenceID": 5, "context": "Another way is by combining the neural network cost with a target language model cost (Chan et al., 2015).", "startOffset": 86, "endOffset": 105}, {"referenceID": 14, "context": "3 (Rao et al., 2015) EN-IPA 26.", "startOffset": 2, "endOffset": 20}, {"referenceID": 1, "context": "Experimental evidence from humans also supports the notion of separate and distinct processing of proper nouns and other nouns (Adorni et al., 2014).", "startOffset": 127, "endOffset": 148}], "year": 2016, "abstractText": "Transliteration is a key component of machine translation systems and software internationalization. This paper demonstrates that neural sequence-to-sequence models obtain state of the art or close to state of the art results on existing datasets. In an effort to make machine transliteration accessible, we open source a new Arabic to English transliteration dataset and our trained models.", "creator": "LaTeX with hyperref package"}}}