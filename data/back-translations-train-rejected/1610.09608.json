{"id": "1610.09608", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Oct-2016", "title": "A Theoretical Study of The Relationship Between Whole An ELM Network and Its Subnetworks", "abstract": "A biological neural network is constituted by numerous subnetworks and modules with different functionalities. For an artificial neural network, the relationship between a network and its subnetworks is also important and useful for both theoretical and algorithmic research, i.e. it can be exploited to develop incremental network training algorithm or parallel network training algorithm. In this paper we explore the relationship between an ELM neural network and its subnetworks. To the best of our knowledge, we are the first to prove a theorem that shows an ELM neural network can be scattered into subnetworks and its optimal solution can be constructed recursively by the optimal solutions of these subnetworks. Based on the theorem we also present two algorithms to train a large ELM neural network efficiently: one is a parallel network training algorithm and the other is an incremental network training algorithm. The experimental results demonstrate the usefulness of the theorem and the validity of the developed algorithms.", "histories": [["v1", "Sun, 30 Oct 2016 06:34:19 GMT  (235kb,D)", "http://arxiv.org/abs/1610.09608v1", "3 figures"]], "COMMENTS": "3 figures", "reviews": [], "SUBJECTS": "cs.LG cs.NE", "authors": ["enmei tu", "guanghao zhang", "lily rachmawati", "eshan rajabally", "guang-bin huang"], "accepted": false, "id": "1610.09608"}, "pdf": {"name": "1610.09608.pdf", "metadata": {"source": "CRF", "title": "A Theoretical Study of The Relationship Between Whole An ELM Network and Its Subnetworks", "authors": ["Enmei Tu", "Guanghao Zhang", "Lily Rachmawati", "Eshan Rajabally", "Guang-Bin Huang"], "emails": ["Rolls-Royce@NTU"], "sections": [{"heading": null, "text": "This year it is so far that it is only a matter of time before it is so far, until it is so far."}, {"heading": "II. A BRIEF REVIEW OF EXTREME LEARNING MACHINE", "text": "In an ELM study, the input weight and bias values of the training samples are randomly generated and then fixed by the entire training and prediction process without any coordination. Suppose the training set is {x1, x2,..., and each sample is a real number. For the real functional regression yk is a real number. For the multicultural classification c is the number of classes and yk is a class indicator vector whose entries are all 0, except that the entry is 1 if the sample xk belongs to class i. Let us denote X = (x1, x2, xn).Rd \u00b7 n and Y = (1, y2,...)."}, {"heading": "III. RELATIONSHIP BETWEEN ELM NETWORK AND ITS SUBNETWORKS", "text": "It has been shown that biological neural networks contain numerous subnets that have different functionalities and work in coordination to optimize the entire neural system. Therefore, the relationships between the subnets and the entire neural system are of great importance in neural science and have become the most popular research topic in neural science. For artificial neural networks, similar relationships between a network and its subnetworks are also important and useful for both theoretical and algorithmic research because they can be used to study the properties of the network and develop various training algorithms. Figure 1 shows a network and its subnets. However, as far as we know, this relationship has not been well studied. In this section, we demonstrate that an ELM network has a close relationship to its subnetworks. In an ELM network, we assume that the input layer weights and bias are randomly generated. Once they are established, the network structure is determined as the only variable to be learned during the training process."}, {"heading": "IV. APPLICATIONS OF THE THEOREM", "text": "In this section we will show the usefulness of the theory. We will develop two methods for the formation of large ELM networks, using the relationship derived from the previous section: the first method is a hierarchical algorithm and the second method is a block-by-block incremental algorithm."}, {"heading": "A. Hierarchical Network Training Algorithm", "text": "This year, the time has come for it to be able to find a solution that is capable of finding a solution that adapts to the needs of the people."}, {"heading": "V. EXPERIMENTAL RESULTS", "text": "In fact, most of them are able to put themselves in the world, at a time when they are able, when they are able to understand the world, and when they are able to understand the world."}, {"heading": "VI. CONCLUSIONS", "text": "In this thesis we will theoretically investigate the relationship between an ELM network and its sub-networks. We will prove a theorem that shows that the optimal solution of an ELM network consists in a linear transformation of the optimal solutions of its sub-networks. This theorem has the potential to be used to develop various efficient ELM training algorithms. Thus, for example, we developed two algorithms for the formation of a large ELM network: a hierarchical training algorithm and an incremental training algorithm. The validity of both algorithms will be proven by experiments.For future work, we will focus on the development of more efficient algorithms for the formation of large ELM networks based on this theorem and theoretically investigate the criteria for the recursive formation of sub-networks for the construction of a large ELM network."}, {"heading": "ACKNOWLEDGMENT", "text": "This work was carried out within the Rolls-Royce @ NTU Corporate Lab with support from the National Research Foundation (NRF) Singapore as part of the Corp Lab @ University Scheme."}], "references": [{"title": "Brief introduction of back propagation (bp) neural network algorithm and its improvement", "author": ["J. Li", "J.-h. Cheng", "J.-y. Shi", "F. Huang"], "venue": "Advances in Computer Science and Information Engineering. Springer, 2012, pp. 553\u2013558.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2012}, {"title": "Manifold regularization: A geometric framework for learning from  labeled and unlabeled examples", "author": ["M. Belkin", "P. Niyogi", "V. Sindhwani"], "venue": "Journal of machine learning research, vol. 7, no. Nov, pp. 2399\u20132434, 2006.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2006}, {"title": "A novel graphbased k-means for nonlinear manifold clustering and representative selection", "author": ["E. Tu", "L. Cao", "J. Yang", "N. Kasabov"], "venue": "Neurocomputing, vol. 143, pp. 109\u2013122, 2014.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2014}, {"title": "A graph-based semi-supervised k nearest-neighbor method for nonlinear manifold distributed data classification", "author": ["E. Tu", "Y. Zhang", "L. Zhu", "J. Yang", "N. Kasabov"], "venue": "Information Sciences, vol. 367368, pp. 673 \u2013 688, 2016.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2016}, {"title": "Semi-supervised learning", "author": ["M.F.A. Hady", "F. Schwenker"], "venue": "Handbook on Neural Information Processing. Springer, 2013, pp. 215\u2013239.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2013}, {"title": "Deformed graph laplacian for semisupervised learning", "author": ["C. Gong", "T. Liu", "D. Tao", "K. Fu", "E. Tu", "J. Yang"], "venue": "IEEE transactions on neural networks and learning systems, vol. 26, no. 10, pp. 2261\u20132274, 2015.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2015}, {"title": "An experimental comparison of semi-supervised learning algorithms for multispectral image classification", "author": ["E. Tu", "J. Yang", "J. Fang", "Z. Jia", "N. Kasabov"], "venue": "Photogrammetric Engineering & Remote Sensing, vol. 79, no. 4, pp. 347\u2013357, 2013.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2013}, {"title": "Posterior distribution learning (pdl): A novel supervised learning framework using unlabeled samples to improve classification performance", "author": ["E. Tu", "J. Yang", "N. Kasabov", "Y. Zhang"], "venue": "Neurocomputing, vol. 157, pp. 173\u2013 186, 2015.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "Universal approximation using incremental constructive feedforward networks with random hidden nodes", "author": ["G.-B. Huang", "L. Chen", "C.-K. Siew"], "venue": "Neural Networks, IEEE Transactions on, vol. 17, no. 4, pp. 879\u2013892, 2006.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2006}, {"title": "Extreme learning machine: theory and applications", "author": ["G.-B. Huang", "Q.-Y. Zhu", "C.-K. Siew"], "venue": "Neurocomputing, vol. 70, no. 1, pp. 489\u2013501, 2006.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2006}, {"title": "Extreme learning machine for regression and multiclass classification", "author": ["G.-B. Huang", "H. Zhou", "X. Ding", "R. Zhang"], "venue": "Systems, Man, and Cybernetics, Part B: Cybernetics, IEEE Transactions on, vol. 42, no. 2, pp. 513\u2013529, 2012.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2012}, {"title": "What are extreme learning machines? filling the gap between frank rosenblatts dream and john von neumanns puzzle", "author": ["G.-B. Huang"], "venue": "Cognitive Computation, vol. 7, no. 3, pp. 263\u2013278, 2015.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2015}, {"title": "Incremental learning in human action recognition based on snippets", "author": ["R. Minhas", "A.A. Mohammed", "Q. Wu"], "venue": "Circuits and Systems for Video Technology, IEEE Transactions on, vol. 22, no. 11, pp. 1529\u20131541, 2012.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2012}, {"title": "Semisupervised and unsupervised extreme learning machines", "author": ["G. Huang", "S. Song", "J.N. Gupta", "C. Wu"], "venue": "Cybernetics, IEEE Transactions on, vol. 44, no. 12, pp. 2405\u20132417, 2014.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2014}, {"title": "Image super-resolution by extreme learning machine", "author": ["L. An", "B. Bhanu"], "venue": "Image processing (ICIP), 2012 19th IEEE International Conference on. IEEE, 2012, pp. 2209\u20132212.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2012}, {"title": "High-performance extreme learning machines: a complete toolbox for big data applications", "author": ["A. Akusok", "K.-M. Bjork", "Y. Miche", "A. Lendasse"], "venue": "Access, IEEE,  8 vol. 3, pp. 1011\u20131025, 2015.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}, {"title": "Gpu-accelerated and parallelized elm ensembles for large-scale regression", "author": ["M. Van Heeswijk", "Y. Miche", "E. Oja", "A. Lendasse"], "venue": "Neurocomputing, vol. 74, no. 16, pp. 2430\u20132437, 2011.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2011}, {"title": "An os-elm based distributed ensemble classification framework in p2p networks", "author": ["Y. Sun", "Y. Yuan", "G. Wang"], "venue": "Neurocomputing, vol. 74, no. 16, pp. 2438\u2013 2443, 2011.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2011}, {"title": "Exploiting intrinsic variability of filamentary resistive memory for extreme learning machine architectures", "author": ["M. Suri", "V. Parmar"], "venue": "Nanotechnology, IEEE Transactions on, vol. 14, no. 6, pp. 963\u2013968, 2015.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2015}, {"title": "Parallel extreme learning machine for regression based on mapreduce", "author": ["Q. He", "T. Shang", "F. Zhuang", "Z. Shi"], "venue": "Neurocomputing, vol. 102, pp. 52\u201358, 2013.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2013}, {"title": "Mr-elm: a mapreduce-based framework for large-scale elm training in big data era", "author": ["J. Chen", "H. Chen", "X. Wan", "G. Zheng"], "venue": "Neural Computing and Applications, vol. 27, no. 1, pp. 101\u2013110, 2016.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2016}, {"title": "Matrix mathematics: theory, facts, and formulas", "author": ["D.S. Bernstein"], "venue": null, "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2009}, {"title": "Inversefree extreme learning machine with optimal information updating.", "author": ["S. Li", "Z. You", "H. Guo", "X. Luo", "Z. Zhao"], "venue": "IEEE transactions on cybernetics,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "These data contain valuable information which usually appears in forms of complex patterns residing in the data and highly challenges most of current machine learning methods (such as back propagation network [1]) on effectiveness and/or efficiency.", "startOffset": 209, "endOffset": 212}, {"referenceID": 1, "context": "Recently manifold learning [2\u20134] and semisupervised learning [5\u20138] have drawn much attention due to their capability of learning some low dimensional distribution properties from high dimensional input space, but researchers are still faced with imperative requirements of overcoming inefficiency or even incapability, due to insatiable memory and CPU demands, of training models with huge amounts of data.", "startOffset": 27, "endOffset": 32}, {"referenceID": 2, "context": "Recently manifold learning [2\u20134] and semisupervised learning [5\u20138] have drawn much attention due to their capability of learning some low dimensional distribution properties from high dimensional input space, but researchers are still faced with imperative requirements of overcoming inefficiency or even incapability, due to insatiable memory and CPU demands, of training models with huge amounts of data.", "startOffset": 27, "endOffset": 32}, {"referenceID": 3, "context": "Recently manifold learning [2\u20134] and semisupervised learning [5\u20138] have drawn much attention due to their capability of learning some low dimensional distribution properties from high dimensional input space, but researchers are still faced with imperative requirements of overcoming inefficiency or even incapability, due to insatiable memory and CPU demands, of training models with huge amounts of data.", "startOffset": 27, "endOffset": 32}, {"referenceID": 4, "context": "Recently manifold learning [2\u20134] and semisupervised learning [5\u20138] have drawn much attention due to their capability of learning some low dimensional distribution properties from high dimensional input space, but researchers are still faced with imperative requirements of overcoming inefficiency or even incapability, due to insatiable memory and CPU demands, of training models with huge amounts of data.", "startOffset": 61, "endOffset": 66}, {"referenceID": 5, "context": "Recently manifold learning [2\u20134] and semisupervised learning [5\u20138] have drawn much attention due to their capability of learning some low dimensional distribution properties from high dimensional input space, but researchers are still faced with imperative requirements of overcoming inefficiency or even incapability, due to insatiable memory and CPU demands, of training models with huge amounts of data.", "startOffset": 61, "endOffset": 66}, {"referenceID": 6, "context": "Recently manifold learning [2\u20134] and semisupervised learning [5\u20138] have drawn much attention due to their capability of learning some low dimensional distribution properties from high dimensional input space, but researchers are still faced with imperative requirements of overcoming inefficiency or even incapability, due to insatiable memory and CPU demands, of training models with huge amounts of data.", "startOffset": 61, "endOffset": 66}, {"referenceID": 7, "context": "Recently manifold learning [2\u20134] and semisupervised learning [5\u20138] have drawn much attention due to their capability of learning some low dimensional distribution properties from high dimensional input space, but researchers are still faced with imperative requirements of overcoming inefficiency or even incapability, due to insatiable memory and CPU demands, of training models with huge amounts of data.", "startOffset": 61, "endOffset": 66}, {"referenceID": 8, "context": "The Extreme Learning Machine (ELM) [9\u201311] was proposed as a single-hidden layer neural network for regression and classification problems due to its capability of universal approximation of almost any nonlinear or piecewise continuous function.", "startOffset": 35, "endOffset": 41}, {"referenceID": 9, "context": "The Extreme Learning Machine (ELM) [9\u201311] was proposed as a single-hidden layer neural network for regression and classification problems due to its capability of universal approximation of almost any nonlinear or piecewise continuous function.", "startOffset": 35, "endOffset": 41}, {"referenceID": 10, "context": "The Extreme Learning Machine (ELM) [9\u201311] was proposed as a single-hidden layer neural network for regression and classification problems due to its capability of universal approximation of almost any nonlinear or piecewise continuous function.", "startOffset": 35, "endOffset": 41}, {"referenceID": 11, "context": "Therefore, an ELM can achieve extremely fast training speed and meanwhile is able to attain a better generalization ability than other conventional methods [12].", "startOffset": 156, "endOffset": 160}, {"referenceID": 12, "context": "Moreover, extensive researches have shown the wide range of successful applications beyond just mathematical approximation, including human action recognition [13], semi-supervised and unsupervised clustering [14], image super resolution [15] and so on.", "startOffset": 159, "endOffset": 163}, {"referenceID": 13, "context": "Moreover, extensive researches have shown the wide range of successful applications beyond just mathematical approximation, including human action recognition [13], semi-supervised and unsupervised clustering [14], image super resolution [15] and so on.", "startOffset": 209, "endOffset": 213}, {"referenceID": 14, "context": "Moreover, extensive researches have shown the wide range of successful applications beyond just mathematical approximation, including human action recognition [13], semi-supervised and unsupervised clustering [14], image super resolution [15] and so on.", "startOffset": 238, "endOffset": 242}, {"referenceID": 15, "context": "Training efficiency for a large volume of training data may be another weakness even with a specific high performance toolbox such as [16].", "startOffset": 134, "endOffset": 138}, {"referenceID": 16, "context": "For a large scale ELM network learning problem, many researchers are devoted to developing ELM training methods to learn complex patterns from a large amount of data: Heeswijk et al [17] proposed a GPU-accelerated and parallelized method for big data learning.", "startOffset": 182, "endOffset": 186}, {"referenceID": 17, "context": "An OS-ELM based ensemble classification method in super-peer P2P network [18] is proposed for online-sequential ELM training by similar intuition of parallelization training.", "startOffset": 73, "endOffset": 77}, {"referenceID": 18, "context": "A high performance toolbox of ELM [19] focused on boosting training by CPU, GPU and HDF5 file format to achieve large scale training, fast file storage and easy installation.", "startOffset": 34, "endOffset": 38}, {"referenceID": 19, "context": "He et al [20] proposed a parallel ELM algorithm based on MapReduce, in which the matrix calculation for Moore-Penrose was decomposed and accelerated.", "startOffset": 9, "endOffset": 13}, {"referenceID": 20, "context": "A general framework based on MapReduce [21] is proposed by dividing the hidden layer into several groups, running a basic ELM training method for each group and then combining the output of all groups with same weight as the final output.", "startOffset": 39, "endOffset": 43}, {"referenceID": 8, "context": "ELM theory has proven that if g is a nonlinear continuous function [9] and a and b are randomly generated according to any continues probability distribution, then universal approximation property would be satisfied, which means that as the hidden layer neuron number m increases, the network can theoretically approximate any complex function with sufficient accuracy.", "startOffset": 67, "endOffset": 70}, {"referenceID": 0, "context": "The input weight a and bias b are usually generated from uniform distribution [-1, 1].", "startOffset": 78, "endOffset": 85}, {"referenceID": 10, "context": "Since the input weight and bias are randomly generated and fixed as constants, the output weight W is the only parameter that needs to be tuned in network training process and can be obtained by ridge regression with global optimality [11]", "startOffset": 235, "endOffset": 239}, {"referenceID": 13, "context": "To handle this problem, Huang et al [14] restrict W to a linear combination of rows of H , i.", "startOffset": 36, "endOffset": 40}, {"referenceID": 21, "context": "Noting that ( I2m \u03b1 +H H ) is a positive definite matrix, these two conditions are naturally met according to the following lemma [23]:", "startOffset": 130, "endOffset": 134}, {"referenceID": 21, "context": "Alternatively, from lemma 1 we know that the matrices A, C, SA and SC are all invertible, so the inverse of the partitioned matrix can also be written as [23] [ A B B C ]\u22121", "startOffset": 154, "endOffset": 158}, {"referenceID": 20, "context": "Here we actually also prove that a direct combination of subnetworks\u2019 output weight/output is not optimal, as in [21], since matrix Z is not equal to identity matrix.", "startOffset": 113, "endOffset": 117}, {"referenceID": 22, "context": "In this case the updating process can be implemented without explicitly computing any matrix inverse [24].", "startOffset": 101, "endOffset": 105}], "year": 2016, "abstractText": "A biological neural network is constituted by numerous subnetworks and modules with different functionalities. For an artificial neural network, the relationship between a network and its subnetworks is also important and useful for both theoretical and algorithmic research, i.e. it can be exploited to develop incremental network training algorithm or parallel network training algorithm. In this paper we explore the relationship between an ELM neural network and its subnetworks. To the best of our knowledge, we are the first to prove a theorem that shows an ELM neural network can be scattered into subnetworks and its optimal solution can be constructed recursively by the optimal solutions of these subnetworks. Based on the theorem we also present two algorithms to train a large ELM neural network efficiently: one is a parallel network training algorithm and the other is an incremental network training algorithm. The experimental results demonstrate the usefulness of the theorem and the validity of the developed algorithms.", "creator": "TeX"}}}