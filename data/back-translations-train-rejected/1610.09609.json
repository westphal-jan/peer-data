{"id": "1610.09609", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Oct-2016", "title": "Generalized Haar Filter based Deep Networks for Real-Time Object Detection in Traffic Scene", "abstract": "Vision-based object detection is one of the fundamental functions in numerous traffic scene applications such as self-driving vehicle systems and advance driver assistance systems (ADAS). However, it is also a challenging task due to the diversity of traffic scene and the storage, power and computing source limitations of the platforms for traffic scene applications. This paper presents a generalized Haar filter based deep network which is suitable for the object detection tasks in traffic scene. In this approach, we first decompose a object detection task into several easier local regression tasks. Then, we handle the local regression tasks by using several tiny deep networks which simultaneously output the bounding boxes, categories and confidence scores of detected objects. To reduce the consumption of storage and computing resources, the weights of the deep networks are constrained to the form of generalized Haar filter in training phase. Additionally, we introduce the strategy of sparse windows generation to improve the efficiency of the algorithm. Finally, we perform several experiments to validate the performance of our proposed approach. Experimental results demonstrate that the proposed approach is both efficient and effective in traffic scene compared with the state-of-the-art.", "histories": [["v1", "Sun, 30 Oct 2016 07:02:57 GMT  (2856kb,D)", "http://arxiv.org/abs/1610.09609v1", null]], "reviews": [], "SUBJECTS": "cs.CV cs.AI cs.NE", "authors": ["keyu lu", "jian li", "xiangjing an", "hangen he"], "accepted": false, "id": "1610.09609"}, "pdf": {"name": "1610.09609.pdf", "metadata": {"source": "CRF", "title": "Generalized Haar Filter based Deep Networks for Real-Time Object Detection in Traffic Scene", "authors": ["Keyu Lu", "Jian Li", "Xiangjing An", "Hangen He"], "emails": ["keyu.lu@nudt.edu.cn;", "keyu.lu@alumni.ubc.ca"], "sections": [{"heading": null, "text": "Tags: Generalized Hair Filter; Deep Networks; Object Detection; Traffic Scene"}, {"heading": "1 Introduction", "text": "Recent advances in self-driving vehicles and predictive driver assistance systems (ADAS) have attracted a lot of attention and interest from researchers and automotive manufacturers. [Image sensor plays an important role in these areas due to its faster response, lower price and lower power consumption compared to other popular sensors such as LiDAR and millimeter wave radar. However, the image sensor also has the ability to capture rich information from the traffic scene (such as brightness, color and texture). [1] What is beneficial to CNN's object recognition is one of the basic functions for self-driving vehicle systems and predictive driver assistance systems (ADAS), which need to detect and verify objects around them if they are hazardous to the host vehicle. However, object detection in the traffic scene is still challenging. On the one hand, traffic scenes are diverse and the presence of objects (e.g. vehicles and pedestrians) Xi9v: 60ss1 are unpredictable."}, {"heading": "2 Generalized Haar Filter based Deep Networks", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Architecture of the Deep Networks", "text": "To solve the problem of object detection, we construct a deep network that simultaneously outputs the Bounding Box, Category and Confidence Score of the detected object via two output channels: localization channel and classification channel (see Fig. 1). The positioning channel focuses on the Bounding Box regression and outputs a 4-dimensional vector (dx1, dx2, dy1, dy2) used to describe a Bounding Box (see Fig. 2). The classification channel outputs the Category and Confidence Score, which is indicated by a 2-dimensional vector (l, s). As shown in Fig. 1, our deep network consists of 11 convolution layers, 4 maxpooling layers and 1 softmax layer. To reduce memory consumption, several low-level features with localization and classification channels that focus on each of these, 1 pool focus, 4 and 2 pooling layers have different problems."}, {"heading": "2.2 Generalized Haar Filter based Weights Design", "text": "This year, it is so far that it is only a matter of time before it is ready, until it is ready."}, {"heading": "2.3 Multi-Task Training", "text": "It is possible that we consider the loss of Lcla as a traditional softmax loss form.It is possible to consider the loss of Lcla as a traditional softmax loss form.It is possible to consider the loss of Lcla as a traditional softmax loss form.It is possible that we consider the loss of Lcla as a traditional softmax loss form.It is possible to consider the loss of Lcla as a traditional softmax loss form.It is possible that we obtain a filter index of hair and a multiplication factor of ki."}, {"heading": "3 Sparse Windows Generation", "text": "Because of the regression-based localization channel, our deep network is less sensitive to the scaling and shifting of the input object. Therefore, instead of the traditional paradigm of dense sliding windows, we use a sparse sliding window strategy to achieve real-time object detection in the traffic scene. Furthermore, in most traffic scene object detection systems, the camera is mounted at a fixed position (e.g., on the top of a vehicle window). Accordingly, objects (e.g. vehicles and pedestrians) generate the predefined site-specific patterns in images, thus determining the potential appearance of objects in the images based on the perspective geometry of the given scene. Based on perspective geometry, a number of sparse windows can be generated according to the scale and shifting tolerance of our deep network. Finally, the locations and categories of objects in the given image can be efficiently determined by running our deep network on these sparser windows."}, {"heading": "3.1 Sparse Sliding-Window Strategy", "text": "To achieve that we are able to adjust the objects in different sizes, we define a size ratio for each input window. As shown in Fig. 5, letWs denotes the size of each input window, and Ls denotes the size of each object that borders the square. The size ratio between the object that borders the square and the corresponding input window is represented by Rs: Rs = LsWs. (14) We then create an image pyramid by adjusting the given image at different scales. For each enlarged image in the image pyramid, we assume that each deep network is responsible for objects with Rs in the range of 0.5 \u0445 0.7 (objects with Rs outside this range would be detected in another image pyramid). By setting the sliding window step to 0.3, we can ensure that each object is fully contained in at least one window. Thus, our approach is able to base the advantages of the regression method based on the ression of the ression of the local and resorption of the 14 based on the analogous method."}, {"heading": "3.2 Perspective Geometry", "text": "This year it has come to the point where it will be able to do the aforementioned for the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green for the green the green the green the green the green the green"}, {"heading": "4 Experimental Results", "text": "The experiments in this section focus mainly on vehicle recognition in the traffic scene, which is an important topic in a wide range of applications such as autonomous driving, autonomous navigation and driver assistance systems (ADAS). [21] In this paper, the performance of our approach is evaluated using a widely used dataset: TME motorway dataset [20], which is designed to detect and locate vehicles in challenging traffic situations with different lighting conditions and complex traffic situations [20]. In the following experiments, the output-bounding boxes of our approach are refined by medium displacement rather than maximum suppression, and we use an IoU threshold of 0.7 to determine the accuracy of the detection. All experiments in this section are conducted on a GTX1080 GPU.13."}, {"heading": "4.1 Generalized Haar filter based weights vs traditional weights", "text": "Because our deep network has two output channels: classification channel and localization channel. We use the classification and localization error of training and test as performance indicators for the following experiments. The classification error of training and test is defined as: Ercla = FP + FNN, (23) where N is the total number of training or test samples, FP and FN respectively the number of positive and negative samples that are incorrectly classified. For the localization channel, the error of training and test is defined as follows: Erloc = N \u00b2 i = 1% of the total number of training or test samples, FP and FN = 2 4N, (24) where the vector d = (dx1, dx2, dy1, dy2) represents the output of the localization channel, and d = (dx \u00b2 1, dx \u00b2 2, dy \u00b2 1, dy \u00b2, dy \u00b2 2) is the generalization channel."}, {"heading": "4.2 Storage and computing resources consumptions", "text": "In the experiment, the number of selected filters is # 32, and all weights are stored using singleprecision floating point format (32 bits). In the deep network (see Figure 1), the size of the Convex5 x 3, and the rest of the convolution cores have the size of 1 \u00d7 1. Since the weights of the size 1 \u00d7 1 consume much less resources than those of the size 3 \u00d7 3, we only evaluate the dimensions of the Convex5 x and their impact on resource consumption in Table 1 (column of memes), the networks with G-hair weights are able to drastically reduce the memory resources (about 0.8n2 times, n = 3 in this work). This is due to the fact that only one filter index and one multiplication factor are required to store the memory resources."}, {"heading": "4.3 Comparing with state-of-the-art methods", "text": "In the following experiments, \"Ours\" represents our complete method based on G-hair weights and the stage of sparse window generation. \"Our approaches are the same as those in row 7 of Table 1.As, shown in Fig. 9, thanks to the strong representation of deep neural networks, regression based on deep networks (such as YOLO [15] and our approach) tend to function better than traditional craft methods."}, {"heading": "5 Conclusions and Future Works", "text": "In this paper, we present a novel network system for object detection tasks in the traffic scene. In this system, we introduce a local regression strategy for accurate object detection. Compared to traditional global regression-based object detection tasks, the local regression task is easier to handle without the support of complex or large-scale networks. Accordingly, the local regression tasks are accomplished by using several tiny deep networks that simultaneously output the bounding boxes, categories and confidence values of the captured objects. In order to meet the storage, power and computing sources requirements of the platforms for traffic applications, the weights of the deep networks are limited to the form of a generalized hair filter during the training phase. In addition, we implement the strategy of economical window generation to reduce the runtime of our system. In the experiments, we first evaluate the performance and generalization capability of our generalized hair filter-based weights by comparing them with traditional weights."}, {"heading": "Acknowledgements", "text": "This work is supported by the National Natural Science Foundation of China (grant number 61473303)."}], "references": [{"title": "Vision sensor-based road detection for field robot navigation, Sensors", "author": ["K. Lu", "J. Li", "X. An", "H. He"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2015}, {"title": "Robust real-time face detection, International Journal of Computer Vision (IJCV", "author": ["P. Viola", "M.J. Jones"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2004}, {"title": "Object detection with discriminatively trained part-based models, IEEE Transactions on Pattern Analysis and Machine Intelligence", "author": ["P.F. Felzenszwalb", "R.B. Girshick", "D. McAllester", "D. Ramanan"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2010}, {"title": "Imagenet classification with deep convolutional neural networks, Advances in Neural Information", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "Processing Systems", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2012}, {"title": "End-to-end text recognition with convolutional neural networks", "author": ["T. Wang", "D.J. Wu", "A. Coates", "A.Y. Ng"], "venue": "in: International Conference on Pattern Recognition (ICPR),", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2012}, {"title": "Rich feature hierarchies for accurate object detection and semantic segmentation", "author": ["R. Girshick", "J. Donahue", "T. Darrell", "J. Malik"], "venue": "in: IEEE International Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2014}, {"title": "Spatial pyramid pooling in deep convolutional networks for visual recognition", "author": ["K. He", "X. Zhang", "S. Ren"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2014}, {"title": "Selective search for object recognition", "author": ["J. Uijlings", "K. van de Sande", "T. Gevers", "A. Smeulders"], "venue": "in: International Journal of Computer Vision (IJCV),", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2013}, {"title": "Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding, Fiber", "author": ["S. Han", "H. Mao", "W.J. Dally"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2016}, {"title": "Real-time compressive tracking", "author": ["K. Zhang", "L. Zhang", "M.-H. Yang"], "venue": "in: European Conference on Computer Vision (ECCV),", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2012}, {"title": "Visual tracking with online multiple instance learning", "author": ["P. Doll\u00e1r", "Z. Tu", "H. Tao", "S. Belongie"], "venue": "in: IEEE International Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2009}, {"title": "An efficient method for solving nonlinear unconstrained min-max problem, Master\u2019s thesis, Xi\u2019An University of science and technology, Xi\u2019An", "author": ["Z. Yu"], "venue": null, "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2011}, {"title": "A system for real-time detection and tracking of vehicles from a single car-mounted camera", "author": ["C. Caraffi", "T. Voj\u0131\u0301\u0159", "J. Trefn\u00fd", "J. \u0160ochman", "J. Matas"], "venue": "in: IEEE Intelligent Transportation Systems Conference,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2012}, {"title": "Monocular precrash vehicle detection: features and classifiers", "author": ["Z. Sun", "G. Bebis", "R. Miller"], "venue": "IEEE Transactions on Image Processing (TIP)", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2006}, {"title": "Computing\u2019s energy problem (and what we can do about it)", "author": ["M. Horowitz"], "venue": "in: IEEE International Solid State Circuits Conference,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2014}, {"title": "Signifredi, A coarse-to-fine vehicle detector running in real-time", "author": ["L. Castangia", "P. Grisleri", "P. Medici", "A.A. Prioletti"], "venue": "in: IEEE Intelligent Transportation Systems Conference,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2014}], "referenceMentions": [{"referenceID": 0, "context": "Moreover, vision sensor has the ability to capture rich information from traffic scene (such as luminance, color and texture) [1], which is beneficial for object detection.", "startOffset": 126, "endOffset": 129}, {"referenceID": 3, "context": "Convolutional Neural Network (CNN) [6] is one of the most popular forms of deep networks.", "startOffset": 35, "endOffset": 38}, {"referenceID": 4, "context": "By using the strategies of local receptive fields, weight sharing and spatial pooling [7], CNN has made a breakthrough in computer vision and image processing.", "startOffset": 86, "endOffset": 89}, {"referenceID": 3, "context": "For instance, AlexNet [6], which is a type of CNN, has made a startling achievement in the competition of ILSVRC-2012 [2] and demonstrated its superiority in image classification.", "startOffset": 22, "endOffset": 25}, {"referenceID": 1, "context": "Firstly, CNN has a multitude of convolutions that have to be calculated, it would be rather inefficient in object detection if using traditional dense sliding windows paradigm [4,5].", "startOffset": 176, "endOffset": 181}, {"referenceID": 2, "context": "Firstly, CNN has a multitude of convolutions that have to be calculated, it would be rather inefficient in object detection if using traditional dense sliding windows paradigm [4,5].", "startOffset": 176, "endOffset": 181}, {"referenceID": 5, "context": "introduced the framework of region proposal based CNN (called R-CNN) [10] and successfully applied it to object detection.", "startOffset": 69, "endOffset": 73}, {"referenceID": 5, "context": "The main idea of region proposal based CNN is performing CNNs on candidate bounding-boxes (called \u201cproposals\u201d) which have potential to contain objects [10].", "startOffset": 151, "endOffset": 155}, {"referenceID": 6, "context": "This method combines R-CNN with SPPnet [11].", "startOffset": 39, "endOffset": 43}, {"referenceID": 7, "context": "However, in both R-CNN and Fast R-CNN, the proposals are generated by Selective Search [12], which is quite inefficient and thus limits the detection speed of these methods.", "startOffset": 87, "endOffset": 91}, {"referenceID": 7, "context": "To overcome this limitation, Faster R-CNN [13] proposed to used Region Proposal Networks (RPNs) to generate proposals instead of Selective Search [12].", "startOffset": 146, "endOffset": 150}, {"referenceID": 8, "context": "Despite the fact that deep neural networks have achieved state-of-the-art performance in object detection, they consume considerable storage, computing resources and power [16].", "startOffset": 172, "endOffset": 176}, {"referenceID": 1, "context": "Haar-like filters have been successfully applied in object detection owing to their strong representation and high efficiency [4,17].", "startOffset": 126, "endOffset": 132}, {"referenceID": 9, "context": "Haar-like filters have been successfully applied in object detection owing to their strong representation and high efficiency [4,17].", "startOffset": 126, "endOffset": 132}, {"referenceID": 10, "context": "Instead of using a fixed number of rectangles and configuration types, generalized Haar filters are based on arbitrary configurations and number of rectangles [18].", "startOffset": 159, "endOffset": 163}, {"referenceID": 10, "context": "Unlike original generalized Haar filters with arbitrary configuration and number of rectangles [18], in our work, the configuration and number of rectangles are obtained in a data-driven way.", "startOffset": 95, "endOffset": 99}, {"referenceID": 9, "context": "Unlike approaches in [17,4,18] where Haar filters are operated based on integral images.", "startOffset": 21, "endOffset": 30}, {"referenceID": 1, "context": "Unlike approaches in [17,4,18] where Haar filters are operated based on integral images.", "startOffset": 21, "endOffset": 30}, {"referenceID": 10, "context": "Unlike approaches in [17,4,18] where Haar filters are operated based on integral images.", "startOffset": 21, "endOffset": 30}, {"referenceID": 11, "context": "According to [19], a maximum function for Z = {z1, .", "startOffset": 13, "endOffset": 17}, {"referenceID": 11, "context": "The continuity and differentiability have been proofed by [19].", "startOffset": 58, "endOffset": 62}, {"referenceID": 12, "context": "7, we illustrate the final sparse windows in each image of the image pyramid using the TME Motorway dataset [20], which is a challenging and widely-used dataset in vehicle detection.", "startOffset": 108, "endOffset": 112}, {"referenceID": 13, "context": "The experiments in this section mainly focus on vehicle detections in traffic scene, which is an important issue in a wide range of applications such as autonomous driving, autonomous navigation and advance driver assistance systems (ADAS) [21].", "startOffset": 240, "endOffset": 244}, {"referenceID": 12, "context": "In this work, the performance of our approach is evaluated on a broadly used datasets: TME motorway dataset [20], which is designed for vehicle detection and localization in challenging traffic scene with various lighting conditions and complex traffic situations [20].", "startOffset": 108, "endOffset": 112}, {"referenceID": 12, "context": "In this work, the performance of our approach is evaluated on a broadly used datasets: TME motorway dataset [20], which is designed for vehicle detection and localization in challenging traffic scene with various lighting conditions and complex traffic situations [20].", "startOffset": 264, "endOffset": 268}, {"referenceID": 12, "context": "The experiment is also performed on TME motorway dataset [20].", "startOffset": 57, "endOffset": 61}, {"referenceID": 14, "context": "In addition, power consumption is directly influenced by storage and computing resources utilizations [22].", "startOffset": 102, "endOffset": 106}, {"referenceID": 12, "context": "Caraffi [20] Castangia [23] YOLO [14] SSD500 [15] Ours (Traditional Weights) Ours", "startOffset": 8, "endOffset": 12}, {"referenceID": 15, "context": "Caraffi [20] Castangia [23] YOLO [14] SSD500 [15] Ours (Traditional Weights) Ours", "startOffset": 23, "endOffset": 27}, {"referenceID": 12, "context": "1024 \u00d7 768 in TME motorway dataset [20]) have to be resized to 500 \u00d7 500 in SSD500 [15], which leads to the lacking of resolution of small objects such as vehicles and pedestrians that are far away from camera.", "startOffset": 35, "endOffset": 39}, {"referenceID": 12, "context": "Caraffi [20] 0.", "startOffset": 8, "endOffset": 12}, {"referenceID": 15, "context": "1s Castangia [23] 0.", "startOffset": 13, "endOffset": 17}], "year": 2016, "abstractText": "Vision-based object detection is one of the fundamental functions in numerous traffic scene applications such as self-driving vehicle systems and advance driver assistance systems (ADAS). However, it is also a challenging task due to the diversity of traffic scene and the storage, power and computing source limitations of the platforms for traffic scene applications. This paper presents a generalized Haar filter based deep network which is suitable for the object detection tasks in traffic scene. In this approach, we first decompose a object detection task into several easier local regression tasks. Then, we handle the local regression tasks by using several tiny deep networks which simultaneously output the bounding boxes, categories and confidence scores of detected objects. To reduce the consumption of storage and computing resources, the weights of the deep networks are constrained to the form of generalized Haar filter in training phase. Additionally, we introduce the strategy of sparse windows generation to improve the efficiency of the algorithm. Finally, we perform several experiments to validate the performance of our proposed approach. Experimental results demonstrate that the proposed approach is both efficient and effective in traffic scene compared with the state-of-the-art.", "creator": "LaTeX with hyperref package"}}}