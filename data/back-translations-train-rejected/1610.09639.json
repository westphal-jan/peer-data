{"id": "1610.09639", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Oct-2016", "title": "Compact Deep Convolutional Neural Networks With Coarse Pruning", "abstract": "The learning capability of a neural network improves with increasing depth at higher computational costs. Wider layers with dense kernel connectivity patterns furhter increase this cost and may hinder real-time inference. We propose feature map and kernel level pruning for reducing the computational complexity of a deep convolutional neural network. Pruning feature maps reduces the width of a layer and hence does not need any sparse representation. Further, kernel pruning converts the dense connectivity pattern into a sparse one. Due to coarse nature, these pruning granularities can be exploited by GPUs and VLSI based implementations. We propose a simple and generic strategy to choose the least adversarial pruning masks for both granularities. The pruned networks are retrained which compensates the loss in accuracy. We obtain the best pruning ratios when we prune a network with both granularities. Experiments with the CIFAR-10 dataset show that more than 85% sparsity can be induced in the convolution layers with less than 1% increase in the missclassification rate of the baseline network.", "histories": [["v1", "Sun, 30 Oct 2016 11:57:20 GMT  (132kb,D)", "http://arxiv.org/abs/1610.09639v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.NE", "authors": ["sajid anwar", "wonyong sung"], "accepted": false, "id": "1610.09639"}, "pdf": {"name": "1610.09639.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["COARSE PRUNING", "Sajid Anwar", "Wonyong Sung"], "emails": ["sajid@dsp.snu.ac.kr,", "wysung@snu.ac.kr"], "sections": [{"heading": "1 INTRODUCTION", "text": "This year, it is more than ever before in the history of the city."}, {"heading": "2 RELATED WORK", "text": "In the literature, the network intersection has been studied by several researchers: Han et al. (2015b; a); Yu et al. (2012); Castellano et al. (1997); Collins & Kohli (2014); Stepniewski & Keane (1997); Reed (1993); Collins & Kohli (2014) have proposed a technique that uses irregular scarcity to reduce computational complexity in revolutionary and fully interconnected layers. However, they have not discussed how the sparse representation will affect computational use.The work of Han et al. (2015b; a) introduces fine-grained scarcity into a network by pruning scalar weights. If the absolute size of a weight is less than a scalar threshold, the weight is curtailed, which favors learning with low weights and trains the network with the L1 / L2 standard."}, {"heading": "3 PRUNING CANDIDATE SELECTION", "text": "This year, it is closer than ever before in the history of the country."}, {"heading": "4 FEATURE MAP AND KERNEL PRUNING", "text": "This year it is more than ever before in the history of the city."}, {"heading": "5 EXPERIMENTAL RESULTS", "text": "In this section, we present detailed experimental results with the CIFAR-10 and SVHN datasets Krizhevsky & Hinton (2009): During training and backcutting, we use the stochastic gradient Descent (SGD) with a mini margin size of 128 and RMS Prop Tieleman & Hinton (2012).We train all networks with the batch normalization Ioffe & Szegedy (2015).We do not trim the network in small steps, but cut it with a shot at a given intersection, followed by retraining. The experimental results are reported in the corresponding two subsections."}, {"heading": "5.1 CIFAR-10", "text": "The CIFAR-10 data set includes samples from ten classes: aircraft, automobile, bird, cat, deer, dog, frog, horse, ship and truck. The training set consists of 50,000 RGB samples and we assign 20% of these samples as a validation set. The test set contains 10,000 samples and each sample has a resolution of 32 x 32 x RGB. We evaluate the proposed crop granularities using two networks. CNNsmall and CNNlarge. CNNsmall has six folding layers and two overlapping max pooling layers. We report on the network architecture using an alphanumeric string, as in Courbariaux et al. (2015) and outlined in Table 1. The (2 x 128C3) represents two folding layers, each with 128 feature maps and 3 x 3 folding cores. MP2 represents 3 x 3 overlapped maxpooling layer with a strip size of 2."}, {"heading": "5.1.1 FEATURE MAP AND KERNEL LEVEL PRUNING", "text": "The pruning of a single function board results in zeros of all incoming and outgoing weights and thus in a significant deterioration of the network classification. The pruning of the function board for CNNcifar is shown in Fig. 4a with a line marked in red. The sparseness reported here is for Conv2 to Conv6. We do not prune the first conversion layer, as it only has 3 x 128 x (3 x 3) = 3456 weights. Horizontal solidity shows the baseline MCR of 16.26%, while the dashed line has a tolerance limit of 1%. Formation of the network with the batch normalization Ioffe & Szegedy (2015) allows us to prune a network directly rather than taking small steps."}, {"heading": "5.1.2 COMBINATIONS OF KERNEL AND FEATURE MAP PRUNING", "text": "In this section, we will discuss the various trimming granularities that are applied in different combinations. First, we will apply the feature map and core trim to the CNNsmall network in different sequences. By trimming the feature map, we can achieve a 60% scarcity within the budget of 1% increase in MCR. But at this stage of trimming, the learning ability of the network is severely impaired. So, we will adopt a 50% trimming of the network, where the CNNsmall is reduced to (128C3 \u2212 89C3) -MP3- (89C3) -MP3 (179C3 \u2212 179C3) -256FC-10Softmax. Since trimming is applied only to Conv2 \u2212 Conv6, therefore, in Fig. 4. Pruning ratios are calculated only for these layers. This network is then subject to kernel-level trimming."}, {"heading": "5.2 SVHN", "text": "The SVHN dataset consists of 32 x 32 x 3 truncated images of house numbers [Netzer et al. 2011] and bears similarities to the MNIST handwritten digit recognition dataset [LeCun et al. 1998]. Classification is difficult because there may be more than one digit in the sample and the goal is to identify one digit in the middle of a patch.The dataset consists of 73,257 digits for training, 26,032 for testing and 53,1131 in addition to training. The additional set consists of simple samples and can extend the training set. We generate a validation set of 6000 samples consisting of 4000 samples from the training set and 2000 samples from the additional [Sermanet al. 2012].The network architecture is described as follows: (2 x 64C3) -MP2- (2 x 128C3) -MP2- (2 x 128C3) -MP2- (2 x 1283) -FC -1253.5, we can report the normalization rate in a similar way to the network."}, {"heading": "6 CONCLUDING REMARKS", "text": "We discussed that the cost of a sparse presentation with rough trimming can be avoided. We demonstrated a simple and generic algorithm for selecting the best trimming mask. We conducted experiments with several benchmarks and networks and showed that the proposed technique has good scalability. We will be researching online trimming in the future to take advantage of runtime."}, {"heading": "ACKNOWLEDGMENT", "text": "This work was funded by the National Research Foundation of Korea (NRF), which is funded by the Korean Government (MSIP) (No 2015R1A2A1A10056051)."}], "references": [{"title": "Fixed point optimization of deep convolutional neural networks for object recognition", "author": ["Sajid Anwar", "Kyuyeon Hwang", "Wonyong Sung"], "venue": "In Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "Anwar et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Anwar et al\\.", "year": 2015}, {"title": "Structured pruning of deep convolutional neural networks", "author": ["Sajid Anwar", "Kyuyeon Hwang", "Wonyong Sung"], "venue": "arXiv preprint arXiv:1512.08571,", "citeRegEx": "Anwar et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Anwar et al\\.", "year": 2015}, {"title": "An iterative pruning algorithm for feedforward neural networks", "author": ["Giovanna Castellano", "Anna Maria Fanelli", "Marcello Pelillo"], "venue": "Neural Networks, IEEE Transactions on,", "citeRegEx": "Castellano et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Castellano et al\\.", "year": 1997}, {"title": "High performance convolutional neural networks for document processing", "author": ["Kumar Chellapilla", "Sidd Puri", "Patrice Simard"], "venue": "In Tenth International Workshop on Frontiers in Handwriting Recognition. Suvisoft,", "citeRegEx": "Chellapilla et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Chellapilla et al\\.", "year": 2006}, {"title": "Memory bounded deep convolutional networks", "author": ["Maxwell D Collins", "Pushmeet Kohli"], "venue": "arXiv preprint arXiv:1412.1442,", "citeRegEx": "Collins and Kohli.,? \\Q2014\\E", "shortCiteRegEx": "Collins and Kohli.", "year": 2014}, {"title": "Binaryconnect: Training deep neural networks with binary weights during propagations", "author": ["Matthieu Courbariaux", "Yoshua Bengio", "Jean-Pierre David"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Courbariaux et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Courbariaux et al\\.", "year": 2015}, {"title": "Large scale distributed deep networks", "author": ["Jeffrey Dean", "Greg Corrado", "Rajat Monga", "Kai Chen", "Matthieu Devin", "Mark Mao", "Andrew Senior", "Paul Tucker", "Ke Yang", "Quoc V Le"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Dean et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Dean et al\\.", "year": 2012}, {"title": "A deep neural network compression pipeline: Pruning, quantization, huffman encoding", "author": ["Song Han", "Huizi Mao", "William J Dally"], "venue": "arXiv preprint arXiv:1510.00149,", "citeRegEx": "Han et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Han et al\\.", "year": 2015}, {"title": "Learning both weights and connections for efficient neural network", "author": ["Song Han", "Jeff Pool", "John Tran", "William Dally"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Han et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Han et al\\.", "year": 2015}, {"title": "Fixed-point feedforward deep neural network design using weights+", "author": ["Kyuyeon Hwang", "Wonyong Sung"], "venue": "In Signal Processing Systems (SiPS),", "citeRegEx": "Hwang and Sung.,? \\Q2014\\E", "shortCiteRegEx": "Hwang and Sung.", "year": 2014}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["Sergey Ioffe", "Christian Szegedy"], "venue": "arXiv preprint arXiv:1502.03167,", "citeRegEx": "Ioffe and Szegedy.,? \\Q2015\\E", "shortCiteRegEx": "Ioffe and Szegedy.", "year": 2015}, {"title": "Learning multiple layers of features from tiny", "author": ["Alex Krizhevsky", "Geoffrey Hinton"], "venue": null, "citeRegEx": "Krizhevsky and Hinton.,? \\Q2009\\E", "shortCiteRegEx": "Krizhevsky and Hinton.", "year": 2009}, {"title": "Fast convnets using group-wise brain damage", "author": ["Vadim Lebedev", "Victor Lempitsky"], "venue": "arXiv preprint arXiv:1506.02515,", "citeRegEx": "Lebedev and Lempitsky.,? \\Q2015\\E", "shortCiteRegEx": "Lebedev and Lempitsky.", "year": 2015}, {"title": "Gradient-based learning applied to document recognition", "author": ["Yann LeCun", "L\u00e9on Bottou", "Yoshua Bengio", "Patrick Haffner"], "venue": "Proceedings of the IEEE,", "citeRegEx": "LeCun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "Pruning filters for efficient convnets", "author": ["Hao Li", "Asim Kadav", "Igor Durdanovic", "Hanan Samet", "Hans Peter Graf"], "venue": "arXiv preprint arXiv:1608.08710,", "citeRegEx": "Li et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Li et al\\.", "year": 2016}, {"title": "Fast training of convolutional networks through ffts", "author": ["Michael Mathieu", "Mikael Henaff", "Yann LeCun"], "venue": "arXiv preprint arXiv:1312.5851,", "citeRegEx": "Mathieu et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mathieu et al\\.", "year": 2013}, {"title": "Channel-level acceleration of deep face representations", "author": ["Adam Polyak", "Lior Wolf"], "venue": "Access, IEEE,", "citeRegEx": "Polyak and Wolf.,? \\Q2015\\E", "shortCiteRegEx": "Polyak and Wolf.", "year": 2015}, {"title": "Pruning algorithms-a survey", "author": ["Russell Reed"], "venue": "Neural Networks, IEEE Transactions on,", "citeRegEx": "Reed.,? \\Q1993\\E", "shortCiteRegEx": "Reed.", "year": 1993}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["Karen Simonyan", "Andrew Zisserman"], "venue": "arXiv preprint arXiv:1409.1556,", "citeRegEx": "Simonyan and Zisserman.,? \\Q2014\\E", "shortCiteRegEx": "Simonyan and Zisserman.", "year": 2014}, {"title": "Pruning backpropagation neural networks using modern stochastic optimisation techniques", "author": ["Slawomir W Stepniewski", "Andy J Keane"], "venue": "Neural Computing & Applications,", "citeRegEx": "Stepniewski and Keane.,? \\Q1997\\E", "shortCiteRegEx": "Stepniewski and Keane.", "year": 1997}, {"title": "Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude", "author": ["Tijmen Tieleman", "Geoffrey Hinton"], "venue": "COURSERA: Neural Networks for Machine Learning,", "citeRegEx": "Tieleman and Hinton.,? \\Q2012\\E", "shortCiteRegEx": "Tieleman and Hinton.", "year": 2012}, {"title": "Exploiting sparseness in deep neural networks for large vocabulary speech recognition", "author": ["Dong Yu", "Frank Seide", "Gang Li", "Li Deng"], "venue": "In 2012 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "Yu et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Yu et al\\.", "year": 2012}], "referenceMentions": [{"referenceID": 4, "context": "The network reported in Dean et al. (2012) has 1.", "startOffset": 24, "endOffset": 43}, {"referenceID": 4, "context": "The network reported in Dean et al. (2012) has 1.7 Billion parameters and is trained on tens of thousands of CPU cores. Similarly Simonyan & Zisserman (2014) has employed 16-18 layers and achieved excellent classification results on the ImageNet dataset.", "startOffset": 24, "endOffset": 158}, {"referenceID": 4, "context": "The network reported in Dean et al. (2012) has 1.7 Billion parameters and is trained on tens of thousands of CPU cores. Similarly Simonyan & Zisserman (2014) has employed 16-18 layers and achieved excellent classification results on the ImageNet dataset. The high computationally complexity of wide and deep networks is a major obstacle in porting the benefits of deep learning to resource limited devices. Therefore, many researchers have proposed ideas to accelerate deep networks for real-time inference Yu et al. (2012); Han et al.", "startOffset": 24, "endOffset": 524}, {"referenceID": 4, "context": "The network reported in Dean et al. (2012) has 1.7 Billion parameters and is trained on tens of thousands of CPU cores. Similarly Simonyan & Zisserman (2014) has employed 16-18 layers and achieved excellent classification results on the ImageNet dataset. The high computationally complexity of wide and deep networks is a major obstacle in porting the benefits of deep learning to resource limited devices. Therefore, many researchers have proposed ideas to accelerate deep networks for real-time inference Yu et al. (2012); Han et al. (2015b;a); Mathieu et al. (2013); Anwar et al.", "startOffset": 24, "endOffset": 569}, {"referenceID": 0, "context": "(2013); Anwar et al. (2015b). Network pruning is one promising techique that first learns a function with a suficiently large sized network followed by removing less important connections Yu et al.", "startOffset": 8, "endOffset": 29}, {"referenceID": 0, "context": "(2013); Anwar et al. (2015b). Network pruning is one promising techique that first learns a function with a suficiently large sized network followed by removing less important connections Yu et al. (2012); Han et al.", "startOffset": 8, "endOffset": 205}, {"referenceID": 0, "context": "(2013); Anwar et al. (2015b). Network pruning is one promising techique that first learns a function with a suficiently large sized network followed by removing less important connections Yu et al. (2012); Han et al. (2015b); Anwar et al.", "startOffset": 8, "endOffset": 225}, {"referenceID": 0, "context": "(2013); Anwar et al. (2015b). Network pruning is one promising techique that first learns a function with a suficiently large sized network followed by removing less important connections Yu et al. (2012); Han et al. (2015b); Anwar et al. (2015b). This enables smaller networks to inherit knowledge from the large sized predecessor networks and exhibit comparable level of performance.", "startOffset": 8, "endOffset": 247}, {"referenceID": 0, "context": "(2013); Anwar et al. (2015b). Network pruning is one promising techique that first learns a function with a suficiently large sized network followed by removing less important connections Yu et al. (2012); Han et al. (2015b); Anwar et al. (2015b). This enables smaller networks to inherit knowledge from the large sized predecessor networks and exhibit comparable level of performance. The works of Han et al. (2015b;a) introduce fine grained sparsity in a network by pruning scalar weights. Due to unstructured sparsity, the authors employ compressed sparse row/column (CSR/CSC) for sparse representation. Thus the fine grained irregular sparsity cannot be easily translated into computational speedups. Sparsity in a deep convolutional neural network (CNN) can be induced at various levels. Figure 1 shows four pruning granularities. At the coarsest level, a full hidden layer can be pruned. This is shown with a red colored rectangle in Fig. 1(a). Layer wise pruning affects the depth of the network and a deep network can be converted into a shallow network. Increasing the depth improves the network performance and layer-wise pruning therefore demand intelligent techniques to mitigate the performance degradation. The next pruning granularity is removing feature maps Polyak & Wolf (2015); Anwar et al.", "startOffset": 8, "endOffset": 1296}, {"referenceID": 0, "context": "(2013); Anwar et al. (2015b). Network pruning is one promising techique that first learns a function with a suficiently large sized network followed by removing less important connections Yu et al. (2012); Han et al. (2015b); Anwar et al. (2015b). This enables smaller networks to inherit knowledge from the large sized predecessor networks and exhibit comparable level of performance. The works of Han et al. (2015b;a) introduce fine grained sparsity in a network by pruning scalar weights. Due to unstructured sparsity, the authors employ compressed sparse row/column (CSR/CSC) for sparse representation. Thus the fine grained irregular sparsity cannot be easily translated into computational speedups. Sparsity in a deep convolutional neural network (CNN) can be induced at various levels. Figure 1 shows four pruning granularities. At the coarsest level, a full hidden layer can be pruned. This is shown with a red colored rectangle in Fig. 1(a). Layer wise pruning affects the depth of the network and a deep network can be converted into a shallow network. Increasing the depth improves the network performance and layer-wise pruning therefore demand intelligent techniques to mitigate the performance degradation. The next pruning granularity is removing feature maps Polyak & Wolf (2015); Anwar et al. (2015b). Feature map pruning removes a large number of kernels and is therefore very destructive in nature.", "startOffset": 8, "endOffset": 1318}, {"referenceID": 5, "context": "This sparsity can be induced in much higher rates but demands sparse representation for computational benefits Han et al. (2015b). Therefore the high pruning ratios do not directly translate into computational speedups.", "startOffset": 111, "endOffset": 130}, {"referenceID": 0, "context": "The reference work of Anwar et al. (2015b) analysed feature map pruning with intra-kernel strided sparsity.", "startOffset": 22, "endOffset": 43}, {"referenceID": 6, "context": "In the literature, network pruning has been studied by several researches Han et al. (2015b;a); Yu et al. (2012); Castellano et al.", "startOffset": 74, "endOffset": 113}, {"referenceID": 2, "context": "(2012); Castellano et al. (1997); Collins & Kohli (2014); Stepniewski & Keane (1997); Reed (1993).", "startOffset": 8, "endOffset": 33}, {"referenceID": 2, "context": "(2012); Castellano et al. (1997); Collins & Kohli (2014); Stepniewski & Keane (1997); Reed (1993).", "startOffset": 8, "endOffset": 57}, {"referenceID": 2, "context": "(2012); Castellano et al. (1997); Collins & Kohli (2014); Stepniewski & Keane (1997); Reed (1993).", "startOffset": 8, "endOffset": 85}, {"referenceID": 2, "context": "(2012); Castellano et al. (1997); Collins & Kohli (2014); Stepniewski & Keane (1997); Reed (1993). Collins & Kohli (2014) have proposed a technique where irregular sparsity is used to reduce the computational complexity in convolutional and fully connected layers.", "startOffset": 8, "endOffset": 98}, {"referenceID": 2, "context": "(2012); Castellano et al. (1997); Collins & Kohli (2014); Stepniewski & Keane (1997); Reed (1993). Collins & Kohli (2014) have proposed a technique where irregular sparsity is used to reduce the computational complexity in convolutional and fully connected layers.", "startOffset": 8, "endOffset": 122}, {"referenceID": 14, "context": "(b) This Figure explains the idea presented in Li et al. (2016) and shows three layers, L1, L2 and L3.", "startOffset": 47, "endOffset": 64}, {"referenceID": 0, "context": "Convolutions are unrolled to matrix-matrix multiplication in Chellapilla et al. (2006) for efficient implementation.", "startOffset": 61, "endOffset": 87}, {"referenceID": 0, "context": "Convolutions are unrolled to matrix-matrix multiplication in Chellapilla et al. (2006) for efficient implementation. The work of Lebedev & Lempitsky (2015) also induce intra-kernel sparsity in a convolutional layer.", "startOffset": 61, "endOffset": 156}, {"referenceID": 0, "context": "Convolutions are unrolled to matrix-matrix multiplication in Chellapilla et al. (2006) for efficient implementation. The work of Lebedev & Lempitsky (2015) also induce intra-kernel sparsity in a convolutional layer. Their target is efficient computation by unrolling convolutions as matrx-matrix multiplication. Their sparse representation is not also simple because each kernel has an equally sized pruning mask. A recently published work propose sparsity at a higher granularity and induce channel level sparsity in a CNN network for deep face application Polyak & Wolf (2015). The work of Castellano et al.", "startOffset": 61, "endOffset": 579}, {"referenceID": 0, "context": "The work of Castellano et al. (1997); Collins & Kohli (2014); Stepniewski & Keane (1997); Reed (1993) utilize unstructured fine grained sparsity in a neural network.", "startOffset": 12, "endOffset": 37}, {"referenceID": 0, "context": "The work of Castellano et al. (1997); Collins & Kohli (2014); Stepniewski & Keane (1997); Reed (1993) utilize unstructured fine grained sparsity in a neural network.", "startOffset": 12, "endOffset": 61}, {"referenceID": 0, "context": "The work of Castellano et al. (1997); Collins & Kohli (2014); Stepniewski & Keane (1997); Reed (1993) utilize unstructured fine grained sparsity in a neural network.", "startOffset": 12, "endOffset": 89}, {"referenceID": 0, "context": "The work of Castellano et al. (1997); Collins & Kohli (2014); Stepniewski & Keane (1997); Reed (1993) utilize unstructured fine grained sparsity in a neural network.", "startOffset": 12, "endOffset": 102}, {"referenceID": 0, "context": "Fixed point optimization for deep neural networks is employed by Anwar et al. (2015a); Hwang & Sung (2014); Sung et al.", "startOffset": 65, "endOffset": 86}, {"referenceID": 0, "context": "Fixed point optimization for deep neural networks is employed by Anwar et al. (2015a); Hwang & Sung (2014); Sung et al.", "startOffset": 65, "endOffset": 107}, {"referenceID": 21, "context": "Further the pruned network is retrained to compensate for the pruning losses Yu et al. (2012). For a specific pruning ratio, we search for the best pruning masks which afflicts the least adversary on the pruned network.", "startOffset": 77, "endOffset": 94}, {"referenceID": 14, "context": "(a) Best of N random masks vs Li et al. (2016) 0 0.", "startOffset": 30, "endOffset": 47}, {"referenceID": 14, "context": "We further explain and compare this method with the weight sum criterion proposed in Li et al. (2016) and shown in Fig.", "startOffset": 85, "endOffset": 102}, {"referenceID": 14, "context": "We further explain and compare this method with the weight sum criterion proposed in Li et al. (2016) and shown in Fig. 2b. The set of filters or kernels from the previous layer constitute a group. This is shown with the similar color in Fig. 2b. According to Li et al. (2016), the absolute sum of weights determine the importance of a feature map.", "startOffset": 85, "endOffset": 277}, {"referenceID": 14, "context": "We further explain and compare this method with the weight sum criterion proposed in Li et al. (2016) and shown in Fig. 2b. The set of filters or kernels from the previous layer constitute a group. This is shown with the similar color in Fig. 2b. According to Li et al. (2016), the absolute sum of weights determine the importance of a feature map. Suppose that in Fig.2b, the Layer L2 undergoes feature map pruning. The weight sum criterion computes the absolute weight sum at S1, S2 and S3. If we further suppose that the pruning ratio is 1/3, then the min(S1, S2, S3) is pruned. All the incoming and outgoing kernels from the pruned feature map are also removed. We argue that the sign of a weight in kernel plays important role in well-known feature extractors and therefore this is not a good criterion. We compare the performance of the two algoirthms and Fig. 3a shows the experimental results. These results present the network status before any retraining is conducted. We report the performance degradation in the network classifcation against the pruning ratio. From Fig. 3a, we can observe that our proposed method outperforms the weight sum method particularly for higher pruning ratios. This is attributed to evaluating pruning candidates in combinations. The criterion in Li et al. (2016) evaluates the importance of a pruning unit in isolation while our proposed approach evaluates several paths through the network and selects the best one.", "startOffset": 85, "endOffset": 1304}, {"referenceID": 3, "context": "Feature map pruning does not need any sparse representation and the pruned network can be implemented in a conventional way, convolution lowering Chellapilla et al. (2006) or convolution with FFTs Mathieu et al.", "startOffset": 146, "endOffset": 172}, {"referenceID": 3, "context": "Feature map pruning does not need any sparse representation and the pruned network can be implemented in a conventional way, convolution lowering Chellapilla et al. (2006) or convolution with FFTs Mathieu et al. (2013). The main", "startOffset": 146, "endOffset": 219}, {"referenceID": 12, "context": "In LeNet LeCun et al. (1998), the second convolution layer has 6 \u00d7 16 feature maps and the kernel connectivity has a fixed sparse pattern.", "startOffset": 9, "endOffset": 29}, {"referenceID": 12, "context": "In LeNet LeCun et al. (1998), the second convolution layer has 6 \u00d7 16 feature maps and the kernel connectivity has a fixed sparse pattern. With kernel pruning, we learn this pattern and achieve the best possible pruning ratios. We first select pruning candidates with the criterion outlined in Section 2. The pruned network is then retrained to compensate for the losses incurred due to pruning. Figure 3b shows the feature map and kernel level pruning applied to MNIST LeCun et al. (1998) network.", "startOffset": 9, "endOffset": 490}, {"referenceID": 3, "context": "One downside of the unconstrained kernel pruning is that convolutions can not be unrolled as matrix-matrix multiplications Chellapilla et al. (2006). However, customized VLSI implementations and FFT based convolutions do not employ convolution unrolling.", "startOffset": 123, "endOffset": 149}, {"referenceID": 3, "context": "One downside of the unconstrained kernel pruning is that convolutions can not be unrolled as matrix-matrix multiplications Chellapilla et al. (2006). However, customized VLSI implementations and FFT based convolutions do not employ convolution unrolling. Mathieu et. al., have proposed FFT based convolutions for faster CNN training and evaluation Mathieu et al. (2013). The GPU based parallel implementation showed very good speedups.", "startOffset": 123, "endOffset": 370}, {"referenceID": 3, "context": "One downside of the unconstrained kernel pruning is that convolutions can not be unrolled as matrix-matrix multiplications Chellapilla et al. (2006). However, customized VLSI implementations and FFT based convolutions do not employ convolution unrolling. Mathieu et. al., have proposed FFT based convolutions for faster CNN training and evaluation Mathieu et al. (2013). The GPU based parallel implementation showed very good speedups. As commonly known that the IFFT (FFT (kerenel) \u00d7 FFT (fmap)) = kernel \u2217 fmap, the kernel level pruning can relieve this task. Although the kernel size is small, massive reusability enables the use of FFT. The FFT of each kernel is computed only once and reused for multiple input vectors in a mini-batch. In a feed forward and backward path, the summations can be carried in the FFT domain and once the sum is available, the IFFT can be performed Mathieu et al. (2013). Similarly, a customized VLSI based implementation can also benefit from the kernel level pruning.", "startOffset": 123, "endOffset": 905}, {"referenceID": 5, "context": "We report the network architecture with an alphanumeric string as reported in Courbariaux et al. (2015) and outlined in Table 1.", "startOffset": 78, "endOffset": 104}, {"referenceID": 13, "context": "2011] and bears similarity with the MNIST handwritten digit recognition dataset [LeCun et al. 1998].", "startOffset": 80, "endOffset": 99}], "year": 2016, "abstractText": "The learning capability of a neural network improves with increasing depth at higher computational costs. Wider layers with dense kernel connectivity patterns furhter increase this cost and may hinder real-time inference. We propose feature map and kernel level pruning for reducing the computational complexity of a deep convolutional neural network. Pruning feature maps reduces the width of a layer and hence does not need any sparse representation. Further, kernel pruning converts the dense connectivity pattern into a sparse one. Due to coarse nature, these pruning granularities can be exploited by GPUs and VLSI based implementations. We propose a simple and generic strategy to choose the least adversarial pruning masks for both granularities. The pruned networks are retrained which compensates the loss in accuracy. We obtain the best pruning ratios when we prune a network with both granularities. Experiments with the CIFAR-10 dataset show that more than 85% sparsity can be induced in the convolution layers with less than 1% increase in the missclassification rate of the baseline network.", "creator": "LaTeX with hyperref package"}}}