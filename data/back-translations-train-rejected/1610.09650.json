{"id": "1610.09650", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Oct-2016", "title": "Deep Model Compression: Distilling Knowledge from Noisy Teachers", "abstract": "The remarkable successes of deep learning models across various applications have resulted in the design of deeper networks that can solve complex problems. However, the increasing depth of such models also results in a higher storage and runtime complexity, which restricts the deployability of such very deep models on mobile and portable devices, which have limited storage and battery capacity. While many methods have been proposed for deep model compression in recent years, almost all of them have focused on reducing storage complexity. In this work, we extend the teacher-student framework for deep model compression, since it has the potential to address runtime and train time complexity too. We propose a simple methodology to include a noise-based regularizer while training the student from the teacher, which provides a healthy improvement in the performance of the student network. Our experiments on the CIFAR-10, SVHN and MNIST datasets show promising improvement, with the best performance on the CIFAR-10 dataset. We also conduct a comprehensive empirical evaluation of the proposed method under related settings on the CIFAR-10 dataset to show the promise of the proposed approach.", "histories": [["v1", "Sun, 30 Oct 2016 13:54:39 GMT  (259kb,D)", "http://arxiv.org/abs/1610.09650v1", "Submitted to WACV,2017"], ["v2", "Wed, 2 Nov 2016 16:32:23 GMT  (259kb,D)", "http://arxiv.org/abs/1610.09650v2", "9 pages, 3 figures"]], "COMMENTS": "Submitted to WACV,2017", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["bharat bhusan sau", "vineeth n balasubramanian"], "accepted": false, "id": "1610.09650"}, "pdf": {"name": "1610.09650.pdf", "metadata": {"source": "CRF", "title": "Deep Model Compression: Distilling Knowledge from Noisy Teachers", "authors": ["Bharat Bhusan Sau", "Vineeth N. Balasubramanian"], "emails": ["cs14mtech11002@iith.ac.in", "vineethnb@iith.ac.in"], "sections": [{"heading": "1. Introduction", "text": "In this context, it should be mentioned that this is a very complex matter, which is a very complex and complex matter."}, {"heading": "2. Background and Related Work", "text": "This year, it has reached the stage where it will be able to take the lead."}, {"heading": "3. Proposed Methodology", "text": "Since our method is based on the teacher-student ratio, i.e. the \"dark knowledge\" framework for deep model compression, we start with a brief overview of teacher-student learning methods in Section 3.1, describe such a method in Section 3.2 (which we use as a basis for comparison), and then provide our methodology in Section 3.3. Then, in Section 3.4, we show how the proposed method corresponds to a noise-based regulation of the teacher."}, {"heading": "3.1. Teacher-Student Learning", "text": "In the context of the teacher-student model in deep learning, the teacher is a pre-formed deep model that is used to train another (typically flat) model called the student. In this context, there has been limited previous work, as described in Section 2. However, there are significant advantages to using a teacher-student model that goes beyond pure model compression, as observed by Hinton et al. [12]: \u2022 The \"dark knowledge\" present in teacher equipment functions as a powerful target-cum regulator for the student model, as it provides soft targets that exchange helpful information. \u2022 The convergence is typically faster than the use of just 0 / 1 hard labels due to the soft targets that help the training. \u2022 A small amount of training data is generally sufficient to train the student network.These advantages motivate us to expand this idea by loud teachers. We now describe one of the teacher-student methods that serve as the basis for our experiments."}, {"heading": "3.2. Student Learning using Logit Regression", "text": "Ba and Caruana [1] proposed a method to train the student directly on the basis of the protocol probability values z, also called logits, which is the output of the shift prior to Softmax activation. The student network is trained in a regression environment using the logits, its training data being minimized during the training by: {(x (1), z (1),...., (x (i), z (i),...., (x (n), z (n)). The L2 loss function is derived from: L (x, z, \u03b8) = 12T Africa i-g (x (i); \u03b8) \u2212 z (i) and 22 (1), where: \u2022 T is the minibatch size \u2022 x (i) which is the corresponding Logit output of the pre-teacher for x (i) \u2022 \u03b8 the set of student model parameters \u2022 g (x)."}, {"heading": "3.3. \u2018Noisy Teachers\u2019: Student Learning using Logit Perturbation", "text": "The performance of shallow models in the teacher-pupil framework has been significantly improved by the methods proposed by Hinton et al. (12), as well as Ba and Caruana in [1]. In this paper, we explore this line of work further by asking the question: What if a pupil learns from several teachers? Analogous to human learning environments, where a pupil can improve his understanding of a subject by learning the same subject from several experts in the field, we assume that the pupil's performance will improve under such an attitude. However, instead of learning directly from several teachers, we propose a methodology to simulate the effect of multiple teachers by injecting noise and disrupting a teacher's logit outputs. Perturbed outputs, not only simulate a multiple-teacher setting, but also results in the loss layer, which produces the effect of a regulator."}, {"heading": "3.4. Equivalence to Noise-Based Regularization", "text": "It has long been known that noise data in education help regulate a model (one of the earliest papers to show this was [26]). Bishop showed in [2] that the addition of an L2 regulation term in the loss function is tantamount to the addition of Gaussian noise in the input data. The regulated loss function is given as: L (x), \u03b8, z) = L (x, \u03b8, z) (5), where x \"x\" corresponds to Gaussian noise, L (x) is the L2 loss equivalent to noise input data, L (x, \u03b8, z) is the L2 loss for original input data, and R (\u03b8) is the L2 regulator. In our method, we disrupt the target output with noise instead of the input data. It is now trivial to show that the impairment of the target output, the value of the teaching grid, is equal."}, {"heading": "4. Experimental Results", "text": "We evaluated our method using three benchmark datasets: MNIST [18] and SVHN [22] for digit recognition and CIFAR-10 [15] for natural image recognition - each of which is described below. Stochastic Gradient Descent (SGD) is used to train all networks used with a mini-batch size of 64. ADAM [14], which combines notions of dynamics and adaptive learning rates, is used to adjust the learning rate in each iteration of the SGD. Convergence was determined by testing with a hold-out validation set. [12] For experiments in this section, we compare the performance of our method with the base performance of the teaching student method proposed by Ba and Caruana. In addition, we conducted preliminary experiments to compare the proposed method with the work of Hinton et al. [12]; however, we did not find it trivial to determine the temperature that is maximized in the performance of their method."}, {"heading": "4.1. MNIST", "text": "MNIST [18] is a popular 10-class (0-9) handwritten digit recognition dataset with 50,000 images and 10,000 images in the validation set. All samples are 28 x 28 grayscale images. No pre-processing is performed on the training data to be consistent with previous work on this dataset. Teacher Network: We use a modified network of LeNet [17] as the teacher network on this dataset. LeNet has two volume layers and a fully connected layer followed by a 10-way classifier. We shorten the configuration of the teacher network to: [C5 (S1P0) @ 20-MP2 (S2)] [C5 (S1P0) @ 50-MP2 (S2)] - FC500 - FC10, with the following conversion layer as follows, with the following number of in-diction of the kernel size, i.e. C5 \u00b7 S5 = 1 x 1)."}, {"heading": "4.2. SVHN", "text": "The Street View House Numbers (SVHN) [22] is a Realworld Image Dataset that contains cropped digits in house numbers from Google Street View images. It contains a much higher number of training and test samples than MNIST. It has 73257 training samples, 26032 test samples, along with 531131 additional samples that can be used for training, each of which is a 32 x 32 RGB image. As in previous work [8] that used this dataset, we have selected 400 samples per category from the training set and 200 samples per category from the additional set to form a validation set of 6000 samples (which is used to determine whether the training has sufficiently converged). The remaining 598388 samples are combined to form the training set. We have also used all data using local contrast normalization as in [13]. Teacher Network: We used the Network-in-network [20] our teacher network."}, {"heading": "4.3. CIFAR-10", "text": "CIFAR-10 [15] is a popular dataset for small-scale image recognition. The dataset contains 10 classes of natural images with a total of 50,000 training samples and 10,000 test samples, each of which is a 32 x 32 RGB image. We pre-processed the data by subtracting the mean per pixel, and also took a mirror image of the samples for data enlargement during training (increasing the training set to 100,000 images).Teacher Network: We used the same teacher network (the network-in-network model) as for the SVHN dataset (Section 4.2). Student Network: The student network we use is also a modified version of the LeNet architecture with two revolutionary layers and a fully connected layer. The architecture of our student network for this dataset can be specified as follows: [C5 (S1P2) @ 64-MP2 (S2)]."}, {"heading": "5. Discussions and Analysis", "text": "In this section we will perform a comprehensive analysis of the performance of the proposed method under different conditions: different teacher noise, comparison between a noisy teacher and a student who is directly regulated by noise, equivalence of this noisy teacher framework with the learning of several teachers, and comparison of the proposed noisy teacher approach with other regulation techniques. Our experimental results in this section are all conducted on the CIFAR 10 dataset."}, {"heading": "5.1. Varying Noise in the Teacher", "text": "In Section 4.3, we selected samples from a minibatch with a fixed probability \u03b1 = 0.5 and varied \u03c3 to experiment with different noise levels. In this section, we fix \u03c3 (standard deviation of Gaussian noise) and vary the probability \u03b1 of the number of Logit values on which noise is added. Results are in Table 4. For this test, we considered for all experiments constant \u03c3 = 0.6. The teacher and student networks are the same as in Section 4.3. The table shows that higher values of \u03b1 help the student to perform better. Performance is best at \u03b1 = 0.8, i.e., when about 80% of the samples of a minibatch are selected for interference. Our empirical studies showed that the value of \u03b1, for which a student network can provide the best result, depends on the performance gap between the base student and the teacher network. If this gap is large, a higher level of noise is required to help achieve this gap, which is better to train the student and its performance is 4.2."}, {"heading": "5.2. Noise in Teacher vs Noise in Student", "text": "In the experiments in Section 4.3, we saw that the noise of the teacher, i.e. the disturbance of the Logit outputs of the teacher network, has a positive regulating effect on the student and helps him to achieve a higher performance than the baseline (only Logit regression). In this section, we ask whether such a performance improvement can be achieved by applying the basic method directly and instead regulating the student network during the training (using weight loss), which can be considered as a direct addition of noise to the student during the training. The crucial difference between these two constellations is that the teacher network is already being trained while the student network is being trained. Therefore, the Logit output of a loud teacher remains the same for a sample during the entire training phase, while in the case of a loud student this is constantly changing. In this experiment, we modified the student's Logit output in the same way as we believe for the teacher network, when we expect the teacher network to use the same results as the student in a similar section 4.- and we expect the student to use the same results in a similar section 4.-."}, {"heading": "5.3. Comparison with DropOut", "text": "While adding noise to the student is a form of regulation (as discussed in Section 3.4), we also compared the performance of the proposed method with another commonly used regulation method: DropOut [30]. We used the same teacher-student network as described in Section 4.3, adding a DropOut layer after the fully connected layer of the student network. We examined the student's performance under different dropout ratios (i.e. with varying probability of dropping nodes).The results are summarized in Table 5. We note that DropOut does not perform as well as the proposed noisy teacher-regulator. The best improvement over the baseline that DropOut achieved was a 1.53% reduction in the error rate, while the proposed method performed significantly better (as in Table 3) with a best reduction in the error rate of 3.26%."}, {"heading": "5.4. More Results", "text": "In this section, we will describe other experiments we have conducted to examine related topics: (i) the effects of the random noise level in each iteration of the training; and (ii) the learning of multiple teachers. Effect of the random noise levels: As described in Section 3, we have used a Gaussian disturbance to disrupt the teacher's logit output in this work. In all previous experiments, we have used a fixed value (standard deviation or noise level, as we have called it) in all iterations. So, the total disturbance is, in a sense, controlled. To investigate the effects of more uncontrolled noise in mini batches of training, we will examine the effect of using a random value in each mini-batch iteration that causes the perturbation in different iterations. We will select a value of the unit in [0.01, 1], and generate Gaussian noise accordingly, to disrupt teacher outputs."}, {"heading": "5.5. Runtime Compression of Shallow Students", "text": "Considering that the key application of the proposed idea in Deep Learning Applications is contained in the model compression, we examine the issue of compression in this section. Newer methods, especially in the last 2 years, have performed very well in reducing memory complexity (e.g. [9]). However, the compressed networks, in most such cases, must be decompressed before use in real-time, resulting in higher runtime complexity. The reduced storage complexity of the shallow student networks is evident due to the reduced depth (similar, reduced bandwidth complexity is also evident given a pre-trained teacher. In particular, we study the runtime complexity of the shallow storage and runtime complexity of student networks in this section. We are building three shallow networks of depth 41, which require significantly lower compression than the deep networks used today, for the CIFAR-10 Dataset."}, {"heading": "6. Conclusions", "text": "Our noise-based control method helped the flat student model to become significantly better than the teacher-student algorithm. The proposed method can also be seen as a simulation of multi-teacher learning, bringing student models closer to the teacher's performance."}], "references": [{"title": "Do deep nets really need to be deep? In Advances in neural information processing", "author": ["J. Ba", "R. Caruana"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2014}, {"title": "Training with noise is equivalent to tikhonov regularization", "author": ["C.M. Bishop"], "venue": "Neural computation, 7(1):108\u2013116,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1995}, {"title": "Model compression", "author": ["C. Bucilu", "R. Caruana", "A. Niculescu-Mizil"], "venue": "Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 535\u2013541. ACM,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2006}, {"title": "Compressing neural networks with the hashing trick", "author": ["W. Chen", "J.T. Wilson", "S. Tyree", "K.Q. Weinberger", "Y. Chen"], "venue": "CoRR, abs/1504.04788,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2015}, {"title": "Predicting parameters in deep learning", "author": ["M. Denil", "B. Shakibi", "L. Dinh", "N. de Freitas"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2013}, {"title": "Exploiting linear structure within convolutional networks for efficient evaluation", "author": ["E.L. Denton", "W. Zaremba", "J. Bruna", "Y. LeCun", "R. Fergus"], "venue": "Advances in Neural Information Processing Systems, pages 1269\u20131277,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2014}, {"title": "Compressing deep convolutional networks using vector quantization", "author": ["Y. Gong", "L. Liu", "M. Yang", "L. Bourdev"], "venue": "arXiv preprint arXiv:1412.6115,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2014}, {"title": "Maxout networks", "author": ["I.J. Goodfellow", "D. Warde-Farley", "M. Mirza", "A.C. Courville", "Y. Bengio"], "venue": "ICML (3), 28:1319\u20131327,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2013}, {"title": "Deep compression: Compressing deep neural network with pruning, trained quantization and huffman coding", "author": ["S. Han", "H. Mao", "W.J. Dally"], "venue": "CoRR, abs/1510.00149, 2,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning both weights and connections for efficient neural network", "author": ["S. Han", "J. Pool", "J. Tran", "W. Dally"], "venue": "Advances in Neural Information Processing Systems, pages 1135\u20131143,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep residual learning for image recognition", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "arXiv preprint arXiv:1512.03385,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "Distilling the knowledge in a neural network", "author": ["G. Hinton", "O. Vinyals", "J. Dean"], "venue": "arXiv preprint arXiv:1503.02531,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2015}, {"title": "What is the best multi-stage architecture for object recognition", "author": ["K. Jarrett", "K. Kavukcuoglu", "Y. Lecun"], "venue": "IEEE 12th International Conference on Computer Vision,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2009}, {"title": "Adam: A method for stochastic optimization", "author": ["D. Kingma", "J. Ba"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning multiple layers of features from tiny images", "author": ["A. Krizhevsky", "G. Hinton"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2009}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "Advances in neural information processing systems, pages 1097\u20131105,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2012}, {"title": "Handwritten digit recognition with a back-propagation network", "author": ["B.B. Le Cun", "J.S. Denker", "D. Henderson", "R.E. Howard", "W. Hubbard", "L.D. Jackel"], "venue": "Advances in neural information processing systems. Citeseer,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1990}, {"title": "Gradientbased learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE, 86(11):2278\u20132324,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1998}, {"title": "Lazy evaluation of convolutional filters", "author": ["S. Leroux", "S. Bohez", "C. De Boom", "E. De Coninck", "T. Verbelen", "B. Vankeirsbilck", "P. Simoens", "B. Dhoedt"], "venue": "arXiv preprint arXiv:1605.08543,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2016}, {"title": "Network in network", "author": ["M. Lin", "Q. Chen", "S. Yan"], "venue": "arXiv preprint arXiv:1312.4400,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2013}, {"title": "Compressing deep neural networks using a rank-constrained topology", "author": ["P. Nakkiran", "R. Alvarez", "R. Prabhavalkar", "C. Parada"], "venue": null, "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2015}, {"title": "Reading digits in natural images with unsupervised feature learning", "author": ["Y. Netzer", "T. Wang", "A. Coates", "A. Bissacco", "B. Wu", "A.Y. Ng"], "venue": null, "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2011}, {"title": "Tensorizing neural networks", "author": ["A. Novikov", "D. Podoprikhin", "A. Osokin", "D.P. Vetrov"], "venue": "Advances in Neural Information Processing Systems, pages 442\u2013450,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2015}, {"title": "Fitnets: Hints for thin deep nets", "author": ["A. Romero", "N. Ballas", "S.E. Kahou", "A. Chassang", "C. Gatta", "Y. Bengio"], "venue": "arXiv preprint arXiv:1412.6550,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2014}, {"title": "Low-rank matrix factorization for deep neural network training with high-dimensional output targets", "author": ["T.N. Sainath", "B. Kingsbury", "V. Sindhwani", "E. Arisoy", "B. Ramabhadran"], "venue": "2013 IEEE International Conference on Acoustics, Speech and Signal Processing, pages 6655\u20136659. IEEE,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2013}, {"title": "Creating artificial neural networks that generalize", "author": ["J. Sietsma", "R.J. Dow"], "venue": "Neural networks, 4(1):67\u201379,", "citeRegEx": "26", "shortCiteRegEx": null, "year": 1991}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "arXiv preprint arXiv:1409.1556,", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2014}, {"title": "Compression of deep neural networks on the fly", "author": ["G. Souli\u00e9", "V. Gripon", "M. Robert"], "venue": "arXiv preprint arXiv:1509.08745,", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2015}, {"title": "Data-free parameter pruning for deep neural networks", "author": ["S. Srinivas", "R.V. Babu"], "venue": "arXiv preprint arXiv:1507.06149,", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2015}, {"title": "Dropout: a simple way to prevent neural networks from overfitting", "author": ["N. Srivastava", "G.E. Hinton", "A. Krizhevsky", "I. Sutskever", "R. Salakhutdinov"], "venue": "Journal of Machine Learning Research, 15(1):1929\u20131958,", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2014}, {"title": "Going deeper with convolutions", "author": ["C. Szegedy", "W. Liu", "Y. Jia", "P. Sermanet", "S. Reed", "D. Anguelov", "D. Erhan", "V. Vanhoucke", "A. Rabinovich"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 1\u20139,", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2015}, {"title": "Do deep convolutional nets really need to be deep (or even convolutional)", "author": ["G. Urban", "K.J. Geras", "S.E. Kahou", "O. Aslan", "S. Wang", "R. Caruana", "A. Mohamed", "M. Philipose", "M. Richardson"], "venue": "arXiv preprint arXiv:1603.05691,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2016}], "referenceMentions": [{"referenceID": 15, "context": "[16] in 2012, deep learning has become very popular and replaced classical computer vision in a wide variety of real-world applications.", "startOffset": 0, "endOffset": 4}, {"referenceID": 30, "context": "With the introduction of networks like GoogLeNet [31], VGGNet [27] and ResNets [11] in recent times, networks are becoming deeper and deeper.", "startOffset": 49, "endOffset": 53}, {"referenceID": 26, "context": "With the introduction of networks like GoogLeNet [31], VGGNet [27] and ResNets [11] in recent times, networks are becoming deeper and deeper.", "startOffset": 62, "endOffset": 66}, {"referenceID": 10, "context": "With the introduction of networks like GoogLeNet [31], VGGNet [27] and ResNets [11] in recent times, networks are becoming deeper and deeper.", "startOffset": 79, "endOffset": 83}, {"referenceID": 10, "context": "Impressive progress has been made to train very deep networks with the invention of newer methods (as in [11]), as well as the relatively easier availability of computational resources today.", "startOffset": 105, "endOffset": 109}, {"referenceID": 26, "context": "the VGGNet network [27] requires 540 MB of storage, which is not suitable for a mobile device).", "startOffset": 19, "endOffset": 23}, {"referenceID": 9, "context": "in [10], running large networks on mobile devices increases memory access, which in turn consumes considerable battery power.", "startOffset": 3, "endOffset": 7}, {"referenceID": 0, "context": "One approach among existing methods, which is not explored well enough, which can address this gap is the teacher-student approach ([1][3][12]) to deep model compression.", "startOffset": 132, "endOffset": 135}, {"referenceID": 2, "context": "One approach among existing methods, which is not explored well enough, which can address this gap is the teacher-student approach ([1][3][12]) to deep model compression.", "startOffset": 135, "endOffset": 138}, {"referenceID": 11, "context": "One approach among existing methods, which is not explored well enough, which can address this gap is the teacher-student approach ([1][3][12]) to deep model compression.", "startOffset": 138, "endOffset": 142}, {"referenceID": 3, "context": "[4] proposed a HashedNets model which used a low-cost hash function to group weights into hash buckets to share parameters where each hash bucket denotes a single parameter.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[7] used k-means clustering to quantize the weights in fully connected layers and achieved upto 24x compression rate for their CNN network with only 1% loss on accuracy on the ImageNet challenge.", "startOffset": 0, "endOffset": 3}, {"referenceID": 27, "context": "[28] used a regularization technique instead to coarsely quantize the weights of fully connected layers.", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "[10] proposed to reduce the number of parameters by pruning weights which are below a threshold after the network is trained.", "startOffset": 0, "endOffset": 4}, {"referenceID": 8, "context": "They extended this work by using Huffman encoding [9] to reduce the number of parameters further.", "startOffset": 50, "endOffset": 53}, {"referenceID": 18, "context": "[19] aimed at reducing computations by ignoring the convolutional filters which produce least activational strength.", "startOffset": 0, "endOffset": 4}, {"referenceID": 28, "context": "Srinivas and Babu [29] explored the redundancy among neurons, and proposed a data-free pruning methodology to remove redundant neurons.", "startOffset": 18, "endOffset": 22}, {"referenceID": 2, "context": "[3] where they created synthetic data by labeling unlabeled data with a teacher model.", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "Ba and Caruana [1] proposed to train the student model by mimicking the logit values of the teacher model.", "startOffset": 15, "endOffset": 18}, {"referenceID": 23, "context": "[24] extended this work by using intermediate hidden layer outputs as target values for training a student model.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[12] generalized this method by introducing a temperature variable in the softmax function.", "startOffset": 0, "endOffset": 4}, {"referenceID": 4, "context": "[5], Sainath et al.", "startOffset": 0, "endOffset": 3}, {"referenceID": 24, "context": "[25] and Nakkiran et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "[21] all adopted low-rank decomposition to compress the weights in different layers.", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "[23] converted the dense weight matrices of the fully connected layers to the Tensor Train format, such that the number of parameters is reduced by huge factor while preserving the expressive power of the layer.", "startOffset": 0, "endOffset": 4}, {"referenceID": 5, "context": "[6] that use matrix factorization attempt to speed up operations, for instance in the convolutional layer, but do not provide a holistic solution to the issues of complexity mentioned so far.", "startOffset": 0, "endOffset": 3}, {"referenceID": 11, "context": "[12]: \u2022 The \u2018dark knowledge\u2019 present in the teacher outputs works as a powerful target-cum-regularizer for the student model, as it provides soft targets that share helpful information.", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "Ba and Caruana [1] proposed a method to train the student directly on the log probability values z, also called logits, which is the output of the layer before softmax activation.", "startOffset": 15, "endOffset": 18}, {"referenceID": 11, "context": "in [12], as well as Ba and Caruana in [1].", "startOffset": 3, "endOffset": 7}, {"referenceID": 0, "context": "in [12], as well as Ba and Caruana in [1].", "startOffset": 38, "endOffset": 41}, {"referenceID": 25, "context": "It has been long established that noisy data in training helps to regularize a model (one of the earliest works showing this was [26]).", "startOffset": 129, "endOffset": 133}, {"referenceID": 1, "context": "Bishop showed in [2] that adding an L2 regularization term in the loss function is equivalent to adding Gaussian noise in the input data.", "startOffset": 17, "endOffset": 20}, {"referenceID": 17, "context": "We evaluated our method on three benchmark datasets: MNIST [18] and SVHN [22] for digit recognition, and CIFAR-10 [15] for natural image recognition - each of which is described below.", "startOffset": 59, "endOffset": 63}, {"referenceID": 21, "context": "We evaluated our method on three benchmark datasets: MNIST [18] and SVHN [22] for digit recognition, and CIFAR-10 [15] for natural image recognition - each of which is described below.", "startOffset": 73, "endOffset": 77}, {"referenceID": 14, "context": "We evaluated our method on three benchmark datasets: MNIST [18] and SVHN [22] for digit recognition, and CIFAR-10 [15] for natural image recognition - each of which is described below.", "startOffset": 114, "endOffset": 118}, {"referenceID": 13, "context": "ADAM [14], which combines the ideas of momentum and adaptive learning rates, is used to adjust the learning rate in each iteration of SGD.", "startOffset": 5, "endOffset": 9}, {"referenceID": 0, "context": "For experiments in this section, we compare the performance of our method against the baseline performance of the teacherstudent method as proposed by Ba and Caruana [1].", "startOffset": 166, "endOffset": 169}, {"referenceID": 11, "context": "[12]; however, we found it non-trivial to identify the temperature at which their method\u2019s performance is maximized.", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "In fact, we found it to give worse performance than [1] in our studies, and hence used [1] as the baseline comparison.", "startOffset": 52, "endOffset": 55}, {"referenceID": 0, "context": "In fact, we found it to give worse performance than [1] in our studies, and hence used [1] as the baseline comparison.", "startOffset": 87, "endOffset": 90}, {"referenceID": 17, "context": "MNIST [18] is a popular dataset for handwritten digit recognition with 10 classes (0-9).", "startOffset": 6, "endOffset": 10}, {"referenceID": 16, "context": "Teacher Network: We use a modified network of LeNet [17] as the teacher network on this dataset.", "startOffset": 52, "endOffset": 56}, {"referenceID": 11, "context": "[12], as our student network.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "The Street View House Numbers (SVHN) [22] is a realworld image dataset containing cropped digits in house numbers from Google Street View images.", "startOffset": 37, "endOffset": 41}, {"referenceID": 7, "context": "As in earlier work [8] that have used this dataset, we selected 400 samples per category from the training set and 200 samples per category from the additional set to constitute a validation set of 6000 samples (which is used to decide if the training has converged sufficiently).", "startOffset": 19, "endOffset": 22}, {"referenceID": 12, "context": "We also preprocessed all the data using local contrast normalization as in [13].", "startOffset": 75, "endOffset": 79}, {"referenceID": 19, "context": "Teacher Network: We used the Network-in-Network [20] model as our teacher network for this dataset.", "startOffset": 48, "endOffset": 52}, {"referenceID": 31, "context": "[32].", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "CIFAR-10 [15] is a popular dataset for small-scale image recognition.", "startOffset": 9, "endOffset": 13}, {"referenceID": 29, "context": "4), we also compared the performance of the proposed method against another often used regularization method: DropOut [30].", "startOffset": 118, "endOffset": 122}, {"referenceID": 15, "context": "3 and (ii) A modified version of Alexnet [16](Teacher2).", "startOffset": 41, "endOffset": 45}, {"referenceID": 8, "context": "[9]).", "startOffset": 0, "endOffset": 3}], "year": 2016, "abstractText": "The remarkable successes of deep learning models across various applications have resulted in the design of deeper networks that can solve complex problems. However, the increasing depth of such models also results in a higher storage and runtime complexity, which restricts the deployability of such very deep models on mobile and portable devices, which have limited storage and battery capacity. While many methods have been proposed for deep model compression in recent years, almost all of them have focused on reducing storage complexity. In this work, we extend the teacher-student framework for deep model compression, since it has the potential to address runtime and train time complexity too. We propose a simple methodology to include a noise-based regularizer while training the student from the teacher, which provides a healthy improvement in the performance of the student network. Our experiments on the CIFAR-10, SVHN and MNIST datasets show promising improvement, with the best performance on the CIFAR-10 dataset. We also conduct a comprehensive empirical evaluation of the proposed method under related settings on the CIFAR-10 dataset to show the promise of the proposed approach.", "creator": "LaTeX with hyperref package"}}}