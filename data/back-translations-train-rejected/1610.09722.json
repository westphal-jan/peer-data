{"id": "1610.09722", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Oct-2016", "title": "Represent, Aggregate, and Constrain: A Novel Architecture for Machine Reading from Noisy Sources", "abstract": "In order to extract event information from text, a machine reading model must learn to accurately read and interpret the ways in which that information is expressed. But it must also, as the human reader must, aggregate numerous individual value hypotheses into a single coherent global analysis, applying global constraints which reflect prior knowledge of the domain.", "histories": [["v1", "Sun, 30 Oct 2016 22:33:47 GMT  (347kb,D)", "http://arxiv.org/abs/1610.09722v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["jason naradowsky", "sebastian riedel"], "accepted": false, "id": "1610.09722"}, "pdf": {"name": "1610.09722.pdf", "metadata": {"source": "CRF", "title": "Represent, Aggregate, and Constrain: A Novel Architecture for Machine Reading from Noisy Sources", "authors": ["Jason Naradowsky", "Sebastian Riedel"], "emails": ["s.riedel}@cs.ucl.ac.uk"], "sections": [{"heading": null, "text": "In this paper, we focus on the task of extracting information about airplane crashes from clusters of related news articles whose labels are derived by remote monitoring. In contrast to earlier machine-readable work, we assume that while most target values occur frequently in most clusters, they may also be missing or incorrect. We are introducing a novel neural architecture to explicitly model the noisy nature of the data and deal with these above-mentioned learning problems.Our models are uniformly designed and achieve an improvement of more than 12.1 F1 over previous work, although they use significantly less linguistic annotation.We apply factor-graph constraints to promote more coherent event analysis, formulating beliefs propagation conclusions within the transitions of a relapsing neural network. We show that this technique additionally improves the maximum formula 1 by up to 2.8 points, resulting in a relative improvement of 50% over the prior technology."}, {"heading": "1 Introduction", "text": "(...). (...). (...). (...). (...). (...). (...). (...). \"(...).\" (...). \"(...). (...).\" (...). (...). \"(...).\" (...). \"(...). (...).\" (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (). (...). (...). (...). (...). (). (). (). (). (). (...). (). (). (...). (). (...). (). (...). (). (). (). (). (...). (...). (). (). (...). (...). (). (). (). (). (). (). (). (...). ("}, {"heading": "1.1 The Case for Aggregation", "text": "Effective aggregation techniques can be critical in identifying accurate information from noisy sources. Figure 1 shows an example of our problem scenario. An IR component retrieves individual documents based on the query, and sample sentences are shown for each document. The goal is to extract the correct value, of which there can be many mentions in the text for each slot. Sentences in d1 express a target slot, the number of deaths, but the mention corresponds to an incorrect value. This is a common error in early news reports. Documents d3 and d4 also express this slot, with mentions of the correct value, but with less certainty. A model that focuses on a single mention with a high score, at the expense of breadth, makes a false prediction. In comparison, a model that learns to collect evidence for each value correctly across multiple mentions across the cluster can identify the correct information and circumvent this problem."}, {"heading": "2 Model", "text": "In this section, we describe the three modeling components of our proposed architecture: 1. Representation and evaluation, in which a task-specific encoding is generated for each mention and evaluated in relation to each slot.2. Aggregation, in which the results of the individual value mentions are summarized to account for each slot-value pairing.3. Restriction, in which we model additional dependencies between pairs of values and between slots to promote a more reasonable interpretation of the event as a whole. Let's start with the definition of terminology.A message cluster c is a set of documents, {d1,..., d | c |} c, in which each document is described by a string of words, d = (w1,..., w | d |).One mention is an occurrence of a value in its textual context. For each value v \u0121V, there are potentially many mentions of v in the cluster, defined as m-M (v) These were listed in the 2014 Stannop (LNP)."}, {"heading": "2.1 Representations and Scoring", "text": "This representation functions as a general \"reading\" or encryption of the mention, regardless of the type of slots for which it is later considered. This differs from previous machine-read research, where the model provides a query-specific reading of the document or reads the document several times when answering a single query (Hermann et al., 2015). As in previous work, an embedding of the context of a mention serves as a representation. We construct an embedding matrix E-Re-n, where e is the dimensionality of the embedding and n is the number of words in the cluster, which are fixed during the training. All mentions are masked and receive the same uniform vector instead of a pre-trained embedding. From this matrix, we embed the context using a two-layer revolutionary network (CNN), with a detailed discussion of the sentence in the 2015 classification similarly used for tasks (transmission)."}, {"heading": "2.1.1 Scoring", "text": "After generating a representation r (m) for each mention m, a slot-specific attention mechanism \u03c6mention (m, s) \u0394R represents the compatibility of mention m with slot s. Let R-Rn \u00b7 r be the representation matrix consisting of all r (m), and k (m) is the index of m in R. We create a separate embedding for each mention s-Rr and use it to maintain all mentions in the cluster as follows: us = R\u03c0s (1) as = Softmax (us) (2) \u03c6mention (m, s) = a s k (m) (3) The mentions values reflect an interpretation of the encoding of the value in relation to the slot. Softmax normalizes the values across each slot, attracts attention and creates a competition between mentions. This encourages the model to participate in mentions with the most characteristic context for each slot."}, {"heading": "2.2 Aggregating Mention-level Scores", "text": "For values repeatedly mentioned in the message cluster, mention values must be aggregated to obtain a single value. In this section, we will (1) describe how the correct aggregation method can better reflect how the gold labels are applied to the data, (2) how domain knowledge can be incorporated into the aggregation, and (3) how aggregation can be used as a dynamic approach to identifying missing information."}, {"heading": "2.2.1 Aggregation as a Model of Distant Supervision", "text": "In the traditional view of remote supervision (Mintz et al., 2009), if a mention in an1We is also experimented with sequential context models, but observes a negligible effect on performance if a 1-best decoding strategy is pursued. External knowledge base is assumed to be an expression of its role in the knowledge base, and it receives the appropriate label. This assumption is not always true, and the resulting false labels are often cited as a source of training noise (Riedel et al., 2010; Hoffmann et al., 2011). However, aggregation of all mention values provides a more accurate reflection on how far-flung supervisory labels are applied to the data. If one assigns a label to each mention and constructs a loss by directly using the mention values (mention), it would replicate the hard labeling of the traditional remote supervision scenario."}, {"heading": "2.2.2 Weighted Aggregation", "text": "The aggregation methods mentioned above consistently combine mention values, but in many areas one can have prior knowledge of which mentions should contribute more to the overall result. It is easy to add a separate weight \u03b1m for each mention m to the proposed aggregation methods, for example to create a weighted sum aggregation: \u03c6value (v, s) = \u2211 m (v) \u03b1m \u00b7 \u03c6mention (m, s) (6) These weights can be learned from data, or they can be defined heuristically on the basis of a priori beliefs. Here we present two such heuristic methods. Topic-based aggregation articles of course differ from the current event, often including comparisons to related events and summaries of past events. Each such instance introduces additional noise into the system, since the contexts of current and non-current mentions are often similar."}, {"heading": "2.2.3 Known Unknowns", "text": "A difficult machine read problem, which occurs only in loud text sources where the correct values may not be present in the cluster, is determining whether a value should be predicted at all. A common solution to dealing with such missing values is to use a threshold below which the model returns zero. However, even a separate threshold for each slot would not fully capture the nature of the problem. Determining whether a value is missing is a trade-off between two factors: (1) how strongly the mention values support a non-zero response, and (2) how much general information is given about this event and this slot. We include both factors by expanding the definition of R and its use in Equation 1-Equation 3 to include not just mentions, but all words. Any non-mention word is treated as mentioning the zero value: The value loss (v = zero, s) = D = c \u00b2 w \u00b2 f \u00b2 f \u00b2 f (7) is the mention of M \u00b2 (7)."}, {"heading": "2.3 Global Constraints", "text": "While the combination of learned representations and aggregation produces a distinct effective system, its predictions may reflect a lack of world knowledge. For example, we want to prevent the model from predicting the same value for multiple slots, as this is not common. Following recent work in computer vision suggesting a differentiated interpretation of the propagation conclusion (Ross et al., 2011; Zheng et al., 2015), we present a relapsing neural network (RNN) that implements conclusions under this limitation."}, {"heading": "2.3.1 Belief Propagation as an RNN", "text": "A factor graph is a graphical model that factorises the model function using a two-sided graph consisting of variables and factors. Variables maintain their beliefs about their values and factors specify the values via the configuration of these values for the variables that adjacent them. We restrict model output by applying a factor diagram model to the values it produces. A combinatorial logic factor that takes the value v is represented in the factor graph by a Boolean variable Xv, s. Each Xv, s is associated with a local factor uv, the initial potential of which is derived from the qualifying values (v, s). A combinatorial logic factor, exactly-1 (Smith and Eisner, 2008) is created for each slot connected across all values, and (2) is created for each value connected across all slots. This is illustrated in Figure 2."}, {"heading": "3 Data", "text": "The Stanford Plane Crash Dataset (Reschke et al., 2014) is a small dataset consisting of 80 aircraft crashes, each of which is paired with a series of related news articles, 40 of which are reserved for training and 40 for testing, with the average cluster containing more than 2,000 mentions.2 Gold labels for each cluster come from Wikipedia information boxes and cover up to 15 slots, 8 of which are used for evaluation (Figure 3).We follow the same normalization process as Reschke et al. (2014), limiting cluster size to the first 200 documents and further reducing the number of duplicate documents to prevent distortions in aggregation."}, {"heading": "4 Experiments", "text": "In all experiments we train with adaptive online gradient updates (Adam, see Kingma and Ba (2014)). Model architecture and parameter values have been adapted to the development set and are as follows (selected values in bold): \u2022 CNN Layer 1 filter width: [3, 5, 8, 10] \u2022 CNN Layer 2 filter width: [0, 3, 5, 8, 10] \u2022 CNN Layer 1 dim: [5, 10, 15, 20] \u2022 CNN Layer 2 dim: [0, 5, 10, 15, 20] 2Although it should be noted that only 33 of the training clusters and only 27 of the test clusters contain documents from which information can be extracted. \u2022 max pooling [True, False] \u2022 Learning rate: [0.001, 0.003, 0.005, 15, 20] \u2022 L2 regulation: [0.001, 0.003, 0.005, 0.005, 0.01] \u2022 Drop-out rate: 1-0.5, [0.6, 0.7, 0.8, 0.9, the number of preferences is determined."}, {"heading": "4.1 Systems", "text": "We evaluate four categories of architecture: Existing baselines Reschke et al. (2014) proposed several methods for event extraction in this scenario. We compare them with three notable examples from this work: \u2022 Reschke CRF: a conditional random field model. \u2022 Reschke Noisy-OR: a sequence tagger with a \"noisy-OR\" form of aggregation that prevents the model from predicting the same value for multiple slots. \u2022 Reschke Best: a sequence tagger that uses a cost-sensitive classifier optimized with SEARN (thumb \u0301 III et al., 2009), a learning-to-search framework. Each of these models uses features derived from dependency trees, local contexts (unigram / partof-speech features for up to 5 adjacent words), sentence context (bag-of-word / part-of-speech), words / part-of-of-speech. \""}, {"heading": "4.2 Evaluation", "text": "We evaluate the configurations of our proposed architecture based on three metrics: the first is a modified version of standard precision, recall, and formula 1. It deviates from the standard protocol by (1) providing a full recall for all slots if a single predicted value is contained in the gold slot; (2) penalizing only slots for which gold values can be found in the text; and (3) limiting the candidate values to the units proposed by Stanford's NER system and included in the data set's release. Similarly, eight of the fifteen slots are used for evaluation; and the second valuation variable we present is standard accuracy, recall, and formula 1, especially for zero values. We also evaluate the models based on the mean mutual ranking (MRR). In calculating the F1-based rating measure, we decode the model by taking the single highest value for each slot. However, this does not necessarily reflect the overall quality of the RQ in the cluster."}, {"heading": "4.3 Results", "text": "Results are presented in Table 1. Compared to previous work, it is clear that our configuration of our RAC architecture with sum-based aggregation outperforms the best existing systems by at least 9.8 F1. Compared to the various Mention CNN systems, it is clear that this improvement is not the result of different features or the use of pre-trained word embeddings or even the representational power of CNN-based embeddings. Rather, it is due to holistic training with aggregation and loss function at cluster level. Aggregation results In terms of aggregation, sum-based methods consistently outperform their maximum counterparts, indicating that the use of redundancy of information in message clusters is beneficial to the task. Topic-based aggregation is statistically significant over standard sum aggregation (p \u2264 0.0215) and produces the most powerful unrestricted aggregation of the system.Data-based aggregation has no statistically significant advantage over aggregation."}, {"heading": "4.4 Effects of Global Constraints", "text": "Performing an iteration of LBP inference results in our highest performance, an F1 of 44.9, which is 14.9 points higher than Reschke's best system and a statistically significant improvement over the unrestricted model (p \u2264 0.0313). Improvements continue throughout the training, as shown in Figure 3.Additional Iterations. This effect is largely based on the assumption that the constraint inference does not absolutely stick in the data. For example, multiple slots may have zero value, and zero is a common value for a number of slots. Performing the constraint inference for a single iteration promotes a 1: 1 representation of values on slots, but does not prohibit it. This result also implies that a hard heuristic decoding time would not be as effective."}, {"heading": "4.5 Error Analysis", "text": "We randomly selected 15 examples that our best model incorrectly predicts, of which we find that three were incorrectly identified in the gold data as errors from the distance monitoring hypothesis (i.e. that the \"zero chance\" for 0 survivors is marked if the number of survivors in the cluster was not mentioned) and should not be predicted. Six were clearly expressed and should be predictable, with strongly correlated keywords present in the context window but assigned low values by the model. We believe in a richer representation that combines the generalization of CNNs with the discrete signal of ngram features (Lei et al., 2015) that can solve some of these problems. Four of the remaining errors appear to be attributable to aggregation errors. Specifically, the occurrence of a specific punctuation mark with far above-average frequency resulted in it being predicted for three slots. While these could be filtered out as a more general solution, a representation based on the actual mention of this problem can be built on (frequently)."}, {"heading": "5 Related Work", "text": "Our work is thematically similar to the work in the field of multi-documentation extraction (Mann and Yarowsky, 2005) and summary (Barzilay et al., 1999), where the content of many input documents is unified into a coherent understanding. However, in addition to the many modeling options we propose, the data sets used in the existing work are not linked to specific events; the same denociation of the nature of these tasks has clear implications for automated fact checking (Vlachos and Riedel, 2014), but no comparable models that currently exist. In contrast, the IDEST system by Krause et al. (2015) is an example of the automatically constructed clusters of news articles to train an event embedding model. However, the focus of IDEST is on improving event illustration, not information extraction, which is reflected in its comparatively simple and heuristically driven extraction."}, {"heading": "5.1 Connections to Pointer Networks", "text": "A pointer network uses a softmax to normalize a vector of the size of the input, to generate an output distribution through the dictionary of inputs (Vinyals et al., 2015). This assumes that the input vector is the size of the dictionary, and that each occurrence is evaluated independently of others. If an element appears repeatedly during input, each occurrence competes not only with other elements, but also with its duplicates. Here, the scoring and aggregation steps of our proposed architecture can be considered together as a pointer network in which there is redundancy in the input that respects an underlying grouping. Here, Softmax normalizes the values via the input vector, and the aggregation step returns an output distribution across the dictionary of input."}, {"heading": "6 Conclusion and Future Work", "text": "In this paper, we present an automated reading architecture designed to effectively read collections of documents in noisy, less controlled scenarios where information may be missing or inaccurate. By using attention-based mention scoring, cluster-wide aggregation of these scores, and global constraints to discourage unlikely solutions, we improve the state of the art in this task by 14.9 FS1. In future work, the foundations laid here could be applied to larger datasets and contribute to the development of such data. Larger noisy datasets would allow the differentiated constraints and weighted aggregations to be incorporated during optimization and optimized in terms of data. Furthermore, we consider the inclusion of graphical model conclusions in neural architectures to be a powerful new tool and potentially an important step toward incorporating higher-level reasoning and prior knowledge into neural models of LNP."}], "references": [{"title": "Information fusion in the context of multi-document summarization", "author": ["Kathleen R. McKeown", "Michael Elhadad"], "venue": "In Proceedings of the 37th Annual Meeting of the Association", "citeRegEx": "Barzilay et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Barzilay et al\\.", "year": 1999}, {"title": "Search-based structured prediction. Machine Learning Journal (MLJ)", "author": ["John Langford", "Daniel Marcu"], "venue": null, "citeRegEx": "III et al\\.,? \\Q2009\\E", "shortCiteRegEx": "III et al\\.", "year": 2009}, {"title": "Teaching machines to read and comprehend", "author": ["Tom\u00e1s Kocisk\u00fd", "Edward Grefenstette", "Lasse Espeholt", "Will Kay", "Mustafa Suleyman", "Phil Blunsom"], "venue": null, "citeRegEx": "Hermann et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hermann et al\\.", "year": 2015}, {"title": "The goldilocks principle: Reading children\u2019s books with explicit memory representations. CoRR", "author": ["Hill et al.2015] Felix Hill", "Antoine Bordes", "Sumit Chopra", "Jason Weston"], "venue": null, "citeRegEx": "Hill et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hill et al\\.", "year": 2015}, {"title": "Knowledge-based weak supervision for information extraction of overlapping relations", "author": ["Congle Zhang", "Xiao Ling", "Luke Zettlemoyer", "Daniel S. Weld"], "venue": "In Proceedings of the 49th Annual Meet-", "citeRegEx": "Hoffmann et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Hoffmann et al\\.", "year": 2011}, {"title": "Text understanding with the attention sum reader network. In Association for Computational Linguistics (ACL)", "author": ["Kadlec et al.2016] Rudolf Kadlec", "Martin Schmid", "Ond\u0159ej Bajgar", "Jan Kleindienst"], "venue": null, "citeRegEx": "Kadlec et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Kadlec et al\\.", "year": 2016}, {"title": "A convolutional neural network for modelling sentences", "author": ["Edward Grefenstette", "Phil Blunsom"], "venue": "In Proceedings of the 52nd Annual Meeting of the Association", "citeRegEx": "Kalchbrenner et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2014}, {"title": "Convolutional neural networks for sentence classification", "author": ["Yoon Kim"], "venue": "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Kim.,? \\Q2014\\E", "shortCiteRegEx": "Kim.", "year": 2014}, {"title": "Adam: A method for stochastic optimization. CoRR, abs/1412.6980", "author": ["Kingma", "Ba2014] Diederik P. Kingma", "Jimmy Ba"], "venue": null, "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Idest: Learning a distributed representation for event patterns", "author": ["Enrique Alfonseca", "Katja Filippova", "Daniele Pighin"], "venue": "In Proceedings of the 2015 Conference of the North American Chapter of the As-", "citeRegEx": "Krause et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Krause et al\\.", "year": 2015}, {"title": "Molding cnns for text: non-linear, non-consecutive convolutions", "author": ["Lei et al.2015] Tao Lei", "Regina Barzilay", "Tommi Jaakkola"], "venue": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Lei et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lei et al\\.", "year": 2015}, {"title": "Neural relation extraction with selective attention over instances", "author": ["Lin et al.2016] Yankai Lin", "Shiqi Shen", "Zhiyuan Liu", "Huanbo Luan", "Maosong Sun"], "venue": "In Proceedings of the 54th Annual Meeting of the Association for Computational Linguis-", "citeRegEx": "Lin et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2016}, {"title": "Multi-field information extraction and cross-document fusion", "author": ["Mann", "Yarowsky2005] Gideon S. Mann", "David Yarowsky"], "venue": "In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics,", "citeRegEx": "Mann et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Mann et al\\.", "year": 2005}, {"title": "The Stanford CoreNLP natural language processing toolkit", "author": ["Mihai Surdeanu", "John Bauer", "Jenny Finkel", "Steven J. Bethard", "David McClosky"], "venue": null, "citeRegEx": "Manning et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Manning et al\\.", "year": 2014}, {"title": "Neural variational inference for text processing. CoRR, abs/1511.06038", "author": ["Miao et al.2015] Yishu Miao", "Lei Yu", "Phil Blunsom"], "venue": null, "citeRegEx": "Miao et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Miao et al\\.", "year": 2015}, {"title": "Distant supervision for relation extraction without labeled data", "author": ["Mintz et al.2009] Mike Mintz", "Steven Bills", "Rion Snow", "Dan Jurafsky"], "venue": "In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th Interna-", "citeRegEx": "Mintz et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Mintz et al\\.", "year": 2009}, {"title": "A decomposable attention model for natural language inference", "author": ["Oscar T\u00e4ckstr\u00f6m", "Dipanjan Das", "Jakob Uszkoreit"], "venue": null, "citeRegEx": "Parikh et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Parikh et al\\.", "year": 2016}, {"title": "Glove: Global vectors for word representation", "author": ["Richard Socher", "Christopher D. Manning"], "venue": "In Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Pennington et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Event extraction using distant supervision", "author": ["Martin Jankowiak", "Mihai Surdeanu", "Christopher D. Manning", "Dan Jurafsky"], "venue": "In Proceedings of the 9th edition of the Language Resources", "citeRegEx": "Reschke et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Reschke et al\\.", "year": 2014}, {"title": "MCTest: A challenge dataset for the open-domain machine comprehension of text", "author": ["Richardson", "Christopher J.C. Burges", "Erin Renshaw."], "venue": "Proceedings of the 2013 Conference on Empirical", "citeRegEx": "Richardson et al\\.,? 2013", "shortCiteRegEx": "Richardson et al\\.", "year": 2013}, {"title": "Modeling relations and their mentions without labeled text", "author": ["Limin Yao", "Andrew McCallum"], "venue": "In Proceedings of the 2010 European Conference on Machine Learning and Knowledge Discovery in Databases:", "citeRegEx": "Riedel et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Riedel et al\\.", "year": 2010}, {"title": "Learning message-passing inference machines for structured prediction", "author": ["Ross et al.2011] Stephane Ross", "Daniel Munoz", "Martial Hebert", "J. Andrew Bagnell"], "venue": "In IEEE Conference on Computer Vision and Pattern Recognition (CVPR)", "citeRegEx": "Ross et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Ross et al\\.", "year": 2011}, {"title": "Dependency parsing by belief propagation", "author": ["Smith", "Eisner2008] David Smith", "Jason Eisner"], "venue": "In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Smith et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Smith et al\\.", "year": 2008}, {"title": "Iterative alternating neural attention for machine", "author": ["Phillip Bachman", "Yoshua Bengio"], "venue": "reading. CoRR,", "citeRegEx": "Sordoni et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Sordoni et al\\.", "year": 2016}, {"title": "Fact checking: Task definition and dataset construction", "author": ["Vlachos", "Riedel2014] Andreas Vlachos", "Sebastian Riedel"], "venue": "In Proceedings of the ACL 2014 Workshop on Language Technologies and Computational Social Science,", "citeRegEx": "Vlachos et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Vlachos et al\\.", "year": 2014}, {"title": "Towards ai-complete question answering: A set of prerequisite toy tasks. CoRR, abs/1502.05698", "author": ["Weston et al.2015] Jason Weston", "Antoine Bordes", "Sumit Chopra", "Tomas Mikolov"], "venue": null, "citeRegEx": "Weston et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Weston et al\\.", "year": 2015}, {"title": "Distant supervision for relation extraction via piecewise convolutional neural networks", "author": ["Zeng et al.2015] Daojian Zeng", "Kang Liu", "Yubo Chen", "Jun Zhao"], "venue": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Lan-", "citeRegEx": "Zeng et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zeng et al\\.", "year": 2015}, {"title": "Conditional random fields as recurrent neural networks", "author": ["Zheng et al.2015] Shuai Zheng", "Sadeep Jayasumana", "Bernardino Romera-Paredes", "Vibhav Vineet", "Zhizhong Su", "Dalong Du", "Chang Huang", "Philip Torr"], "venue": null, "citeRegEx": "Zheng et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zheng et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 2, "context": "Whether identifying target entities for simple cloze style queries (Hermann et al., 2015; Miao et al., 2015), or reasoning over short passages of artificially generated text (Weston et al.", "startOffset": 67, "endOffset": 108}, {"referenceID": 14, "context": "Whether identifying target entities for simple cloze style queries (Hermann et al., 2015; Miao et al., 2015), or reasoning over short passages of artificially generated text (Weston et al.", "startOffset": 67, "endOffset": 108}, {"referenceID": 25, "context": ", 2015), or reasoning over short passages of artificially generated text (Weston et al., 2015), short stories (Richardson et al.", "startOffset": 73, "endOffset": 94}, {"referenceID": 19, "context": ", 2015), short stories (Richardson et al., 2013), or children\u2019s stories (Hill et al.", "startOffset": 23, "endOffset": 48}, {"referenceID": 3, "context": ", 2013), or children\u2019s stories (Hill et al., 2015), these systems all assume that the corresponding text is the unique source of information necessary for answering the query \u2013 one that not only contains the answer, but does not contain misleading or otherwise contradictory information.", "startOffset": 31, "endOffset": 50}, {"referenceID": 13, "context": "These have been annotated in the data using Stanford CoreNLP (Manning et al., 2014).", "startOffset": 61, "endOffset": 83}, {"referenceID": 2, "context": "This differs from some previous machine reading research where the model provides a query-specific reading of the document, or reads the document multiple times when answering a single query (Hermann et al., 2015).", "startOffset": 191, "endOffset": 213}, {"referenceID": 7, "context": "CNNs have been used in a similar manner for a number of information extraction and classification tasks (Kim, 2014; Zeng et al., 2015) and are capable of producing rich sentence representations (Kalchbrenner et al.", "startOffset": 104, "endOffset": 134}, {"referenceID": 26, "context": "CNNs have been used in a similar manner for a number of information extraction and classification tasks (Kim, 2014; Zeng et al., 2015) and are capable of producing rich sentence representations (Kalchbrenner et al.", "startOffset": 104, "endOffset": 134}, {"referenceID": 6, "context": ", 2015) and are capable of producing rich sentence representations (Kalchbrenner et al., 2014).", "startOffset": 67, "endOffset": 94}, {"referenceID": 15, "context": "In the traditional view of distant supervision (Mintz et al., 2009), if a mention is found in an", "startOffset": 47, "endOffset": 67}, {"referenceID": 20, "context": "This assumption does not always hold, and the resulting spurious labels are frequently cited as a source of training noise (Riedel et al., 2010; Hoffmann et al., 2011).", "startOffset": 123, "endOffset": 167}, {"referenceID": 4, "context": "This assumption does not always hold, and the resulting spurious labels are frequently cited as a source of training noise (Riedel et al., 2010; Hoffmann et al., 2011).", "startOffset": 123, "endOffset": 167}, {"referenceID": 20, "context": "If the most clearly expressed mentions correspond to correct values, max aggregation can be an effective strategy (Riedel et al., 2010).", "startOffset": 114, "endOffset": 135}, {"referenceID": 21, "context": "Following recent work in computer vision which proposes a differentiable interpretation of belief propagation inference (Ross et al., 2011; Zheng et al., 2015), we present a recurrent neural network (RNN) which implements inference under this constraint.", "startOffset": 120, "endOffset": 159}, {"referenceID": 27, "context": "Following recent work in computer vision which proposes a differentiable interpretation of belief propagation inference (Ross et al., 2011; Zheng et al., 2015), we present a recurrent neural network (RNN) which implements inference under this constraint.", "startOffset": 120, "endOffset": 159}, {"referenceID": 18, "context": "The Stanford Plane Crash Dataset (Reschke et al., 2014) is a small data set consisting of 80 plane crash events, each paired with a set of related news articles.", "startOffset": 33, "endOffset": 55}, {"referenceID": 18, "context": "We follow the same entity normalization procedure as Reschke et al. (2014), limit the cluster size to the first 200 documents, and further reduce the number of duplicate documents to prevent biases in aggregation.", "startOffset": 53, "endOffset": 75}, {"referenceID": 17, "context": "The pre-trained word embeddings are 200-dimensional GLoVe embeddings (Pennington et al., 2014).", "startOffset": 69, "endOffset": 94}, {"referenceID": 18, "context": "Existing baselines Reschke et al. (2014) proposed several methods for event extraction in this scenario.", "startOffset": 19, "endOffset": 41}, {"referenceID": 5, "context": "EE-AS Reader Kadlec et al. (2016) present AS Reader, a state-of-the-art model for clozestyle QA.", "startOffset": 13, "endOffset": 34}, {"referenceID": 18, "context": "The first is a modified version of standard precision, recall, and F1, as proposed by Reschke et al. (2014). It deviates from the standard protocol by (1) awarding full recall for any slot when a single predicted value is contained in the gold slot, (2) only penalizing slots for which there are findable gold values in the text, and (3) limiting candidate values to the set of entities proposed by the Stanford NER system and included in the data set release.", "startOffset": 86, "endOffset": 108}, {"referenceID": 10, "context": "We belief a richer representation which combines the generalization of CNNs with the discrete signal of ngram features (Lei et al., 2015) may solve some of these issues.", "startOffset": 119, "endOffset": 137}, {"referenceID": 0, "context": "Multi-document and Paraphrase-driven IE Our work is thematically similar to work in multi-document information extraction (Mann and Yarowsky, 2005) and summarization (Barzilay et al., 1999), where the content of many input documents is unified into a cohesive understanding.", "startOffset": 166, "endOffset": 189}, {"referenceID": 9, "context": "In contrast, the the IDEST system of Krause et al. (2015) is an example of previous work which uses automatically constructed clusters of news articles in order to train an event embedding model.", "startOffset": 37, "endOffset": 58}, {"referenceID": 2, "context": "Attention and Aggregation in Machine Reading In terms of reading methodology, our scoring method is a slot-specific interpretation of the attentive reader (Hermann et al., 2015), and our sum aggregation is closelyrelated to Kadlec et al.", "startOffset": 155, "endOffset": 177}, {"referenceID": 23, "context": "Recent machine reading models have used an iterative attention to refine model predictions (Sordoni et al., 2016).", "startOffset": 91, "endOffset": 113}, {"referenceID": 2, "context": "Attention and Aggregation in Machine Reading In terms of reading methodology, our scoring method is a slot-specific interpretation of the attentive reader (Hermann et al., 2015), and our sum aggregation is closelyrelated to Kadlec et al. (2016), with differences described previously in Sec.", "startOffset": 156, "endOffset": 245}, {"referenceID": 2, "context": "Attention and Aggregation in Machine Reading In terms of reading methodology, our scoring method is a slot-specific interpretation of the attentive reader (Hermann et al., 2015), and our sum aggregation is closelyrelated to Kadlec et al. (2016), with differences described previously in Sec. 4.1. A similar method is found in the entailment model of Parikh et al. (2016), where alignment scores (between a premise and a hypothesis) are generated via attention and summed.", "startOffset": 156, "endOffset": 371}, {"referenceID": 13, "context": "Extensions to Distant Supervision Aggregation in our framework is a means to weaken strong distant supervision assumptions, and, unlike Mintz et al. (2009), it does not assume that each mention-level occurrence of a value must express the given relation.", "startOffset": 136, "endOffset": 156}, {"referenceID": 4, "context": "In this respect, it exists as a fully-differentiable analog to the work of Hoffmann et al. (2011), and closely related to the \u201cexpressed at least once\u201d constraint of Riedel et al.", "startOffset": 75, "endOffset": 98}, {"referenceID": 4, "context": "In this respect, it exists as a fully-differentiable analog to the work of Hoffmann et al. (2011), and closely related to the \u201cexpressed at least once\u201d constraint of Riedel et al. (2010). Both allow the model to ignore mislabeled instances in certain circumstances.", "startOffset": 75, "endOffset": 187}, {"referenceID": 4, "context": "In this respect, it exists as a fully-differentiable analog to the work of Hoffmann et al. (2011), and closely related to the \u201cexpressed at least once\u201d constraint of Riedel et al. (2010). Both allow the model to ignore mislabeled instances in certain circumstances. Recently, Lin et al. (2016) have also proposed the use of neural mechanisms to reduce the effect of mislabeled instances, using attention to select the most useful sentences for relation extraction, as we use attention to select the most informative mentions.", "startOffset": 75, "endOffset": 294}], "year": 2016, "abstractText": "In order to extract event information from text, a machine reading model must learn to accurately read and interpret the ways in which that information is expressed. But it must also, as the human reader must, aggregate numerous individual value hypotheses into a single coherent global analysis, applying global constraints which reflect prior knowledge of the domain. In this work we focus on the task of extracting plane crash event information from clusters of related news articles whose labels are derived via distant supervision. Unlike previous machine reading work, we assume that while most target values will occur frequently in most clusters, they may also be missing or incorrect. We introduce a novel neural architecture to explicitly model the noisy nature of the data and to deal with these aforementioned learning issues. Our models are trained end-to-end and achieve an improvement of more than 12.1 F1 over previous work, despite using far less linguistic annotation. We apply factor graph constraints to promote more coherent event analyses, with belief propagation inference formulated within the transitions of a recurrent neural network. We show this technique additionally improves maximum F1 by up to 2.8 points, resulting in a relative improvement of 50% over the previous state-of-the-art.", "creator": "LaTeX with hyperref package"}}}