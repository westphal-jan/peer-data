{"id": "1610.09756", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-Oct-2016", "title": "Towards Deep Learning in Hindi NER: An approach to tackle the Labelled Data Scarcity", "abstract": "In this paper we describe an end to end Neural Model for Named Entity Recognition NER) which is based on Bi-Directional RNN-LSTM's. Almost all NER systems for Hindi use Language Specific features and handcrafted rules with gazetteers. Our model is language independent and uses no domain specific features or any handcrafted rules. Our models rely on semantic information in the form of word vectors which are learnt by an unsupervised learning algorithm on an unannotated corpus. Our model attained state of the art per- formance in both English and Hindi which is a morphologically rich language without the use of any morphological analysis or without using gazetteers of any sort.", "histories": [["v1", "Mon, 31 Oct 2016 01:31:52 GMT  (198kb,D)", "http://arxiv.org/abs/1610.09756v1", "6 pages"], ["v2", "Wed, 16 Nov 2016 17:15:14 GMT  (178kb,D)", "http://arxiv.org/abs/1610.09756v2", "7 pages"]], "COMMENTS": "6 pages", "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["vinayak athavale", "shreenivas bharadwaj", "monik pamecha", "ameya prabhu", "manish shrivastava"], "accepted": false, "id": "1610.09756"}, "pdf": {"name": "1610.09756.pdf", "metadata": {"source": "CRF", "title": "Towards Deep Learning in Hindi NER: An approach to tackle the Labelled Data Sparsity", "authors": ["Vinayak Athavale", "Ameya Prabhu", "Manish Shrivastava"], "emails": ["vinayak.athavale@research.iiit.ac.in,", "vshreenivasbharadwaj@gmail.com", "monik.pamecha@djsce.edu.in,", "ameya.prabhu@research.iiit.ac.in", "m.shrivastava@iiit.ac.in"], "sections": [{"heading": "1 Introduction", "text": "In fact, it is the case that most of us are able to keep to the rules that they have imposed on themselves. (...) It is not the case that they are able to understand the rules. (...) It is not the case that they keep to the rules. (...) It is not the case that they keep to the rules. (...) It is not the case that they keep to the rules. (...) It is not the case that they keep to the rules. (...) It is not the case that they keep to the rules. (...) It is not the case that they keep to the rules. (...) It is not the case that they keep to the rules. (...) It is the case. (...) It is the case. (...) It is. (...) It is the case. (...) It is. (...) It is. (...) It is. (It is. (...) It is. (It is. (...) It is. (It is. (It is.) It is. (It is. (...) It is. (It is. (It is.) It is. (It is. (It is.) It is. (It is. (It is.) It is. (It is. (It is.). (It is. (It is. (It is.). (It is. (It is.). (It is. (It is.). (It is. (It is.). (It is. (It is. (it.). (It is. (It is.). (it. (It is.). (It is. (It is.). (It is."}, {"heading": "2 Related Work", "text": "Previous approaches in NER can be roughly classified into rule-based approaches and learning-based approaches. Rule-based approaches include the system developed by Ralph Grishman in 1995, which uses a large dictionary of named entities (R. Grishman et al., 1995).Another model was created for NER using large lists of names, places, etc., in 1996 (Wakao et al., 1996).A huge drawback of these systems is that a huge list has to be created and the results cannot be determined for any previously unseen entity. They lacked the discovery of new named entities that do not exist in the dictionary, and also cases where the word appeared in the dictionary but was not a named entity. This is an even bigger problem for Indian languages, which would often be agglutinative in nature, hence creating dictionaries called entidels."}, {"heading": "3 Proposed Approach", "text": "Due to the recent success in the Deep Learning Frameworks, we have tried to apply the techniques to Indian long-term data such as Hindi. But the biggest challenge in these approaches is to learn, despite the scarcity of marked data, one of the core problems in adapting deep learning approaches to this area. We propose to use the enormous amount of unlabeled data available in this area. Trained recursive neural networks (RNN) usually have to learn the recurring layer as well as the embedding layer for each word. Usually, the embedding layer needs a large amount of data to create good embedding. We formulate a two-step methodology to use the unlabeled data: In the first step, we use unlabeled corpora. We learn Skip-gram (Mikolov et al al al al al., 2013) embedding and GloVe (Pennington et, 2014) based on these models."}, {"heading": "3.1 Generating Word Embeddings for Hindi", "text": "There are two models introduced by: (Mikolov et al., 2013) CBOW and Skipgram. The latter shows that it works better in English corpses for a variety of tasks, so it is more generalized. So we use the Skip-gram-based approach. The latest method of generating wordvectors was GloVe, which is similar in nature to the Skipgram-based model. It trains embedding with local window context using co-occurrence matrices. The GloVe model is trained on the unequal entries of a global co-occurrence matrix of all words in the corpus, which creates a frequency cooccurence matrix that contains information about how often words come together in a given corpus. The GloVe model is trained on the unequal entries of a global co-occurrence matrix of all words in the corpus."}, {"heading": "3.2 Network Architecture", "text": "The architecture of neural networks is described in Figure 2. We trained deep neural networks, which consist of either one or two recurring layers, as the described dataset was small. In architecture, we have an embedding layer followed by one or two recurring layers, as specified in the experiments, followed by the Softmax layer. We experimented with three different types of recurring layers: Vannilla RNN, LSTM and bidirectional LSTM, to test which layer would be most suitable for the NER task. For embedding the 2http: / / example.com layer, it is initialized with the concatenation of the worm vector and the one hot vector specifying its POS tag. The POS tagging task is generally considered to be a very useful feature for entity detection, so it was a reliable feature. This hypothesis was confirmed as the embedding of POS tags improved by POS% 3."}, {"heading": "4 Experiments", "text": "We conduct extensive experiments to validate our methodology. In this section, we have described in detail the data sets we use and the experimental setup, presented our results and presented a number of observations on these results."}, {"heading": "4.1 Datasets", "text": "We are testing the effectiveness of our approach in the ICON 2013 NLP Tools Contest Dataset for Hindi language courses, along with the cross-validation of our methodology on the established CoNLL 2003 English named entity recognition dataset (Sang et al., 2003)."}, {"heading": "4.1.1 ICON 2013 NLP Tools Contest Dataset", "text": "The data set contains essentially 11 entity types: Organization (ORG), Person (PER), Location (LOC), Entertainment, Facilities, Artifact, Living Beings, Locomotives, Plants, Materials, and Diseases. The rest of the corpus was designated as Non-entities (O). The data set was randomly divided into three areas: Train, Development, and Test in the ratios of 70%, 17%, and 13%. The training set consists of 3,199 sets of 56,801 tokens, the development set contains 707 sets of 12,882 tokens, and the test set contains 571 sets of 10,396 tokens."}, {"heading": "4.1.2 CoNLL 2003 Dataset", "text": "We conduct extensive experiments with the CoNLL 2003 Named Entity Recognition dataset, which consists primarily of a collection of Reuters Newswire articles commented on for NER with four entity types: Person (PER), Location (LOC), Organization (ORG), Miscellaneous (MISC), and Non-Entity Elements marked as (O). Data is provided with a training set of 15,000 sets consisting of approximately 203,000 tokens and a development set of 3466 sets consisting of approximately 51,000 tokens and a test set of 3684 sets consisting of approximately 46,435 tokens. We use the standard evaluation scripts provided along with the dataset to evaluate the performance of our methodology. The scripts use the F1 score to evaluate the performance of the models."}, {"heading": "4.2 Experimental Setup", "text": "We use this architecture for the network due to the limitation of the dataset size due to sparse labeled data. We used an NVIDIA 970 GTX GPU and a 4.00 GHz Intel i7-4790 processor with 64 GB RAM to train our models. As the datasets in this area expand, we want to extend our approach to larger architectures. Results obtained on the ICON 2013 NLP Tools dataset are summarized in Table 2. We cross-validated our approach with the use of the CoNLL 2003 dataset in English. Results are summarized in Table 1. We are able to achieve state-of-the-art accuracy without using additional information such as gazetters, chunks, or the use of non-handcrafted features considered essential for the NER task of chunking."}, {"heading": "4.3 Observations", "text": "The neural networks that did not have word vector-based initializations did not perform well in the NER task as predicted, which can be attributed to the scarcity of data available in the NER task. We also observed that networks that consist of a recurring layer perform equally well or even better than networks with two recurring layers. We believe that this would confirm our hypothesis that increasing the number of parameters can lead to revision. We could see significant performance improvements after using LSTMRNN instead of vanilla RNN, which are due to the ability of LSTMs to model long dependencies. In addition, the bidirectional RNN achieved a significant improvement in accuracy over the others, indicating that the inclusion of word context around (both forwards and backwards) the word is very useful."}, {"heading": "5 Conclusion", "text": "We show that the performance of deep learning-based approaches to entity detection can significantly outperform many other approaches that include rule-based systems or handmade features. Bi-directional LSTM incorporates distances that provide wider context and also alleviate the problem of hands-free speech. Given the small amount of data, our proposed approach effectively utilizes LSTM-based approaches by incorporating pre-3Code, which is available at www.example.com, rather than learning it from data, as it could be learned in an unattended learning environment.4 We could extend this approach to many Indian languages as we do not need a very large annotated corpus. If larger datasets with labels are developed, we would like to explore deeper neural network architectures in the new system and try to learn the neural networks from scratch.4"}], "references": [{"title": "Neural Architectures for Named Entity Recognition", "author": ["Subramanian", "Kazuya Kawakami", "Chris Dyer"], "venue": "Proceedings of NAACL", "citeRegEx": "Subramanian et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Subramanian et al\\.", "year": 2016}, {"title": "Natural Language Processing (almost) from Scratch", "author": ["Collobert", "Jason Weston", "Leon Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Collobert et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Distributed Representations of Words and Phrases and their Compositionality", "author": ["Ilya Sutskever", "Kai Chen", "Greg Corrado", "Jeffrey Dean"], "venue": "Proceedings of Neural Information Processing Systems,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "GloVe: Global Vectors for Word Representation", "author": ["Richard Socher", "Christopher D. Mannin"], "venue": "Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Pennington et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Long Short-Term Memory", "author": ["Jrgen Schmidhuber"], "venue": "Journal Neural Computation archive Volume 9 Issue", "citeRegEx": "Hochreiter and Schmidhuber.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Adam: A Method for Stochastic Optimization. International Conference for Learning Representations", "author": ["Jimmy Ba"], "venue": null, "citeRegEx": "Kingma and Ba.,? \\Q2015\\E", "shortCiteRegEx": "Kingma and Ba.", "year": 2015}, {"title": "Bidirectional Recurrent Neural Networks", "author": ["Kuldip K. Paliwal"], "venue": "IEEE TRANSACTIONS ON SIGNAL PROCESSING,", "citeRegEx": "Schuster and Paliwal.,? \\Q1997\\E", "shortCiteRegEx": "Schuster and Paliwal.", "year": 1997}, {"title": "Introduction to the CoNLL-2003 Shared Task: Language-Independent Named Entity Recognition. CoNLL-2003", "author": ["Fien De Meulder"], "venue": null, "citeRegEx": "Sang and Meulder.,? \\Q2003\\E", "shortCiteRegEx": "Sang and Meulder.", "year": 2003}, {"title": "The NYU system for MUC-6 or Wheres the Syntax", "author": [], "venue": "Sixth Message Understanding Conference", "citeRegEx": "Grishman.,? \\Q1995\\E", "shortCiteRegEx": "Grishman.", "year": 1995}, {"title": "Japanese named entity recognition based on a simple rule generator and decision tree learning. proceedings of the Association for Computational Linguistics, pages 306-313.", "author": [], "venue": null, "citeRegEx": "Isozaki.,? \\Q2001\\E", "shortCiteRegEx": "Isozaki.", "year": 2001}, {"title": "Use of Support Vector Machines in extended named entity recognition. CoNLL-2002", "author": ["Nigel Collier"], "venue": null, "citeRegEx": "Takeuchi and Collier,? \\Q2002\\E", "shortCiteRegEx": "Takeuchi and Collier", "year": 2002}, {"title": "Nymble: a high performance learning name-finder", "author": ["Scott Miller", "Richard Schwartz", "Ralph Weischedel"], "venue": "Fifth conference on Applied natural language processing,", "citeRegEx": "Bikel et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Bikel et al\\.", "year": 1997}, {"title": "Phrase clustering for discriminative learning", "author": ["Lin et al.2009] Dekang Lin", "Xiaoyun Wu"], "venue": "Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing", "citeRegEx": "Lin and Wu.,? \\Q2009\\E", "shortCiteRegEx": "Lin and Wu.", "year": 2009}, {"title": "Joint named entity recognition and disambiguation", "author": ["Luo et al.2015] Gang Luo", "Xiaojiang Huang", "ChinYew Lin", "Zaiqing Nie"], "venue": "Proc. EMNLP,", "citeRegEx": "Luo et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Luo et al\\.", "year": 2015}, {"title": "Dropout: A Simple Way to Prevent Neural Networks from Overfitting", "author": ["Geoffrey Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Srivastava et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 2014}, {"title": "SSF: Shakti Standard Format Guide", "author": ["Rajeev Sangal", "Dipti Misra Sharma"], "venue": null, "citeRegEx": "Bharati et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Bharati et al\\.", "year": 2009}, {"title": "Named Entity Recognizer for Indian Languages", "author": ["S Malarkodi C", "K Marimuthu"], "venue": null, "citeRegEx": "Lalitha et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Lalitha et al\\.", "year": 2013}, {"title": "A Hybrid Approach for Named Entity and Sub-Type Tagging", "author": ["R. Srihari et al.2000] Srihari", "C. Niu", "W. Li"], "venue": "Proceedings of the sixth conference on applied natural language processing,", "citeRegEx": "Srihari et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Srihari et al\\.", "year": 2000}, {"title": "On the importance of initialization and momentum in deep learning", "author": ["James Martens", "George Dahl", "Geoffrey Hinton"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Sutskever et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2013}, {"title": "Learning Long-Term Dependencies with Gradient Descent is difficult", "author": ["Bengio et al.1994] Yoshua Bengio", "Patrice Simard", "Paolo Frasconi"], "venue": "IEEE Transactions on Neural Networks", "citeRegEx": "Bengio et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 1994}], "referenceMentions": [{"referenceID": 11, "context": "Markov Models (HMM)(Bikel et al., 1997), Conditional Random Field (CRF) (Das et al.", "startOffset": 19, "endOffset": 39}, {"referenceID": 17, "context": "An example of this is (Srihari et al., 2000) who use a combination of both handcrafted rules along with HMM and ME.", "startOffset": 22, "endOffset": 44}, {"referenceID": 1, "context": "Also, there are many approaches which combine NER with other tasks like (Collobert et al., 2011) (POS Tagging and NER along with Chunking and SRL tasks) and (Luo et al.", "startOffset": 72, "endOffset": 96}, {"referenceID": 13, "context": ", 2011) (POS Tagging and NER along with Chunking and SRL tasks) and (Luo et al., 2015) (combining Entity Linking and NER) which have produced state-ofthe-art results on English datasets.", "startOffset": 68, "endOffset": 86}, {"referenceID": 2, "context": "We learn Skip-gram (Mikolov et al., 2013) based embeddings and GloVe (Pennington et al.", "startOffset": 19, "endOffset": 41}, {"referenceID": 3, "context": ", 2013) based embeddings and GloVe (Pennington et al., 2014) embeddings on those corpora.", "startOffset": 35, "endOffset": 60}, {"referenceID": 18, "context": "As various approaches have proved, a good initialization is crucial to learning good models and train faster (Sutskever et al., 2013).", "startOffset": 109, "endOffset": 133}, {"referenceID": 19, "context": "We know that Vanilla RNN\u2019s suffer from not being able to model long term dependencies (Bengio et al., 1994) Hence we use the LSTM variant of the RNN (Hochreiter et al.", "startOffset": 86, "endOffset": 107}, {"referenceID": 2, "context": "There are two models introduced by: (Mikolov et al., 2013)", "startOffset": 36, "endOffset": 58}, {"referenceID": 14, "context": "We have used Dropout training (Srivastava et al., 2014) to reduce overfitting in our models and help in combining the predictions of the Bi-Directional LSTM.", "startOffset": 30, "endOffset": 55}], "year": 2017, "abstractText": "In this paper we describe an end to end Neural Model for Named Entity Recognition (NER) which is based on BiDirectional RNN-LSTM\u2019s. Almost all NER systems for Hindi use Language Specific features and handcrafted rules with gazetteers. Our model is language independent and uses no domain specific features or any handcrafted rules. Our models rely on semantic information in the form of word vectors which are learnt by an unsupervised learning algorithm on an unannotated corpus. Our model attained state of the art performance in both English and Hindi which is a morphologically rich language without the use of any morphological analysis or without using gazetteers of any sort.", "creator": "LaTeX with hyperref package"}}}