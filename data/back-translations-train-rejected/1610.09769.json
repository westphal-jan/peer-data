{"id": "1610.09769", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-Oct-2016", "title": "Meta-Path Guided Embedding for Similarity Search in Large-Scale Heterogeneous Information Networks", "abstract": "Most real-world data can be modeled as heterogeneous information networks (HINs) consisting of vertices of multiple types and their relationships. Search for similar vertices of the same type in large HINs, such as bibliographic networks and business-review networks, is a fundamental problem with broad applications. Although similarity search in HINs has been studied previously, most existing approaches neither explore rich semantic information embedded in the network structures nor take user's preference as a guidance.", "histories": [["v1", "Mon, 31 Oct 2016 03:15:02 GMT  (1964kb,D)", "http://arxiv.org/abs/1610.09769v1", null]], "reviews": [], "SUBJECTS": "cs.SI cs.LG", "authors": ["jingbo shang", "meng qu", "jialu liu", "lance m kaplan", "jiawei han", "jian peng"], "accepted": false, "id": "1610.09769"}, "pdf": {"name": "1610.09769.pdf", "metadata": {"source": "CRF", "title": "Meta-Path Guided Embedding for Similarity Search in Large-Scale Heterogeneous Information Networks", "authors": ["Jingbo Shang", "Meng Qu", "Jialu Liu", "Lance M. Kaplan", "Jiawei Han", "Jian Peng"], "emails": ["jianpeng}@illinois.edu", "2jialu@google.com", "3lance.m.kaplan.civ@mail.mil"], "sections": [{"heading": null, "text": "To accommodate user preferences in defining similarity semantics, our proposed framework, ESim, accepts custom meta paths as guidance to learn vertex vectors in a user-preferred embedding space. In addition, an efficient and parallel sample-based optimization algorithm has been developed to learn embedding in large-scale HINs. Extensive experiments on large-scale HINs of the real world show a marked improvement in ESim's effectiveness over several state-of-the-art algorithms and its scalability."}, {"heading": "1. INTRODUCTION", "text": "In fact, it is the case that most of them are able to survive themselves without there being a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is not in which there is a process in which there is a process in which there is a process in which there is a process in which there is not in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is not in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process"}, {"heading": "2. RELATED WORK", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Meta-Path Guided Similarity Search", "text": "PathSim [26] defines the similarity between two vertices of the same type by the normalized number of path instances following a user-defined metapath between any pair of vertices. [26] shows that the PathSim measurement captures better peer similarity semantics than random, run-based similarity measurements such as P-PageRank [12] and SimRank [11]. Furthermore, [27] shows that user guidance can be converted into a weighted combination of metapaths. However, PathSim does not investigate the similarity embedded in the structure of a HIN. Moreover, PathSim does not have the vectors of vertices, which can make further analysis more efficient, such as clusters."}, {"heading": "2.2 Embedding-based Similarity Search", "text": "Recently, the embedding technique, which aims to learn low-dimensional vector representations for entities while maintaining environments, has received increasing attention due to its great performance in many different types of tasks. As a specific and concrete scenario, the embedding of homogeneous networks containing deepenings of the same type has recently been investigated. LINE [29] and DeepWalk [20] use the network connection information to construct latent vectors for classifying entities and linkage predictions. DCA [4] starts with the personalized PageRank, but performs further decomposition to obtain better protein-protein interaction predictions in biological networks. However, these homogeneous models may not capture information about entity types and about relationships between different typical entities in HINs. There are also embedding techniques developed for HINs. [For example, text can be used simultaneously to suggest chalet for both deep neural use]."}, {"heading": "2.3 Similarity Search in Vector Spaces", "text": "Various similarity metrics have been developed for vector spaces in various tasks, such as cosine similarity [24], Jaccard coefficient [14] and p-norm distance [5]. In our work, we use cosine similarity directly based on embedding vectors of vertices. Much effort has been made to optimize the efficiency of top-k search for the nearest neighbor [33, 13, 9, 18, 2]. We adopt these existing efficient similarity search techniques to support online queries."}, {"heading": "3. PRELIMINARIES", "text": "Definition 1. Definition 1. Heterogeneous information network is an information network in which both vertices and edges are associated with different types. < M is the set of edge types in the network, and E \u2264 \u2212 P is the edge. An edge in a heterogeneous information network is an ordered triplet instance e = < u, r > Vertex has its own type, where u and v have two typed vertices with this edge and r the edge type.In a general heterogeneous information network, there could be several typed edges between the same two vertices and the edge. Of course, the definition above supports all of these cases. To explain it better, we use a bibliographic network as an example."}, {"heading": "4. METHODOLOGY", "text": "In this section, we formulate a probabilistic embedding model to incorporate meta paths, inspired by many previous studies. We propose an efficient and scalable optimization algorithm based on the sample of path instances following the given meta path. Our proposed efficient sampling methods are the most important steps and will therefore be discussed separately."}, {"heading": "4.1 Probabilistic Embedding Model Incorporating Meta-Paths", "text": "The fundamental idea of our approach is to obtain the HIN structure in the learned embeddings, so that we assume that the depressions that occur in many areas also have greater significance in other areas (such as those in which they move). (F, V, M, M). (F, V, M, M). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D. (D). (D). (D. (D). (D). (D. (D). (D). (D. (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D. (D). (D). (D). (D). (D). (D). (D). (D. (D. (D). (D. (D). (D). (D. (D). (D). (D). (D. (D). (D). (D). (D). (D. (D). (D). (D). (D). (D. (D). (D). (D). (D). (D). (D). (D). (D. (D). (D). (D). ("}, {"heading": "4.2 Optimization Algorithm", "text": "The number of vertex pairs < u, v > connected by some path instances that follow at least one of the user-specified meta paths can be O (V | 2) at worst, which is too large to store or process when it is at the level of millions and even billions, making it impossible to handle the projected networks directly via the meta paths. Therefore, capturing a subset of path instances according to their distribution is the best and most practical choice when going through each path instance per iteration. Even if the network itself contains a large number of edges, our method is still very efficient. The details of our training framework are shown in Algorithm 1. Once a path instance follows the meta path M, the gradient method was used to update the parameters, xv, pM, qM and MikroM one by one."}, {"heading": "5. EXPERIMENTS", "text": "In this section, we evaluate our proposed ESim model by comparing it quantitatively and qualitatively with several state-of-the-art methods on two large real-time HINs and evaluating the sensitivity of parameters and the efficiency of ESim."}, {"heading": "5.1 Datasets", "text": "The first dataset, DBLP, is a bibliographic network in computer science that includes essays (P \u2212 A), authors (A), venues (V), and terms (T) as four types of hollows, and considers P as the center in a star network. There are 3 types of undirected edges: P \u2212 A, P \u2212 V, and P \u2212 T. The interesting metapaths that users can specify are the co-authorship metapath A \u2212 P \u2212 A, the metapath A \u2212 P \u2212 A, and the common term metapath A \u2212 P \u2212 A. We have the following two groups that are labeled by human experts. \u2022 Research field YYP, which are grouped into the four areas labeled by human experts in the DBLP that are used in the evaluation of PathSim."}, {"heading": "5.2 Experimental Setting", "text": "We select different metaphor paths for different datasets to see how the metaphor paths reflect any pair of selected parity and affect performance. In addition, we perform grid searches against different groupings to get the best weights of different metaphor paths for ESim models, compared to algorithms and notations. We select the previous state-of-the-art metaphor guided similarity search algorithm problem, PathSim, which has been reported to beat many other similarity search methods, for example SimRank [11], P-PageRank [10], random walk, and paired random walk. In addition, we also consider (heterogeneous) methods such as LINE and PTE, which beat other embedding methods such as graph factorization [1] and DeepWalk [20]. Further details about these methods are as follows \u2022 PathSim [26] a metaphor guided search by application options."}, {"heading": "5.3 AUC Evaluations", "text": "Although it is difficult to obtain the names of the detailed rankings of all pairs of vertices, it is relatively easy to make an estimate based on the names of vertex groups l (\u00b7). Given the ranking problem for each individual vertex u, if we evaluate the other vertices based on the similarity values, it is very natural that we expect the vertices from the same group (similar) to be at the top of the leaderboard, while the unequal vertices are at the bottom of the list. Specifically, we define the AUC score as follows. For a better measure of comparison, the AUC score from the same group (similar) should be at the top of the leaderboard."}, {"heading": "5.4 Visualizations", "text": "In fact, it is so that most of them are able to survive themselves if they do not see themselves able to unfold. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are not able to survive themselves. (...)"}, {"heading": "5.5 Parameter Sensitivity", "text": "We select the best performing ESim-pair models to investigate parameter sensitivity, such as the use of A \u2212 P \u2212 V \u2212 P \u2212 A in the Research Area group, the use of A \u2212 P \u2212 A in the Research Group and the use of B \u2212 R \u2212 W \u2212 R \u2212 B. We vary the parameter values and see how the performance of the AUC changes. Based on the curves in Fig. 2 (a), we can conclude that determining the dimension (d) of vertex embedding vectors such as 50 is reasonable because too small d cannot sufficiently capture semantics, while too large d can lead to a revision problem. Fig. 2 (b) shows that the AUC values are not sensitive to the negative sample ratio K and K = 5. As shown in Fig. 2 (c), as more samples are optimized during training, the AUC values maintain an increasing trend and eventually converge."}, {"heading": "5.6 Scalability and Parallelization", "text": "We study the efficiency of ESim by considering both scalability and parallelization, as shown in Figure 3 (a). We try different proportions of network size (i.e. | V | + | E |) in the two networks and execute our most powerful models, i.e. ESim pairs using A \u2212 P \u2212 V \u2212 P \u2212 A and A \u2212 P \u2212 A on the DBLP dataset and ESim pairs using B \u2212 R \u2212 W \u2212 R \u2212 B on the Yelp dataset. Based on these curves, runtime is linear to the size of networks, while the longer metapath takes slightly more time, which is in line with our previous theoretical time complexity analysis. We vary the number of working cores and execute our models on the DBLP and Yelp datasets. The results are plotted in Figure 3 (b)."}, {"heading": "6. CONCLUSIONS", "text": "Our proposed model, ESim, integrates predefined metapaths and network structures to learn vertex embedding vectors. The similarity, defined by the cosine similarity between vertex embedding vectors of the same type, has proven its effectiveness and surpasses previous state-of-the-art algorithms on two large-scale vertex embedding vectors. ESim's efficiency has also been evaluated and proved scalable. There are several directions to expand this work. Firstly, instead of similarities between vertices of the same type, the relevance between vertices of different types can be studied. Secondly, a mechanism could be developed to automatically learn and extract interesting metapaths or their weighted combinations from user-defined rankings or preferences."}, {"heading": "7. REFERENCES", "text": "[1] A. Ahmed, N. Shervashidze, S. Narayanamurthy, V. Josifovski, and A. J. Smola. Distributed large-scale natural graph factorization. In WWW, pp. 37-48, Republic and Canton of Geneva, Switzerland, 2013. International World Wide Web Conferences Steering Committee. [2] A. Andoni and P. Indyk."}], "references": [{"title": "Distributed large-scale natural graph factorization", "author": ["A. Ahmed", "N. Shervashidze", "S. Narayanamurthy", "V. Josifovski", "A.J. Smola"], "venue": "WWW, pages 37\u201348, Republic and Canton of Geneva, Switzerland", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2013}, {"title": "Near-optimal hashing algorithms for approximate nearest neighbor in high dimensions", "author": ["A. Andoni", "P. Indyk"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2008}, {"title": "Heterogeneous network embedding via deep architectures", "author": ["S. Chang", "W. Han", "J. Tang", "G.-J. Qi", "C.C. Aggarwal", "T.S. Huang"], "venue": "SIGKDD, pages 119\u2013128, New York, NY, USA", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2015}, {"title": "Diffusion component analysis: Unraveling functional topology in biological networks", "author": ["H. Cho", "B. Berger", "J. Peng"], "venue": "Research in Computational Molecular Biology, pages 62\u201364. Springer", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2015}, {"title": "Theory of Hp spaces", "author": ["P.L. Duren"], "venue": "volume 38. IMA", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1970}, {"title": "Traversing knowledge graphs in vector space", "author": ["K. Gu", "J. Miller", "P. Liang"], "venue": "EMNLP 2015", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2015}, {"title": "Noise-contrastive estimation of unnormalized statistical models", "author": ["M.U. Gutmann", "A. Hyv\u00e4rinen"], "venue": "with applications to natural image statistics. JMLR, 13(1):307\u2013361", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2012}, {"title": "Network lasso: Clustering and optimization in large graphs", "author": ["D. Hallac", "J. Leskovec", "S. Boyd"], "venue": "SIGKDD, pages 387\u2013396, New York, NY, USA", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "Approximate nearest neighbors: Towards removing the curse of dimensionality", "author": ["P. Indyk", "R. Motwani"], "venue": "STOC, pages 604\u2013613, New York, NY, USA", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1998}, {"title": "When the web meets the cell: using personalized pagerank for analyzing protein interaction networks", "author": ["G. Iv\u00e1n", "V. Grolmusz"], "venue": "Bioinformatics, 27(3):405\u2013407", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2011}, {"title": "Simrank: A measure of structural-context similarity", "author": ["G. Jeh", "J. Widom"], "venue": "SIGKDD, pages 538\u2013543, New York, NY, USA", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2002}, {"title": "Scaling personalized web search", "author": ["G. Jeh", "J. Widom"], "venue": "WWW, pages 271\u2013279, New York, NY, USA", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2003}, {"title": "The sr-tree: An index structure for high-dimensional nearest neighbor queries", "author": ["N. Katayama", "S. Satoh"], "venue": "SIGMOD, pages 369\u2013380, New York, NY, USA", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1997}, {"title": "Distance between sets", "author": ["M. Levandowsky", "D. Winter"], "venue": "Nature, 234(5323):34\u201335", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1971}, {"title": "Discovering meta-paths in large heterogeneous information networks", "author": ["C. Meng", "R. Cheng", "S. Maniu", "P. Senellart", "W. Zhang"], "venue": "WWW, pages 754\u2013764. International World Wide Web Conferences Steering Committee", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2015}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["T. Mikolov", "I. Sutskever", "K. Chen", "G.S. Corrado", "J. Dean"], "venue": "NIPS, pages 3111\u20133119", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2013}, {"title": "A fast and simple algorithm  for training neural probabilistic language models", "author": ["A. Mnih", "Y.W. Teh"], "venue": "ICML", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2012}, {"title": "Fast approximate nearest neighbors with automatic algorithm configuration", "author": ["M. Muja", "D.G. Lowe"], "venue": "VISAPP (1), 2", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2009}, {"title": "Glove: Global vectors for word representation", "author": ["J. Pennington", "R. Socher", "C.D. Manning"], "venue": "EMNLP, volume 14, pages 1532\u20131543", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2014}, {"title": "Deepwalk: Online learning of social representations", "author": ["B. Perozzi", "R. Al-Rfou", "S. Skiena"], "venue": "SIGKDD, pages 701\u2013710, New York, NY, USA", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2014}, {"title": "Hogwild: A lock-free approach to parallelizing stochastic gradient descent", "author": ["B. Recht", "C. Re", "S. Wright", "F. Niu"], "venue": "NIPS, pages 693\u2013701", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2011}, {"title": "Measuring word significance using distributed representations of words", "author": ["A.M. Schakel", "B.J. Wilson"], "venue": "arXiv preprint arXiv:1508.02297", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2015}, {"title": "A parallel and efficient algorithm for learning to match", "author": ["J. Shang", "T. Chen", "H. Li", "Z. Lu", "Y. Yu"], "venue": "ICDM, pages 971\u2013976. IEEE", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2014}, {"title": "Modern information retrieval: A brief overview", "author": ["A. Singhal"], "venue": "IEEE Data Eng. Bull., 24(4):35\u201343", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2001}, {"title": "Mining heterogeneous information networks: A structural analysis approach", "author": ["Y. Sun", "J. Han"], "venue": "SIGKDD Explor. Newsl.,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2013}, {"title": "Pathsim: Meta path-based top-k similarity search in heterogeneous information networks", "author": ["Y. Sun", "J. Han", "X. Yan", "P.S. Yu", "T. Wu"], "venue": "VLDB 2011", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2011}, {"title": "Integrating meta-path selection with user-guided object clustering in heterogeneous information networks", "author": ["Y. Sun", "B. Norick", "J. Han", "X. Yan", "P.S. Yu", "X. Yu"], "venue": "SIGKDD, pages 1348\u20131356, New York, NY, USA", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2012}, {"title": "Pte: Predictive text embedding through large-scale heterogeneous text networks", "author": ["J. Tang", "M. Qu", "Q. Mei"], "venue": "SIGKDD, pages 1165\u20131174. ACM", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2015}, {"title": "Line: Large-scale information network embedding", "author": ["J. Tang", "M. Qu", "M. Wang", "M. Zhang", "J. Yan", "Q. Mei"], "venue": "WWW, pages 1067\u20131077, Republic and Canton of Geneva, Switzerland", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2015}, {"title": "Visualizing data using t-sne", "author": ["L. Van der Maaten", "G. Hinton"], "venue": "JMLR, 9(2579-2605):85,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2008}, {"title": "An efficient method for generating discrete random variables with general distributions", "author": ["A.J. Walker"], "venue": "ACM Trans. Math. Softw.,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 1977}, {"title": "Representation learning of knowledge graphs with entity descriptions", "author": ["R. Xie", "Z. Liu", "J. Jia", "H. Luan", "M. Sun"], "venue": "AAAI", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2016}, {"title": "Data structures and algorithms for nearest neighbor search in general metric spaces", "author": ["P.N. Yianilos"], "venue": "SODA, pages 311\u2013321, Philadelphia, PA, USA", "citeRegEx": "33", "shortCiteRegEx": null, "year": 1993}, {"title": "User guided entity similarity search using meta-path selection in heterogeneous information networks", "author": ["X. Yu", "Y. Sun", "B. Norick", "T. Mao", "J. Han"], "venue": "CIKM, pages 2025\u20132029, New York, NY, USA", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2012}], "referenceMentions": [{"referenceID": 25, "context": "Modeling data in the real world as heterogeneous information networks (HINs) can capture rich data semantics and facilitate various applications [26, 11, 8, 29, 23, 25].", "startOffset": 145, "endOffset": 168}, {"referenceID": 10, "context": "Modeling data in the real world as heterogeneous information networks (HINs) can capture rich data semantics and facilitate various applications [26, 11, 8, 29, 23, 25].", "startOffset": 145, "endOffset": 168}, {"referenceID": 7, "context": "Modeling data in the real world as heterogeneous information networks (HINs) can capture rich data semantics and facilitate various applications [26, 11, 8, 29, 23, 25].", "startOffset": 145, "endOffset": 168}, {"referenceID": 28, "context": "Modeling data in the real world as heterogeneous information networks (HINs) can capture rich data semantics and facilitate various applications [26, 11, 8, 29, 23, 25].", "startOffset": 145, "endOffset": 168}, {"referenceID": 22, "context": "Modeling data in the real world as heterogeneous information networks (HINs) can capture rich data semantics and facilitate various applications [26, 11, 8, 29, 23, 25].", "startOffset": 145, "endOffset": 168}, {"referenceID": 24, "context": "Modeling data in the real world as heterogeneous information networks (HINs) can capture rich data semantics and facilitate various applications [26, 11, 8, 29, 23, 25].", "startOffset": 145, "endOffset": 168}, {"referenceID": 25, "context": ", [26, 25, 29, 28]).", "startOffset": 2, "endOffset": 18}, {"referenceID": 24, "context": ", [26, 25, 29, 28]).", "startOffset": 2, "endOffset": 18}, {"referenceID": 28, "context": ", [26, 25, 29, 28]).", "startOffset": 2, "endOffset": 18}, {"referenceID": 27, "context": ", [26, 25, 29, 28]).", "startOffset": 2, "endOffset": 18}, {"referenceID": 25, "context": "PathSim [26] proposes to use meta-paths to define and guide similarity search in HIN, with good success.", "startOffset": 8, "endOffset": 12}, {"referenceID": 28, "context": "Along another line of study, network-embedding techniques have been recently explored for homogeneous information networks, which treat all vertices and edges as of the same type, represented by LINE [29].", "startOffset": 200, "endOffset": 204}, {"referenceID": 27, "context": "An alternative and better way is to first project the HIN to several bipartite (assuming user-given meta-paths are symmetric) or homogeneous networks and then apply the edge-wise HIN embedding technique, such as PTE [28].", "startOffset": 216, "endOffset": 220}, {"referenceID": 24, "context": ", venues and papers) [25] and leads to a rather densely connected network (since many authors may publish in many venues).", "startOffset": 21, "endOffset": 25}, {"referenceID": 25, "context": "PathSim [26] defines the similarity between two vertices of the same type by the normalized count of path instances following a user-specified meta-path between any pair of vertices.", "startOffset": 8, "endOffset": 12}, {"referenceID": 25, "context": "[26] shows that the PathSim measure captures better peer similarity semantics than random walk-based similarity measures, such as P-PageRank [12] and SimRank [11].", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[26] shows that the PathSim measure captures better peer similarity semantics than random walk-based similarity measures, such as P-PageRank [12] and SimRank [11].", "startOffset": 141, "endOffset": 145}, {"referenceID": 10, "context": "[26] shows that the PathSim measure captures better peer similarity semantics than random walk-based similarity measures, such as P-PageRank [12] and SimRank [11].", "startOffset": 158, "endOffset": 162}, {"referenceID": 26, "context": "Moreover, [27] shows that user guidance can be transformed to a weighted combination of meta-paths.", "startOffset": 10, "endOffset": 14}, {"referenceID": 28, "context": "LINE [29] and DeepWalk [20] utilize the network link information to construct latent vectors for vertex classification and link prediction.", "startOffset": 5, "endOffset": 9}, {"referenceID": 19, "context": "LINE [29] and DeepWalk [20] utilize the network link information to construct latent vectors for vertex classification and link prediction.", "startOffset": 23, "endOffset": 27}, {"referenceID": 3, "context": "DCA [4] starts from the personalized PageRank but does further decomposition to get better protein-protein interaction predictions in biology networks.", "startOffset": 4, "endOffset": 7}, {"referenceID": 2, "context": "propose to incorporate deep neural networks to train embedding vectors for both text and images at the same time [3].", "startOffset": 113, "endOffset": 116}, {"referenceID": 27, "context": "Under a supervised setting, PTE [28] utilizes labels of words and constructs bipartite HINs to learn predictive embedding vectors for words.", "startOffset": 32, "endOffset": 36}, {"referenceID": 5, "context": "Embedding techniques have been also applied to knowledge graphs to resolve question-answering tasks [6] and retain knowledge relations between entities [32].", "startOffset": 100, "endOffset": 103}, {"referenceID": 31, "context": "Embedding techniques have been also applied to knowledge graphs to resolve question-answering tasks [6] and retain knowledge relations between entities [32].", "startOffset": 152, "endOffset": 156}, {"referenceID": 23, "context": "Various similarity metrics have been designed for vector spaces in different tasks, such as cosine similarity [24], Jaccard coefficient [14], and the p-norm distance [5].", "startOffset": 110, "endOffset": 114}, {"referenceID": 13, "context": "Various similarity metrics have been designed for vector spaces in different tasks, such as cosine similarity [24], Jaccard coefficient [14], and the p-norm distance [5].", "startOffset": 136, "endOffset": 140}, {"referenceID": 4, "context": "Various similarity metrics have been designed for vector spaces in different tasks, such as cosine similarity [24], Jaccard coefficient [14], and the p-norm distance [5].", "startOffset": 166, "endOffset": 169}, {"referenceID": 32, "context": "Many efforts have been paid on optimizing the efficiency of top-k nearest neighbor search [33, 13, 9, 18, 2].", "startOffset": 90, "endOffset": 108}, {"referenceID": 12, "context": "Many efforts have been paid on optimizing the efficiency of top-k nearest neighbor search [33, 13, 9, 18, 2].", "startOffset": 90, "endOffset": 108}, {"referenceID": 8, "context": "Many efforts have been paid on optimizing the efficiency of top-k nearest neighbor search [33, 13, 9, 18, 2].", "startOffset": 90, "endOffset": 108}, {"referenceID": 17, "context": "Many efforts have been paid on optimizing the efficiency of top-k nearest neighbor search [33, 13, 9, 18, 2].", "startOffset": 90, "endOffset": 108}, {"referenceID": 1, "context": "Many efforts have been paid on optimizing the efficiency of top-k nearest neighbor search [33, 13, 9, 18, 2].", "startOffset": 90, "endOffset": 108}, {"referenceID": 33, "context": "Such a user-guided approach without adopting the embedding framework has been studied in [34, 27, 15].", "startOffset": 89, "endOffset": 101}, {"referenceID": 26, "context": "Such a user-guided approach without adopting the embedding framework has been studied in [34, 27, 15].", "startOffset": 89, "endOffset": 101}, {"referenceID": 14, "context": "Such a user-guided approach without adopting the embedding framework has been studied in [34, 27, 15].", "startOffset": 89, "endOffset": 101}, {"referenceID": 18, "context": "In particular, we encode the meta-path through the following formulation inspired from [19, 23]:", "startOffset": 87, "endOffset": 95}, {"referenceID": 22, "context": "In particular, we encode the meta-path through the following formulation inspired from [19, 23]:", "startOffset": 87, "endOffset": 95}, {"referenceID": 15, "context": "\u03b3 is a widely used parameter to control the effect of overly-popular vertices, which is usually 3/4 inspired from [16].", "startOffset": 114, "endOffset": 118}, {"referenceID": 6, "context": "Since learning rich vertex embeddings does not require the accurate probability of each path instance, we adopt the NCE for optimization, which has been proved to significantly accelerate the training without cutting down the embedding qualities [7, 17].", "startOffset": 246, "endOffset": 253}, {"referenceID": 16, "context": "Since learning rich vertex embeddings does not require the accurate probability of each path instance, we adopt the NCE for optimization, which has been proved to significantly accelerate the training without cutting down the embedding qualities [7, 17].", "startOffset": 246, "endOffset": 253}, {"referenceID": 6, "context": "We can do this because the NCE objective encourages the model to be approximately normalized and recovers a perfectly normalized model if the model class contains the data distribution [7, 17].", "startOffset": 185, "endOffset": 192}, {"referenceID": 16, "context": "We can do this because the NCE objective encourages the model to be approximately normalized and recovers a perfectly normalized model if the model class contains the data distribution [7, 17].", "startOffset": 185, "endOffset": 192}, {"referenceID": 15, "context": "The above expectation is also studied in [16], which replaces \u2206e1 eL|M with \u2211L i=1 f(ui, vi,M) for ease of computation and names the method negative sampling.", "startOffset": 41, "endOffset": 45}, {"referenceID": 21, "context": "(1) because the norm of vectors xu and xv do not help the similarity search task [22].", "startOffset": 81, "endOffset": 85}, {"referenceID": 17, "context": "Moreover, cosine similarity is equivalent to Euclidean distance when ||xu|| = ||xv|| = 1, which makes the top-k similar vertices of the given vertex u able to be efficiently solved using approximate nearest neighbors search [18] after normalizations.", "startOffset": 224, "endOffset": 228}, {"referenceID": 20, "context": "For example, Hogwild [21] provides a general and lock-free strategy for fully parallelizing any stochastic gradient descent algorithms in a shared memory.", "startOffset": 21, "endOffset": 25}, {"referenceID": 26, "context": "Such user-guided meta-path generation has been studied in [27] without considering embedding.", "startOffset": 58, "endOffset": 62}, {"referenceID": 30, "context": "We have adopted the alias method [31] to make the online sampling from any discrete distribution O(1).", "startOffset": 33, "endOffset": 37}, {"referenceID": 20, "context": "It has been proved that stochastic gradient descent can be fully parallelized without locks [21].", "startOffset": 92, "endOffset": 96}, {"referenceID": 25, "context": "We use the 4-area grouping in DBLP labeled by human experts, which was used when evaluating PathSim [26].", "startOffset": 100, "endOffset": 104}, {"referenceID": 10, "context": "We select the previous state-of-the-art algorithm in the meta-path guided similarity search problem, PathSim, which has been reported to beat many other simiarity search methods, for example, SimRank [11], P-PageRank [10], random walk, and pairwise random walk.", "startOffset": 200, "endOffset": 204}, {"referenceID": 9, "context": "We select the previous state-of-the-art algorithm in the meta-path guided similarity search problem, PathSim, which has been reported to beat many other simiarity search methods, for example, SimRank [11], P-PageRank [10], random walk, and pairwise random walk.", "startOffset": 217, "endOffset": 221}, {"referenceID": 0, "context": "In addition, we also consider (heterogeneous) network embedding methods, such as LINE and PTE, which beat other embedding methods like graph factorization [1] and DeepWalk [20].", "startOffset": 155, "endOffset": 158}, {"referenceID": 19, "context": "In addition, we also consider (heterogeneous) network embedding methods, such as LINE and PTE, which beat other embedding methods like graph factorization [1] and DeepWalk [20].", "startOffset": 172, "endOffset": 176}, {"referenceID": 25, "context": "\u2022 PathSim [26] is a meta-path guided similarity search algorithm which utilizes the normalized count of path instances following the user selected meta-path between any pair of vertices.", "startOffset": 10, "endOffset": 14}, {"referenceID": 28, "context": "\u2022 LINE [29] is an embedding algorithm specifically designed for homogeneous networks, which considers both first and second order information in a network (i.", "startOffset": 7, "endOffset": 11}, {"referenceID": 27, "context": "\u2022 PTE [28] decomposes a HIN to a set of edgewise bipartite networks and then learn embedding vectors.", "startOffset": 6, "endOffset": 10}, {"referenceID": 15, "context": "The parameter \u03b3 controlling the effect of overly-popular vertices is set to 3/4 inspired from [16].", "startOffset": 94, "endOffset": 98}, {"referenceID": 29, "context": "Their embedding vectors are projected to a 2-D space using the t-SNE package [30], which is a nonlinear dimensionality reduction technique and well suited for projecting high-dimensional data into a low dimensional space.", "startOffset": 77, "endOffset": 81}], "year": 2016, "abstractText": "Most real-world data can be modeled as heterogeneous information networks (HINs) consisting of vertices of multiple types and their relationships. Search for similar vertices of the same type in large HINs, such as bibliographic networks and business-review networks, is a fundamental problem with broad applications. Although similarity search in HINs has been studied previously, most existing approaches neither explore rich semantic information embedded in the network structures nor take user\u2019s preference as a guidance. In this paper, we re-examine similarity search in HINs and propose a novel embedding-based framework. It models vertices as low-dimensional vectors to explore network structureembedded similarity. To accommodate user preferences at defining similarity semantics, our proposed framework, ESim, accepts user-defined meta-paths as guidance to learn vertex vectors in a user-preferred embedding space. Moreover, an efficient and parallel sampling-based optimization algorithm has been developed to learn embeddings in large-scale HINs. Extensive experiments on real-world large-scale HINs demonstrate a significant improvement on the effectiveness of ESim over several state-of-the-art algorithms as well as its scalability.", "creator": "LaTeX with hyperref package"}}}