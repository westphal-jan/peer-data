{"id": "1610.09778", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-Oct-2016", "title": "DPPred: An Effective Prediction Framework with Concise Discriminative Patterns", "abstract": "In the literature, two series of models have been proposed to address prediction problems including classification and regression. Simple models, such as generalized linear models, have ordinary performance but strong interpretability on a set of simple features. The other series, including tree-based models, organize numerical, categorical and high dimensional features into a comprehensive structure with rich interpretable information in the data.", "histories": [["v1", "Mon, 31 Oct 2016 03:43:04 GMT  (7559kb,D)", "http://arxiv.org/abs/1610.09778v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.AI", "authors": ["jingbo shang", "meng jiang", "wenzhu tong", "jinfeng xiao", "jian peng", "jiawei han"], "accepted": false, "id": "1610.09778"}, "pdf": {"name": "1610.09778.pdf", "metadata": {"source": "CRF", "title": "DPPred: An Effective Prediction Framework with Concise Discriminative Patterns", "authors": ["Jingbo Shang", "Meng Jiang", "Wenzhu Tong", "Jinfeng Xiao", "Jian Peng", "Jiawei Han"], "emails": [], "sections": [{"heading": null, "text": "Index Terms - Discrimination Patterns, Generalized Linear Model, Tree Models, Classification, RegressionF"}, {"heading": "1 INTRODUCTION", "text": "In fact, the fact is that most of them are able to survive themselves, and that they are able to survive themselves, \"he told the Deutsche Presse-Agentur in an interview with the\" S\u00fcddeutsche Zeitung \"(Saturday).\" I don't think I'm able to survive myself, \"he told the Deutsche Presse-Agentur in Berlin.\" I believe that I'm able to survive myself, and that I'm not able to survive myself, \"he said in an interview with the\" S\u00fcddeutsche Zeitung \"(Saturday).\" I don't think I'm able to survive myself. \""}, {"heading": "2 RELATED WORK", "text": "In this section, we review existing methods related to DPPRED, including pattern-based classification models, tree-based models, and approaches to pattern selection."}, {"heading": "2.1 Pattern-based Classification", "text": "Li et al. proposed a classification method CMAR based on multiple class association rules [22]. Yin et al. extended it to CPAR based on predictive association rules [34]. In addition to the association rules, direct discriminatory pattern mining was also proposed to achieve effective performance [3], [4], [10]. However, these approaches have several serious problems. Firstly, the enormous number of frequent patterns leads to expensive calculation costs for pattern generation and selection. Secondly, the number of selected patterns can still amount to thousands, limiting interpretability and causing the inefficiency of the classification model. Thirdly, these models are unable to solve regression tasks. Furthermore, the discrediting of continuous variables depends too much on parameter setting to generate robust performance. Recently, Dong et al. suggested using patterns at a different angle, but not being divided into different patterns on the basis of the data and on the basis of different patterns."}, {"heading": "2.2 Tree-based Models", "text": "Traditional ensemble methods that use multiple trees, such as random forest [2] and gradient-enhancing decision trees [12], alleviate the problem of overmatch. Ren et al. showed that global refinement could perform better because the growth and pruning processes in different trees are independent [28]. Typically, they encode each tree as a flat index list and each instance as a binary vector indexed by the trees [28], [16], [9], [27], [24]. Vens et al. transferred the binary vectors into an inner product core, with the comparison engine and the increase in classification by two layers being more accurate [32]."}, {"heading": "2.3 Pattern Selection", "text": "The simple selection of patterns with the highest independent heuristics such as information gain and Gini index is limited to very simple tasks due to redundancy and overpass problems [20]. Given the labels, i.e. the types for classification or the real numbers for regression, LASSO [30] is widely used in both feature selection and forward selection [6]. Due to the relatively large number of discriminatory candidate patterns, reverse selection is not appropriate in our problem. Our proposed DPPRED framework adopts the LASSO and forward selection methods for selecting discriminatory patterns. Their performance has been compared and discussed in the experimental section."}, {"heading": "3 PRELIMINARIES", "text": "This section defines both the problem and the important concepts used in this essay."}, {"heading": "3.1 Problem Formulation", "text": "In a predictive task (classification or regression), the data are a set of n examples in a d-dimensional attribute space together with their names (x1, y1), (x2, y2),..., (xn, yn), in case the values in example xi can be either continuous (numerical) or discrete (categorical). Since categorical characteristics can be transformed into multiple binary dummy indicators, we can assume that xi-Rd is without the loss of generality. The label yi is either a class indicator (type) or a real number depending on the specific task. In earlier pattern-based models, e.g. DDPMINE [4], patterns are extracted from categorised values, and therefore they can only handle the continuous variables after careful manual discretization."}, {"heading": "3.2 Definition", "text": "First, we define a set of concepts to derive the discriminatory patterns. < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < <"}, {"heading": "4 OUR DPPRED FRAMEWORK", "text": "This section first presents an overview of DPPRED and then introduces the details of each component of this framework as well as the theoretical complexity of time."}, {"heading": "4.1 The Overview of DPPRED", "text": "Figure 2 provides an overview of our DPPRED framework. First, it learns a limited, tree-based model with the training data. By adopting each prefix path from the root of a tree to each of its non-leaf nodes as a discriminatory pattern, a large pool of discriminatory patterns is available for another top-k discrimination pattern selection. Two different solutions, forward selection and LASSO, are used to select top-k discrimination patterns based on their performance using a generalized linear model. Both solutions have demonstrated high accuracy in the experiments. The corresponding linear model with the selected top-k discrimination patterns is transferred to macro credits on new examples. Our DPPRED is extremely fast and memory efficient."}, {"heading": "4.2 Discriminative Pattern Generation", "text": "The first component in the DPPRED framework is the generation of high-quality discrimination patterns, as shown in Algorithm 1. We use tree pocket to reference the number of cases that fall into a particular node in the decision tree. Random decision tree [2] introduces randomness using discriminatory training data by randomly selecting features and splitting values when a large tree pocket is split into two smaller ones. Random decision trees are created, and for each tree all prefix paths from its root to non-leaf nodes are treated as discriminatory patterns. Due to the predictivity of decision trees, such generated patterns are highly effective in the specific prediction task. Note that the decision tree is built with different loss functions in different tasks that could be obtained in classification tasks or the mean square error in regression tasks. Algorithm 1: discriminatory pattern generation: minimum required number of instances (T), the number of square instances (T) and the number of trees in the regression tasks."}, {"heading": "4.3 Pattern Space Construction", "text": "After pattern generation, DPPRED assigns the instances in the original attribute space to a new pattern space, using the set of discriminatory patterns discovered by tree models, as in Algorithm 2. For each discriminatory pattern, there is a corresponding binary dimension that describes whether the instances satisfy the pattern or not. Since the dimension of the pattern space corresponds to the number of discriminatory patterns, which is a very large number after the generation phase, we must continue to select a limited number of patterns, thus making the pattern space small and efficient. It is also worth noting that this mapping process can be completely parallelized for speed. Algorithm 2: Patterspace Construction Require: n Instances (xi), a discriminatory pattern that P Return: n Instances in the pattern space (x \u2032 i) for i = 1 to n dox \u2032 i \u2190 0 for the j pattern Pj in P if doxi \u2032, then j \u2032 (Return x)."}, {"heading": "4.4 Top-k Pattern Selection", "text": "Once a large pool of discrimination patterns has been generated, another top-k selection needs to be made to identify the most informative and interpretable patterns. A naive method is to use heuristic functions such as information gathering and Gini index to evaluate the importance of different patterns for the prediction task and to select the best-rated patterns. However, the effects of lace patterns based on simple heuristic values can have a large amount of overlap, and therefore their combination does not work optimally. Therefore, in order to achieve the best performance and find complementary patterns, we propose two effective solutions: Pre-selection and LASSO, which make decisions based on the effects of pattern combinations rather than looking at different patterns independently."}, {"heading": "4.4.1 Forward Pattern Selection", "text": "Instead of an exhausted search for all possible combinations of k-discrimination patterns, forward selection gradually adds the discrimination patterns, while each newly added discrimination pattern is the best choice at this point in time [6], providing an efficient approximation to the exhausted search. To be more precise, when the first k-discrimination patterns are fixed, the algorithm empirically adds another discrimination pattern, so that the new set of k + 1 patterns achieves the best training performance in the general linear model, as in Algorithm 3. As mentioned above, the use of training accuracy is very useful when training and test data have the same distribution."}, {"heading": "4.4.2 LASSO based Pattern Selection", "text": "The L1 regularization (i.e., LASSO [30]) is designed in such a way that the weight vector pattern is thin by setting a non-negative parameter \u03bb, whereby the features with a non-zero weight of the algorithm 3: Top k pattern discrimination pattern Selection: Forward Require: n training examples (xi, yi), a set of discriminating patterns P and k Return: Top k discriminating pattern Pk and a generalized linear model f (\u00b7) Pk pattern Selection for t = 1 to k dofor each pattern p in P do x \u2032 \u2190 \u2190 Construction pattern space (x, Pk-p {p}) using the algorithm 2 g (\u00b7) a generalized linear model [29] on (x \u2032 i, yi) s training performance p p p in P do x \u2032 \u2190 \u2190 \u2190 examples Construction pattern space (x, Pk) constructed space (maxeark \u00b2) construction space (construction space} x x x) is a general construction space (construction space} p x x)."}, {"heading": "4.5 Prediction", "text": "Once the top k discrimination patterns are determined, DPPRED first maps them into the learned pattern space for each upcoming new test instance and then applies the pre-trained generalized linear model to calculate the prediction, as shown in Algorithm 5. Since the number of patterns is limited, both the pattern space mapping and the prediction of the generalized linear model will be extremely fast."}, {"heading": "4.6 Time Complexity Analysis", "text": "To build a single random decider tree with depth threshold D and minimum canal size \u03c3 by assuming both number of random characters and random partitions as small and fixed constants, the time complexity is O (nD), since the total number of instances on each plane of the tree is n. Therefore, the time complexity of generating T-trees O (TnD) is in the generation step. For the selection step, the complexity is mainly determined by the number of discriminatory patterns induced by T random decider trees, which depends on the total number of non-leaf nodes. Since the maximum depth of a single tree is D, there is an upper limit on the number of leaf nodes 2D. Based on the size of the canal, the number of leaf nodes should not be more than thin e. Since the trees here are all binary trees, the number of leaf nodes is one (the number of nodes by the number of nodes greater than the number of leaves)."}, {"heading": "5 EXPERIMENTS", "text": "In this section, we conduct extensive experiments to demonstrate the interpretability, efficiency and effectiveness of our proposed DPPRED framework. First, we present our experimental settings, discuss efficiency and interpretability, and then announce the results of classification and regression tasks, as well as parameter analysis."}, {"heading": "5.1 Experimental Settings", "text": "In this section, the data sets, basic methods and learning tasks in our experiments are presented."}, {"heading": "5.1.1 Datasets", "text": "First, we generate synthetic datasets where the characteristics are patient demographics and laboratory test results and the label is whether the patient has a disease to demonstrate the interpretability of DPPRED. Provided physicians can diagnose the disease based on some rules based on this information, it can be verified that the top discrimination patterns selected by DPPRED are consistent with the actual diagnostic rules. Several real classification and regression datasets from the UCI Machine Learning Repository are used in the experiments, as shown in Table 1 with statistics on the number of cases and number of characteristics. In the adult, hypo and sick datasets, the ratio of standard traction / test splitting is 2: 1. For the other classification and regression datasets, we divide the datasets in train / test (2: 1) by unbiased data collection as pre-processing data collection."}, {"heading": "5.1.2 Baseline Methods", "text": "DDPMINE [4] is a state-of-the-art discriminatory pattern-based algorithm. Initially, it discredits the continuous variables in such a way that a common pattern-mining algorithm could be applied. Frequent and discriminatory patterns are used to construct new attribute space and all classical classifiers could continue to be used. DDPMINE focuses only on classification tasks and is not applicable to regression experiments.Random Forest (RF) [2] is another basic method that uses the same parameters as the random forests used in DPPRED, with the exception of D. These two complex models (i.e. difficult to interpret) are expected to perform slightly better than DPPRED, as the main contributions of the DPPRED method are capable of both classification and regression tasks."}, {"heading": "5.1.3 Classification and Regression Tasks", "text": "In DPPRED, the default parameter setting for the classification tasks is T = 100, D = 6, \u03c3 = 10, k = 20. Since the continuous labels are more complex than the individual class labels in the classification, it is natural to include more patterns. Therefore, the default setting is T = 100, D = 6, \u03c3 = 10, k = 30. We will show results that use both forward selection (DPPRED-F) and LASSO (DPPRED-L) to select the top k discrimination patterns. We will thoroughly examine the effects of the parameters, such as the number of selected discrimination patterns k and the number of trees in the random forest T. Therefore, we set the other parameters as their default values and vary the parameter value to examine their effects."}, {"heading": "5.2 Efficiency and Interpretability", "text": "In the experiments, DDPMINE requires 100 to 1,000 patterns, while DPPRED requires only 20, indicating a significant reduction in predictive patterns. Furthermore, without any limitations, the random forest will contain more than 10,000 knots (i.e. patterns), which is much more expensive. Although the evaluation of random forest patterns for a single test instance only traverses a number of knots, which is the sum of depths in different trees, it always requires more than 1,000 trusses in the experiments. Therefore, DPPRED is the most efficient model for testing new instances, compared to DDPMINE and random forest, being about 20 to 50 times faster in practice than the depth in different trees, requiring more than 1,000 trusses in the experiments. DPPRED is the most efficient model for testing new instances, compared to DPMINE and random forest."}, {"heading": "5.3 Effectiveness in Classification", "text": "DPPRED-F and DPPREDL always have a higher accuracy than DDPMINE. An important reason for this advantage is that the candidate patterns generated by tree-based models in DPPRED are much more discriminatory and therefore more effective in the specific classification task than the common but less useful patterns extracted in DDPMINE. Apart from sick data sets, DPPRED-F has the highest accuracy, while DPPRED-L works best on sick data sets. It seems that DPPRED-F works a little better than DPPRED-L. However, their results are quite close together and are better than those of DPPRED-L, which works best on sick data sets."}, {"heading": "5.4 Effectiveness in Regression", "text": "Since DDPMINE is not applicable to regression tasks, we are comparing DPPRED only with DT, RF and LRF. Note that these two methods are highly complicated and therefore have very limited interpretative capability. RMSE results and average differences compared to DPPRED are shown in Table 4. In contrast to the results in classification data sets, complex models outperform DPPRED in all data sets, although the difference is not very significant. This is justifiable because, unlike the individual class names, the real value forecast increases the degree of difficulty. Although we have slightly increased the number of top samples, sample bag representations based on a small number of patterns still have some limitations in predicting a real value. For example, there are at most 230 different examples in the constructed sample space, which means that there are at most 230 different predicted values, but infinite real numbers are probably the true value of a real value for a real value of a real value. For example, there are no more than 230 different examples in the constructed sample space, meaning that there are at most 230 different predicted values, but infinite real numbers are still comparable to the true value of a real value of a real value of a real value of a real value."}, {"heading": "5.5 Effectiveness in High Dimensions", "text": "We are interested in high-dimensional datasets (i.e., at least 100 dimensions) because DDPMINE is not effective in large-dimensional data. To compare with DDPMINE, we use classification datasets whose number of dimensions is at least 100 and no regression datasets are used. As the dimension of the original attribute space grows, it is reasonable to increase the depth threshold D as well as the number of trees T to include higher-order interactions and increase the number of candidates for discriminatory patterns. Therefore, we use D = 10 and T = 200. In the meantime, the dimension of the mapped pattern space may also need to be increased due to the higher complexity of the problems. As a result, we use k = 50 in nomao- and musk-datasets. However, we have retained k = 20 in madelon datasets because many Dnoises.As shown in Table 5, DPPREDPMINE can always exceed DDRS data and generate DDRS results that are highly comparable to those of DRS, although RF is very difficult to generate."}, {"heading": "5.6 Parameter Analysis", "text": "In this section we will examine in detail the parameters, including the number of top patterns k and the number of trees in the random forest T."}, {"heading": "5.6.1 The Number of Top Discriminative Patterns", "text": "The most interesting parameter in DPPRED is k, the number of discrimination patterns used in the final generalized linear model. It controls the model size of the generalized linear model used for prediction, and thus affects its efficiency. As the default value of k is 20 for classification tasks and 30 for regression tasks and its effectiveness has been proven in previous experiments, we vary in this experiment k from 1 to 40 to see the trends in both training and testing accuracy on different data sets. Three representative classification data sets (adults, hypo and sick) and three regression data sets (bicycle, crime and parkinsons) are used in this experiment. As shown in Figure 3, performance on test data always follows the trend of performance on training data and performance as k increases in both classification and regression tasks (accuracy increases) and three regression data sets (bicycle, crime and parking)."}, {"heading": "5.6.2 The Number of Trees in the Model", "text": "Another important parameter in DPPRED is the number of trees needed to create the large pool of discrimination patterns. As already mentioned, a single tree is not enough to generate so many patterns, so there is a strong motivation to try T = 1 as an extreme case. As before, the default value of 100 works well in previous experiments, and therefore we vary T in {1, 10, 50, 100, 500, 1,000} to see trends in training and test accuracy. As before, three sets of data for classification and regression tasks are presented in the experiments. Figure 4 illustrates the results of the classification or regression data sets. If T = 1, the performance is much lower than others, meaning that only a single decision tree is not sufficient for a diverse pattern pool. Too few trees generally cannot guarantee a high coverage of effective patterns, especially when the data set is large and the dimensions are high. An increasing number of trees leads to better diversity of candidate patterns, such as ASZ."}, {"heading": "6 NOVEL MARKER DISCOVERY FOR ALS PATIENT STRATIFICATION", "text": "In this section, we use DPPRED to analyze the prognosis and stratification of ALS patients. Unlike other diseases, such as many cancers, which can clearly be classified into subtypes with different survival rates, no significant signals have been identified to explain the different survival times (from less than one year to over 10 years) of ALS patients. Such a broad spectrum makes it difficult to predict disease progression and survival, and indicates quite a heterogeneity of underlying diseases. There may be different subgroups of patients, each with its own causes and prognoses."}, {"heading": "6.1 ALS Dataset", "text": "In fact, most of them are able to determine for themselves what they want and what they want."}, {"heading": "6.2 Data Processing", "text": "The clinical variables about a patient contain 3 types of data: static categorical, static continuous, and longitudinal continuous variables. Static variables are time-dependent, while longitudinal variables are measured several times for each patient and are likely to change over time. Any static categorical variable with k categories is replaced by k + 1 binary characteristics, with the additional variable indicating whether the variable is missing. A static continuous variable is simply a continuous characteristic. Any longitudinal continuous variable {x, t} where x-Rn is the n measurements and t-Rn the times of n measurements in ascending order is converted into 12 continuous characteristics by including some statistics of {x, t} and a derived sequence."}, {"heading": "6.3 Task Description", "text": "In precision medicine, we assume that there are some implicit groups underlying patients, such as the subtypes of a particular disease. Formally, we define the patient cluster as the following. Definition 5: Diagnosis-stratified patient clusters are G-disjoint patient groups, so patients within the same group are similar, and there are different top-k patterns of clinical variables in clusters, suggesting that some patients are instead evaluated with a modified version of ALSFRS-R from 0 to 48, with \"R1.Dyspnea,\" \"R2.Orthopnea,\" and \"R3.Respiratory Insufficiency,\" each ranging from 0 to 4.3. To ensure consistency of scales between patients for whom patients with ALSFRS-R have only one global disease pattern, no ALSFRS substances are used to calculate the sum of questions 1-9 and R1."}, {"heading": "6.4 DPPRED for ALS patient stratification", "text": "As shown in Figure 5, prognosis analysis and stratification for ALS patients work as follows: \u2022 Discover global Kg patterns based on all patients; \u2022 Divide patients into G-groups based on the global patterns discovered; \u2022 Discover local Kl patterns within each patient cluster; \u2022 Construct the bag patterns for each patient based on all global patterns and 0 0.2 0.4 0.6 1 1.2 1.4Team 10: Linear RegressionTeam 9: Multivariate regressionTeam 8: Linear RegressionTeam 7: Predict Bases of Importance: Support vector regr.Team 6: Non-par. RegressionTeam 5: Random forestTeam 4: Random forestTeam 3: Random forestTeam 2: Random forestTeam 1: Bayesian treesDPMedbased on these Kg + Kl discriminative patterns; \u2022 Predict by the generalized linear model. We use PPRED to discover global and local patterns."}, {"heading": "6.5 Results and Discussion", "text": "DPPRED with 3 patient clusters achieves an RMSE of 0.5306 on the validation file, which is only 0.6% away from the RMSE. Linear combination of discrimination patterns with DPPRED includes 28 clinical variables containing a small subset of all available variables."}, {"heading": "7 CONCLUSIONS", "text": "In this paper, we propose an effective and concise, discriminatory, pattern-based prediction model (DPPRED) to solve the classification and regression problems and ensure high interpretation capability with a low number of discriminatory patterns. Specifically, DPPRED first trains a restricted multi-tree model based on training data and then extracts the prefix paths from root nodes to non-leaf nodes in all trees as discriminatory patterns for candidates. The size of discriminatory patterns is compressed by selecting the most effective pattern combinations according to their predictive performance in a generalized linear model. Instead of selecting the patterns independently using heuristics, DPPRED finds the best combination using forward selection or LASSO, which avoids the overlap effect between similar patterns. Extensive experiments show that DPPRED is able to model high-order interactions and provide a small number of comparable DPPRED signals to better interpret the PME or PME model for human use."}], "references": [{"title": "Latent dirichlet allocation", "author": ["D.M. Blei", "A.Y. Ng", "M.I. Jordan"], "venue": "JMLR, 3:993\u20131022,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2003}, {"title": "Using random forest to learn imbalanced data", "author": ["C. Chen", "A. Liaw", "L. Breiman"], "venue": "University of California, Berkeley,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2004}, {"title": "Discriminative frequent pattern analysis for effective classification", "author": ["H. Cheng", "X. Yan", "J. Han", "C.-W. Hsu"], "venue": "Data Engineering, 2007. ICDE 2007. IEEE 23rd International Conference on, pages 716\u2013 725. IEEE,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2007}, {"title": "Direct discriminative pattern mining for effective classification", "author": ["H. Cheng", "X. Yan", "J. Han", "P.S. Yu"], "venue": "Data Engineering, 2008. ICDE 2008. IEEE 24th International Conference on, pages 169\u2013178. IEEE,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2008}, {"title": "Mining top-k covering rule groups for gene expression data", "author": ["G. Cong", "K.-L. Tan", "A.K. Tung", "X. Xu"], "venue": "Proceedings of the 2005 ACM SIGMOD international conference on Management of data, pages 670\u2013681. ACM,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2005}, {"title": "Backward, forward and stepwise automated subset selection algorithms: Frequency of obtaining authentic and noise variables", "author": ["S. Derksen", "H. Keselman"], "venue": "British Journal of Mathematical and Statistical Psychology, 45(2):265\u2013282,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1992}, {"title": "Frequent substructure-based approaches for classifying chemical compounds", "author": ["M. Deshpande", "M. Kuramochi", "N. Wale", "G. Karypis"], "venue": "TKDE, 17(8):1036\u20131050,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2005}, {"title": "Pattern aided classification", "author": ["G. Dong", "V. Taslimitehrani"], "venue": "Proceedings of 2016 SIAM international conference on Data Mining,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2016}, {"title": "Drop: an svm domain linker predictor trained with optimal features selected by random forest", "author": ["T. Ebina", "H. Toh", "Y. Kuroda"], "venue": "Bioinformatics, 27(4):487\u2013494,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2011}, {"title": "Direct mining of discriminative and essential frequent patterns via model-based search tree", "author": ["W. Fan", "K. Zhang", "H. Cheng", "J. Gao", "X. Yan", "J. Han", "P. Yu", "O. Verscheure"], "venue": "Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 230\u2013238. ACM,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2008}, {"title": "glmnet: Lasso and elasticnet regularized generalized linear models", "author": ["J. Friedman", "T. Hastie", "R. Tibshirani"], "venue": "R package version, 1,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2009}, {"title": "Greedy function approximation: a gradient boosting machine", "author": ["J.H. Friedman"], "venue": "Annals of statistics, pages 1189\u20131232,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2001}, {"title": "Bagging gradientboosted trees for high precision, low variance ranking models", "author": ["Y. Ganjisaffar", "R. Caruana", "C.V. Lopes"], "venue": "Proceedings of the 34th international ACM SIGIR conference on Research and development in Information Retrieval, pages 85\u201394. ACM,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2011}, {"title": "Placebo-controlled phase i/ii studies of minocycline in amyotrophic lateral sclerosis", "author": ["P. Gordon", "D. Moore", "D. Gelinas", "C. Qualls", "M. Meister", "J. Werner", "M. Mendoza", "J. Mass", "G. Kushner", "R. Miller"], "venue": "Neurology, 62(10):1845\u20131847,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2004}, {"title": "Applied logistic regression", "author": ["D.W. Hosmer Jr", "S. Lemeshow"], "venue": "John Wiley & Sons,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2004}, {"title": "Discriminative tree-based feature mapping", "author": ["M. Kobetski", "J. Sullivan"], "venue": "Intelligence, 34(3),", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2011}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "Advances in neural information processing systems, pages 1097\u20131105,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2012}, {"title": "An application of boosting to graph classification", "author": ["T. Kudo", "E. Maeda", "Y. Matsumoto"], "venue": "Advances in neural information processing systems, pages 729\u2013736,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2004}, {"title": "Crowdsourced analysis of clinical trial data to predict amyotrophic lateral sclerosis progression", "author": ["R. K\u00fcffner", "N. Zach", "R. Norel", "J. Hawe", "D. Schoenfeld", "L. Wang", "G. Li", "L. Fang", "L. Mackey", "O. Hardiman"], "venue": "Nature biotechnology,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2015}, {"title": "Information gain and divergence-based feature selection for machine learning-based text categorization", "author": ["C. Lee", "G.G. Lee"], "venue": "Information processing & management, 42(1):155\u2013165,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2006}, {"title": "The spectrum kernel: A string kernel for svm protein classification", "author": ["C.S. Leslie", "E. Eskin", "W.S. Noble"], "venue": "Pacific symposium on biocomputing, volume 7, pages 566\u2013575. World Scientific,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2002}, {"title": "Cmar: Accurate and efficient classification based on multiple class-association rules", "author": ["W. Li", "J. Han", "J. Pei"], "venue": "Data Mining, 2001. ICDM 2001, Proceedings IEEE International Conference on, pages 369\u2013376. IEEE,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2001}, {"title": "Text classification using string kernels", "author": ["H. Lodhi", "C. Saunders", "J. Shawe-Taylor", "N. Cristianini", "C. Watkins"], "venue": "JMLR, 2:419\u2013 444,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2002}, {"title": "Intelligible models for classification and regression", "author": ["Y. Lou", "R. Caruana", "J. Gehrke"], "venue": "Proceedings of the 18th ACM SIGKDD international conference on Knowledge discovery and data mining,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2012}, {"title": "Accurate intelligible models with pairwise interactions", "author": ["Y. Lou", "R. Caruana", "J. Gehrke", "G. Hooker"], "venue": "Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 623\u2013631. ACM,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2013}, {"title": "Integrating classification and association rule mining", "author": ["B.L.W.H.Y. Ma"], "venue": "Proceedings of the fourth international conference on knowledge discovery and data mining,", "citeRegEx": "26", "shortCiteRegEx": null, "year": 1998}, {"title": "Fast discriminative visual codebooks using randomized clustering forests", "author": ["F. Moosmann", "B. Triggs", "F. Jurie"], "venue": "Twentieth Annual Conference on Neural Information Processing Systems (NIPS\u201906), pages 985\u2013992. MIT Press,", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2007}, {"title": "Global refinement of random forest", "author": ["S. Ren", "X. Cao", "Y. Wei", "J. Sun"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 723\u2013730,", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2015}, {"title": "Least squares support vector machine classifiers", "author": ["J.A. Suykens", "J. Vandewalle"], "venue": "Neural processing letters, 9(3):293\u2013300,", "citeRegEx": "29", "shortCiteRegEx": null, "year": 1999}, {"title": "Regression shrinkage and selection via the lasso", "author": ["R. Tibshirani"], "venue": "Journal of the Royal Statistical Society. Series B (Methodological), pages 267\u2013288,", "citeRegEx": "30", "shortCiteRegEx": null, "year": 1996}, {"title": "Lazy associative classification", "author": ["A. Veloso", "W. Meira", "M.J. Zaki"], "venue": "Data Mining, 2006. ICDM\u201906. Sixth International Conference on, pages 645\u2013654. IEEE,", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2006}, {"title": "Random forest based feature induction", "author": ["C. Vens", "F. Costa"], "venue": "Data Mining (ICDM), 2011 IEEE 11th International Conference on, pages 744\u2013753. IEEE,", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2011}, {"title": "Harmony: Efficiently mining the best rules for classification", "author": ["J. Wang", "G. Karypis"], "venue": "Proceedings of 2005 SIAM international conference on Data Mining, volume 5, pages 205\u2013216. SIAM,", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2005}, {"title": "Cpar: Classification based on predictive association rules", "author": ["X. Yin", "J. Han"], "venue": "Proceedings of 2003 SIAM international conference on Data Mining, volume 3, pages 369\u2013376. SIAM,", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2003}], "referenceMentions": [{"referenceID": 14, "context": "One line has ordinary performance with strong interpretability on a set of simple features, but meets a serious bottleneck when modeling complex high-order interactions between features, such as linear regression, logistic regression [15], and support vector machine [29].", "startOffset": 234, "endOffset": 238}, {"referenceID": 28, "context": "One line has ordinary performance with strong interpretability on a set of simple features, but meets a serious bottleneck when modeling complex high-order interactions between features, such as linear regression, logistic regression [15], and support vector machine [29].", "startOffset": 267, "endOffset": 271}, {"referenceID": 1, "context": "The other line consists of models that are more often studied for their high accuracy, for example, tree-based models including random forest [2] and gradient boosted trees [13] as well as the neural network models [17], which model nonlinear relationships with high-order combinations of different features.", "startOffset": 142, "endOffset": 145}, {"referenceID": 12, "context": "The other line consists of models that are more often studied for their high accuracy, for example, tree-based models including random forest [2] and gradient boosted trees [13] as well as the neural network models [17], which model nonlinear relationships with high-order combinations of different features.", "startOffset": 173, "endOffset": 177}, {"referenceID": 16, "context": "The other line consists of models that are more often studied for their high accuracy, for example, tree-based models including random forest [2] and gradient boosted trees [13] as well as the neural network models [17], which model nonlinear relationships with high-order combinations of different features.", "startOffset": 215, "endOffset": 219}, {"referenceID": 14, "context": "However, their lower interpretability and high complexity prevent practitioners from deploying in practice [15].", "startOffset": 107, "endOffset": 111}, {"referenceID": 25, "context": "Many pattern-based models have been proposed in the last decade to construct high-order patterns from the large set of features, including association rule-based methods on categorical data [26], [22], [34], [5], [33], [31] and frequent pattern-based algorithms on text data [23], [21] and graph data [18], [7].", "startOffset": 190, "endOffset": 194}, {"referenceID": 21, "context": "Many pattern-based models have been proposed in the last decade to construct high-order patterns from the large set of features, including association rule-based methods on categorical data [26], [22], [34], [5], [33], [31] and frequent pattern-based algorithms on text data [23], [21] and graph data [18], [7].", "startOffset": 196, "endOffset": 200}, {"referenceID": 33, "context": "Many pattern-based models have been proposed in the last decade to construct high-order patterns from the large set of features, including association rule-based methods on categorical data [26], [22], [34], [5], [33], [31] and frequent pattern-based algorithms on text data [23], [21] and graph data [18], [7].", "startOffset": 202, "endOffset": 206}, {"referenceID": 4, "context": "Many pattern-based models have been proposed in the last decade to construct high-order patterns from the large set of features, including association rule-based methods on categorical data [26], [22], [34], [5], [33], [31] and frequent pattern-based algorithms on text data [23], [21] and graph data [18], [7].", "startOffset": 208, "endOffset": 211}, {"referenceID": 32, "context": "Many pattern-based models have been proposed in the last decade to construct high-order patterns from the large set of features, including association rule-based methods on categorical data [26], [22], [34], [5], [33], [31] and frequent pattern-based algorithms on text data [23], [21] and graph data [18], [7].", "startOffset": 213, "endOffset": 217}, {"referenceID": 30, "context": "Many pattern-based models have been proposed in the last decade to construct high-order patterns from the large set of features, including association rule-based methods on categorical data [26], [22], [34], [5], [33], [31] and frequent pattern-based algorithms on text data [23], [21] and graph data [18], [7].", "startOffset": 219, "endOffset": 223}, {"referenceID": 22, "context": "Many pattern-based models have been proposed in the last decade to construct high-order patterns from the large set of features, including association rule-based methods on categorical data [26], [22], [34], [5], [33], [31] and frequent pattern-based algorithms on text data [23], [21] and graph data [18], [7].", "startOffset": 275, "endOffset": 279}, {"referenceID": 20, "context": "Many pattern-based models have been proposed in the last decade to construct high-order patterns from the large set of features, including association rule-based methods on categorical data [26], [22], [34], [5], [33], [31] and frequent pattern-based algorithms on text data [23], [21] and graph data [18], [7].", "startOffset": 281, "endOffset": 285}, {"referenceID": 17, "context": "Many pattern-based models have been proposed in the last decade to construct high-order patterns from the large set of features, including association rule-based methods on categorical data [26], [22], [34], [5], [33], [31] and frequent pattern-based algorithms on text data [23], [21] and graph data [18], [7].", "startOffset": 301, "endOffset": 305}, {"referenceID": 6, "context": "Many pattern-based models have been proposed in the last decade to construct high-order patterns from the large set of features, including association rule-based methods on categorical data [26], [22], [34], [5], [33], [31] and frequent pattern-based algorithms on text data [23], [21] and graph data [18], [7].", "startOffset": 307, "endOffset": 310}, {"referenceID": 2, "context": "Recently, a novel series of models, the discriminative pattern-based models [3], [4], have demonstrated their advantages over the traditional models.", "startOffset": 76, "endOffset": 79}, {"referenceID": 3, "context": "Recently, a novel series of models, the discriminative pattern-based models [3], [4], have demonstrated their advantages over the traditional models.", "startOffset": 81, "endOffset": 84}, {"referenceID": 13, "context": "Among the set of important clinical variables (rows) that DPPRED discovered from the dataset of the Prize4Life Challenge 2012, two highlighted ones have later been experimentally verified that they have extremely high correlations with the ALS disease [35], [14], [19].", "startOffset": 258, "endOffset": 262}, {"referenceID": 18, "context": "Among the set of important clinical variables (rows) that DPPRED discovered from the dataset of the Prize4Life Challenge 2012, two highlighted ones have later been experimentally verified that they have extremely high correlations with the ALS disease [35], [14], [19].", "startOffset": 264, "endOffset": 268}, {"referenceID": 13, "context": "These two factors were not found by the top teams in the Challenge but there is indirect experimental and logical evidence for their being actually worth further study [35], [14], [19].", "startOffset": 174, "endOffset": 178}, {"referenceID": 18, "context": "These two factors were not found by the top teams in the Challenge but there is indirect experimental and logical evidence for their being actually worth further study [35], [14], [19].", "startOffset": 180, "endOffset": 184}, {"referenceID": 21, "context": "proposed a classification method CMAR based on multiple class-association rules [22].", "startOffset": 80, "endOffset": 84}, {"referenceID": 33, "context": "extended it to CPAR based on predictive association rules [34].", "startOffset": 58, "endOffset": 62}, {"referenceID": 2, "context": "Besides the association rules, direct discriminative pattern mining was proposed to generate effective performance [3], [4], [10].", "startOffset": 115, "endOffset": 118}, {"referenceID": 3, "context": "Besides the association rules, direct discriminative pattern mining was proposed to generate effective performance [3], [4], [10].", "startOffset": 120, "endOffset": 123}, {"referenceID": 9, "context": "Besides the association rules, direct discriminative pattern mining was proposed to generate effective performance [3], [4], [10].", "startOffset": 125, "endOffset": 129}, {"referenceID": 7, "context": "proposed to utilize patterns in a different angle, where data are partitioned based on patterns, and complex models are trained independently in different partitions [8].", "startOffset": 166, "endOffset": 169}, {"referenceID": 1, "context": "Traditional ensemble methods using multiple trees, such as random forest [2] and gradient boosting decision trees [12], alleviate the over-fitting issue.", "startOffset": 73, "endOffset": 76}, {"referenceID": 11, "context": "Traditional ensemble methods using multiple trees, such as random forest [2] and gradient boosting decision trees [12], alleviate the over-fitting issue.", "startOffset": 114, "endOffset": 118}, {"referenceID": 27, "context": "showed that the global refinement could provide better performance because the growth and pruning processes in different trees are independent [28].", "startOffset": 143, "endOffset": 147}, {"referenceID": 27, "context": "Typically, they encoded each tree as a flat index list and each instance as a binary vector indexed by the trees [28], [16], [9], [27], [24].", "startOffset": 113, "endOffset": 117}, {"referenceID": 15, "context": "Typically, they encoded each tree as a flat index list and each instance as a binary vector indexed by the trees [28], [16], [9], [27], [24].", "startOffset": 119, "endOffset": 123}, {"referenceID": 8, "context": "Typically, they encoded each tree as a flat index list and each instance as a binary vector indexed by the trees [28], [16], [9], [27], [24].", "startOffset": 125, "endOffset": 128}, {"referenceID": 26, "context": "Typically, they encoded each tree as a flat index list and each instance as a binary vector indexed by the trees [28], [16], [9], [27], [24].", "startOffset": 130, "endOffset": 134}, {"referenceID": 23, "context": "Typically, they encoded each tree as a flat index list and each instance as a binary vector indexed by the trees [28], [16], [9], [27], [24].", "startOffset": 136, "endOffset": 140}, {"referenceID": 31, "context": "transferred the binary vectors into an inner product kernel space using a support vector machine and showed the increase of classification accuracy [32].", "startOffset": 148, "endOffset": 152}, {"referenceID": 24, "context": "Furthermore, pairwise interactions have also been studied to fit a two-layer-tree model for accurate classification and regression [25].", "startOffset": 131, "endOffset": 135}, {"referenceID": 27, "context": "For example, in [28], after many efforts on pruning, the model size of the pruned random forest was still at megabytes and thus the prediction was too slow to support real-time applications.", "startOffset": 16, "endOffset": 20}, {"referenceID": 19, "context": "Simply selecting patterns with the highest independent heuristics such as information gain and gini index is limited to very simple tasks due to the redundancy and over-fitting problems [20].", "startOffset": 186, "endOffset": 190}, {"referenceID": 29, "context": ", the types for classification or the real numbers for regression, LASSO [30] is widely used in feature selection tasks as well as forward selection [6].", "startOffset": 73, "endOffset": 77}, {"referenceID": 5, "context": ", the types for classification or the real numbers for regression, LASSO [30] is widely used in feature selection tasks as well as forward selection [6].", "startOffset": 149, "endOffset": 152}, {"referenceID": 3, "context": ", DDPMINE [4], patterns are extracted from categorical values and thus they are only able to handle the continuous variables after careful manual discretization, which is tricky and often requires prior knowledge about the data.", "startOffset": 10, "endOffset": 13}, {"referenceID": 3, "context": ", DDPMINE [4], the practitioners have to discretize values of continuous variables prior to pattern mining.", "startOffset": 10, "endOffset": 13}, {"referenceID": 1, "context": "The random decision tree [2] introduces the randomness via bootstrapping training data, randomly selecting features and splitting values when dividing a large tree bag into two smaller ones.", "startOffset": 25, "endOffset": 28}, {"referenceID": 1, "context": "P \u2190 \u2205 for t = 1 to T do Build a random decision tree [2] with maximum depth D and minimum tree bag size \u03c3.", "startOffset": 53, "endOffset": 56}, {"referenceID": 1, "context": "As one of the most famous multi-tree based models, random forest [2]", "startOffset": 65, "endOffset": 68}, {"referenceID": 5, "context": "Instead of exhausted search of all possible combinations of k discriminative patterns, forward selection gradually adds the discriminative patterns one by one while each newly added discriminative pattern is the best choice at that time [6], which provides an efficient approximation of the exhausted search.", "startOffset": 237, "endOffset": 240}, {"referenceID": 29, "context": ", LASSO [30]) is designed to make the weight vector sparse by tuning a nonnegative parameter \u03bb, where the features with non-zero weight will be the", "startOffset": 8, "endOffset": 12}, {"referenceID": 28, "context": "Require: n training examples (xi, yi), a set of discriminative patterns P and k Return: top-k discriminative patterns set Pk and a generalized linear model f(\u00b7) Pk \u2190 \u2205 for t = 1 to k do for each pattern p in P do x\u2032 \u2190 construct pattern space(x,Pk \u222a {p}) using Algorithm 2 g(\u00b7)\u2190 a generalized linear model [29] on (xi, yi) perp \u2190 g(\u00b7)\u2019s training performance Pk \u2190 Pk \u222a {arg maxp perp} x\u2032 \u2190 construct pattern space(x,Pk) f(\u00b7)\u2190 a generalized linear model on (xi, yi) return Pk, f(\u00b7)", "startOffset": 305, "endOffset": 309}, {"referenceID": 10, "context": "The LASSO implementation in GLMNET [11] is adopted in this thesis, whose loss function is the cross entropy.", "startOffset": 35, "endOffset": 39}, {"referenceID": 0, "context": "Furthermore, to make the errors in different datasets comparable, min-max normalization is adopted to scale the continuous labels into [0, 1].", "startOffset": 135, "endOffset": 141}, {"referenceID": 3, "context": "DDPMINE [4] is a previous state-of-the-art discriminative pattern based algorithm.", "startOffset": 8, "endOffset": 11}, {"referenceID": 1, "context": "Random Forest (RF) [2] is another baseline method using same parameters as those in the random forest used in DPPRED, except for D.", "startOffset": 19, "endOffset": 22}, {"referenceID": 0, "context": "4) Lab Test 2 (LT2): continuous values in [0, 1].", "startOffset": 42, "endOffset": 48}, {"referenceID": 2, "context": "DDPMINE outperforms decision tree and support vector machine on all these datasets [3], [4].", "startOffset": 83, "endOffset": 86}, {"referenceID": 3, "context": "DDPMINE outperforms decision tree and support vector machine on all these datasets [3], [4].", "startOffset": 88, "endOffset": 91}, {"referenceID": 2, "context": "DDPMINE is a previous state-of-the-art pattern-based classification method, which outperforms traditional classification models including decision tree and support vector machine [3][4].", "startOffset": 179, "endOffset": 182}, {"referenceID": 3, "context": "DDPMINE is a previous state-of-the-art pattern-based classification method, which outperforms traditional classification models including decision tree and support vector machine [3][4].", "startOffset": 182, "endOffset": 185}, {"referenceID": 0, "context": "To make the errors in different datasets comparable, min-max normalization is adopted to scale the continuous labels into [0, 1].", "startOffset": 122, "endOffset": 128}, {"referenceID": 18, "context": "2012, a subset of PRO-ACT data was constructed with the aim to crowdsource the challenge of ALS prognosis as a data mining task, which is known as the DREAM-Phil Bowen ALS Prediction Prize4Life Challenge (\u201cthe 2012 challenge\u201d for short in this section) [19].", "startOffset": 253, "endOffset": 257}, {"referenceID": 18, "context": "It has predicted ALS progression from clinical data better than clinicians do, and can potentially reduce the cost of future ALS trials by $6-million [19].", "startOffset": 150, "endOffset": 154}, {"referenceID": 18, "context": "A detailed description of the data can be found in the supplement of [19].", "startOffset": 69, "endOffset": 73}, {"referenceID": 0, "context": "For the patient clustering, by making analogy from bag-of-words to bag-of-patterns, we adopt Latent Dirichlet Allocation (LDA) algorithm [1]", "startOffset": 137, "endOffset": 140}, {"referenceID": 13, "context": "05) when minocycline, a drug that can delay the progression of ALS, is applied [35], [14].", "startOffset": 85, "endOffset": 89}, {"referenceID": 18, "context": "respiratory\u201d is reported by several among the top 5 teams in the 2012 challenge [19] and also by DPPRED, it should not be surprising that the respiratory rate is also in the list.", "startOffset": 80, "endOffset": 84}], "year": 2016, "abstractText": "In the literature, two series of models have been proposed to address prediction problems including classification and regression. Simple models, such as generalized linear models, have ordinary performance but strong interpretability on a set of simple features. The other series, including tree-based models, organize numerical, categorical and high dimensional features into a comprehensive structure with rich interpretable information in the data. In this paper, we propose a novel Discriminative Pattern-based Prediction framework (DPPRED) to accomplish the prediction tasks by taking their advantages of both effectiveness and interpretability. Specifically, DPPRED adopts the concise discriminative patterns that are on the prefix paths from the root to leaf nodes in the tree-based models. DPPRED selects a limited number of the useful discriminative patterns by searching for the most effective pattern combination to fit generalized linear models. Extensive experiments show that in many scenarios, DPPRED provides competitive accuracy with the state-of-the-art as well as the valuable interpretability for developers and experts. In particular, taking a clinical application dataset as a case study, our DPPRED outperforms the baselines by using only 40 concise discriminative patterns out of a potentially exponentially large set of patterns.", "creator": "LaTeX with hyperref package"}}}