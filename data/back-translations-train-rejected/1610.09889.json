{"id": "1610.09889", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-Oct-2016", "title": "Chinese Poetry Generation with Planning based Neural Network", "abstract": "Chinese poetry generation is a very challenging task in natural language processing. In this paper, we propose a novel two-stage poetry generating method which first plans the sub-topics of the poem according to the user's writing intent, and then generates each line of the poem sequentially, using a modified recurrent neural network encoder-decoder framework. The proposed planning-based method can ensure that the generated poem is coherent and semantically consistent with the user's intent. A comprehensive evaluation with human judgments demonstrates that our proposed approach outperforms the state-of-the-art poetry generating methods and the poem quality is somehow comparable to human poets.", "histories": [["v1", "Mon, 31 Oct 2016 12:16:39 GMT  (250kb,D)", "http://arxiv.org/abs/1610.09889v1", "Accepted at COLING 2016"], ["v2", "Wed, 7 Dec 2016 03:56:19 GMT  (262kb,D)", "http://arxiv.org/abs/1610.09889v2", "Accepted paper at COLING 2016"]], "COMMENTS": "Accepted at COLING 2016", "reviews": [], "SUBJECTS": "cs.CL cs.AI", "authors": ["zhe wang", "wei he", "hua wu", "haiyang wu", "wei li", "haifeng wang", "enhong chen"], "accepted": false, "id": "1610.09889"}, "pdf": {"name": "1610.09889.pdf", "metadata": {"source": "CRF", "title": "Chinese Poetry Generation with Planning based Neural Network", "authors": ["Zhe Wang", "Wei He", "Hua Wu", "Haiyang Wu", "Wei Li", "Haifeng Wang", "Enhong Chen"], "emails": ["xiaose@mail.ustc.edu.cn,", "cheneh@ustc.edu.cn", "wanghaifeng}@baidu.com"], "sections": [{"heading": "1 Introduction", "text": "In fact, it is as if most of them are able to follow the rules they have imposed on themselves. (...) In fact, it is as if they are able to determine themselves. (...) In fact, it is as if they are able to determine themselves. (...) It is as if they were able to determine themselves. (...) It is as if they were able to determine themselves. (...) It is as if they were able to determine themselves. (...) It is as if they were able to determine what they want. (...) It is as if they do it as if they want to. (...) It is as if they do it as if they want to. (...) It is as if they want to. (...) It is as if they want to do it. (...)"}, {"heading": "2 Related Work", "text": "Oliveira et al. (2009; 2012; 2014) proposed a method of poetry generation based on semantic and grammatical templates. Netzer et al. (2009) used a method based on word association standards. Tosa et al. (2008) and Wu et al. (2009) used an approach of phrase search for Japanese poetry generation. Greene et al. (2010) used statistical methods for analyzing, generating and translating rhythmic poetry. Colton et al. (2012) described a corpus-based poetry generation system that uses templates to construct poems according to the given constraints. Yan et al. (2013) considered the generation of poetry as an optimization problem based on a framework of summary with multiple constraints. Manuration (2004; 2012) and Zhou et al (2010) used algorithms for generating poems."}, {"heading": "3 Approaches", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Overview", "text": "Inspired by the observation that a human poet must first make a sketch before writing a poem, we propose a planning-based poetry generation approach (PPG), which first creates a sketch according to the user's writing intention, and then generates the poem. Our PPG system takes the user's writing intention as input, which can be a word, sentence, or document, and then generates a poem in two steps: poem planning and poem generation. PPG's two-step process is illustrated in Figure 1. Suppose we write a poem consisting of N lines, where li represents the i line of the poem. In the poem planning phase, the input query is converted into N keywords (k1, k2,..., kN), where ki is the i-th keyword representing the sub-theme for the i line of the poem, and then generated as a string \u2212 li after each of 1."}, {"heading": "3.2 Poem Planning", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.2.1 Keyword Extraction", "text": "At the poem planning stage, it is assumed that the number of keywords extracted from the Q input query must correspond to the number of lines N in the poem, which can ensure that each line contains only one keyword as a subtopic. If the user's Q input query is too long, we must extract the most important N words and maintain the original order as a keyword order to satisfy the requirement.We use the TextRank algorithm (Mihalcea and Tarau, 2004) to evaluate the meaning of the words. It is a graph-based ranking algorithm based on PageRank (Brin and Page, 1998). Each candidate word is represented by a vertex in the graph, and the edges are added between two words according to their occurrence (Mihalcea and Tarau, 2004); the edge weight is determined according to the total number of coexistence strengths of the two words."}, {"heading": "3.2.2 Keyword Expansion", "text": "If the user's input query Q is too short to extract enough keywords, we need to expand some new keywords until the requirement of the keyword count is met. We use two different methods for keyword expansion. RNLM-based method. We use a Recurrent Neural Network Language Model (RNLM) (Mikolov et al., 2010) to predict the following keywords according to the previous order of the keywords: ki = argmaxk P (k | k1: i \u2212 1), where ki is the i-th keyword, and k1: i \u2212 1 is the previous keyword order. The training of the RNNLM needs training consisting of keyword sequences of poems, with a keyword representing the subtopic of a line. We automatically generate the training corpus from the collected poems."}, {"heading": "3.3 Poem Generation", "text": "At the Poem Generation stage, the poem is generated line by line. Each line is generated using the keyword specified by the Poem Planning model (max. weight and all preceding text as input), which can be considered a sequence-to-sequence mapping problem, with the slight difference that the input consists of two different types of sequences: the keyword specified by the Poem Planning model and the previously generated text of the poem. We modify the framework of an attention-based RNN encoder decoder (RNN enc-dec) (Bahdanau et al., 2014) to support multiple sequences as inputs.Given a keyword k that has Tk characters, i.e. k = {a1, aTk}, and the preceding text x that has Tx characters, i.e. x = {x1, x2, xx}, we first encode a sequence into states."}, {"heading": "4 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Dataset", "text": "In this work, we focus on creating a Chinese quadruple line that has 4 lines and each line has the same length of 5 or 7 characters. We collected 76,859 quadruple lines from the Internet and randomly selected 2,000 poems for validation, 2,000 poems for testing, and the rest for training. All poems in the training set are first segmented into words using a CRF-based word segmentation system. We then calculate the TextRank value for each word. As a keyword for line extension, the word with the highest TextRank value is selected (see Section 3.2.2). For knowledge-based enhancement, we use Baidu Baike1 and Wikipedia as additional knowledge sources. After extracting four key word sequences from the lines of a rain square, we generate the entire text for the proposed three-line model."}, {"heading": "4.2 Training", "text": "For the proposed attention-based RNN enc-dec model, we chose the 6,000 most commonly used characters as vocabulary for both the source and target pages. The word embedding dimensionality is 512 and was initialized by word2vec (Mikolov et al., 2013). The recurring hidden layers of the decoder and two encoders contained 512 hidden units. Parameters of our model were randomly initialized via a uniform distribution with support [-0.08 0.08]. The model was trained with the AdaDelta algorithm (Zeiler, 2012), with the minibatch set to 128. The final model is selected according to the perplexity on the validation set."}, {"heading": "4.3 Evaluation", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.3.1 Evaluation Metrics", "text": "It is well known that accurate evaluation of the text generation system is difficult, such as poem generation and dialogue word generation (Zhang and Lapata, 2014; Schatzmann et al., 2005; Mou et al., 2016). There are thousands of ways to generate an appropriate and relative poem or dialogue word on a specific topic, with limited references impossible to cover all correct results. Liu et al. (2016) recently showed that the automatic dialogue word rating metrics based on overlap are adapted, according to a cooperative online encyclopedia provided by the Chinese search engine Baidu: http: / / baike.baidu.com.as BLEU and METEOR have little correlation with human evaluation. Therefore, we are conducting a human study to evaluate the poem generation models. Below (He et al., 2012; Yan et al., 2013; Zhang al., 2014; we are evaluating the human poetry rating system, 2014 and 2014; and 4 respectively)."}, {"heading": "4.3.2 Baselines", "text": "We implemented several methods for generating poems as foundations and used the same pre-processing method for all methods. A method for generating Chinese poems based on statistical machine translation (He et al., 2012). A poem is iteratively generated by \"translating\" the previous line into the next line. A method for generating textual sequences (Graves, 2013) proposed by Mikolov et al. (2010). The lines of a poem are joined together as a character sequence used to train the RNLM.RNNPG. In the approach of the RNN-based poem generator (Zhang and Lapata, 2014), the first line is generated by a standard RNNLM and then all other lines are iteratively generated based on a context vector encoded from the previous lines."}, {"heading": "4.3.3 Results", "text": "We can see that our proposed method, Planning based Poetry Generation (PPG), exceeds all base models in terms of averages; the results are consistent with both the settings of 5 and 7 character poetry generations; the poems generated by SMT are better in poetry than RNNLM, showing that the translation-based method can better capture the mapping relationship between two adjacent lines; ANMT is a strong baseline that performs better than SMT, RNNLM and RNPG, but lower than our approach; both ANMT and PPG use the attention-based enc-dec framework; the main difference is that our method defines the sub-themes for each line before generating the poem; the ANMT method merely translates the preceding text into the next line; without the guide of sub-themes, the system tends to view the more general, but less potent sub-themes for each line as opposed to our sub-based approach, which can be very effective."}, {"heading": "4.4 Automatic Generation vs. Human Poet", "text": "We performed an interesting evaluation that directly compares our automated poetry generation system to human poems, similar to the Turing Test (Turing, 1950). We randomly selected twenty poems from the test set written by ancient Chinese poets. We used the titles of these poems as input and generated a total of twenty poems through our automated generation system. Therefore, the machine-generated poems were on the same subject as the human-written poems. Then, we asked some human assessors to distinguish the human-written poems from the machine-generated poems. We had a total of forty assessors, all of whom were well educated and had a bachelor or or's degree, four of whom were professional in Chinese literature and were assigned to the expert group. The other thirty-six assessors were assigned to the normal group. In the blind test, we showed a poem pair and their titles to the assessor engineer at any time."}, {"heading": "4.5 Generation Examples", "text": "Apart from the old poems in Table 6, our method can generate poems based on any modern term. Table 7 shows some examples. The title of the left poem in Table 7 is \"beer,\" the keywords given in our poem planning model are \"beer,\" \"aroma,\" \"cool\" and \"drunken.\" The title of the right poem is a named entity (Xin Bing) that was a famous writer. Besides \"Xin Bing,\" the poem planning system generates three keywords: \"source flow,\" \"stars\" and \"past,\" all of which are related to the author's works."}, {"heading": "5 Conclusion and Future Work", "text": "In this paper, we proposed a novel two-step method of creating poems that first explicitly breaks down the writing intentions of the user into a series of subthemes, and then iteratively generates a poem using a modified RNN encoder decoder framework. The modified RNN encoder decoder model has two encoders that can encode both the subtopic and the preceding text. Assessment by human experts shows that our approach exceeds all basic models and that the quality of the poem is somehow comparable to human poets. We have also shown that our approach, using encyclopedias an additional source of knowledge, can expand the user's input into suitable subthemes for creating poems. In the future, we will explore other methodologies for planning topics, such as PLSA, LDA or word2vec. We will also apply our approach to other forms of literary genres, such as Song iamics, Qu, or other languages, etc."}, {"heading": "6 Acknowledgments", "text": "This research was supported by China's National Basic Research Programme (973 Programme No. 2014CB340505), China's National Key Research and Development Programme (Funding No. 2016YFB1000904), the National Science Foundation for Distinguished Young Scholars of China (Funding No. 61325010) and the Basic Research Fund for the Central Universities of China (Funding No. WK2350000001). We thank Xuan Liu, Qi Liu, Tong Xu, Linli Xu, Biao Chang and the anonymous reviewers for their insightful comments and suggestions."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Kyunghyun Cho", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1409.0473", "citeRegEx": "Bahdanau et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "The anatomy of a large-scale hypertextual web search engine", "author": ["Brin", "Page1998] Sergey Brin", "Lawrence Page"], "venue": "Computer Networks,", "citeRegEx": "Brin et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Brin et al\\.", "year": 1998}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "author": ["Cho et al.2014] Kyunghyun Cho", "Bart Van Merri\u00ebnboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1406.1078", "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Full-face poetry generation", "author": ["Colton et al.2012] Simon Colton", "Jacob Goodwin", "Tony Veale"], "venue": "In ICCC", "citeRegEx": "Colton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Colton et al\\.", "year": 2012}, {"title": "Generating sequences with recurrent neural networks. arXiv preprint arXiv:1308.0850", "author": ["Alex Graves"], "venue": null, "citeRegEx": "Graves.,? \\Q2013\\E", "shortCiteRegEx": "Graves.", "year": 2013}, {"title": "Automatic analysis of rhythmic poetry with applications to generation and translation", "author": ["Greene et al.2010] Erica Greene", "Tugba Bodrumlu", "Kevin Knight"], "venue": null, "citeRegEx": "Greene et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Greene et al\\.", "year": 2010}, {"title": "Generating chinese classical poems with statistical machine translation models", "author": ["He et al.2012] Jing He", "Ming Zhou", "Long Jiang"], "venue": "In Twenty-Sixth AAAI Conference on Artificial Intelligence", "citeRegEx": "He et al\\.,? \\Q2012\\E", "shortCiteRegEx": "He et al\\.", "year": 2012}, {"title": "Generating chinese couplets using a statistical mt approach", "author": ["Jiang", "Zhou2008] Long Jiang", "Ming Zhou"], "venue": "In Proceedings of the 22nd International Conference on Computational Linguistics-Volume", "citeRegEx": "Jiang et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Jiang et al\\.", "year": 2008}, {"title": "How not to evaluate your dialogue system: An empirical study of unsupervised evaluation metrics for dialogue response generation", "author": ["Liu et al.2016] Chia-Wei Liu", "Ryan Lowe", "Iulian V Serban", "Michael Noseworthy", "Laurent Charlin", "Joelle Pineau"], "venue": "arXiv preprint arXiv:1603.08023", "citeRegEx": "Liu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2016}, {"title": "Using genetic algorithms to create meaningful poetic text", "author": ["Graeme Ritchie", "Henry Thompson"], "venue": "Journal of Experimental & Theoretical Artificial Intelligence,", "citeRegEx": "Manurung et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Manurung et al\\.", "year": 2012}, {"title": "Generating topical poetry", "author": ["Marjan Ghazvininejad", "Xing Shi", "Kevin Knight"], "venue": null, "citeRegEx": "Ghazvininejad et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Ghazvininejad et al\\.", "year": 2016}, {"title": "Textrank: Bringing order into text", "author": ["Mihalcea", "Tarau2004] Rada Mihalcea", "Paul Tarau"], "venue": null, "citeRegEx": "Mihalcea et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Mihalcea et al\\.", "year": 2004}, {"title": "Recurrent neural network based language model", "author": ["Martin Karafi\u00e1t", "Lukas Burget", "Jan Cernock\u1ef3", "Sanjeev Khudanpur"], "venue": "In INTERSPEECH,", "citeRegEx": "Mikolov et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2010}, {"title": "Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781", "author": ["Kai Chen", "Greg Corrado", "Jeffrey Dean"], "venue": null, "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Sequence to backward and forward sequences: A content-introducing approach to generative short-text conversation", "author": ["Mou et al.2016] Lili Mou", "Yiping Song", "Rui Yan", "Ge Li", "Lu Zhang", "Zhi Jin"], "venue": "In Proceedings the 26th International Conference on Computational Linguistics", "citeRegEx": "Mou et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Mou et al\\.", "year": 2016}, {"title": "Gaiku: Generating haiku with word associations norms", "author": ["Netzer et al.2009] Yael Netzer", "David Gabay", "Yoav Goldberg", "Michael Elhadad"], "venue": "In Proceedings of the Workshop on Computational Approaches to Linguistic Creativity,", "citeRegEx": "Netzer et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Netzer et al\\.", "year": 2009}, {"title": "Adapting a generic platform for poetry generation to produce spanish poems", "author": ["Raquel Herv\u00e1s", "Alberto D\u0131\u0301az", "Pablo Gerv\u00e1s"], "venue": "In ICCC", "citeRegEx": "Oliveira et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Oliveira et al\\.", "year": 2014}, {"title": "Automatic generation of poetry: an overview", "author": ["Hugo Gon\u00e7alo Oliveira"], "venue": null, "citeRegEx": "Oliveira.,? \\Q2009\\E", "shortCiteRegEx": "Oliveira.", "year": 2009}, {"title": "Poetryme: a versatile platform for poetry generation. Computational Creativity, Concept Invention, and General Intelligence, 1:21", "author": ["Hugo Gon\u00e7alo Oliveira"], "venue": null, "citeRegEx": "Oliveira.,? \\Q2012\\E", "shortCiteRegEx": "Oliveira.", "year": 2012}, {"title": "Quantitative evaluation of user simulation techniques for spoken dialogue systems", "author": ["Kallirroi Georgila", "Steve Young"], "venue": "In 6th SIGdial Workshop on DISCOURSE and DIALOGUE", "citeRegEx": "Schatzmann et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Schatzmann et al\\.", "year": 2005}, {"title": "Sequence to sequence learning with neural networks. In Advances in neural information processing systems, pages 3104\u20133112", "author": ["Oriol Vinyals", "Quoc V Le"], "venue": null, "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Hitch haiku: An interactive supporting system for composing haiku poem", "author": ["Tosa et al.2008] Naoko Tosa", "Hideto Obara", "Michihiko Minoh"], "venue": "In Entertainment Computing-ICEC", "citeRegEx": "Tosa et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Tosa et al\\.", "year": 2008}, {"title": "Chinese song iambics generation with neural attention-based model. CoRR, abs/1604.06274", "author": ["Wang et al.2016] Qixin Wang", "Tianyi Luo", "Dong Wang", "Chao Xing"], "venue": null, "citeRegEx": "Wang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2016}, {"title": "A summary of rhyming constraints of chinese poems", "author": ["Li Wang"], "venue": null, "citeRegEx": "Wang.,? \\Q2002\\E", "shortCiteRegEx": "Wang.", "year": 2002}, {"title": "New hitch haiku: An interactive renku poem composition supporting tool applied for sightseeing navigation system", "author": ["Wu et al.2009] Xiaofeng Wu", "Naoko Tosa", "Ryohei Nakatsu"], "venue": "In Entertainment Computing\u2013", "citeRegEx": "Wu et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Wu et al\\.", "year": 2009}, {"title": "i, poet: Automatic chinese poetry composition through a generative summarization framework under constrained optimization", "author": ["Yan et al.2013] Rui Yan", "Han Jiang", "Mirella Lapata", "Shou-De Lin", "Xueqiang Lv", "Xiaoming Li"], "venue": "In IJCAI", "citeRegEx": "Yan et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Yan et al\\.", "year": 2013}, {"title": "Generating chinese classical poems with rnn encoder-decoder", "author": ["Yi et al.2016] Xiaoyuan Yi", "Ruoyu Li", "Maosong Sun"], "venue": null, "citeRegEx": "Yi et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Yi et al\\.", "year": 2016}, {"title": "Adadelta: An adaptive learning rate method. CoRR, abs/1212.5701", "author": ["Matthew D. Zeiler"], "venue": null, "citeRegEx": "Zeiler.,? \\Q2012\\E", "shortCiteRegEx": "Zeiler.", "year": 2012}, {"title": "Chinese poetry generation with recurrent neural networks", "author": ["Zhang", "Lapata2014] Xingxing Zhang", "Mirella Lapata"], "venue": "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Zhang et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2014}, {"title": "Genetic algorithm and its implementation of automatic generation of chinese songci", "author": ["Zhou et al.2010] Cheng-Le Zhou", "Wei You", "Xiaojun Ding"], "venue": "Journal of Software,", "citeRegEx": "Zhou et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Zhou et al\\.", "year": 2010}], "referenceMentions": [{"referenceID": 23, "context": "The principles of a quatrain include: The poem consists of four lines and each line has five or seven characters; every character has a particular tone, Ping (the level tone) or Ze (the downward tone); the last character of the second and last line in a quatrain must belong to the same rhyme category (Wang, 2002).", "startOffset": 302, "endOffset": 314}, {"referenceID": 21, "context": "Most approaches employ rules or templates (Tosa et al., 2008; Wu et al., 2009; Netzer et al., 2009; Oliveira, 2009; Oliveira, 2012), genetic algorithms (Manurung, 2004; Zhou et al.", "startOffset": 42, "endOffset": 131}, {"referenceID": 24, "context": "Most approaches employ rules or templates (Tosa et al., 2008; Wu et al., 2009; Netzer et al., 2009; Oliveira, 2009; Oliveira, 2012), genetic algorithms (Manurung, 2004; Zhou et al.", "startOffset": 42, "endOffset": 131}, {"referenceID": 15, "context": "Most approaches employ rules or templates (Tosa et al., 2008; Wu et al., 2009; Netzer et al., 2009; Oliveira, 2009; Oliveira, 2012), genetic algorithms (Manurung, 2004; Zhou et al.", "startOffset": 42, "endOffset": 131}, {"referenceID": 17, "context": "Most approaches employ rules or templates (Tosa et al., 2008; Wu et al., 2009; Netzer et al., 2009; Oliveira, 2009; Oliveira, 2012), genetic algorithms (Manurung, 2004; Zhou et al.", "startOffset": 42, "endOffset": 131}, {"referenceID": 18, "context": "Most approaches employ rules or templates (Tosa et al., 2008; Wu et al., 2009; Netzer et al., 2009; Oliveira, 2009; Oliveira, 2012), genetic algorithms (Manurung, 2004; Zhou et al.", "startOffset": 42, "endOffset": 131}, {"referenceID": 29, "context": ", 2009; Oliveira, 2009; Oliveira, 2012), genetic algorithms (Manurung, 2004; Zhou et al., 2010; Manurung et al., 2012), summarization methods (Yan et al.", "startOffset": 60, "endOffset": 118}, {"referenceID": 9, "context": ", 2009; Oliveira, 2009; Oliveira, 2012), genetic algorithms (Manurung, 2004; Zhou et al., 2010; Manurung et al., 2012), summarization methods (Yan et al.", "startOffset": 60, "endOffset": 118}, {"referenceID": 25, "context": ", 2012), summarization methods (Yan et al., 2013) and statistical machine translation methods (Jiang and Zhou, 2008; He et al.", "startOffset": 31, "endOffset": 49}, {"referenceID": 6, "context": ", 2013) and statistical machine translation methods (Jiang and Zhou, 2008; He et al., 2012) to generate poems.", "startOffset": 52, "endOffset": 91}, {"referenceID": 22, "context": "More recently, deep learning methods have emerged as a promising discipline, which considers the poetry generation as a sequence-to-sequence generation problem (Zhang and Lapata, 2014; Wang et al., 2016; Yi et al., 2016).", "startOffset": 160, "endOffset": 220}, {"referenceID": 26, "context": "More recently, deep learning methods have emerged as a promising discipline, which considers the poetry generation as a sequence-to-sequence generation problem (Zhang and Lapata, 2014; Wang et al., 2016; Yi et al., 2016).", "startOffset": 160, "endOffset": 220}, {"referenceID": 12, "context": "The approach generates the first line from the given keywords with a recurrent neural network language model (RNNLM) (Mikolov et al., 2010) and then the subsequent lines are generated sequentially by accumulating the status of the lines that have been generated so far.", "startOffset": 117, "endOffset": 139}, {"referenceID": 10, "context": "Netzer et al. (2009) employed a method based on word association measures.", "startOffset": 0, "endOffset": 21}, {"referenceID": 10, "context": "Netzer et al. (2009) employed a method based on word association measures. Tosa et al. (2008) and Wu et al.", "startOffset": 0, "endOffset": 94}, {"referenceID": 10, "context": "Netzer et al. (2009) employed a method based on word association measures. Tosa et al. (2008) and Wu et al. (2009) used a phrase search approach for Japanese poem generation.", "startOffset": 0, "endOffset": 115}, {"referenceID": 4, "context": "Greene et al. (2010) applied statistical methods to analyze, generate and translate rhythmic poetry.", "startOffset": 0, "endOffset": 21}, {"referenceID": 3, "context": "Colton et al. (2012) described a corpus-based poetry generation system that uses templates to construct poems according to the given constrains.", "startOffset": 0, "endOffset": 21}, {"referenceID": 3, "context": "Colton et al. (2012) described a corpus-based poetry generation system that uses templates to construct poems according to the given constrains. Yan et al. (2013) considered the poetry generation as an optimization problem based on a summarization framework with several constraints.", "startOffset": 0, "endOffset": 163}, {"referenceID": 3, "context": "Colton et al. (2012) described a corpus-based poetry generation system that uses templates to construct poems according to the given constrains. Yan et al. (2013) considered the poetry generation as an optimization problem based on a summarization framework with several constraints. Manurung (2004; 2012) and Zhou et al. (2010) used genetic algorithms for generating poems.", "startOffset": 0, "endOffset": 329}, {"referenceID": 3, "context": "Colton et al. (2012) described a corpus-based poetry generation system that uses templates to construct poems according to the given constrains. Yan et al. (2013) considered the poetry generation as an optimization problem based on a summarization framework with several constraints. Manurung (2004; 2012) and Zhou et al. (2010) used genetic algorithms for generating poems. An important approach to poem generation is based on statistical machine translation (SMT). Jiang and Zhou (2008) used an SMT-based model in generating Chinese couplets which can be regarded as simplified regulated verses with only two lines.", "startOffset": 0, "endOffset": 489}, {"referenceID": 3, "context": "Colton et al. (2012) described a corpus-based poetry generation system that uses templates to construct poems according to the given constrains. Yan et al. (2013) considered the poetry generation as an optimization problem based on a summarization framework with several constraints. Manurung (2004; 2012) and Zhou et al. (2010) used genetic algorithms for generating poems. An important approach to poem generation is based on statistical machine translation (SMT). Jiang and Zhou (2008) used an SMT-based model in generating Chinese couplets which can be regarded as simplified regulated verses with only two lines. The first line is regarded as the source language and translated into the second line. He et al. (2012) extended this method to generate quatrains by translating the previous line to the next line sequentially.", "startOffset": 0, "endOffset": 722}, {"referenceID": 3, "context": "Colton et al. (2012) described a corpus-based poetry generation system that uses templates to construct poems according to the given constrains. Yan et al. (2013) considered the poetry generation as an optimization problem based on a summarization framework with several constraints. Manurung (2004; 2012) and Zhou et al. (2010) used genetic algorithms for generating poems. An important approach to poem generation is based on statistical machine translation (SMT). Jiang and Zhou (2008) used an SMT-based model in generating Chinese couplets which can be regarded as simplified regulated verses with only two lines. The first line is regarded as the source language and translated into the second line. He et al. (2012) extended this method to generate quatrains by translating the previous line to the next line sequentially. Recently, deep learning methods achieve great success in poem generation. Zhang and Lapata (2014) proposed a quatrain generation model based on recurrent neural network (RNN).", "startOffset": 0, "endOffset": 927}, {"referenceID": 3, "context": "Colton et al. (2012) described a corpus-based poetry generation system that uses templates to construct poems according to the given constrains. Yan et al. (2013) considered the poetry generation as an optimization problem based on a summarization framework with several constraints. Manurung (2004; 2012) and Zhou et al. (2010) used genetic algorithms for generating poems. An important approach to poem generation is based on statistical machine translation (SMT). Jiang and Zhou (2008) used an SMT-based model in generating Chinese couplets which can be regarded as simplified regulated verses with only two lines. The first line is regarded as the source language and translated into the second line. He et al. (2012) extended this method to generate quatrains by translating the previous line to the next line sequentially. Recently, deep learning methods achieve great success in poem generation. Zhang and Lapata (2014) proposed a quatrain generation model based on recurrent neural network (RNN). The approach generates the first line from the given keywords with a recurrent neural network language model (RNNLM) (Mikolov et al., 2010) and then the subsequent lines are generated sequentially by accumulating the status of the lines that have been generated so far. Wang et al. (2016) generated the Chinese Song iambics", "startOffset": 0, "endOffset": 1294}, {"referenceID": 20, "context": "Marjan Ghazvininejad and Knight (2016) proposed a poetry generation algorithm that first generates the rhyme words related to the given keyword and then generated the whole poem according to the rhyme words with an encoder-decoder model (Sutskever et al., 2014).", "startOffset": 237, "endOffset": 261}, {"referenceID": 6, "context": "Second, we use planning-based method to determine the topic of the poem according to the user\u2019s input, with each line having one specific sub-topic, which guarantees that the generated poem is coherent and well organized, therefore avoiding the problem of the previous method that only the first line is guaranteed to be related to the user\u2019s intent while the next lines may be irrelevant with the intention due to the coherent decay problem (He et al., 2012; Zhang and Lapata, 2014; Wang et al., 2016; Yi et al., 2016).", "startOffset": 442, "endOffset": 519}, {"referenceID": 22, "context": "Second, we use planning-based method to determine the topic of the poem according to the user\u2019s input, with each line having one specific sub-topic, which guarantees that the generated poem is coherent and well organized, therefore avoiding the problem of the previous method that only the first line is guaranteed to be related to the user\u2019s intent while the next lines may be irrelevant with the intention due to the coherent decay problem (He et al., 2012; Zhang and Lapata, 2014; Wang et al., 2016; Yi et al., 2016).", "startOffset": 442, "endOffset": 519}, {"referenceID": 26, "context": "Second, we use planning-based method to determine the topic of the poem according to the user\u2019s input, with each line having one specific sub-topic, which guarantees that the generated poem is coherent and well organized, therefore avoiding the problem of the previous method that only the first line is guaranteed to be related to the user\u2019s intent while the next lines may be irrelevant with the intention due to the coherent decay problem (He et al., 2012; Zhang and Lapata, 2014; Wang et al., 2016; Yi et al., 2016).", "startOffset": 442, "endOffset": 519}, {"referenceID": 29, "context": "Third, the rhythm or tone in (Zhou et al., 2010; Yan et al., 2013; Zhang and Lapata, 2014; Yi et al., 2016; Marjan Ghazvininejad and Knight, 2016) is controlled by rules or extra structures, while our model can automatically learn constrains from the training corpus.", "startOffset": 29, "endOffset": 146}, {"referenceID": 25, "context": "Third, the rhythm or tone in (Zhou et al., 2010; Yan et al., 2013; Zhang and Lapata, 2014; Yi et al., 2016; Marjan Ghazvininejad and Knight, 2016) is controlled by rules or extra structures, while our model can automatically learn constrains from the training corpus.", "startOffset": 29, "endOffset": 146}, {"referenceID": 26, "context": "Third, the rhythm or tone in (Zhou et al., 2010; Yan et al., 2013; Zhang and Lapata, 2014; Yi et al., 2016; Marjan Ghazvininejad and Knight, 2016) is controlled by rules or extra structures, while our model can automatically learn constrains from the training corpus.", "startOffset": 29, "endOffset": 146}, {"referenceID": 26, "context": "Finally, our poem generation model has a simpler structure compared with those in (Zhang and Lapata, 2014; Yi et al., 2016).", "startOffset": 82, "endOffset": 123}, {"referenceID": 20, "context": "Wang et al. (2016) did not consider the generation of the first line.", "startOffset": 0, "endOffset": 19}, {"referenceID": 20, "context": "Wang et al. (2016) did not consider the generation of the first line. Therefore, the first line is provided by users and must be a well-written sentence of the poem. Yi et al. (2016) extended this approach to generate Chinese quatrains.", "startOffset": 0, "endOffset": 183}, {"referenceID": 20, "context": "Wang et al. (2016) did not consider the generation of the first line. Therefore, the first line is provided by users and must be a well-written sentence of the poem. Yi et al. (2016) extended this approach to generate Chinese quatrains. The problem of generating the first line is resolved by a separate neural machine translation (NMT) model which takes one keyword as input and translates it into the first line. Marjan Ghazvininejad and Knight (2016) proposed a poetry generation algorithm that first generates the rhyme words related to the given keyword and then generated the whole poem according to the rhyme words with an encoder-decoder model (Sutskever et al.", "startOffset": 0, "endOffset": 454}, {"referenceID": 12, "context": "We use a Recurrent Neural Network Language Model (RNNLM) (Mikolov et al., 2010) to predict the subsequent keywords according to the preceding sequence of keywords: ki = argmaxk P (k|k1:i\u22121), where ki is the i-th keyword and k1:i\u22121 is the preceding keywords sequence.", "startOffset": 57, "endOffset": 79}, {"referenceID": 0, "context": "We modify the framework of an attention based RNN encoder-decoder (RNN enc-dec) (Bahdanau et al., 2014) to support multiple sequences as input.", "startOffset": 80, "endOffset": 103}, {"referenceID": 2, "context": ", xTx}, we first encode k into a sequence of hidden states [r1 : rTk ], and x into [h1 : hTx ], with bi-directional Gated Recurrent Unit (GRU) (Cho et al., 2014) models.", "startOffset": 143, "endOffset": 161}, {"referenceID": 13, "context": "The word embedding dimensionality is 512 and initialized by word2vec (Mikolov et al., 2013).", "startOffset": 69, "endOffset": 91}, {"referenceID": 27, "context": "The model was trained with the AdaDelta algorithm (Zeiler, 2012), where the minibatch was set to be 128.", "startOffset": 50, "endOffset": 64}, {"referenceID": 19, "context": "1 Evaluation Metrics It is well known that accurate evaluation of text generation system is difficult, such as the poetry generation and dialog response generation (Zhang and Lapata, 2014; Schatzmann et al., 2005; Mou et al., 2016).", "startOffset": 164, "endOffset": 231}, {"referenceID": 14, "context": "1 Evaluation Metrics It is well known that accurate evaluation of text generation system is difficult, such as the poetry generation and dialog response generation (Zhang and Lapata, 2014; Schatzmann et al., 2005; Mou et al., 2016).", "startOffset": 164, "endOffset": 231}, {"referenceID": 8, "context": "Liu et al. (2016) has recently shown that the overlap-based automatic evaluation metrics adapted for dialog responses, such A collaborative online encyclopedia provided by Chinese search engine Baidu: http://baike.", "startOffset": 0, "endOffset": 18}, {"referenceID": 6, "context": "Following (He et al., 2012; Yan et al., 2013; Zhang and Lapata, 2014), we use four evaluation standards for human evaluators to judge the poems: \u201cPoeticness\u201d, \u201cFluency\u201d, \u201cCoherence\u201d, \u201cMeaning\u201d.", "startOffset": 10, "endOffset": 69}, {"referenceID": 25, "context": "Following (He et al., 2012; Yan et al., 2013; Zhang and Lapata, 2014), we use four evaluation standards for human evaluators to judge the poems: \u201cPoeticness\u201d, \u201cFluency\u201d, \u201cCoherence\u201d, \u201cMeaning\u201d.", "startOffset": 10, "endOffset": 69}, {"referenceID": 6, "context": "A Chinese poetry generation method based on Statistical Machine Translation (He et al., 2012).", "startOffset": 76, "endOffset": 93}, {"referenceID": 4, "context": "A method for generating textual sequences (Graves, 2013), which is proposed by Mikolov et al.", "startOffset": 42, "endOffset": 56}, {"referenceID": 0, "context": "The main difference is that in ANMT, the machine translation system is a standard attention based RNN enc-dec framework (Bahdanau et al., 2014).", "startOffset": 120, "endOffset": 143}, {"referenceID": 3, "context": "A method for generating textual sequences (Graves, 2013), which is proposed by Mikolov et al. (2010). The lines of a poem are concatenated together as a character sequence which is used to train the RNNLM.", "startOffset": 43, "endOffset": 101}], "year": 2016, "abstractText": "Chinese poetry generation is a very challenging task in natural language processing. In this paper, we propose a novel two-stage poetry generating method which first plans the sub-topics of the poem according to the user\u2019s writing intent, and then generates each line of the poem sequentially, using a modified recurrent neural network encoder-decoder framework. The proposed planningbased method can ensure that the generated poem is coherent and semantically consistent with the user\u2019s intent. A comprehensive evaluation with human judgments demonstrates that our proposed approach outperforms the state-of-the-art poetry generating methods and the poem quality is somehow comparable to human poets.", "creator": "LaTeX with hyperref package"}}}