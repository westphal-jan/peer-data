{"id": "1610.09903", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-Oct-2016", "title": "Learning Runtime Parameters in Computer Systems with Delayed Experience Injection", "abstract": "Learning effective configurations in computer systems without hand-crafting models for every parameter is a long-standing problem. This paper investigates the use of deep reinforcement learning for runtime parameters of cloud databases under latency constraints. Cloud services serve up to thousands of concurrent requests per second and can adjust critical parameters by leveraging performance metrics. In this work, we use continuous deep reinforcement learning to learn optimal cache expirations for HTTP caching in content delivery networks. To this end, we introduce a technique for asynchronous experience management called delayed experience injection, which facilitates delayed reward and next-state computation in concurrent environments where measurements are not immediately available. Evaluation results show that our approach based on normalized advantage functions and asynchronous CPU-only training outperforms a statistical estimator.", "histories": [["v1", "Mon, 31 Oct 2016 12:57:25 GMT  (495kb,D)", "http://arxiv.org/abs/1610.09903v1", "Deep Reinforcement Learning Workshop, NIPS 2016"]], "COMMENTS": "Deep Reinforcement Learning Workshop, NIPS 2016", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["michael schaarschmidt", "felix gessert", "valentin dalibard", "eiko yoneki"], "accepted": false, "id": "1610.09903"}, "pdf": {"name": "1610.09903.pdf", "metadata": {"source": "CRF", "title": "Learning Runtime Parameters in Computer Systems with Delayed Experience Injection", "authors": ["Michael Schaarschmidt", "Felix Gessert"], "emails": ["michael.schaarschmidt@cl.cam.ac.uk", "gessert@informatik.uni-hamburg.de", "valentin.dalibard@cl.cam.ac.uk", "eiko.yoneki@cl.cam.ac.uk"], "sections": [{"heading": "1 Introduction", "text": "In recent years, reinforcement learning algorithms (RL) have been successfully combined with deep neural networks as function approximators [1, 2, 3]. Neural networks can capture structure in the environment of high-dimensional raw data and efficiently generalize across large government spaces. Therefore, reinforcement learning techniques (DRL) provide a powerful end-to-end learning model of sensory inputs without prior knowledge of environmental dynamics. However, the problem with this paper is the use of DRL to provide real-time controllers for such problems. The key idea of this paper is that for comparatively small state dimensions (< 100) and tasks with weaker structure, no advanced offline training is required to implement effective controllers."}, {"heading": "2 Background and related work", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Preliminaries", "text": "The parameter learning problem corresponds to the setting of an infinite horizon discounted Q = Q function, in which an agent interacts with an environment described by the states and aims to learn a policy that governs which action a country in each state has to take [5]. At each discrete time step, the agent takes an action according to its current policy \u03c0 (a | s), transitions to a new state + 1 according to the (often stochastic) environmental dynamics, and observes a reward decline. The objective of the agent is to maximize cumulative expected rewards R = E [a | s], discounting future rewards according to future value. This is often achieved by learning a Q function that represents the expected return when starting from a state, taking actions with the highest Q value and taking subsequent actions thereafter. Mnih et al. have shown how deep neural networks can be used as memory functions by using a variety of complex tasks."}, {"heading": "2.2 Related work", "text": "Our work is conceptually most similar to that of Tesauro et al. \"s on resource allocation in data centers [10, 11, 12], which used a perceptron with a single hidden layer to make server allocation decisions for different applications. Their approach aims to maximize the expected sum of payments from service agreements and minimize penalties for service objectives not met. Their state included the mean arrival rate of HTTP requests and the number of currently assigned servers. The same approach has also been successfully applied to the power management of web servers [13].It is noteworthy that their solution relies on a hybrid approach where initial values are improved by a parametric model. Our work is similarly based on arrival rates of certain events, but shifts the learning of decisions at the global state and server level to decisions per request and request level. RL has also been used for the automatic configuration of virtual feeder X.14 or shared feeder [15]."}, {"heading": "3 Problem overview", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Estimating cache expirations", "text": "We look at the problem of learning parameters for cloud database services at one granularity per request. For each request, the database server can set response parameters that affect client performance. Multiple clients (e.g. mobile devices) can query and update the same entries in a single database. In this essay, we deal with the problem of estimating time-to-live (TTL) cache expiration times for dynamically changing query results that we are introducing now. A query q executed by a client is executed by a database and returns a series of result records of varying cardinality n, identified by their unique keys k1,.., kn. Query results can be compared for a specified time interval t = TTL in server-driven caches such as Content Delivery Networks (CDNs) or reverse proxy caches. When a key k is updated, all cache requests are returned to TS for a specified time interval t = TTL in server-driven caches such as content delivery networks (CDNs) or reverse proxy caches."}, {"heading": "3.2 Simulation environment", "text": "We have implemented a Monte Carlo simulation inspired by the Yahoo! Cloud Serving Benchmark (YCSB). YCSB is a benchmark suite for cloud databases and defines a number of typical web workloads (e.g. read-dominant, scan-intensive, write-dominant). Our implementation provides the same workforce as the requirement, the number of workers and the number of workers that together form multiple layers (clients, caches, databases). YCSB provides throughput and latency histograms. Our implementation offers the same workforce, but instead of providing a client interface and workforce, we stack multiple layers (clients, caches, databases) together and replicate a web connection semantics. That is, YCSB operates on a synchronous thread-per-request database."}, {"heading": "4 Estimating TTLs", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 True TTL", "text": "We start by looking at a hypothetically optimal strategy. Ideally, TTLs expire just before an update invalidates the respective cached result. We define the true TTL as the interval between serving the query and the query result invalid by writing w. In our simulation, therefore, we can capture the optimal action for each step after validating the respective query. As this does not capture true TTLs for queries that expire from the cache without validation, we measure the \"theoretical\" true TTL for queries that are not currently cached by evaluating which queries would be invalid if they had not expired."}, {"heading": "4.2 Baseline solution", "text": "For a Poisson process, the arrival times of events have an exponential cumulative distribution function (CDF), i.e. each of the identically and independently distributed random variables Xi has the cumulative density F (x; \u03bb) = 1 \u2212 e (\u2212 \u03bbx) for x \u2265 0 and means 1 / \u03bb. Currently, we assume that for each database record, an estimate of the rate of incoming write occurs over a certain period of time. Then, the result set of a query of cardinality n can be regarded as a series of independent exponentially distributed random variables Xi... Xn with different write rates \u03bbw1,...,... we expect the TTL to be applied to any element of the result set for the next update, i.e. that Xmin = XII value is given."}, {"heading": "4.3 TTL estimation with NAFs", "text": "TTL estimation is an appealing problem for reinforcement learning solutions, as it provides a natural way to deal with time-dependent and noisy feedback loops in control problems. We move on to modelling the TTL estimation problem using NAFs. First, the previous solution does not distinguish between requests that are frequently read and queries that are very rarely requested, i.e. it does not take into account cache error rates. Second, the basic solution cannot handle scant information in the state well: For most objects, write rate information might not be available as no (or often partial) information is needed to fall back on defaults. We use individual data sets to learn TTLs for query results. This is preferable to use a coding of a query itself as a state, as many equivalent query strings lead to the same result. Using record-level metrics allows for easier overlapping of general query sets and results."}, {"heading": "4.4 Delayed experience injection", "text": "In standard RL semantics, the agent moves sequentially through a Markov decision-making process, taking steps and recording transitions of status, action, reward and next-state. Thus, when using a replay memory, learning is decoupled from the current state and actions by scanning transitions from memory to perform a mini-batch gradient descent. Thus, if the desired runtime measurements for reward and next-state are not immediately available and the agent has to deal with many simultaneous requests, the application must track the \"invaluable\" transitions and decide when to complete them. Algorithm 1 Asynchronous NAF with delayed experience injection. Starting point of empty replay memory memory memory R\u2190 Initialize Q network Q (s, \u00b5 | Q) with random weights that initialize the target network Q 'with the number of weights QE-Q'."}, {"heading": "5 Evaluation", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 Setup", "text": "We implemented our simulator in Java 8 (for YCSB compatibility) and used deeplearning4j [28] (0.5.0) for NAF implementation. We set 10 clients with 6 simultaneous connections each to execute a combined target throughput of 1,000 (asynchronous) operations per second. They accessed 10,000 documents with 1,000 unique queries under different workloads. Updates and queries were extracted from a Zipfian distribution (Zipfian constant 0.6). Each workload was executed for 30 minutes on a commodity 4 core desktop machine and the results were averaged over five passes. Query result sizes were set to lie between [1, 20] documents by selecting sampling ranges from mN (10.5) (resp. N (5, 2)."}, {"heading": "5.2 Results", "text": "This year, it is more than ever before in the history of the country in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is not a country, but in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country and in which it is a country, in which it is a country and in which it is a country."}, {"heading": "6 Conclusion and future work", "text": "We have introduced the concept of delayed experience injection to capture asynchronous reward / next-state semantics in concurrent environments where relevant metrics are available later on. The key idea of our work is that rather than learning global parameters from global metrics, DRL can facilitate decisions per requirement based on fine-grained metrics. Results show that our NAF-based approach can surpass a statistical estimate of the TTL estimation problem by using available runtime information. In future work, we will explore in more detail how dynamic reward adjustments can be set in relation to specific service-level objectives in non-stationary environments. We have also made the simplistic assumption that a single node backend receives all incoming requests, so future work in this area will need to examine coordination and distributed learning in infrastructures where each node observes only part of the environment rather than observing each node individually."}, {"heading": "Acknowledgements", "text": "This work was supported by the EPSRC (Scholarship Reference EP / M508007 / 1) and a Computer Laboratory Premium Scholarship (Sansom Scholarship)."}], "references": [{"title": "Playing atari with deep reinforcement learning", "author": ["Volodymyr Mnih", "Koray Kavukcuoglu", "David Silver", "Alex Graves", "Ioannis Antonoglou", "Daan Wierstra", "Martin Riedmiller"], "venue": "arXiv preprint arXiv:1312.5602,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2013}, {"title": "Human-level control through deep reinforcement learning", "author": ["Volodymyr Mnih", "Koray Kavukcuoglu", "David Silver", "Andrei A Rusu", "Joel Veness", "Marc G Bellemare", "Alex Graves", "Martin Riedmiller", "Andreas K Fidjeland", "Georg Ostrovski"], "venue": "Nature, 518(7540):529\u2013533,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2015}, {"title": "Deep Reinforcement Learning with Double Q-learning", "author": ["H. van Hasselt", "A. Guez", "D. Silver"], "venue": "ArXiv e-prints,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2015}, {"title": "Continuous deep q-learning with model-based acceleration", "author": ["Shixiang Gu", "Timothy Lillicrap", "Ilya Sutskever", "Sergey Levine"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2016}, {"title": "Reinforcement learning: An introduction, volume 1", "author": ["Richard S Sutton", "Andrew G Barto"], "venue": "MIT press Cambridge,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1998}, {"title": "Continuous control with deep reinforcement learning", "author": ["Timothy P. Lillicrap", "Jonathan J. Hunt", "Alexander Pritzel", "Nicolas Heess", "Tom Erez", "Yuval Tassa", "David Silver", "Daan Wierstra"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2015}, {"title": "Utility-function-driven resource allocation in autonomic systems", "author": ["G. Tesauro", "R. Das", "W.E. Walsh", "J.O. Kephart"], "venue": "In Autonomic Computing,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2005}, {"title": "A hybrid reinforcement learning approach to autonomic resource allocation", "author": ["G. Tesauro", "N.K. Jong", "R. Das", "M.N. Bennani"], "venue": "In Proceedings of the 2006 IEEE International Conference on Autonomic Computing,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2006}, {"title": "Online resource allocation using decompositional reinforcement learning", "author": ["Gerald Tesauro"], "venue": "In AAAI,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2005}, {"title": "Managing power consumption and performance of computing systems using reinforcement learning", "author": ["Gerald Tesauro", "Rajarshi Das", "Hoi Chan", "Jeffrey Kephart", "David Levine", "Freeman Rawson", "Charles Lefurgy"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2007}, {"title": "A distributed self-learning approach for elastic provisioning of virtualized cloud resources", "author": ["Jia Rao", "Xiangping Bu", "Cheng-Zhong Xu", "Kun Wang"], "venue": "In Modeling, Analysis & Simulation of Computer and Telecommunication Systems (MASCOTS),", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2011}, {"title": "Url: A unified reinforcement learning approach for autonomic cloud management", "author": ["Cheng-Zhong Xu", "Jia Rao", "Xiangping Bu"], "venue": "Journal of Parallel and Distributed Computing,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2012}, {"title": "Enabling dynamic content caching for database-driven web sites", "author": ["K. Sel\u00e7uk Candan", "Wen-Syan Li", "Qiong Luo", "Wang-Pin Hsiung", "Divyakant Agrawal"], "venue": "In SIGMOD,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2001}, {"title": "Experiences with coralcdn: A five-year operational view", "author": ["Michael J. Freedman"], "venue": "Proc NSDI,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2010}, {"title": "An analysis of facebook photo caching", "author": ["Qi Huang", "Ken Birman", "Robbert van Renesse", "Wyatt Lloyd", "Sanjeev Kumar", "Harry C. Li"], "venue": "In Proceedings of the Twenty-Fourth ACM Symposium on Operating Systems Principles,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2013}, {"title": "Asynchronous methods for deep reinforcement learning", "author": ["Volodymyr Mnih", "Adri\u00e0 Puigdom\u00e8nech Badia", "Mehdi Mirza", "Alex Graves", "Timothy P. Lillicrap", "Tim Harley", "David Silver", "Koray Kavukcuoglu"], "venue": null, "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2016}, {"title": "Massively parallel methods for deep reinforcement learning", "author": ["Arun Nair", "Praveen Srinivasan", "Sam Blackwell", "Cagdas Alcicek", "Rory Fearon", "Alessandro De Maria", "Vedavyas Panneershelvam", "Mustafa Suleyman", "Charles Beattie", "Stig Petersen"], "venue": "arXiv preprint arXiv:1507.04296,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2015}, {"title": "Deep Reinforcement Learning for Robotic Manipulation", "author": ["S. Gu", "E. Holly", "T. Lillicrap", "S. Levine"], "venue": "ArXiv e-prints,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2016}, {"title": "The cache sketch: Revisiting expiration-based caching in the age of cloud data management", "author": ["Felix Gessert", "Michael Schaarschmidt", "Wolfram Wingerath", "Steffen Friedrich", "Norbert Ritter"], "venue": "BTW \u201915,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2015}, {"title": "High performance browser networking", "author": ["Ilya Grigorik"], "venue": "O\u2019Reilly Media, [S.l.],", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2013}, {"title": "Benchmarking cloud serving systems with ycsb", "author": ["Brian F. Cooper", "Adam Silberstein", "Erwin Tam", "Raghu Ramakrishnan", "Russell Sears"], "venue": "In Proceedings of the 1st ACM Symposium on Cloud Computing,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2010}, {"title": "Ycsb++: Benchmarking and performance debugging advanced features in scalable table stores", "author": ["Swapnil Patil", "Milo Polte", "Kai Ren", "Wittawat Tantisiriroj", "Lin Xiao", "Julio L\u00f3pez", "Garth Gibson", "Adam Fuchs", "Billie Rinaldi"], "venue": "In Proceedings of the 2Nd ACM Symposium on Cloud Computing,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2011}, {"title": "Web caching and zipf-like distributions: evidence and implications", "author": ["L. Breslau", "Pei Cao", "Li Fan", "G. Phillips", "S. Shenker"], "venue": "In INFOCOM \u201999. Eighteenth Annual Joint Conference of the IEEE Computer and Communications Societies. Proceedings. IEEE,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 1999}, {"title": "Learning from Delayed Rewards", "author": [], "venue": "PhD thesis, King\u2019s College,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 1989}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba"], "venue": null, "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2014}], "referenceMentions": [{"referenceID": 0, "context": "In recent years, reinforcement learning (RL) algorithms have been successfully combined with deep neural networks as function approximators [1, 2, 3].", "startOffset": 140, "endOffset": 149}, {"referenceID": 1, "context": "In recent years, reinforcement learning (RL) algorithms have been successfully combined with deep neural networks as function approximators [1, 2, 3].", "startOffset": 140, "endOffset": 149}, {"referenceID": 2, "context": "In recent years, reinforcement learning (RL) algorithms have been successfully combined with deep neural networks as function approximators [1, 2, 3].", "startOffset": 140, "endOffset": 149}, {"referenceID": 3, "context": "Specifically, we modify normalized advantage functions [4], a recently introduced method for continuous deep reinforcement learning, to learn optimal cache expiration durations for dynamically changing query results.", "startOffset": 55, "endOffset": 58}, {"referenceID": 4, "context": "The parameter learning problem conforms to the setting of an infinite-horizon discounted Markov decision process where an agent interacts with an environment described by states s \u2208 S and aims to learn a policy \u03c0 that governs which action a \u2208 A to take in each state [5].", "startOffset": 267, "endOffset": 270}, {"referenceID": 1, "context": "have demonstrated how deep neural networks can be used as value functions for a variety of complex tasks by utilising a replay memory of stored experiences, and using a second value function to stabilize learning (fixed Q-target) [2].", "startOffset": 230, "endOffset": 233}, {"referenceID": 3, "context": "In this work, we utilize normalized advantage functions (NAFs), which have recently been suggested as an effective method for continuous DRL [4].", "startOffset": 141, "endOffset": 144}, {"referenceID": 5, "context": "deep deterministic policy gradients [7]), NAFs avoid the use of a second actor or policy network that needs to be trained separately.", "startOffset": 36, "endOffset": 39}, {"referenceID": 6, "context": "\u2019s work on resource allocation in data centres [10, 11, 12].", "startOffset": 47, "endOffset": 59}, {"referenceID": 7, "context": "\u2019s work on resource allocation in data centres [10, 11, 12].", "startOffset": 47, "endOffset": 59}, {"referenceID": 8, "context": "\u2019s work on resource allocation in data centres [10, 11, 12].", "startOffset": 47, "endOffset": 59}, {"referenceID": 9, "context": "The same approach has also been successfully applied to power management of web servers [13].", "startOffset": 88, "endOffset": 92}, {"referenceID": 10, "context": "RL has also been employed for auto-configuration of Xen virtual machines [14, 15].", "startOffset": 73, "endOffset": 81}, {"referenceID": 11, "context": "RL has also been employed for auto-configuration of Xen virtual machines [14, 15].", "startOffset": 73, "endOffset": 81}, {"referenceID": 12, "context": "initially explored the notion of invalidation-based caching for web content [16], as opposed to treating web caches as static content stores or media distribution servers [17, 18].", "startOffset": 76, "endOffset": 80}, {"referenceID": 13, "context": "initially explored the notion of invalidation-based caching for web content [16], as opposed to treating web caches as static content stores or media distribution servers [17, 18].", "startOffset": 171, "endOffset": 179}, {"referenceID": 14, "context": "initially explored the notion of invalidation-based caching for web content [16], as opposed to treating web caches as static content stores or media distribution servers [17, 18].", "startOffset": 171, "endOffset": 179}, {"referenceID": 15, "context": "Prior approaches on thread-parallel or distributed DRL such as A3C [19] or Gorila [20] accelerate training by having learners operate on separate copies of single-threaded environments (e.", "startOffset": 67, "endOffset": 71}, {"referenceID": 16, "context": "Prior approaches on thread-parallel or distributed DRL such as A3C [19] or Gorila [20] accelerate training by having learners operate on separate copies of single-threaded environments (e.", "startOffset": 82, "endOffset": 86}, {"referenceID": 17, "context": "have also recently applied distributed asynchronous NAFs to shared learning of 3D robot manipulation tasks [21].", "startOffset": 107, "endOffset": 111}, {"referenceID": 18, "context": "Further, every invalidation creates the potential for stale reads, as clients can retrieve stale cached results while the invalidation is propagated to all cache edges [22].", "startOffset": 168, "endOffset": 172}, {"referenceID": 19, "context": "In contrast, small TTLs increase client latencies significantly if the database server is physically remote since web performance is primarily governed by round-trip latency [23].", "startOffset": 174, "endOffset": 178}, {"referenceID": 20, "context": "We have implemented a Monte Carlo simulation inspired by the Yahoo! cloud serving benchmark (YCSB) [24, 25].", "startOffset": 99, "endOffset": 107}, {"referenceID": 21, "context": "We have implemented a Monte Carlo simulation inspired by the Yahoo! cloud serving benchmark (YCSB) [24, 25].", "startOffset": 99, "endOffset": 107}, {"referenceID": 18, "context": "If another client requests a cached entry before an invalidation has been completed, a stale read occurs [22].", "startOffset": 105, "endOffset": 109}, {"referenceID": 22, "context": "We note that we expect most queries not to be invalidated frequently (or at all) due to the power-law nature of web workloads [26].", "startOffset": 126, "endOffset": 130}, {"referenceID": 17, "context": "Hence, NAF-DEI also solves a different problem than the recently introduced distributed asynchronous NAF [21], where multiple controllers sequentially collect experiences without delay in the experience computation itself.", "startOffset": 105, "endOffset": 109}, {"referenceID": 23, "context": "Further, the difference between DEI and the standard delayed reward assignment problem [27] is that DEI deals with concurrent delayed credit assignment.", "startOffset": 87, "endOffset": 91}, {"referenceID": 0, "context": "Query result sizes were set to be between [1, 20] documents by sampling scan ranges fromN (10, 5) (resp.", "startOffset": 42, "endOffset": 49}, {"referenceID": 16, "context": "Query result sizes were set to be between [1, 20] documents by sampling scan ranges fromN (10, 5) (resp.", "startOffset": 42, "endOffset": 49}, {"referenceID": 24, "context": "Updates were performed using an Adam [29]", "startOffset": 37, "endOffset": 41}, {"referenceID": 22, "context": "Due to the Zipf nature of web workloads [26], most queries and updates will concentrate on a small sets of \"hot\" database records for which it might be feasible to track runtime information and use them for specific predictions.", "startOffset": 40, "endOffset": 44}], "year": 2016, "abstractText": "Learning effective configurations in computer systems without hand-crafting models for every parameter is a long-standing problem. This paper investigates the use of deep reinforcement learning for runtime parameters of cloud databases under latency constraints. Cloud services serve up to thousands of concurrent requests per second and can adjust critical parameters by leveraging performance metrics. In this work, we use continuous deep reinforcement learning to learn optimal cache expirations for HTTP caching in content delivery networks. To this end, we introduce a technique for asynchronous experience management called delayed experience injection, which facilitates delayed reward and next-state computation in concurrent environments where measurements are not immediately available. Evaluation results show that our approach based on normalized advantage functions and asynchronous CPU-only training outperforms a statistical estimator.", "creator": "LaTeX with hyperref package"}}}