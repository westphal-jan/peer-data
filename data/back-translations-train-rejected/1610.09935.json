{"id": "1610.09935", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-Oct-2016", "title": "Knowledge Questions from Knowledge Graphs", "abstract": "We address the novel problem of automatically generating quiz-style knowledge questions from a knowledge graph such as DBpedia. Questions of this kind have ample applications, for instance, to educate users about or to evaluate their knowledge in a specific domain. To solve the problem, we propose an end-to-end approach. The approach first selects a named entity from the knowledge graph as an answer. It then generates a structured triple-pattern query, which yields the answer as its sole result. If a multiple-choice question is desired, the approach selects alternative answer options. Finally, our approach uses a template-based method to verbalize the structured query and yield a natural language question. A key challenge is estimating how difficult the generated question is to human users. To do this, we make use of historical data from the Jeopardy! quiz show and a semantically annotated Web-scale document collection, engineer suitable features, and train a logistic regression classifier to predict question difficulty. Experiments demonstrate the viability of our overall approach.", "histories": [["v1", "Mon, 31 Oct 2016 14:27:07 GMT  (440kb,D)", "http://arxiv.org/abs/1610.09935v1", null], ["v2", "Tue, 1 Nov 2016 05:39:25 GMT  (382kb,D)", "http://arxiv.org/abs/1610.09935v2", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["dominic seyler", "mohamed yahya", "klaus berberich"], "accepted": false, "id": "1610.09935"}, "pdf": {"name": "1610.09935.pdf", "metadata": {"source": "CRF", "title": "Knowledge Questions from Knowledge Graphs", "authors": ["Dominic Seyler", "Mohamed Yahya", "Klaus Berberich"], "emails": ["dseyler2@illinois.edu", "myahya@mpi-inf.mpg.de", "klaus.berberich@htwsaar.de"], "sections": [{"heading": "1. INTRODUCTION", "text": "The question is also that Barack Obama is married to MichelleObama and was born on August 4, 1961. Textual knowledge captures how named entities and their relationships are designated in natural language, for example, Barack H. Obama. \"Easily extensible data formats such as RDF are often used to store KGs, making it easy to complete them with additional facts without having to worry about a predefined scheme. RDF stores facts as (subject, predicate, object) triples, which can then be used with SPARQL as a simple but powerful structured query language."}, {"heading": "2. PRELIMINARIES AND PROBLEM STATEMENT", "text": "This year, it has come to the point that it has never come as far as it has this year."}, {"heading": "3. QUERY GENERATION", "text": "The first step in our pipeline is to create a query that has a unique answer in the KG. This query serves as a basis for generating a question that is shown to human participants, and the unique answer will be the one that a participant must provide in order to correctly answer the question. As is customary in quizzes to ensure that a question has a single answer, it simplifies the verification of the answer. Entering into the query generation step is a topic T. The unique answer to the generated query will be a unit that is randomly drawn from the KG. The query generation is guided by the following desiderata: i) The query should contain at least a triple pattern, which is crucial when the query is verbalized to generate a question (e.g. \"Which President.\"), and ii) the units mentioned in the query should not give any obvious clues about the answer identity. Subsequently, we present the challenges of achieving these and our solutions to each of these challenges."}, {"heading": "3.1 Answer Type Selection", "text": "The question of entities always requires a type that is either implicit (e.g. \"who\" for the person and \"where\" for the location) or explicit (e.g. \"which president.\") Here we address the problem of selecting a type to refer to the answer entity in the question. Other types tend to contain a large number of types, and associate an entity with several types. Such types are easy for the average person to understand and typically appear in a text that talks about an entity (e.g. president, lawyer). However, other types are artefacts of attempts to have an ontologically complete and formally solid type system. Such types are only in the context of a type, but not on their own (e.g. the entity or thing)."}, {"heading": "3.2 Triple Pattern Generation", "text": "We now have an answer unit e and one of its semantic types, which are used to refer to e in the question. We now need to create a query (which includes the type constraint t), the unique answer of which is about kg e. We are concentrating here on questions with unknown units, since these are the ones we can use Jeopardy! data to train our difficulty level to [17]. In principle, we can also allow unknown relationships or types if we have the right training data. To create a query is to select facts where e is either the subject or the object, and to turn them into triple patterns by replacing e with a variable (? x). Not all facts can be used here, as some will reveal too much about the answer and make the question too trivial. Other facts will be redundant given the facts that are already being used. Elimination of the text overlap with the answer, we overlap the first constraint we have on a fact that appears to be that the surface forms of the question do not overlap with the surface shapes of the question."}, {"heading": "4. DIFFICULTY ESTIMATION", "text": "We now describe our approach to estimating the difficulty of replying to the request for knowledge generated in Section 3. There are several, seemingly contradictory signals that affect the difficulty of a question. As already discussed, you might expect that any question posed after a popular instance like BarackObama is easy. If, however, we ask: \"This president from Illinois has won a Grammy Award,\" think probably only a few people think of BarackObama. We use a classification model that is trained on a body of questions, paired with their difficulties, to predict the difficulty of the question. Note, however, that the difficulty is based on the request and not on their verbalization, which we generate in the next section. Our goal here is to create questions that measure the actual knowledge and not the linguistic skills. We pass this point on in Section 5.Because we rely on supervised training for the difficulty assessment, $, $, $, $, $, $, $, $, $, $, $, $, $, $, $, $, $, $, $, $, $, $, $, $, $, $, $, $, $, $, 500, $, $, $, $, $, $, 500, $, $, $, $, $, $, $, $, 500, $, $, $, $, $, $, $, $, $, $, $, $, $, $, $, $, $, $, $, $, $, $, $, $, $, $, $, $, $, $, $, $, $, $, $, $, $, $, $, $, $, $, $, $, $, $, $, $, $, $, $, $, $, $, $, $, and, $, $, $, $, $, $, $, $, $, $, $, $, $, $, $, $, $, $, $, $, $, $, $, $, $, and, $, $, $, $,"}, {"heading": "4.1 Data Preparation", "text": "The larger goal is to estimate the difficulty of answering questions generated from a knowledge diagram, so we limit ourselves to a subset of Jeopardy! questions that can be answered by Yago [42], with all entities mentioned in the question being in Yago, and all relationships linking these entities are generic enough to apply to a different Jeopardy! / Yago. We say that a question can be answered in the form of Yago if i) all entities mentioned in the question and their answers are in Yago, and ii) all relationships linking these entities are covered by Yago, we will automatically annotate the questions with Yago entities that the Stanford CoreNLP entity recognizes (NER) [18] in conjunction with the AIDA tool for named entities disambiguations."}, {"heading": "4.2 Difficulty Classifier", "text": "In fact, most of us are able to go in search of a solution."}, {"heading": "5. QUERY VERBALIZATION", "text": "We now turn to the problem of verbalizing queries, turning a query constructed in Section 3 into a question of natural language. A person can digest this question without the technical knowledge required to understand a triple query. Our ultimate goal is to construct well-shaped questions that are easy to intelligible.The goal of our questions is to test expertise, as opposed to linguistic proficiency, and the way in which a question is formulated is not a factor in predicting its difficulty. This guides our approach to verbalizing queries, ensuring consistency in the formulation of questions.We rely on a handmade verbalizing template and automatically generated encyclopaedias to turn a query into a question. The verbalizing template determines where the various components of the query appear in the question. The lexicon serves as a bridge between entries in knowledge graphs and natural language. We start with the description of our template, and then move on to our lexicon-making process."}, {"heading": "5.1 Verbalization Template", "text": "Our approach to verbalizing queries is based on templates. Such approaches are standard in the literature for creating natural language [26, 34]. We adopt a template inspired by the Jeopardy! quiz game show shown in Figure 3. Most of the work is done in the verbalize.Input: Query, Q = {q1,..., qn} Qtype: = {qi, Q | has predicate type} = {qt1,.., qtm} Qinstance: = Q\\ Qtype = {qi1,., qil} This verbalize (qt1),..., and verbalize (qtm) verbalize (qi1),.., and verbalize (qil). Figure 3: Verbalization TemplateThe verbalize function takes a triple pattern and produces its verbalization. How this verbalization is executed depends on the nature of the triple pattern. More specifically, there are three different patterns in our object."}, {"heading": "5.2 Verbalization Lexicons", "text": "It is therefore important that we map every semantic element of tophrases that can be used to put it in a natural language chain such as a question.Entities To verbalize entities, we follow the approach of Hoffart et al. [25] and rely on the fact that our entities come from Wikipedia. We resort to Wikipedia to extract surface shapes of our entities. For each entity e, we collect the surface shapes of all links to e-Wikipedia entries. We consider this text as a possible verbalization of e.The above process extracts many specious verbalizations of an entity e. To overcome this problem, we associate the number of times it has been used to link the Wikipedia entry and limit ourselves to the five most common entries that we add to the lexicon for the entry."}, {"heading": "6. MULTIPLE-CHOICE QUESTIONS", "text": "The last component in our question generation framework turns a question into a multiple-choice question. This has several advantages: In general, it is easier to manage a multiple-choice question because the problem of answer checking can be fully mechanized, especially in cases where questions are not managed by a computer, where things like completion suggestions can ensure canonical answers. In general, the use of multiple-choice questions is widespread, as in tests like the GRE.Turning a question into multiple-choice questions requires deflection mechanisms: units that are presented to the user as answer candidates but are actually false answers. Of course, not all units are reasonable deflection mechanisms. A negative example would be units that have nothing to do with the question. Distraction mechanisms should ideally be associated with the right answer unit, but in general, it should be possible to call a distraction mechanism that would replace multiple-choice with this problem."}, {"heading": "6.1 Distractor Generation", "text": "Our starting point for creating deflection maneuvers is the Q = {q1,..., qn} query created in Section 3, which formed the basis of the verbalized question in Section 5. Starting with a query, we have a relatively simple but powerful scheme for generating deflection maneuvers. By removing one or more triple patterns from Q, we obtain a Q \u00b2 Q query that has more than one response unit. All but one of these units are a false answer to Q.The relaxation scheme described above can generate a large number of candidate deflectors, but not all of the relaxes remain close to the original query. If a relaxation deviates too much from Q, the deflection maneuvers obtained become meaningless. We apply two relaxation constraints that are used to generate deflectors: (i) a semantic type restriction and (ii) a distance restriction between the deflection."}, {"heading": "6.2 Distractor Confusability", "text": "All things being equal, a multiple-choice question can be made more or less difficult by the choice of deflectors. However, if one of the deflection mechanisms is highly confusable with the answer unit, the multiple-choice question is easily confused with the answer unit. On the basis of this observation, we consider a deflection mechanism to be confusable if it is likely to be the answer to the original question based on our difficulty model. This means that if an entity is very likely the answer to a question that is directed to a different entity, that entity pair must be similar. We can therefore define the confusion between the answer to the ea question and a deflection mechanism as follows: conf (Q, ea, edist) = 1 \u2212 P (diff (Q, ea) = easy) \u2212 P (diff (Q, edist) = easy) | Since we have more than one deflection mechanism in a multiple-choice question, we can refer to the one mentioned above."}, {"heading": "7. EXPERIMENTAL EVALUATION", "text": "In the following section, we evaluate our approach to the generation of knowledge questions using knowledge graphs. We are conducting two user studies focusing on the evaluation of the difficulty model and our diversion generation framework."}, {"heading": "7.1 Human Assessment of Difficulty", "text": "An important motivation for automating the difficulty assessment of questions is the fact that it is difficult for the average person to judge what constitutes an easy or difficult question. Beinborn et al. [7] has already shown this result for language competence tests, where language teachers have proven to be poor at predicting the difficulty of questions when they take into account the actual performance of students. We would like to see if the same applies to our environment. In order to create fair and informative tests, it is critical that we are able to correctly assess the difficulty of a question. We start with the assumption that the creators of Jeopardy! are good at automatically assessing questions. Evidence of this was discussed in Section 4, where we showed that there is a correlation between the monetary value of a question and the probability that it will be answered incorrectly by Jeopardy! participants. In our experiment, we want to show how well the average person can predict the difficulty of a question."}, {"heading": "7.2 Question Difficulty Classification", "text": "We start by looking at the quality of our scheme for assigning difficulty levels to questions. The scheme is described in Section 4, where the possible difficulty levels D = \"easy, hard.\" We train our logistics regression classifier on 500 Jeopardy! questions, which are commented on in Section 4. Through ten-fold cross-validation, our classifier was able to correctly identify the difficulty levels of questions with an accuracy of 66.4%. In order to get an insight into the meaningfulness of our features, we conducted a feature ablation study in which we looked at the results for all combinations of our features. In this part, we grouped our features into three classes: \u2022 SAL: \"highlighting features\" as in Table 1, with additional log transformation of highlighting values to deal with long-tail entities. \u2022 COH: \"coherence\" features in Table 1. \u2022 TYPE: \"per rough table 1.4-semantic features,\" as all of these features are activated in this series. \""}, {"heading": "7.3 User Study on Difficulty Estimation", "text": "In the following, we will conduct an experiment on how well our classifier matches the relative difficulty rating of humans for questions generated by our system. It is important to note that we are asking people for a relative difficulty rating, as opposed to absolute difficulties, as we have shown in Section 7.1 that humans are not very competent in judging absolute difficulties. For the user study, we sampled a group of 50 entities with at least 5 non-typical facts at Yago. For each entity, we created a series of three questions and compared them with the response unit for human annotators. Annotators were asked these questions according to their relative difficulty and were allowed to skip a series of questions about an entity if they were not familiar with the entity. Then, we compared the correlation between the order given by each of the human annotators and the output of our logistic regression classifier, which we used in the case of \"Inperfection 1,\" in Kendall's case. \""}, {"heading": "7.4 Distractors Confusability", "text": "We now turn to evaluating the generation of deflectors for multiple choice questions. Our goal is to accurately predict the confusion of a deflector based on the correct answer to a question. In Section 6.2, we presented our scheme for quantifying the confusion of deflectors and how it fits into a multiple choice question. We hereby evaluate our approach. For this experiment, we automatically generate 10,000 multiple choice questions. Each question has three possible answers, which are the correct answer and two possible distractions. Subsequently, we limited ourselves to 400 multiple choice questions, the deflection pair of which makes the greatest difference in confusion. This was done to maximize the likelihood that study participants will actually be able to distinguish the more confusing from the less confusing distractions."}, {"heading": "8. RELATED WORK", "text": "The generation of language proficiency tests has been addressed in several papers [21, 32, 35]. Here the focus is on generating cloze (fill-in-the-blank) tests. Beinborn et al. [7] presents an approach to predicting the difficulty of answering such questions with multiple blanks, where SVMs are trained on four classes of characteristics that consider individual blanks, their candidate answers, their dependence on other blanks, and the general difficulty of the question. Asking Generation for Reading Comprehension is aimed at evaluating knowledge from text corpora. These include general Wikipedia knowledge [8, 24] and specialized areas such as medical texts [2, 46]. While the above papers focus on the generalization of a question from a single document, Questimator [22] is generalized multiple-choice questions from the textual Wikipedia corpus taking into account several documents that relate to a single topic in order to produce a work in that area."}, {"heading": "9. CONCLUSION", "text": "We proposed a holistic approach to the novel problem of generating quiz-style knowledge questions from knowledge diagrams. Our approach addresses the challenges associated with this problem, especially assessing the difficulty of generated questions. To this end, we develop appropriate features and train a model of questionability based on historical data from the Jeopardy! quiz show that has been proven to outperform humans in this difficult task. A working prototype implementing our approach is available at: https: / / gate.d5.mpi-inf.mpg.de / q2g"}, {"heading": "10. REFERENCES", "text": "[1] J Archive. http: / / j-archive.com. [2] M. Agarwal and P. Mannem. Automatic gap-fill question generation from text books. In BEA, 2011. [3] T. Alsubait et al. Generating multiple choice questions from ontologies: Lessons learnt. In OWLED, 2014. [4] S. Auer et al. DBpedia: A Nucleus for a Web of Open Data. In ISWC / ASWC, 2007. [5] H. Bast et al. Semantic Search on Text and Knowledge Bases. Foundations and Trends in IR, 10 (2-3). Educational."}], "references": [{"title": "Automatic gap-fill question generation from text books", "author": ["M. Agarwal", "P. Mannem"], "venue": "BEA", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2011}, {"title": "Generating multiple choice questions from ontologies: Lessons learnt", "author": ["T. Alsubait"], "venue": "In OWLED,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2014}, {"title": "DBpedia: A Nucleus for a Web of Open Data", "author": ["S. Auer"], "venue": "In ISWC/ASWC,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2007}, {"title": "Semantic Search on Text and Knowledge Bases", "author": ["H. Bast"], "venue": "Foundations and Trends in IR,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2016}, {"title": "More Accurate Question Answering on Freebase", "author": ["H. Bast", "E. Haussmann"], "venue": "CIKM", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2015}, {"title": "Predicting the Difficulty of Language Proficiency", "author": ["L. Beinborn"], "venue": "Tests. TACL,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2014}, {"title": "Automatic generation of multiple choice questions using wikipedia", "author": ["A.S. Bhatia"], "venue": "In PReMI,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2013}, {"title": "Effective and efficient entity search in RDF data", "author": ["R. Blanco"], "venue": "In ISWC,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2011}, {"title": "Freebase: a Collaboratively Created Graph Database for Structuring Human Knowledge", "author": ["K.D. Bollacker"], "venue": "In SIGMOD,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2008}, {"title": "Estimating the Query Difficulty for Information Retrieval", "author": ["D. Carmel", "E. Yom-Tov"], "venue": "Morgan & Claypool Publishers", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2010}, {"title": "A Coefficient of Agreement for Nominal Scales", "author": ["J. Cohen"], "venue": "Educational and Psychological Measurement, 20(1):37", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1960}, {"title": "KBQA: an Online Template Based Question Answering System over Freebase", "author": ["W. Cui"], "venue": "In IJCAI,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2016}, {"title": "Spartiqulation \u2013 Verbalizing SPARQL Queries", "author": ["B. Ell"], "venue": "In ILD Workshop,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2012}, {"title": "editor", "author": ["C. Fellbaum"], "venue": "WordNet: an Electronic Lexical Database. MIT Press", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1998}, {"title": "Introduction to \u201dthis is watson", "author": ["D.A. Ferrucci"], "venue": "IBM Journal of Research and Development", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2012}, {"title": "Building Watson: An Overview of the DeepQA Project", "author": ["D.A. Ferrucci"], "venue": "AI Magazine,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2010}, {"title": "Incorporating Non-local Information into Information Extraction Systems by Gibbs Sampling", "author": ["J.R. Finkel"], "venue": "In ACL,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2005}, {"title": "Measuring Nominal Scale Agreement among Many Raters", "author": ["J.L. Fleiss"], "venue": "Psychological Bulletin", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1971}, {"title": "FACC1: Freebase annotation of ClueWeb", "author": ["E. Gabrilovich"], "venue": "corpora, Version", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2013}, {"title": "How to Generate Cloze Questions from Definitions: A Syntactic Approach", "author": ["D.M. Gates"], "venue": "AAAI", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2011}, {"title": "Questimator: Generating Knowledge Assessments for Arbitrary Topics", "author": ["Q. Guo"], "venue": "In IJCAI,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2016}, {"title": "Automatic Acquisition of Hyponyms from Large Text Corpora", "author": ["M.A. Hearst"], "venue": "COLING", "citeRegEx": "23", "shortCiteRegEx": null, "year": 1992}, {"title": "Question Generation via Overgenerating Transformations and Ranking", "author": ["M. Heilman", "N.A. Smith"], "venue": "Technical report", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2009}, {"title": "Robust Disambiguation of Named Entities in Text", "author": ["J. Hoffart"], "venue": "In EMNLP,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2011}, {"title": "editors", "author": ["N. Indurkhya", "F.J. Damerau"], "venue": "Handbook of Natural Language Processing. Chapman and Hall/CRC", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2010}, {"title": "Explaining Structured Queries in Natural Language", "author": ["G. Koutrika"], "venue": "In ICDE,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2010}, {"title": "The Measurement of Observer Agreement for Categorical Data", "author": ["J.R. Landis", "G.G. Koch"], "venue": "Biometrics, Vol. 33", "citeRegEx": "28", "shortCiteRegEx": null, "year": 1977}, {"title": "Question difficulty estimation in community question answering services", "author": ["J. Liu"], "venue": "In EMNLP,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2013}, {"title": "Scaling up question-answering to linked data", "author": ["V. L\u00f3pez"], "venue": "In EKAW,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2010}, {"title": "Distant supervision for relation extraction without labeled data", "author": ["M. Mintz"], "venue": "In ACL,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2009}, {"title": "Automatic Cloze-Questions Generation", "author": ["A. Narendra"], "venue": "In RANLP,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2013}, {"title": "Sorry, I Don\u2019T Speak SPARQL: Translating SPARQL Queries into Natural Language", "author": ["A.-C. Ngonga Ngomo"], "venue": "In WWW,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2013}, {"title": "Building Natural Language Generation Systems", "author": ["E. Reiter", "R. Dale"], "venue": "Cambridge University Press", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2000}, {"title": "Discriminative Approach to Fill-in-the-Blank Quiz Generation for Language Learners", "author": ["K. Sakaguchi"], "venue": "In ACL,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2013}, {"title": "When a knowledge base is not enough: Question answering over knowledge bases with external text data", "author": ["D. Savenkov", "E. Agichtein"], "venue": "SIGIR", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2016}, {"title": "Generating factoid questions with recurrent neural networks: The 30m factoid question-answer corpus", "author": ["I.V. Serban"], "venue": "In ACL,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2016}, {"title": "Generating quiz questions from knowledge graphs", "author": ["D. Seyler"], "venue": "In WWW,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2015}, {"title": "Automated question generation for quality control in human computation tasks", "author": ["D. Seyler"], "venue": "In WebSci,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2016}, {"title": "Question answering on interlinked data", "author": ["S. Shekarpour"], "venue": "In WWW,", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2013}, {"title": "Domain-specific question generation from a knowledge base", "author": ["L. Song", "L. Zhao"], "venue": "arXiv", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2016}, {"title": "Yago: A Core of Semantic Knowledge", "author": ["F.M. Suchanek"], "venue": "In WWW,", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2007}, {"title": "Yago2s: Modular high-quality information extraction with an application to flight planning", "author": ["F.M. Suchanek"], "venue": "In BTW,", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2013}, {"title": "Template-based question answering over RDF data", "author": ["C. Unger"], "venue": "In WWW,", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2012}, {"title": "A regularized competition model for question difficulty estimation in community question answering services", "author": ["Q. Wang"], "venue": "In EMNLP,", "citeRegEx": "45", "shortCiteRegEx": "45", "year": 2014}, {"title": "Automatic question generation for learning evaluation in medicine", "author": ["W. Wang"], "venue": "In ICWL,", "citeRegEx": "46", "shortCiteRegEx": "46", "year": 2007}, {"title": "What Is the Longest River in the USA? Semantic Parsing for Aggregation Questions", "author": ["K. Xu"], "venue": "In AAAI,", "citeRegEx": "47", "shortCiteRegEx": "47", "year": 2015}, {"title": "Answering Questions with Complex Semantic Constraints on Open Knowledge Bases", "author": ["P. Yin"], "venue": "In CIKM,", "citeRegEx": "48", "shortCiteRegEx": "48", "year": 2015}, {"title": "Learning to estimate query difficulty: including applications to missing content detection and distributed information retrieval", "author": ["E. Yom-Tov"], "venue": "In SIGIR,", "citeRegEx": "49", "shortCiteRegEx": "49", "year": 2005}, {"title": "Natural language question answering over RDF: a graph data driven approach", "author": ["L. Zou"], "venue": "In SIGMOD,", "citeRegEx": "50", "shortCiteRegEx": "50", "year": 2014}], "referenceMentions": [{"referenceID": 40, "context": "Knowledge graphs (KGs) such as YAGO [42] and DBpedia [4] contain facts about real-world named entities.", "startOffset": 36, "endOffset": 40}, {"referenceID": 2, "context": "Knowledge graphs (KGs) such as YAGO [42] and DBpedia [4] contain facts about real-world named entities.", "startOffset": 53, "endOffset": 56}, {"referenceID": 37, "context": "Crowdsourcing is one concrete use case as outlined in [39].", "startOffset": 54, "endOffset": 58}, {"referenceID": 8, "context": "Knowledge Graphs (KGs) such as as Freebase [10], Yago [42], and DBpedia [4] describe entities E (e.", "startOffset": 43, "endOffset": 47}, {"referenceID": 40, "context": "Knowledge Graphs (KGs) such as as Freebase [10], Yago [42], and DBpedia [4] describe entities E (e.", "startOffset": 54, "endOffset": 58}, {"referenceID": 2, "context": "Knowledge Graphs (KGs) such as as Freebase [10], Yago [42], and DBpedia [4] describe entities E (e.", "startOffset": 72, "endOffset": 75}, {"referenceID": 41, "context": "More specifically, we use Yago2s [43] as our reference knowledge graph in this work.", "startOffset": 33, "endOffset": 37}, {"referenceID": 13, "context": "Yago2s is automatically constructed by combining information extraction over Wikipedia infoboxes and categories with the lexical database WordNet [15].", "startOffset": 146, "endOffset": 150}, {"referenceID": 18, "context": "To compute signals necessary for estimating question difficulty, we make use of the ClueWeb09/12 document collections and the FACC annotations provided by Google [20].", "startOffset": 162, "endOffset": 166}, {"referenceID": 21, "context": "several attorneys including BarackObama\u201d These patterns are inspired by Hearst [23].", "startOffset": 79, "endOffset": 83}, {"referenceID": 13, "context": "Relying on the fact that our semantic types are WordNet synsets [15], we use the lexicon that comes with WordNet (e.", "startOffset": 64, "endOffset": 68}, {"referenceID": 15, "context": "We focus here on questions with unknown entities as these are the ones we can use Jeopardy! data to train our difficulty classifier on [17].", "startOffset": 135, "endOffset": 139}, {"referenceID": 40, "context": "The larger goal is to estimate the difficulty of answering queries generated from a knowledge graph, so we restrict ourselves to a subset of the Jeopardy! questions answerable from Yago [42], which we collected as described below.", "startOffset": 186, "endOffset": 190}, {"referenceID": 16, "context": "To find these questions, we automatically annotate the questions with Yago entities using the Stanford CoreNLP named entity recognizer (NER) [18] in conjunction with the AIDA tool for named entity disambiguation [25].", "startOffset": 141, "endOffset": 145}, {"referenceID": 23, "context": "To find these questions, we automatically annotate the questions with Yago entities using the Stanford CoreNLP named entity recognizer (NER) [18] in conjunction with the AIDA tool for named entity disambiguation [25].", "startOffset": 212, "endOffset": 216}, {"referenceID": 24, "context": "Such approaches are standard in the natural language generation literature [26, 34].", "startOffset": 75, "endOffset": 83}, {"referenceID": 32, "context": "Such approaches are standard in the natural language generation literature [26, 34].", "startOffset": 75, "endOffset": 83}, {"referenceID": 23, "context": "[25] and rely on the fact that our entities come from Wikipedia.", "startOffset": 0, "endOffset": 4}, {"referenceID": 29, "context": "Following the distant supervision assumption [31], we hypothesize that \u2018w1.", "startOffset": 45, "endOffset": 49}, {"referenceID": 5, "context": "[7] has already shown this result for language proficiency tests, where language teachers were shown to be bad at predicting the difficulty of questions when considering the actual performance of students.", "startOffset": 0, "endOffset": 3}, {"referenceID": 17, "context": "Table 3 shows the agreement between each pair of human evaluators and the majority vote difficulty assessment using Fleiss\u2019 Kappa [19].", "startOffset": 130, "endOffset": 134}, {"referenceID": 26, "context": "When looking at pairwise agreement between evaluators, it ranges from fair to moderate [28].", "startOffset": 87, "endOffset": 91}, {"referenceID": 10, "context": "521, indicating moderate agreement [12].", "startOffset": 35, "endOffset": 39}, {"referenceID": 19, "context": "The generation of language proficiency tests has been tackled in several works [21, 32, 35].", "startOffset": 79, "endOffset": 91}, {"referenceID": 30, "context": "The generation of language proficiency tests has been tackled in several works [21, 32, 35].", "startOffset": 79, "endOffset": 91}, {"referenceID": 33, "context": "The generation of language proficiency tests has been tackled in several works [21, 32, 35].", "startOffset": 79, "endOffset": 91}, {"referenceID": 5, "context": "[7] presents an approach for predicting the difficulty of answering such questions with multiple blanks using SVMs trained on four classes of features that look at individual blanks, their candidate answers, their dependence on other blanks, and the overall question difficulty.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "This includes including general Wikipedia knowledge [8, 24] and specialized domain such as medical texts [2, 46].", "startOffset": 52, "endOffset": 59}, {"referenceID": 22, "context": "This includes including general Wikipedia knowledge [8, 24] and specialized domain such as medical texts [2, 46].", "startOffset": 52, "endOffset": 59}, {"referenceID": 0, "context": "This includes including general Wikipedia knowledge [8, 24] and specialized domain such as medical texts [2, 46].", "startOffset": 105, "endOffset": 112}, {"referenceID": 44, "context": "This includes including general Wikipedia knowledge [8, 24] and specialized domain such as medical texts [2, 46].", "startOffset": 105, "endOffset": 112}, {"referenceID": 20, "context": "While the above works focus on generating a question from a single document, Questimator [22] generates multiple choice questions from the textual Wikipedia corpus by considering multiple documents related to a single topic to produce a question.", "startOffset": 89, "endOffset": 93}, {"referenceID": 22, "context": "Work in this area has mostly taken the approach of overgeneration and ranking [24, 46].", "startOffset": 78, "endOffset": 86}, {"referenceID": 44, "context": "Work in this area has mostly taken the approach of overgeneration and ranking [24, 46].", "startOffset": 78, "endOffset": 86}, {"referenceID": 1, "context": "Recent work has started to look at the problem of generating questions, including multiple choice ones, from KGs and ontologies [3, 38, 41, 37].", "startOffset": 128, "endOffset": 143}, {"referenceID": 36, "context": "Recent work has started to look at the problem of generating questions, including multiple choice ones, from KGs and ontologies [3, 38, 41, 37].", "startOffset": 128, "endOffset": 143}, {"referenceID": 39, "context": "Recent work has started to look at the problem of generating questions, including multiple choice ones, from KGs and ontologies [3, 38, 41, 37].", "startOffset": 128, "endOffset": 143}, {"referenceID": 35, "context": "Recent work has started to look at the problem of generating questions, including multiple choice ones, from KGs and ontologies [3, 38, 41, 37].", "startOffset": 128, "endOffset": 143}, {"referenceID": 31, "context": "for SPARQL [33, 14], and Koutrika et al.", "startOffset": 11, "endOffset": 19}, {"referenceID": 12, "context": "for SPARQL [33, 14], and Koutrika et al.", "startOffset": 11, "endOffset": 19}, {"referenceID": 25, "context": "for SQL [27], with a focus on usability.", "startOffset": 8, "endOffset": 12}, {"referenceID": 24, "context": "Similar to our approach, these earlier works take a template-based approach to verbalization, which are very widely used on the natural language generation from logical form such as SPARQL queries [26, 34].", "startOffset": 197, "endOffset": 205}, {"referenceID": 32, "context": "Similar to our approach, these earlier works take a template-based approach to verbalization, which are very widely used on the natural language generation from logical form such as SPARQL queries [26, 34].", "startOffset": 197, "endOffset": 205}, {"referenceID": 7, "context": "Much recent work has focused on keyword search [9] and question answering, rather than generation, from knowledge graphs [6, 13, 30, 40, 44, 47, 50], possibly in combination with textual data [5, 36, 48].", "startOffset": 47, "endOffset": 50}, {"referenceID": 4, "context": "Much recent work has focused on keyword search [9] and question answering, rather than generation, from knowledge graphs [6, 13, 30, 40, 44, 47, 50], possibly in combination with textual data [5, 36, 48].", "startOffset": 121, "endOffset": 148}, {"referenceID": 11, "context": "Much recent work has focused on keyword search [9] and question answering, rather than generation, from knowledge graphs [6, 13, 30, 40, 44, 47, 50], possibly in combination with textual data [5, 36, 48].", "startOffset": 121, "endOffset": 148}, {"referenceID": 28, "context": "Much recent work has focused on keyword search [9] and question answering, rather than generation, from knowledge graphs [6, 13, 30, 40, 44, 47, 50], possibly in combination with textual data [5, 36, 48].", "startOffset": 121, "endOffset": 148}, {"referenceID": 38, "context": "Much recent work has focused on keyword search [9] and question answering, rather than generation, from knowledge graphs [6, 13, 30, 40, 44, 47, 50], possibly in combination with textual data [5, 36, 48].", "startOffset": 121, "endOffset": 148}, {"referenceID": 42, "context": "Much recent work has focused on keyword search [9] and question answering, rather than generation, from knowledge graphs [6, 13, 30, 40, 44, 47, 50], possibly in combination with textual data [5, 36, 48].", "startOffset": 121, "endOffset": 148}, {"referenceID": 45, "context": "Much recent work has focused on keyword search [9] and question answering, rather than generation, from knowledge graphs [6, 13, 30, 40, 44, 47, 50], possibly in combination with textual data [5, 36, 48].", "startOffset": 121, "endOffset": 148}, {"referenceID": 48, "context": "Much recent work has focused on keyword search [9] and question answering, rather than generation, from knowledge graphs [6, 13, 30, 40, 44, 47, 50], possibly in combination with textual data [5, 36, 48].", "startOffset": 121, "endOffset": 148}, {"referenceID": 3, "context": "Much recent work has focused on keyword search [9] and question answering, rather than generation, from knowledge graphs [6, 13, 30, 40, 44, 47, 50], possibly in combination with textual data [5, 36, 48].", "startOffset": 192, "endOffset": 203}, {"referenceID": 34, "context": "Much recent work has focused on keyword search [9] and question answering, rather than generation, from knowledge graphs [6, 13, 30, 40, 44, 47, 50], possibly in combination with textual data [5, 36, 48].", "startOffset": 192, "endOffset": 203}, {"referenceID": 46, "context": "Much recent work has focused on keyword search [9] and question answering, rather than generation, from knowledge graphs [6, 13, 30, 40, 44, 47, 50], possibly in combination with textual data [5, 36, 48].", "startOffset": 192, "endOffset": 203}, {"referenceID": 14, "context": "Of course, question answering has a long history, with one of the major highlights being IBM\u2019s Watson [16], which won the Jeopardy! game show combining both structured and unstructured sources for answering.", "startOffset": 102, "endOffset": 106}, {"referenceID": 27, "context": "This topic has received attention lately in community question answering [29, 45], by using a competition-based approach that tries to capture how much skill a question requires for answering.", "startOffset": 73, "endOffset": 81}, {"referenceID": 43, "context": "This topic has received attention lately in community question answering [29, 45], by using a competition-based approach that tries to capture how much skill a question requires for answering.", "startOffset": 73, "endOffset": 81}, {"referenceID": 9, "context": "There has also been work on estimating query difficulty in the context of information retrieval [11, 49] to learn an estimator that predicts the expected precision of the query by analyzing the overlap between the results of the full query and the results of its sub-queries.", "startOffset": 96, "endOffset": 104}, {"referenceID": 47, "context": "There has also been work on estimating query difficulty in the context of information retrieval [11, 49] to learn an estimator that predicts the expected precision of the query by analyzing the overlap between the results of the full query and the results of its sub-queries.", "startOffset": 96, "endOffset": 104}], "year": 2017, "abstractText": "We address the novel problem of automatically generating quiz-style knowledge questions from a knowledge graph such as DBpedia. Questions of this kind have ample applications, for instance, to educate users about or to evaluate their knowledge in a specific domain. To solve the problem, we propose an end-to-end approach. The approach first selects a named entity from the knowledge graph as an answer. It then generates a structured triple-pattern query, which yields the answer as its sole result. If a multiplechoice question is desired, the approach selects alternative answer options. Finally, our approach uses a template-based method to verbalize the structured query and yield a natural language question. A key challenge is estimating how difficult the generated question is to human users. To do this, we make use of historical data from the Jeopardy! quiz show and a semantically annotated Web-scale document collection, engineer suitable features, and train a logistic regression classifier to predict question difficulty. Experiments demonstrate the viability of our overall approach.", "creator": "LaTeX with hyperref package"}}}