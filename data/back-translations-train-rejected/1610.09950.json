{"id": "1610.09950", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-Oct-2016", "title": "From Node Embedding To Community Embedding", "abstract": "Most of the existing graph embedding methods focus on nodes, which aim to output a vector representation for each node in the graph such that two nodes being \"close\" on the graph are close too in the low-dimensional space. Despite the success of embedding individual nodes for graph analytics, we notice that an important concept of embedding communities (i.e., groups of nodes) is missing. Embedding communities is useful, not only for supporting various community-level applications, but also to help preserve community structure in graph embedding. In fact, we see community embedding as providing a higher-order proximity to define the node closeness, whereas most of the popular graph embedding methods focus on first-order and/or second-order proximities. To learn the community embedding, we hinge upon the insight that community embedding and node embedding reinforce with each other. As a result, we propose ComEmbed, the first community embedding method, which jointly optimizes the community embedding and node embedding together. We evaluate ComEmbed on real-world data sets. We show it outperforms the state-of-the-art baselines in both tasks of node classification and community prediction.", "histories": [["v1", "Mon, 31 Oct 2016 14:50:41 GMT  (467kb,D)", "http://arxiv.org/abs/1610.09950v1", null], ["v2", "Thu, 14 Sep 2017 06:35:39 GMT  (540kb,D)", "http://arxiv.org/abs/1610.09950v2", "Code available atthis https URL"]], "reviews": [], "SUBJECTS": "cs.SI cs.AI", "authors": ["vincent w zheng", "sandro cavallari", "hongyun cai", "kevin chen-chuan chang", "erik cambria"], "accepted": false, "id": "1610.09950"}, "pdf": {"name": "1610.09950.pdf", "metadata": {"source": "CRF", "title": "From Node Embedding To Community Embedding", "authors": ["Vincent W. Zheng", "Sandro Cavallari", "Hongyun Cai", "Kevin Chen-Chuan Chang", "Erik Cambria"], "emails": ["hongyun.c}@adsc.com.sg,", "sandro001@e.ntu.edu.sg,", "kcchang@illinois.edu,", "cambria@ntu.edu.sg"], "sections": [{"heading": "Introduction", "text": "It is becoming increasingly popular, thanks to the proliferation of various social media (e.g. blogs, Flickr, Twitter) and many other types of information networks (e.g. DBLP, knowledge graphics).In order to process and analyze graph data effectively, we often have to think about how to adequately represent the graph. Graph embedding is a mainstream graph representation framework (Roweis and Saul 2000) in which graph representations of graph representations are projected into a low-dimensional space, such as classification, clustering and so on.Conventions, Grapembedding focuses on nodes - it tries to represent a vector representation for each node in the graph so that two nodes are \"close\" to the graph representation."}, {"heading": "Related Work", "text": "Most embedding methods focus on node embedding. Previous methods such as MDS (Cox and Cox 2000), LLE (Roweis and Saul 2000), IsoMap (Tenenbaum, de Silva, and Langford 2000), and Laplacian eigenmap (Belkin and Niyogi 2001) typically aim to solve the leading eigenvectors of graph affinity matrices as node embedding. Recent methods typically rely on neural networks to learn representation for each node, using either flat architectures (Yang, Cohen, and Salakhutdinov 2016). Xie et al. 2016; Perozzi, Al-Rfou, and Skiena 2014; Tang et al. 2015; Grover and Leskovec 2016) or deep architectures (Niepert, Ahmed, and Kutzkov 2016; Wang, Cui, and Zhal 2015)."}, {"heading": "Problem Formulation", "text": "In this paper, we imagine that we represent each community as a Gaussian component characterized by a vector that identifies the community center and a covariance matrix that indicates its member nodes. \"Formally, we define: Definition 1 (Community Embedding) A community embedding for a community Embedding for a community characterized by a vector that identifies the community center and a covariance matrix."}, {"heading": "Inference", "text": "To solve Eq. 9, we coordinate the parentage between Eq. 9, we coordinate the parentage between Eq. 9, we coordinate the parentage between Eq. 9, we coordinate the parentage between Eq. 9, we coordinate the parentage between Eq. 9, we coordinate the parentage between Eq. 9, we coordinate the parentage between Eq. 9, we optimize, we optimize, we optimize, we minimize the objective function. Since the objective function is limited, we ultimately achieve the conversion. Fixing. 9, we combine, we optimize, we both."}, {"heading": "Experiments", "text": "We use two real datasets3: BlogCatalog and Flickr, then we split the entire dataset. We each have 10,312 nodes, 333,983 edges, and 39 node labels. Flickr's dataset has 80,513 nodes, 5,899,882 edges, and 195 node labels. We evaluate the number of communities that we capture as the number of different labels in the dataset. As a parent task, we use the entire graph for each community and then the prediction of the communities for each node. As our datasets are labeled, we set the number of communities as the number of different labels in the dataset. As a parent task, we use the entire graph for each community and then the prediction of the communities for each node. In the node classification, each node is placed in one of the multiple labels. We follow (Perozzi, Al Rfou) to place the individual nodes on the first order of the 2014 datassignments."}, {"heading": "Conclusion", "text": "In this paper, we examine the problem of embedding communities in the graph. The problem is new because most of the existing methods of embedding graphs focus on individual nodes and not on a group of nodes. We observe that the embedding of communities and the embedding of nodes mutually reinforce each other. On the other hand, good embedding of communities also helps to obtain good embedding of communities, since clustering is then done via nodes with good representations. Together, we optimize the embedding of nodes and the embedding of communities. We evaluate our method using real data sets and show that it outperforms state-of-the-art baselines by at least 4.0% -5.5% (conductivity) and 5.3% -11.2% (NMI)."}], "references": [{"title": "Laplacian eigenmaps and spectral techniques for embedding and clustering", "author": ["M. Belkin", "P. Niyogi"], "venue": "NIPS, 585\u2013591.", "citeRegEx": "Belkin and Niyogi,? 2001", "shortCiteRegEx": "Belkin and Niyogi", "year": 2001}, {"title": "Pattern Recognition and Machine Learning (Information Science and Statistics)", "author": ["C.M. Bishop"], "venue": "Secaucus, NJ, USA: Springer-Verlag New York, Inc.", "citeRegEx": "Bishop,? 2006", "shortCiteRegEx": "Bishop", "year": 2006}, {"title": "Deep neural networks for learning graph representations", "author": ["S. Cao", "W. Lu", "Q. Xu"], "venue": "AAAI, 1145\u20131152.", "citeRegEx": "Cao et al\\.,? 2016", "shortCiteRegEx": "Cao et al\\.", "year": 2016}, {"title": "Libsvm: A library for support vector machines", "author": ["Chang", "C.-C.", "Lin", "C.-J."], "venue": "ACM Trans. Intell. Syst. Technol. 2(3):27:1\u201327:27.", "citeRegEx": "Chang et al\\.,? 2011", "shortCiteRegEx": "Chang et al\\.", "year": 2011}, {"title": "Heterogeneous network embedding via deep architectures", "author": ["S. Chang", "W. Han", "J. Tang", "G. Qi", "C.C. Aggarwal", "T.S. Huang"], "venue": "KDD, 119\u2013128.", "citeRegEx": "Chang et al\\.,? 2015", "shortCiteRegEx": "Chang et al\\.", "year": 2015}, {"title": "Multidimensional Scaling, Second Edition", "author": ["T.F. Cox", "M. Cox"], "venue": "Chapman and Hall/CRC, 2 edition.", "citeRegEx": "Cox and Cox,? 2000", "shortCiteRegEx": "Cox and Cox", "year": 2000}, {"title": "node2vec: Scalable feature learning for networks", "author": ["A. Grover", "J. Leskovec"], "venue": "KDD.", "citeRegEx": "Grover and Leskovec,? 2016", "shortCiteRegEx": "Grover and Leskovec", "year": 2016}, {"title": "Heat kernel based community detection", "author": ["K. Kloster", "D.F. Gleich"], "venue": "KDD, 1386\u20131395.", "citeRegEx": "Kloster and Gleich,? 2014", "shortCiteRegEx": "Kloster and Gleich", "year": 2014}, {"title": "Community detection via measure space embedding", "author": ["M. Kozdoba", "S. Mannor"], "venue": "NIPS, 2890\u20132898.", "citeRegEx": "Kozdoba and Mannor,? 2015", "shortCiteRegEx": "Kozdoba and Mannor", "year": 2015}, {"title": "Neural word embedding as implicit matrix factorization", "author": ["O. Levy", "Y. Goldberg"], "venue": "NIPS, 2177\u20132185.", "citeRegEx": "Levy and Goldberg,? 2014", "shortCiteRegEx": "Levy and Goldberg", "year": 2014}, {"title": "Contextdependent knowledge graph embedding", "author": ["Y. Luo", "Q. Wang", "B. Wang", "L. Guo"], "venue": "EMNLP, 1656\u2013 1661.", "citeRegEx": "Luo et al\\.,? 2015", "shortCiteRegEx": "Luo et al\\.", "year": 2015}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["T. Mikolov", "I. Sutskever", "K. Chen", "G.S. Corrado", "J. Dean"], "venue": "NIPS, 3111\u20133119.", "citeRegEx": "Mikolov et al\\.,? 2013", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Learning convolutional neural networks for graphs", "author": ["M. Niepert", "M. Ahmed", "K. Kutzkov"], "venue": "ICML, 2014\u2013 2023.", "citeRegEx": "Niepert et al\\.,? 2016", "shortCiteRegEx": "Niepert et al\\.", "year": 2016}, {"title": "Asymmetric transitivity preserving graph embedding", "author": ["M. Ou", "P. Cui", "J. Pei", "Z. Zhang", "W. Zhu"], "venue": "KDD, 1105\u20131114.", "citeRegEx": "Ou et al\\.,? 2016", "shortCiteRegEx": "Ou et al\\.", "year": 2016}, {"title": "Deepwalk: Online learning of social representations", "author": ["B. Perozzi", "R. Al-Rfou", "S. Skiena"], "venue": "KDD, 701\u2013710.", "citeRegEx": "Perozzi et al\\.,? 2014", "shortCiteRegEx": "Perozzi et al\\.", "year": 2014}, {"title": "Nonlinear dimensionality reduction by locally linear embedding", "author": ["S.T. Roweis", "L.K. Saul"], "venue": "Science 290(5500):2323\u20132326.", "citeRegEx": "Roweis and Saul,? 2000", "shortCiteRegEx": "Roweis and Saul", "year": 2000}, {"title": "Line: Large-scale information network embedding", "author": ["J. Tang", "M. Qu", "M. Wang", "M. Zhang", "J. Yan", "Q. Mei"], "venue": "WWW, 1067\u20131077.", "citeRegEx": "Tang et al\\.,? 2015", "shortCiteRegEx": "Tang et al\\.", "year": 2015}, {"title": "A global geometric framework for nonlinear dimensionality reduction", "author": ["J.B. Tenenbaum", "V. de Silva", "J.C. Langford"], "venue": "Science", "citeRegEx": "Tenenbaum et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Tenenbaum et al\\.", "year": 2000}, {"title": "Learning deep representations for graph clustering", "author": ["F. Tian", "B. Gao", "Q. Cui", "E. Chen", "T. Liu"], "venue": "AAAI, 1293\u20131299.", "citeRegEx": "Tian et al\\.,? 2014", "shortCiteRegEx": "Tian et al\\.", "year": 2014}, {"title": "Structural deep network embedding", "author": ["D. Wang", "P. Cui", "W. Zhu"], "venue": "KDD, 1225\u20131234.", "citeRegEx": "Wang et al\\.,? 2016", "shortCiteRegEx": "Wang et al\\.", "year": 2016}, {"title": "Representation learning of knowledge graphs with entity descriptions", "author": ["R. Xie", "Z. Liu", "J. Jia", "H. Luan", "M. Sun"], "venue": "AAAI, 2659\u20132665.", "citeRegEx": "Xie et al\\.,? 2016", "shortCiteRegEx": "Xie et al\\.", "year": 2016}, {"title": "Modularity based community detection with deep learning", "author": ["L. Yang", "X. Cao", "D. He", "C. Wang", "X. Wang", "W. Zhang"], "venue": "IJCAI, 2252\u20132258.", "citeRegEx": "Yang et al\\.,? 2016", "shortCiteRegEx": "Yang et al\\.", "year": 2016}, {"title": "Revisiting semi-supervised learning with graph embeddings", "author": ["Z. Yang", "W.W. Cohen", "R. Salakhutdinov"], "venue": "ICML, 40\u201348.", "citeRegEx": "Yang et al\\.,? 2016", "shortCiteRegEx": "Yang et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 15, "context": "Graph embedding is a mainstream graph representation framework (Roweis and Saul 2000; Chang et al. 2015; Niepert, Ahmed, and Kutzkov 2016; Xie et al. 2016), which aims to project a graph into a low-dimensional space for further analytic tasks, such as classification, clustering and so on.", "startOffset": 63, "endOffset": 155}, {"referenceID": 4, "context": "Graph embedding is a mainstream graph representation framework (Roweis and Saul 2000; Chang et al. 2015; Niepert, Ahmed, and Kutzkov 2016; Xie et al. 2016), which aims to project a graph into a low-dimensional space for further analytic tasks, such as classification, clustering and so on.", "startOffset": 63, "endOffset": 155}, {"referenceID": 20, "context": "Graph embedding is a mainstream graph representation framework (Roweis and Saul 2000; Chang et al. 2015; Niepert, Ahmed, and Kutzkov 2016; Xie et al. 2016), which aims to project a graph into a low-dimensional space for further analytic tasks, such as classification, clustering and so on.", "startOffset": 63, "endOffset": 155}, {"referenceID": 0, "context": "1) first-order proximity (Tenenbaum, de Silva, and Langford 2000; Belkin and Niyogi 2001), which considers two nodes as close if they are direct neighbors to each other in the graph; 2) second-order proximity (Perozzi, Al-Rfou, and Skiena 2014; Grover and Leskovec 2016), which considers two nodes as close if they share similar neighbors in the graph; 3) combination of first-order proximity and second-order proximity (Tang et al.", "startOffset": 25, "endOffset": 89}, {"referenceID": 6, "context": "1) first-order proximity (Tenenbaum, de Silva, and Langford 2000; Belkin and Niyogi 2001), which considers two nodes as close if they are direct neighbors to each other in the graph; 2) second-order proximity (Perozzi, Al-Rfou, and Skiena 2014; Grover and Leskovec 2016), which considers two nodes as close if they share similar neighbors in the graph; 3) combination of first-order proximity and second-order proximity (Tang et al.", "startOffset": 209, "endOffset": 270}, {"referenceID": 16, "context": "1) first-order proximity (Tenenbaum, de Silva, and Langford 2000; Belkin and Niyogi 2001), which considers two nodes as close if they are direct neighbors to each other in the graph; 2) second-order proximity (Perozzi, Al-Rfou, and Skiena 2014; Grover and Leskovec 2016), which considers two nodes as close if they share similar neighbors in the graph; 3) combination of first-order proximity and second-order proximity (Tang et al. 2015; Wang, Cui, and Zhu 2016), which considers two nodes as close if they are directly linked and also share similar neighbors.", "startOffset": 420, "endOffset": 463}, {"referenceID": 16, "context": "\u2022 Preserve community structure in graph embedding: the state of the art such as DeepWalk (Perozzi, Al-Rfou, and Skiena 2014), LINE (Tang et al. 2015) and node2vec (Grover and Leskovec 2016) is unable to preserve communities in the embedded space.", "startOffset": 131, "endOffset": 149}, {"referenceID": 6, "context": "2015) and node2vec (Grover and Leskovec 2016) is unable to preserve communities in the embedded space.", "startOffset": 19, "endOffset": 45}, {"referenceID": 1, "context": "To represent a community, we are motivated by the Gaussian Mixture Model (Bishop 2006) to see each community as a Gaussian component.", "startOffset": 73, "endOffset": 86}, {"referenceID": 13, "context": "There is few graph embedding work that considers higher-order proximity, and their definitions of higher-order proximity are not based on communities (Cao, Lu, and Xu 2016; Ou et al. 2016).", "startOffset": 150, "endOffset": 188}, {"referenceID": 21, "context": "There is some work on using community to improve the node embedding (Yang et al. 2016), or using node embedding to achieve better community detection (Tian et al.", "startOffset": 68, "endOffset": 86}, {"referenceID": 18, "context": "2016), or using node embedding to achieve better community detection (Tian et al. 2014).", "startOffset": 69, "endOffset": 87}, {"referenceID": 5, "context": "For example, earlier methods, such as MDS (Cox and Cox 2000), LLE (Roweis and Saul 2000), IsoMap (Tenenbaum, de Silva, and Langford 2000) and Laplacian eigenmap (Belkin and Niyogi 2001), typically aim to solve the leading eigenvectors of graph affinity matrices as node embedding.", "startOffset": 42, "endOffset": 60}, {"referenceID": 15, "context": "For example, earlier methods, such as MDS (Cox and Cox 2000), LLE (Roweis and Saul 2000), IsoMap (Tenenbaum, de Silva, and Langford 2000) and Laplacian eigenmap (Belkin and Niyogi 2001), typically aim to solve the leading eigenvectors of graph affinity matrices as node embedding.", "startOffset": 66, "endOffset": 88}, {"referenceID": 0, "context": "For example, earlier methods, such as MDS (Cox and Cox 2000), LLE (Roweis and Saul 2000), IsoMap (Tenenbaum, de Silva, and Langford 2000) and Laplacian eigenmap (Belkin and Niyogi 2001), typically aim to solve the leading eigenvectors of graph affinity matrices as node embedding.", "startOffset": 161, "endOffset": 185}, {"referenceID": 20, "context": "Recent methods typically rely on neural networks to learn the representation for each node, with either shallow architectures (Yang, Cohen, and Salakhutdinov 2016; Xie et al. 2016; Perozzi, Al-Rfou, and Skiena 2014; Tang et al. 2015; Grover and Leskovec 2016) or deep architectures (Niepert, Ahmed, and Kutzkov 2016; Wang, Cui, and Zhu 2016; Chang et al.", "startOffset": 126, "endOffset": 259}, {"referenceID": 16, "context": "Recent methods typically rely on neural networks to learn the representation for each node, with either shallow architectures (Yang, Cohen, and Salakhutdinov 2016; Xie et al. 2016; Perozzi, Al-Rfou, and Skiena 2014; Tang et al. 2015; Grover and Leskovec 2016) or deep architectures (Niepert, Ahmed, and Kutzkov 2016; Wang, Cui, and Zhu 2016; Chang et al.", "startOffset": 126, "endOffset": 259}, {"referenceID": 6, "context": "Recent methods typically rely on neural networks to learn the representation for each node, with either shallow architectures (Yang, Cohen, and Salakhutdinov 2016; Xie et al. 2016; Perozzi, Al-Rfou, and Skiena 2014; Tang et al. 2015; Grover and Leskovec 2016) or deep architectures (Niepert, Ahmed, and Kutzkov 2016; Wang, Cui, and Zhu 2016; Chang et al.", "startOffset": 126, "endOffset": 259}, {"referenceID": 4, "context": "2015; Grover and Leskovec 2016) or deep architectures (Niepert, Ahmed, and Kutzkov 2016; Wang, Cui, and Zhu 2016; Chang et al. 2015).", "startOffset": 54, "endOffset": 132}, {"referenceID": 10, "context": "Other than node embedding, there is some attempt to learn edge embedding (Luo et al. 2015), which aims to learn the embedding for both entities (i.", "startOffset": 73, "endOffset": 90}, {"referenceID": 6, "context": "Most existing methods focus on first-order proximity and/or second-order proximity (Perozzi, Al-Rfou, and Skiena 2014; Grover and Leskovec 2016; Tang et al. 2015; Wang, Cui, and Zhu 2016), as discussed in the introduction.", "startOffset": 83, "endOffset": 187}, {"referenceID": 16, "context": "Most existing methods focus on first-order proximity and/or second-order proximity (Perozzi, Al-Rfou, and Skiena 2014; Grover and Leskovec 2016; Tang et al. 2015; Wang, Cui, and Zhu 2016), as discussed in the introduction.", "startOffset": 83, "endOffset": 187}, {"referenceID": 11, "context": "1) DeepWalk is a realization of SkipGram (Mikolov et al. 2013) on the graph data, where DeepWalk treats each node as a \u201cword\u201d and each path as a \u201csentence\u201d; 2) SkipGram implicitly factorizes a matrix based on the word-word coocurrence (Levy and Goldberg 2014).", "startOffset": 41, "endOffset": 62}, {"referenceID": 9, "context": "2013) on the graph data, where DeepWalk treats each node as a \u201cword\u201d and each path as a \u201csentence\u201d; 2) SkipGram implicitly factorizes a matrix based on the word-word coocurrence (Levy and Goldberg 2014).", "startOffset": 178, "endOffset": 202}, {"referenceID": 13, "context": "Both (Ou et al. 2016) and (Cao, Lu, and Xu 2016) design and factorize a higher-order node-node proximity matrix by PageRank or Katz index.", "startOffset": 5, "endOffset": 21}, {"referenceID": 18, "context": "It is common to use node embedding results for community detection (Tian et al. 2014; Kozdoba and Mannor 2015), but they do not have the notion of community in their node embedding.", "startOffset": 67, "endOffset": 110}, {"referenceID": 8, "context": "It is common to use node embedding results for community detection (Tian et al. 2014; Kozdoba and Mannor 2015), but they do not have the notion of community in their node embedding.", "startOffset": 67, "endOffset": 110}, {"referenceID": 21, "context": "There is some work that allows community feedback to guide the node embedding (Yang et al. 2016), but again it lacks the concept of community embedding and its community feedback requires extra supervision on must-links.", "startOffset": 78, "endOffset": 96}, {"referenceID": 16, "context": "Specifically, for each edge (vi, vj) \u2208 E, we follow LINE (Tang et al. 2015) to model the likelihood of first-order proximity as", "startOffset": 57, "endOffset": 75}, {"referenceID": 11, "context": "As the summarization in the softmax function is time consuming, we follow (Mikolov et al. 2013) to use negative sampling to replace the summation term in the softmax function.", "startOffset": 74, "endOffset": 95}, {"referenceID": 16, "context": "We follow (Tang et al. 2015) to define Pn(u) \u221d r u , where ru is node u\u2019s degree.", "startOffset": 10, "endOffset": 28}, {"referenceID": 1, "context": "Finally, as GMM is known as a probabilistic version of K-means clustering (Bishop 2006), Eq.", "startOffset": 74, "endOffset": 87}, {"referenceID": 1, "context": "According to (Bishop 2006), we can easily optimize (\u03a8,\u03a3) by expectation maximization, and fortunately we have closedform update for each parameter as:", "startOffset": 13, "endOffset": 26}, {"referenceID": 7, "context": "In community prediction, we use both conductance (Kloster and Gleich 2014) and normalized mutual information (NMI) (Tian et al.", "startOffset": 49, "endOffset": 74}, {"referenceID": 18, "context": "In community prediction, we use both conductance (Kloster and Gleich 2014) and normalized mutual information (NMI) (Tian et al. 2014) as the evaluation metrics.", "startOffset": 115, "endOffset": 133}, {"referenceID": 16, "context": "\u2022 LINE (Tang et al. 2015): it models both the first-order and second-order proximities for node embedding.", "startOffset": 7, "endOffset": 25}, {"referenceID": 6, "context": "\u2022 node2vec (Grover and Leskovec 2016): it extends DeepWalk to exploit homophily and structural roles for node embedding.", "startOffset": 11, "endOffset": 37}], "year": 2016, "abstractText": "Most of the existing graph embedding methods focus on nodes, which aim to output a vector representation for each node in the graph such that two nodes being \u201cclose\u201d on the graph are close too in the low-dimensional space. Despite the success of embedding individual nodes for graph analytics, we notice that an important concept of embedding communities (i.e., groups of nodes) is missing. Embedding communities is useful, not only for supporting various community-level applications, but also to help preserve community structure in graph embedding. In fact, we see community embedding as providing a higher-order proximity to define the node closeness, whereas most of the popular graph embedding methods focus on first-order and/or second-order proximities. To learn the community embedding, we hinge upon the insight that community embedding and node embedding reinforce with each other. As a result, we propose ComEmbed, the first community embedding method, which jointly optimizes the community embedding and node embedding together. We evaluate ComEmbed on real-world data sets. We show it outperforms the state-of-the-art baselines in both tasks of node classification and community prediction.", "creator": "TeX"}}}