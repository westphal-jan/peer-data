{"id": "1610.09975", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-Oct-2016", "title": "Neural Speech Recognizer: Acoustic-to-Word LSTM Model for Large Vocabulary Speech Recognition", "abstract": "We present results that show it is possible to build a competitive, greatly simplified, large vocabulary continuous speech recognition system with whole words as acoustic units. We model the output vocabulary of about 100,000 words directly using deep bi-directional LSTM RNNs with CTC loss. The model is trained on 125,000 hours of semi-supervised acoustic training data, which enables us to alleviate the data sparsity problem for word models. We show that the CTC word models work very well as an end-to-end all-neural speech recognition model without the use of traditional context-dependent sub-word phone units that require a pronunciation lexicon, and without any language model removing the need to decode. We demonstrate that the CTC word models perform better than a strong, more complex, state-of-the-art baseline with sub-word units.", "histories": [["v1", "Mon, 31 Oct 2016 15:36:42 GMT  (28kb,D)", "http://arxiv.org/abs/1610.09975v1", null]], "reviews": [], "SUBJECTS": "cs.CL cs.LG cs.NE", "authors": ["hagen soltau", "hank liao", "hasim sak"], "accepted": false, "id": "1610.09975"}, "pdf": {"name": "1610.09975.pdf", "metadata": {"source": "CRF", "title": "Neural Speech Recognizer: Acoustic-to-Word LSTM Model for Large Vocabulary Speech Recognition", "authors": ["Hagen Soltau", "Hank Liao", "Hasim Sak"], "emails": ["soltau@google.com", "hankliao@google.com", "hasim@google.com"], "sections": [{"heading": "1 Introduction", "text": "In the past, the best speech recognition systems have used many complex modeling techniques to improve accuracy."}, {"heading": "2 Neural Speech Recognizer", "text": "\"Here we describe the techniques we used to build the NSR: a single neural network model that is able to advance precise speech recognition without searching or decoding.\" The NSR model has a deep LSTM RNN architecture built by stacking multiple LSTM layers. As the bi-directional RNN models have better accuracy and our application is offline speech recognition, we use two LSTM layers at each depth - one in the front and another in the backward direction over time via the input sequence. Both layers are connected to previous and backward-facing layers. We train the NSR model with the CTC loss criterion [12], which is a sequence alignment technique with a softmax output level that has an additional unit for the blank label that is used not to output a label at a given time."}, {"heading": "3 Experimental Setup", "text": "YouTube is a video-sharing site with over one billion users. To improve accessibility, Google has features for recognizing YouTube videos using automatic voice recognition technology. Although the quality of captions generated can vary and is generally no better than those generated by humans, they can be produced on a large scale. Overall, users found them helpful: Google received a breakthrough technology award from the US National Association of the Deaf for automatic captions on YouTube in 2015. For this work, we evaluate our models using videos sampled from Google Preferred Channel on YouTube [20]. The test kit consists of 296 videos in 13 categories, with each video lasting an average of 5 minutes. The total test duration is about 25 hours and 250,000 words [21]. Since most of our training data is not monitored, an important question is how valuable this type of data is for training acoustic models. In all of our experiments, we keep our language model constant and use a 5 gram model with a vocabulary of 500,000 words."}, {"heading": "3.1 Conventional Context Dependent Phone Models", "text": "The acoustic model is based on 650 hours of monitored training data from YouTube, Google Videos and Broadcast News described in [24]. The acoustic model is a three-stage HMM with 6400 CD triphone states. This system gave us a word error rate of 29.0% on the Google Preferred test set as shown in Table 1. By training using a sequence-based state MBR criterion and using a two-pass customized decoding setup, the best we could do was a 650-hour training set of 24.0%. In contrast, we added more semi-monitored training data: at 5000 hours we reduced the error rate to 21.2% for the same model size. As we have more data available and models that can capture a longer time context, we show results for single-stage CD phone sessions [25]; this results in a relative improvement of 4% over the three-stage models. This type of model improves with the amount of training data and little difference between CTC and CE."}, {"heading": "3.2 Neural Speech Recognizer", "text": "In fact, most of them are able to play by the rules that they have adopted in recent years."}, {"heading": "4 Conclusions", "text": "We introduced our Neural Speech Recognizer: a large continuous neural speech recognition system that eliminates the need for a pronunciation lexicon and a decoder. 125,000 hours of training data using public captions allow us to train a large and powerful bidirectional LSTM RNN model for speech recognition with a CTC loss for predicting words. Neural Speech Recognizer can model a written vocabulary of 100,000 words including numerical units. Unlike many end-to-end systems that compromise the accuracy of system simplicity, our final system performs better than a well-trained, contextual phone-based system that achieves a word error rate of 13.4% in a difficult YouTube video delivery task."}], "references": [{"title": "Towards end-to-end speech recognition with recurrent neural networks", "author": ["Alex Graves", "Navdeep Jaitly"], "venue": "In Proc. ICML,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2016}, {"title": "EESEN: End-to-end speech recognition using deep RNN models and WFST-based decoding", "author": ["Yajie Miao", "Mohammad Gowayyed", "Florian Metze"], "venue": "In Proc. ASRU,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2015}, {"title": "End-to-end attention-based large vocabulary speech recognition", "author": ["Dzmitry Bahdanau", "Jan Chorowski", "Dmitriy Serdyuk", "Philemon Brakel", "Yoshua Bengio"], "venue": "In Proc. ICASSP,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2016}, {"title": "Towards end-to-end speech recognition with deep convolutional neural networks", "author": ["Ying Zhang", "Mohammad Pezeshki", "Phil\u00e9mon Brakel", "Saizheng Zhang", "C\u00e9sar Laurent", "Yoshua Bengio", "Aaron Courville"], "venue": "In Proc. Interspeech,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2016}, {"title": "Deep Speech 2: End-to-end speech recognition in English and Mandarin", "author": ["Dario Amodei"], "venue": "In Proc. ICML,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2016}, {"title": "On training the recurrent neural network encoder-decoder for large vocabulary end-to-end speech recognition", "author": ["Liang Lu", "Xing-Xing Zhang", "Steve Renals"], "venue": "In Proc. ICASSP,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2016}, {"title": "Listen, attend and spell: A neural network for large vocabulary conversational speech recognition", "author": ["William Chan", "Navdeep Jaitly", "Quoc V. Le", "Oriol Vinyals"], "venue": "In Proc. ICASSP,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2016}, {"title": "Context dependent modelling of phones in continuous speech using decision trees", "author": ["L.R. Bahl", "P.V. de Souza", "P.S. Gopalakrishnan", "D. Nahamoo", "M.A. Picheny"], "venue": "In Proc. DARPA Speech and Natural Language Processing Workshop,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1991}, {"title": "Tree-based state tying for high accuracy acoustic modelling", "author": ["S.J. Young", "J.J. Odell", "P.C. Woodland"], "venue": "In Proc. ARPA Workshop on Human Language Technology,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1994}, {"title": "A time-delay neural network architecture for isolated word recognition", "author": ["Kevin J. Lang", "Alex H. Waibel", "Geoffrey E. Hinton"], "venue": "Neural Networks,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1990}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural Computation,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1997}, {"title": "Connectionist temporal classification: Labelling unsegmented sequence data with recurrent neural networks", "author": ["Alex Graves", "Santiago Fern\u00e1ndez", "Faustino Gomez", "J\u00fcrgen Schmidhuber"], "venue": "In Proc. ICML,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2006}, {"title": "Fast and accurate recurrent neural network acoustic models for speech recognition", "author": ["Ha\u015fim Sak", "Andrew Senior", "Kanishka Rao", "Fran\u00e7oise Beaufays"], "venue": "arXiv preprint arXiv:1507.06947,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2015}, {"title": "Bidirectional recurrent neural networks", "author": ["Mike Schuster", "Kuldip K. Paliwal"], "venue": "Signal Processing, IEEE Transactions on,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1997}, {"title": "Learning acoustic frame labeling for speech recognition with recurrent neural networks", "author": ["Ha\u015fim Sak", "Andrew Senior", "Kanishka Rao", "Ozan Irsoy", "Alex Graves", "Fran\u00e7oise Beaufays", "Johan Schalkwyk"], "venue": "In Proc. ICASSP,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2015}, {"title": "Language model verbalization for automatic speech recognition", "author": ["Ha\u015fim Sak", "Fran\u00e7oise Beaufays", "Kaisuke Nakajima", "Cyril Allauzen"], "venue": "In Proc. ICASSP,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2013}, {"title": "Large scale distributed deep networks", "author": ["Jeffrey Dean", "Greg Corrado", "Rajat Monga", "Kai Chen", "Matthieu Devin", "Quoc V. Le", "Mark Z. Mao", "Marc\u2019Aurelio Ranzato", "Andrew W. Senior", "Paul A. Tucker", "Ke Yang", "Andrew Y. Ng"], "venue": "In NIPS,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2012}, {"title": "Long Short-Term Memory Recurrent Neural Network Architectures for Large Scale Acoustic Modeling", "author": ["Hasim Sak", "Andrew Senior", "Francoise Beaufays"], "venue": "In Proc. Interspeech,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2014}, {"title": "Learning N-gram language models from uncertain data", "author": ["Vitaly Kuznetsov", "Hank Liao", "Mehryar Mohri", "Michael Riley", "Brian Roark"], "venue": "In Proc. Interspeech,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2016}, {"title": "LibriSpeech: an ASR corpus based on public domain audio books", "author": ["Vassil Panayotov", "Guoguo Chen", "Daniel Povey", "Sanjeev Khudanpur"], "venue": "In Proc. ICASSP,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2015}, {"title": "A big data approach to acoustic model training corpus selection", "author": ["Olga Kapralova", "John Alex", "Eugene Weinstein", "Pedro Moreno", "Olivier Siohan"], "venue": "In Proc. Interspeech,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2014}, {"title": "Large scale deep neural network acoustic modeling with semi-supervised training data for youtube video transcription", "author": ["Hank Liao", "Erik McDermott", "Andrew Senior"], "venue": "In Proc. ASRU,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2013}, {"title": "Context dependent phone models for LSTM RNN acoustic modelling", "author": ["A. Senior", "H. Sak", "I. Shafran"], "venue": "In Proc. ICASSP,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "End-to-end speech recognition with neural networks has been a goal for the machine learning and speech processing communities [1\u20137].", "startOffset": 126, "endOffset": 131}, {"referenceID": 1, "context": "End-to-end speech recognition with neural networks has been a goal for the machine learning and speech processing communities [1\u20137].", "startOffset": 126, "endOffset": 131}, {"referenceID": 2, "context": "End-to-end speech recognition with neural networks has been a goal for the machine learning and speech processing communities [1\u20137].", "startOffset": 126, "endOffset": 131}, {"referenceID": 3, "context": "End-to-end speech recognition with neural networks has been a goal for the machine learning and speech processing communities [1\u20137].", "startOffset": 126, "endOffset": 131}, {"referenceID": 4, "context": "End-to-end speech recognition with neural networks has been a goal for the machine learning and speech processing communities [1\u20137].", "startOffset": 126, "endOffset": 131}, {"referenceID": 5, "context": "End-to-end speech recognition with neural networks has been a goal for the machine learning and speech processing communities [1\u20137].", "startOffset": 126, "endOffset": 131}, {"referenceID": 6, "context": "End-to-end speech recognition with neural networks has been a goal for the machine learning and speech processing communities [1\u20137].", "startOffset": 126, "endOffset": 131}, {"referenceID": 7, "context": "In the past, the best speech recognition systems have used many complex modeling techniques for improving accuracy: for example the use of hand-crafted feature representations, speaker or environment adaptation with feature or affine transformations, and context-dependent (CD) phonetic models with decision tree clustering [8, 9] to name a few.", "startOffset": 324, "endOffset": 330}, {"referenceID": 8, "context": "In the past, the best speech recognition systems have used many complex modeling techniques for improving accuracy: for example the use of hand-crafted feature representations, speaker or environment adaptation with feature or affine transformations, and context-dependent (CD) phonetic models with decision tree clustering [8, 9] to name a few.", "startOffset": 324, "endOffset": 330}, {"referenceID": 9, "context": "While some attempts had been made to model words directly, in particular for isolated word recognition with very limited vocabularies [10], the dominant approach is to model clustered CD sub-word units instead.", "startOffset": 134, "endOffset": 138}, {"referenceID": 10, "context": "It was previously found that the combination of a LSTM RNN model\u2019s [11] memorization capacity and the ability of CTC loss [12] to learn an alignment between acoustic input and label sequences allows a neural network that can recognize whole words to be trained [13].", "startOffset": 67, "endOffset": 71}, {"referenceID": 11, "context": "It was previously found that the combination of a LSTM RNN model\u2019s [11] memorization capacity and the ability of CTC loss [12] to learn an alignment between acoustic input and label sequences allows a neural network that can recognize whole words to be trained [13].", "startOffset": 122, "endOffset": 126}, {"referenceID": 12, "context": "It was previously found that the combination of a LSTM RNN model\u2019s [11] memorization capacity and the ability of CTC loss [12] to learn an alignment between acoustic input and label sequences allows a neural network that can recognize whole words to be trained [13].", "startOffset": 261, "endOffset": 265}, {"referenceID": 5, "context": "[6] use an encoder-decoder model of the conditional probability of the full word output sequence given the input sequence.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "Instead of word outputs, letters can be generated directly in a grapheme-based neural network recognizer [3, 7]; while good results are obtained for a large vocabulary task, they are not quite comparable to the phone-based baseline system.", "startOffset": 105, "endOffset": 111}, {"referenceID": 6, "context": "Instead of word outputs, letters can be generated directly in a grapheme-based neural network recognizer [3, 7]; while good results are obtained for a large vocabulary task, they are not quite comparable to the phone-based baseline system.", "startOffset": 105, "endOffset": 111}, {"referenceID": 13, "context": "Since the bidirectional RNN models [14] have better accuracy and our application is offline speech recognition, we use two LSTM layers at each depth\u2014one operating in the forward and another operating in the backward direction in time over the input sequence.", "startOffset": 35, "endOffset": 39}, {"referenceID": 11, "context": "We train the NSR model with the CTC loss criterion [12] which is a sequence alignment/labeling technique with a softmax output layer that has an additional unit for the blank label used to represent outputting no label at a given time.", "startOffset": 51, "endOffset": 55}, {"referenceID": 14, "context": "[15]", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "For written-to-spoken domain mapping a FST verbalization model is used [16].", "startOffset": 71, "endOffset": 75}, {"referenceID": 16, "context": "We train the models in a distributed manner using asynchronous stochastic gradient descent (ASGD) with a large number of machines [17, 18].", "startOffset": 130, "endOffset": 138}, {"referenceID": 17, "context": "We train the models in a distributed manner using asynchronous stochastic gradient descent (ASGD) with a large number of machines [17, 18].", "startOffset": 130, "endOffset": 138}, {"referenceID": 0, "context": "For training stability, we clip the activations of memory cells to [-50, 50], and the gradients to [-1, 1] range.", "startOffset": 99, "endOffset": 106}, {"referenceID": 18, "context": "The total test set duration is roughly 25 hours and 250,000 words [21].", "startOffset": 66, "endOffset": 70}, {"referenceID": 4, "context": "While others have used read speech corpora [5, 22] or unsupervised methods [23] to gather thousands or even tens of thousands of hours of labeled training data, we apply an approach first described in [24] but now scaled up to build a training set of over 125,000 hours.", "startOffset": 43, "endOffset": 50}, {"referenceID": 19, "context": "While others have used read speech corpora [5, 22] or unsupervised methods [23] to gather thousands or even tens of thousands of hours of labeled training data, we apply an approach first described in [24] but now scaled up to build a training set of over 125,000 hours.", "startOffset": 43, "endOffset": 50}, {"referenceID": 20, "context": "While others have used read speech corpora [5, 22] or unsupervised methods [23] to gather thousands or even tens of thousands of hours of labeled training data, we apply an approach first described in [24] but now scaled up to build a training set of over 125,000 hours.", "startOffset": 75, "endOffset": 79}, {"referenceID": 21, "context": "While others have used read speech corpora [5, 22] or unsupervised methods [23] to gather thousands or even tens of thousands of hours of labeled training data, we apply an approach first described in [24] but now scaled up to build a training set of over 125,000 hours.", "startOffset": 201, "endOffset": 205}, {"referenceID": 21, "context": "The initial acoustic model was trained on 650 hours of supervised training data that comes from YouTube, Google Videos, and Broadcast News described in [24].", "startOffset": 152, "endOffset": 156}, {"referenceID": 22, "context": "Since we have more data available, and models that can capture longer temporal context, we show results for single-state CD phone units [25]; this gives a 4% relative improvement over the 3-state triphone models.", "startOffset": 136, "endOffset": 140}], "year": 2016, "abstractText": "We present results that show it is possible to build a competitive, greatly simplified, large vocabulary continuous speech recognition system with whole words as acoustic units. We model the output vocabulary of about 100,000 words directly using deep bi-directional LSTM RNNs with CTC loss. The model is trained on 125,000 hours of semi-supervised acoustic training data, which enables us to alleviate the data sparsity problem for word models. We show that the CTC word models work very well as an end-to-end all-neural speech recognition model without the use of traditional context-dependent sub-word phone units that require a pronunciation lexicon, and without any language model removing the need to decode. We demonstrate that the CTC word models perform better than a strong, more complex, state-of-the-art baseline with sub-word units.", "creator": "LaTeX with hyperref package"}}}