{"id": "1610.09996", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-Oct-2016", "title": "End-to-End Answer Chunk Extraction and Ranking for Reading Comprehension", "abstract": "This paper proposes dynamic chunk reader (DCR), an end-to-end neural reading comprehension (RC) model that is able to extract and rank a set of answer candidates from a given document to answer questions. DCR is able to predict answers of variable lengths, whereas previous neural RC models primarily focused on predicting single tokens or entities. DCR encodes a document and an input question with recurrent neural networks, and then applies a word-by-word attention mechanism to acquire question-aware representations for the document, followed by the generation of chunk representations and a ranking module to propose the top-ranked chunk as the answer. Experimental results show that DCR achieves state-of-the-art exact match and F1 scores on the SQuAD dataset.", "histories": [["v1", "Mon, 31 Oct 2016 16:14:08 GMT  (1507kb,D)", "http://arxiv.org/abs/1610.09996v1", "Submitted to AAAI"], ["v2", "Wed, 2 Nov 2016 17:55:32 GMT  (1507kb,D)", "http://arxiv.org/abs/1610.09996v2", "Submitted to AAAI"]], "COMMENTS": "Submitted to AAAI", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["yang yu", "wei zhang", "kazi hasan", "mo yu", "bing xiang", "bowen zhou"], "accepted": false, "id": "1610.09996"}, "pdf": {"name": "1610.09996.pdf", "metadata": {"source": "CRF", "title": "End-to-End Answer Chunk Extraction and Ranking for Reading Comprehension", "authors": ["Yang Yu", "Wei Zhang", "Kazi Hasan", "Mo Yu", "Bing Xiang", "Bowen Zhou"], "emails": ["zhou}@us.ibm.com"], "sections": [{"heading": "Introduction", "text": "In both cases, an answer limit is either easy to determine or already given. Contrary to the above two assumptions for RCQA scenario, people can ask questions about both entities (factoid) and non-explanatory reasons. QQA answering is the task of answering a question with a piece of text from related documents (s) 2016. A variety of neural models has recently been suggested to obtain either a single entity or a single token as an answer from a particular candidate (Hermann et al. 2015; Kadlec et al. 2016; Trischler et al. 2016b; Dhingra et al. 2016; Chen, Bolton et al. 2016; Manning et al.) In both cases, an answer limit is either easy to determine or already given. Deviating from the above two assumptions for RCQA scenario, people can ask questions about both entities (factoid) and non-explanations."}, {"heading": "Problem Definition", "text": "Table 1 shows an example of our RC setting, where the goal is to answer a question Qi, factoid (Q1) or non-factoid (Q2 and Q3), based on a supporting passage Pi, by selecting a sequence of Ai Pi text as the answer. Qi, Pi and Ai are all word sequences in which each word is taken from a vocabulary, V. The i-th instance in the study group is a triple answer in the form of (Pi, Qi, Ai), in which Pi = (pi1,.., pi | Pi = (qi1,., qi., qi | Qi |) and Ai = (ai1, ai1, Ai, Ai, Ai \u00b7, Ai \u00b7, ai \u00b7 V)."}, {"heading": "Baseline: Chunk-and-Rank Pipeline with Neural RC", "text": "In this section, we modified a state-of-the-art RC system for cloze-style tasks for our response extraction purpose, to see how much gap we have for the two types of tasks, and to inspire our end-to-end system in the next section. To make the cloze-style RC system to make chunk-level decisions, we use the RC model to generate functions for chunks that will be used further in a feature-based marker like in (Rajpurkar et al. 2016). As a result, this baseline can be viewed as the deep learning counterpart of the system in (Rajpurkar et al. 2016). It has two main components: 1) a standalone response chunker that is trained to produce overlapping candidate chunks, and 2) a neural RC model that is used to rate each word in a given passage."}, {"heading": "Dynamic Chunk Reader", "text": "The Dynamic Reader (DCR) is shown in Figure 1, which is derived from the baseline we have built, DCR is considered superior for three reasons: First, because each reader evaluates the relevance of the password to the question. First, these components are all within a single, end-to-end model that is in a common man.2We have tried to use more than one level in gated attention readers, but no improvement. DCR works in four steps. First, the encoder level encodes the password and question separately by using bi-directional neural networks (RNN). Second, the attention layer calculates the relevance of each password to the question."}, {"heading": "Experiments", "text": "In fact, most of them will be able to put themselves in a different world, in which they are able to move, in which they move, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they, in which they, in which they live, in which they, in which they live, in which they live, in which they, in which they live."}, {"heading": "Related Work", "text": "Attentive Reader was the first neural model for factoid RCQA (Hermann et al. 2015). It uses bidirectional RNN (Cho et al., 2014; Chung et al., 2014) to encode documents and queries, and use query representation to match with any token from the document. Attention Sum Reader (Kadlec et al. 2016) also simplifies the model to predict only positions of correct response in the document, and training speed and test accuracy are greatly improved on both CNN / Daily Mail Dataset. (Chen, Bolton, and Manning 2016) also Attention Reader and reported higher accuracy. Window-based Memory Networks (MemN2N) is introduced along with the CBT Dataset al (Hill et al. 2015), which does not use RNN encoders, but contexts as memory and contexts as memory and questions with embedded contexts. These models are explicit."}, {"heading": "Conclusion", "text": "Unlike the previously proposed factoid RCQA models, the proposed model, the Dynamic Chunk Reader, is not limited to predicting a single named entity as an answer or selecting an answer from a small, pre-defined list of candidates. Rather, it is able to answer both factoid and non-factoid questions as it learns to select answer blocks that are suitable for an initial question. DCR achieves this goal with a common deep-learning model, enhanced by a novel attention mechanism and five simple but effective features. Error analysis shows that the DCR model performs well, but needs to be improved in predicting longer answers that are not normally factoid."}], "references": [], "referenceMentions": [], "year": 2017, "abstractText": "This paper proposes dynamic chunk reader (DCR), an end-toend neural reading comprehension (RC) model that is able to extract and rank a set of answer candidates from a given document to answer questions. DCR is able to predict answers of variable lengths, whereas previous neural RC models primarily focused on predicting single tokens or entities. DCR encodes a document and an input question with recurrent neural networks, and then applies a word-by-word attention mechanism to acquire question-aware representations for the document, followed by the generation of chunk representations and a ranking module to propose the top-ranked chunk as the answer. Experimental results show that DCR achieves stateof-the-art exact match and F1 scores on the SQuAD dataset (Rajpurkar et al. 2016).", "creator": "LaTeX with hyperref package"}}}