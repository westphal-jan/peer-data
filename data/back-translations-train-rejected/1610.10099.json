{"id": "1610.10099", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-Oct-2016", "title": "Neural Machine Translation in Linear Time", "abstract": "We present a neural architecture for sequence processing. The ByteNet is a stack of two dilated convolutional neural networks, one to encode the source sequence and one to decode the target sequence, where the target network unfolds dynamically to generate variable length outputs. The ByteNet has two core properties: it runs in time that is linear in the length of the sequences and it preserves the sequences' temporal resolution. The ByteNet decoder attains state-of-the-art performance on character-level language modelling and outperforms the previous best results obtained with recurrent neural networks. The ByteNet also achieves a performance on raw character-level machine translation that approaches that of the best neural translation models that run in quadratic time. The implicit structure learnt by the ByteNet mirrors the expected alignments between the sequences.", "histories": [["v1", "Mon, 31 Oct 2016 19:56:39 GMT  (5893kb,D)", "http://arxiv.org/abs/1610.10099v1", "11 pages"], ["v2", "Wed, 15 Mar 2017 18:09:51 GMT  (12174kb,D)", "http://arxiv.org/abs/1610.10099v2", "9 pages"]], "COMMENTS": "11 pages", "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["nal kalchbrenner", "lasse espeholt", "karen simonyan", "aaron van den oord", "alex graves", "koray kavukcuoglu"], "accepted": false, "id": "1610.10099"}, "pdf": {"name": "1610.10099.pdf", "metadata": {"source": "CRF", "title": "Neural Machine Translation in Linear Time", "authors": ["Nal Kalchbrenner", "Lasse Espeholt", "Karen Simonyan", "A\u00e4ron van den Oord", "Alex Graves", "Koray Kavukcuoglu"], "emails": ["nalk@google.com", "lespeholt@google.com", "simonyan@google.com", "avdnoord@google.com", "gravesa@google.com", "korayk@google.com"], "sections": [{"heading": "1 Introduction", "text": "In neural language modelling, a neural network estimates a distribution over sequences of words or characters belonging to a particular language (Bengio et al., 2003). In neural machine translation, the network estimates a distribution over sequences in the target language conditionated on a given sequence in the source language (Kalchbrenner and Blunsom, 2013) Recurrent neural networks (RNN) are powerful sequence models (Hochreiter and Schmidhuber, 1997) and are widely used in language modelling (Mikolov et al, 2010), yet they have a potential disadvantage. RNNNs have a inherently serial structure that prevents them from running parallel along the sequence length."}, {"heading": "2 Neural Translation Model", "text": "Considering a string s from a source language, a neural translation model estimates a distribution p (t | s) over strings t of a target language. The distribution indicates the probability that astring t is a translation of s. A product of conditions on the tokens in the target t = t0,..., tN leads to a comprehensible formulation of the distribution: p (t | s) = N \u0121i = 0 p (ti | t < i, s) (1) Each conditional factor expresses complex and far-reaching dependencies between source and target tokens. The strings are usually sentences of the respective languages; the tokens are words or, as in the present case, characters. The network that models p (t | s) consists of two subnetworks, a source network that processes the source strings into a representation and a target network that uses the source strings to generate the target strings in an arbitrary model (Kalenbrunsom and Blunsom, the author network, which is also useful for the target network)."}, {"heading": "2.1 Desiderata", "text": "Beyond these basic characteristics, the definition of a neural translation model does not determine a unique neural architecture, so we seek to identify some desiderates. (i) The runtime of the network should be linear in the length of the source and target strings. This is all the more urgent the longer the strings are or when characters are used as characters. Operations that run parallel along the sequence length can also be beneficial to reduce computing time. (ii) The size of the source representation should be linear in the length of the source strings, i.e. it should maintain resolution and not have a constant size. This should not burden the model with an additional memory step before translation. More generally, the size of a representation should be proportional to the amount of information it represents or predicts. A related design council affects the path that is traversed by forward and backward signals in the network between a (source or target) input and a predicted output."}, {"heading": "3 ByteNet", "text": "The proposed ByteNet architecture consists of a target network that is stacked on a source network and generates variable-length outputs by dynamic deployment; the target network, the so-called ByteNet decoder, is a language model that consists of one-dimensional Convolutionary Layers that dilate (Section 3.3) and are masked (Section 3.2); the source network processes the source string into a representation and consists of one-dimensional Convolutionary Layers that dilate but are not masked. Figure 1 shows the two networks and their combination in ByteNet."}, {"heading": "3.1 Dynamic Unfolding", "text": "To accommodate source and target sequences of different lengths, the ByteNet uses dynamic unfolding. The source network forms a representation with the same width as the source sequence. At each step, the target network takes the corresponding column of the source representation as input until the target network generates the end-of-the-sequence symbol. The source representation is zero: if the target network creates symbols that exceed the length of the source sequence, the corresponding conditioning column is set to zero. In the latter case, the predictions of the target network are conditioned by source and target representations from previous steps. Figure 2 shows the dynamic unfolding process."}, {"heading": "3.2 Masked One-dimensional Convolutions", "text": "For a target string t = t0,..., tn embeds each of the first n tokens t0,..., tn \u2212 1 via an overview table (the n tokens t1,..., tn serve as targets for the predictions).The resulting embedding is concatenated to a tensor of size 1 \u00d7 n \u00d7 2d, d being the number of inner channels in the network.The target network applies masked one-dimensional waves (van den Oord et al., 2016b) to the embedding sensor, which has a masked core of size k. Masquerading ensures that information from future tokens does not affect the prediction of the current tok.The operation can be performed either by removing some of the weights on a wider core of size 2k \u2212 1 or by filling the output card."}, {"heading": "3.3 Dilation", "text": "The masked waves use the dilatation to increase the receptive field of the target network (Chen et al., 2014; Yu and Koltun, 2015), allowing the receptive field to grow exponentially in terms of the depth of the networks as opposed to linear. We use a dilatation scheme in which the dilatation rates per layer double, up to a maximum rate r (for our experiments r = 16). The scheme is repeated several times in the network, always starting from a dilatation rate of 1 (van den Oord et al., 2016a; Kalchbrenner et al., 2016b)."}, {"heading": "3.4 Residual Blocks", "text": "Each layer is wrapped in a residual block containing additional shaft layers with size 1 filters (He et al., 2015). We adopt two variants of the residual blocks, one with ReLUs used in the machine translation experiments, and one with multiplicative units (Kalchburner et al., 2016b) used in the language modeling experiments. Figure 3 shows the two variants of the blocks."}, {"heading": "3.5 Sub-Batch Normalization", "text": "We are introducing a modification of batch normalization (BN) (Ioffe and Szegedy, 2015) to make it applicable to target nets and decoders. Standard BN calculates the mean and variance of activations of a given revolutionary layer along the batch, height and width dimensions. In a decoder, the standard BN operation at training time would provide average activations along all tokens in the input target sequence, and the BN output for each target character would contain information about the subsequent tokens. This breaks the conditioning structure of Equation 1, as the following tokens cannot yet be predicted. To work around this problem, we present the Sub-Batch Normalization (SubBN). It is a variant of BN in which a batch of training samples is divided into two parts: the main batch and the auxiliary batch."}, {"heading": "4 Model Comparison", "text": "In this section, we will analyze the properties of various previous and current neural translation models. For a more comprehensive analysis, we will also consider two recurring variants of the ByteNet architecture family, which we do not evaluate in the experiments."}, {"heading": "4.1 Recurrent ByteNets", "text": "The ByteNet consists of two stacked source and target networks, with the uppermost network dynamically adapting to the output length. This type of connection of source and target networks is not bound to the strictly revolutionary networks. We can consider two variants of the ByteNet, which use recursive networks for one or both subnetworks (see Figure 4). The first variant replaces the revolutionary target network with a recursive one, which is similarly stacked and dynamically unfolded. The second variant replaces the conventional source network with a recursive network, namely a bidirectional RNN. The target RNN is set to the bidirectional source network. We see that the RNN-Enc-Dec network (Sutskever et al., 2014; Cho et al., 2014) is a recursive ByteNet, in which all connections between source and target - with the exception of the first connecting s0 and t0 - are separated."}, {"heading": "4.2 Comparison of Properties", "text": "In our comparison, we look at the following neural translation models: the Recurrent Continuous Translation Model (RCTM) 1 and 2 (Kalchburner and Blunsom, 2013); the RNN Enc-Dec (Sutskever et al., 2014; Cho et al., 2014); the RNN Enc-Dec Att with the attentive pooling mechanism (Bahdanau et al., 2014), of which there are a few variations (Luong et al., 2015; Chung et al., 2016a); the Grid LSTM translation model (Kalchburner et al., 2016a), which uses a multidimensional architecture; the Extended Neural GPU model (Kaiser and Bengio, 2016), which has a revolutionary RNN architecture; the two revolutionary ByteNet and the two Recurrent ByteNet variants."}, {"heading": "5 Character Prediction", "text": "We use the version of the hutter price of the Wikipedia dataset and follow the standard split, where the first 90 million bytes are used for training, the next 5 million bytes are used for validation, and the last 5 million bytes for testing (Chung et al., 2015). The total number of characters in the vocabulary is 205. The ByteNet decoder we use for the result has 25 residual blocks divided into five sets of five blocks each; for the five blocks in each sentence, the dilatation rates are 1,2,4,8 and 16, respectively. The masked core is size 3. This results in a receptive field of 315 characters. The number of hidden units d is 892. For this task, we use multiplicative residual blocks and sub-BN (fig. 3 right); we do not use bags of n-grams for input."}, {"heading": "6 Character-Level Machine Translation", "text": "In fact, the fact is that most of them will be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be in the position in which they are able to be able to be able to be able to be able to be able to be able to be able to be in the position in which they are able to be in the position."}, {"heading": "7 Conclusion", "text": "We introduced ByteNet, a neural translation model that has a linear runtime, decouples memorization translation, and has short signal propagation paths for tokens in sequences. We have shown that the ByteNet decoder is a state-of-the-art character-level language model based on a tortuous neural network that significantly outperforms recurring language models. We have also shown that the ByteNet generalizes the RNN EncDec architecture, achieving promising results for character-level machine translation while maintaining linear runtime complexity. We have uncovered the latent structure that the ByteNet has learned, and found that it reflects the expected alignment between the tokens in sentences."}], "references": [{"title": "Deep neural network language models", "author": ["Ebru Arisoy", "Tara N. Sainath", "Brian Kingsbury", "Bhuvana Ramabhadran"], "venue": "In Proceedings of the NAACL-HLT 2012 Workshop. Association for Computational Linguistics,", "citeRegEx": "Arisoy et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Arisoy et al\\.", "year": 2012}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio"], "venue": "CoRR, abs/1409.0473,", "citeRegEx": "Bahdanau et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "A neural probabilistic language model", "author": ["Yoshua Bengio", "R\u00e9jean Ducharme", "Pascal Vincent", "Christian Jauvin"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Bengio et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2003}, {"title": "Semantic image segmentation with deep convolutional nets and fully connected crfs", "author": ["Liang-Chieh Chen", "George Papandreou", "Iasonas Kokkinos", "Kevin Murphy", "Alan L. Yuille"], "venue": "CoRR, abs/1412.7062,", "citeRegEx": "Chen et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2014}, {"title": "Learning phrase representations using RNN encoder-decoder for statistical machine translation", "author": ["Kyunghyun Cho", "Bart van Merrienboer", "\u00c7aglar G\u00fcl\u00e7ehre", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio"], "venue": "CoRR, abs/1406.1078,", "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Gated feedback recurrent neural networks", "author": ["Junyoung Chung", "Caglar G\u00fcl\u00e7ehre", "Kyunghyun Cho", "Yoshua Bengio"], "venue": "CoRR, abs/1502.02367,", "citeRegEx": "Chung et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chung et al\\.", "year": 2015}, {"title": "Hierarchical multiscale recurrent neural networks", "author": ["Junyoung Chung", "Sungjin Ahn", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1609.01704,", "citeRegEx": "Chung et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Chung et al\\.", "year": 2016}, {"title": "A character-level decoder without explicit segmentation for neural machine translation", "author": ["Junyoung Chung", "Kyunghyun Cho", "Yoshua Bengio"], "venue": "In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "Chung et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Chung et al\\.", "year": 2016}, {"title": "Eu-bridge mt: Combined machine translation", "author": ["Markus Freitag", "Stephan Peitz", "Joern Wuebker", "Hermann Ney", "Matthias Huck", "Rico Sennrich", "Nadir Durrani", "Maria Nadejde", "Philip Williams", "Philipp Koehn", "Teresa Herrmann", "Eunah Cho", "Alex Waibel"], "venue": "In ACL 2014 Ninth Workshop on Statistical Machine Translation,", "citeRegEx": "Freitag et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Freitag et al\\.", "year": 2014}, {"title": "Generating sequences with recurrent neural networks", "author": ["Alex Graves"], "venue": "arXiv preprint arXiv:1308.0850,", "citeRegEx": "Graves.,? \\Q2013\\E", "shortCiteRegEx": "Graves.", "year": 2013}, {"title": "Deep residual learning for image recognition", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"], "venue": "arXiv preprint arXiv:1512.03385,", "citeRegEx": "He et al\\.,? \\Q2015\\E", "shortCiteRegEx": "He et al\\.", "year": 2015}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "Hochreiter and Schmidhuber.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Gradient flow in recurrent nets: the difficulty of learning long-term dependencies", "author": ["Sepp Hochreiter", "Yoshua Bengio", "Paolo Frasconi"], "venue": null, "citeRegEx": "Hochreiter et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 2001}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["Sergey Ioffe", "Christian Szegedy"], "venue": "In ICML,", "citeRegEx": "Ioffe and Szegedy.,? \\Q2015\\E", "shortCiteRegEx": "Ioffe and Szegedy.", "year": 2015}, {"title": "Can active memory replace attention", "author": ["Lukasz Kaiser", "Samy Bengio"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "Kaiser and Bengio.,? \\Q2016\\E", "shortCiteRegEx": "Kaiser and Bengio.", "year": 2016}, {"title": "Recurrent continuous translation models", "author": ["Nal Kalchbrenner", "Phil Blunsom"], "venue": "In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Kalchbrenner and Blunsom.,? \\Q2013\\E", "shortCiteRegEx": "Kalchbrenner and Blunsom.", "year": 2013}, {"title": "Grid long short-term memory", "author": ["Nal Kalchbrenner", "Ivo Danihelka", "Alex Graves"], "venue": "International Conference on Learning Representations,", "citeRegEx": "Kalchbrenner et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2016}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik P. Kingma", "Jimmy Ba"], "venue": "CoRR, abs/1412.6980,", "citeRegEx": "Kingma and Ba.,? \\Q2014\\E", "shortCiteRegEx": "Kingma and Ba.", "year": 2014}, {"title": "Achieving open vocabulary neural machine translation with hybrid word-character models", "author": ["Minh-Thang Luong", "Christopher D. Manning"], "venue": null, "citeRegEx": "Luong and Manning.,? \\Q2016\\E", "shortCiteRegEx": "Luong and Manning.", "year": 2016}, {"title": "Effective approaches to attentionbased neural machine translation", "author": ["Minh-Thang Luong", "Hieu Pham", "Christopher D. Manning"], "venue": "In EMNLP,", "citeRegEx": "Luong et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Recurrent neural network based language model", "author": ["Tomas Mikolov", "Martin Karafi\u00e1t", "Luk\u00e1s Burget", "Jan Cernock\u00fd", "Sanjeev Khudanpur"], "venue": "INTERSPEECH", "citeRegEx": "Mikolov et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2010}, {"title": "Recurrent memory array structures", "author": ["Kamil Rocki"], "venue": "arXiv preprint arXiv:1607.03085,", "citeRegEx": "Rocki.,? \\Q2016\\E", "shortCiteRegEx": "Rocki.", "year": 2016}, {"title": "Deep inside convolutional networks: Visualising image classification models and saliency maps", "author": ["Karen Simonyan", "Andrea Vedaldi", "Andrew Zisserman"], "venue": null, "citeRegEx": "Simonyan et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Simonyan et al\\.", "year": 2013}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V. Le"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Wavenet: A generative model for raw audio", "author": ["Aaron van den Oord", "Sander Dieleman", "Heiga Zen", "Karen Simonyan", "Oriol Vinyals", "Alex Graves", "Nal Kalchbrenner", "Andrew Senior", "Koray Kavukcuoglu"], "venue": "CoRR, abs/1609.03499,", "citeRegEx": "Oord et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Oord et al\\.", "year": 2016}, {"title": "Pixel recurrent neural networks", "author": ["A\u00e4ron van den Oord", "Nal Kalchbrenner", "Koray Kavukcuoglu"], "venue": "In ICML,", "citeRegEx": "Oord et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Oord et al\\.", "year": 2016}, {"title": "Edinburgh\u2019s syntax-based systems at WMT 2015", "author": ["Philip Williams", "Rico Sennrich", "Maria Nadejde", "Matthias Huck", "Philipp Koehn"], "venue": "In Proceedings of the Tenth Workshop on Statistical Machine Translation,", "citeRegEx": "Williams et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Williams et al\\.", "year": 2015}, {"title": "On multiplicative integration with recurrent neural networks", "author": ["Yuhuai Wu", "Saizheng Zhang", "Ying Zhang", "Yoshua Bengio", "Ruslan Salakhutdinov"], "venue": "arXiv preprint arXiv:1606.06630,", "citeRegEx": "Wu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Wu et al\\.", "year": 2016}, {"title": "Multi-scale context aggregation by dilated convolutions", "author": ["Fisher Yu", "Vladlen Koltun"], "venue": "CoRR, abs/1511.07122,", "citeRegEx": "Yu and Koltun.,? \\Q2015\\E", "shortCiteRegEx": "Yu and Koltun.", "year": 2015}, {"title": "Deep recurrent models with fast-forward connections for neural machine translation", "author": ["Jie Zhou", "Ying Cao", "Xuguang Wang", "Peng Li", "Wei Xu"], "venue": "arXiv preprint arXiv:1606.04199,", "citeRegEx": "Zhou et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Zhou et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 2, "context": "In neural language modelling, a neural network estimates a distribution over sequences of words or characters that belong to a given language (Bengio et al., 2003).", "startOffset": 142, "endOffset": 163}, {"referenceID": 15, "context": "In the latter case the network can be thought of as composed of two sub-networks, a source network that processes the source sequence into a representation and a target network that uses the representation of the source to generate the target sequence (Kalchbrenner and Blunsom, 2013) Recurrent neural networks (RNN) are powerful sequence models (Hochreiter and Schmidhuber, 1997) and are widely used in language modelling (Mikolov et al.", "startOffset": 252, "endOffset": 284}, {"referenceID": 11, "context": "In the latter case the network can be thought of as composed of two sub-networks, a source network that processes the source sequence into a representation and a target network that uses the representation of the source to generate the target sequence (Kalchbrenner and Blunsom, 2013) Recurrent neural networks (RNN) are powerful sequence models (Hochreiter and Schmidhuber, 1997) and are widely used in language modelling (Mikolov et al.", "startOffset": 346, "endOffset": 380}, {"referenceID": 20, "context": "In the latter case the network can be thought of as composed of two sub-networks, a source network that processes the source sequence into a representation and a target network that uses the representation of the source to generate the target sequence (Kalchbrenner and Blunsom, 2013) Recurrent neural networks (RNN) are powerful sequence models (Hochreiter and Schmidhuber, 1997) and are widely used in language modelling (Mikolov et al., 2010), yet they have a potential drawback.", "startOffset": 423, "endOffset": 445}, {"referenceID": 12, "context": "The larger the distance the harder it is to learn dependencies between the points (Hochreiter et al., 2001).", "startOffset": 82, "endOffset": 107}, {"referenceID": 15, "context": "A number of neural architectures have been proposed for modelling translation (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014; Bahdanau et al., 2014; Kalchbrenner et al., 2016a; Kaiser and Bengio, 2016).", "startOffset": 78, "endOffset": 228}, {"referenceID": 23, "context": "A number of neural architectures have been proposed for modelling translation (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014; Bahdanau et al., 2014; Kalchbrenner et al., 2016a; Kaiser and Bengio, 2016).", "startOffset": 78, "endOffset": 228}, {"referenceID": 4, "context": "A number of neural architectures have been proposed for modelling translation (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014; Bahdanau et al., 2014; Kalchbrenner et al., 2016a; Kaiser and Bengio, 2016).", "startOffset": 78, "endOffset": 228}, {"referenceID": 1, "context": "A number of neural architectures have been proposed for modelling translation (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014; Bahdanau et al., 2014; Kalchbrenner et al., 2016a; Kaiser and Bengio, 2016).", "startOffset": 78, "endOffset": 228}, {"referenceID": 14, "context": "A number of neural architectures have been proposed for modelling translation (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014; Bahdanau et al., 2014; Kalchbrenner et al., 2016a; Kaiser and Bengio, 2016).", "startOffset": 78, "endOffset": 228}, {"referenceID": 23, "context": "The ByteNet with recurrent sub-networks may be viewed as a strict generalization of the RNN Enc-Dec network (Sutskever et al., 2014; Cho et al., 2014) (Sect.", "startOffset": 108, "endOffset": 150}, {"referenceID": 4, "context": "The ByteNet with recurrent sub-networks may be viewed as a strict generalization of the RNN Enc-Dec network (Sutskever et al., 2014; Cho et al., 2014) (Sect.", "startOffset": 108, "endOffset": 150}, {"referenceID": 20, "context": "In contrast to neural language models based on RNNs (Mikolov et al., 2010) or on feed-forward networks (Bengio et al.", "startOffset": 52, "endOffset": 74}, {"referenceID": 2, "context": ", 2010) or on feed-forward networks (Bengio et al., 2003; Arisoy et al., 2012), the ByteNet Decoder is based on a novel convolutional structure designed to capture a very long range of past inputs.", "startOffset": 36, "endOffset": 78}, {"referenceID": 0, "context": ", 2010) or on feed-forward networks (Bengio et al., 2003; Arisoy et al., 2012), the ByteNet Decoder is based on a novel convolutional structure designed to capture a very long range of past inputs.", "startOffset": 36, "endOffset": 78}, {"referenceID": 22, "context": "We use gradient-based visualization (Simonyan et al., 2013) to reveal the latent structure that arises between the source and target sequences in the ByteNet.", "startOffset": 36, "endOffset": 59}, {"referenceID": 15, "context": "The network that models p(t|s) is composed of two sub-networks, a source network that processes the source string into a representation and a target network that uses the source representation to generate the target string (Kalchbrenner and Blunsom, 2013).", "startOffset": 223, "endOffset": 255}, {"referenceID": 12, "context": "Shorter paths whose length is decoupled from the sequence distance between the two tokens have the potential to better propagate the signals (Hochreiter et al., 2001) and to let the network learn long-range dependencies more easily.", "startOffset": 141, "endOffset": 166}, {"referenceID": 10, "context": "Figure 3: Left: Residual block with ReLUs (He et al., 2015) adapted for decoders.", "startOffset": 42, "endOffset": 59}, {"referenceID": 3, "context": "The masked convolutions use dilation to increase the receptive field of the target network (Chen et al., 2014; Yu and Koltun, 2015).", "startOffset": 91, "endOffset": 131}, {"referenceID": 28, "context": "The masked convolutions use dilation to increase the receptive field of the target network (Chen et al., 2014; Yu and Koltun, 2015).", "startOffset": 91, "endOffset": 131}, {"referenceID": 10, "context": "Each layer is wrapped in a residual block that contains additional convolutional layers with filters of size 1 (He et al., 2015).", "startOffset": 111, "endOffset": 128}, {"referenceID": 13, "context": "We introduce a modification to Batch Normalization (BN) (Ioffe and Szegedy, 2015) in order to make it applicable to target networks and decoders.", "startOffset": 56, "endOffset": 81}, {"referenceID": 23, "context": "We can see that the RNN Enc-Dec network (Sutskever et al., 2014; Cho et al., 2014) is a Recurrent ByteNet where all connections between source and target \u2013 except for the first one that connects s0 and t0 \u2013 have been severed.", "startOffset": 40, "endOffset": 82}, {"referenceID": 4, "context": "We can see that the RNN Enc-Dec network (Sutskever et al., 2014; Cho et al., 2014) is a Recurrent ByteNet where all connections between source and target \u2013 except for the first one that connects s0 and t0 \u2013 have been severed.", "startOffset": 40, "endOffset": 82}, {"referenceID": 15, "context": "In our comparison we consider the following neural translation models: the Recurrent Continuous Translation Model (RCTM) 1 and 2 (Kalchbrenner and Blunsom, 2013); the RNN Enc-Dec (Sutskever et al.", "startOffset": 129, "endOffset": 161}, {"referenceID": 23, "context": "In our comparison we consider the following neural translation models: the Recurrent Continuous Translation Model (RCTM) 1 and 2 (Kalchbrenner and Blunsom, 2013); the RNN Enc-Dec (Sutskever et al., 2014; Cho et al., 2014); the RNN Enc-Dec Att with the attentional pooling mechanism (Bahdanau et al.", "startOffset": 179, "endOffset": 221}, {"referenceID": 4, "context": "In our comparison we consider the following neural translation models: the Recurrent Continuous Translation Model (RCTM) 1 and 2 (Kalchbrenner and Blunsom, 2013); the RNN Enc-Dec (Sutskever et al., 2014; Cho et al., 2014); the RNN Enc-Dec Att with the attentional pooling mechanism (Bahdanau et al.", "startOffset": 179, "endOffset": 221}, {"referenceID": 1, "context": ", 2014); the RNN Enc-Dec Att with the attentional pooling mechanism (Bahdanau et al., 2014) of which there are a few variations (Luong et al.", "startOffset": 68, "endOffset": 91}, {"referenceID": 19, "context": ", 2014) of which there are a few variations (Luong et al., 2015; Chung et al., 2016a); the Grid LSTM translation model (Kalchbrenner et al.", "startOffset": 44, "endOffset": 85}, {"referenceID": 14, "context": ", 2016a) that uses a multi-dimensional architecture; the Extended Neural GPU model (Kaiser and Bengio, 2016) that has a convolutional RNN architecture; the ByteNet and the two Recurrent ByteNet variants.", "startOffset": 83, "endOffset": 108}, {"referenceID": 18, "context": "The RNN Enc-Dec, however, does not preserve the source sequence resolution, a feature that aggravates learning for long sequences such as those in character-level machine translation (Luong and Manning, 2016).", "startOffset": 183, "endOffset": 208}, {"referenceID": 5, "context": "We use the Hutter Prize version of the Wikipedia dataset and follow the standard split where the first 90 million bytes are used for training, the next 5 million bytes are used for validation and the last 5 million bytes are used for testing (Chung et al., 2015).", "startOffset": 242, "endOffset": 262}, {"referenceID": 9, "context": "Model Test Stacked LSTM (Graves, 2013) 1.", "startOffset": 24, "endOffset": 38}, {"referenceID": 5, "context": "67 GF-LSTM (Chung et al., 2015) 1.", "startOffset": 11, "endOffset": 31}, {"referenceID": 21, "context": "42 Recurrent Memory Array Structures (Rocki, 2016) 1.", "startOffset": 37, "endOffset": 50}, {"referenceID": 29, "context": "9 RNN Enc-Dec Att + deep (Zhou et al., 2016) 20.", "startOffset": 25, "endOffset": 44}, {"referenceID": 8, "context": "Result (1) is from (Freitag et al., 2014), result (2) is from (Williams et al.", "startOffset": 19, "endOffset": 41}, {"referenceID": 26, "context": ", 2014), result (2) is from (Williams et al., 2015), results (3) are from (Luong et al.", "startOffset": 28, "endOffset": 51}, {"referenceID": 19, "context": ", 2015), results (3) are from (Luong et al., 2015) and results (4) are from (Chung et al.", "startOffset": 30, "endOffset": 50}, {"referenceID": 17, "context": "For the optimization we use Adam (Kingma and Ba, 2014) with a learning rate of 10\u22122 and a weight decay term of 10\u22125.", "startOffset": 33, "endOffset": 54}, {"referenceID": 11, "context": "All the results except for the ByteNet result are obtained using some variant of the LSTM recurrent neural network (Hochreiter and Schmidhuber, 1997).", "startOffset": 115, "endOffset": 149}, {"referenceID": 1, "context": "In contrast with the attentional pooling mechanism (Bahdanau et al., 2014), this general technique allows us to inspect not just dependencies of the outputs on the source inputs, but also dependencies of the outputs on previous target inputs, or on any other neural network layers.", "startOffset": 51, "endOffset": 74}], "year": 2016, "abstractText": "We present a neural architecture for sequence processing. The ByteNet is a stack of two dilated convolutional neural networks, one to encode the source sequence and one to decode the target sequence, where the target network unfolds dynamically to generate variable length outputs. The ByteNet has two core properties: it runs in time that is linear in the length of the sequences and it preserves the sequences\u2019 temporal resolution. The ByteNet decoder attains state-of-the-art performance on character-level language modelling and outperforms the previous best results obtained with recurrent neural networks. The ByteNet also achieves a performance on raw character-level machine translation that approaches that of the best neural translation models that run in quadratic time. The implicit structure learnt by the ByteNet mirrors the expected alignments between the sequences.", "creator": "LaTeX with hyperref package"}}}