{"id": "1611.00050", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-Oct-2016", "title": "Exploiting Spatio-Temporal Structure with Recurrent Winner-Take-All Networks", "abstract": "We propose a convolutional recurrent neural network, with Winner-Take-All dropout for high dimensional unsupervised feature learning in multi-dimensional time series. We apply the proposedmethod for object recognition with temporal context in videos and obtain better results than comparable methods in the literature, including the Deep Predictive Coding Networks previously proposed by Chalasani and Principe.Our contributions can be summarized as a scalable reinterpretation of the Deep Predictive Coding Networks trained end-to-end with backpropagation through time, an extension of the previously proposed Winner-Take-All Autoencoders to sequences in time, and a new technique for initializing and regularizing convolutional-recurrent neural networks.", "histories": [["v1", "Mon, 31 Oct 2016 21:16:46 GMT  (3299kb)", "https://arxiv.org/abs/1611.00050v1", "under review"], ["v2", "Wed, 15 Mar 2017 16:01:43 GMT  (3299kb)", "http://arxiv.org/abs/1611.00050v2", "under review"]], "COMMENTS": "under review", "reviews": [], "SUBJECTS": "cs.LG cs.CV", "authors": ["eder santana", "matthew emigh", "pablo zegers", "jose c principe"], "accepted": false, "id": "1611.00050"}, "pdf": {"name": "1611.00050.pdf", "metadata": {"source": "CRF", "title": "Exploiting Spatio-Temporal Structure with Recurrent Winner-Take-All Networks", "authors": ["Eder Santana", "Matthew Emigh", "Pablo Zegers", "Jose C Principe"], "emails": [], "sections": [{"heading": null, "text": "This type of learning is not only the way of learning itself, but also the way of learning that is used in the world of the Internet and computer technology. (This is the way of learning that can be observed in the world of learning in the multidimensional world.) We apply the proposed method for object recognition in videos and get better results than comparable methods in literature, including the Deep Predictive Coding Networks, previously used by Chalasani and Principe.Our contributions can be understood as a vivid interpretation of the interpretation of videos and achieve better results than comparable methods in literature, including the Deep Predictive Coding Networks, previously used by Chalasani and Principe.Our contributions can be considered as a way of learning in the multidimensional time series."}, {"heading": "II. DEEP PREDICTIVE CODING NETWORKS", "text": "In fact, it is the case that most of them are in a position to go into a different world, in which they are able to move, in which they move, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they, in which they live, in which they, in which they live, in which they live, in which they live, in which they live."}, {"heading": "III. RECURRENT WINNER-TAKE-ALL NETWORK", "text": "This year, it has reached the point where it will be able to put itself at the top of the list."}, {"heading": "IV. RELATED WORK", "text": "This research is related to DPCNs and a larger family of deeply unattended neural networks [24] [30] [17]. The above Winner-Take-All Autoencoders (WTA-AE) [8] consist of a deep revolutionary encoder and a single-layer revolutionary decoder that inspired our choice. WTA-AE disregards all elements of a revolutionary channel map, but the largest that forces the entire system to learn robust, sparse properties for reconstruction. With the proposed Convolutionary RNN, our method can be considered a natural extension of WTAAE. Unsupervised Learning with temporal context, previously studied by Goroshin et al. [3] Wang and Gupta [31] Their approach is based on metric learning of related frames in video, but their approaches have not been able to learn long-term dependencies, as they only required a simple omezanic innovation between frames."}, {"heading": "V. EXPERIMENTS", "text": "This year is the highest in the history of the country."}, {"heading": "VI. PRELIMINARY RESULTS AND FUTURE WORK", "text": "In all our experiments, we study the extraction of individual characteristics, i.e. we do not use pooling or streaking. In the experiments not discussed in this paper, we explore pooling and unpooling in the proposed architecture. However, this did not lead to a significant improvement in the results. Nevertheless, it was found that the layer training could be carried out in different dimensions. They explored a Convolutionary WTA-AE on the raw data. They explored the characteristics in the top of the pooled characteristics. They explored the proposed architecture in a different way. The first way led to a breakdown of contemporary characteristics in a single map."}, {"heading": "VII. CONCLUSIONS", "text": "In this paper, we propose RWTA, a profound recursive neural network with winner-take-all drop-out extraction of traits from time series. Our contributions were threefold: 1) a scalable, consistently differentiable reinterpretation of the sparse spatio-temporal trait extraction in Deep Predictive Coding Networks [7]; 2) an extension of Winner-Take-All autoencoders [8] to time using dynamic neural networks; 3) a new technique for initializing and regulating [11] Convolutionary Recursive Neural Networks [9] [10]. We demonstrated that our method outperforms DPCNs and other similar methods in contextual object recognition."}, {"heading": "APPENDIX A RWTA LEARNED INVARIANCES", "text": "Suppose that the input stream xt, where t is a countable index, is fed into two modules = = K = \u00b7 In addition, a static and a recursive neural network = K = K (i.e. a limited neural network) is fed into each: o E t = e (xt) (10) o R = r (xt, ot \u2212 1), (11) where r is a recursive neural network and the state ot \u2212 1 provides the context for the prediction, these two outputs are fed into a Siamese decoder, the other two outputs E = d (o E) (12) f R = d (o R t) (13) training is performed in such a way that the following expression is minimized: E = x x x x t (xt \u2212 f E \u2212 1) 2 + xxt (xt) 2 + xt (xt) 2 (o) the main effect of the WTA algorithm [?] [?] when applied to E and divides the input space into the corresponding volume is R."}], "references": [{"title": "Learning to predict: Exposure to temporal sequences facilitates prediction of future events", "author": ["R. Baker", "M. Dexter", "T.E. Hardwicke", "A. Goldstone", "Z. Kourtzi"], "venue": "Vision Research, vol. 99, pp. 124 \u2013 133, 2014.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2014}, {"title": "Temporal coherence, natural image sequences, and the visual cortex", "author": ["J. Hurri", "A. Hyv\u00e4rinen"], "venue": "Advances in Neural Information Processing Systems, 2002, pp. 141\u2013148.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2002}, {"title": "Unsupervised learning of spatiotemporally coherent metrics", "author": ["R. Goroshin", "J. Bruna", "J. Tompson", "D. Eigen", "Y. LeCun"], "venue": "The IEEE International Conference on Computer Vision (ICCV), December 2015.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2015}, {"title": "Unsupervised learning of visual representations using videos", "author": ["X. Wang", "A. Gupta"], "venue": "CoRR, vol. abs/1505.00687, 2015. [Online]. Available: http://arxiv.org/abs/1505.00687", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning to see by moving", "author": ["P. Agrawal", "J. Carreira", "J. Malik"], "venue": "CoRR, vol. abs/1505.01596, 2015. [Online]. Available: http://arxiv.org/abs/1505.01596  UNDER REVIEW  8", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep predictive coding networks", "author": ["R. Chalasani", "J. Principe"], "venue": "Workshop at International Conference on Learning Representations (ICLR), 2013.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2013}, {"title": "Context dependent encoding using convolutional dynamic networks", "author": ["R. Chalasani", "J.C. Principe"], "venue": "Neural Networks and Learning Systems, IEEE Transactions on, vol. 26, no. 9, pp. 1992\u20132004, 2015.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1992}, {"title": "Winner-take-all autoencoders", "author": ["A. Makhzani", "B.J. Frey"], "venue": "Advances in Neural Information Processing Systems, 2015, pp. 2773\u20132781.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "Recurrent convolutional neural network for object recognition", "author": ["M. Liang", "X. Hu"], "venue": "CVPR, June 2015.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2015}, {"title": "Convolutional lstm network: A machine learning approach for precipitation nowcasting", "author": ["X. Shi", "Z. Chen", "H. Wang", "D.-Y. Yeung", "W.-K. Wong", "W.-c. Woo"], "venue": "arXiv preprint arXiv:1506.04214, 2015.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "Multi-task sequence to sequence learning", "author": ["M. Luong", "Q.V. Le", "I. Sutskever", "O. Vinyals", "L. Kaiser"], "venue": "CoRR, vol. abs/1511.06114, 2015. [Online]. Available: http://arxiv.org/abs/1511.06114", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "Supervised translation-invariant sparse coding", "author": ["J. Yang", "K. Yu", "T. Huang"], "venue": "Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference on. IEEE, 2010, pp. 3517\u20133524.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2010}, {"title": "Learning convolutional feature hierarchies for visual recognition", "author": ["K. Kavukcuoglu", "P. Sermanet", "Y.-L. Boureau", "K. Gregor", "M. Mathieu", "Y.L. Cun"], "venue": "Advances in neural information processing systems, 2010, pp. 1090\u20131098.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2010}, {"title": "Deconvolutional networks", "author": ["M.D. Zeiler", "D. Krishnan", "G.W. Taylor", "R. Fergus"], "venue": "Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference on. IEEE, 2010, pp. 2528\u20132535.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2010}, {"title": "Video-based face recognition using probabilistic appearance manifolds", "author": ["K. Lee", "J. Ho", "M. Yang", "D. Kriegman"], "venue": "IEEE Conf. On Computer Vision and Pattern Recognition, vol. 1, pp. 313\u2013320, 2003.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2003}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "Advances in neural information processing systems, 2012, pp. 1097\u20131105.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2012}, {"title": "Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations", "author": ["H. Lee", "R. Grosse", "R. Ranganath", "A.Y. Ng"], "venue": "Proceedings of the 26th Annual International Conference on Machine Learning. ACM, 2009, pp. 609\u2013616.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2009}, {"title": "On rectified linear units for speech processing", "author": ["M.D. Zeiler", "M. Ranzato", "R. Monga", "M. Mao", "K. Yang", "Q.V. Le", "P. Nguyen", "A. Senior", "V. Vanhoucke", "J. Dean"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on. IEEE, 2013, pp. 3517\u20133521.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2013}, {"title": "Reducing the dimensionality of data with neural networks", "author": ["G.E. Hinton", "R.R. Salakhutdinov"], "venue": "Science, vol. 313, no. 5786, pp. 504\u2013507, 2006.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2006}, {"title": "Generating sequences with recurrent neural networks", "author": ["A. Graves"], "venue": "arXiv preprint arXiv:1308.0850, 2013.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2013}, {"title": "Convolutional lstm network: A machine learning approach for precipitation nowcasting", "author": ["S. Xingjian", "Z. Chen", "H. Wang", "D.-Y. Yeung", "W.-k. Wong", "W.-c. WOO"], "venue": "Advances in Neural Information Processing Systems, 2015, pp. 802\u2013810.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2015}, {"title": "Recurrent convolutional neural network for object recognition", "author": ["M. Liang", "X. Hu"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2015, pp. 3367\u20133375.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2015}, {"title": "Training recurrent neural networks", "author": ["I. Sutskever"], "venue": "Ph.D. dissertation, University of Toronto, 2013.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2013}, {"title": "Emergence of simple-cell receptive field properties by learning a sparse code for natural images", "author": ["B.A. Olshausen"], "venue": "Nature, vol. 381, no. 6583, pp. 607\u2013609, 1996.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 1996}, {"title": "A fast iterative shrinkage-thresholding algorithm for linear inverse problems", "author": ["A. Beck", "M. Teboulle"], "venue": "SIAM journal on imaging sciences, vol. 2, no. 1, pp. 183\u2013202, 2009.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2009}, {"title": "Learning fast approximations of sparse coding", "author": ["K. Gregor", "Y. LeCun"], "venue": "Proceedings of the 27th International Conference on Machine Learning (ICML-10), 2010, pp. 399\u2013406.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2010}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors", "author": ["G.E. Hinton", "N. Srivastava", "A. Krizhevsky", "I. Sutskever", "R.R. Salakhutdinov"], "venue": "arXiv preprint arXiv:1207.0580, 2012.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2012}, {"title": "Separate visual pathways for perception and action", "author": ["M.A. Goodale", "A.D. Milner"], "venue": "Trends in neurosciences, vol. 15, no. 1, pp. 20\u2013 25, 1992.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 1992}, {"title": "Backpropagation through time: what it does and how to do it", "author": ["P.J. Werbos"], "venue": "Proceedings of the IEEE, vol. 78, no. 10, pp. 1550\u20131560, 1990.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 1990}, {"title": "Unsupervised learning of visual representations by solving jigsaw puzzles", "author": ["M. Noroozi", "P. Favaro"], "venue": "arXiv preprint arXiv:1603.09246, 2016.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2016}, {"title": "Unsupervised learning of visual representations using videos", "author": ["X. Wang", "A. Gupta"], "venue": "2015 IEEE International Conference on Computer Vision (ICCV), Dec 2015, pp. 2794\u20132802.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2015}, {"title": "Network in network", "author": ["M. Lin", "Q. Chen", "S. Yan"], "venue": "CoRR, vol. abs/1312.4400, 2013. [Online]. Available: http://arxiv.org/abs/1312.4400", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2013}, {"title": "Spatio-temporal video autoencoder with differentiable memory", "author": ["V. Patraucean", "A. Handa", "R. Cipolla"], "venue": "arXiv preprint arXiv:1511.06309, 2015.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep predictive coding networks for video prediction and unsupervised learning", "author": ["W. Lotter", "G. Kreiman", "D. Cox"], "venue": "arXiv preprint arXiv:1605.08104, 2016.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2016}, {"title": "Adam: A method for stochastic optimization", "author": ["D. Kingma", "J. Ba"], "venue": "arXiv preprint arXiv:1412.6980, 2014.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2014}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["X. Glorot", "Y. Bengio"], "venue": "International conference on artificial intelligence and statistics, 2010, pp. 249\u2013256.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2010}, {"title": "Columbia object image library (coil-20)", "author": ["S.A. Nene", "S.K. Nayar", "H. Murase"], "venue": "technical report CUCS-005-96, Tech. Rep., 1996.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 1996}, {"title": "Deep learning from temporal coherence in video", "author": ["H. Mobahi", "R. Collobert", "J. Weston"], "venue": "Proceedings of the 26th Annual International Conference on Machine Learning. ACM, 2009, pp. 737\u2013744.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2009}, {"title": "Learning hierarchical invariant spatio-temporal features for action recognition with independent subspace analysis", "author": ["Q.V. Le", "W.Y. Zou", "S.Y. Yeung", "A.Y. Ng"], "venue": "CVPR. IEEE, 2011, pp. 3361\u20133368.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2011}, {"title": "Manifold discriminant analysis", "author": ["R. Wang", "X. Chen"], "venue": "Computer Vision and Pattern Recognition, 2009. CVPR 2009. IEEE Conference on. IEEE, 2009, pp. 429\u2013436.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2009}, {"title": "Sparse approximated nearest points for image set classification", "author": ["Y. Hu", "A.S. Mian", "R. Owens"], "venue": "Computer vision and pattern recognition (CVPR), 2011 IEEE conference on. IEEE, 2011, pp. 121\u2013128.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2011}, {"title": "Two-stream convolutional networks for action recognition in videos", "author": ["K. Simonyan", "A. Zisserman"], "venue": "Advances in Neural Information Processing Systems, 2014, pp. 568\u2013576.", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning spatiotemporal features with 3d convolutional networks", "author": ["D. Tran", "L. Bourdev", "R. Fergus", "L. Torresani", "M. Paluri"], "venue": "arXiv preprint arXiv:1412.0767, 2014.", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 0, "context": "It has been suggested that temporal information could be a exploited as a possible source of supervision[1].", "startOffset": 104, "endOffset": 107}, {"referenceID": 1, "context": "This smooth, temporally coherent representation is biologically plausible and has been shown to self-organize V1-like Gabor filters when applied to learn image transition representations in video [2].", "startOffset": 196, "endOffset": 199}, {"referenceID": 2, "context": "A machine learning application of this paradigm is to use temporal coherence as a proxy for learning sensory representations without strong supervision or explicit labels [3][4][5].", "startOffset": 171, "endOffset": 174}, {"referenceID": 3, "context": "A machine learning application of this paradigm is to use temporal coherence as a proxy for learning sensory representations without strong supervision or explicit labels [3][4][5].", "startOffset": 174, "endOffset": 177}, {"referenceID": 4, "context": "A machine learning application of this paradigm is to use temporal coherence as a proxy for learning sensory representations without strong supervision or explicit labels [3][4][5].", "startOffset": 177, "endOffset": 180}, {"referenceID": 5, "context": "Chalasani and Principe [6] developed a hierarchical, distributed, generative architecture called Deep Predictive Coding Networks (DPCN) that learns with free energy to build spatio-temporal shift-invariant representations of input video streams.", "startOffset": 23, "endOffset": 26}, {"referenceID": 5, "context": "First, we evolved the self-organizing object recognition in video work of Chalasani and Principe [6][7] by developing a scalable counterpart to the DPCN architecture and algorithms.", "startOffset": 97, "endOffset": 100}, {"referenceID": 6, "context": "First, we evolved the self-organizing object recognition in video work of Chalasani and Principe [6][7] by developing a scalable counterpart to the DPCN architecture and algorithms.", "startOffset": 100, "endOffset": 103}, {"referenceID": 7, "context": "Second, we build our contributions on top of recent findings in convolutional winner-take-all autoencoders [8] and convolutionalrecurrent neural networks [9][10].", "startOffset": 107, "endOffset": 110}, {"referenceID": 8, "context": "Second, we build our contributions on top of recent findings in convolutional winner-take-all autoencoders [8] and convolutionalrecurrent neural networks [9][10].", "startOffset": 154, "endOffset": 157}, {"referenceID": 9, "context": "Second, we build our contributions on top of recent findings in convolutional winner-take-all autoencoders [8] and convolutionalrecurrent neural networks [9][10].", "startOffset": 157, "endOffset": 161}, {"referenceID": 10, "context": "[11] showed that RNNs benefit from unsupervised pre-training and multi-task learning.", "startOffset": 0, "endOffset": 4}, {"referenceID": 5, "context": "Chalasani and Principe [6][7] proposed a generative, dynamical, hierarchical model with sparse coefficients st:", "startOffset": 23, "endOffset": 26}, {"referenceID": 6, "context": "Chalasani and Principe [6][7] proposed a generative, dynamical, hierarchical model with sparse coefficients st:", "startOffset": 26, "endOffset": 29}, {"referenceID": 11, "context": "DPCNs can be extended to handle large images by substituting the projection matrices A,B with convolutions and the latent codes st,ut with feature maps, similarly to convolutional sparse coding [12].", "startOffset": 194, "endOffset": 198}, {"referenceID": 6, "context": "Chalasani and Principe [7] used two layer convolutional DPCNs to extract features ut that were then used to train SVMs for classification surpassing several other sparse auto-encoding [13] and deconvolutional [14] techniques in accuracy on Caltech 101, Honda/UCSD faces [15], Celebrity Faces, and Coil-100 datasets.", "startOffset": 23, "endOffset": 26}, {"referenceID": 12, "context": "Chalasani and Principe [7] used two layer convolutional DPCNs to extract features ut that were then used to train SVMs for classification surpassing several other sparse auto-encoding [13] and deconvolutional [14] techniques in accuracy on Caltech 101, Honda/UCSD faces [15], Celebrity Faces, and Coil-100 datasets.", "startOffset": 184, "endOffset": 188}, {"referenceID": 13, "context": "Chalasani and Principe [7] used two layer convolutional DPCNs to extract features ut that were then used to train SVMs for classification surpassing several other sparse auto-encoding [13] and deconvolutional [14] techniques in accuracy on Caltech 101, Honda/UCSD faces [15], Celebrity Faces, and Coil-100 datasets.", "startOffset": 209, "endOffset": 213}, {"referenceID": 14, "context": "Chalasani and Principe [7] used two layer convolutional DPCNs to extract features ut that were then used to train SVMs for classification surpassing several other sparse auto-encoding [13] and deconvolutional [14] techniques in accuracy on Caltech 101, Honda/UCSD faces [15], Celebrity Faces, and Coil-100 datasets.", "startOffset": 270, "endOffset": 274}, {"referenceID": 15, "context": "In fact, previous experiments with DPCNs failed to learn better features with more than two layers, which is not usually the case for other deep learning architectures either supervised [16] or unsupervised [17].", "startOffset": 186, "endOffset": 190}, {"referenceID": 16, "context": "In fact, previous experiments with DPCNs failed to learn better features with more than two layers, which is not usually the case for other deep learning architectures either supervised [16] or unsupervised [17].", "startOffset": 207, "endOffset": 211}, {"referenceID": 15, "context": "This allows for simpler model building blocks which are still powerful enough for vision [16] and can be possibly extended to other datasets, such as audio [18].", "startOffset": 89, "endOffset": 93}, {"referenceID": 17, "context": "This allows for simpler model building blocks which are still powerful enough for vision [16] and can be possibly extended to other datasets, such as audio [18].", "startOffset": 156, "endOffset": 160}, {"referenceID": 18, "context": "Also, without end-to-end connection the model cannot be fully fine-tuned with supervised learning, which has been shown to improve results on auto-encoders [19].", "startOffset": 156, "endOffset": 160}, {"referenceID": 19, "context": "RNNs are particularly appropriate for this framework as they have a long history [20] of successfully modeling dynamical systems.", "startOffset": 81, "endOffset": 85}, {"referenceID": 7, "context": "Furthermore, in place of using computationally expensive EM algorithms to compute the sparse states and causes, we use convolutional autoencoders with Winner-Take-All [8] regularization to encode the states in feedforward manner.", "startOffset": 167, "endOffset": 170}, {"referenceID": 20, "context": "Convolutional-recurrent neural networks [21][22] are RNNs where all the input-to-state and state-to-state transformations are implemented with convolutions.", "startOffset": 40, "endOffset": 44}, {"referenceID": 21, "context": "Convolutional-recurrent neural networks [21][22] are RNNs where all the input-to-state and state-to-state transformations are implemented with convolutions.", "startOffset": 44, "endOffset": 48}, {"referenceID": 19, "context": "RNNs can be trained in several ways for sequence prediction [20][23]; here we focus on training our ConvRNN to predict the next frame of the sequence.", "startOffset": 60, "endOffset": 64}, {"referenceID": 22, "context": "RNNs can be trained in several ways for sequence prediction [20][23]; here we focus on training our ConvRNN to predict the next frame of the sequence.", "startOffset": 64, "endOffset": 68}, {"referenceID": 23, "context": "In autoencoders, sparsity can be imposed as a constraint on the objective function [24].", "startOffset": 83, "endOffset": 87}, {"referenceID": 24, "context": "FISTA [25]) or learned approaches (ex.", "startOffset": 6, "endOffset": 10}, {"referenceID": 25, "context": "LISTA [26]).", "startOffset": 6, "endOffset": 10}, {"referenceID": 26, "context": "Recent research has shown that simple regularization techniques such as Dropout [27] combined with ReLU activations are enough to learn sparse representations without extra penalties in the cost functions.", "startOffset": 80, "endOffset": 84}, {"referenceID": 7, "context": "Makhzani and Frey proposed Winner-Take-All (WTA) Autoencoders [8] which use aggressive Dropout, where all the elements but the strongest of a convolutional map are zeroed out.", "startOffset": 62, "endOffset": 65}, {"referenceID": 27, "context": "This architecture was inspired by the dorsal and ventral streams hypothesis in the human visual cortex [28].", "startOffset": 103, "endOffset": 107}, {"referenceID": 28, "context": ", T ] and adapt all the parameters using backpropagation through time (BPTT) [29].", "startOffset": 77, "endOffset": 81}, {"referenceID": 23, "context": "This research is related to DPCNs and a larger family of deep unsupervised neural networks [24][30][17].", "startOffset": 91, "endOffset": 95}, {"referenceID": 29, "context": "This research is related to DPCNs and a larger family of deep unsupervised neural networks [24][30][17].", "startOffset": 95, "endOffset": 99}, {"referenceID": 16, "context": "This research is related to DPCNs and a larger family of deep unsupervised neural networks [24][30][17].", "startOffset": 99, "endOffset": 103}, {"referenceID": 7, "context": "The aforementioned Winner-Take-All Autoencoders (WTA-AE) [8] consist of a deep convolutional encoder and a single layer convolutional decoder, which inspired our choice.", "startOffset": 57, "endOffset": 60}, {"referenceID": 2, "context": "[3] Wang and Gupta [31].", "startOffset": 0, "endOffset": 3}, {"referenceID": 30, "context": "[3] Wang and Gupta [31].", "startOffset": 19, "endOffset": 23}, {"referenceID": 21, "context": "Convolutional RNNs were proposed simultaneously by several authors [22][21] as an extension of Network-in-Networks [32] architectures where each convolutional layer of a CNN are themselves deep networks.", "startOffset": 67, "endOffset": 71}, {"referenceID": 20, "context": "Convolutional RNNs were proposed simultaneously by several authors [22][21] as an extension of Network-in-Networks [32] architectures where each convolutional layer of a CNN are themselves deep networks.", "startOffset": 71, "endOffset": 75}, {"referenceID": 31, "context": "Convolutional RNNs were proposed simultaneously by several authors [22][21] as an extension of Network-in-Networks [32] architectures where each convolutional layer of a CNN are themselves deep networks.", "startOffset": 115, "endOffset": 119}, {"referenceID": 21, "context": "al [22] proposed to make each convolutional layer a fixed input RNN.", "startOffset": 3, "endOffset": 7}, {"referenceID": 20, "context": "[21] and Patraucean et.", "startOffset": 0, "endOffset": 4}, {"referenceID": 32, "context": "[33] on the other hand used temporal context in videos for weather and video forecasts.", "startOffset": 0, "endOffset": 4}, {"referenceID": 33, "context": "al [34].", "startOffset": 3, "endOffset": 7}, {"referenceID": 34, "context": "We trained the networks with batches of size 100 for 3 epochs (a total of 1800 updates) using the Adam learning rule [35].", "startOffset": 117, "endOffset": 121}, {"referenceID": 35, "context": "Both networks were equally initialized using the Glorot uniform method [36].", "startOffset": 71, "endOffset": 75}, {"referenceID": 36, "context": "The COIL-100 dataset [37] consists of 100 videos of different objects.", "startOffset": 21, "endOffset": 25}, {"referenceID": 36, "context": "The classification protocol proposed in the COIL-100 [37] uses 4 frames per video as labeled samples, the frames corresponding to angles 0\u25e6, 90\u25e6, 180\u25e6 and 270\u25e6.", "startOffset": 53, "endOffset": 57}, {"referenceID": 6, "context": "Chalasani and Principe[7] and Mobahi et.", "startOffset": 22, "endOffset": 25}, {"referenceID": 37, "context": "[38] used the entire dataset for unsupervised pre-training.", "startOffset": 0, "endOffset": 4}, {"referenceID": 6, "context": "DPCN no context [7] 79.", "startOffset": 16, "endOffset": 19}, {"referenceID": 38, "context": "45 Stacked ISA + temporal [39] 87 ConvNets + Temporal [38] 92.", "startOffset": 26, "endOffset": 30}, {"referenceID": 37, "context": "45 Stacked ISA + temporal [39] 87 ConvNets + Temporal [38] 92.", "startOffset": 54, "endOffset": 58}, {"referenceID": 6, "context": "25 DPCN + temporal + top down [7] 98.", "startOffset": 30, "endOffset": 33}, {"referenceID": 6, "context": "In accordance with the test procedure of Chalasani and Principe [7], a linear SVM was trained using these features and labels indicating the identity of the face.", "startOffset": 64, "endOffset": 67}, {"referenceID": 6, "context": "Table III summarizes the results for 50 frames, 100 frames, and the full video, comparing with 3 other methods, including the original convolutional implementation of DPCN [7].", "startOffset": 172, "endOffset": 175}, {"referenceID": 6, "context": "The results for the 3 other methods were taken from [7].", "startOffset": 52, "endOffset": 55}, {"referenceID": 7, "context": "Nevertheless, Makhzani and Frey [8] showed that layer wise training at different scales improved their results.", "startOffset": 32, "endOffset": 35}, {"referenceID": 41, "context": "On the other hand, methods based on supervised learning with deep convolutional neural networks using hand-engineered spatial flow features can obtain a recognition rates > 80% [42][43].", "startOffset": 177, "endOffset": 181}, {"referenceID": 42, "context": "On the other hand, methods based on supervised learning with deep convolutional neural networks using hand-engineered spatial flow features can obtain a recognition rates > 80% [42][43].", "startOffset": 181, "endOffset": 185}, {"referenceID": 10, "context": "[11] showed that multitask learning can improve the overall results of the supervised learning task.", "startOffset": 0, "endOffset": 4}, {"referenceID": 6, "context": "Our contributions were threefold: 1) a scalable, end-to-end differentiable reinterpretation of the sparse spatio-temporal feature extraction in Deep Predictive Coding Networks [7]; 2) an extension of Winner-Take-All Autoencoders [8] to time using dynamic neural networks; 3) a new technique for initializing and regularizing [11] convolutional recurrent neural networks [9][10].", "startOffset": 176, "endOffset": 179}, {"referenceID": 7, "context": "Our contributions were threefold: 1) a scalable, end-to-end differentiable reinterpretation of the sparse spatio-temporal feature extraction in Deep Predictive Coding Networks [7]; 2) an extension of Winner-Take-All Autoencoders [8] to time using dynamic neural networks; 3) a new technique for initializing and regularizing [11] convolutional recurrent neural networks [9][10].", "startOffset": 229, "endOffset": 232}, {"referenceID": 10, "context": "Our contributions were threefold: 1) a scalable, end-to-end differentiable reinterpretation of the sparse spatio-temporal feature extraction in Deep Predictive Coding Networks [7]; 2) an extension of Winner-Take-All Autoencoders [8] to time using dynamic neural networks; 3) a new technique for initializing and regularizing [11] convolutional recurrent neural networks [9][10].", "startOffset": 325, "endOffset": 329}, {"referenceID": 8, "context": "Our contributions were threefold: 1) a scalable, end-to-end differentiable reinterpretation of the sparse spatio-temporal feature extraction in Deep Predictive Coding Networks [7]; 2) an extension of Winner-Take-All Autoencoders [8] to time using dynamic neural networks; 3) a new technique for initializing and regularizing [11] convolutional recurrent neural networks [9][10].", "startOffset": 370, "endOffset": 373}, {"referenceID": 9, "context": "Our contributions were threefold: 1) a scalable, end-to-end differentiable reinterpretation of the sparse spatio-temporal feature extraction in Deep Predictive Coding Networks [7]; 2) an extension of Winner-Take-All Autoencoders [8] to time using dynamic neural networks; 3) a new technique for initializing and regularizing [11] convolutional recurrent neural networks [9][10].", "startOffset": 373, "endOffset": 377}, {"referenceID": 39, "context": "Sequences Lengths MDA [40] SANP [41] CDN [7] Proposed Method", "startOffset": 22, "endOffset": 26}, {"referenceID": 40, "context": "Sequences Lengths MDA [40] SANP [41] CDN [7] Proposed Method", "startOffset": 32, "endOffset": 36}, {"referenceID": 6, "context": "Sequences Lengths MDA [40] SANP [41] CDN [7] Proposed Method", "startOffset": 41, "endOffset": 44}, {"referenceID": 35, "context": "We also showed that this method can be used as an initialization technique for supervised convolutional RNNs, obtaning better results than Glorot initialization [36].", "startOffset": 161, "endOffset": 165}], "year": 2017, "abstractText": "We propose a convolutional recurrent neural network, with Winner-Take-All dropout for high dimensional unsupervised feature learning in multi-dimensional time series. We apply the proposedmethod for object recognition with temporal context in videos and obtain better results than comparable methods in the literature, including the Deep Predictive Coding Networks previously proposed by Chalasani and Principe.Our contributions can be summarized as a scalable reinterpretation of the Deep Predictive Coding Networks trained end-to-end with backpropagation through time, an extension of the previously proposed Winner-Take-All Autoencoders to sequences in time, and a new technique for initializing and regularizing convolutional-recurrent neural networks.", "creator": "LaTeX with hyperref package"}}}