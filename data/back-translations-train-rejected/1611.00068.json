{"id": "1611.00068", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-Oct-2016", "title": "RNN Approaches to Text Normalization: A Challenge", "abstract": "This paper presents a challenge to the community: given a large corpus of written text aligned to its normalized spoken form, train an RNN to learn the correct normalization function. We present a data set of general text where the normalizations were generated using an existing text normalization component of a text-to-speech system. This data set will be released open-source in the near future.", "histories": [["v1", "Mon, 31 Oct 2016 22:42:02 GMT  (461kb)", "http://arxiv.org/abs/1611.00068v1", "17 pages, 13 tables, 3 figures"], ["v2", "Tue, 24 Jan 2017 19:51:12 GMT  (461kb)", "http://arxiv.org/abs/1611.00068v2", "17 pages, 13 tables, 3 figures"]], "COMMENTS": "17 pages, 13 tables, 3 figures", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["richard sproat", "navdeep jaitly"], "accepted": false, "id": "1611.00068"}, "pdf": {"name": "1611.00068.pdf", "metadata": {"source": "CRF", "title": "RNN Approaches to Text Normalization: A Challenge", "authors": ["Richard Sproat", "Navdeep Jaitly"], "emails": ["rws@google.com", "ndjaitly@google.com"], "sections": [{"heading": null, "text": "This paper presents a challenge to the community: Given a large corpus of written text adapted to its normalized spoken form, train an RNN to learn the correct normalization function. We also present a dataset of generic text in which the normalizations were generated using an existing text normalization component of a text-to-language system. This dataset will be released as open source in the near future. We also present our own experiments with this dataset with a variety of different RNN architectures. While some of the architectures actually provide very good results in terms of overall accuracy, the errors produced are problematic as they would convey the completely wrong message if such a system were used in a language application. On the other hand, we show that a simple FST-based filter can mitigate these errors and achieve a level of accuracy that cannot be achieved by the RNN alone. Although our conclusions on this point are largely negative, we can't normalize the textual problem by arguing that a huge amount of text is what we can normalize."}, {"heading": "1 Introduction", "text": "In recent years, there has been a major shift in language and language technology: the field has been taken over by deep learning approaches. For example, at a recent NAACL conference, well over half of the papers were in some way related to embedding or deep or recurring neural networks. This change is certainly justified by the impressive performance gains shown in a number of areas from image processing, handwriting recognition, acoustic modelling in automatic speech recognition (ASR), parametric speech synthesis for text translation, analysis, and name play."}, {"heading": "2 Why text normalization is different", "text": "In fact, most of them are able to survive on their own, and they are able to survive on their own."}, {"heading": "3 Prior work on text normalization", "text": "Text normalization has a long history in language technology, dating back to the earliest work on complete TTS synthesis (Allen et al., 1987). Sproat (1996) provided a unified model for most text normalization problems with regard to weighted finite state transducers (WFSTs). The first work to treat the problem of text normalization essentially as a language modeling problem was (Sproat et al., 2001). Recent machine learning papers specifically related to TTS text normalization include (Sproat, 2010; Roark and Sproat, 2014). In recent years, there has been a lot of work focusing on social media (Xia et al., 2006; Choudhury et al., 2007; Kobus et al., 2008; Beaufort et al., 2010; Kaufmann et al., 2010; Liu et al., 2011; Motus et al, 2012 Liu al; these are common, Liu et al; 2012 Liu et al."}, {"heading": "4 Dataset", "text": "Our data consist of 1.1 billion words of the English text, and 290 million words of the Russian text, from Wikipedia regions that could be decrypted as UTF8, divided into sentences, and run through the Google TTS system of Kestrel text normalization (Ebden and Sproat, 2014) to produce verbalizations. The format of the annotated data is as described in Figure 1 above. The majority of the rules are produced using the Thrax Finite State Development System (Roark et al), by entering and classifying the tokens, and then verbalizing each token according to its semiotic class. The majority of the rules are using the Thrax Finite State Development System (Roark et al), 2012. The statistical components of the system include morphosyntactic taggers for languages such as Russian with complex morphology, 2 a statistical transliteration module."}, {"heading": "5 Experiment 1: Text normalization using LSTMs", "text": "The first approach depends on the observation that text normalization can be divided into two subproblems: \u2022 What are the possible normalizations of this token and \u2022 which one is appropriate for the given context? The first - the channel - can be handled in a context-independent manner, enumerating the set of possible normalizations: i.e. 123 could be a hundred and twenty-three years, a twenty-third, or a twenty-third. The second requires context: In 123 King Ave., the correct reading in American English would normally be twenty-three times. The first component is a transmission problem from string to string. Furthermore, sincere WFSTs can be used to handle most or all of the needed transmissions (Sproat, 1996), the relationship between input and output strings is regular, so complex network architectures comprising, for example, stacks should not be needed. In order to input, the string must be recognized in the sense of a sequence, because it must be fully recognizable as a string of its constituents in a word."}, {"heading": "5.1 LSTM architecture", "text": "We train two LSTM models, one for the channel and one for the language model. Data usage of each word during the training is outlined in Table 1. For the channel model, the LSTM learns to map from a sequence of characters to one or more marks of the output. For most input tokens, this means that it will choose to leave it alone, that is, to assign it to punctuation, or, in the case of punctuation, to map it out of anything else that corresponds to silence. For other tokens, it must choose to verbalize it in a variety of different ways. For the language model, the system reads the words either from the input when it is assigned to punctuation, or from the output when it is mapped out of something else. For the channel model LSTM, we have used a bidirectional sequence-to-sequence model, reported in (Rao et al, 2015), in two configurations, one forward and two backwards."}, {"heading": "5.2 Decoding", "text": "First, for each position in the output of the channel model, we truncate the predicted output symbols. If a hypothesis has a very high probability (default 0.98), we eliminate all other predictions at that position: in most cases, this happens because the channel model is very safe in most input positions by itself. Also, we truncate all hypotheses with a very low probability (default 0.05). Finally, at each output position, we keep only the n best hypotheses: in our experiments, we kept n = 5. We then use the resulting truncated vectors to populate the corresponding positions in the input to the LM, with the channel probability for each token. This channel probability is multiplied at each output position by the LM probability times by an LM weighting factor (1 in our experiments). This method of combining channel and LM probabilities can be considered as a greater probability for a difference between the arc position between the M and the main transfer channel (where M is not the overweighted)."}, {"heading": "5.3 Results", "text": "The first point to note is that the overall performance is quite complex: accuracy is about 99% for English and 98% for Russian. But almost all of this can be traced back to the model that predicts for most input marks and punctuation marks. To be sure, these are decisions that a text normalization system has to make: each such system has to decide to leave an input token alone, or to associate it with silence. However, as we noted in the introduction, you usually get no credit for making these decisions correctly, and if you look at the interesting cases, the performance collapses, with the lowest performance predictably found for cases like TIME, which are not very common in these data. Also, the deep models generally tend to be better than the flat models, although this is not true in all cases - e.g. MONEY in English. This in itself is a useful health claims test, as we would expect improvements with deeper models."}, {"heading": "6 Experiment 2: Attention-based RNN sequence-to-sequence models", "text": "That is, instead of having a separate \"channel\" and a \"language model phase,\" we model the whole task as one in which we classify a sequence of input characters into a sequence of output words using a tensor flow (Abadi et al., 2015) model with an attention mechanism (Mnih et al., 2014). Attention models are particularly good for sequence-to-sequence problems, as they continuously update the decoder with information about the state of the encoder, thus improving the relationship between input and output sequences."}, {"heading": "6.1 Results", "text": "This, in turn, suggests that modeling the problem as a pure sequence-to-sequence transmission as an alternative to the previously described source channel approach is quite practicable; some errors are pointed out in Table 7. These errors are reminiscent of several errors of the LSTM system in Table 4. On the other hand, it must be admitted that in English, the only clear error of this type found in Table 7. Again, as with the source channel approach, there is evidence that the system learns a lot by heart but does not just memorize. In English, 90.6% of the cases not found in the training data were correctly produced (compared with 99.8% of the cases seen); in Russian, 86.7% of the cases not seen were correct (compared with 99.4% of the cases seen)."}, {"heading": "6.2 Results on \u201creasonable\u201d-sized data sets.", "text": "In order to develop a system for a new language, one needs a system that can be trained on records of a size that one might expect a team of native speakers to label them. Assuming that one is willing to invest a few weeks of work with a small team, it is not impossible that one could label about 10 million words of Wikipedia-like text. 7With this point in mind, we have retrained the systems to 11.4 million English characters from the beginning of the original training and 11.9 million characters of Russian. The system was trained for about 7 days for both languages until the system reached a perplexity on held data of 1.002 and for Russian of 1.007. The results are presented in Table 8. Overall performance is not very different from the system trained on the larger data set, and in some places it is actually better."}, {"heading": "7 Finite-state filters", "text": "This year it is more than ever before."}, {"heading": "8 Discussion and the challenge", "text": "In fact, it is the case that most of them are able to survive themselves, and that they are able to survive themselves by searching for themselves. (...) It is not the case that they are able to survive themselves. (...) It is as if they were able to survive themselves. (...) It is as if they were able to survive themselves. (...) It is as if they were able to survive themselves. (...) It is as if they were able to survive themselves. (...) (...) (...) (...) (...) () (...) () (...) () (...) () (...) () (...) () (...) () () () ()) (...) (...) () () () () (...) () () (...) () () (...) () () (...) () () (...) () () () ()) () () () () () () ()) ()) (() ()) () () () () ()) ()) (()) () () ()) () ()) () () ()) () () ()) () ()) () ()) () ()) () ()) () () () ()) () ()) () ()) () ()) () ()) () () ()) () () () () () () ()) () () ()) () () () () ()) () () () () () () () ()) () () () ()) () () () () ()) () () () () () () () () () () () () () () () () () () () () () ()) () () () () ()) () () () ()) () ((()))) (()) () (())) () (((())))) (((()))) (("}, {"heading": "Acknowledgements", "text": "We thank Alexander Gutkin for producing the original code to normalize the kestrel text and Kyle Gorman for producing the data. Alexander Gutkin also checked a sample of Russian data to estimate the error rate of kestrels. We also thank Alexander and Kyle as well as Brian Roark and Suyoun Yoon for comments on earlier versions of this work. Finally, Hasim Sak for his help with the LSTM models reported in Experiment 1."}], "references": [{"title": "TensorFlow: Large-scale machine learning on heterogeneous systems. Software available from tensorflow.org", "author": ["tenberg", "MartinWicke", "YuanYu", "andXiaoqiang Zheng"], "venue": null, "citeRegEx": "tenberg et al\\.,? \\Q2015\\E", "shortCiteRegEx": "tenberg et al\\.", "year": 2015}, {"title": "Distributed representation and estimation of WFSTbased n-gram models", "author": ["Cyril Allauzen", "Michael Riley", "Brian Roark."], "venue": "ACL SIGFSM Workshop on Statistical NLP and Weighted Automata.", "citeRegEx": "Allauzen et al\\.,? 2016", "shortCiteRegEx": "Allauzen et al\\.", "year": 2016}, {"title": "From Text to Speech: The MITalk System", "author": ["Jonathan Allen", "Sharon M. Hunnicutt", "Dennis Klatt."], "venue": "Cambridge University Press.", "citeRegEx": "Allen et al\\.,? 1987", "shortCiteRegEx": "Allen et al\\.", "year": 1987}, {"title": "Incorporating discrete translation lexicons into neural machine translation", "author": ["Philip Arthur", "Graham Neubig", "Satoshi Nakamura."], "venue": "EMNLP, Austin, TX.", "citeRegEx": "Arthur et al\\.,? 2016", "shortCiteRegEx": "Arthur et al\\.", "year": 2016}, {"title": "Personalized normalization for a multilingual chat system", "author": ["Ai Ti Aw", "Lian Hau Lee."], "venue": "ACL, pages 31\u201336, Jeju Island, Korea.", "citeRegEx": "Aw and Lee.,? 2012", "shortCiteRegEx": "Aw and Lee.", "year": 2012}, {"title": "Shared tasks of the 2015 workshop on noisy user-generated text: Twitter lexical normalization and named entity recognition", "author": ["Timothy Baldwin", "Young-Bum Kim", "Marie Catherine de Marneffe", "Alan Ritter", "Bo Han", "Wei Xu."], "venue": "InWNUT.", "citeRegEx": "Baldwin et al\\.,? 2015", "shortCiteRegEx": "Baldwin et al\\.", "year": 2015}, {"title": "A hybrid rule/model-based finite-state framework for normalizing SMS messages", "author": ["Richard Beaufort", "Sophie Roekhaut", "Louise-Am\u00e9lie Cougnon", "C\u00e9drick Fairon."], "venue": "ACL, pages 770\u2013779, Uppsala, Sweden.", "citeRegEx": "Beaufort et al\\.,? 2010", "shortCiteRegEx": "Beaufort et al\\.", "year": 2010}, {"title": "A neural probabilistic language model", "author": ["Yoshua Bengio", "R\u00e9jean Ducharme", "Pascal Vincent", "Christian Janvin."], "venue": "Journal of Machine Learning Research, 3:1137\u20131155, March.", "citeRegEx": "Bengio et al\\.,? 2003", "shortCiteRegEx": "Bengio et al\\.", "year": 2003}, {"title": "Training speech recognition using captions, March 26", "author": ["S.S. Bharadwaj", "S.B. Medapati."], "venue": "US Patent App. 14/037,144.", "citeRegEx": "Bharadwaj and Medapati.,? 2015", "shortCiteRegEx": "Bharadwaj and Medapati.", "year": 2015}, {"title": "A transitionbased system for joint part-of-speech tagging and labeled non-projective dependency parsing", "author": ["Bernd Bohnet", "Joakim Nivre."], "venue": "EMNLPCoNLL, pages 1455\u20131465, Jeju Island, Korea.", "citeRegEx": "Bohnet and Nivre.,? 2012", "shortCiteRegEx": "Bohnet and Nivre.", "year": 2012}, {"title": "Listen, attend and spell: A neural network for large vocabulary conversational speech recognition", "author": ["William Chan", "Navdeep Jaitly", "Quoc V. Le", "Oriol Vinyals."], "venue": "ICASSP, pages 4960\u20134964.", "citeRegEx": "Chan et al\\.,? 2016", "shortCiteRegEx": "Chan et al\\.", "year": 2016}, {"title": "Learning phrase representations using RNN encoder-decoder for statistical machine translation", "author": ["Kyunghyun Cho", "Bart van Merrienboer", "\u00c7aglar G\u00fcl\u00e7ehre", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio."], "venue": "CoRR, abs/1406.1078.", "citeRegEx": "Cho et al\\.,? 2014", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Investigation and modeling of the structure of texting language", "author": ["Monojit Choudhury", "Rahul Saraf", "Vijit Jain", "Sudesha Sarkar", "Anupam Basu."], "venue": "International Journal of Document Analysis and Recognition, 10:157\u2013174.", "citeRegEx": "Choudhury et al\\.,? 2007", "shortCiteRegEx": "Choudhury et al\\.", "year": 2007}, {"title": "Normalizing tweets with edit scripts and recurrent neural embeddings", "author": ["Grzegorz Chrupa\u0142a."], "venue": "ACL, Singapore.", "citeRegEx": "Chrupa\u0142a.,? 2014", "shortCiteRegEx": "Chrupa\u0142a.", "year": 2014}, {"title": "The Kestrel TTS text normalization system", "author": ["Peter Ebden", "Richard Sproat."], "venue": "Natural Language Engineering, 21(3):1\u201321.", "citeRegEx": "Ebden and Sproat.,? 2014", "shortCiteRegEx": "Ebden and Sproat.", "year": 2014}, {"title": "Svmtool: A general pos tagger generator based on support vector machines", "author": ["Jes\u00fas Gim\u00e9nez", "Llu\u00eds M\u00e0rquez."], "venue": "Proceedings of the 4th LREC, Lisbon, Portugal.", "citeRegEx": "Gim\u00e9nez and M\u00e0rquez.,? 2004", "shortCiteRegEx": "Gim\u00e9nez and M\u00e0rquez.", "year": 2004}, {"title": "Minimally supervised models for number normalization", "author": ["Kyle Gorman", "Richard Sproat."], "venue": "Transactions of the Association for Computational Linguistics.", "citeRegEx": "Gorman and Sproat.,? 2016", "shortCiteRegEx": "Gorman and Sproat.", "year": 2016}, {"title": "Towards end-toend speech recognition with recurrent neural networks", "author": ["Alex Graves", "Navdeep Jaitly."], "venue": "ICML, pages 1764\u20131772.", "citeRegEx": "Graves and Jaitly.,? 2014", "shortCiteRegEx": "Graves and Jaitly.", "year": 2014}, {"title": "Connectionist temporal classification: Labeling unsegmented sequence data with recurrent neural networks", "author": ["Alex Graves", "Santiago Fern\u00e1ndez", "Gaustino Gomez", "J\u00fcrgen Schmidhuber."], "venue": "ICML, pages 369\u2013 376.", "citeRegEx": "Graves et al\\.,? 2006", "shortCiteRegEx": "Graves et al\\.", "year": 2006}, {"title": "Social text normalization using contextual graph random walks", "author": ["Hany Hassan", "Arul Menezes."], "venue": "ACL, pages 1577\u20131586.", "citeRegEx": "Hassan and Menezes.,? 2013", "shortCiteRegEx": "Hassan and Menezes.", "year": 2013}, {"title": "Named entity transcription with pair n-gram models", "author": ["Martin Jansche", "Richard Sproat."], "venue": "NEWS \u201909, pages 32\u201335, Singapore.", "citeRegEx": "Jansche and Sproat.,? 2009", "shortCiteRegEx": "Jansche and Sproat.", "year": 2009}, {"title": "Syntactic normalization of Twitter messages", "author": ["Max Kaufmann."], "venue": "International Conference on NLP.", "citeRegEx": "Kaufmann.,? 2010", "shortCiteRegEx": "Kaufmann.", "year": 2010}, {"title": "Normalizing SMS: are two metaphors better than one", "author": ["Catherine Kobus", "Fran\u00e7ois Yvon", "G\u00e9raldine Damnati"], "venue": "In COLING,", "citeRegEx": "Kobus et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Kobus et al\\.", "year": 2008}, {"title": "Insertion, deletion, or substitution? Normalizing text messages without pre-categorization nor supervision", "author": ["Fei Liu", "Fuliang Weng", "Bingqing Wang", "Yang Liu."], "venue": "ACL, pages 71\u201376, Portland, Oregon, USA.", "citeRegEx": "Liu et al\\.,? 2011", "shortCiteRegEx": "Liu et al\\.", "year": 2011}, {"title": "A broadcoverage normalization system for social media language", "author": ["Fei Liu", "Fuliang Weng", "Xiao Jiang."], "venue": "ACL, pages 1035\u20131044, Jeju Island, Korea. Association for Computational Linguistics.", "citeRegEx": "Liu et al\\.,? 2012a", "shortCiteRegEx": "Liu et al\\.", "year": 2012}, {"title": "Joint inference of named entity recognition and normalization for tweets", "author": ["Xiaohua Liu", "Ming Zhou", "Xiangyang Zhou", "Zhongyang Fu", "FuruWei."], "venue": "ACL, pages 526\u2013535, Jeju Island, Korea.", "citeRegEx": "Liu et al\\.,? 2012b", "shortCiteRegEx": "Liu et al\\.", "year": 2012}, {"title": "Recurrent neural network based language model", "author": ["Tomas Mikolov", "Martin Karafi\u00e1t", "Lukas Burget", "Jan Cernock\u1ef3", "Sanjeev Khudanpur."], "venue": "Interspeech, volume 2, page 3.", "citeRegEx": "Mikolov et al\\.,? 2010", "shortCiteRegEx": "Mikolov et al\\.", "year": 2010}, {"title": "NCSU SAS WOOKHEE: A deep contextual long-short term memory model for text normalization", "author": ["Wookhee Min", "Bradford Mott."], "venue": "WNUT.", "citeRegEx": "Min and Mott.,? 2015", "shortCiteRegEx": "Min and Mott.", "year": 2015}, {"title": "Recurrent models of visual attention", "author": ["Volodymyr Mnih", "Nicolas Heess", "Alex Graves", "Koray Kavukcuoglu."], "venue": "NIPS, pages 2204\u20132212.", "citeRegEx": "Mnih et al\\.,? 2014", "shortCiteRegEx": "Mnih et al\\.", "year": 2014}, {"title": "Weighted finite-state transducers in speech recognition", "author": ["Mehryar Mohri", "Fernando Pereira", "Michael Riley."], "venue": "Computer Speech & Language, 16(1):69\u2013", "citeRegEx": "Mohri et al\\.,? 2002", "shortCiteRegEx": "Mohri et al\\.", "year": 2002}, {"title": "A character-level machine translation approach for normalization of SMS abbreviations", "author": ["Deana Pennell", "Yang Liu."], "venue": "IJCNLP.", "citeRegEx": "Pennell and Liu.,? 2011", "shortCiteRegEx": "Pennell and Liu.", "year": 2011}, {"title": "Grapheme-to-phoneme conversion using long short-term memory recurrent neural networks", "author": ["Kanishka Rao", "Fuchun Peng", "Ha\u015fim Sak", "Fran\u00e7oise Beaufays."], "venue": "ICASSP, pages 4225\u20134229.", "citeRegEx": "Rao et al\\.,? 2015", "shortCiteRegEx": "Rao et al\\.", "year": 2015}, {"title": "Weighting finite-state transductions with neural context", "author": ["Pushpendre Rastogi", "Ryan Cotterell", "Jason Eisner."], "venue": "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,", "citeRegEx": "Rastogi et al\\.,? 2016", "shortCiteRegEx": "Rastogi et al\\.", "year": 2016}, {"title": "Hippocratic abbreviation expansion", "author": ["Brian Roark", "Richard Sproat."], "venue": "ACL, pages 364\u2013369.", "citeRegEx": "Roark and Sproat.,? 2014", "shortCiteRegEx": "Roark and Sproat.", "year": 2014}, {"title": "The OpenGrm open-source finite-state grammar software libraries", "author": ["Brian Roark", "Richard Sproat", "Cyril Allauzen", "Michael Riley", "Jeffrey Sorensen", "Terry Tai."], "venue": "ACL, pages 61\u201366.", "citeRegEx": "Roark et al\\.,? 2012", "shortCiteRegEx": "Roark et al\\.", "year": 2012}, {"title": "Applications of maximum entropy rankers to problems in spoken language processing", "author": ["Richard Sproat", "Keith Hall."], "venue": "Interspeech, pages 761\u2013764.", "citeRegEx": "Sproat and Hall.,? 2014", "shortCiteRegEx": "Sproat and Hall.", "year": 2014}, {"title": "Normalization of non-standard words", "author": ["Richard Sproat", "Alan Black", "Stanley Chen", "Shankar Kumar", "Mari Ostendorf", "Christopher Richards."], "venue": "Computer Speech and Language, 15(3):287\u2013333.", "citeRegEx": "Sproat et al\\.,? 2001", "shortCiteRegEx": "Sproat et al\\.", "year": 2001}, {"title": "Multilingual text analysis for textto-speech synthesis", "author": ["Richard Sproat."], "venue": "Natural Language Engineering, 2(4):369\u2013380.", "citeRegEx": "Sproat.,? 1996", "shortCiteRegEx": "Sproat.", "year": 1996}, {"title": "Lightly supervised learning of text normalization: Russian number names", "author": ["Richard Sproat."], "venue": "IEEE SLT, pages 436\u2013441.", "citeRegEx": "Sproat.,? 2010", "shortCiteRegEx": "Sproat.", "year": 2010}, {"title": "Text-to-Speech Synthesis", "author": ["Paul Taylor."], "venue": "Cambridge University Press, Cambridge.", "citeRegEx": "Taylor.,? 2009", "shortCiteRegEx": "Taylor.", "year": 2009}, {"title": "A phonetic-based approach to Chinese chat text normalization", "author": ["Yunqing Xia", "Kam-Fai Wong", "Wenjie Li."], "venue": "ACL, pages 993\u20131000, Sydney, Australia.", "citeRegEx": "Xia et al\\.,? 2006", "shortCiteRegEx": "Xia et al\\.", "year": 2006}, {"title": "A log-linear model for unsupervised text normalization", "author": ["Yi Yang", "Jacob Eisenstein."], "venue": "EMNLP, pages 61\u201372.", "citeRegEx": "Yang and Eisenstein.,? 2013", "shortCiteRegEx": "Yang and Eisenstein.", "year": 2013}], "referenceMentions": [{"referenceID": 36, "context": "In the original written form there are two nonstandard words (Sproat et al., 2001), namely the two measure expressions 6ft and 150lb.", "startOffset": 61, "endOffset": 82}, {"referenceID": 39, "context": "In this case both examples are instances of the same semiotic class (Taylor, 2009), namely measure phrases.", "startOffset": 68, "endOffset": 82}, {"referenceID": 8, "context": "For speech recognition acoustic model training, one could in theory use closed captioning (Bharadwaj and Medapati, 2015), which again is produced", "startOffset": 90, "endOffset": 120}, {"referenceID": 31, "context": "sequence-to-sequence LSTM that has been successfully applied to the somewhat similar problem of grapheme-to-phoneme conversion (Rao et al., 2015),", "startOffset": 127, "endOffset": 145}, {"referenceID": 10, "context": "sion problem (Chan et al., 2016).", "startOffset": 13, "endOffset": 32}, {"referenceID": 2, "context": "nology, dating back to the earliest work on full TTS synthesis (Allen et al., 1987).", "startOffset": 63, "endOffset": 83}, {"referenceID": 2, "context": "nology, dating back to the earliest work on full TTS synthesis (Allen et al., 1987). Sproat (1996) provided a unifying model for most text normalization problems in terms of weighted finite-state transducers (WFSTs).", "startOffset": 64, "endOffset": 99}, {"referenceID": 36, "context": "normalization as essentially a language modeling problem was (Sproat et al., 2001).", "startOffset": 61, "endOffset": 82}, {"referenceID": 38, "context": "More recent machine learning work specifically addressed to TTS text normalization include (Sproat, 2010; Roark and Sproat, 2014; Sproat and Hall, 2014).", "startOffset": 91, "endOffset": 152}, {"referenceID": 33, "context": "More recent machine learning work specifically addressed to TTS text normalization include (Sproat, 2010; Roark and Sproat, 2014; Sproat and Hall, 2014).", "startOffset": 91, "endOffset": 152}, {"referenceID": 35, "context": "More recent machine learning work specifically addressed to TTS text normalization include (Sproat, 2010; Roark and Sproat, 2014; Sproat and Hall, 2014).", "startOffset": 91, "endOffset": 152}, {"referenceID": 40, "context": "In the last few years there has been a lot of work that focuses on social media (Xia et al., 2006; Choudhury et al., 2007; Kobus et al., 2008; Beaufort et al., 2010; Kaufmann, 2010; Liu et al., 2011; Pennell and Liu, 2011; Aw and Lee, 2012; Liu et al., 2012a; Liu et al., 2012b; Hassan andMenezes, 2013; Yang and Eisenstein, 2013).", "startOffset": 80, "endOffset": 330}, {"referenceID": 12, "context": "In the last few years there has been a lot of work that focuses on social media (Xia et al., 2006; Choudhury et al., 2007; Kobus et al., 2008; Beaufort et al., 2010; Kaufmann, 2010; Liu et al., 2011; Pennell and Liu, 2011; Aw and Lee, 2012; Liu et al., 2012a; Liu et al., 2012b; Hassan andMenezes, 2013; Yang and Eisenstein, 2013).", "startOffset": 80, "endOffset": 330}, {"referenceID": 22, "context": "In the last few years there has been a lot of work that focuses on social media (Xia et al., 2006; Choudhury et al., 2007; Kobus et al., 2008; Beaufort et al., 2010; Kaufmann, 2010; Liu et al., 2011; Pennell and Liu, 2011; Aw and Lee, 2012; Liu et al., 2012a; Liu et al., 2012b; Hassan andMenezes, 2013; Yang and Eisenstein, 2013).", "startOffset": 80, "endOffset": 330}, {"referenceID": 6, "context": "In the last few years there has been a lot of work that focuses on social media (Xia et al., 2006; Choudhury et al., 2007; Kobus et al., 2008; Beaufort et al., 2010; Kaufmann, 2010; Liu et al., 2011; Pennell and Liu, 2011; Aw and Lee, 2012; Liu et al., 2012a; Liu et al., 2012b; Hassan andMenezes, 2013; Yang and Eisenstein, 2013).", "startOffset": 80, "endOffset": 330}, {"referenceID": 21, "context": "In the last few years there has been a lot of work that focuses on social media (Xia et al., 2006; Choudhury et al., 2007; Kobus et al., 2008; Beaufort et al., 2010; Kaufmann, 2010; Liu et al., 2011; Pennell and Liu, 2011; Aw and Lee, 2012; Liu et al., 2012a; Liu et al., 2012b; Hassan andMenezes, 2013; Yang and Eisenstein, 2013).", "startOffset": 80, "endOffset": 330}, {"referenceID": 23, "context": "In the last few years there has been a lot of work that focuses on social media (Xia et al., 2006; Choudhury et al., 2007; Kobus et al., 2008; Beaufort et al., 2010; Kaufmann, 2010; Liu et al., 2011; Pennell and Liu, 2011; Aw and Lee, 2012; Liu et al., 2012a; Liu et al., 2012b; Hassan andMenezes, 2013; Yang and Eisenstein, 2013).", "startOffset": 80, "endOffset": 330}, {"referenceID": 30, "context": "In the last few years there has been a lot of work that focuses on social media (Xia et al., 2006; Choudhury et al., 2007; Kobus et al., 2008; Beaufort et al., 2010; Kaufmann, 2010; Liu et al., 2011; Pennell and Liu, 2011; Aw and Lee, 2012; Liu et al., 2012a; Liu et al., 2012b; Hassan andMenezes, 2013; Yang and Eisenstein, 2013).", "startOffset": 80, "endOffset": 330}, {"referenceID": 4, "context": "In the last few years there has been a lot of work that focuses on social media (Xia et al., 2006; Choudhury et al., 2007; Kobus et al., 2008; Beaufort et al., 2010; Kaufmann, 2010; Liu et al., 2011; Pennell and Liu, 2011; Aw and Lee, 2012; Liu et al., 2012a; Liu et al., 2012b; Hassan andMenezes, 2013; Yang and Eisenstein, 2013).", "startOffset": 80, "endOffset": 330}, {"referenceID": 24, "context": "In the last few years there has been a lot of work that focuses on social media (Xia et al., 2006; Choudhury et al., 2007; Kobus et al., 2008; Beaufort et al., 2010; Kaufmann, 2010; Liu et al., 2011; Pennell and Liu, 2011; Aw and Lee, 2012; Liu et al., 2012a; Liu et al., 2012b; Hassan andMenezes, 2013; Yang and Eisenstein, 2013).", "startOffset": 80, "endOffset": 330}, {"referenceID": 25, "context": "In the last few years there has been a lot of work that focuses on social media (Xia et al., 2006; Choudhury et al., 2007; Kobus et al., 2008; Beaufort et al., 2010; Kaufmann, 2010; Liu et al., 2011; Pennell and Liu, 2011; Aw and Lee, 2012; Liu et al., 2012a; Liu et al., 2012b; Hassan andMenezes, 2013; Yang and Eisenstein, 2013).", "startOffset": 80, "endOffset": 330}, {"referenceID": 41, "context": "In the last few years there has been a lot of work that focuses on social media (Xia et al., 2006; Choudhury et al., 2007; Kobus et al., 2008; Beaufort et al., 2010; Kaufmann, 2010; Liu et al., 2011; Pennell and Liu, 2011; Aw and Lee, 2012; Liu et al., 2012a; Liu et al., 2012b; Hassan andMenezes, 2013; Yang and Eisenstein, 2013).", "startOffset": 80, "endOffset": 330}, {"referenceID": 13, "context": "cludes (Chrupa\u0142a, 2014; Min and Mott, 2015).", "startOffset": 7, "endOffset": 43}, {"referenceID": 27, "context": "cludes (Chrupa\u0142a, 2014; Min and Mott, 2015).", "startOffset": 7, "endOffset": 43}, {"referenceID": 5, "context": "The latter work, for example, achieved second place in the constrained track of the ACL 2015 W-NUT Normalization of Noisy Text (Baldwin et al., 2015), achieving an F1 score of 81.", "startOffset": 127, "endOffset": 149}, {"referenceID": 14, "context": "Wikipedia regions that could be decoded as UTF8, divided into sentences, and run through the Google TTS system\u2019s Kestrel text normalization system (Ebden and Sproat, 2014) to produce verbalizations.", "startOffset": 147, "endOffset": 171}, {"referenceID": 14, "context": "As described in (Ebden and Sproat, 2014), Kestrel\u2019s verbalizations are produced by first tokenizing the input and classifying the tokens, and then verbalizing each token according to its semi-", "startOffset": 16, "endOffset": 40}, {"referenceID": 34, "context": "The majority of the rules are hand-built using the Thrax finite-state grammar development system (Roark et al., 2012).", "startOffset": 97, "endOffset": 117}, {"referenceID": 20, "context": "Statistical components of the system include morphosyntactic taggers for languages like Russian with complex morphology,2 a statistical transliterationmodule (Jansche and Sproat, 2009), and a statistical model to determine if capitalized tokens should be read as words or letter sequences (Sproat and Hall, 2014).", "startOffset": 158, "endOffset": 184}, {"referenceID": 35, "context": "Statistical components of the system include morphosyntactic taggers for languages like Russian with complex morphology,2 a statistical transliterationmodule (Jansche and Sproat, 2009), and a statistical model to determine if capitalized tokens should be read as words or letter sequences (Sproat and Hall, 2014).", "startOffset": 289, "endOffset": 312}, {"referenceID": 15, "context": "2The morphosyntactic tagger is an SVM model using hand-tuned features that classify the morphological bundle for each word independently, similar to SVMTool (Gim\u00e9nez and M\u00e0rquez, 2004) and MateTagger (Bohnet and Nivre, 2012).", "startOffset": 157, "endOffset": 184}, {"referenceID": 9, "context": "2The morphosyntactic tagger is an SVM model using hand-tuned features that classify the morphological bundle for each word independently, similar to SVMTool (Gim\u00e9nez and M\u00e0rquez, 2004) and MateTagger (Bohnet and Nivre, 2012).", "startOffset": 200, "endOffset": 224}, {"referenceID": 37, "context": "Furthermore, sinceWFSTs can be used to handle most or all of the needed transductions (Sproat, 1996), the relation between the input and output strings is regular, so that complex network architectures involving, say, stacks should not be", "startOffset": 86, "endOffset": 100}, {"referenceID": 31, "context": "in (Rao et al., 2015) in two configurations: one with two forward and two backward hidden layers, henceforth the shallow model; and one with three forward and three backward hidden layers, henceforth the deep model.", "startOffset": 3, "endOffset": 21}, {"referenceID": 18, "context": "The output layer is a connectionist temporal classification (CTC) (Graves et al., 2006) layer with a softmax error function.", "startOffset": 66, "endOffset": 87}, {"referenceID": 1, "context": "For comparison the perplexities for a 5-gram WFST language model with Katz backoff trained using the toolkit reported in (Allauzen et al., 2016) on the same data and evaluated on the same held-out data are given in parentheses.", "startOffset": 121, "endOffset": 144}, {"referenceID": 29, "context": "nel and LM probabilities can be thought of a as a poor-man\u2019s equivalent of the composition of a channel and LM weighted finite-state transducer (Mohri et al., 2002): the main difference is that there is no", "startOffset": 144, "endOffset": 164}, {"referenceID": 16, "context": "In both languages we find examples where the system gets numbers wrong, evidently because it is hard for the system to learn from the training data exactly how to map from digit sequences to number names \u2014 see also (Gorman and Sproat, 2016).", "startOffset": 215, "endOffset": 240}, {"referenceID": 7, "context": "This property is the whole basis of word embeddings (Bengio et al., 2003) and other similar techniques.", "startOffset": 52, "endOffset": 73}, {"referenceID": 3, "context": "Interestingly, a recent paper (Arthur et al., 2016) dis-", "startOffset": 30, "endOffset": 51}, {"referenceID": 28, "context": ", 2015) model with an attention mechanism (Mnih et al., 2014).", "startOffset": 42, "endOffset": 61}, {"referenceID": 10, "context": "The Tensor Flow implementation used is essentially the same as that reported in (Chan et al., 2016).", "startOffset": 80, "endOffset": 99}, {"referenceID": 11, "context": "similar to how MT has been treated as a sequenceto-sequence problem (Cho et al., 2014), and simply", "startOffset": 68, "endOffset": 86}, {"referenceID": 10, "context": "The architecture follows closely that of (Chan et al., 2016).", "startOffset": 41, "endOffset": 60}, {"referenceID": 10, "context": "The reader is referred to (Chan et al., 2016) for more details of the framework.", "startOffset": 26, "endOffset": 45}, {"referenceID": 34, "context": "We constructed a Thrax grammar (Roark et al., 2012) to cover MEASURE and MONEY expres-", "startOffset": 31, "endOffset": 51}, {"referenceID": 16, "context": ", we know that kg can be kilogram or kilograms, as well as a number FST that is learned from a few hundred number names using the algorithm described in (Gorman and Sproat, 2016).", "startOffset": 153, "endOffset": 178}, {"referenceID": 32, "context": "We note in passing that this method is more or less the opposite approach to that of (Rastogi et al., 2016).", "startOffset": 85, "endOffset": 107}, {"referenceID": 2, "context": "The first is that our characterization of what is important for a text normalization is idiosyncratic: what justification do we have for saying that, for example, a text normalization must get dates correct? But the response to that is obvious: the various semiotic classes are precisely where most of the effort has been devoted in developing traditional approaches to text normalization for TTS dating back to the 1970\u2019s (Allen et al., 1987), for the simple reason that a TTS system ought to be able to know how to read something like Sep 12, 2014.", "startOffset": 423, "endOffset": 443}], "year": 2016, "abstractText": "This paper presents a challenge to the community: given a large corpus of written text aligned to its normalized spoken form, train an RNN to learn the correct normalization function. We present a data set of general text where the normalizations were generated using an existing text normalization component of a text-to-speech system. This data set will be released open-source in the near future. We also present our own experiments with this data set with a variety of different RNN architectures. While some of the architectures do in fact produce very good results when measured in terms of overall accuracy, the errors that are produced are problematic, since they would convey completely the wrong message if such a system were deployed in a speech application. On the other hand, we show that a simple FST-based filter can mitigate those errors, and achieve a level of accuracy not achievable by the RNN alone. Though our conclusions are largely negative on this point, we are actually not arguing that the text normalization problem is intractable using an pure RNN approach, merely that it is not going to be something that can be solved merely by having huge amounts of annotated text data and feeding that to a general RNN model. Andwhenwe open-source our data, we will be providing a novel data set for sequenceto-sequence modeling in the hopes that the the community can find better solutions.", "creator": "Preview"}}}