{"id": "1611.00126", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Nov-2016", "title": "Improving Twitter Sentiment Classification via Multi-Level Sentiment-Enriched Word Embeddings", "abstract": "Most of existing work learn sentiment-specific word representation for improving Twitter sentiment classification, which encoded both n-gram and distant supervised tweet sentiment information in learning process. They assume all words within a tweet have the same sentiment polarity as the whole tweet, which ignores the word its own sentiment polarity. To address this problem, we propose to learn sentiment-specific word embedding by exploiting both lexicon resource and distant supervised information. We develop a multi-level sentiment-enriched word embedding learning method, which uses parallel asymmetric neural network to model n-gram, word level sentiment and tweet level sentiment in learning process. Experiments on standard benchmarks show our approach outperforms state-of-the-art methods.", "histories": [["v1", "Tue, 1 Nov 2016 04:48:09 GMT  (118kb)", "http://arxiv.org/abs/1611.00126v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["shufeng xiong"], "accepted": false, "id": "1611.00126"}, "pdf": {"name": "1611.00126.pdf", "metadata": {"source": "CRF", "title": "Improving Twitter Sentiment Classification via Multi-Level Sentiment-Enriched Word Embeddings", "authors": ["Shufeng Xiong"], "emails": [], "sections": [{"heading": null, "text": "ar Xiv: 161 1,00 126v 1 [cs.C L] 1N ov2 01"}, {"heading": "Introduction", "text": "It is one of the largest micro-blog sites on the Internet and has emerged as an important source of online opinion and sentiment indices. (As a result of its massive, diverse and growing user base, the opinion information contained in Twitter can be successfully used for many tasks, such as Tweet sentiment sentiment predictions (Si et al. 2013), political monitoring (Pla and Hurtado 2014) and, hence, public sentiment about social events (Bollen, Mao and Pepe 2011). Therefore, the ability to identify positive, negative and neutral opinions is fundamental. Researchers have proposed many approaches to improve the performance of Twitter classification (Davidov, Tsur, and Rappoport 2010)."}, {"heading": "Multi-Level Sentiment-Enriched Word Embedding for Twitter Sentiment Classification", "text": "In this paper, we argue that it is important to jointly model sentiment information at the word and tweet level in order to learn good sentiment-specific word embedding. In this way, not only are existing sentiment lexicon resources fully utilized, but also remote monitored Twitter corpus information in a uniform representational framework. In other words, n-gram and multi-level sentiment information are both encoded in Word embedding, which can improve sentiment classification performance. Figure 1 (c) describes the architecture of the representation learning network MSWE, which is used to encode sentiment information in Word embedding. The network uses Tweet as input. First, the MSWE model divides the input of tweets into multiple windows, i.e. n windows. Then n window is the actual input of n left subnetworks, and all windows are the input of a subnetwork and the actual subnetwork gives the gram."}, {"heading": "Sentiment-Specific Word Embedding", "text": "Our model aims to learn sentimental word embedding, which is more advantageous for Twitter sentiment classification than ordinary word embedding. (Sentiment-specific word embedding model comes from the C & W model (Collobert et al. 2011), which learns word embedding from n-gram contexts by using negative sampling. (When learning to represent a word w, they chose their contextual words c in a window t as a positive sample, in which w is in the middle position of c. To get negative sample, switch the word w with another word w to form mutational contexts c. (Its training goal is that the context c is expected to give the context c a higher language model value than the mutational context c.) The optimization function islossngm (c) = max (0, 1 \u2212 f ngm) + gultive (c) (c) (fnm), where gf (f) is f (f), f (f), f (f), f (f), f (f), f (f), f (f), f (f), f (f), f (f), f (f), f (f), f (f, f (f), f (f), f (f), f (f), f, f (f), f (f), f (f), f, f (f, f, f, f, f, f (c, f), f, f, f (f), f, f, f, f (f, f, f, f, f, f, f, f, f, f (f, f, f, f, f, f, f, f, f, f, f, f, f (f, f, f, f, f, f, f, f, f, f, f (f, f, f, f, f, f, f, f, f, f, f (f, f, f, f, f, f, f, f, f, f, f, f (f, f, f, f, f, f, f, f, f, f (f, f, f, f, f, f, f, f, f, f, f, f, f,"}, {"heading": "Multi-Level Sentiment-Enriched Word Embedding", "text": "It is an important resource for sentiment classification (Pang and Lee 2008; Feldman 2013; Socher et al. 2013). In particular (Socher et al. 2013). (Sentiment Treebank, which is based on the sentiment feeling and further comments of the sentiment label of the syntactically plausible formulation of sentences, which may not include all available word combinations. (Nevertheless, compared with word combinations, the basic sentiment-bared words are changed relatively little over time.) Therefore, we propose word level of lexicon for the coding of individual sentiment information and at the same time model tweet level for the consideration of the composition-sentiment information of Words. An intuitive approach is to consider a new optimization in practice in SSWE and TEWE."}, {"heading": "Sentiment Classification with Sentiment-Enriched Word Embedding", "text": "After learning sentimental-specific word embedding, it can be used to classify feelings by using existing monitored learning frames (e.g. SVM), using a neural network model to perform the classification task. The architecture of the model is shown in Figure 2. First, a sinuous layer with multiple filters applied in the input tweet and represented by using learned sentimental-specific word embedding. Then, a max pooling layer takes the maximum value as a feature of each shaft filter. The next hidden layer is a fully connected layer with ReLU activation, which is used to learn hidden feature representation. The last layer is a fully connected layer with 2-dimensional output, which is used to predict the positive / negative polarity distribution with Softmax. We use dropouts on the input layer and hidden layer for updating parameters, which is trained by the classifier to use."}, {"heading": "Experimental Evaluation", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Datasets and Settings", "text": "To demonstrate the effectiveness of the proposed method, we conduct experiments with the following two sets of data: 1) SemEval2013, which is a standard benchmark for the classification of Twitter feelings (Nakov and Sara 2013); 2) CST (Context-Sensitive Twitter), which is the latest benchmark for Twitter emotion classification tasks (Ren et al. 2016a); (Ren et al. 2016a) cracked tweets for basic opinion and their context to evaluate their model, which uses the contextual tweets as auxiliary data for the classification of Twitter feelings. In our experiments, we use only the basic data and not both basic and contextual tweets, as our model does not currently take the contextual tweets into account. Table 1 contains detailed information about each data set. The rating metric is Macro-F1 of the positive and negative categories. Hyper parameters for the hyperparameters should be set carefully to facilitate comparison of the result chains."}, {"heading": "Results of Comparison Experiments", "text": "The results of our models compared to other methods are shown in Table 3. All methods can be divided into two categories: traditional classifiers with different properties and classifiers for neural networks. In the first category, SSWE achieves the best performance, using Word embedding functions that encode both n-gram and sentiment information. Since C & W and Word2vec functions do not explicitly evaluate sentiment information, they are relatively weak. The NRC system performs better than other partners except SSWE because it uses sentiment lexicographs and many manually designed functions. In the second category, of course, the classifier can use word embedding for classification. Both TSWE and CNNM-Local, which used other additional information as sentiment, currently achieved the best performance in SemEval2013 and SST, respectively. Under the same condition that only sentiment information is used, our model performs better than them. Both our method and the NRC Local, which currently uses additional information and SST, respectively, achieve better performance than SST."}, {"heading": "Effect of parameter \u03b2", "text": "As stated in Equation (15), \u03b2 is the trade-off between information at the word and tweet level. We match \u03b2 to the development theorem of SemEval2013. For another coefficient \u03b1, we follow (Tang et al. 2014) to set it to 0.5. Figure 3 shows the macro F1 values of MSWE with different \u03b2 on SemEval2013 development theorem. It shows that MSWE performs better when \u03b2 = 0.8, which means that N-gram and word feeling from the lexicon are taken more into account. The model with \u03b2 = 1 stands for SSWE, but the training sentiment labeling of the word comes from the lexicon, and \u03b2 = 0 stands only for the use of sentiment information at the tweet level. The model with \u03b2 = 0 provides lower performance, which shows that the Ngram information is indispensable evidence for the Twitter sentiment classification. Consequently, in our concluding experiments, we set \u03b2 to 0.8."}, {"heading": "Related Work", "text": "For the classification of word correlations within context windows, many papers follow traditional sentiment classification methods (Pang, Lee and Vaithyanathan et al. 2015), which use machine learning methods to train a classifier for Twitter sentiment. Besides commonly used text characteristics, there are some remote monitored functions that can be used (Go, Bhayani, and Huang 2009).Many studies use these massive, loud tweets as training data or a resource for Twitter sentiment classification (Hu et al. 2013).Unlike previous studies, our approach uses remote, monitored information as well as lexicon knowledge for word embedding, which is a combination of noisylabeled tweets as training resources and knowledge base. There is a large body of work on embedding words in learning processes (Mikolov et al. 2013; Collobert et al. 2011; Pennington, Socher, Manning, 2014)."}, {"heading": "Conclusion", "text": "This paper proposes to use both the word-sentiment label from the lexicon and the tweet-sentiment label from remote monitored information to train the sentimental embedding of words. As this two pieces of information overlap at the word and tweet level, we are developing a multi-level, sentimental embedding method. Our method naturally integrates word embedding information as a folding result at the tweet level and simultaneously models word embedding and mood information. When using Learned word embedding in Twitter sentiment classification, it achieves the best results in standard benchmarks. For our future work, we plan to integrate more information that could improve Twitter sentiment classification."}], "references": [{"title": "Robust sentiment detection on twitter from biased and noisy data", "author": ["L. Barbosa", "J. Feng"], "venue": "Proceedings of COLING, COLING \u201910, 36\u201344. Bollen, J.; Mao, H.; and Pepe, A. 2011. Modeling public mood and emotion: Twitter sentiment and socio-economic", "citeRegEx": "Barbosa and Feng,? 2010", "shortCiteRegEx": "Barbosa and Feng", "year": 2010}, {"title": "Multitask learning", "author": ["R. Caruana"], "venue": "Machine Learning", "citeRegEx": "Caruana,? 1997", "shortCiteRegEx": "Caruana", "year": 1997}, {"title": "Multi-task recurrent neural network for immediacy prediction", "author": ["X. Chu", "W. Ouyang", "W. Yang", "X. Wang"], "venue": "The IEEE International Conference on Computer Vision (ICCV).", "citeRegEx": "Chu et al\\.,? 2015", "shortCiteRegEx": "Chu et al\\.", "year": 2015}, {"title": "A unified architecture for natural language processing: Deep neural networks with multitask learning", "author": ["R. Collobert", "J. Weston"], "venue": "Proceedings of ICML, ICML \u201908, 160\u2013167.", "citeRegEx": "Collobert and Weston,? 2008", "shortCiteRegEx": "Collobert and Weston", "year": 2008}, {"title": "Natural language processing (almost) from scratch", "author": ["R. Collobert", "J. Weston", "L.E.O. Bottou", "M. Karlen", "K. Kavukcuoglu", "P. Kuksa"], "venue": "J. Mach. Learn. Res. 12:2493\u20132537.", "citeRegEx": "Collobert et al\\.,? 2011", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Enhanced sentiment learning using twitter hashtags and smileys", "author": ["D. Davidov", "O. Tsur", "A. Rappoport"], "venue": "Coling 2010: Posters, 241\u2013249.", "citeRegEx": "Davidov et al\\.,? 2010", "shortCiteRegEx": "Davidov et al\\.", "year": 2010}, {"title": "Multi-task learning for multiple language translation", "author": ["D. Dong", "H. Wu", "W. He", "D. Yu", "H. Wang"], "venue": "Proceedings of ACL, 1723\u20131732.", "citeRegEx": "Dong et al\\.,? 2015", "shortCiteRegEx": "Dong et al\\.", "year": 2015}, {"title": "Techniques and applications for sentiment analysis", "author": ["R. Feldman"], "venue": "Commun. ACM 56(4):82\u201389.", "citeRegEx": "Feldman,? 2013", "shortCiteRegEx": "Feldman", "year": 2013}, {"title": "Twitter sentiment classification using distant supervision", "author": ["A. Go", "R. Bhayani", "L. Huang"], "venue": "CS224N Project Report, Stanford 1:12.", "citeRegEx": "Go et al\\.,? 2009", "shortCiteRegEx": "Go et al\\.", "year": 2009}, {"title": "Mining and summarizing customer reviews", "author": ["M. Hu", "B. Liu"], "venue": "Proceedings of KDD, KDD \u201904, 168\u2013177.", "citeRegEx": "Hu and Liu,? 2004", "shortCiteRegEx": "Hu and Liu", "year": 2004}, {"title": "Unsupervised sentiment analysis with emotional signals", "author": ["X. Hu", "J. Tang", "H. Gao", "H. Liu"], "venue": "Proceedings of WWW, 607\u2013618. ACM.", "citeRegEx": "Hu et al\\.,? 2013", "shortCiteRegEx": "Hu et al\\.", "year": 2013}, {"title": "Distributed representations of sentences and documents", "author": ["Q.V. Le", "Mikolov."], "venue": "ICML.", "citeRegEx": "Le and Mikolov.,? 2014", "shortCiteRegEx": "Le and Mikolov.", "year": 2014}, {"title": "Representation learning using multi-task deep neural networks for semantic classification and information retrieval", "author": ["X. Liu", "J. Gao", "X. He", "L. Deng", "K. Duh", "Y.-Y. Wang"], "venue": "Proceedings of NAACL, 912\u2013921.", "citeRegEx": "Liu et al\\.,? 2015", "shortCiteRegEx": "Liu et al\\.", "year": 2015}, {"title": "Learning contextsensitive word embeddings with neural tensor skip-gram model", "author": ["P. Liu", "X. Qiu", "X. Huang"], "venue": "IJCAI.", "citeRegEx": "Liu et al\\.,? 2015", "shortCiteRegEx": "Liu et al\\.", "year": 2015}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["T. Mikolov", "I. Sutskever", "K. Chen", "G.S. Corrado", "J. Dean"], "venue": "Burges, C. J. C.; Bottou, L.; Welling, M.; Ghahramani, Z.; and Weinberger, K. Q., eds., Proc. NIPS, Proc. NIPS, 3111\u20133119. Curran", "citeRegEx": "Mikolov et al\\.,? 2013", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Nrc-canada: Building the state-of-the-art in sentiment analysis of tweets", "author": ["S. Mohammad", "K. Svetlana"], "venue": "Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Proceedings of the Seventh International Workshop on Semantic Evaluation (SemEval", "citeRegEx": "Mohammad and Svetlana,? 2013", "shortCiteRegEx": "Mohammad and Svetlana", "year": 2013}, {"title": "Sentiment analysis in Twitter with lightweight discourse analysis", "author": ["S. Mukherjee", "P. Bhattacharyya"], "venue": "Proceedings of COLING 2012, 1847\u20131864.", "citeRegEx": "Mukherjee and Bhattacharyya,? 2012", "shortCiteRegEx": "Mukherjee and Bhattacharyya", "year": 2012}, {"title": "Semeval-2013 task 2: Sentiment analysis in twitter", "author": ["P. Nakov", "R. Sara"], "venue": "Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Proceedings of the Seventh International Workshop on Semantic Evaluation (SemEval 2013), 312\u2013320.", "citeRegEx": "Nakov and Sara,? 2013", "shortCiteRegEx": "Nakov and Sara", "year": 2013}, {"title": "Opinion mining and sentiment analysis", "author": ["B. Pang", "L. Lee"], "venue": "Foundations and Trends in Information Retrieval 2(1-2):1\u2013135.", "citeRegEx": "Pang and Lee,? 2008", "shortCiteRegEx": "Pang and Lee", "year": 2008}, {"title": "Thumbs up? sentiment classification using machine learning techniques", "author": ["B. Pang", "L. Lee", "S. Vaithyanathan"], "venue": "Proceedings of EMNLP, EMNLP, 79\u201386.", "citeRegEx": "Pang et al\\.,? 2002", "shortCiteRegEx": "Pang et al\\.", "year": 2002}, {"title": "Glove: Global vectors for word representation", "author": ["J. Pennington", "R. Socher", "C. Manning"], "venue": "Proc. EMNLP, Proc. EMNLP, 1532\u20131543. Association for Computational Linguistics.", "citeRegEx": "Pennington et al\\.,? 2014", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Political tendency identification in twitter using sentiment analysis techniques", "author": ["F. Pla", "Hurtado", "L.I.S.-F."], "venue": "Proceedings of COLING, 183\u2013192.", "citeRegEx": "Pla et al\\.,? 2014", "shortCiteRegEx": "Pla et al\\.", "year": 2014}, {"title": "Contextsensitive twitter sentiment classification using neural network", "author": ["Y. Ren", "Y. Zhang", "M. Zhang", "D. Ji"], "venue": null, "citeRegEx": "Ren et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Ren et al\\.", "year": 2016}, {"title": "Improving twitter sentiment classification using topic-enriched multiprototype word embeddings", "author": ["Y. Ren", "Y. Zhang", "M. Zhang", "D. Ji"], "venue": null, "citeRegEx": "Ren et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Ren et al\\.", "year": 2016}, {"title": "Exploiting topic based twitter sentiment for stock prediction", "author": ["J. Si", "A. Mukherjee", "B. Liu", "Q. Li", "H. Li", "X. Deng"], "venue": "Proceedings of ACL, 24\u201329.", "citeRegEx": "Si et al\\.,? 2013", "shortCiteRegEx": "Si et al\\.", "year": 2013}, {"title": "Semi-supervised recursive autoencoders for predicting sentiment distributions", "author": ["R. Socher", "J. Pennington", "E.H. Huang", "A.Y. Ng", "C.D. Manning"], "venue": "Proceedings of EMNLP, 151\u2013161.", "citeRegEx": "Socher et al\\.,? 2011", "shortCiteRegEx": "Socher et al\\.", "year": 2011}, {"title": "Recursive deep models for semantic compositionality over a sentiment treebank", "author": ["R. Socher", "A. Perelygin", "J.Y. Wu", "J. Chuang", "C.D. Manning", "A.Y. Ng", "C. Potts"], "venue": "Proceedings of EMNLP, 1642. Citeseer.", "citeRegEx": "Socher et al\\.,? 2013", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Learning sentiment-specific word embedding for twitter sentiment classification", "author": ["D. Tang", "F. Wei", "M. Zhou", "T. Liu", "B. Qin"], "venue": "ACL, ACL.", "citeRegEx": "Tang et al\\.,? 2014", "shortCiteRegEx": "Tang et al\\.", "year": 2014}, {"title": "Learning semantic representations of users and products for document level sentiment classification", "author": ["D. Tang", "B. Qin", "T. Liu"], "venue": "Proceedings of ACL, 1014\u20131023.", "citeRegEx": "Tang et al\\.,? 2015", "shortCiteRegEx": "Tang et al\\.", "year": 2015}, {"title": "Sentiment-specific representation learning for document-level sentiment analysis", "author": ["D. Tang"], "venue": "Proceedings of WSDM, WSDM, 447\u2013452. ACM.", "citeRegEx": "Tang,? 2015", "shortCiteRegEx": "Tang", "year": 2015}, {"title": "Baselines and bigrams: Simple, good sentiment and topic classification", "author": ["S. Wang", "C.D. Manning"], "venue": "Proceedings ofACL, 90\u201394. Association for Computational Linguistics.", "citeRegEx": "Wang and Manning,? 2012", "shortCiteRegEx": "Wang and Manning", "year": 2012}, {"title": "Recognizing contextual polarity in phrase-level sentiment analysis", "author": ["T. Wilson", "J. Wiebe", "P. Hoffmann"], "venue": "Proceedings of EMNLP, HLT \u201905, 347\u2013354.", "citeRegEx": "Wilson et al\\.,? 2005", "shortCiteRegEx": "Wilson et al\\.", "year": 2005}, {"title": "Learning continuous word embedding with metadata for question retrieval in community question answering", "author": ["G. Zhou", "T. He", "J. Zhao", "P. Hu"], "venue": "Proceedings of ACL, 250\u2013259.", "citeRegEx": "Zhou et al\\.,? 2015", "shortCiteRegEx": "Zhou et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 24, "context": "As a result of its massive, diverse, and rising user base, the containing opinion information in Twitter has been successfully used to many tasks, such as stock market movement prediction (Si et al. 2013), political monitoring (Pla and Hurtado 2014) and inferring public mood about social events (Bollen, Mao, and Pepe 2011).", "startOffset": 188, "endOffset": 204}, {"referenceID": 0, "context": "Researchers have proposed many approaches to improve Twitter classification performance (Davidov, Tsur, and Rappoport 2010; Barbosa and Feng 2010; Mukherjee and Bhattacharyya 2012).", "startOffset": 88, "endOffset": 180}, {"referenceID": 16, "context": "Researchers have proposed many approaches to improve Twitter classification performance (Davidov, Tsur, and Rappoport 2010; Barbosa and Feng 2010; Mukherjee and Bhattacharyya 2012).", "startOffset": 88, "endOffset": 180}, {"referenceID": 4, "context": ", word level and document(sentence) level, for natural language processing tasks (Le and Mikolov 2014; Collobert et al. 2011; Tang, Qin, and Liu 2015).", "startOffset": 81, "endOffset": 150}, {"referenceID": 4, "context": "Traditional word embedding methods (Collobert et al. 2011; Mikolov et al. 2013) model the syntactic context information of words.", "startOffset": 35, "endOffset": 79}, {"referenceID": 14, "context": "Traditional word embedding methods (Collobert et al. 2011; Mikolov et al. 2013) model the syntactic context information of words.", "startOffset": 35, "endOffset": 79}, {"referenceID": 27, "context": "Based on them, (Tang et al. 2014) proposed Sentiment-Specific Word Embedding (SSWE) learning method for Twitter sentiment classification, which aims to tackle the problem that two word with opposite polarity", "startOffset": 15, "endOffset": 33}, {"referenceID": 9, "context": "However, existing work only exploit Twitter overall sentiment label for learning sentiment-specific word embedding, although there are many state-of-the-art sentiment lexicons (Hu and Liu 2004; Wilson, Wiebe, and Hoffmann 2005), which list common sentiment-baring words with its polarity.", "startOffset": 176, "endOffset": 227}, {"referenceID": 29, "context": "For exploiting tweet sentiment label, (Tang 2015) and (Ren et al.", "startOffset": 38, "endOffset": 49}, {"referenceID": 18, "context": "On the other hand, word sentiment polarity from lexicon is still another useful information for sentiment classification (Pang and Lee 2008; Feldman 2013; Socher et al. 2013).", "startOffset": 121, "endOffset": 174}, {"referenceID": 7, "context": "On the other hand, word sentiment polarity from lexicon is still another useful information for sentiment classification (Pang and Lee 2008; Feldman 2013; Socher et al. 2013).", "startOffset": 121, "endOffset": 174}, {"referenceID": 26, "context": "On the other hand, word sentiment polarity from lexicon is still another useful information for sentiment classification (Pang and Lee 2008; Feldman 2013; Socher et al. 2013).", "startOffset": 121, "endOffset": 174}, {"referenceID": 3, "context": "Inspired by recent work for Multi-task deep Learning (Collobert and Weston 2008), we propose to learn word embedding by exploiting multi-level sentiment representation objective optimization approach.", "startOffset": 53, "endOffset": 80}, {"referenceID": 4, "context": "Sentimentspecific word embedding model stems from C&W model (Collobert et al. 2011), which learns word representation from n-gram contexts by using negative sampling.", "startOffset": 60, "endOffset": 83}, {"referenceID": 27, "context": "To address this problem, (Tang et al. 2014) proposed a sentiment-specific word embedding SSWE model base on C&W model.", "startOffset": 25, "endOffset": 43}, {"referenceID": 18, "context": "Existing work demonstrate that sentiment lexicon is an important resource for sentiment classification (Pang and Lee 2008; Feldman 2013; Socher et al. 2013).", "startOffset": 103, "endOffset": 156}, {"referenceID": 7, "context": "Existing work demonstrate that sentiment lexicon is an important resource for sentiment classification (Pang and Lee 2008; Feldman 2013; Socher et al. 2013).", "startOffset": 103, "endOffset": 156}, {"referenceID": 26, "context": "Existing work demonstrate that sentiment lexicon is an important resource for sentiment classification (Pang and Lee 2008; Feldman 2013; Socher et al. 2013).", "startOffset": 103, "endOffset": 156}, {"referenceID": 26, "context": "In particular, (Socher et al. 2013) developed Sentiment Treebank, which is based on word sentiment and further annotates the sentiment label of syntactically plausible phrase of sentences.", "startOffset": 15, "endOffset": 35}, {"referenceID": 2, "context": "As shown in Figure 1(c), we exploit two sub networks for modelling two level optimization objectives by using similar framework as multi-task learning (Chu et al. 2015; Collobert and Weston 2008; Dong et al. 2015).", "startOffset": 151, "endOffset": 213}, {"referenceID": 3, "context": "As shown in Figure 1(c), we exploit two sub networks for modelling two level optimization objectives by using similar framework as multi-task learning (Chu et al. 2015; Collobert and Weston 2008; Dong et al. 2015).", "startOffset": 151, "endOffset": 213}, {"referenceID": 6, "context": "As shown in Figure 1(c), we exploit two sub networks for modelling two level optimization objectives by using similar framework as multi-task learning (Chu et al. 2015; Collobert and Weston 2008; Dong et al. 2015).", "startOffset": 151, "endOffset": 213}, {"referenceID": 9, "context": "The sentiment polarity is from existing sentiment lexicon, in our experiments, we use the lexicon from (Hu and Liu 2004).", "startOffset": 103, "endOffset": 120}, {"referenceID": 17, "context": "Datasets For demonstrating the effectiveness of the proposed method, we perform experiments on the following two datasets: 1) SemEval2013, which is a standard Twitter sentiment classification benchmark (Nakov and Sara 2013); 2) CST (Context-Sensitive Twitter), which is the latest benchmark for Twitter sentiment classification task (Ren et al.", "startOffset": 202, "endOffset": 223}, {"referenceID": 27, "context": "For another coefficient \u03b1, we follow (Tang et al. 2014) to set it as 0.", "startOffset": 37, "endOffset": 55}, {"referenceID": 10, "context": "Many studies use these massive noisy labelled tweets as training data or auxiliary source for Twitter sentiment classification (Hu et al. 2013).", "startOffset": 127, "endOffset": 143}, {"referenceID": 14, "context": "There is a large body of work on word embedding learning (Mikolov et al. 2013; Collobert et al. 2011; Pennington, Socher, and Manning 2014).", "startOffset": 57, "endOffset": 139}, {"referenceID": 4, "context": "There is a large body of work on word embedding learning (Mikolov et al. 2013; Collobert et al. 2011; Pennington, Socher, and Manning 2014).", "startOffset": 57, "endOffset": 139}, {"referenceID": 27, "context": "42 SVM + C&W (Tang et al. 2014) 75.", "startOffset": 13, "endOffset": 31}, {"referenceID": 27, "context": "89 SVM + Word2vec (Tang et al. 2014) 76.", "startOffset": 18, "endOffset": 36}, {"referenceID": 27, "context": "31 NBSVM (Tang et al. 2014) 75.", "startOffset": 9, "endOffset": 27}, {"referenceID": 27, "context": "28 RAE (Tang et al. 2014) 75.", "startOffset": 7, "endOffset": 25}, {"referenceID": 15, "context": "12 NRC (Top System in SemEval) (Mohammad and Svetlana 2013) 84.", "startOffset": 31, "endOffset": 59}, {"referenceID": 27, "context": "24 SSWE (Tang et al. 2014) 84.", "startOffset": 8, "endOffset": 26}, {"referenceID": 30, "context": "NBSVM: a state-of-the-art performer which trades-off between Naive Bayes and NB-enhanced SVM (Wang and Manning 2012).", "startOffset": 93, "endOffset": 116}, {"referenceID": 25, "context": "RAE: Recursive Autoencoders with pre-trained word vectors from Wikipedia (Socher et al. 2011).", "startOffset": 73, "endOffset": 93}, {"referenceID": 15, "context": "NRC: the top-performed system in SemEval 2013 Twitter sentiment classification track which incorporates diverse sentiment lexicons and many manually designed features (Mohammad and Svetlana 2013).", "startOffset": 167, "endOffset": 195}, {"referenceID": 27, "context": "SSWE: SVM classifier with SSWE word embedding features (Tang et al. 2014).", "startOffset": 55, "endOffset": 73}, {"referenceID": 27, "context": "ficial than common embedding (Liu, Qiu, and Huang 2015; Ren et al. 2016b; Tang et al. 2014; Zhou et al. 2015).", "startOffset": 29, "endOffset": 109}, {"referenceID": 32, "context": "ficial than common embedding (Liu, Qiu, and Huang 2015; Ren et al. 2016b; Tang et al. 2014; Zhou et al. 2015).", "startOffset": 29, "endOffset": 109}, {"referenceID": 27, "context": "For sentiment classification, (Tang et al. 2014) proposed to integrate the sentiment information of tweets into neural network to learn sentiment specific word embedding.", "startOffset": 30, "endOffset": 48}, {"referenceID": 27, "context": "(Tang et al. 2014) added a new optimization objective on the top layer of C&W model (Collobert et al.", "startOffset": 0, "endOffset": 18}, {"referenceID": 4, "context": "2014) added a new optimization objective on the top layer of C&W model (Collobert et al. 2011), and it is able to add more optimization objective, such as topic distribution (Ren et al.", "startOffset": 71, "endOffset": 94}, {"referenceID": 1, "context": "Our method uses similar schema as multi-task learning, which is firstly proposed in (Caruana 1997).", "startOffset": 84, "endOffset": 98}, {"referenceID": 3, "context": "For natural language processing community, a notable work about multitask learning was proposed by (Collobert and Weston 2008).", "startOffset": 99, "endOffset": 126}, {"referenceID": 12, "context": "(Liu et al. 2015) proposed to jointly train semantic classification and information retrieval, which have more shared layers between two tasks.", "startOffset": 0, "endOffset": 17}], "year": 2016, "abstractText": "Most of existing work learn sentiment-specific word representation for improving Twitter sentiment classification, which encoded both n-gram and distant supervised tweet sentiment information in learning process. They assume all words within a tweet have the same sentiment polarity as the whole tweet, which ignores the word its own sentiment polarity. To address this problem, we propose to learn sentimentspecific word embedding by exploiting both lexicon resource and distant supervised information. We develop a multi-level sentiment-enriched word embedding learning method, which uses parallel asymmetric neural network to model n-gram, word level sentiment and tweet level sentiment in learning process. Experiments on standard benchmarks show our approach outperforms state-of-the-art methods.", "creator": "dvips(k) 5.991 Copyright 2011 Radical Eye Software"}}}