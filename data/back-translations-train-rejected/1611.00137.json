{"id": "1611.00137", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Nov-2016", "title": "Embedding Deep Metric for Person Re-identication A Study Against Large Variations", "abstract": "Person re-identification is challenging due to the large variations of pose, illumination, occlusion and camera view. Owing to these variations, the pedestrian data is distributed as highly-curved manifolds in the feature space, despite the current convolutional neural networks (CNN)'s capability of feature extraction. However, the distribution is unknown, so it is difficult to use the geodesic distance when comparing two samples. In practice, the current deep embedding methods use the Euclidean distance for the training and test. On the other hand, the manifold learning methods suggest to use the Euclidean distance in the local range, combining with the graphical relationship between samples, for approximating the geodesic distance. From this point of view, selecting suitable positive i.e. intra-class) training samples within a local range is critical for training the CNN embedding, especially when the data has large intra-class variations. In this paper, we propose a novel moderate positive sample mining method to train robust CNN for person re-identification, dealing with the problem of large variation. In addition, we improve the learning by a metric weight constraint, so that the learned metric has a better generalization ability. Experiments show that these two strategies are effective in learning robust deep metrics for person re-identification, and accordingly our deep model significantly outperforms the state-of-the-art methods on several benchmarks of person re-identification. Therefore, the study presented in this paper may be useful in inspiring new designs of deep models for person re-identification.", "histories": [["v1", "Tue, 1 Nov 2016 06:03:48 GMT  (2735kb,D)", "http://arxiv.org/abs/1611.00137v1", "Published in ECCV2016. arXiv admin note: substantial text overlap witharXiv:1511.07545"]], "COMMENTS": "Published in ECCV2016. arXiv admin note: substantial text overlap witharXiv:1511.07545", "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["hailin shi", "yang yang", "xiangyu zhu", "shengcai liao", "zhen lei", "weishi zheng", "stan z li"], "accepted": false, "id": "1611.00137"}, "pdf": {"name": "1611.00137.pdf", "metadata": {"source": "CRF", "title": "Embedding Deep Metric for Person Re-identification: A Study Against Large Variations", "authors": ["Hailin Shi", "Yang Yang", "Xiangyu Zhu", "Shengcai Liao", "Zhen Lei", "Weishi Zheng", "Stan Z. Li"], "emails": ["hailin.shi@nlpr.ia.ac.cn", "yang.yang@nlpr.ia.ac.cn", "xiangyu.zhu@nlpr.ia.ac.cn", "scliao@nlpr.ia.ac.cn", "zlei@nlpr.ia.ac.cn", "szli@nlpr.ia.ac.cn"], "sections": [{"heading": null, "text": "Keywords: Personality Identification, Deep Learning, CNN"}, {"heading": "1 Introduction", "text": "Given that these two are very complex issues, it is not surprising that they are human beings who, in the past, have been human beings as well as human beings."}, {"heading": "2 Related Work", "text": "Positive Sample Mining. The hard negative mining strategy [30] has been applied to face recognition. In person recognition, IDLA [1] has also used hard negative mining methods for training. By forcing the model to focus on the hard negatives close to the decision boundary, hard negative mining improves training efficiency and the performance of the model. In this paper, we find that selecting moderate positive samples is also a major problem for re-identification of the learner. Moderate positives are as critical to network training as hard negatives, especially when the data show large class-internal differences. However, there is hardly any previous attempt in this aspect to learn deep embedding. In our approach, we propose the novel strategy of moderate positive mining methods to address the problem. We try the moderate positives for Mahtivtraining and avoid the hard ones resulting from extreme class-internal variations in pedestrian data. We empirically note that this strategy effectively improves (see section 4.Identification Strategy)."}, {"heading": "3 Proposed Method", "text": "In this section, we will first present the moderate positive mining method, then return to DDML and introduce the weight restriction."}, {"heading": "3.1 Moderate Positive Mining", "text": "In fact, most of them are able to trump themselves. (...) Most of them are able to trump themselves. (...) Most of them are able to trump themselves. (...) Most of them are able to trump themselves. (...) Most of them are able to trump themselves. (...) Most of them are able to trump themselves. (...) Most of them are able to trump themselves. (...) Most of them are able to trump themselves. (...) Most of them are able to trump themselves. (...) Most of them are able to trump themselves. (...)"}, {"heading": "3.2 Weight Constraint for Deep Metric Learning", "text": "Once the CNN extraction is the characteristics of a symmetric semi-definition, the metric levels are subsequently executed to calculate the distance as shown in Fig. 3. The metric learning layer is like the structure proposed in the DDML [11], and its learning ability is enhanced by a weight limitation. The two sets of pedestrians I1 and I2 are the corresponding characteristics of the anchor, the positive and negative samples.Revisited DDML The Mahalanobis distance is formulated as d (x1) = x2) TM (1), where x2 (x2), where x2 (x2), M is a symmetric positive semi-definition."}, {"heading": "4 Experiments", "text": "Our methodology is implemented through a modification of the CUDA-Convnet [14] framework. We report on the evaluation using the one-shot standard protocol using three common human identification benchmarks, namely CUHK03 [17], CUHK01 [16] and VIPeR [9]. We begin by describing the CNN architecture we used for feature extraction, then report on the evaluation of the CUHK03 validation set to analyze the impact of moderate positive mining (Section 4.2), weight restriction (Section 4.3) and CNN architecture (Section 4.4), then compare our performance with the most advanced methods for CUHK03 and CUHK01 (Section 4.5 and Section 4.6) and finally show that the proposed methodology performs well on the small VIPeR [9] dataset and achieves competitive results (Section 4.7)."}, {"heading": "4.1 CNN architecture", "text": "The CNN consists of 3 branches with the details shown in Fig. 4. The input image is normalized to 128 x 64 RGB. Subsequently, it is divided into three overlapping 64 x 64 color fields, each of which is charged by a branch. Each branch consists of 3 Convolutionary layers and 2 Pooling layers. There is no parameter exchange between branches. Subsequently, the 3 branches are completed by a FC layer with ReLU activation. Finally, the output feature vector x is calculated by an additional FC layer with linear activation. To ensure computational stability, the features are normalized before being sent to the metric learning layers. CNN and the metric layers are learned together by backward propagation. Our network has much lower weights (0.84M parameters) compared to the previous best methods on CUHK03 & 01 (IDLA [1], 2.32M] and VIR Peep6 [Feature], 26M]."}, {"heading": "4.2 Analysis of Moderate Positive Mining", "text": "The standard protocol randomly selects 1,169 people for training, 100 for validation and 100 for testing. We train CNN with a Softmax classification based on the training set. Softmax's results correspond to the identities. To demonstrate the benefit of moderate positive mining, we compare the performance on the validation set with and without moderate positive mining. The absence of moderate positive mining results in significant performance derogation (blue) and rank-1 identification rates are shown in Fig. 5 (a). We find that the collaboration of moderate positive mining and hard negative mining produces the best results (red line). The absence of moderate positive mining results in significant performance derogation (blue), reflecting that the manifold is poorly learned when all positive pairs are used indiscriminately."}, {"heading": "4.3 Analysis of Weight Constraint", "text": "In Fig. 6 (a), we show the spectra of the matrix M. We also show the corresponding rank-1 identification rates in Fig. 6 (b). At \u03bb = 102, the singular values at 1 are nearly constant, which means that the metric layers yield almost the Euclidean distance, leading to the low deviation and high distortion. As the divergence persists, the matrix exhibits different singular values, implying that the learned measurement fits well with the training data, but exhibits rather an overmatch. Therefore, a moderate value of \u03bb leads to a target conflict between variance and distortion, which is a suitable choice for good performance (Fig. 6)."}, {"heading": "4.4 Analysis of Untied Branches", "text": "We show the learned filters of unbound branches in Fig. 7 (a). The network has learned remarkable color representations consistent with the results of IDLA [1]. Since we use unbound weights between branches, each branch learns different filters from its own part. As shown in Fig. 7 (a), where each line demonstrates a filter set from a branch, we can find that each branch has its own color accentuation. For example, the middle branch tends to violet and blue, while the lower branch has filters of obviously brighter colors than the other two. This is because pedestrian images have a regular appearance of the human body. Each part has its own color distribution. Therefore, the branches learn the partially specific filters, taking into account the morphological information for the characteristics. We compare the performances with and without bound weights between branches in Fig. We expand the number of filters in the bound branch network so that it is roughly equal to the number of branches tied."}, {"heading": "4.5 Performance on CUHK03", "text": "The images are randomly (0-5 pixels) cropped and stretched on the horizon and vertically to regain size. In accordance with the validation results (Section 4.3), we set the parameter \u03bb = 10 \u2212 2 in all subsequent experiments. We compare our performance with conventional methods and deep learning methods. Traditional methods include LOMO-XQDA [19], KISSME [13], LDM [10], RANK [24], eSDC [40], SDALF [7], LMNN [32], ITML [5], Euclid [40]. Deep learning methods include FPNN [17] and IDLA [1]."}, {"heading": "4.6 Performance on CUHK01", "text": "The CUHK01 dataset contains 971 subjects, each of whom has 4 images below 2 camera views. In accordance with the protocol in [16], the dataset is divided into a training set of 871 subjects and a test set of 100 subjects. We train the network on CUHK03 and refine it further on CUHK01, as the same setting with the state-of-the-art method IDLA [1]. We compare our approach with the aforementioned methods. The CMC curves and rank-1 identification rates are shown in Fig. 9 (a). Our approach achieves the best result (the red line) with 69% rank-1 identification rate. In addition, to verify the limitation of the CUHK01 dataset, we incorporate the recently published market 1501 [42] into the training. As training data increases, our network performs better (the red line) with the one marked as \"our\" line. \""}, {"heading": "4.7 Performance on VIPeR", "text": "The VIPeR [9] dataset includes 632 people, each of whom has 2 images from two different cameras. Although VIPeR is a small dataset that is not suitable for training CNN, we are still interested in the performance of this demanding task. It is randomly divided into two subgroups, each of which has non-overlapping subjects of the same size. Results are shown in Figure 9 (b). We compare our model with IDLA [1], DeepFeature [6], Visual Word [37], Saliency Matching (SalMatch), Patch Matching (PatMatch) [39], ELF [8], LMNR [3], eBiCov [21], Patch Matching (PatMatch) [PADMatch] [39], Filter Rate [PAD91] and Filter Rate [420]."}, {"heading": "5 Conclusion", "text": "Although CNN has a strong ability to extract traits, pedestrian data follow pedestrian data due to the large variations in the very irregular distribution in the trait space. To address the problem and train the robust embedding, the positive training samples should be consciously selected. In this article, we propose a novel method of moderate positive mining for embedding robust depth meters for the recognition of people. We find that the removal of moderately positive samples is critical for training deep networks, especially when it comes to difficult data with large class variations (e.g. pedestrians).The moderate positive mining method dynamically selects suitable positive pairs to learn robust embedding that adapts to the diverse data. In addition, we propose weight limitation to achieve the good robustness of the excessive problem in personnel identification. As a result of these improvements, our method achieves state-of-the-of-the-art measurement values for CUHK03 and CUt, whereby the net weight class results can be improved most during the VIR person's weight class."}, {"heading": "6 Acknowledgement", "text": "This work has been supported by the National Key Research and Development Plan (Grant No.2016YFC0801003), the Chinese Projects of the National Natural Science Foundation # 61473291, # 61572501, # 61502491, # 61572536, the NVIDIA GPU Donation Program, and AuthenMetric R & D Funds."}], "references": [{"title": "An improved deep learning architecture for person re-identification", "author": ["E. Ahmed", "M. Jones", "T.K. Marks"], "venue": "Computer Vision and Pattern Recognition (CVPR), 2015 IEEE Conference on. IEEE", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2015}, {"title": "Multiple-shot human reidentification by mean riemannian covariance grid", "author": ["S. Bak", "E. Corvee", "F. Bremond", "M. Thonnat"], "venue": "Advanced Video and SignalBased Surveillance (AVSS), 2011 8th IEEE International Conference on. pp. 179\u2013 184. IEEE", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2011}, {"title": "Multiple-shot person reidentification by chromatic and epitomic analyses", "author": ["L. Bazzani", "M. Cristani", "A. Perina", "V. Murino"], "venue": "Pattern Recognition Letters 33(7), 898\u2013903", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2012}, {"title": "Laplacian eigenmaps for dimensionality reduction and data representation", "author": ["M. Belkin", "P. Niyogi"], "venue": "Neural computation 15(6), 1373\u20131396", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2003}, {"title": "Information-theoretic metric learning", "author": ["J.V. Davis", "B. Kulis", "P. Jain", "S. Sra", "I.S. Dhillon"], "venue": "Proceedings of the 24th international conference on Machine learning. pp. 209\u2013216. ACM", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2007}, {"title": "Deep feature learning with relative distance comparison for person re-identification", "author": ["S. Ding", "L. Lin", "G. Wang", "H. Chao"], "venue": "Pattern Recognition", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2015}, {"title": "Person reidentification by symmetry-driven accumulation of local features", "author": ["M. Farenzena", "L. Bazzani", "A. Perina", "V. Murino", "M. Cristani"], "venue": "Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference on. pp. 2360\u2013 2367. IEEE", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2010}, {"title": "Person reidentification using spatiotemporal appearance", "author": ["N. Gheissari", "T.B. Sebastian", "R. Hartley"], "venue": "Computer Vision and Pattern Recognition, 2006 IEEE Computer Society Conference on. vol. 2, pp. 1528\u20131535. IEEE", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2006}, {"title": "Evaluating appearance models for recognition, reacquisition, and tracking", "author": ["D. Gray", "S. Brennan", "H. Tao"], "venue": "Proc. IEEE International Workshop on Performance Evaluation for Tracking and Surveillance (PETS). vol. 3. Citeseer", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2007}, {"title": "Is that you? metric learning approaches for face identification", "author": ["M. Guillaumin", "J. Verbeek", "C. Schmid"], "venue": "Computer Vision, 2009 IEEE 12th International Conference on. pp. 498\u2013505. IEEE", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2009}, {"title": "Discriminative deep metric learning for face verification in the wild", "author": ["J. Hu", "J. Lu", "Y.P. Tan"], "venue": "Computer Vision and Pattern Recognition (CVPR), 2014 IEEE Conference on. pp. 1875\u20131882. IEEE", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2014}, {"title": "Joint learning for attribute-consistent person re-identification", "author": ["S. Khamis", "C.H. Kuo", "V.K. Singh", "V.D. Shet", "L.S. Davis"], "venue": "Computer Vision-ECCV 2014 Workshops. pp. 134\u2013146. Springer", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2014}, {"title": "Large scale metric learning from equivalence constraints", "author": ["M. Koestinger", "M. Hirzer", "P. Wohlhart", "P.M. Roth", "H. Bischof"], "venue": "Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on. pp. 2288\u20132295. IEEE", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2012}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "Advances in neural information processing systems. pp. 1097\u20131105", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2012}, {"title": "Locally aligned feature transforms across views", "author": ["W. Li", "X. Wang"], "venue": "Computer Vision and Pattern Recognition (CVPR), 2013 IEEE Conference on. pp. 3594\u2013 3601. IEEE", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2013}, {"title": "Human reidentification with transferred metric learning", "author": ["W. Li", "R. Zhao", "X. Wang"], "venue": "ACCV (1). pp. 31\u201344", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2012}, {"title": "Deepreid: Deep filter pairing neural network for person re-identification", "author": ["W. Li", "R. Zhao", "T. Xiao", "X. Wang"], "venue": "Computer Vision and Pattern Recognition (CVPR), 2014 IEEE Conference on. pp. 152\u2013159. IEEE", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning locallyadaptive decision functions for person verification", "author": ["Z. Li", "S. Chang", "F. Liang", "T.S. Huang", "L. Cao", "J.R. Smith"], "venue": "Computer Vision and Pattern Recognition (CVPR), 2013 IEEE Conference on. pp. 3610\u20133617. IEEE", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2013}, {"title": "Person re-identification by local maximal occurrence representation and metric learning", "author": ["S. Liao", "Y. Hu", "X. Zhu", "S.Z. Li"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 2197\u20132206", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2015}, {"title": "Person re-identification: What features are important? In: Computer Vision\u2013ECCV 2012", "author": ["C. Liu", "S. Gong", "C.C. Loy", "X. Lin"], "venue": "Workshops and Demonstrations. pp. 391\u2013401. Springer", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2012}, {"title": "Bicov: a novel image representation for person reidentification and face verification", "author": ["B. Ma", "Y. Su", "F. Jurie"], "venue": "British Machive Vision Conference. pp. 11\u2013pages", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2012}, {"title": "Multivariate statistical methods: a primer", "author": ["B.F. Manly"], "venue": "CRC Press", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2004}, {"title": "Saliency weighted features for person reidentification", "author": ["N. Martinel", "C. Micheloni", "G.L. Foresti"], "venue": "Computer Vision-ECCV 2014 Workshops. pp. 191\u2013208. Springer", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2014}, {"title": "Metric learning to rank", "author": ["B. McFee", "G.R. Lanckriet"], "venue": "Proceedings of the 27th International Conference on Machine Learning (ICML-10). pp. 775\u2013782", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2010}, {"title": "Pcca: A new approach for distance learning from sparse pairwise constraints", "author": ["A. Mignon", "F. Jurie"], "venue": "Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on. pp. 2666\u20132672. IEEE", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2012}, {"title": "Learning to rank in person reidentification with metric ensembles", "author": ["S. Paisitkriangkrai", "C. Shen", "A. van den Hengel"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 1846\u20131855", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep face recognition", "author": ["O.M. Parkhi", "A. Vedaldi", "A. Zisserman"], "venue": "Proceedings of the British Machine Vision", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2015}, {"title": "Local fisher discriminant analysis for pedestrian re-identification", "author": ["S. Pedagadi", "J. Orwell", "S. Velastin", "B. Boghossian"], "venue": "Computer Vision and Pattern Recognition (CVPR), 2013 IEEE Conference on. pp. 3318\u20133325. IEEE", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2013}, {"title": "Nonlinear dimensionality reduction by locally linear embedding", "author": ["S.T. Roweis", "L.K. Saul"], "venue": "Science 290(5500), 2323\u20132326", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2000}, {"title": "Facenet: A unified embedding for face recognition and clustering", "author": ["F. Schroff", "D. Kalenichenko", "J. Philbin"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2015}, {"title": "A global geometric framework for nonlinear dimensionality reduction", "author": ["J.B. Tenenbaum", "V. De Silva", "J.C. Langford"], "venue": "science 290(5500), 2319\u20132323", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2000}, {"title": "Distance metric learning for large margin nearest neighbor classification", "author": ["K.Q. Weinberger", "J. Blitzer", "L.K. Saul"], "venue": "Advances in neural information processing systems. pp. 1473\u20131480", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2005}, {"title": "Person re-identification using kernelbased metric learning methods", "author": ["F. Xiong", "M. Gou", "O. Camps", "M. Sznaier"], "venue": "Computer Vision\u2013ECCV 2014, pp. 1\u201316. Springer", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2014}, {"title": "Salient color names for person re-identification", "author": ["Y. Yang", "J. Yang", "J. Yan", "S. Liao", "D. Yi", "S.Z. Li"], "venue": "Computer Vision\u2013ECCV 2014, pp. 536\u2013551. Springer", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep metric learning for practical person re-identification", "author": ["D. Yi", "Z. Lei", "S.Z. Li"], "venue": "arXiv preprint arXiv:1407.4979", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2014}, {"title": "Prism: Person re-identification via structured matching", "author": ["Z. Zhang", "V. Saligrama"], "venue": "arxiv preprint. IEEE Transaction on Pattern Analysis and Machine Intelligence", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2015}, {"title": "A novel visual word co-occurrence model for person re-identification", "author": ["Z. Zhang", "Y. Chen", "V. Saligrama"], "venue": "Computer Vision-ECCV 2014 Workshops. pp. 122\u2013133. Springer", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2014}, {"title": "Group membership prediction", "author": ["Z. Zhang", "Y. Chen", "V. Saligrama"], "venue": "Computer Vision (ICCV), 2015 IEEE International Conference on. IEEE", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2015}, {"title": "Person re-identification by salience matching", "author": ["R. Zhao", "W. Ouyang", "X. Wang"], "venue": "Computer Vision (ICCV), 2013 IEEE International Conference on. pp. 2528\u20132535. IEEE", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2013}, {"title": "Unsupervised salience learning for person reidentification", "author": ["R. Zhao", "W. Ouyang", "X. Wang"], "venue": "Computer Vision and Pattern Recognition (CVPR), 2013 IEEE Conference on. pp. 3586\u20133593. IEEE", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2013}, {"title": "Learning mid-level filters for person reidentification", "author": ["R. Zhao", "W. Ouyang", "X. Wang"], "venue": "Computer Vision and Pattern Recognition (CVPR), 2014 IEEE Conference on. pp. 144\u2013151. IEEE", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2014}, {"title": "Scalable person reidentification: A benchmark", "author": ["L. Zheng", "L. Shen", "L. Tian", "S. Wang", "J. Wang", "Q. Tian"], "venue": "Computer Vision, IEEE International Conference on", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2015}, {"title": "Person re-identification by probabilistic relative distance comparison", "author": ["W.S. Zheng", "S. Gong", "T. Xiang"], "venue": "Computer Vision and Pattern Recognition (CVPR), 2011 IEEE Conference on. pp. 649\u2013656. IEEE", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2011}], "referenceMentions": [{"referenceID": 33, "context": "The traditional methods work at improving suitable hand-crafted features [34,41], or good metric for comparison [26,13,15,18,23,39,36,38], or both of them [12,19,33,37].", "startOffset": 73, "endOffset": 80}, {"referenceID": 40, "context": "The traditional methods work at improving suitable hand-crafted features [34,41], or good metric for comparison [26,13,15,18,23,39,36,38], or both of them [12,19,33,37].", "startOffset": 73, "endOffset": 80}, {"referenceID": 25, "context": "The traditional methods work at improving suitable hand-crafted features [34,41], or good metric for comparison [26,13,15,18,23,39,36,38], or both of them [12,19,33,37].", "startOffset": 112, "endOffset": 137}, {"referenceID": 12, "context": "The traditional methods work at improving suitable hand-crafted features [34,41], or good metric for comparison [26,13,15,18,23,39,36,38], or both of them [12,19,33,37].", "startOffset": 112, "endOffset": 137}, {"referenceID": 14, "context": "The traditional methods work at improving suitable hand-crafted features [34,41], or good metric for comparison [26,13,15,18,23,39,36,38], or both of them [12,19,33,37].", "startOffset": 112, "endOffset": 137}, {"referenceID": 17, "context": "The traditional methods work at improving suitable hand-crafted features [34,41], or good metric for comparison [26,13,15,18,23,39,36,38], or both of them [12,19,33,37].", "startOffset": 112, "endOffset": 137}, {"referenceID": 22, "context": "The traditional methods work at improving suitable hand-crafted features [34,41], or good metric for comparison [26,13,15,18,23,39,36,38], or both of them [12,19,33,37].", "startOffset": 112, "endOffset": 137}, {"referenceID": 38, "context": "The traditional methods work at improving suitable hand-crafted features [34,41], or good metric for comparison [26,13,15,18,23,39,36,38], or both of them [12,19,33,37].", "startOffset": 112, "endOffset": 137}, {"referenceID": 35, "context": "The traditional methods work at improving suitable hand-crafted features [34,41], or good metric for comparison [26,13,15,18,23,39,36,38], or both of them [12,19,33,37].", "startOffset": 112, "endOffset": 137}, {"referenceID": 37, "context": "The traditional methods work at improving suitable hand-crafted features [34,41], or good metric for comparison [26,13,15,18,23,39,36,38], or both of them [12,19,33,37].", "startOffset": 112, "endOffset": 137}, {"referenceID": 11, "context": "The traditional methods work at improving suitable hand-crafted features [34,41], or good metric for comparison [26,13,15,18,23,39,36,38], or both of them [12,19,33,37].", "startOffset": 155, "endOffset": 168}, {"referenceID": 18, "context": "The traditional methods work at improving suitable hand-crafted features [34,41], or good metric for comparison [26,13,15,18,23,39,36,38], or both of them [12,19,33,37].", "startOffset": 155, "endOffset": 168}, {"referenceID": 32, "context": "The traditional methods work at improving suitable hand-crafted features [34,41], or good metric for comparison [26,13,15,18,23,39,36,38], or both of them [12,19,33,37].", "startOffset": 155, "endOffset": 168}, {"referenceID": 36, "context": "The traditional methods work at improving suitable hand-crafted features [34,41], or good metric for comparison [26,13,15,18,23,39,36,38], or both of them [12,19,33,37].", "startOffset": 155, "endOffset": 168}, {"referenceID": 0, "context": "The re-identification methods by deep learning [1,6,17,35] incorporate the two above-mentioned aspects (feature extraction and metric learning) of person re-identification into an integrated framework.", "startOffset": 47, "endOffset": 58}, {"referenceID": 5, "context": "The re-identification methods by deep learning [1,6,17,35] incorporate the two above-mentioned aspects (feature extraction and metric learning) of person re-identification into an integrated framework.", "startOffset": 47, "endOffset": 58}, {"referenceID": 16, "context": "The re-identification methods by deep learning [1,6,17,35] incorporate the two above-mentioned aspects (feature extraction and metric learning) of person re-identification into an integrated framework.", "startOffset": 47, "endOffset": 58}, {"referenceID": 34, "context": "The re-identification methods by deep learning [1,6,17,35] incorporate the two above-mentioned aspects (feature extraction and metric learning) of person re-identification into an integrated framework.", "startOffset": 47, "endOffset": 58}, {"referenceID": 16, "context": "The FPNN [17] algorithm introduced a patch matching layer for the CNN part for the first time.", "startOffset": 9, "endOffset": 13}, {"referenceID": 0, "context": "[1] proposed an improved deep learning architecture (IDLA) with cross-input neighborhood differences and patch summary features.", "startOffset": 0, "endOffset": 3}, {"referenceID": 34, "context": "As for the metric learning part, DML [35] adopted the cosine similarity and Binomial deviance.", "startOffset": 37, "endOffset": 41}, {"referenceID": 5, "context": "DeepFeature [6] adopted the Euclidean distance and triplet loss.", "startOffset": 12, "endOffset": 15}, {"referenceID": 0, "context": "Some others [1,17] used the logistic loss to directly form a binary classification problem of whether the input image pair belongs to the same identity.", "startOffset": 12, "endOffset": 18}, {"referenceID": 16, "context": "Some others [1,17] used the logistic loss to directly form a binary classification problem of whether the input image pair belongs to the same identity.", "startOffset": 12, "endOffset": 18}, {"referenceID": 0, "context": "\u2013 For training the CNN, the hard negative mining strategy has been used in [1,27,30].", "startOffset": 75, "endOffset": 84}, {"referenceID": 26, "context": "\u2013 For training the CNN, the hard negative mining strategy has been used in [1,27,30].", "startOffset": 75, "endOffset": 84}, {"referenceID": 29, "context": "\u2013 For training the CNN, the hard negative mining strategy has been used in [1,27,30].", "startOffset": 75, "endOffset": 84}, {"referenceID": 30, "context": "As argued in some manifold learning methods [31,29,4], it is effective to use the local Euclidean distance, combining with the graphical relationship between samples, to approximate the geodesic distance.", "startOffset": 44, "endOffset": 53}, {"referenceID": 28, "context": "As argued in some manifold learning methods [31,29,4], it is effective to use the local Euclidean distance, combining with the graphical relationship between samples, to approximate the geodesic distance.", "startOffset": 44, "endOffset": 53}, {"referenceID": 3, "context": "As argued in some manifold learning methods [31,29,4], it is effective to use the local Euclidean distance, combining with the graphical relationship between samples, to approximate the geodesic distance.", "startOffset": 44, "endOffset": 53}, {"referenceID": 29, "context": "The hard negative mining strategy [30] has been used for face recognition.", "startOffset": 34, "endOffset": 38}, {"referenceID": 0, "context": "In person re-identification, IDLA [1] also adopted hard negative mining for the training.", "startOffset": 34, "endOffset": 37}, {"referenceID": 5, "context": "A commonly used metric by deep learning methods is the Euclidean distance [6,30,27].", "startOffset": 74, "endOffset": 83}, {"referenceID": 29, "context": "A commonly used metric by deep learning methods is the Euclidean distance [6,30,27].", "startOffset": 74, "endOffset": 83}, {"referenceID": 26, "context": "A commonly used metric by deep learning methods is the Euclidean distance [6,30,27].", "startOffset": 74, "endOffset": 83}, {"referenceID": 21, "context": "Therefore, using the Mahalanobis distance is a better choice for multivariate metric [22].", "startOffset": 85, "endOffset": 89}, {"referenceID": 10, "context": "In the area of face recognition, DDML [11] implemented the Mahalanobis metric in their network, but without any constraint.", "startOffset": 38, "endOffset": 42}, {"referenceID": 16, "context": "1(a) shows some hard positive cases in the data set of CUHK03 [17].", "startOffset": 62, "endOffset": 66}, {"referenceID": 5, "context": "The usual way is to use the Mahalanobis distance (or the special case Euclidean) [6,30,11] which is a suitable metric in the ideal condition (Fig.", "startOffset": 81, "endOffset": 90}, {"referenceID": 29, "context": "The usual way is to use the Mahalanobis distance (or the special case Euclidean) [6,30,11] which is a suitable metric in the ideal condition (Fig.", "startOffset": 81, "endOffset": 90}, {"referenceID": 10, "context": "The usual way is to use the Mahalanobis distance (or the special case Euclidean) [6,30,11] which is a suitable metric in the ideal condition (Fig.", "startOffset": 81, "endOffset": 90}, {"referenceID": 30, "context": "On the other hand, the manifold learning methods [31,29,4] suggest to use the Euclidean distance (or heat kernel) in the local range, combining with the graphical relationship between samples, for approximating the geodesic distance.", "startOffset": 49, "endOffset": 58}, {"referenceID": 28, "context": "On the other hand, the manifold learning methods [31,29,4] suggest to use the Euclidean distance (or heat kernel) in the local range, combining with the graphical relationship between samples, for approximating the geodesic distance.", "startOffset": 49, "endOffset": 58}, {"referenceID": 3, "context": "On the other hand, the manifold learning methods [31,29,4] suggest to use the Euclidean distance (or heat kernel) in the local range, combining with the graphical relationship between samples, for approximating the geodesic distance.", "startOffset": 49, "endOffset": 58}, {"referenceID": 10, "context": "The metric learning layer is like the structure proposed in DDML [11], and its learning is improved via a weight constraint.", "startOffset": 65, "endOffset": 69}, {"referenceID": 21, "context": "Compared with the Mahalanobis distance, the Euclidean distance has less discriminability but better generalization ability, because it does not take account of the scales and the correlation across dimensions [22].", "startOffset": 209, "endOffset": 213}, {"referenceID": 13, "context": "Our method is implemented via remodifying the CUDA-Convnet [14] framework.", "startOffset": 59, "endOffset": 63}, {"referenceID": 16, "context": "CUHK03 [17], CUHK01 [16] and VIPeR [9].", "startOffset": 7, "endOffset": 11}, {"referenceID": 15, "context": "CUHK03 [17], CUHK01 [16] and VIPeR [9].", "startOffset": 20, "endOffset": 24}, {"referenceID": 8, "context": "CUHK03 [17], CUHK01 [16] and VIPeR [9].", "startOffset": 35, "endOffset": 38}, {"referenceID": 8, "context": "Finally, we show the proposed method also performs well on the small data-set of VIPeR [9] and gains competitive results (Section 4.", "startOffset": 87, "endOffset": 90}, {"referenceID": 0, "context": "84M parameters) compared with the previous best methods on CUHK03&01 (IDLA [1], 2.", "startOffset": 75, "endOffset": 78}, {"referenceID": 5, "context": "32M) and VIPeR (DeepFeature [6], 26M).", "startOffset": 28, "endOffset": 31}, {"referenceID": 34, "context": "DML [35] adopted a similar architecture but with tied weights between branches.", "startOffset": 4, "endOffset": 8}, {"referenceID": 0, "context": "The network has learned remarkable color representations, which is coherent with the results of IDLA [1].", "startOffset": 101, "endOffset": 104}, {"referenceID": 18, "context": "The traditional methods include LOMO-XQDA [19], KISSME [13], LDM [10], RANK [24], eSDC [40], SDALF [7], LMNN [32], ITML [5], Euclid [40].", "startOffset": 42, "endOffset": 46}, {"referenceID": 12, "context": "The traditional methods include LOMO-XQDA [19], KISSME [13], LDM [10], RANK [24], eSDC [40], SDALF [7], LMNN [32], ITML [5], Euclid [40].", "startOffset": 55, "endOffset": 59}, {"referenceID": 9, "context": "The traditional methods include LOMO-XQDA [19], KISSME [13], LDM [10], RANK [24], eSDC [40], SDALF [7], LMNN [32], ITML [5], Euclid [40].", "startOffset": 65, "endOffset": 69}, {"referenceID": 23, "context": "The traditional methods include LOMO-XQDA [19], KISSME [13], LDM [10], RANK [24], eSDC [40], SDALF [7], LMNN [32], ITML [5], Euclid [40].", "startOffset": 76, "endOffset": 80}, {"referenceID": 39, "context": "The traditional methods include LOMO-XQDA [19], KISSME [13], LDM [10], RANK [24], eSDC [40], SDALF [7], LMNN [32], ITML [5], Euclid [40].", "startOffset": 87, "endOffset": 91}, {"referenceID": 6, "context": "The traditional methods include LOMO-XQDA [19], KISSME [13], LDM [10], RANK [24], eSDC [40], SDALF [7], LMNN [32], ITML [5], Euclid [40].", "startOffset": 99, "endOffset": 102}, {"referenceID": 31, "context": "The traditional methods include LOMO-XQDA [19], KISSME [13], LDM [10], RANK [24], eSDC [40], SDALF [7], LMNN [32], ITML [5], Euclid [40].", "startOffset": 109, "endOffset": 113}, {"referenceID": 4, "context": "The traditional methods include LOMO-XQDA [19], KISSME [13], LDM [10], RANK [24], eSDC [40], SDALF [7], LMNN [32], ITML [5], Euclid [40].", "startOffset": 120, "endOffset": 123}, {"referenceID": 39, "context": "The traditional methods include LOMO-XQDA [19], KISSME [13], LDM [10], RANK [24], eSDC [40], SDALF [7], LMNN [32], ITML [5], Euclid [40].", "startOffset": 132, "endOffset": 136}, {"referenceID": 16, "context": "The deep learning methods include FPNN [17] and IDLA [1].", "startOffset": 39, "endOffset": 43}, {"referenceID": 0, "context": "The deep learning methods include FPNN [17] and IDLA [1].", "startOffset": 53, "endOffset": 56}, {"referenceID": 15, "context": "According to the protocol in [16], the data set is divided into a training set of 871 subjects and a test set of 100.", "startOffset": 29, "endOffset": 33}, {"referenceID": 0, "context": "We train the network on CUHK03, and further fine-tune it on CUHK01, as the same setting with the state-of-the-art method IDLA [1].", "startOffset": 126, "endOffset": 129}, {"referenceID": 41, "context": "Besides, to inspect the limitation of the data set CUHK01, we involve the recently released Market1501 [42] into the training.", "startOffset": 103, "endOffset": 107}, {"referenceID": 0, "context": "line with the fact [1] that the learned filters in network mainly focus on image colors (as shown in Fig.", "startOffset": 19, "endOffset": 22}, {"referenceID": 8, "context": "The VIPeR [9] data set includes 632 subjects, each of which has 2 images from two different cameras.", "startOffset": 10, "endOffset": 13}, {"referenceID": 0, "context": "We compare our model with IDLA [1], DeepFeature [6], visual word (visWord) [37], saliency matching (SalMatch), patch matching (PatMatch) [39], ELF [8], PRSVM [3], LMNNR [2], eBiCov [21], local Fisher discriminant analysis (LF) [28], PRDC [43], aPRDC [20], PCCA [25], mid-level filters (mFilter) [41] and the fusion of mFilter and LADF [18].", "startOffset": 31, "endOffset": 34}, {"referenceID": 5, "context": "We compare our model with IDLA [1], DeepFeature [6], visual word (visWord) [37], saliency matching (SalMatch), patch matching (PatMatch) [39], ELF [8], PRSVM [3], LMNNR [2], eBiCov [21], local Fisher discriminant analysis (LF) [28], PRDC [43], aPRDC [20], PCCA [25], mid-level filters (mFilter) [41] and the fusion of mFilter and LADF [18].", "startOffset": 48, "endOffset": 51}, {"referenceID": 36, "context": "We compare our model with IDLA [1], DeepFeature [6], visual word (visWord) [37], saliency matching (SalMatch), patch matching (PatMatch) [39], ELF [8], PRSVM [3], LMNNR [2], eBiCov [21], local Fisher discriminant analysis (LF) [28], PRDC [43], aPRDC [20], PCCA [25], mid-level filters (mFilter) [41] and the fusion of mFilter and LADF [18].", "startOffset": 75, "endOffset": 79}, {"referenceID": 38, "context": "We compare our model with IDLA [1], DeepFeature [6], visual word (visWord) [37], saliency matching (SalMatch), patch matching (PatMatch) [39], ELF [8], PRSVM [3], LMNNR [2], eBiCov [21], local Fisher discriminant analysis (LF) [28], PRDC [43], aPRDC [20], PCCA [25], mid-level filters (mFilter) [41] and the fusion of mFilter and LADF [18].", "startOffset": 137, "endOffset": 141}, {"referenceID": 7, "context": "We compare our model with IDLA [1], DeepFeature [6], visual word (visWord) [37], saliency matching (SalMatch), patch matching (PatMatch) [39], ELF [8], PRSVM [3], LMNNR [2], eBiCov [21], local Fisher discriminant analysis (LF) [28], PRDC [43], aPRDC [20], PCCA [25], mid-level filters (mFilter) [41] and the fusion of mFilter and LADF [18].", "startOffset": 147, "endOffset": 150}, {"referenceID": 2, "context": "We compare our model with IDLA [1], DeepFeature [6], visual word (visWord) [37], saliency matching (SalMatch), patch matching (PatMatch) [39], ELF [8], PRSVM [3], LMNNR [2], eBiCov [21], local Fisher discriminant analysis (LF) [28], PRDC [43], aPRDC [20], PCCA [25], mid-level filters (mFilter) [41] and the fusion of mFilter and LADF [18].", "startOffset": 158, "endOffset": 161}, {"referenceID": 1, "context": "We compare our model with IDLA [1], DeepFeature [6], visual word (visWord) [37], saliency matching (SalMatch), patch matching (PatMatch) [39], ELF [8], PRSVM [3], LMNNR [2], eBiCov [21], local Fisher discriminant analysis (LF) [28], PRDC [43], aPRDC [20], PCCA [25], mid-level filters (mFilter) [41] and the fusion of mFilter and LADF [18].", "startOffset": 169, "endOffset": 172}, {"referenceID": 20, "context": "We compare our model with IDLA [1], DeepFeature [6], visual word (visWord) [37], saliency matching (SalMatch), patch matching (PatMatch) [39], ELF [8], PRSVM [3], LMNNR [2], eBiCov [21], local Fisher discriminant analysis (LF) [28], PRDC [43], aPRDC [20], PCCA [25], mid-level filters (mFilter) [41] and the fusion of mFilter and LADF [18].", "startOffset": 181, "endOffset": 185}, {"referenceID": 27, "context": "We compare our model with IDLA [1], DeepFeature [6], visual word (visWord) [37], saliency matching (SalMatch), patch matching (PatMatch) [39], ELF [8], PRSVM [3], LMNNR [2], eBiCov [21], local Fisher discriminant analysis (LF) [28], PRDC [43], aPRDC [20], PCCA [25], mid-level filters (mFilter) [41] and the fusion of mFilter and LADF [18].", "startOffset": 227, "endOffset": 231}, {"referenceID": 42, "context": "We compare our model with IDLA [1], DeepFeature [6], visual word (visWord) [37], saliency matching (SalMatch), patch matching (PatMatch) [39], ELF [8], PRSVM [3], LMNNR [2], eBiCov [21], local Fisher discriminant analysis (LF) [28], PRDC [43], aPRDC [20], PCCA [25], mid-level filters (mFilter) [41] and the fusion of mFilter and LADF [18].", "startOffset": 238, "endOffset": 242}, {"referenceID": 19, "context": "We compare our model with IDLA [1], DeepFeature [6], visual word (visWord) [37], saliency matching (SalMatch), patch matching (PatMatch) [39], ELF [8], PRSVM [3], LMNNR [2], eBiCov [21], local Fisher discriminant analysis (LF) [28], PRDC [43], aPRDC [20], PCCA [25], mid-level filters (mFilter) [41] and the fusion of mFilter and LADF [18].", "startOffset": 250, "endOffset": 254}, {"referenceID": 24, "context": "We compare our model with IDLA [1], DeepFeature [6], visual word (visWord) [37], saliency matching (SalMatch), patch matching (PatMatch) [39], ELF [8], PRSVM [3], LMNNR [2], eBiCov [21], local Fisher discriminant analysis (LF) [28], PRDC [43], aPRDC [20], PCCA [25], mid-level filters (mFilter) [41] and the fusion of mFilter and LADF [18].", "startOffset": 261, "endOffset": 265}, {"referenceID": 40, "context": "We compare our model with IDLA [1], DeepFeature [6], visual word (visWord) [37], saliency matching (SalMatch), patch matching (PatMatch) [39], ELF [8], PRSVM [3], LMNNR [2], eBiCov [21], local Fisher discriminant analysis (LF) [28], PRDC [43], aPRDC [20], PCCA [25], mid-level filters (mFilter) [41] and the fusion of mFilter and LADF [18].", "startOffset": 295, "endOffset": 299}, {"referenceID": 17, "context": "We compare our model with IDLA [1], DeepFeature [6], visual word (visWord) [37], saliency matching (SalMatch), patch matching (PatMatch) [39], ELF [8], PRSVM [3], LMNNR [2], eBiCov [21], local Fisher discriminant analysis (LF) [28], PRDC [43], aPRDC [20], PCCA [25], mid-level filters (mFilter) [41] and the fusion of mFilter and LADF [18].", "startOffset": 335, "endOffset": 339}, {"referenceID": 17, "context": "39%) is obtained by a combination of two methods (mFilter+LADF) [18].", "startOffset": 64, "endOffset": 68}, {"referenceID": 5, "context": "The identification rate by DeepFeature [6] is close to ours at rank 1, but much lower at higher ranks.", "startOffset": 39, "endOffset": 42}], "year": 2016, "abstractText": "Person re-identification is challenging due to the large variations of pose, illumination, occlusion and camera view. Owing to these variations, the pedestrian data is distributed as highly-curved manifolds in the feature space, despite the current convolutional neural networks (CNN)\u2019s capability of feature extraction. However, the distribution is unknown, so it is difficult to use the geodesic distance when comparing two samples. In practice, the current deep embedding methods use the Euclidean distance for the training and test. On the other hand, the manifold learning methods suggest to use the Euclidean distance in the local range, combining with the graphical relationship between samples, for approximating the geodesic distance. From this point of view, selecting suitable positive (i.e. intra-class) training samples within a local range is critical for training the CNN embedding, especially when the data has large intra-class variations. In this paper, we propose a novel moderate positive sample mining method to train robust CNN for person re-identification, dealing with the problem of large variation. In addition, we improve the learning by a metric weight constraint, so that the learned metric has a better generalization ability. Experiments show that these two strategies are effective in learning robust deep metrics for person re-identification, and accordingly our deep model significantly outperforms the state-of-the-art methods on several benchmarks of person re-identification. Therefore, the study presented in this paper may be useful in inspiring new designs of deep models for person re-identification.", "creator": "LaTeX with hyperref package"}}}