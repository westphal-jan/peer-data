{"id": "1611.00144", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Nov-2016", "title": "Product-based Neural Networks for User Response Prediction", "abstract": "Predicting user responses, such as clicks and conversions, is of great importance and has found its usage in many Web applications including recommender systems, web search and online advertising. The data in those applications is mostly categorical and contains multiple fields; a typical representation is to transform it into a high-dimensional sparse binary feature representation via one-hot encoding. Facing with the extreme sparsity, traditional models may limit their capacity of mining shallow patterns from the data, i.e. low-order feature combinations. Deep models like deep neural networks, on the other hand, cannot be directly applied for the high-dimensional input because of the huge feature space. In this paper, we propose a Product-based Neural Networks (PNN) with an embedding layer to learn a distributed representation of the categorical data, a product layer to capture interactive patterns between inter-field categories, and further fully connected layers to explore high-order feature interactions. Our experimental results on two large-scale real-world ad click datasets demonstrate that PNNs consistently outperform the state-of-the-art models on various metrics.", "histories": [["v1", "Tue, 1 Nov 2016 07:10:22 GMT  (1578kb,D)", "http://arxiv.org/abs/1611.00144v1", "6 pages, 5 figures, ICDM2016"]], "COMMENTS": "6 pages, 5 figures, ICDM2016", "reviews": [], "SUBJECTS": "cs.LG cs.IR", "authors": ["yanru qu", "han cai", "kan ren", "weinan zhang", "yong yu", "ying wen", "jun wang"], "accepted": false, "id": "1611.00144"}, "pdf": {"name": "1611.00144.pdf", "metadata": {"source": "CRF", "title": "Product-based Neural Networks for User Response Prediction", "authors": ["Yanru Qu", "Han Cai", "Kan Ren", "Weinan Zhang", "Yong Yu", "Ying Wen", "Jun Wang"], "emails": ["yyu}@apex.sjtu.edu.cn", "j.wang}@cs.ucl.ac.uk"], "sections": [{"heading": null, "text": "In this case, it is a real distraction, capable of confining itself to a real distraction."}, {"heading": "II. RELATED WORK", "text": "The response prediction problem is usually formulated as a binary classification problem where the probability of prediction or cross entropy as a training target has not been taken into account [14]. The ranges under ROC curve (AUC) and Relative Information Gain (RIG) are common yardsticks for evaluating response behavior in Xiv: 161 1.00 144v 1 [cs.L G] 1N ov2 016 [15]. From the modeling perspective, linear logistic regression (LR) [5], [16] and nonlinear gradients to increase decision trees (GBDT) [4] and factoring machines (FM) [6] are widely used in industrial applications. However, these models are limited in obtaining latent patterns or displaying learning quality characteristics. Deep learning is capable of researching high-order latent patterns, as well as generalizing, expressive data representations in the representation of DNN."}, {"heading": "III. DEEP LEARNING FOR CTR ESTIMATION", "text": "We use the CTR estimate in online advertising [14] as a working example to fully formulate our model and explore performance using various metrics, the task being to build a predictive model to estimate the likelihood that a user will click on a particular ad in a particular context. Each sample of data consists of several fields of categorical data such as user information (city, hour, etc.), publisher information (domain, ad slot, etc.) and ad information (creative ID, campaign ID, etc.) [19]. All information is presented as a categorical vector in multiple fields, with each field (e.g. city) encoded as in Section I. Such field-by-coded representation leads to a curse of dimensionality and enormous sparseness [12]. In addition, there are local dependencies and hierarchical structures between the fields [1]."}, {"heading": "A. Product-based Neural Network", "text": "The ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref for the ref"}, {"heading": "C. Outer Product-based Neural Network", "text": "In contrast, the outer product of the vector takes a pair of vectors and creates a matrix. IPNN defines the interaction of features by an inner vector product, while in this section we discuss the outer product-based Neural Network (OPNN). The only difference between IPNN and OPNN is the quadratic term p. In OPNN, we define the interaction of features as g (fi, fj) = fif T j. Thus, for each element in p, pi, j, j, and RM \u00b7 M is a quadratic matrix. To calculate l1, the spatial complexity is O (D1M2N2), and the time complexity is also O (D1M2N2). Remember that D1 and M are the hyperparameters of the network architecture, and N is the number of input fields. To reduce complexity, we can use the great idea of s \u00b7 s \u00b7 \u00b7 s \u00b7 s \u00b7 s \u00b7 s \u00b7 s \u00b7 s \u00b7 s \u00b7 s \u00b7 s \u00b7 s \u00b7 s \u00b7 s \u00b7 \u00b7 s \u00b7 m \u00b7 m \u00b7 m \u00b7 m \u00b7 m \u00b7 m \u00b7 m \u00b7 m \u00b7 m \u00b7 m \u00b7 m \u00b7 m \u00b7 m \u00b7 m \u00b7 s \u00b7 m \u00b7 m \u00b7 1 \u00b7 m \u00b7 s \u00b7 m \u00b7 m \u00b7 m \u00b7 m \u00b7 m \u00b7 m \u00b7 m \u00b7 s \u00b7 m \u00b7 m \u00b7 m \u00b7 m \u00b7 s \u00b7 m \u00b7 m \u00b7 m \u00b7 m \u00b7 m \u00b7 s \u00b7 m \u00b7 m \u00b7 m \u00b7 m \u00b7 m \u00b7 s \u00b7 m \u00b7 m \u00b7 m \u00b7 m \u00b7 m \u00b7 m \u00b7 s \u00b7 m \u00b7 m \u00b7 m \u00b7 s \u00b7 m \u00b7 s \u00b7 m \u00b7 m \u00b7 m \u00b7 s \u00b7 m \u00b7 m \u00b7 m \u00b7 s \u00b7 m \u00b7 s \u00b7 m \u00b7 m \u00b7 m \u00b7 m \u00b7 m \u00b7 m, in this section, the vector takes a vector pair of vector and creates a matrix."}, {"heading": "D. Discussions", "text": "Compared to FNN [12], PNN has a product level. If you remove a part of the product level, PNN is identical to FNN. PNN is quite similar to FM with the internal product engineer [20]: If there is no hidden level and the output level is simply combined with a uniform weight, PNN is identical to FM. Inspired by Net2Net [21], we can first train a part of PNN (e.g. the FNN or FM part) as an initialization and then start letting the retransmission go across the entire network. The result PNN should be at least as good as FNN or FM. In general, PNN uses product layers to examine the interactions between the characteristics. Vector products can be considered as a series of additional / multiplicatory operations. Inner product and outer product are just two implementations. In fact, we can define more general or more complicated product layers in order to better use PNN as an additional product to be used for interaction."}, {"heading": "IV. EXPERIMENTS", "text": "In this section we present our experiments in detail, including data sets, data processing, experiment setup, model comparison and associated analyses1. In our experiments, PNN models outperform important current models in the CTR estimation task on two real data sets."}, {"heading": "A. Datasets", "text": "1) Criteo: Criteo 1TB click log2 is a famous benchmarking dataset in the signage industry. We select 7 consecutive days of rehearsals for the training and the next 1 day for evaluation. Due to the huge volume of data and the high bias, we apply negative down sampling data to this dataset. We define the down sampling ratio as w, the predicted CTR as p, the recalibrated CTR q should be q = p / (p + 1 \u2212 pw) [4]. After down sampling and feature mapping, we get a dataset that includes 79.38M instances with 1.64M feature dimensions. 2) iPinYou: The iPinYou dataset3 is another real download dataset for ad click logs over 10 days. After a hot encoding, we get a dataset that includes 19.50M instances with 1.64M feature dimensions, the Pindataset3 is another pindataset over 10 days."}, {"heading": "B. Model Comparison", "text": "We compare 7 models in our experiments implemented with TensorFlow4 and trained with Stochastic Gradient Descent (SGD).LR: LR is the most widely used linear model in industrial applications [22]. It is easy to implement and quick to learn, but not able to capture nonlinear information.FM: FM has many successful applications in recommendation systems and prediction tasks for users [20]. FM explores interactions between functions that are effective in sparse data. FNN: FNN is proposed in [12] because it can capture latent patterns of categorical data in high order. CCPM: CCPM: CCPM: CCPM is a revolutionary model for click prediction [13]. This model efficiently learns local-global characteristics. However, CCPM is highly dependent on the alignment of characteristics and lacks interpretation. IPNN: PNN: PNN: PNN: PNN: PNN: PNN: PNN: PNN: PNN has a product layer to the product layer PNN C. This product layer * has a product layer to the product layer PNN III, and a product layer to the product layer PNN * a product layer to the product layer."}, {"heading": "C. Evaluation Metrics", "text": "Four parameters are tested in our experiments: AUC: Area under ROC curve is a widely used parameter for assessing classification problems. Some studies confirm AUC as a good parameter for the CTR estimate [15].RIG: Relative Information Gain, RIG = 1 \u2212 NE, where NE is the Normalized Cross Entropy [4]. We also use Log Loss (Eq. (9)) and root mean square error (RMSE) as additional parameters."}, {"heading": "D. Performance Comparison", "text": "This year, the number of new registrations rose by 0.2 percent compared to the previous year, while the number of new registrations rose by 0.2 percent compared to the previous year."}, {"heading": "E. Ablation Study on Network Architecture", "text": "In this section we will discuss the effects of the neural network architecture. For IPNN and OPNN we will consider three hyperparameters (or settings): (i) embedding layer size, (ii) net depth and (iii) activation effects with other neural networks and PNN * is just a combination of IPNN and OPNN, we compare FNN and OPNN in this context.) Embedding layer is to be converted to dense real vectors."}, {"heading": "V. CONCLUSION AND FUTURE WORK", "text": "In this paper, we proposed an in-depth neural network model with a novel architecture, namely product-based neural network, to improve the predictive power of DNN when working with categorical data. And, we chose CTR estimation as a working example. By researching trait interactions, PNN promises to learn latent high-order patterns on categorical multi-field data. We designed two types of PNN: IPNN based on the inner product and OPNN based on the outer product. We also discussed solutions to reduce complexity, making PNN efficient and scalable. (ii) Our experimental results showed that PNN outperforms the other state-of-the-art models in 4 metrics on 2 datasets. In summary, we arrive at the following conclusions: (i) By studying trait interactions, PNN gains better capacities on categorical data from multiple fields. (ii) Since PNN is both effective at meeting the most important characteristics of PNN and iN, iii is effective at identifying the PNN."}], "references": [{"title": "Response prediction using collaborative filtering with hierarchies and side-information", "author": ["A.K. Menon", "K.-P. Chitrapura", "S. Garg"], "venue": "SIGKDD. ACM, 2011, pp. 141\u2013149.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2011}, {"title": "Optimizing web search using web click-through data", "author": ["G.-R. Xue", "H.-J. Zeng", "Z. Chen", "Y. Yu", "W.-Y. Ma", "W. Xi", "W. Fan"], "venue": "CIKM, 2004.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2004}, {"title": "Optimal real-time bidding for display advertising", "author": ["W. Zhang", "S. Yuan", "J. Wang"], "venue": "SIGKDD. ACM, 2014, pp. 1077\u20131086.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2014}, {"title": "Practical lessons from predicting clicks on ads at facebook", "author": ["X. He", "J. Pan", "O. Jin"], "venue": "Proceedings of the Eighth International Workshop on Data Mining for Online Advertising. ACM, 2014, pp. 1\u20139.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2014}, {"title": "Estimating conversion rate in display advertising from past erformance data", "author": ["K.-c. Lee", "B. Orten", "A. Dasdan"], "venue": "SIGKDD. ACM, 2012, pp. 768\u2013776.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2012}, {"title": "Factorization machines with follow-the-regularized-leader for ctr prediction in display advertising", "author": ["A.-P. Ta"], "venue": "IEEE BigData. IEEE, 2015, pp. 2889\u20132891.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2015}, {"title": "Bid landscape forecasting in online ad exchange marketplace", "author": ["Y. Cui", "R. Zhang", "W. Li"], "venue": "SIGKDD. ACM, 2011, pp. 265\u2013273.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2011}, {"title": "Deep learning", "author": ["Y. LeCun", "Y. Bengio", "G. Hinton"], "venue": "Nature, 2015.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "NIPS, 2012, pp. 1097\u2013 1105.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2012}, {"title": "Speech recognition with deep recurrent neural networks", "author": ["A. Graves", "A.-r. Mohamed", "G. Hinton"], "venue": "ICASSP. IEEE, 2013, pp. 6645\u2013 6649.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["T. Mikolov", "I. Sutskever", "K. Chen"], "venue": "NIPS, 2013, pp. 3111\u20133119.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2013}, {"title": "Deep learning over multi-field categorical data: A case study on user response prediction", "author": ["W. Zhang", "T. Du", "J. Wang"], "venue": "ECIR, 2016.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2016}, {"title": "A convolutional click prediction model", "author": ["Q. Liu", "F. Yu", "S. Wu"], "venue": "CIKM. ACM, 2015, pp. 1743\u20131746.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "Predicting clicks: estimating the click-through rate for new ads", "author": ["M. Richardson", "E. Dominowska", "R. Ragno"], "venue": "WWW. ACM, 2007, pp. 521\u2013530.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2007}, {"title": "Web-scale bayesian clickthrough rate prediction for sponsored search advertising in microsoft\u2019s bing search engine", "author": ["T. Graepel", "J.Q. Candela", "T. Borchert"], "venue": "ICML, 2010, pp. 13\u201320.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2010}, {"title": "User response learning for directly optimizing campaign performance in display advertising", "author": ["K. Ren", "W. Zhang", "Y. Rong", "H. Zhang", "Y. Yu", "J. Wang"], "venue": "CIKM, 2016.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2016}, {"title": "Sequential click prediction for sponsored search with recurrent neural networks", "author": ["Y. Zhang", "H. Dai", "C. Xu"], "venue": "arXiv preprint arXiv:1404.5772, 2014.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2014}, {"title": "Training product unit neural networks", "author": ["A.P. Engelbrecht", "A. Engelbrecht", "A. Ismail"], "venue": "1999.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1999}, {"title": "Real-time bidding benchmarking with ipinyou dataset", "author": ["W. Zhang", "S. Yuan", "J. Wang"], "venue": "arXiv:1407.7073, 2014.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2014}, {"title": "Factorization machines", "author": ["S. Rendle"], "venue": "ICDM. IEEE, 2010, pp. 995\u2013 1000.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2010}, {"title": "Net2net: Accelerating learning via knowledge transfer", "author": ["T. Chen", "I. Goodfellow", "J. Shlens"], "venue": "ICLR, 2016.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2016}, {"title": "Ad click prediction: a view from the trenches", "author": ["H.B. McMahan", "G. Holt", "D. Sculley"], "venue": "SIGKDD. ACM, 2013, pp. 1222\u20131230.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2013}], "referenceMentions": [{"referenceID": 0, "context": ", in a given context [1].", "startOffset": 21, "endOffset": 24}, {"referenceID": 1, "context": "This predicted probability indicates the user\u2019s interest on the specific item such as a news article, a commercial item or an advertising post, which influences the subsequent decision making such as document ranking [2] and ad bidding [3].", "startOffset": 217, "endOffset": 220}, {"referenceID": 2, "context": "This predicted probability indicates the user\u2019s interest on the specific item such as a news article, a commercial item or an advertising post, which influences the subsequent decision making such as document ranking [2] and ad bidding [3].", "startOffset": 236, "endOffset": 239}, {"referenceID": 3, "context": "The data collection in these IR tasks is mostly in a multifield categorical form, for example, [Weekday=Tuesday, Gender=Male, City=London], which is normally transformed into high-dimensional sparse binary features via onehot encoding [4].", "startOffset": 235, "endOffset": 238}, {"referenceID": 0, "context": "[0, 1, 0, 0, 0, 0, 0] } {{ }", "startOffset": 0, "endOffset": 21}, {"referenceID": 0, "context": "Weekday=Tuesday [0, 1] }{{}", "startOffset": 16, "endOffset": 22}, {"referenceID": 4, "context": "Many machine learning models, including linear logistic regression [5], non-linear gradient boosting decision trees [4] and factorization machines [6], have been proposed to work on such high-dimensional sparse binary features and produce high quality user response predictions.", "startOffset": 67, "endOffset": 70}, {"referenceID": 3, "context": "Many machine learning models, including linear logistic regression [5], non-linear gradient boosting decision trees [4] and factorization machines [6], have been proposed to work on such high-dimensional sparse binary features and produce high quality user response predictions.", "startOffset": 116, "endOffset": 119}, {"referenceID": 5, "context": "Many machine learning models, including linear logistic regression [5], non-linear gradient boosting decision trees [4] and factorization machines [6], have been proposed to work on such high-dimensional sparse binary features and produce high quality user response predictions.", "startOffset": 147, "endOffset": 150}, {"referenceID": 6, "context": "However, these models highly depend on feature engineering in order to capture highorder latent patterns [7].", "startOffset": 105, "endOffset": 108}, {"referenceID": 7, "context": "Recently, deep neural networks (DNNs) [8] have shown great capability in classification and regression tasks, including computer vision [9], speech recognition [10] and natural language processing [11].", "startOffset": 38, "endOffset": 41}, {"referenceID": 8, "context": "Recently, deep neural networks (DNNs) [8] have shown great capability in classification and regression tasks, including computer vision [9], speech recognition [10] and natural language processing [11].", "startOffset": 136, "endOffset": 139}, {"referenceID": 9, "context": "Recently, deep neural networks (DNNs) [8] have shown great capability in classification and regression tasks, including computer vision [9], speech recognition [10] and natural language processing [11].", "startOffset": 160, "endOffset": 164}, {"referenceID": 10, "context": "Recently, deep neural networks (DNNs) [8] have shown great capability in classification and regression tasks, including computer vision [9], speech recognition [10] and natural language processing [11].", "startOffset": 197, "endOffset": 201}, {"referenceID": 11, "context": "In order to improve the multi-field categorical data interaction, [12] presented an embedding methodology based on pre-training of a factorization machine.", "startOffset": 66, "endOffset": 70}, {"referenceID": 0, "context": "Previous work [1], [6] has shown that local dependencies between features from different fields can be effectively explored by feature vector \u201cproduct\u201d operations instead of \u201cadd\u201d operations.", "startOffset": 14, "endOffset": 17}, {"referenceID": 5, "context": "Previous work [1], [6] has shown that local dependencies between features from different fields can be effectively explored by feature vector \u201cproduct\u201d operations instead of \u201cadd\u201d operations.", "startOffset": 19, "endOffset": 22}, {"referenceID": 11, "context": "To utilize the learning ability of neural networks and mine the latent patterns of data in a more effective way than MLPs, in this paper we propose Product-based Neural Network (PNN) which (i) starts from an embedding layer without pretraining as used in [12], and (ii) builds a product layer based on the embedded feature vectors to model the inter-field feature interactions, and (iii) further distills the high-order feature patterns with fully connected MLPs.", "startOffset": 255, "endOffset": 259}, {"referenceID": 5, "context": "The extensive experimental results on two large-scale realworld datasets demonstrate the consistent superiority of our model over state-of-the-art user response prediction models [6], [13], [12] on various metrics.", "startOffset": 179, "endOffset": 182}, {"referenceID": 12, "context": "The extensive experimental results on two large-scale realworld datasets demonstrate the consistent superiority of our model over state-of-the-art user response prediction models [6], [13], [12] on various metrics.", "startOffset": 184, "endOffset": 188}, {"referenceID": 11, "context": "The extensive experimental results on two large-scale realworld datasets demonstrate the consistent superiority of our model over state-of-the-art user response prediction models [6], [13], [12] on various metrics.", "startOffset": 190, "endOffset": 194}, {"referenceID": 13, "context": "The response prediction problem is normally formulated as a binary classification problem with prediction likelihood or cross entropy as the training objective [14].", "startOffset": 160, "endOffset": 164}, {"referenceID": 14, "context": "[15].", "startOffset": 0, "endOffset": 4}, {"referenceID": 4, "context": "From the modeling perspective, linear logistic regression (LR) [5], [16] and non-linear gradient boosting decision trees (GBDT) [4] and factorization machines (FM) [6] are widely used in industrial applications.", "startOffset": 63, "endOffset": 66}, {"referenceID": 15, "context": "From the modeling perspective, linear logistic regression (LR) [5], [16] and non-linear gradient boosting decision trees (GBDT) [4] and factorization machines (FM) [6] are widely used in industrial applications.", "startOffset": 68, "endOffset": 72}, {"referenceID": 3, "context": "From the modeling perspective, linear logistic regression (LR) [5], [16] and non-linear gradient boosting decision trees (GBDT) [4] and factorization machines (FM) [6] are widely used in industrial applications.", "startOffset": 128, "endOffset": 131}, {"referenceID": 5, "context": "From the modeling perspective, linear logistic regression (LR) [5], [16] and non-linear gradient boosting decision trees (GBDT) [4] and factorization machines (FM) [6] are widely used in industrial applications.", "startOffset": 164, "endOffset": 167}, {"referenceID": 10, "context": "Deep learning is able to explore high-order latent patterns as well as generalizing expressive data representations [11].", "startOffset": 116, "endOffset": 120}, {"referenceID": 11, "context": "Factorization-machine supported neural networks (FNN) was proposed in [12] to learn embedding vectors of categorical data via pre-trained FM.", "startOffset": 70, "endOffset": 74}, {"referenceID": 12, "context": "Convolutional Click Prediction Model (CCPM) was proposed in [13] to predict ad click by convolutional neural networks (CNN).", "startOffset": 60, "endOffset": 64}, {"referenceID": 16, "context": "Recurrent neural networks (RNN) was leveraged to model the user queries as a series of user context to predict the ad click behavior [17].", "startOffset": 133, "endOffset": 137}, {"referenceID": 17, "context": "Product unit neural network (PUNN) [18] was proposed to build high-order combinations of the inputs.", "startOffset": 35, "endOffset": 39}, {"referenceID": 13, "context": "We take CTR estimation in online advertising [14] as a working example to formulate our model and explore the performance on various metrics.", "startOffset": 45, "endOffset": 49}, {"referenceID": 18, "context": ") [19].", "startOffset": 2, "endOffset": 6}, {"referenceID": 11, "context": "Such a field-wise one-hot encoding representation results in curse of dimensionality and enormous sparsity [12].", "startOffset": 107, "endOffset": 111}, {"referenceID": 0, "context": "Besides, there exist local dependencies and hierarchical structures among fields [1].", "startOffset": 81, "endOffset": 84}, {"referenceID": 19, "context": "In FM, feature interaction is defined as the inner product of two feature vectors [20].", "startOffset": 82, "endOffset": 86}, {"referenceID": 19, "context": "Inspired by FM [20], we come up with the idea of matrix factorization to reduce complexity.", "startOffset": 15, "endOffset": 19}, {"referenceID": 11, "context": "Compared with FNN [12], PNN has a product layer.", "startOffset": 18, "endOffset": 22}, {"referenceID": 19, "context": "With the inner product operator, PNN is quite similar with FM [20]: if there is no hidden layer and the output layer is simply summing up with uniform weight, PNN is identical to FM.", "startOffset": 62, "endOffset": 66}, {"referenceID": 20, "context": "Inspired by Net2Net [21], we can firstly train a part of PNN (e.", "startOffset": 20, "endOffset": 24}, {"referenceID": 3, "context": "Define the down-sampling ratio as w, the predicted CTR as p, the recalibrated CTR q should be q = p/(p + 1\u2212p w ) [4].", "startOffset": 113, "endOffset": 116}, {"referenceID": 21, "context": "LR: LR is the most widely used linear model in industrial applications [22].", "startOffset": 71, "endOffset": 75}, {"referenceID": 19, "context": "FM: FM has many successful applications in recommender systems and user response prediction tasks [20].", "startOffset": 98, "endOffset": 102}, {"referenceID": 11, "context": "FNN: FNN is proposed in [12], being able to capture highorder latent patterns of multi-field categorical data.", "startOffset": 24, "endOffset": 28}, {"referenceID": 12, "context": "CCPM: CCPM is a convolutional model for click prediction [13].", "startOffset": 57, "endOffset": 61}, {"referenceID": 14, "context": "Besides, some work validates AUC as a good measurement in CTR estimation [15].", "startOffset": 73, "endOffset": 77}, {"referenceID": 3, "context": "RIG: Relative Information Gain, RIG = 1 \u2212 NE, where NE is the Normalized Cross Entropy [4].", "startOffset": 87, "endOffset": 90}, {"referenceID": 10, "context": "Take word embedding as an example [11], an embedding vector contains the information of the word and its context, and indicates the relationships between words.", "startOffset": 34, "endOffset": 38}, {"referenceID": 11, "context": "We take the idea of embedding layer from [12].", "startOffset": 41, "endOffset": 45}, {"referenceID": 11, "context": "This is supported by [12].", "startOffset": 21, "endOffset": 25}], "year": 2016, "abstractText": "Predicting user responses, such as clicks and conversions, is of great importance and has found its usage in many Web applications including recommender systems, web search and online advertising. The data in those applications is mostly categorical and contains multiple fields; a typical representation is to transform it into a high-dimensional sparse binary feature representation via one-hot encoding. Facing with the extreme sparsity, traditional models may limit their capacity of mining shallow patterns from the data, i.e. low-order feature combinations. Deep models like deep neural networks, on the other hand, cannot be directly applied for the high-dimensional input because of the huge feature space. In this paper, we propose a Product-based Neural Networks (PNN) with an embedding layer to learn a distributed representation of the categorical data, a product layer to capture interactive patterns between interfield categories, and further fully connected layers to explore high-order feature interactions. Our experimental results on two large-scale real-world ad click datasets demonstrate that PNNs consistently outperform the state-of-the-art models on various metrics.", "creator": "LaTeX with hyperref package"}}}