{"id": "1611.00183", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Nov-2016", "title": "Local Subspace-Based Outlier Detection using Global Neighbourhoods", "abstract": "Outlier detection in high-dimensional data is a challenging yet important task, as it has applications in, e.g., fraud detection and quality control. State-of-the-art density-based algorithms perform well because they 1) take the local neighbourhoods of data points into account and 2) consider feature subspaces. In highly complex and high-dimensional data, however, existing methods are likely to overlook important outliers because they do not explicitly take into account that the data is often a mixture distribution of multiple components.", "histories": [["v1", "Tue, 1 Nov 2016 11:22:26 GMT  (1090kb,D)", "http://arxiv.org/abs/1611.00183v1", "Short version accepted at IEEE BigData 2016"]], "COMMENTS": "Short version accepted at IEEE BigData 2016", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["bas van stein", "matthijs van leeuwen", "thomas b\\\"ack"], "accepted": false, "id": "1611.00183"}, "pdf": {"name": "1611.00183.pdf", "metadata": {"source": "CRF", "title": "Local Subspace-Based Outlier Detection using Global Neighbourhoods", "authors": ["Bas van Stein", "Matthijs van Leeuwen"], "emails": ["b.van.stein@liacs.leidenuniv.nl", "m.van.leeuwen@liacs.leidenuniv.nl", "t.h.w.baeck@liacs.leidenuniv.nl"], "sections": [{"heading": null, "text": "So it is not as if this is a real starting point that is being overlooked by existing methods, and it confirms that the global perspective should be kept in mind even when spotting local outliers. It may be that it will be of interest to automatically detect suspicious network events; in a factory that identifies raw materials or products with very different characteristics, it may be that in any case, a bank may be interested in detecting fraudulent transactions. In each of these applications, the data is highly dimensioned, and each data point is a potential outlier."}, {"heading": "II. RELATED WORK", "text": "Although most previous work on outlier factors has been done in statistics, there are also clustering-based [9], nearest neighbor-based [10], classification-based [11], and spectral-based [12] outlier detection algorithms. Statistical approaches can be as: distribution-based [2], where a standard distribution is used to match the data; distance-based [13], where distance to neighboring points is used to classify outliers against non-outliers; and density-based, where the density of a point group is estimated to determine an outlier score. While classification, clustering, and distribution-based algorithms aim to find global outliers by comparing the individual data (a representation of) to determine the complete datasets, distance, and density algorithms. Next, we describe the methods that are most relevant to our paper: local outlier factor (the local LOF) to introduce."}, {"heading": "III. THE PROBLEM", "text": "Many outliers (and data mining) algorithms assume - either implicitly or explicitly - that the data are individual cases stemming from an underlying distribution, i.e. that the data is considered to be data coming from a particular distribution category. In practice, however, many datasets are mixed distributions of multiple components, for example by considering a dataset D2 consisting of a mixture of two components C1 and C2, drawn by two different distributions, i.e. D2 = C1, C1 = q1 = q1, and C2 = q2. Consider, for example, a dataset D2 consisting of a mixture of two components C1 and C2, drawn by two different distributions."}, {"heading": "IV. PRELIMINARIES", "text": "In this section we will briefly describe LoOP [5] and HiCS [6], since we will build on both techniques for our own algorithm, which we will present in the next section. In addition, the main reason for choosing LoOP is that it is very similar to the known LOF method, but normalizes the outlier factors to probabilities, which greatly simplifies the interpretation. Furthermore, we use an adapted version of the HiCS algorithm to search for relevant subspaces when there are no set of candidate subspaces known in advance. LoOP [5] Given the neighborhood size k and the data point d, LoOP calculates the probability that d is an outlier. This probability is derived from a so-called default distance from d to reference points S. (d, S) = outlier subspaces (d, s) 2 | S |, (1) where dist (x, y) is the distance between x and y."}, {"heading": "V. THE GLOSS ALGORITHM", "text": "We introduce GLOSS for Global-Local Outliers in SubSpaces, an algorithm for determining local, density-based subtempo outliers in global neighborhoods, as in Problem 1. At a high level, GLOSS is calculated for each data point in the data (2-3), and then an outlier probability is calculated for each data point considered, relative to its global neighborhood (4-9). Finally, these outlier probabilities are returned as a result (10). Since the algorithm calculates an outlier probability for each combination of data point and subspace, the probabilities for each considered subspace must be summarized to classify the data points by outlier probability. As we are interested in strong outlier probabilities in each subspace, for example, for this combination of data point and subspace we will use an outlier probability that is very low for both F-points and F-points."}, {"heading": "A. Global Local Outlier Probabilities", "text": "First, we introduce the extended default distance inspired by LoOP, which contains 1) a feature subspace F and 2) a global neighborhood relationship G: \u03c3 (dF, Gd) = \u221a s-Gd-Dist (dF, sF) 2 | Gd |, (4), where dF and sF are abbreviations for \u03c0F (d) and \u03c0F (s), respectively, and Gd is the global neighborhood defined as Gd = NNk (d). Then, we define the probabilistic global local failure factor PGLOF using the target distance defined in the previous paragraph together with the extended default distance as: PGLOF\u03bb, Gd (dF) = pdist (\u03bb, dF, Gd) IEs-Gd [pdist (\u03bb, s, Gs, Gs) IEs-Gd) IEs-Gd: \u2212 1 (5) finally, a substance space failure probability F: pF, data spaces selected after GLOF, but all Gd-points (subdist)."}, {"heading": "B. Subspace Search", "text": "GLOSS can either perform a sub-space search or use a given set of relevant sub-spaces. In the latter case, the sub-space search (line 1 in algorithm 1) is skipped. By parameterizing this procedure, we allow background knowledge to be used to reduce the number of sub-spaces whenever possible, thereby avoiding an exponential search for sub-spaces and thus reducing runtime. In the manufacturing case study that we will present in section VI-D, for example, there is a natural collection of sub-spaces that can be exploited. When sub-space search is enabled, the HiCS search method is used. However, instead of testing each feature of a candidate sub-space against the remaining sub-space characteristics, GLOSS tests each sub-space characteristic against the rest of the entire feature space and emphasizes the relationship between local and global spaces. Therefore, the algorithm searches for sub-spaces that have a high contrast compared to the global feature space, since the sub-space search parameters are similar to those described in CS and HiS IV."}, {"heading": "VI. EXPERIMENTS", "text": "We evaluate GLOSS using 1) synthetic data, 2) benchmark data with implanted outliers, 3) benchmark data with the outlier class, and 4) a real data set provided by an industry partner. Source code and experimental setup are also on the Github repository 2.The second and third experiments are available in the pre-release of this paper on arXiv3. In the first experiment, we simulate an (unbalanced) boolean classification task in sub-sections VI-A, where the class names are 1) outliers and 2) no outliers. This is a very common approach to outlier detection, as objective evaluation is otherwise very difficult. Performance is quantified by 1) Area Under the Curve (AUC) of the ROC curve and 2) Runtime.We compare GLOSS with LoOP, LOF, HiCS, and LoOP, which are considered by a variant of LoOP to be outliers in every OP and 2-D."}, {"heading": "A. Synthetic Data", "text": "Setup We first develop a generative model to generate data with known outliers that meet the assumptions of our problem: the data is a mixture of samples from different distributions, and outliers have values that are sampled from a different distribution for a random subspace. Formally, the generative process generates a data set D with features F and clusters C, with each cluster c \u00b2 C assigned a random center and a variance \u03c32c. Each data point d \u00b2 D is uniformly assigned to one of the clusters, which is called C (d), and then sampled from a normal distribution with specified center and deviation: D \u00b2 D: d \u00b2 N (d), \u03c32C (d). After generating the mixed data set, outliers O are introduced by changing a random subset of features for some of the data points."}, {"heading": "B. Benchmark Data with Implanted Outliers", "text": "Setup Next, we compare GLOSS with its competitors by using a large set of well-known benchmark data from the UCI Machine Learning Repository [20]: Ann Thyroid, Arrhythmia, Glass, Diabetes, Ionosphere, Pen Digits 16, Segments, Ailerons, Pole, Waveform 5000, Mfeat Fourier, and Optdigs. Previous work usually considered the minority class an \"outlier class\" for evaluation purposes, but this would clearly not demonstrate the strengths of our approach: We assume that the data is a mixture of components (i.e. classes), and we look for outliers within these classes. We therefore use the UCI datasets as examples of realistic data and implant artificial outliers. That is, we select a random sample of 10% of the data points and transform that data into an outlier by replacing a randomly selected subspace with the values of another class."}, {"heading": "C. Benchmark Data with Minority Class as Outliers", "text": "Setup We do not expect the use of the minority class of a dataset as an outlier class to demonstrate the strengths of our approach. However, we do not want our improved algorithm to perform worse when it comes to detecting regular local outliers. Therefore, we also compare GLOSS with its competitors by using the same benchmark datasets, but with outliers defined as an \"outlier class\" by the more common method of using the minority class. Apart from that, we use the same structure and parameters as in Section VI-B. Table IV shows the average AUC values achieved over ten runs per dataset, and the results show that GLOSS is fairly on par with the state of the art, showing that our proposed is capable of detecting \"regular\" outliers as well as those that GLOSS identifies but fails to use other methods (see previous subsections)."}, {"heading": "D. Case Study: Outlier Detection for Quality Control", "text": "It is only a matter of time before it will happen, until it will happen."}, {"heading": "VII. CONCLUSIONS", "text": "Motivated by a real-world problem in the automotive industry, we introduced the generic local subpace outlier in the global neighborhood problem and GLOSS, an algorithm that solves this problem. To enable accurate detection of local subspace outliers in high-dimensional data consisting of a mixture of components, GLOSS uses selected neighborhoods in the global data space. The experiments show that GLOSS exceeds state-of-the-art algorithms in detecting local subspace outliers. Furthermore, the experiments show that GLOSS is not only able to detect local subspace outliers, but that GLOSS is on par with the current state of the art in regular outlier detection, confirming that the global perspective should also be kept in mind when performing local outliers."}], "references": [{"title": "A survey of outlier detection methodologies", "author": ["V.J. Hodge", "J. Austin"], "venue": "Artificial Intelligence Review, vol. 22, no. 2, pp. 85\u2013126, 2004.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2004}, {"title": "Outliers in statistical data", "author": ["V. Bamnett", "T. Lewis"], "venue": "Journal of the Royal Statistical Society. Series A (General), vol. 141, no. 4, 1994.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1994}, {"title": "LOF: Identifying Density-Based Local Outliers", "author": ["M.M. Breunig", "H.-P. Kriegel", "R.T. Ng", "J. Sander"], "venue": "ACM SIGMOD Record, vol. 29, no. 2, pp. 93\u2013104, jun 2000. [Online]. Available: http://portal.acm.org/citation.cfm?doid= 335191.335388", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2000}, {"title": "Loci: Fast outlier detection using the local correlation integral", "author": ["S. Papadimitriou", "H. Kitagawa", "P.B. Gibbons", "C. Faloutsos"], "venue": "Data Engineering, 2003. Proceedings. 19th International Conference on, pp. 315\u2013326, 2003.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2003}, {"title": "LoOP: local outlier probabilities", "author": ["H.-P. Kriegel", "P. Kr\u00f6ger", "E. Schubert", "A. Zimek"], "venue": "Proceedings of the 18th ACM conference on Information and knowledge management, pp. 1649\u20131652, 2009. [Online]. Available: http://doi.acm.org/10.1145/1645953.1646195", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2009}, {"title": "Hics: high contrast subspaces for density-based outlier ranking", "author": ["F. Keller", "E. M\u00fcller", "K. B\u00f6hm"], "venue": "Data Engineering (ICDE), 2012 IEEE 28th International Conference on. IEEE, 2012, pp. 1037\u20131048.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2012}, {"title": "Outlier detection in axis-parallel subspaces of high dimensional data", "author": ["H.-P. Kriegel", "P. Kr\u00f6ger", "E. Schubert", "A. Zimek"], "venue": "Advances in Knowledge Discovery and Data Mining. Springer, 2009, pp. 831\u2013838.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2009}, {"title": "Outlier detection in arbitrarily oriented subspaces", "author": ["H. Kriegel", "P. Kroger", "E. Schubert", "A. Zimek"], "venue": "Data Mining (ICDM), 2012 IEEE 12th International Conference on. IEEE, 2012, pp. 379\u2013388.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2012}, {"title": "Outlier detection using clustering methods: a data cleaning application", "author": ["A. Loureiro", "L. Torgo", "C. Soares"], "venue": "Proceedings of KDNet Symposium on Knowledge-based Systems for the Public Sector. Bonn, Germany, 2004.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2004}, {"title": "Classification based outlier detection techniques", "author": ["S. Upadhyaya", "K. Singh"], "venue": "International Journal of Computer Trends and Technology, vol. 3, no. 2, pp. 294\u2013298, 2012.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2012}, {"title": "Outlier detection for stationary time series", "author": ["K. Choy"], "venue": "Journal of Statistical Planning and Inference, vol. 99, no. 2, pp. 111\u2013127, 2001.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2001}, {"title": "Distance-based outliers: algorithms and applications", "author": ["E.M. Knorr", "R.T. Ng", "V. Tucakov"], "venue": "The VLDB Journal\u2013 The International Journal on Very Large Data Bases, vol. 8, no. 3-4, pp. 237\u2013253, 2000.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2000}, {"title": "Incremental Local Outlier Detection for Data Streams", "author": ["D. Pokrajac", "A. Lazarevic", "L.J. Latecki"], "venue": "Computational Intelligence and Data Mining, 2007. CIDM 2007. IEEE Symposium on, no. February, pp. 504\u2013515, 2007.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2007}, {"title": "Gls-sod: a generalized local statistical approach for spatial outlier detection", "author": ["F. Chen", "C.-T. Lu", "A.P. Boedihardjo"], "venue": "Proceedings of the 16th ACM SIGKDD international conference on Knowledge discovery and data mining. ACM, 2010, pp. 1069\u20131078.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2010}, {"title": "Spatial outlier detection: Random walk based approaches", "author": ["X. Liu", "C.-T. Lu", "F. Chen"], "venue": "Proceedings of the 18th SIGSPATIAL International Conference on Advances in Geographic Information Systems. ACM, 2010, pp. 370\u2013379.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2010}, {"title": "Local outlier detection reconsidered: a generalized view on locality with applications to spatial, video, and network outlier detection", "author": ["E. Schubert", "A. Zimek", "H.-P. Kriegel"], "venue": "Data Mining and Knowledge Discovery, vol. 28, no. 1, pp. 190\u2013237, 2014.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2014}, {"title": "Outlier ranking via subspace analysis in multiple views of the data", "author": ["E. Muller", "I. Assent", "P. Iglesias", "Y. Mulle", "K. Bohm"], "venue": "Data Mining (ICDM), 2012 IEEE 12th International Conference on. IEEE, 2012, pp. 529\u2013538.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2012}, {"title": "Hierarchical density estimates for data clustering, visualization, and outlier detection", "author": ["R.J.G.B. Campello", "D. Moulavi", "A. Zimek", "J. Sander"], "venue": "ACM Trans. Knowl. Discov. Data, vol. 10, no. 1, pp. 5:1\u20135:51, Jul. 2015. [Online]. Available: http://doi.acm.org/10.1145/2733381", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2015}, {"title": "UCI machine learning repository", "author": ["K. Bache", "M. Lichman"], "venue": "http://archive.ics.uci.edu/ml, 2013.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2013}, {"title": "Stamping plant 4.0\u2013basics for the application of data mining methods in manufacturing car body parts", "author": ["S. Purr", "J. Meinhardt", "A. Lipp", "A. Werner", "M. Ostermair", "B. Gl\u00fcck"], "venue": "Key Engineering Materials, vol. 639. Trans Tech Publ, 2015, pp. 21\u201330.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "Outlier detection [1] is an important task that has applications in many domains.", "startOffset": 18, "endOffset": 21}, {"referenceID": 1, "context": "Many traditional outlier detection methods [2]", "startOffset": 43, "endOffset": 46}, {"referenceID": 2, "context": "To address both this problem and the limitations of (global) outlier detection, local outlier detection methods [3]\u2013[5] have been proposed over the past few decades.", "startOffset": 112, "endOffset": 115}, {"referenceID": 4, "context": "To address both this problem and the limitations of (global) outlier detection, local outlier detection methods [3]\u2013[5] have been proposed over the past few decades.", "startOffset": 116, "endOffset": 119}, {"referenceID": 5, "context": "To further improve on this, local subspace outlier detection methods [6]\u2013[8] have been introduced.", "startOffset": 69, "endOffset": 72}, {"referenceID": 7, "context": "To further improve on this, local subspace outlier detection methods [6]\u2013[8] have been introduced.", "startOffset": 73, "endOffset": 76}, {"referenceID": 4, "context": "Following this, our second contribution is the introduction of the GLOSS algorithm, which combines our ideas on outlier detection using global neighbourhoods with techniques from LoOP [5] and HiCS [6].", "startOffset": 184, "endOffset": 187}, {"referenceID": 5, "context": "Following this, our second contribution is the introduction of the GLOSS algorithm, which combines our ideas on outlier detection using global neighbourhoods with techniques from LoOP [5] and HiCS [6].", "startOffset": 197, "endOffset": 200}, {"referenceID": 8, "context": "Although most previous work on outlier detection has been done in statistics, there are also clustering-based [9], nearest neighbour-based [10], classification-based [11] and spectral-based [12] outlier detection algorithms.", "startOffset": 110, "endOffset": 113}, {"referenceID": 9, "context": "Although most previous work on outlier detection has been done in statistics, there are also clustering-based [9], nearest neighbour-based [10], classification-based [11] and spectral-based [12] outlier detection algorithms.", "startOffset": 166, "endOffset": 170}, {"referenceID": 10, "context": "Although most previous work on outlier detection has been done in statistics, there are also clustering-based [9], nearest neighbour-based [10], classification-based [11] and spectral-based [12] outlier detection algorithms.", "startOffset": 190, "endOffset": 194}, {"referenceID": 1, "context": "Statistical approaches can be categorised as: distribution-based [2], where a standard distribution is used to fit the data; distancebased [13], where the distance to neighbouring points are used to classify outliers versus non-outliers; and densitybased, where the density of a group of points is estimated to determine an outlier score.", "startOffset": 65, "endOffset": 68}, {"referenceID": 11, "context": "Statistical approaches can be categorised as: distribution-based [2], where a standard distribution is used to fit the data; distancebased [13], where the distance to neighbouring points are used to classify outliers versus non-outliers; and densitybased, where the density of a group of points is estimated to determine an outlier score.", "startOffset": 139, "endOffset": 143}, {"referenceID": 2, "context": "Local Outlier Factor (LOF) [3] was the first algorithm to introduce the concept of local density to identify outliers.", "startOffset": 27, "endOffset": 30}, {"referenceID": 12, "context": "Quite some modifications and/or enhancements of LOF, such as the Incremental Local Outlier Factor (ILOF) [14] algorithm, have been proposed.", "startOffset": 105, "endOffset": 109}, {"referenceID": 3, "context": "Local Correlation Integral (LOCI) [4] detects outliers and groups of outliers (small clusters) using the multigranularity deviation factor (MDEF).", "startOffset": 34, "endOffset": 37}, {"referenceID": 4, "context": "Local Outlier Probabilities (LoOP) [5] is also similar to LOF but does not provide an outlier factor.", "startOffset": 35, "endOffset": 38}, {"referenceID": 6, "context": "Subspace Outlier Detection (SOD) [7] is an algorithm that", "startOffset": 33, "endOffset": 36}, {"referenceID": 7, "context": "searches for outliers in meaningful subspaces of the data space or even in arbitrarily-oriented subspaces [8].", "startOffset": 106, "endOffset": 109}, {"referenceID": 13, "context": "Other work in the area of spatial data uses special spatial attributes to define neighbourhood and usually one other attribute to find outliers that deviate in this attribute given its spatial neighbours [15], [16].", "startOffset": 204, "endOffset": 208}, {"referenceID": 14, "context": "Other work in the area of spatial data uses special spatial attributes to define neighbourhood and usually one other attribute to find outliers that deviate in this attribute given its spatial neighbours [15], [16].", "startOffset": 210, "endOffset": 214}, {"referenceID": 16, "context": "1 Outlier Ranking (OutRank) [18] determines the degree of outlierness of points using subspace analysis.", "startOffset": 28, "endOffset": 32}, {"referenceID": 5, "context": "High Contrast Subspaces(HiCS) [6] is a state-of-the-art algorithm that searches for high contrast subspaces in which to perform local outlier detection.", "startOffset": 30, "endOffset": 33}, {"referenceID": 17, "context": "Other recent work such as [19] combines density clustering with local and global outlier detection.", "startOffset": 26, "endOffset": 30}, {"referenceID": 15, "context": "1More details and a comparison of these algorithms can be found in [17].", "startOffset": 67, "endOffset": 71}, {"referenceID": 5, "context": "outlier detection algorithm such as HiCS [6].", "startOffset": 41, "endOffset": 44}, {"referenceID": 4, "context": "In this section we briefly describe LoOP [5] and HiCS [6], as we will build upon both techniques for our own algorithm, which we will introduce in the next section.", "startOffset": 41, "endOffset": 44}, {"referenceID": 5, "context": "In this section we briefly describe LoOP [5] and HiCS [6], as we will build upon both techniques for our own algorithm, which we will introduce in the next section.", "startOffset": 54, "endOffset": 57}, {"referenceID": 4, "context": "LoOP [5] Given neighbourhood size k and data point d, LoOP computes the probability that d is an outlier.", "startOffset": 5, "endOffset": 8}, {"referenceID": 5, "context": "HiCS [6] HiCS is an algorithm that performs an Apriorilike, bottom-up search for subspaces manifesting a high contrast, i.", "startOffset": 5, "endOffset": 8}, {"referenceID": 1, "context": "dimension randomly drawn from [0, 2], [0, 3], [0, 5] or [0, 10] (\u03c3c is fixed to 1).", "startOffset": 30, "endOffset": 36}, {"referenceID": 2, "context": "dimension randomly drawn from [0, 2], [0, 3], [0, 5] or [0, 10] (\u03c3c is fixed to 1).", "startOffset": 38, "endOffset": 44}, {"referenceID": 4, "context": "dimension randomly drawn from [0, 2], [0, 3], [0, 5] or [0, 10] (\u03c3c is fixed to 1).", "startOffset": 46, "endOffset": 52}, {"referenceID": 2, "context": "Results Figure 2 shows ROC curves for all algorithms per dimensionality, using 3 clusters and \u03bc drawn from [0, 3].", "startOffset": 107, "endOffset": 113}, {"referenceID": 18, "context": "Setup We next compare GLOSS to its competitors using a large set of well-known benchmark data from the UCI machine learning repository [20]: Ann Thyroid, Arrhythmia, Glass, Diabetes, Ionosphere, Pen Digits 16, Segments, Ailerons, Pol, Waveform 5000, Mfeat Fourier and Optdigits.", "startOffset": 135, "endOffset": 139}, {"referenceID": 2, "context": "ROC curves for each algorithm and per dimensionality, with \u03bc randomly drawn from [0, 3] and 2 clusters per dataset.", "startOffset": 81, "endOffset": 87}, {"referenceID": 19, "context": "The most important measurements [21], and the ones we use, are Impoc, quantifying magnetic properties of the steel, and Oil levels, quantifying the amount of oil on the coil.", "startOffset": 32, "endOffset": 36}], "year": 2016, "abstractText": "Outlier detection in high-dimensional data is a challenging yet important task, as it has applications in, e.g., fraud detection and quality control. State-of-the-art densitybased algorithms perform well because they 1) take the local neighbourhoods of data points into account and 2) consider feature subspaces. In highly complex and high-dimensional data, however, existing methods are likely to overlook important outliers because they do not explicitly take into account that the data is often a mixture distribution of multiple components. We therefore introduce GLOSS, an algorithm that performs local subspace outlier detection using global neighbourhoods. Experiments on synthetic data demonstrate that GLOSS more accurately detects local outliers in mixed data than its competitors. Moreover, experiments on real-world data show that our approach identifies relevant outliers overlooked by existing methods, confirming that one should keep an eye on the global perspective even when doing local outlier detection.", "creator": "TeX"}}}