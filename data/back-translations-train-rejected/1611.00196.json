{"id": "1611.00196", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Nov-2016", "title": "Recurrent Neural Network Language Model Adaptation Derived Document Vector", "abstract": "In many natural language processing (NLP) tasks, a document is commonly modeled as a bag of words using the term frequency-inverse document frequency (TF-IDF) vector. One major shortcoming of the frequency-based TF-IDF feature vector is that it ignores word orders that carry syntactic and semantic relationships among the words in a document, and they can be important in some NLP tasks such as genre classification. This paper proposes a novel distributed vector representation of a document: a simple recurrent-neural-network language model (RNN-LM) or a long short-term memory RNN language model (LSTM-LM) is first created from all documents in a task; some of the LM parameters are then adapted by each document, and the adapted parameters are vectorized to represent the document. The new document vectors are labeled as DV-RNN and DV-LSTM respectively. We believe that our new document vectors can capture some high-level sequential information in the documents, which other current document representations fail to capture. The new document vectors were evaluated in the genre classification of documents in three corpora: the Brown Corpus, the BNC Baby Corpus and an artificially created Penn Treebank dataset. Their classification performances are compared with the performance of TF-IDF vector and the state-of-the-art distributed memory model of paragraph vector (PV-DM). The results show that DV-LSTM significantly outperforms TF-IDF and PV-DM in most cases, and combinations of the proposed document vectors with TF-IDF or PV-DM may further improve performance.", "histories": [["v1", "Tue, 1 Nov 2016 12:14:02 GMT  (357kb,D)", "http://arxiv.org/abs/1611.00196v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["wei li", "brian kan wing mak"], "accepted": false, "id": "1611.00196"}, "pdf": {"name": "1611.00196.pdf", "metadata": {"source": "CRF", "title": "Recurrent Neural Network Language Model Adaptation Derived Document Vector", "authors": ["Wei Li", "Brian Kan Wing MAK"], "emails": ["wliax@cse.ust.hk", "mak@cse.ust.hk"], "sections": [{"heading": null, "text": "Index terms: classification of document genres, embedding of documents, recurring neural network, long-term short-term memory network"}, {"heading": "1 Introduction", "text": "In fact, it is as if most of us are able to abide by the rules that they have imposed on themselves. (...) In fact, it is as if they are able to understand the rules of the market. (...) In fact, it is as if they are able to abide by the rules of the market. (...) It is not as if they are able to abide by the rules of the market. (...) It is as if they are not abiding by the rules of the market. (...) It is as if they are abiding by the rules of the market. (...) It is as if they are abiding by the rules of the market. \"(...) It is as if they are abiding by the rules of the market. (...) It is as if they are abiding by the rules of the market. (...)"}, {"heading": "2 Recurrent Neural Network Language Modeling (RNN-LM and LSTM-LM)", "text": "The recursive language model for neural networks (RNNLM) and long-term memory (LSTM-LM) is the most advanced language method (Mikolov et al., 2010; Mikolov et al., 2011; Bengio et al., 2006). Figure 1 shows the simple RNN with an output layer factorized by classes. In this model, the input is the current word wt and the output layer is factorized by a (word) class layer. The input word is projected onto a distributed representation by the input matrix U. Recursive matrix H helps to memorize the word history. Training of the above simple RNN may suffer from the problem of exploiting or disappearing gradients (Hochreiter and Schmidhuber, 1997; Sundermeyer et al., 2012). We also propose a network structure for DV-LSTM-LSTM, based on LST100-LDVL100, which refers to the LST100 as the LST100 layer."}, {"heading": "3 Document Vectorization by RNN-LM / LSTM-LM Adaptation", "text": "From here, for simplicity's sake, we would call our approach \"adaptation\" (it can also be considered a retraining).The following is the basic vectorization procedure: Step 1: Train a parent LM using all the documents in the training corpus that serve as the initial model for adaptation. Step 2: Adapt the parent LM with every document in the training corpus1Step 3: Extract model parameters that are of interest to you from the adapted LM and vectorize them to create the DV for adaptation documentation.1Not all the model parameters in the parent LM are necessarily adapted.Only the levels shown in Figure 1, Figure 2 in green are adapted in this essay."}, {"heading": "3.1 Derivation of DV-RNN from RNN-LM adaptation", "text": "As shown in Figure 1, an RNN-LM consists of 4 weight matrices: the input projection matrix U, the recursive matrix H, the output matrix Q, and the word class matrix K. For deriving DV-RNN of a document, we select to fit only H and K matrices, leaving the matrices U and Q intact during the fit. After the fit, H and / or K are vectorized to h = vec (H) or k = vec (K), enumerating the matrix parameters column by column."}, {"heading": "3.2 Derivation of DV-LSTM from LSTM-LM adaptation", "text": "The structure of an LSTM cell is more complex than a simple RNN cell. To make a robust LSTMLM fit, we limit ourselves to adjusting the distortions in the sigmoid layer bl (in DV-L100LSTM100), the LSTM layer bm, and the class layer bc. LSTM distortions bm consist of 4 bias subvectors: forget-gate distortions bmf, input-gate distortions bmi, output-gate distortions bmc. For DV-L100-LSTM100, the distortion of all four gates is updated."}, {"heading": "4 Experimental Evaluation: Text Genre Classification", "text": "The DV-RNN and DV-L100-LSTM100 were evaluated using the generic classification of documents in three corpora: the Brown Corpus (Francis and Kucera, 1979), the Baby Dataset of the British National Corpus (BNC) (Burnard, 2003), and an artificially created dataset of Penn Treebank (Marcus et al., 1995)."}, {"heading": "4.1 Corpora used for text genre classification", "text": "The Penn Treebank Dataset (PTB) was artificially extracted from the Penn Treebank Corpus by removing the documents with the available genre tags (Webber, 2009; Plank, 2009). Because the number of files in each genre was very unbalanced, we limited the number of documents in each genre to no more than 100 and removed the errata genre (since there are very few documents in this genre). In addition, we removed short documents with less than 200 words. As a result, the dataset totaled 239 documents in 4 genres (38 highlights, 95 essays, 42 letters and 64 messages). For the Brown Corpus, the subgenres were merged under the fictional genre, bringing the total number of genres to 10 (Wu et al., 2010)."}, {"heading": "4.2 Training of the document vector DV-LSTM", "text": "The Neural Network Language Modeling Toolkit (RWTHLM) of RWTH Aachen (Sundermeyer et al., 2015; Sundermeyer et al., 2014) was used to train all LSTM LMs and to adapt them to DV-LSTM. Word classes were determined using the Brown clustering method."}, {"heading": "4.3 Training of the paragraph vector PV-DM", "text": "The paragraph vectors (PV-DMs) (Le and Mikolov, 2014) were trained for each document in a corpus using the Gensim toolkit (vRehu-Vrek and Soyka, 2010).The PV-DMs were trained with an initial learning rate of 0.025 with 20 epochs. It was found that PV-DMs of 500 dimensions consistently perform well in all three corporations; they are referred to as PV500. A tenth of the narrowly set data is randomly scanned for fine-tuning, and a grid search for the optimal hyperparameter combination is performed in each set of assignments.The combination of the hyperparameters is the permutation of each parameter value in Table 2. The respective optimal combinations of the hyperparameters are then selected for each task in order to better performance of PV-DM.Table 3 summarizes the dimensions of different characteristic vectors used for experiments."}, {"heading": "4.4 Experimental Results", "text": "This year, it has reached the point where it will be able to retaliate until it is able to retaliate."}, {"heading": "5 Conclusions and Future Works", "text": "In the current task, DV-LSTM performs well when used independently or in conjunction with TF-IDF or PV-DM, and is better than DV-RNN. Because DV-LSTM is more compact than DV-RNN and its better performance, DV-LSTM is preferred, except that DV-RNNs are faster to train. In the future, we would study its effectiveness in other NLP issues such as topic classification and mood detection. Finally, we believe that the concept of our new DV can be applied to other sequence data of different lengths (such as gene sequences and stock prices) by treating the sequence data as a document from which we can create a fixed-length DV to represent them."}], "references": [{"title": "Analysis of the paragraph vector model for information retrieval", "author": ["Qingyao Ai", "Liu Yang", "Jiafeng Guo", "W Bruce Croft."], "venue": "Proceedings of the 2016 ACM on International Conference on the Theory of Information Retrieval, pages 133\u2013142. ACM.", "citeRegEx": "Ai et al\\.,? 2016", "shortCiteRegEx": "Ai et al\\.", "year": 2016}, {"title": "Neural probabilistic language models", "author": ["Yoshua Bengio", "Holger Schwenk", "Jean-S\u00e9bastien Sen\u00e9cal", "Fr\u00e9deric Morin", "Jean-Luc Gauvain."], "venue": "Innovations in Machine Learning, pages 137\u2013186. Springer.", "citeRegEx": "Bengio et al\\.,? 2006", "shortCiteRegEx": "Bengio et al\\.", "year": 2006}, {"title": "Reference guide for BNC Baby", "author": ["Lou Burnard"], "venue": null, "citeRegEx": "Burnard.,? \\Q2003\\E", "shortCiteRegEx": "Burnard.", "year": 2003}, {"title": "Improving methods for single-label text categorization", "author": ["Ana Margarida de Jesus Cardoso Cachopo."], "venue": "Ph.D. thesis, Universidade T\u00e9cnica de Lisboa.", "citeRegEx": "Cachopo.,? 2007", "shortCiteRegEx": "Cachopo.", "year": 2007}, {"title": "Document embedding with paragraph vectors", "author": ["Andrew M Dai", "Christopher Olah", "Quoc V Le."], "venue": "arXiv preprint arXiv:1507.07998.", "citeRegEx": "Dai et al\\.,? 2015", "shortCiteRegEx": "Dai et al\\.", "year": 2015}, {"title": "The form is the substance: Classification of genres in text", "author": ["Nigel Dewdney", "Carol VanEss-Dykema", "Richard MacMillan."], "venue": "Proceedings of the Workshop on Human Language Technology and Knowledge Management, pages 1\u20138. Association for Computa-", "citeRegEx": "Dewdney et al\\.,? 2001", "shortCiteRegEx": "Dewdney et al\\.", "year": 2001}, {"title": "Approximate statistical tests for comparing supervised classification learning algorithms", "author": ["Thomas G Dietterich."], "venue": "Neural computation, 10(7):1895\u20131923.", "citeRegEx": "Dietterich.,? 1998", "shortCiteRegEx": "Dietterich.", "year": 1998}, {"title": "Brown corpus manual", "author": ["W. Nelson Francis", "Henry Kucera."], "venue": "Brown University.", "citeRegEx": "Francis and Kucera.,? 1979", "shortCiteRegEx": "Francis and Kucera.", "year": 1979}, {"title": "Towards genre classification for IR in the workplace", "author": ["Luanne Freund", "Charles L.A. Clarke", "Elaine G Toms."], "venue": "Proceedings of the 1st International Conference on Information Interaction in Context, pages 30\u201336. ACM.", "citeRegEx": "Freund et al\\.,? 2006", "shortCiteRegEx": "Freund et al\\.", "year": 2006}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "Neural computation, 9(8):1735\u2013 1780.", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Recognizing text genres with simple metrics using discriminant analysis", "author": ["Jussi Karlgren", "Douglass Cutting."], "venue": "Proceedings of the 15th Conference on Computational Linguistics, volume 2, pages 1071\u2013 1075. Association for Computational Linguistics.", "citeRegEx": "Karlgren and Cutting.,? 1994", "shortCiteRegEx": "Karlgren and Cutting.", "year": 1994}, {"title": "Automatic detection of text genre", "author": ["Brett Kessler", "Geoffrey Numberg", "Hinrich Sch\u00fctze."], "venue": "Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics and Eighth Conference of the European Chapter of the Association for Computa-", "citeRegEx": "Kessler et al\\.,? 1997", "shortCiteRegEx": "Kessler et al\\.", "year": 1997}, {"title": "Skip-thought vectors", "author": ["Ryan Kiros", "Yukun Zhu", "Ruslan R Salakhutdinov", "Richard Zemel", "Raquel Urtasun", "Antonio Torralba", "Sanja Fidler."], "venue": "C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama,", "citeRegEx": "Kiros et al\\.,? 2015", "shortCiteRegEx": "Kiros et al\\.", "year": 2015}, {"title": "Recurrent convolutional neural networks for text classification", "author": ["Siwei Lai", "Liheng Xu", "Kang Liu", "Jun Zhao."], "venue": "AAAI, pages 2267\u20132273.", "citeRegEx": "Lai et al\\.,? 2015", "shortCiteRegEx": "Lai et al\\.", "year": 2015}, {"title": "Distributed representations of sentences and documents", "author": ["Quoc V Le", "Tomas Mikolov."], "venue": "arXiv preprint arXiv:1405.4053.", "citeRegEx": "Le and Mikolov.,? 2014", "shortCiteRegEx": "Le and Mikolov.", "year": 2014}, {"title": "Text genre classification with genre-revealing and subjectrevealing features", "author": ["Yong-Bae Lee", "Sung Hyon Myaeng."], "venue": "Proceedings of the 25th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 145\u2013", "citeRegEx": "Lee and Myaeng.,? 2002", "shortCiteRegEx": "Lee and Myaeng.", "year": 2002}, {"title": "Treebank 2", "author": ["M. Marcus", "Beatrice Santorini", "Mary Ann Marcinkiewicz."], "venue": "Linguistic Data Consortium, Philadelphia.", "citeRegEx": "Marcus et al\\.,? 1995", "shortCiteRegEx": "Marcus et al\\.", "year": 1995}, {"title": "Recurrent neural network based language model", "author": ["Tomas Mikolov", "Martin Karafi\u00e1t", "Lukas Burget", "Jan Cernock\u1ef3", "Sanjeev Khudanpur."], "venue": "pages 1045\u20131048.", "citeRegEx": "Mikolov et al\\.,? 2010", "shortCiteRegEx": "Mikolov et al\\.", "year": 2010}, {"title": "Extensions of recurrent neural network language model", "author": ["Tom\u00e1vs Mikolov", "Stefan Kombrink", "Luk\u00e1vs Burget", "Jan Honza vCernock\u1ef3", "Sanjeev Khudanpur."], "venue": "pages 5528\u20135531.", "citeRegEx": "Mikolov et al\\.,? 2011", "shortCiteRegEx": "Mikolov et al\\.", "year": 2011}, {"title": "Assessing approaches to genre classification", "author": ["Philipp Petrenz."], "venue": "Ph.D. thesis, M. Sc. Thesis, School of Informatics, University of Edinburgh.", "citeRegEx": "Petrenz.,? 2009", "shortCiteRegEx": "Petrenz.", "year": 2009}, {"title": "PTB/PDTB files belonging to different genres", "author": ["Barbara Plank."], "venue": "http://www.let.rug. nl/ \u0303bplank/metadata/genre_files_ updated.html. Accessed: 2015-9-30.", "citeRegEx": "Plank.,? 2009", "shortCiteRegEx": "Plank.", "year": 2009}, {"title": "Software Framework for Topic Modelling with Large Corpora", "author": ["Radim vReh\u016fvrek", "Petr Sojka."], "venue": "Proceedings of the LREC 2010 Workshop on New Challenges for NLP Frameworks, pages 45\u201350, Valletta, Malta, May. ELRA. http://is.muni.cz/", "citeRegEx": "vReh\u016fvrek and Sojka.,? 2010", "shortCiteRegEx": "vReh\u016fvrek and Sojka.", "year": 2010}, {"title": "Relevance weighting of search terms", "author": ["Stephen E. Robertson", "K. Sparck Jones."], "venue": "Journal of the American Society for Information Science, 27(3):129\u2013146.", "citeRegEx": "Robertson and Jones.,? 1976", "shortCiteRegEx": "Robertson and Jones.", "year": 1976}, {"title": "Text genre detection using common word frequencies", "author": ["Efstathios Stamatatos", "Nikos Fakotakis", "George Kokkinakis."], "venue": "Proceedings of the 18th Conference on Computational Linguistics, volume 2, pages 808\u2013814. Association for Computational Lin-", "citeRegEx": "Stamatatos et al\\.,? 2000", "shortCiteRegEx": "Stamatatos et al\\.", "year": 2000}, {"title": "LSTM neural networks for language modeling", "author": ["Martin Sundermeyer", "Ralf Schl\u00fcter", "Hermann Ney."], "venue": "pages 194\u2013197.", "citeRegEx": "Sundermeyer et al\\.,? 2012", "shortCiteRegEx": "Sundermeyer et al\\.", "year": 2012}, {"title": "RWTHLM \u2014 the RWTH Aachen University neural network language modeling toolkit", "author": ["Martin Sundermeyer", "Ralf Schl\u00fcter", "Hermann Ney."], "venue": "pages 2093\u20132097.", "citeRegEx": "Sundermeyer et al\\.,? 2014", "shortCiteRegEx": "Sundermeyer et al\\.", "year": 2014}, {"title": "From feedforward to recurrent LSTM neural networks for language modeling", "author": ["Martin Sundermeyer", "Hermann Ney", "Ralf Schl\u00fcter."], "venue": "23(3):517\u2013529.", "citeRegEx": "Sundermeyer et al\\.,? 2015", "shortCiteRegEx": "Sundermeyer et al\\.", "year": 2015}, {"title": "Improved semantic representations from tree-structured long short-term memory networks", "author": ["Kai Sheng Tai", "Richard Socher", "Christopher D Manning."], "venue": "arXiv preprint arXiv:1503.00075.", "citeRegEx": "Tai et al\\.,? 2015", "shortCiteRegEx": "Tai et al\\.", "year": 2015}, {"title": "Automatic genre classification via n-grams of part-of-speech tags", "author": ["Xiaoyan Tang", "Jing Cao."], "venue": "Procedia-Social and Behavioral Sciences, 198:474\u2013 478.", "citeRegEx": "Tang and Cao.,? 2015", "shortCiteRegEx": "Tang and Cao.", "year": 2015}, {"title": "Genre distinctions for discourse in the Penn TreeBank", "author": ["Bonnie Webber."], "venue": "Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP, volume 2, pages", "citeRegEx": "Webber.,? 2009", "shortCiteRegEx": "Webber.", "year": 2009}, {"title": "Exploring the use of linguistic features in domain and genre classification", "author": ["Maria Wolters", "Mathias Kirsten."], "venue": "Proceedings of the Ninth Conference on European Chapter of the Association for Computational Linguistics, pages 142\u2013149. Association for Computa-", "citeRegEx": "Wolters and Kirsten.,? 1999", "shortCiteRegEx": "Wolters and Kirsten.", "year": 1999}, {"title": "Finegrained genre classification using structural learning algorithms", "author": ["Zhili Wu", "Katja Markert", "Serge Sharoff."], "venue": "Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 749\u2013759. Association for Computational Lin-", "citeRegEx": "Wu et al\\.,? 2010", "shortCiteRegEx": "Wu et al\\.", "year": 2010}, {"title": "Hierarchical attention networks for document classification", "author": ["Zichao Yang", "Diyi Yang", "Chris Dyer", "Xiaodong He", "Alex Smola", "Eduard Hovy."], "venue": "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguis-", "citeRegEx": "Yang et al\\.,? 2016", "shortCiteRegEx": "Yang et al\\.", "year": 2016}, {"title": "Character-level convolutional networks for text classification", "author": ["Xiang Zhang", "Junbo Zhao", "Yann LeCun."], "venue": "Advances in Neural Information Processing Systems, pages 649\u2013657.", "citeRegEx": "Zhang et al\\.,? 2015", "shortCiteRegEx": "Zhang et al\\.", "year": 2015}, {"title": "Dependency sensitive convolutional neural networks for modeling sentences and documents", "author": ["Rui Zhang", "Honglak Lee", "Dragomir Radev."], "venue": "Proceedings of NAACL-HLT, pages 1512\u20131521.", "citeRegEx": "Zhang et al\\.,? 2016", "shortCiteRegEx": "Zhang et al\\.", "year": 2016}, {"title": "A c-lstm neural network for text classification", "author": ["Chunting Zhou", "Chonglin Sun", "Zhiyuan Liu", "Francis Lau."], "venue": "arXiv preprint arXiv:1511.08630.", "citeRegEx": "Zhou et al\\.,? 2015", "shortCiteRegEx": "Zhou et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 22, "context": "One of the most popular document vectors is the term frequency-inverse document frequency (TF-IDF) feature vector (Robertson and Jones, 1976).", "startOffset": 114, "endOffset": 141}, {"referenceID": 10, "context": "(Karlgren and Cutting, 1994; Kessler et al., 1997; Wolters and Kirsten, 1999; Stamatatos et al., 2000; Dewdney et al., 2001; Lee and Myaeng, 2002; Freund et al., 2006; Petrenz, 2009; Webber, 2009).", "startOffset": 0, "endOffset": 196}, {"referenceID": 11, "context": "(Karlgren and Cutting, 1994; Kessler et al., 1997; Wolters and Kirsten, 1999; Stamatatos et al., 2000; Dewdney et al., 2001; Lee and Myaeng, 2002; Freund et al., 2006; Petrenz, 2009; Webber, 2009).", "startOffset": 0, "endOffset": 196}, {"referenceID": 30, "context": "(Karlgren and Cutting, 1994; Kessler et al., 1997; Wolters and Kirsten, 1999; Stamatatos et al., 2000; Dewdney et al., 2001; Lee and Myaeng, 2002; Freund et al., 2006; Petrenz, 2009; Webber, 2009).", "startOffset": 0, "endOffset": 196}, {"referenceID": 23, "context": "(Karlgren and Cutting, 1994; Kessler et al., 1997; Wolters and Kirsten, 1999; Stamatatos et al., 2000; Dewdney et al., 2001; Lee and Myaeng, 2002; Freund et al., 2006; Petrenz, 2009; Webber, 2009).", "startOffset": 0, "endOffset": 196}, {"referenceID": 5, "context": "(Karlgren and Cutting, 1994; Kessler et al., 1997; Wolters and Kirsten, 1999; Stamatatos et al., 2000; Dewdney et al., 2001; Lee and Myaeng, 2002; Freund et al., 2006; Petrenz, 2009; Webber, 2009).", "startOffset": 0, "endOffset": 196}, {"referenceID": 15, "context": "(Karlgren and Cutting, 1994; Kessler et al., 1997; Wolters and Kirsten, 1999; Stamatatos et al., 2000; Dewdney et al., 2001; Lee and Myaeng, 2002; Freund et al., 2006; Petrenz, 2009; Webber, 2009).", "startOffset": 0, "endOffset": 196}, {"referenceID": 8, "context": "(Karlgren and Cutting, 1994; Kessler et al., 1997; Wolters and Kirsten, 1999; Stamatatos et al., 2000; Dewdney et al., 2001; Lee and Myaeng, 2002; Freund et al., 2006; Petrenz, 2009; Webber, 2009).", "startOffset": 0, "endOffset": 196}, {"referenceID": 19, "context": "(Karlgren and Cutting, 1994; Kessler et al., 1997; Wolters and Kirsten, 1999; Stamatatos et al., 2000; Dewdney et al., 2001; Lee and Myaeng, 2002; Freund et al., 2006; Petrenz, 2009; Webber, 2009).", "startOffset": 0, "endOffset": 196}, {"referenceID": 29, "context": "(Karlgren and Cutting, 1994; Kessler et al., 1997; Wolters and Kirsten, 1999; Stamatatos et al., 2000; Dewdney et al., 2001; Lee and Myaeng, 2002; Freund et al., 2006; Petrenz, 2009; Webber, 2009).", "startOffset": 0, "endOffset": 196}, {"referenceID": 3, "context": "TF-IDF Document vectorization has a major drawback(Cachopo, 2007; Le and Mikolov, 2014): it ignores word orders and other sequential information in a document.", "startOffset": 50, "endOffset": 87}, {"referenceID": 14, "context": "TF-IDF Document vectorization has a major drawback(Cachopo, 2007; Le and Mikolov, 2014): it ignores word orders and other sequential information in a document.", "startOffset": 50, "endOffset": 87}, {"referenceID": 14, "context": "Besides TF-IDF, another notable document vectorization is the paragraph vector(Le and Mikolov, 2014).", "startOffset": 78, "endOffset": 100}, {"referenceID": 14, "context": "Paragraph vectors that learned from a distributed memory model (PV-DM) is a succinct distributed representation of sentences or paragraphs (Le and Mikolov, 2014; Dai et al., 2015; Ai et al., 2016).", "startOffset": 139, "endOffset": 196}, {"referenceID": 4, "context": "Paragraph vectors that learned from a distributed memory model (PV-DM) is a succinct distributed representation of sentences or paragraphs (Le and Mikolov, 2014; Dai et al., 2015; Ai et al., 2016).", "startOffset": 139, "endOffset": 196}, {"referenceID": 0, "context": "Paragraph vectors that learned from a distributed memory model (PV-DM) is a succinct distributed representation of sentences or paragraphs (Le and Mikolov, 2014; Dai et al., 2015; Ai et al., 2016).", "startOffset": 139, "endOffset": 196}, {"referenceID": 12, "context": "Moreover, Skip-Thought Vectors (Kiros et al., 2015) also show superior performances against bag-of-words model.", "startOffset": 31, "endOffset": 51}, {"referenceID": 17, "context": "Our approach is to adapt/retrain a simple recurrent neural network language model (RNN-LM) (Mikolov et al., 2010) or a long short-term memory RNN language model (LSTM-LM) (Sundermeyer et al.", "startOffset": 91, "endOffset": 113}, {"referenceID": 24, "context": ", 2010) or a long short-term memory RNN language model (LSTM-LM) (Sundermeyer et al., 2012) with a document, and then vectorize the retrained/adapted model parameters to obtain its document vector (labeled as DV-RNN and DV-LSTM) from RNN-LM and LSTM-LM respectively.", "startOffset": 65, "endOffset": 91}, {"referenceID": 27, "context": "In tasks such as text classification or sentiment analysis, different task-specific models have been proposed (Tai et al., 2015; Lai et al., 2015; Yang et al., 2016; Zhou et al., 2015; Zhang et al., 2015; Tang et al., ; Zhang et al., 2016).", "startOffset": 110, "endOffset": 239}, {"referenceID": 13, "context": "In tasks such as text classification or sentiment analysis, different task-specific models have been proposed (Tai et al., 2015; Lai et al., 2015; Yang et al., 2016; Zhou et al., 2015; Zhang et al., 2015; Tang et al., ; Zhang et al., 2016).", "startOffset": 110, "endOffset": 239}, {"referenceID": 32, "context": "In tasks such as text classification or sentiment analysis, different task-specific models have been proposed (Tai et al., 2015; Lai et al., 2015; Yang et al., 2016; Zhou et al., 2015; Zhang et al., 2015; Tang et al., ; Zhang et al., 2016).", "startOffset": 110, "endOffset": 239}, {"referenceID": 35, "context": "In tasks such as text classification or sentiment analysis, different task-specific models have been proposed (Tai et al., 2015; Lai et al., 2015; Yang et al., 2016; Zhou et al., 2015; Zhang et al., 2015; Tang et al., ; Zhang et al., 2016).", "startOffset": 110, "endOffset": 239}, {"referenceID": 33, "context": "In tasks such as text classification or sentiment analysis, different task-specific models have been proposed (Tai et al., 2015; Lai et al., 2015; Yang et al., 2016; Zhou et al., 2015; Zhang et al., 2015; Tang et al., ; Zhang et al., 2016).", "startOffset": 110, "endOffset": 239}, {"referenceID": 34, "context": "In tasks such as text classification or sentiment analysis, different task-specific models have been proposed (Tai et al., 2015; Lai et al., 2015; Yang et al., 2016; Zhou et al., 2015; Zhang et al., 2015; Tang et al., ; Zhang et al., 2016).", "startOffset": 110, "endOffset": 239}, {"referenceID": 17, "context": "Recurrent neural network language model (RNNLM) and long short-term memory (LSTM-LM) language model is the state-of-the-art language method (Mikolov et al., 2010; Mikolov et al., 2011; Bengio et al., 2006).", "startOffset": 140, "endOffset": 205}, {"referenceID": 18, "context": "Recurrent neural network language model (RNNLM) and long short-term memory (LSTM-LM) language model is the state-of-the-art language method (Mikolov et al., 2010; Mikolov et al., 2011; Bengio et al., 2006).", "startOffset": 140, "endOffset": 205}, {"referenceID": 1, "context": "Recurrent neural network language model (RNNLM) and long short-term memory (LSTM-LM) language model is the state-of-the-art language method (Mikolov et al., 2010; Mikolov et al., 2011; Bengio et al., 2006).", "startOffset": 140, "endOffset": 205}, {"referenceID": 9, "context": "Training the above simple RNN may suffer from the problem of exploding or vanishing gradients (Hochreiter and Schmidhuber, 1997; Sundermeyer et al., 2012).", "startOffset": 94, "endOffset": 154}, {"referenceID": 24, "context": "Training the above simple RNN may suffer from the problem of exploding or vanishing gradients (Hochreiter and Schmidhuber, 1997; Sundermeyer et al., 2012).", "startOffset": 94, "endOffset": 154}, {"referenceID": 7, "context": "The DV-RNN, and DV-L100-LSTM100 were evaluated on the genre classification of documents in three corpora: the Brown Corpus (Francis and Kucera, 1979), the British National Corpus (BNC) Baby dataset (Burnard, 2003), and an artificially created dataset from Penn Treebank (Marcus et al.", "startOffset": 123, "endOffset": 149}, {"referenceID": 2, "context": "The DV-RNN, and DV-L100-LSTM100 were evaluated on the genre classification of documents in three corpora: the Brown Corpus (Francis and Kucera, 1979), the British National Corpus (BNC) Baby dataset (Burnard, 2003), and an artificially created dataset from Penn Treebank (Marcus et al.", "startOffset": 198, "endOffset": 213}, {"referenceID": 16, "context": "The DV-RNN, and DV-L100-LSTM100 were evaluated on the genre classification of documents in three corpora: the Brown Corpus (Francis and Kucera, 1979), the British National Corpus (BNC) Baby dataset (Burnard, 2003), and an artificially created dataset from Penn Treebank (Marcus et al., 1995).", "startOffset": 270, "endOffset": 291}, {"referenceID": 29, "context": "The Penn Treebank dataset(PTB) was artificially extracted from the Penn Treebank Corpus by taking out the documents that have the available genre tags provided by (Webber, 2009; Plank, 2009).", "startOffset": 163, "endOffset": 190}, {"referenceID": 20, "context": "The Penn Treebank dataset(PTB) was artificially extracted from the Penn Treebank Corpus by taking out the documents that have the available genre tags provided by (Webber, 2009; Plank, 2009).", "startOffset": 163, "endOffset": 190}, {"referenceID": 31, "context": "For the Brown Corpus, the sub-genres under the fiction genre were merged so that the total number of genres was 10 (Wu et al., 2010).", "startOffset": 115, "endOffset": 132}, {"referenceID": 26, "context": "The RWTH Aachen University Neural Network Language Modeling Toolkit (RWTHLM) (Sundermeyer et al., 2015; Sundermeyer et al., 2014) was used for training all LSTM-LMs and adapting them to produce the DV-LSTMs.", "startOffset": 77, "endOffset": 129}, {"referenceID": 25, "context": "The RWTH Aachen University Neural Network Language Modeling Toolkit (RWTHLM) (Sundermeyer et al., 2015; Sundermeyer et al., 2014) was used for training all LSTM-LMs and adapting them to produce the DV-LSTMs.", "startOffset": 77, "endOffset": 129}, {"referenceID": 14, "context": "The paragraph vectors (PV-DMs) (Le and Mikolov, 2014) were trained for each document in a corpus using the Gensim toolkit (vReh\u016fvrek and Sojka, 2010).", "startOffset": 31, "endOffset": 53}, {"referenceID": 21, "context": "The paragraph vectors (PV-DMs) (Le and Mikolov, 2014) were trained for each document in a corpus using the Gensim toolkit (vReh\u016fvrek and Sojka, 2010).", "startOffset": 122, "endOffset": 149}, {"referenceID": 28, "context": "The baseline results labeled with * are quoted from (Tang and Cao, 2015; Wu et al., 2010).", "startOffset": 52, "endOffset": 89}, {"referenceID": 31, "context": "The baseline results labeled with * are quoted from (Tang and Cao, 2015; Wu et al., 2010).", "startOffset": 52, "endOffset": 89}, {"referenceID": 6, "context": "To evaluate the statistical significance of the performance difference between each vector, we also adopt the pair sample t-test (Dietterich, 1998).", "startOffset": 129, "endOffset": 147}], "year": 2016, "abstractText": "In many natural language processing (NLP) tasks, a document is commonly modeled as a bag of words using the term frequencyinverse document frequency (TF-IDF) vector. One major shortcoming of the frequencybased TF-IDF feature vector is that it ignores word orders that carry syntactic and semantic relationships among the words in a document, and they can be important in some NLP tasks such as genre classification. This paper proposes a novel distributed vector representation of a document: a simple recurrentneural-network language model (RNN-LM) or a long short-term memory RNN language model (LSTM-LM) is first created from all documents in a task; some of the LM parameters are then adapted by each document, and the adapted parameters are vectorized to represent the document. The new document vectors are labeled as DV-RNN and DV-LSTM respectively. We believe that our new document vectors can capture some high-level sequential information in the documents, which other current document representations fail to capture. The new document vectors were evaluated in the genre classification of documents in three corpora: the Brown Corpus, the BNC Baby Corpus and an artificially created Penn Treebank dataset. Their classification performances are compared with the performance of TF-IDF vector and the state-of-the-art distributed memory model of paragraph vector (PV-DM). The results show that DV-LSTM significantly outperforms TF-IDF and PV-DM in most cases, and combinations of the proposed document vectors with TF-IDF or PVDM may further improve performance.", "creator": "LaTeX with hyperref package"}}}