{"id": "1611.00301", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Nov-2016", "title": "Recurrent Neural Radio Anomaly Detection", "abstract": "We introduce a powerful recurrent neural network based method for novelty detection to the application of detecting radio anomalies. This approach holds promise in significantly increasing the ability of naive anomaly detection to detect small anomalies in highly complex complexity multi-user radio bands. We demonstrate the efficacy of this approach on a number of common real over the air radio communications bands of interest and quantify detection performance in terms of probability of detection an false alarm rates across a range of interference to band power ratios and compare to baseline methods.", "histories": [["v1", "Tue, 1 Nov 2016 17:17:26 GMT  (4437kb,D)", "http://arxiv.org/abs/1611.00301v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["timothy j o'shea", "t charles clancy", "robert w mcgwier"], "accepted": false, "id": "1611.00301"}, "pdf": {"name": "1611.00301.pdf", "metadata": {"source": "CRF", "title": "Recurrent Neural Radio Anomaly Detection", "authors": ["Timothy J. O\u2019Shea", "T. Charles Clancy", "Robert W. McGwier"], "emails": ["oshea@vt.edu", "tcc@vt.edu", "rwmcgwi@vt.edu"], "sections": [{"heading": null, "text": "There is a significant number of existing work on this topic in theory and application. [13] Here, we focus primarily on the reconstruction of novelties. [11] Applications for detecting anomalies have been discussed, but not widely used outside of a few small niche applications.Radioanomalies have been found in wireless sensor networks such as in [11] [9], but most of these applications focus on detecting changes in sensor data (temperature, pressure, etc.) or on expert functions occurring in high-level physical domains."}, {"heading": "II. APPROACH", "text": "Recently, the use of recursive neural networks to form a predictive reconstruction has been proposed as part of a new detector for complex time = 1. In this work, a long-term short-term memory [2] (LSTM) based on a recursive network is used to train a time series predictor on a training set, which is then used to calculate an expected error distribution against the real signal, which is well characterized for non-abnormal behavior. In some cases, the sequence learning capacity of LSTM has shown that the Hidden Markov model [10] and the Kalman-based linear predictors are exceeded because it is able to take into account a much more complex nonlinear representation of the state (does not make the Markov assumption), short- and long-term transition dependencies, and complex nonlinear output mapping dependencies than both of these previous models."}, {"heading": "III. WIDE-BAND RADIO COMMUNICATIONS TIME-SERIES", "text": "Time series in the radio domain are quite complex models, especially when looking at the broadband aggregates of multiple channels at different frequencies, each with its own temporal and spectral channel access scheme and each carrying random randomized data bits that do not normally repeat themselves, such as preambles, low entropy headers and pilots. A time sequence model for such an overall signal must then be able to take into account short-term expected symbol transitions and pulse formation of each carrier and its channel variations, as well as the symbols representing increased traffic and application sequences representing user behavior. In both layers, we must be able to model the sum of all users and emitters in a single common medium on one or more channels."}, {"heading": "IV. PREDICTOR MODELS", "text": "Here we describe each model f ({xk,..., xk + N \u2212 1, \u03b8NN) that we use to predict our next samples from the time order. To be fair, we normalize each predictor to 32 samples of input samples and 4 predicted output samples. We include several base models and a number of state-of-the-art models to learn neural network capabilities."}, {"heading": "A. Kalman Sequence Predictor", "text": "We use a 3rd order Unscented Kalman Filter / Predictor, similar to the one described in [12], which is implemented with the FilterPy module [18] and is our performance benchmark for this essay. It implements a traditional Kalman novelty detector, just as one could do without a learned prediction model. In this case, the adaptive filter is set online during operation and the error distribution is characterized thereon."}, {"heading": "B. DNN Sequence Predictor", "text": "In our Dense Neural Network (DNN) model (Figure 3), we train a na\u00efve, fully connected network as a neural network baseline with a high number of free parameters and strong dropout that allows it to learn a fully unrestricted mapping between input and output samples, which will allow us to compare other specialized / restricted architectures such as revolutionary and recurring variants to fit the model."}, {"heading": "C. Raw LSTM Sequence Predictor", "text": "In the LSTM-based sequence prediction model (Figure 4) we implement a 2-layer LSTM followed by 2 fully connected layers culminating in linear activation for regression of complex continuously evaluated sample output values. We regulate between each layer with a dropout of 0.5 and use the correct LSTM weight and activation dropout as described in [14] and implemented in Keras [15]."}, {"heading": "D. DCNN1 Sequence Predictor", "text": "In Dilated Convolutional Neural Network 1 (DCNN1) (Figure 5), we introduce a simple extended folding layer at the front end of a simple, fully connected neural network to enable learning of coil characteristics in increments of 2."}, {"heading": "E. DCNN2 Sequence Predictor", "text": "We model our Dilated Convolutional Neural Network 2 (DCNN2) architecture on a greatly simplified version of Google's WaveNet architecture [19], which has demonstrated a strong ability to learn raw time series representations on acoustic speech data. Here, we use two layers of extended coils, each being a residual block [16] containing identical layers of hyperbolic tan (TanH) and multiple sigmoid activation, followed by a 1x1 coil layer for dimension reduction."}, {"heading": "V. MODEL OPTIMIZATION", "text": "Before evaluating detection performance, we simply try to minimize the mean square error of the prediction function in order to select our network parameters \u03b8NN and our architecture and hyperparameters for the predictive value. From the first experiment, optimal network architectures seem to vary slightly from dataset to dataset, but at the moment we are trying to use a single set of network parameters for all datasets. A much more extensive hyperparameter search is really desirable here to find the most suitable network structures. We hope that this will happen more comprehensively in future work, as part of this work we are only trying out a handful of architectures derived from proven architectures in previous work on similar tasks."}, {"heading": "VI. PERFORMANCE EVALUATION", "text": "To evaluate a number of different classes of synthetic anomalies in each detected RF database (t), we need to locate a number of different types of synthetic anomalies in each detected / detected RF database (t). We have the anomaly classes that we are able to move in a single frequency, but they are each in the same time span in which we consider the anomaly classes: \u2022 The anomaly classes that we have expressed as n (t) (t) = exp (t).Fc / Fs) for t (t).Fs The anomaly classes that we have expressed as n (t). \u2022 We-Time Broadband Bursts (Sinc / 2): expressed as n (t) = sinc (t (t (t \u2212 ts + 2) Fc / Fs) for."}, {"heading": "VII. CONCLUSION", "text": "In this paper, we have demonstrated how the neural network reconstruction anomaly detector can be used to detect anomalies occurring within the band on multiple real-world broadband systems via the airband bands of interest, and the results have shown that we achieve our best performance advantage over the Kalman novelty detector methods, especially in environments with structured radio signals where prediction of the temporal sequence model works best. We believe that this is an important finding that demonstrates the feasibility of this form of frequency change monitoring and provides some starting points for improvements in more traditional methods for detecting time series changes. We have evaluated several neural predictor models and demonstrated that both the LSTM model and potentially the DCNN model are practicable at a low SNR level, while for analog modulation (FM broadcast) there are less differences between the performance of the detectors and these candidate networks."}, {"heading": "ACKNOWLEDGMENT", "text": "The authors would like to thank the Bradley Department of Electrical and Computer Engineering at Virginia Polytechnic Institute and State University, the Hume Center and DARPA for their generous support in this work, which was developed with the support of the MTO Office of the Defense Advanced Research Projects Agency (DARPA) as part of the grant HR0011-16-1-0002. The views, opinions and / or results expressed are those of the author and should not be interpreted to represent the official views or strategies of the Department of Defense or the U.S. government."}], "references": [{"title": "Constant false alarm rate processing in search radars(receiver output noise control)", "author": ["V.G. Hansen"], "venue": "Radar- Present and future, pp. 325\u2013332, 1973.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1973}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural computation, vol. 9, no. 8, pp. 1735\u2013 1780, 1997.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1997}, {"title": "Intrusion detection in wireless ad-hoc networks", "author": ["Y. Zhang", "W. Lee"], "venue": "Proceedings of the 6th annual international conference on Mobile computing and networking, ACM, 2000, pp. 275\u2013283.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2000}, {"title": "Novelty detection in learning systems", "author": ["S. Marsland"], "venue": "Neural computing surveys, vol. 3, no. 2, pp. 157\u2013195, 2003.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2003}, {"title": "Gnu radio: tools for exploring the radio frequency spectrum", "author": ["E. Blossom"], "venue": "Linux journal, vol. 2004, no. 122, p. 4, 2004.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2004}, {"title": "Usrp users and developers guide", "author": ["M. Ettus"], "venue": "Ettus Research LLC, 2005.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2005}, {"title": "Cognitive radio architecture", "author": ["III J. Mitola"], "venue": "Cooperation in Wireless Networks: Principles and Applications, Springer, 2006, pp. 243\u2013311.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2006}, {"title": "An overview of anomaly detection techniques: existing solutions and latest technological trends", "author": ["A. Patcha", "J.-M. Park"], "venue": "Computer networks, vol. 51, no. 12, pp. 3448\u20133470, 2007.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2007}, {"title": "Anomaly detection in wireless sensor networks", "author": ["S. Rajasegarar", "C. Leckie", "M. Palaniswami"], "venue": "IEEE Wireless Communications, vol. 15, no. 4, pp. 34\u201340, 2008.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2008}, {"title": "A novel connectionist system for unconstrained handwriting recognition", "author": ["A. Graves", "M. Liwicki", "S. Fern\u00e1ndez", "R. Bertolami", "H. Bunke", "J. Schmidhuber"], "venue": "IEEE transactions on pattern analysis and machine intelligence, vol. 31, no. 5, pp. 855\u2013868, 2009.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2009}, {"title": "Anomaly detection in wireless sensor networks: a survey", "author": ["M. Xie", "S. Han", "B. Tian", "S. Parvin"], "venue": "Journal of Network and Computer Applications, vol. 34, no. 4, pp. 1302\u20131325, 2011.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2011}, {"title": "Second-order kalman filters using multi-complex step derivatives", "author": ["V. Vittaldev", "R.P. Russell", "N. Arora", "D. Gaylor"], "venue": "American Astronomial Society, vol. 204, 2012.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2012}, {"title": "A review of novelty detection", "author": ["M.A. Pimentel", "D.A. Clifton", "L. Clifton", "L. Tarassenko"], "venue": "Signal Processing, vol. 99, pp. 215\u2013249, 2014.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2014}, {"title": "Recurrent neural network regularization", "author": ["W. Zaremba", "I. Sutskever", "O. Vinyals"], "venue": "arXiv preprint arXiv:1409.2329, 2014.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep residual learning for image recognition", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "arXiv preprint arXiv:1512.03385, 2015.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}, {"title": "Long short term memory networks for anomaly detection in time series", "author": ["P. Malhotra", "L. Vig", "G. Shroff", "P. Agarwal"], "venue": "Proceedings, Presses universitaires de Louvain, 2015, p. 89.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2015}, {"title": "Kalman and bayesian filters in python, [Online]. Available: https://github.com/rlabbe/ Kalman- and- Bayesian- Filters - in- Python/ (visited on 10/28/2016)", "author": ["R.R. Labbe"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2016}, {"title": "Wavenet: a generative model for raw audio", "author": ["A. v. d. Oord", "S. Dieleman", "H. Zen", "K. Simonyan", "O. Vinyals", "A. Graves", "N. Kalchbrenner", "A. Senior", "K. Kavukcuoglu"], "venue": "arXiv preprint arXiv:1609.03499, 2016.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 3, "context": "There is a significant body of existing work on the subject in theory and applications [4] [13], we focus here primarily on reconstruction based novelty detection.", "startOffset": 87, "endOffset": 90}, {"referenceID": 12, "context": "There is a significant body of existing work on the subject in theory and applications [4] [13], we focus here primarily on reconstruction based novelty detection.", "startOffset": 91, "endOffset": 95}, {"referenceID": 6, "context": "In radio, applications of anomaly detection have been discussed [7] but not widely used outside of a few small niche applications.", "startOffset": 64, "endOffset": 67}, {"referenceID": 10, "context": "Radio anomaly detection has been leveraged somewhat in wireless sensor networks such as in [11] [8] [3] [9], but most of these applications focus on detecting changes in sensor data (temperature, pressure, etc), or expert features rather than on anomalies occurring in the high rate raw physical layer radio signal itself.", "startOffset": 91, "endOffset": 95}, {"referenceID": 7, "context": "Radio anomaly detection has been leveraged somewhat in wireless sensor networks such as in [11] [8] [3] [9], but most of these applications focus on detecting changes in sensor data (temperature, pressure, etc), or expert features rather than on anomalies occurring in the high rate raw physical layer radio signal itself.", "startOffset": 96, "endOffset": 99}, {"referenceID": 2, "context": "Radio anomaly detection has been leveraged somewhat in wireless sensor networks such as in [11] [8] [3] [9], but most of these applications focus on detecting changes in sensor data (temperature, pressure, etc), or expert features rather than on anomalies occurring in the high rate raw physical layer radio signal itself.", "startOffset": 100, "endOffset": 103}, {"referenceID": 8, "context": "Radio anomaly detection has been leveraged somewhat in wireless sensor networks such as in [11] [8] [3] [9], but most of these applications focus on detecting changes in sensor data (temperature, pressure, etc), or expert features rather than on anomalies occurring in the high rate raw physical layer radio signal itself.", "startOffset": 104, "endOffset": 107}, {"referenceID": 15, "context": "Recently, the use of recurrent neural networks to form a predictive reconstruction as part of a novelty detector has been proposed [17] and demonstrated to function quite well on several time series datasets including electrocardiograms, physical telemetry signals, and power consumption metrics.", "startOffset": 131, "endOffset": 135}, {"referenceID": 1, "context": "In this work a long short term memory [2] (LSTM) based recurrent network is used to train a time series predictor on a training set which is then used to compute an expected error distribution vs the real signal which is well characterized for non-anomalous behavior.", "startOffset": 38, "endOffset": 41}, {"referenceID": 9, "context": "The sequence learning capacity of LSTM has in some cases been shown to exceed that of Hidden Markov model [10] and Kalman based linear predictors, because it is able to take into account a much more complex nonlinear representation of state (does not make the Markov assumption), short and long term transition dependencies, and complex nonlinear output mapping dependencies than either of these prior models are capable of.", "startOffset": 106, "endOffset": 110}, {"referenceID": 0, "context": "Our threshold \u03c4 can be fit using an F\u03b2 or typical constant false alarm rate (CFAR) sorts of analysis [1] if a decent dataset of \u201danomalous\u201d behavior is known, or a false alarm rate on \u201dnormal\u201d behavior is used as the metric.", "startOffset": 101, "endOffset": 104}, {"referenceID": 5, "context": "Recordings are conducted with an Ettus Research B200-mini [6] which uses the Analog Devices AD9361 RFIC front-end and stored to disk for analysis using GNU Radio [5].", "startOffset": 58, "endOffset": 61}, {"referenceID": 4, "context": "Recordings are conducted with an Ettus Research B200-mini [6] which uses the Analog Devices AD9361 RFIC front-end and stored to disk for analysis using GNU Radio [5].", "startOffset": 162, "endOffset": 165}, {"referenceID": 11, "context": "We use a 3rd order Unscented Kalman Filter/Predictor similar to that described in [12].", "startOffset": 82, "endOffset": 86}, {"referenceID": 16, "context": "This is implemented using the FilterPy module [18] and forms our performance benchmark for this paper.", "startOffset": 46, "endOffset": 50}, {"referenceID": 13, "context": "5 and using proper LSTM weight and activation dropout as described in [14] and implemented in Keras [15].", "startOffset": 70, "endOffset": 74}, {"referenceID": 17, "context": "We model our Dilated Convolutional Neural Network 2 (DCNN2) architecture on a vastly simplified version of Google\u2019s WaveNet architecture [19] which has demonstrated a strong ability to learn raw time series representations on acoustic voice data.", "startOffset": 137, "endOffset": 141}, {"referenceID": 14, "context": "Here we use two levels of dilated convolutions where each is a residual block [16] containing identical layers with hyperbolic tan (TanH) and sigmoid activations merged multiplicatively, followed by a 1x1 convolutional layer for dimensionality reduction.", "startOffset": 78, "endOffset": 82}], "year": 2016, "abstractText": "We introduce a powerful recurrent neural network based method for novelty detection to the application of detecting radio anomalies. This approach holds promise in significantly increasing the ability of naive anomaly detection to detect small anomalies in highly complex complexity multi-user radio bands. We demonstrate the efficacy of this approach on a number of common real over the air radio communications bands of interest and quantify detection performance in terms of probability of detection an false alarm rates across a range of interference to band power ratios and compare to baseline methods.", "creator": "LaTeX with hyperref package"}}}