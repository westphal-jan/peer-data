{"id": "1611.00354", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Nov-2016", "title": "Faster decoding for subword level Phrase-based SMT between related languages", "abstract": "A common and effective way to train translation systems between related languages is to consider sub-word level basic units. However, this increases the length of the sentences resulting in increased decoding time. The increase in length is also impacted by the specific choice of data format for representing the sentences as subwords. In a phrase-based SMT framework, we investigate different choices of decoder parameters as well as data format and their impact on decoding time and translation accuracy. We suggest best options for these settings that significantly improve decoding time with little impact on the translation accuracy.", "histories": [["v1", "Tue, 1 Nov 2016 19:56:36 GMT  (20kb)", "http://arxiv.org/abs/1611.00354v1", "Accepted at VarDial3 (Third Workshop on NLP for Similar Languages, Varieties and Dialects) collocated with COLING 2016; 7 pages"]], "COMMENTS": "Accepted at VarDial3 (Third Workshop on NLP for Similar Languages, Varieties and Dialects) collocated with COLING 2016; 7 pages", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["anoop kunchukuttan", "pushpak bhattacharyya"], "accepted": false, "id": "1611.00354"}, "pdf": {"name": "1611.00354.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["anoopk@cse.iitb.ac.in", "pb@cse.iitb.ac.in"], "sections": [{"heading": null, "text": "ar Xiv: 161 1.00 354v 1 [cs.C L] 1N ov2 016"}, {"heading": "1 Introduction", "text": "In fact, most of them are able to trump themselves, and they are also able to trump themselves. (...) Most of them are able to trump themselves. (...) Most of them are able to trump themselves. (...) Most of them are able to trump themselves. (...) Most of them are able to trump themselves. (...) Most of them are able to trump themselves. (...) Most of them are able to trump themselves. (...) Most of them are able to trump themselves. (...) Most of them are able to trump themselves. (...) Most of them are able to trump themselves. \"(...) Most of them have trumped themselves.\""}, {"heading": "2 Factors affecting decoding time", "text": "This section describes the factors that influence the decoding time examined in this paper."}, {"heading": "2.1 Unit of translation", "text": "The decoding time of a sentence is proportional to the length of the sentence (in relation to the basic units), and the use of subwords obviously leads to an increase in the sentence length. Various units have been suggested for translation (characters, letter n-gram, orthographic syllable, morphem, etc.) We analysed the average length of the input sentence for four language pairs (Hindi-Malayalam, Malayalam-Hindi, Bengali-Hindi, Telugu-Malayalam) on the ILCI corpus (Jha, 2012).The average length of a character-level input set is seven times the input time at word level, while it is four times as long as the word-level input time for orthographic syllable level representation."}, {"heading": "2.2 Format for sentence representation", "text": "The length of the sentence to be deciphered also depends on how the subword units are represented. We compare three popular representation formats illustrated in Table 1: \u2022 Boundary Marker: The subword at the boundary of a word is extended by a marker. There is a marker subword, either the first or the last one chosen in accordance with convention. Such representation has been used in previous work, largely in conjunction with representation at the morph level. \u2022 Internal Marker: Each subword internal to the word is extended by a marker. This representation has rarely been used, for example, the byte code encoding representation used by the Neural Machine Translation System of the University of Edinburgh (Williams et al., 2016; Sennrich et al., 2016). \u2022 Space Marker: The subword units are not changed, but the intervention boundary is represented by a blank marker. Most work on translation between related languages has this format.For boundary and internal marker, the individual marker completion of the sentence may not vary in each case (some of the inner marker lengths may vary)."}, {"heading": "2.3 Decoder Parameters", "text": "Given the basic unit and data format, some important decoder parameters used to control the search space can affect the decoding time. Essentially, the decoder is a search algorithm, and we examined important settings related to two search algorithms used in the Moses SMT system: (i) batch decoding, (ii) cube clipping (Chiang, 2007). We examined the following parameters: \u2022 Beam size: This parameter controls the size of the bar that maintains the best hypotheses for partial translation generated at any given time during batch decoding. \u2022 Table Limit: Each source phrase in the phrase table may have several translation options. This parameter determines how many of these options are taken into account during stack decoding. \u2022 Cube Pruning Pop Limit: In the case of cube clipping, the parameter limits the number of hypotheses to be created for each one."}, {"heading": "3 Experimental Setup", "text": "In this section we describe the language pairs and data sets used, the details of our experiments and the evaluation methodology."}, {"heading": "3.1 Languages and Dataset", "text": "We experimented with four language pairs (Bengali-Hindi, Malayalam-Hindi, Hindi-Malayalam and Telugu-Malayalam). Telugu and Malayalam belong to the Dravidian language family, which is agglutinative. Bengali and Hindi are Indo-Aryan languages with a relatively poor morphology. The selected language pairs cover different combinations of morphological complexity between source and target languages. For our experiments, we used the multilingual ILCI corpus (Jha, 2012), which consists of sentences from the fields of tourism and health."}, {"heading": "3.2 System details", "text": "As an example of the representation unit at subword level, we have investigated the orthographic syllable (OS) (Kunchukuttan and Bhattacharyya, 2016) in our experiments. OS is a linguistically motivated variable unit of length of translation consisting of one or more consonants followed by a vowel (a C + V unit), but our methodology is not specific to each subword unit. Therefore, the results and observations should also apply to other subword units. We used the Indian NLP library 1 for orthographic syllabification. Phrase-based SMT systems were trained with OS as the basic unit. We used the Moses system (Koehn et al., 2007), with mgiza2 for alignment, growth diag-final and heuristics for symmetrization of word alignments."}, {"heading": "3.3 Evaluation", "text": "We use BLEU (Papineni et al., 2002) to evaluate translation accuracy. We use the sum of user and system time minus the time for loading the phrase table (all reported by Moses) to determine the time for decoding the test sentence."}, {"heading": "4 Effect of decoder parameters", "text": "In fact, we are in a position to go in search of a solution that would enable us, would enable us to go in search of a solution that would enable us to find a solution that would enable us, would enable us to put ourselves in a position, would enable us to put ourselves in a position, would enable us to find a solution that would enable us to be able to be put in a position, would enable us to put ourselves in the position that we are in."}, {"heading": "5 Effect of corpus format", "text": "For these experiments, we used the following decoder parameters: cube pruning with cube pruning poplimit = 1000 for tuning and testing. Table 4 shows the results of our experiments with different body formats. The internal boundary marker format has a lower translation accuracy than the other two formats, whose translation accuracy is comparable. In terms of decoding time, no single format across all languages is better than the others. Therefore, it is recommended to use the space or boundary format for phrase-based SMT systems. Neural MT systems based on encoder decoder architectures, especially without attention mechanisms, are more sensitive to sentence lengths, so we assume that the boundary marker format may be more suitable."}, {"heading": "6 Related Work", "text": "In the literature on translation between related languages, it has been recognized in the past that the increasing length of translation at the subword level poses a challenge for both training and decoding (Vilar et al., 2007).The alignment of long sentences is computationally expensive, so most work focused on corpora with short sentences (e.g. OPUS (Tiedemann, 2009b))) (Tiedemann, 2009a; Nakov and Tiedemann, 2012; Tiedemann, 2012).To make the alignment feasible, Vilar et al. (2007) used the formulation table, which will have shorter parallel segments, as a parallel corpus for the formation of subword models. Tiedemann and Nakov (2013) also examined the reduction of the formulation table by pruning, which actually improved the translation quality for character-level models. (The authors did not report on decoding speed, but it is possible that decoding speed may be improved, as decoding speed may be generally."}, {"heading": "7 Conclusion and Future Work", "text": "We systematically examine the choice of data format for the representation of subword units in sentences and various decoder parameters that influence the decoding time in a phrase-based SMT environment. Our studies (using OS and characters as base units) show that the use of cube pruning during tuning, as well as tests with a lower value of the stack pop limit parameter, significantly improves the decoding time with minimal change in translation quality. Two data formats, the spaces and the boundary markers, produce roughly equal results in terms of both translation accuracy and decoding time. As the tuning step includes a decoder in the loop, these settings also reduce the tuning time. We plan to investigate the reduction in the time required for alignment."}, {"heading": "Acknowledgments", "text": "We thank the Technology Development for Indian Languages (TDIL) program and the Department of Electronics & Information Technology, Govt. of India for their support and the anonymous reviewers for their feedback."}], "references": [{"title": "Statistical machine translation between related languages", "author": ["Mitesh Khapra", "Anoop Kunchukuttan"], "venue": "In NAACL Tutorials", "citeRegEx": "Bhattacharyya et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Bhattacharyya et al\\.", "year": 2016}, {"title": "Batch tuning strategies for statistical machine translation", "author": ["Cherry", "Foster2012] Colin Cherry", "George Foster"], "venue": "In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies", "citeRegEx": "Cherry et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Cherry et al\\.", "year": 2012}, {"title": "Hierarchical phrase-based translation", "author": ["David Chiang"], "venue": "Computational Linguistics,", "citeRegEx": "Chiang.,? \\Q2007\\E", "shortCiteRegEx": "Chiang.", "year": 2007}, {"title": "Boosting performance of a statistical machine translation system using dynamic parallelism", "author": ["Juan C Pichel", "Jos\u00e9 C Cabaleiro", "Tom\u00e1s F Pena"], "venue": "Journal of Computational Science", "citeRegEx": "Fern\u00e1ndez et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Fern\u00e1ndez et al\\.", "year": 2016}, {"title": "Faster phrase-based decoding by refining feature state", "author": ["Michael Kayser", "Christopher D Manning"], "venue": "In Annual Meeting-Association For Computational Linguistics,", "citeRegEx": "Heafield et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Heafield et al\\.", "year": 2014}, {"title": "Kenlm: Faster and smaller language model queries", "author": ["Kenneth Heafield"], "venue": "In Proceedings of the Sixth Workshop on Statistical Machine Translation", "citeRegEx": "Heafield.,? \\Q2011\\E", "shortCiteRegEx": "Heafield.", "year": 2011}, {"title": "Fast, scalable phrase-based smt decoding", "author": ["Hoang et al.2016] Hieu Hoang", "Nikolay Bogoychev", "Lane Schwartz", "Marcin Junczys-Dowmunt"], "venue": "In arXiv Pre-print arXiv:1610.04265", "citeRegEx": "Hoang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Hoang et al\\.", "year": 2016}, {"title": "Forest rescoring: Faster decoding with integrated language models", "author": ["Huang", "Chiang2007] Liang Huang", "David Chiang"], "venue": "In Annual Meeting-Association For Computational Linguistics", "citeRegEx": "Huang et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2007}, {"title": "The TDIL program and the Indian Language Corpora Initiative", "author": ["Girish Nath Jha"], "venue": "In Language Resources and Evaluation Conference", "citeRegEx": "Jha.,? \\Q2012\\E", "shortCiteRegEx": "Jha.", "year": 2012}, {"title": "A space-efficient phrase table implementation using minimal perfect hash functions", "author": ["Marcin Junczys-Dowmunt"], "venue": "In International Conference on Text, Speech and Dialogue", "citeRegEx": "Junczys.Dowmunt.,? \\Q2012\\E", "shortCiteRegEx": "Junczys.Dowmunt.", "year": 2012}, {"title": "Moses: Open source toolkit for statistical machine translation", "author": ["Koehn et al.2007] Philipp Koehn", "Hieu Hoang", "Alexandra Birch", "Chris Callison-Burch", "Marcello Federico", "Nicola Bertoldi", "Brooke Cowan", "Wade Shen", "Christine Moran", "Richard Zens"], "venue": "In Proceedings of the 45th Annual Meeting of the ACL on Interactive Poster and Demonstration Sessions", "citeRegEx": "Koehn et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Koehn et al\\.", "year": 2007}, {"title": "Orthographic syllable as basic unit for smt between related languages", "author": ["Kunchukuttan", "Pushpak Bhattacharyya"], "venue": "In Empirical Methods in Natural Language Processing", "citeRegEx": "Kunchukuttan et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Kunchukuttan et al\\.", "year": 2016}, {"title": "Combining word-level and characterlevel models for machine translation between closely-related languages", "author": ["Nakov", "Tiedemann2012] Preslav Nakov", "J\u00f6rg Tiedemann"], "venue": "In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Short Papers-Volume", "citeRegEx": "Nakov et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Nakov et al\\.", "year": 2012}, {"title": "Bleu: a method for automatic evaluation of machine translation", "author": ["Salim Roukos", "Todd Ward", "Wei-Jing Zhu"], "venue": "In Proceedings of the 40th annual meeting on association for computational linguistics,", "citeRegEx": "Papineni et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "Neural machine translation of rare words with subword units", "author": ["Barry Haddow", "Alexandra Birch"], "venue": null, "citeRegEx": "Sennrich et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Sennrich et al\\.", "year": 2016}, {"title": "Linguistic areas and language history", "author": ["Sarah Thomason"], "venue": "In lLanguages in Contact", "citeRegEx": "Thomason.,? \\Q2000\\E", "shortCiteRegEx": "Thomason.", "year": 2000}, {"title": "Analyzing the use of character-level translation with sparse and noisy datasets. In RANLP", "author": ["Tiedemann", "Nakov2013] J\u00f6rg Tiedemann", "Preslav Nakov"], "venue": null, "citeRegEx": "Tiedemann et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Tiedemann et al\\.", "year": 2013}, {"title": "Character-based psmt for closely related languages", "author": ["J\u00f6rg Tiedemann"], "venue": "In Proceedings of the 13th Conference of the European Association for Machine Translation (EAMT", "citeRegEx": "Tiedemann.,? \\Q2009\\E", "shortCiteRegEx": "Tiedemann.", "year": 2009}, {"title": "News from opus-a collection of multilingual parallel corpora with tools and interfaces. In Recent advances in natural language processing", "author": ["J\u00f6rg Tiedemann"], "venue": null, "citeRegEx": "Tiedemann.,? \\Q2009\\E", "shortCiteRegEx": "Tiedemann.", "year": 2009}, {"title": "Character-based pivot translation for under-resourced languages and domains", "author": ["J\u00f6rg Tiedemann"], "venue": "In Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics", "citeRegEx": "Tiedemann.,? \\Q2012\\E", "shortCiteRegEx": "Tiedemann.", "year": 2012}, {"title": "Can we translate letters", "author": ["Vilar et al.2007] David Vilar", "Jan-T Peter", "Hermann Ney"], "venue": "In Proceedings of the Second Workshop on Statistical Machine Translation", "citeRegEx": "Vilar et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Vilar et al\\.", "year": 2007}, {"title": "Edinburghs statistical machine translation systems for wmt16", "author": ["Rico Sennrich", "Maria Nadejde", "Matthias Huck", "Barry Haddow", "Ondrej Bojar"], "venue": "In Proceedings of the First Conference on Machine Translation", "citeRegEx": "Williams et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Williams et al\\.", "year": 2016}, {"title": "Efficient phrase-table representation for machine translation with applications to online mt and speech translation", "author": ["Zens", "Ney2007] Richard Zens", "Hermann Ney"], "venue": "In HLT-NAACL", "citeRegEx": "Zens et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Zens et al\\.", "year": 2007}], "referenceMentions": [{"referenceID": 0, "context": "Related languages are those that exhibit lexical and structural similarities on account of sharing a common ancestry or being in contact for a long period of time (Bhattacharyya et al., 2016).", "startOffset": 163, "endOffset": 191}, {"referenceID": 15, "context": "Prolonged contact leads to convergence of linguistic properties even if the languages are not related by ancestry and could lead to the formation of linguistic areas (Thomason, 2000).", "startOffset": 166, "endOffset": 182}, {"referenceID": 20, "context": "Subword units like character (Vilar et al., 2007; Tiedemann, 2009a), character n-gram (Tiedemann and Nakov, 2013) and orthographic syllables (Kunchukuttan and Bhattacharyya, 2016) have been explored and have been shown to improve translation quality to varying degrees.", "startOffset": 29, "endOffset": 67}, {"referenceID": 8, "context": "We analysed the average length of the input sentence on four language pairs (Hindi-Malayalam, Malayalam-Hindi, Bengali-Hindi, Telugu-Malayalam) on the ILCI corpus (Jha, 2012).", "startOffset": 163, "endOffset": 174}, {"referenceID": 21, "context": "This representation has been used rarely, one example being the Byte Code Encoding representation used by University of Edinburgh\u2019s Neural Machine Translation system (Williams et al., 2016; Sennrich et al., 2016).", "startOffset": 166, "endOffset": 212}, {"referenceID": 14, "context": "This representation has been used rarely, one example being the Byte Code Encoding representation used by University of Edinburgh\u2019s Neural Machine Translation system (Williams et al., 2016; Sennrich et al., 2016).", "startOffset": 166, "endOffset": 212}, {"referenceID": 2, "context": "The decoder is essentially a search algorithm, and we investigated important settings related to two search algorithms used in the Moses SMT system: (i) stack decoding, (ii) cube-pruning (Chiang, 2007).", "startOffset": 187, "endOffset": 201}, {"referenceID": 8, "context": "We used the multilingual ILCI corpus for our experiments (Jha, 2012), consisting of sentences from tourism and health domains.", "startOffset": 57, "endOffset": 68}, {"referenceID": 10, "context": "We used the Moses system (Koehn et al., 2007), with mgiza2 for alignment, the grow-diag-final-and heuristic for symmetrization of word alignments, and Batch MIRA (Cherry and Foster, 2012) for tuning.", "startOffset": 25, "endOffset": 45}, {"referenceID": 20, "context": "Since data sparsity is a lesser concern due to small vocabulary size and higher order n-grams are generally trained for translation using sub-word units (Vilar et al., 2007), we trained 10-gram language models.", "startOffset": 153, "endOffset": 173}, {"referenceID": 13, "context": "3 Evaluation We use BLEU (Papineni et al., 2002) for evaluating translation accuracy.", "startOffset": 25, "endOffset": 48}, {"referenceID": 2, "context": "We also experimented with cube-pruning (Chiang, 2007), a faster decoding method first proposed for use with hierarchical PBSMT.", "startOffset": 39, "endOffset": 53}, {"referenceID": 20, "context": "It has been recognized in the past literature on translation between related languages that the increased length of subword level translation is challenge for training as well as decoding (Vilar et al., 2007).", "startOffset": 188, "endOffset": 208}, {"referenceID": 19, "context": "OPUS (Tiedemann, 2009b)) (Tiedemann, 2009a; Nakov and Tiedemann, 2012; Tiedemann, 2012).", "startOffset": 25, "endOffset": 87}, {"referenceID": 5, "context": "Some of the prominent efforts include efficient language models (Heafield, 2011), lazy loading (Zens and Ney, 2007), phrase-table design (Junczys-Dowmunt, 2012), multi-core environment issues (Fern\u00e1ndez et al.", "startOffset": 64, "endOffset": 80}, {"referenceID": 9, "context": "Some of the prominent efforts include efficient language models (Heafield, 2011), lazy loading (Zens and Ney, 2007), phrase-table design (Junczys-Dowmunt, 2012), multi-core environment issues (Fern\u00e1ndez et al.", "startOffset": 137, "endOffset": 160}, {"referenceID": 3, "context": "Some of the prominent efforts include efficient language models (Heafield, 2011), lazy loading (Zens and Ney, 2007), phrase-table design (Junczys-Dowmunt, 2012), multi-core environment issues (Fern\u00e1ndez et al., 2016), efficient memory allocation (Hoang et al.", "startOffset": 192, "endOffset": 216}, {"referenceID": 6, "context": ", 2016), efficient memory allocation (Hoang et al., 2016), alternative stack configurations (Hoang et al.", "startOffset": 37, "endOffset": 57}, {"referenceID": 6, "context": ", 2016), alternative stack configurations (Hoang et al., 2016) and alternative decoding algorithms like cube pruning (Chiang, 2007).", "startOffset": 42, "endOffset": 62}, {"referenceID": 2, "context": ", 2016) and alternative decoding algorithms like cube pruning (Chiang, 2007).", "startOffset": 62, "endOffset": 76}, {"referenceID": 4, "context": "Prior work on comparing stack decoding and cube-pruning has been limited to word-level models (Huang and Chiang, 2007; Heafield et al., 2014).", "startOffset": 94, "endOffset": 141}, {"referenceID": 11, "context": "OPUS (Tiedemann, 2009b)) (Tiedemann, 2009a; Nakov and Tiedemann, 2012; Tiedemann, 2012). To make alignment feasible, Vilar et al. (2007) used the phrase table learnt from word-level alignment, which will have shorter parallel segments, as parallel corpus for training subword-level models.", "startOffset": 6, "endOffset": 137}, {"referenceID": 11, "context": "OPUS (Tiedemann, 2009b)) (Tiedemann, 2009a; Nakov and Tiedemann, 2012; Tiedemann, 2012). To make alignment feasible, Vilar et al. (2007) used the phrase table learnt from word-level alignment, which will have shorter parallel segments, as parallel corpus for training subword-level models. Tiedemann and Nakov (2013) also investigated reducing the size of the phrase table by pruning, which actually improved translation quality for character level models.", "startOffset": 6, "endOffset": 317}, {"referenceID": 2, "context": "Hoang et al. (2016) provide a good overview of various approaches to optimizing decoders.", "startOffset": 0, "endOffset": 20}], "year": 2016, "abstractText": "A common and effective way to train translation systems between related languages is to consider sub-word level basic units. However, this increases the length of the sentences resulting in increased decoding time. The increase in length is also impacted by the specific choice of data format for representing the sentences as subwords. In a phrase-based SMT framework, we investigate different choices of decoder parameters as well as data format and their impact on decoding time and translation accuracy. We suggest best options for these settings that significantly improve decoding time with little impact on the translation accuracy.", "creator": "LaTeX with hyperref package"}}}