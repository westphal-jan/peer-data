{"id": "1611.00440", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Nov-2016", "title": "And the Winner is ...: Bayesian Twitter-based Prediction on 2016 U.S. Presidential Election", "abstract": "This paper describes a Naive-Bayesian predictive model for 2016 U.S. Presidential Election based on Twitter data. We use 33,708 tweets gathered since December 16, 2015 until February 29, 2016. We introduce a simpler data preprocessing method to label the data and train the model. The model achieves 95.8% accuracy on 10-fold cross validation and predicts Ted Cruz and Bernie Sanders as Republican and Democratic nominee respectively. It achieves a comparable result to those in its competitor methods.", "histories": [["v1", "Wed, 2 Nov 2016 01:45:28 GMT  (126kb,D)", "http://arxiv.org/abs/1611.00440v1", "This is the non-final version of the paper. The final version is published in the IC3INA 2016 Conference (3-5 Oct. 2016,this http URL). All citation should be directed to the final version"]], "COMMENTS": "This is the non-final version of the paper. The final version is published in the IC3INA 2016 Conference (3-5 Oct. 2016,this http URL). All citation should be directed to the final version", "reviews": [], "SUBJECTS": "cs.IR cs.CL cs.SI", "authors": ["elvyna tunggawan", "yustinus eko soelistio"], "accepted": false, "id": "1611.00440"}, "pdf": {"name": "1611.00440.pdf", "metadata": {"source": "CRF", "title": "And the Winner is ...: Bayesian Twitter-based Prediction on 2016 U.S. Presidential Election", "authors": ["Elvyna Tunggawan"], "emails": ["thoeng.elvyna@student.umn.ac.id", "yustinus.eko@umn.ac.id"], "sections": [{"heading": null, "text": "I. INTRODUCTION Presidential elections are an important moment for any country, including the United States. Their economic policies, as determined by the government, affect the economies of other countries. [1] In the 2016 US presidential election, Republican and Democratic candidates used Twitter as a campaign medium. Previous research has predicted the outcome of the US presidential election via Twitter. [2], [3], [4], [5] Some of them have proven that Twitter data can supplement or even predict poll results, following the increasing improvement in text mining research [7], [11], [12], [13], [13]. Some of the most recent studies are [4], [3], [9]. Below we discuss these three recent studies and explain how our study relates to their own. The first study is conducted by [4], which influences the mood of US presidential candidates by calculating sensory perception."}, {"heading": "II. DATA PREPARATION", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. Data Collection", "text": "We have collected 371,264 tweets using the Twitter Streaming API on Tweepy [3] from December 16, 2015 to February 29, 2016.We use # Elektion2016 as a search term because it is the official hashtag during the 2016 US presidential election cycle and covers conversations across all candidates. We separate tweets per period, i.e. seven days. Figure 1 shows the tweet frequency distribution with an average of 37,126.4 tweets per period and a standard deviation of 27,823.82 tweets. Data collection from January 20 to January 26, 2016 is limited due to resource constraints. Data is stored as JSON files. Each line of the JSON files represents a tweet consisting of 26 main attributes, such as, ID, text, retweet count and length. We only use the content of the at attributes and text attributes generated, as this research focuses on the mood toward the candidates at a given time, with no geographical location or other information collected in English."}, {"heading": "B. Data Preprocessing", "text": "We edit the data by (1) removing URLs and images, and (2) filtering tweets bearing the candidate's name. Hashtags, mentions, and retweets are not removed to retain the original meaning of a tweet. We only store tweets that meet the two requirements, as shown in Table 1. The first example does not change the content of the tweet because there are no URLs or images, and it contains the name of a candidate: Bernie Sanders. The second example shows a removed tweet that does not contain the candidate's name. The pre-editing phase changes the content of the third tweet. It removes the URLs and still keeps the tweet because it contains \"Hillary Clinton\" and \"Donald Trump.\" The pre-editing phase removes 41% of the data (Figure 2)."}, {"heading": "C. Data Labeling", "text": "The pre-made tweets are manually labeled by 11 commentators who understand English. All commentators receive either a grade as part of their coursework or as a souvenir for their work. The label specified consists of the desired candidate and the mood. Commentators interpret the tweet and decide who the tweet is related to. If they believe that the tweets do not refer to a particular candidate or understand the content, they cannot \"clearly\" choose the label. Otherwise, they can assign it to a candidate and designate it as positive or negative. We divide the tweets and commentators into three groups (Table II), labeling as many tweets as possible since January 24 to April 16, 2016. The validity of the label is determined by majority rule [6]. Each tweet is distributed to three or five commentators and is valid if there is a label that occurs the most. As a final step in data processing, we remove all tweets labeled \"unclear.\" Figure 3 shows the distribution of tweets referring to Bernie Sanders, Donald Sanders and Hillary Sanders."}, {"heading": "III. METHODOLOGY", "text": "In this section TABLE II TWEETS DISTRIBUTION ON ON LABELING STAGETweet Period Number of Annotators December 16,2015-January 19, 2016 5January 27-February 2, 2016 3Fig. 3. Sentiment Distribution by Candidatesdescripts: (1) the model training, (2) model accuracy test, and (3) prediction accuracy test."}, {"heading": "A. Model Training", "text": "Our models are trained with Naive Bayes Classifier. We have a model that represents each candidate, so we have 15 trained models. We use the nltk.classify module on the Natural Language Toolkit library on Python. We use the marked data collected from December 16, 2015 to February 2, 2016 as training data for our models. The rest of our marked data is used to evaluate the models."}, {"heading": "B. Model Accuracy Test", "text": "Accuracy is calculated by checking the confusion matrix [14], [15] and its F1 score [16]. In some folds, the models predict the mood in the extreme value (i.e. only positive or negative results).Because of these cases, we cannot calculate the F1 score of the Chris Christie model. Average accuracy and the F1 score are 95.8% and 0.96 respectively. F1 score only measures how well the model works in predicting positive mood, so we propose a modified F1 score by reversing the formula. Formula F1 score shows how well the model predicts negative feelings."}, {"heading": "C. Prediction Accuracy Test", "text": "The models use tweets collected from February 3 to 9, 2016, as prediction input. The prediction follows two steps: (1) We calculate the positive sentiment from tweets and consider the number of positive sentiment as the probability that a candidate will be nominated, and (2) we sort the candidates by the number of their positive mood. The ranking is compared with the poll results on RealClearPolitics.com. We calculate the error rate (E) by dividing the difference of the poll rank with our predicted rank by the number of candidates (ei).ei = | Poi \u2212 Prei | (2) E = \u2211 n i ei n (3), where 1 \u2264 i \u2264 n and n correspond to the number of candidates. Po and Pre are the poll and prediction values associated with RealClearPolitics.com and the model respectively."}, {"heading": "IV. RESULTS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. Model Accuracy Test", "text": "The models have good accuracy and an F1 value (Table III). It shows that the model can predict the test data almost perfectly (95.8%), with the result slightly better in a positive mood than in a negative mood, as evidenced by the greater value of F1 than F1. The test results do not show the exact effect of the training data and model accuracy. Models with a lower number of training data (e.g. Huckabees, Santorum) achieve higher accuracy than models with a higher number of training data (e.g. Trump's, Clinton's), while the lowest accuracy is achieved by Kasich, who is trained with a small number of training data. The undefined value of F1 and F1 values in Christie's, Gilmore's and Santorum's models shows extreme predictions of these models."}, {"heading": "B. Prediction Accuracy Test", "text": "We use tweets from February 3-9, 2016, to input our models of the stated candidate. We rank the prediction result by sorting the number of positive predictions for each candidate. Among Democrats, Bernie Sanders leads with 3,335 tweets, followed by Martin O'Malley (14 tweets) and Hillary Clinton (none). Republican predictions are (1) Ted Cruz (1,432 tweets), (2) Marco Rubio (1,239 tweets), (3) Rand Paul (645 tweets), (4) Rick Santorum (186 tweets), (5) John Kasich (133 tweets), (6) Carly Fiorina (88 tweets), (7) Mike Huckabee (11 tweets) and (8) Jim Gilmore (5 tweets). The other Republican candidates do not have a positive prediction, so we place them at the bottom of the list."}, {"heading": "V. DISCUSSION", "text": "Using simple pre-processed data, our naive Bayesian model successfully achieves an accuracy of 95.8% with 10-fold mutual validation and achieves 54.8% accuracy in predicting the outcome of the survey. The model predicts Ted Cruz and Bernie Sanders as Republican and Democratic Party candidates, respectively. Based on the positive predictions, it predicts that Bernie Sanders will be elected President of the United States in 2016. Although it has an accuracy of 95.8% during the model test, the prediction of the model does not represent the survey. Table III shows that the accuracy of this model does not depend on the number of training data. Model with less training data (e.g. Mike Huckabees) may perform perfectly during the model test and only miss one rank in the prediction, whereas model with more training data (e.g. Donald Trump) may perform worse. To see how the model accuracy is affected by the number of training data, we train more models for each candidate based on first tweets and use them to predict Donald Sanders and the next 4000 models."}, {"heading": "VI. CONCLUSION", "text": "We have developed naive Bayesian prediction models for the 2016 US presidential election. We use the official hashtag and a simple pre-processing method to process the data without changing its meaning. Our model achieves an accuracy of 95.8% in the model test and predicts the poll with an accuracy of 54.8%. The model predicts that Bernie Sanders and Ted Cruz will be the Democratic and Republican Party candidates, respectively, and Bernie Sanders will win the election."}], "references": [{"title": "US Election Note: Economic Policy after 2012, Americas", "author": ["Stokes", "Bruce"], "venue": "Programme AMP PP,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2012}, {"title": "The Predictive Power of Social Media: On the Predictability of U.S", "author": ["Kazem Jahanbakhsh", "Yumi Moon"], "venue": "Presidential Elections using Twitter,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2014}, {"title": "From Tweets to Polls: Linking Text Sentiment to Public Opinion Time Series", "author": ["O\u2019Connor", "Brendan"], "venue": "International AAAI Conference on Weblogs and Social Media,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2010}, {"title": "Predicting US Primary Elections with Twitter", "author": ["Shi", "Lei"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2012}, {"title": "On the Robustness of Majority Rule", "author": ["Partha Dasgupta", "Eric Maskin"], "venue": "Journal of the European Economic Association,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2008}, {"title": "Simple Text Mining for Sentiment Analysis of Political Figure using Naive Bayes Classifier Method", "author": ["Yustinus E. Soelistio", "Martinus R.S. Surendra"], "venue": "The Proceedings of The 7th ICTS,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2013}, {"title": "On Using Twitter to Monitor Political Sentiment and Predict Election Results", "author": ["Adam Bermingham", "Alan F. Smeaton"], "venue": "Proceedings of the Workshop on Sentiment Analysis where AI meets Psychology (SAAIP),", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2011}, {"title": "Analyzing Twitter Sentiment of the 2016", "author": ["Chin", "Delenn"], "venue": "Presidential Candidates,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2015}, {"title": "Limits of Electoral Predictions using Twitter", "author": ["Gayo-Avello", "Daniel"], "venue": "Proceedings of the Fifth International AAAI Conference on Weblogs and Social Media,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2011}, {"title": "SWASH: A Naive Bayes Classifier for Tweet Sentiment Identification", "author": ["Talbot", "Ruth"], "venue": "Proceedings of the 9th International Workshop on Semantic Evaluation,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2015}, {"title": "Citius: A Naive-Bayes Strategy for Sentiment Analysis on English Tweets", "author": ["Pablo Gamallo", "Marcos Garcia"], "venue": "Proceedings of the 8th International Workshop on Semantic Evaluation,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2014}, {"title": "Sentiment Analysis of Political Tweets: Towards an Accurate Classifier", "author": ["Bakliwal", "Akhsat"], "venue": "Proceedings of the Workshop on Language in Social Media,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2013}, {"title": "Data Mining Concepts and Techniques 2nd Edition", "author": ["Jiawei Han", "Micheline Kamber"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2006}, {"title": "Data Mining: Practical Machine Learning Tools and Techniques Third Edition", "author": ["Witten", "Ian H"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2011}], "referenceMentions": [{"referenceID": 0, "context": "Their economic policies, which are set by the government, affect the economy of other countries [1].", "startOffset": 96, "endOffset": 99}, {"referenceID": 1, "context": "presidential election using Twitter [2], [3], [4], [5].", "startOffset": 41, "endOffset": 44}, {"referenceID": 2, "context": "presidential election using Twitter [2], [3], [4], [5].", "startOffset": 46, "endOffset": 49}, {"referenceID": 3, "context": "presidential election using Twitter [2], [3], [4], [5].", "startOffset": 51, "endOffset": 54}, {"referenceID": 5, "context": "This follows the increasing improvement in the text mining researches [7], [8], [11], [12], [13].", "startOffset": 70, "endOffset": 73}, {"referenceID": 6, "context": "This follows the increasing improvement in the text mining researches [7], [8], [11], [12], [13].", "startOffset": 75, "endOffset": 78}, {"referenceID": 9, "context": "This follows the increasing improvement in the text mining researches [7], [8], [11], [12], [13].", "startOffset": 80, "endOffset": 84}, {"referenceID": 10, "context": "This follows the increasing improvement in the text mining researches [7], [8], [11], [12], [13].", "startOffset": 86, "endOffset": 90}, {"referenceID": 11, "context": "This follows the increasing improvement in the text mining researches [7], [8], [11], [12], [13].", "startOffset": 92, "endOffset": 96}, {"referenceID": 2, "context": "Some of the most recent studies are [4], [3], [2], [9].", "startOffset": 36, "endOffset": 39}, {"referenceID": 1, "context": "Some of the most recent studies are [4], [3], [2], [9].", "startOffset": 41, "endOffset": 44}, {"referenceID": 7, "context": "Some of the most recent studies are [4], [3], [2], [9].", "startOffset": 51, "endOffset": 54}, {"referenceID": 2, "context": "The first study is done by [4], which analyzed the sentiment on 2008 U.", "startOffset": 27, "endOffset": 30}, {"referenceID": 1, "context": "Presidential Election polls using Naive Bayesian models [3].", "startOffset": 56, "endOffset": 59}, {"referenceID": 2, "context": "Previous researches either set the sentiment of a tweet directly based on a subjectivity lexicon [4] or preprocessed the tweet using a complex preprocessing method [2], [3].", "startOffset": 97, "endOffset": 100}, {"referenceID": 1, "context": "Previous researches either set the sentiment of a tweet directly based on a subjectivity lexicon [4] or preprocessed the tweet using a complex preprocessing method [2], [3].", "startOffset": 169, "endOffset": 172}, {"referenceID": 1, "context": "[3] not only removed URLs, mentions, retweets, hashtags, numbers and stop words; but also tokenized the tweets and added not on negative words.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "We gathered 371,264 tweets using Twitter Streaming API on Tweepy [3] since December 16, 2015 until February 29, 2016.", "startOffset": 65, "endOffset": 68}, {"referenceID": 4, "context": "The validity of the label is determined by means of majority rule [6].", "startOffset": 66, "endOffset": 69}, {"referenceID": 12, "context": "The accuracy is calculated by checking the confusion matrix [14], [15] and its F1 score [16].", "startOffset": 66, "endOffset": 70}, {"referenceID": 13, "context": "The accuracy is calculated by checking the confusion matrix [14], [15] and its F1 score [16].", "startOffset": 88, "endOffset": 92}, {"referenceID": 8, "context": "Otherwise, Twitter might not be used to predict the actual polls [10].", "startOffset": 65, "endOffset": 69}], "year": 2016, "abstractText": "This paper describes a Naive-Bayesian predictive model for 2016 U.S. Presidential Election based on Twitter data. We use 33,708 tweets gathered since December 16, 2015 until February 29, 2016. We introduce a simpler data preprocessing method to label the data and train the model. The model achieves 95.8% accuracy on 10-fold cross validation and predicts Ted Cruz and Bernie Sanders as Republican and Democratic nominee respectively. It achieves a comparable result to those in its competitor methods.", "creator": "LaTeX with hyperref package"}}}