{"id": "1611.00481", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Nov-2016", "title": "Online Multi-view Clustering with Incomplete Views", "abstract": "In the era of big data, it is common to have data with multiple modalities or coming from multiple sources, known as \"multi-view data\". Multi-view clustering provides a natural way to generate clusters from such data. Since different views share some consistency and complementary information, previous works on multi-view clustering mainly focus on how to combine various numbers of views to improve clustering performance. However, in reality, each view may be incomplete, i.e., instances missing in the view. Furthermore, the size of data could be extremely huge. It is unrealistic to apply multi-view clustering in large real-world applications without considering the incompleteness of views and the memory requirement. None of previous works have addressed all these challenges simultaneously. In this paper, we propose an online multi-view clustering algorithm, OMVC, which deals with large-scale incomplete views. We model the multi-view clustering problem as a joint weighted nonnegative matrix factorization problem and process the multi-view data chunk by chunk to reduce the memory requirement. OMVC learns the latent feature matrices for all the views and pushes them towards a consensus. We further increase the robustness of the learned latent feature matrices in OMVC via lasso regularization. To minimize the influence of incompleteness, dynamic weight setting is introduced to give lower weights to the incoming missing instances in different views. More importantly, to reduce the computational time, we incorporate a faster projected gradient descent by utilizing the Hessian matrices in OMVC. Extensive experiments conducted on four real data demonstrate the effectiveness of the proposed OMVC method.", "histories": [["v1", "Wed, 2 Nov 2016 06:29:46 GMT  (436kb,D)", "http://arxiv.org/abs/1611.00481v1", null], ["v2", "Sun, 6 Nov 2016 18:05:35 GMT  (436kb,D)", "http://arxiv.org/abs/1611.00481v2", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["weixiang shao", "lifang he", "chun-ta lu", "philip s yu"], "accepted": false, "id": "1611.00481"}, "pdf": {"name": "1611.00481.pdf", "metadata": {"source": "META", "title": "Online Multi-view Clustering with Incomplete Views", "authors": ["Weixiang Shao", "Lifang He", "Chun-ta Lu", "Philip S. Yu"], "emails": ["psyu}@uic.edu", "lifanghescut@gmail.com"], "sections": [{"heading": null, "text": "This year is the highest in the history of the country."}, {"heading": "II. PRELIMINARIES", "text": "In this section, we briefly describe the problem of online multi-view clustering with incomplete views, and then introduce background knowledge of clustering using non-negative matrix factorization. Table I summarizes the notations used throughout the essay."}, {"heading": "A. Problem Description", "text": "Suppose we are given N instances in nv incomplete views {X (v), v = 1, 2,..., nv}, where X (v) and RDv \u00b7 N represent the data in the v-th view and Dv is the dimensionality of the characteristics in the v-th view. Any view can be incomplete, i.e., any of the views can have instances that are missing in a view. We define an indicator matrix of instance views M, RN \u00b7 nv bymi, j = {1 if the i-th instance is in the j-th view. 0 otherwise. (1) where each column of M represents the instance presence in a view."}, {"heading": "B. Backgrounds", "text": "Let's call X-RD \u00b7 N + the nonnegative data matrix, in which each column represents an instance > q and each row represents a feature. (Nonnegative Matrix Factorization (NMF) aims to factorize the data matrix X into two nonnegative matrices. (Thus, we call the two nonnegative matrices U-RD \u00b7 R + and V-RN \u00b7 R +. Here R is the desired reduced dimensionality. (To facilitate discussion, let's call U the base matrix and Vthe latent feature matrix.) The objective function of NMF can be formulated as below: min U, VL = X \u2212 UVT \u00b2 2F s.t. U \u2265 0, V \u2265 0, (2) where it is the Frobenius standard of the matrix. In clustering problems, the latent feature matrix-U is used to extract the clustering solution."}, {"heading": "III. ONLINE MULTI-VIEW CLUSTERING", "text": "The proposed online multi-view cluster algorithm processes the data in a streaming way with low computing and storage complexity. First, we will describe how the objective function can be derived."}, {"heading": "A. Objective of OMVC", "text": "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #"}, {"heading": "B. Dynamic Weight Setting for Missing Instances", "text": "In the previous discussion, we mentioned that we will fill the missing instances with the averages of the features and assign different weights to them. We would like to assign lower weights to the less informative estimates (averaged instances) and higher weights to the more informative estimates. However, since all the data cannot be stored, the data can only be read in the streaming method. Therefore, the averages cannot be calculated directly. Instead of filling the missing instances with the global averages, we fill the missing instances with the dynamic (current) average when we read in a new data point / block: x (v) t = \u2211 t i = 1mi, vx (v) i \u0445 ti = 1mi, v, (10), which can be efficiently calculated for all incoming missing instances. The weight w (v) t, t is dynamically adjusted to the current percentage of available features with respect to v: w (v) t, t = {1, if t appears different from the missing instances, the percentage is lower than v (11)."}, {"heading": "IV. OPTIMIZATION ALGORITHMS", "text": "In this section we first solve the objective function of OMVC from the previous section. Then we discuss the single-pass OMVC and multi-pass OMVC algorithms. Convergence and complexity analysis is given at the end of this section."}, {"heading": "A. Solution", "text": "From Eq. (9) we can see that at all times we have t (v = > J (v) (v) (v) (v) (v), {V (v) t (v) t (v) t (v) t (v) t (v) t (v) t (v) t (v) t (v) t (v) and V (v) t (v) t (v) t (v) v (v) v (v) t (v) t (v) t (v) t (v) t (v) t (v) t (v) t) (v) t) (v) t) (v) v) v (v) t (v) t) v (v) t (v) t) v (v) t (v) t) v (v) t) v (v) t) v (v) t) v (v) t) v (v) t) v (v) t) v (v) t) v (v) t) v (v) t) v (v) t) v (v) t) v (v (v) t) v (v) t) v (v) t) v (v) t) v (v (v) t) t) v (v (v) t) t) v (v (v) t) v (v) t) v (v (v) t) t) v (v (v) t) v (v (v) t) t) v (v (v) t) v (v (v) t) t) v (v (v (v) t) v (v (v) t)."}, {"heading": "B. One-Pass OMVC", "text": "The complete procedure of the one-pass algorithm is shown in algorithm 1. Several important points must be considered. First, we do not need to recalculate t any new A (v) t and B (v) t at any time. We just need to recalculate V (v) t W (v) t (v) t (v) i W (v) t V (v) t T i and add them to the old A (v) t \u2212 1 and B (v) t \u2212 1.A (v) t = A (v) t \u2212 1 + V (v) iT W (v) t (v) t (v) i (26) B (v) t (v) t \u2212 1 + X (v) i W (v) t (v) i (27). Second, we need to calculate in the algorithm the inversion of the Hessic matrix (with dimension K \u00d7 K) in the iteration. The calculation cost of an inverse matrix V (v) is the largest number of V (V)."}, {"heading": "C. Multi-Pass OMVC", "text": "In many other applications it is possible to perform multiple passes. In OMVC with one pass, the latent consensus attribute matrix V \u0445, which represents the cluster assignment / possibility, is calculated in a sequential, greedy way. It is expected that the performance of the cluster for the first coming data points may not be satisfactory. However, in OMVC with one pass, the performance for these earlier data points can be improved. In OMVC with one pass, {U (v)} can be updated in the previous pass. {A (v)} and {B (v)} from the previous pass can be used and updated. Also, the weights for missing instances will be more accurate after the first pass. Therefore, the performance of the cluster star is expected to be better than the performance of an OMVC with one pass."}, {"heading": "D. Convergence and Complexity", "text": "The convergence of the proposed OMVC 1 algorithms is a direct consequence of Proposition 5 [10]. OMV's next problems are complex.THEOREM 1: Each boundary point of the sequence {U (v) k, V (v) tk, V (v) tk, V) generated by Algorithm 1 is a stationary point of Eq. (19) Proof: The convergence of \"block descent\" can be regarded as the \"block descent method.\" (19) The convergence of the \"block descent\" methods, Grippo and Sciandrone [10] have shown that in the case of three blocks, the convergence of variables can be minimized with corresponding constraints and the remaining two blocks fixed. With respect to the convergence of the \"block descent\" methods, Grippo and Sciandrone [10] have shown that the convergence of three blocks can be ensured by requiring only a strict quasicity of the component."}, {"heading": "V. EXPERIMENT", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. Dataset", "text": "In this paper, two small datasets and two large datasets are used to evaluate the proposed OMVC method. \u2022 The summary of the datasets is shown in Table II, and the details of the datasets are as follows: \u2022 Web Knowledge Base (WebKB) 1: It is a subset of web documents from four universities [27]. The data contain 1,051 documents from two classes, course or non-course. Each document has two views: 1) the textual content of the website and 2) the anchor text on other websites that link to the website. \u2022 Handwritten Dutch Digit Recognition (Digit) 2: These data contain 2,000 handwritten digits (\"0\" - \"9\") extracted from a collection of maps. Five views are used in our experiments: (1) 76 Fourier coefficients of character forms, (2) 216 profile correlations, (2) 216 carhune-lot coefficients."}, {"heading": "B. Comparison Methods", "text": "The differences between these comparison methods are summarized in Table III, and the details of the comparison methods are as follows: \u2022 OMVC: OMVC is the online multi-view clustering method proposed in this paper. To facilitate comparison, we specify the same \u03b1vs (\u03b2vs) for all views. \u2022 MultiNMF: MultiNMF is the most advanced offline multi-view clustering method based on a common non-negative matrix factorization [20] for complete views. \u2022 MIC: MIC is one of the most recent papers to address the offline multi-view clustering problem with 4https: / / archive.ics.uci.edu / ml / datasets / YouTube + Multiview + Video + Games + DatasetIt is worth noting that MIC and MultiNMF are offline methods that take into account all data and can often perform better than on-line methods."}, {"heading": "C. Experiment Setup", "text": "To simulate situations with missing instances, we randomly delete instances (0% to 40%) from each view to render the views incomplete. As all methods except ONMF have multiple parameters, we search the grid for all parameters in the comparison methods and present the best results obtained. In addition, MultiNMF, ONMF-I and ONMF-DA cannot process incomplete views. To apply these methods, we fill the missing instances with average features. In the evaluation, we use K means to obtain the cluster solution from the consensus latent feature matrix. Since K means depends on the initialization, we repeat clusters 20 times with random initialization and report the average performance."}, {"heading": "D. Results", "text": "This year it has come to the point that it will only be a matter of time before it will happen, until it does."}, {"heading": "E. Convergence Study", "text": "The average loss is defined as follows: L (t) = 1 min {s \u00b7 t, N} nv (v) i (v) i (v) i \u2212 V (v) iT) W (v) i (n) i (v) i (v) i (c) i (n) i (n) i (v) i (v) i (v) i (v) i (v) i (n) i (n) i (n) i (n) i (v) i (n) i (n) i (n) i (n) i (n) i (n) i (n) i (n) i (n) i (n) n (n) n (n) n n (n) n n (n (n (n) n n n (n (n (n) n (n (n (n (n) n (n (n (n (n) n n (n (n (n) n n n (n (n) n n n n (n (n) n n n n n n (n (n) n n n (n) n n n n n n (n n (n) n n (n) n n n n (n) n) n (n) n) n (n) n) n n (n) n) n (n) n) n (n n) n n n (n n) n) n (n) n n n n) n (n) n n) n (n) n) n (n) n n n n n (n) n) n (n n n n) n (n) n n n n (n n n n n) n (n) n n (n) n (n) n (n) n) n n (n) n n (n (n) n (n n n) n (n n) n) n (n n (n n) n (n n) n (n) n (n) n (n (n) n (n) n (n) n (n) n n) n (n) n (n (n) n (n) n (n) n) n (n) n (n) n (n (n) n) n (n (n (n) n) n (n) n n (n"}, {"heading": "F. Sensitivity Analysis", "text": "In the proposed methods, there are two groups of parameters: {\u03b1v} and {\u03b2v}. Here, we examine the effects of the two sets of parameters. As mentioned in the previous section, we set \u03b1v (\u03b2v) so that they are the same for all views. For simplicity, we assume that \u03b1v = \u03b1 and \u03b2v = \u03b2 for all views. We run OMVC with different values for \u03b1 and \u03b2 on WebKB and Digit data. To save space, we show the results only in Fig. 13, because we have similar observations in AC.From Fig. 13a, we can determine that OMVC performs best when \u03b1 is about 10 \u2212 2 and \u03b2 is about 10 \u2212 7. Parameter \u03b1 controls the importance of the correction between views and consensus. If it gets too small, the consensus has little to contribute to learning each view, and performance will decrease."}, {"heading": "VI. RELATED WORK", "text": "In the introduction, we discussed four categories of multi-view clustering algorithms. Here, we deal in particular with several sub-view-based multi-view clustering methods [8, 20, 25, 29]. [8], however, proposed a CCA-based multi-view clustering method to learn the sub-space in which the correlations between the views are maximized. [20] approached the problem by learning a common consensus based on a co-regulated common NMF framework. Recently [29], it was proposed to solve multi-view clustering with at least one overall view based on CCA. Later [25, 17], two NMF-based methods were proposed that can solve multi-view clustering even without a complete view. Although different methods were proposed to integrate heterogeneous views, none of the previous methods can efficiently process MMF data that do not fit into the memory data, even without a complete view. [16]"}, {"heading": "VII. CONCLUSION", "text": "To achieve this goal, a common NMF algorithm is used to include not only individual matrix factorizations, but also to minimize the discrepancies between the latent attribute matrices and consensus. By dynamically weighting missing instances, OMVC minimizes the negative impact of the missing data. OMVC also accelerates the rarity of learned latent attribute matrices by introducing lasso regulation, which makes the method robust against noise and outliers. Most importantly, OMVC does not hold the entire data matrix in memory, drastically reducing spatial complexity. It processes the data individually (or chunk for chunk), learns the latent attribute, and updates the large data matrix on a comparative basis of simultaneous experiments conducted with other real methods."}, {"heading": "ACKNOWLEDGMENT", "text": "This work is supported in part by NSF grants IIS-1526499 and CNS-1626432. We thank NVIDIA Corporation for their support by donating the Titan X GPU used for this research."}], "references": [{"title": "Learning from multiple partially observed views - an application to multilingual text categorization", "author": ["M.R. Amini", "N. Usunier", "C. Goutte"], "venue": "NIPS", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2009}, {"title": "Nonlinear Programming", "author": ["D. Bertsekas"], "venue": "Athena Scientific", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1999}, {"title": "Projected newton methods for optimization problems with simple constraints", "author": ["D.P. Bertsekas"], "venue": "SIAM Journal on control and Optimization, 20(2):221\u2013246", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1982}, {"title": "Multi-View Clustering", "author": ["S. Bickel", "T. Scheffer"], "venue": "ICDM", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2004}, {"title": "Multiview Clustering: A Late Fusion Approach Using Latent Models", "author": ["E. Bruno", "S. Marchand-Maillet"], "venue": "SIGIR", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2009}, {"title": "Detect and Track Latent Factors with Online Nonnegative Matrix Factorization", "author": ["B. Cao", "D. Shen", "J.-T. Sun", "X. Wang", "Q. Yang", "Z. Chen"], "venue": "IJCAI", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2007}, {"title": "Diversityinduced multi-view subspace clustering", "author": ["X. Cao", "C. Zhang", "H. Fu", "S. Liu", "H. Zhang"], "venue": "CVPR", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2015}, {"title": "Multi-view Clustering via Canonical Correlation Analysis", "author": ["K. Chaudhuri", "S.M. Kakade", "K. Livescu", "K. Sridharan"], "venue": "ICML", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2009}, {"title": "A Matrix Factorization Approach for Integrating Multiple Data Views", "author": ["D. Greene", "P. Cunningham"], "venue": "ECML PKDD", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2009}, {"title": "On the convergence of the block nonlinear gauss\u2013seidel method under convex constraints", "author": ["L. Grippo", "M. Sciandrone"], "venue": "Operations Research Letters, 26(3):127\u2013136", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2000}, {"title": "Online nonnegative matrix factorization with robust stochastic approximation", "author": ["N. Guan", "D. Tao", "Z. Luo", "B. Yuan"], "venue": "IEEE Transactions on Neural Networks and Learning Systems, 23(7):1087\u20131099", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2012}, {"title": "Non-negative sparse coding", "author": ["P. Hoyer"], "venue": "12th IEEE Workshop on Neural Networks for Signal Processing, pages 557\u2013565. IEEE", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2002}, {"title": "Weighted Nonnegative Matrix Factorization", "author": ["Y. Kim", "S. Choi"], "venue": "ICASSP", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2009}, {"title": "A Co-training Approach for Multiview Spectral Clustering", "author": ["A. Kumar", "H.D. III"], "venue": "ICML", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2011}, {"title": "Co-regularized Multi-view Spectral Clustering", "author": ["A. Kumar", "P. Rai", "H.D. III"], "venue": "NIPS", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2011}, {"title": "Learning the Parts of Objects by Nonnegative Matrix Factorization", "author": ["D. Lee", "S. Seung"], "venue": "Nature, 401:788\u2013791", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1999}, {"title": "Partial Multi-View Clustering", "author": ["S. Li", "Y. Jiang", "Z. Zhou"], "venue": "AAAI", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2014}, {"title": "Sound field reproduction using the lasso", "author": ["G. Lilis", "D. Angelosante", "G. Giannakis"], "venue": "IEEE Transactions on Audio, Speech, and Language Processing, 18(8):1902\u20131912", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2010}, {"title": "Projected gradient methods for nonnegative matrix factorization", "author": ["C. Lin"], "venue": "Neural computation, 19(10):2756\u20132779", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2007}, {"title": "Multi-View Clustering via Joint Nonnegative Matrix Factorization", "author": ["J. Liu", "C. Wang", "J. Gao", "J. Han"], "venue": "SDM", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2013}, {"title": "A General Model for Multiple View Unsupervised Learning", "author": ["B. Long", "P.S. Yu", "Z. Zhang"], "venue": "SDM", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2008}, {"title": "On using nearlyindependent feature families for high precision and confidence", "author": ["O. Madani", "M. Georg", "D.A. Ross"], "venue": "Machine Learning, 92:457\u2013477", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2013}, {"title": "Edge weight regularization over multiple graphs for similarity learning", "author": ["P. Muthukrishnan", "D. Radev", "Q. Mei"], "venue": "ICDM", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2010}, {"title": "Online unsupervised multi-view feature selection", "author": ["W. Shao", "L. He", "C.-T. Lu", "X. Wei", "P.S. Yu"], "venue": "ICDM", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2003}, {"title": "and P", "author": ["W. Shao", "L. He"], "venue": "S. Yu. Multiple Incomplete Views Clustering via Weighted Nonnegative Matrix Factorization with L2,1 Regularization. In ECML PKDD", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2015}, {"title": "Clustering on multiple incomplete datasets via collective kernel learning", "author": ["W. Shao", "X. Shi", "P. Yu"], "venue": "ICDM", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2013}, {"title": "Beyond the point cloud: from transductive to semi-supervised learning", "author": ["V. Sindhwani", "P. Niyogi", "M. Belkin"], "venue": "ICML", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2005}, {"title": "A survey of multi-view machine learning", "author": ["S. Sun"], "venue": "Neural Computing and Applications, 23(7-8):2031\u20132038", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2013}, {"title": "Multiview Clustering with Incomplete Views", "author": ["A. Trivedi", "P. Rai", "H. Daum\u00e9 III", "S. DuVall"], "venue": "NIPS 2010: Workshop on Machine Learning for Social Computing", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2010}, {"title": "Efficient document clustering via online nonnegative matrix factorizations", "author": ["F. Wang", "P. Li", "A. K\u00f6nig"], "venue": "SDM", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2011}, {"title": "Document clustering based on non-negative matrix factorization", "author": ["W. Xu", "X. Liu", "Y. Gong"], "venue": "SIGIR", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2003}], "referenceMentions": [{"referenceID": 27, "context": "By exploiting these characteristics between multi-view data, multi-view learning can obtain better performance of learning tasks than relying on just one single view [28].", "startOffset": 166, "endOffset": 170}, {"referenceID": 3, "context": "Multi-view clustering [4], as one of basic tasks of multiview learning, provides a natural way for generating clusters from multi-view data.", "startOffset": 22, "endOffset": 25}, {"referenceID": 27, "context": "Existing multi-view clustering algorithms can be roughly categorized into four categories [28].", "startOffset": 90, "endOffset": 94}, {"referenceID": 7, "context": "Methods in the first category are based on subspace [8, 15, 20, 25], which learn a latent space so that different views are comparable in that space.", "startOffset": 52, "endOffset": 67}, {"referenceID": 14, "context": "Methods in the first category are based on subspace [8, 15, 20, 25], which learn a latent space so that different views are comparable in that space.", "startOffset": 52, "endOffset": 67}, {"referenceID": 19, "context": "Methods in the first category are based on subspace [8, 15, 20, 25], which learn a latent space so that different views are comparable in that space.", "startOffset": 52, "endOffset": 67}, {"referenceID": 24, "context": "Methods in the first category are based on subspace [8, 15, 20, 25], which learn a latent space so that different views are comparable in that space.", "startOffset": 52, "endOffset": 67}, {"referenceID": 3, "context": "Methods in the second category are co-training based algorithms [4, 14], which obtain the clustering results in an iterative manner.", "startOffset": 64, "endOffset": 71}, {"referenceID": 13, "context": "Methods in the second category are co-training based algorithms [4, 14], which obtain the clustering results in an iterative manner.", "startOffset": 64, "endOffset": 71}, {"referenceID": 6, "context": "The third category aims to learn a unified similarity matrix among multi-view data, which serves as affinity matrix for final clustering [7, 23].", "startOffset": 137, "endOffset": 144}, {"referenceID": 22, "context": "The third category aims to learn a unified similarity matrix among multi-view data, which serves as affinity matrix for final clustering [7, 23].", "startOffset": 137, "endOffset": 144}, {"referenceID": 4, "context": "The last category is called late fusion [5, 9, 21].", "startOffset": 40, "endOffset": 50}, {"referenceID": 8, "context": "The last category is called late fusion [5, 9, 21].", "startOffset": 40, "endOffset": 50}, {"referenceID": 20, "context": "The last category is called late fusion [5, 9, 21].", "startOffset": 40, "endOffset": 50}, {"referenceID": 16, "context": "In order to deal with this problem, different approaches have been explored [17, 25, 26, 29].", "startOffset": 76, "endOffset": 92}, {"referenceID": 24, "context": "In order to deal with this problem, different approaches have been explored [17, 25, 26, 29].", "startOffset": 76, "endOffset": 92}, {"referenceID": 25, "context": "In order to deal with this problem, different approaches have been explored [17, 25, 26, 29].", "startOffset": 76, "endOffset": 92}, {"referenceID": 28, "context": "In order to deal with this problem, different approaches have been explored [17, 25, 26, 29].", "startOffset": 76, "endOffset": 92}, {"referenceID": 28, "context": "[29] is the first to deal with incomplete views by utilizing information from one complete view to refer to the kernel of incomplete views.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "[17, 26] are among the first attempts to solve multiview clustering with none of the views complete.", "startOffset": 0, "endOffset": 8}, {"referenceID": 25, "context": "[17, 26] are among the first attempts to solve multiview clustering with none of the views complete.", "startOffset": 0, "endOffset": 8}, {"referenceID": 24, "context": "[25, 17] are the first attempts to solve multiple incomplete views clustering based on nonnegative matrix factorization (NMF).", "startOffset": 0, "endOffset": 8}, {"referenceID": 16, "context": "[25, 17] are the first attempts to solve multiple incomplete views clustering based on nonnegative matrix factorization (NMF).", "startOffset": 0, "endOffset": 8}, {"referenceID": 12, "context": "Inspired by the idea of weighted NMF [13], we use a dynamic weight setting to give lower weights to the incoming missing instances in different views.", "startOffset": 37, "endOffset": 41}, {"referenceID": 11, "context": "As one of the most commonly used regularization, `1 (Lasso) regularization has been successfully applied in many algorithms [12, 17, 18].", "startOffset": 124, "endOffset": 136}, {"referenceID": 16, "context": "As one of the most commonly used regularization, `1 (Lasso) regularization has been successfully applied in many algorithms [12, 17, 18].", "startOffset": 124, "endOffset": 136}, {"referenceID": 17, "context": "As one of the most commonly used regularization, `1 (Lasso) regularization has been successfully applied in many algorithms [12, 17, 18].", "startOffset": 124, "endOffset": 136}, {"referenceID": 1, "context": "A common solution is to use an alternating way to update U and V [2].", "startOffset": 65, "endOffset": 68}, {"referenceID": 15, "context": "One of the most well-known algorithms for implementing the alternating update rules is the multiplicative update approach in [16], which iteratively updates U and V by", "startOffset": 125, "endOffset": 129}, {"referenceID": 18, "context": "Another algorithm that solves this problem is Projected Gradient Descent (PGD) [19].", "startOffset": 79, "endOffset": 83}, {"referenceID": 2, "context": "To further accelerate the solving process, we borrow the idea from Newton\u2019s method [3] by utilizing the second-order information (i.", "startOffset": 83, "endOffset": 86}, {"referenceID": 11, "context": "Additionally, considering the nature of incomplete views, we adopt `1 norm to enforce the sparsity of the latent feature matrix, which is robust to noises and outliers and widely used in many algorithms [12, 17].", "startOffset": 203, "endOffset": 211}, {"referenceID": 16, "context": "Additionally, considering the nature of incomplete views, we adopt `1 norm to enforce the sparsity of the latent feature matrix, which is robust to noises and outliers and widely used in many algorithms [12, 17].", "startOffset": 203, "endOffset": 211}, {"referenceID": 1, "context": "For choosing an appropriate step size \u03b3k, we consider the simple and effective Armijo rule along the projection described in [2], that is, \u03b3k = \u03b7k , and \u03c6k is the first nonnegative integer such that", "startOffset": 125, "endOffset": 128}, {"referenceID": 2, "context": "Bertsekas [3] has proved that by selecting the step sizes 1, \u03b2, \u03b2, \u00b7 \u00b7 \u00b7 , \u03b3k > 0 satisfying (17) always exists and every limit point of {U k } is a stationary point of (12).", "startOffset": 10, "endOffset": 13}, {"referenceID": 18, "context": "Following [19], to reduce the computational cost, inequality (17) can be reformulated as:", "startOffset": 10, "endOffset": 14}, {"referenceID": 29, "context": "Even if K is very large in some cases, we can use other ways to approximate the inverse, such as the diagonal approximation [30].", "startOffset": 124, "endOffset": 128}, {"referenceID": 1, "context": "Proof: The Algorithm 1 can be viewed as the \u201cblock coordinate descent\u201d method in bound-constrained optimization [2], where sequentially one block of variables is minimized under corresponding constraints and the remaining two blocks are fixed.", "startOffset": 112, "endOffset": 115}, {"referenceID": 9, "context": "Regarding the convergence of \u201cblock coordinate descent\u201d methods, Grippo and Sciandrone [10] have shown that for the case of three blocks, the convergence can be ensured by requiring only the strict quasiconvexity of the objective function with respect to one component.", "startOffset": 87, "endOffset": 91}, {"referenceID": 9, "context": "Therefore, the proof of Theorem 1 is an immediate consequence of Proposition 5 of [10].", "startOffset": 82, "endOffset": 86}, {"referenceID": 18, "context": "According to [19], the complexity for searching the step size satisfying Eq.", "startOffset": 13, "endOffset": 17}, {"referenceID": 19, "context": "The complexity of some most recent NMF based off-line multi-view clustering methods such as MultiNMF [20] and MIC [25] are O(touttinnvKDN), which is in the same order as OMVC.", "startOffset": 101, "endOffset": 105}, {"referenceID": 24, "context": "The complexity of some most recent NMF based off-line multi-view clustering methods such as MultiNMF [20] and MIC [25] are O(touttinnvKDN), which is in the same order as OMVC.", "startOffset": 114, "endOffset": 118}, {"referenceID": 26, "context": "The summary of the datasets is shown in Table II, and the details of the datasets are as follows: \u2022 Web Knowledge Base (WebKB)1: It is a subset of web documents from four universities [27].", "startOffset": 184, "endOffset": 188}, {"referenceID": 0, "context": "\u2022 Reuters Multilingual Text Data (Reuters)3: This data contains features of 111,740 documents originally written in five different languages (English, French, German, Spanish and Italian), and their translations, over a common set of 6 topic categories [1].", "startOffset": 253, "endOffset": 256}, {"referenceID": 21, "context": "\u2022 YouTube Multiview Video Games (YouTube)4: This data contains about 120,000 videos from 31 classes corresponding to 30 popular video games and other games [22].", "startOffset": 156, "endOffset": 160}, {"referenceID": 19, "context": "\u2022 MultiNMF: MultiNMF is the state-of-the-art off-line multi-view clustering method based on joint nonnegative matrix factorization [20] for complete views.", "startOffset": 131, "endOffset": 135}, {"referenceID": 24, "context": "edu/ml/datasets/YouTube+Multiview +Video+Games+Dataset incomplete data via weighted joint NMF [25].", "startOffset": 94, "endOffset": 98}, {"referenceID": 29, "context": "\u2022 ONMF: ONMF is an online document clustering algorithm for single view using NMF [30].", "startOffset": 82, "endOffset": 86}, {"referenceID": 30, "context": "In our experiments, two widely used evaluation metrics, accuracy (AC) and normalized mutual information (NMI), are used to measure the clustering performance [31].", "startOffset": 158, "endOffset": 162}, {"referenceID": 3, "context": "Multi-view clustering [4] provides a natural way for generating clusters from multi-view data.", "startOffset": 22, "endOffset": 25}, {"referenceID": 7, "context": "Here we particularly address several subspace based multi-view clustering methods [8, 20, 25, 29].", "startOffset": 82, "endOffset": 97}, {"referenceID": 19, "context": "Here we particularly address several subspace based multi-view clustering methods [8, 20, 25, 29].", "startOffset": 82, "endOffset": 97}, {"referenceID": 24, "context": "Here we particularly address several subspace based multi-view clustering methods [8, 20, 25, 29].", "startOffset": 82, "endOffset": 97}, {"referenceID": 28, "context": "Here we particularly address several subspace based multi-view clustering methods [8, 20, 25, 29].", "startOffset": 82, "endOffset": 97}, {"referenceID": 7, "context": "[8] proposed a CCA based multi-view clustering method to learn the subspace, in which the correlations among views are maximized.", "startOffset": 0, "endOffset": 3}, {"referenceID": 19, "context": "[20] approached the problem by learning a common consensus based on a co-regularized joint NMF framework.", "startOffset": 0, "endOffset": 4}, {"referenceID": 28, "context": "Recently, [29] proposed to solve multi-view clustering with at least one complete view based on CCA.", "startOffset": 10, "endOffset": 14}, {"referenceID": 24, "context": "Later, [25, 17] proposed two NMF based methods that can solve multi-view clustering even without any complete view.", "startOffset": 7, "endOffset": 15}, {"referenceID": 16, "context": "Later, [25, 17] proposed two NMF based methods that can solve multi-view clustering even without any complete view.", "startOffset": 7, "endOffset": 15}, {"referenceID": 15, "context": "Nonnegative matrix factorization [16], especially online NMF is the second area that is related to our work.", "startOffset": 33, "endOffset": 37}, {"referenceID": 5, "context": "NMF was first proposed to handle really large data or streaming data [6].", "startOffset": 69, "endOffset": 72}, {"referenceID": 10, "context": "For example, [11] proposed an efficient online NMF algorithm that takes one chunk of samples per step and updates the bases via robust stochastic approximation.", "startOffset": 13, "endOffset": 17}, {"referenceID": 29, "context": "[30] proposed an online NMF algorithm for document clustering.", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "[24] added graph regularization to an online joint NMF framwork for multi-view feature selection.", "startOffset": 0, "endOffset": 4}], "year": 2016, "abstractText": "In the era of big data, it is common to have data with multiple modalities or coming from multiple sources, known as \u201cmulti-view data\u201d. Multi-view clustering provides a natural way to generate clusters from such data. Since different views share some consistency and complementary information, previous works on multi-view clustering mainly focus on how to combine various numbers of views to improve clustering performance. However, in reality, each view may be incomplete, i.e., instances missing in the view. Furthermore, the size of data could be extremely huge. It is unrealistic to apply multi-view clustering in large real-world applications without considering the incompleteness of views and the memory requirement. None of previous works have addressed all these challenges simultaneously. In this paper, we propose an online multiview clustering algorithm, OMVC, which deals with large-scale incomplete views. We model the multi-view clustering problem as a joint weighted nonnegative matrix factorization problem and process the multi-view data chunk by chunk to reduce the memory requirement. OMVC learns the latent feature matrices for all the views and pushes them towards a consensus. We further increase the robustness of the learned latent feature matrices in OMVC via lasso regularization. To minimize the influence of incompleteness, dynamic weight setting is introduced to give lower weights to the incoming missing instances in different views. More importantly, to reduce the computational time, we incorporate a faster projected gradient descent by utilizing the Hessian matrices in OMVC. Extensive experiments conducted on four real data demonstrate the effectiveness of the proposed OMVC method. Keywords-Multi-view clustering; Online algorithm; Incomplete views; Nonnegative matrix factorization", "creator": "LaTeX with hyperref package"}}}