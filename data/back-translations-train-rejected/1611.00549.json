{"id": "1611.00549", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Nov-2016", "title": "Inferring Coupling of Distributed Dynamical Systems via Transfer Entropy", "abstract": "In this work, we are interested in structure learning for a set of spatially distributed dynamical systems, where individual subsystems are coupled via latent variables and observed through a filter. We represent this model as a directed acyclic graph (DAG) that characterises the unidirectional coupling between subsystems. Standard approaches to structure learning are not applicable in this framework due to the hidden variables, however we can exploit the properties of certain dynamical systems to formulate exact methods based on state space reconstruction. We approach the problem by using reconstruction theorems to analytically derive a tractable expression for the KL-divergence of a candidate DAG from the observed dataset. We show this measure can be decomposed as a function of two information-theoretic measures, transfer entropy and stochastic interaction. We then present two mathematically robust scoring functions based on transfer entropy and statistical independence tests. These results support the previously held conjecture that transfer entropy can be used to infer effective connectivity in complex networks.", "histories": [["v1", "Wed, 2 Nov 2016 11:23:54 GMT  (52kb,D)", "http://arxiv.org/abs/1611.00549v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["oliver m cliff", "mikhail prokopenko", "robert fitch"], "accepted": false, "id": "1611.00549"}, "pdf": {"name": "1611.00549.pdf", "metadata": {"source": "CRF", "title": "Inferring Coupling of Distributed Dynamical Systems via Transfer Entropy", "authors": ["Oliver M. Cliff", "Mikhail Prokopenko", "Robert Fitch"], "emails": ["o.cliff@acfr.usyd.edu.au", "mikhail.prokopenko@sydney.edu.au", "robert.fitch@uts.edu.au"], "sections": [{"heading": "1 Introduction", "text": "This year, it has got to the point where there is only one person who is able to try to find a solution, to find a solution that is capable, that is able to find a solution, that is able to find a solution, that is able to find a solution, that is able to find a solution, that is able to find a solution, that is able to find a solution, that is able to find a solution, that is able to find a solution, that is able to find a solution, that is able to find a solution."}, {"heading": "2 Related Work", "text": "A complex network with non-trivial properties that leads to emergent behavior that is not usually found in traditional areas of graphics theory. [1] This concept was popularized by the basic work of Watts and Strolz. [2] Instead, we focus on the structural problem of machine learning, where the goal is to establish relationships between individual systems. [3] In particular, we are interested in systems in which the subsystems are unidirectionally coupled. In addition to complex networks, these types of systems have been introduced under a variety of specific terms, such as spatially distributed dynamic systems."}, {"heading": "3 Background", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Notation", "text": "In this thesis we will consider a collection of stationary stochastic temporal processes Z. Each process Zi includes a sequence of random variables (Zi1, Z i 2,.., Z i N) with realization (z i 1, z i 2,.., z i N) for countable time indices n. Given these processes, we can calculate probability distributions for each variable by using relative frequencies or density estimation techniques [52, 53, 54].1 Unless otherwise stated, Xin is a latent (hidden) variable, Y i n is an observed variable, and Zin is an arbitrary variable; thus Zltn = {Xn, Y n} is the collection of all hidden and observed temporal variables n."}, {"heading": "3.2 Learning Nonlinear Dynamical Networks", "text": "We are interested in modeling discrete dynamic systems, where the state is a vector of real numbers given by a point xn on a compact d-dimensional manifoldM. Instead, a map f: M \u2192 M describes the temporal evolution of the state at any given time, so that the state at the next time index xn + 1 = f (xn). Furthermore, in many practical scenarios we have no direct access to xn and can observe it instead by a measuring function: M \u2192 RM, which gives a scalar representation yn (xn) of the latent state. We assume that the multivariate system is factoralized and modeled as DAG with spatially distributed dynamic subsystems called synchronous GDS."}, {"heading": "4 Network Scoring Functions", "text": "In previous work, we have shown how to calculate the log liquidity function for synchronous GDS. In this section, we will review the literature on structural learning for DBNs, focusing on the factorized distribution in Equation (3), and then present our proposed approach to structural learning based on conditional KL deviation. We will focus on the methods for learning the synchronous GDS structure using the score and search paradigm [22], which can be as follows: Specified is a data set D = (y1, y2,.., yN) of multivariate observations, find a DAG G-G problem, such as this G-G problem = arg max G-g (B: D), (4), where g (B: D) is a scoring function that measures the degree of suitability of a candidate DAG-G for data set D, and G is the problem that poses all the problems of the GD structure, for which we find the optimal G-11 function."}, {"heading": "4.1 Prior work", "text": "A common approach to developing a score is to consider the posterior probability of the network structure G, given data D. Using the Bayes rule, we can write this distribution as p (G | D), p (D | G), p (G), p (G), p (G), p (G), p (G), p (G), p (G), p (G), p (G), p (G), p (G), p (G), p (G), p (G), p (G), p (G), the probability can be written in relation to distributions via network parameters [44] p (D | G) =, p (D | G) =, p (D | G), p (G), p (D), p (D), p (G), p (G), p (G), d), p (G), p (G), p (G)."}, {"heading": "4.2 Proposed approach", "text": "In order to overcome the problem of parameterization of distributions, we consider in this paper the different problem of finding an optimal DBN structure as a search for an economical factorized distribution that best represents the common distribution. De Campos [50] proposes to use the KL divergence as a natural information theoretical approach to quantify the similarity of these distributions for a BN. We extend this approach to the DBN structural learning problem by comparing the conditional KL divergence (zn) n), i.e. we compare the common and factorized distributions of time periods taking into account the entire history, 2DKL (pD-pB) = DKL (pD-pB) = pD (zn-pD + 1-z (n) n)."}, {"heading": "5 Computing the conditional KL divergence", "text": "In this section, we use theorems for the reconstruction of state space based on Takens \"groundbreaking work [32] in order to obtain a comprehensible form of conditional KL divergence (9). Subsequently, we reformulate this expression as the sum of two information-theoretical terms for use in our scoring functions (later described). 2Vinh et al. [51] applied the MIT algorithm [50] to DBN structural learning with complete data, but did not explicitly derive the results from conditional KL divergence. Here, we show a complete derivation for the case with latent variables."}, {"heading": "5.1 A tractable expression via state space reconstruction", "text": "To calculate the distributions in (9), we use the Bundle Delay Embedding Theorem (33) to reformulate the factoralized distribution (nominal value) and the Delay Embedding Theorem (57) for the common distribution (denominator). < We describe these theorems in detail in Appendix A, together with the technical assumptions required for (f,). The first step is to reproduce our previous result for calculating the factored distribution (denominator) in Eq. (9).Lemma 1 (Cliff et al. [3]) in the light of an observed dataset D = (y1, y2)."}, {"heading": "5.2 Information-theoretic interpretation", "text": "Before presenting the main theorems of the work, we will first introduce the concepts of collective transfer entropy and stochastic interaction (< G = > msY = > Results of collective transfer entropy and stochastic interaction. Collective transfer entropy calculates the transfer of information between a series of M source processes and a single destination process [17]. Consider the set of Y = {Y} of source processes. We can use collective transfer entropy from Y to the destination process X as a function of conditional entropy (7) termsTY \u2192 X = H (Xn + 1 | X) \u2212 H (Xn + 1 | X) n (n) n, < Y i) n, (Y i) n >) n, (n >) (23) Stochastic interaction measures the complexity of dynamic systems by quantifying the excess of information in the time beyond the system we proceed from."}, {"heading": "6 Scoring functions based on transfer entropy", "text": "There are a number of ways to create a candidate graph based on Theorem 4. Here we present a logical sequence of this theorem, from which we derive two values: (1) transfer entropy with analytical independence tests (TEA) and (2) transfer entropy with empirical independence tests (TEE). First, we will show that an approach based on maximum probability is not sufficient to learn structures."}, {"heading": "6.1 The maximum likelihood approach", "text": "A common method for deriving a score is the minimization of the KL divergence between graph and empirical distributions [15, 60]. This score is, of course, derived from Theorem 4. The following conclusion shows that in practice it is sufficient to maximize the collective transfer entropy alone to minimize the KL divergence for a synchronous GDS. The minimal KL divergence of a candidate graph G from the empirical dataset D corresponds to the maximum transfer entropy Y, i.e., arg min G-G-DKL (pD-pB) = arg max G-G m \u00b2 i = 1 T {Y-j} j \u2192 Y i. (29) The stochastic term of interaction Y in (25) is defined with respect to permanent variables Y, i.e., each variable Y in + 1 is conditioned only to its own past Y."}, {"heading": "6.2 Penalising transfer entropy by independence tests", "text": "Based on the maximum probability score (31), we propose to use independence tests to define two results of practical value < si > Y scores. Here, we draw on the result of de Campos [50], who derived a scoring function for BN structure learning based on conditional mutual information and statistical significance tests, called MIT (mutual information tests). The central idea is to use collective transfer entropy T < Y ij > i score results to measure the degree of interaction between each Vi subsystem and its parent subsystems. G (V i), but also to punish this term with a value based on significance tests. As with the MIT score, this gives a principled way to recalculate the transfer entropy when there are more edges in the graph. To develop our scores, we form a zero test action < Y > HT, where there is no statistical hypothesis."}, {"heading": "6.3 Analysis of the scores", "text": "Given the TEA and TEA scoring capabilities, the optimal graph G * can be found by using any search method via DAGs. An exhaustive search listing and evaluating DAGs is insoluble because the search space is extremely exponential in terms of the number of scoring functions (such as 2O (M2)). It is therefore common to use local search methods such as greedy climbing, flooding in the basin, and taboo searching [22]. In this section, we will discuss two properties of the scoring functions that facilitate these searches: decomposability and score equivalence. A decomposable score is a sum of local scores that depend only on a variable and its parents, i.e. g (B: D) = M scoring functions that facilitate these searches: decomposability and score equivalence equivalence."}, {"heading": "7 Discussion and future work", "text": "We have presented a principal method for learning the structure of a synchronous GDS based on collective transfer entropy and independence tests. We have derived this method analytically by reformulating the KL divergence of factorized variables from common distributions of a network, and then used generalized versions of Taken's embedding theorem to calculate densities that include hidden and observed variables. Decomposition of KL divergence in Theorem 4 captures an interesting parallel between fully observable systems and partially observable systems. De Campos [50] has previously shown that CLL divergence in a fully observable system is given by the difference between multi-information [64] and opposite information."}, {"heading": "Acknowledgements", "text": "This work was partially supported by the Australian Centre for Field Robotics, the New South Wales Government and the Faculty of Engineering and Information Technology at the University of Sydney as part of the Faculty Research Cluster Program. Special thanks go to Joseph Lizier, Ju \ufffd rgen Jost and Wolfram Martens for their contribution to dynamic systems."}, {"heading": "Appendix A. Embedding theory", "text": "We refer here to the embedding theory as the investigation of the (hidden) state xn-M of a dynamic system from a sequence of observations in xR. This section will deal with reconstruction theories that define the conditions under which we can apply delay embedding to restore the original dynamics f from this observed time series. In differential topology, an embedding refers to a smooth map sp.: M \u2192 N between manifold M and N if it can diffeomorphically transfer the dynamics to its image. In Taken's secondary work on turbulent flow [32], he proposed a map composed of delayed observations that can be used to reconstruct the dynamics for typical (f,), that is, to fix some things (the embedding dimension) and the delay (the delay), the delay of the map that is given."}], "references": [{"title": "Complex networks: Structure and dynamics", "author": ["S. Boccaletti", "V. Latora", "Y. Moreno", "M. Chavez", "D.-U. Hwang"], "venue": "Phys. Rep., vol. 424, no. 4, pp. 175\u2013308, 2006.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2006}, {"title": "An Introduction to Sequential Dynamical Systems", "author": ["H. Mortveit", "C. Reidys"], "venue": "Springer Science & Business Media,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2007}, {"title": "An information criterion for inferring coupling in distributed dynamical systems", "author": ["O.M. Cliff", "M. Prokopenko", "R. Fitch"], "venue": "Front. Robot. AI, 2016.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2016}, {"title": "Measuring information transfer", "author": ["T. Schreiber"], "venue": "Phys. Rev. Lett., vol. 85, no. 2, pp. 461\u2013464, 2000.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2000}, {"title": "Delayed spatiotemporal interactions and coherent structure in multi-agent team dynamics", "author": ["O.M. Cliff", "J.T. Lizier", "P. Wang", "X.R. Wang", "O. Obst", "M. Prokopenko"], "venue": "Art. Life, vol. 23, no. 1, 2016.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2016}, {"title": "Scalable identification of stable positive systems", "author": ["J. Umenberger", "I.R. Manchester"], "venue": "Proc. of IEEE CDC, 2016.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2016}, {"title": "Detecting causality in complex ecosystems", "author": ["G. Sugihara", "R. May", "H. Ye", "C.-h. Hsieh", "E. Deyle", "M. Fogarty", "S. Munch"], "venue": "Science, vol. 338, no. 6106, pp. 496\u2013500, 2012.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2012}, {"title": "Transfer entropy \u2013 a model-free measure of effective connectivity for the neurosciences", "author": ["R. Vicente", "M. Wibral", "M. Lindner", "G. Pipa"], "venue": "J. Comp. Neurosci., vol. 30, no. 1, pp. 45\u201367, 2011.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2011}, {"title": "A statistical framework to infer delay and direction of information flow from measurements of complex systems", "author": ["J. Schumacher", "T. Wunderle", "P. Fries", "F. J\u00e4kel", "G. Pipa"], "venue": "Neural Computation, vol. 27, no. 8, pp. 1555\u20131608, 2015.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning Bayesian networks: Approaches and issues", "author": ["R. Daly", "Q. Shen", "J.S. Aitken"], "venue": "Knowl. Eng. Rev., vol. 26, no. 2, pp. 99\u2013157, 2011.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2011}, {"title": "Learning equivalence classes of Bayesian-network structures", "author": ["D.M. Chickering"], "venue": "J. Mach. Learn. Res., vol. 2, pp. 445\u2013498, 2002.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2002}, {"title": "Information theory and an extension of the maximum likelihood principle", "author": ["H. Akaike"], "venue": "Proc. of IEEE ISIT, pp. 267\u2013281, 1973.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1973}, {"title": "Estimating the dimension of a model", "author": ["G. Schwarz"], "venue": "Ann. Statist., vol. 6, no. 2, pp. 461\u2013464, 1978.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1978}, {"title": "Modeling by shortest data description", "author": ["J. Rissanen"], "venue": "Automatica, vol. 14, no. 5, pp. 465\u2013471, 1978.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1978}, {"title": "Learning Bayesian belief networks: An approach based on the MDL principle", "author": ["W. Lam", "F. Bacchus"], "venue": "Comp. Intell., vol. 10, no. 3, pp. 269\u2013293, 1994.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1994}, {"title": "Temporal infomax leads to almost deterministic dynamical systems", "author": ["N. Ay", "T. Wennekers"], "venue": "Neurocomputing, vol. 52, pp. 461\u2013466, 2003.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2003}, {"title": "Information modification and particle collisions in distributed computation", "author": ["J.T. Lizier", "M. Prokopenko", "A.Y. Zomaya"], "venue": "Chaos, vol. 20, no. 3, pp. 037109\u201313, 2010.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2010}, {"title": "Dynamic Bayesian Networks: Representation, Inference and Learning", "author": ["K. Murphy"], "venue": "PhD thesis, UC Berkeley,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2002}, {"title": "Multivariate construction of effective computational networks from observational data.", "author": ["J.T. Lizier", "M. Rubinov"], "venue": "ArXiV Preprint,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2012}, {"title": "Collective dynamics of \u2019small-world\u2019 networks", "author": ["D.J. Watts", "S.H. Strogatz"], "venue": "Nature, vol. 393, no. 6684, pp. 409\u201310, 1998.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 1998}, {"title": "Emergence of scaling in random networks", "author": ["A.-L. Barab\u00e1si", "R. Albert"], "venue": "Science, vol. 286, no. 5439, pp. 509\u2013512, 1999.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1999}, {"title": "Probabilistic graphical models: Principles and techniques", "author": ["D. Koller", "N. Friedman"], "venue": "MIT press,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2009}, {"title": "Nonlinear time series analysis", "author": ["H. Kantz", "T. Schreiber"], "venue": "Cambridge university press,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2004}, {"title": "Generalized synchronization, predictability, and equivalence of unidirectionally coupled dynamical systems", "author": ["L. Kocarev", "U. Parlitz"], "venue": "Physical Review Letters, vol. 76, no. 11, p. 1816, 1996.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 1816}, {"title": "Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference", "author": ["J. Pearl"], "venue": null, "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2014}, {"title": "Entropy per unit time as a metric invariant of automorphisms", "author": ["A.N. Kolmogorov"], "venue": "Dokl. Akad. Nauk SSSR, vol. 124, pp. 754\u2013755, 1959. 15", "citeRegEx": "26", "shortCiteRegEx": null, "year": 1959}, {"title": "On the notion of entropy of a dynamical system", "author": ["Y.G. Sinai"], "venue": "Dokl. Akad. Nauk SSSR, vol. 124, pp. 768\u2013771, 1959.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 1959}, {"title": "Investigating causal relations by econometric models and cross-spectral methods", "author": ["C.W. Granger"], "venue": "Econometrica, pp. 424\u2013438, 1969.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 1969}, {"title": "The bidirectional communication theory \u2013 a generalization of information theory", "author": ["H. Marko"], "venue": "IEEE Trans. Commun., vol. 21, no. 12, pp. 1345\u20131351, 1973.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 1973}, {"title": "Granger causality and transfer entropy are equivalent for Gaussian variables", "author": ["L. Barnett", "A.B. Barrett", "A.K. Seth"], "venue": "Phys. Rev. Lett., vol. 103, p. e238701, 2009.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2009}, {"title": "Differentiating information transfer and causal effect", "author": ["J.T. Lizier", "M. Prokopenko"], "venue": "Eur. Phys. J B., vol. 73, no. 4, pp. 605\u2013615, 2010.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2010}, {"title": "Detecting strange attractors in turbulence", "author": ["F. Takens"], "venue": "Dynamical Systems and Turbulence, vol. 898 of Lecture Notes in Math., pp. 366\u2013381, 1981.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 1981}, {"title": "Takens embedding theorems for forced and stochastic systems", "author": ["J. Stark", "D.S. Broomhead", "M.E. Davies", "J. Huke"], "venue": "Nonlinear Anal. Theory Methods Appl., vol. 30, no. 9, pp. 5303\u20135314, 1997.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 1997}, {"title": "Organization, development and function of complex brain networks", "author": ["O. Sporns", "D.R. Chialvo", "M. Kaiser", "C.C. Hilgetag"], "venue": "Trends Cogn. Sci., vol. 8, no. 9, pp. 418\u2013425, 2004.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2004}, {"title": "Structural and functional brain networks: from connections to cognition", "author": ["H.-J. Park", "K. Friston"], "venue": "Science, vol. 342, no. 6158, p. 1238411, 2013.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2013}, {"title": "Towards quantifying interaction networks in a football match", "author": ["O.M. Cliff", "J.T. Lizier", "X.R. Wang", "P. Wang", "O. Obst", "M. Prokopenko"], "venue": "RoboCup 2013: Robot World Cup XVII, pp. 1\u201313, Springer, 2013.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2013}, {"title": "Structure of a global network of financial companies based on transfer entropy", "author": ["L. Sandoval"], "venue": "Entropy, vol. 16, no. 8, pp. 4443\u20134482, 2014.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2014}, {"title": "Using information-theoretic principles to analyze and evaluate complex adaptive supply network architectures", "author": ["J. Rodewald", "J. Colombi", "K. Oyama", "A. Johnson"], "venue": "Procedia Computer Sci., vol. 61, pp. 147\u2013152, 2015.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2015}, {"title": "Model identification using correlation-based inference and transfer entropy estimation", "author": ["C. Damiani", "P. Lecca"], "venue": "Proc. of IEEE EMS, pp. 129\u2013134, 2011.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2011}, {"title": "Properties of Bayesian belief network learning algorithms", "author": ["R.R. Bouckaert"], "venue": "Proc. of AUAI UAI, pp. 102\u2013109, 1994.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 1994}, {"title": "A tutorial on learning Bayesian networks", "author": ["D. Heckerman"], "venue": "Innovations in Bayesian Net., vol. 156 of Studies in Comp. Intell., pp. 33\u201382, Springer, 1995.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 1995}, {"title": "Learning Bayesian networks: the combination of knowledge and statistical data", "author": ["D. Heckerman", "D. Geiger", "D.M. Chickering"], "venue": "Mach. Learn., vol. 20, no. 3, pp. 20\u2013197, 1995.", "citeRegEx": "42", "shortCiteRegEx": null, "year": 1995}, {"title": "Theory refinement on Bayesian networks", "author": ["W. Buntine"], "venue": "Proc. of AUAI UAI, pp. 52\u201360, 1991.", "citeRegEx": "43", "shortCiteRegEx": null, "year": 1991}, {"title": "Learning the structure of dynamic probabilistic networks", "author": ["N. Friedman", "K. Murphy", "S. Russell"], "venue": "Proc. of AUAI UAI, pp. 139\u2013147, 1998.", "citeRegEx": "44", "shortCiteRegEx": null, "year": 1998}, {"title": "Local learning in probabilistic networks with hidden variables", "author": ["S. Russell", "J. Binder", "D. Koller", "K. Kanazawa"], "venue": "Proc. of AAAI IJCAI, vol. 95, pp. 1146\u20131152, 1995.", "citeRegEx": "45", "shortCiteRegEx": null, "year": 1995}, {"title": "Adaptive probabilistic networks with hidden variables", "author": ["J. Binder", "D. Koller", "S. Russell", "K. Kanazawa"], "venue": "Mach. Learn., vol. 29, no. 2-3, pp. 213\u2013244, 1997.", "citeRegEx": "46", "shortCiteRegEx": null, "year": 1997}, {"title": "Using hidden nodes in Bayesian networks", "author": ["C.-K. Kwoh", "D.F. Gillies"], "venue": "Artif. Intell., vol. 88, no. 1, pp. 1\u201338, 1996.", "citeRegEx": "47", "shortCiteRegEx": null, "year": 1996}, {"title": "Approximating posterior distributions in belief networks using mixtures", "author": ["C.M. Bishop", "N. Lawrence", "T. Jaakkola", "M. Jordan"], "venue": "Proc. of NIPS, p. 416, 1998.", "citeRegEx": "48", "shortCiteRegEx": null, "year": 1998}, {"title": "A Bayesian approach to learning Bayesian networks with local structure", "author": ["D.M. Chickering", "D. Heckerman", "C. Meek"], "venue": "Proc. of AUAI UAI, pp. 80\u201389, 1997.", "citeRegEx": "49", "shortCiteRegEx": null, "year": 1997}, {"title": "A scoring function for learning Bayesian networks based on mutual information and conditional independence tests", "author": ["L.M. de Campos"], "venue": "J. Mach. Learn. Res., vol. 7, pp. 2149\u20132187, Dec. 2006. 16", "citeRegEx": "50", "shortCiteRegEx": null, "year": 2006}, {"title": "GlobalMIT: learning globally optimal dynamic Bayesian network with the mutual information test criterion", "author": ["N.X. Vinh", "M. Chetty", "R. Coppel", "P.P. Wangikar"], "venue": "Bioinformatics, vol. 27, no. 19, pp. 2765\u20132766, 2011.", "citeRegEx": "51", "shortCiteRegEx": null, "year": 2011}, {"title": "Sample estimate of the entropy of a random vector", "author": ["L. Kozachenko", "N.N. Leonenko"], "venue": "Probl. Peredachi Inf., vol. 23, no. 2, pp. 9\u201316, 1987.", "citeRegEx": "52", "shortCiteRegEx": null, "year": 1987}, {"title": "Estimating mutual information", "author": ["A. Kraskov", "H. St\u00f6gbauer", "P. Grassberger"], "venue": "Phys. Rev. E, vol. 69, no. 6, p. 066138, 2004.", "citeRegEx": "53", "shortCiteRegEx": null, "year": 2004}, {"title": "Binless strategies for estimation of information from neural data", "author": ["J.D. Victor"], "venue": "Phys. Rev. E, vol. 66, no. 5, p. 051903, 2002.", "citeRegEx": "54", "shortCiteRegEx": null, "year": 1903}, {"title": "A new look at the statistical model identification", "author": ["H. Akaike"], "venue": "IEEE Trans. Autom. Control, vol. 19, no. 6, pp. 716\u2013723, 1974.", "citeRegEx": "55", "shortCiteRegEx": null, "year": 1974}, {"title": "Information Theory, Inference and Learning Algorithms", "author": ["D.J.C. MacKay"], "venue": "Cambridge university press,", "citeRegEx": "56", "shortCiteRegEx": "56", "year": 2003}, {"title": "Generalized theorems for nonlinear state space reconstruction", "author": ["E.R. Deyle", "G. Sugihara"], "venue": "PLOS ONE, vol. 6, no. 3, p. e18295, 2011.", "citeRegEx": "57", "shortCiteRegEx": null, "year": 1829}, {"title": "Dynamical properties of strongly interacting Markov chains", "author": ["N. Ay", "T. Wennekers"], "venue": "Neural Net., vol. 16, no. 10, pp. 1483\u20131497, 2003.", "citeRegEx": "58", "shortCiteRegEx": null, "year": 2003}, {"title": "Integrated information increases with fitness in the evolution of animats", "author": ["J.A. Edlund", "N. Chaumont", "A. Hintze", "C. Koch", "G. Tononi", "C. Adami"], "venue": "PLOS Comp. Bio., vol. 7, no. 10, p. e1002236, 2011.", "citeRegEx": "59", "shortCiteRegEx": null, "year": 2011}, {"title": "Discretizing continuous attributes while learning Bayesian networks", "author": ["N. Friedman", "M. Goldszmidt"], "venue": "Proc. of ICML, pp. 157\u2013165, 1996.", "citeRegEx": "60", "shortCiteRegEx": null, "year": 1996}, {"title": "JIDT: an information-theoretic toolkit for studying the dynamics of complex systems", "author": ["J.T. Lizier"], "venue": "Front. Robot. AI, vol. 1, no. 11, 2014.", "citeRegEx": "61", "shortCiteRegEx": null, "year": 2014}, {"title": "Transfer entropy as a log-likelihood ratio", "author": ["L. Barnett", "T. Bossomaier"], "venue": "Physical review letters, vol. 109, no. 13, p. 138105, 2012.", "citeRegEx": "62", "shortCiteRegEx": null, "year": 2012}, {"title": "Searching for Bayesian network structures in the space of restricted acyclic partially directed graphs", "author": ["S. Acid", "L.M. de Campos"], "venue": "J. Artif. Intell. Res., pp. 445\u2013490, 2003.", "citeRegEx": "63", "shortCiteRegEx": null, "year": 2003}, {"title": "The multiinformation function as a tool for measuring stochastic dependence", "author": ["M. Studen\u1ef3", "J. Vejnarov\u00e1"], "venue": "Learning in graphical models, pp. 261\u2013297, Springer, 1998.", "citeRegEx": "64", "shortCiteRegEx": null, "year": 1998}, {"title": "Markov models from data by simple nonlinear time series predictors in delay embedding spaces", "author": ["M. Ragwitz", "H. Kantz"], "venue": "Phys. Rev. E, vol. 65, no. 5, p. 056201, 2002.", "citeRegEx": "65", "shortCiteRegEx": null, "year": 2002}, {"title": "Optimal embedding parameters: a modelling paradigm", "author": ["M. Small", "C.K. Tse"], "venue": "Physica D, vol. 194, no. 3, pp. 283\u2013296, 2004.", "citeRegEx": "66", "shortCiteRegEx": null, "year": 2004}, {"title": "The reconstruction theorem for endomorphisms", "author": ["F. Takens"], "venue": "Bull. Braz. Math. Soc., vol. 33, no. 2, pp. 231\u2013262, 2002. 17", "citeRegEx": "67", "shortCiteRegEx": null, "year": 2002}], "referenceMentions": [{"referenceID": 0, "context": "Complex networks are capable of modelling a wide array of important phenomena in both natural and artificial environments [1].", "startOffset": 122, "endOffset": 125}, {"referenceID": 1, "context": "We represent this network as a type of probabilistic graphical model termed a synchronous graph dynamical system (GDS) [2, 3].", "startOffset": 119, "endOffset": 125}, {"referenceID": 2, "context": "We represent this network as a type of probabilistic graphical model termed a synchronous graph dynamical system (GDS) [2, 3].", "startOffset": 119, "endOffset": 125}, {"referenceID": 3, "context": "We propose a solution based on the concept of transfer entropy, which is a measure that detects the directed information-theoretic dependency between random processes [4].", "startOffset": 167, "endOffset": 170}, {"referenceID": 4, "context": "The problem of inferring this coupling is an important multidisciplinary study in fields such as multi-agent systems [5, 6], ecology [7], neuroscience [8, 9], and various others studying artificial and biological systems [1].", "startOffset": 117, "endOffset": 123}, {"referenceID": 5, "context": "The problem of inferring this coupling is an important multidisciplinary study in fields such as multi-agent systems [5, 6], ecology [7], neuroscience [8, 9], and various others studying artificial and biological systems [1].", "startOffset": 117, "endOffset": 123}, {"referenceID": 6, "context": "The problem of inferring this coupling is an important multidisciplinary study in fields such as multi-agent systems [5, 6], ecology [7], neuroscience [8, 9], and various others studying artificial and biological systems [1].", "startOffset": 133, "endOffset": 136}, {"referenceID": 7, "context": "The problem of inferring this coupling is an important multidisciplinary study in fields such as multi-agent systems [5, 6], ecology [7], neuroscience [8, 9], and various others studying artificial and biological systems [1].", "startOffset": 151, "endOffset": 157}, {"referenceID": 8, "context": "The problem of inferring this coupling is an important multidisciplinary study in fields such as multi-agent systems [5, 6], ecology [7], neuroscience [8, 9], and various others studying artificial and biological systems [1].", "startOffset": 151, "endOffset": 157}, {"referenceID": 0, "context": "The problem of inferring this coupling is an important multidisciplinary study in fields such as multi-agent systems [5, 6], ecology [7], neuroscience [8, 9], and various others studying artificial and biological systems [1].", "startOffset": 221, "endOffset": 224}, {"referenceID": 9, "context": "Exact methods are known for fully observable systems [10], however, these are not applicable because the state variables in dynamical systems are latent.", "startOffset": 53, "endOffset": 57}, {"referenceID": 10, "context": "Such a measure can then be used to solve the two subproblems that comprise structure learning, evaluation and identification [11], and hence find the optimal model that explains the data.", "startOffset": 125, "endOffset": 129}, {"referenceID": 11, "context": "This concept is commonly expressed via information theory, where an established technique is to evaluate the encoding length of the data, given the model [12, 13, 14].", "startOffset": 154, "endOffset": 166}, {"referenceID": 12, "context": "This concept is commonly expressed via information theory, where an established technique is to evaluate the encoding length of the data, given the model [12, 13, 14].", "startOffset": 154, "endOffset": 166}, {"referenceID": 13, "context": "This concept is commonly expressed via information theory, where an established technique is to evaluate the encoding length of the data, given the model [12, 13, 14].", "startOffset": 154, "endOffset": 166}, {"referenceID": 14, "context": "The simplest model should aim to minimise code length [15], and therefore we can simplify our problem to that of minimising KL divergence for the synchronous GDS.", "startOffset": 54, "endOffset": 58}, {"referenceID": 15, "context": "We show that this measure can be decomposed as the difference between two well-known informationtheoretic measures, stochastic interaction [16] and collective transfer entropy [17].", "startOffset": 139, "endOffset": 143}, {"referenceID": 16, "context": "We show that this measure can be decomposed as the difference between two well-known informationtheoretic measures, stochastic interaction [16] and collective transfer entropy [17].", "startOffset": 176, "endOffset": 180}, {"referenceID": 17, "context": "We establish this result by first representing discrete-time multivariate dynamical systems as dynamic Bayesian networks (DBNs) [18].", "startOffset": 128, "endOffset": 132}, {"referenceID": 18, "context": "Interestingly, transfer entropy has already been used in practice for inferring effective networks [19] with encouraging empirical results.", "startOffset": 99, "endOffset": 103}, {"referenceID": 0, "context": "A complex network is a graph with non-trivial topological features that gives rise to emergent behaviour not typically seen in more traditional fields of graph theory [1].", "startOffset": 167, "endOffset": 170}, {"referenceID": 19, "context": "This concept was popularised by the seminal work of Watts and Strogatz [20] on small-world networks and Barab\u00e1si and Albert [21] on scale-free networks.", "startOffset": 71, "endOffset": 75}, {"referenceID": 20, "context": "This concept was popularised by the seminal work of Watts and Strogatz [20] on small-world networks and Barab\u00e1si and Albert [21] on scale-free networks.", "startOffset": 124, "endOffset": 128}, {"referenceID": 0, "context": "Since then, most of the complex network literature focuses on characterising the structure and dynamics of known biological, physical, and artificial networks [1].", "startOffset": 159, "endOffset": 162}, {"referenceID": 21, "context": "We instead focus on the structure learning problem, a general paradigm in machine learning where the goal is to infer relationships between the variables within a system [22].", "startOffset": 170, "endOffset": 174}, {"referenceID": 22, "context": "Besides complex networks, these types of systems have been introduced under a variety of more specific terms, such as spatially distributed dynamical systems [23, 9] and master-slave configurations [24].", "startOffset": 158, "endOffset": 165}, {"referenceID": 8, "context": "Besides complex networks, these types of systems have been introduced under a variety of more specific terms, such as spatially distributed dynamical systems [23, 9] and master-slave configurations [24].", "startOffset": 158, "endOffset": 165}, {"referenceID": 23, "context": "Besides complex networks, these types of systems have been introduced under a variety of more specific terms, such as spatially distributed dynamical systems [23, 9] and master-slave configurations [24].", "startOffset": 198, "endOffset": 202}, {"referenceID": 22, "context": "In this paper we use the discrete-time formulation, where a map is obtained numerically by integrating ODEs or recording observations at discrete-time intervals [23].", "startOffset": 161, "endOffset": 165}, {"referenceID": 24, "context": "when the experimenter can not intervene with the dataset [25].", "startOffset": 57, "endOffset": 61}, {"referenceID": 25, "context": "In early work, Kolmogorov [26] introduced the concept of classification of dynamical systems by information rates, leading to a generalisation of entropy of an information source [27].", "startOffset": 26, "endOffset": 30}, {"referenceID": 26, "context": "In early work, Kolmogorov [26] introduced the concept of classification of dynamical systems by information rates, leading to a generalisation of entropy of an information source [27].", "startOffset": 179, "endOffset": 183}, {"referenceID": 27, "context": "Following this, Granger [28] proposed Granger causality for quantifying the predictability of one variable from another.", "startOffset": 24, "endOffset": 28}, {"referenceID": 6, "context": "Although this measure has been used numerous times in identifying coupling, a limiting assumption of Granger causality is the key requirement of linearity, implying subsystems can be understood as individual parts [7].", "startOffset": 214, "endOffset": 217}, {"referenceID": 3, "context": "Schreiber [4] extended the ideas of Granger and introduced transfer entropy using the concept of finite-order Markov processes to quantify the information transfer between coupled nonlinear systems (although this idea was expressed earlier by Marko [29] as an information-theoretic interpretation of predictability).", "startOffset": 10, "endOffset": 13}, {"referenceID": 28, "context": "Schreiber [4] extended the ideas of Granger and introduced transfer entropy using the concept of finite-order Markov processes to quantify the information transfer between coupled nonlinear systems (although this idea was expressed earlier by Marko [29] as an information-theoretic interpretation of predictability).", "startOffset": 249, "endOffset": 253}, {"referenceID": 17, "context": ", Kalman models [18]), where transfer entropy and Granger causality are equivalent [30].", "startOffset": 16, "endOffset": 20}, {"referenceID": 29, "context": ", Kalman models [18]), where transfer entropy and Granger causality are equivalent [30].", "startOffset": 83, "endOffset": 87}, {"referenceID": 30, "context": ", the analysis in Lizier and Prokopenko [31] ).", "startOffset": 40, "endOffset": 44}, {"referenceID": 6, "context": "Recently, a number of measures have been proposed to infer coupling between distributed dynamical systems based on state space reconstruction theorems [7, 9, 3].", "startOffset": 151, "endOffset": 160}, {"referenceID": 8, "context": "Recently, a number of measures have been proposed to infer coupling between distributed dynamical systems based on state space reconstruction theorems [7, 9, 3].", "startOffset": 151, "endOffset": 160}, {"referenceID": 2, "context": "Recently, a number of measures have been proposed to infer coupling between distributed dynamical systems based on state space reconstruction theorems [7, 9, 3].", "startOffset": 151, "endOffset": 160}, {"referenceID": 6, "context": "[7] assumed Granger\u2019s definition of causality as a quantification of predictability and proposed a method labelled convergent cross-mapping (CCM).", "startOffset": 0, "endOffset": 3}, {"referenceID": 31, "context": "This history is the delay reconstruction map described by Takens\u2019 Delay Embedding Theorem [32].", "startOffset": 90, "endOffset": 94}, {"referenceID": 8, "context": "[9] used the Bundle Delay Embedding Theorem [33] infer causality and perform inference via Gaussian processes.", "startOffset": 0, "endOffset": 3}, {"referenceID": 32, "context": "[9] used the Bundle Delay Embedding Theorem [33] infer causality and perform inference via Gaussian processes.", "startOffset": 44, "endOffset": 48}, {"referenceID": 2, "context": "Finally, we recently presented similar work on deriving an information criterion for learning the structure of distributed dynamical systems [3].", "startOffset": 141, "endOffset": 144}, {"referenceID": 2, "context": "In this paper we extend this framework by proposing two scoring functions: one that is comparable to the information criterion presented in [3] in that it is applicable for discrete and linearly-coupled Gaussian variables; and another that allows for non-parametric density estimation techniques and thus make no assumptions about the underlying distributions.", "startOffset": 140, "endOffset": 143}, {"referenceID": 33, "context": "A related line of inquiry is recovering effective networks: networks that reveal the \u201ceffective structure\u201d of an observed system [34, 35].", "startOffset": 129, "endOffset": 137}, {"referenceID": 34, "context": "A related line of inquiry is recovering effective networks: networks that reveal the \u201ceffective structure\u201d of an observed system [34, 35].", "startOffset": 129, "endOffset": 137}, {"referenceID": 7, "context": ", in computational neuroscience [8, 19]; multi-agent systems [36, 5]; financial markets [37]; supply-chain networks [38]; and gene regulatory networks [39].", "startOffset": 32, "endOffset": 39}, {"referenceID": 18, "context": ", in computational neuroscience [8, 19]; multi-agent systems [36, 5]; financial markets [37]; supply-chain networks [38]; and gene regulatory networks [39].", "startOffset": 32, "endOffset": 39}, {"referenceID": 35, "context": ", in computational neuroscience [8, 19]; multi-agent systems [36, 5]; financial markets [37]; supply-chain networks [38]; and gene regulatory networks [39].", "startOffset": 61, "endOffset": 68}, {"referenceID": 4, "context": ", in computational neuroscience [8, 19]; multi-agent systems [36, 5]; financial markets [37]; supply-chain networks [38]; and gene regulatory networks [39].", "startOffset": 61, "endOffset": 68}, {"referenceID": 36, "context": ", in computational neuroscience [8, 19]; multi-agent systems [36, 5]; financial markets [37]; supply-chain networks [38]; and gene regulatory networks [39].", "startOffset": 88, "endOffset": 92}, {"referenceID": 37, "context": ", in computational neuroscience [8, 19]; multi-agent systems [36, 5]; financial markets [37]; supply-chain networks [38]; and gene regulatory networks [39].", "startOffset": 116, "endOffset": 120}, {"referenceID": 38, "context": ", in computational neuroscience [8, 19]; multi-agent systems [36, 5]; financial markets [37]; supply-chain networks [38]; and gene regulatory networks [39].", "startOffset": 151, "endOffset": 155}, {"referenceID": 3, "context": "Most of the results build on Schreiber\u2019s work [4] and assume the system to be composed of finite-order Markov chains; we extend this notion by showing that transfer entropy can also reveal the effective structure of distributed dynamical systems.", "startOffset": 46, "endOffset": 49}, {"referenceID": 2, "context": "In prior work [3], we have connected the log-likelihood ratio of a distributed dynamical system and transfer entropy.", "startOffset": 14, "endOffset": 17}, {"referenceID": 17, "context": "In order to evaluate the quality of a network structure, we adopt the framework of DBNs [18].", "startOffset": 88, "endOffset": 92}, {"referenceID": 10, "context": "In Bayesian network (BN) structure learning literature, there is an already mature research topic called the evaluation problem, which is aimed at deriving a measure that can be used to score candidate graphs, given a dataset [11].", "startOffset": 226, "endOffset": 230}, {"referenceID": 39, "context": "A number of mathematically sound techniques exist for the evaluation problem in a fully observed BN [40, 41, 42, 43], most of which can be readily extended to the DBN case [44].", "startOffset": 100, "endOffset": 116}, {"referenceID": 40, "context": "A number of mathematically sound techniques exist for the evaluation problem in a fully observed BN [40, 41, 42, 43], most of which can be readily extended to the DBN case [44].", "startOffset": 100, "endOffset": 116}, {"referenceID": 41, "context": "A number of mathematically sound techniques exist for the evaluation problem in a fully observed BN [40, 41, 42, 43], most of which can be readily extended to the DBN case [44].", "startOffset": 100, "endOffset": 116}, {"referenceID": 42, "context": "A number of mathematically sound techniques exist for the evaluation problem in a fully observed BN [40, 41, 42, 43], most of which can be readily extended to the DBN case [44].", "startOffset": 100, "endOffset": 116}, {"referenceID": 43, "context": "A number of mathematically sound techniques exist for the evaluation problem in a fully observed BN [40, 41, 42, 43], most of which can be readily extended to the DBN case [44].", "startOffset": 172, "endOffset": 176}, {"referenceID": 44, "context": "[45] and Binder et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 45, "context": "[46] use gradient descent to find parameters with possible hidden variables, and then extended their work to continuous nodes and DBNs.", "startOffset": 0, "endOffset": 4}, {"referenceID": 46, "context": "Kwoh and Gillies [47] use an ad hoc method to invent hidden nodes for unexplained data.", "startOffset": 17, "endOffset": 21}, {"referenceID": 47, "context": "[48] focused on solutions for cases specific to a sigmoid network with mixtures.", "startOffset": 0, "endOffset": 4}, {"referenceID": 48, "context": "[49] propose using the decomposability of the functions for efficient Monte Carlo methods that avoid this caveat.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "Interestingly, the analogous concept of maximising mutual information has been previously derived as a measure to recover fully observed BNs [15, 40, 50] and DBNs [51].", "startOffset": 141, "endOffset": 153}, {"referenceID": 39, "context": "Interestingly, the analogous concept of maximising mutual information has been previously derived as a measure to recover fully observed BNs [15, 40, 50] and DBNs [51].", "startOffset": 141, "endOffset": 153}, {"referenceID": 49, "context": "Interestingly, the analogous concept of maximising mutual information has been previously derived as a measure to recover fully observed BNs [15, 40, 50] and DBNs [51].", "startOffset": 141, "endOffset": 153}, {"referenceID": 50, "context": "Interestingly, the analogous concept of maximising mutual information has been previously derived as a measure to recover fully observed BNs [15, 40, 50] and DBNs [51].", "startOffset": 163, "endOffset": 167}, {"referenceID": 51, "context": "Given these processes, we can compute probability distributions of each variable by counting relative frequencies or by density estimation techniques [52, 53, 54].", "startOffset": 150, "endOffset": 162}, {"referenceID": 52, "context": "Given these processes, we can compute probability distributions of each variable by counting relative frequencies or by density estimation techniques [52, 53, 54].", "startOffset": 150, "endOffset": 162}, {"referenceID": 53, "context": "Given these processes, we can compute probability distributions of each variable by counting relative frequencies or by density estimation techniques [52, 53, 54].", "startOffset": 150, "endOffset": 162}, {"referenceID": 32, "context": "Furthermore, in many practical scenarios, we do not have access to xn directly, and can instead observe it through a measurement function \u03c8 :M\u2192 R that yields a scalar representation yn = \u03c8(xn) of the latent state [33, 23].", "startOffset": 213, "endOffset": 221}, {"referenceID": 22, "context": "Furthermore, in many practical scenarios, we do not have access to xn directly, and can instead observe it through a measurement function \u03c8 :M\u2192 R that yields a scalar representation yn = \u03c8(xn) of the latent state [33, 23].", "startOffset": 213, "endOffset": 221}, {"referenceID": 2, "context": "This definition is restated from [3] as follows.", "startOffset": 33, "endOffset": 36}, {"referenceID": 2, "context": "The global dynamics and observations can therefore be described by the set of local functions [3]:", "startOffset": 94, "endOffset": 97}, {"referenceID": 2, "context": "The discrete-time mapping for the dynamics (1) and measurement function (2) can be modelled as a DBN in order to facilitate structure learning of the graph [3].", "startOffset": 156, "endOffset": 159}, {"referenceID": 43, "context": ") by a prior BN and a two-time-slice BN (2TBN) [44].", "startOffset": 47, "endOffset": 51}, {"referenceID": 43, "context": "[44].", "startOffset": 0, "endOffset": 4}, {"referenceID": 2, "context": "To model the synchronous GDS as a DBN, we associate each subsystem vertex V i with a state variable X n and an observation variable Y i n; the parents of subsystem V i are denoted \u03a0G(V ) [3].", "startOffset": 187, "endOffset": 190}, {"referenceID": 2, "context": "Thus, we can build the edge set E in the GDS by means of the DBN [3], i.", "startOffset": 65, "endOffset": 68}, {"referenceID": 2, "context": "The distributions for the dynamics (1) and observation (2) maps of M arbitrary subsystems can therefore be factorised according to the DBN structure such that [3]", "startOffset": 159, "endOffset": 162}, {"referenceID": 21, "context": "We focus on the methods for learning the synchronous GDS structure using the score and search paradigm [22], which can be stated as: given a dataset D = (y1,y2, .", "startOffset": 103, "endOffset": 107}, {"referenceID": 10, "context": "Finding the optimal graph G\u2217 in (4) requires solutions to the two subproblems that comprise structure learning: the evaluation problem and the identification problem [11].", "startOffset": 166, "endOffset": 170}, {"referenceID": 43, "context": "The likelihood can be written in terms of distributions over network parameters [44] p(D | G) = \u222b p(D | G,\u0398)p(\u0398 | G)d\u0398.", "startOffset": 80, "endOffset": 84}, {"referenceID": 2, "context": "log-likelihood `(\u0398\u0302G : D), the model dimension (number of parameters) C(G), and the dataset size f(N), given by the general form [3]", "startOffset": 129, "endOffset": 132}, {"referenceID": 54, "context": "When f(N) = 1, we have the Akaike information criterion (AIC) score [55], f(N) = log(N)/2 is the Bayesian information criterion (BIC) score [13], and f(N) = 0 gives the maximum likelihood score.", "startOffset": 68, "endOffset": 72}, {"referenceID": 12, "context": "When f(N) = 1, we have the Akaike information criterion (AIC) score [55], f(N) = log(N)/2 is the Bayesian information criterion (BIC) score [13], and f(N) = 0 gives the maximum likelihood score.", "startOffset": 140, "endOffset": 144}, {"referenceID": 2, "context": "We have recently shown that state space reconstruction (see Appendix A) can be used to compute the log-likelihood of (3) as a difference of conditional entropy terms [3]:", "startOffset": 166, "endOffset": 169}, {"referenceID": 55, "context": "where H(Z |W ) is the entropy of variable Z conditioned on W [56],", "startOffset": 61, "endOffset": 65}, {"referenceID": 49, "context": "De Campos [50] proposes using the KL divergence as a natural information-theoretic approach to quantifying the similarity of these distributions for a BN.", "startOffset": 10, "endOffset": 14}, {"referenceID": 31, "context": "In this section we use state space reconstruction theorems based on Takens\u2019 seminal work [32] to obtain a tractable form of the conditional KL divergence (9).", "startOffset": 89, "endOffset": 93}, {"referenceID": 50, "context": "[51] applied the MIT algorithm [50] to DBN structure learning with complete data, however did not derive the results explicitly from conditional KL divergence.", "startOffset": 0, "endOffset": 4}, {"referenceID": 49, "context": "[51] applied the MIT algorithm [50] to DBN structure learning with complete data, however did not derive the results explicitly from conditional KL divergence.", "startOffset": 31, "endOffset": 35}, {"referenceID": 32, "context": "In order to compute the distributions in (9), we use the Bundle Delay Embedding Theorem [33] to reformulate the factorised distribution (denominator), and the Delay Embedding Theorem for Multivariate Observation Functions [57] for the joint distribution (numerator).", "startOffset": 88, "endOffset": 92}, {"referenceID": 56, "context": "In order to compute the distributions in (9), we use the Bundle Delay Embedding Theorem [33] to reformulate the factorised distribution (denominator), and the Delay Embedding Theorem for Multivariate Observation Functions [57] for the joint distribution (numerator).", "startOffset": 222, "endOffset": 226}, {"referenceID": 2, "context": "[3]).", "startOffset": 0, "endOffset": 3}, {"referenceID": 56, "context": "For convenience, Lemma 2 restates part of the delay embedding theorem in [57] in terms of subsystems of a synchronous GDS and establishes existence of a map G for predicting future observations from a history of observations.", "startOffset": 73, "endOffset": 77}, {"referenceID": 56, "context": "[57] in terms of subsystems.", "startOffset": 0, "endOffset": 4}, {"referenceID": 56, "context": "where \u03c4 i is the lag, \u03ba is the embedding dimension of the ith subsystem, and \u2211 i \u03ba i = 2d+ 1 [57].", "startOffset": 93, "endOffset": 97}, {"referenceID": 3, "context": "Transfer entropy detects the directed exchange of information between random processes by marginalising out common history and static correlations between variables; it is thus considered a measure of information transfer within a system [4].", "startOffset": 238, "endOffset": 241}, {"referenceID": 16, "context": "The collective transfer entropy computes the information transfer between a set of M source processes and a single destination process [17].", "startOffset": 135, "endOffset": 139}, {"referenceID": 15, "context": "Stochastic interaction measures the complexity of dynamical systems by quantifying the excess of information processed, in time, by the system beyond the information processed by each of the nodes [16, 58, 59].", "startOffset": 197, "endOffset": 209}, {"referenceID": 57, "context": "Stochastic interaction measures the complexity of dynamical systems by quantifying the excess of information processed, in time, by the system beyond the information processed by each of the nodes [16, 58, 59].", "startOffset": 197, "endOffset": 209}, {"referenceID": 58, "context": "Stochastic interaction measures the complexity of dynamical systems by quantifying the excess of information processed, in time, by the system beyond the information processed by each of the nodes [16, 58, 59].", "startOffset": 197, "endOffset": 209}, {"referenceID": 15, "context": "Note that the original definition assumed a first-order Markov process [16], and here we have extended stochastic interaction to arbitrary \u03ba-order Markov chains.", "startOffset": 71, "endOffset": 75}, {"referenceID": 14, "context": "A common method to derive a score is to minimise the KL divergence between graph and empirical distributions [15, 60].", "startOffset": 109, "endOffset": 117}, {"referenceID": 59, "context": "A common method to derive a score is to minimise the KL divergence between graph and empirical distributions [15, 60].", "startOffset": 109, "endOffset": 117}, {"referenceID": 49, "context": "Here, we draw on the result of de Campos [50], who derived a scoring function for BN structure learning based on conditional mutual information and statistical significance tests, called MIT (mutual information tests).", "startOffset": 41, "endOffset": 45}, {"referenceID": 60, "context": "1b was computed via a kernal box method (computed by the JIDT, see [61] for details).", "startOffset": 67, "endOffset": 71}, {"referenceID": 61, "context": "Fortunately, in the case of discrete and linear-Gaussian systems, the distribution 2NT\u3008Y \u3009j\u2192Y i is known to asymptotically approach the \u03c7-distribution [62].", "startOffset": 151, "endOffset": 155}, {"referenceID": 60, "context": "We can derive a more general form of the TEA score (32) via surrogate measurements T\u3008Y \u3009j\u2192Y i under the assumption of H0 [61].", "startOffset": 121, "endOffset": 125}, {"referenceID": 18, "context": "This same technique has been used by Lizier and Rubinov [19] to derive a greedy structure learning algorithm for effective network analysis.", "startOffset": 56, "endOffset": 60}, {"referenceID": 22, "context": "We can alternatively limit the number of surrogates Ns to d\u03b1/(1 \u2212 \u03b1)e and take the maximum as T\u03b1,si [23], however taking a larger number of surrogate Ns will improve the validity of the distribution T (s).", "startOffset": 100, "endOffset": 104}, {"referenceID": 21, "context": "It is therefore common to employ local search methods such as greedy hill climbing, basin flooding and tabu search [22].", "startOffset": 115, "endOffset": 119}, {"referenceID": 49, "context": "g(V ,\u03a0G(V ) : D) = g(V ,\u03a0G(V ) : N V ,\u03a0G(V i)), where N V ,\u03a0G(V i) are sufficient statistics for the set of variables V i \u222a\u03a0G(V ) in D [50].", "startOffset": 135, "endOffset": 139}, {"referenceID": 49, "context": "This approach is more efficient as it allows for caching the results of \u03c7\u03b1,{lij}j incrementally [50].", "startOffset": 96, "endOffset": 100}, {"referenceID": 49, "context": "This issue can be resolved by penalising the score conservatively by using the maximum permutation of the \u03c7\u03b1,{lij}j value; an in-depth explanation of this approach can be found in de Campos\u2019 [50] discussion of the maximum penalty permutation (Theorem 2) and Shur-concavity (Theorem 3) of the penalty term.", "startOffset": 191, "endOffset": 195}, {"referenceID": 10, "context": "Score-equivalence in BN structure learning simplifies the evaluation and identification problems by constraining the search space to a set of essential graphs, which is a set of equivalence classes over DAGs [11].", "startOffset": 208, "endOffset": 212}, {"referenceID": 49, "context": "Because TEA and TEE are specific cases of the MIT score [50], they are not scoreequivalent.", "startOffset": 56, "endOffset": 60}, {"referenceID": 62, "context": "However, they do satisfy the less demanding property of equivalence in the space of restricted partially directed acyclic graphs (RPDAGs) [63].", "startOffset": 138, "endOffset": 142}, {"referenceID": 62, "context": "With a decomposable scoring function, searching in the space of RPDAGs is more efficient than searching through essential graphs, and has been shown to yield better local optima than other local search techniques in practice [63].", "startOffset": 225, "endOffset": 229}, {"referenceID": 49, "context": "De Campos [50] showed previously that the KL", "startOffset": 10, "endOffset": 14}, {"referenceID": 63, "context": "divergence in a fully observable system is given by the difference between multi-information [64] and mutual information.", "startOffset": 93, "endOffset": 97}, {"referenceID": 56, "context": "4 Specifically, a condition for generalised Takens\u2019 theorems to hold is that the observation functions {\u03c8} are injective [57, 33].", "startOffset": 121, "endOffset": 129}, {"referenceID": 32, "context": "4 Specifically, a condition for generalised Takens\u2019 theorems to hold is that the observation functions {\u03c8} are injective [57, 33].", "startOffset": 121, "endOffset": 129}, {"referenceID": 33, "context": "For example, the notion of equivalence classes in BN structure learning should lend some insight into the area of effective network analysis [34, 35].", "startOffset": 141, "endOffset": 149}, {"referenceID": 34, "context": "For example, the notion of equivalence classes in BN structure learning should lend some insight into the area of effective network analysis [34, 35].", "startOffset": 141, "endOffset": 149}, {"referenceID": 49, "context": "We have presented the TEA (32) and TEE (33) scores above based on the MIT scoring function [50].", "startOffset": 91, "endOffset": 95}, {"referenceID": 64, "context": "There are numerous approaches to recover the time delay \u03c4 and embedding dimension \u03ba for use in transfer entropy [65, 66].", "startOffset": 112, "endOffset": 120}, {"referenceID": 65, "context": "There are numerous approaches to recover the time delay \u03c4 and embedding dimension \u03ba for use in transfer entropy [65, 66].", "startOffset": 112, "endOffset": 120}, {"referenceID": 31, "context": "In Takens seminal work on turbulent flow [32], he proposed a map \u03a6f,\u03c8 :M\u2192 R, that is composed of delayed observations, can be used to reconstruct the dynamics for typical (f, \u03c8).", "startOffset": 41, "endOffset": 45}, {"referenceID": 31, "context": "Theorem 5 (Delay Embedding Theorem for Diffeomorphisms [32]).", "startOffset": 55, "endOffset": 59}, {"referenceID": 32, "context": "An important consequence of this result is that we can define a map F = \u03a6f,\u03c8 \u25e6 f \u25e6 \u03a6\u22121 f,\u03c8 on \u03a6f,\u03c8, such that y (\u03ba) n+1 = F(y (\u03ba) n ) [33].", "startOffset": 134, "endOffset": 138}, {"referenceID": 49, "context": "The set of periodic points A of Although it is not derived in [50], it is trivial to show the first two terms constitute multi-information.", "startOffset": 62, "endOffset": 66}, {"referenceID": 66, "context": "Theorem 6 (Delay Embedding Theorem for Endomorphisms [67]).", "startOffset": 53, "endOffset": 57}, {"referenceID": 66, "context": "As a result of Theorem 6, a sequence of \u03ba successive measurements from a system determines the system state at the end of the sequence of measurements [67].", "startOffset": 151, "endOffset": 155}, {"referenceID": 8, "context": "However, encouraging empirical results in [9] support the conjecture that they can both be generalised to the case of endomorphisms by taking a negative time delay, as is done in Theorem 6 above.", "startOffset": 42, "endOffset": 45}, {"referenceID": 32, "context": "[33] and deals with a skew-product system.", "startOffset": 0, "endOffset": 4}, {"referenceID": 32, "context": "Theorem 7 (Bundle Delay Embedding Theorem [33]).", "startOffset": 42, "endOffset": 46}, {"referenceID": 6, "context": "Recently, Sugihara and Deyle [7] showed that multivariate mappings also form an embedding, with minor changes to the technical assumptions underlying Takens\u2019 original theorem.", "startOffset": 29, "endOffset": 32}, {"referenceID": 56, "context": "Theorem 8 (Delay Embedding Theorem for Multivariate Observation Functions [57]).", "startOffset": 74, "endOffset": 78}], "year": 2016, "abstractText": "In this work, we are interested in structure learning for a set of spatially distributed dynamical systems, where individual subsystems are coupled via latent variables and observed through a filter. We represent this model as a directed acyclic graph (DAG) that characterises the unidirectional coupling between subsystems. Standard approaches to structure learning are not applicable in this framework due to the hidden variables, however we can exploit the properties of certain dynamical systems to formulate exact methods based on state space reconstruction. We approach the problem by using reconstruction theorems to analytically derive a tractable expression for the KL-divergence of a candidate DAG from the observed dataset. We show this measure can be decomposed as a function of two informationtheoretic measures, transfer entropy and stochastic interaction. We then present two mathematically robust scoring functions based on transfer entropy and statistical independence tests. These results support the previously held conjecture that transfer entropy can be used to infer effective connectivity in complex networks.", "creator": "LaTeX with hyperref package"}}}