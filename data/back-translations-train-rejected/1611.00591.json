{"id": "1611.00591", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Sep-2016", "title": "Deep Neural Networks for HDR imaging", "abstract": "We propose novel methods of solving two tasks using Convolutional Neural Networks, firstly the task of generating HDR map of a static scene using differently exposed LDR images of the scene captured using conventional cameras and secondly the task of finding an optimal tone mapping operator that would give a better score on the TMQI metric compared to the existing methods. We quantitatively show the performance of our networks and illustrate the cases where our networks performs good as well as bad.", "histories": [["v1", "Sun, 4 Sep 2016 16:20:13 GMT  (3521kb,D)", "http://arxiv.org/abs/1611.00591v1", "9 pages"]], "COMMENTS": "9 pages", "reviews": [], "SUBJECTS": "cs.CV cs.LG cs.NE", "authors": ["kshiteej sheth"], "accepted": false, "id": "1611.00591"}, "pdf": {"name": "1611.00591.pdf", "metadata": {"source": "CRF", "title": "Indian Institute of Technology Gandhinagar Deep Neural Networks for HDR imaging", "authors": [], "emails": [], "sections": [{"heading": null, "text": "Indian Institute of Technology GandhinagarDeep Neural Networks for HDR imagingAuthor: Kshiteej ShethSupervisor: Dr. Shanmuganathan Raman3. November 2016 - - - - - - - - - - - - - - - - ar Xiv: 161 1.00 591v 1 [cs.C V] 4S ep2 016Contents1. Abstract2. Introduction 3. CNN Architecture4. Implementation details5. Results 6. References"}, {"heading": "1 Abstract", "text": "We propose novel methods to solve two tasks using Convolutionary Neural Networks: first, the task of generating a static scene using differently exposed LDR images of the scene taken with conventional cameras, and second, the task of finding an optimal tone mapping operator that would provide a better value of TMQI metrics compared to existing methods. We show quantitatively the performance of our networks and illustrate the cases where our networks perform well and poorly."}, {"heading": "2 Introduction", "text": "Natural scenes have a wide range of intensity values (i.e. a large dynamic range), and conventional non-HDR cameras cannot capture this range in a single image. By controlling various factors, one of which is the exposure time of the shot, we can capture a specific window in the entire dynamic range of the scene. Therefore, we need several \"low dynamic range\" images of the scene to get the full information about the scene. Fig.1 illustrates an example. The internal processing pipeline of the camera is highly non-linear, i.e. the pixel intensity value at the location (i, j), Zij is equal to f (Eij \u2206 t), where \"t\" is the exposure time of the shot, and Eij is the radiation value at the location (i, j). f (x) is known as the camera response function of that particular camera, which is a non-linear function. Given the values of Zij for differently exposed images of the same scene, we can obtain a rougher image (the pipeline)."}, {"heading": "3 CNN Architecture", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 LDR2HDR network architecture", "text": "We have collected a dataset of 957 HDR images from the Internet from the websites of various groups around the world working on HDR imaging, and the camera response functions of the cameras with which these photos were taken. We use these to generate LDR images from the HDR images, the exposure values of which are taken from a list of exposure times (a geometric progression with the first term 1, common ratio 4, and the last term equal 49. Our network takes as input a stack of 5 RGB LDR images of a particular scene, each of which has a different exposure time. In the initial experiments, we set the exposure of these LDR images to [1,8,64,512,4096], and then we proceeded to an adaptive exposure method in which we first select the LDR image of a scene giving the maximum value of entropy (entropy of a single channel image is calculated as the \"sum of all possible histograms\" in the \"sip\")."}, {"heading": "3.2 HDR2ToneMap network architecture", "text": "We first create a dataset with the existing 957 HDR images. Then, we use the sound imaging operators provided in the HDR toolbox by Francesco Banterle et al. and use them to create different sound imaging images for each HDR and to run the TMQI metric on each of the sound images, and select the one that yields the highest TMQI score. Some are local and others are global sound imaging operators, so our approach is not fully justified. We then train a revolutionary neural network whose input is the HDR image and whose corresponding truth is the best sound imaging that corresponds to the TMQI metric. We use 3x3 convectors in the first layer, followed by 1x1 convectors in the follow-up stories. We get this intuition from the works of Mertens et al. whose method yielded the best QTMI score for most of the time. In their work, the final intensity value at the DR location depends on it (as in the corresponding Hanzy imaging)."}, {"heading": "4 Implementation Details", "text": "For all data processing tasks we use MATLAB, and for the implementation and testing of our neural networks we use the Torch framework in the Lua scripting language, and most models are trained on a single NVIDIA GeForce GT 730 graphics processor, although for a short time we had access to an HPC node that had 2 GPUs, a Tesla K20C, and a Titan X. \u2022 Independently of processing two different batches on the two GPUs, we did multi-GPU training of our models with the following algorithm. \u2022 Have the same network on the 2 GPUs at the beginning of each iteration in an era. \u2022 Processing two different batches on the two GPUs and copying via the accumulated gradients on one of the GPUs to the other by adding them to the accumulated gradients on the others that we do not transfer to the other GPUs."}, {"heading": "5 Results", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 LDR2HDR Results", "text": "In the results, we present the graph of the training error versus the number of eras for the best three models (Fig. 3.) The test error for the best model is 0.09345. Visual results are shown below. It is clear that further experiments are needed to validate the hyperparameters in order to find the optimal architecture for the task. The network is clearly able to generate plausible results for some of the colors, but not for all (Fig. 3 and 4). Likewise, it is clear that the network is able to generate outputs without hyposaturation of regions that have high and low radiation values in the same image, proving that the dynamic range of output is quite high."}, {"heading": "5.2 HDR2Tonemap Results", "text": "In the results we present the results of the final validated architecture for cases where the network performs both good and bad (Fig. 7. and 8.) The final test error for this model after 28 training epochs is 0.002764 (Fig. 6.) We also present the representation of the training error versus the number of epochs for this model."}, {"heading": "6 References", "text": "1.) Debevec, Paul E., and Jitendra Malik. \"Recovery of Highly Dynamic Beam Cards from Photographs.\" ACM SIGGRAPH 2008 Classes. ACM, 2008. 2.) Mitsunaga, Tomoo, and Shree K. Nayar. \"Radiometric Self-Calibration.\" Computer Vision and Pattern Recognition, 1999. IEEE Computer Society Conference on.. Vol. 1. IEEE, 1999. 3.) Krizhevsky, Alex, Ilya Sutskever, and Geoffrey E. Hinton. \"Imagenet Classification with Deep Convolutionary Neural Networks.\" Advances in Neural Information Processing Systems. 2012. 4.) Krizhevsky, Alex, Ilya Sutskever, and Geoffrey E. Hinton. \"Imagenet Classification with Deep Convolutionary Neural Networks.\" Advances in Neural Information Processing Systems. 2012. 5.) Wang, Zhou, et al. \"Image Quality from 6- to 6- 6- Mega-4 Identity.\""}], "references": [], "referenceMentions": [], "year": 2016, "abstractText": null, "creator": "LaTeX with hyperref package"}}}