{"id": "1611.00714", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Nov-2016", "title": "Scalable Semi-Supervised Learning over Networks using Nonsmooth Convex Optimization", "abstract": "We propose a scalable method for semi-supervised (transductive) learning from massive network-structured datasets. Our approach to semi-supervised learning is based on representing the underlying hypothesis as a graph signal with small total variation. Requiring a small total variation of the graph signal representing the underlying hypothesis corresponds to the central smoothness assumption that forms the basis for semi-supervised learning, i.e., input points forming clusters have similar output values or labels. We formulate the learning problem as a nonsmooth convex optimization problem which we solve by appealing to Nesterovs optimal first-order method for nonsmooth optimization. We also provide a message passing formulation of the learning method which allows for a highly scalable implementation in big data frameworks.", "histories": [["v1", "Wed, 2 Nov 2016 18:27:53 GMT  (1572kb)", "http://arxiv.org/abs/1611.00714v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.DC", "authors": ["alexander jung", "alfred o hero iii", "alexandru mara", "sabeur aridhi"], "accepted": false, "id": "1611.00714"}, "pdf": {"name": "1611.00714.pdf", "metadata": {"source": "CRF", "title": "Scalable Semi-Supervised Learning over Networks using Nonsmooth Convex Optimization", "authors": ["Alexander Jung", "Alfred O. Hero III", "Alexandru Mara", "Sabeur Aridhi"], "emails": ["hero@eecs.umich.edu"], "sections": [{"heading": null, "text": "This year is the highest in the history of the country."}, {"heading": "Contributions and Outline", "text": "In Section II, we formulate semi-supervised learning using a graph model for the observed data as a convex optimization problem. By adapting Nesterov's nonsmooth convex optimization method, which is reviewed in Section III, we propose an efficient learning algorithm in Section IV. Subsequently, in Section V, we present a message-rich formulation of our learning algorithm that requires only a local update of the information."}, {"heading": "Notation", "text": "Matrices are denoted by uppercase letters in bold (e.g. A) and column vectors by lowercase letters in bold (e.g. x).The ith input of vector x is denoted by xi. and the input in the ith series and jth column of matrix A is Ai,. For vectors x, y-RN and matrices X, Y-RN \u00b7 N we define the inner products < x, y > 2: = x-xiyi and < X, Y > F: = \u2211 i, j Xi, jYi, j with induced norms. < x-x, x > X-F: = \u221a < X, X > F. For a generic Hilbert space H we denote its inner product by < \u00b7 \u00b7, j Xi, jYi, jYi, j with induced norms."}, {"heading": "II. PROBLEM FORMULATION", "text": "\"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"."}, {"heading": "III. OPTIMAL NONSMOOTH CONVEX OPTIMIZATION", "text": "We will now briefly return to a recently proposed method [19] for solving nonsmooth convex optimization problems, i.e. optimization problems with non-differentiable objective function, such as (7). This method utilizes a specific structure present in the problems (7). Specifically, this optimization method is based on (i) approaching a nonsmooth objective function through a smooth proxy and (ii) applying an optimal first-order (gradient-based) method to minimize this proxy.Consider a structured convex optimization problem of generic Formx, i.e., B: H1 \u2192 H2 is a linear operator of an end-dimensional Hilbert space H1 < u, Bx > H2 \u2212 g (u). (9) Here B: H1 \u2192 H2 is a linear operator of an end-dimensional Hilbert space H2 < Bx > Hu \u2212 g."}, {"heading": "Smooth Approximation of Nonsmooth Objective", "text": "To solve the non-smooth problem (9), we approach the non-differentiable problem (1) (2)."}, {"heading": "Optimal Gradient Method for Smooth Minimization", "text": "For the solution of the smooth optimization problem (17), which is a proxy for the original non-smooth problem (9), we apply an optimal method of first order (1) [1]. This method achieves the optimal worst-case rate of convergence between all gradient-based methods [18], [19]. We summarize this method for the solution (17) in Alg. 1, which input the smooth parameters of Alg. 1, an initial assumption x0 and a valid Lipschitz constant for the gradient (14), i.e., the satisfaction of L + 1 (1 / 2), which as input the smooth parameters of Alg. 1, one can monitor the relative decrease in the objective function f.x (x). [1, Sec. 3.5.] Another option used in our numerical experiments (cf."}, {"heading": "IV. EFFICIENT LEARNING OF GRAPH SIGNALS", "text": "We will now show that the semi-supervised learning problem (7) can be reformulated in the general form (9) (9), which will allow us to apply Alg. 1 to semi-supervised learning of big data, i.e., of high-dimensional heterogeneous data, over networks. To this end, we must introduce the graph gradient operator (7) as a mapping of the Hilbert space RN with internal product. < b > 2 = aTb into the Hilbert space RN \u00b7 N with internal product. < B > F = Tr {Tr {S} [13], [15]. In particular, the gradient operator (G)."}, {"heading": "V. MESSAGE PASSING FORMULATIONS", "text": "Steps 11-15 and 17-21 ofAlgorithm 3 Semi-Supervised Learning via Nesterov's MethodInput: dataset D with empirical graph G, subset S = {i1,. iM} of dataset D: dataset D with empirical graph G: empirical graph G: empirical graph G: empirix P\u00b5 (x) (cf.). Steps 11-15 and 17-21 ofAlgorithm 3 Semi-Supervised Learning via Nesterov's MethodInput: dataset D with empirical graph G: empirical graph G, subset S = {i1,."}, {"heading": "VI. NUMERICAL EXPERIMENTS", "text": "We judge the performance (of the accelerated version) of the proposed learning algorithm (i) i i (i) i (i) i (i) i (i) i (i) i (i) i (i) i (i) i (i) i (i) i (i) i (i) i (i) i) i (i) i (i) i (i) i (i) i (i) i) i (i) i (i) i) i (i) i (i) i) i (i) i (i) i) i (i) i (i) i i) i (i) i i (i) i i (i) i (i) i (i) i (i) i) i (i) i (i) i (i) i) i (i) i (i) (i) i (i) i) i (i) i (i) i (i) i (i) i (i) i (i) i (i) i (i) i (i) i) i (i) i (i) i (i) i (i) i (i) i) i (i) i (i) i) i (i) i (i) i) i (i) (i) i) i (i) (i) i) i (i) i (i) (i) i) i (i) (i) i) i (i) (i) i) i (i) i (i) i (i) (i) i) i (i) (i) (i) i) i (i) i) i (i) i) i (i) (i) i (i) i) i (i) i) i (i) i (i) i (i) (i) (i) i (i) (i) i) i (i) i (i) (i) (i) (i) (i) (i) (i) i) (i) (i) (i) (i) (i) i) (i) i) i (i) (i) i (i) (i) (i) (i) i) i (i) (i) i (i) i (i) (i) (i) (i)"}, {"heading": "VII. CONCLUSIONS", "text": "The problem of semi-supervised learning from massive data over networks was formulated as a non-smooth convex optimization problem based on punishing total label variation. We applied Nesterov's smoothing technique to this optimization problem to obtain an efficient learning algorithm that can capitalize on vast amounts of unlabeled data using only a few labeled data points. Furthermore, we proposed implementing the learning method as message delivery via the underlying data graph. This message delivery algorithm can be easily implemented in a big data platform such as AKKA to enable scalable learning algorithms. Future work includes expanding the optimization framework to adapt loss functions that differ from the mean angular error that better characterizes the training error for discreetly rated or categorized labels."}, {"heading": "APPENDIX A PROOF OF LEMMA IV.2", "text": "We only cite the derivative of (42), since the derivative of (45) is very similar. \u2212 k = k = k = k = k = k = k = k = k = k = k = k = c = c = c = c = c = c = c = c = c = c = c = c = c = c c = c c = c c = c = c = c c = c = c c = c = c = c = c = c = c = c = c = c = c c = c c = c c c = c c c = c c = c = c c c = c c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c = c c c c c = c = c = c c c c c = c c c c c c c = c c c c c c c c c c c c c c c c"}], "references": [{"title": "NESTA: a fast and accurate first-order method for sparse recovery", "author": ["S. Becker", "J. Bobin", "E.J. Cand\u00e8s"], "venue": "SIAM Journal on Imaging Sciences,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2011}, {"title": "Pattern Recognition and Machine Learning", "author": ["C.M. Bishop"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2006}, {"title": "Fastest mixing markov chain on a graph", "author": ["S. Boyd", "P. Diaconis", "L. Xiao"], "venue": "SIAM Review,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2004}, {"title": "Convex Optimization", "author": ["S. Boyd", "L. Vandenberghe"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2004}, {"title": "An algorithm for total variation minimization and applications", "author": ["A. Chambolle"], "venue": "Journal of Mathematical imaging and vision,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2004}, {"title": "Semi-Supervised Learning", "author": ["O. Chapelle", "B. Sch\u00f6lkopf", "A. Zien", "editors"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2006}, {"title": "Big Data over Networks", "author": ["S. Cui", "A. Hero", "Z.-Q. Luo", "J. Moura", "editors"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2016}, {"title": "What do we know about the metropolis algorithm", "author": ["P. Diaconis"], "venue": "Journal of Computer and System Sciences,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1998}, {"title": "High-dimensional data analysis: The curses and blessings of dimensionality", "author": ["D.L. Donoho"], "venue": "In Amer. Math. Soc. Lecture:\u201cMath challenges of the 21st century\u201d,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2000}, {"title": "Community detection", "author": ["S. Fortunato"], "venue": "in graphs. arXiv,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2009}, {"title": "Efficient graph signal recovery over big networks", "author": ["G. Hannak", "P. Berger", "G. Matz", "A. Jung"], "venue": "In Proc. Asilomar Conf. Signals, Sstems,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2016}, {"title": "First-order methods for nonsmooth convex large-scale optimization, I: General purpose methods", "author": ["A. Juditsky", "A. Nemirovski"], "venue": "Optimization for Machine Learning,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2011}, {"title": "Scalable graph signal recovery for big data over networks", "author": ["A. Jung", "P. Berger", "G. Hannak", "G. Matz"], "venue": "In 2016 IEEE 17th International Workshop on Signal Processing Advances in Wireless Communications (SPAWC),", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2016}, {"title": "Big data: The next frontier for innovation, competition, and productivity", "author": ["J. Mayika", "B. Brown", "J. Bughin", "R. Dobbs", "C. Roxburgh", "A.H. Byers"], "venue": "McKinsey Global Institute,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2011}, {"title": "Distributed decorrelation in sensor networks with application to distributed particle filtering", "author": ["M. Moldaschl", "W.N. Gansterer", "O. Hlinka", "F. Meyer", "F. Hlawatsch"], "venue": null, "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2014}, {"title": "Introductory lectures on convex optimization, volume 87 of Applied Optimization", "author": ["Y. Nesterov"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2004}, {"title": "Smooth minimization of non-smooth functions", "author": ["Y. Nesterov"], "venue": "Math. Program.,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2005}, {"title": "Networks: An Introduction", "author": ["M. Newman"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2010}, {"title": "Innovations diffusion: A spatial sampling scheme for distributed estimation and detection", "author": ["Z. Quan", "W.J. Kaiser", "A.H. Sayed"], "venue": "IEEE Trans. Signal Processing,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2009}, {"title": "The emerging field of signal processing on graphs: Extending high-dimensional data analysis to networks and other irregular domains", "author": ["D.I. Shuman", "S.K. Narang", "P. Frossard", "A. Ortega", "P. Vandergheynst"], "venue": "IEEE Signal Processing Magazine,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2013}, {"title": "Hadoop: The Definitive Guide", "author": ["T. White"], "venue": null, "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2009}, {"title": "Distributed covariance estimation in Gaussian graphical models", "author": ["A. Wiesel", "A.O. Hero"], "venue": "IEEE Trans. Signal Processing,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2012}, {"title": "Akka Concurrency", "author": ["D. Wyatt"], "venue": "Artima Incorporation,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2013}, {"title": "Distributed average consensus with least-mean-square deviation", "author": ["L. Xiao", "S. Boyd", "S.-J. Kim"], "venue": "Journal of Parallel and Distributed Computing,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2007}], "referenceMentions": [{"referenceID": 6, "context": ", \u201cBig Data\u201d [9], [11], [16], [23].", "startOffset": 13, "endOffset": 16}, {"referenceID": 8, "context": ", \u201cBig Data\u201d [9], [11], [16], [23].", "startOffset": 18, "endOffset": 22}, {"referenceID": 13, "context": ", \u201cBig Data\u201d [9], [11], [16], [23].", "startOffset": 24, "endOffset": 28}, {"referenceID": 20, "context": ", \u201cBig Data\u201d [9], [11], [16], [23].", "startOffset": 30, "endOffset": 34}, {"referenceID": 20, "context": ", there are statistical variations due to missing labels, labeling errors, or poor data curation [23].", "startOffset": 97, "endOffset": 101}, {"referenceID": 14, "context": "Quite often, these two notions of graph structure coincide: in a wireless sensor network, the graph modeling the communication links between nodes and the graph formed by statistical dependencies between sensor measurements resemble each other since both graphs are induced by the nodes mutual proximity [17], [21], [24].", "startOffset": 304, "endOffset": 308}, {"referenceID": 18, "context": "Quite often, these two notions of graph structure coincide: in a wireless sensor network, the graph modeling the communication links between nodes and the graph formed by statistical dependencies between sensor measurements resemble each other since both graphs are induced by the nodes mutual proximity [17], [21], [24].", "startOffset": 310, "endOffset": 314}, {"referenceID": 21, "context": "Quite often, these two notions of graph structure coincide: in a wireless sensor network, the graph modeling the communication links between nodes and the graph formed by statistical dependencies between sensor measurements resemble each other since both graphs are induced by the nodes mutual proximity [17], [21], [24].", "startOffset": 316, "endOffset": 320}, {"referenceID": 5, "context": "analogous to making the smoothness assumption of semi-supervised learning [7]: signals that are connected by an edge in the graph have similar labels.", "startOffset": 74, "endOffset": 77}, {"referenceID": 9, "context": "In other words, the graph signal is expected to reflect the underlying graph structure in the sense that the labels of signals on closely connected nodes have high mutual correlation and thus these signals form close-knit clusters or communities [12].", "startOffset": 246, "endOffset": 250}, {"referenceID": 5, "context": "This is the basis for many well-known label propagation methods [7].", "startOffset": 64, "endOffset": 67}, {"referenceID": 19, "context": "In contrast, the approach proposed in this paper is based on using (graph) total variation [22], which provides a more natural match between smoothness and the community structure of the data, i.", "startOffset": 91, "endOffset": 95}, {"referenceID": 17, "context": "An key parameter for the characterization of a graph is the maximum node degree [20]", "startOffset": 80, "endOffset": 84}, {"referenceID": 5, "context": "In order to learn the entire labeling x from the initial labels {yi}i\u2208S , we invoke the basic smoothness assumption for semi-supervised learning [7]: If two points z1, z2 are close, with respect to a given topology on the input space Z , then so should be the corresponding labels x1, x2, with respect to some distance measure on the label space R.", "startOffset": 145, "endOffset": 148}, {"referenceID": 19, "context": "Using this interpretation, we measure the smoothness of the labels via the (local) gradient \u2207ix at node i\u2208V , given as [22] ( \u2207ix )", "startOffset": 119, "endOffset": 123}, {"referenceID": 19, "context": "The (global) smoothness of the labels xi is then quantified by the total variation [22]: \u2016x\u2016TV := \u2211", "startOffset": 83, "endOffset": 87}, {"referenceID": 1, "context": "We highlight that the term \u201clabel\u201d is typically reserved for discrete-valued or categorial output variables xi [3].", "startOffset": 111, "endOffset": 114}, {"referenceID": 3, "context": "The convex optimization problems (7) and (8) are related by convex duality [2], [5]: For each choice for \u03b5 there is a choice for \u03bb (and vice-versa) such that the solutions of (7) and (8) coincide.", "startOffset": 80, "endOffset": 83}, {"referenceID": 0, "context": "However, the relation between \u03b5 and \u03bb for this equivalence to hold is non-trivial and determining the corresponding \u03bb for a given \u03b5 is as challenging as solving the problem (7) itself [1].", "startOffset": 184, "endOffset": 187}, {"referenceID": 17, "context": "Finally, for a dataset D whose empirical graph G is composed of several (weakly connected) components [20], the learning problem (7) decompose into independent subproblems, i.", "startOffset": 102, "endOffset": 106}, {"referenceID": 16, "context": "OPTIMAL NONSMOOTH CONVEX OPTIMIZATION We will now briefly review a recently proposed method [19] for solving nonsmooth convex optimization problems, i.", "startOffset": 92, "endOffset": 96}, {"referenceID": 0, "context": "Optimal Gradient Method for Smooth Minimization For solving the smooth optimization problem (17), being a proxy for the original nonsmooth problem (9), we apply an optimal first-order method [1], [19].", "startOffset": 191, "endOffset": 194}, {"referenceID": 16, "context": "Optimal Gradient Method for Smooth Minimization For solving the smooth optimization problem (17), being a proxy for the original nonsmooth problem (9), we apply an optimal first-order method [1], [19].", "startOffset": 196, "endOffset": 200}, {"referenceID": 15, "context": "This method achieves the optimal worst-case rate of convergence among all gradient based methods [18], [19].", "startOffset": 97, "endOffset": 101}, {"referenceID": 16, "context": "This method achieves the optimal worst-case rate of convergence among all gradient based methods [18], [19].", "startOffset": 103, "endOffset": 107}, {"referenceID": 11, "context": "This iteration complexity is essentially optimal for any first-order (sub-)gradient method solving problems of the form (9) [14].", "startOffset": 124, "endOffset": 128}, {"referenceID": 0, "context": "As discussed in [1], an effective approach to speed up the convergence of Alg.", "startOffset": 16, "endOffset": 19}, {"referenceID": 10, "context": "To this end, we need to introduce the graph gradient operator \u2207G as a mapping from the Hilbert space R endowed with inner product \u3008a,b\u30092=ab into the Hilbert space R endowed with inner product \u3008A,B\u3009F=Tr{AB} [13], [15].", "startOffset": 206, "endOffset": 210}, {"referenceID": 12, "context": "To this end, we need to introduce the graph gradient operator \u2207G as a mapping from the Hilbert space R endowed with inner product \u3008a,b\u30092=ab into the Hilbert space R endowed with inner product \u3008A,B\u3009F=Tr{AB} [13], [15].", "startOffset": 212, "endOffset": 216}, {"referenceID": 4, "context": "Moreover, the above definitions for the gradient and divergence operator over complex networks are straightforward generalizations of the well-known gradient and divergence operator for grid graphs representing 2D-images [6].", "startOffset": 221, "endOffset": 224}, {"referenceID": 3, "context": "By the KKT conditions for constrained convex optimization problems [5], [15],", "startOffset": 67, "endOffset": 70}, {"referenceID": 12, "context": "By the KKT conditions for constrained convex optimization problems [5], [15],", "startOffset": 72, "endOffset": 76}, {"referenceID": 23, "context": "5 implement a finite number K of iterations of the average consensus algorithm, using MetropolisHastings weights [26] , for (approximately) computing the sums (1/N) \u2211 j\u2208V bj = (1/N) \u2211 j\u2208S(yj \u2212 qj) and (1/N) \u2211 j\u2208V b\u0303j = (1/N) \u2211 j\u2208S(yj \u2212 q\u0303j), respectively.", "startOffset": 113, "endOffset": 117}, {"referenceID": 2, "context": "The choice for the number K of avarage consensus iterations can be guided by a wide range of results characterzing the convergence rate of average conensus [4], [10], [26].", "startOffset": 156, "endOffset": 159}, {"referenceID": 7, "context": "The choice for the number K of avarage consensus iterations can be guided by a wide range of results characterzing the convergence rate of average conensus [4], [10], [26].", "startOffset": 161, "endOffset": 165}, {"referenceID": 23, "context": "The choice for the number K of avarage consensus iterations can be guided by a wide range of results characterzing the convergence rate of average conensus [4], [10], [26].", "startOffset": 167, "endOffset": 171}, {"referenceID": 22, "context": "5 using the big data framework AKKA [25], which is a toolkit for building distributed and resilient message-driven applications.", "startOffset": 36, "endOffset": 40}, {"referenceID": 10, "context": "We then compared the runtime of the AKKA implementation to the centralized implementation in MATLAB used in [13].", "startOffset": 108, "endOffset": 112}], "year": 2016, "abstractText": "We propose a scalable method for semi-supervised (transductive) learning from massive network-structured datasets. Our approach to semi-supervised learning is based on representing the underlying hypothesis as a graph signal with small total variation. Requiring a small total variation of the graph signal representing the underlying hypothesis corresponds to the central smoothness assumption that forms the basis for semi-supervised learning, i.e., input points forming clusters have similar output values or labels. We formulate the learning problem as a nonsmooth convex optimization problem which we solve by appealing to Nesterov\u2019s optimal first-order method for nonsmooth optimization. We also provide a message passing formulation of the learning method which allows for a highly scalable implementation in big data frameworks.", "creator": "LaTeX with hyperref package"}}}