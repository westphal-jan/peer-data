{"id": "1611.00736", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Nov-2016", "title": "Extensions and Limitations of the Neural GPU", "abstract": "The Neural GPU is a recent model that can learn algorithms such as multi-digit binary addition and binary multiplication in a way that generalizes to inputs of arbitrary length. We show that there are two simple ways of improving the performance of the Neural GPU: by carefully designing a curriculum, and by increasing model size. The latter requires careful memory management, as a naive implementation of the Neural GPU is memory intensive. We find that these techniques to increase the set of algorithmic problems that can be solved by the Neural GPU: we have been able to learn to perform all the arithmetic operations (and generalize to arbitrarily long numbers) when the arguments are given in the decimal representation (which, surprisingly, has not been possible before). We have also been able to train the Neural GPU to evaluate long arithmetic expressions with multiple operands that require respecting the precedence order of the operands, although these have succeeded only in their binary representation, and not with 100\\% accuracy.", "histories": [["v1", "Wed, 2 Nov 2016 19:18:17 GMT  (1804kb,D)", "http://arxiv.org/abs/1611.00736v1", null], ["v2", "Fri, 4 Nov 2016 20:46:40 GMT  (2731kb,D)", "http://arxiv.org/abs/1611.00736v2", null]], "reviews": [], "SUBJECTS": "cs.NE cs.AI", "authors": ["eric price", "wojciech zaremba", "ilya sutskever"], "accepted": false, "id": "1611.00736"}, "pdf": {"name": "1611.00736.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Wojciech Zaremba", "Ilya Sutskever"], "emails": ["ecprice@cs.utexas.edu", "woj@openai.com", "ilyasu@openai.com"], "sections": [{"heading": "1 INTRODUCTION", "text": "This year it has come to the point where it is a reactionary party that is able to retaliate."}, {"heading": "2 RELATED WORK", "text": "The problem of learning algorithms from data has been studied in the field of program synthesis (Nordin, 1997; Liang et al., 2013; Wineberg & Oppacher, 1994; Solomonoff, 1964) and genetic programming (Holland, 1992; Goldberg, 1989; Gomez et al., 2008), approaches typically aimed at directly producing the source code of an algorithm that solves the problem specified by the training data. A more recent approach to algorithm learning seeks to harness the power of neural networks and their learning algorithms. Neural networks are inherently robust and capable of handling \"imprecise\" data (such as images or text) that can be circumvented in models that work directly with the source code. There are a number of neural network architectures that implement this idea: the Neural Turing Machine (NTM) (Graves et al al al al al al al al al al, 2014), Learning Grid with LM (STemaliba STemba, M 2014)."}, {"heading": "3 MODEL", "text": "In fact, it is the case that most of them are able to abide by the rules that they have imposed on themselves, and that they are able to abide by the rules that they have imposed on themselves. (...) In fact, it is the case that they are able to abide by the rules. (...) In fact, it is the case that they are able to abide by the rules. (...) In fact, it is the case that they are able to abide by the rules. (...) It is as if they are able to break the rules. \"(...)"}, {"heading": "4 IMPROVEMENTS TO THE NEURAL GPU", "text": "This year it is so far that it is a run for the green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green-green"}, {"heading": "5 GENERALIZATION", "text": "Addition is a well-defined algorithm, so knowledge of its mechanics is sufficient to add arbitrary numbers. However, our trained model generalizes perfectly to > 99% of uniformly random test cases (where a single-digit error in the whole 100-digit number is considered an error), but we have found that it fails in a much larger fraction of \"structured\" examples, for example, in cases where a number is carried over many steps, the probability that one bit will fail k times, in uniformly random binary input, is 2 \u2212 k. Training and testing on random examples will therefore not notice the failure of more than 20 bits. We find that the model fails in test cases requiring more than 20 steps of transfer, as in Fig. 6.Addition with long tracks. We observe similar generalization problems with multiplication with multiplication."}, {"heading": "6 GLOBAL OPERATION", "text": "The Neural GPU is a cellular automaton that is a Turing complete computational model (Chopard & Droz, 1998; Wolfram, 1984).However, the automaton is often mathematically inefficient compared to the von Neumann architecture.It is difficult for a cellular automaton to move data globally because the entirety of its computation works locally at every step.We wanted to understand the importance of global data movement for solving algorithmic tasks.The Neural GPU could be made more powerful by adding a global operation to each of its computational steps. We briefly tried to use attention that shifts data, but we were unable to improve empirical outcomes."}, {"heading": "7 CONCLUSION", "text": "In this paper, we investigated the generalizability of the Neural GPU. We discovered that larger Neural GPUs generalize better, and provided examples of curricula that allowed the Neural GPU to solve tasks that it previously could not solve. Finally, we showed that its generalization is incomplete, because while it has been successfully generalized to longer inputs, there are still highly structured test cases that lead to the model's failure. It is desirable to develop learning methods that can be used to learn algorithms that achieve perfect generalization. One way to make progress is to investigate how the model can benefit from additional sources of information that are not present in the task itself."}, {"heading": "8 ACKNOWLEDGMENT", "text": "We would like to thank Rafal Jozefowicz for useful discussions and comments."}], "references": [{"title": "Tensorflow: Large-scale machine learning on heterogeneous systems, 2015", "author": ["Abadi", "Mart\u0131n"], "venue": "Software available from tensorflow. org,", "citeRegEx": "Abadi and Mart\u0131n,? \\Q2015\\E", "shortCiteRegEx": "Abadi and Mart\u0131n", "year": 2015}, {"title": "Learning efficient algorithms with hierarchical attentive memory", "author": ["Andrychowicz", "Marcin", "Kurach", "Karol"], "venue": "arXiv preprint arXiv:1602.03218,", "citeRegEx": "Andrychowicz et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Andrychowicz et al\\.", "year": 2016}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Bahdanau", "Dzmitry", "Cho", "Kyunghyun", "Bengio", "Yoshua"], "venue": "In ICLR 2015,", "citeRegEx": "Bahdanau et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Curriculum learning", "author": ["Bengio", "Yoshua", "Louradour", "J\u00e9r\u00f4me", "Collobert", "Ronan", "Weston", "Jason"], "venue": "In Proceedings of the 26th annual international conference on machine learning,", "citeRegEx": "Bengio et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2009}, {"title": "Genetic Algorithms in Search, Optimization and Machine Learning", "author": ["Goldberg", "David E"], "venue": "AddisonWesley Longman Publishing Co., Inc.,", "citeRegEx": "Goldberg and E.,? \\Q1989\\E", "shortCiteRegEx": "Goldberg and E.", "year": 1989}, {"title": "Accelerated neural evolution through cooperatively coevolved synapses", "author": ["Gomez", "Faustino", "Schmidhuber", "J\u00fcrgen", "Miikkulainen", "Risto"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Gomez et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Gomez et al\\.", "year": 2008}, {"title": "Adaptive computation time for recurrent neural networks", "author": ["Graves", "Alex"], "venue": "arXiv preprint arXiv:1603.08983,", "citeRegEx": "Graves and Alex.,? \\Q2016\\E", "shortCiteRegEx": "Graves and Alex.", "year": 2016}, {"title": "Neural turing machines", "author": ["Graves", "Alex", "Wayne", "Greg", "Danihelka", "Ivo"], "venue": "arXiv preprint arXiv:1410.5401,", "citeRegEx": "Graves et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2014}, {"title": "Learning to transduce with unbounded memory", "author": ["Grefenstette", "Edward", "Hermann", "Karl Moritz", "Suleyman", "Mustafa", "Blunsom", "Phil"], "venue": "arXiv preprint arXiv:1506.02516,", "citeRegEx": "Grefenstette et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Grefenstette et al\\.", "year": 2015}, {"title": "Memoryefficient backpropagation through time", "author": ["Gruslys", "Audr\u016bnas", "Munos", "Remi", "Danihelka", "Ivo", "Lanctot", "Marc", "Graves", "Alex"], "venue": "arXiv preprint arXiv:1606.03401,", "citeRegEx": "Gruslys et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Gruslys et al\\.", "year": 2016}, {"title": "Adaptation in Natural and Artificial Systems: An Introductory Analysis with Applications to Biology, Control and Artificial Intelligence", "author": ["Holland", "John H"], "venue": null, "citeRegEx": "Holland and H.,? \\Q1992\\E", "shortCiteRegEx": "Holland and H.", "year": 1992}, {"title": "Inferring algorithmic patterns with stack-augmented recurrent nets", "author": ["Joulin", "Armand", "Mikolov", "Tomas"], "venue": "arXiv preprint arXiv:1503.01007,", "citeRegEx": "Joulin et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Joulin et al\\.", "year": 2015}, {"title": "Neural gpus learn algorithms", "author": ["Kaiser", "\u0141ukasz", "Sutskever", "Ilya"], "venue": "arXiv preprint arXiv:1511.08228,", "citeRegEx": "Kaiser et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kaiser et al\\.", "year": 2015}, {"title": "Grid long short-term memory", "author": ["Kalchbrenner", "Nal", "Danihelka", "Ivo", "Graves", "Alex"], "venue": "arXiv preprint arXiv:1507.01526,", "citeRegEx": "Kalchbrenner et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2015}, {"title": "Neural random-access machines", "author": ["Kurach", "Karol", "Andrychowicz", "Marcin", "Sutskever", "Ilya"], "venue": "arXiv preprint arXiv:1511.06392,", "citeRegEx": "Kurach et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kurach et al\\.", "year": 2015}, {"title": "Learning dependency-based compositional semantics", "author": ["Liang", "Percy", "Jordan", "Michael I", "Klein", "Dan"], "venue": "Computational Linguistics,", "citeRegEx": "Liang et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Liang et al\\.", "year": 2013}, {"title": "Training deep and recurrent networks with hessian-free optimization", "author": ["Martens", "James", "Sutskever", "Ilya"], "venue": "In Neural networks: Tricks of the trade,", "citeRegEx": "Martens et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Martens et al\\.", "year": 2012}, {"title": "Evolutionary program induction of binary machine code and its applications", "author": ["Nordin", "Peter"], "venue": "Krehl Munster,", "citeRegEx": "Nordin and Peter.,? \\Q1997\\E", "shortCiteRegEx": "Nordin and Peter.", "year": 1997}, {"title": "Overfeat: Integrated recognition, localization and detection using convolutional networks", "author": ["Sermanet", "Pierre", "Eigen", "David", "Zhang", "Xiang", "Mathieu", "Micha\u00ebl", "Fergus", "Rob", "LeCun", "Yann"], "venue": "arXiv preprint arXiv:1312.6229,", "citeRegEx": "Sermanet et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Sermanet et al\\.", "year": 2013}, {"title": "A formal theory of inductive inference", "author": ["Solomonoff", "Ray J"], "venue": "Part I. Information and control,", "citeRegEx": "Solomonoff and J.,? \\Q1964\\E", "shortCiteRegEx": "Solomonoff and J.", "year": 1964}, {"title": "Dropout: a simple way to prevent neural networks from overfitting", "author": ["Srivastava", "Nitish", "Hinton", "Geoffrey E", "Krizhevsky", "Alex", "Sutskever", "Ilya", "Salakhutdinov", "Ruslan"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Srivastava et al\\.,? \\Q1929\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 1929}, {"title": "Weakly supervised memory networks", "author": ["Sukhbaatar", "Sainbayar", "Szlam", "Arthur", "Weston", "Jason", "Fergus", "Rob"], "venue": "arXiv preprint arXiv:1503.08895,", "citeRegEx": "Sukhbaatar et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sukhbaatar et al\\.", "year": 2015}, {"title": "The nature of statistical learning theory", "author": ["Vapnik", "Vladimir"], "venue": "Springer Science & Business Media,", "citeRegEx": "Vapnik and Vladimir.,? \\Q2013\\E", "shortCiteRegEx": "Vapnik and Vladimir.", "year": 2013}, {"title": "A representation scheme to perform program induction in a canonical genetic algorithm. In Parallel Problem Solving from NaturePPSN", "author": ["Wineberg", "Mark", "Oppacher", "Franz"], "venue": null, "citeRegEx": "Wineberg et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Wineberg et al\\.", "year": 1994}, {"title": "Cellular automata as models of complexity", "author": ["Wolfram", "Stephen"], "venue": "Nature, 311(5985):419\u2013424,", "citeRegEx": "Wolfram and Stephen.,? \\Q1984\\E", "shortCiteRegEx": "Wolfram and Stephen.", "year": 1984}, {"title": "Learning to execute", "author": ["Zaremba", "Wojciech", "Sutskever", "Ilya"], "venue": "arXiv preprint arXiv:1410.4615,", "citeRegEx": "Zaremba et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zaremba et al\\.", "year": 2014}, {"title": "Reinforcement learning neural turing machines", "author": ["Zaremba", "Wojciech", "Sutskever", "Ilya"], "venue": "arXiv preprint arXiv:1505.00521,", "citeRegEx": "Zaremba et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zaremba et al\\.", "year": 2015}, {"title": "Learning simple algorithms from examples", "author": ["Zaremba", "Wojciech", "Mikolov", "Tomas", "Joulin", "Armand", "Fergus", "Rob"], "venue": "arXiv preprint arXiv:1511.07275,", "citeRegEx": "Zaremba et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zaremba et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 15, "context": "This problem lies at the core of program induction, an old field that has significant past work (Nordin, 1997; Liang et al., 2013; Wineberg & Oppacher, 1994; Solomonoff, 1964; Holland, 1992; Goldberg, 1989; Gomez et al., 2008).", "startOffset": 96, "endOffset": 226}, {"referenceID": 5, "context": "This problem lies at the core of program induction, an old field that has significant past work (Nordin, 1997; Liang et al., 2013; Wineberg & Oppacher, 1994; Solomonoff, 1964; Holland, 1992; Goldberg, 1989; Gomez et al., 2008).", "startOffset": 96, "endOffset": 226}, {"referenceID": 7, "context": "More recently, researchers have begun investigating this problem using the deep learning techniques of neural network function approximation and stochastic gradient descent (Graves et al., 2014; Zaremba & Sutskever, 2015; Kaiser & Sutskever, 2015; Kurach et al., 2015; Andrychowicz & Kurach, 2016).", "startOffset": 173, "endOffset": 297}, {"referenceID": 14, "context": "More recently, researchers have begun investigating this problem using the deep learning techniques of neural network function approximation and stochastic gradient descent (Graves et al., 2014; Zaremba & Sutskever, 2015; Kaiser & Sutskever, 2015; Kurach et al., 2015; Andrychowicz & Kurach, 2016).", "startOffset": 173, "endOffset": 297}, {"referenceID": 15, "context": "The problem of learning algorithms from data has been investigated in the field of program synthesis (Nordin, 1997; Liang et al., 2013; Wineberg & Oppacher, 1994; Solomonoff, 1964) and genetic programming (Holland, 1992; Goldberg, 1989; Gomez et al.", "startOffset": 101, "endOffset": 180}, {"referenceID": 5, "context": ", 2013; Wineberg & Oppacher, 1994; Solomonoff, 1964) and genetic programming (Holland, 1992; Goldberg, 1989; Gomez et al., 2008).", "startOffset": 77, "endOffset": 128}, {"referenceID": 7, "context": "There exist a number of neural network architecture that implement this idea: the Neural Turing Machine (NTM) (Graves et al., 2014), learning programs with LSTM (Zaremba & Sutskever, 2014), grid LSTM (Kalchbrenner et al.", "startOffset": 110, "endOffset": 131}, {"referenceID": 13, "context": ", 2014), learning programs with LSTM (Zaremba & Sutskever, 2014), grid LSTM (Kalchbrenner et al., 2015), the Stack RNN (Joulin & Mikolov, 2015), the Neural DeQue (Grefenstette et al.", "startOffset": 76, "endOffset": 103}, {"referenceID": 8, "context": ", 2015), the Stack RNN (Joulin & Mikolov, 2015), the Neural DeQue (Grefenstette et al., 2015), End-to-End Memory Networks (Sukhbaatar et al.", "startOffset": 66, "endOffset": 93}, {"referenceID": 21, "context": ", 2015), End-to-End Memory Networks (Sukhbaatar et al., 2015), Hierarchical Attentive Memory (Andrychowicz & Kurach, 2016), Neural random-access machines (Kurach et al.", "startOffset": 36, "endOffset": 61}, {"referenceID": 14, "context": ", 2015), Hierarchical Attentive Memory (Andrychowicz & Kurach, 2016), Neural random-access machines (Kurach et al., 2015).", "startOffset": 100, "endOffset": 121}, {"referenceID": 26, "context": "While some models (Zaremba & Sutskever, 2015; Zaremba et al., 2015; Graves, 2016) can in principle learn the correct runtime for a given algorithm, in practice it has not been possible to learn algorithms requiring superlinear runtime, such as integer multiplication.", "startOffset": 18, "endOffset": 81}, {"referenceID": 13, "context": "The Grid LSTM (Kalchbrenner et al., 2015) is a powerful architecture that can learn 15-digit decimal addition.", "startOffset": 14, "endOffset": 41}, {"referenceID": 3, "context": "In particular, all the aforementioned models require a curriculum learning (Bengio et al., 2009; Zaremba & Sutskever, 2014) in order to successfully learn sophisticated functions, and the results in this paper are no different.", "startOffset": 75, "endOffset": 123}, {"referenceID": 18, "context": "This approach is sometimes used in object detection, where the same convolutional neural network is applied on images of variable size (Sermanet et al., 2013).", "startOffset": 135, "endOffset": 158}, {"referenceID": 13, "context": "The architecture most similar to Neural GPU is the Grid LSTM (Kalchbrenner et al., 2015).", "startOffset": 61, "endOffset": 88}, {"referenceID": 2, "context": "To successfully train the Neural GPU, Kaiser & Sutskever (2015) used the following techniques: \u2022 The architecture is that of a gated recurrent unit (GRU) through depth (Bahdanau et al., 2014).", "startOffset": 168, "endOffset": 191}, {"referenceID": 12, "context": "The architecture most similar to Neural GPU is the Grid LSTM (Kalchbrenner et al., 2015). It has been shown to learn 15 digit long decimal addition task, although it has not yet been shown to generalize to inputs of length greater than the training data. To successfully train the Neural GPU, Kaiser & Sutskever (2015) used the following techniques: \u2022 The architecture is that of a gated recurrent unit (GRU) through depth (Bahdanau et al.", "startOffset": 62, "endOffset": 319}, {"referenceID": 2, "context": "To successfully train the Neural GPU, Kaiser & Sutskever (2015) used the following techniques: \u2022 The architecture is that of a gated recurrent unit (GRU) through depth (Bahdanau et al., 2014). \u2022 Tanh cutoff: the hyperbolic tangent activations used by the GRU are truncated once they reach a critical upper (and lower) bound. The hope is that this makes the results more \u201cdigital\u201d. \u2022 The use of Dropout Srivastava et al. (2014). \u2022 Instead of using the same weights at every pair of layers, the Neural GPU starts out by cycling through 6 independent sets of weights, which are gradually annealed to become identical as optimization progresses.", "startOffset": 169, "endOffset": 427}, {"referenceID": 9, "context": "A third way would be to use the methods described by Martens & Sutskever (2012); Gruslys et al. (2016), however we have not experimented with them.", "startOffset": 81, "endOffset": 103}], "year": 2016, "abstractText": "The Neural GPU is a recent model that can learn algorithms such as multi-digit binary addition and binary multiplication in a way that generalizes to inputs of arbitrary length. We show that there are two simple ways of improving the performance of the Neural GPU: by carefully designing a curriculum, and by increasing model size. The latter requires careful memory management, as a naive implementation of the Neural GPU is memory intensive. We find that these techniques to increase the set of algorithmic problems that can be solved by the Neural GPU: we have been able to learn to perform all the arithmetic operations (and generalize to arbitrarily long numbers) when the arguments are given in the decimal representation (which, surprisingly, has not been possible before). We have also been able to train the Neural GPU to evaluate long arithmetic expressions with multiple operands that require respecting the precedence order of the operands, although these have succeeded only in their binary representation, and not with 100% accuracy. In addition, we attempt to gain insight into the Neural GPU by understanding its failure modes. We find that Neural GPUs that correctly generalize to arbitrarily long numbers still fail to compute the correct answer on highly-symmetric, atypical inputs: for example, a Neural GPU that achieves near-perfect generalization on decimal multiplication of up to 100-digit long numbers can fail on 000000 . . . 002\u00d7 000000 . . . 002 while succeeding at 2\u00d7 2. These failure modes are reminiscent of adversarial examples.", "creator": "LaTeX with hyperref package"}}}