{"id": "1611.00740", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Nov-2016", "title": "Why and When Can Deep -- but Not Shallow -- Networks Avoid the Curse of Dimensionality: a Review", "abstract": "The paper reviews an emerging body of theoretical results on deep learning including the conditions under which it can be exponentially better than shallow learning. Deep convolutional networks represent an important special case of these conditions, though weight sharing is not the main reason for their exponential advantage. Explanation of a few key theorems is provided together with new results, open problems and conjectures.", "histories": [["v1", "Wed, 2 Nov 2016 19:35:52 GMT  (1453kb,D)", "http://arxiv.org/abs/1611.00740v1", null], ["v2", "Tue, 22 Nov 2016 06:15:39 GMT  (1982kb,D)", "http://arxiv.org/abs/1611.00740v2", null], ["v3", "Tue, 29 Nov 2016 21:24:07 GMT  (1983kb,D)", "http://arxiv.org/abs/1611.00740v3", null], ["v4", "Wed, 25 Jan 2017 01:09:40 GMT  (1983kb,D)", "http://arxiv.org/abs/1611.00740v4", null], ["v5", "Sat, 4 Feb 2017 09:10:41 GMT  (2894kb,D)", "http://arxiv.org/abs/1611.00740v5", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["tomaso poggio", "hrushikesh mhaskar", "lorenzo rosasco", "brando miranda", "qianli liao"], "accepted": false, "id": "1611.00740"}, "pdf": {"name": "1611.00740.pdf", "metadata": {"source": "CRF", "title": "Why and When Can Deep \u2013 but Not Shallow \u2013 Networks Avoid the Curse of Dimensionality", "authors": ["Tomaso Poggio", "Hrushikesh Mhaskar", "Lorenzo Rosasco", "Brando Miranda", "Qianli Liao"], "emails": [], "sections": [{"heading": null, "text": "This work was supported by the Center for Brains, Minds and Machines (CBMM), which is funded by the NSF STC award CCF - 1231216. H.M. is partially funded by ARO Grant W911NF-15-10385.1ar Xiv: 1Why and When Can Deep - but Not Shallow - Networks Avoid the Curse of DimensionalityTomaso Poggio1 Hrushikesh Mhaskar2 Lorenzo Rosasco1 Brando Miranda1 Qianli Liao1 1Center for Brains, Minds, and Machines, McGovern Institute for Brain Research, Massachusetts Institute of Technology, Cambridge, MA, 02139.2Department of Mathematics, California Institute of Technology, Pasadena, CA 91125; Institute of Mathematical Sciences, Claremont Graduate University, Claremont, CA 91711Abstract: The Paper reviews an emerging corpus of theoretical results on deep learning, including conditions under which particular advantages may not be decisive."}, {"heading": "1 A theory of deep learning", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "1.1 Introduction", "text": "There are at least three main groups of theoretical questions about deep neural networks: the first group of questions revolves around the power of architecture - what classes of functions can it approach and learn well? The second group of questions revolves around the learning process: Why is SGD (Stochastic Gradient Descent) so inappropriately efficient, at least outwardly? Are there good local minimums in deep networks? Are they easier to find in deep networks than in flat networks? The third group of questions concerns the generalization properties of deep learning, as deep networks seem to suffer much less from revision than classical flat networks."}, {"heading": "1.2 Relevance for hackers", "text": "In this paper, we summarize our most recent results, which have been published in several online papers in recent months (CBMM memos 35, 37, 41, 45, 54 [Anselmi et al., 2015b], [Poggio et al., 2015c], [Poggio et al., 2015a], [Mhaskar et al., 2016], [Mhaskar and Poggio, 2016]) and the answers to why and when deep networks are better than flat. Then, we describe several main theorems, as well as some assumptions and open questions. The most important implications that might be relevant in practice are: 1. Among the deep learning architectures used in practice, xxx2 xh2 xh2 are for all practical purposes only deep revolutionary architectures - but not tightly connected deep networks - guaranteed to be much better than a layer architecture such as core machines (for specific problems, see next); 2. the problems where deep networks are exponentially better than other hierarchical \u00b7 xh3, xh5, constitutional (1) (constitutional), xxxx5, constitutional, (5)."}, {"heading": "2 Previous theoretical work", "text": "In fact, it is the case that most people who are able are able to determine for themselves what they want and what they do not want. (...) In fact, it is the case that people are able to determine for themselves what they want and what they do not want. (...) In fact, it is the case that people are able to determine for themselves what they want. (...) It is not the case that people are able to determine for themselves what they want. (...) It is the case that people are able to determine for themselves what they want. (...) It is the case that people are able to determine for themselves what they want. (...) It is as if people are able to determine for themselves what they want. (...) It is as if they want, as if they want to do it, as if they want to do it. (...)"}, {"heading": "3 Main results", "text": "In this section we present theorems about the approximation properties of flat and deep networks."}, {"heading": "3.1 Degree of approximation", "text": "The general paradigm is as follows: We are interested in determining how complex a network should be in order to theoretically guarantee the approximation of an unknown target function f to a given accuracy > 0. To measure accuracy, we need a norm for a normalized linear space X. As we will see, the norm used in the results of this work is the sup norm in accordance with the standard selection in approximation theory. Note, however, that from the point of view of machine learning, the relevant norm is the L2 norm. In this sense, several of our results are stronger than necessary. In our main result on compositionality, on the other hand, we require the sup norm in order to be immediately useful in a context of machine learning. Let VN be the norm for all networks of a given type with complexity N, which we consider here as the total number of units in the network (e.g. all flat networks with N units in the class) assuming that the complexity of the network is sufficient."}, {"heading": "3.2 Previous main results", "text": "These are the conditions under which deep networks are \"better\" than flat networks in approximate functions. Thus, we compare flat (a hidden layer) networks with deep networks as shown in Figure 1. Both types of networks use the same small number of operations - dot products, linear combinations, a fixed nonlinear function of a variable, possibly convolution and pooling. Specifically, each node in the network contains a certain number of units. A unit is a neuron that computes such nodes. < x, w > + b, (2) where the vector of weights on the vector is input x. Both t and the real number b are parameters that are tuned by learning. \u2022 Both are linear combinations of such nodes that i = 1 ci (< x, ti > + bi) + correspond. The logic of our theories is as follows. \u2022 Both shallow (a) and deep (b) networks are universal."}, {"heading": "3.3 Extension: functions composed by a hierarchy of functions with bounded effective dimensionality", "text": "This year, it is more than ever before in the history of the city."}, {"heading": "3.4 Approximation results for shallow and deep networks with standard, non-smooth ReLUs", "text": "The results we have described so far use smooth activation functions. We have already mentioned why a loosening of the smoothing assumption should not fundamentally change our results. While studies on the properties of smooth activation neural networks are plentiful, the results on non-smooth activation functions may be much more sparse. In fact, in the latter case, as we have already noted, estimates relating to the L2 standard are necessary to control the propagation of errors from one layer to the next, see theorem 2. Results in this direction are not applicable to deduplicate boundaries for compositional networks. Indeed, as we have already noted, estimates in the single standard are required to control the propagation of errors from one layer to the next, see theorem 2. Results in this direction are in [Mhaskar, 2004], and more recently in [Bach, 2014] and [Mhaskar and Poggio, 2016]."}, {"heading": "3.5 Generalization bounds", "text": "Our estimate of the number of units and parameters required for a deep network to approximate compositional functions with an error G allows the use of one of several available boundaries for the generalization error of the network to derive sample complexity boundaries. As an example, we consider Theorem 16.2 in [Anthony and Bartlett, 2002], which provides the following sample for a generalization error G with a probability of at least 1 \u2212 \u03b4 in a network where the W parameters (weights and distortions) intended to minimize empirical error (the theorem is given in the standard ERM setup) are expressed in k-bits: M (G, \u03b4) \u2264 22G (kW log 2 + log) (10) This suggests the following comparison between flat and deep compositional (here binary tree-like networks) (here binary tree-like units), which ensure a network size that ensures the same approximation error."}, {"heading": "4 Connections with the theory of Boolean functions", "text": "The approach is that most people are able to decide for themselves what they want and what they want."}, {"heading": "5 Notes on a theory of compositional computation", "text": "In fact, it is the case that most of them are able to abide by the rules that they have imposed on themselves. (...) Most of them are not able to abide by the rules. (...) Most of them are not able to understand the rules. (...) Most of them are able to understand themselves. (...) Most of them are not able to understand themselves. (...) Most of them are not able to understand the rules. (...) Most of them are not able to understand themselves. (...) Most of them are not able to understand themselves. (...) Most of them are not able to understand themselves. (...) Most of them are able to understand themselves. (...)"}, {"heading": "6 Why are compositional functions so common or important?", "text": "First, we formalize the requirements for the algorithms of local compositionality: \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7"}, {"heading": "Acknowledgment", "text": "This work was supported by the Center for Brains, Minds and Machines (CBMM), funded by the NSF STC Award CCF - 1231216. HNM was partially supported by the ARO Grant W911NF-15-1-0385."}, {"heading": "1 Boolean Functions", "text": "One of the most important tools for theoretical computer scientists to study the functions of n Boolean variables, their associated circuit design and several related learning problems is the Fourier transformation via the abelian group Zn2. This is known as the Fourier analysis via the Boolean cube {\u2212 1, 1} n. The Fourier expansion of a Boolean function f: {\u2212 1, 1} n \u2192 {\u2212 1, 1} or even a real Boolean function f: {\u2212 1, 1} n \u2192 [\u2212 1, 1] is its representation as a real polynomial, which is multilinear due to the Boolean nature of its variables. Thus, for Boolean functions their Fourier representation is identical to their polynomic representation. In this paper, we use the two terms interchangeable. In contrast to functions of real variables, the complete Fourier expansion is more precise than an approximation. There is no need to distinguish between trigometric parameters and the normal parameters."}, {"heading": "2 Does Physics or Neuroscience imply compositionality?", "text": "In fact, most people who are able to determine themselves are able to determine for themselves what they want and what they want."}, {"heading": "3 Splines: some observations", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Additive and Tensor Product Splines", "text": "Additive and tensor product splines are two alternatives to radial cores for multidimensional functional approximation. It is well known that the three techniques follow the classical Tikhonov regularization and correspond to single-layer layered networks with square losses or SVM losses. 1Tegmark and Lin [Lin, 2016] have also suggested that a sequence of generative processes can be considered a Markov sequence that can be inverted to create an inference problem with a similar compositional structure. However, the resulting compositionality they describe does not correspond to our idea of hierarchical locality and therefore our theorems cannot be used to support their claims. We remember the extension of classical spline approximation techniques to multidimensional functions."}, {"heading": "3.1.1 Tensor product splines", "text": "The best known multivariate extension of one-dimensional splines is based on the use of radial nuclei such as Gauss or the basic multi-quadrilateral radial function. An alternative to the choice of a radial function is a tensor product type of the basic function, i.e. a function of FormK (x) = \u0442dj = 1k (xj), where xj is the j-th coordinate of the vector x and k (x) is the inverse Fourier transform associated with a Tikhonov stabilizer (see [Girosi et al., 1995]). We note that the choice of the basic Gaussian function for k (x) results in a Gaussian radial approximation scheme with K (x) = e \u2212 \u04452."}, {"heading": "3.1.2 Additive splines", "text": "Additive approximation schemes can also be derived within the framework of regularization theory. By additive approximation we mean an approximation of the formf (x) = d \u2211 \u00b5 = 1 f\u00b5 (x \u00b5) (13), where x\u00b5 is the \u00b5-th component of the input vector x and f\u00b5. Additive models are well known in statistics (at least since Stone, 1985) and can be considered a generalization of linear models. They are attractive because they essentially represent an overlap of one-dimensional functions, have low complexity and share with linear models the property that the effects of the different variables can be studied separately. The resulting scheme is very similar to Projection Pursuit Regression x ltots. We refer to [Girosi et al, 1995] for references and discussions on how such approximations can arise from regularization variables."}, {"heading": "3.2 Hierarchical Splines", "text": "Consider an additive approximation scheme (see subsection) in which a function of the d variables is approximated by an expression such as asf (x) = d \u2211 i \u03c6i (x i) (15), in which xi is the i-th component of the input vector x and \u03c6i are one-dimensional spline approximations. However, for linear splines, tensor product-related splines must be used. The new alternative we propose here is hierarchical additive splines, which, in the case of a 2-layer hierarchy, can use the form (x) = K-layer j-spline (d-i), is the use of tensor product-related splines. The new alternative we propose here is hierarchical additive splines, which, in the case of a 2-layer hierarchy, are the form (x) = K-layer j-spline units (d-i)."}, {"heading": "4 On multivariate function approximation", "text": "Do we see a multivariate function f: [0, 1] d \u2192 R, discredited by Tensor-Basic functions:?????? (i1,..., xd):??????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????"}, {"heading": "4.1 Neural Networks: polynomial approach", "text": "One of the decisions listed above leads to basic polynomial functions. The standard approach to the proof of the degree of the approximation functions usespolynomials = 21. It can be summarized in three steps: 1. With Hk we show the linear space of the polynomials of degree k in Rn and with Pk = k s = 0Hs the linear space of the degree polynomials at most k in n variables. Sentence r = (n + k) = dimHk and denote the space of univariant polynomials of degree at most k We remember that the number of monomials in a polynomial of degree is at most k in d variables of the total degree \u2264 N (d) and can be written as a linear combination of the same number of terms of form (< w, x > + b) N. We first prove that the number of monomials in a polynomial of the polynomials of the polynomials of the polynomial of the polynomial of the polynomial is not linear."}, {"heading": "4.2 Neural Networks: splines and look-up tables", "text": "Another set of basic functions for discretization consists of splines. In particular, for simplicity, we focus on indicator functions on partitions of [0, 1], i.e. piecemeal constant splines. Another attractive choice is hair base functions. If we focus on the binary case, Section 4.3 tells the full story, which does not need to be repeated here. We just add a note to specify a partition Let us assume that a = x1 < x2 \u00b7 \u00b7 < xm = b points are given and set x the maximum separation between any two points. \u2022 If f-C [a, b] then gives such a note for each > 0, that if we play x < \u03b4 x (x) \u2212 Sf (x) \u2212 Sf (x) |; for all x [a, b], where Sf is the spline interpolance of f. \u2022 if f-C2 [a, b] then for all < x is a part of Sf (x) where Sf is \u2212 line."}, {"heading": "4.3 Non-smooth ReLUs: how deep nets may work in reality", "text": "However, it is interesting to consider other types of approximation that can better capture what a deep neural network with the ReLU activation functions implements in practice as a result of minimizing empirical risk. The logic of the argument is simple: \u2022 Consider the constituent functions of the binary tree, that is functions of two variables as g (xx1, x2). Assume that g is Lipschitz with Lipschitz constant L. Then for any it is possible to set a partition of x1, x2 on the unit square that allows piecewise constant approximation of g with sup norm."}, {"heading": "5 Vector Quantization and Hierarchical Vector Quantization", "text": "Let's start with the observation that a network of radial Gaussian-like units becomes, in extreme cases, a limitation of functionality = four Girsian units = an observation table with entries corresponding to the centers. It can be described in terms of soft vector quantization (VQ) (see Section 6.3 in Poggio and Girosi, [Poggio and Girosi, 1989]). Note that hierarchical VQ (called HVQ) can be even more efficient than VQ in terms of memory requirements (see e.g. [Mihalik, 1992]). This suggests that a hierarchy of HBF layers may be similar (depending on what weights are determined by learning) to HVQ. Note that compression is achieved when parts can be reused in higher layers such as in revolutionary networks. Note that the center of a unit at the level n of a \"evolutionary\" hierarchical \"x\" is a combination of parts fed by each of the parts x."}, {"heading": "6 Pooling and invariance", "text": "(This section looks at the important case of deep networks with translation symmetry, which corresponds to a higher unit than is the case in the neighborhood). (This section looks at the important case of deep networks with translation symmetry, which corresponds to the revolutionary networks - the most powerful neural networks so far. (The networks we discussed in the previous section must have no \"weight distribution\" and must not be convolutional.) If we assume that all weights in each layer are \"divided,\" that is, the weights in one node are only shifts in another node. (The layer performs a convolution, which is indexed in our one-dimensional networks of this work.) < x, git > + b, (26) where the vector t of gi theory is transformed. (Anselmi and Poggio, 2016) corresponds to a higher unit."}, {"heading": "7 Optimization of compositional functions and Bezout theorem", "text": ") Consider minimizing the quadratic loss of a mesh described by a binary tree graph with a polynomial activation function = 1 increasing function, such as the quadratic activation function. The function calculated by the network is a polynomial in the input variables. The critical points reached by setting the weights to zero for the gradients wrt are solutions for polynomial equations in weights wi, j. In all these cases, Bezout theoretically provides an estimate of the number of local minima as a function of the architecture of the network. The number of distinct zeros (counting points at infinity, using the projective space, assigning an appropriate multiplicity to each intersection point, and excluding a degenerated case) of the functions is exactly equal to the product of the respective square equation."}, {"heading": "7.1 Convergence of greedy techniques in deep networks", "text": "Theorem 4.2 in [Poggio, 1975] can be adapted to prove the convergence of an iteration method that optimizes the best correction in layer m to the last layer and then the iteration.The steps are: I) The optimal approximation of the zero degree and the sequence of the optimal corrections to the layer n. II) The optimal corrections of the result of the step are calculated for all degrees, starting with the degree zero order. III) The iteration leads to a series of layers (i = 1, 2, \u00b7, n) and the associated mean square errors. The iteration algorithm outlined here (adapted to Katzenelson et al., 1964) provides a meaningful result in each step. The convergence of the iteration algorithm and the uniqueness of the optimal estimator to an estimator that does not affect the mean square limit error is not in the next magnitude algorithm 4.2."}], "references": [{"title": "TensorFlow: Large-scale machine learning on heterogeneous systems. Software available from tensorflow.org", "author": ["P. Tucker", "V. Vanhoucke", "V. Vasudevan", "F. Vi\u00e9gas", "O. Vinyals", "P. Warden", "M. Wattenberg", "M. Wicke", "Y. Yu", "X. Zheng"], "venue": null, "citeRegEx": "Tucker et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Tucker et al\\.", "year": 2015}, {"title": "Unsupervised learning of invariant representations with low sample complexity: the magic of sensory cortex or a new framework for machine learning", "author": ["Anselmi et al", "F. 2014] Anselmi", "J. Leibo", "L. Rosasco", "J. Mutch", "A. Tacchetti", "T. Poggio"], "venue": "Center for Brains, Minds and Machines (CBMM) Memo No", "citeRegEx": "al. et al\\.,? \\Q2014\\E", "shortCiteRegEx": "al. et al\\.", "year": 2014}, {"title": "Unsupervised learning of invariant representations", "author": ["Anselmi et al", "F. 2015a] Anselmi", "J.Z. Leibo", "L. Rosasco", "J. Mutch", "A. Tacchetti", "T. Poggio"], "venue": "Theoretical Computer Science", "citeRegEx": "al. et al\\.,? \\Q2015\\E", "shortCiteRegEx": "al. et al\\.", "year": 2015}, {"title": "Visual Cortex and Deep Networks", "author": ["Anselmi", "Poggio", "F. 2016] Anselmi", "T. Poggio"], "venue": null, "citeRegEx": "Anselmi et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Anselmi et al\\.", "year": 2016}, {"title": "Deep convolutional network are hierarchical kernel machines. Center for Brains, Minds and Machines (CBMM) Memo No", "author": ["Anselmi et al", "F. 2015b] Anselmi", "L. Rosasco", "C. Tan", "T. Poggio"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q2015\\E", "shortCiteRegEx": "al. et al\\.", "year": 2015}, {"title": "Neural Network Learning - Theoretical Foundations", "author": ["Anthony", "Bartlett", "M. 2002] Anthony", "P. Bartlett"], "venue": null, "citeRegEx": "Anthony et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Anthony et al\\.", "year": 2002}, {"title": "Representations properties of multilayer feedforward networks", "author": ["B. Moore", "Poggio", "B. 1998] B. Moore", "T. Poggio"], "venue": "Abstracts of the First annual INNS meeting,", "citeRegEx": "Moore et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Moore et al\\.", "year": 1998}, {"title": "Scaling learning algorithms towards ai", "author": ["Bengio", "LeCun", "Y. 2007] Bengio", "Y. LeCun"], "venue": null, "citeRegEx": "Bengio et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2007}, {"title": "Random search for hyper-parameter optimization", "author": ["Bergstra", "Bengio", "J. 2012] Bergstra", "Y. Bengio"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "Bergstra et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Bergstra et al\\.", "year": 2012}, {"title": "Neural networks for localized approximation", "author": ["Chui et al", "C.K. 1994] Chui", "X. Li", "H.N. Mhaskar"], "venue": "Mathematics of Computation,", "citeRegEx": "al. et al\\.,? \\Q1994\\E", "shortCiteRegEx": "al. et al\\.", "year": 1994}, {"title": "Limitations of the approximation capabilities of neural networks with one hidden layer", "author": ["Chui et al", "C.K. 1996] Chui", "X. Li", "H.N. Mhaskar"], "venue": "Advances in Computational Mathematics,", "citeRegEx": "al. et al\\.,? \\Q1996\\E", "shortCiteRegEx": "al. et al\\.", "year": 1996}, {"title": "On the expressive power of deep learning: a tensor analysis", "author": ["Cohen et al", "N. 2015] Cohen", "O. Sharir", "A. Shashua"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q2015\\E", "shortCiteRegEx": "al. et al\\.", "year": 2015}, {"title": "Condiciones para que una funcion infinitamente derivable sea un polinomio", "author": ["Corominas", "Balaguer", "E. 1954] Corominas", "F.S. Balaguer"], "venue": "Revista matema\u0301tica hispanoamericana,", "citeRegEx": "Corominas et al\\.,? \\Q1954\\E", "shortCiteRegEx": "Corominas et al\\.", "year": 1954}, {"title": "Some theorems on distribution functions", "author": ["Cramer", "Wold", "H. 1936] Cramer", "H. Wold"], "venue": "J. London Math. Soc.,", "citeRegEx": "Cramer et al\\.,? \\Q1936\\E", "shortCiteRegEx": "Cramer et al\\.", "year": 1936}, {"title": "Y", "author": ["O. Delalleau", "Bengio"], "venue": "(2011). Shallow vs. deep sum-product networks. In Advances in Neural Information Processing Systems 24: 25th Annual Conference on Neural Information Processing Systems", "citeRegEx": "Delalleau and Bengio. 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "Optimal nonlinear approximation", "author": ["DeVore et al", "R.A. 1989] DeVore", "R. Howard", "C.A. Micchelli"], "venue": "Manuscripta mathematica,", "citeRegEx": "al. et al\\.,? \\Q1989\\E", "shortCiteRegEx": "al. et al\\.", "year": 1989}, {"title": "Regularization theory and neural networks architectures", "author": ["Girosi et al", "F. 1995] Girosi", "M. Jones", "T. Poggio"], "venue": "Neural Computation,", "citeRegEx": "al. et al\\.,? \\Q1995\\E", "shortCiteRegEx": "al. et al\\.", "year": 1995}, {"title": "Representation properties of networks: Kolmogorov\u2019s theorem is irrelevant", "author": ["Girosi", "Poggio", "F. 1989] Girosi", "T. Poggio"], "venue": "Neural Computation,", "citeRegEx": "Girosi et al\\.,? \\Q1989\\E", "shortCiteRegEx": "Girosi et al\\.", "year": 1989}, {"title": "Networks and the best approximation property", "author": ["Girosi", "Poggio", "F. 1990] Girosi", "T. Poggio"], "venue": "Biological Cybernetics,", "citeRegEx": "Girosi et al\\.,? \\Q1990\\E", "shortCiteRegEx": "Girosi et al\\.", "year": 1990}, {"title": "Adam: A method for stochastic optimization. CoRR, abs/1412.6980", "author": ["Kingma", "Ba", "D.P. 2014] Kingma", "J. Ba"], "venue": null, "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Bridging the gap between residual learning, recurrent neural networks and visual cortex. Center for Brains, Minds and Machines", "author": ["Liao", "Poggio", "Q. 2016] Liao", "T. Poggio"], "venue": "(CBMM) Memo No. 47,", "citeRegEx": "Liao et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Liao et al\\.", "year": 2016}, {"title": "Why does deep and cheap learning work so well", "author": ["Lin", "H. 2016] Lin", "M. Tegmark"], "venue": null, "citeRegEx": "Lin et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2016}, {"title": "Constant depth circuits, fourier transform, and learnability", "author": ["al. Linial et", "Linial N", "Y. M"], "venue": "Journal of the ACM,", "citeRegEx": "et et al\\.,? \\Q1993\\E", "shortCiteRegEx": "et et al\\.", "year": 1993}, {"title": "A provably efficient algorithm for training deep networks. CoRR, abs/1304.7045", "author": ["Livni et al", "R. 2013] Livni", "S. Shalev-Shwartz", "O. Shamir"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q2013\\E", "shortCiteRegEx": "al. et al\\.", "year": 2013}, {"title": "Learning real and boolean functions: When is deep better than shallow? Center for Brains, Minds and Machines", "author": ["Mhaskar et al", "H. 2016] Mhaskar", "Q. Liao", "T. Poggio"], "venue": "(CBMM) Memo No. 45,", "citeRegEx": "al. et al\\.,? \\Q2016\\E", "shortCiteRegEx": "al. et al\\.", "year": 2016}, {"title": "Deep versus shallow networks: an approximation theory perspective. Center for Brains, Minds and Machines (CBMM", "author": ["Mhaskar", "Poggio", "H. 2016] Mhaskar", "T. Poggio"], "venue": "Memo No. 54,", "citeRegEx": "Mhaskar et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Mhaskar et al\\.", "year": 2016}, {"title": "Neural networks for localized approximation of real functions", "author": ["Mhaskar", "H.N. 1993b] Mhaskar"], "venue": "In Neural Networks for Processing", "citeRegEx": "Mhaskar and Mhaskar,? \\Q1993\\E", "shortCiteRegEx": "Mhaskar and Mhaskar", "year": 1993}, {"title": "Perceptrons: An Introduction to Computational Geometry. The MIT Press, ISBN 0-26263022-2", "author": ["Minsky", "Papert", "M. 1972] Minsky", "S. Papert"], "venue": null, "citeRegEx": "Minsky et al\\.,? \\Q1972\\E", "shortCiteRegEx": "Minsky et al\\.", "year": 1972}, {"title": "On the number of linear regions of deep neural networks", "author": ["Montufar et al", "G.F. 2014] Montufar", "R. Pascanu", "K. Cho", "Y. Bengio"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "al. et al\\.,? \\Q2014\\E", "shortCiteRegEx": "al. et al\\.", "year": 2014}, {"title": "On the relationship between generalization error, hypothesis complexity and sample complexity in regularization networks", "author": ["Niyogi", "Girosi", "P. 1996] Niyogi", "F. Girosi"], "venue": "Neural Computation,", "citeRegEx": "Niyogi et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Niyogi et al\\.", "year": 1996}, {"title": "Itheory on depth vs width: hierarchical function composition", "author": ["Poggio et al", "T. 2015a] Poggio", "F. Anselmi", "L. Rosasco"], "venue": "CBMM memo 041", "citeRegEx": "al. et al\\.,? \\Q2015\\E", "shortCiteRegEx": "al. et al\\.", "year": 2015}, {"title": "A theory of networks for approximation and learning", "author": ["Poggio", "Girosi", "T. 1989] Poggio", "F. Girosi"], "venue": "Laboratory, Massachusetts Institute of Technology, A.I. memo n1140", "citeRegEx": "Poggio et al\\.,? \\Q1989\\E", "shortCiteRegEx": "Poggio et al\\.", "year": 1989}, {"title": "On the representation of multi-input systems: Computational properties of polynomial algorithms", "author": ["Poggio", "Reichardt", "T. 1980] Poggio", "W. Reichardt"], "venue": "Biological Cybernetics,", "citeRegEx": "Poggio et al\\.,? \\Q1980\\E", "shortCiteRegEx": "Poggio et al\\.", "year": 1980}, {"title": "Notes on hierarchical splines, dclns and i-theory", "author": ["Poggio et al", "T. 2015b] Poggio", "L. Rosaco", "A. Shashua", "N. Cohen", "F. Anselmi"], "venue": "CBMM memo 037", "citeRegEx": "al. et al\\.,? \\Q2015\\E", "shortCiteRegEx": "al. et al\\.", "year": 2015}, {"title": "Notes on hierarchical splines, dclns and i-theory", "author": ["Poggio et al", "T. 2015c] Poggio", "L. Rosasco", "A. Shashua", "N. Cohen", "F. Anselmi"], "venue": "Technical report,", "citeRegEx": "al. et al\\.,? \\Q2015\\E", "shortCiteRegEx": "al. et al\\.", "year": 2015}, {"title": "The mathematics of learning: Dealing with data. Notices of the American Mathematical Society (AMS), 50(5):537\u2013544", "author": ["Poggio", "Smale", "T. 2003] Poggio", "S. Smale"], "venue": null, "citeRegEx": "Poggio et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Poggio et al\\.", "year": 2003}, {"title": "Hierarchical models of object recognition in cortex", "author": ["Riesenhuber", "Poggio", "M. 1999] Riesenhuber", "T. Poggio"], "venue": "Nature Neuroscience,", "citeRegEx": "Riesenhuber et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Riesenhuber et al\\.", "year": 1999}, {"title": "M", "author": ["Telgarsky"], "venue": "(2015). Representation benefits of deep feedforward networks. arXiv preprint arXiv:1509.08101v2 [cs.LG] 29 Sep", "citeRegEx": "Telgarsky. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Performance-optimized hierarchical models predict neural responses in higher visual cortex", "author": ["Yamins et al", "D. 2014] Yamins", "H. Hong", "C. Cadieu", "E. Solomon", "D. Seibert", "J. DiCarlo"], "venue": "Proceedings of the National Academy of Sciences", "citeRegEx": "al. et al\\.,? \\Q2014\\E", "shortCiteRegEx": "al. et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 25, "context": ", 2015b] [Mhaskar et al., 2016] and in [Liao and Poggio, .", "startOffset": 9, "endOffset": 31}, {"referenceID": 37, "context": "\u2022 A recent theorem by Telgarsky [Telgarsky, 2015] can be summarized as saying that a certain family of classification problems with real-valued inputs cannot be approximated well by shallow networks with fewer than exponentially many nodes whereas a deep network achieves zero error.", "startOffset": 32, "endOffset": 49}], "year": 2016, "abstractText": "The paper reviews an emerging body of theoretical results on deep learning including the conditions under which it can be exponentially better than shallow learning. Deep convolutional networks represent an important special case of these conditions, though weight sharing is not the main reason for their exponential advantage. Explanation of a few key theorems is provided together with new results, open problems and conjectures. This work was supported by the Center for Brains, Minds and Machines (CBMM), funded by NSF STC award CCF 1231216. H.M. is supported in part by ARO Grant W911NF-15-10385. 1 ar X iv :1 61 1. 00 74 0v 1 [ cs .L G ] 2 N ov 2 01 6 Why and When Can Deep \u2013 but Not Shallow \u2013 Networks Avoid the Curse of Dimensionality Tomaso Poggio Hrushikesh Mhaskar Lorenzo Rosasco Brando Miranda Qianli Liao Center for Brains, Minds, and Machines, McGovern Institute for Brain Research, Massachusetts Institute of Technology, Cambridge, MA, 02139. Department of Mathematics, California Institute of Technology, Pasadena, CA 91125; Institute of Mathematical Sciences, Claremont Graduate University, Claremont, CA 91711 Abstract: The paper reviews an emerging body of theoretical results on deep learning including the conditions under which it can be exponentially better than shallow learning. Deep convolutional networks represent an important special case of these conditions, though weight sharing is not the main reason for their exponential advantage. Explanation of a few key theorems is provided together with new results, open problems and conjectures. The paper reviews an emerging body of theoretical results on deep learning including the conditions under which it can be exponentially better than shallow learning. Deep convolutional networks represent an important special case of these conditions, though weight sharing is not the main reason for their exponential advantage. Explanation of a few key theorems is provided together with new results, open problems and conjectures.", "creator": "TeX"}}}