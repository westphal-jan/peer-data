{"id": "1611.00847", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Nov-2016", "title": "Deep Convolutional Neural Network Design Patterns", "abstract": "Recent research in the deep learning field has produced a plethora of new architectures. At the same time, a growing number of groups are applying deep learning to new applications and problems. Many of these groups might be composed of inexperienced deep learning practitioners who are baffled by the dizzying array of architecture choices and therefore use an older architecture, such as Alexnet. Here, we are attempting to bridge this gap by mining the collective knowledge contained in recent deep learning research to discover underlying principles for designing neural network architectures. In addition, we describe several architectural innovations, including Fractal of FractalNet, Stagewise Boosting Networks, and Taylor Series Networks (our Caffe code and prototxt files will be made publicly available). We hope others are inspired to build on this preliminary work.", "histories": [["v1", "Wed, 2 Nov 2016 23:48:04 GMT  (676kb,D)", "http://arxiv.org/abs/1611.00847v1", "Submitted as a conference paper at ICLR 2017"], ["v2", "Fri, 4 Nov 2016 00:49:46 GMT  (294kb,D)", "http://arxiv.org/abs/1611.00847v2", "Submitted as a conference paper at ICLR 2017"], ["v3", "Mon, 14 Nov 2016 14:10:41 GMT  (297kb,D)", "http://arxiv.org/abs/1611.00847v3", "Submitted as a conference paper at ICLR 2017"]], "COMMENTS": "Submitted as a conference paper at ICLR 2017", "reviews": [], "SUBJECTS": "cs.LG cs.CV cs.NE", "authors": ["leslie n smith", "nicholay topin"], "accepted": false, "id": "1611.00847"}, "pdf": {"name": "1611.00847.pdf", "metadata": {"source": "CRF", "title": "DEEP CONVOLUTIONAL NEURAL NETWORK DESIGN PATTERNS", "authors": ["Leslie N. Smith", "Nicholay Topin"], "emails": ["leslie.smith@nrl.navy.mil", "ntopin1@umbc.edu"], "sections": [{"heading": "1 INTRODUCTION", "text": "Recently, there has been a large selection of articles on new neural network architectures, especially in relation to residual networks, such as he et al. (2015; 2016); Zhang et al. (2016); Huang et al. (2016b). This has motivated us to consider these architectures as a potential source of universal design principles, which is particularly relevant because many inexperienced practitioners are currently trying to apply deep learning to various new applications. A lack of guidance leads to beginners practicing deep learning and ignoring Alexnet (or some such standard architectures), regardless of their appropriateness for their application.This wealth of research results is also an opportunity to identify elements that provide benefits in specific contexts. We ask some basic questions: Are there universal principles of deep network design that have emerged from the collective knowledge of deep learning? What architectural choices work best in a given context? Which architectural principles do Alexander architectural principles seem to have been elegantly described by Alexander Architects in 1979 or Alexander Architects in 1979? What architectures do Alexander Principles seem to have Alexander architectures?"}, {"heading": "2 RELATED WORK", "text": "There are a number of studies that deal with the question of how such a development can occur."}, {"heading": "3 DESIGN PATTERNS", "text": "The book \"Neural Networks: Tricks of the Trade\" (Orr & Mu \ufffd ller, 2003) contains recommendations for network models, but without reference to the enormous amount of research in recent years. Szegedy et al. (2015b) is probably the closest to this work, where the authors describe some design principles based on their experiences. We have specifically reviewed the literature to identify similarities and reduce their designs to basic elements that can be considered design patterns. It seems clear to us that when reviewing the literature some design options appear elegant, others less so. In this section we will discuss some of these elements. Here, we will first describe a few design patterns at the highest level and then suggest some more detailed ones."}, {"heading": "3.1 HIGH LEVEL ARCHITECTURE DESIGN", "text": "This year, it has reached the point where it will be able to retaliate."}, {"heading": "3.2 DETAILED ARCHITECTURE DESIGN", "text": "A common denominator in many of the most successful architectures is to facilitate the work of each layer. Use of very deep networks is one such example, as each layer only needs to change the input incrementally, which partly explains the success of residual networks, since the output of a layer is likely to be similar to the input of the layer, making the work of the layer easier. Also, this is part of the motivation behind the Proliferate Paths design pattern, but this concept of simple work of each layer extends beyond that. An example of Design Patterns 8: Incremental Feature Construction is the use of short jump lengths in ResNets. A recent paper (Alain & Bengio) has shown that skipping the length of layer 64 in a network of depth 128 led to a first part of the net of dead weights that is avoidable."}, {"heading": "3.2.1 JOINING BRANCHES: CONCATENATION VERSUS SUMMATION/MEAN VERSUS MAXOUT", "text": "In this section we propose some simple rules to decide how to combine branches. Summing is one of the most common ways to combine results. Summing / Mean divides the work of convergence between branches and leads to design patterns 12: Summation Joining. Summing is the preferred connection mechanism for residual networks because it allows the network to calculate corrective terms (i.e., residuals) rather than the entire signal. The difference between sum and fractal joining (mean) is best understood by considering the drop path (Huang et al. 2016b)."}, {"heading": "4 EXPERIMENTS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 ARCHITECTURAL INNOVATIONS", "text": "While the main focus of this paper is on explaining basic design principles, some architectural innovations helped us discover, and in this section we will describe these innovations.First, we recommended combining summation / mean, concatenation, and maximum connectivity mechanisms with different roles within a single architecture. Next, Design Pattern 2 to propagate branches prompted us to modify the large-scale sequential module pattern in the FractalNet architecture. Instead of lining up the modules for maximum depth, we arranged the modules in a fractal pattern, as shown in 1b, which we called Fractal of the FractalNet network (FoF), which swaps depth for a greater number of paths."}, {"heading": "4.1.1 FREEZE-DROP-PATH AND STAGEWISE BOOSTING NETWORKS (SBN)", "text": "Drop-path was introduced by Huang et al. (2016b). It works by arbitrarily removing branches during an iteration of training, as if this path did not exist in the network. Symmetry considerations led us to an opposite method, which we called Freeze-path. Instead of removing a branch from the network during training, we freeze the weights as if the learning rate were set to zero. A similar idea was proposed for recurring neural networks (Krueger et al. 2016). The combined usefulness of Drop-path and Freeze-path, which we call Freeze-Drop-path, is best explained in the non-stochastic case. Figure 1 shows an example of a fraction of the FractalNet architecture. Let's say we start training only the most left branch and use Drop-path on all other branches. This branch should train fast, as it has relatively few parameters in comparison to the entire network, allowing for the next branch to be active and the second branch is safe in this branch."}, {"heading": "4.1.2 TAYLOR SERIES NETWORKS (TSN)", "text": "Taylor series is a classic and well-known method of function approximation. Since neural networks are also function approximators, it is only a short jump from the SBNs to consider the branches of this network as terms in a Taylor extension, implying the squaring of the second branch before the sum connection unit, analogous to the second term in the Taylor expansion. Likewise, the third branch would be divided into cubes. We call this \"Taylor Series Networks\" (TSN) and there is precedence for polynomic networks (Livni et al. 2014) and multiplication in networks (e.g. Lin et al. 2015 in the literature. The implementation details of this TSN are similar to the SBN and are discussed in the appendix."}, {"heading": "4.2 RESULTS", "text": "The experiments in this section are primarily based on empirical validation of the architectural innovations described above, but not to test them fully. We allow for a more complete evaluation of future work. Table 1 compares the final test accuracy results for CIFAR-10 and CIFAR-100 in a series of experiments. Accuracy is calculated as the mean of the last 6 test accuracies calculated over the last 3000 iterations (out of 100,000 iterations) of the training. Results from the original FractalNet (Larsson et al. 2016) are given in the first row of the table and we use as a basis. Figure 2 compares the test accuracy of the original FractalNet architectures with a few modifications advocated by design patterns."}, {"heading": "5 CONCLUSION", "text": "We hope that these design patterns will be useful for both experienced practitioners who want to advance state-of-the-art technology as well as for beginners who want to apply deep learning to new applications. There is a wide range of potential follow-up work, some of which we have listed here as future work. Here, our efforts are primarily limited to Residual Classification Networks, but we hope that this work will inspire others to develop new design patterns for recursive neural networks, deep reinforcement learning architectures and beyond."}, {"heading": "A RELATIONSHIPS BETWEEN RESIDUAL ARCHITECTURES", "text": "In fact, most of them will be able to move to another world in which they are able, in which they are able to move."}], "references": [{"title": "Understanding intermediate layers using linear classifier probes", "author": ["Guillaume Alain", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1610.01644,", "citeRegEx": "Alain and Bengio.,? \\Q2016\\E", "shortCiteRegEx": "Alain and Bengio.", "year": 2016}, {"title": "The timeless way of building, volume 1", "author": ["Christopher Alexander"], "venue": null, "citeRegEx": "Alexander.,? \\Q1979\\E", "shortCiteRegEx": "Alexander.", "year": 1979}, {"title": "Xception: Deep learning with depthwise separable convolution", "author": ["Fran\u00e7ois Chollet"], "venue": "arXiv preprint arXiv:1610.02357,", "citeRegEx": "Chollet.,? \\Q2016\\E", "shortCiteRegEx": "Chollet.", "year": 2016}, {"title": "The elements of statistical learning, volume 1. Springer series in statistics", "author": ["Jerome Friedman", "Trevor Hastie", "Robert Tibshirani"], "venue": null, "citeRegEx": "Friedman et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Friedman et al\\.", "year": 2001}, {"title": "Design patterns: elements of reusable object-oriented software", "author": ["Erich Gamma", "Richard Helm", "Ralph Johnson", "John Vlissides"], "venue": "Pearson Education India,", "citeRegEx": "Gamma et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Gamma et al\\.", "year": 1995}, {"title": "Noisy activation functions", "author": ["Caglar Gulcehre", "Marcin Moczulski", "Misha Denil", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1603.00391,", "citeRegEx": "Gulcehre et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Gulcehre et al\\.", "year": 2016}, {"title": "Deep pyramidal residual networks", "author": ["Dongyoon Han", "Jiwhan Kim", "Junmo Kim"], "venue": "arXiv preprint arXiv:1610.02915,", "citeRegEx": "Han et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Han et al\\.", "year": 2016}, {"title": "Deep residual learning for image recognition", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"], "venue": "arXiv preprint arXiv:1512.03385,", "citeRegEx": "He et al\\.,? \\Q2015\\E", "shortCiteRegEx": "He et al\\.", "year": 2015}, {"title": "Identity mappings in deep residual networks", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"], "venue": "arXiv preprint arXiv:1603.05027,", "citeRegEx": "He et al\\.,? \\Q2016\\E", "shortCiteRegEx": "He et al\\.", "year": 2016}, {"title": "Frankenstein: Learning deep face representations using small data", "author": ["Guosheng Hu", "Xiaojiang Peng", "Yongxin Yang", "Timothy Hospedales", "Jakob Verbeek"], "venue": "arXiv preprint arXiv:1603.06470,", "citeRegEx": "Hu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Hu et al\\.", "year": 2016}, {"title": "Densely connected convolutional networks", "author": ["Gao Huang", "Zhuang Liu", "Kilian Q Weinberger"], "venue": "arXiv preprint arXiv:1608.06993,", "citeRegEx": "Huang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2016}, {"title": "Deep networks with stochastic depth", "author": ["Gao Huang", "Yu Sun", "Zhuang Liu", "Daniel Sedra", "Kilian Weinberger"], "venue": "arXiv preprint arXiv:1603.09382,", "citeRegEx": "Huang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2016}, {"title": "Decision forests, convolutional networks and the models in-between", "author": ["Yani Ioannou", "Duncan Robertson", "Darko Zikic", "Peter Kontschieder", "Jamie Shotton", "Matthew Brown", "Antonio Criminisi"], "venue": "arXiv preprint arXiv:1603.01250,", "citeRegEx": "Ioannou et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Ioannou et al\\.", "year": 2016}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["Sergey Ioffe", "Christian Szegedy"], "venue": "arXiv preprint arXiv:1502.03167,", "citeRegEx": "Ioffe and Szegedy.,? \\Q2015\\E", "shortCiteRegEx": "Ioffe and Szegedy.", "year": 2015}, {"title": "Caffe: Convolutional architecture for fast feature embedding", "author": ["Yangqing Jia", "Evan Shelhamer", "Jeff Donahue", "Sergey Karayev", "Jonathan Long", "Ross Girshick", "Sergio Guadarrama", "Trevor Darrell"], "venue": "In Proceedings of the 22nd ACM international conference on Multimedia,", "citeRegEx": "Jia et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Jia et al\\.", "year": 2014}, {"title": "Driving in the matrix: Can virtual worlds replace human-generated annotations for real world tasks", "author": ["Matthew Johnson-Roberson", "Charles Barto", "Rounak Mehta", "Sharath Nittur Sridhar", "Ram Vasudevan"], "venue": null, "citeRegEx": "Johnson.Roberson et al\\.,? \\Q1983\\E", "shortCiteRegEx": "Johnson.Roberson et al\\.", "year": 1983}, {"title": "The unreasonable effectiveness of noisy data for fine-grained recognition", "author": ["Jonathan Krause", "Benjamin Sapp", "Andrew Howard", "Howard Zhou", "Alexander Toshev", "Tom Duerig", "James Philbin", "Li Fei-Fei"], "venue": "arXiv preprint arXiv:1511.06789,", "citeRegEx": "Krause et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Krause et al\\.", "year": 2015}, {"title": "Learning multiple layers of features from tiny images", "author": ["Alex Krizhevsky", "Geoffrey Hinton"], "venue": null, "citeRegEx": "Krizhevsky and Hinton.,? \\Q2009\\E", "shortCiteRegEx": "Krizhevsky and Hinton.", "year": 2009}, {"title": "Imagenet classification with deep convolutional neural networks. In Advances in neural information processing", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E Hinton"], "venue": null, "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Zoneout: Regularizing rnns by randomly preserving hidden activations", "author": ["David Krueger", "Tegan Maharaj", "J\u00e1nos Kram\u00e1r", "Mohammad Pezeshki", "Nicolas Ballas", "Nan Rosemary Ke", "Anirudh Goyal", "Yoshua Bengio", "Hugo Larochelle", "Aaron Courville"], "venue": "arXiv preprint arXiv:1606.01305,", "citeRegEx": "Krueger et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Krueger et al\\.", "year": 2016}, {"title": "Fractalnet: Ultra-deep neural networks without residuals", "author": ["Gustav Larsson", "Michael Maire", "Gregory Shakhnarovich"], "venue": "arXiv preprint arXiv:1605.07648,", "citeRegEx": "Larsson et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Larsson et al\\.", "year": 2016}, {"title": "Streaming normalization: Towards simpler and more biologically-plausible normalizations for online and recurrent learning", "author": ["Qianli Liao", "Kenji Kawaguchi", "Tomaso Poggio"], "venue": "arXiv preprint arXiv:1610.06160,", "citeRegEx": "Liao et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Liao et al\\.", "year": 2016}, {"title": "Competitive multi-scale convolution", "author": ["Zhibin Liao", "Gustavo Carneiro"], "venue": "arXiv preprint arXiv:1511.05635,", "citeRegEx": "Liao and Carneiro.,? \\Q2015\\E", "shortCiteRegEx": "Liao and Carneiro.", "year": 2015}, {"title": "Bilinear cnn models for fine-grained visual recognition", "author": ["Tsung-Yu Lin", "Aruni RoyChowdhury", "Subhransu Maji"], "venue": "In Proceedings of the IEEE International Conference on Computer Vision, pp", "citeRegEx": "Lin et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2015}, {"title": "On the computational efficiency of training neural networks", "author": ["Roi Livni", "Shai Shalev-Shwartz", "Ohad Shamir"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Livni et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Livni et al\\.", "year": 2014}, {"title": "Convolutional residual memory networks", "author": ["Joel Moniz", "Christopher Pal"], "venue": "arXiv preprint arXiv:1606.05262,", "citeRegEx": "Moniz and Pal.,? \\Q2016\\E", "shortCiteRegEx": "Moniz and Pal.", "year": 2016}, {"title": "Neural networks: tricks of the trade", "author": ["Genevieve B Orr", "Klaus-Robert M\u00fcller"], "venue": null, "citeRegEx": "Orr and M\u00fcller.,? \\Q2003\\E", "shortCiteRegEx": "Orr and M\u00fcller.", "year": 2003}, {"title": "Deconstructing the ladder network architecture", "author": ["Mohammad Pezeshki", "Linxi Fan", "Philemon Brakel", "Aaron Courville", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1511.06430,", "citeRegEx": "Pezeshki et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Pezeshki et al\\.", "year": 2015}, {"title": "Deep learning made easier by linear transformations in perceptrons", "author": ["Tapani Raiko", "Harri Valpola", "Yann LeCun"], "venue": "In AISTATS,", "citeRegEx": "Raiko et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Raiko et al\\.", "year": 2012}, {"title": "Semisupervised learning with ladder networks", "author": ["Antti Rasmus", "Mathias Berglund", "Mikko Honkala", "Harri Valpola", "Tapani Raiko"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Rasmus et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Rasmus et al\\.", "year": 2015}, {"title": "Data programming: Creating large training sets, quickly", "author": ["Alexander Ratner", "Christopher De Sa", "Sen Wu", "Daniel Selsam", "Christopher R\u00e9"], "venue": "arXiv preprint arXiv:1605.07723,", "citeRegEx": "Ratner et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Ratner et al\\.", "year": 2016}, {"title": "Imagenet large scale visual recognition challenge", "author": ["Olga Russakovsky", "Jia Deng", "Hao Su", "Jonathan Krause", "Sanjeev Satheesh", "Sean Ma", "Zhiheng Huang", "Andrej Karpathy", "Aditya Khosla", "Michael Bernstein"], "venue": "International Journal of Computer Vision,", "citeRegEx": "Russakovsky et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Russakovsky et al\\.", "year": 2015}, {"title": "Weight normalization: A simple reparameterization to accelerate training of deep neural networks", "author": ["Tim Salimans", "Diederik P Kingma"], "venue": "arXiv preprint arXiv:1602.07868,", "citeRegEx": "Salimans and Kingma.,? \\Q2016\\E", "shortCiteRegEx": "Salimans and Kingma.", "year": 2016}, {"title": "Convolutional neural fabrics", "author": ["Shreyas Saxena", "Jakob Verbeek"], "venue": "arXiv preprint arXiv:1606.02492,", "citeRegEx": "Saxena and Verbeek.,? \\Q2016\\E", "shortCiteRegEx": "Saxena and Verbeek.", "year": 2016}, {"title": "Boosting neural networks", "author": ["Holger Schwenk", "Yoshua Bengio"], "venue": "Neural Computation,", "citeRegEx": "Schwenk and Bengio.,? \\Q2000\\E", "shortCiteRegEx": "Schwenk and Bengio.", "year": 2000}, {"title": "Weighted residuals for very deep networks", "author": ["Falong Shen", "Gang Zeng"], "venue": "arXiv preprint arXiv:1605.08831,", "citeRegEx": "Shen and Zeng.,? \\Q2016\\E", "shortCiteRegEx": "Shen and Zeng.", "year": 2016}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["Karen Simonyan", "Andrew Zisserman"], "venue": "arXiv preprint arXiv:1409.1556,", "citeRegEx": "Simonyan and Zisserman.,? \\Q2014\\E", "shortCiteRegEx": "Simonyan and Zisserman.", "year": 2014}, {"title": "Swapout: Learning an ensemble of deep architectures", "author": ["Saurabh Singh", "Derek Hoiem", "David Forsyth"], "venue": "arXiv preprint arXiv:1605.06465,", "citeRegEx": "Singh et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Singh et al\\.", "year": 2016}, {"title": "Gradual dropin of layers to train very deep neural networks", "author": ["Leslie N Smith", "Emily M Hand", "Timothy Doster"], "venue": "arXiv preprint arXiv:1511.06951,", "citeRegEx": "Smith et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Smith et al\\.", "year": 2015}, {"title": "Gradual dropin of layers to train very deep neural networks", "author": ["Leslie N Smith", "Emily M Hand", "Timothy Doster"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "Smith et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Smith et al\\.", "year": 2016}, {"title": "Striving for simplicity: The all convolutional net", "author": ["Jost Tobias Springenberg", "Alexey Dosovitskiy", "Thomas Brox", "Martin Riedmiller"], "venue": "arXiv preprint arXiv:1412.6806,", "citeRegEx": "Springenberg et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Springenberg et al\\.", "year": 2014}, {"title": "Dropout: a simple way to prevent neural networks from overfitting", "author": ["Nitish Srivastava", "Geoffrey E Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Srivastava et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 2014}, {"title": "Training very deep networks", "author": ["Rupesh K Srivastava", "Klaus Greff", "J\u00fcrgen Schmidhuber"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Srivastava et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 2015}, {"title": "Understanding locally competitive networks", "author": ["Rupesh Kumar Srivastava", "Jonathan Masci", "Faustino Gomez", "J\u00fcrgen Schmidhuber"], "venue": "arXiv preprint arXiv:1410.1165,", "citeRegEx": "Srivastava et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 2014}, {"title": "Going deeper with convolutions", "author": ["Christian Szegedy", "Wei Liu", "Yangqing Jia", "Pierre Sermanet", "Scott Reed", "Dragomir Anguelov", "Dumitru Erhan", "Vincent Vanhoucke", "Andrew Rabinovich"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "Szegedy et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Szegedy et al\\.", "year": 2015}, {"title": "Rethinking the inception architecture for computer vision", "author": ["Christian Szegedy", "Vincent Vanhoucke", "Sergey Ioffe", "Jonathon Shlens", "Zbigniew Wojna"], "venue": "arXiv preprint arXiv:1512.00567,", "citeRegEx": "Szegedy et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Szegedy et al\\.", "year": 2015}, {"title": "Inception-v4, inception-resnet and the impact of residual connections on learning", "author": ["Christian Szegedy", "Sergey Ioffe", "Vincent Vanhoucke"], "venue": "arXiv preprint arXiv:1602.07261,", "citeRegEx": "Szegedy et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Szegedy et al\\.", "year": 2016}, {"title": "Resnet in resnet: Generalizing residual architectures", "author": ["Sasha Targ", "Diogo Almeida", "Kevin Lyman"], "venue": "arXiv preprint arXiv:1603.08029,", "citeRegEx": "Targ et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Targ et al\\.", "year": 2016}, {"title": "Residual networks are exponential ensembles of relatively shallow networks", "author": ["Andreas Veit", "Michael Wilber", "Serge Belongie"], "venue": "arXiv preprint arXiv:1605.06431,", "citeRegEx": "Veit et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Veit et al\\.", "year": 2016}, {"title": "Under review as a conference paper at ICLR", "author": ["Sebastien C Wong", "Adam Gatt", "Victor Stamatescu", "Mark D McDonnell"], "venue": null, "citeRegEx": "Wong et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Wong et al\\.", "year": 2017}, {"title": "Residual networks", "author": ["Ke Zhang", "Miao Sun", "Tony X Han", "Xingfang Yuan", "Liru Guo", "Tao Liu"], "venue": null, "citeRegEx": "Zhang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 5, "context": "Recently, there has been a large assortment of articles on new neural network architectures, especially regarding Residual Networks, such as He et al. (2015; 2016); Larsson et al. (2016); Zhang et al.", "startOffset": 141, "endOffset": 187}, {"referenceID": 5, "context": "Recently, there has been a large assortment of articles on new neural network architectures, especially regarding Residual Networks, such as He et al. (2015; 2016); Larsson et al. (2016); Zhang et al. (2016); Huang et al.", "startOffset": 141, "endOffset": 208}, {"referenceID": 5, "context": "Recently, there has been a large assortment of articles on new neural network architectures, especially regarding Residual Networks, such as He et al. (2015; 2016); Larsson et al. (2016); Zhang et al. (2016); Huang et al. (2016b). This has motivated us to take a high-level look at these architectures as a potential source for universal principles of design.", "startOffset": 141, "endOffset": 230}, {"referenceID": 1, "context": "We ask some fundamental questions: Do universal principles of deep network design exist? Can these principles be mined from the collective knowledge on deep learning? Which architectural choices work best in any given context? Which architectures or parts of architectures seem elegant? Design patterns were first described by Christopher Alexander (Alexander (1979)) in regards to the architectures of buildings and towns.", "startOffset": 339, "endOffset": 367}, {"referenceID": 1, "context": "We ask some fundamental questions: Do universal principles of deep network design exist? Can these principles be mined from the collective knowledge on deep learning? Which architectural choices work best in any given context? Which architectures or parts of architectures seem elegant? Design patterns were first described by Christopher Alexander (Alexander (1979)) in regards to the architectures of buildings and towns. Alexander wrote of a timeless quality in architecture that \u201clives\u201d and this quality is enabled by building based on universal principles. The basis of design patterns is that they resolve a conflict of forces in a given context and lead to an equilibrium analogous to the ecological balance in nature. Design patterns are both highly specific, making them clear to follow, and flexible so they can be adapted to different environments and situations. Inspired by Alexander\u2019s work, the \u201cgang of four\u201d (Gamma et al. (1995)) applied the concept of design patterns to the architecture of object-oriented software.", "startOffset": 339, "endOffset": 945}, {"referenceID": 7, "context": "Unfortunately, we cannot do justice to this body of work, so we instead focus on recent innovations in convolutional neural networks architectures and, in particular, on the family of Residual Network variants (He et al., 2015).", "startOffset": 210, "endOffset": 227}, {"referenceID": 42, "context": "introduced Highway Networks (Srivastava et al., 2015), which use a gating mechanism to decide whether to combine the input with the layer outputs, and showed how this allowed the training of very deep networks.", "startOffset": 28, "endOffset": 53}, {"referenceID": 38, "context": "The DropIn technique (Smith et al., 2015; 2016) describes both a gradual change and a stochastic method in allowing the input to skip layers in order to train very deep networks.", "startOffset": 21, "endOffset": 47}, {"referenceID": 8, "context": "The authors followed up with another paper (He et al., 2016) where they investigate why identity mappings help and report results for a network with more than a thousand layers.", "startOffset": 43, "endOffset": 60}, {"referenceID": 46, "context": "The Inception-v4 paper (Szegedy et al., 2016) describes the impact of residual connections on their Inception architecture and compared these results with the results from an updated Inception design.", "startOffset": 23, "endOffset": 45}, {"referenceID": 47, "context": "The Resnet in Resnet paper (Targ et al., 2016) suggests an architecture with connections in a layer.", "startOffset": 27, "endOffset": 46}, {"referenceID": 20, "context": "At the same time, the FractalNet paper (Larsson et al., 2016) demonstrated training deep networks with a symmetrically repeating architectural pattern.", "startOffset": 39, "endOffset": 61}, {"referenceID": 37, "context": "In Swapout (Singh et al., 2016), each layer can be dropped, skipped, used normally, or combined with a residual.", "startOffset": 11, "endOffset": 31}, {"referenceID": 50, "context": "For Residual of Residual Networks (Zhang et al., 2016), the authors propose adding a hierarchy of skip connections where the input can skip a layer, a module, or any number of modules.", "startOffset": 34, "endOffset": 54}, {"referenceID": 7, "context": "Unfortunately, we cannot do justice to this body of work, so we instead focus on recent innovations in convolutional neural networks architectures and, in particular, on the family of Residual Network variants (He et al., 2015). We start with the Network In Network (Lin et al., 2013), which describes a hierarchical network with a small network design repeatedly embedded in the overall architecture. Szegedy et al. (2015a) incorporated this idea into their Inception architecture.", "startOffset": 211, "endOffset": 425}, {"referenceID": 7, "context": "Unfortunately, we cannot do justice to this body of work, so we instead focus on recent innovations in convolutional neural networks architectures and, in particular, on the family of Residual Network variants (He et al., 2015). We start with the Network In Network (Lin et al., 2013), which describes a hierarchical network with a small network design repeatedly embedded in the overall architecture. Szegedy et al. (2015a) incorporated this idea into their Inception architecture. Later, these authors proposed modifications to the original inception design (Szegedy et al., 2015b). A similar concept was contained in the multi-scale convolution architecture (Liao & Carneiro, 2015). In the meantime, Batch Normalization (Ioffe & Szegedy, 2015) was presented as a unit within the network that makes training the network faster and easier. Prior to the introduction of residual networks there were a few papers suggesting skip connections. Skip connections were proposed by Raiko et al. (2012). Srivastava, et al.", "startOffset": 211, "endOffset": 995}, {"referenceID": 7, "context": "Unfortunately, we cannot do justice to this body of work, so we instead focus on recent innovations in convolutional neural networks architectures and, in particular, on the family of Residual Network variants (He et al., 2015). We start with the Network In Network (Lin et al., 2013), which describes a hierarchical network with a small network design repeatedly embedded in the overall architecture. Szegedy et al. (2015a) incorporated this idea into their Inception architecture. Later, these authors proposed modifications to the original inception design (Szegedy et al., 2015b). A similar concept was contained in the multi-scale convolution architecture (Liao & Carneiro, 2015). In the meantime, Batch Normalization (Ioffe & Szegedy, 2015) was presented as a unit within the network that makes training the network faster and easier. Prior to the introduction of residual networks there were a few papers suggesting skip connections. Skip connections were proposed by Raiko et al. (2012). Srivastava, et al. introduced Highway Networks (Srivastava et al., 2015), which use a gating mechanism to decide whether to combine the input with the layer outputs, and showed how this allowed the training of very deep networks. The DropIn technique (Smith et al., 2015; 2016) describes both a gradual change and a stochastic method in allowing the input to skip layers in order to train very deep networks. The concept of stochastic depth via a drop-path method was introduced by Huang et al. (2016b). Residual Networks were introduced by He et al.", "startOffset": 211, "endOffset": 1499}, {"referenceID": 7, "context": "Unfortunately, we cannot do justice to this body of work, so we instead focus on recent innovations in convolutional neural networks architectures and, in particular, on the family of Residual Network variants (He et al., 2015). We start with the Network In Network (Lin et al., 2013), which describes a hierarchical network with a small network design repeatedly embedded in the overall architecture. Szegedy et al. (2015a) incorporated this idea into their Inception architecture. Later, these authors proposed modifications to the original inception design (Szegedy et al., 2015b). A similar concept was contained in the multi-scale convolution architecture (Liao & Carneiro, 2015). In the meantime, Batch Normalization (Ioffe & Szegedy, 2015) was presented as a unit within the network that makes training the network faster and easier. Prior to the introduction of residual networks there were a few papers suggesting skip connections. Skip connections were proposed by Raiko et al. (2012). Srivastava, et al. introduced Highway Networks (Srivastava et al., 2015), which use a gating mechanism to decide whether to combine the input with the layer outputs, and showed how this allowed the training of very deep networks. The DropIn technique (Smith et al., 2015; 2016) describes both a gradual change and a stochastic method in allowing the input to skip layers in order to train very deep networks. The concept of stochastic depth via a drop-path method was introduced by Huang et al. (2016b). Residual Networks were introduced by He et al. (2015), which described their network that won the ImageNet Challenge.", "startOffset": 211, "endOffset": 1554}, {"referenceID": 7, "context": "Unfortunately, we cannot do justice to this body of work, so we instead focus on recent innovations in convolutional neural networks architectures and, in particular, on the family of Residual Network variants (He et al., 2015). We start with the Network In Network (Lin et al., 2013), which describes a hierarchical network with a small network design repeatedly embedded in the overall architecture. Szegedy et al. (2015a) incorporated this idea into their Inception architecture. Later, these authors proposed modifications to the original inception design (Szegedy et al., 2015b). A similar concept was contained in the multi-scale convolution architecture (Liao & Carneiro, 2015). In the meantime, Batch Normalization (Ioffe & Szegedy, 2015) was presented as a unit within the network that makes training the network faster and easier. Prior to the introduction of residual networks there were a few papers suggesting skip connections. Skip connections were proposed by Raiko et al. (2012). Srivastava, et al. introduced Highway Networks (Srivastava et al., 2015), which use a gating mechanism to decide whether to combine the input with the layer outputs, and showed how this allowed the training of very deep networks. The DropIn technique (Smith et al., 2015; 2016) describes both a gradual change and a stochastic method in allowing the input to skip layers in order to train very deep networks. The concept of stochastic depth via a drop-path method was introduced by Huang et al. (2016b). Residual Networks were introduced by He et al. (2015), which described their network that won the ImageNet Challenge. They were able to extend the depth of a network from tens to hundreds of layers and in doing so, improve the network\u2019s performance. The authors followed up with another paper (He et al., 2016) where they investigate why identity mappings help and report results for a network with more than a thousand layers. The research community took notice of this architecture and several modifications to the original design were soon proposed. The Inception-v4 paper (Szegedy et al., 2016) describes the impact of residual connections on their Inception architecture and compared these results with the results from an updated Inception design. The Resnet in Resnet paper (Targ et al., 2016) suggests an architecture with connections in a layer. Veit et al. (2016) provided an understanding of Residual Networks as an ensemble of relatively shallow networks.", "startOffset": 211, "endOffset": 2374}, {"referenceID": 44, "context": "The closest to this work is probably Szegedy et al. (2015b) where the authors describe a few design principles based on their experiences.", "startOffset": 37, "endOffset": 60}, {"referenceID": 31, "context": "Several researchers have pointed out that the winners of the ImageNet Challenge (Russakovsky et al., 2015) have successively used deeper networks (as seen in, Krizhevsky et al.", "startOffset": 80, "endOffset": 106}, {"referenceID": 20, "context": "Recent examples include FractalNet (Larsson et al. 2016), Xception (Chollet 2016), and Decision Forest Convolutional Networks (Ioannou et al.", "startOffset": 35, "endOffset": 56}, {"referenceID": 12, "context": "2016), Xception (Chollet 2016), and Decision Forest Convolutional Networks (Ioannou et al. 2016).", "startOffset": 75, "endOffset": 96}, {"referenceID": 40, "context": "Simplicity was exemplified in the paper \u201dStriving for Simplicity\u201d (Springenberg et al. 2014) by achieving state-of-the-art results with fewer types of units.", "startOffset": 66, "endOffset": 92}, {"referenceID": 20, "context": "We also noted a special degree of elegance in the FractalNet (Larsson et al. 2016) design, which we attributed to the symmetry of its structure.", "startOffset": 61, "endOffset": 82}, {"referenceID": 8, "context": ", 2015) have successively used deeper networks (as seen in, Krizhevsky et al. (2012), Szegedy et al.", "startOffset": 60, "endOffset": 85}, {"referenceID": 8, "context": ", 2015) have successively used deeper networks (as seen in, Krizhevsky et al. (2012), Szegedy et al. (2015a), Simonyan & Zisserman (2014), He et al.", "startOffset": 60, "endOffset": 109}, {"referenceID": 8, "context": ", 2015) have successively used deeper networks (as seen in, Krizhevsky et al. (2012), Szegedy et al. (2015a), Simonyan & Zisserman (2014), He et al.", "startOffset": 60, "endOffset": 138}, {"referenceID": 4, "context": "(2015a), Simonyan & Zisserman (2014), He et al. (2015)).", "startOffset": 38, "endOffset": 55}, {"referenceID": 4, "context": "(2015a), Simonyan & Zisserman (2014), He et al. (2015)). It is also apparent from the ImageNet Challenge that multiplying the number of paths through the network is a recent trend; a trend that is apparent when looking from Alexnet to Inception to ResNets. For example, Veit et al. (2016) show that ResNets can be considered to be an exponential ensemble of networks with different lengths.", "startOffset": 38, "endOffset": 289}, {"referenceID": 2, "context": "2016), Xception (Chollet 2016), and Decision Forest Convolutional Networks (Ioannou et al. 2016). We even will go so far as to predict that the winner of this year\u2019s ImageNet Challenge will do so by increasing the number of branches in their architecture rather than continuing to increase the depth. Scientists have embraced simplicity/parsimony for centuries. Simplicity was exemplified in the paper \u201dStriving for Simplicity\u201d (Springenberg et al. 2014) by achieving state-of-the-art results with fewer types of units. We add this as Design Pattern 3: Strive for Simplicity; to use fewer types of layers in order to keep the network as simple as possible. We also noted a special degree of elegance in the FractalNet (Larsson et al. 2016) design, which we attributed to the symmetry of its structure. Architectural symmetry is typically considered a sign of beauty and quality, so we add it here as Design Pattern 4: Increase Symmetry. In addition to its symmetry, FractalNets also adheres to the Proliferate Paths design pattern so it is the baseline of our experiments in Section 4. Examination of trade-offs in order to understand the relevant forces is an essential element of design patterns. One fundamental trade-off is to maximize representational power versus compression of redundant and non-discriminating information. It is universal in all convolutional neural networks, from the data to the final convolutional layer, that the activations are downsampled and the number of channels increased. As is exemplified in Deep Pyramidal Residual Networks (Han et al. (2016)), which leads to Design Pattern 5: Pyramid Shape where there should be an overall smooth downsampling throughout the architecture and the downsampling should be combined with an increase in the number of channels.", "startOffset": 17, "endOffset": 1581}, {"referenceID": 21, "context": "The Batch Normalization (Ioffe & Szegedy 2015) paper attributes the benefits to handling internal covariate shift, while the authors of streaming normalization (Liao et al. 2016) express that it might be otherwise.", "startOffset": 160, "endOffset": 178}, {"referenceID": 10, "context": "The difference between sum and fractal-join (mean) is best understood by considering drop-path (Huang et al. 2016b). In a residual network where the input skip connection is always present, summation causes the convolutional layers to learn the residual (the difference from the input). On the other hand, in networks with several branches where any branch can be dropped, such as FractalNet (Larsson et al. (2016)), using the mean is preferable as it keeps the output smooth as branches are randomly dropped.", "startOffset": 96, "endOffset": 415}, {"referenceID": 10, "context": "The difference between sum and fractal-join (mean) is best understood by considering drop-path (Huang et al. 2016b). In a residual network where the input skip connection is always present, summation causes the convolutional layers to learn the residual (the difference from the input). On the other hand, in networks with several branches where any branch can be dropped, such as FractalNet (Larsson et al. (2016)), using the mean is preferable as it keeps the output smooth as branches are randomly dropped. Some researchers seem to prefer concatenation ( e.g., Szegedy et al. (2015a)).", "startOffset": 96, "endOffset": 587}, {"referenceID": 41, "context": "Maxout has been used for competition, as in locally competitive networks (Srivastava et al. 2014b) and competitive multi-scale networks Liao & Carneiro (2015). Maxout chooses only one of the activations and leads to Design Pattern 14: MaxOut for Competition.", "startOffset": 74, "endOffset": 159}, {"referenceID": 19, "context": "A similar idea has been proposed for recurrent neural networks (Krueger et al. 2016).", "startOffset": 63, "endOffset": 84}, {"referenceID": 3, "context": "We used freeze-drop-path as the final join in the FoF architecture in Figure 1b and named it Stagewise Boosting Networks (SBN) because they are analogous to stagewise boosting (Friedman et al. 2001).", "startOffset": 176, "endOffset": 198}, {"referenceID": 9, "context": "Drop-path was introduced by Huang et al. (2016b). It works by randomly removing branches during an iteration of training, as though that path doesn\u2019t exist in the network.", "startOffset": 28, "endOffset": 49}, {"referenceID": 24, "context": "We call this \u201cTaylor Series Networks\u201d (TSN) and there is precedence for polynomial networks (Livni et al. 2014) and multiplication in networks (e.", "startOffset": 92, "endOffset": 111}, {"referenceID": 20, "context": "The results from the original FractalNet (Larsson et al. 2016) are given in the first row of the table and we use as our baseline.", "startOffset": 41, "endOffset": 62}], "year": 2016, "abstractText": "Recent research in the deep learning field has produced a plethora of new architectures. At the same time, a growing number of groups are applying deep learning to new applications and problems. Many of these groups might be composed of inexperienced deep learning practitioners who are baffled by the dizzying array of architecture choices and therefore use an older architecture, such as Alexnet. Here, we are attempting to bridge this gap by mining the collective knowledge contained in recent deep learning research to discover underlying principles for designing neural network architectures. In addition, we describe several architectural innovations, including Fractal of FractalNet, Stagewise Boosting Networks, and Taylor Series Networks (our Caffe code and prototxt files will be made publicly available upon acceptance to ICLR). We hope others are inspired to build on this preliminary work.", "creator": "LaTeX with hyperref package"}}}