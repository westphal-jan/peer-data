{"id": "1611.00862", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Nov-2016", "title": "Quantile Reinforcement Learning", "abstract": "In reinforcement learning, the standard criterion to evaluate policies in a state is the expectation of (discounted) sum of rewards. However, this criterion may not always be suitable, we consider an alternative criterion based on the notion of quantiles. In the case of episodic reinforcement learning problems, we propose an algorithm based on stochastic approximation with two timescales. We evaluate our proposition on a simple model of the TV show, Who wants to be a millionaire.", "histories": [["v1", "Thu, 3 Nov 2016 02:28:53 GMT  (252kb,D)", "http://arxiv.org/abs/1611.00862v1", "AWRL 2016"]], "COMMENTS": "AWRL 2016", "reviews": [], "SUBJECTS": "cs.LG cs.AI", "authors": ["hugo gilbert", "paul weng"], "accepted": false, "id": "1611.00862"}, "pdf": {"name": "1611.00862.pdf", "metadata": {"source": "CRF", "title": "Quantile Reinforcement Learning", "authors": ["Hugo Gilbert", "Paul Weng", "Gilbert Weng"], "emails": ["hugo.gilbert@lip6.fr", "paweng@cmu.edu"], "sections": [{"heading": null, "text": "Keywords: learning amplification, quantile, ordinal decision model, stochastic approach on two time scales"}, {"heading": "1. Introduction", "text": "In practice, such numerical rewards may not be available, for example, if the agent gives proper feedback (e.g. \"good,\" \"bad,\" and so on)."}, {"heading": "2. Related Work", "text": "Much of the research on MDPs (Boussard et al., 2010) looked at decision criteria that differ from the standard criteria (i.e., expected discounted sum of rewards, expected total rewards, or expected average rewards). In this context, he considered in particular several cases where policy preferences are based only on sums of rewards: expected usefulness (EU), likely limitations, and medium variance formulations. In this context, he showed the sufficiency of work in a state area, the preferences received so far. Filar et al. (1989) examined decision criteria that are variance versions of the standard versions. They formulated the optimization problem obtained as a non-linear program. (1998), he optimized the likelihood that the overall reward will be higher than a certain minimum sum. Additionally, he examined the use of decision criteria in the artificial intelligence community, Liu and KoenEU (2006)."}, {"heading": "3. Background", "text": "In this section, we provide the background information necessary to present our algorithm, to learn a policy that is optimal for the quantitative criterion."}, {"heading": "3.1. Markov Decision Process", "text": "Markov Decision Processes (MDPs) provide a powerful formalism for modeling and solving the following decision problems (Puterman, 1994).A finite horizon MDP is formally defined as a tuple MT = (S, A, P, R, s0), where: \u2022 T is a finite horizon, \u2022 S is a finite series of states, \u2022 A is a finite series of actions, \u2022 P: S \u00b7 A \u00b7 S \u2192 R is a transitional function with P (s, a, s), where the probability of reaching a state is s \u00b2 when an action is performed in states, \u2022 R: S \u00b7 A \u2192 R is a limited reward function, and \u2022 s0 \u0432S is an initial state.In this model, an agent selects an action at each step to perform in its current state what they can observe."}, {"heading": "3.2. Reinforcement Learning", "text": "As we interact with its environment, an RL agent tries to learn a good strategy by trial and error. In order to make a finite horizon of MDPs learnable, we assume that the decision-making process is repeated infinitely. That is, when Horizon T is reached, we assume that the agent automatically returns to the initial state and starts the problem anew. A simple algorithm to solve such an RL problem is the Q learning algorithm (see Algorithm 1), which estimates the Q function: Qt (s, a) = R (s, a) + \u03b3 s \"s\" s \"S P (s, a, s\") Vt \u2212 1 (s \"). And obviously we have: Vt (s) = max a\" A \u2212 n. \"In Algorithm 1, Line 1 generally depends on the Qt \u2212 1 (s\") probability that a Qt \u2212 1 (s \") strategy (s) is best selected (s)."}, {"heading": "3.3. Limits of standard criteria", "text": "The standard decision criteria used in the MDPs, which are based on expectations, may not be reasonable in some situations. Firstly, the reward function R is unfortunately not known in many cases. In these cases, one can try to restore the reward function of a human expert (Ng and Russell, 2000; Regan and Boutilier, 2009; Weng and Zanuttini, 2013). However, even for an expert, the collection of the reward function can be stressful. In reverse reinforcement learning (Ng and Russell, 2000), it is assumed that the expert knows anData: MT = (S, A, G, P, R, s0) MDP result: Q beginQ0 (s, a). (s, a). (s, a). (s, a)...................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................."}, {"heading": "3.4. MDP with End States", "text": "In this paper, we work with episodic MDPs with end states. Such an MDP is formally defined as TupelMT = (S, A, G, P, s0), where S, A, P, s0 are defined as before, G S is a finite set of end states and T is a finite maximum horizon (i.e., a end state is reached after at most T time steps.) We refer to episode as a story that begins from s0 and is in a final state of G. We assume that a preference relationship to end states is defined: We write g \u2032 ig when the end state g is preferred to end state g. \"Without loss of generality, we assume that G = {g1,... gn} and end states are ordered with increasing preference, i.e. g1, g2, which means."}, {"heading": "3.5. Quantile Criterion", "text": "We define quantities of distributions over terminal states of G. Let us define quantities of distributions over terminal states. (Let us define quantities of distributions over terminal states.) Let us define quantities of distributions over terminal states. (Let us define quantities of distributions over terminal states.) Let us define quantities of distributions over terminal states. (Let us define quantities of distributions over terminal states.) Let us define quantities of distribution criteria as quantities of distribution criteria, which do not require the ordinary counterpart of the mean, but only an order. The formal definition of quantities can be defined as quantities of distributions criteria. Quantities (Let quantities be defined as quantities of quantities)."}, {"heading": "4. Quantile-based Reinforcement Learning", "text": "In this section we first present the problem solved in this essay and some useful properties, and then we present our algorithm called Quantile Q-learning (or QQ-learning for short), which is an extension of Q-learning and uses a stochastic approximation technique on two time scales."}, {"heading": "4.1. Problem Statement", "text": "We assume that the underlying MDP is an episodic end-state MDP. Let it be a fixed parameter. Formally, the problem of determining a policy that is optimal for the lower / upper quantitative level can be defined as follows: \u03c0 = arg max quantitative quantitative level. (4) We focus on learning a policy that is deterministic and Markovian. The optimal lower / upper quantitative levels satisfy the following dimensions: Lemma 1 The optimal lower quantitative level - quantitative level q quantitative level satisfies: q quantitative level = min {g). (5) F quantitative level (g) = mental level F quantitative level (g) and the optimal upper quantitative level - quantitative level satisfies: q quantitative level."}, {"heading": "4.2. QQ-learning", "text": "Since the lower and upper quantities are not known, we will vary the parameter \u03b8 during the learning steps in R + and refine the definition of the previous reward functions to ensure that they are well defined for all subjects as well as for all subjects. \u2212 A similar approach can be developed for the lower quantities. \u2212 To find the optimal upper quantity, one could use the strategy Qgorithm 2 described in Algorithm 2. the value V-quantity-quantity-quantity-quantity-quantity-quantity-quantity-quantity-quantity-quantity-quantity-quantity-quantity-quantity-quantity-quantity-quantity-quantity-quantity-quantity-quantity-quantity-quantity-quantity-quantity-quantity-quantity-quantity-quantity-quantity-quantity-quantity-quantity-quantity-quantity-quantity-quantity-quantity-quantity-quantity-quantity-quantity-quantity-quantity-quantity-quantity-quantity-quantity-quantity-quantity-quantity-quantity-quantity-quantity-quantity-quantity-quantity-quantity-quantity-quantity-quantity-quantity-quantity-quantity-quantity-quantity-quantity-quantity-quantity-quantity-quantity-quantity-quantity-quantity-quantity-quantity-quantity-quantity-quantity-quantity-quantity-quantity-quantity-quantity-quantity-quantity-quantity-quantity-quantity-quantity-quantity-quantity-quantity-quantity-quantity-quantity-quantity-quantity-quantity-quantity-quantity-quantity-quantity-quantity-quantity-quantity-quantity-quantity-quantity-quantity-quantity-quantity-quantity-quantity-quantity-quantity-quantity-quantity-quantity-quantity-quantity-quantity-quantity-quantity-quantity-quantity-quantity-quantity-quantity-quantity-quantity-quantity-quantity-quantity-quantity-quantity-quantity-quantity-quantity"}, {"heading": "5. Experimental Results", "text": "To demonstrate the soundness of our approach, we evaluate our algorithm on the Who Wants to Be a Millionaire domain. Below, we present the experimental results."}, {"heading": "5.1. Domain", "text": "In this popular television game show, a participant must answer a maximum of 15 multiple choice questions (with four possible answers) with increasing difficulty, with the sums increasing and the pot roughly doubling with each question. At each step, the participant can choose to walk with the money currently won. If he answers incorrectly, all winnings will be lost except what was earned on a \"guarantee point\" (Questions 5 and 10).The player is allowed 3 lifelines (50: 50, eliminating two choices, asking the audience and asking a friend for suggestions), each of which can only be used once. We used the first model of the Spanish version of the 2003 game presented by Perea and Puerto (2007).The probability of answering correctly depends on the number of questions and is increased by the lifelines used (if any)."}, {"heading": "5.2. Results", "text": "We record the results (see Figure 1) obtained for two different learning sequences, one with 1 million learning steps and the other with 10 million learning steps. We can verify that Equation 11 is fulfilled. We optimize the upper quantile with \u03c4 = 0.3. In the course of the learning process, we obtain a vector f of frequencies with which each final state has been reached. We define the quantity value as a cross-product between f and the vector of rewards that is achieved when each final state is reached taking into account the current value of inequality. In other words, the value of the score is the value of the non-stationary policy that has been played since the beginning of the learning process. In addition, for each iteration, we calculate the V-value of inequality (s0), the optimal value in s0 taking into account the current value of inequality \u2212 In Figures 1 (a) and 1 (b), inequality is increased by increasing inequality (see Figure 1)."}, {"heading": "6. Conclusion", "text": "We have presented an algorithm for learning a policy that is optimal for the quantitative criterion in the learning environment of amplification when the MDP has a special structure that responds to repeated episodic decision problems. It is based on a stochastic approach with two time scales (Borkar, 2008). Our proposal will be validated experimentally in the field of Who Wants to Become a Millionaire. As a future work, it would be interesting to investigate how to choose the learning rate \u03b1n to ensure rapid convergence. Furthermore, our approach could be extended to settings other than episodic MDPs. Furthermore, it would be interesting to investigate whether gradient-based algorithms for quantity optimization could be developed based on the fact that a quantile is the solution to an optimization problem where the objective function is piecemeal (Koenker, 2005)."}], "references": [{"title": "Autonomous helicopter aerobatics through apprenticeship learning", "author": ["Pieter Abbeel", "Adam Coates", "Andrew Y. Ng"], "venue": "International Journal of Robotics Research,", "citeRegEx": "Abbeel et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Abbeel et al\\.", "year": 2010}, {"title": "S\u00e9bag. April: Active preference-learning based reinforcement learning", "author": ["R. Akrour", "M. Schoenauer"], "venue": "In ECML PKDD, Lecture Notes in Computer Science,", "citeRegEx": "Akrour et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Akrour et al\\.", "year": 2012}, {"title": "Markov decision processes with average value-at-risk criteria", "author": ["Nicole B\u00e4uerle", "Jonathan Ott"], "venue": "Mathematical Methods of Operations Research,", "citeRegEx": "B\u00e4uerle and Ott.,? \\Q2011\\E", "shortCiteRegEx": "B\u00e4uerle and Ott.", "year": 2011}, {"title": "Benefits of quantile regression for the analysis of customer lifetime value in a contractual setting: An application in financial services", "author": ["D.F. Benoit", "D. Van den Poel"], "venue": "Expert Systems with Applications,", "citeRegEx": "Benoit and Poel.,? \\Q2009\\E", "shortCiteRegEx": "Benoit and Poel.", "year": 2009}, {"title": "An analog of the minimax theorem for vector payoffs", "author": ["D. Blackwell"], "venue": "Pacific Journal of Mathematics,", "citeRegEx": "Blackwell.,? \\Q1956\\E", "shortCiteRegEx": "Blackwell.", "year": 1956}, {"title": "Risk-constrained Markov decision processes", "author": ["V. Borkar", "Rahul Jain"], "venue": "IEEE Transactions on Automatic Control,", "citeRegEx": "Borkar and Jain.,? \\Q2014\\E", "shortCiteRegEx": "Borkar and Jain.", "year": 2014}, {"title": "Stochastic approximation : a dynamical systems viewpoint. Cambridge university press New Delhi, Cambridge, 2008. ISBN 978-0-521-51592-4", "author": ["Vivek S. Borkar"], "venue": "URL http://opac.inria.fr/ record=b1132816", "citeRegEx": "Borkar.,? \\Q2008\\E", "shortCiteRegEx": "Borkar.", "year": 2008}, {"title": "Stochastic approximation with time scales", "author": ["V.S. Borkar"], "venue": "Systems & Control Letters,", "citeRegEx": "Borkar.,? \\Q1997\\E", "shortCiteRegEx": "Borkar.", "year": 1997}, {"title": "Markov Decision Processes in Artificial Intelligence, chapter Non-Standard Criteria", "author": ["M. Boussard", "M. Bouzid", "A.I. Mouaddib", "R. Sabbadin", "P. Weng"], "venue": null, "citeRegEx": "Boussard et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Boussard et al\\.", "year": 2010}, {"title": "Meet your expectations with guarantees: beyond worst-case synthesis in quantitative games", "author": ["V\u00e9ronique Bruy\u00e8re", "Emmanuel Filiot", "Mickael Randour", "Jean-Fran\u00e7ois Raskin"], "venue": "In STACS,", "citeRegEx": "Bruy\u00e8re et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bruy\u00e8re et al\\.", "year": 2014}, {"title": "Preference-based reinforcement learning", "author": ["R. Busa-Fekete", "B. Sz\u00f6renyi", "P. Weng", "W. Cheng", "E. H\u00fcllermeier"], "venue": "In European Workshop on Reinforcement Learning, Dagstuhl Seminar,", "citeRegEx": "Busa.Fekete et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Busa.Fekete et al\\.", "year": 2013}, {"title": "Preferencebased Reinforcement Learning: Evolutionary Direct Policy Search using a Preference-based Racing Algorithm", "author": ["Robert Busa-Fekete", "Balazs Szorenyi", "Paul Weng", "Weiwei Cheng", "Eyke H\u00fcllermeier"], "venue": "Machine Learning,", "citeRegEx": "Busa.Fekete et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Busa.Fekete et al\\.", "year": 2014}, {"title": "Algorithms for cvar optimization in MDPs", "author": ["Yinlam Chow", "Mohammad Ghavamzadeh"], "venue": "In NIPS,", "citeRegEx": "Chow and Ghavamzadeh.,? \\Q2014\\E", "shortCiteRegEx": "Chow and Ghavamzadeh.", "year": 2014}, {"title": "Dynamo: amazon\u2019s highly available key-value store", "author": ["G. DeCandia", "D. Hastorun", "M. Jampani", "G. Kakulapati", "A. Lakshman", "A. Pilchin", "S. Sivasubramanian", "P. Vosshall", "W. Vogels"], "venue": "ACM SIGOPS Operating Systems Review,", "citeRegEx": "DeCandia et al\\.,? \\Q2007\\E", "shortCiteRegEx": "DeCandia et al\\.", "year": 2007}, {"title": "Percentile optimization in uncertain Markov decision processes with application to efficient exploration", "author": ["E. Delage", "S. Mannor"], "venue": "In ICML,", "citeRegEx": "Delage and Mannor.,? \\Q2007\\E", "shortCiteRegEx": "Delage and Mannor.", "year": 2007}, {"title": "Resolute choice in sequential decision problems with multiple priors", "author": ["H\u00e9l\u00e8ne Fargier", "Gildas Jeantet", "Olivier Spanjaard"], "venue": "In IJCAI,", "citeRegEx": "Fargier et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Fargier et al\\.", "year": 2011}, {"title": "Percentiles and Markovian decision processes", "author": ["Jerzy A. Filar"], "venue": "Operations Research Letters,", "citeRegEx": "Filar.,? \\Q1983\\E", "shortCiteRegEx": "Filar.", "year": 1983}, {"title": "Variance-penalized Markov decision processes", "author": ["Jerzy A. Filar", "L.C.M. Kallenberg", "Huey-Miin Lee"], "venue": "Mathematics of Operations Research,", "citeRegEx": "Filar et al\\.,? \\Q1989\\E", "shortCiteRegEx": "Filar et al\\.", "year": 1989}, {"title": "An axiomatic characterization of skew-symmetric bilinear functionals, with applications to utility theory", "author": ["P.C. Fishburn"], "venue": "Economics Letters,", "citeRegEx": "Fishburn.,? \\Q1981\\E", "shortCiteRegEx": "Fishburn.", "year": 1981}, {"title": "Preference-based reinforcement learning: A formal framework and a policy iteration algorithm", "author": ["J. F\u00fcrnkranz", "E. H\u00fcllermeier", "W. Cheng", "S.H. Park"], "venue": "Machine Learning,", "citeRegEx": "F\u00fcrnkranz et al\\.,? \\Q2012\\E", "shortCiteRegEx": "F\u00fcrnkranz et al\\.", "year": 2012}, {"title": "Solving MDPs with skew symmetric bilinear utility functions", "author": ["Hugo Gilbert", "Olivier Spanjaard", "Paolo Viappiani", "Paul Weng"], "venue": null, "citeRegEx": "Gilbert et al\\.,? \\Q1989\\E", "shortCiteRegEx": "Gilbert et al\\.", "year": 1989}, {"title": "Model-free reinforcement learning with skew-symmetric bilinear utilities", "author": ["Hugo Gilbert", "Bruno Zanuttini", "Paolo Viappiani", "Paul Weng", "Esther Nicart"], "venue": "In International Conference on Uncertainty in Artificial Intelligence (UAI),", "citeRegEx": "Gilbert et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Gilbert et al\\.", "year": 2016}, {"title": "Pure stationary optimal strategies in Markov decision processes", "author": ["Hugo Gimbert"], "venue": "In STACS,", "citeRegEx": "Gimbert.,? \\Q2007\\E", "shortCiteRegEx": "Gimbert.", "year": 2007}, {"title": "Implementing resolute choice under uncertainty", "author": ["Jean-Yves Jaffray"], "venue": "In UAI,", "citeRegEx": "Jaffray.,? \\Q1998\\E", "shortCiteRegEx": "Jaffray.", "year": 1998}, {"title": "Value-at-Risk: The New Benchmark for Managing", "author": ["Philippe Jorion"], "venue": "Financial Risk. McGraw-Hill,", "citeRegEx": "Jorion.,? \\Q2006\\E", "shortCiteRegEx": "Jorion.", "year": 2006}, {"title": "A learning scheme for blackwell\u2019s approachability in mdps and stackelberg stochastic games", "author": ["D. Kalathil", "V.S. Borkar", "R. Jain"], "venue": "Arxiv,", "citeRegEx": "Kalathil et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kalathil et al\\.", "year": 2014}, {"title": "Quantile Regression", "author": ["R. Koenker"], "venue": "Cambridge university press,", "citeRegEx": "Koenker.,? \\Q2005\\E", "shortCiteRegEx": "Koenker.", "year": 2005}, {"title": "Risk-sensitive planning with one-switch utility functions: Value iteration", "author": ["Y. Liu", "S. Koenig"], "venue": "In AAAI,", "citeRegEx": "Liu and Koenig.,? \\Q2005\\E", "shortCiteRegEx": "Liu and Koenig.", "year": 2005}, {"title": "Functional value iteration for decision-theoretic planning with general utility functions", "author": ["Y. Liu", "S. Koenig"], "venue": "In AAAI,", "citeRegEx": "Liu and Koenig.,? \\Q2006\\E", "shortCiteRegEx": "Liu and Koenig.", "year": 2006}, {"title": "Rationality and dynamic choice: Foundational explorations", "author": ["E. McClennen"], "venue": "Cambridge university press,", "citeRegEx": "McClennen.,? \\Q1990\\E", "shortCiteRegEx": "McClennen.", "year": 1990}, {"title": "Human-level control through deep reinforcement learning", "author": ["Volodymyr Mnih", "Koray Kavukcuoglu", "David Silver", "Andrei A. Rusu", "Joel Veness", "Marc G. Bellemare", "Alex Graves", "Martin Riedmiller", "Andreas K. Fidjeland", "Georg Ostrovski", "Stig Petersen", "Charles Beattie", "Amir Sadik", "Ioannis Antonoglou", "Helen King", "Dharshan Kumaran", "Daan Wierstra", "Shane Legg", "Demis Hassabis"], "venue": "Nature, 518:529\u2013533,", "citeRegEx": "Mnih et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2015}, {"title": "Algorithms for inverse reinforcement learning", "author": ["A.Y. Ng", "S. Russell"], "venue": null, "citeRegEx": "Ng and Russell.,? \\Q2000\\E", "shortCiteRegEx": "Ng and Russell.", "year": 2000}, {"title": "Dynamic programming analysis of the TV game who wants to be a millionaire", "author": ["F. Perea", "J. Puerto"], "venue": "European Journal of Operational Research,", "citeRegEx": "Perea and Puerto.,? \\Q2007\\E", "shortCiteRegEx": "Perea and Puerto.", "year": 2007}, {"title": "Markov decision processes: discrete stochastic dynamic programming", "author": ["M.L. Puterman"], "venue": null, "citeRegEx": "Puterman.,? \\Q1994\\E", "shortCiteRegEx": "Puterman.", "year": 1994}, {"title": "Percentile queries in multi-dimensional Markov decision processes", "author": ["Mickael Randour", "Jean-Fran\u00e7ois Raskin", "Ocan Sankur"], "venue": "CoRR, abs/1410.4801,", "citeRegEx": "Randour et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Randour et al\\.", "year": 2014}, {"title": "Regret based reward elicitation for Markov decision processes. In UAI, pages 444\u2013451", "author": ["K. Regan", "C. Boutilier"], "venue": null, "citeRegEx": "Regan and Boutilier.,? \\Q2009\\E", "shortCiteRegEx": "Regan and Boutilier.", "year": 2009}, {"title": "Quantile maximization in decision theory", "author": ["M.J. Rostek"], "venue": "Review of Economic Studies,", "citeRegEx": "Rostek.,? \\Q2010\\E", "shortCiteRegEx": "Rostek.", "year": 2010}, {"title": "Qualitative multi-armed bandits: A quantile-based approach", "author": ["Bal\u00e1zs Sz\u00f6r\u00e9nyi", "R\u00f3bert Busa-Fekete", "Paul Weng", "Eyke H\u00fcllermeier"], "venue": "In ICML,", "citeRegEx": "Sz\u00f6r\u00e9nyi et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sz\u00f6r\u00e9nyi et al\\.", "year": 2015}, {"title": "Temporal difference learning and td-gammon", "author": ["Gerald Tesauro"], "venue": "Communications of the ACM,", "citeRegEx": "Tesauro.,? \\Q1995\\E", "shortCiteRegEx": "Tesauro.", "year": 1995}, {"title": "Markov decision processes with ordinal rewards: Reference point-based preferences", "author": ["P. Weng"], "venue": "In ICAPS,", "citeRegEx": "Weng.,? \\Q2011\\E", "shortCiteRegEx": "Weng.", "year": 2011}, {"title": "Ordinal decision models for Markov decision processes", "author": ["P. Weng"], "venue": "In ECAI,", "citeRegEx": "Weng.,? \\Q2012\\E", "shortCiteRegEx": "Weng.", "year": 2012}, {"title": "Interactive value iteration for Markov decision processes with unknown rewards", "author": ["P. Weng", "B. Zanuttini"], "venue": "In IJCAI,", "citeRegEx": "Weng and Zanuttini.,? \\Q2013\\E", "shortCiteRegEx": "Weng and Zanuttini.", "year": 2013}, {"title": "Utility, probabilistic constraints, mean and variance of discounted rewards in Markov decision processes", "author": ["D.J. White"], "venue": "OR Spektrum,", "citeRegEx": "White.,? \\Q1987\\E", "shortCiteRegEx": "White.", "year": 1987}, {"title": "QPRED: Using quantile predictions to improve power usage for private clouds", "author": ["R. Wolski", "J. Brevik"], "venue": "Technical report,", "citeRegEx": "Wolski and Brevik.,? \\Q2014\\E", "shortCiteRegEx": "Wolski and Brevik.", "year": 2014}, {"title": "Sample complexity of risk-averse bandit-arm selection", "author": ["Jia Yuan Yu", "Evdokia Nikolova"], "venue": "In IJCAI,", "citeRegEx": "Yu and Nikolova.,? \\Q2013\\E", "shortCiteRegEx": "Yu and Nikolova.", "year": 2013}, {"title": "Optimization models for the first arrival target distribution function in discrete time", "author": ["Stella X. Yu", "Yuanlie Lin", "Pingfan Yan"], "venue": "Journal of mathematical analysis and applications,", "citeRegEx": "Yu et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Yu et al\\.", "year": 1998}, {"title": "Spoken dialogue management as planning and acting under uncertainty", "author": ["B. Zhang", "Q. Cai", "J. Mao", "E Chang", "B. Guo"], "venue": "In 7th European Conference on Speech Communication and Technology,", "citeRegEx": "Zhang et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2001}], "referenceMentions": [{"referenceID": 38, "context": "Examples of such systems abound: expert backgammon player (Tesauro, 1995), dialogue systems (Zhang et al.", "startOffset": 58, "endOffset": 73}, {"referenceID": 46, "context": "Examples of such systems abound: expert backgammon player (Tesauro, 1995), dialogue systems (Zhang et al., 2001), acrobatic helicopter flight (Abbeel et al.", "startOffset": 92, "endOffset": 112}, {"referenceID": 0, "context": ", 2001), acrobatic helicopter flight (Abbeel et al., 2010) or human-level video game player (Mnih et al.", "startOffset": 37, "endOffset": 58}, {"referenceID": 30, "context": ", 2010) or human-level video game player (Mnih et al., 2015).", "startOffset": 41, "endOffset": 60}, {"referenceID": 1, "context": "More generally, preferencebased reinforcement learning (Akrour et al., 2012; F\u00fcrnkranz et al., 2012; Busa-Fekete et al., 2013, 2014) has been proposed to tackle situations where the only available preferential information concerns pairwise comparisons of histories.", "startOffset": 55, "endOffset": 132}, {"referenceID": 19, "context": "More generally, preferencebased reinforcement learning (Akrour et al., 2012; F\u00fcrnkranz et al., 2012; Busa-Fekete et al., 2013, 2014) has been proposed to tackle situations where the only available preferential information concerns pairwise comparisons of histories.", "startOffset": 55, "endOffset": 132}, {"referenceID": 24, "context": "In finance, it is a risk measure and is known as Value-at-Risk (Jorion, 2006).", "startOffset": 63, "endOffset": 77}, {"referenceID": 13, "context": "For its cloud computing services, Amazon reports (DeCandia et al., 2007) that they optimize the 99.", "startOffset": 49, "endOffset": 72}, {"referenceID": 43, "context": "In fact, decisions in the web industry are often made based on quantiles (Wolski and Brevik, 2014; DeCandia et al., 2007).", "startOffset": 73, "endOffset": 121}, {"referenceID": 13, "context": "In fact, decisions in the web industry are often made based on quantiles (Wolski and Brevik, 2014; DeCandia et al., 2007).", "startOffset": 73, "endOffset": 121}, {"referenceID": 8, "context": "A great deal of research on MDPs (Boussard et al., 2010) considered decision criteria different to the standard ones (i.", "startOffset": 33, "endOffset": 56}, {"referenceID": 8, "context": "A great deal of research on MDPs (Boussard et al., 2010) considered decision criteria different to the standard ones (i.e., expected discounted sum of rewards, expected total rewards or expected average rewards). For instance, in the operations research community, White (1987) notably considered different cases where preferences over policies only depend on sums of rewards: Expected Utility (EU), probabilistic constraints and mean-variance formulations.", "startOffset": 34, "endOffset": 278}, {"referenceID": 8, "context": "A great deal of research on MDPs (Boussard et al., 2010) considered decision criteria different to the standard ones (i.e., expected discounted sum of rewards, expected total rewards or expected average rewards). For instance, in the operations research community, White (1987) notably considered different cases where preferences over policies only depend on sums of rewards: Expected Utility (EU), probabilistic constraints and mean-variance formulations. In this context, he showed the sufficiency of working in a state space augmented with the sum of rewards obtained so far. Filar et al. (1989) investigated decision criteria that are variance-penalized versions of the standard ones.", "startOffset": 34, "endOffset": 600}, {"referenceID": 8, "context": "A great deal of research on MDPs (Boussard et al., 2010) considered decision criteria different to the standard ones (i.e., expected discounted sum of rewards, expected total rewards or expected average rewards). For instance, in the operations research community, White (1987) notably considered different cases where preferences over policies only depend on sums of rewards: Expected Utility (EU), probabilistic constraints and mean-variance formulations. In this context, he showed the sufficiency of working in a state space augmented with the sum of rewards obtained so far. Filar et al. (1989) investigated decision criteria that are variance-penalized versions of the standard ones. They formulated the obtained optimization problem as a non-linear program. Yu et al. (1998) optimized the probability that the total reward becomes higher than a certain threshold.", "startOffset": 34, "endOffset": 782}, {"referenceID": 18, "context": "(2015) investigated the use of Skew-Symmetric Bilinear (SSB) utility (Fishburn, 1981) functions \u2014 a generalization of EU that enables intransitive behaviors and violation of the independence axiom \u2014 as decision criteria in finite-horizon MDPs.", "startOffset": 69, "endOffset": 85}, {"referenceID": 1, "context": "Interestingly, SSB also encompasses probabilistic dominance, a decision criterion that is employed in preference-based sequential decision-making (Akrour et al., 2012; F\u00fcrnkranz et al., 2012; Busa-Fekete et al., 2013, 2014).", "startOffset": 146, "endOffset": 223}, {"referenceID": 19, "context": "Interestingly, SSB also encompasses probabilistic dominance, a decision criterion that is employed in preference-based sequential decision-making (Akrour et al., 2012; F\u00fcrnkranz et al., 2012; Busa-Fekete et al., 2013, 2014).", "startOffset": 146, "endOffset": 223}, {"referenceID": 34, "context": "This work has also been extended to the multiobjective setting (Randour et al., 2014).", "startOffset": 63, "endOffset": 85}, {"referenceID": 16, "context": "In MDPs with ordinal rewards (Weng, 2011, 2012; Filar, 1983), quantile-based decision models were proposed to compute policies that maximize a quantile using linear programming.", "startOffset": 29, "endOffset": 60}, {"referenceID": 4, "context": "This method has recently been exploited in achievability problems (Blackwell, 1956) in the context of multiobjective MDPs (Kalathil et al.", "startOffset": 66, "endOffset": 83}, {"referenceID": 25, "context": "This method has recently been exploited in achievability problems (Blackwell, 1956) in the context of multiobjective MDPs (Kalathil et al., 2014) and for learning SSB-optimal policies (Gilbert et al.", "startOffset": 122, "endOffset": 145}, {"referenceID": 21, "context": ", 2014) and for learning SSB-optimal policies (Gilbert et al., 2016).", "startOffset": 46, "endOffset": 68}, {"referenceID": 6, "context": "In the continuation of this work, Gilbert et al. (2015) investigated the use of Skew-Symmetric Bilinear (SSB) utility (Fishburn, 1981) functions \u2014 a generalization of EU that enables intransitive behaviors and violation of the independence axiom \u2014 as decision criteria in finite-horizon MDPs.", "startOffset": 34, "endOffset": 56}, {"referenceID": 1, "context": "Interestingly, SSB also encompasses probabilistic dominance, a decision criterion that is employed in preference-based sequential decision-making (Akrour et al., 2012; F\u00fcrnkranz et al., 2012; Busa-Fekete et al., 2013, 2014). In theoretical computer science, sophisticated decision criteria have also been studied in MDPs. For instance, Gimbert (2007) proved that many decision criteria based on expectation (of limsup, parity.", "startOffset": 147, "endOffset": 351}, {"referenceID": 1, "context": "Interestingly, SSB also encompasses probabilistic dominance, a decision criterion that is employed in preference-based sequential decision-making (Akrour et al., 2012; F\u00fcrnkranz et al., 2012; Busa-Fekete et al., 2013, 2014). In theoretical computer science, sophisticated decision criteria have also been studied in MDPs. For instance, Gimbert (2007) proved that many decision criteria based on expectation (of limsup, parity... of rewards) admit a stationary deterministic optimal policy. Bruy\u00e8re et al. (2014) considered sophisticated preferences over policies, which amounts to searching for policies that maximize the standard criterion while ensuring an expected sum of rewards higher than a threshold with probability higher than a fixed value.", "startOffset": 147, "endOffset": 512}, {"referenceID": 1, "context": "Interestingly, SSB also encompasses probabilistic dominance, a decision criterion that is employed in preference-based sequential decision-making (Akrour et al., 2012; F\u00fcrnkranz et al., 2012; Busa-Fekete et al., 2013, 2014). In theoretical computer science, sophisticated decision criteria have also been studied in MDPs. For instance, Gimbert (2007) proved that many decision criteria based on expectation (of limsup, parity... of rewards) admit a stationary deterministic optimal policy. Bruy\u00e8re et al. (2014) considered sophisticated preferences over policies, which amounts to searching for policies that maximize the standard criterion while ensuring an expected sum of rewards higher than a threshold with probability higher than a fixed value. This work has also been extended to the multiobjective setting (Randour et al., 2014). Recent work in Markov decision process and reinforcement learning considered conditional Value-at-risk (CVaR), a criterion related to quantile, as a risk measure. B\u00e4uerle and Ott (2011) proved the existence of deterministic wealth-Markovian policies optimal with respect to CVaR.", "startOffset": 147, "endOffset": 1024}, {"referenceID": 1, "context": "Interestingly, SSB also encompasses probabilistic dominance, a decision criterion that is employed in preference-based sequential decision-making (Akrour et al., 2012; F\u00fcrnkranz et al., 2012; Busa-Fekete et al., 2013, 2014). In theoretical computer science, sophisticated decision criteria have also been studied in MDPs. For instance, Gimbert (2007) proved that many decision criteria based on expectation (of limsup, parity... of rewards) admit a stationary deterministic optimal policy. Bruy\u00e8re et al. (2014) considered sophisticated preferences over policies, which amounts to searching for policies that maximize the standard criterion while ensuring an expected sum of rewards higher than a threshold with probability higher than a fixed value. This work has also been extended to the multiobjective setting (Randour et al., 2014). Recent work in Markov decision process and reinforcement learning considered conditional Value-at-risk (CVaR), a criterion related to quantile, as a risk measure. B\u00e4uerle and Ott (2011) proved the existence of deterministic wealth-Markovian policies optimal with respect to CVaR. Chow and Ghavamzadeh (2014) proposed gradient-based algorithms for CVaR optimization.", "startOffset": 147, "endOffset": 1146}, {"referenceID": 1, "context": "Interestingly, SSB also encompasses probabilistic dominance, a decision criterion that is employed in preference-based sequential decision-making (Akrour et al., 2012; F\u00fcrnkranz et al., 2012; Busa-Fekete et al., 2013, 2014). In theoretical computer science, sophisticated decision criteria have also been studied in MDPs. For instance, Gimbert (2007) proved that many decision criteria based on expectation (of limsup, parity... of rewards) admit a stationary deterministic optimal policy. Bruy\u00e8re et al. (2014) considered sophisticated preferences over policies, which amounts to searching for policies that maximize the standard criterion while ensuring an expected sum of rewards higher than a threshold with probability higher than a fixed value. This work has also been extended to the multiobjective setting (Randour et al., 2014). Recent work in Markov decision process and reinforcement learning considered conditional Value-at-risk (CVaR), a criterion related to quantile, as a risk measure. B\u00e4uerle and Ott (2011) proved the existence of deterministic wealth-Markovian policies optimal with respect to CVaR. Chow and Ghavamzadeh (2014) proposed gradient-based algorithms for CVaR optimization. In contrast, Borkar and Jain (2014) used CVaR in constraints instead of as objective function.", "startOffset": 147, "endOffset": 1240}, {"referenceID": 1, "context": "Interestingly, SSB also encompasses probabilistic dominance, a decision criterion that is employed in preference-based sequential decision-making (Akrour et al., 2012; F\u00fcrnkranz et al., 2012; Busa-Fekete et al., 2013, 2014). In theoretical computer science, sophisticated decision criteria have also been studied in MDPs. For instance, Gimbert (2007) proved that many decision criteria based on expectation (of limsup, parity... of rewards) admit a stationary deterministic optimal policy. Bruy\u00e8re et al. (2014) considered sophisticated preferences over policies, which amounts to searching for policies that maximize the standard criterion while ensuring an expected sum of rewards higher than a threshold with probability higher than a fixed value. This work has also been extended to the multiobjective setting (Randour et al., 2014). Recent work in Markov decision process and reinforcement learning considered conditional Value-at-risk (CVaR), a criterion related to quantile, as a risk measure. B\u00e4uerle and Ott (2011) proved the existence of deterministic wealth-Markovian policies optimal with respect to CVaR. Chow and Ghavamzadeh (2014) proposed gradient-based algorithms for CVaR optimization. In contrast, Borkar and Jain (2014) used CVaR in constraints instead of as objective function. Closer to our work, several quantile-based decision models have been investigated in different contexts. In uncertain MDPs where the parameters of the transition and reward functions are imprecisely known, Delage and Mannor (2007) presented and investigated a quantile-like criterion to capture the trade-off between optimistic and pessimistic viewpoints on an uncertain MDP.", "startOffset": 147, "endOffset": 1530}, {"referenceID": 1, "context": "Interestingly, SSB also encompasses probabilistic dominance, a decision criterion that is employed in preference-based sequential decision-making (Akrour et al., 2012; F\u00fcrnkranz et al., 2012; Busa-Fekete et al., 2013, 2014). In theoretical computer science, sophisticated decision criteria have also been studied in MDPs. For instance, Gimbert (2007) proved that many decision criteria based on expectation (of limsup, parity... of rewards) admit a stationary deterministic optimal policy. Bruy\u00e8re et al. (2014) considered sophisticated preferences over policies, which amounts to searching for policies that maximize the standard criterion while ensuring an expected sum of rewards higher than a threshold with probability higher than a fixed value. This work has also been extended to the multiobjective setting (Randour et al., 2014). Recent work in Markov decision process and reinforcement learning considered conditional Value-at-risk (CVaR), a criterion related to quantile, as a risk measure. B\u00e4uerle and Ott (2011) proved the existence of deterministic wealth-Markovian policies optimal with respect to CVaR. Chow and Ghavamzadeh (2014) proposed gradient-based algorithms for CVaR optimization. In contrast, Borkar and Jain (2014) used CVaR in constraints instead of as objective function. Closer to our work, several quantile-based decision models have been investigated in different contexts. In uncertain MDPs where the parameters of the transition and reward functions are imprecisely known, Delage and Mannor (2007) presented and investigated a quantile-like criterion to capture the trade-off between optimistic and pessimistic viewpoints on an uncertain MDP. The quantile criterion they use is different to ours as it takes into account the uncertainty present in the parameters of the MDP. In MDPs with ordinal rewards (Weng, 2011, 2012; Filar, 1983), quantile-based decision models were proposed to compute policies that maximize a quantile using linear programming. While quantiles in those works are defined on distributions over ordinal rewards, quantiles in this paper are defined on distributions over histories. More recently, in the machine learning community, quantile-based criteria have been proposed in the multi-armed bandit (MAB) setting, a special case of reinforcement learning. Yu and Nikolova (2013) proposed an algorithm in the pure exploration setting for different risk measures, including Value-at-Risk.", "startOffset": 147, "endOffset": 2335}, {"referenceID": 1, "context": "Interestingly, SSB also encompasses probabilistic dominance, a decision criterion that is employed in preference-based sequential decision-making (Akrour et al., 2012; F\u00fcrnkranz et al., 2012; Busa-Fekete et al., 2013, 2014). In theoretical computer science, sophisticated decision criteria have also been studied in MDPs. For instance, Gimbert (2007) proved that many decision criteria based on expectation (of limsup, parity... of rewards) admit a stationary deterministic optimal policy. Bruy\u00e8re et al. (2014) considered sophisticated preferences over policies, which amounts to searching for policies that maximize the standard criterion while ensuring an expected sum of rewards higher than a threshold with probability higher than a fixed value. This work has also been extended to the multiobjective setting (Randour et al., 2014). Recent work in Markov decision process and reinforcement learning considered conditional Value-at-risk (CVaR), a criterion related to quantile, as a risk measure. B\u00e4uerle and Ott (2011) proved the existence of deterministic wealth-Markovian policies optimal with respect to CVaR. Chow and Ghavamzadeh (2014) proposed gradient-based algorithms for CVaR optimization. In contrast, Borkar and Jain (2014) used CVaR in constraints instead of as objective function. Closer to our work, several quantile-based decision models have been investigated in different contexts. In uncertain MDPs where the parameters of the transition and reward functions are imprecisely known, Delage and Mannor (2007) presented and investigated a quantile-like criterion to capture the trade-off between optimistic and pessimistic viewpoints on an uncertain MDP. The quantile criterion they use is different to ours as it takes into account the uncertainty present in the parameters of the MDP. In MDPs with ordinal rewards (Weng, 2011, 2012; Filar, 1983), quantile-based decision models were proposed to compute policies that maximize a quantile using linear programming. While quantiles in those works are defined on distributions over ordinal rewards, quantiles in this paper are defined on distributions over histories. More recently, in the machine learning community, quantile-based criteria have been proposed in the multi-armed bandit (MAB) setting, a special case of reinforcement learning. Yu and Nikolova (2013) proposed an algorithm in the pure exploration setting for different risk measures, including Value-at-Risk. Carpentier and Valko (2014) studied the problem of identifying arms with extreme payoffs, a particular case of quantiles.", "startOffset": 147, "endOffset": 2471}, {"referenceID": 1, "context": "Interestingly, SSB also encompasses probabilistic dominance, a decision criterion that is employed in preference-based sequential decision-making (Akrour et al., 2012; F\u00fcrnkranz et al., 2012; Busa-Fekete et al., 2013, 2014). In theoretical computer science, sophisticated decision criteria have also been studied in MDPs. For instance, Gimbert (2007) proved that many decision criteria based on expectation (of limsup, parity... of rewards) admit a stationary deterministic optimal policy. Bruy\u00e8re et al. (2014) considered sophisticated preferences over policies, which amounts to searching for policies that maximize the standard criterion while ensuring an expected sum of rewards higher than a threshold with probability higher than a fixed value. This work has also been extended to the multiobjective setting (Randour et al., 2014). Recent work in Markov decision process and reinforcement learning considered conditional Value-at-risk (CVaR), a criterion related to quantile, as a risk measure. B\u00e4uerle and Ott (2011) proved the existence of deterministic wealth-Markovian policies optimal with respect to CVaR. Chow and Ghavamzadeh (2014) proposed gradient-based algorithms for CVaR optimization. In contrast, Borkar and Jain (2014) used CVaR in constraints instead of as objective function. Closer to our work, several quantile-based decision models have been investigated in different contexts. In uncertain MDPs where the parameters of the transition and reward functions are imprecisely known, Delage and Mannor (2007) presented and investigated a quantile-like criterion to capture the trade-off between optimistic and pessimistic viewpoints on an uncertain MDP. The quantile criterion they use is different to ours as it takes into account the uncertainty present in the parameters of the MDP. In MDPs with ordinal rewards (Weng, 2011, 2012; Filar, 1983), quantile-based decision models were proposed to compute policies that maximize a quantile using linear programming. While quantiles in those works are defined on distributions over ordinal rewards, quantiles in this paper are defined on distributions over histories. More recently, in the machine learning community, quantile-based criteria have been proposed in the multi-armed bandit (MAB) setting, a special case of reinforcement learning. Yu and Nikolova (2013) proposed an algorithm in the pure exploration setting for different risk measures, including Value-at-Risk. Carpentier and Valko (2014) studied the problem of identifying arms with extreme payoffs, a particular case of quantiles. Finally, Sz\u00f6r\u00e9nyi et al. (2015) investigated MAB problems where a quantile is optimized instead of the mean.", "startOffset": 147, "endOffset": 2597}, {"referenceID": 33, "context": "Markov Decision Process Markov Decision Processes (MDPs) offer a powerful formalism to model and solve sequential decision-making problems (Puterman, 1994).", "startOffset": 139, "endOffset": 155}, {"referenceID": 31, "context": "In those cases, one can try to recover the reward function from a human expert (Ng and Russell, 2000; Regan and Boutilier, 2009; Weng and Zanuttini, 2013).", "startOffset": 79, "endOffset": 154}, {"referenceID": 35, "context": "In those cases, one can try to recover the reward function from a human expert (Ng and Russell, 2000; Regan and Boutilier, 2009; Weng and Zanuttini, 2013).", "startOffset": 79, "endOffset": 154}, {"referenceID": 41, "context": "In those cases, one can try to recover the reward function from a human expert (Ng and Russell, 2000; Regan and Boutilier, 2009; Weng and Zanuttini, 2013).", "startOffset": 79, "endOffset": 154}, {"referenceID": 31, "context": "In inverse reinforcement learning (Ng and Russell, 2000), the expert is assumed to know an", "startOffset": 34, "endOffset": 56}, {"referenceID": 35, "context": "In interactive settings (Regan and Boutilier, 2009; Weng and Zanuttini, 2013), this elicitation process can be cognitively very complex as it requires to balance several criteria in a complex manner and as it can imply a large number of parameters.", "startOffset": 24, "endOffset": 77}, {"referenceID": 41, "context": "In interactive settings (Regan and Boutilier, 2009; Weng and Zanuttini, 2013), this elicitation process can be cognitively very complex as it requires to balance several criteria in a complex manner and as it can imply a large number of parameters.", "startOffset": 24, "endOffset": 77}, {"referenceID": 36, "context": "More generally, quantiles, which have been axiomatically characterized by Rostek (2010), define decision criteria that have the nice property of not requiring numeric valuations, but only an order.", "startOffset": 74, "endOffset": 88}, {"referenceID": 40, "context": "When the lower and the upper quantiles differ, one may define the quantile as a function of the lower and upper quantiles (Weng, 2012).", "startOffset": 122, "endOffset": 134}, {"referenceID": 29, "context": "In decision theory (McClennen, 1990), three approaches have been considered for such kinds of decision criteria:", "startOffset": 19, "endOffset": 36}, {"referenceID": 23, "context": "Sophisticated resolute choice approach (Jaffray, 1998; Fargier et al., 2011): apply a policy \u03c0 (chosen at the beginning) that trades off between how much \u03c0 is optimal for all horizons T, T \u2212 1, .", "startOffset": 39, "endOffset": 76}, {"referenceID": 15, "context": "Sophisticated resolute choice approach (Jaffray, 1998; Fargier et al., 2011): apply a policy \u03c0 (chosen at the beginning) that trades off between how much \u03c0 is optimal for all horizons T, T \u2212 1, .", "startOffset": 39, "endOffset": 76}, {"referenceID": 32, "context": "We used the first model of the Spanish 2003 version of the game presented by Perea and Puerto (2007). The probability of answering correctly is a function of the question\u2019s number and increased by the lifelines used (if any).", "startOffset": 77, "endOffset": 101}, {"referenceID": 6, "context": "It is based on stochastic approximation with two timescales (Borkar, 2008).", "startOffset": 60, "endOffset": 74}, {"referenceID": 26, "context": "Besides, it would also be interesting to explore whether gradient-based algorithms could be developed for the optimization of quantiles, based on the fact that a quantile is solution of an optimization problem where the objective function is piecewise linear (Koenker, 2005).", "startOffset": 259, "endOffset": 274}], "year": 2016, "abstractText": "In reinforcement learning, the standard criterion to evaluate policies in a state is the expectation of (discounted) sum of rewards. However, this criterion may not always be suitable, we consider an alternative criterion based on the notion of quantiles. In the case of episodic reinforcement learning problems, we propose an algorithm based on stochastic approximation with two timescales. We evaluate our proposition on a simple model of the TV show, Who wants to be a millionaire.", "creator": "LaTeX with hyperref package"}}}