{"id": "1611.00898", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Nov-2016", "title": "Low Rank Approximation with Entrywise $\\ell_1$-Norm Error", "abstract": "We study the $\\ell_1$-low rank approximation problem, where for a given $n \\times d$ matrix $A$ and approximation factor $\\alpha \\geq 1$, the goal is to output a rank-$k$ matrix $\\widehat{A}$ for which", "histories": [["v1", "Thu, 3 Nov 2016 07:13:20 GMT  (146kb,D)", "http://arxiv.org/abs/1611.00898v1", null]], "reviews": [], "SUBJECTS": "cs.DS cs.CC cs.LG", "authors": ["zhao song", "david p woodruff", "peilin zhong"], "accepted": false, "id": "1611.00898"}, "pdf": {"name": "1611.00898.pdf", "metadata": {"source": "CRF", "title": "Low Rank Approximation with Entrywise `1-Norm Error", "authors": ["Zhao Song", "David P. Woodruff", "Peilin Zhong"], "emails": ["zhaos@utexas.edu", "dpwoodru@us.ibm.com", "peilin.zhong@columbia.edu"], "sections": [{"heading": null, "text": "This error measure is known to be more robust than the Frobenius norm in the presence of outliers and is given in models where Gauss \"assumptions about noise may not be correct. Gillis and Vavasis presented the problem as NP-hard and suggested a number of heuristics. We asked at several points whether there are approach algorithms. We give the first detectable approach algorithms for\" low-ranking approach factor 1 \"and show that it is possible to achieve approach factor \u03b1 = (log d) \u00b7 poly (k) in nnz (A) + (n + d) poly (k) time."}, {"heading": "1 Introduction", "text": "Two well-studied linear approaches in numerical algebra are regression and low marginal approximation."}, {"heading": "1.1 Our Results", "text": "We give the first efficient algorithms for a general class of algorithms based on linear sketches that guarantee an approximate approximation. We give the first efficient algorithms for an approximate approximation to a provable approximation to an approximate approximation to an actual approximation. We assume that we can assume a constant approximation to a constant approximation factor that provides an exponential improvement over the previous approximation factor of O (A) + n (A) provided that it is not too large, and is the polynomial time for each k. In addition, our time is optimal up to a constant factor such as a constant approximation to a relative algorithm. We also give a hard instance for our algorithms that exclude a logbook approximation to k."}, {"heading": "1.2 Technical Overview", "text": "It is not as if we would be able to find such a solution if we were able to find the i-th columns of V and A, respectively, the i-th columns of V and A. We could solve this with linear programming, although this is not helpful for our reasoning. Instead, we are excited by the recent advances in sketching for linear algebra (see, for example, [Woo14b] for a survey, we could choose and solve a random matrix S."}, {"heading": "1.3 Several Theorem Statements, an Algorithm, and a Roadmap", "text": "Algorithm 1 Main meta algorithm 1: Method L1LowRankApprox (A, n, d, k). Theorem 1,2 2: Select the sketch matrix S (a cauchy matrix or a sparse cauchy matrix.) 3: Calculate Sa, form C of Ci \u2190 arg minx, xSA \u2212 Ai, 1. Form B = C \u00b7 SA. 4: Select \u2212 sketch matrices T1, R, D, T2 (cauchy matrices or sparse cauchy matrices.) 5: Solve minX, Y-T1BRXYDBT2 \u2212 kB, Y-DB. 7: end procedureTheorem 1,2 (informal version of theorem C.6). Given A-Rn \u00d7 d, there is an algorithm that gives in nnz (A + d) \u00b7 poly (k) time."}, {"heading": "Contents", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "1 Introduction 1", "text": "1.1 Our results......................................................................................................................."}, {"heading": "A Notation 22", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "B Preliminaries 22", "text": "......................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................"}, {"heading": "D Contraction and Dilation Bound for `1 44", "text": "In the second half of the year, the number of deaths over the past decade will rise to more than 1,000."}, {"heading": "G Hardness results for Cauchy matrices, row subset selection, OSE 68", "text": "G.1 Hard instance for Cauchy matrices............................. 68 G.2 Hard instance for selection of row subgroups..................... 72 G.3 Hard instance for embedding forgotten subranges and more selection of row subgroups.... 77G.3.1 Definitions........................................................."}, {"heading": "H Hardness 91", "text": "H.1 Previous results................................................................................................................................."}, {"heading": "I Limited independent Cauchy random variables 101", "text": "I.1 Notations and tools............................................ 101 I.2 Analysis of limited independent Cauchy variables.................. 101"}, {"heading": "J Streaming Setting 104", "text": "J.1 Definitions......................................................................................................................................................................................................................."}, {"heading": "K Distributed Setting 108", "text": "K.1 Definitions..........................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................."}, {"heading": "L Experiments and Discussions 115", "text": "L.1 Setup..............................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................."}, {"heading": "A Notation", "text": "Let N + specify the set of positive integers. Let [n] define the set {1, 2, \u00b7 \u00b7, n} for each p \u2264 [1, 2], the \"p norm of a vector x-p = (d = 1 | xi | p) 1 / p. Let the\" p norm of a matrix A-Rn \u00b7 d specify the number of non-zero entries of A. Let det (A) specify the determinant of a square matrix C for the determinant of a square matrix C for the determinant of A. Let A > specify the transpose of A. Let A \u2020 specify the Frobenius norm of matrix A for the number of non-zero entries of A. Let det (A) specify the determinant of a square matrix C for the determinant of A > the transpose of A. Let A \u2020 specify the Moore-Penrose pseudo-inverse of A. Let us specify the inverse of A for the column A and the insertion of S \u2212"}, {"heading": "B Preliminaries", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "B.1 Polynomial system verifier", "text": "Renegar [Ren92b, Ren92b] and Basu et al. [BPR96] are independently an algorithm for the decision problem of the existential theory of the real world to decide the truth or falsity of a theorem (x1, \u00b7 \u00b7 F (f1, \u00b7 \u00b7, fm), where F is a quantifier-free Boolean formula with atoms of real algebraic geometry (fi) =, we refer the reader to [BPR05] and [Bas14].Theorem B.1 (Decision Problem [Ren92a, Ren92b, BPR96]."}, {"heading": "B.3 Lewis weights", "text": "Definition B.6. For a matrix A, ai is a series of A, and ai (= (Ai) > is a column vector. The statistical leverage value of a series ai is such that the \"p Lewis weights w for each series i (A) def = a > i (A > A) \u2212 1ai = 1 (A > A) \u2212 1 / 2ai * 22.For a matrix A and norm p, the\" p Lewis weights w are the unique weights, so for each series we have i havewi = \u03c4i (W 1 / 2 \u2212 1 / pA).or equivalentlya > i (A > W 1 \u2212 2 / pA) \u2212 1ai = w 2 / p i.Lemma B.7 (Lemma 2.4 of [CP15] and Lemma 7 of [CLM + 15]."}, {"heading": "B.4 Frobenious norm and `2 relaxation", "text": "Theorem B.9 (Generalized rank-confined matrix approximations, Theorem 2 in [FT07]) Given matrices A-Rn \u00b7 d, B-Rn \u00b7 p, and C-Rq \u00b7 d, let the SVD of B = UBCV > B and the SVD of C be C = UBU > CV > C. Then, B-Rn \u2212 p, and B-Rp, and the best rank-k approximation to UBU > BAVCV > Rp \u00b7 q, and the SVD of C = BAVCU > BAVCV > C, where (UBU > BAVCV > C) is of most k and denotes the best rank-k approximation to UBU > BAVCV > C Rp \u00b7 d in Frobenious norm.Claim B.10 ('2 relaxation of \"p-regression)."}, {"heading": "C.4 O\u0303(k)-approximation for an arbitrary matrix A", "text": "Algorithm 4 O (k) approach Algorithm1: Method L1LowRankApproxK (A, n, d, k). Theorem C.7 2: r \u2190 O (k log k), m \u2190 t1 \u2190 O (r log r), t2 \u2190 O (m logm). 3: Guess a diagonal matrix R-Rd \u00b7 d with only r 1s.. R selects r columns of A-Rn \u00b7 d. 4: Calculate a sample and recalculation of the matrix D-Rn \u00b7 n, T1-Rn according to the blank weights of AR, and let them have m, t1 non-zero entries on the diagrams. 5: Calculate a sample and recalculation of the matrix T > 2-Rd \u00b7 d according to the Lewis weights of (DA) >, and let them have t2 non-zero entries on the diagram."}, {"heading": "Let U\u2217 \u2208 Rn\u00d7k, V \u2217 \u2208 Rk\u00d7d satisfy", "text": "\"We cannot know if the number of non-admissions is in the diagonal of S = 0, then Ri = 0, then Ri = 0, then Ri = 0, then Ri = 0. Because the number of non-admissions (R > A >) is so high that the number of non-admissions (R > A >) is so high that the number of non-admissions (S > A >) is so high, then Ri = 1, and if the number of non-admissions (R > A >) is so high, then Ri = 1, then Ri = 0. Since the number of non-admissions (R > A >) is so high, we can have the number of non-admissions (R > A >) as low as the number of non-admissions (A >).Rk \u00b7 d The number of non-admissions (A = 1) is so high, then Ri = 0, then Ri = 0."}, {"heading": "C.6 CUR decomposition for an arbitrary matrix A", "text": "Theorem C.14 (A, n, d, k).3 (A).3 (A).3 (A).3 (A).3 (A).4 (A).4 (A).4 (A).4 (A).4 (A).4 (A).4 (A).4 (A).4 (A).4 (A).4 (A).4 (A).4 (A).4 (A).4 (A).4 (A).4 (A).4 (A).4 (A).4 (A).4 (A).4 (A).4 (A).4 (A).5 (A).5 (A).5 (A).5 (A).5 (A).5 (A).5 (A).5 (A).5 (A).5 (A).5 (A).5 (A).5 (A).5 (A).5 (A).5 (A).5 (A).5 (A).5 (A).6 (.6).6 (6).6 (6).A) (6 (6).A).6 (6 (6).A) (6 (6).A).6 (6 (.A) (6 (6).A).6 (6 (6).A (6 (6).A).6 (6 (6) (6 (6).A) (6 (6 (6).A).6 (6 (6 (6).A).A).6 (6 (6 (6 (6).A).A).6 (6 (6 (6 (.A).A).6 (6 (.A).6 (6 (6).A).6 (.A).6 (.6 (.6 (6).A).6 (.6 (6 (6).A).6 (.6 (.A).6 (.A).3 (.3 (A).3 (A).3 (A).3 (A).3 (A).3 (A).3 (A).3"}, {"heading": "Notice that U\u0302B2 = AD2(B2D2)\u2020(D1B1)\u2020D1A. Setting", "text": "C = AD2, Rn \u00b7 d2, U = (B2D2) \u2020 (D1B1) \u2020 Rd2 \u00b7 d1, and R = D1A Rd1 \u00b7 d, we get the desired CUR decomposition, EAD2 C \u00b7 (B2D2) \u2020 (D1B1) U \u00b7 D1A R \u2212 A 1 \u2264 poly (k) log (d) OPT. with rank (CUR) = k. Overall, the transit time is O (nz (A) + (n + d) poly (k).C.7 rank-r matrix B"}, {"heading": "C.7.1 Properties", "text": "Matrix A-A-A-A-A-A-A-A-A-A-B-B-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-B-A-B-A-A-A-A-A-A-B-A-A-A-A-A-A-B-A-A-A-A-A-A-A-B-A-A-A-A-A-A-A-A-B-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-B-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A A-A-A A-A"}, {"heading": "D Contraction and Dilation Bound for `1", "text": "In Section D.1 some basic definitions are given, in Section D.2 some properties implied by contraction and dilation limits. Section D.3 shows the no-dilation problem for a dense Cauchy transformation. Section D.4 and D.5 shows the no-contraction problem for dense Cauchy transformations. Section D.6 and D.7 shows the results for sparse Cauchy transformations and Lewis weights."}, {"heading": "D.1 Definitions", "text": "Definition D.1. If matrix M, Rn, Rm, k, and matrix S, Rn, k, if matrix S, Rm and k, if matrix S, Rm and k, and matrix S, Rm and n, then S, if matrix S, Rm and k, and if matrix S, Rm and n, then S, if matrix S, Rk, Rm, Rk, Rk, Rk, Rk, Rk, Rk and Ux, has a c2 contraction at most to U. Definition D.3. Matrix U, Rn, K, A, Rn and V, Rk, Rk, Rk, Rk, Rk, Rk, Rk, Rk, Rk, Rk, Rk, Rk, Rk, Rk, Rk, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K."}, {"heading": "D.2 Properties", "text": "D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D"}, {"heading": "D.3 Cauchy embeddings, no dilation", "text": "Lemma D.12. Define the optimal solution for min U-Rn-k, V-Rk-d-UV-A-1. Select a Cauchy matrix S with m rows and be rescalized by \u0432 (1 / m). We have the method used in [Ind06] and [CDMI + 13]. Repair the optimal U-Rn-K and V-O (log d), then the optimal U-Rn-V-V-A-A-1 applies with a probability of at least 99 / 100. Proof."}, {"heading": "By Bayes rule and \u03be = \u03be \u2229 \u03bei, Pr[\u03be|\u03bei] Pr[\u03bei] = Pr[\u03be \u2229 \u03bei] = Pr[\u03be], which implies that Pr[\u03be|\u03bei] =", "text": "Pr [vi] / Pr [vi] [vi] i [v] i [v] i [v] i [v] i [v] i [v] i [v] i (v] i [v] i [v] i [v] i (v] i [v] i [v] i (v] i [v] i [v] i [v] i [v] i [v] i [v] i [v] i (v] i [v] i [v] i [v] i [v] i (v] i [v] i [v] i [v] i [v] i [v] i \"i\" i \"i\" i \"i\" i \"i\" \"i\" \"i\" \"i\" \"i\" \"i\" \"i\" \"i\" \"i\" \"i\" i \"i\" i \"i\" i \"i\" i."}, {"heading": "D.4 Cauchy embeddings, no contraction", "text": "We prove that if we opt for a Cauchy matrix S, then for a fixed optimal solution U \u00b2 of minU, V \u00b2 UV \u00b2 A \u00b2 1, and for all V \u00b2 1, we have the high probability of finding the optimal solution from min. U \u00b2 V \u00b2 A \u00b2 1 up to a certain constant.Lemma D.14 Define U \u00b2 Rn \u00b2 k, V \u00b2 Rk \u00b2 d up to a certain constant.11 Define U \u00b2 Rn \u00b2 n up to a certain constant.Let m = O (k log k), S \u00b2 Rm \u00b2 n up to a random matrix with each entry a random matrix."}, {"heading": "D.6 Sparse Cauchy transform", "text": "This section provides proof of two lemmas in relation to the sparse Cauchy transformation."}, {"heading": "E.1 Definitions", "text": "x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x,"}, {"heading": "E.2 Properties", "text": "Lemma E.7. Given the matrices A-Rn \u00b7 d, U-Rn \u00b7 k, let V-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P-P"}, {"heading": "E.3 Tools and inequalities", "text": "We offer the triangle inequality for the p-norm, Fact E.14. For each p-norm and q-norm, Fact E.13. For each p-norm and each p-norm, each p-norm and each p-norm."}, {"heading": "F.1 Definitions", "text": "D & # 8222; D & # 8222; D & # 8222; D & # 8220; D & # 8222; D & # 8220; D & # 8222; D & # 8222; D & # 8222; -D & # 8222; -D & # 8222; -D & # 8222; -D & # 8222; -D & # 8222; -D & # 8222; -D & # 8222; -D & # 8222; -D & # 822; -D & # 822; -222; D & # 222; -2D & # 222; -222; -2D & # 822; -222; -822; -822; -822; -822; -2D & # 822; -2D & # 222; -222; -2D & # 222; -222; -2D & # 222; -222; -2D & # 222; -222; -222; -2D & # 222; -222; D & # 222; -222; D & # 222; D & # 222; D & # 222; D & # 222; D & # 222; D & # 222; D & # 222; D & # 222; D & # 222; D & # 222; D & # 222; D & # 222; D & # 222; D & # 222; D & # 222; D & # 222; D & # 222; D & # 222; D & # 222; D & # 222; D & # 222; D & # 222; D & # 222; D & # 222; D & # 222; D & # 222; D & # 2D & # 222; D & # 222; D & # 222; D & # 222; D & # 2D & # 222; D & # 222; D & # 222; D & # 222; D & # 2D & # 222; D & # 222; D; D & # 222; D & #"}, {"heading": "G Hardness results for Cauchy matrices, row subset selection, OSE", "text": "\u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7"}, {"heading": "G.3.1 Definitions", "text": "We first give the definition of the total variation distance and Kullback Leibler divergence.D = > Definition G.9. [LPW09, Ver14] The total variation distance between two probability variables P and Q on the measurable space (X, F) is defined as, DTV (P, Q) = sup A, F (A) \u2212 P (A) \u2212 Q (A) |. The Kullback Leibler (KL) divergence of P and Q is defined as, DKL (P | Q) = sup A, F (P) = X (log dQ) dQ) dP.Lemma G.10. [Pin60, Tsy09, CK11] Pinsker's inequality states that, if P and Q are two probability distributions on a measurable (X, F), thenDTV (P, Q).D)."}, {"heading": "G.3.2 Main results", "text": "Lemma G.16. Let V-Rn-m be a matrix with orthonormal columns, and let A-Rn-k be a random matrix with each entry drawn by i.i.d. Gauss-N (0, 1). We denote the distribution Di over Di-R (m + 1) -k, where Di = [V > A-Ai] is."}, {"heading": "If \u2016(V >)i\u201622 < 12 , then", "text": "(K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K). (K).). (K). (K). (K). (K).). (K). (K). (K). (K). (K). (K).). (K)."}, {"heading": "H Hardness", "text": "This section presents our hardness results. Section H.1 contains some useful tools from the literature. Section H.2 shows that it is difficult to get a multiplicative error. Assuming the ETH is correct, we will provide a stronger hardness result in Section H.3. Section H.4 extends the result from the rank-1 case to the rank-k case."}, {"heading": "H.1 Previous results", "text": "The definition H1 (\"A,\" \"A,\" \"A,\" \"A,\" \"A,\" \"A,\" \"A,\" \"A,\" \"A,\" \"A,\" \"A,\" \"A,\" \"A,\" \"A,\" \"A,\" \"A,\" \"A,\" \"A,\" \"A,\" \"A,\" \"A,\" \"A,\" \"\" A, \"\" A, \"\" A, \"\" A, \"\" A, \"\" A, \"\" A, \"\" A, \"\" A, \"A,\" A, \"A,\" A, \"A,\" A, \"A,\" \"A,\" \"A,\" \"A,\" \"A,\" \"A,\" A, \"A,\" A, \"A,\" A, \"A,\" A, \"A,\" A, \"A,\" A, \"A,\" A, \"\" A, \"A,\" \"A,\" A, \"A,\" A, \"A,\" \"A,\" A, \"A,\" A, \"A,\" A, \"A,\" A, \"\" A, \"A,\" A, \"A,\" A, \"A,\" A, \"A,\" A, \"A,\" \"A,\" A, \"A,\" A, \"A,\" A, \"A,\" A, \"\" A, \"A,\" A, \"A,\" A, \"A,\" A, \"A, A, A, A,\" \"\" \"A, A, A, A, A, A, A, A, A, A,\" \"\" \"\" \"A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A,\" \"\" \"\" \"\" \"A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A,"}, {"heading": "I Limited independent Cauchy random variables", "text": "This section presents the basic terms with limited independent Cauchy variables used in Section J and K. In Section I.1 we provide some notations, definitions and tools from previous work. Section I.2 contains the main result."}, {"heading": "I.1 Notations and Tools", "text": "To optimize the communication complexity of our dictated algorithms, we show that, instead of using completely independent Cauchy variables, Poly (k, d) -wise independent Cauchy variables suffice. We start by specifying two useful Lemmas from previous work [KNW10]. Lemma I.1 (Lemma 2.2 in [KNW10]) there is a 0 > 0 so that the following values apply. Let n have a positive integer and 0 < 0 < p < p < p < p < p < p < p; p < p; p < p < p < p < p < p; p < p; p < p; p; p < p; p < p; p < p < p < p p; p p; p p p p; p p p; p p p p; p p p p p p; p p p p p; p p p p; p p p p; p p p p; p p p; p p p; p p p; p p p; p p; p p p p; p p p; p p; p p p; p p; p p; p p; p; p p p; p p; p p; p p p; p p; p; p p p; p p; p p; p p; p p; p p p; p; p p p; p; p p; p p; p p; p; p p; p p p p; p; p; p p p; p; p; p p; p; p; p; p p; p; p; p; p; p; p p; p; p; p p; p; p; p; p; p; p; p; p; p; p p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p"}, {"heading": "J Streaming Setting", "text": "Section J.1 provides some notations and definitions about the streaming model for line updates and the turnstile streaming model. For some recent developments in streaming for line updates and turnstile streaming models, we refer readers to [CW09, KL11, GP13, Lib13, KLM + 14, BWZ16] and the references contained therein. Section J.2 presents our turnstile streaming algorithm. Section J.3 presents our streaming algorithm for line updates."}, {"heading": "J.1 Definitions", "text": "The definition J.1 (Row update model) is only a single conversion to the single stream. (D) The definition J.1 (R) is the only one that exists. (D) The definition J.2 (D) is the only one that exists. (D) The definition J.2 (D) is the only one that exists. (D) The definition J.2 (D) is the only one that exists. (D) The definition J.2 (D) is the only one that exists. (D) The definition J.2 (D) is the only one that exists. (D) The definition J.2 (D) is the only one that exists. (D) The definition J.2 (D) is the only one that exists. (D) The definition J.2 (D) is the only one that exists. (D) The definition J.2 (D) is the only one that exists. (D) The definition J.2 (D) is the only one that exists. (D) The definition J.2 (D) is the only one that exists."}, {"heading": "K Distributed Setting", "text": "Section K.1 provides some notation and definitions for the row partition distribution model (k)."}, {"heading": "K.1 Definitions", "text": "There is only one problem: \"There is only one problem.\" - \"There is only one problem.\" - \"There is no solution.\" - \"There is no solution.\" - \"There is no solution.\" - \"There is no solution.\" - \"There is no solution.\" - \"There is no solution.\" - \"There is no solution.\" - \"There is no solution.\" - \"There is no solution.\" - \"There is no solution.\" - \"There is no solution.\" - \"There is no solution.\" - \"There is no solution.\" - \"There is no solution.\" - \"There is no solution.\" - \"There is no solution.\" - \"There is no solution.\" - \"There is no solution.\" - \"There is no solution.\" - \"There is no solution.\""}, {"heading": "L Experiments and Discussions", "text": "In this section we present some counter-examples for the other heuristic algorithms, so that the heuristic algorithms for these examples can output a solution with a very \"bad\" approximation ratio, i.e. nc, where c > 0 and the input matrix have the size n \u00b7 n. We observe not only that heuristic algorithms sometimes perform very poorly in practice, but also provide a theoretical proof."}, {"heading": "L.1 Setup", "text": "We have the R package of [KK05, Kwa08, BDB13] from https: / / cran.r-project.org / web / packages / pcaL1 / index.html. We also implemented our algorithm and the r1-pca algorithm [DZHZ06] using the R language. The version of the R language is 3.0.2. We conducted experiments on a machine with Intel X5550 @ 2.67GHz CPU and 24G memory. Operating system of this machine is Linux Ubuntu 14.04.5 LTS. All experiments were conducted in single-thread mode."}, {"heading": "L.2 Counterexample for [DZHZ06]", "text": "The goal is to find a rank k = 1 approximation for matrix A. For each B matrix [0, 0.5), we define A-Rn \u00b7 n asA = [n1.5 + 00 0] + [0 0 0 0 B], (45) where B-R (n \u2212 1) \u00b7 (n \u2212 1) is all 1s matrix. It is immediate that the optimal costs are no more than n1.5 +. However, when using the algorithm in [DZHZ06], the costs are at least equal (n2). Therefore, using the algorithm [DZHZ06] to solve \"a low-ranking approximation problem on A, we cannot achieve an approximation ratio better than n0.5 \u2212."}, {"heading": "L.3 Counterexample for [BDB13]", "text": "The objective is to achieve a rank k = 1 approximation of Matrix A. \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7"}, {"heading": "L.4 Counterexample for [Kwa08]", "text": "We show that the algorithm [Kwa08] cannot achieve a better approximation ratio = > 1.5 = > 1.5 = > 1 = > 1 = > 1 = > 1 = > 1 = > 1 = > 1 = = 1 = 1 = 1 = 1 = 1 = 1 = 2 = 1 = 1 = 1 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 {{1}, {n = 3, 2 = 3 = 3 = 3 = 3 = 3 = 3}}, {n, 2: 2}}, {1 \u2212 1}, 1 \u2212 n}, {n, 3: 1 \u2212 n, 3: 1 \u2212 n."}, {"heading": "L.5 Counterexample for [KK05]", "text": "We show that there are matrices that look like the algorithm of [KK05] cannot achieve an approximate ratio better than vice versa. Their algorithm has two different kinds of approximation. We provide counter-examples for each of the initializations \u2212 \u2212 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 0 + I, (48), in which ratio A and II are defined as, A = nc 0 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 0 \u00b7 \u00b7 \u00b7 \u00b7 0 \u00b7 0 \u00b7 0 + I, (48). Let us consider the rank-1 approximation problem for this matrix A. The optimal cost is at most n \u2212 1. Let us execute their algorithm. The starting vectors are u (0), I) and v (0). We define two properties for a given vector y Rn. The property I is for all i."}, {"heading": "L.6 Counterexample for all", "text": "In Figure 5 we use KK05r (resp. Kwak08r) to achieve better than nmin (\u03b3, 0.5 \u2212) approximation ratio. We present our most important experimental results in Figure 5. Both [KK05] and [Kwa08] have two different ways of initialization. In Figure 5 we use KK05r (resp. Kwak08r) to approximate the time between nmin (\u03b3, 0.5 \u2212)."}, {"heading": "L.7 Discussion for Robust PCA [CLMW11]", "text": "A popular method is the robust PCA [CLMW11], which, given a matrix A = 1, tries to find a matrix L for which the matrix L + 1 + 2 is minimized with relative errors, where \u03bb > 0 is a tuning parameter and matrix A is a block diagonal matrix of rank k and n = k2 (b + 1). Furthermore, the first k / 2 blocks are b \u00b7 b matrices of all 1s, while the next k / 2 blocks are only a single value b on the diagonal. Then, the solution of the above problem L could be the first k / 2 blocks of A. The total cost of the matrix is b \u00b7 b matrices of all 1s, while the next k / 2 blocks represent only a single value b on the diagonal."}, {"heading": "M Acknowledgments", "text": "The authors thank Alexandr Andoni, Saugata Basu, Cho-Jui Hsieh, Daniel Hsu, Chi Jin, Fu Li, Ankur Moitra, Cameron Musco, Richard Peng, Eric Price, Govind Ramnarayan, James Renegar and Clifford Stein for useful discussions and Jiyan Yang, Yinlam Chow, Christopher R\u00e9 and Michael Mahoney for sharing the code."}], "references": [], "referenceMentions": [], "year": 2016, "abstractText": "<lb>We study the `1-low rank approximation problem, where for a given n \u00d7 d matrix A and<lb>approximation factor \u03b1 \u2265 1, the goal is to output a rank-k matrix \u00c2 for which<lb>\u2016A\u2212 \u00c2\u20161 \u2264 \u03b1 \u00b7 min<lb>rank-k matrices A\u2032 \u2016A\u2212A\u20161,<lb>where for an n\u00d7 d matrix C, we let<lb>\u2016C\u20161 =<lb>\u2211n<lb>i=1<lb>\u2211d<lb>j=1 |Ci,j |. This error measure is known to<lb>be more robust than the Frobenius norm in the presence of outliers and is indicated in models<lb>where Gaussian assumptions on the noise may not apply. The problem was shown to be NP-hard<lb>by Gillis and Vavasis and a number of heuristics have been proposed. It was asked in multiple<lb>places if there are any approximation algorithms.<lb>We give the first provable approximation algorithms for `1-low rank approximation, showing<lb>that it is possible to achieve approximation factor \u03b1 = (log d)\u00b7poly(k) in nnz(A)+(n+d) poly(k)<lb>time, where nnz(A) denotes the number of non-zero entries of A. If k is constant, we further<lb>improve the approximation ratio toO(1) with a poly(nd)-time algorithm. Under the Exponential<lb>Time Hypothesis, we show there is no poly(nd)-time algorithm achieving a (1 + 1<lb>log1+\u03b3(nd) )-<lb>approximation, for \u03b3 > 0 an arbitrarily small constant, even when k = 1.<lb>We give a number of additional results for `1-low rank approximation: nearly tight upper and<lb>lower bounds for column subset selection, CUR decompositions, extensions to low rank approx-<lb>imation with respect to `p-norms for 1 \u2264 p < 2 and earthmover distance, low-communication<lb>distributed protocols and low-memory streaming algorithms, algorithms with limited random-<lb>ness, and bicriteria algorithms. We also give a preliminary empirical evaluation. \u2217Work done while visiting IBM Almaden.<lb>ar<lb>X<lb>iv<lb>:1<lb>61<lb>1.<lb>00<lb>89<lb>8v<lb>1<lb>[<lb>cs<lb>.D<lb>S]<lb>3<lb>N<lb>ov<lb>2<lb>01<lb>6", "creator": "LaTeX with hyperref package"}}}