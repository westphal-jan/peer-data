{"id": "1611.01055", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Nov-2016", "title": "Learning Locomotion Skills Using DeepRL: Does the Choice of Action Space Matter?", "abstract": "The use of deep reinforcement learning allows for high-dimensional state descriptors, but little is known about how the choice of action representation impacts the learning difficulty and the resulting performance. We compare the impact of four different action parameterizations (torques, muscle-activations, target joint angles, and target joint-angle velocities) in terms of learning time, policy robustness, motion quality, and policy query rates. Our results are evaluated on a gait-cycle imitation task for multiple planar articulated figures and multiple gaits. We demonstrate that the local feedback provided by higher-level action parameterizations can significantly impact the learning, robustness, and quality of the resulting policies.", "histories": [["v1", "Thu, 3 Nov 2016 15:15:00 GMT  (1866kb,D)", "http://arxiv.org/abs/1611.01055v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.GR cs.RO", "authors": ["xue bin peng", "michiel van de panne"], "accepted": false, "id": "1611.01055"}, "pdf": {"name": "1611.01055.pdf", "metadata": {"source": "CRF", "title": "LEARNING LOCOMOTION SKILLS USING DEEPRL: DOES", "authors": ["SPACE MATTER", "Xue Bin Peng", "Michiel van de Panne"], "emails": ["xbpeng@cs.ubc.ca", "van@cs.ubc.ca"], "sections": [{"heading": "1 INTRODUCTION", "text": "The introduction of deep learning models to enhance learning (RL) made it possible to fall back directly on high-dimensional, low-threshold state characteristics."}, {"heading": "2 BACKGROUND", "text": "Our task is structured as a standardized amplification, in which an actor interacts with its environment in accordance with a policy to maximize a reward signal. (s) = p (a) \u2212 s represents the conditional probability function of selecting an action \u00e0 la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la"}, {"heading": "3 TASK REPRESENTATION", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 REFERENCE MOTION", "text": "In our task, the goal of a policy is to imitate a given reference movement, which consists of a sequence of kinematic poses q \u0445 t in reduced coordinates. The reference velocity q \u0445 \u0445 t at a given time t is approximated by the finite difference q \u0445 t \u00b2 t + 4t \u2212 q \u0445 t4t. Reference movements are generated either by using a recorded simulation result of an existing controller (\"Sim\") or by manually created keyframes. Since handmade reference movements may not be physically feasible, the goal is to closely reproduce a movement and at the same time satisfy physical constraints."}, {"heading": "3.2 STATES", "text": "In order to define the state of the active substance, a characteristic transformation \u03a6 (q, q) is used to extract a series of characteristics from the coordinate-reduced pose q and the velocity q \u0430. Characteristics consist of the height of the root (pelvis) from the ground, the position of each limb in relation to the root and the centre of gravity of the mass velocity of each limb. In the formation of a policy to imitate a cyclic reference movement, knowledge of the movement phase can help facilitate learning. Therefore, we supplement the state characteristics with a series of target characteristics \u03a6 (q-t, q-t-t), resulting in a combined state represented by st = (qt, q-t), \u03a6 (q-t, q-t-t). Similar results can also be achieved by providing a single motion phase variable as a state characteristic, as shown in Figure 12 (supplementary material)."}, {"heading": "3.3 ACTIONS", "text": "In this context, it is worth mentioning that the two models are a reactionary group that is able to confine itself to the reactionary domains. (It is the reactionary parameters that are in the reactionary movement.) (It is the reactionary parameters that are in the reactionary movement.) Each action represents a series of targets that are in the reactionary movement. (It is the reactionary parameters that are in the reactionary movement.) (It is the reactionary parameters that are in the reactionary movement.) Each action represents a series of targets that are in the reactionary movement. (It is the reactionary parameters that are in the reactionary movement, that are in the reactionary movement and in the reactionary movement. (It is the reactionary parameters that are in the reactionary movement.)"}, {"heading": "3.4 REWARD", "text": "The reward function consists of a weighted sum of terms that encourage policy makers to pursue a reference movement.r = wposerpose + wvelrvel + wendrend + wrootrroot + wcomrcomwpose = 0.5, wvel = 0.05, wend = 0.15, wroot = 0.1, wcom = 0.2Details of each term are available in the supplementary material. rpose punishes the deviation of the character position from the reference position. rend and rroot take into account the positioning error of the end effectors and roots. rcom punishes deviations in the center of the mass velocity from the reference movement."}, {"heading": "3.5 INITIAL STATE DISTRIBUTION", "text": "We design the initial state distribution, p0 (s), to sample states uniformly along the reference trajectory. At the beginning of each episode, q * and q * * are scanned from the reference trajectory and used to initialize the agent's pose and speed, helping the agent explore states near the target trajectory."}, {"heading": "4 ACTOR-CRITIC LEARNING ALGORITHM", "text": "Instead of using the temporal difference directly, we use a positive temporal difference (PTD) update as proposed by Van Hasselt (2012).A (s, a) = I [\u03b4 > 0] = {1, \u03b4 > 00, otherwise\u03b4 = r + \u03b3V (s \") \u2212 V (s) Unlike more conventional political gradient methods, PTD is less sensitive to the scale of advantage function and avoids instabilities that may arise from negative TD updates. For a Gaussian policy, a negative TD update shifts the mean of distribution away from an observed action and effectively shifts the mean towards an unknown action that cannot be better than the current medium action (Van Hasselt, 2012). Anticipating that these updates converge with the true political gradient, but for stochastic estimates of the political gradient, these updates may cause the agent to engage in undesirable behavior, which subsequent experiences are influenced by the agent."}, {"heading": "5 RESULTS", "text": "The reasons for this are manifold: \"I do not believe that people are able to survive by themselves,\" he says, \"but I believe that they will not do it.\" \"I believe so,\" he says, \"but I do not believe that they will do it.\" \"I believe already,\" he says, \"I do not believe that they will do it.\" \"I do not believe that they will.\" \"No.\" \"No.\" \"No.\" \"No.\" No. \"No.\" No. \"No.\" No. \"No.\" No. \"No.\" No. \"No.\" No. \"No.\" \"No.\" \"\" No. \"\" \"No.\" \"\" No. \"\" \"\" No. \"\" \"\" No. \"\" \"\" No. \"\" \"\" No. \"\" \"No.\" No. \"No.\" No. \"No.\" No. \"No.\" No. \"No.\" No. \"No.\" No. \"No.\" No. \"No.\" No. \"No.\" No. \"No.\""}, {"heading": "6 RELATED WORK", "text": "DeepRL has made impressive recent advances in controlling learning movements, i.e., solving problems with continuous action control through reinforcement learning. All four types of action we have explored so far in machine learning literature use an approach of actors and critics with experiential replay to learn skills for an octopus arm (triggered by a simple muscle model) and a planar half cheetah (triggered by joint-based PD controllers). Recent work on deterministic political gradients (Lillicrap et al., 2015) and on RL benchmarks, such as OpenAI Gym, generally use shared torques as a space for action, as have the test suites in recent work (Schulman et al., 2015) on the use of generalized benefit controls (Lillicrap et al.), which have recently come into use."}, {"heading": "7 CONCLUSIONS", "text": "Our experiments suggest that action parameters that incorporate basic local feedback, such as PD target angles, MTU activations or target speeds, can improve political performance and learning speed across different movements and character morphologies. Such models more accurately reflect the embodied character of control in biomechanical systems, and the role of mechanical components in shaping the overall dynamics of movements and their control. The difference between low and high action parameters grows with the complexity of the characters, with high-level parameterization more gracefully scaled to complex character types. Our results were demonstrated only on planar, articulated number simulations; extending to 3D remains the current work. Tuning actuator parameters for complex action models such as MTUs remain a challenge. Although our actuator optimization technology is capable of improving performance compared to manual parameters, the resulting parameters are not optimal for the task at hand."}], "references": [{"title": "Intelligence by mechanics", "author": ["Reinhard Blickhan", "Andre Seyfarth", "Hartmut Geyer", "Sten Grimmer", "Heiko Wagner", "Michael G\u00fcnther"], "venue": "Philosophical Transactions of the Royal Society of London A: Mathematical, Physical and Engineering Sciences,", "citeRegEx": "Blickhan et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Blickhan et al\\.", "year": 2007}, {"title": "Locomotion skills for simulated quadrupeds", "author": ["Stelian Coros", "Andrej Karpathy", "Ben Jones", "Lionel Reveret", "Michiel van de Panne"], "venue": "ACM Transactions on Graphics,", "citeRegEx": "Coros et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Coros et al\\.", "year": 2011}, {"title": "Feature-based locomotion controllers", "author": ["Martin de Lasa", "Igor Mordatch", "Aaron Hertzmann"], "venue": "In ACM Transactions on Graphics (TOG),", "citeRegEx": "Lasa et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Lasa et al\\.", "year": 2010}, {"title": "Flexible muscle-based locomotion for bipedal creatures", "author": ["Thomas Geijtenbeek", "Michiel van de Panne", "A. Frank van der Stappen"], "venue": "ACM Transactions on Graphics,", "citeRegEx": "Geijtenbeek et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Geijtenbeek et al\\.", "year": 2013}, {"title": "Positive force feedback in bouncing gaits", "author": ["Hartmut Geyer", "Andre Seyfarth", "Reinhard Blickhan"], "venue": "Proc. Royal Society of London B: Biological Sciences,", "citeRegEx": "Geyer et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Geyer et al\\.", "year": 2003}, {"title": "Deep reinforcement learning for robotic manipulation", "author": ["Shixiang Gu", "Ethan Holly", "Timothy Lillicrap", "Sergey Levine"], "venue": "arXiv preprint arXiv:1610.00633,", "citeRegEx": "Gu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Gu et al\\.", "year": 2016}, {"title": "Deep reinforcement learning in parameterized action", "author": ["Matthew J. Hausknecht", "Peter Stone"], "venue": "space. CoRR,", "citeRegEx": "Hausknecht and Stone.,? \\Q2015\\E", "shortCiteRegEx": "Hausknecht and Stone.", "year": 2015}, {"title": "Actor-critic algorithms", "author": ["Vijay Konda", "John Tsitsiklis"], "venue": "In SIAM Journal on Control and Optimization,", "citeRegEx": "Konda and Tsitsiklis.,? \\Q2000\\E", "shortCiteRegEx": "Konda and Tsitsiklis.", "year": 2000}, {"title": "End-to-end training of deep visuomotor policies", "author": ["Sergey Levine", "Chelsea Finn", "Trevor Darrell", "Pieter Abbeel"], "venue": "CoRR, abs/1504.00702,", "citeRegEx": "Levine et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Levine et al\\.", "year": 2015}, {"title": "Continuous control with deep reinforcement learning", "author": ["Timothy P. Lillicrap", "Jonathan J. Hunt", "Alexander Pritzel", "Nicolas Heess", "Tom Erez", "Yuval Tassa", "David Silver", "Daan Wierstra"], "venue": "CoRR, abs/1509.02971,", "citeRegEx": "Lillicrap et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lillicrap et al\\.", "year": 2015}, {"title": "Guided learning of control graphs for physicsbased characters", "author": ["Libin Liu", "Michiel van de Panne", "KangKang Yin"], "venue": "ACM Transactions on Graphics,", "citeRegEx": "Liu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2016}, {"title": "Control implications of musculoskeletal mechanics. In Engineering in Medicine and Biology Society, 1995", "author": ["GE Loeb"], "venue": "IEEE 17th Annual Conference,", "citeRegEx": "Loeb.,? \\Q1995\\E", "shortCiteRegEx": "Loeb.", "year": 1995}, {"title": "Interactive control of diverse complex characters with neural networks", "author": ["Igor Mordatch", "Kendall Lowrey", "Galen Andrew", "Zoran Popovic", "Emanuel Todorov"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Mordatch et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mordatch et al\\.", "year": 2015}, {"title": "Highdimensional continuous control using generalized advantage estimation", "author": ["John Schulman", "Philipp Moritz", "Sergey Levine", "Michael I. Jordan", "Pieter Abbeel"], "venue": "CoRR, abs/1506.02438,", "citeRegEx": "Schulman et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Schulman et al\\.", "year": 2015}, {"title": "Deterministic policy gradient algorithms", "author": ["David Silver", "Guy Lever", "Nicolas Heess", "Thomas Degris", "Daan Wierstra", "Martin Riedmiller"], "venue": "In Proc. International Conference on Machine Learning,", "citeRegEx": "Silver et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Silver et al\\.", "year": 2014}, {"title": "Policy gradient methods for reinforcement learning with function approximation", "author": ["R. Sutton", "D. Mcallester", "S. Singh", "Y. Mansour"], "venue": null, "citeRegEx": "Sutton et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Sutton et al\\.", "year": 2001}, {"title": "Learning bicycle stunts", "author": ["Jie Tan", "Yuting Gu", "C. Karen Liu", "Greg Turk"], "venue": "ACM Trans. Graph.,", "citeRegEx": "Tan et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Tan et al\\.", "year": 2014}, {"title": "Reinforcement learning in continuous state and action spaces", "author": ["Hado Van Hasselt"], "venue": "In Reinforcement Learning,", "citeRegEx": "Hasselt.,? \\Q2012\\E", "shortCiteRegEx": "Hasselt.", "year": 2012}, {"title": "Optimizing locomotion controllers using biologically-based actuators and objectives", "author": ["Jack M. Wang", "Samuel R. Hamner", "Scott L. Delp", "Vladlen Koltun", "More Specifically"], "venue": "ACM Trans. Graph,", "citeRegEx": "Wang et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2012}, {"title": "Autonomous reinforcement learning with experience replay", "author": ["Pawe\u0142 Wawrzy\u0144Ski", "Ajay Kumar Tanwani"], "venue": "Neural Networks,", "citeRegEx": "Wawrzy\u0144Ski and Tanwani.,? \\Q2013\\E", "shortCiteRegEx": "Wawrzy\u0144Ski and Tanwani.", "year": 2013}], "referenceMentions": [{"referenceID": 9, "context": "As a result, deep reinforcement learning (DeepRL) has demonstrated impressive capabilities, such as developing control policies that can map from input image pixels to output joint torques (Lillicrap et al., 2015).", "startOffset": 189, "endOffset": 213}, {"referenceID": 11, "context": "Loeb coins the term preflexes (Loeb, 1995) to describe these effects, and their impact on motion control has been described as providing intelligence by mechanics (Blickhan et al.", "startOffset": 30, "endOffset": 42}, {"referenceID": 0, "context": "Loeb coins the term preflexes (Loeb, 1995) to describe these effects, and their impact on motion control has been described as providing intelligence by mechanics (Blickhan et al., 2007).", "startOffset": 163, "endOffset": 186}, {"referenceID": 0, "context": ", Coros et al. (2011); Geijtenbeek et al.", "startOffset": 2, "endOffset": 22}, {"referenceID": 0, "context": ", Coros et al. (2011); Geijtenbeek et al. (2013). Relatedly, the choice of action parameterization is a design decision whose impact is not yet well understood.", "startOffset": 2, "endOffset": 49}, {"referenceID": 15, "context": "The gradient of the expected reward O\u03b8J(\u03c0\u03b8) can be determined according to the policy gradient theorem (Sutton et al., 2001), which provides a direction of improvement to adjust the policy parameters \u03b8.", "startOffset": 103, "endOffset": 124}, {"referenceID": 14, "context": "where d\u03b8(s) = \u222b S \u2211T t=0 \u03b3 p0(s0)p(s0 \u2192 s|t, \u03c0\u03b8)ds0 is the discounted state distribution, where p0(s) represents the initial state distribution, and p(s0 \u2192 s|t, \u03c0\u03b8) models the likelihood of reaching state s by starting at s0 and following the policy \u03c0\u03b8(s, a) for t steps (Silver et al., 2014).", "startOffset": 271, "endOffset": 292}, {"referenceID": 13, "context": "The choice of advantage function gives rise to a family of policy gradient algorithms, but in this work, we will focus on the one-step temporal difference advantage function (Schulman et al., 2015)", "startOffset": 174, "endOffset": 197}, {"referenceID": 18, "context": "To simplify control and reduce the number of internal state parameters per MTU, the policies directly control muscle activations instead of indirectly through excitations (Wang et al., 2012).", "startOffset": 171, "endOffset": 190}, {"referenceID": 17, "context": "Detailed modeling and implementation information are available in Wang et al. (2012). Each MTU is modeled as a contractile element (CE) attached to a serial elastic element (SE) and parallel elastic element (PE).", "startOffset": 66, "endOffset": 85}, {"referenceID": 4, "context": "Analytic forms are available in Geyer et al. (2003). Activations are bounded between [0, 1].", "startOffset": 32, "endOffset": 52}, {"referenceID": 17, "context": "Instead of directly using the temporal difference advantage function, we adapt a positive temporal difference (PTD) update as proposed by Van Hasselt (2012).", "startOffset": 142, "endOffset": 157}, {"referenceID": 3, "context": "MTU Actuator Optimization: Actuation models such as MTUs are defined by further parameters whose values impact performance (Geijtenbeek et al., 2013).", "startOffset": 123, "endOffset": 149}, {"referenceID": 3, "context": "MTU Actuator Optimization: Actuation models such as MTUs are defined by further parameters whose values impact performance (Geijtenbeek et al., 2013). Geyer et al. (2003) uses existing anatomical estimates for humans to determine MTU parameters, but such data is not be available for more arbitrary creatures.", "startOffset": 124, "endOffset": 171}, {"referenceID": 3, "context": "MTU Actuator Optimization: Actuation models such as MTUs are defined by further parameters whose values impact performance (Geijtenbeek et al., 2013). Geyer et al. (2003) uses existing anatomical estimates for humans to determine MTU parameters, but such data is not be available for more arbitrary creatures. Alternatively, Geijtenbeek et al. (2013) uses covariance matrix adaptation (CMA), a derivative-free evolutionary search strategy, to simultaneously optimize MTU and policy parameters.", "startOffset": 124, "endOffset": 351}, {"referenceID": 9, "context": "Recent work on deterministic policy gradients (Lillicrap et al., 2015) and on RL benchmarks, e.", "startOffset": 46, "endOffset": 70}, {"referenceID": 13, "context": "OpenAI Gym, generally use joint torques as the action space, as do the test suites in recent work (Schulman et al., 2015) on using generalized advantage estimation.", "startOffset": 98, "endOffset": 121}, {"referenceID": 8, "context": "Other recent work uses: the PR2 effort control interface as a proxy for torque control (Levine et al., 2015); joint velocities (Gu et al.", "startOffset": 87, "endOffset": 108}, {"referenceID": 5, "context": ", 2015); joint velocities (Gu et al., 2016); velocities under an implicit control policy (Mordatch et al.", "startOffset": 26, "endOffset": 43}, {"referenceID": 12, "context": ", 2016); velocities under an implicit control policy (Mordatch et al., 2015); or provide abstract actions (Hausknecht & Stone, 2015).", "startOffset": 53, "endOffset": 76}, {"referenceID": 11, "context": "Work in biomechanics has long recognized the embodied nature of the control problem and the view that musculotendon systems provide \u201cpreflexes\u201d (Loeb, 1995) that effectively provide a form intelligence by mechanics (Blickhan et al.", "startOffset": 144, "endOffset": 156}, {"referenceID": 0, "context": "Work in biomechanics has long recognized the embodied nature of the control problem and the view that musculotendon systems provide \u201cpreflexes\u201d (Loeb, 1995) that effectively provide a form intelligence by mechanics (Blickhan et al., 2007), as well as allowing for energy storage.", "startOffset": 215, "endOffset": 238}, {"referenceID": 16, "context": ", 2010), joint velocities for skilled bicycle stunts (Tan et al., 2014), muscle models for locomotion (Wang et al.", "startOffset": 53, "endOffset": 71}, {"referenceID": 18, "context": ", 2014), muscle models for locomotion (Wang et al., 2012; Geijtenbeek et al., 2013), mixed use of feed-forward torques and joint target angles (Coros et al.", "startOffset": 38, "endOffset": 83}, {"referenceID": 3, "context": ", 2014), muscle models for locomotion (Wang et al., 2012; Geijtenbeek et al., 2013), mixed use of feed-forward torques and joint target angles (Coros et al.", "startOffset": 38, "endOffset": 83}, {"referenceID": 1, "context": ", 2013), mixed use of feed-forward torques and joint target angles (Coros et al., 2011), and joint target angles computed by learned linear (time-indexed) feedback strategies (Liu et al.", "startOffset": 67, "endOffset": 87}, {"referenceID": 10, "context": ", 2011), and joint target angles computed by learned linear (time-indexed) feedback strategies (Liu et al., 2016).", "startOffset": 95, "endOffset": 113}], "year": 2016, "abstractText": "The use of deep reinforcement learning allows for high-dimensional state descriptors, but little is known about how the choice of action representation impacts the learning difficulty and the resulting performance. We compare the impact of four different action parameterizations (torques, muscle-activations, target joint angles, and target joint-angle velocities) in terms of learning time, policy robustness, motion quality, and policy query rates. Our results are evaluated on a gaitcycle imitation task for multiple planar articulated figures and multiple gaits. We demonstrate that the local feedback provided by higher-level action parameterizations can significantly impact the learning, robustness, and quality of the resulting policies.", "creator": "LaTeX with hyperref package"}}}