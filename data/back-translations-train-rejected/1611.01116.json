{"id": "1611.01116", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Nov-2016", "title": "Binary Paragraph Vectors", "abstract": "Recently Le &amp; Mikolov described two log-linear models, called Paragraph Vector, that can be used to learn state-of-the-art distributed representations of documents. Inspired by this work we present Binary Paragraph Vectors, simple neural networks that learn short binary codes for fast information retrieval. We show that binary paragraph vectors outperform autoencoder-based binary codes, despite using fewer bits. We also evaluate their precision in transfer learning settings, where binary codes are inferred for documents unrelated to the training corpus. Results from these experiments indicate that Binary Paragraph Vectors can capture semantics relevant for various domain-specific documents. Finally, we present a model that simultaneously learns short binary codes and longer, real-valued representations. This model can be used to rapidly retrieve a short list of highly relevant documents from a large document collection.", "histories": [["v1", "Thu, 3 Nov 2016 18:10:35 GMT  (285kb,D)", "https://arxiv.org/abs/1611.01116v1", "Under review as a conference paper at ICLR 2017"], ["v2", "Mon, 14 Nov 2016 17:29:34 GMT  (285kb,D)", "http://arxiv.org/abs/1611.01116v2", "Under review as a conference paper at ICLR 2017"], ["v3", "Fri, 9 Jun 2017 14:33:06 GMT  (354kb,D)", "http://arxiv.org/abs/1611.01116v3", "Accepted to appear as a regular paper at the 2nd Workshop on Representation Learning for NLP at ACL 2017"]], "COMMENTS": "Under review as a conference paper at ICLR 2017", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["karol grzegorczyk", "marcin kurdziel"], "accepted": false, "id": "1611.01116"}, "pdf": {"name": "1611.01116.pdf", "metadata": {"source": "CRF", "title": "Binary Paragraph Vectors", "authors": ["Karol Grzegorczyk", "Marcin Kurdziel"], "emails": ["kgr@agh.edu.pl", "kurdziel@agh.edu.pl"], "sections": [{"heading": "1 Introduction", "text": "This year it is more than ever before."}, {"heading": "2 Binary paragraph vector models", "text": "This year it is more than ever before."}, {"heading": "3 Experiments", "text": "This year, it is only a matter of time before an agreement is reached."}, {"heading": "3.1 Transfer learning", "text": "In the experiments presented so far, we have been provided with training sets of documents similar to the documents for which we derive binary codes. One could ask whether it is possible to use binary paragraph vectors without collecting a domain-specific training set? For example, what if we had to hash documents that are not associated with any available domain-specific corpus? One solution might be to build the model with a large generic text corpus covering a variety of domains. Lau and Baldwin (2016) evaluated this approach for real-world sales vectors with promising results. However, it is not obvious whether short binary codes would work well in similar environments. To illuminate this question, we trained binary PV-DBOW without bigrams on Wikipedia and concluded binary codes for the test parts of the 20 newsgroups and RCV1 datasets. The results are presented in Table 3 and Figure 5."}, {"heading": "3.2 Retrieval with Real-Binary models", "text": "As I said, it is very important that we are able to get to grips with the problems and how we can solve them."}, {"heading": "4 Conclusion", "text": "In this article, we introduced simple neural networks that learn short binary codes for text documents. Our networks extend Paragraph Vector by introducing a sigmoid nonlinearity before Softmax, which predicts words in documents. Binary codes derived with the proposed networks achieve higher retrieval accuracy than semantic hashing codes on two popular information retrieval benchmarks. They also retain a large portion of their precision when trained on an unrelated text corpus. Finally, we presented a network that learns short binary codes and longer, real representations at the same time. The best codes in our experiments were concluded with binary PV-DBOW networks. The binary PV-DM model did not perform so well. Li et al. (2015) made similar observations for paragraph vector models, arguing that in the distributed memory model the word context takes up a large part of the burden of predicting the central word code from the good document model, hence BOB could focus on the future strategy of the future document."}, {"heading": "Acknowledgments", "text": "This research is supported by the National Science Centre, Poland grant number 2013 / 09 / B / ST6 / 01549 \"Interactive Visual Text Analytics (IVTA): Development of new, user-driven text mining and visualization methods for large text corpora exploration.\" This research was carried out partly with the support of the project \"HPC Infrastructure for Grand Challenges of Science and Engineering,\" co-funded by the European Regional Development Fund under the Operational Programme Innovative Economy. This research was partly carried out by PL-Grid Infrastructure.A Visualization of Binary PV-CodesFor an additional comparison with semantic hashing, we used t-distributed Stochastic Neighbor Embedding (van der Maaten and Hinton, 2008) to construct two-dimensional visualizations of codes learned from Binary PV-DBOW using bigrams."}], "references": [{"title": "Estimating or propagating gradients through stochastic neurons for conditional computation", "author": ["Yoshua Bengio", "Nicholas L\u00e9onard", "Aaron Courville."], "venue": "arXiv preprint arXiv:1308.3432 .", "citeRegEx": "Bengio et al\\.,? 2013", "shortCiteRegEx": "Bengio et al\\.", "year": 2013}, {"title": "Similarity estimation tech", "author": ["Moses S Charikar"], "venue": null, "citeRegEx": "Charikar.,? \\Q2002\\E", "shortCiteRegEx": "Charikar.", "year": 2002}, {"title": "On using very large", "author": ["Yoshua Bengio"], "venue": null, "citeRegEx": "Bengio.,? \\Q2015\\E", "shortCiteRegEx": "Bengio.", "year": 2015}, {"title": "The digital universe in 2020: Big data, bigger digital shadows, and biggest growth in the far east", "author": ["John Gantz", "David Reinsel."], "venue": "Technical report, IDC.", "citeRegEx": "Gantz and Reinsel.,? 2012", "shortCiteRegEx": "Gantz and Reinsel.", "year": 2012}, {"title": "Iterative quantization: A procrustean approach to learning binary codes", "author": ["Yunchao Gong", "Svetlana Lazebnik."], "venue": "Computer Vision and Pattern Recognition (CVPR), 2011 IEEE Conference on. IEEE, pages 817\u2013824.", "citeRegEx": "Gong and Lazebnik.,? 2011", "shortCiteRegEx": "Gong and Lazebnik.", "year": 2011}, {"title": "Noisecontrastive estimation: A new estimation principle for unnormalized statistical models", "author": ["Michael Gutmann", "Aapo Hyv\u00e4rinen."], "venue": "International Conference on Artificial Intelligence and Statistics. pages 297\u2013304.", "citeRegEx": "Gutmann and Hyv\u00e4rinen.,? 2010", "shortCiteRegEx": "Gutmann and Hyv\u00e4rinen.", "year": 2010}, {"title": "Approximate nearest neighbors: towards removing the curse of dimensionality", "author": ["Piotr Indyk", "Rajeev Motwani."], "venue": "Proceedings of the thirtieth annual ACM symposium on Theory of computing. ACM, pages 604\u2013613.", "citeRegEx": "Indyk and Motwani.,? 1998", "shortCiteRegEx": "Indyk and Motwani.", "year": 1998}, {"title": "Cumulated gain-based evaluation of ir techniques", "author": ["Kalervo J\u00e4rvelin", "Jaana Kek\u00e4l\u00e4inen."], "venue": "ACM Transactions on Information Systems (TOIS) 20(4):422\u2013446.", "citeRegEx": "J\u00e4rvelin and Kek\u00e4l\u00e4inen.,? 2002", "shortCiteRegEx": "J\u00e4rvelin and Kek\u00e4l\u00e4inen.", "year": 2002}, {"title": "Skip-thought vectors", "author": ["Ryan Kiros", "Yukun Zhu", "Ruslan R Salakhutdinov", "Richard Zemel", "Raquel Urtasun", "Antonio Torralba", "Sanja Fidler."], "venue": "Advances in neural information processing systems. pages 3294\u20133302.", "citeRegEx": "Kiros et al\\.,? 2015", "shortCiteRegEx": "Kiros et al\\.", "year": 2015}, {"title": "Using very deep autoencoders for content-based image retrieval", "author": ["Alex Krizhevsky", "Geoffrey E Hinton."], "venue": "Proceedings of the 19th European Symposium on Artificial Neural Networks. pages 489\u2013494.", "citeRegEx": "Krizhevsky and Hinton.,? 2011", "shortCiteRegEx": "Krizhevsky and Hinton.", "year": 2011}, {"title": "An empirical evaluation of doc2vec with practical insights into document embedding generation", "author": ["Jey Han Lau", "Timothy Baldwin."], "venue": "Proceedings of the 1st Workshop on Representation Learning for NLP. Association for Computational Linguistics,", "citeRegEx": "Lau and Baldwin.,? 2016", "shortCiteRegEx": "Lau and Baldwin.", "year": 2016}, {"title": "Distributed representations of sentences and documents", "author": ["Quoc Le", "Tomas Mikolov."], "venue": "Proceedings of The 31st International Conference on Machine Learning. pages 1188\u20131196.", "citeRegEx": "Le and Mikolov.,? 2014", "shortCiteRegEx": "Le and Mikolov.", "year": 2014}, {"title": "Learning document embeddings by predicting n-grams for sentiment classification of long movie reviews", "author": ["Bofang Li", "Tao Liu", "Xiaoyong Du", "Deyuan Zhang", "Zhe Zhao."], "venue": "arXiv preprint arXiv:1512.08183 .", "citeRegEx": "Li et al\\.,? 2015", "shortCiteRegEx": "Li et al\\.", "year": 2015}, {"title": "Deep learning of binary hash codes for fast image retrieval", "author": ["Kevin Lin", "Huei Fang Yang", "Jen Hao Hsiao", "Chu Song Chen."], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops. pages 27\u201335.", "citeRegEx": "Lin et al\\.,? 2015", "shortCiteRegEx": "Lin et al\\.", "year": 2015}, {"title": "Multimodal similarity-preserving hashing", "author": ["Jonathan Masci", "Michael M Bronstein", "Alexander M Bronstein", "J\u00fcrgen Schmidhuber."], "venue": "IEEE transactions on pattern analysis and machine intelligence 36(4):824\u2013830.", "citeRegEx": "Masci et al\\.,? 2014", "shortCiteRegEx": "Masci et al\\.", "year": 2014}, {"title": "Efficient estimation of word representations in vector space", "author": ["Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean."], "venue": "arXiv preprint arXiv:1301.3781 .", "citeRegEx": "Mikolov et al\\.,? 2013", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Fast search in hamming space with multiindex hashing", "author": ["Mohammad Norouzi", "Ali Punjani", "David J Fleet."], "venue": "Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on. IEEE, pages 3108\u20133115.", "citeRegEx": "Norouzi et al\\.,? 2012", "shortCiteRegEx": "Norouzi et al\\.", "year": 2012}, {"title": "Semantic hashing", "author": ["Ruslan Salakhutdinov", "Geoffrey E Hinton."], "venue": "International Journal of Approximate Reasoning 50(7):969\u2013978.", "citeRegEx": "Salakhutdinov and Hinton.,? 2009", "shortCiteRegEx": "Salakhutdinov and Hinton.", "year": 2009}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["Nitish Srivastava", "Geoffrey E Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov."], "venue": "The Journal of Machine Learning Research 15(1):1929\u20131958.", "citeRegEx": "Srivastava et al\\.,? 2014", "shortCiteRegEx": "Srivastava et al\\.", "year": 2014}, {"title": "Visualizing data using t-SNE", "author": ["Laurens van der Maaten", "Geoffrey Hinton."], "venue": "Journal of Machine Learning Research 9(Nov):2579\u20132605.", "citeRegEx": "Maaten and Hinton.,? 2008", "shortCiteRegEx": "Maaten and Hinton.", "year": 2008}, {"title": "Hashing for similarity search: A survey", "author": ["Jingdong Wang", "Heng Tao Shen", "Jingkuan Song", "Jianqiu Ji."], "venue": "arXiv preprint arXiv:1408.2927 .", "citeRegEx": "Wang et al\\.,? 2014", "shortCiteRegEx": "Wang et al\\.", "year": 2014}, {"title": "Semantic hashing using tags and topic modeling", "author": ["Qifan Wang", "Dan Zhang", "Luo Si."], "venue": "Proceedings of the 36th international ACM SIGIR conference on Research and development in information retrieval. ACM, pages 213\u2013222.", "citeRegEx": "Wang et al\\.,? 2013", "shortCiteRegEx": "Wang et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 6, "context": ", Locality Sensitive Hashing (Indyk and Motwani, 1998), relies on hashing data into short, locality-preserving binary codes (Wang et al.", "startOffset": 29, "endOffset": 54}, {"referenceID": 20, "context": ", Locality Sensitive Hashing (Indyk and Motwani, 1998), relies on hashing data into short, locality-preserving binary codes (Wang et al., 2014).", "startOffset": 124, "endOffset": 143}, {"referenceID": 3, "context": "Gantz and Reinsel (2012), for example, claim that the amount of digital data in the world doubles every two years.", "startOffset": 0, "endOffset": 25}, {"referenceID": 3, "context": "Gantz and Reinsel (2012), for example, claim that the amount of digital data in the world doubles every two years. This trend underpins efforts to develop algorithms that can efficiently search for relevant information in huge datasets. One class of such algorithms, represented by, e.g., Locality Sensitive Hashing (Indyk and Motwani, 1998), relies on hashing data into short, locality-preserving binary codes (Wang et al., 2014). The codes can then be used to group the data into buckets, thereby enabling sublinear search for relevant information, or for fast comparison of data items. Most of the algorithms from this family are data-oblivious, i.e. can generate hashes for any type of data. Nevertheless, some methods target specific kind of input data, like text or image. In this work we focus on learning binary codes for text documents. An important work in this direction has been presented by Salakhutdinov and Hinton (2009). Their semantic hashing leverages autoencoders with sigmoid bottleneck layer to learn binary codes from a word-count bag-of-words (BOW) representation.", "startOffset": 0, "endOffset": 936}, {"referenceID": 3, "context": "Gantz and Reinsel (2012), for example, claim that the amount of digital data in the world doubles every two years. This trend underpins efforts to develop algorithms that can efficiently search for relevant information in huge datasets. One class of such algorithms, represented by, e.g., Locality Sensitive Hashing (Indyk and Motwani, 1998), relies on hashing data into short, locality-preserving binary codes (Wang et al., 2014). The codes can then be used to group the data into buckets, thereby enabling sublinear search for relevant information, or for fast comparison of data items. Most of the algorithms from this family are data-oblivious, i.e. can generate hashes for any type of data. Nevertheless, some methods target specific kind of input data, like text or image. In this work we focus on learning binary codes for text documents. An important work in this direction has been presented by Salakhutdinov and Hinton (2009). Their semantic hashing leverages autoencoders with sigmoid bottleneck layer to learn binary codes from a word-count bag-of-words (BOW) representation. Salakhutdinov & Hinton report that binary codes allow for up to 20-fold improvement in document ranking speed, compared to real-valued representation of the same dimensionality. Moreover, they demonstrate that semantic hashing codes used as an initial document filter can improve precision of TFIDF-based retrieval. Learning binary representation from BOW, however, has its disadvantages. First, word-count representation, and in turn the learned codes, are not in itself stronger than TFIDF. Second, BOW is an inefficient representation: even for moderate-size vocabularies BOW vectors can have thousands of dimensions. Learning fully-connected autoencoders for such highdimensional vectors is impractical. Salakhutdinov & Hinton restricted the BOW vocabulary in their experiments to 2000 most frequent words. Binary codes have also been applied to crossmodal retrieval where text is one of the modalities. Specifically, Wang et al. (2013) incorporated tag information that often accompany text documents, while Masci et al.", "startOffset": 0, "endOffset": 2029}, {"referenceID": 3, "context": "Gantz and Reinsel (2012), for example, claim that the amount of digital data in the world doubles every two years. This trend underpins efforts to develop algorithms that can efficiently search for relevant information in huge datasets. One class of such algorithms, represented by, e.g., Locality Sensitive Hashing (Indyk and Motwani, 1998), relies on hashing data into short, locality-preserving binary codes (Wang et al., 2014). The codes can then be used to group the data into buckets, thereby enabling sublinear search for relevant information, or for fast comparison of data items. Most of the algorithms from this family are data-oblivious, i.e. can generate hashes for any type of data. Nevertheless, some methods target specific kind of input data, like text or image. In this work we focus on learning binary codes for text documents. An important work in this direction has been presented by Salakhutdinov and Hinton (2009). Their semantic hashing leverages autoencoders with sigmoid bottleneck layer to learn binary codes from a word-count bag-of-words (BOW) representation. Salakhutdinov & Hinton report that binary codes allow for up to 20-fold improvement in document ranking speed, compared to real-valued representation of the same dimensionality. Moreover, they demonstrate that semantic hashing codes used as an initial document filter can improve precision of TFIDF-based retrieval. Learning binary representation from BOW, however, has its disadvantages. First, word-count representation, and in turn the learned codes, are not in itself stronger than TFIDF. Second, BOW is an inefficient representation: even for moderate-size vocabularies BOW vectors can have thousands of dimensions. Learning fully-connected autoencoders for such highdimensional vectors is impractical. Salakhutdinov & Hinton restricted the BOW vocabulary in their experiments to 2000 most frequent words. Binary codes have also been applied to crossmodal retrieval where text is one of the modalities. Specifically, Wang et al. (2013) incorporated tag information that often accompany text documents, while Masci et al. (2014) employed siamese neural networks to learn single binary representation for text and image data.", "startOffset": 0, "endOffset": 2121}, {"referenceID": 5, "context": "One can also use noise-contrastive estimation (Gutmann and Hyv\u00e4rinen, 2010) or importance sampling (Cho et al.", "startOffset": 46, "endOffset": 75}, {"referenceID": 13, "context": "Mikolov et al. (2013) proposed loglinear models that learn distributed representations of words by predicting a central word from its context (CBOW model) or by predicting context words given the central word (Skip-gram model).", "startOffset": 0, "endOffset": 22}, {"referenceID": 10, "context": "The CBOW model was then extended by Le and Mikolov (2014) to learn distributed representations of documents.", "startOffset": 36, "endOffset": 58}, {"referenceID": 8, "context": "An alternative approach to learning representation of pieces of text has been recently described by Kiros et al. (2015). Networks proposed therein, inspired by the Skip-gram model, learn to predict surrounding sentences given the center sentence.", "startOffset": 100, "endOffset": 120}, {"referenceID": 13, "context": "One inspiration for binary paragraph vectors comes from a recent work by Lin et al. (2015) on learning binary codes for images.", "startOffset": 73, "endOffset": 91}, {"referenceID": 13, "context": "One inspiration for binary paragraph vectors comes from a recent work by Lin et al. (2015) on learning binary codes for images. Specifically, we introduce a sigmoid layer to the paragraph vector models, and train it in a way that encourages binary activations. We demonstrate that the resultant binary paragraph vectors significantly outperform semantic hashing codes. We also evaluate binary paragraph vectors in transfer learning settings, where training and inference are carried out on unrelated text corpora. Finally, we study models that simultaneously learn short binary codes for document filtering and longer, real-valued representations for ranking. While Lin et al. (2015) employed a supervised criterion to learn image codes, binary paragraph vectors remain unsupervised models: they learn to predict words in documents.", "startOffset": 73, "endOffset": 684}, {"referenceID": 17, "context": "Salakhutdinov and Hinton (2009), for example, used semantic hashing codes for initial filtering and TF-IDF for ranking.", "startOffset": 0, "endOffset": 32}, {"referenceID": 11, "context": "Le and Mikolov (2014) suggest that in PV-DM, a context of the central word can be constructed by either concatenating or averaging the document vector and the embeddings of the surrounding words.", "startOffset": 0, "endOffset": 22}, {"referenceID": 14, "context": "In semantic hashing autoencoders Salakhutdinov and Hinton (2009) added noise to the sigmoid coding layer.", "startOffset": 33, "endOffset": 65}, {"referenceID": 7, "context": "Another approach was used by Krizhevsky and Hinton (2011) in autoencoders that learned binary codes for small images.", "startOffset": 29, "endOffset": 58}, {"referenceID": 0, "context": "We experimented with the methods used in semantic hashing and Krizhevsky\u2019s autoencoders, as well as with the two biased gradient estimators for stochastic binary neurons discussed by Bengio et al. (2013). We also investigated the slope annealing trick (Chung et al.", "startOffset": 183, "endOffset": 204}, {"referenceID": 12, "context": "Results reported by (Li et al., 2015) indicate that performance of PV-DBOW can be improved by including n-grams in the model.", "startOffset": 20, "endOffset": 37}, {"referenceID": 7, "context": "We assess the performance of our models with precision-recall curves and two popular information retrieval metrics, namely mean average precision (MAP) and the normalized discounted cumulative gain at the 10th result (NDCG@10) (J\u00e4rvelin and Kek\u00e4l\u00e4inen, 2002).", "startOffset": 227, "endOffset": 258}, {"referenceID": 17, "context": "In this case we adopted the relevancy measure used by Salakhutdinov and Hinton (2009). That is, the relevancy is calculated as the fraction of overlapping labels in a retrieved document and the query document.", "startOffset": 54, "endOffset": 86}, {"referenceID": 17, "context": "In this case we adopted the relevancy measure used by Salakhutdinov and Hinton (2009). That is, the relevancy is calculated as the fraction of overlapping labels in a retrieved document and the query document. Overall, our selection of test datasets and relevancy measures for 20 Newsgroups and RCV1 follows Salakhutdinov and Hinton (2009), enabling comparison with semantic hashing codes.", "startOffset": 54, "endOffset": 340}, {"referenceID": 18, "context": "During training we employ dropout (Srivastava et al., 2014) in the embedding layer.", "startOffset": 34, "endOffset": 59}, {"referenceID": 18, "context": "During training we employ dropout (Srivastava et al., 2014) in the embedding layer. To facilitate models with large vocabularies, we approximate the gradients with respect to the softmax logits using the method described by Cho et al. (2015). Binary PV-DM networks use the same number of dimensions for document codes and word embeddings.", "startOffset": 35, "endOffset": 242}, {"referenceID": 1, "context": "We experimented with two standard hashing algorithms, namely random hyperplane projection (Charikar, 2002) and iterative quantization (Gong and Lazebnik, 2011).", "startOffset": 90, "endOffset": 106}, {"referenceID": 4, "context": "We experimented with two standard hashing algorithms, namely random hyperplane projection (Charikar, 2002) and iterative quantization (Gong and Lazebnik, 2011).", "startOffset": 134, "endOffset": 159}, {"referenceID": 10, "context": "Lau and Baldwin (2016) evaluated this approach for real-valued paragraph vectors, with promising results.", "startOffset": 0, "endOffset": 23}, {"referenceID": 17, "context": "As pointed out by Salakhutdinov and Hinton (2009), when working with large text collections one can use short binary codes for indexing and a representation with more capacity for ranking.", "startOffset": 18, "endOffset": 50}, {"referenceID": 16, "context": "with multi-index hashing (Norouzi et al., 2012).", "startOffset": 25, "endOffset": 47}, {"referenceID": 11, "context": "Li et al. (2015) made similar observations for Paragraph Vector models, and argue that in distributed memory model the word context takes a lot of the burden of predicting the central word from the document code.", "startOffset": 0, "endOffset": 17}, {"referenceID": 11, "context": "It is also worth noting that Le and Mikolov (2014) constructed paragraph vectors by combining DM and DBOW representations.", "startOffset": 29, "endOffset": 51}], "year": 2017, "abstractText": "Recently Le & Mikolov described two log-linear models, called Paragraph Vector, that can be used to learn state-ofthe-art distributed representations of documents. Inspired by this work, we present Binary Paragraph Vector models: simple neural networks that learn short binary codes for fast information retrieval. We show that binary paragraph vectors outperform autoencoder-based binary codes, despite using fewer bits. We also evaluate their precision in transfer learning settings, where binary codes are inferred for documents unrelated to the training corpus. Results from these experiments indicate that binary paragraph vectors can capture semantics relevant for various domainspecific documents. Finally, we present a model that simultaneously learns short binary codes and longer, real-valued representations. This model can be used to rapidly retrieve a short list of highly relevant documents from a large document collection.", "creator": "LaTeX with hyperref package"}}}