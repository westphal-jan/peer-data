{"id": "1611.01142", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Nov-2016", "title": "Using a Deep Reinforcement Learning Agent for Traffic Signal Control", "abstract": "Ensuring transportation systems are efficient is a priority for modern society. Technological advances have made it possible for transportation systems to collect large volumes of varied data on an unprecedented scale. We propose a traffic signal control system which takes advantage of this new, high quality data, with minimal abstraction compared to other proposed systems. We apply modern deep reinforcement learning methods to build a truly adaptive traffic signal control agent in the traffic microsimulator SUMO. We propose a new state space, the discrete traffic state encoding, which is information dense. The discrete traffic state encoding is used as input to a deep convolutional neural network, trained using Q-learning with experience replay. Our agent was compared against a one hidden layer neural network traffic signal control agent and reduces average cumulative delay by 82%, average queue length by 66% and average travel time by 20%.", "histories": [["v1", "Thu, 3 Nov 2016 19:46:19 GMT  (1981kb,D)", "http://arxiv.org/abs/1611.01142v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.SY", "authors": ["wade genders", "saiedeh razavi"], "accepted": false, "id": "1611.01142"}, "pdf": {"name": "1611.01142.pdf", "metadata": {"source": "CRF", "title": "Using a Deep Reinforcement Learning Agent for Traffic Signal Control", "authors": ["Wade Gendersa", "Saiedeh Razavib"], "emails": ["genderwt@mcmaster.ca", "razavi@mcmaster.ca"], "sections": [{"heading": null, "text": "It is one of the greatest challenges in the history of the European Union, in which the EU accession shares are modelled on the EU accession shares."}, {"heading": "II. LITERATURE REVIEW", "text": "In this context, it should be noted that the measures in question are measures that have been taken in recent years."}, {"heading": "III. PROPOSED SYSTEM", "text": "Attempting to solve the problem of traffic light control through reinforcement learning requires a formulation of the problem in the language of reinforcement learning, in particular the definition of a state room S, an action room A and a reward room R."}, {"heading": "A. State Space", "text": "We propose the DTSE as a suitable state space S in this research, inspired by a common technique for calculating the discrediting and quantification of continuous units. For each lane that approaches the intersection, the DTSE discredits a length l of the track segment, starting at the stop, in cells of length c. The selection of c will change the behavior of the system. If c is many times greater than the average vehicle length, the individual dynamics of each vehicle will be lost, but the calculation costs will be reduced. If c is much smaller than the average vehicle length, the individual vehicle dynamics will be maintained, but the computer-related costs will increase, perhaps unnecessarily. We mention the selection of c is important for this research, which we select c in a simplified way to evaluate the proposed system. The DTSE consists of three vectors, the first representing the presence of a vehicle or not in the cell."}, {"heading": "B. Action Space", "text": "After observing the state of the environment, the agent must select an action from the list of all available actions. In this research, the possible actions of the agent are related to the configuration of the traffic light phases (i.e. the combination of traffic lights that control the individual lanes for the entire intersection) and, for reasons of simplicity and human understanding, are assigned to a compass direction that indicates the traffic light phases (i.e. the color of the traffic light phases) and abbreviated for abbreviation reasons. For explicit reasons, a green light phase means that vehicles can drive through the intersection, yellow warnings for vehicles preparing to stop, and red means that vehicles should stop and not drive through the intersection. Possible actions are North-South-Green (EEC), North-South-Advance-Left-Green (NSLG), East-Advance-Left-Measures (EWLG)."}, {"heading": "C. Reward", "text": "The reward is an element that distinguishes enhanced learning from other types of machine learning; the development of a government action policy that maximizes cumulative long-term reward is what the actor is looking for. Compared to other types of machine learning, in which correct actions are given by instruction, enhanced learning lets the actor evaluate actions through interaction with the environment. How to select the appropriate reward for a given task is an unanswered problem in traditional reinforced learning1. It would be desirable if the actor could choose his own reward instead of requiring an expert to define it, and is therefore a goal of many active researchers.In the context of traffic light control, various rewards have been proposed, such as changing the number of driven vehicles, changing the cumulative vehicle delay and changing the vehicle throughput. The reward rt + 1 is a consequence of performing a selected action by a particular state, or a specific state measure of delay, whereby the reward may be regarded as 1."}, {"heading": "D. Agent", "text": "This year, more than ever before in the history of the country in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is not a country, but in which it is a country, a country, a city and a country."}, {"heading": "IV. EXPERIMENTAL SETUP AND TRAINING", "text": "In fact, it is such that it is a matter of a way in which people in the USA and in other parts of the world put themselves into another world, in which they put themselves into another world, in which they put themselves into another world, in which they put themselves into another world, in which they put themselves into another world, in which they put themselves into themselves and in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they live, in which they, in which they live, in which they, in which they live, in which they, in which they live, in which they, in which they live, in which they, in which they live, in which they, in which they live, in which they, in which they live, in which they, in which they, they, in which they, they, in which they, they, in which they live, they, in which they, they, in which they, they, in which they, they, in which they, they, in which they live, they, in which they, they, in which they, in which they, they, live, in which they, they, in which they, they, live, in which they, they, in which they, live, they, in which they, they, in which they, live, they, in which they, in which they, live, they, in which they, live, they, they, in which, live, they, in which, live, they, they, in which, they, they, in which, live, they, they, in which, they, they, in which, live, they, in which, they, they, in which, live, they, in which, they, they, in which, they, in which, live, they, they, they, in which, live, they, in which, they, they, in which, in which, they, in which, live, they, in which, they, live, they, in which, they, in which, they, they, they, in which, in which, they, in which, in which, they, live, they, in which, they, they, in which, in which, live, they, they,"}, {"heading": "V. RESULTS AND DISCUSSION", "text": "The performance of the proposed DQTSCA was evaluated in terms of common traffic metrics: throughput, queue length, travel time and cumulative delay. The performance of the agent in terms of traffic metrics during learning can be seen in Figures 2, 3, 4 and 5. The performance of the agent in terms of achieving the reward during learning can be seen in Figure 6. The performance of the agent in terms of exploiting performance during an epoch is also shown, in Figure 7 exclusively the exploitation after training in the figure. 8. Initially, during learning, the agent is predominantly engaged in research (i.e., performing random actions), attempting to explore the agent's action value function, the performance of the agent in terms of traffic metrics shows large differences and it achieves negative reward (i.e., punishment). Due to the actions of the agent, many vehicles in total queues are low, unretarded and the performance of the proposed DQTSCA is low."}, {"heading": "VI. CONCLUSION", "text": "(2 452, 257) (2 048) (3) (3) (3) (3) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4 (4) (4) (4) (4) (4) (4) (4 (4) (4) (4) (4 (4) (4) (4) (4 (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4) (4 (4) (4) (4) (4) (4) (4) (4) (4) ("}, {"heading": "ACKNOWLEDGMENT", "text": "The authors would like to pay tribute to the authors of the various programming libraries used in this research, as well as the comments and support of friends, family and colleagues."}], "references": [{"title": "Human-level control through deep reinforcement learning", "author": ["V. Mnih", "K. Kavukcuoglu", "D. Silver", "A.A. Rusu", "J. Veness", "M.G. Bellemare", "A. Graves", "M. Riedmiller", "A.K. Fidjeland", "G. Ostrovski"], "venue": "Nature, vol. 518, no. 7540, pp. 529\u2013533, 2015.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2015}, {"title": "Temporal difference learning and td-gammon", "author": ["G. Tesauro"], "venue": "Communications of the ACM, vol. 38, no. 3, pp. 58\u201368, 1995.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1995}, {"title": "Receptive fields, binocular interaction and functional architecture in the cat\u2019s visual cortex", "author": ["D.H. Hubel", "T.N. Wiesel"], "venue": "The Journal of physiology, vol. 160, no. 1, pp. 106\u2013154, 1962.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1962}, {"title": "Receptive fields and functional architecture of monkey striate cortex", "author": ["\u2014\u2014"], "venue": "The Journal of physiology, vol. 195, no. 1, pp. 215\u2013243, 1968.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1968}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE, vol. 86, no. 11, pp. 2278\u20132324, 1998.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1998}, {"title": "Urban traffic signal control using reinforcement learning agents", "author": ["P. Balaji", "X. German", "D. Srinivasan"], "venue": "IET Intelligent Transport Systems, vol. 4, no. 3, pp. 177\u2013188, 2010.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2010}, {"title": "Reinforcement learning with function approximation for traffic signal control", "author": ["L. Prashanth", "S. Bhatnagar"], "venue": "IEEE Transactions on Intelligent Transportation Systems, vol. 12, no. 2, pp. 412\u2013421, 2011.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2011}, {"title": "Multiagent reinforcement learning for integrated network of adaptive traffic signal controllers (marlin-atsc): methodology and large-scale application on downtown toronto", "author": ["S. El-Tantawy", "B. Abdulhai", "H. Abdelgawad"], "venue": "IEEE Transactions on Intelligent Transportation Systems, vol. 14, no. 3, pp. 1140\u20131150, 2013.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2013}, {"title": "Traffic light control using sarsa with three state representations", "author": ["T.L. Thorpe", "C.W. Anderson"], "venue": "Citeseer, Tech. Rep., 1996.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1996}, {"title": "Multi-agent reinforcement learning for traffic light control", "author": ["M. Wiering"], "venue": "ICML, 2000, pp. 1151\u20131158.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2000}, {"title": "Optimizing traffic lights in a cellular automaton model for city traffic", "author": ["E. Brockfeld", "R. Barlovic", "A. Schadschneider", "M. Schreckenberg"], "venue": "Physical Review E, vol. 64, no. 5, p. 056132, 2001.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2001}, {"title": "Reinforcement learning for true adaptive traffic signal control", "author": ["B. Abdulhai", "R. Pringle", "G.J. Karakoulas"], "venue": "Journal of Transportation Engineering, vol. 129, no. 3, pp. 278\u2013285, 2003.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2003}, {"title": "Qlearning based traffic optimization in management of signal timing plan", "author": ["Y.K. Chin", "N. Bolong", "A. Kiring", "S.S. Yang", "K.T.K. Teo"], "venue": "International Journal of Simulation, Systems, Science & Technology, vol. 12, no. 3, pp. 29\u201335, 2011.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2011}, {"title": "Holonic multi-agent system for traffic signals control", "author": ["M. Abdoos", "N. Mozayani", "A.L. Bazzan"], "venue": "Engineering Applications of Artificial Intelligence, vol. 26, no. 5, pp. 1575\u20131587, 2013.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2013}, {"title": "Reinforcement learning-based multi-agent system for network traffic signal control", "author": ["I. Arel", "C. Liu", "T. Urbanik", "A. Kohls"], "venue": "IET Intelligent Transport Systems, vol. 4, no. 2, pp. 128\u2013135, 2010.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2010}, {"title": "Design of reinforcement learning parameters for seamless application of adaptive traffic signal control", "author": ["S. El-Tantawy", "B. Abdulhai", "H. Abdelgawad"], "venue": "Journal of Intelligent Transportation Systems, vol. 18, no. 3, pp. 227\u2013245, 2014.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2014}, {"title": "An experimental review of reinforcement learning algorithms for adaptive traffic signal control", "author": ["P. Mannion", "J. Duggan", "E. Howley"], "venue": "Autonomic Road Transport Support Systems. Springer, 2016, pp. 47\u201366.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2016}, {"title": "Looking at vehicles on the road: A survey of vision-based vehicle detection, tracking, and behavior analysis", "author": ["S. Sivaraman", "M.M. Trivedi"], "venue": "IEEE Transactions on Intelligent Transportation Systems, vol. 14, no. 4, pp. 1773\u20131795, 2013.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2013}, {"title": "Its eprimer module 13: Connected vehicles", "author": ["C. Hill", "G. Krueger"], "venue": "[Online; accessed 2016-09-07]. [Online]. Available: https: //www.pcb.its.dot.gov/eprimer/module13.aspx", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2016}, {"title": "Q-learning", "author": ["C.J. Watkins", "P. Dayan"], "venue": "Machine learning, vol. 8, no. 3-4, pp. 279\u2013292, 1992.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 1992}, {"title": "Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude", "author": ["T. Tieleman", "G. Hinton"], "venue": "COURSERA: Neural Networks for Machine Learning, vol. 4, no. 2, 2012.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2012}, {"title": "Recent development and applications of SUMO - Simulation of Urban MObility", "author": ["D. Krajzewicz", "J. Erdmann", "M. Behrisch", "L. Bieker"], "venue": "International Journal On Advances in Systems and Measurements, vol. 5, no. 3&4, pp. 128\u2013138, December 2012.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2012}, {"title": "Keras", "author": ["F. Chollet"], "venue": "https://github.com/fchollet/keras, 2015.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2015}, {"title": "Theano: A Python framework for fast computation of mathematical expressions", "author": ["Theano Development Team"], "venue": "arXiv e-prints, vol. abs/1605.02688, may 2016. [Online]. Available: http://arxiv.org/abs/ 1605.02688", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2016}, {"title": "SciPy: Open source scientific tools for Python", "author": ["E. Jones", "T. Oliphant", "P. Peterson"], "venue": "2001, [Online; accessed 2016-09-06]. [Online]. Available: http://www.scipy.org/", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2001}, {"title": "An empirical analysis of vehicle time headways on rural two-lane two-way roads", "author": ["R. Riccardo", "G. Massimiliano"], "venue": "Procedia-Social and Behavioral Sciences, vol. 54, pp. 865\u2013874, 2012.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2012}, {"title": "Speed and time headway distribution under mixed traffic condition", "author": ["Maurya A.", "DEY S.", "DAS S."], "venue": "Journal of the Eastern Asia Society for Transportation Studies, vol. 11, no. 0, pp. 1774\u20131792, 2015.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2015}, {"title": "Why there are complementary learning systems in the hippocampus and neocortex: insights from the successes and failures of connectionist models of learning and memory.", "author": ["J.L. McClelland", "B.L. McNaughton", "R.C. O\u2019Reilly"], "venue": "Psychological review,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 1995}, {"title": "Play it again: reactivation of waking experience and memory", "author": ["J. ONeill", "B. Pleydell-Bouverie", "D. Dupret", "J. Csicsvari"], "venue": "Trends in neurosciences, vol. 33, no. 5, pp. 220\u2013229, 2010.  9", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2010}], "referenceMentions": [{"referenceID": 0, "context": "advancements from the domain of artificial intelligence [1] to develop a new traffic signal controller.", "startOffset": 56, "endOffset": 59}, {"referenceID": 1, "context": "Function approximators, such as artificial neural networks, have been used in reinforcement learning to approximate value functions when the agent\u2019s representation of the environment, or state space, becomes too large [2].", "startOffset": 218, "endOffset": 221}, {"referenceID": 2, "context": "network architecture, are inspired by biological research on the animal visual cortex [3][4] and have displayed impressive performance [5].", "startOffset": 86, "endOffset": 89}, {"referenceID": 3, "context": "network architecture, are inspired by biological research on the animal visual cortex [3][4] and have displayed impressive performance [5].", "startOffset": 89, "endOffset": 92}, {"referenceID": 4, "context": "network architecture, are inspired by biological research on the animal visual cortex [3][4] and have displayed impressive performance [5].", "startOffset": 135, "endOffset": 138}, {"referenceID": 5, "context": "Previous research using reinforcement learning for traffic signal control has yielded impressive results [6][7][8],", "startOffset": 105, "endOffset": 108}, {"referenceID": 6, "context": "Previous research using reinforcement learning for traffic signal control has yielded impressive results [6][7][8],", "startOffset": 108, "endOffset": 111}, {"referenceID": 7, "context": "Previous research using reinforcement learning for traffic signal control has yielded impressive results [6][7][8],", "startOffset": 111, "endOffset": 114}, {"referenceID": 8, "context": "[9][10][11][12].", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "[9][10][11][12].", "startOffset": 3, "endOffset": 7}, {"referenceID": 10, "context": "[9][10][11][12].", "startOffset": 7, "endOffset": 11}, {"referenceID": 11, "context": "[9][10][11][12].", "startOffset": 11, "endOffset": 15}, {"referenceID": 9, "context": "Previous research efforts have defined the state space as some attribute of traffic, the number of queued vehicles [10][12][13][14] and traffic flow [15][6] the", "startOffset": 115, "endOffset": 119}, {"referenceID": 11, "context": "Previous research efforts have defined the state space as some attribute of traffic, the number of queued vehicles [10][12][13][14] and traffic flow [15][6] the", "startOffset": 119, "endOffset": 123}, {"referenceID": 12, "context": "Previous research efforts have defined the state space as some attribute of traffic, the number of queued vehicles [10][12][13][14] and traffic flow [15][6] the", "startOffset": 123, "endOffset": 127}, {"referenceID": 13, "context": "Previous research efforts have defined the state space as some attribute of traffic, the number of queued vehicles [10][12][13][14] and traffic flow [15][6] the", "startOffset": 127, "endOffset": 131}, {"referenceID": 14, "context": "Previous research efforts have defined the state space as some attribute of traffic, the number of queued vehicles [10][12][13][14] and traffic flow [15][6] the", "startOffset": 149, "endOffset": 153}, {"referenceID": 5, "context": "Previous research efforts have defined the state space as some attribute of traffic, the number of queued vehicles [10][12][13][14] and traffic flow [15][6] the", "startOffset": 153, "endOffset": 156}, {"referenceID": 14, "context": "The action space has been defined as all available signal phases [15][8] or restricted to green phases only [6][13][14].", "startOffset": 65, "endOffset": 69}, {"referenceID": 7, "context": "The action space has been defined as all available signal phases [15][8] or restricted to green phases only [6][13][14].", "startOffset": 69, "endOffset": 72}, {"referenceID": 5, "context": "The action space has been defined as all available signal phases [15][8] or restricted to green phases only [6][13][14].", "startOffset": 108, "endOffset": 111}, {"referenceID": 12, "context": "The action space has been defined as all available signal phases [15][8] or restricted to green phases only [6][13][14].", "startOffset": 111, "endOffset": 115}, {"referenceID": 13, "context": "The action space has been defined as all available signal phases [15][8] or restricted to green phases only [6][13][14].", "startOffset": 115, "endOffset": 119}, {"referenceID": 14, "context": "The most common reward definitions are change in delay [15][8] and change in queued vehicles [6][13][14].", "startOffset": 55, "endOffset": 59}, {"referenceID": 7, "context": "The most common reward definitions are change in delay [15][8] and change in queued vehicles [6][13][14].", "startOffset": 59, "endOffset": 62}, {"referenceID": 5, "context": "The most common reward definitions are change in delay [15][8] and change in queued vehicles [6][13][14].", "startOffset": 93, "endOffset": 96}, {"referenceID": 12, "context": "The most common reward definitions are change in delay [15][8] and change in queued vehicles [6][13][14].", "startOffset": 96, "endOffset": 100}, {"referenceID": 13, "context": "The most common reward definitions are change in delay [15][8] and change in queued vehicles [6][13][14].", "startOffset": 100, "endOffset": 104}, {"referenceID": 15, "context": "signal control research, the reader is referred to [16] and [17].", "startOffset": 51, "endOffset": 55}, {"referenceID": 16, "context": "signal control research, the reader is referred to [16] and [17].", "startOffset": 60, "endOffset": 64}, {"referenceID": 8, "context": "However, some previous research has proposed a similar, less abstracted, yet limited, state definition [9], from which our research acknowledges", "startOffset": 103, "endOffset": 106}, {"referenceID": 8, "context": "The addition of second speed vector is an extension beyond [9], as their state definition only consists of a vector representing the presence of a vehicle.", "startOffset": 59, "endOffset": 62}, {"referenceID": 17, "context": "Video cameras [18] are becoming more common as sensor devices at intersections and vehicles with wireless communication capabilities", "startOffset": 14, "endOffset": 18}, {"referenceID": 18, "context": ", Connected Vehicles [19]) are expected to be deployed in the near future.", "startOffset": 21, "endOffset": 25}, {"referenceID": 0, "context": "We model the agent controlling the traffic signals as a deep convolutional Q-network [1].", "startOffset": 85, "endOffset": 88}, {"referenceID": 2, "context": "Convolutional neural networks are a variant of artificial neural networks inspired by biological research that emulate the architecture of the animal visual cortex [3][4], making them adept at perception tasks.", "startOffset": 164, "endOffset": 167}, {"referenceID": 3, "context": "Convolutional neural networks are a variant of artificial neural networks inspired by biological research that emulate the architecture of the animal visual cortex [3][4], making them adept at perception tasks.", "startOffset": 167, "endOffset": 170}, {"referenceID": 19, "context": "is Q-Learning [20], which is used to develop an optimal action-selection policy.", "startOffset": 14, "endOffset": 18}, {"referenceID": 0, "context": "Both the learning rate and the discount factor are parameters of Q-learning and are \u03b3, \u03b1 \u2208 [0, 1].", "startOffset": 91, "endOffset": 97}, {"referenceID": 20, "context": "We use the RMSprop [21] gradient descent algorithm with an \u03b1 of 0.", "startOffset": 19, "endOffset": 23}, {"referenceID": 21, "context": "22 [22].", "startOffset": 3, "endOffset": 7}, {"referenceID": 22, "context": "neural network was implemented using Keras [23] and Theano [24] Python libraries.", "startOffset": 43, "endOffset": 47}, {"referenceID": 23, "context": "neural network was implemented using Keras [23] and Theano [24] Python libraries.", "startOffset": 59, "endOffset": 63}, {"referenceID": 24, "context": "Additional optimized functionality was provided by NumPy and SciPy [25] libraries.", "startOffset": 67, "endOffset": 71}, {"referenceID": 25, "context": "mated by different probability distributions [26][27].", "startOffset": 45, "endOffset": 49}, {"referenceID": 26, "context": "mated by different probability distributions [26][27].", "startOffset": 49, "endOffset": 53}, {"referenceID": 27, "context": "The agent is trained using a biologically inspired process known as experience replay [28][29][30].", "startOffset": 86, "endOffset": 90}, {"referenceID": 28, "context": "The agent is trained using a biologically inspired process known as experience replay [28][29][30].", "startOffset": 90, "endOffset": 94}, {"referenceID": 25, "context": "Flow Rate (Vehicles/Hour) Distribution Parameters (\u03b1, \u03b2) 0-150 Inverse Weibull [26] (0.", "startOffset": 79, "endOffset": 83}, {"referenceID": 26, "context": "8) 250-450 Burr [27] (1.", "startOffset": 16, "endOffset": 20}], "year": 2016, "abstractText": "Ensuring transportation systems are efficient is a priority for modern society. Technological advances have made it possible for transportation systems to collect large volumes of varied data on an unprecedented scale. We propose a traffic signal control system which takes advantage of this new, high quality data, with minimal abstraction compared to other proposed systems. We apply modern deep reinforcement learning methods to build a truly adaptive traffic signal control agent in the traffic microsimulator SUMO. We propose a new state space, the discrete traffic state encoding, which is information dense. The discrete traffic state encoding is used as input to a deep convolutional neural network, trained using Q-learning with experience replay. Our agent was compared against a one hidden layer neural network traffic signal control agent and reduces average cumulative delay by 82%, average queue length by 66% and average travel time by 20%.", "creator": "LaTeX with hyperref package"}}}