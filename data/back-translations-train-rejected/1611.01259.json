{"id": "1611.01259", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Nov-2016", "title": "Generalized Topic Modeling", "abstract": "Recently there has been significant activity in developing algorithms with provable guarantees for topic modeling. In standard topic models, a topic (such as sports, business, or politics) is viewed as a probability distribution $\\vec a_i$ over words, and a document is generated by first selecting a mixture $\\vec w$ over topics, and then generating words i.i.d. from the associated mixture $A{\\vec w}$. Given a large collection of such documents, the goal is to recover the topic vectors and then to correctly classify new documents according to their topic mixture.", "histories": [["v1", "Fri, 4 Nov 2016 03:45:03 GMT  (1119kb,D)", "http://arxiv.org/abs/1611.01259v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.CL cs.DS cs.IR", "authors": ["avrim blum", "nika haghtalab"], "accepted": false, "id": "1611.01259"}, "pdf": {"name": "1611.01259.pdf", "metadata": {"source": "CRF", "title": "Generalized Topic Modeling", "authors": ["Avrim Blum", "Nika Haghtalab"], "emails": ["avrim@cs.cmu.edu", "nhaghtal@cs.cmu.edu"], "sections": [{"heading": null, "text": "In this paper, we look at a broad generalization of this framework, in which words are no longer drawn i.i.d. and instead a topic is a complex distribution across paragraphs. Since one could not even hope to represent such a distribution in general (even if paragraphs are given using a natural representation of characteristics), we aim instead to learn directly a document classifier. That is, we want to learn a predictor that accurately predicts its mix of topics in a new document without explicitly learning the distributions. We present several natural conditions under which this can be done efficiently and discuss issues such as noise tolerance and sample complexity in this model. More generally, our model can be considered a generalization of the multi-view or co-training setting in machine learning. \u2022 Partly supported by the National Science Foundation, CCF-1525971 and CCF-1535967 grant. Partly supported by the National Science Foundation, CCF-1525971 grant and a Microsoft Research Fellowship grant."}, {"heading": "1 Introduction", "text": "It's not as if this is a purely formal question. It's not as if it's a purely formal question. It's as if it's a purely formal question. It's as if it's a purely formal question. It's as if it's a purely formal question. It's as if it's a formal question. It's as if it's a formal question. It's as if it's a formal question. It's as if it's a formal question. It's as if it's a formal question. It's as if it's a formal question. It's as if it's a formal question."}, {"heading": "2 Preliminaries", "text": "We assume that it is a real case, in which it is a case, in which it is a case, in which it is a case, in which it is a case, in which it is a case, in which it is a case, in which it is a case, in which it is a case, in which it is a case, in which it is a case, in which it is a case, in which it is a case, in which it is a case, in which it is a case, in which it is a case, in which it is a case, in which it is a case, in which it is a case, in which it is a case, in which it is a case, in which it is a case, in which it is a case, in which it is a case, in which it is a case, in which it is a case, in which it is a case, in which it is a case, in which it is a case, in which it is a case, in which it is a case, in which it is a case, in which it is a case, in which it is a case, in which it is a case, in which it is a case in which it is a case, in which it is a case in which it is a case, in which it is a case in which it is a case, in which it is a case in which it is a case in which it is a case, in which it is a case in which it is a case in which it is a case, in which it is a case in which it is a case in which it is a case, in which it is a case in which it is a case in which it is a case in which it is a case is a case, in which it is a case is a case in which it is a case in which it is a case, in which it is a case in which it is a case is a case in which it is a case, in which it is a case is a case in which it is a case in which it is a case in which it is a case, in which it is a case is a case in which it is a case, in which it is a case in which it is a case in which it is a case in which it is a case, in which it is a case, in which it is a case in which it is a case, in which it is a case"}, {"heading": "3 An Easier Case with Simplifying Assumptions", "text": "In fact, it is such that it is a matter of a way in which people act in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world"}, {"heading": "4 Relaxing the Assumptions", "text": "In this section we relax the two most important simplistic assumptions from Section 3. We relax the assumptions on non-noisy documents and allow a large part of the documents not to satisfy the existence of \"almostpure\" documents. We further develop the approach discussed in the previous section and introduce efficient algorithms that roughly restore the subject vectors in this setting. Setting: We assume that each sampled document has a non-neggligible probability of being non-noisy and with the remaining probability that the two views of the document will be affected by additive Gaussian noise, regardless of the fact that for a given sample (x1, x2), with the probability p0 > 0 the algorithms will be preserved (x1, x2) and with the remaining probability."}, {"heading": "4.1 Proof of Lemma 4.2 \u2014 Phase 1", "text": "For j {1, 2}, let Xj and X-j be n > m matrices with the ith column is xji and x-j i, respectively. As we have shown in Lemma 3.1, with high probability rank (X1 \u2212 X2) = n \u2212 k. Note that the zero space of the columns of X1 \u2212 X2 is spanned by the left singular vectors of X1 \u2212 X2 corresponding to their k zero singular values. Likewise, we consider the space spanned by the k least left singular vectors of X-1 \u2212 X-2. We show that the zero space of the columns of X1 \u2212 X2 can be spanned through the space spanned by the k least left singular vectors of X-1 \u2212 X-2 within any desirable accuracy, given a sufficiently large number of samples.Let D = X1 \u2212 X2 and D = X-2. \""}, {"heading": "4.1.1 Concentration", "text": "It remains to prove properties 1 and 2. \u2212 We briefly describe our approach to obtaining concentration results and prove that if the number of samples m is large enough, with a high probability that the number of samples m is large enough, with a high probability that the number of samples m is large enough, with a high probability that the number of samples m is large enough, with a high probability that the number of samples m is large enough. \u2212 \u2212 2\u03c32In \u2212 1mDD > in terms of error matrices. \u2212 2mD we have a sufficient probability that the number of samples m is large enough."}, {"heading": "4.2 Proof of Lemma 4.3 \u2014 Denoising Step", "text": "It is not the first time that we have been able to record the number of points we are able to record in a class. (...) It is not the first time that we have been able to record the number of points within the class, and it is the second time that we have been able to record the number of points within the class. (...) It is the second time that we have been able to record the number of points within the class. (...) It is the second time that we have been able to record the number of points within the class. (...) It is the second time that we have been able to record the number of points within the class. (...) It is the second time that we have been able to record the number of points within the class. (...) It is the second time that we have been able to record the number of points within the class. (...) It is the first time that we have been able to record the number of points within the class. (...) It is the third time that we have been able to record the number within the class. (...) It is the third time that we have been able to record the number within the class."}, {"heading": "4.3 Proof of Lemma 4.4 \u2014 Phase 2", "text": "On a high level we are looking at two balls around each projected point x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x"}, {"heading": "5 Additional Results, Extensions, and Open Problems", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 Sample Complexity Lower bound", "text": "However, since the number of classes can be much smaller than the number of characters, one might hope to restore v1,.., vk, with a number of samples that are polynomial in k and not in n. At this point, we show that in general, samples v1,.. vk regardless of the value of k. To facilitate exposure, leave k = 1 and note that in this case each sample should be purely of one type. Suppose that the class vector v is promised to be in the set C = {vj | vj '= 1 / \u221a 2 if \"= 2j \u2212 1 or 2j, otherwise vj' = 0}. Suppose that instances (x 1j, x 2j) are such that the\" th coordinate of x1j \"is x."}, {"heading": "5.2 Alternative Noise Models", "text": "Consider the problem of restoring v1,.., vk in the presence of agnostic noise, where for a fraction of the samples (x1, x2), x1 and x2 correspond to different mixing weights. Furthermore, we assume that the distribution over the instance space is so rich that any subspace other than span {v1,.., vk} is incompatible with a number of instances of not negligible density. 4 Since the VC dimension of the set of dimensional subspaces in Rn min {k, n \u2212 k}, from the point of view of information theory, one can recover span {v1,.., vk} as the only subspace incompatible with less than O () fraction of the O samples (k2). Furthermore, we can detect and remove any noisy sample for which the two views of the sample are inconsistent."}, {"heading": "5.3 General function f(\u00b7)", "text": "Consider the general model described in Section 2, where fi (x) = f (vi \u00b7 x) for an unknown, strictly increasing function f: R + \u2192 [0, 1] is described in such a way that f (0) = 0. We describe how the variations of the techniques discussed so far can extend to this more general environment. Therefore, let us consider the non-noisy case. Since f is a strictly increasing function, f (vi \u00b7 x1) = f (vi \u00b7 x2), if and only if vi \u00b7 x1 = vi \u00b7 x2. Therefore, we can restore the span (v1,., vk) using the same approach as in Phase 1 of algorithm 1. Although the projection of x by definition of pseudoinverse matrices is still represented by x = i (vi \u00b7 x) ai, this is not necessarily a convex combination of ai's more than a confrontation of ai's. This is due to the fact that values greater than x depend on vi in x1."}, {"heading": "A Omitted Proof from Section 3 \u2014 No Noise", "text": "Yyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyy yyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyy yyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyy yyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyy yyyyyyyyyyyyyyyyyyyyyyyyyyyy"}, {"heading": "B Technical Spectral Lemmas", "text": "Proposition B.1 (Davis and Kahan (1970) sin \u03b8 theorem). Let B, B, B, Rp and V = (vr,.., vs) and V = (v,.., v, s) be the orthonormal eigenvectors, each of the size r,.., and V = (vr,., vs) and V = (v,., v,.) be the orthonormal eigenvectors, each of the size r,.....,... and...... (v,.,.). Let. (v,.)."}, {"heading": "C Omitted Proof from Section 4.1 \u2014 Phase 1", "text": "C.1 proof of claim 4.6Let ei and di be the ith row of E and D. Then ED > = 1 = 1 = 1 = 2 = 2 = 2 = 2 = 2 = 2 = 3. Let Si = 1 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2"}, {"heading": "D Omitted Proof from Section 4.2 \u2014 Denoising", "text": "D.1 Evidence of claim 4.8 It should be remembered that for each i-point [k] with the probability \u03b3 = g (\u2032 / (8k\u03b1) of P an almost pure weight vector w is generated, so that the point is not loud independently of each other with the probability p0. Therefore, there is a p0\u03b3 density at non-loud points, which is almost exclusively of class i. Note that for such points x, x, x Px \u2212 ai = 1 wjaj \u2212 ai = 1 wjaj \u2212 ai = 1 wjaj \u2212 i = 1 wjaj \u2212 i = 1 wjaj \u2212 q = k (\u2032 / (((8k\u03b1)) (\u03b1) \u2264 8 applies. Since the allegation applies: \"P \u2212 P-P-P-P-P-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T."}, {"heading": "E Omitted Proof from Section 4.3 \u2014 Phase 2", "text": "E.1 Dropped proof from claim 4.11 Here we prove that x-x-CH (S-Bd + \"(a-i)) then x-CH (a-i) exists so that x-x-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i"}, {"heading": "F Proof of Theorem 5.1 \u2014 Lower Bound", "text": "For the simplicity of the presentation, we assume that n is a multiple of k. Moreover, in this proof, we assume the term (xi, x \"i) to represent the two views of the ith sample. For each vector u\" Rn \"and i\" k \"we use (u\" i \") to describe the\" i \"dimensional pattern of u\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i.\" \"\" i. \"\" i. \"\" i. \"\" i. \"\" i. \"\" i. \"\" i. \"\" i. \"i.\" \"i.\" i. \"\" i. \"i.\" i. \"i.\" \"i.\" i. \"i.\" \"i.\" i. \"i.\" \"i.\" i. \"i.\" i. \"i.\" i. \"\" i. \"i.\" i. \"i.\" \"i.\" i. \"i.\" i. \"i.\" i. \"i.\" \"i.\" i. \"i.\" i. \"i.\" i. \"i.\" i. \"i.\" i. \"i.\" i. \"i.\" i. \"i.\" i. \"i.\" i. \"i.\" i. \"i.\" i. \"i.\" i. \"i.\" i. \"i.\" i. \"i.\" i. \"i.\" i. \"i.\" i. \"i.\" i. \"i.\" i. \"i.\" i. \"i.\" i. \""}], "references": [{"title": "Tensor decompositions for learning latent variable models", "author": ["A. Anandkumar", "R. Ge", "D. Hsu", "S.M. Kakade", "M. Telgarsky"], "venue": "Journal of Machine Learning Research, 15(1):2773\u20132832.", "citeRegEx": "Anandkumar et al\\.,? 2014", "shortCiteRegEx": "Anandkumar et al\\.", "year": 2014}, {"title": "A spectral algorithm for latent dirichlet allocation", "author": ["A. Anandkumar", "Liu", "Y.-k.", "D.J. Hsu", "D.P. Foster", "S.M. Kakade"], "venue": "Advances in Neural Information Processing Systems, pages 917\u2013925.", "citeRegEx": "Anandkumar et al\\.,? 2012", "shortCiteRegEx": "Anandkumar et al\\.", "year": 2012}, {"title": "A practical algorithm for topic modeling with provable guarantees", "author": ["S. Arora", "R. Ge", "Y. Halpern", "D.M. Mimno", "A. Moitra", "D. Sontag", "Y. Wu", "M. Zhu"], "venue": "Proceedings of the 29th International Conference on Machine Learning (ICML), pages 280\u2013288.", "citeRegEx": "Arora et al\\.,? 2013", "shortCiteRegEx": "Arora et al\\.", "year": 2013}, {"title": "Computing a nonnegative matrix factorization\u2013 provably", "author": ["S. Arora", "R. Ge", "R. Kannan", "A. Moitra"], "venue": "Proceedings of the 44th Annual ACM Symposium on Theory of Computing (STOC), pages 145\u2013162. ACM.", "citeRegEx": "Arora et al\\.,? 2012a", "shortCiteRegEx": "Arora et al\\.", "year": 2012}, {"title": "Learning topic models \u2013 going beyond svd", "author": ["S. Arora", "R. Ge", "A. Moitra"], "venue": "Proceedings of the 53rd Symposium on Foundations of Computer Science (FOCS), pages 1\u201310.", "citeRegEx": "Arora et al\\.,? 2012b", "shortCiteRegEx": "Arora et al\\.", "year": 2012}, {"title": "Co-training and expansion: Towards bridging theory and practice", "author": ["Balcan", "M.-F.", "A. Blum", "K. Yang"], "venue": "Advances in Neural Information Processing Systems, pages 89\u201396.", "citeRegEx": "Balcan et al\\.,? 2004", "shortCiteRegEx": "Balcan et al\\.", "year": 2004}, {"title": "A provable svd-based algorithm for learning topics in dominant admixture corpus", "author": ["T. Bansal", "C. Bhattacharyya", "R. Kannan"], "venue": "Advances in Neural Information Processing Systems, pages 1997\u20132005.", "citeRegEx": "Bansal et al\\.,? 2014", "shortCiteRegEx": "Bansal et al\\.", "year": 2014}, {"title": "Latent dirichlet allocation", "author": ["D.M. Blei", "A.Y. Ng", "M.I. Jordan"], "venue": "Journal of machine Learning research, 3(Jan):993\u20131022.", "citeRegEx": "Blei et al\\.,? 2003", "shortCiteRegEx": "Blei et al\\.", "year": 2003}, {"title": "Combining labeled and unlabeled data with co-training", "author": ["A. Blum", "T. Mitchell"], "venue": "Proceedings of the 11th Conference on Computational Learning Theory (COLT), pages 92\u2013100. ACM.", "citeRegEx": "Blum and Mitchell,? 1998", "shortCiteRegEx": "Blum and Mitchell", "year": 1998}, {"title": "Semi-Supervised Learning", "author": ["O. Chapelle", "B. Schlkopf", "A. Zien"], "venue": "The MIT Press, 1st edition.", "citeRegEx": "Chapelle et al\\.,? 2010", "shortCiteRegEx": "Chapelle et al\\.", "year": 2010}, {"title": "Pac generalization bounds for co-training", "author": ["S. Dasgupta", "M.L. Littman", "D. McAllester"], "venue": "Advances in Neural Information Processing Systems, 1:375\u2013382.", "citeRegEx": "Dasgupta et al\\.,? 2002", "shortCiteRegEx": "Dasgupta et al\\.", "year": 2002}, {"title": "The rotation of eigenvectors by a perturbation", "author": ["C. Davis", "W.M. Kahan"], "venue": "iii. SIAM Journal on Computing, 7(1):1\u201346.", "citeRegEx": "Davis and Kahan,? 1970", "shortCiteRegEx": "Davis and Kahan", "year": 1970}, {"title": "Algorithms and hardness for robust subspace recovery", "author": ["M. Hardt", "A. Moitra"], "venue": "Proceedings of the 26th Conference on Computational Learning Theory (COLT), pages 354\u2013375.", "citeRegEx": "Hardt and Moitra,? 2013", "shortCiteRegEx": "Hardt and Moitra", "year": 2013}, {"title": "Probabilistic latent semantic analysis", "author": ["T. Hofmann"], "venue": "Proceedings of the Fifteenth conference on Uncertainty in artificial intelligence, pages 289\u2013296. Morgan Kaufmann Publishers Inc.", "citeRegEx": "Hofmann,? 1999", "shortCiteRegEx": "Hofmann", "year": 1999}, {"title": "Disentangling gaussians", "author": ["A.T. Kalai", "A. Moitra", "G. Valiant"], "venue": "Communications of the ACM, 55(2):113\u2013120.", "citeRegEx": "Kalai et al\\.,? 2012", "shortCiteRegEx": "Kalai et al\\.", "year": 2012}, {"title": "Settling the polynomial learnability of mixtures of gaussians", "author": ["A. Moitra", "G. Valiant"], "venue": "Proceedings of the 53rd Symposium on Foundations of Computer Science (FOCS), pages 93\u2013102. IEEE.", "citeRegEx": "Moitra and Valiant,? 2010", "shortCiteRegEx": "Moitra and Valiant", "year": 2010}, {"title": "Latent semantic indexing: A probabilistic analysis", "author": ["C.H. Papadimitriou", "H. Tamaki", "P. Raghavan", "S. Vempala"], "venue": "Proceedings of the seventeenth ACM SIGACT-SIGMOD-SIGART symposium on Principles of database systems, pages 159\u2013168. ACM.", "citeRegEx": "Papadimitriou et al\\.,? 1998", "shortCiteRegEx": "Papadimitriou et al\\.", "year": 1998}, {"title": "Matrix perturbation theory (computer science and scientific computing)", "author": ["G. Stewart", "Sun", "J.-G"], "venue": null, "citeRegEx": "Stewart et al\\.,? \\Q1990\\E", "shortCiteRegEx": "Stewart et al\\.", "year": 1990}, {"title": "On the perturbation of pseudo-inverses, projections and linear least squares problems", "author": ["G.W. Stewart"], "venue": "SIAM Review, 19(4):634\u2013662.", "citeRegEx": "Stewart,? 1977", "shortCiteRegEx": "Stewart", "year": 1977}, {"title": "A survey of multi-view machine learning", "author": ["S. Sun"], "venue": "Neural computing and applications, 23:2031\u2013 2038.", "citeRegEx": "Sun,? 2013", "shortCiteRegEx": "Sun", "year": 2013}, {"title": "An introduction to matrix concentration inequalities", "author": ["J.A. Tropp"], "venue": "arXiv preprint arXiv:1501.01571.", "citeRegEx": "Tropp,? 2015", "shortCiteRegEx": "Tropp", "year": 2015}, {"title": "Introduction to the non-asymptotic analysis of random matrices", "author": ["R. Vershynin"], "venue": "arXiv preprint arXiv:1011.3027.", "citeRegEx": "Vershynin,? 2010", "shortCiteRegEx": "Vershynin", "year": 2010}, {"title": "Perturbation bounds in connection with singular value decomposition", "author": ["Wedin", "P.-\u00c5."], "venue": "BIT Numerical Mathematics, 12(1):99\u2013111.", "citeRegEx": "Wedin and P..\u00c5.,? 1972", "shortCiteRegEx": "Wedin and P..\u00c5.", "year": 1972}, {"title": "Rp\u00d7p be symmetric, with eigen values", "author": [], "venue": "B Technical Spectral Lemmas Proposition", "citeRegEx": "B and \u2208,? \\Q1970\\E", "shortCiteRegEx": "B and \u2208", "year": 1970}, {"title": "PV\u0302 , where PV and PV\u0302 are the projection matrices for V and V\u0302", "author": ["V\u0302 ) = PV"], "venue": "(Vershynin,", "citeRegEx": "\u2212,? \\Q2010\\E", "shortCiteRegEx": "\u2212", "year": 2010}], "referenceMentions": [{"referenceID": 6, "context": "Topic modeling is an area with significant recent work in the intersection of algorithms and machine learning (Arora et al., 2012a,b, 2013; Anandkumar et al., 2012, 2014; Bansal et al., 2014).", "startOffset": 110, "endOffset": 191}, {"referenceID": 0, "context": ", 2012a,b, 2013; Anandkumar et al., 2012, 2014; Bansal et al., 2014). In topic modeling, a topic (such as sports, business, or politics) is modeled as a probability distribution over words, expressed as a vector ai. A document is generated by first selecting a mixture w over topics, such as 80% sports and 20% business, and then choosing words i.i.d. from the associated mixture distribution, which in this case would be 0.8asports + 0.2abusiness. Given a large collection of such documents (and some assumptions about the distributions ai as well as the distribution over mixture vectors w) the goal is to recover the topic vectors ai and then to use the ai to correctly classify new documents according to their topic mixtures. Algorithms for this problem have been developed with strong provable guarantees even when documents consist of only two or three words each Arora et al. (2012b); Anandkumar et al.", "startOffset": 17, "endOffset": 892}, {"referenceID": 0, "context": ", 2012a,b, 2013; Anandkumar et al., 2012, 2014; Bansal et al., 2014). In topic modeling, a topic (such as sports, business, or politics) is modeled as a probability distribution over words, expressed as a vector ai. A document is generated by first selecting a mixture w over topics, such as 80% sports and 20% business, and then choosing words i.i.d. from the associated mixture distribution, which in this case would be 0.8asports + 0.2abusiness. Given a large collection of such documents (and some assumptions about the distributions ai as well as the distribution over mixture vectors w) the goal is to recover the topic vectors ai and then to use the ai to correctly classify new documents according to their topic mixtures. Algorithms for this problem have been developed with strong provable guarantees even when documents consist of only two or three words each Arora et al. (2012b); Anandkumar et al. (2012); Papadimitriou et al.", "startOffset": 17, "endOffset": 918}, {"referenceID": 0, "context": ", 2012a,b, 2013; Anandkumar et al., 2012, 2014; Bansal et al., 2014). In topic modeling, a topic (such as sports, business, or politics) is modeled as a probability distribution over words, expressed as a vector ai. A document is generated by first selecting a mixture w over topics, such as 80% sports and 20% business, and then choosing words i.i.d. from the associated mixture distribution, which in this case would be 0.8asports + 0.2abusiness. Given a large collection of such documents (and some assumptions about the distributions ai as well as the distribution over mixture vectors w) the goal is to recover the topic vectors ai and then to use the ai to correctly classify new documents according to their topic mixtures. Algorithms for this problem have been developed with strong provable guarantees even when documents consist of only two or three words each Arora et al. (2012b); Anandkumar et al. (2012); Papadimitriou et al. (1998). In addition, algorithms based on this problem formulation perform well empirically on standard datasets Blei et al.", "startOffset": 17, "endOffset": 947}, {"referenceID": 0, "context": ", 2012a,b, 2013; Anandkumar et al., 2012, 2014; Bansal et al., 2014). In topic modeling, a topic (such as sports, business, or politics) is modeled as a probability distribution over words, expressed as a vector ai. A document is generated by first selecting a mixture w over topics, such as 80% sports and 20% business, and then choosing words i.i.d. from the associated mixture distribution, which in this case would be 0.8asports + 0.2abusiness. Given a large collection of such documents (and some assumptions about the distributions ai as well as the distribution over mixture vectors w) the goal is to recover the topic vectors ai and then to use the ai to correctly classify new documents according to their topic mixtures. Algorithms for this problem have been developed with strong provable guarantees even when documents consist of only two or three words each Arora et al. (2012b); Anandkumar et al. (2012); Papadimitriou et al. (1998). In addition, algorithms based on this problem formulation perform well empirically on standard datasets Blei et al. (2003); Hofmann (1999).", "startOffset": 17, "endOffset": 1071}, {"referenceID": 0, "context": ", 2012a,b, 2013; Anandkumar et al., 2012, 2014; Bansal et al., 2014). In topic modeling, a topic (such as sports, business, or politics) is modeled as a probability distribution over words, expressed as a vector ai. A document is generated by first selecting a mixture w over topics, such as 80% sports and 20% business, and then choosing words i.i.d. from the associated mixture distribution, which in this case would be 0.8asports + 0.2abusiness. Given a large collection of such documents (and some assumptions about the distributions ai as well as the distribution over mixture vectors w) the goal is to recover the topic vectors ai and then to use the ai to correctly classify new documents according to their topic mixtures. Algorithms for this problem have been developed with strong provable guarantees even when documents consist of only two or three words each Arora et al. (2012b); Anandkumar et al. (2012); Papadimitriou et al. (1998). In addition, algorithms based on this problem formulation perform well empirically on standard datasets Blei et al. (2003); Hofmann (1999). As a theoretical model for document generation, however, an obvious problem with the standard topic modeling framework is that documents are not actually created by independently drawing words from some distribution.", "startOffset": 17, "endOffset": 1087}, {"referenceID": 0, "context": ", 2012a,b, 2013; Anandkumar et al., 2012, 2014; Bansal et al., 2014). In topic modeling, a topic (such as sports, business, or politics) is modeled as a probability distribution over words, expressed as a vector ai. A document is generated by first selecting a mixture w over topics, such as 80% sports and 20% business, and then choosing words i.i.d. from the associated mixture distribution, which in this case would be 0.8asports + 0.2abusiness. Given a large collection of such documents (and some assumptions about the distributions ai as well as the distribution over mixture vectors w) the goal is to recover the topic vectors ai and then to use the ai to correctly classify new documents according to their topic mixtures. Algorithms for this problem have been developed with strong provable guarantees even when documents consist of only two or three words each Arora et al. (2012b); Anandkumar et al. (2012); Papadimitriou et al. (1998). In addition, algorithms based on this problem formulation perform well empirically on standard datasets Blei et al. (2003); Hofmann (1999). As a theoretical model for document generation, however, an obvious problem with the standard topic modeling framework is that documents are not actually created by independently drawing words from some distribution. Better would be a model in which sentences are drawn i.i.d. from a distribution over sentences (this would at least produce grammatical objects and allow for meaningful correlation among related words within a topic, like shooting a free throw or kicking a field goal). Even better would be paragraphs drawn i.i.d. from a distribution over paragraphs (this would at least produce coherent paragraphs). Or, even better, how about a model in which paragraphs are drawn non-independently, so that the second paragraph in a document can depend on what the first paragraph was saying, though presumably with some amount of additional entropy as well? This is the type of model we study here. Note that an immediate problem with considering such a model is that now the task of learning an explicit distribution (over sentences or paragraphs) is hopeless. While a distribution over words can be reasonably viewed as a probability vector, one could not hope to learn or even represent an explicit distribution over sentences or paragraphs. Indeed, except in cases of plagiarism, one would not expect to see the same paragraph twice in the entire corpus. Moreover, this is likely to be true even if we assume paragraphs have some natural feature-vector representation. Instead, we bypass this issue by aiming to directly learn a predictor for documents\u2014that is, a function that given a document, predicts its mixture over topics\u2014without explicitly learning topic distributions. Another way to think of this is that our goal is not to learn a model that could be used to write a new document, but instead just a model that could be used to classify a document written by others. This is much as in standard supervised learning where algorithms such as SVMs learn a decision boundary (such as a linear separator) for making predictions on the labels of examples without explicitly learning the distributions D+ and D\u2212 over positive and negative examples respectively. However, our setting is unsupervised (we are not given labeled data containing the correct classifications of the documents in the training set) and furthermore, rather than each data item belonging to one of the k classes (topics), each data item belongs to a mixture of the k topics. Our goal is given a new data item to output what that mixture is. We begin by describing our high level theoretical formulation. This formulation can be viewed as a generalization both of standard topic modeling and of a setting known as multi-view learning or co-training Blum and Mitchell (1998); Dasgupta et al.", "startOffset": 17, "endOffset": 3847}, {"referenceID": 0, "context": ", 2012a,b, 2013; Anandkumar et al., 2012, 2014; Bansal et al., 2014). In topic modeling, a topic (such as sports, business, or politics) is modeled as a probability distribution over words, expressed as a vector ai. A document is generated by first selecting a mixture w over topics, such as 80% sports and 20% business, and then choosing words i.i.d. from the associated mixture distribution, which in this case would be 0.8asports + 0.2abusiness. Given a large collection of such documents (and some assumptions about the distributions ai as well as the distribution over mixture vectors w) the goal is to recover the topic vectors ai and then to use the ai to correctly classify new documents according to their topic mixtures. Algorithms for this problem have been developed with strong provable guarantees even when documents consist of only two or three words each Arora et al. (2012b); Anandkumar et al. (2012); Papadimitriou et al. (1998). In addition, algorithms based on this problem formulation perform well empirically on standard datasets Blei et al. (2003); Hofmann (1999). As a theoretical model for document generation, however, an obvious problem with the standard topic modeling framework is that documents are not actually created by independently drawing words from some distribution. Better would be a model in which sentences are drawn i.i.d. from a distribution over sentences (this would at least produce grammatical objects and allow for meaningful correlation among related words within a topic, like shooting a free throw or kicking a field goal). Even better would be paragraphs drawn i.i.d. from a distribution over paragraphs (this would at least produce coherent paragraphs). Or, even better, how about a model in which paragraphs are drawn non-independently, so that the second paragraph in a document can depend on what the first paragraph was saying, though presumably with some amount of additional entropy as well? This is the type of model we study here. Note that an immediate problem with considering such a model is that now the task of learning an explicit distribution (over sentences or paragraphs) is hopeless. While a distribution over words can be reasonably viewed as a probability vector, one could not hope to learn or even represent an explicit distribution over sentences or paragraphs. Indeed, except in cases of plagiarism, one would not expect to see the same paragraph twice in the entire corpus. Moreover, this is likely to be true even if we assume paragraphs have some natural feature-vector representation. Instead, we bypass this issue by aiming to directly learn a predictor for documents\u2014that is, a function that given a document, predicts its mixture over topics\u2014without explicitly learning topic distributions. Another way to think of this is that our goal is not to learn a model that could be used to write a new document, but instead just a model that could be used to classify a document written by others. This is much as in standard supervised learning where algorithms such as SVMs learn a decision boundary (such as a linear separator) for making predictions on the labels of examples without explicitly learning the distributions D+ and D\u2212 over positive and negative examples respectively. However, our setting is unsupervised (we are not given labeled data containing the correct classifications of the documents in the training set) and furthermore, rather than each data item belonging to one of the k classes (topics), each data item belongs to a mixture of the k topics. Our goal is given a new data item to output what that mixture is. We begin by describing our high level theoretical formulation. This formulation can be viewed as a generalization both of standard topic modeling and of a setting known as multi-view learning or co-training Blum and Mitchell (1998); Dasgupta et al. (2002); Chapelle et al.", "startOffset": 17, "endOffset": 3871}, {"referenceID": 0, "context": ", 2012a,b, 2013; Anandkumar et al., 2012, 2014; Bansal et al., 2014). In topic modeling, a topic (such as sports, business, or politics) is modeled as a probability distribution over words, expressed as a vector ai. A document is generated by first selecting a mixture w over topics, such as 80% sports and 20% business, and then choosing words i.i.d. from the associated mixture distribution, which in this case would be 0.8asports + 0.2abusiness. Given a large collection of such documents (and some assumptions about the distributions ai as well as the distribution over mixture vectors w) the goal is to recover the topic vectors ai and then to use the ai to correctly classify new documents according to their topic mixtures. Algorithms for this problem have been developed with strong provable guarantees even when documents consist of only two or three words each Arora et al. (2012b); Anandkumar et al. (2012); Papadimitriou et al. (1998). In addition, algorithms based on this problem formulation perform well empirically on standard datasets Blei et al. (2003); Hofmann (1999). As a theoretical model for document generation, however, an obvious problem with the standard topic modeling framework is that documents are not actually created by independently drawing words from some distribution. Better would be a model in which sentences are drawn i.i.d. from a distribution over sentences (this would at least produce grammatical objects and allow for meaningful correlation among related words within a topic, like shooting a free throw or kicking a field goal). Even better would be paragraphs drawn i.i.d. from a distribution over paragraphs (this would at least produce coherent paragraphs). Or, even better, how about a model in which paragraphs are drawn non-independently, so that the second paragraph in a document can depend on what the first paragraph was saying, though presumably with some amount of additional entropy as well? This is the type of model we study here. Note that an immediate problem with considering such a model is that now the task of learning an explicit distribution (over sentences or paragraphs) is hopeless. While a distribution over words can be reasonably viewed as a probability vector, one could not hope to learn or even represent an explicit distribution over sentences or paragraphs. Indeed, except in cases of plagiarism, one would not expect to see the same paragraph twice in the entire corpus. Moreover, this is likely to be true even if we assume paragraphs have some natural feature-vector representation. Instead, we bypass this issue by aiming to directly learn a predictor for documents\u2014that is, a function that given a document, predicts its mixture over topics\u2014without explicitly learning topic distributions. Another way to think of this is that our goal is not to learn a model that could be used to write a new document, but instead just a model that could be used to classify a document written by others. This is much as in standard supervised learning where algorithms such as SVMs learn a decision boundary (such as a linear separator) for making predictions on the labels of examples without explicitly learning the distributions D+ and D\u2212 over positive and negative examples respectively. However, our setting is unsupervised (we are not given labeled data containing the correct classifications of the documents in the training set) and furthermore, rather than each data item belonging to one of the k classes (topics), each data item belongs to a mixture of the k topics. Our goal is given a new data item to output what that mixture is. We begin by describing our high level theoretical formulation. This formulation can be viewed as a generalization both of standard topic modeling and of a setting known as multi-view learning or co-training Blum and Mitchell (1998); Dasgupta et al. (2002); Chapelle et al. (2010); Balcan et al.", "startOffset": 17, "endOffset": 3895}, {"referenceID": 0, "context": ", 2012a,b, 2013; Anandkumar et al., 2012, 2014; Bansal et al., 2014). In topic modeling, a topic (such as sports, business, or politics) is modeled as a probability distribution over words, expressed as a vector ai. A document is generated by first selecting a mixture w over topics, such as 80% sports and 20% business, and then choosing words i.i.d. from the associated mixture distribution, which in this case would be 0.8asports + 0.2abusiness. Given a large collection of such documents (and some assumptions about the distributions ai as well as the distribution over mixture vectors w) the goal is to recover the topic vectors ai and then to use the ai to correctly classify new documents according to their topic mixtures. Algorithms for this problem have been developed with strong provable guarantees even when documents consist of only two or three words each Arora et al. (2012b); Anandkumar et al. (2012); Papadimitriou et al. (1998). In addition, algorithms based on this problem formulation perform well empirically on standard datasets Blei et al. (2003); Hofmann (1999). As a theoretical model for document generation, however, an obvious problem with the standard topic modeling framework is that documents are not actually created by independently drawing words from some distribution. Better would be a model in which sentences are drawn i.i.d. from a distribution over sentences (this would at least produce grammatical objects and allow for meaningful correlation among related words within a topic, like shooting a free throw or kicking a field goal). Even better would be paragraphs drawn i.i.d. from a distribution over paragraphs (this would at least produce coherent paragraphs). Or, even better, how about a model in which paragraphs are drawn non-independently, so that the second paragraph in a document can depend on what the first paragraph was saying, though presumably with some amount of additional entropy as well? This is the type of model we study here. Note that an immediate problem with considering such a model is that now the task of learning an explicit distribution (over sentences or paragraphs) is hopeless. While a distribution over words can be reasonably viewed as a probability vector, one could not hope to learn or even represent an explicit distribution over sentences or paragraphs. Indeed, except in cases of plagiarism, one would not expect to see the same paragraph twice in the entire corpus. Moreover, this is likely to be true even if we assume paragraphs have some natural feature-vector representation. Instead, we bypass this issue by aiming to directly learn a predictor for documents\u2014that is, a function that given a document, predicts its mixture over topics\u2014without explicitly learning topic distributions. Another way to think of this is that our goal is not to learn a model that could be used to write a new document, but instead just a model that could be used to classify a document written by others. This is much as in standard supervised learning where algorithms such as SVMs learn a decision boundary (such as a linear separator) for making predictions on the labels of examples without explicitly learning the distributions D+ and D\u2212 over positive and negative examples respectively. However, our setting is unsupervised (we are not given labeled data containing the correct classifications of the documents in the training set) and furthermore, rather than each data item belonging to one of the k classes (topics), each data item belongs to a mixture of the k topics. Our goal is given a new data item to output what that mixture is. We begin by describing our high level theoretical formulation. This formulation can be viewed as a generalization both of standard topic modeling and of a setting known as multi-view learning or co-training Blum and Mitchell (1998); Dasgupta et al. (2002); Chapelle et al. (2010); Balcan et al. (2004); Sun (2013).", "startOffset": 17, "endOffset": 3917}, {"referenceID": 0, "context": ", 2012a,b, 2013; Anandkumar et al., 2012, 2014; Bansal et al., 2014). In topic modeling, a topic (such as sports, business, or politics) is modeled as a probability distribution over words, expressed as a vector ai. A document is generated by first selecting a mixture w over topics, such as 80% sports and 20% business, and then choosing words i.i.d. from the associated mixture distribution, which in this case would be 0.8asports + 0.2abusiness. Given a large collection of such documents (and some assumptions about the distributions ai as well as the distribution over mixture vectors w) the goal is to recover the topic vectors ai and then to use the ai to correctly classify new documents according to their topic mixtures. Algorithms for this problem have been developed with strong provable guarantees even when documents consist of only two or three words each Arora et al. (2012b); Anandkumar et al. (2012); Papadimitriou et al. (1998). In addition, algorithms based on this problem formulation perform well empirically on standard datasets Blei et al. (2003); Hofmann (1999). As a theoretical model for document generation, however, an obvious problem with the standard topic modeling framework is that documents are not actually created by independently drawing words from some distribution. Better would be a model in which sentences are drawn i.i.d. from a distribution over sentences (this would at least produce grammatical objects and allow for meaningful correlation among related words within a topic, like shooting a free throw or kicking a field goal). Even better would be paragraphs drawn i.i.d. from a distribution over paragraphs (this would at least produce coherent paragraphs). Or, even better, how about a model in which paragraphs are drawn non-independently, so that the second paragraph in a document can depend on what the first paragraph was saying, though presumably with some amount of additional entropy as well? This is the type of model we study here. Note that an immediate problem with considering such a model is that now the task of learning an explicit distribution (over sentences or paragraphs) is hopeless. While a distribution over words can be reasonably viewed as a probability vector, one could not hope to learn or even represent an explicit distribution over sentences or paragraphs. Indeed, except in cases of plagiarism, one would not expect to see the same paragraph twice in the entire corpus. Moreover, this is likely to be true even if we assume paragraphs have some natural feature-vector representation. Instead, we bypass this issue by aiming to directly learn a predictor for documents\u2014that is, a function that given a document, predicts its mixture over topics\u2014without explicitly learning topic distributions. Another way to think of this is that our goal is not to learn a model that could be used to write a new document, but instead just a model that could be used to classify a document written by others. This is much as in standard supervised learning where algorithms such as SVMs learn a decision boundary (such as a linear separator) for making predictions on the labels of examples without explicitly learning the distributions D+ and D\u2212 over positive and negative examples respectively. However, our setting is unsupervised (we are not given labeled data containing the correct classifications of the documents in the training set) and furthermore, rather than each data item belonging to one of the k classes (topics), each data item belongs to a mixture of the k topics. Our goal is given a new data item to output what that mixture is. We begin by describing our high level theoretical formulation. This formulation can be viewed as a generalization both of standard topic modeling and of a setting known as multi-view learning or co-training Blum and Mitchell (1998); Dasgupta et al. (2002); Chapelle et al. (2010); Balcan et al. (2004); Sun (2013). We then describe several natural assumptions under which we can indeed efficiently solve the problem, learning accurate topic mixture predictors.", "startOffset": 17, "endOffset": 3929}, {"referenceID": 0, "context": "Existing work in topic modeling, such as Arora et al. (2012b); Anandkumar et al.", "startOffset": 41, "endOffset": 62}, {"referenceID": 0, "context": "(2012b); Anandkumar et al. (2014), provide elegant procedures for handling large noise that is caused by drawing only 2 or 3 words according to the distribution induced by x.", "startOffset": 9, "endOffset": 34}, {"referenceID": 2, "context": "Phase 2 is similar to that of Arora et al. (2012b). Algorithm 1 formalizes the details of this approach.", "startOffset": 30, "endOffset": 51}, {"referenceID": 6, "context": "data sets (Bansal et al., 2014).", "startOffset": 10, "endOffset": 31}, {"referenceID": 11, "context": "Now, using the Davis and Kahan (1970) or Wedin (1972) sin \u03b8 theorem (see Proposition B.", "startOffset": 15, "endOffset": 38}, {"referenceID": 11, "context": "Now, using the Davis and Kahan (1970) or Wedin (1972) sin \u03b8 theorem (see Proposition B.", "startOffset": 15, "endOffset": 54}, {"referenceID": 20, "context": "We use the Matrix Bernstein inequality (Tropp, 2015), described in Appendix B, to demonstrate the concentration of \u2016 1 mDE > + 1 mED \u20162.", "startOffset": 39, "endOffset": 52}, {"referenceID": 12, "context": "For the problem of recovering a kdimensional nullspace, Hardt and Moitra (2013) introduced an efficient algorithm that tolerates agnostic noise up to = O(k/n).", "startOffset": 56, "endOffset": 80}, {"referenceID": 10, "context": "Can we learn ai\u2019s within error of using polynomially many samples? Note that when D is only supported on the corners of \u2206, this problem reduces to learning mixture of Gaussians, for which there is a wealth of literature on estimating Gaussian means and mixture weights (Dasgupta et al., 2002; Kalai et al., 2012; Moitra and Valiant, 2010).", "startOffset": 269, "endOffset": 338}, {"referenceID": 14, "context": "Can we learn ai\u2019s within error of using polynomially many samples? Note that when D is only supported on the corners of \u2206, this problem reduces to learning mixture of Gaussians, for which there is a wealth of literature on estimating Gaussian means and mixture weights (Dasgupta et al., 2002; Kalai et al., 2012; Moitra and Valiant, 2010).", "startOffset": 269, "endOffset": 338}, {"referenceID": 15, "context": "Can we learn ai\u2019s within error of using polynomially many samples? Note that when D is only supported on the corners of \u2206, this problem reduces to learning mixture of Gaussians, for which there is a wealth of literature on estimating Gaussian means and mixture weights (Dasgupta et al., 2002; Kalai et al., 2012; Moitra and Valiant, 2010).", "startOffset": 269, "endOffset": 338}], "year": 2016, "abstractText": "Recently there has been significant activity in developing algorithms with provable guarantees for topic modeling. In standard topic models, a topic (such as sports, business, or politics) is viewed as a probability distribution ai over words, and a document is generated by first selecting a mixture w over topics, and then generating words i.i.d. from the associated mixture Aw. Given a large collection of such documents, the goal is to recover the topic vectors and then to correctly classify new documents according to their topic mixture. In this work we consider a broad generalization of this framework in which words are no longer assumed to be drawn i.i.d. and instead a topic is a complex distribution over sequences of paragraphs. Since one could not hope to even represent such a distribution in general (even if paragraphs are given using some natural feature representation), we aim instead to directly learn a document classifier. That is, we aim to learn a predictor that given a new document, accurately predicts its topic mixture, without learning the distributions explicitly. We present several natural conditions under which one can do this efficiently and discuss issues such as noise tolerance and sample complexity in this model. More generally, our model can be viewed as a generalization of the multi-view or co-training setting in machine learning. \u2217Supported in part by National Science Foundation grants CCF-1525971 and CCF-1535967. \u2020Supported in part by National Science Foundation grant CCF-1525971 and by a Microsoft Research Graduate Fellowship and an IBM Ph.D Fellowship. ar X iv :1 61 1. 01 25 9v 1 [ cs .L G ] 4 N ov 2 01 6", "creator": "LaTeX with hyperref package"}}}