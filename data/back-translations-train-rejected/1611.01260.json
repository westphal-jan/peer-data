{"id": "1611.01260", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Nov-2016", "title": "Learning Identity Mappings with Residual Gates", "abstract": "We propose a technique to augment network layers by adding a linear gating mechanism, which provides a way to learn identity mappings by optimizing only one parameter. We also introduce a new metric which served as basis for the technique. It captures the difficulty involved in learning identity mappings for different types of network models, and provides a new theoretical intuition for the increased depths of models such as Highway and Residual Networks. We propose a new model, the Gated Residual Network, which is the result when augmenting Residual Networks. Experimental results show that augmenting layers grants increased performance, less issues with depth, and more layer independence -- fully removing them does not cripple the model. We evaluate our method on MNIST using fully-connected networks and on CIFAR-10 using Wide ResNets, achieving a relative error reduction of more than 8% in the latter when compared to the original model.", "histories": [["v1", "Fri, 4 Nov 2016 04:34:38 GMT  (556kb,D)", "http://arxiv.org/abs/1611.01260v1", null], ["v2", "Thu, 29 Dec 2016 01:36:47 GMT  (832kb,D)", "http://arxiv.org/abs/1611.01260v2", null]], "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["pedro h p savarese", "leonardo o mazza", "daniel r figueiredo"], "accepted": false, "id": "1611.01260"}, "pdf": {"name": "1611.01260.pdf", "metadata": {"source": "CRF", "title": "LEARNING IDENTITY MAPPINGS WITH RESIDUAL GATES", "authors": [], "emails": ["savarese@land.ufrj.br"], "sections": [{"heading": "1 INTRODUCTION", "text": "In fact, most of them will be able to move to another world, in which they are able, in which they are able to move, and in which they are able to change the world."}, {"heading": "2 SQUARED DISTANCE TO IDENTITY (SDI)", "text": "Consider a classical, fully connected ReLU network with layers defined as u = ReLU (< x, W >), the last step holding, since x is an output from a previous ReLU layer, and ReLU (x) initializes as identity matrix I. The layer is u = ReLU (x) as ReLU (x) illustrates this idea.Therefore, we can always add as many layers as we want to to a network without compromising its performance, which is consistent with the hypothesis that depth is largely a problem with the non-convex nature of optimization."}, {"heading": "2.1 CLASSICAL NEURAL NETWORKS", "text": "Hidden layers in classic fully connected ReLU networks have figures u = ReLU (< x, W >) and degenerate when W = I: u = ReLU (< x, I >) = ReLU (x) = x. for the preload, b = ~ 0 is also required. The SDI for the weight parameters is represented by W = I in equation 1. For the preload, we have b = = ~ 0 in equation 2: M = E [(W \u2212 I) 2] B = E [(b \u2212 ~ 0) 2] For its initialization (He et al. (2015a)), we have \u00b5 = \u00b5b = 0 and \u03c32 = \u03c32b = 2 n in equations 15, 16 and 17 from the appendix: Mij = {2 n + 1 if i = j 2 = j (4) Bi = 2n (5)."}, {"heading": "2.2 HIGHWAY NETWORKS", "text": "ReLU fully connected highway networks have hidden layers defined as follows: u = ReLU (< x, W >).g (< x, Wt >) Degeneration in identity mappings occurs when W = I, b = ~ 0 or g (< x, Wt >) = 0. In the first case: u = x.g (< x, Wt >) + x. (1 \u2212 g (< x, Wt >)) = xAs in the second case: g (< x, Wt >) approaches 0 as < x, Wt > goes into negative infinity because g is the sigmoid function. We focus on this case. Degeneration should occur independently of x, hence Wt = 0: M = SDI (Wt) = 2 = E [(Wt \u2212 appappappa).n We set the requirement = b = b = b = c = a constant result for Soller = 4 (Soller = 4)."}, {"heading": "2.3 RESIDUAL NETWORKS", "text": "A remaining, fully connected ReLU layer is defined as: u = ReLU (< x, W >) + xIt degenerates in case W = 0, b = ~ 0: M = E [(W \u2212 0) 2] B = E [(b \u2212 ~ 0) 2] Both measurements have already been calculated (equations 5 and 9): Mij = 2n (12) Bi = 2n (13) And the TSDI is: TSDI = n \u2211 i = 1 n \u2211 j = 1 Mij + n \u2211 i = 1 Bi = n2 2n + n2n = 2n + 2 (14)"}, {"heading": "2.4 COMPARISON", "text": "For the latter, the parameters of each neuron - a row in the W matrix - must reach a different state. This property is also observed in the M matrix: elements in its diagonal are different from the rest. Each neuron receives errors propagated backwards from the upper layers, and therefore has no information about the behavior of other neurons in its layer. These two facts suggest that degeneration in classical layers requires coordination between neurons in the same layer, although they do not communicate directly. Conversely, all elements in M are equal for highway and residual networks, so no coordination is required to learn identity mapping. For the B vector, all elements are equal for the three types of networks. A comparison of the total angle distance to identity shows that residual networks have the smallest value among the three networks."}, {"heading": "3 AUGMENTATION", "text": "A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A A A A A A-A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A"}, {"heading": "4 EXPERIMENTS", "text": "All models were implemented on Keras (Chollet (2015)) and executed on a Geforce GTX 970M. Larger models or more complex datasets such as ImageNet (Russakovsky et al. (2015)) were not examined due to hardware limitations."}, {"heading": "4.1 MNIST", "text": "In fact, it is that we see ourselves in a position to assert ourselves, that we are in a position to put ourselves at the top, and that we are in a position to assert ourselves, that we are in a position, that we are in a position to move to another world, that we are in a position to put ourselves in another world, that we are in a position, that we are in, that we are in, that we are in, that we are in, but that we are in, that we are in, that we are in, that we are in, that we are in."}, {"heading": "4.2 CIFAR-10", "text": "The CIFAR-10 dataset (Krizhevsky (2009)) consists of 60,000 color images, each with 32 x 32 pixels; the dataset has a total of 10 classes, including images of cats, birds, and airplanes, for example. Remaining networks have state-of-the-art results on CIFAR-10. We test GResnets and Wide GResNets (Zagoruyko & Komodakis (2016) and compare them with their original, non-augmented models. We use pre-activation results, as described in He et al. (2016), along with the same learning scheme. SGD with Nesterov Momentum are used to train the network, where the momentum is set to 0.9. We set an initial learning rate of 0.1, which decreases by a factor of 10, Resources, Resources, Resources, Resources, Resources, Resources, Resources, Resources, Resources, Resources, Resources, Resources, Resources, Resources, Resources, Resources."}, {"heading": "5 CONCLUSION", "text": "Unlike highway or residual networks, layers generated by our technology require the optimization of just one parameter to degenerate into identity. We have analyzed the square parameter distance when learning identity mappings and shown that our technique reduces this measure to a small constant. By designing our method so that randomly initialized layers are always close to identity mappings, models are more robust in depth after augmentation. We have shown that augmentative ResNets can provide a model that can regulate the residues we called gated residual networks. This model has performed better in all of our experiments, and we believe it can be used to outperform the state of the art in benchmark tasks with negligible additional training times and parameters."}, {"heading": "6 ACKNOWLEDGEMENTS", "text": "We thank Leonardo Mazza, Daniel Ratton and Carlos Pedreira for many informative conversations."}, {"heading": "7 APPENDIX", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "7.1 SDI OF CLASSICAL NETWORKS", "text": "Have: M = E [(W \u2212 I) 2] B = E [(b \u2212 ~ 0) 2] For i = j: Mij = E [(Wij \u2212 1) 2] = E [W 2ij] \u2212 2E [Wij] + 1 = V ar [Wij] + E2 [Wij] \u2212 2E [Wij] + 1 = \u03c32 + \u00b52 \u2212 2\u00b5 + 1 = \u03c32 + (\u00b52 \u2212 1) 2 (15) And for i 6 = j: Mij = E [(Wij \u2212 0) 2] = E [W 2ij] = V ar [Wij] + E 2 [Wij] = \u03c32 + \u00b52 (16), where chill and \u00b5 are the mean and standard deviations when initializing W. For the bias: Bi = E [(bi \u2212 0) 2] = E [b2i] = V ar [bi] + E 2 [bi] = \u03c32b + \u00b5 2 b (17) Here are digits \u00b5b and the mean deviation."}, {"heading": "7.2 SDI OF HIGHWAY NETWORKS", "text": "SDI (Wt) for highway layers have values corresponding to i 6 = j elements in SDI (W) for classical networks (equations 7 and 16): Mij = \u03c3 2 + \u00b52 (18) For bias we use Equation 8: Bi = E [(bt i \u2212 c) 2] = E [bt 2 i] \u2212 2cE [bt i] + E [c2] = V ar [bt i] + E 2 [bt i] \u2212 2cE [bt i] + c2 = \u03c32b + \u00b5 2 b \u2212 2c\u00b5b + c2 = \u03c32b + (\u00b5b \u2212 c) 2 (19)"}], "references": [{"title": "Learning long-term dependencies with gradient descent is difficult", "author": ["Y. Bengio", "P. Simard", "P Frasconi"], "venue": "IEEE Transactions on Neural Networks,", "citeRegEx": "Bengio et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 1994}, {"title": "Greedy layer-wise training of deep networks", "author": ["Y. Bengio", "P. Lamblin", "D Popovici", "H Larochelle"], "venue": null, "citeRegEx": "Bengio et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2007}, {"title": "On the complexity of neural network classifiers: A comparison between shallow and deep architectures", "author": ["Monica Bianchini", "Franco Scarselli"], "venue": "IEEE Transactions on Neural Networks and Learning Systems,", "citeRegEx": "Bianchini and Scarselli.,? \\Q2014\\E", "shortCiteRegEx": "Bianchini and Scarselli.", "year": 2014}, {"title": "The Power of Depth for Feedforward Neural Networks", "author": ["R. Eldan", "O. Shamir"], "venue": "ArXiv e-prints,", "citeRegEx": "Eldan and Shamir.,? \\Q2015\\E", "shortCiteRegEx": "Eldan and Shamir.", "year": 2015}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["X. Glorot", "Y. Bengio"], "venue": "AISTATS,,", "citeRegEx": "Glorot and Bengio.,? \\Q2010\\E", "shortCiteRegEx": "Glorot and Bengio.", "year": 2010}, {"title": "Deep sparse rectifier neural networks", "author": ["Xavier Glorot", "Antoine Bordes", "Yoshua Bengio"], "venue": "Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics (AISTATS-11),", "citeRegEx": "Glorot et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Glorot et al\\.", "year": 2011}, {"title": "Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": null, "citeRegEx": "He et al\\.,? \\Q2015\\E", "shortCiteRegEx": "He et al\\.", "year": 2015}, {"title": "Deep Residual Learning for Image Recognition. ArXiv e-prints, December 2015b", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": null, "citeRegEx": "He et al\\.,? \\Q2015\\E", "shortCiteRegEx": "He et al\\.", "year": 2015}, {"title": "Identity Mappings in Deep Residual Networks", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "ArXiv e-prints,", "citeRegEx": "He et al\\.,? \\Q2016\\E", "shortCiteRegEx": "He et al\\.", "year": 2016}, {"title": "Deep Networks with Stochastic Depth", "author": ["G. Huang", "Y. Sun", "Z. Liu", "D. Sedra", "K. Weinberger"], "venue": "ArXiv e-prints,", "citeRegEx": "Huang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2016}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["S. Ioffe", "C. Szegedy"], "venue": null, "citeRegEx": "Ioffe and Szegedy.,? \\Q2015\\E", "shortCiteRegEx": "Ioffe and Szegedy.", "year": 2015}, {"title": "Adam: A Method for Stochastic Optimization", "author": ["D. Kingma", "J. Ba"], "venue": "ArXiv e-prints,", "citeRegEx": "Kingma and Ba.,? \\Q2014\\E", "shortCiteRegEx": "Kingma and Ba.", "year": 2014}, {"title": "Learning multiple layers of features from tiny images", "author": ["Alex Krizhevsky"], "venue": "Technical report,", "citeRegEx": "Krizhevsky.,? \\Q2009\\E", "shortCiteRegEx": "Krizhevsky.", "year": 2009}, {"title": "Exploring strategies for training deep neural networks", "author": ["Hugo Larochelle", "Yoshua Bengio", "J\u00e9r\u00f4me Louradour", "Pascal Lamblin"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "Larochelle et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Larochelle et al\\.", "year": 2009}, {"title": "FractalNet: Ultra-Deep Neural Networks without Residuals", "author": ["G. Larsson", "M. Maire", "G. Shakhnarovich"], "venue": "ArXiv e-prints,", "citeRegEx": "Larsson et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Larsson et al\\.", "year": 2016}, {"title": "Gradient-based learning applied to document recognition", "author": ["Yann Lecun", "Lon Bottou", "Yoshua Bengio", "Patrick Haffner"], "venue": "In Proceedings of the IEEE,", "citeRegEx": "Lecun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Lecun et al\\.", "year": 1998}, {"title": "On the Number of Linear Regions of Deep Neural Networks", "author": ["G. Mont\u00fafar", "R. Pascanu", "K. Cho", "Y. Bengio"], "venue": null, "citeRegEx": "Mont\u00fafar et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Mont\u00fafar et al\\.", "year": 2014}, {"title": "Rectified linear units improve restricted boltzmann machines", "author": ["Vinod Nair", "Geoffrey E. Hinton"], "venue": "In Johannes Frnkranz and Thorsten Joachims (eds.), Proceedings of the 27th International Conference on Machine Learning", "citeRegEx": "Nair and Hinton.,? \\Q2010\\E", "shortCiteRegEx": "Nair and Hinton.", "year": 2010}, {"title": "Benefits of depth in neural networks", "author": ["M. Telgarsky"], "venue": "ArXiv e-prints,", "citeRegEx": "Telgarsky.,? \\Q2016\\E", "shortCiteRegEx": "Telgarsky.", "year": 2016}, {"title": "Revise Saturated Activation Functions", "author": ["B. Xu", "R. Huang", "M. Li"], "venue": "ArXiv e-prints,", "citeRegEx": "Xu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 8, "context": "1 INTRODUCTION As deep networks started to achieve state-of-the-art results on many computer vision tasks, increasing the depth of models without compromising its training has become a central problem to machine learning (Larochelle et al. (2009)).", "startOffset": 222, "endOffset": 247}, {"referenceID": 0, "context": "Many problems challenge the training of neural networks, including vanishing/exploding gradients (Bengio et al. (1994)), saturating activation functions (Xu et al.", "startOffset": 98, "endOffset": 119}, {"referenceID": 0, "context": "Many problems challenge the training of neural networks, including vanishing/exploding gradients (Bengio et al. (1994)), saturating activation functions (Xu et al. (2016)) and poor weight initialization (Glorot & Bengio (2010)).", "startOffset": 98, "endOffset": 171}, {"referenceID": 0, "context": "Many problems challenge the training of neural networks, including vanishing/exploding gradients (Bengio et al. (1994)), saturating activation functions (Xu et al. (2016)) and poor weight initialization (Glorot & Bengio (2010)).", "startOffset": 98, "endOffset": 227}, {"referenceID": 0, "context": "Many problems challenge the training of neural networks, including vanishing/exploding gradients (Bengio et al. (1994)), saturating activation functions (Xu et al. (2016)) and poor weight initialization (Glorot & Bengio (2010)). Techniques such as unsupervised pre-training (Bengio et al. (2007)), nonsaturating activation functions (Nair & Hinton (2010)) and normalization (Ioffe & Szegedy (2015)) target these issues and enable the training of deeper networks.", "startOffset": 98, "endOffset": 296}, {"referenceID": 0, "context": "Many problems challenge the training of neural networks, including vanishing/exploding gradients (Bengio et al. (1994)), saturating activation functions (Xu et al. (2016)) and poor weight initialization (Glorot & Bengio (2010)). Techniques such as unsupervised pre-training (Bengio et al. (2007)), nonsaturating activation functions (Nair & Hinton (2010)) and normalization (Ioffe & Szegedy (2015)) target these issues and enable the training of deeper networks.", "startOffset": 98, "endOffset": 355}, {"referenceID": 0, "context": "Many problems challenge the training of neural networks, including vanishing/exploding gradients (Bengio et al. (1994)), saturating activation functions (Xu et al. (2016)) and poor weight initialization (Glorot & Bengio (2010)). Techniques such as unsupervised pre-training (Bengio et al. (2007)), nonsaturating activation functions (Nair & Hinton (2010)) and normalization (Ioffe & Szegedy (2015)) target these issues and enable the training of deeper networks.", "startOffset": 98, "endOffset": 398}, {"referenceID": 0, "context": "Many problems challenge the training of neural networks, including vanishing/exploding gradients (Bengio et al. (1994)), saturating activation functions (Xu et al. (2016)) and poor weight initialization (Glorot & Bengio (2010)). Techniques such as unsupervised pre-training (Bengio et al. (2007)), nonsaturating activation functions (Nair & Hinton (2010)) and normalization (Ioffe & Szegedy (2015)) target these issues and enable the training of deeper networks. However, stacking more than a dozen layers still remained an open problem. Recently, models such as Residual Networks (He et al. (2015b)) and Highway Neural Networks (Srivastava et al.", "startOffset": 98, "endOffset": 600}, {"referenceID": 0, "context": "Many problems challenge the training of neural networks, including vanishing/exploding gradients (Bengio et al. (1994)), saturating activation functions (Xu et al. (2016)) and poor weight initialization (Glorot & Bengio (2010)). Techniques such as unsupervised pre-training (Bengio et al. (2007)), nonsaturating activation functions (Nair & Hinton (2010)) and normalization (Ioffe & Szegedy (2015)) target these issues and enable the training of deeper networks. However, stacking more than a dozen layers still remained an open problem. Recently, models such as Residual Networks (He et al. (2015b)) and Highway Neural Networks (Srivastava et al. (2015)) permitted the design of networks with hundreds of layers, also providing significant improvements on computer vision tasks.", "startOffset": 98, "endOffset": 655}, {"referenceID": 0, "context": "Many problems challenge the training of neural networks, including vanishing/exploding gradients (Bengio et al. (1994)), saturating activation functions (Xu et al. (2016)) and poor weight initialization (Glorot & Bengio (2010)). Techniques such as unsupervised pre-training (Bengio et al. (2007)), nonsaturating activation functions (Nair & Hinton (2010)) and normalization (Ioffe & Szegedy (2015)) target these issues and enable the training of deeper networks. However, stacking more than a dozen layers still remained an open problem. Recently, models such as Residual Networks (He et al. (2015b)) and Highway Neural Networks (Srivastava et al. (2015)) permitted the design of networks with hundreds of layers, also providing significant improvements on computer vision tasks. Although different in design, Highway and Residual networks share the idea of free information flow through layers: shortcut connections make optimization easier due to a shorter path between the lower layers and the network\u2019s error function. On the theoretical side, it is suggested that depth contributes exponentially more to the representational capacity of networks than width (Eldan & Shamir (2015) Telgarsky (2016) Bianchini & Scarselli (2014) Mont\u00fafar et al.", "startOffset": 98, "endOffset": 1185}, {"referenceID": 0, "context": "Many problems challenge the training of neural networks, including vanishing/exploding gradients (Bengio et al. (1994)), saturating activation functions (Xu et al. (2016)) and poor weight initialization (Glorot & Bengio (2010)). Techniques such as unsupervised pre-training (Bengio et al. (2007)), nonsaturating activation functions (Nair & Hinton (2010)) and normalization (Ioffe & Szegedy (2015)) target these issues and enable the training of deeper networks. However, stacking more than a dozen layers still remained an open problem. Recently, models such as Residual Networks (He et al. (2015b)) and Highway Neural Networks (Srivastava et al. (2015)) permitted the design of networks with hundreds of layers, also providing significant improvements on computer vision tasks. Although different in design, Highway and Residual networks share the idea of free information flow through layers: shortcut connections make optimization easier due to a shorter path between the lower layers and the network\u2019s error function. On the theoretical side, it is suggested that depth contributes exponentially more to the representational capacity of networks than width (Eldan & Shamir (2015) Telgarsky (2016) Bianchini & Scarselli (2014) Mont\u00fafar et al.", "startOffset": 98, "endOffset": 1202}, {"referenceID": 0, "context": "Many problems challenge the training of neural networks, including vanishing/exploding gradients (Bengio et al. (1994)), saturating activation functions (Xu et al. (2016)) and poor weight initialization (Glorot & Bengio (2010)). Techniques such as unsupervised pre-training (Bengio et al. (2007)), nonsaturating activation functions (Nair & Hinton (2010)) and normalization (Ioffe & Szegedy (2015)) target these issues and enable the training of deeper networks. However, stacking more than a dozen layers still remained an open problem. Recently, models such as Residual Networks (He et al. (2015b)) and Highway Neural Networks (Srivastava et al. (2015)) permitted the design of networks with hundreds of layers, also providing significant improvements on computer vision tasks. Although different in design, Highway and Residual networks share the idea of free information flow through layers: shortcut connections make optimization easier due to a shorter path between the lower layers and the network\u2019s error function. On the theoretical side, it is suggested that depth contributes exponentially more to the representational capacity of networks than width (Eldan & Shamir (2015) Telgarsky (2016) Bianchini & Scarselli (2014) Mont\u00fafar et al.", "startOffset": 98, "endOffset": 1231}, {"referenceID": 0, "context": "Many problems challenge the training of neural networks, including vanishing/exploding gradients (Bengio et al. (1994)), saturating activation functions (Xu et al. (2016)) and poor weight initialization (Glorot & Bengio (2010)). Techniques such as unsupervised pre-training (Bengio et al. (2007)), nonsaturating activation functions (Nair & Hinton (2010)) and normalization (Ioffe & Szegedy (2015)) target these issues and enable the training of deeper networks. However, stacking more than a dozen layers still remained an open problem. Recently, models such as Residual Networks (He et al. (2015b)) and Highway Neural Networks (Srivastava et al. (2015)) permitted the design of networks with hundreds of layers, also providing significant improvements on computer vision tasks. Although different in design, Highway and Residual networks share the idea of free information flow through layers: shortcut connections make optimization easier due to a shorter path between the lower layers and the network\u2019s error function. On the theoretical side, it is suggested that depth contributes exponentially more to the representational capacity of networks than width (Eldan & Shamir (2015) Telgarsky (2016) Bianchini & Scarselli (2014) Mont\u00fafar et al. (2014)).", "startOffset": 98, "endOffset": 1254}, {"referenceID": 0, "context": "Many problems challenge the training of neural networks, including vanishing/exploding gradients (Bengio et al. (1994)), saturating activation functions (Xu et al. (2016)) and poor weight initialization (Glorot & Bengio (2010)). Techniques such as unsupervised pre-training (Bengio et al. (2007)), nonsaturating activation functions (Nair & Hinton (2010)) and normalization (Ioffe & Szegedy (2015)) target these issues and enable the training of deeper networks. However, stacking more than a dozen layers still remained an open problem. Recently, models such as Residual Networks (He et al. (2015b)) and Highway Neural Networks (Srivastava et al. (2015)) permitted the design of networks with hundreds of layers, also providing significant improvements on computer vision tasks. Although different in design, Highway and Residual networks share the idea of free information flow through layers: shortcut connections make optimization easier due to a shorter path between the lower layers and the network\u2019s error function. On the theoretical side, it is suggested that depth contributes exponentially more to the representational capacity of networks than width (Eldan & Shamir (2015) Telgarsky (2016) Bianchini & Scarselli (2014) Mont\u00fafar et al. (2014)). This agrees with the increasing depth of winning architectures on challenges such as ImageNet (He et al. (2015b) Szegedy et al.", "startOffset": 98, "endOffset": 1369}, {"referenceID": 0, "context": "Many problems challenge the training of neural networks, including vanishing/exploding gradients (Bengio et al. (1994)), saturating activation functions (Xu et al. (2016)) and poor weight initialization (Glorot & Bengio (2010)). Techniques such as unsupervised pre-training (Bengio et al. (2007)), nonsaturating activation functions (Nair & Hinton (2010)) and normalization (Ioffe & Szegedy (2015)) target these issues and enable the training of deeper networks. However, stacking more than a dozen layers still remained an open problem. Recently, models such as Residual Networks (He et al. (2015b)) and Highway Neural Networks (Srivastava et al. (2015)) permitted the design of networks with hundreds of layers, also providing significant improvements on computer vision tasks. Although different in design, Highway and Residual networks share the idea of free information flow through layers: shortcut connections make optimization easier due to a shorter path between the lower layers and the network\u2019s error function. On the theoretical side, it is suggested that depth contributes exponentially more to the representational capacity of networks than width (Eldan & Shamir (2015) Telgarsky (2016) Bianchini & Scarselli (2014) Mont\u00fafar et al. (2014)). This agrees with the increasing depth of winning architectures on challenges such as ImageNet (He et al. (2015b) Szegedy et al. (2014)).", "startOffset": 98, "endOffset": 1391}, {"referenceID": 0, "context": "Many problems challenge the training of neural networks, including vanishing/exploding gradients (Bengio et al. (1994)), saturating activation functions (Xu et al. (2016)) and poor weight initialization (Glorot & Bengio (2010)). Techniques such as unsupervised pre-training (Bengio et al. (2007)), nonsaturating activation functions (Nair & Hinton (2010)) and normalization (Ioffe & Szegedy (2015)) target these issues and enable the training of deeper networks. However, stacking more than a dozen layers still remained an open problem. Recently, models such as Residual Networks (He et al. (2015b)) and Highway Neural Networks (Srivastava et al. (2015)) permitted the design of networks with hundreds of layers, also providing significant improvements on computer vision tasks. Although different in design, Highway and Residual networks share the idea of free information flow through layers: shortcut connections make optimization easier due to a shorter path between the lower layers and the network\u2019s error function. On the theoretical side, it is suggested that depth contributes exponentially more to the representational capacity of networks than width (Eldan & Shamir (2015) Telgarsky (2016) Bianchini & Scarselli (2014) Mont\u00fafar et al. (2014)). This agrees with the increasing depth of winning architectures on challenges such as ImageNet (He et al. (2015b) Szegedy et al. (2014)). He et al. (2015b) showed that, by construction, one can increase a network\u2019s depth while preserving its performance \u2013 therefore retaining the best local minimum.", "startOffset": 98, "endOffset": 1411}, {"referenceID": 5, "context": "Lastly, we show how sparse encoding (Glorot et al. (2011)) can be applied to augmented layers: transposing the sparsity from neurons to layers provides a form to prune them entirely from the network.", "startOffset": 37, "endOffset": 58}, {"referenceID": 5, "context": "Lastly, we show how sparse encoding (Glorot et al. (2011)) can be applied to augmented layers: transposing the sparsity from neurons to layers provides a form to prune them entirely from the network. We show that, unlike residual layers, their augmented counterparts retain good performance after more than half of its layers have been completely removed. We present results on two experiments. First, we test augmented fully-connected residual networks on MNIST, showing performance increase and how layer pruning affects the model. Second, we augment Wide ResNets (Zagoruyko & Komodakis (2016)) and test them on CIFAR-10, observing indications that our method can be used to surpass state-of-the-art results.", "startOffset": 37, "endOffset": 596}, {"referenceID": 6, "context": "For He\u2019s initialization (He et al. (2015a)), we have \u03bc = \u03bcb = 0 and \u03c3 = \u03c3 b = 2 n in Equations 15, 16 and 17 from the appendix:", "startOffset": 25, "endOffset": 43}, {"referenceID": 6, "context": "u = g(k)f(x,W ) + (1\u2212 g(k))x = g(k)(fr(x,W ) + x) + (1\u2212 g(k))x = g(k)fr(x,W ) + x The augmentation maintains the shortcut connection unaltered, which according to He et al. (2016) is a desired property when designing residual blocks.", "startOffset": 163, "endOffset": 180}, {"referenceID": 12, "context": "1 MNIST The MNIST dataset (Lecun et al. (1998)) is composed of 60, 000 greyscale images with 28 \u00d7 28 pixels.", "startOffset": 27, "endOffset": 47}, {"referenceID": 6, "context": "Initial tests with pre-activations (He et al. (2016)) resulted in poor performance on the validation set, therefore we opted for the traditional Dot-BN-ReLU layer when designing Residual Networks.", "startOffset": 36, "endOffset": 53}, {"referenceID": 6, "context": "Initial tests with pre-activations (He et al. (2016)) resulted in poor performance on the validation set, therefore we opted for the traditional Dot-BN-ReLU layer when designing Residual Networks. Identity shortcut connections connected every 2 layers, as conventional. All networks were trained using Adam (Kingma & Ba (2014)) with Nesterov momentum (Dozat) for a total of 100 epochs using mini-batches of size 128.", "startOffset": 36, "endOffset": 327}, {"referenceID": 8, "context": "2 CIFAR-10 The CIFAR-10 dataset (Krizhevsky (2009)) consists of 60, 000 color images with 32 \u00d7 32 pixels each.", "startOffset": 33, "endOffset": 51}, {"referenceID": 8, "context": "2 CIFAR-10 The CIFAR-10 dataset (Krizhevsky (2009)) consists of 60, 000 color images with 32 \u00d7 32 pixels each. The dataset has a total of 10 classes, including pictures of cats, birds and airplanes, for example. Residual Networks have suppressed state-of-the-art results on CIFAR-10. We test GResnets and Wide GResNets (Zagoruyko & Komodakis (2016)) and compare them with their original, nonaugmented models.", "startOffset": 33, "endOffset": 349}, {"referenceID": 6, "context": "We use pre-activation ResNets as described in He et al. (2016), along with the same learning scheme.", "startOffset": 46, "endOffset": 63}, {"referenceID": 6, "context": "We use pre-activation ResNets as described in He et al. (2016), along with the same learning scheme. SGD with Nesterov Momentum are used to train the network, where the momentum is set to 0.9. We set an initial learning rate of 0.1, which decreases by a factor of 10 after 40% and 60% of the total training epochs. ResNets were trained for 200 epochs, while Wide ResNets for 100 due to slower training time. Images are padded with 4 pixels. Random horizontal flips with 50% probability and random crops of size 32 \u00d7 32 are used when training the network. We also calculate the pixel mean from the images in the training set, and subtract it from all images. Acc Original Augmented (Gated) Resnet 5 7.16% 6.67% WideResNet (3,4) 5.36% 4.90% Table 4: Test error rates on the CIFAR-10 dataset, for ResNets, Wide ResNets and their augmented counterparts. Table 4 shows the test error for two architectures: a ResNet with n = 5, and a Wide ResNet with k = 3, n = 4. These specific models were chosen according to the their training time \u2013 it was unfeasible to train deeper or wider models with the current hardware. Both augmented models performed better than their original counterparts. Approximately 9% relative error reduction was achieved with only 15 and 9 extra parameters, respectively for ResNet-5 and Wide ResNet-(3,4). Method Params Accuracy ResNet-110 (He et al. (2015b)) 1.", "startOffset": 46, "endOffset": 1377}, {"referenceID": 6, "context": "We use pre-activation ResNets as described in He et al. (2016), along with the same learning scheme. SGD with Nesterov Momentum are used to train the network, where the momentum is set to 0.9. We set an initial learning rate of 0.1, which decreases by a factor of 10 after 40% and 60% of the total training epochs. ResNets were trained for 200 epochs, while Wide ResNets for 100 due to slower training time. Images are padded with 4 pixels. Random horizontal flips with 50% probability and random crops of size 32 \u00d7 32 are used when training the network. We also calculate the pixel mean from the images in the training set, and subtract it from all images. Acc Original Augmented (Gated) Resnet 5 7.16% 6.67% WideResNet (3,4) 5.36% 4.90% Table 4: Test error rates on the CIFAR-10 dataset, for ResNets, Wide ResNets and their augmented counterparts. Table 4 shows the test error for two architectures: a ResNet with n = 5, and a Wide ResNet with k = 3, n = 4. These specific models were chosen according to the their training time \u2013 it was unfeasible to train deeper or wider models with the current hardware. Both augmented models performed better than their original counterparts. Approximately 9% relative error reduction was achieved with only 15 and 9 extra parameters, respectively for ResNet-5 and Wide ResNet-(3,4). Method Params Accuracy ResNet-110 (He et al. (2015b)) 1.7M 6.61% Stochastic Depth (Huang et al. (2016)) 10.", "startOffset": 46, "endOffset": 1427}, {"referenceID": 6, "context": "We use pre-activation ResNets as described in He et al. (2016), along with the same learning scheme. SGD with Nesterov Momentum are used to train the network, where the momentum is set to 0.9. We set an initial learning rate of 0.1, which decreases by a factor of 10 after 40% and 60% of the total training epochs. ResNets were trained for 200 epochs, while Wide ResNets for 100 due to slower training time. Images are padded with 4 pixels. Random horizontal flips with 50% probability and random crops of size 32 \u00d7 32 are used when training the network. We also calculate the pixel mean from the images in the training set, and subtract it from all images. Acc Original Augmented (Gated) Resnet 5 7.16% 6.67% WideResNet (3,4) 5.36% 4.90% Table 4: Test error rates on the CIFAR-10 dataset, for ResNets, Wide ResNets and their augmented counterparts. Table 4 shows the test error for two architectures: a ResNet with n = 5, and a Wide ResNet with k = 3, n = 4. These specific models were chosen according to the their training time \u2013 it was unfeasible to train deeper or wider models with the current hardware. Both augmented models performed better than their original counterparts. Approximately 9% relative error reduction was achieved with only 15 and 9 extra parameters, respectively for ResNet-5 and Wide ResNet-(3,4). Method Params Accuracy ResNet-110 (He et al. (2015b)) 1.7M 6.61% Stochastic Depth (Huang et al. (2016)) 10.2M 4.91% ResNet-1001 (He et al. (2016)) 10.", "startOffset": 46, "endOffset": 1470}, {"referenceID": 6, "context": "We use pre-activation ResNets as described in He et al. (2016), along with the same learning scheme. SGD with Nesterov Momentum are used to train the network, where the momentum is set to 0.9. We set an initial learning rate of 0.1, which decreases by a factor of 10 after 40% and 60% of the total training epochs. ResNets were trained for 200 epochs, while Wide ResNets for 100 due to slower training time. Images are padded with 4 pixels. Random horizontal flips with 50% probability and random crops of size 32 \u00d7 32 are used when training the network. We also calculate the pixel mean from the images in the training set, and subtract it from all images. Acc Original Augmented (Gated) Resnet 5 7.16% 6.67% WideResNet (3,4) 5.36% 4.90% Table 4: Test error rates on the CIFAR-10 dataset, for ResNets, Wide ResNets and their augmented counterparts. Table 4 shows the test error for two architectures: a ResNet with n = 5, and a Wide ResNet with k = 3, n = 4. These specific models were chosen according to the their training time \u2013 it was unfeasible to train deeper or wider models with the current hardware. Both augmented models performed better than their original counterparts. Approximately 9% relative error reduction was achieved with only 15 and 9 extra parameters, respectively for ResNet-5 and Wide ResNet-(3,4). Method Params Accuracy ResNet-110 (He et al. (2015b)) 1.7M 6.61% Stochastic Depth (Huang et al. (2016)) 10.2M 4.91% ResNet-1001 (He et al. (2016)) 10.2M 4.62% FractalNet (Larsson et al. (2016)) 38.", "startOffset": 46, "endOffset": 1517}, {"referenceID": 6, "context": "We use pre-activation ResNets as described in He et al. (2016), along with the same learning scheme. SGD with Nesterov Momentum are used to train the network, where the momentum is set to 0.9. We set an initial learning rate of 0.1, which decreases by a factor of 10 after 40% and 60% of the total training epochs. ResNets were trained for 200 epochs, while Wide ResNets for 100 due to slower training time. Images are padded with 4 pixels. Random horizontal flips with 50% probability and random crops of size 32 \u00d7 32 are used when training the network. We also calculate the pixel mean from the images in the training set, and subtract it from all images. Acc Original Augmented (Gated) Resnet 5 7.16% 6.67% WideResNet (3,4) 5.36% 4.90% Table 4: Test error rates on the CIFAR-10 dataset, for ResNets, Wide ResNets and their augmented counterparts. Table 4 shows the test error for two architectures: a ResNet with n = 5, and a Wide ResNet with k = 3, n = 4. These specific models were chosen according to the their training time \u2013 it was unfeasible to train deeper or wider models with the current hardware. Both augmented models performed better than their original counterparts. Approximately 9% relative error reduction was achieved with only 15 and 9 extra parameters, respectively for ResNet-5 and Wide ResNet-(3,4). Method Params Accuracy ResNet-110 (He et al. (2015b)) 1.7M 6.61% Stochastic Depth (Huang et al. (2016)) 10.2M 4.91% ResNet-1001 (He et al. (2016)) 10.2M 4.62% FractalNet (Larsson et al. (2016)) 38.6M 4.60% Wide ResNet (4,10) (Zagoruyko & Komodakis (2016)) 36.", "startOffset": 46, "endOffset": 1579}], "year": 2016, "abstractText": "We propose a technique to augment network layers by adding a linear gating mechanism, which provides a way to learn identity mappings by optimizing only one parameter. We also introduce a new metric which served as basis for the technique. It captures the difficulty involved in learning identity mappings for different types of network models, and provides a new theoretical intuition for the increased depths of models such as Highway and Residual Networks. We propose a new model, the Gated Residual Network, which is the result when augmenting Residual Networks. Experimental results show that augmenting layers grants increased performance, less issues with depth, and more layer independence \u2013 fully removing them does not cripple the model. We evaluate our method on MNIST using fully-connected networks and on CIFAR-10 using Wide ResNets, achieving a relative error reduction of more than 8% in the latter when compared to the original model.", "creator": "LaTeX with hyperref package"}}}