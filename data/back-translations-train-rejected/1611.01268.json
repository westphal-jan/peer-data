{"id": "1611.01268", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Nov-2016", "title": "Semantic Noise Modeling for Better Representation Learning", "abstract": "Latent representation learned from multi-layered neural networks via hierarchical feature abstraction enables recent success of deep learning. Under the deep learning framework, generalization performance highly depends on the learned latent representation which is obtained from an appropriate training scenario with a task-specific objective on a designed network model. In this work, we propose a novel latent space modeling method to learn better latent representation. We designed a neural network model based on the assumption that good base representation can be attained by maximizing the total correlation between the input, latent, and output variables. From the base model, we introduce a semantic noise modeling method which enables class-conditional perturbation on latent space to enhance the representational power of learned latent feature. During training, latent vector representation can be stochastically perturbed by a modeled class-conditional additive noise while maintaining its original semantic feature. It implicitly brings the effect of semantic augmentation on the latent space. The proposed model can be easily learned by back-propagation with common gradient-based optimization algorithms. Experimental results show that the proposed method helps to achieve performance benefits against various previous approaches. We also provide the empirical analyses for the proposed class-conditional perturbation process including t-SNE visualization.", "histories": [["v1", "Fri, 4 Nov 2016 05:52:17 GMT  (8737kb,D)", "http://arxiv.org/abs/1611.01268v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.NE", "authors": ["hyo-eun kim", "sangheum hwang", "kyunghyun cho"], "accepted": false, "id": "1611.01268"}, "pdf": {"name": "1611.01268.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Hyo-Eun Kim", "Sangheum Hwang"], "emails": ["shwang}@lunit.io", "kyunghyun.cho@nyu.edu"], "sections": [{"heading": "1 INTRODUCTION", "text": "This year, it has reached the point where it will be able to retaliate."}, {"heading": "2 METHODOLOGY", "text": "In a traditional forward-looking neural network model (Figure 1 (a)), the output Y of the input data (X = Y) is compared with its actual label, and the error is propagated backwards from top to bottom, implicitly resulting in a task-specific latent representation Z of the input data (Watanabe, 1960). We assume that a good latent representation Z is achieved by maximizing the dependence between a set of random variables X, Y, and Z known as total correlation or multiinformation. Note that the total correlation to the sum of all paired mutual information is the same. Total correlation C (X, Y) for given random variables X, Y, and Z under condition P (Y | Z) P (X) P (X) (X) P (X) from the relationship between the random variables (in Figure 1 (a) can be reduced to: C (X, Y)."}, {"heading": "3 RELATED WORKS", "text": "Previous work on deep neural networks for supervised learning can normally be divided into two types, as shown in Figure 2; (a) a general forward-looking neural network model (LeCun et al., 1998; Krizhevsky et al., 2012; Simonyan & Zisserman, 2015; He et al., 2016) and (b) a common learning model that simultaneously optimizes unsupervised and monitored targets (Zhao et al., 2015; Zhang et al., 2016; Cho & Chen, 2014). Here are the corresponding objective functions: Min.: Min.: Visualization of the learning model: Min.: 2} LNLL (y, t) (10) Min. Prop."}, {"heading": "4 EXPERIMENTS", "text": "For quantitative analysis, we compare the proposed methodology with previous approaches described in Section 3: a traditional, forward-led, supervised learning model and a common learning model with two different types of reconstruction losses (reconstruction only with the first layer or with all intermediate layers including the first layer).The proposed methodology includes a basic model in Figure 1 (b) and a stochastic disturbance model in Figure 1 (c).In particular, in the stochastic disturbance model, we compare the random and class-related disturbances and present some qualitative analyses on the significance of the proposed disturbance methodology."}, {"heading": "4.1 DATASETS", "text": "We are experimenting with two public data sets: MNIST and CIFAR-10. MNIST (10 classes) consists of 50k, 10k and 10k 28 x 28 grayscale images for training, validation and test data sets respectively. CIFAR-10 (10 classes) consists of 50k and 10k 32 x 32 3-channel images for training and test sets respectively. We divide the 50k CIFAR-10 training images into 40k and 10k respectively for training and validation. Experiments are conducted with different sizes of the training set (from 10 examples per class to the entire training set) to test the effectiveness of the proposed model in terms of generalization performance under different sizes of the training set."}, {"heading": "4.2 IMPLEMENTATION", "text": "Figure 4 shows the architecture of the neural network model used in this experiment. W's are convolution or fully connected weights (biases are excluded for visual brevity).Three convolution (3 x 3 (2) 32, 3 x 3 (2) 64, 3 x 3 (2) 96, each element being the size of the filter core and (step) the number of filters and two fully connected (the number of output nodes 128 and 10 respectively) layers are used for activating MNIST. Four convolution (5 x 5 (1) 64, 3 x 3 (2) 64, 3 x 3 (2) 64, and 3 x 3 (2) 96) and three fully connected layers (128, 128 and 10 nodes) are used for activating MNIST. Four convolution (5 x 5 (1) 64, 3 x 3 (2), 3 x 3 (2) 64 x 3 (2) 64, and 3 x 3 (2) 96) are used for activating MNIST and three fully connected layers are used for the output layers."}, {"heading": "4.3 QUANTITATIVE ANALYSIS", "text": "Table 1 shows the classification performance of previous approaches and of the proposed methods. Three earlier approaches (a traditional feed-forward model, a common learning model with input reconstruction loss, and a common learning model with reconstruction losses of all intermediate layers including the input layer) are compared with three proposed methods (the base model in Figure 1 (b) and the stochastic model in Figure 1 (c) with two different disturbance methods; random and class-related). As expected, the maximization of the overall correlation (proposed basis) yields better latent representation, and the model with class-related disturbance (proposed disturbance (class-related disturbance) yields the best performance among all comparison targets. In MNIST in particular, the error rate of the \"proposed disturbance (class-related)\" with 2k per class-related training examples is lower than the error rate of all previous work with the total training rate (approximately 5k per class)."}, {"heading": "4.4 QUALITATIVE ANALYSIS", "text": "As already mentioned, random disturbances are not guaranteed by adding unstructured noise directly to the latent representation. We have compared two different disturbance methods (random and class-related) by reconstructing the examples described in Figure 1 (b). The first row is the type of disturbance, and the rest is the reconstruction of its disturbed lateral representations. Based on the architecture described in Figure 1, we have created five different lateral representations according to the type of disturbance, and the second row reconstructs the disturbed lateral vectors according to the definition."}, {"heading": "5 DISCUSSION", "text": "The presented model simultaneously optimizes both supervised and unsupervised losses based on the assumption that better latent representation can be achieved by maximizing the overall correlation of all random variables defined by the Standard Model. Specifically, the stochastic disturbance process achieved by modelling class-related additive noise during training increases the representational power of latent space. We can expect the proposed semantic modeling process to improve generalization performance in supervised learning with implicit semantic augmentation effect on latent space. Intuitively, the model architecture can be extended to semi-supervised learning as it is implemented as a common optimization of supervised and unsupervised goals. However, for semi-supervised learning, there is a logical link between features learned from labeled and undescribed data to consider future learning needs as additive."}], "references": [{"title": "TensorFlow: Large-scale machine learning on heterogeneous systems", "author": ["cent Vanhoucke", "Vijay Vasudevan", "Fernanda Vi\u00e9gas", "Oriol Vinyals", "Pete Warden", "Martin Wattenberg", "Martin Wicke", "Yuan Yu", "Xiaoqiang Zheng"], "venue": null, "citeRegEx": "Vanhoucke et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Vanhoucke et al\\.", "year": 2015}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio"], "venue": "In International Conference on Learning Representations (ICLR),", "citeRegEx": "Bahdanau et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Greedy layer-wise training of deep networks", "author": ["Yoshua Bengio", "Pascal Lamblin", "Dan Popovici", "Hugo Larochelle"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Bengio et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2007}, {"title": "Classifying and visualizing motion capture sequences using deep neural networks", "author": ["Kyunghyun Cho", "Xi Chen"], "venue": "In International Conference on Computer Vision Theory and Applications,", "citeRegEx": "Cho and Chen.,? \\Q2014\\E", "shortCiteRegEx": "Cho and Chen.", "year": 2014}, {"title": "A unified architecture for natural language processing: Deep neural networks with multitask learning", "author": ["Ronan Collobert", "Jason Weston"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "Collobert and Weston.,? \\Q2008\\E", "shortCiteRegEx": "Collobert and Weston.", "year": 2008}, {"title": "Multi-prediction deep boltzmann machines", "author": ["Ian Goodfellow", "Mehdi Mirza", "Aaron Courville", "Yoshua Bengio"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Goodfellow et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2013}, {"title": "Speech recognition with deep recurrent neural networks", "author": ["Alex Graves", "Abdel-rahman Mohamed", "Geoffrey Hinton"], "venue": "In International conference on acoustics, speech and signal processing,", "citeRegEx": "Graves et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2013}, {"title": "Deep residual learning for image recognition", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"], "venue": "In Computer Vision and Pattern Recognition", "citeRegEx": "He et al\\.,? \\Q2016\\E", "shortCiteRegEx": "He et al\\.", "year": 2016}, {"title": "Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups", "author": ["Geoffrey Hinton", "Li Deng", "Dong Yu", "George E Dahl", "Abdel-rahman Mohamed", "Navdeep Jaitly", "Andrew Senior", "Vincent Vanhoucke", "Patrick Nguyen", "Tara N Sainath"], "venue": "Signal Processing Magazine, IEEE,", "citeRegEx": "Hinton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2012}, {"title": "A fast learning algorithm for deep belief nets", "author": ["Geoffrey E. Hinton", "Simon Osindero", "Yee Whye Teh"], "venue": "Neural Computation,", "citeRegEx": "Hinton et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2006}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba"], "venue": "In International Conference on Learning Representations (ICLR),", "citeRegEx": "Kingma and Ba.,? \\Q2015\\E", "shortCiteRegEx": "Kingma and Ba.", "year": 2015}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E Hinton"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Classification using discriminative restricted boltzmann machines", "author": ["Hugo Larochelle", "Yoshua Bengio"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "Larochelle and Bengio.,? \\Q2008\\E", "shortCiteRegEx": "Larochelle and Bengio.", "year": 2008}, {"title": "Gradient-based learning applied to document recognition", "author": ["Yann LeCun", "L\u00e9on Bottou", "Yoshua Bengio", "Patrick Haffner"], "venue": "Proceedings of the IEEE,", "citeRegEx": "LeCun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "Visualizing data using t-sne", "author": ["Laurens van der Maaten", "Geoffrey Hinton"], "venue": "Journal of Machine Learning Research (JMLR),", "citeRegEx": "Maaten and Hinton.,? \\Q2008\\E", "shortCiteRegEx": "Maaten and Hinton.", "year": 2008}, {"title": "Stacked convolutional autoencoders for hierarchical feature extraction", "author": ["Jonathan Masci", "Ueli Meier", "Dan Cire\u015fan", "J\u00fcrgen Schmidhuber"], "venue": "In International Conference on Artificial Neural Networks,", "citeRegEx": "Masci et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Masci et al\\.", "year": 2011}, {"title": "Semisupervised learning with ladder networks", "author": ["Antti Rasmus", "Mathias Berglund", "Mikko Honkala", "Harri Valpola", "Tapani Raiko"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Rasmus et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Rasmus et al\\.", "year": 2015}, {"title": "Deep boltzmann machines", "author": ["Ruslan Salakhutdinov", "Geoffrey E Hinton"], "venue": "In Artificial Intelligence and Statistics Conference (AISTATS),", "citeRegEx": "Salakhutdinov and Hinton.,? \\Q2009\\E", "shortCiteRegEx": "Salakhutdinov and Hinton.", "year": 2009}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["Karen Simonyan", "Andrew Zisserman"], "venue": "In International Conference on Learning Representations (ICLR),", "citeRegEx": "Simonyan and Zisserman.,? \\Q2015\\E", "shortCiteRegEx": "Simonyan and Zisserman.", "year": 2015}, {"title": "Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion", "author": ["Pascal Vincent", "Hugo Larochelle", "Isabelle Lajoie", "Yoshua Bengio", "Pierre-Antoine Manzagol"], "venue": "Journal of Machine Learning Research (JMLR),", "citeRegEx": "Vincent et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Vincent et al\\.", "year": 2010}, {"title": "Information theoretical analysis of multivariate correlation", "author": ["Satosi Watanabe"], "venue": "IBM Journal of research and development,", "citeRegEx": "Watanabe.,? \\Q1960\\E", "shortCiteRegEx": "Watanabe.", "year": 1960}, {"title": "Augmenting supervised neural networks with unsupervised objectives for large-scale image classification", "author": ["Yuting Zhang", "Kibok Lee", "Honglak Lee"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "Zhang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2016}, {"title": "Stacked what-where auto-encoders", "author": ["Junbo Zhao", "Michael Mathieu", "Ross Goroshin", "Yann Lecun"], "venue": "In International Conference on Learning Representations (ICLR),", "citeRegEx": "Zhao et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zhao et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 11, "context": "Under that point of view, deep learning has been achieved many breakthroughs in several domains such as computer vision (Krizhevsky et al., 2012; Simonyan & Zisserman, 2015; He et al., 2016), natural language processing (Collobert & Weston, 2008; Bahdanau et al.", "startOffset": 120, "endOffset": 190}, {"referenceID": 7, "context": "Under that point of view, deep learning has been achieved many breakthroughs in several domains such as computer vision (Krizhevsky et al., 2012; Simonyan & Zisserman, 2015; He et al., 2016), natural language processing (Collobert & Weston, 2008; Bahdanau et al.", "startOffset": 120, "endOffset": 190}, {"referenceID": 1, "context": ", 2016), natural language processing (Collobert & Weston, 2008; Bahdanau et al., 2015), and speech recognition (Hinton et al.", "startOffset": 37, "endOffset": 86}, {"referenceID": 8, "context": ", 2015), and speech recognition (Hinton et al., 2012; Graves et al., 2013).", "startOffset": 32, "endOffset": 74}, {"referenceID": 6, "context": ", 2015), and speech recognition (Hinton et al., 2012; Graves et al., 2013).", "startOffset": 32, "endOffset": 74}, {"referenceID": 19, "context": "During the past decade, researchers focused on unsupervised representation learning and achieved several remarkable landmarks on deep learning history (Vincent et al., 2010; Hinton et al., 2006; Salakhutdinov & Hinton, 2009).", "startOffset": 151, "endOffset": 224}, {"referenceID": 9, "context": "During the past decade, researchers focused on unsupervised representation learning and achieved several remarkable landmarks on deep learning history (Vincent et al., 2010; Hinton et al., 2006; Salakhutdinov & Hinton, 2009).", "startOffset": 151, "endOffset": 224}, {"referenceID": 2, "context": "In terms of utilizing good base features for supervised learning, the base representation learned from unsupervised learning can be a good solution for supervised tasks (Bengio et al., 2007; Masci et al., 2011).", "startOffset": 169, "endOffset": 210}, {"referenceID": 15, "context": "In terms of utilizing good base features for supervised learning, the base representation learned from unsupervised learning can be a good solution for supervised tasks (Bengio et al., 2007; Masci et al., 2011).", "startOffset": 169, "endOffset": 210}, {"referenceID": 5, "context": "Instead of the two stage learning strategy (unsupervised pre-training followed by supervised fine-tuning), several works focused on a joint learning model which optimizes unsupervised and supervised objectives concurrently, resulting in better generalization performance (Goodfellow et al., 2013; Larochelle & Bengio, 2008; Rasmus et al., 2015; Zhao et al., 2015; Zhang et al., 2016; Cho & Chen, 2014).", "startOffset": 271, "endOffset": 401}, {"referenceID": 16, "context": "Instead of the two stage learning strategy (unsupervised pre-training followed by supervised fine-tuning), several works focused on a joint learning model which optimizes unsupervised and supervised objectives concurrently, resulting in better generalization performance (Goodfellow et al., 2013; Larochelle & Bengio, 2008; Rasmus et al., 2015; Zhao et al., 2015; Zhang et al., 2016; Cho & Chen, 2014).", "startOffset": 271, "endOffset": 401}, {"referenceID": 22, "context": "Instead of the two stage learning strategy (unsupervised pre-training followed by supervised fine-tuning), several works focused on a joint learning model which optimizes unsupervised and supervised objectives concurrently, resulting in better generalization performance (Goodfellow et al., 2013; Larochelle & Bengio, 2008; Rasmus et al., 2015; Zhao et al., 2015; Zhang et al., 2016; Cho & Chen, 2014).", "startOffset": 271, "endOffset": 401}, {"referenceID": 21, "context": "Instead of the two stage learning strategy (unsupervised pre-training followed by supervised fine-tuning), several works focused on a joint learning model which optimizes unsupervised and supervised objectives concurrently, resulting in better generalization performance (Goodfellow et al., 2013; Larochelle & Bengio, 2008; Rasmus et al., 2015; Zhao et al., 2015; Zhang et al., 2016; Cho & Chen, 2014).", "startOffset": 271, "endOffset": 401}, {"referenceID": 20, "context": "We assume that good latent representation Z is attained by maximizing the dependency among a set of random variables X , Y , and Z, which is known as total correlation or multiinformation (Watanabe, 1960).", "startOffset": 188, "endOffset": 204}, {"referenceID": 19, "context": "(2) It is known that maximizing \u2212H(X|Z) can be formulated as minimizing the reconstruction error between the input x (sampled from X) and its reconstruction xR under the general audo-encoder framework (Vincent et al., 2010).", "startOffset": 201, "endOffset": 223}, {"referenceID": 13, "context": "Previous works on deep neural networks for supervised learning can be categorized into two types as shown in Figure 2; (a) a general feed-forward neural network model (LeCun et al., 1998; Krizhevsky et al., 2012; Simonyan & Zisserman, 2015; He et al., 2016), and (b) a joint learning model which optimizes unsupervised and supervised objectives at the same time (Zhao et al.", "startOffset": 167, "endOffset": 257}, {"referenceID": 11, "context": "Previous works on deep neural networks for supervised learning can be categorized into two types as shown in Figure 2; (a) a general feed-forward neural network model (LeCun et al., 1998; Krizhevsky et al., 2012; Simonyan & Zisserman, 2015; He et al., 2016), and (b) a joint learning model which optimizes unsupervised and supervised objectives at the same time (Zhao et al.", "startOffset": 167, "endOffset": 257}, {"referenceID": 7, "context": "Previous works on deep neural networks for supervised learning can be categorized into two types as shown in Figure 2; (a) a general feed-forward neural network model (LeCun et al., 1998; Krizhevsky et al., 2012; Simonyan & Zisserman, 2015; He et al., 2016), and (b) a joint learning model which optimizes unsupervised and supervised objectives at the same time (Zhao et al.", "startOffset": 167, "endOffset": 257}, {"referenceID": 22, "context": ", 2016), and (b) a joint learning model which optimizes unsupervised and supervised objectives at the same time (Zhao et al., 2015; Zhang et al., 2016; Cho & Chen, 2014).", "startOffset": 112, "endOffset": 169}, {"referenceID": 21, "context": ", 2016), and (b) a joint learning model which optimizes unsupervised and supervised objectives at the same time (Zhao et al., 2015; Zhang et al., 2016; Cho & Chen, 2014).", "startOffset": 112, "endOffset": 169}, {"referenceID": 16, "context": "Figure 3: Ladder network; a representative model for semi-supervised learning (Rasmus et al., 2015).", "startOffset": 78, "endOffset": 99}, {"referenceID": 16, "context": "Another type of the joint learning model, a ladder network (Figure 3), was introduced for semisupervised learning (Rasmus et al., 2015).", "startOffset": 114, "endOffset": 135}], "year": 2016, "abstractText": "Latent representation learned from multi-layered neural networks via hierarchical feature abstraction enables recent success of deep learning. Under the deep learning framework, generalization performance highly depends on the learned latent representation which is obtained from an appropriate training scenario with a taskspecific objective on a designed network model. In this work, we propose a novel latent space modeling method to learn better latent representation. We designed a neural network model based on the assumption that good base representation can be attained by maximizing the total correlation between the input, latent, and output variables. From the base model, we introduce a semantic noise modeling method which enables class-conditional perturbation on latent space to enhance the representational power of learned latent feature. During training, latent vector representation can be stochastically perturbed by a modeled class-conditional additive noise while maintaining its original semantic feature. It implicitly brings the effect of semantic augmentation on the latent space. The proposed model can be easily learned by back-propagation with common gradient-based optimization algorithms. Experimental results show that the proposed method helps to achieve performance benefits against various previous approaches. We also provide the empirical analyses for the proposed class-conditional perturbation process including t-SNE visualization.", "creator": "LaTeX with hyperref package"}}}