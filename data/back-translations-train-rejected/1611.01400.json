{"id": "1611.01400", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Nov-2016", "title": "Learning to Rank Scientific Documents from the Crowd", "abstract": "Finding related published articles is an important task in any science, but with the explosion of new work in the biomedical domain it has become especially challenging. Most existing methodologies use text similarity metrics to identify whether two articles are related or not. However biomedical knowledge discovery is hypothesis-driven. The most related articles may not be ones with the highest text similarities. In this study, we first develop an innovative crowd-sourcing approach to build an expert-annotated document-ranking corpus. Using this corpus as the gold standard, we then evaluate the approaches of using text similarity to rank the relatedness of articles. Finally, we develop and evaluate a new supervised model to automatically rank related scientific articles. Our results show that authors' ranking differ significantly from rankings by text-similarity-based models. By training a learning-to-rank model on a subset of the annotated corpus, we found the best supervised learning-to-rank model (SVM-Rank) significantly surpassed state-of-the-art baseline systems.", "histories": [["v1", "Fri, 4 Nov 2016 14:43:44 GMT  (271kb,D)", "http://arxiv.org/abs/1611.01400v1", "12 pages, 1 figure"]], "COMMENTS": "12 pages, 1 figure", "reviews": [], "SUBJECTS": "cs.IR cs.CL cs.DL cs.LG cs.SI", "authors": ["jesse m lingeman", "hong yu"], "accepted": false, "id": "1611.01400"}, "pdf": {"name": "1611.01400.pdf", "metadata": {"source": "CRF", "title": "Learning to Rank Scientific Documents from the Crowd", "authors": ["Jesse M Lingeman", "Hong Yu"], "emails": ["lingeman@cs.umass.edu", "hong.yu@umassmed.edu"], "sections": [{"heading": null, "text": "This year is the highest in the history of the country."}, {"heading": "Materials and Methods", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Benchmark Datasets", "text": "In order to develop and evaluate ranking algorithms, we need a benchmark dataset. However, to the best of our knowledge, we do not have an openly available benchmark dataset for bibliographic query-by-document systems. Therefore, we have created such a benchmark dataset. Creating a benchmark dataset is a daunting labor-intensive task, and especially a scientific challenge, because you have to master the technical jargon of a scientific article, and such experts are not easy to find using traditional crowd-sourcing technologies (e.g. AMT). For our task, the ideal annotator for each of our articles is the authors themselves. Publishers typically have a clear knowledge of the references they cite and their scientific significance for their publication, and therefore can be outstanding judges in ranking the reference articles."}, {"heading": "Learning to Rank", "text": "In fact, most of them will be able to play by the rules they have established in the past."}, {"heading": "Features", "text": "The question of the meaning of the term and the meaning of the term, which is quoted in the individual areas of the term, is not only the question of the meaning of the term, but also the question of the meaning of the term, the term, the concept, the concept, the concept, the concept, the concept, the concept, the concept, the concept, the concept, the concept, the concept, the concept, the concept, the concept, the concept, the concept, the concept, the concept, the concept, the concept, the concept, the concept, the concept, the concept, the concept, the concept, the concept, the concept, the concept, the concept, the concept, the concept, the concept, the concept, the concept, the concept, the concept, the concept, the concept, the concept, the concept, the concept, the concept, the concept, the concept, the term, the concept, the concept, the concept, the concept, the concept, the concept, the term, the concept, the concept, the term, the term, the meaning, the term, the term, the term, the term, the meaning, the term, the term, the term, the meaning, the term, the term, the"}, {"heading": "Baseline Systems", "text": "We compare our system to a variety of baselines. (1) rank by the number of times a quote is mentioned in the document. (2) rank by the number of times the quote is cited in literature (citation effect). (3) rank by Google Scholar Related Articles. (4) rank by TF * IDF weighted cosinal similarity. (5) rank by a model trained in text similarity rankings. The first two base systems are models in which values are ranked from the highest to the lowest to generate the ranking. The idea is that the number of times a quote is mentioned in an article or the citation effect can already be good indicators of its proximity. The text similarity model is trained using the same characteristics and methods used by the annotation model to generate the ranking."}, {"heading": "Evaluation Measures", "text": "NDCG Normalized Discounted Cumulative Gain (NDCG) is a common measure for comparing a list of estimated document-relevance judgments with a list of known judgments (Croft et al. (2010). To calculate NDCG, we first calculate a ranking of Discounted Cumulative Gain (DCG) as: DCG = rel1 + N \u2211 i = 1 2reli \u2212 1 log2 (1), where Reli is the relevance judgment at position i. Intuitively, DCG punishes the recovery of documents that are not relevant (reli = 0). However, DCG is an unlimited value. To compare the DCG between two models, we need to normalize it. To do this, we use the ideal DCG (IDCG), i.e. the maximum possible DCG judgments given the relevance judgments."}, {"heading": "Forward Feature Selection", "text": "The selection of forward functions is done by iterative testing of each trait; the most powerful trait is retained in the model, and another sweep occurs over the remaining traits, continuing until all traits have been selected; this approach allows us to study the effects of trait combinations and the effect of too many or too few traits; and we can assess which traits and trait combinations are strongest."}, {"heading": "Results", "text": "We first compare our gold standard with the baselines. A random baseline is provided as a reference. Since all the documents we evaluate are relevant, NDCG will be quite high, simply by chance. We find that the number of Timesa documents mentioned in the commented document is significantly better than the random baseline or the impact on the citation. The more often a document is mentioned in a paper, the more likely it is that the author rates it as important. Interestingly, we see a negative correlation with the effects on the citation. The more often a document is mentioned in the literature, the less likely it is that it is important. These results are listed in Table 1. Further, we rank the raw values of the features and compare them with our gold standard to obtain a baseline (Table 2). The most powerful text similarity feature is the similarity between the abstraction of the commented document and the summary of the cited document, the less likely it is to compare the raw values of the features and it is important to obtain these baseline results in order to evaluate them in the table 1.Continue to the baseline characteristics."}, {"heading": "Discussion", "text": "We have found that the authors interpret the references they have quoted in a substantially different way from the authors themselves. Our results show that the fragmentation of a document into a set of characteristics that are able to capture this difference plays a key role. While the similarity between text and language is indeed important, the number of people listed in a document is also mentioned as important characteristics in the literature. The more often a quote is mentioned in the text, the more likely it is that it is important to be quoted in the articles. We have also found that the acceptance of people quoted in the literature is negative."}, {"heading": "Acknowledgments", "text": "We would like to thank all authors who took the time to answer our Citation Ranking Survey. This work is supported by Na-tional Institutes of Health with the grant number 1R01GM095476. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.ReferencesBrin, S. and Page, L. (2007). The anatomy of a large-scale hypertextual web search engine, 1998. In the Seventh World Wide Web Conference, pages 107-117. Proceedings of the Seventh World Wide Web Conference.Cao, Y., Xu, J., Liu, T.-Y., Li, H., Huang, Y. and Hon, H.-W. (2006). Adapting ranking SVM to document retrieval., In The 29th annual international ACM SIGIR conference, pages 186-193, New York, New York, York."}], "references": [], "referenceMentions": [], "year": 2016, "abstractText": "Motivation: Finding related published articles is an important task in any science, but with the explosion of new work in the biomedical domain it has become especially challenging. Most existing methodologies use text similarity metrics to identify whether two articles are related or not. However biomedical knowledge discovery is hypothesis-driven. The most related articles may not be ones with the highest text similarities. In this study, we first develop an innovative crowd-sourcing approach to build an expert-annotated document-ranking corpus. Using this corpus as the gold standard, we then evaluate the approaches of using text similarity to rank the relatedness of articles. Finally, we develop and evaluate a new supervised model to automatically rank related scientific articles. Results: Our results show that authors\u2019 ranking differ significantly from rankings by textsimilarity-based models. By training a learning-to-rank model on a subset of the annotated corpus, we found the best supervised learning-to-rank model (SVM) significantly surpassed state-of-the-art baseline systems. Availability: Code and data are both publicly available. \u2020", "creator": "LaTeX with hyperref package"}}}