{"id": "1611.01436", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Nov-2016", "title": "Learning Recurrent Span Representations for Extractive Question Answering", "abstract": "The reading comprehension task, that asks questions about a given evidence document, is a central problem in natural language understanding. Recent formulations of this task have typically focused on answer selection from a set of candidates pre-defined manually or through the use of an external NLP pipeline. However, Rajpurkar et al. (2016) recently released the SQuAD dataset in which the answers can be arbitrary strings from the supplied text. In this paper, we focus on this answer extraction task, presenting a novel model architecture that efficiently builds fixed length representations of all spans in the evidence document with a recurrent network. We show that scoring explicit span representations significantly improves performance over other approaches that factor the prediction into separate predictions about words or start and end markers. Our approach improves upon the best published results of Wang &amp; Jiang (2016) by 5% and decreases the error of Rajpurkar et al.'s baseline by &gt; 50%.", "histories": [["v1", "Fri, 4 Nov 2016 16:12:46 GMT  (132kb,D)", "http://arxiv.org/abs/1611.01436v1", null], ["v2", "Fri, 17 Mar 2017 18:11:12 GMT  (131kb,D)", "http://arxiv.org/abs/1611.01436v2", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["kenton lee", "shimi salant", "tom kwiatkowski", "ankur parikh", "dipanjan das", "jonathan berant"], "accepted": false, "id": "1611.01436"}, "pdf": {"name": "1611.01436.pdf", "metadata": {"source": "CRF", "title": "EXTRACTIVE QUESTION ANSWERING", "authors": ["Kenton Lee", "Tom Kwiatkowski", "Ankur Parikh", "Dipanjan Das"], "emails": ["kentonl@cs.washington.edu", "dipanjand}@google.com"], "sections": [{"heading": "1 INTRODUCTION", "text": "A primary goal of natural language processing is to develop systems that can answer questions about the content of documents. Reading comprehension is of practical interest - we want computers to be able to read the text of the world and then answer our questions - and because we believe it requires a deep understanding of language, it has also become a flagship task in NLP research. A number of reading comprehension data sets have been developed that focus on selecting answers from a small number of alternatives defined by annotators (Richardson et al., 2013) or existing NLP pipelines that cannot be trained end-to-end (Hill et al., 2016; Hermann et al., 2015). Subsequently, the models proposed for this task have tended to make use of the limited number of candidates, with their predictions based on attention weights at mention level (Hermann et al., 2015), or network memories (Hill et al)."}, {"heading": "2 EXTRACTIVE QUESTION ANSWERING", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 TASK DEFINITION", "text": "Detailed systems for answering questions enter a question q = {q0,.., qn} and a text passage p = {p0,.., pm} from which they predict a single response span a = < astart, aend >, which is represented as a pair of indexes in p. Machines learned Extensive systems for answering questions, such as the one presented here, learn a prediction function f (q, p) \u2192 a from a training data set of < q, p, a > triples."}, {"heading": "2.2 RELATED WORK", "text": "For the SQUAD dataset, the original paper by Rajpurkar et al. (2016) implements a linear model with sparse features based on n-grams and part-of-speech tags in the question and the candidate's answer. Other than lexical characteristics, they also use syntactic information in the form of dependency pathways to extract more general features. They set a strong starting point for the following work and also presented a depth analysis that shows that lexical and syntactic characteristics contribute most to the performance of their model. Subsequent work by Wang & Jiang (2016) use an end-to-end neural network method that uses a match-LSTM to model the question and the passage, and uses pointer networks (Vinyals et al., 2015) to extract the response span of the passage from the passage. This model relies on toxic decoding and falls back on performance in comparison to our model."}, {"heading": "3 MODEL", "text": "We propose a model architecture called RASOR2, illustrated in Figure 1, which explicitly calculates the embedding of representations for candidate response spans. In most structured prediction problems (e.g. sequence labeling or analysis), the number of possible output structures is exponential in input2An abbreviation for Recurrent Span Representations, pronounced as razor-sharp length, and the calculation of representations for each candidate is prohibitively expensive. To calculate these chip representations, we need to list information from the passage and question for each candidate in a trivial and comprehensible way. This allows for an expressive model that calculates common representations for each response that can be globally normalized during learning. To calculate these chip representations, we need to summarize information from the passage and question for each candidate in response. For the example in Figure 1, RASOR calculates an embedding of the common word:"}, {"heading": "3.1 SCORING ANSWER SPANS", "text": "The goal of our extractive response system to questions is to predict the individual best response margin among all candidates from passage p, known as A (p). Therefore, we define a probability distribution across all possible response ranges taking into account question q and passage p, and the predictor function finds the response margin with the highest probability: f (q, p): = argmax a (p) P (a | q, p) (1) One might be tempted to introduce independence assumptions that would allow for cheaper decryption. For example, this distribution may be modeled as (1) product of conditional independent distributions (binary) for each word or (2) as a product of conditional independent distributions (via words) for the start and end indices of the response span. However, in Section 5.2, we show that such independence assumptions violate the accuracy of the model (binary), and instead we only assume a defined representation of each candidate with a softer result and a normalized response span."}, {"heading": "3.2 RASOR: RECURRENT SPAN REPRESENTATION", "text": "The previously defined probability distribution depends on the representation of the response span, ha. In the calculation of ha, we assume access to representations of individual passwords, which have been extended by a representation of the question. We refer to these question-focused password embeddings as {p \u0445 1,..., p \u0445 m} and describe their creation in Section 3.3. To reuse the calculation for common substructures, we use a bidirectional LSTM (Hochreiter & Schmidhuber, 1997) to encode the left and right context of each p \u0445 i (passage-level BiLSTM in Figure 1), which allows us to simply link the bidirectional LSTM outputs (BiLSTM) at the end points of a span to encode their inner and outer information together (span of the embedding in Figure 1): {p \u043c \u2032 1,."}, {"heading": "3.3 QUESTION-FOCUSED PASSAGE WORD EMBEDDING", "text": "The calculation of the question-focused passage word embedding {p * 1,.., p * m} requires the integration of question information into the passage. The architecture for this integration is flexible and probably depends on the nature of the dataset. For the SQUAD dataset, we note that both passage-oriented and pass-independent questions are effective at embedding this contextual information, and experiments will show that their advantages are complementary. To include these questions, we simply associate them with the password word word embedding (question-focused password embedding in Figure 1). We use fixed pre-formed embedding to represent question and password embedding. Therefore, the notation for the words is interchangeable with their embedded representations. Question-independent password embedding The first component simply provides the preformed password embedding with the embedded password."}, {"heading": "3.4 LEARNING", "text": "Given the model specification above, learning is simple: we simply maximize the log probability of the correct response candidates and propagate the errors end-to-end."}, {"heading": "4 EXPERIMENTAL SETUP", "text": "We represent each of the words in the question and the document with 300-dimensional GloVe embeddings trained on a corpus of 840 billion words (Pennington et al., 2014) These embeddings cover 200,000 words and all words from the vocabulary (OOV) and are projected onto one of 1 million randomly initialized 300d embeddings. We pair the inputs and forget gates in our LSTMs, as described in Greff et al. (2016), and use a single fallmask to apply suspensions in all LSTM time steps, as proposed by Gal & Ghahramani. Hidden layers in the feeding neural networks use reflected linear units (Nair & Hinton, 2010), and the response candidates are limited to layers with a maximum of 30 words. To select the final model configuration, we performed network searches on: the dimensionality of the layer 1, the layer 1-layer, the layer 1-states, and the layer 1-states, the layer 1;"}, {"heading": "5 RESULTS", "text": "We train with the 80k (question, passage, response span) triples in the SQUAD training set and report the results of the 10k examples in the SQUAD development set. Due to copyright restrictions, we are currently unable to upload our models to Codalab4, which is required for the blind SQUAD test set, but we are working with Rajpurkar et al. to fix this, and this work will be updated with test numbers as soon as possible. All results are calculated using the official SQUAD evaluation script, which reports exact response match and overlap of unigrams between the predicted answer and the closest response from the three reference answers in the SQUAD development set."}, {"heading": "5.1 COMPARISONS TO OTHER WORK", "text": "Our model with recurrent span representations (RASOR) is compared with all previously published systems as the probability of distribution is measured by the index 5. Rajpurkar et al. (2016) published a logistic regression baseline as well as human performance on the SQUAD task. The logistic regression baseline uses the output of an existing syntactic parser both as a constraint on the permitted response span, and as a method of creating sparse characteristics for a response-centric scoring model. Although we do not have access to an external representation of the linguistic structure, RASOR achieves an error reduction of more than 50% above this baseline, both in terms of exact match and in terms of human performance on the upper boundary. More closely related to RASOR is the boundary model with match LSTMs and pointer networks from Wang & Jiang (2016).Their model similarly uses recurrent networks to learn the embedding of each password in the question and the final context of the question."}, {"heading": "5.2 MODEL VARIATIONS", "text": "This year it has come to the point where it will be able to eren.n"}, {"heading": "6 ANALYSIS", "text": "Figure 2 shows how the performance of RASOR and the endpoint predictor introduced in Section 5.2 deteriorates with the length of their predictions. It is clear that the explicit modelling of interactions between the endmarkers becomes increasingly important as the length of the span increases. Figure 3 shows attention masks for both RASOR questions. Meanwhile, the passage-oriented question presentation pays the most attention to the words that could be associated with the answer in the passage (\"brought,\" \"against\") or the answer category (\"people\"). Meanwhile, the passage-oriented question presentation devotes the most attention to similar words. The top predictions for both examples are all valid syntactic components, and they all have the correct semantic category. However, RASOR assigns the erroneous third prediction \"british\" to almost as much probability mass as the correct prediction of the third person is found to be found to be \"british,\" where RASOR can show the correct proximity of a phrase. \""}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "KyungHyun Cho", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1409.0473,", "citeRegEx": "Bahdanau et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "A thorough examination of the cnn/daily mail reading comprehension task", "author": ["Danqi Chen", "Jason Bolton", "Christopher D. Manning"], "venue": "In Proceedings of ACL,", "citeRegEx": "Chen et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2016}, {"title": "A theoretically grounded application of dropout in recurrent neural networks", "author": ["Yarin Gal", "Zoubin Ghahramani"], "venue": "Proceedings of NIPS,", "citeRegEx": "Gal and Ghahramani.,? \\Q2016\\E", "shortCiteRegEx": "Gal and Ghahramani.", "year": 2016}, {"title": "LSTM: A search space odyssey", "author": ["Klaus Greff", "Rupesh Kumar Srivastava", "Jan Koutn\u0131\u0301k", "Bas R. Steunebrink", "J\u00fcrgen Schmidhuber"], "venue": "IEEE Transactions on Neural Networks and Learning Systems,", "citeRegEx": "Greff et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Greff et al\\.", "year": 2016}, {"title": "Teaching machines to read and comprehend", "author": ["Karl Moritz Hermann", "Tomas Kocisky", "Edward Grefenstette", "Lasse Espeholt", "Will Kay", "Mustafa Suleyman", "Phil Blunsom"], "venue": "In Proceedings of NIPS,", "citeRegEx": "Hermann et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hermann et al\\.", "year": 2015}, {"title": "The goldilocks principle: Reading children\u2019s books with explicit memory representations", "author": ["Felix Hill", "Antoine Bordes", "Sumit Chopra", "Jason Weston"], "venue": "In Proceedings of ICLR,", "citeRegEx": "Hill et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Hill et al\\.", "year": 2016}, {"title": "Long Short-term Memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "Hochreiter and Schmidhuber.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba"], "venue": "Proceedings of ICLR,", "citeRegEx": "Kingma and Ba.,? \\Q2015\\E", "shortCiteRegEx": "Kingma and Ba.", "year": 2015}, {"title": "Dataset and neural recurrent sequence labeling model for open-domain factoid question answering", "author": ["Peng Li", "Wei Li", "Zhengyan He", "Xuguang Wang", "Ying Cao", "Jie Zhou", "Wei Xu"], "venue": null, "citeRegEx": "Li et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Li et al\\.", "year": 2016}, {"title": "Rectified linear units improve restricted boltzmann machines", "author": ["Vinod Nair", "Geoffrey E Hinton"], "venue": "In Proceedings of ICML,", "citeRegEx": "Nair and Hinton.,? \\Q2010\\E", "shortCiteRegEx": "Nair and Hinton.", "year": 2010}, {"title": "A decomposable attention model for natural language inference", "author": ["Ankur P Parikh", "Oscar T\u00e4ckstr\u00f6m", "Dipanjan Das", "Jakob Uszkoreit"], "venue": "In Proceedings of EMNLP,", "citeRegEx": "Parikh et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Parikh et al\\.", "year": 2016}, {"title": "Glove: Global vectors for word representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher D Manning"], "venue": "In Proceedings of EMNLP,", "citeRegEx": "Pennington et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "SQuAD: 100, 000+ questions for machine comprehension of text", "author": ["Pranav Rajpurkar", "Jian Zhang", "Konstantin Lopyrev", "Percy Liang"], "venue": "In Proceedings of EMNLP,", "citeRegEx": "Rajpurkar et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Rajpurkar et al\\.", "year": 2016}, {"title": "Mctest: A challenge dataset for the open-domain machine comprehension of text", "author": ["Matthew Richardson", "Christopher JC Burges", "Erin Renshaw"], "venue": "In Proceedings of EMNLP,", "citeRegEx": "Richardson et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Richardson et al\\.", "year": 2013}, {"title": "Cloze procedure: A new tool for measuring readability", "author": ["Wilson Taylor"], "venue": "Journalism Quarterly,", "citeRegEx": "Taylor.,? \\Q1953\\E", "shortCiteRegEx": "Taylor.", "year": 1953}, {"title": "Convolutional neural networks vs. convolution kernels: Feature engineering for answer sentence reranking", "author": ["Kateryna Tymoshenko", "Daniele Bonadiman", "Alessandro Moschitti"], "venue": "In Proceedings of NAACL,", "citeRegEx": "Tymoshenko et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Tymoshenko et al\\.", "year": 2016}, {"title": "Building a question answering test collection", "author": ["Ellen M. Voorhees", "Dawn M. Tice"], "venue": "In Proceedings of SIGIR,", "citeRegEx": "Voorhees and Tice.,? \\Q2000\\E", "shortCiteRegEx": "Voorhees and Tice.", "year": 2000}, {"title": "Inner attention based recurrent neural networks for answer selection", "author": ["Bingning Wang", "Kang Liu", "Jun Zhao"], "venue": "In Proceedings of ACL,", "citeRegEx": "Wang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2016}, {"title": "Machine comprehension using match-lstm and answer pointer", "author": ["Shuohang Wang", "Jing Jiang"], "venue": "arXiv preprint arXiv:1608.07905,", "citeRegEx": "Wang and Jiang.,? \\Q2016\\E", "shortCiteRegEx": "Wang and Jiang.", "year": 2016}, {"title": "Wikiqa: A challenge dataset for open-domain question answering", "author": ["Yi Yang", "Wen-tau Yih", "Christopher Meek"], "venue": "In Proceedings of EMNLP,", "citeRegEx": "Yang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 12, "context": "However, Rajpurkar et al. (2016) recently released the SQUAD dataset in which the answers can be arbitrary strings from the supplied text.", "startOffset": 9, "endOffset": 33}, {"referenceID": 12, "context": "However, Rajpurkar et al. (2016) recently released the SQUAD dataset in which the answers can be arbitrary strings from the supplied text. In this paper, we focus on this answer extraction task, presenting a novel model architecture that efficiently builds fixed length representations of all spans in the evidence document with a recurrent network. We show that scoring explicit span representations significantly improves performance over other approaches that factor the prediction into separate predictions about words or start and end markers. Our approach improves upon the best published results of Wang & Jiang (2016) by 5% and decreases the error of Rajpurkar et al.", "startOffset": 9, "endOffset": 626}, {"referenceID": 13, "context": "A number of reading comprehension datasets have been developed that focus on answer selection from a small set of alternatives defined by annotators (Richardson et al., 2013) or existing NLP pipelines that cannot be trained end-to-end (Hill et al.", "startOffset": 149, "endOffset": 174}, {"referenceID": 5, "context": ", 2013) or existing NLP pipelines that cannot be trained end-to-end (Hill et al., 2016; Hermann et al., 2015).", "startOffset": 68, "endOffset": 109}, {"referenceID": 4, "context": ", 2013) or existing NLP pipelines that cannot be trained end-to-end (Hill et al., 2016; Hermann et al., 2015).", "startOffset": 68, "endOffset": 109}, {"referenceID": 4, "context": "Subsequently, the models proposed for this task have tended to make use of the limited set of candidates, basing their predictions on mention-level attention weights (Hermann et al., 2015), or centering classifiers (Chen et al.", "startOffset": 166, "endOffset": 188}, {"referenceID": 1, "context": ", 2015), or centering classifiers (Chen et al., 2016), or network memories (Hill et al.", "startOffset": 34, "endOffset": 53}, {"referenceID": 5, "context": ", 2016), or network memories (Hill et al., 2016) on candidate locations.", "startOffset": 29, "endOffset": 48}, {"referenceID": 1, "context": ", 2015), or centering classifiers (Chen et al., 2016), or network memories (Hill et al., 2016) on candidate locations. Recently, Rajpurkar et al. (2016) released the less restricted SQUAD dataset1 that does not place any constraints on the set of allowed answers, other than that they should be drawn from the evidence document.", "startOffset": 35, "endOffset": 153}, {"referenceID": 1, "context": ", 2015), or centering classifiers (Chen et al., 2016), or network memories (Hill et al., 2016) on candidate locations. Recently, Rajpurkar et al. (2016) released the less restricted SQUAD dataset1 that does not place any constraints on the set of allowed answers, other than that they should be drawn from the evidence document. Rajpurkar et al. proposed a baseline system that chooses answers from the constituents identified by an existing syntactic parser. This allows them to prune the O(N) answer candidates in each document of length N , but it also effectively renders 20.7% of all questions unanswerable. Subsequent work by Wang & Jiang (2016) significantly improve upon this baseline by using an endto-end neural network architecture to identify answer spans by labeling either individual words, or the start and end of the answer span.", "startOffset": 35, "endOffset": 652}, {"referenceID": 14, "context": "A task that is closely related to extractive question answering is the Cloze task (Taylor, 1953), in which the goal is to predict a concealed span from a declarative sentence given a passage of supporting text.", "startOffset": 82, "endOffset": 96}, {"referenceID": 19, "context": "Relevant datasets include datasets from the annual TREC evaluations (Voorhees & Tice, 2000) and WikiQA (Yang et al., 2015), where the latter dataset specifically focused on Wikipedia passages.", "startOffset": 103, "endOffset": 122}, {"referenceID": 10, "context": "For the SQUAD dataset, the original paper from Rajpurkar et al. (2016) implemented a linear model with sparse features based on n-grams and part-of-speech tags present in the question and the candidate answer.", "startOffset": 47, "endOffset": 71}, {"referenceID": 10, "context": "For the SQUAD dataset, the original paper from Rajpurkar et al. (2016) implemented a linear model with sparse features based on n-grams and part-of-speech tags present in the question and the candidate answer. Other than lexical features, they also used syntactic information in the form of dependency paths to extract more general features. They set a strong baseline for following work and also presented an in depth analysis, showing that lexical and syntactic features contribute most strongly to their model\u2019s performance. Subsequent work by Wang & Jiang (2016) use an end-to-end neural network method that uses a Match-LSTM to model the question and the passage, and uses pointer networks (Vinyals et al.", "startOffset": 47, "endOffset": 567}, {"referenceID": 3, "context": "Recently, Hermann et al. (2015) presented a Cloze dataset in which the task is to predict the correct entity in an incomplete sentence given an abstractive summary of a news article.", "startOffset": 10, "endOffset": 32}, {"referenceID": 1, "context": "Although this dataset is large and varied in domain, recent analysis by Chen et al. (2016) shows that simple models can achieve close to the human upper bound.", "startOffset": 72, "endOffset": 91}, {"referenceID": 0, "context": "The alignments are computed via neural attention (Bahdanau et al., 2014), and we use the variant proposed by Parikh et al.", "startOffset": 49, "endOffset": 72}, {"referenceID": 0, "context": "The alignments are computed via neural attention (Bahdanau et al., 2014), and we use the variant proposed by Parikh et al. (2016), where attention scores are dot products between non-linear mappings of word embeddings.", "startOffset": 50, "endOffset": 130}, {"referenceID": 8, "context": "This representation is a bidirectional generalization of the question representation recently proposed by Li et al. (2016) for a different question-answering task.", "startOffset": 106, "endOffset": 123}, {"referenceID": 11, "context": "We represent each of the words in the question and document using 300 dimensional GloVe embeddings trained on a corpus of 840bn words (Pennington et al., 2014).", "startOffset": 134, "endOffset": 159}, {"referenceID": 3, "context": "We couple the input and forget gates in our LSTMs, as described in Greff et al. (2016), and we use a single dropout mask to apply dropout across all LSTM time-steps as proposed by Gal & Ghahramani (2016).", "startOffset": 67, "endOffset": 87}, {"referenceID": 3, "context": "We couple the input and forget gates in our LSTMs, as described in Greff et al. (2016), and we use a single dropout mask to apply dropout across all LSTM time-steps as proposed by Gal & Ghahramani (2016). Hidden layers in the feed forward neural networks use rectified linear units (Nair & Hinton, 2010).", "startOffset": 67, "endOffset": 204}, {"referenceID": 12, "context": "Rajpurkar et al. (2016) published a logistic regression baseline as well as human performance on the SQUAD task.", "startOffset": 0, "endOffset": 24}, {"referenceID": 8, "context": "Li et al. (2016) proposed a sequence-labeling scheme that is similar to the above baseline (BIO sequence prediction in Table 2b).", "startOffset": 0, "endOffset": 17}], "year": 2016, "abstractText": "The reading comprehension task, that asks questions about a given evidence document, is a central problem in natural language understanding. Recent formulations of this task have typically focused on answer selection from a set of candidates pre-defined manually or through the use of an external NLP pipeline. However, Rajpurkar et al. (2016) recently released the SQUAD dataset in which the answers can be arbitrary strings from the supplied text. In this paper, we focus on this answer extraction task, presenting a novel model architecture that efficiently builds fixed length representations of all spans in the evidence document with a recurrent network. We show that scoring explicit span representations significantly improves performance over other approaches that factor the prediction into separate predictions about words or start and end markers. Our approach improves upon the best published results of Wang & Jiang (2016) by 5% and decreases the error of Rajpurkar et al.\u2019s baseline by > 50%.", "creator": "LaTeX with hyperref package"}}}