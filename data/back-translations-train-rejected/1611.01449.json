{"id": "1611.01449", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Nov-2016", "title": "Semi-supervised deep learning by metric embedding", "abstract": "Deep networks are successfully used as classification models yielding state-of-the-art results when trained on a large number of labeled samples. These models, however, are usually much less suited for semi-supervised problems because of their tendency to overfit easily when trained on small amounts of data. In this work we will explore a new training objective that is targeting a semi-supervised regime with only a small subset of labeled data. This criterion is based on a deep metric embedding over distance relations within the set of labeled samples, together with constraints over the embeddings of the unlabeled set. The final learned representations are discriminative in euclidean space, and hence can be used with subsequent nearest-neighbor classification using the labeled samples.", "histories": [["v1", "Fri, 4 Nov 2016 16:39:20 GMT  (102kb,D)", "http://arxiv.org/abs/1611.01449v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["elad hoffer", "nir ailon"], "accepted": false, "id": "1611.01449"}, "pdf": {"name": "1611.01449.pdf", "metadata": {"source": "CRF", "title": "SEMI-SUPERVISED DEEP LEARNING BY METRIC EM- BEDDING", "authors": ["Elad Hoffer"], "emails": ["ehoffer@tx.technion.ac.il", "nailon@cs.technion.ac.il"], "sections": [{"heading": null, "text": "Deep networks are successfully used as state-of-the-art classification models that provide results when trained on a large number of labeled samples, but these models are generally much less suitable for semi-monitored problems as they easily overlap when trained on small amounts of data. In this thesis, we will examine a new training goal aimed at a semi-monitored system with only a small subset of labeled data based on a deep metric embedding over distance relationships within the set of labeled samples, along with limitations on embedding the unlabeled set. The ultimately learned representations are discriminatory in Euclidean space and can therefore be used with subsequent classification near neighbors based on the labeled samples."}, {"heading": "1 INTRODUCTION", "text": "Deep neural networks have shown that they work very well on various classification problems, which often lead to unclear results. Key motivation for using these models is the assumption of the hierarchical nature of the underlying problem. This assumption is reflected in the structure of NNs, which are composed of several stacked layers of linear transformations followed by non-linear activation functions. The hierarchical property of NNNs is usually a soft, linear transformation, indicating the probability of each class that can be trained with the known goal of each sample."}, {"heading": "2 RELATED WORK", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 LEARNING METRIC EMBEDDING", "text": "One type of such metric embedding is the \"Siamese Network\" framework introduced by Bromley et al. (1993) and later used in the work of Chopra et al. (2005). This problem was recently addressed by Schroff et al. (2015) when the number of classes is too large or is to change over time, as in the case of facial verification, where a face contained in an image must be compared with another image of a face. This problem was recently tackled by Schroff et al. (2015) to train an evolutionary network model based on examples. Learning characteristics through metric embedding also showed Hoffer & Ailon (2015) to provide competitive classification accuracy compared to conventional cross-entropy regressions. This work is also related to Rippel et al al al al al al. (2015), which assigned the magnetic loss - a neural insert loss for each of the metallic embedding losses for each of the metallic grading losses."}, {"heading": "2.2 SEMI-SUPERVISED LEARNING BY ADVERSARIAL REGULARIZATION", "text": "Regularization techniques can often be interpreted as earlier than model parameters or structures, such as Lp regularization via network weights or activations. More recently, neural network-specific regularizations that cause noise within the training process, such as Srivastava et al. (2014); Wan et al. (2013); Szegedy et al. (2015), have been shown to be highly beneficial in avoiding overhauling. Another recent observation by Goodfellow et al. (2015) is that training based on reconciliatory examples, input factors that have been misclassified with minor disruption, can improve generalization. This fact has been investigated by Feng et al. (2016) and found to offer remarkable improvements over the semi-monitored regime of Miyato et al. (2015)."}, {"heading": "2.3 SEMI-SUPERVISED LEARNING BY AUXILIARY RECONSTRUCTION LOSS", "text": "A stacked what-where autocoder from Zhao et al. (2015) calculates a set of complementary variables that allow reconstruction when a layer implements multiple-to-one mapping. Rasmus et al. (2015) conductor networks - use lateral connections to allow higher levels of an auto encoder to focus on invariant abstract features by applying a layer-by-layer cost function.Generative adversarial network (GAN) is a recently introduced model that can be used in an unattended manner Goodfellow et al. (2014). Adversarial generative models use a set of networks that are trained to distinguish between data sampled from the true underlying distribution (e.g. a set of images), and a separate generative network trained to confuse the first network. Springentially, the 2015 distribution is generated by the networks that aggregated the learned data into (similar to the classes aggregated into) 1."}, {"heading": "2.4 SEMI-SUPERVISED LEARNING BY ENTROPY MINIMIZATION", "text": "Another method of semi-supervised learning introduced by Grandvalet & Bengio (2004) is to minimize entropy over the expected class distribution in unlabeled examples; the regularization of minimal entropy can be considered a precursor that favors minimal overlap between observed classes; it can also be considered a generalization of the \"self-training\" method described by Triguero et al. (2015), which reintroduces unlabeled examples after the previous classification of the model; and it is also related to the \"Transductive suport vector machines\" (TSVM) Vapnik & Vapnik (1998), which introduce a maximum margin objective over labeled and unlabeled examples."}, {"heading": "3 OUR CONTRIBUTION: NEIGHBOR EMBEDDING FOR SEMI-SUPERVISED LEARNING", "text": "In this paper, we deal with a semi-monitored environment in which learning takes place on the basis of data of which only a small subset is labeled. (Given observed sets of labeled data XL = (x, y) li = 1 and unlabeled data XU = {x} ni = l + 1, in which x-X, y-C, we would like to learn a classifier f: X \u2192 C to test a minimal expected error on some invisible test data X. We will make a few assumptions regarding the given data: \u2022 The number of labeled examples is small compared to the total observed set l n. (Structure assumptions - samples within the same structure (such as a cluster or multiple) are more likely to share the same label. This assumption is shared with many other semi-supervised approaches discussed in Chapelle et al. (2009), Weston et al. (2012).With these assumptions, we are motivated to learn a metric embedding that forms clusters."}, {"heading": "4 LEARNING BY DISTANCE COMPARISONS", "text": "We will define a discrete distribution for the embedded distance between a sample x-X and examples z1,..., zc-XL designated as examples z1,..., zc-XL, each belonging to a different class: P (x; z1,..., zc) i = e \u2212 female F (x) \u2212 F (zi), 2 \u0445 c j = 1 e \u2212 female F (x) \u2212 F (zj), 2, i-c {1... c} (1) This definition assigns to the sample x a probability P (x; z1,..., zc) i, which according to a 1-nn classification rule is classified into class i when z1,... zc neighbors are specified. It is similar to the stochastically adjacent formulation by Goldberger et al. (2004) and will allow us to specify the two underlying targets as measures of this distribution."}, {"heading": "4.1 DISTANCE RATIO CRITERION", "text": "In relation to the target (i), we will use a sample xl-XL from the labeled sentence belonging to class k-C, and another set of labeled examples z1,..., zc-XL. In this thesis, we will minimize the cross entropy between I (xl) and the distance distribution of x in relation to z1,..., zc1: L (xl, z1,..., zc) L = H (I (xl), P (xl; z1,..., zc) (2). This is in fact a slightly modified version of the distance loss introduced in Hoffer & Ailon (2015). L (xl, z1,..., zc) L = \u2212 log e \u2212 F (xl) \u2212 F (zk).This distance loss is embedded in relation to the same class (xl, z1,..., zc).2 This loss is compared to the other classes (zk)."}, {"heading": "4.2 MINIMUM ENTROPY CRITERION", "text": "Another part of the optimized criterion, inspired by Grandvalet & Bengio (2004), aims to reduce the overlap between the different classes of unlabeled samples. We will promote this goal by minimizing the entropy of the underlying distance distribution of x, again in relation to the designated samples z1,..., zc1: L (x, z1,..., zc) U = H (P (x; z1,..., zc))) (4), the asL (x, z1,..., zc) U = \u2212 c \u00b2 i = 1e \u2212 F (x) \u2212 F (zi), 2 \u00b2 cj = 1 e \u2212 F (x) \u2212 F (zj) \u2212 f (zj), 2 \u00b7 log e \u2212 F (x) \u2212 F (zi)."}, {"heading": "5 QUALITIES OF NEIGHBOR EMBEDDING", "text": "We will now discuss some observed properties of adjacent embedding and their usefulness for semi-monitored regimes using neural network models."}, {"heading": "5.1 REDUCING OVERFIT", "text": "Normally, when using NNs for classification, cross-entropy loss minimization is used by using a fixed one-hot indicator (similar to 2) as a target for each marked example, maximizing the log probability of the correct label. This form of optimization over a fixed target tends to lead to overadjustment of the neural network, especially for small marked sets. This was discussed and addressed recently by Szegedy et al. (2015) by adding additional random noise to the targets, effectively smoothing out the cross-entropy target distribution. This regulation technique empirically led to a better generalization by reducing overadjustment over the training set."}, {"heading": "5.2 EMBEDDING INTO EUCLIDEAN SPACE", "text": "By developing the model for embedding characteristics that are discriminatory in relation to their distance in the Euclidean space, we can achieve good classification accuracy with the help of a simple nearby classifier, which allows an interpretation of semantic relationships in the Euclidean space, which can be useful for various tasks such as obtaining information or transferring learning."}, {"heading": "5.3 INCORPORATING PRIOR KNOWLEDGE", "text": "We also point out that prior knowledge of an existing problem can be incorporated into the expected measures in relation to the distance distribution 1. For example, knowledge of the relative distance between classes can be used to replace I (x) as the target distribution in Equation 3, and knowledge of overlap between classes can be used to loosen the constraint in Equation 5."}, {"heading": "6 EXPERIMENTS", "text": "All experiments were conducted using the Torch7 framework by Collobert et al. (2011). Code to reproduce these results will therefore be available at: https: / / github.com / eladhoffer / SemiSupContrast. For each experiment, we selected a small random subset of examples, selecting a balanced number of examples from each class and labeling them with XL. The remaining training images are used without their labels to form XU. Finally, we tested our final accuracy with a disjoint series of Xtest examples. No data augmentation was applied to the training sessions. In each iteration, we consistently sampled a set of labeled examples z1,... z | C | \u0441XL. Additionally, batches of uniformly sampled examples were re-sampled from the labeled group XL, and the unlabeled sets XU. A batch size of b = 32 was used for all experiments, with a total of one sample set of Sampled \u00b7 2 \u00b7 C | Ib for each sample set."}, {"heading": "6.1 RESULTS ON MNIST", "text": "The MNIST database of handwritten digits, introduced by LeCun et al. (1998), is one of the most widely studied benchmarks for image classification, containing 60,000 examples of handwritten digits from 0 to 9 for training and 10,000 additional examples of tests where each sample is a 28 x 28 pixel grayscale image. We followed previous work (Weston et al., 2012), (Zhao et al., 2015), Rasmus et al. (2015) and used a semi-supervised regime in which only 100 samples (10 for each class) were used together with their labels as XL. For the embedding network, we used a revolutionary network with 5-revolution layers, followed by a ReLU non-linearity and a stack normalization layer."}, {"heading": "6.2 RESULTS ON CIFAR-10", "text": "Cifar-10, introduced by Krizhevsky & Hinton (2009), is a benchmark image classification dataset with 50,000 training images and 10,000 test images, with image sizes of 32 x 32 pixels with color. Classes include airplanes, cars, birds, cats, deer, dogs, frogs, horses, ships and trucks. Using a commonly used system, we trained on 4,000 randomly selected samples (400 for each class). As a wavy embedding network, we used a network similar to that of Lin et al. (2014), described in Table 3. The test results will be shown in Table 2. As can be observed, we achieve competitive results with the state of the art in this system. We also point out that current best results come from generative models such as Springenberg (2016) and Salimans et al. (2016), which follow an elaborate and computer-intensive training process, compared with our approach."}, {"heading": "7 CONCLUSIONS", "text": "In this paper, we have demonstrated how neural networks can be used to learn in a semi-monitored environment by using small sets of labeled data, replacing the classification target with a metric embedding target. We have introduced a target for semi-monitored learning, which is formulated as minimizing entropy via distance coding distribution, in line with standard techniques for forming deep neural networks and does not require modification of the embedding model. By using the method in this paper, we have been able to achieve state-of-the-art results at MNIST with only 100 labeled examples and competitive results from the Cifar10 dataset. We speculate that this form of learning is beneficial to neural network models by reducing their propensity to overlap with small sets of training data."}, {"heading": "8 APPENDIX", "text": "It is not the first time that the EU Commission has taken such a step."}], "references": [{"title": "Signature verification using a siamese time delay neural network", "author": ["Jane Bromley", "James W Bentz", "L\u00e9on Bottou", "Isabelle Guyon", "Yann LeCun", "Cliff Moore", "Eduard S\u00e4ckinger", "Roopak Shah"], "venue": "International Journal of Pattern Recognition and Artificial Intelligence,", "citeRegEx": "Bromley et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Bromley et al\\.", "year": 1993}, {"title": "Semi-supervised learning (chapelle, o. et al., eds.; 2006)[book reviews", "author": ["Olivier Chapelle", "Bernhard Scholkopf", "Alexander Zien"], "venue": "IEEE Transactions on Neural Networks,", "citeRegEx": "Chapelle et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Chapelle et al\\.", "year": 2009}, {"title": "Learning a similarity metric discriminatively, with application to face verification", "author": ["Sumit Chopra", "Raia Hadsell", "Yann LeCun"], "venue": "In Computer Vision and Pattern Recognition,", "citeRegEx": "Chopra et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Chopra et al\\.", "year": 2005}, {"title": "Torch7: A matlab-like environment for machine learning", "author": ["Ronan Collobert", "Koray Kavukcuoglu", "Cl\u00e9ment Farabet"], "venue": "In BigLearn, NIPS Workshop, number EPFL-CONF-192376,", "citeRegEx": "Collobert et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Discriminative unsupervised feature learning with convolutional neural networks", "author": ["Alexey Dosovitskiy", "Jost Tobias Springenberg", "Martin Riedmiller", "Thomas Brox"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Dosovitskiy et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Dosovitskiy et al\\.", "year": 2014}, {"title": "Ensemble robustness of deep learning algorithms", "author": ["Jiashi Feng", "Tom Zahavy", "Bingyi Kang", "Huan Xu", "Shie Mannor"], "venue": "arXiv preprint arXiv:1602.02389,", "citeRegEx": "Feng et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Feng et al\\.", "year": 2016}, {"title": "Neighbourhood components analysis", "author": ["Jacob Goldberger", "Geoffrey E Hinton", "Sam T Roweis", "Ruslan Salakhutdinov"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Goldberger et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Goldberger et al\\.", "year": 2004}, {"title": "Generative adversarial nets", "author": ["Ian Goodfellow", "Jean Pouget-Abadie", "Mehdi Mirza", "Bing Xu", "David Warde-Farley", "Sherjil Ozair", "Aaron Courville", "Yoshua Bengio"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Goodfellow et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2014}, {"title": "Large-scale feature learning with spikeand-slab sparse coding", "author": ["Ian J. Goodfellow", "Aaron Courville", "Yoshua Bengio"], "venue": null, "citeRegEx": "Goodfellow et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2012}, {"title": "Explaining and harnessing adversarial examples", "author": ["Ian J Goodfellow", "Jonathon Shlens", "Christian Szegedy"], "venue": null, "citeRegEx": "Goodfellow et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2015}, {"title": "Semi-supervised learning by entropy minimization", "author": ["Yves Grandvalet", "Yoshua Bengio"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Grandvalet and Bengio.,? \\Q2004\\E", "shortCiteRegEx": "Grandvalet and Bengio.", "year": 2004}, {"title": "Deep metric learning using triplet network", "author": ["Elad Hoffer", "Nir Ailon"], "venue": "In Similarity-Based Pattern Recognition,", "citeRegEx": "Hoffer and Ailon.,? \\Q2015\\E", "shortCiteRegEx": "Hoffer and Ailon.", "year": 2015}, {"title": "Direct modeling of complex invariances for visual object features", "author": ["Ka Y Hui"], "venue": "In Proceedings of the 30th International Conference on Machine Learning", "citeRegEx": "Hui.,? \\Q2013\\E", "shortCiteRegEx": "Hui.", "year": 2013}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["Sergey Ioffe", "Christian Szegedy"], "venue": "In Proceedings of The 32nd International Conference on Machine Learning,", "citeRegEx": "Ioffe and Szegedy.,? \\Q2015\\E", "shortCiteRegEx": "Ioffe and Szegedy.", "year": 2015}, {"title": "Learning multiple layers of features from tiny images", "author": ["Alex Krizhevsky", "Geoffrey Hinton"], "venue": "Computer Science Department,", "citeRegEx": "Krizhevsky and Hinton.,? \\Q2009\\E", "shortCiteRegEx": "Krizhevsky and Hinton.", "year": 2009}, {"title": "Gradient-based learning applied to document recognition", "author": ["Yann LeCun", "L\u00e9on Bottou", "Yoshua Bengio", "Patrick Haffner"], "venue": "Proceedings of the IEEE,", "citeRegEx": "LeCun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "Distributional smoothing by virtual adversarial", "author": ["Takeru Miyato", "Shin-ichi Maeda", "Masanori Koyama", "Ken Nakae", "Shin Ishii"], "venue": "examples. stat,", "citeRegEx": "Miyato et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Miyato et al\\.", "year": 2015}, {"title": "A method of solving a convex programming problem with convergence rate o (1/k2)", "author": ["Yurii Nesterov"], "venue": null, "citeRegEx": "Nesterov.,? \\Q1983\\E", "shortCiteRegEx": "Nesterov.", "year": 1983}, {"title": "Unsupervised representation learning with deep convolutional generative adversarial networks", "author": ["Alec Radford", "Luke Metz", "Soumith Chintala"], "venue": "arXiv preprint arXiv:1511.06434,", "citeRegEx": "Radford et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Radford et al\\.", "year": 2015}, {"title": "Semisupervised learning with ladder networks", "author": ["Antti Rasmus", "Mathias Berglund", "Mikko Honkala", "Harri Valpola", "Tapani Raiko"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Rasmus et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Rasmus et al\\.", "year": 2015}, {"title": "Cnn features off-theshelf: an astounding baseline for recognition", "author": ["Ali Razavian", "Hossein Azizpour", "Josephine Sullivan", "Stefan Carlsson"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops,", "citeRegEx": "Razavian et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Razavian et al\\.", "year": 2014}, {"title": "Metric learning with adaptive density discrimination", "author": ["Oren Rippel", "Manohar Paluri", "Piotr Dollar", "Lubomir Bourdev"], "venue": "stat, 1050:18,", "citeRegEx": "Rippel et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Rippel et al\\.", "year": 2015}, {"title": "Improved techniques for training gans", "author": ["Tim Salimans", "Ian Goodfellow", "Wojciech Zaremba", "Vicki Cheung", "Alec Radford", "Xi Chen"], "venue": "arXiv preprint arXiv:1606.03498,", "citeRegEx": "Salimans et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Salimans et al\\.", "year": 2016}, {"title": "Facenet: A unified embedding for face recognition and clustering", "author": ["Florian Schroff", "Dmitry Kalenichenko", "James Philbin"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "Schroff et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Schroff et al\\.", "year": 2015}, {"title": "Unsupervised and semi-supervised learning with categorical generative adversarial networks", "author": ["Jost Tobias Springenberg"], "venue": "In International Conference on Learning Representations (ICLR)", "citeRegEx": "Springenberg.,? \\Q2016\\E", "shortCiteRegEx": "Springenberg.", "year": 2016}, {"title": "Dropout: a simple way to prevent neural networks from overfitting", "author": ["Nitish Srivastava", "Geoffrey E Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Srivastava et al\\.,? \\Q1929\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 1929}, {"title": "Rethinking the inception architecture for computer vision", "author": ["Christian Szegedy", "Vincent Vanhoucke", "Sergey Ioffe", "Jonathon Shlens", "Zbigniew Wojna"], "venue": "arXiv preprint arXiv:1512.00567,", "citeRegEx": "Szegedy et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Szegedy et al\\.", "year": 2015}, {"title": "Self-labeled techniques for semi-supervised learning: taxonomy, software and empirical study", "author": ["Isaac Triguero", "Salvador Garc\u0131\u0301a", "Francisco Herrera"], "venue": "Knowledge and Information Systems,", "citeRegEx": "Triguero et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Triguero et al\\.", "year": 2015}, {"title": "Statistical learning theory, volume 1", "author": ["Vladimir Naumovich Vapnik", "Vlamimir Vapnik"], "venue": null, "citeRegEx": "Vapnik and Vapnik.,? \\Q1998\\E", "shortCiteRegEx": "Vapnik and Vapnik.", "year": 1998}, {"title": "Regularization of neural networks using dropconnect", "author": ["Li Wan", "Matthew Zeiler", "Sixin Zhang", "Yann L Cun", "Rob Fergus"], "venue": "In Proceedings of the 30th International Conference on Machine Learning", "citeRegEx": "Wan et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Wan et al\\.", "year": 2013}, {"title": "Deep learning via semisupervised embedding", "author": ["Jason Weston", "Fr\u00e9d\u00e9ric Ratle", "Hossein Mobahi", "Ronan Collobert"], "venue": "In Neural Networks: Tricks of the Trade,", "citeRegEx": "Weston et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Weston et al\\.", "year": 2012}, {"title": "Stacked what-where auto-encoders", "author": ["Junbo Zhao", "Michael Mathieu", "Ross Goroshin", "Yann Lecun"], "venue": "arXiv preprint arXiv:1506.02351,", "citeRegEx": "Zhao et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zhao et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 20, "context": "These representative features, although not explicitly part of the training objective, were shown to be useful in subsequent tasks in the same domain as demonstrated by Razavian et al. (2014). One serious problem occurring in neural network is their susceptibility to overfit over the training data.", "startOffset": 169, "endOffset": 192}, {"referenceID": 20, "context": "These representative features, although not explicitly part of the training objective, were shown to be useful in subsequent tasks in the same domain as demonstrated by Razavian et al. (2014). One serious problem occurring in neural network is their susceptibility to overfit over the training data. Due to this fact, a considerable part of modern neural network research is devoted to regularization techniques and heuristics such as Srivastava et al. (2014); Ioffe & Szegedy (2015); Wan et al.", "startOffset": 169, "endOffset": 460}, {"referenceID": 20, "context": "These representative features, although not explicitly part of the training objective, were shown to be useful in subsequent tasks in the same domain as demonstrated by Razavian et al. (2014). One serious problem occurring in neural network is their susceptibility to overfit over the training data. Due to this fact, a considerable part of modern neural network research is devoted to regularization techniques and heuristics such as Srivastava et al. (2014); Ioffe & Szegedy (2015); Wan et al.", "startOffset": 169, "endOffset": 484}, {"referenceID": 20, "context": "These representative features, although not explicitly part of the training objective, were shown to be useful in subsequent tasks in the same domain as demonstrated by Razavian et al. (2014). One serious problem occurring in neural network is their susceptibility to overfit over the training data. Due to this fact, a considerable part of modern neural network research is devoted to regularization techniques and heuristics such as Srivastava et al. (2014); Ioffe & Szegedy (2015); Wan et al. (2013); Szegedy et al.", "startOffset": 169, "endOffset": 503}, {"referenceID": 20, "context": "These representative features, although not explicitly part of the training objective, were shown to be useful in subsequent tasks in the same domain as demonstrated by Razavian et al. (2014). One serious problem occurring in neural network is their susceptibility to overfit over the training data. Due to this fact, a considerable part of modern neural network research is devoted to regularization techniques and heuristics such as Srivastava et al. (2014); Ioffe & Szegedy (2015); Wan et al. (2013); Szegedy et al. (2015), to allow the networks to generalize to unseen data samples.", "startOffset": 169, "endOffset": 526}, {"referenceID": 0, "context": "One kind of such metric embedding is the \u201cSiamese network\u201d framework introduced by Bromley et al. (1993) and later used in the works of Chopra et al.", "startOffset": 83, "endOffset": 105}, {"referenceID": 0, "context": "One kind of such metric embedding is the \u201cSiamese network\u201d framework introduced by Bromley et al. (1993) and later used in the works of Chopra et al. (2005). One use for this methods is when the number of classes is too large or expected to vary over time, as in the case of face verification, where a face contained in an image has to compared against another image of a face.", "startOffset": 83, "endOffset": 157}, {"referenceID": 0, "context": "One kind of such metric embedding is the \u201cSiamese network\u201d framework introduced by Bromley et al. (1993) and later used in the works of Chopra et al. (2005). One use for this methods is when the number of classes is too large or expected to vary over time, as in the case of face verification, where a face contained in an image has to compared against another image of a face. This problem was recently tackled by Schroff et al. (2015) for training a convolutional network model on triplets of examples.", "startOffset": 83, "endOffset": 437}, {"referenceID": 0, "context": "One kind of such metric embedding is the \u201cSiamese network\u201d framework introduced by Bromley et al. (1993) and later used in the works of Chopra et al. (2005). One use for this methods is when the number of classes is too large or expected to vary over time, as in the case of face verification, where a face contained in an image has to compared against another image of a face. This problem was recently tackled by Schroff et al. (2015) for training a convolutional network model on triplets of examples. Learning features by metric embedding was also shown by Hoffer & Ailon (2015) to provide competitive classification accuracy compare to conventional cross-entropy regression.", "startOffset": 83, "endOffset": 583}, {"referenceID": 0, "context": "One kind of such metric embedding is the \u201cSiamese network\u201d framework introduced by Bromley et al. (1993) and later used in the works of Chopra et al. (2005). One use for this methods is when the number of classes is too large or expected to vary over time, as in the case of face verification, where a face contained in an image has to compared against another image of a face. This problem was recently tackled by Schroff et al. (2015) for training a convolutional network model on triplets of examples. Learning features by metric embedding was also shown by Hoffer & Ailon (2015) to provide competitive classification accuracy compare to conventional cross-entropy regression. This work is also related to Rippel et al. (2015), who introduced Magnet loss - a metric embedding approach for fine-grained classification.", "startOffset": 83, "endOffset": 730}, {"referenceID": 0, "context": "One kind of such metric embedding is the \u201cSiamese network\u201d framework introduced by Bromley et al. (1993) and later used in the works of Chopra et al. (2005). One use for this methods is when the number of classes is too large or expected to vary over time, as in the case of face verification, where a face contained in an image has to compared against another image of a face. This problem was recently tackled by Schroff et al. (2015) for training a convolutional network model on triplets of examples. Learning features by metric embedding was also shown by Hoffer & Ailon (2015) to provide competitive classification accuracy compare to conventional cross-entropy regression. This work is also related to Rippel et al. (2015), who introduced Magnet loss - a metric embedding approach for fine-grained classification. The Magnet loss is based on learning the distribution of distances for each sample, from K clusters assigned for each classified class. It then uses an intermediate k-means clustering, to reposition the different assigned clusters. This proved to allow better accuracy than both margin-based Triplet loss, and softmax regression. Using metric embedding with neural network was also specifically shown to provide good results in the semi-supervised learning setting as seen in Weston et al. (2012).", "startOffset": 83, "endOffset": 1318}, {"referenceID": 20, "context": "More recently, neural network specific regularizations that induce noise within the training process such as Srivastava et al. (2014); Wan et al.", "startOffset": 109, "endOffset": 134}, {"referenceID": 20, "context": "More recently, neural network specific regularizations that induce noise within the training process such as Srivastava et al. (2014); Wan et al. (2013); Szegedy et al.", "startOffset": 109, "endOffset": 153}, {"referenceID": 20, "context": "More recently, neural network specific regularizations that induce noise within the training process such as Srivastava et al. (2014); Wan et al. (2013); Szegedy et al. (2015) proved to be highly beneficial to avoid overfitting.", "startOffset": 109, "endOffset": 176}, {"referenceID": 6, "context": "Another recent observation by Goodfellow et al. (2015) is that training on adversarial examples, inputs that were found to be misclassified under small perturbation, can improve generalization.", "startOffset": 30, "endOffset": 55}, {"referenceID": 5, "context": "This fact was explored by Feng et al. (2016) and found to provide notable improvements to the semi supervised regime by Miyato et al.", "startOffset": 26, "endOffset": 45}, {"referenceID": 5, "context": "This fact was explored by Feng et al. (2016) and found to provide notable improvements to the semi supervised regime by Miyato et al. (2015).", "startOffset": 26, "endOffset": 141}, {"referenceID": 24, "context": "A stacked what-where autoencoder by Zhao et al. (2015) computes a set of complementary variables that enable reconstruction whenever a layer implements a many-to-one mapping.", "startOffset": 36, "endOffset": 55}, {"referenceID": 15, "context": "Ladder networks by Rasmus et al. (2015) - use lateral connections to allow higher levels of an auto-encoder to focus on invariant abstract features by applying a layer-wise cost function.", "startOffset": 19, "endOffset": 40}, {"referenceID": 7, "context": "Generative adversarial network (GAN) is a recently introduced model that can be used in an unsupervised fashion Goodfellow et al. (2014). Adversarial Generative Models use a set of networks, one trained to discriminate between data sampled from the true underlying distribution (e.", "startOffset": 112, "endOffset": 137}, {"referenceID": 7, "context": "Generative adversarial network (GAN) is a recently introduced model that can be used in an unsupervised fashion Goodfellow et al. (2014). Adversarial Generative Models use a set of networks, one trained to discriminate between data sampled from the true underlying distribution (e.g., a set of images), and a separate generative network trained to be an adversary trying to confuse the first network. By propagating the gradient through the paired networks, the model learns to generate samples that are distributed similarly to the source data. As shown by Radford et al. (2015), this model can create useful latent representations for subsequent classification tasks.", "startOffset": 112, "endOffset": 580}, {"referenceID": 7, "context": "Generative adversarial network (GAN) is a recently introduced model that can be used in an unsupervised fashion Goodfellow et al. (2014). Adversarial Generative Models use a set of networks, one trained to discriminate between data sampled from the true underlying distribution (e.g., a set of images), and a separate generative network trained to be an adversary trying to confuse the first network. By propagating the gradient through the paired networks, the model learns to generate samples that are distributed similarly to the source data. As shown by Radford et al. (2015), this model can create useful latent representations for subsequent classification tasks. The usage for these models for semi-supervised learning was further developed by Springenberg (2016) and Salimans et al.", "startOffset": 112, "endOffset": 771}, {"referenceID": 7, "context": "Generative adversarial network (GAN) is a recently introduced model that can be used in an unsupervised fashion Goodfellow et al. (2014). Adversarial Generative Models use a set of networks, one trained to discriminate between data sampled from the true underlying distribution (e.g., a set of images), and a separate generative network trained to be an adversary trying to confuse the first network. By propagating the gradient through the paired networks, the model learns to generate samples that are distributed similarly to the source data. As shown by Radford et al. (2015), this model can create useful latent representations for subsequent classification tasks. The usage for these models for semi-supervised learning was further developed by Springenberg (2016) and Salimans et al. (2016), by adding a N + 1 way classifier (number of classes + and additional \u201cfake\u201d class) to the discriminator.", "startOffset": 112, "endOffset": 798}, {"referenceID": 27, "context": "described by Triguero et al. (2015), in which unlabeled examples are re-introduced after being labeled with the previous classification of the model.", "startOffset": 13, "endOffset": 36}, {"referenceID": 27, "context": "described by Triguero et al. (2015), in which unlabeled examples are re-introduced after being labeled with the previous classification of the model. This is also related to the \u201cTransductive suport vector machines\u201d (TSVM) Vapnik & Vapnik (1998) which introduces a maximum margin objective over both labeled and unlabeled examples.", "startOffset": 13, "endOffset": 246}, {"referenceID": 1, "context": "This assumption is shared with many other semisupervised approaches as discussed in Chapelle et al. (2009),Weston et al.", "startOffset": 84, "endOffset": 107}, {"referenceID": 1, "context": "This assumption is shared with many other semisupervised approaches as discussed in Chapelle et al. (2009),Weston et al. (2012).", "startOffset": 84, "endOffset": 128}, {"referenceID": 6, "context": "It is similar to the stochastic-nearestneighbors formulation of Goldberger et al. (2004), and will allow us to state the two underlying objectives as measures over this distribution.", "startOffset": 64, "endOffset": 89}, {"referenceID": 26, "context": "This was lately discussed and addressed by Szegedy et al. (2015) using added random noise to the targets, effectively smoothing the cross-entropy target distribution.", "startOffset": 43, "endOffset": 65}, {"referenceID": 3, "context": "All experiments were conducted using the Torch7 framework by Collobert et al. (2011). Code reproducing these results will by available at https://github.", "startOffset": 61, "endOffset": 85}, {"referenceID": 3, "context": "All experiments were conducted using the Torch7 framework by Collobert et al. (2011). Code reproducing these results will by available at https://github.com/eladhoffer/ SemiSupContrast. For every experiment we chose a small random subset of examples, with a balanced number from each class and denoted by XL. The remaining training images are used without their labeled to form XU . Finally, we test our final accuracy with a disjoint set of examples Xtest. No data augmentation was applied to the training sets. In each iteration we sampled uniformly a set of labeled examples z1, ...z|C| \u2208 XL. In addition, batches of uniformly sampled examples were also sampled again from the labeled set XL, and the unlabeled set XU . A batch-size of b = 32 was used for all experiments, totaling a sampled set of 2 \u00b7 b + |C| examples for each iteration, where |C| = 10 for both datasets. We used 6 as optimization criterion, where \u03bb1 = \u03bb2 = 1. Optimization was done using the Accelerated-gradient method by Nesterov (1983) with an initial learning rate of lr0 = 0.", "startOffset": 61, "endOffset": 1012}, {"referenceID": 28, "context": "Model Test error % EmbedCNN Weston et al. (2012) 7.", "startOffset": 28, "endOffset": 49}, {"referenceID": 28, "context": "Model Test error % EmbedCNN Weston et al. (2012) 7.75 SWWAE Zhao et al. (2015) 9.", "startOffset": 28, "endOffset": 79}, {"referenceID": 19, "context": "17 Ladder network Rasmus et al. (2015) 0.", "startOffset": 18, "endOffset": 39}, {"referenceID": 19, "context": "17 Ladder network Rasmus et al. (2015) 0.89 (\u00b1 0.50) Conv-CatGAN Springenberg (2016) 1.", "startOffset": 18, "endOffset": 85}, {"referenceID": 30, "context": "We followed previous works ((Weston et al., 2012),(Zhao et al.", "startOffset": 28, "endOffset": 49}, {"referenceID": 31, "context": ", 2012),(Zhao et al., 2015),Rasmus et al.", "startOffset": 8, "endOffset": 27}, {"referenceID": 15, "context": "The MNIST database of handwritten digits introduced by LeCun et al. (1998) is one of the most studied dataset benchmark for image classification.", "startOffset": 55, "endOffset": 75}, {"referenceID": 15, "context": "The MNIST database of handwritten digits introduced by LeCun et al. (1998) is one of the most studied dataset benchmark for image classification. The dataset contains 60,000 examples of handwritten digits from 0 to 9 for training and 10,000 additional examples for testing, where each sample is a 28 x 28 pixel gray level image. We followed previous works ((Weston et al., 2012),(Zhao et al., 2015),Rasmus et al. (2015)) and used semi-supervised regime in which only 100 samples (10 for each class) were used as XL along with their labels.", "startOffset": 55, "endOffset": 420}, {"referenceID": 15, "context": "The MNIST database of handwritten digits introduced by LeCun et al. (1998) is one of the most studied dataset benchmark for image classification. The dataset contains 60,000 examples of handwritten digits from 0 to 9 for training and 10,000 additional examples for testing, where each sample is a 28 x 28 pixel gray level image. We followed previous works ((Weston et al., 2012),(Zhao et al., 2015),Rasmus et al. (2015)) and used semi-supervised regime in which only 100 samples (10 for each class) were used as XL along with their labels. For the embedding network, we used a convolutional network with 5-convolutional layers, where each layer is followed by a ReLU non-linearity and batch-normalization layer Ioffe & Szegedy (2015). The full network structure is described in Appendix table 3.", "startOffset": 55, "endOffset": 734}, {"referenceID": 6, "context": "Model Test error % Spike-and-Slab Sparse Coding Goodfellow et al. (2012) 31.", "startOffset": 48, "endOffset": 73}, {"referenceID": 6, "context": "Model Test error % Spike-and-Slab Sparse Coding Goodfellow et al. (2012) 31.9 View-Invariant k-means Hui (2013) 27.", "startOffset": 48, "endOffset": 112}, {"referenceID": 4, "context": "7) Exemplar-CNN Dosovitskiy et al. (2014) 23.", "startOffset": 16, "endOffset": 42}, {"referenceID": 4, "context": "7) Exemplar-CNN Dosovitskiy et al. (2014) 23.4 (\u00b1 0.2) Ladder network Rasmus et al. (2015) 20.", "startOffset": 16, "endOffset": 91}, {"referenceID": 4, "context": "7) Exemplar-CNN Dosovitskiy et al. (2014) 23.4 (\u00b1 0.2) Ladder network Rasmus et al. (2015) 20.04 (\u00b1 0.47) Conv-CatGan Springenberg (2016) 19.", "startOffset": 16, "endOffset": 138}, {"referenceID": 4, "context": "7) Exemplar-CNN Dosovitskiy et al. (2014) 23.4 (\u00b1 0.2) Ladder network Rasmus et al. (2015) 20.04 (\u00b1 0.47) Conv-CatGan Springenberg (2016) 19.58 (\u00b1 0.58) ImprovedGan Salimans et al. (2016) 18.", "startOffset": 16, "endOffset": 188}, {"referenceID": 23, "context": "We also note that current best results are from generative models such as Springenberg (2016) and Salimans et al.", "startOffset": 74, "endOffset": 94}, {"referenceID": 22, "context": "We also note that current best results are from generative models such as Springenberg (2016) and Salimans et al. (2016) that follow an elaborate and computationally heavy training procedure compared with our approach.", "startOffset": 98, "endOffset": 121}], "year": 2016, "abstractText": "Deep networks are successfully used as classification models yielding state-ofthe-art results when trained on a large number of labeled samples. These models, however, are usually much less suited for semi-supervised problems because of their tendency to overfit easily when trained on small amounts of data. In this work we will explore a new training objective that is targeting a semi-supervised regime with only a small subset of labeled data. This criterion is based on a deep metric embedding over distance relations within the set of labeled samples, together with constraints over the embeddings of the unlabeled set. The final learned representations are discriminative in euclidean space, and hence can be used with subsequent nearest-neighbor classification using the labeled samples.", "creator": "LaTeX with hyperref package"}}}