{"id": "1611.01457", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Nov-2016", "title": "Multi-task learning with deep model based reinforcement learning", "abstract": "In recent years, model-free methods that use deep learning have achieved great success in many different reinforcement learning environments. Most successful approaches focus on solving a single task, while multi-task reinforcement learning remains an open problem. In this paper, we present a model based approach to deep reinforcement learning which we use to solve different tasks simultaneously. We show that our approach not only does not degrade but actually benefits from learning multiple tasks. For our model, we also present a new kind of recurrent neural network inspired by residual networks that decouples memory from computation allowing to model complex environments that do not require lots of memory. The code will be released before ICLR 2017.", "histories": [["v1", "Fri, 4 Nov 2016 17:20:22 GMT  (260kb,D)", "http://arxiv.org/abs/1611.01457v1", null], ["v2", "Fri, 11 Nov 2016 12:48:31 GMT  (260kb,D)", "http://arxiv.org/abs/1611.01457v2", null], ["v3", "Mon, 22 May 2017 09:08:44 GMT  (256kb,D)", "http://arxiv.org/abs/1611.01457v3", null], ["v4", "Tue, 23 May 2017 18:52:37 GMT  (256kb,D)", "http://arxiv.org/abs/1611.01457v4", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["asier mujika"], "accepted": false, "id": "1611.01457"}, "pdf": {"name": "1611.01457.pdf", "metadata": {"source": "CRF", "title": "MULTI-TASK LEARNING WITH DEEP MODEL BASED REINFORCEMENT LEARNING", "authors": ["Asier Mujika"], "emails": ["asierm@student.ethz.ch"], "sections": [{"heading": "1 INTRODUCTION", "text": "Recently, we have had a lot of success in applying neural networks to learning, achieving multiple Q-networks in many ATARI games (Mnih et al. (2015); Mnih et al. (2016). Most of these algorithms are based on Q-Learning, which is a model-free approach to learning in any situation, but do not learn an explicit model of the environment. Apart from that, learning in multiple games at the same time remains an open problem, as these approaches are greatly degraded when the number of tasks is increased. In contrast, we present a model that is based on multiple tasks that can be learned simultaneously."}, {"heading": "3 PREDICTIVE REINFORCEMENT LEARNING", "text": "In order to avoid the disadvantages of deep Q-learning, we introduce Predictive Reinforcement Learning (PRL). In our approach, we separate the understanding of the environment from the strategy. This has the advantage that we can simultaneously learn from different strategies and simultaneously play strategies that are completely different from those from which it learns. We will also argue that this approach facilitates generalization, but before we present it, we must define what we want to solve."}, {"heading": "3.1 PREDICTION PROBLEM", "text": "The problem we want to solve is this: In view of the current state of the environment and the actions we will take in the future, the question arises as to how our score will change over time. To formalize this problem, we introduce the following notation: \u2022 ai: Observing the environment at a time i. In the case of ATARI games, this corresponds to the pixels of the screen. \u2022 ri: The total accumulated reward at a time i. In the case of ATARI games, this corresponds to the score in the game. \u2022 ci: The control at a time i. In the case of ATARI games, this corresponds to the input of the ATARI controller: top, right, shoot, etc. 1We do not explain the process, but Mnih et al. (2015) give a good explanation of how this happens. Then, we want to solve the following problem: For a certain time i and a positive integer k, let the input to our model be an observation and predict a series of future controls."}, {"heading": "3.2 MODEL", "text": "We have defined what we want to solve, but we have yet to specify how we will implement a model that will do so. We will use neural networks for this and divide it into three different networks as follows: \u2022 Perception: This network reads a state ai and converts it into a lower dimensional vector h0, which will be used by prediction and evaluation. \u2022 Prediction: For each j,...., k} this network reads the vector hj \u2212 1 and the corresponding control vector ci + j and creates a vector hj, which will be used in the next steps of prediction and evaluation. Note that this is in fact a recursive neural network. \u2022 Rating: For each j,...., k} this network reads the current vector hj of the prediction and predicts the difference in the score between the original time and the current one, i.e. ri + j \u2212 ri.Figure 2 illustrates the model. Observe that what we actually want to solve, and now we can explain the problem in more detail using the component."}, {"heading": "3.2.1 PERCEPTION", "text": "As we said earlier, the idea of this network is to transform the high-dimensional input into a low-dimensional vector that contains only the information necessary to predict the score. In the case of video games, it is easy to recognize that such a vector exists. Input will consist of thousands of pixels, but all we are interested in is the position of some key objects, such as the main character or the enemies. This information can easily be encoded with very few neurons. In our experiments, we convert an input consisting of 28K pixels into a vector with only 100 real values. To do this, we use deep revolutionary networks. These networks have recently achieved superhuman performance in very complex image recognition tasks (He et al., 2015). In fact, it has been observed that the upper layers in these models can learn low-dimensional abstract representations of this model."}, {"heading": "3.2.2 PREDICTION", "text": "For the prediction network, we present a new type of recursive network based on residual neural networks (He et al., 2015) that is particularly suitable for our task and achieves better results than an LSTM (Hochreiter & Schmidhuber, 1997) with a similar number of parameters in our first tests. Residual Recurrent Neural Network (RRNN) We define the RRNN in Figure 3 using the following notation: LN is the layer normalization function (Ba et al., 2016) that normalizes activations to have a median of 0 and a standard deviation of 1. \"\u00b7\" is the concatenation of two vectors. f can be a parameterizable and differentiable function, e.g. a multi-layer perceptron. As in residual networks, instead of calculating what the new state of the network should be, we calculate how it should change (ri)."}, {"heading": "3.2.3 VALUATION", "text": "The evaluation network reads the h-vector at the time i + j and outputs the change of the reward for this time step, i.e. ri + j \u2212 rj. Nevertheless, it is a central component of our model, since it allows the decoupling of the representation learned from the prediction from the reward function. Consider, for example, a robot in a real environment. If the perception learns to capture the physical properties of all surrounding objects (shape, mass, velocity, etc.), and the prediction learns to perform a physical simulation of the environment, this model can be used for any task in this environment, only the evaluation would need to be changed."}, {"heading": "3.3 STRATEGY", "text": "So to test our model in the experiments, we decided to hardcode a strategy, where we create a set of future controls uniformly at random and then select the one that would maximize our reward, since the probability of dying is low enough. Therefore, the games we tried were carefully chosen so that they do not need very sophisticated and long-term strategies.2The limit is not narrow, but it is sufficient for our purposes and easy to prove. However, our approach learns a predictive model that is independent of any strategy, and this can be beneficial in two ways. First, the model can play a strategy that is completely different from the one it learns from. Apart from that, learning a predictive model is a very difficult task to surpass. Consider a game with 10 possible control inputs and a training set in which we consider the next 25 steps."}, {"heading": "4 EXPERIMENTS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 ENVIRONMENT", "text": "Our experiments were conducted on a computer with a GeForce GTX 980 GPU and an Intel Xeon E5-2630 CPU. For the neural network, we used the Torch7 framework, and for the ATARI simulations, Alewrap, a Lua wrapper for the Arcade Learning Environment (Bellemare et al., 2015).4.2 MODELFor the Perception, we used a network inspired by deep residual networks (He et al., 2015).Figure 4 shows the architecture. The reason for this is that even if perception is relatively flat when the prediction network is unfolded over time, the depth of the resulting model is over 50 layers deeper. For prediction, we use a dual recurrent neural network. Table 1 describes the network used for the f function. Finally, Table 2 illustrates the evaluation network."}, {"heading": "4.3 SETUP", "text": "In our experiments, we trained on three different ATARI games simultaneously: Breakout, Pong and Demon Attack. We process the images using the same technique from Mnih et al. (2015).We take the maximum from the last 2 images to obtain a single 84 x 84 black-and-white image for the current observation. The input into perception is a 4 x 84 x 84 tensor with the last 4 observations. This is necessary to be able to use a feed-forward network for perception. If we observed a single image, it would not be possible to derive the speed and direction of a moving object. This would force us to use a recursive network on perception, which makes the training of the entire model much slower. To train the prediction, we unfold the network over time (25 time steps) and treat the model as a feed-forward network with divided weights. For our assessment, we give two values from the first, the probability that we will generate the first, the second, the higher step in the score."}, {"heading": "4.4 GENERATING DATA", "text": "To generate the data, we store tuples (ai, C = {ci + 1,... ci + 25}, R = {ri + 1 \u2212 ri,.. ri + 25 \u2212 ri}) while playing the game. That is, for each i we store the following: \u2022 ai: A 4 \u00b7 84 \u00b7 84 tensor, which contains 4 consecutive black and white images of size 84 \u00b7 84. \u2022 C: For j {i + 1,. i + 25}, each CJ is a three-dimensional vector encoding the control actions performed at that time. \u2022 R: For j {i + 1, the second dimension corresponds to the horizontal actions and the third to the vertical actions. For example, [1, \u2212 1, 0] represent the pressing of shoot and leave. \u2022 R: For j {i + 1,."}, {"heading": "4.5 TRAINING", "text": "To begin with, we generate 400K training cases per game by playing randomly, giving us a total of 1.2M training cases. Then, for subsequent iterations, we generate 200K additional training cases per game (a total of 600K) and train again on the entire dataset. That is, first we have 1.2 M training cases, then 1.8 M, then 2.4M, etc. Training is performed in a supervised manner as shown in Figure 2b. ai and C are given as input to the network and R as target. We minimize cross entropy loss by means of mini-stack descent. To speed up the process, instead of training a new network in each iteration, we continue to train the model from the previous iteration. This has the effect that we would train much more on the initial training cases, while the youngest would have an ever-decreasing effect as the training mark grows. To avoid this, we fix three weight drops during training."}, {"heading": "4.6 RESULTS", "text": "We have trained a model on the three games for a total of 19 iterations, equivalent to 4M time steps per game (74 hours of playing time at 60Hz), and each iteration takes about two hours on our hardware. In addition, we have trained an individual model for each game for 4M time steps. In each model, we have reduced the training duration so that the number of parameter updates per game is the same as in the case of multiple tasks. Unless transfer learning takes place, one would expect performance to deteriorate in the multi-task model. Figure 5 shows not only that there is no deterioration in pong and demon attack, but also a significant improvement in breakout. This confirms our initial belief that our approach is particularly well suited to multi-task learning. We have also argued that our model can potentially play a very different strategy from the one it has observed. Table 3 shows that this is indeed the case. A model that learned only by chance is capable of playing at least seven times."}, {"heading": "5 DISCUSSION", "text": "We have shown that it can beat human performance in three different tasks simultaneously and that it can benefit from learning multiple tasks. However, the model has two areas that can be addressed in future work: long-term dependencies and instability during training. The first can potentially be solved by combining our approach with Q-learning-based techniques. To reduce instability, balancing the training set or trampling hard training cases could alleviate the problem. Finally, we have also presented a new type of recurring network that can be very useful for problems that require little memory and a lot of computing power."}, {"heading": "ACKNOWLEDGMENTS", "text": "I would like to thank Angelika Steger and Florian Meier for their hardware support in the final experiments and comments on earlier versions of the essay."}], "references": [{"title": "The arcade learning environment: An evaluation platform for general agents", "author": ["Marc G. Bellemare", "Yavar Naddaf", "Joel Veness", "Michael Bowling"], "venue": "In IJCAI International Joint Conference on Artificial Intelligence,", "citeRegEx": "Bellemare et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bellemare et al\\.", "year": 2015}, {"title": "Deep Residual Learning for Image Recognition", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"], "venue": "Arxiv.Org, 7(3):171\u2013180,", "citeRegEx": "He et al\\.,? \\Q2015\\E", "shortCiteRegEx": "He et al\\.", "year": 2015}, {"title": "Long Short-Term Memory", "author": ["Sepp Hochreiter", "Urgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "Hochreiter and Schmidhuber.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Deep visual-semantic alignments for generating image descriptions", "author": ["Andrej Karpathy", "Fei Fei Li"], "venue": "In Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "Karpathy and Li.,? \\Q2015\\E", "shortCiteRegEx": "Karpathy and Li.", "year": 2015}, {"title": "Human-level control through deep reinforcement learning", "author": ["Volodymyr Mnih", "Koray Kavukcuoglu", "David Silver", "Andrei a Rusu", "Joel Veness", "Marc G Bellemare", "Alex Graves", "Martin Riedmiller", "Andreas K Fidjeland", "Georg Ostrovski", "Stig Petersen", "Charles Beattie", "Amir Sadik", "Ioannis Antonoglou", "Helen King", "Dharshan Kumaran", "Daan Wierstra", "Shane Legg", "Demis Hassabis"], "venue": null, "citeRegEx": "Mnih et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2015}, {"title": "Asynchronous Methods for Deep Reinforcement", "author": ["Volodymyr Mnih", "Adri\u00e0 Puigdom\u00e8nech Badia", "Mehdi Mirza", "Alex Graves", "Timothy P Lillicrap", "Tim Harley", "David Silver", "Koray Kavukcuoglu"], "venue": "Learning. arXiv,", "citeRegEx": "Mnih et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2016}, {"title": "Heat treatment inhibits skeletal muscle atrophy of glucocorticoidinduced myopathy in rats", "author": ["Y. Morimoto", "Y. Kondo", "H. Kataoka", "Y. Honda", "R. Kozu", "J. Sakamoto", "J. Nakano", "T. Origuchi", "T. Yoshimura", "M. Okita"], "venue": "Physiological Research,", "citeRegEx": "Morimoto et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Morimoto et al\\.", "year": 2015}, {"title": "Actor-Mimic: Deep Multitask and Transfer Reinforcement Learning", "author": ["Emilio Parisotto", "Jimmy Lei Ba", "Ruslan Salakhutdinov"], "venue": "arXiv preprint, pp", "citeRegEx": "Parisotto et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Parisotto et al\\.", "year": 2015}, {"title": "Policy Distillation", "author": ["Andrei A Rusu", "Sergio Gomez Colmenarejo", "Caglar Gulcehre", "Guillaume Desjardins", "James Kirkpatrick", "Razvan Pascanu", "Volodymyr Mnih", "Koray Kavukcuoglu", "Raia Hadsell"], "venue": "arXiv, pp", "citeRegEx": "Rusu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Rusu et al\\.", "year": 2015}, {"title": "Learning a Driving Simulator. arXiv, 2016", "author": ["Eder Santana", "George Hotz"], "venue": "URL http:// arxiv.org/abs/1608.01230", "citeRegEx": "Santana and Hotz.,? \\Q2016\\E", "shortCiteRegEx": "Santana and Hotz.", "year": 2016}, {"title": "On Learning to Think: Algorithmic Information Theory for Novel Combinations of Reinforcement Learning Controllers and Recurrent Neural World Models", "author": ["J\u00fcrgen Schmidhuber"], "venue": "arXiv, pp", "citeRegEx": "Schmidhuber.,? \\Q2015\\E", "shortCiteRegEx": "Schmidhuber.", "year": 2015}, {"title": "Understanding Neural Networks Through Deep Visualization", "author": ["Jason Yosinski", "Jeff Clune", "Anh Nguyen", "Thomas Fuchs", "Hod Lipson"], "venue": "International Conference on Machine Learning - Deep Learning Workshop 2015, pp", "citeRegEx": "Yosinski et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Yosinski et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 4, "context": "Recently, there has been a lot of success in applying neural networks to reinforcement learning, achieving super-human performance in many ATARI games (Mnih et al. (2015); Mnih et al.", "startOffset": 152, "endOffset": 171}, {"referenceID": 4, "context": "Recently, there has been a lot of success in applying neural networks to reinforcement learning, achieving super-human performance in many ATARI games (Mnih et al. (2015); Mnih et al. (2016)).", "startOffset": 152, "endOffset": 191}, {"referenceID": 10, "context": "The idea of learning predictive models has been previously proposed (Schmidhuber (2015); Santana & Hotz (2016)), but all of them focus on learning the predictive models in an unsupervised way.", "startOffset": 69, "endOffset": 88}, {"referenceID": 10, "context": "The idea of learning predictive models has been previously proposed (Schmidhuber (2015); Santana & Hotz (2016)), but all of them focus on learning the predictive models in an unsupervised way.", "startOffset": 69, "endOffset": 111}, {"referenceID": 4, "context": "In recent years, approaches that use Deep Q-learning have achieved great success, making an important breakthrough when Mnih et al. (2015) presented a neural network architecture that was able to achieve human performance on many different ATARI games, using just the pixels in the screen as input.", "startOffset": 120, "endOffset": 139}, {"referenceID": 7, "context": "Rusu et al. (2015) and Parisotto et al.", "startOffset": 0, "endOffset": 19}, {"referenceID": 7, "context": "(2015) and Parisotto et al. (2015) have managed to successfully learn multiple tasks using Q-learning.", "startOffset": 11, "endOffset": 35}, {"referenceID": 4, "context": "We do not explain the process, but Mnih et al. (2015) give a good explanation on how this is done.", "startOffset": 35, "endOffset": 54}, {"referenceID": 1, "context": "These networks have recently achieved super-human performance in very complex image recognition tasks (He et al., 2015).", "startOffset": 102, "endOffset": 119}, {"referenceID": 1, "context": "These networks have recently achieved super-human performance in very complex image recognition tasks (He et al., 2015). In fact, it has been observed that the upper layers in these models learn lower dimensional abstract representations of the input (Yosinski et al. (2015), Karpathy & Li (2015)).", "startOffset": 103, "endOffset": 275}, {"referenceID": 1, "context": "These networks have recently achieved super-human performance in very complex image recognition tasks (He et al., 2015). In fact, it has been observed that the upper layers in these models learn lower dimensional abstract representations of the input (Yosinski et al. (2015), Karpathy & Li (2015)).", "startOffset": 103, "endOffset": 297}, {"referenceID": 1, "context": "2 PREDICTION For the Prediction network, we present a new kind of recurrent network based on residual neural networks (He et al., 2015), which is specially well suited for our task and it achieved better results than an LSTM (Hochreiter & Schmidhuber, 1997) with a similar number of parameters in our initial tests.", "startOffset": 118, "endOffset": 135}, {"referenceID": 1, "context": "As shown by He et al. (2015) this prevents vanishing gradients or optimization difficulties.", "startOffset": 12, "endOffset": 29}, {"referenceID": 0, "context": "For the neural network, we have used the Torch7 framework and for the ATARI simulations, we have used Alewrap, which is a Lua wrapper for the Arcade Learning Environment (Bellemare et al., 2015).", "startOffset": 170, "endOffset": 194}, {"referenceID": 6, "context": "Figure 4: Each layer is followed by a Batch Normalization (Morimoto et al., 2015) and a Rectifier Linear Unit.", "startOffset": 58, "endOffset": 81}, {"referenceID": 1, "context": "For the Perception, we used a network inspired in deep residual networks (He et al., 2015).", "startOffset": 73, "endOffset": 90}, {"referenceID": 4, "context": "We preprocess the images following the same technique of Mnih et al. (2015). We take the maximum from the last 2 frames to get a single 84 \u00d7 84 black and white image for the current observation.", "startOffset": 57, "endOffset": 76}, {"referenceID": 4, "context": "(c) Demon Attack Figure 5: Comparison between an agent that learns the three games simultaneously (continuous blue), one that learns each game individually (dashed red) and the score of human testers (horizontal green) as reported by Mnih et al. (2015).", "startOffset": 234, "endOffset": 253}], "year": 2016, "abstractText": "In recent years, model-free methods that use deep learning have achieved great success in many different reinforcement learning environments. Most successful approaches focus on solving a single task, while multi-task reinforcement learning remains an open problem. In this paper, we present a model based approach to deep reinforcement learning which we use to solve different tasks simultaneously. We show that our approach not only does not degrade but actually benefits from learning multiple tasks. For our model, we also present a new kind of recurrent neural network inspired by residual networks that decouples memory from computation allowing to model complex environments that do not require lots of memory. The code will be released before ICLR 2017.", "creator": "LaTeX with hyperref package"}}}