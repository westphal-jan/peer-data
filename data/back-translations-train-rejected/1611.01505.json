{"id": "1611.01505", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Nov-2016", "title": "Improving Stochastic Gradient Descent with Feedback", "abstract": "In this paper we propose a simple and efficient method for improving stochastic gradient descent methods by using feedback from the objective function. The method tracks the relative changes in the objective function with a running average, and uses it to adaptively tune the learning rate in stochastic gradient descent. We specifically apply this idea to modify Adam, a popular algorithm for training deep neural networks. We conduct experiments to compare the resulting algorithm, which we call Eve, with state of the art methods used for training deep learning models. We train CNNs for image classification, and RNNs for language modeling and question answering. Our experiments show that Eve outperforms all other algorithms on these benchmark tasks. We also analyze the behavior of the feedback mechanism during the training process.", "histories": [["v1", "Fri, 4 Nov 2016 19:42:45 GMT  (1769kb,D)", "http://arxiv.org/abs/1611.01505v1", "ICLR 2017 Submission"], ["v2", "Thu, 17 Nov 2016 04:39:29 GMT  (1769kb,D)", "http://arxiv.org/abs/1611.01505v2", "Added section 3.1 discussing an assumption of our method. Fixed a typo regarding the lower/upper bound in algorithm section"]], "COMMENTS": "ICLR 2017 Submission", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["jayanth koushik", "hiroaki hayashi"], "accepted": false, "id": "1611.01505"}, "pdf": {"name": "1611.01505.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Jayanth Koushik", "Hiroaki Hayashi"], "emails": ["jkoushik@cs.cmu.edu", "hiroakih@cs.cmu.edu"], "sections": [{"heading": "1 INTRODUCTION", "text": "Despite several ground-breaking results in recent years, the formation of deep learning models remains a challenging problem. Currently, the most common approach is to use a variant of stochastic gradient lineage, a complex, high-dimensional, non-convex, stochastic optimization problem that is not accessible to many standard methods. Many enhancements have been proposed to the basic gradient lineage algorithm - designed to address specific problems in the formation of deep learning models. We will review some of these methods in the next section. Although variants of simple stochastic gradient lineage work quite well in practice, there is room for improvement. This is easily evidenced by the existence of numerous methods to simplify the optimization problem itself, such as weight initialization techniques and normalization methods. In this work, we seek to improve stochastic gradient lineage with a simple method that incorporates feedback from objective function."}, {"heading": "2 RELATED WORK", "text": "Considerable efforts have been made to understand the challenges of depth formation, but intuitively it seems that non-convex optimisation is made more difficult by the presence of several poor local optima. However, this geometric intuition proves to be insufficient in the argument about the high-dimensional case, which, however, deals with the training of deep learners (Bray & Dean, 2007; Dauphin et al., 2014). Furthermore, a recent paper (Kawaguchi, 2016) has shown that deep linear networks, even for non-linear networks, have the same value, and the work also showed that all critical points are not global."}, {"heading": "3 METHOD", "text": "The main component of our proposed method is a feedback term that captures the relative change in the objective value."}, {"heading": "4 EXPERIMENTS", "text": "Now we evaluate our proposed method by comparing Eve with several state-of-the-art algorithms to optimize deep learning models.1. In all experiments, we used ReLU activations and initialized weights according to the scheme proposed by Glorot & Bengio (2010), using size 128 minibatches and linear decay for the learning rate: \u03b1t = \u03b1 / (1 + \u03b3t) (\u03b3 is the decay rate determined by searches over a range of values).In the figures, SGD refers to vanilla stochastic gradient descent and SGD Nesterov to stochastic gradient descent with Nesterov dynamics (Nesterov, 1983), where we set the momentum to 0.9 in all experiments."}, {"heading": "4.1 CONVOLUTIONAL NEURAL NETWORKS", "text": "We first trained a 5-layer convolutionary neural network for the 10-way classification of images from the CIFAR10 dataset (Krizhevsky & Hinton, 2009), which consisted of 2 blocks of 3x0.5 each, followed by 2x2 max pooling and 0.25 dropout (Srivastava et al., 2014), the first block contained 2 layers of 32 filters each, and the second block contained 2 layers of 64 filters each, followed by a fully connected layer of 512 units and a 10-way softmax layer. We trained this model for 500 epochs on the training distribution using various popular methods for forming deep learning models, as well as Eve. For each algorithm, we tried to form learning rates {10 \u2212 2, 10 \u2212 3, 10 \u2212 4} and 4} (for algorithms with suggested standard learning rates)."}, {"heading": "4.2 ANALYSIS OF TUNING COEFFICIENT", "text": "Before looking at the next set of experiments on recurring neural networks, we will first take a closer look at the behavior of the tuning coefficient dt in our algorithm. We will take a closer look at the results of the CNN experiment on CIFAR10 \u2212 \u2212 Figure 2 shows the progress of dt in the entire training and also in two smaller windows. A few things are worth mentioning here that the general trend entails an initial acceleration followed by a decay \u2212 This initial acceleration allows Eve to quickly overtake other methods, making it a faster pace for about 100 epochs. This acceleration is not synonymous with simply starting at a higher learning rate \u2212 in all of our experiments, which we look for across a range of learning values \u2212 The general trend for dt can be explained by looking at the minibatch losses at each iteration (as opposed to the losses calculated over the entire dataset training)."}, {"heading": "4.3 RECURRENT NEURAL NETWORKS", "text": "Finally, we examined our method using relapsing neural networks (RNNs), first choosing an RNN to model character-level language based on the Penn Treebank dataset (Marcus et al., 1993). Specifically, the model consisted of a two-layer character-level gated recurrent unit (Chung et al., 2014) with hidden layers of size 256, 0.5 failure layers between layers and sequences of 100 characters. We used 10 \u2212 3 as the initial learning rate for Adam, Eve and RMSProp. For Adamax, we used 2 x 10 \u2212 3 as the learning rate because it is the proposed value. We used 3 x 10 \u2212 4 for the learning rate of decay. We trained this model for 100 epochs using each of the algorithms. The results plotted in Figure 6c clearly show that our method achieves the best results. Eve optimizes the model for a lower end loss than the other models."}, {"heading": "5 CONCLUSION", "text": "We proposed a simple and efficient method for incorporating feedback into stochastic gradient pedigree algorithms. We used this method to create Eve, a modified version of the Adam algorithm. Experiments with a variety of models showed that the proposed method can help improve the optimization of deep learning models. In future work, we would theoretically analyze our method and its effects. While we have tried to evaluate our Eve algorithm on a variety of tasks, additional experiments on larger problems would further emphasize the strength of our approach."}], "references": [{"title": "Statistics of critical points of gaussian fields on large-dimensional spaces", "author": ["Alan J Bray", "David S Dean"], "venue": "Physical review letters,", "citeRegEx": "Bray and Dean.,? \\Q2007\\E", "shortCiteRegEx": "Bray and Dean.", "year": 2007}, {"title": "Empirical evaluation of gated recurrent neural networks on sequence modeling", "author": ["Junyoung Chung", "Caglar Gulcehre", "KyungHyun Cho", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1412.3555,", "citeRegEx": "Chung et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chung et al\\.", "year": 2014}, {"title": "Identifying and attacking the saddle point problem in high-dimensional non-convex optimization", "author": ["Yann N Dauphin", "Razvan Pascanu", "Caglar Gulcehre", "Kyunghyun Cho", "Surya Ganguli", "Yoshua Bengio"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Dauphin et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Dauphin et al\\.", "year": 2014}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["John Duchi", "Elad Hazan", "Yoram Singer"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Duchi et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Duchi et al\\.", "year": 2011}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["Xavier Glorot", "Yoshua Bengio"], "venue": "In Aistats,", "citeRegEx": "Glorot and Bengio.,? \\Q2010\\E", "shortCiteRegEx": "Glorot and Bengio.", "year": 2010}, {"title": "Deep learning without poor local minima", "author": ["Kenji Kawaguchi"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Kawaguchi.,? \\Q2016\\E", "shortCiteRegEx": "Kawaguchi.", "year": 2016}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "Kingma and Ba.,? \\Q2014\\E", "shortCiteRegEx": "Kingma and Ba.", "year": 2014}, {"title": "Learning multiple layers of features from tiny images", "author": ["Alex Krizhevsky", "Geoffrey Hinton"], "venue": null, "citeRegEx": "Krizhevsky and Hinton.,? \\Q2009\\E", "shortCiteRegEx": "Krizhevsky and Hinton.", "year": 2009}, {"title": "Building a large annotated corpus of english: The penn treebank", "author": ["Mitchell P Marcus", "Mary Ann Marcinkiewicz", "Beatrice Santorini"], "venue": "Computational linguistics,", "citeRegEx": "Marcus et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Marcus et al\\.", "year": 1993}, {"title": "A method for unconstrained convex minimization problem with the rate of convergence o (1/k2)", "author": ["Yurii Nesterov"], "venue": "In Doklady an SSSR,", "citeRegEx": "Nesterov.,? \\Q1983\\E", "shortCiteRegEx": "Nesterov.", "year": 1983}, {"title": "Dropout: a simple way to prevent neural networks from overfitting", "author": ["Nitish Srivastava", "Geoffrey E Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Srivastava et al\\.,? \\Q1929\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 1929}, {"title": "Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude", "author": ["Tijmen Tieleman", "Geoffrey Hinton"], "venue": "COURSERA: Neural Networks for Machine Learning,", "citeRegEx": "Tieleman and Hinton.,? \\Q2012\\E", "shortCiteRegEx": "Tieleman and Hinton.", "year": 2012}, {"title": "Towards ai-complete question answering: A set of prerequisite toy tasks", "author": ["Jason Weston", "Antoine Bordes", "Sumit Chopra", "Alexander M Rush", "Bart van Merri\u00ebnboer", "Armand Joulin", "Tomas Mikolov"], "venue": "arXiv preprint arXiv:1502.05698,", "citeRegEx": "Weston et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Weston et al\\.", "year": 2015}, {"title": "Adadelta: an adaptive learning rate method", "author": ["Matthew D Zeiler"], "venue": "arXiv preprint arXiv:1212.5701,", "citeRegEx": "Zeiler.,? \\Q2012\\E", "shortCiteRegEx": "Zeiler.", "year": 2012}], "referenceMentions": [{"referenceID": 2, "context": "Various empirical and theoretical results (Bray & Dean, 2007; Dauphin et al., 2014) have indicated that the problem in high dimensions arises not from local minima, but rather from saddle points.", "startOffset": 42, "endOffset": 83}, {"referenceID": 5, "context": "Moreover, a recent paper (Kawaguchi, 2016) proved (for deep linear networks, and under reasonable assumptions, also for deep non-linear networks) that all local minima in fact achieve the same value, and are optimal.", "startOffset": 25, "endOffset": 42}, {"referenceID": 3, "context": "To handle this, Adagrad (Duchi et al., 2011) adaptively changes the learning rate for each parameter, performing larger updates for infrequently updated parameters.", "startOffset": 24, "endOffset": 44}, {"referenceID": 13, "context": "Adadelta (Zeiler, 2012) and RMSProp (Tieleman & Hinton, 2012) are two extensions that try to fix this issue.", "startOffset": 9, "endOffset": 23}, {"referenceID": 2, "context": "Various empirical and theoretical results (Bray & Dean, 2007; Dauphin et al., 2014) have indicated that the problem in high dimensions arises not from local minima, but rather from saddle points. Moreover, a recent paper (Kawaguchi, 2016) proved (for deep linear networks, and under reasonable assumptions, also for deep non-linear networks) that all local minima in fact achieve the same value, and are optimal. The work also showed that all critical points which are not global minima are saddle points. Saddle points can seriously hamper the progress of both first and second order methods. Second order methods like Newton\u2019s method are actually attracted to saddle points and are not suitable for high dimensional non-convex optimization. First order methods can escape from saddle points by following directions of negative curvature. However, such saddle points are usually surrounded by regions of small curvature - plateaus. This makes first order methods very slow near saddle points and can create the illusion of a local minimum. To tackle the saddle point problem, Dauphin et al. (2014) propose a second order method that fixes the issue with Newton\u2019s method.", "startOffset": 62, "endOffset": 1099}, {"referenceID": 9, "context": "In the figures, SGD refers to vanilla stochastic gradient descent, and SGD Nesterov refers to stochastic gradient descent with Nesterov momentum (Nesterov, 1983) where we set the momentum to 0.", "startOffset": 145, "endOffset": 161}, {"referenceID": 8, "context": "We first trained a RNN for character-level language modeling on the Penn Treebank dataset (Marcus et al., 1993).", "startOffset": 90, "endOffset": 111}, {"referenceID": 1, "context": "Specifically, the model consisted of a 2-layer character-level Gated Recurrent Unit (Chung et al., 2014) with hidden layers of size 256, 0.", "startOffset": 84, "endOffset": 104}, {"referenceID": 12, "context": "Specifically, we chose two question types among 20 types from the bAbI dataset (Weston et al., 2015), Q19 and Q14.", "startOffset": 79, "endOffset": 100}], "year": 2016, "abstractText": "In this paper we propose a simple and efficient method for improving stochastic gradient descent methods by using feedback from the objective function. The method tracks the relative changes in the objective function with a running average, and uses it to adaptively tune the learning rate in stochastic gradient descent. We specifically apply this idea to modify Adam, a popular algorithm for training deep neural networks. We conduct experiments to compare the resulting algorithm, which we call Eve, with state of the art methods used for training deep learning models. We train CNNs for image classification, and RNNs for language modeling and question answering. Our experiments show that Eve outperforms all other algorithms on these benchmark tasks. We also analyze the behavior of the feedback mechanism during the training process.", "creator": "LaTeX with hyperref package"}}}