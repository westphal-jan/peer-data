{"id": "1611.01547", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Nov-2016", "title": "Automated Generation of Multilingual Clusters for the Evaluation of Distributed Representations", "abstract": "We propose a language-agnostic way of automatically generating sets of semantically similar clusters of entities along with sets of \"outlier\" elements, which may then be used to perform an intrinsic evaluation of word embeddings in the outlier detection task. We used our methodology to create a gold-standard dataset, which we call WikiSem500, and evaluated multiple state-of-the-art embeddings. The results show a correlation between performance on this dataset and performance on sentiment analysis.", "histories": [["v1", "Fri, 4 Nov 2016 21:35:07 GMT  (2182kb,D)", "https://arxiv.org/abs/1611.01547v1", "12 pages"], ["v2", "Tue, 15 Nov 2016 13:21:17 GMT  (2182kb,D)", "http://arxiv.org/abs/1611.01547v2", "12 pages"], ["v3", "Fri, 9 Dec 2016 15:58:37 GMT  (5232kb,D)", "http://arxiv.org/abs/1611.01547v3", "12 pages"], ["v4", "Wed, 21 Dec 2016 17:51:57 GMT  (5233kb,D)", "http://arxiv.org/abs/1611.01547v4", "13 pages"], ["v5", "Wed, 5 Apr 2017 15:26:51 GMT  (5267kb,D)", "http://arxiv.org/abs/1611.01547v5", "Published as a workshop paper at ICLR 2017"]], "COMMENTS": "12 pages", "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["philip blair", "yuval merhav", "joel barry"], "accepted": false, "id": "1611.01547"}, "pdf": {"name": "1611.01547.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Philip Blair", "Yuval Merhav"], "emails": ["pblair@basistech.com", "yuval@basistech.com", "joelb@basistech.com"], "sections": [{"heading": "1 INTRODUCTION", "text": "Most of these data sets are based on word similarity (e.g. that there is a significant amount of work in this area that has resulted in a large number of publicly available data sets)."}, {"heading": "2 RELATED WORK", "text": "The basic idea is that annotators assign similarity values to pairs of words. (2015) and Hill et al. (2016) review many of these data sets. Hill et al. (2016) also argue that the prevailing gold standards for semantic ratings do not measure the ability of models to reflect similarity. Their main argument is that many such benchmarks measure association and kinship and not necessarily similarity, which limits their suitability for a wide range of applications. One of their motivating examples is the word pair \"coffee\" and \"cup,\" which has high similarity ratings in some benchmarks."}, {"heading": "3 GENERATING THE DATASET", "text": "In a format similar to the Camacho-Collados & Navigli (2016) dataset, we created groups of units that were semantically similar to each other, known as \"clusters,\" followed by up to three pairs (depending on availability) of different units or \"outliers,\" each with different degrees of semantic similarity to the cluster. The core thesis behind our design is that our knowledge base, Wikidata (2016), can be treated as a diagram in which the semantic similarity between two elements is inversely proportional to their distance. Informally, we treat Wikidata units that are components of a common entity as clusters (see Figure 1). Starting from this common entity (which we call a \"class\"), we then follow the \"subclass of\" relationships to find a sibling class (see \"American Football Team\" in Figure 1)."}, {"heading": "3.1 REFINING THE DATASET QUALITY", "text": "In fact, it is as if most of us are able to surpass ourselves by putting ourselves at the centre of the public's attention. (...) In fact, most of us are able to put ourselves at the centre of attention. (...) It is as if they put themselves at the centre of the public's attention. (...) It is as if they put themselves at the centre of the public's attention. (...) It is as if they put themselves at the centre. (...) It is as if they put themselves at the centre. (...) It is as if they put themselves at the centre. (...) It is as if they put themselves at the centre. (...) It is as if they put themselves at the centre. (...) It is as if they put themselves at the centre. (...) It is as if they put themselves at the centre. (...) It is as if they put themselves at the centre. (...) It is as if they put themselves at the centre."}, {"heading": "3.2 THE WIKISEM500 DATASET", "text": "Using the heuristics and pre-processing mentioned above, we have generated a data set that we call WikiSem5004. Our data set is formatted as a set of files containing test groups consisting of a cluster and a set of outliers. Test cases can be constructed by taking any outlier of a group with the cluster of that group. Table 1 shows the number of test groups and test cases included for each language. Each group contains a cluster of 7-8 units and up to two units from each of the three outlier classes. Table 2 shows sample clusters from the data set."}, {"heading": "4 EVALUATION", "text": "To clarify, we will first repeat the definitions of the scoring metrics defined by Camacho-Collados & Navigli (2016) with respect to test groups (as opposed to the original definition defined in the form of test cases).The way extra-vocabulary units are handled and results reported makes this distinction significant, as in Section 4.3.3 for Chinese and Japanese units is modified so that the first or last of these units must have identical (non-kana) first or last characters or more than three identical first or last characters. (Since English is not inflexible, we simply use blanks as approximate word boundaries and check that the first or last of these units do not occur too often. (4The dataset is available for download at https: / / github.com / belph / wiki-sem-500The core measure during evaluation is known as the compaccompactness score; given a set W of it, words is: http: / githsem / githki-500ki-we /)."}, {"heading": "4.1 HANDLING OUT-OF-VOCABULARY WORDS", "text": "One thing that Camacho-Collados & Navigli (2016) does not address is the use of non-vocabulary vocabulary (OOV) terms. As our data set is much larger and contains a greater variety of words, we have expanded their work to include additional evaluation provisions that better include the performance of vocabulary sentences trained on different bodies. There are two approaches to dealing with non-vocabulary vocabulary units: use a vector to represent all of these units or discard them completely. The first approach is simpler but has a number of disadvantages; for example, a poor selection of vocabulary units can have a drastic effect on results: for example, an implementation that uses the zero vector as a guard and defines sim (~ x, ~ 0) = 0- ~ x when many non-vocabulary outliers have the same disadvantage in a large number of vector spaces, because we have found negative compact values to be rare."}, {"heading": "4.2 HUMAN BASELINE", "text": "To assess how well embedding in our dataset should work, we performed a human assessment, asking participants to select the outlier from a given test case, giving us a human baseline for accuracy assessment in the dataset. We calculated the non-vocabulary intersection of the emulators shown in Table 4, from which 60 test groups were sampled. Due to the wide range of domain knowledge required to perform well in the dataset, participants were allowed to refer to Wikipedia (but explicitly asked not to use Wikidata), and we collected 447 responses with an overall accuracy of 68.9%. The performance identified is not as high as the baseline described in Camacho-Collados & Navigli (2016), so we performed a second human assessment on a smaller, hand-picked dataset to determine whether a lack of domain knowledge or a systemic problem with our method was responsible."}, {"heading": "4.3 EMBEDDING RESULTS", "text": "We evaluated our datasets on a number of publicly available vector embeddings (Mikolov et al.) that we have in recent years in the USA and other EU countries. (2016), which are based on a combination of the results of the Council of Europe (Koehn, 2005) and the EU (Quasthoff et al., 2006). In the world of vectors, we are word2vec CBOW vectors, and the non-English vectors rely on a combination of the English vector space. Reproduction of the original (unaligned) non-English vectors results in near identical results to the aligned vectors s.dition that we are trained vectors."}, {"heading": "4.4 CORRELATION WITH DOWNSTREAM PERFORMANCE", "text": "In light of recent concerns about the correlation between intrinsic word embedding and performance in downstream tasks, we attempted to investigate the correlation between WikiSem500 performance and extrinsic ratings. We used the embedding of Schnabel et al. (2015) and performed the outlier detection task on them with our datasets. As a baseline measure of how well our dataset correlates with the performance of alternative intrinsic tasks, we compared our assessment with that of Schnabel et al. (2015) on the known analog task (Mikolov et al., 2013a), we established strong correlations between analog task performance and our evaluation of OPP values and accuracy. Figure 3b shows the correlation between the performance of each analog task and extrinsic data collection."}, {"heading": "5 FUTURE WORK", "text": "Based on the positive results we have seen from the WikiSem500 dataset, we intend to release test groups in more languages using the method described in this paper. Furthermore, we plan to further investigate the downstream correlation between performance on our dataset and additional downstream tasks. Although we note a significant correlation between performance on our dataset and a semantically based extrinsic task, the relationship between performance and syntactically based tasks leaves much to be desired. We believe that the approach used in this paper to create our dataset in a system such as WordNet (2010) or Wiktionary (2016) (for multilingual data) could be retrofitted to construct syntactically similar clusters of items in a similar way. We expect that performance on such a dataset would correlate much more strongly with syntactically based extrinsic ratings such as chunking and some of language agging."}, {"heading": "6 CONCLUSION", "text": "We have described a language agnostic technique for creating a dataset that consists of semantically related items by treating a knowledge base as a graph. In addition, we have used this approach to create the WikiSem500 dataset we publish. We show that the performance of this dataset is strongly correlated with downstream performance in mood analysis. This method allows for creating datasets on a much larger scale in a wider variety of languages without the time-consuming task of human creation. In addition, the parallels between Wikidata's graph structure and Camacho-Collados & Navigli's annotation guidelines (2016) maintain the easy-to-understand structure of the original dataset."}, {"heading": "A FORMALIZATION", "text": "We provide now a formal description of the approach to generating our Dataset.Let V is the set of entities in Wikidata (=). For all v1, v2, V, V, V, V, V, V, V, V, V, V, V, V, V, V, V, V, V, V, V, V, V, V, V, V, V, V, V, V, V, V, V, V, V, V, V, V, V, V, V, V, V, V, V, V, V, V, V, V, V, V, V, V, V, V, V, V, V, V, V, V, V, V, V, V, V, V, V, V, V, V, V, V, V, V, V, V, V, V, V, V, V, V, V, V, V, V, V, V, V, V, V, V, V, V, V, V, V, V, V, V, V, V, V, V, V, V, V, V, V, V, V, V, V, V, V, V, V, V, V, V, V, V, V, V, V, V, V, V, V, V, V, V, V, V, V, V, V, V, V, V, V, V, V, V, V, V, V, V, V, V, V, V, V, V, V, V, V, V, V, V, V, V, V, V, V, V, V, V, V, V, V, V, V, V, V, V, V, V, V, V, V, V, V, V, V, V, V, V, V, V, V, V, V, V, V, V, V, V, V, V, V, V, V, V, V, V, V, V, V, V, V, V, V, V, V, V, V, V, V, V, V, V, V, V, V, V, V, V, V, V, V, V, V, V, V,"}], "references": [], "referenceMentions": [], "year": 2017, "abstractText": "We propose a language-agnostic way of automatically generating sets of semantically similar clusters of entities along with sets of \u201coutlier\u201d elements, which may then be used to perform an intrinsic evaluation of word embeddings in the outlier detection task. We used our methodology to create a gold-standard dataset, which we call WikiSem500, and evaluated multiple state-of-the-art embeddings. The results show a correlation between performance on this dataset and performance on sentiment analysis.", "creator": "LaTeX with hyperref package"}}}