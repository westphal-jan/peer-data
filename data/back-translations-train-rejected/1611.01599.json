{"id": "1611.01599", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Nov-2016", "title": "LipNet: End-to-End Sentence-level Lipreading", "abstract": "Lipreading is the task of decoding text from the movement of a speaker's mouth. Traditional approaches separated the problem into two stages: designing or learning visual features, and prediction. More recent deep lipreading approaches are end-to-end trainable (Wand et al., 2016; Chung &amp; Zisserman, 2016a). All existing works, however, perform only word classification, not sentence-level sequence prediction. Studies have shown that human lipreading performance increases for longer words (Easton &amp; Basala, 1982), indicating the importance of features capturing temporal context in an ambiguous communication channel. Motivated by this observation, we present LipNet, a model that maps a variable-length sequence of video frames to text, making use of spatiotemporal convolutions, an LSTM recurrent network, and the connectionist temporal classification loss, trained entirely end-to-end. To the best of our knowledge, LipNet is the first lipreading model to operate at sentence-level, using a single end-to-end speaker-independent deep model to simultaneously learn spatiotemporal visual features and a sequence model. On the GRID corpus, LipNet achieves 93.4% accuracy, outperforming experienced human lipreaders and the previous 79.6% state-of-the-art accuracy.", "histories": [["v1", "Sat, 5 Nov 2016 04:05:18 GMT  (3950kb,D)", "http://arxiv.org/abs/1611.01599v1", null], ["v2", "Fri, 16 Dec 2016 16:09:34 GMT  (1926kb,D)", "http://arxiv.org/abs/1611.01599v2", null]], "reviews": [], "SUBJECTS": "cs.LG cs.CL cs.CV", "authors": ["yannis m assael", "brendan shillingford", "shimon whiteson", "nando de freitas"], "accepted": false, "id": "1611.01599"}, "pdf": {"name": "1611.01599.pdf", "metadata": {"source": "CRF", "title": "LIPNET: SENTENCE-LEVEL LIPREADING", "authors": ["Yannis M. Assael", "Brendan Shillingford", "Shimon Whiteson"], "emails": ["shimon.whiteson@cs.ox.ac.uk", "nando.de.freitas@cs.ox.ac.uk"], "sections": [{"heading": "1 INTRODUCTION", "text": "In fact, it is true that most people who are able to survive themselves, to survive and to survive, are able to survive themselves. \"(S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S."}, {"heading": "2 RELATED WORK", "text": "This year, it is only a matter of time before an agreement is reached."}, {"heading": "3 LIPNET", "text": "LipNet is a neural network architecture for lipreading, which maps sequences of video images of different lengths to text sequences and is continuously trained. In this section we describe the building blocks and architecture of LipNet."}, {"heading": "3.1 SPATIOTEMPORAL CONVOLUTIONS", "text": "A basic 2D folding layer from C channels to C channels (without bias and with step speed) is calculated [conv (x, w)] c \u00b2 ij = C \u00b2 c = 1 kw \u00b2 i \u00b2 = 1 kh \u00b2 j \u00b2 = 1 wc \u00b2 j \u00b2 xc, i + i \u00b2, j + j \u00b2, for input x and weighting w \u00b2 RC \u00b2 \u00b7 C \u00b7 kw \u00d7 kh, where we define xcij = 0 for i, j outside the limits. Spatial-emporal Convolutionary Neural Networks (STCNNs) can process video data by interweaving over time, as well as the spatial dimensions (Karpathy et al., 2014; Ji et al., 2013)."}, {"heading": "3.2 LONG SHORT-TERM MEMORY", "text": "Long Short-Term Memory (LSTM) (Hochreiter & Schmidhuber, 1997) is a kind of recursive neural network (RNN) that improves previous RNNs by adding cells and gates to distribute information about further time steps and learning to control this information flow. We use the standard LSTM formulation with oblivion gates: [\"it,\" f \"t, o\" t, g \"t\" T = Wxzt + Whht \u2212 1 + bit = sigm (\"it\") ft = sigm (\"it\") ot = sigm (o \"t) gt = tanh (g\" t) ct = ft \"ct \u2212 1 + it\" t \"tanh\" (ct), where z: \"z1,\" zT \"is the input sequence to the LSTM\" ot = sigm (\"it\" t \") ot = sigm\" (o \"t\") ot \"t,\" t \"t.\""}, {"heading": "3.3 CONNECTIONIST TEMPORAL CLASSIFICATION", "text": "Connectionist Time Classification (CTC) loss (Graves et al., 2006) is commonly used in modern speech recognition, as it eliminates the need for training data to align input with target outcomes (Amodei et al., 2015; Graves & Jaitly, 2014; Maas et al., 2015). Faced with a model that outputs a sequence of discrete distributions across the token classes (vocabulary), supplemented by a special \"blank\" token, CTC calculates the probability of a sequence by marginalizing all sequences defined as equivalent to that sequence, eliminating the need for alignments and addresses of variable length sequences. Let V denote the set of tokens that classify the model in a single time step of its output (vocabulary), and the blank-augmented vocabulary p = V {}, where the CTC Blank symbol stands, move over the string of characters (the string of characters)."}, {"heading": "3.4 LIPNET ARCHITECTURE", "text": "Figure 1 illustrates the LipNet architecture, which begins with 3 x (spatio-temporal turns, channel-by-channel drop-out, spatial max-pooling), followed by up-sampling in the time dimension. Since it is known that people pronounce about 7 phonemes per second, and since LipNet works at character level, we concluded that the output of 25 tokens per second (the average frame rate of the video) is too limited for CTC. Temporary up-sampling allows for a greater distance between character outputs. This problem is exacerbated when many words have identical consecutive characters, as a CTC space is required between them. Subsequently, the time-up sampling is followed by a bi-LSTM. The bi-LSTM is crucial for efficient further aggregation of the STCNN output. Finally, a feed-forward network is applied at each time step, followed by a soft-up sampling via the vocabulary with STLTC."}, {"heading": "4 LIPREADING EVALUATION", "text": "In this section we evaluate LipNet on the GRID corpus."}, {"heading": "4.1 DATA AUGMENTATION", "text": "Pre-processing: The GRID corpus consists of 34 subjects, each of whom tells 1000 sentences; the videos for speaker 21 are missing, and some others are empty or corrupt, leaving 32839 usable videos; we use data from two male speakers (1 and 2) and two female speakers (20 and 22) for evaluation (3986 videos) and the rest for training (28853 videos); all videos are 3 seconds long at a frame rate of 25 frames per second; the videos were processed with the DLib face detector and the iBug face shape predictor with 68 symbols; using these symbols, we use an affine transformation to extract a bite-centered cutout of the size of 100 x 50 pixels per frame; we standardize the RGB channels across the entire training set to have zero mean and uniform variance.Augmentation: We extend the data set to include simple transformations to reduce the overpass times per word, which results in 15.6 training sessions per word."}, {"heading": "4.2 BASELINES", "text": "To evaluate LipNet, we compare its performance with that of three hearing-impaired people who can lipread, and with two ablation models inspired by more recent cutting-edge work (Chung & Zisserman, 2016a; Wand et al., 2016).Hearing-impaired people: This baseline was conducted by three members of the Oxford Students' Disability Community. After being introduced to the grammar of the GRID corpus, they watched commented videos from the training data set for 10 minutes, then commented on 300 random videos from the assessment data set. If they were unsure, they were asked to choose the most likely answer.Baseline-LSTM: Using the sentence-level LipNet corpus, we replicate the model architecture of the previous GRID corpus State of the Art (Wand et al., 2016). See Appendix A for further implementation details. Baseline-2D: Based on the NET-NET architecture, we replace the NET architecture and ST6- respectively, see Appendix A for further implementation details."}, {"heading": "4.3 PERFORMANCE EVALUATION", "text": "To measure the performance of LipNet and the baselines, we calculate the word error rate (WHO) and the character error rate (CER), standard measures of the performance of ASR models. We produce approximate maximum probability predictions of LipNet by performing CTC beam searches. WHO (or CER) is defined as the minimum number of word insertions, substitutions and deletions required to transform the prediction into the basic truth, divided by the number of words (or characters) in the basic truth. Note that WHO is usually the same when the predicted sentence has the same number of words as the basic truth, especially in our case, as almost all errors are substitution errors. Table 2 summarizes the performance of LipNet compared to the baselines. According to the literature, the accuracy of human preachers is greater than that of WIpreaders (Easton & Basala, 1982; al, 2009)."}, {"heading": "4.4 LEARNED REPRESENTATIONS", "text": "In this section, we analyze the learned representations of LipNet from a phonological perspective. First, we create emphasis visualizations (Simonyan et al., 2013; Zeiler & Fergus, 2014) to illustrate where LipNet has learned to work. Specifically, we feed an input into the model and greedily decipher an output sequence, which results in a CTC alignment u \u0109V (following the notation of sections 3.2 and 3.3). Then, we calculate the gradient of \u0445t p (u \u0441t | x) in relation to the input sequence of video images, but unlike Simonyan et al. (2013), we use guided reverse propagation (Springenberg et al., 2014). Secondly, we train LipNet to predict ARPAbet phonemes in order to analyze visual phonemes using intra-visems and inter-viseme confusion matrices."}, {"heading": "4.4.1 SALIENCY MAPS", "text": "We use highlighting techniques to interpret the learned behavior of LipNet, which show that the model takes into account phonologically important regions in the video. In particular, in Figure 2, we analyze two highlighting visualizations for the words please and lay for speaker 25, based on Ashby (2013). Producing the word requires a large articulation movement at the beginning: the lips are tightly compressed to allow the bilabial plosive / p / (frame 1). Simultaneously, the tongue blade comes into contact with the alveolar crest in anticipation of the following lateral / l /, the lips then divide, allowing the compressed air to escape between the lips (frame 2). Jaw and lips then open wider, seen at a distance between the midpoints of the upper and lower lips, and the lips are distributed (increasing the distance between the corners of the mouth), for the narrow vowel / iy / (frame 3-4)."}, {"heading": "4.4.2 VISEMES", "text": "According to DeLand (1931) and Fisher (1968), Alexander Graham Bell first hypothesized that several phonemes on a given loudspeaker could be visually identical, which was later confirmed, leading to the concept of a visem, a visual equivalent of a phoneme (Woodward & Barber, 1960; Fisher, 1968). For our analysis, we use the phoneme viseme mapping by Neti et al. (2000), which classifies the phonemes into the following categories: lip-rounding based vowels (V), alveolar semivowels (A), alveolar fricatives (B), alveolar fricatives (C), palato-alveolar (E), dental (F), labio-dental (G) and velar (H).The complete mapping can be found in Table 4 in Netendix."}, {"heading": "5 CONCLUSIONS", "text": "We have proposed LipNet, the first model to apply deep learning to the end-to-end learning of a model that maps sequences of sequences of a speaker's mouth into whole sentences; the end-to-end model eliminates the need to segment videos into words before predicting a sentence; LipNet does not require handmade spatio-temporal visual features or a separately trained sequence model; our empirical evaluation demonstrates the importance of spatio-temporal feature extraction and efficient temporal aggregation, confirming the intuition of Easton & Basala (1982); and LipNet far exceeds a human lipreading baseline, showing 7.2 x better performance and 6.6% lower WHO than the state of the art at word level (Wand et al., 2016)."}, {"heading": "ACKNOWLEDGMENTS", "text": "This work was supported by an Oxford-Google DeepMind Graduate Scholarship, the EPSRC and CIFAR. We would also like to thank: NVIDIA for their generous donation of DGX-1 and GTX Titan X GPUs used in our experiments; A'ine Jackson, Brittany Klug and Samantha Pugh for helping to measure the experienced Lipreader baseline; Mitko Sabev for his phonetic guidance; Odysseas Votsis for his help with video production; and Alex Graves and Oiwi Parker Jones for helpful comments."}, {"heading": "A ARCHITECTURE DETAILS", "text": "A.1 IMPLEMENTATIONLipNet is implemented with Torch, the Warp-ctc CTC library (Amodei et al., 2015), and the decoder implementation of StanfordCTC. Network parameters were initialized with Xavier initialization (Glorot & Bengio, 2010), and the LSTM Forge-Gate biases were initialized with 3 unless otherwise specified. Models were initialized with channel-by-channel dropout (dropout rate p = 0.2) after each pool layer and mini-batches of size 50. We used the optimizer Adam (Kingma & Ba, 2014) with a learning rate of 10 \u2212 4 and the standard hyperparameters: a first moment dynamic coefficient of 0.9, a second moment dynamic coefficient of 0.999, and the numerical stability parameter of 0.0."}, {"heading": "B PHONEMES AND VISEMES", "text": "Table 4 shows the phoneme-to-visa cluster of Neti et al. (2000) and Figure 4 shows the complete phoneme confusion matrix of LipNet."}], "references": [{"title": "Improved speaker independent lip reading using speaker adaptive training and deep neural networks", "author": ["I. Almajai", "S. Cox", "R. Harvey", "Y. Lan"], "venue": "In IEEE International Conference on Acoustics, Speech and Signal Processing,", "citeRegEx": "Almajai et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Almajai et al\\.", "year": 2016}, {"title": "Deep Speech 2: End-to-end speech recognition in English and Mandarin", "author": ["D. Amodei", "R. Anubhai", "E. Battenberg", "C. Case", "J. Casper", "B. Catanzaro", "J. Chen", "M. Chrzanowski", "A. Coates", "G. Diamos"], "venue": "arXiv preprint arXiv:1512.02595,", "citeRegEx": "Amodei et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Amodei et al\\.", "year": 2015}, {"title": "Understanding phonetics", "author": ["P. Ashby"], "venue": null, "citeRegEx": "Ashby.,? \\Q2013\\E", "shortCiteRegEx": "Ashby.", "year": 2013}, {"title": "Lip reading in the wild", "author": ["J.S. Chung", "A. Zisserman"], "venue": "In Asian Conference on Computer Vision,", "citeRegEx": "Chung and Zisserman.,? \\Q2016\\E", "shortCiteRegEx": "Chung and Zisserman.", "year": 2016}, {"title": "Out of time: automated lip sync in the wild", "author": ["J.S. Chung", "A. Zisserman"], "venue": "In Workshop on Multi-view Lip-reading,", "citeRegEx": "Chung and Zisserman.,? \\Q2016\\E", "shortCiteRegEx": "Chung and Zisserman.", "year": 2016}, {"title": "An audio-visual corpus for speech perception and automatic speech recognition", "author": ["M. Cooke", "J. Barker", "S. Cunningham", "X. Shao"], "venue": "The Journal of the Acoustical Society of America,", "citeRegEx": "Cooke et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Cooke et al\\.", "year": 2006}, {"title": "Gimson\u2019s pronunciation of English", "author": ["A. Cruttenden"], "venue": null, "citeRegEx": "Cruttenden.,? \\Q2014\\E", "shortCiteRegEx": "Cruttenden.", "year": 2014}, {"title": "Context-dependent pre-trained deep neural networks for largevocabulary speech recognition", "author": ["G.E. Dahl", "D. Yu", "L. Deng", "A. Acero"], "venue": "IEEE Transactions on Audio, Speech, and Language Processing,", "citeRegEx": "Dahl et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Dahl et al\\.", "year": 2012}, {"title": "The story of lip-reading, its genesis and development", "author": ["F. DeLand"], "venue": null, "citeRegEx": "DeLand.,? \\Q1931\\E", "shortCiteRegEx": "DeLand.", "year": 1931}, {"title": "Perceptual dominance during lipreading", "author": ["R.D. Easton", "M. Basala"], "venue": "Perception & Psychophysics,", "citeRegEx": "Easton and Basala.,? \\Q1982\\E", "shortCiteRegEx": "Easton and Basala.", "year": 1982}, {"title": "Formant frequencies of vowels in 13 accents of the british isles", "author": ["E. Ferragne", "F. Pellegrino"], "venue": "Journal of the International Phonetic Association,", "citeRegEx": "Ferragne and Pellegrino.,? \\Q2010\\E", "shortCiteRegEx": "Ferragne and Pellegrino.", "year": 2010}, {"title": "Confusions among visually perceived consonants", "author": ["C.G. Fisher"], "venue": "Journal of Speech, Language, and Hearing Research,", "citeRegEx": "Fisher.,? \\Q1968\\E", "shortCiteRegEx": "Fisher.", "year": 1968}, {"title": "Classification and feature extraction by simplexization", "author": ["Y. Fu", "S. Yan", "T.S. Huang"], "venue": "IEEE Transactions on Information Forensics and Security,", "citeRegEx": "Fu et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Fu et al\\.", "year": 2008}, {"title": "Lip reading using CNN and LSTM", "author": ["A. Garg", "J. Noyola", "S. Bagadia"], "venue": "Technical report,", "citeRegEx": "Garg et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Garg et al\\.", "year": 2016}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["X. Glorot", "Y. Bengio"], "venue": "In Aistats,", "citeRegEx": "Glorot and Bengio.,? \\Q2010\\E", "shortCiteRegEx": "Glorot and Bengio.", "year": 2010}, {"title": "Towards end-to-end speech recognition with recurrent neural networks", "author": ["A. Graves", "N. Jaitly"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Graves and Jaitly.,? \\Q2014\\E", "shortCiteRegEx": "Graves and Jaitly.", "year": 2014}, {"title": "Framewise phoneme classification with bidirectional LSTM and other neural network architectures", "author": ["A. Graves", "J. Schmidhuber"], "venue": "Neural Networks,", "citeRegEx": "Graves and Schmidhuber.,? \\Q2005\\E", "shortCiteRegEx": "Graves and Schmidhuber.", "year": 2005}, {"title": "Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks", "author": ["A. Graves", "S. Fern\u00e1ndez", "F. Gomez", "J. Schmidhuber"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Graves et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2006}, {"title": "Information theoretic feature extraction for audio-visual speech recognition", "author": ["M. Gurban", "J.-P. Thiran"], "venue": "IEEE Transactions on Signal Processing,", "citeRegEx": "Gurban and Thiran.,? \\Q2009\\E", "shortCiteRegEx": "Gurban and Thiran.", "year": 2009}, {"title": "Comparison of human and machine-based lip-reading", "author": ["S. Hilder", "R. Harvey", "B.-J. Theobald"], "venue": "In AVSP, pp", "citeRegEx": "Hilder et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Hilder et al\\.", "year": 2009}, {"title": "Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups", "author": ["G. Hinton", "L. Deng", "D. Yu", "G.E. Dahl", "A.-r. Mohamed", "N. Jaitly", "A. Senior", "V. Vanhoucke", "P. Nguyen", "T.N. Sainath"], "venue": "IEEE Signal Processing Magazine,", "citeRegEx": "Hinton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2012}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "Hochreiter and Schmidhuber.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "3d convolutional neural networks for human action recognition", "author": ["S. Ji", "W. Xu", "M. Yang", "K. Yu"], "venue": "IEEE transactions on pattern analysis and machine intelligence,", "citeRegEx": "Ji et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Ji et al\\.", "year": 2013}, {"title": "Large-scale video classification with convolutional neural networks", "author": ["A. Karpathy", "G. Toderici", "S. Shetty", "T. Leung", "R. Sukthankar", "L. Fei-Fei"], "venue": "In Proceedings of the IEEE conference on Computer Vision and Pattern Recognition,", "citeRegEx": "Karpathy et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Karpathy et al\\.", "year": 2014}, {"title": "Dlib-ml: A machine learning toolkit", "author": ["D.E. King"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "King.,? \\Q2009\\E", "shortCiteRegEx": "King.", "year": 2009}, {"title": "Adam: A method for stochastic optimization", "author": ["D. Kingma", "J. Ba"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "Kingma and Ba.,? \\Q2014\\E", "shortCiteRegEx": "Kingma and Ba.", "year": 2014}, {"title": "Deep learning of mouth shapes for sign language", "author": ["O. Koller", "H. Ney", "R. Bowden"], "venue": "In ICCV Workshop on Assistive Computer Vision and Robotics,", "citeRegEx": "Koller et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Koller et al\\.", "year": 2015}, {"title": "Imagenet classification with deep convolutional neural networks. In Advances in neural information processing", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": null, "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Patch-based representation of visual speech", "author": ["P. Lucey", "S. Sridharan"], "venue": "In Proceedings of the HCSNet workshop on Use of vision in human-computer interaction,", "citeRegEx": "Lucey and Sridharan.,? \\Q2006\\E", "shortCiteRegEx": "Lucey and Sridharan.", "year": 2006}, {"title": "Lexicon-free conversational speech recognition with neural networks", "author": ["A.L. Maas", "Z. Xie", "D. Jurafsky", "A.Y. Ng"], "venue": "In NAACL,", "citeRegEx": "Maas et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Maas et al\\.", "year": 2015}, {"title": "Extraction of visual features for lipreading", "author": ["I. Matthews", "T.F. Cootes", "J.A. Bangham", "S. Cox", "R. Harvey"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "Matthews et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Matthews et al\\.", "year": 2002}, {"title": "Hearing lips and seeing", "author": ["H. McGurk", "J. MacDonald"], "venue": "voices. Nature,", "citeRegEx": "McGurk and MacDonald.,? \\Q1976\\E", "shortCiteRegEx": "McGurk and MacDonald.", "year": 1976}, {"title": "Audio visual speech recognition", "author": ["C. Neti", "G. Potamianos", "J. Luettin", "I. Matthews", "H. Glotin", "D. Vergyri", "J. Sison", "A. Mashari"], "venue": "Technical report,", "citeRegEx": "Neti et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Neti et al\\.", "year": 2000}, {"title": "Multimodal deep learning", "author": ["J. Ngiam", "A. Khosla", "M. Kim", "J. Nam", "H. Lee", "A.Y. Ng"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Ngiam et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Ngiam et al\\.", "year": 2011}, {"title": "Lipreading using convolutional neural network", "author": ["K. Noda", "Y. Yamaguchi", "K. Nakadai", "H.G. Okuno", "T. Ogata"], "venue": "In INTERSPEECH,", "citeRegEx": "Noda et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Noda et al\\.", "year": 2014}, {"title": "Multimodal fusion and learning with uncertain features applied to audiovisual speech recognition", "author": ["G. Papandreou", "A. Katsamanis", "V. Pitsikalis", "P. Maragos"], "venue": "In Workshop on Multimedia Signal Processing,", "citeRegEx": "Papandreou et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Papandreou et al\\.", "year": 2007}, {"title": "Adaptive multimodal fusion by uncertainty compensation with application to audiovisual speech recognition", "author": ["G. Papandreou", "A. Katsamanis", "V. Pitsikalis", "P. Maragos"], "venue": "IEEE Transactions on Audio, Speech, and Language Processing,", "citeRegEx": "Papandreou et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Papandreou et al\\.", "year": 2009}, {"title": "Adaptive multimodal fusion by uncertainty compensation", "author": ["V. Pitsikalis", "A. Katsamanis", "G. Papandreou", "P. Maragos"], "venue": "In INTERSPEECH,", "citeRegEx": "Pitsikalis et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Pitsikalis et al\\.", "year": 2006}, {"title": "300 faces in-the-wild challenge: The first facial landmark localization challenge", "author": ["C. Sagonas", "G. Tzimiropoulos", "S. Zafeiriou", "M. Pantic"], "venue": "In Proceedings of the IEEE International Conference on Computer Vision Workshops,", "citeRegEx": "Sagonas et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Sagonas et al\\.", "year": 2013}, {"title": "Deep inside convolutional networks: Visualising image classification models and saliency maps", "author": ["K. Simonyan", "A. Vedaldi", "A. Zisserman"], "venue": "arXiv preprint arXiv:1312.6034,", "citeRegEx": "Simonyan et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Simonyan et al\\.", "year": 2013}, {"title": "Striving for simplicity: The all convolutional net", "author": ["J.T. Springenberg", "A. Dosovitskiy", "T. Brox", "M. Riedmiller"], "venue": "In ICLR Workshop,", "citeRegEx": "Springenberg et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Springenberg et al\\.", "year": 2014}, {"title": "Audio-visual speech recognition using bimodal-trained bottleneck features for a person with severe hearing", "author": ["Y. Takashima", "R. Aihara", "T. Takiguchi", "Y. Ariki", "N. Mitani", "K. Omori", "K. Nakazono"], "venue": "loss. INTERSPEECH,", "citeRegEx": "Takashima et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Takashima et al\\.", "year": 2016}, {"title": "Lipreading with long short-term memory", "author": ["M. Wand", "J. Koutnik", "J. Schmidhuber"], "venue": "In IEEE International Conference on Acoustics, Speech and Signal Processing,", "citeRegEx": "Wand et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Wand et al\\.", "year": 2016}, {"title": "Phoneme perception in lipreading", "author": ["M.F. Woodward", "C.G. Barber"], "venue": "Journal of Speech, Language, and Hearing Research,", "citeRegEx": "Woodward and Barber.,? \\Q1960\\E", "shortCiteRegEx": "Woodward and Barber.", "year": 1960}, {"title": "Visualizing and understanding convolutional networks", "author": ["M.D. Zeiler", "R. Fergus"], "venue": "In European Conference on Computer Vision, pp", "citeRegEx": "Zeiler and Fergus.,? \\Q2014\\E", "shortCiteRegEx": "Zeiler and Fergus.", "year": 2014}, {"title": "Lipreading with local spatiotemporal descriptors", "author": ["G. Zhao", "M. Barnard", "M. Pietikainen"], "venue": "IEEE Transactions on Multimedia,", "citeRegEx": "Zhao et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Zhao et al\\.", "year": 2009}, {"title": "A review of recent advances in visual speech decoding", "author": ["Z. Zhou", "G. Zhao", "X. Hong", "M. Pietik\u00e4inen"], "venue": "Image and Vision Computing,", "citeRegEx": "Zhou et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zhou et al\\.", "year": 2014}, {"title": "B PHONEMES AND VISEMES Table 4 shows the phoneme to viseme clustering of Neti et al. (2000) and Figure 4 shows LipNet\u2019s full phoneme confusion matrix. Table 4: Phoneme to viseme clustering", "author": ["Neti"], "venue": null, "citeRegEx": "Neti,? \\Q2000\\E", "shortCiteRegEx": "Neti", "year": 2000}], "referenceMentions": [{"referenceID": 42, "context": "More recent deep lipreading approaches are end-to-end trainable (Wand et al., 2016; Chung & Zisserman, 2016a).", "startOffset": 64, "endOffset": 109}, {"referenceID": 11, "context": "1 Most lipreading actuations, besides the lips and sometimes tongue and teeth, are latent and difficult to disambiguate without context (Fisher, 1968; Woodward & Barber, 1960).", "startOffset": 136, "endOffset": 175}, {"referenceID": 11, "context": "1 Most lipreading actuations, besides the lips and sometimes tongue and teeth, are latent and difficult to disambiguate without context (Fisher, 1968; Woodward & Barber, 1960). For example, Fisher (1968) gives 5 categories of visual phonemes (called visemes), out of a list of 23 initial consonant phonemes, that are commonly confused by people when viewing a speaker\u2019s mouth.", "startOffset": 137, "endOffset": 204}, {"referenceID": 5, "context": "Our empirical results on the GRID corpus (Cooke et al., 2006), one of the only public sentence-level datasets, show that LipNet attains a 93.", "startOffset": 41, "endOffset": 61}, {"referenceID": 42, "context": "6% (Wand et al., 2016).", "startOffset": 3, "endOffset": 22}, {"referenceID": 39, "context": "Finally, by applying saliency visualisation techniques (Zeiler & Fergus, 2014; Simonyan et al., 2013), we interpret LipNet\u2019s learned behaviour, showing that the model attends to phonologically important regions in the video.", "startOffset": 55, "endOffset": 101}, {"referenceID": 30, "context": ", optical flow or movement detection), or other types of handcrafted vision pipelines (Matthews et al., 2002; Zhao et al., 2009; Gurban & Thiran, 2009; Papandreou et al., 2007; 2009; Pitsikalis et al., 2006; Lucey & Sridharan, 2006; Papandreou et al., 2009).", "startOffset": 86, "endOffset": 257}, {"referenceID": 45, "context": ", optical flow or movement detection), or other types of handcrafted vision pipelines (Matthews et al., 2002; Zhao et al., 2009; Gurban & Thiran, 2009; Papandreou et al., 2007; 2009; Pitsikalis et al., 2006; Lucey & Sridharan, 2006; Papandreou et al., 2009).", "startOffset": 86, "endOffset": 257}, {"referenceID": 35, "context": ", optical flow or movement detection), or other types of handcrafted vision pipelines (Matthews et al., 2002; Zhao et al., 2009; Gurban & Thiran, 2009; Papandreou et al., 2007; 2009; Pitsikalis et al., 2006; Lucey & Sridharan, 2006; Papandreou et al., 2009).", "startOffset": 86, "endOffset": 257}, {"referenceID": 37, "context": ", optical flow or movement detection), or other types of handcrafted vision pipelines (Matthews et al., 2002; Zhao et al., 2009; Gurban & Thiran, 2009; Papandreou et al., 2007; 2009; Pitsikalis et al., 2006; Lucey & Sridharan, 2006; Papandreou et al., 2009).", "startOffset": 86, "endOffset": 257}, {"referenceID": 36, "context": ", optical flow or movement detection), or other types of handcrafted vision pipelines (Matthews et al., 2002; Zhao et al., 2009; Gurban & Thiran, 2009; Papandreou et al., 2007; 2009; Pitsikalis et al., 2006; Lucey & Sridharan, 2006; Papandreou et al., 2009).", "startOffset": 86, "endOffset": 257}, {"referenceID": 46, "context": "Generalisation across speakers and extraction of motion features is considered an open problem, as noted in a recent review article (Zhou et al., 2014).", "startOffset": 132, "endOffset": 151}, {"referenceID": 33, "context": "Approaches include learning multimodal audio-visual representations (Ngiam et al., 2011), learning visual features as part of a traditional speech-style processing pipeline (e.", "startOffset": 68, "endOffset": 88}, {"referenceID": 0, "context": ") for classifying words and/or phonemes (Almajai et al., 2016; Takashima et al., 2016; Noda et al., 2014; Koller et al., 2015), or combinations thereof (Takashima et al.", "startOffset": 40, "endOffset": 126}, {"referenceID": 41, "context": ") for classifying words and/or phonemes (Almajai et al., 2016; Takashima et al., 2016; Noda et al., 2014; Koller et al., 2015), or combinations thereof (Takashima et al.", "startOffset": 40, "endOffset": 126}, {"referenceID": 34, "context": ") for classifying words and/or phonemes (Almajai et al., 2016; Takashima et al., 2016; Noda et al., 2014; Koller et al., 2015), or combinations thereof (Takashima et al.", "startOffset": 40, "endOffset": 126}, {"referenceID": 26, "context": ") for classifying words and/or phonemes (Almajai et al., 2016; Takashima et al., 2016; Noda et al., 2014; Koller et al., 2015), or combinations thereof (Takashima et al.", "startOffset": 40, "endOffset": 126}, {"referenceID": 41, "context": ", 2015), or combinations thereof (Takashima et al., 2016).", "startOffset": 33, "endOffset": 57}, {"referenceID": 20, "context": "Many of these approaches mirror early progress in applying neural networks for acoustic processing in speech recognition (Hinton et al., 2012).", "startOffset": 121, "endOffset": 142}, {"referenceID": 17, "context": "Sequence prediction in speech recognition: The field of automatic speech recognition (ASR) would not be in the state it is today without modern advances in deep learning, many of which have occurred in the context of ASR (Graves et al., 2006; Dahl et al., 2012; Hinton et al., 2012).", "startOffset": 221, "endOffset": 282}, {"referenceID": 7, "context": "Sequence prediction in speech recognition: The field of automatic speech recognition (ASR) would not be in the state it is today without modern advances in deep learning, many of which have occurred in the context of ASR (Graves et al., 2006; Dahl et al., 2012; Hinton et al., 2012).", "startOffset": 221, "endOffset": 282}, {"referenceID": 20, "context": "Sequence prediction in speech recognition: The field of automatic speech recognition (ASR) would not be in the state it is today without modern advances in deep learning, many of which have occurred in the context of ASR (Graves et al., 2006; Dahl et al., 2012; Hinton et al., 2012).", "startOffset": 221, "endOffset": 282}, {"referenceID": 29, "context": "(2006) drove the movement from deep learning as a component of ASR, to deep ASR systems trained end-to-end (Graves & Jaitly, 2014; Maas et al., 2015; Amodei et al., 2015).", "startOffset": 107, "endOffset": 170}, {"referenceID": 1, "context": "(2006) drove the movement from deep learning as a component of ASR, to deep ASR systems trained end-to-end (Graves & Jaitly, 2014; Maas et al., 2015; Amodei et al., 2015).", "startOffset": 107, "endOffset": 170}, {"referenceID": 46, "context": "Lipreading Datasets: Lipreading datasets (AVICar, AVLetters, AVLetters2, BBC TV, CUAVE, OuluVS1, OuluVS2) are plentiful (Zhou et al., 2014; Chung & Zisserman, 2016a), but most only contain single words or are too small.", "startOffset": 120, "endOffset": 165}, {"referenceID": 5, "context": "One exception is the GRID corpus (Cooke et al., 2006), which has audio and video recordings of 34 speakers who produced 1000 sentences each, for a total of 28 hours across 34000 sentences.", "startOffset": 33, "endOffset": 53}, {"referenceID": 5, "context": ", 2006; Dahl et al., 2012; Hinton et al., 2012). The connectionist temporal classification loss (CTC) of Graves et al. (2006) drove the movement from deep learning as a component of ASR, to deep ASR systems trained end-to-end (Graves & Jaitly, 2014; Maas et al.", "startOffset": 8, "endOffset": 126}, {"referenceID": 42, "context": "Although the GRID corpus contains entire sentences, Wand et al. (2016) consider only the simpler case of predicting isolated words.", "startOffset": 52, "endOffset": 71}, {"referenceID": 27, "context": "Convolutional neural networks (CNNs), containing stacked convolutions operating spatially over an image, have been instrumental in advancing performance in computer visions tasks such as object recognition that receive an image as input (Krizhevsky et al., 2012).", "startOffset": 237, "endOffset": 262}, {"referenceID": 23, "context": "Spatiotemporal convolutional neural networks (STCNNs) can process video data by convolving across time, as well as the spatial dimensions (Karpathy et al., 2014; Ji et al., 2013).", "startOffset": 138, "endOffset": 178}, {"referenceID": 22, "context": "Spatiotemporal convolutional neural networks (STCNNs) can process video data by convolving across time, as well as the spatial dimensions (Karpathy et al., 2014; Ji et al., 2013).", "startOffset": 138, "endOffset": 178}, {"referenceID": 17, "context": "The connectionist temporal classification (CTC) loss (Graves et al., 2006) is widely used in modern speech recognition as it eliminates the need for training data that aligns inputs to target outputs (Amodei et al.", "startOffset": 53, "endOffset": 74}, {"referenceID": 1, "context": ", 2006) is widely used in modern speech recognition as it eliminates the need for training data that aligns inputs to target outputs (Amodei et al., 2015; Graves & Jaitly, 2014; Maas et al., 2015).", "startOffset": 133, "endOffset": 196}, {"referenceID": 29, "context": ", 2006) is widely used in modern speech recognition as it eliminates the need for training data that aligns inputs to target outputs (Amodei et al., 2015; Graves & Jaitly, 2014; Maas et al., 2015).", "startOffset": 133, "endOffset": 196}, {"referenceID": 42, "context": "To evaluate LipNet, we compare its performance to that of three hearing-impaired people who can lipread, as well as two ablation models inspired by recent state-of-the-art work (Chung & Zisserman, 2016a; Wand et al., 2016).", "startOffset": 177, "endOffset": 222}, {"referenceID": 42, "context": "Baseline-LSTM: Using the sentence-level training setup of LipNet, we replicate the model architecture of the previous GRID corpus state of the art (Wand et al., 2016).", "startOffset": 147, "endOffset": 166}, {"referenceID": 42, "context": "Baseline-LSTM: Using the sentence-level training setup of LipNet, we replicate the model architecture of the previous GRID corpus state of the art (Wand et al., 2016). See Appendix A for more implementation details. Baseline-2D: Based on the LipNet architecture, we replace the STCNN with spatial-only convolutions similar to those of Chung & Zisserman (2016a). Notably, contrary to the results we observe with LipNet, Chung & Zisserman (2016a) report 14% and 31% poorer performance of their STCNNs compared to the 2D architectures in their two datasets.", "startOffset": 148, "endOffset": 361}, {"referenceID": 42, "context": "Baseline-LSTM: Using the sentence-level training setup of LipNet, we replicate the model architecture of the previous GRID corpus state of the art (Wand et al., 2016). See Appendix A for more implementation details. Baseline-2D: Based on the LipNet architecture, we replace the STCNN with spatial-only convolutions similar to those of Chung & Zisserman (2016a). Notably, contrary to the results we observe with LipNet, Chung & Zisserman (2016a) report 14% and 31% poorer performance of their STCNNs compared to the 2D architectures in their two datasets.", "startOffset": 148, "endOffset": 445}, {"referenceID": 19, "context": "According to the literature, the accuracy of human lipreaders is around 20% (Easton & Basala, 1982; Hilder et al., 2009).", "startOffset": 76, "endOffset": 120}, {"referenceID": 42, "context": "Interestingly, although Baseline-LSTM replicates the same architecture as Wand et al. (2016) but trains using CTC, our speaker-independent sentence-level baseline performs 1.", "startOffset": 74, "endOffset": 93}, {"referenceID": 39, "context": "First, we create saliency visualisations (Simonyan et al., 2013; Zeiler & Fergus, 2014) to illustrate where LipNet has learned to attend.", "startOffset": 41, "endOffset": 87}, {"referenceID": 40, "context": "(2013), we use guided backpropagation (Springenberg et al., 2014).", "startOffset": 38, "endOffset": 65}, {"referenceID": 39, "context": "First, we create saliency visualisations (Simonyan et al., 2013; Zeiler & Fergus, 2014) to illustrate where LipNet has learned to attend. In particular, we feed an input into the model and greedily decode an output sequence, yielding a CTC alignment \u00fb \u2208 \u1e7c \u2217 (following the notation of Sections 3.2 and 3.3). Then, we compute the gradient of \u2211 t p(\u00fbt|x) with respect to the input video frame sequence, but unlike Simonyan et al. (2013), we use guided backpropagation (Springenberg et al.", "startOffset": 42, "endOffset": 435}, {"referenceID": 2, "context": "In particular, in Figure 2 we analyse two saliency visualisations for the words please and lay for speaker 25, based on Ashby (2013).", "startOffset": 120, "endOffset": 133}, {"referenceID": 11, "context": "This was later verified, giving rise to the concept of a viseme, a visual equivalent of a phoneme (Woodward & Barber, 1960; Fisher, 1968).", "startOffset": 98, "endOffset": 137}, {"referenceID": 8, "context": "According to DeLand (1931) and Fisher (1968), Alexander Graham Bell first hypothesised that multiple phonemes may be visually identical on a given speaker.", "startOffset": 13, "endOffset": 27}, {"referenceID": 8, "context": "According to DeLand (1931) and Fisher (1968), Alexander Graham Bell first hypothesised that multiple phonemes may be visually identical on a given speaker.", "startOffset": 13, "endOffset": 45}, {"referenceID": 8, "context": "According to DeLand (1931) and Fisher (1968), Alexander Graham Bell first hypothesised that multiple phonemes may be visually identical on a given speaker. This was later verified, giving rise to the concept of a viseme, a visual equivalent of a phoneme (Woodward & Barber, 1960; Fisher, 1968). For our analysis, we use the phoneme-to-viseme mapping of Neti et al. (2000), clustering the phonemes into the following categories: Lip-rounding based vowels (V), Alveolar-semivowels (A), Alveolar-fricatives (B), Alveolar (C), Palato-alveolar (D), Bilabial (E), Dental (F), Labio-dental (G), and Velar (H).", "startOffset": 13, "endOffset": 372}, {"referenceID": 8, "context": "According to DeLand (1931) and Fisher (1968), Alexander Graham Bell first hypothesised that multiple phonemes may be visually identical on a given speaker. This was later verified, giving rise to the concept of a viseme, a visual equivalent of a phoneme (Woodward & Barber, 1960; Fisher, 1968). For our analysis, we use the phoneme-to-viseme mapping of Neti et al. (2000), clustering the phonemes into the following categories: Lip-rounding based vowels (V), Alveolar-semivowels (A), Alveolar-fricatives (B), Alveolar (C), Palato-alveolar (D), Bilabial (E), Dental (F), Labio-dental (G), and Velar (H). The full mapping can be found in Table 4 in Appendix A. The GRID corpus contain 31 out of the 39 phonemes in ARPAbet. We compute confusion matrices between phonemes and then group phonemes into viseme clusters, following Neti et al. (2000). Figure 3 shows the confusion matrices of the 3 most confused viseme categories, as well as the confusions between the viseme categories.", "startOffset": 13, "endOffset": 843}, {"referenceID": 6, "context": "private and watches (Cruttenden, 2014).", "startOffset": 20, "endOffset": 38}, {"referenceID": 32, "context": "Finally, the quality of the viseme categorisation of Neti et al. (2000) is confirmed by the fact that the matrix in Figure 3d is diagonal, with only minor confusion between alveolar (C) and palato-alvealoar (D) visemes.", "startOffset": 53, "endOffset": 72}, {"referenceID": 42, "context": "6% WER, 3\u00d7 lower than the word-level state-of-the-art (Wand et al., 2016) in the GRID dataset.", "startOffset": 54, "endOffset": 73}, {"referenceID": 1, "context": "While LipNet is already an empirical success, the deep speech recognition literature (Amodei et al., 2015) suggests that performance will only improve with more data.", "startOffset": 85, "endOffset": 106}, {"referenceID": 1, "context": "While LipNet is already an empirical success, the deep speech recognition literature (Amodei et al., 2015) suggests that performance will only improve with more data. In future work, we hope to demonstrate this by applying LipNet to larger datasets, such as a sentence-level variant of that collected by Chung & Zisserman (2016a).", "startOffset": 86, "endOffset": 330}], "year": 2016, "abstractText": "Lipreading is the task of decoding text from the movement of a speaker\u2019s mouth. Traditional approaches separated the problem into two stages: designing or learning visual features, and prediction. More recent deep lipreading approaches are end-to-end trainable (Wand et al., 2016; Chung & Zisserman, 2016a). All existing works, however, perform only word classification, not sentence-level sequence prediction. Studies have shown that human lipreading performance increases for longer words (Easton & Basala, 1982), indicating the importance of features capturing temporal context in an ambiguous communication channel. Motivated by this observation, we present LipNet, a model that maps a variable-length sequence of video frames to text, making use of spatiotemporal convolutions, an LSTM recurrent network, and the connectionist temporal classification loss, trained entirely end-to-end. To the best of our knowledge, LipNet is the first lipreading model to operate at sentence-level, using a single end-to-end speaker-independent deep model to simultaneously learn spatiotemporal visual features and a sequence model. On the GRID corpus, LipNet achieves 93.4% accuracy, outperforming experienced human lipreaders and the previous 79.6% state-of-the-art accuracy.", "creator": "LaTeX with hyperref package"}}}