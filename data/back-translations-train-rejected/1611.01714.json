{"id": "1611.01714", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Nov-2016", "title": "Beyond Fine Tuning: A Modular Approach to Learning on Small Data", "abstract": "In this paper we present a technique to train neural network models on small amounts of data. Current methods for training neural networks on small amounts of rich data typically rely on strategies such as fine-tuning a pre-trained neural network or the use of domain-specific hand-engineered features. Here we take the approach of treating network layers, or entire networks, as modules and combine pre-trained modules with untrained modules, to learn the shift in distributions between data sets. The central impact of using a modular approach comes from adding new representations to a network, as opposed to replacing representations via fine-tuning. Using this technique, we are able surpass results using standard fine-tuning transfer learning approaches, and we are also able to significantly increase performance over such approaches when using smaller amounts of data.", "histories": [["v1", "Sun, 6 Nov 2016 01:32:39 GMT  (1096kb,D)", "http://arxiv.org/abs/1611.01714v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.CL", "authors": ["ark", "erson", "kyle shaffer", "artem yankov", "court d corley", "nathan o hodas"], "accepted": false, "id": "1611.01714"}, "pdf": {"name": "1611.01714.pdf", "metadata": {"source": "CRF", "title": "BEYOND FINE TUNING: A MODULAR APPROACH TO LEARNING ON SMALL DATA", "authors": ["Aryk Anderson", "Kyle Shaffer", "Artem Yankov", "Courtney D. Corley", "Nathan O. Hodas"], "emails": ["aryk.anderson@eagles.ewu.edu", "kyle.shaffer@pnnl.gov", "artem.yankov@pnnl.gov", "court@pnnl.gov", "nathan.hodas@pnnl.gov"], "sections": [{"heading": "1 INTRODUCTION", "text": "This is especially true for the use of artificial neural networks with millions or billions of parameters. Conventional wisdom gained from the surge in popularity of neural network models suggests that extremely large amounts of data are required to train these models effectively. Indeed, the work of Krizhevsky et al. (2012) is often described as only possible by the development of ImageNet (Russakovsky et al. (2015). As neural networks are researched by practitioners in more specialized areas, the volume of available marked data is also limited. Although training methods have been improved, it is still difficult to train deep learning models on small amounts of data, such as just dozens or hundreds of examples. The current paradigm for solving this problem is not through the use of pre-formed neural networks."}, {"heading": "2 RELATED WORK", "text": "(...). (...). (...). (...). (...). (...). (...). \"(...).\" (...). \"(...).\" (...). (...). \"(...).\" (...). \"(...).\" (...). \"(...).\" (...). \"(...).\" (...). (...). (...). (...). (...). \"(...).\" (...). \"(...).\" (...). \"(...).\" (...). \"(...). (...). (...). (...). (...).\" (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). \"(...\" (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (. (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...)."}, {"heading": "3 MODULAR ARCHITECTURE", "text": "Generally speaking, modular neural networks are guided graphs of pre-trained networks that are linked to additional, untrained networks. Figure 1 shows that only the new components of the network are trained, and the architecture could take the form of simply placing two networks in parallel (the twotowers approach), as shown in Figure 1. In addition, the architecture could connect the modules to the layers of the pre-trained network (the stitch approach), as shown in Figure 2, allowing the network as a whole to retain the original representational power of the pre-trained network. Thus, our modular approach is less than the power of transfer learning. Here, we examine some of the characteristics of these modular architectures, including how they learn new representations and how they function on small amounts of data."}, {"heading": "3.1 LEARNED FILTERS", "text": "In the case of conventional networks, we believe that adding modules to networks helps them learn new domains, because the original modules contain well-trained filters, giving the untrained modules more subtle characteristics that may be more discriminatory. We put the objective function on activating the filters we are exploring. Instead of changing the gradients, we tracked the gradients at the input layer to change the pixels themselves. We initiated the activation of the filters we were exploring."}, {"heading": "3.2 SMALL DATA", "text": "In fact, it is the case that most of us are able to abide by the rules that we have imposed on ourselves. (...) It is not the case that we abide by the rules that we have imposed on ourselves. (...) It is the case that we are able to change the rules. (...) It is not the case that we abide by the rules. (...) It is not the case that we abide by the rules. (...) It is the case that we abide by the rules. (...) It is the case that we abide by the rules. (...) It is the case that we abide by the rules. (...) It is the case that we abide by the rules. (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). \"(...\" (). \"(...\" (...). \"(...\" It is the case that. (... \"(...). (...). (...\" (). (). (). (It is. (). (. (). (). (It is. (...). (. (). (). (. (). (It is. (. (). (). (. ().). (It is. (. (. (). (. (). (.). (It is. (. (. (). (). (. (.). (.).). (It is. (. (. (. (.). (It is.). (It is. (). (. (). (. (It is. (). (.). (. (It is. (.). (). (. (It is. (It is. (.). (). (. (). (. (.). (it is. (). (it is. (. (. (). (). (. (.). (it is. (it is.). (. (). (it is. (.). ("}, {"heading": "4 EXPERIMENTS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 TRANSFER LEARNING FROM CIFAR-10 TO CIFAR-100 WITH STITCH NETWORKS", "text": "In fact, it is the case that most people who are able are able to determine for themselves what they want and what they do not want."}, {"heading": "4.2 STANFORD CARS DATA SET", "text": "The data set from Stanford Cars (Krause et al. (2013), which contains 16,185 images of 196 classes of cars, is an example of a data set for fine-grain categorization. Instead of training a classifier to distinguish between fundamentally different objects such as horses and airplanes, as required in the Large Scale Visual Recognition Challenge (Russakovsky et al. (2015), fine-grain categorization requires learning the classifier subtle differences in variations of the same nature. For example, a classifier trained on the Stanford Cars data set would have to learn to distinguish the features between a BMW X6 SUV from 2012 and an Isuzu Ascender SUV from 2008. In this research, two models are trained on the Stanford Cars data set. Both models use a transfer learning approach, using the not fully networked performance of the VGG16 model (Simonyan & Zisserman)."}, {"heading": "4.3 MODULE FOR LSTM TEXT CLASSIFICATION", "text": "This year, it is as far as ever in the history of the city, where it is as far as never before."}, {"heading": "5 CONCLUSIONS", "text": "We have presented a neuro-modular approach to the transfer of learning content. As we have shown, the new modules often learn functions that complement the functions previously learned in the pre-trained network. We have shown that our approach exceeds traditional fine-tuning, especially when the amount of training data is low - only dozens of examples per class. Further research will explore more efficient architectures and training strategies, but we have shown that our approach to MNIST, CIFARs, the Stanford Cars dataset and IMDB mood works well. Therefore, the modular approach will be a valuable strategy if you have a large pre-trained network, but only a small amount of training data for the transfer task."}, {"heading": "ACKNOWLEDGMENTS", "text": "This work was supported by the US government."}], "references": [{"title": "Tensorflow: Large-scale machine learning on heterogeneous distributed systems", "author": ["Mart\u0131n Abadi", "Ashish Agarwal", "Paul Barham", "Eugene Brevdo", "Zhifeng Chen", "Craig Citro", "Greg S Corrado", "Andy Davis", "Jeffrey Dean", "Matthieu Devin"], "venue": "arXiv preprint arXiv:1603.04467,", "citeRegEx": "Abadi et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Abadi et al\\.", "year": 2016}, {"title": "Deep learning of representations for unsupervised and transfer learning", "author": ["Yoshua Bengio"], "venue": "ICML Unsupervised and Transfer Learning,", "citeRegEx": "Bengio,? \\Q2012\\E", "shortCiteRegEx": "Bengio", "year": 2012}, {"title": "Net2net: Accelerating learning via knowledge transfer", "author": ["Tianqi Chen", "Ian Goodfellow", "Jonathon Shlens"], "venue": "arXiv preprint arXiv:1511.05641,", "citeRegEx": "Chen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2015}, {"title": "Unsupervised domain adaptation by backpropagation", "author": ["Yaroslav Ganin", "Victor Lempitsky"], "venue": "arXiv preprint arXiv:1409.7495,", "citeRegEx": "Ganin and Lempitsky.,? \\Q2014\\E", "shortCiteRegEx": "Ganin and Lempitsky.", "year": 2014}, {"title": "Domain adaptation with conditional transferable components", "author": ["Mingming Gong", "Kun Zhang", "Tongliang Liu", "Dacheng Tao", "Clark Glymour", "Bernhard Sch\u00f6lkopf"], "venue": "In Proceedings of The 33rd International Conference on Machine Learning,", "citeRegEx": "Gong et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Gong et al\\.", "year": 2016}, {"title": "Deep residual learning for image recognition", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"], "venue": "arXiv preprint arXiv:1512.03385,", "citeRegEx": "He et al\\.,? \\Q2015\\E", "shortCiteRegEx": "He et al\\.", "year": 2015}, {"title": "Identity mappings in deep residual networks", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"], "venue": "arXiv preprint arXiv:1603.05027,", "citeRegEx": "He et al\\.,? \\Q2016\\E", "shortCiteRegEx": "He et al\\.", "year": 2016}, {"title": "Spatial transformer networks", "author": ["Max Jaderberg", "Karen Simonyan", "Andrew Zisserman"], "venue": "In Advances in Neural Information Processing Systems, pp. 2017\u20132025,", "citeRegEx": "Jaderberg et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Jaderberg et al\\.", "year": 2015}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "Kingma and Ba.,? \\Q2014\\E", "shortCiteRegEx": "Kingma and Ba.", "year": 2014}, {"title": "3d object representations for fine-grained categorization", "author": ["Jonathan Krause", "Michael Stark", "Jia Deng", "Li Fei-Fei"], "venue": "In Proceedings of the IEEE International Conference on Computer Vision Workshops,", "citeRegEx": "Krause et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Krause et al\\.", "year": 2013}, {"title": "Imagenet classification with deep convolutional neural networks. In Advances in neural information processing", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E Hinton"], "venue": null, "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Aspect specific sentiment analysis using hierarchical deep learning", "author": ["Himabindu Lakkaraju", "Richard Socher", "Chris Manning"], "venue": "In NIPS Workshop on Deep Learning and Representation Learning,", "citeRegEx": "Lakkaraju et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Lakkaraju et al\\.", "year": 2014}, {"title": "Fractalnet: Ultra-deep neural networks without residuals", "author": ["Gustav Larsson", "Michael Maire", "Gregory Shakhnarovich"], "venue": "arXiv preprint arXiv:1605.07648,", "citeRegEx": "Larsson et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Larsson et al\\.", "year": 2016}, {"title": "Learning transferable features with deep adaptation networks", "author": ["Mingsheng Long", "Yue Cao", "Jianmin Wang", "Michael Jordan"], "venue": "In Proceedings of The 32nd International Conference on Machine Learning,", "citeRegEx": "Long et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Long et al\\.", "year": 2015}, {"title": "Deep transfer learning with joint adaptation", "author": ["Mingsheng Long", "Jianmin Wang", "Michael I. Jordan"], "venue": "networks. CoRR,", "citeRegEx": "Long et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Long et al\\.", "year": 2016}, {"title": "Learning word vectors for sentiment analysis", "author": ["Andrew L. Maas", "Raymond E. Daly", "Peter T. Pham", "Dan Huang", "Andrew Y. Ng", "Christopher Potts"], "venue": "In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,", "citeRegEx": "Maas et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Maas et al\\.", "year": 2011}, {"title": "Unsupervised and transfer learning challenge: a deep learning approach", "author": ["Gr\u00e9goire Mesnil", "Yann Dauphin", "Xavier Glorot", "Salah Rifai", "Yoshua Bengio", "Ian J Goodfellow", "Erick Lavoie", "Xavier Muller", "Guillaume Desjardins", "David Warde-Farley"], "venue": "ICML Unsupervised and Transfer Learning,", "citeRegEx": "Mesnil et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Mesnil et al\\.", "year": 2012}, {"title": "The usefulness of past knowledge when learning a new task in deep neural networks", "author": ["Guglielmo Montone", "J Kevin ORegan", "Alexander V Terekhov"], "venue": null, "citeRegEx": "Montone et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Montone et al\\.", "year": 2015}, {"title": "Learning and transferring mid-level image representations using convolutional neural networks", "author": ["Maxime Oquab", "Leon Bottou", "Ivan Laptev", "Josef Sivic"], "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,", "citeRegEx": "Oquab et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Oquab et al\\.", "year": 2014}, {"title": "A survey on transfer learning", "author": ["Sinno Jialin Pan", "Qiang Yang"], "venue": "IEEE Transactions on knowledge and data engineering,", "citeRegEx": "Pan and Yang.,? \\Q2010\\E", "shortCiteRegEx": "Pan and Yang.", "year": 2010}, {"title": "Domain adaptation via transfer component analysis", "author": ["Sinno Jialin Pan", "Ivor W Tsang", "James T Kwok", "Qiang Yang"], "venue": "IEEE Transactions on Neural Networks,", "citeRegEx": "Pan et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Pan et al\\.", "year": 2011}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["Karen Simonyan", "Andrew Zisserman"], "venue": "arXiv preprint arXiv:1409.1556,", "citeRegEx": "Simonyan and Zisserman.,? \\Q2014\\E", "shortCiteRegEx": "Simonyan and Zisserman.", "year": 2014}, {"title": "Direct importance estimation with model selection and its application to covariate shift adaptation", "author": ["Masashi Sugiyama", "Shinichi Nakajima", "Hisashi Kashima", "Paul V Buenau", "Motoaki Kawanabe"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Sugiyama et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Sugiyama et al\\.", "year": 2008}, {"title": "Going deeper with convolutions", "author": ["Christian Szegedy", "Wei Liu", "Yangqing Jia", "Pierre Sermanet", "Scott Reed", "Dragomir Anguelov", "Dumitru Erhan", "Vincent Vanhoucke", "Andrew Rabinovich"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "Szegedy et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Szegedy et al\\.", "year": 2015}, {"title": "Deep domain confusion: Maximizing for domain invariance", "author": ["Eric Tzeng", "Judy Hoffman", "Ning Zhang", "Kate Saenko", "Trevor Darrell"], "venue": "arXiv preprint arXiv:1412.3474,", "citeRegEx": "Tzeng et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Tzeng et al\\.", "year": 2014}, {"title": "Simultaneous deep transfer across domains and tasks", "author": ["Eric Tzeng", "Judy Hoffman", "Trevor Darrell", "Kate Saenko"], "venue": "In Proceedings of the IEEE International Conference on Computer Vision, pp", "citeRegEx": "Tzeng et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Tzeng et al\\.", "year": 2015}, {"title": "Flexible transfer learning under support and model shift", "author": ["Xuezhi Wang", "Jeff Schneider"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Wang and Schneider.,? \\Q1898\\E", "shortCiteRegEx": "Wang and Schneider.", "year": 1898}, {"title": "How transferable are features in deep neural networks", "author": ["Jason Yosinski", "Jeff Clune", "Yoshua Bengio", "Hod Lipson"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Yosinski et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Yosinski et al\\.", "year": 2014}, {"title": "Visualizing and understanding convolutional networks", "author": ["Matthew D Zeiler", "Rob Fergus"], "venue": "In European Conference on Computer Vision,", "citeRegEx": "Zeiler and Fergus.,? \\Q2014\\E", "shortCiteRegEx": "Zeiler and Fergus.", "year": 2014}], "referenceMentions": [{"referenceID": 5, "context": "Indeed the work from Krizhevsky et al. (2012) has commonly been cited as only being possible through the development of ImageNet (Russakovsky et al.", "startOffset": 21, "endOffset": 46}, {"referenceID": 5, "context": "Indeed the work from Krizhevsky et al. (2012) has commonly been cited as only being possible through the development of ImageNet (Russakovsky et al. (2015)).", "startOffset": 21, "endOffset": 156}, {"referenceID": 0, "context": "Bengio et al. (2012) were able to show that transfer of knowledge in networks could be achieved by first training a neural network on a domain for which there is a large amount of data and then retraining that network on a related but different domain via fine-tuning its weights.", "startOffset": 0, "endOffset": 21}, {"referenceID": 0, "context": "Bengio et al. (2012) were able to show that transfer of knowledge in networks could be achieved by first training a neural network on a domain for which there is a large amount of data and then retraining that network on a related but different domain via fine-tuning its weights. Though this approach demonstrated promising results on small data, these models do not retain the ability to function as previously trained. That is, these models end up fine tuning their weights to the new learning task, forgetting many of the important features learned from the previous domain. The utility of pre-training models extends beyond training on small data. It is also used as an effective initialization technique for many complicated models (Jaderberg et al. (2015); Lakkaraju et al.", "startOffset": 0, "endOffset": 763}, {"referenceID": 0, "context": "Bengio et al. (2012) were able to show that transfer of knowledge in networks could be achieved by first training a neural network on a domain for which there is a large amount of data and then retraining that network on a related but different domain via fine-tuning its weights. Though this approach demonstrated promising results on small data, these models do not retain the ability to function as previously trained. That is, these models end up fine tuning their weights to the new learning task, forgetting many of the important features learned from the previous domain. The utility of pre-training models extends beyond training on small data. It is also used as an effective initialization technique for many complicated models (Jaderberg et al. (2015); Lakkaraju et al. (2014)).", "startOffset": 0, "endOffset": 788}, {"referenceID": 0, "context": "Bengio et al. (2012) were able to show that transfer of knowledge in networks could be achieved by first training a neural network on a domain for which there is a large amount of data and then retraining that network on a related but different domain via fine-tuning its weights. Though this approach demonstrated promising results on small data, these models do not retain the ability to function as previously trained. That is, these models end up fine tuning their weights to the new learning task, forgetting many of the important features learned from the previous domain. The utility of pre-training models extends beyond training on small data. It is also used as an effective initialization technique for many complicated models (Jaderberg et al. (2015); Lakkaraju et al. (2014)). This, in addition to the continuing trend of treating specific network layer architectures as modular components to compose more advanced models (He et al. (2015); Larsson et al.", "startOffset": 0, "endOffset": 953}, {"referenceID": 0, "context": "Bengio et al. (2012) were able to show that transfer of knowledge in networks could be achieved by first training a neural network on a domain for which there is a large amount of data and then retraining that network on a related but different domain via fine-tuning its weights. Though this approach demonstrated promising results on small data, these models do not retain the ability to function as previously trained. That is, these models end up fine tuning their weights to the new learning task, forgetting many of the important features learned from the previous domain. The utility of pre-training models extends beyond training on small data. It is also used as an effective initialization technique for many complicated models (Jaderberg et al. (2015); Lakkaraju et al. (2014)). This, in addition to the continuing trend of treating specific network layer architectures as modular components to compose more advanced models (He et al. (2015); Larsson et al. (2016); Szegedy et al.", "startOffset": 0, "endOffset": 976}, {"referenceID": 0, "context": "Bengio et al. (2012) were able to show that transfer of knowledge in networks could be achieved by first training a neural network on a domain for which there is a large amount of data and then retraining that network on a related but different domain via fine-tuning its weights. Though this approach demonstrated promising results on small data, these models do not retain the ability to function as previously trained. That is, these models end up fine tuning their weights to the new learning task, forgetting many of the important features learned from the previous domain. The utility of pre-training models extends beyond training on small data. It is also used as an effective initialization technique for many complicated models (Jaderberg et al. (2015); Lakkaraju et al. (2014)). This, in addition to the continuing trend of treating specific network layer architectures as modular components to compose more advanced models (He et al. (2015); Larsson et al. (2016); Szegedy et al. (2015); Abadi et al.", "startOffset": 0, "endOffset": 999}, {"referenceID": 0, "context": "(2015); Abadi et al. (2016)) informs our work as we seek to use pre-trained models as \u2217Seattle, WA \u2020Richland, WA; AA and NOH contributed equally", "startOffset": 8, "endOffset": 28}, {"referenceID": 14, "context": "Many shallow methods have been published, those that learn feature invariant representations or by approximating value without using an instance\u2019s label (Pan & Yang (2010); Sugiyama et al. (2008); Pan et al.", "startOffset": 173, "endOffset": 196}, {"referenceID": 13, "context": "(2008); Pan et al. (2011); Zhang et al.", "startOffset": 8, "endOffset": 26}, {"referenceID": 13, "context": "(2008); Pan et al. (2011); Zhang et al. (2013); Wang & Schneider (2014); Gong et al.", "startOffset": 8, "endOffset": 47}, {"referenceID": 13, "context": "(2008); Pan et al. (2011); Zhang et al. (2013); Wang & Schneider (2014); Gong et al.", "startOffset": 8, "endOffset": 72}, {"referenceID": 3, "context": "(2013); Wang & Schneider (2014); Gong et al. (2016)).", "startOffset": 33, "endOffset": 52}, {"referenceID": 3, "context": "(2013); Wang & Schneider (2014); Gong et al. (2016)). More recent deep transfer learning methods enable identification of variational factors in the data and align them to disparate domain distributions (Tzeng et al. (2014); Long et al.", "startOffset": 33, "endOffset": 224}, {"referenceID": 3, "context": "(2013); Wang & Schneider (2014); Gong et al. (2016)). More recent deep transfer learning methods enable identification of variational factors in the data and align them to disparate domain distributions (Tzeng et al. (2014); Long et al. (2015); Ganin & Lempitsky (2014); Tzeng et al.", "startOffset": 33, "endOffset": 244}, {"referenceID": 3, "context": "(2013); Wang & Schneider (2014); Gong et al. (2016)). More recent deep transfer learning methods enable identification of variational factors in the data and align them to disparate domain distributions (Tzeng et al. (2014); Long et al. (2015); Ganin & Lempitsky (2014); Tzeng et al.", "startOffset": 33, "endOffset": 270}, {"referenceID": 3, "context": "(2013); Wang & Schneider (2014); Gong et al. (2016)). More recent deep transfer learning methods enable identification of variational factors in the data and align them to disparate domain distributions (Tzeng et al. (2014); Long et al. (2015); Ganin & Lempitsky (2014); Tzeng et al. (2015)).", "startOffset": 33, "endOffset": 291}, {"referenceID": 3, "context": "(2013); Wang & Schneider (2014); Gong et al. (2016)). More recent deep transfer learning methods enable identification of variational factors in the data and align them to disparate domain distributions (Tzeng et al. (2014); Long et al. (2015); Ganin & Lempitsky (2014); Tzeng et al. (2015)). Mesnil et al. (2012) presents the Unsupervised and Transfer Learning Challenge and discusses the important advances that are needed for representation learning, and the importance of deep learning in transfer learning.", "startOffset": 33, "endOffset": 314}, {"referenceID": 3, "context": "(2013); Wang & Schneider (2014); Gong et al. (2016)). More recent deep transfer learning methods enable identification of variational factors in the data and align them to disparate domain distributions (Tzeng et al. (2014); Long et al. (2015); Ganin & Lempitsky (2014); Tzeng et al. (2015)). Mesnil et al. (2012) presents the Unsupervised and Transfer Learning Challenge and discusses the important advances that are needed for representation learning, and the importance of deep learning in transfer learning.Oquab et al. (2014) applied these techniques to mid-level image representations using CNNs.", "startOffset": 33, "endOffset": 531}, {"referenceID": 3, "context": "(2013); Wang & Schneider (2014); Gong et al. (2016)). More recent deep transfer learning methods enable identification of variational factors in the data and align them to disparate domain distributions (Tzeng et al. (2014); Long et al. (2015); Ganin & Lempitsky (2014); Tzeng et al. (2015)). Mesnil et al. (2012) presents the Unsupervised and Transfer Learning Challenge and discusses the important advances that are needed for representation learning, and the importance of deep learning in transfer learning.Oquab et al. (2014) applied these techniques to mid-level image representations using CNNs. Specifically, they showed that image representations learned in visual recognition tasks (ImageNet) can be transferred to other visual recognition tasks (Pascal VOC) efficiently. Further study regarding the transferability of features by Yosinski et al. (2014) showed surprising results that features from distant tasks perform better than random features and that difficulties arise when optimizing splitting networks between co-adapted neurons.", "startOffset": 33, "endOffset": 864}, {"referenceID": 3, "context": "(2013); Wang & Schneider (2014); Gong et al. (2016)). More recent deep transfer learning methods enable identification of variational factors in the data and align them to disparate domain distributions (Tzeng et al. (2014); Long et al. (2015); Ganin & Lempitsky (2014); Tzeng et al. (2015)). Mesnil et al. (2012) presents the Unsupervised and Transfer Learning Challenge and discusses the important advances that are needed for representation learning, and the importance of deep learning in transfer learning.Oquab et al. (2014) applied these techniques to mid-level image representations using CNNs. Specifically, they showed that image representations learned in visual recognition tasks (ImageNet) can be transferred to other visual recognition tasks (Pascal VOC) efficiently. Further study regarding the transferability of features by Yosinski et al. (2014) showed surprising results that features from distant tasks perform better than random features and that difficulties arise when optimizing splitting networks between co-adapted neurons. We build on these results by leveraging existing representations to transfer to target domains without overwriting the pre-trained models through standard fine-tuning approaches. Long et al. (2015) developed the Deep Adaptation Network (DAN) architecture for convolutional neural networks that embed hidden representations of all task-specific layers in a reproducing kernel Hilbert space.", "startOffset": 33, "endOffset": 1248}, {"referenceID": 2, "context": "The Net2Net approach (Chen et al. (2015)) accelerates training of larger neural networks by allowing them to grow gradually using function preserving transformations to transfer information between neural networks.", "startOffset": 22, "endOffset": 41}, {"referenceID": 2, "context": "The Net2Net approach (Chen et al. (2015)) accelerates training of larger neural networks by allowing them to grow gradually using function preserving transformations to transfer information between neural networks. However, it does not guarantee that existing representational power will be preserved on a different task. Gong et al. (2016) consider domain adaptation where transfer from source to domain is modeled as a causal system.", "startOffset": 22, "endOffset": 341}, {"referenceID": 2, "context": "The Net2Net approach (Chen et al. (2015)) accelerates training of larger neural networks by allowing them to grow gradually using function preserving transformations to transfer information between neural networks. However, it does not guarantee that existing representational power will be preserved on a different task. Gong et al. (2016) consider domain adaptation where transfer from source to domain is modeled as a causal system. Under these assumptions, conditional transferable components are extracted which are invariant after location-scale transformations. Long et al. (2016) proposed a new method that overcomes the need for conditional components by comparing joint distributions across domains.", "startOffset": 22, "endOffset": 588}, {"referenceID": 2, "context": "The Net2Net approach (Chen et al. (2015)) accelerates training of larger neural networks by allowing them to grow gradually using function preserving transformations to transfer information between neural networks. However, it does not guarantee that existing representational power will be preserved on a different task. Gong et al. (2016) consider domain adaptation where transfer from source to domain is modeled as a causal system. Under these assumptions, conditional transferable components are extracted which are invariant after location-scale transformations. Long et al. (2016) proposed a new method that overcomes the need for conditional components by comparing joint distributions across domains. Unlike our work, all of these require explicit assumptions or modifications to the pre-trained networks to facilitate adaptation. We note that while writing this paper, the progressive network architecture of Rusu et al. (2016) was released, sharing a number of qualities with our work.", "startOffset": 22, "endOffset": 938}, {"referenceID": 2, "context": "The Net2Net approach (Chen et al. (2015)) accelerates training of larger neural networks by allowing them to grow gradually using function preserving transformations to transfer information between neural networks. However, it does not guarantee that existing representational power will be preserved on a different task. Gong et al. (2016) consider domain adaptation where transfer from source to domain is modeled as a causal system. Under these assumptions, conditional transferable components are extracted which are invariant after location-scale transformations. Long et al. (2016) proposed a new method that overcomes the need for conditional components by comparing joint distributions across domains. Unlike our work, all of these require explicit assumptions or modifications to the pre-trained networks to facilitate adaptation. We note that while writing this paper, the progressive network architecture of Rusu et al. (2016) was released, sharing a number of qualities with our work. Both the results we present here and the progressive networks allow neural networks to extend their knowledge without forgetting previous information. In addition, Montone et al. (2015) discusses a semi-modular approach.", "startOffset": 22, "endOffset": 1183}, {"referenceID": 5, "context": "9% accurate, using the network in He et al. (2016) with 3 residual units, for a total of 28 layers.", "startOffset": 34, "endOffset": 51}, {"referenceID": 5, "context": "Note, the ResNet used is identical to the one describe in He et al. (2015).", "startOffset": 58, "endOffset": 75}, {"referenceID": 7, "context": "The Stanford Cars data set (Krause et al. (2013)), which features 16,185 images of 196 classes of cars, is an example of a data set for fine-grained categorization.", "startOffset": 28, "endOffset": 49}, {"referenceID": 7, "context": "The Stanford Cars data set (Krause et al. (2013)), which features 16,185 images of 196 classes of cars, is an example of a data set for fine-grained categorization. Rather than train a classifier to distinguish between fundamentally different objects like horses and planes, as required in the Large Scale Visual Recognition Challenge (Russakovsky et al. (2015)), fine-grained categorization requires the classifier to learn subtle differences in variations of the same entity.", "startOffset": 28, "endOffset": 362}, {"referenceID": 7, "context": "The Stanford Cars data set (Krause et al. (2013)), which features 16,185 images of 196 classes of cars, is an example of a data set for fine-grained categorization. Rather than train a classifier to distinguish between fundamentally different objects like horses and planes, as required in the Large Scale Visual Recognition Challenge (Russakovsky et al. (2015)), fine-grained categorization requires the classifier to learn subtle differences in variations of the same entity. For example, a classifier trained on the Stanford Cars data set would have to learn distinguishing features between a BMW X6 SUV from 2012 and an Isuzu Ascender SUV from 2008. In this research two models are trained on the Stanford Cars data set. Both models utilize a transfer learning approach by leveraging the non-fully connected output from the VGG16 model (Simonyan & Zisserman (2014)).", "startOffset": 28, "endOffset": 869}, {"referenceID": 5, "context": "The \u201cmodule\u201d model merges the fixed VGG16 features with a ResNet (He et al. (2015)) model, whose output is then fed to two consecutive dense layers of length 256 capped by a softmax layer of length 196.", "startOffset": 66, "endOffset": 83}, {"referenceID": 15, "context": "Previous work has shown deep learning methods to be effective at sentiment classification performance on this dataset (Maas et al. (2011)), however we add to this past work by presenting an analysis that demonstrates the effectiveness of modular networks in the case of extremely small training sets.", "startOffset": 119, "endOffset": 138}], "year": 2016, "abstractText": "In this paper we present a technique to train neural network models on small amounts of data. Current methods for training neural networks on small amounts of rich data typically rely on strategies such as fine-tuning a pre-trained neural network or the use of domain-specific hand-engineered features. Here we take the approach of treating network layers, or entire networks, as modules and combine pre-trained modules with untrained modules, to learn the shift in distributions between data sets. The central impact of using a modular approach comes from adding new representations to a network, as opposed to replacing representations via fine-tuning. Using this technique, we are able surpass results using standard fine-tuning transfer learning approaches, and we are also able to significantly increase performance over such approaches when using smaller amounts of data.", "creator": "LaTeX with hyperref package"}}}