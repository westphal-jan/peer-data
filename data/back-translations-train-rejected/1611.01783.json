{"id": "1611.01783", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Nov-2016", "title": "Domain Adaptation For Formant Estimation Using Deep Learning", "abstract": "In this paper we present a domain adaptation technique for formant estimation using a deep network. We first train a deep learning network on a small read speech dataset. We then freeze the parameters of the trained network and use several different datasets to train an adaptation layer that makes the obtained network universal in the sense that it works well for a variety of speakers and speech domains with very different characteristics. We evaluated our adapted network on three datasets, each of which has different speaker characteristics and speech styles. The performance of our method compares favorably with alternative methods for formant estimation.", "histories": [["v1", "Sun, 6 Nov 2016 14:00:14 GMT  (146kb,D)", "http://arxiv.org/abs/1611.01783v1", null]], "reviews": [], "SUBJECTS": "cs.CL cs.SD", "authors": ["yehoshua dissen", "joseph keshet", "jacob goldberger", "cynthia clopper"], "accepted": false, "id": "1611.01783"}, "pdf": {"name": "1611.01783.pdf", "metadata": {"source": "CRF", "title": "DOMAIN ADAPTATION FOR FORMANT ESTIMATION USING DEEP LEARNING", "authors": ["Yehoshua Dissen", "Joseph Keshet", "Jacob Goldberger", "Cynthia Clopper"], "emails": [], "sections": [{"heading": null, "text": "Index Terms - Form Estimates, Neural Networks, Transfer Learning"}, {"heading": "1. INTRODUCTION", "text": "In our previous paper [1], we applied deep learning methods to the tasks of estimating and tracking formants; the network was trained and tested on the Vocal Tract Resonance (VTR) database [2] and we achieved state-of-the-art results; our goal is to form a single network that works well across a variety of different speakers and speaking areas; a simple approach is not only to use the VTR data sets, but also to describe examples of all the other available data sets; our goal is to build a single network that is well-targeted at a variety of speakers and speaking areas."}, {"heading": "2. ACOUSTIC FEATURES", "text": "A key assumption is that in formant estimation, the entire segment is considered stationary, which is mainly true for monophthongs (pure vowels), and the characteristics are extracted from the entire segment. We use the same sets of characteristics as in our previous paper [1]. The first set is based on the analysis of linear predictive coding (LPC), while the second is based on tone synchronous spectra."}, {"heading": "2.1. LPC-based features", "text": "An LPC model determines the coefficients of a linear prediction model by minimizing the prediction error in the Leastar Xiv: 161 1.01 783v 1 [cs.C L] 6N ov2 016squares sense. The spectrum of the LPC model can be interpreted as an envelope of the language spectrum. The model order p determines how smooth the spectral envelope will be. Low p values represent the rough properties of the spectrum, and if p increases, more of the detailed properties are retained. Beyond a certain p value, the details of the spectrum reflect not only the spectral resonances of the sound, but also the pitch and some noise. A disadvantage of this method is that if p is not well selected (i.e. to correspond to the number of resonances present in the language), the resulting LPC spectrum is not as accurate as desired [5]. Our first series of acoustic characteristics based on a LPC-17 model can be represented between the classification 8 and the LPC-17."}, {"heading": "2.2. Pitch-synchronous spectrum-based features", "text": "The spectrum of a periodic speech signal is known to have an impulse path structure that is in a multiple of the pitch period. An important problem with the direct use of the spectrum to locate formants is that the resonance peaks may fall between two pitch lines and then are not \"visible.\" The LPC model estimates the spectrum to solve this problem. Another method for estimating the spectrum while eliminating the pitch impulse is the use of pitch synchronization spectrum [7]. According to this method, pitch size is taken over. One of the main problems of this method is the need for a very precise pitch estimator. In our previous paper [1] we showed that the use of pitch images of the size of the medium pitch spacing along the segment generates a spectrum where its pickaxes are not contaminated by the pitch. In the latter stage, the resulting quasi-synchronous pitch receptor is used as the first pitch compression by Coscocrete and then by the use of the T."}, {"heading": "3. DATASETS", "text": "For the training and validation of our model, we used three different sets of data. The first is the Vocal Tract Resonance (VTR) corpus [2], which consists of 538 expressions selected as a representative subset of the known and widely used TIMIT corpus, which were divided into 346 expressions for the training set and 192 expressions for the test set. These expressions were commented manually for the first 3 formants and their bandwidths for each 10 msec framework. the fourth formant was described by the automatic tracking algorithm in [8], and is not used here for valuation.The second data set [3] contains segments of the acoustic signal of 20 female native speakers aged 18-22 years with no history of speech or language deficits. Participants were divided equally between two American English dialects (Northern and Midland).asasasaset: Participants read a list of 991 CVC words."}, {"heading": "4. A DOMAIN ADAPTATION NETWORK", "text": "In this section, we describe a network architecture and training method that collectively form a domain adaptation approach to identification; the network we use for estimating data is relatively reliable; the network consists of an input layer with 350 features, 3 fully connected hidden layers of 1024, 512 and 256 neurons, and an initial layer that provides an estimate of the 4 formants. As the network is trained exclusively on the VTR datasets, we denote it more highly than the VTR network. The trained VTR network delivers good results on the test subset of the VTR dataset. However, when applied to other datasets, it is a significant performance degradation. The goal of this study is to train a single network that can be successfully used on different datasets."}, {"heading": "5. RESULTS", "text": "The results of our system, DeepFormant and WaveSurfer on the three datasets we used are shown in Table 1, where the loss is the mean absolute difference in Hz. Note that the clopper dataset was noted for the first and second formants, so the third formant was not evaluated. As shown in the table, we achieved better results across WaveSurfer when comparing our respective estimates with the manually annotated reference. The domain adaptation network shows improvements over DeepFormants in both the Clopper and Hillenbrand datasets without any significant decrease in accuracy in the VTR dataset."}, {"heading": "6. CONCLUSIONS", "text": "In this paper, we have proposed a formant estimation deep learning architecture that is state-of-the-art in all areas of speech and speech that are very different. We have also proposed a training program that confirms the assertion that each component of the network is actually responsible for the task for which it is designed, either formant estimation or domain adaptation. We have demonstrated automated formant estimation tools here that are ready to be added to the methods that sociolinguists use to analyze acoustic data. The tools will be publicly available at https: / / github. com / MLSpeech / Domain Adaptation. In the future, we plan to evaluate the robustness of our method not only for different datasets, but also for noisy environments."}, {"heading": "7. REFERENCES", "text": "[1] Yehoshua Dissen and Joseph Keshet, \"Formant estimation and tracking using deep learning,\" The 17th Annual Conference of the International Speech Communication Association, 2016. [2] Li Deng, Xiaodong Cui, Robert Pruvenok, Yanyi Chen, Safiyy Momen, and Abeer Alwan, \"A database of vocal tract resonance trajectories for research in speech processing,\" in Acoustics, Speech and Signal Processing, 2006. ICASSP 2006 Proceedings. 2006 IEEE International Conference on. IEEE verification, 2006, vol. I-I. [3] Cynthia G Clopper and Terrin N Tamati \"Effects of localized competition and regional dialect on vowel production,\" The Journal of the Acoustical Society of America, vol."}], "references": [{"title": "Formant estimation and tracking using deep learning", "author": ["Yehoshua Dissen", "Joseph Keshet"], "venue": "The 17th Annual Conference of the International Speech Communication Association, 2016.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2016}, {"title": "A database of vocal tract resonance trajectories for research in speech processing", "author": ["Li Deng", "Xiaodong Cui", "Robert Pruvenok", "Yanyi Chen", "Safiyy Momen", "Abeer Alwan"], "venue": "Acoustics, Speech and Signal Processing, 2006. ICASSP 2006 Proceedings. 2006 IEEE International Conference on. IEEE, 2006, vol. 1, pp. I\u2013I.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2006}, {"title": "Effects of local  lexical competition and regional dialect on vowel production", "author": ["Cynthia G Clopper", "Terrin N Tamati"], "venue": "The Journal of the Acoustical Society of America, vol. 136, no. 1, pp. 1\u20134, 2014.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2014}, {"title": "Acoustic characteristics of american english vowels", "author": ["James Hillenbrand", "Laura A Getty", "Michael J Clark", "Kimberlee Wheeler"], "venue": "The Journal of the Acoustical society of America, vol. 97, no. 5, pp. 3099\u20133111, 1995.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1995}, {"title": "Application of prewhitening to ar spectral estimation of EEG", "author": ["Gary E Birch", "Peter Lawrence", "John C Lind", "Robert D Hare"], "venue": "Biomedical Engineering, IEEE Transactions on, vol. 35, no. 8, pp. 640\u2013645, 1988.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1988}, {"title": "Effectiveness of linear prediction characteristics of the speech wave for automatic speaker identification and verification", "author": ["Bishnu S Atal"], "venue": "the Journal of the Acoustical Society of America, vol. 55, no. 6, pp. 1304\u20131312, 1974.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1974}, {"title": "Pitch synchronous spectral analysis scheme for voiced speech", "author": ["Yoav Medan", "Eyal Yair"], "venue": "IEEE Trans. on Acoustics, Speech and Signal Processing, vol. 37, no. 9, pp. 1321\u20131328, 1989.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1989}, {"title": "A structured speech model with continuous hidden dynamics and prediction-residual training for tracking vocal tract resonances", "author": ["Li Deng", "Leo J Lee", "Hagai Attias", "Alex Acero"], "venue": "Acoustics, Speech, and Signal Processing, 2004. Proceedings.(ICASSP\u201904). IEEE International Conference on. IEEE, 2004, vol. 1, pp. I\u2013557.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2004}, {"title": "Wavesurfer-an open source speech tool", "author": ["K\u00e5re Sj\u00f6lander", "Jonas Beskow"], "venue": "Interspeech, 2000, pp. 464\u2013467.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2000}], "referenceMentions": [{"referenceID": 0, "context": "In our previous work [1] we applied deep learning methods to the tasks of formant estimation and tracking.", "startOffset": 21, "endOffset": 24}, {"referenceID": 1, "context": "The network was trained and tested on the Vocal Tract Resonance (VTR) database [2] and we achieved state-of-the art results.", "startOffset": 79, "endOffset": 82}, {"referenceID": 2, "context": "[3] [4]) due to the phenomenon of over-fitting to the speaker and speech domains represented in that database.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "[3] [4]) due to the phenomenon of over-fitting to the speaker and speech domains represented in that database.", "startOffset": 4, "endOffset": 7}, {"referenceID": 0, "context": "We use the same sets of features as in our previous work [1].", "startOffset": 57, "endOffset": 60}, {"referenceID": 4, "context": ", to match the number of resonance present in the speech), then the resulted LPC spectrum is not as accurate as desired [5].", "startOffset": 120, "endOffset": 123}, {"referenceID": 5, "context": "Practically, we use the LPC cepstral coefficients of order 30 as Atals method[6] was a better representation for deep learning classifiers than the LPC coefficients themselves [1].", "startOffset": 77, "endOffset": 80}, {"referenceID": 0, "context": "Practically, we use the LPC cepstral coefficients of order 30 as Atals method[6] was a better representation for deep learning classifiers than the LPC coefficients themselves [1].", "startOffset": 176, "endOffset": 179}, {"referenceID": 6, "context": "Another method to estimate the spectrum while eliminating the pitch impulse train is using the pitch synchronous spectrum [7].", "startOffset": 122, "endOffset": 125}, {"referenceID": 0, "context": "In our previous work [1] we showed that using frames of the size of the median pitch along the segment produces a spectrum, where its picks are not contaminated by the pitch.", "startOffset": 21, "endOffset": 24}, {"referenceID": 1, "context": "The first being the Vocal Tract Resonance (VTR) corpus [2].", "startOffset": 55, "endOffset": 58}, {"referenceID": 7, "context": "The fourth formant was annotated by the automatic tracking algorithm described in [8], and it is not used here for evaluation.", "startOffset": 82, "endOffset": 85}, {"referenceID": 2, "context": "The second dataset [3] contains segments of acoustic signal from 20 female native English speakers aged 18-22 with no history of speech or language deficits.", "startOffset": 19, "endOffset": 22}, {"referenceID": 3, "context": "The third dataset consists of data from a laboratory study conducted by Hillenbrand [4].", "startOffset": 84, "endOffset": 87}, {"referenceID": 8, "context": "VTR WaveSurfer [9] 70 96 154 DeepFormants [1] 48 83 109 Domain Adaptation 50 86 104", "startOffset": 15, "endOffset": 18}, {"referenceID": 0, "context": "VTR WaveSurfer [9] 70 96 154 DeepFormants [1] 48 83 109 Domain Adaptation 50 86 104", "startOffset": 42, "endOffset": 45}, {"referenceID": 0, "context": "Here we present the results of the domain adaptation network compared to our previous work [1] that is based on training only on the VTR data and WaveSurfer [9] a popular tool in", "startOffset": 91, "endOffset": 94}, {"referenceID": 8, "context": "Here we present the results of the domain adaptation network compared to our previous work [1] that is based on training only on the VTR data and WaveSurfer [9] a popular tool in", "startOffset": 157, "endOffset": 160}], "year": 2016, "abstractText": "In this paper we present a domain adaptation technique for formant estimation using a deep network. We first train a deep learning network on a small read speech dataset. We then freeze the parameters of the trained network and use several different datasets to train an adaptation layer that makes the obtained network universal in the sense that it works well for a variety of speakers and speech domains with very different characteristics. We evaluated our adapted network on three datasets, each of which has different speaker characteristics and speech styles. The performance of our method compares favorably with alternative methods for formant estimation.", "creator": "LaTeX with hyperref package"}}}