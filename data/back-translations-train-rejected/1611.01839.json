{"id": "1611.01839", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Nov-2016", "title": "Hierarchical Question Answering for Long Documents", "abstract": "Reading an article and answering questions about its content is a fundamental task for natural language understanding. While most successful neural approaches to this problem rely on recurrent neural networks (RNNs), training RNNs over long documents can be prohibitively slow. We present a novel framework for question answering that can efficiently scale to longer documents while maintaining or even improving performance. Our approach combines a coarse, inexpensive model for selecting one or more relevant sentences and a more expensive RNN that produces the answer from those sentences. A central challenge is the lack of intermediate supervision for the coarse model, which we address using reinforcement learning. Experiments demonstrate state-of-the-art performance on a challenging subset of the WIKIREADING dataset(Hewlett et al., 2016) and on a newly-gathered dataset, while reducing the number of sequential RNN steps by 88% against a standard sequence to sequence model.", "histories": [["v1", "Sun, 6 Nov 2016 20:24:40 GMT  (1598kb,D)", "http://arxiv.org/abs/1611.01839v1", null], ["v2", "Wed, 8 Feb 2017 07:42:34 GMT  (856kb,D)", "http://arxiv.org/abs/1611.01839v2", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["eunsol choi", "daniel hewlett", "alexandre lacoste", "illia polosukhin", "jakob uszkoreit", "jonathan berant"], "accepted": false, "id": "1611.01839"}, "pdf": {"name": "1611.01839.pdf", "metadata": {"source": "CRF", "title": "Hierarchical Question Answering for Long Documents", "authors": ["Eunsol Choi", "Daniel Hewlett", "Alexandre Lacoste"], "emails": ["eunsol@cs.washington.edu", "dhewlett@google.com", "allac@google.com", "ipolosukhin@google.com", "usz@google.com", "joberant@cs.tau.ac.il"], "sections": [{"heading": "1 Introduction", "text": "In fact, most neural models for QA are only a limited number of documents (Miller et al., 2016)."}, {"heading": "2 Problem Setting", "text": "Our task is defined as follows: Starting from a training set of question-document-answer triples {x (i), d (i), y (i)} Ni = 1, our goal is to train a question-answer model that generates an answer-y for a new question-document pair (x, d). A document d is a list of sentences s1, s2,..., s | d |, and we assume that there is a latent subset of sentences from which the answer can be generated. Figure 2 illustrates a training example in which sentence s5 is a key sentence to answer the question."}, {"heading": "3 Data", "text": "In fact it is so that we are able to assert ourselves in a position to assert that we are able to assert ourselves in the world, and that we are able to assert ourselves in the world, that we are able to assert ourselves in the world, that we are in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in, in"}, {"heading": "4 Model", "text": "Our model consists of two parts (see Figure 1): The first part is a quick sentence selection model (Section 4.1) that defines a probability distribution p (s | x, d) over document sets taking into account the input question (x) and the document (d); the second part is a more cost-effective response generation model (Section 4.3) that defines a probability distribution p (y | x, d) over answers given to a question and a document \"summary\" (d) that focuses on the relevant parts of the document."}, {"heading": "4.1 Sentence Selection Model", "text": "In fact, it is the case that the majority of people who live in the USA, live in the USA, in the USA, in Europe, in the USA, in Europe, in the USA, in Europe, in Europe, in the USA, in Europe, in the USA, in Europe, in Europe, in the USA, in the USA, in Europe, in the USA, in Europe, in Europe, in Europe, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the"}, {"heading": "4.2 Document summary", "text": "After calculating an attention distribution via sentences using the sentence selection model, we create a summary that focuses on the parts related to the question using a deterministic soft attention model or a stochastic hard attention model.Hard attention In the hard attention model, we select a sentence sl \u0445 p (s | x, d) and fix the document summary as the selected sentence. At the test date, we select the most likely sentence instead of the selection model. To expand the document summary to include more information, we also examine the selection of K sentences from the document and define the summary as the concatenation of the sampled sentences d = sl1 | sl2 | |.."}, {"heading": "4.3 Answer Generation Model", "text": "State-of-the-art models for answering questions (Chen et al., 2016) use sequence-to-sequence models to encode the document and the question and generate the answer. We focus on developing a quick sentence selection model and do not subscribe to a specific architecture for generating answers. We opt to implement the most advanced sequence-to-sequence model at word level with placeholders, described by 1To prevent repeated sampling of the same sentences, we mask the previously selected sentences and normalize the query tokens (s | x, d) after sampling each example. 2Longer sentences are shortened and shorter sentences are replaced by Hewlett et al. (2016) and will soon be checked here for completeness. This recursive neural network model takes the query tokens, a delimiter, and the document (or in our case the document summary) tokens as input - and then encodes them with a GRT (Recurated with another)."}, {"heading": "5 Learning", "text": "We look at three approaches to learning the parameters of our model. The soft attention model is differentiable and is optimized with the help of learners. (The hard attention model is not differentiable and is optimized using the REINFORCE algorithm (Williams, 1992). Finally, we look at a pipeline approach where we use remote supervision to label sentences for selection and build a sentence selection model that is independent of a response generation model. We define the first sentence, which has a complete match of the response chain as the gold set, and if no such sentence exists, we define the first sentence as the gold set. (3) By labeling gold sets, we can independently form the sentence selection and response generation models."}, {"heading": "6 Experiments", "text": "It is indeed the case that most of us are able to outdo ourselves, \"he told the Deutsche Presse-Agentur.\" But it is not the case that we are able to change the world. \"He added,\" It is not that we are able to change the world. \""}, {"heading": "7 Related Work", "text": "The MCTest (Richardson et al., 2013) and the BioProcess Bank (Berant et al., 2014) are smaller-scale, common-sense datasets. bAbi (Weston et al., 2015) is a synthetic dataset with simulated questions to capture various aspects of thinking. WikiQA is a well-curated, smaller dataset for selecting answers. Jurczyk et al. (2016) presented a dataset on Wikipedia in which sentences are selected and the SQuAD (Rajpurkar et al., 2016) datasets are proposed for answering, but covered shorter documents. Cloze-style questions that answer to datasets (CNN et al., 2015), who has Tkenice et al., 2016), and CBT (Hill et al., 2015) are designed to assess machine understanding but not answer questions."}, {"heading": "8 Conclusion", "text": "We have introduced a hierarchical framework for QA over long documents that quickly focuses on the relevant parts of a document to answer the question. We have shown that our model is more efficient and can match or exceed two sophisticated reading comprehension data sets with state-of-the-art performance. Our framework uses the document structure to handle long documents. In future work, we want to deepen the hierarchy by answering questions across multiple documents and using other structural clues such as paragraphs, titles, etc."}], "references": [{"title": "Learning to compose neural networks for question answering", "author": ["Jacob Andreas", "Marcus Rohrbach", "Trevor Darrell", "Dan Klein."], "venue": "Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics: Human Lan-", "citeRegEx": "Andreas et al\\.,? 2016", "shortCiteRegEx": "Andreas et al\\.", "year": 2016}, {"title": "Multiple object recognition with visual attention", "author": ["Jimmy Ba", "Volodymyr Mnih", "Koray Kavukcuoglu."], "venue": "The International Conference on Learning Representations.", "citeRegEx": "Ba et al\\.,? 2014", "shortCiteRegEx": "Ba et al\\.", "year": 2014}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "Proceedings of the International Conference on Learning Representations.", "citeRegEx": "Bahdanau et al\\.,? 2015", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Modeling biological processes for reading comprehension", "author": ["Jonathan Berant", "Vivek Srikumar", "Pei-Chun Chen", "Abby Vander Linden", "Brittany Harding", "Brad Huang", "Peter Clark", "Christopher D Manning."], "venue": "Proceedings of the Conference of the Empirical Meth-", "citeRegEx": "Berant et al\\.,? 2014", "shortCiteRegEx": "Berant et al\\.", "year": 2014}, {"title": "A thorough examination of the cnn/daily mail reading comprehension task", "author": ["Danqi Chen", "Jason Bolton", "Christopher D. Manning."], "venue": "Association for Computational Linguistics.", "citeRegEx": "Chen et al\\.,? 2016", "shortCiteRegEx": "Chen et al\\.", "year": 2016}, {"title": "Neural summarization by extracting sentences and words", "author": ["Jianpeng Cheng", "Mirella Lapata."], "venue": "Proceedings of the Annual Meeting of the Association for Computational Linguistics.", "citeRegEx": "Cheng and Lapata.,? 2016", "shortCiteRegEx": "Cheng and Lapata.", "year": 2016}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "author": ["Kyunghyun Cho", "Bart Van Merri\u00ebnboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio."], "venue": "Proceedings of the Confer-", "citeRegEx": "Cho et al\\.,? 2014", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Deep reinforcement learning for mention-ranking coreference models", "author": ["Kevin Clark", "Christopher D. Manning."], "venue": "Proceedings of the Conference of the Empirical Methods in Natural Language Processing.", "citeRegEx": "Clark and Manning.,? 2016", "shortCiteRegEx": "Clark and Manning.", "year": 2016}, {"title": "Deep reinforcement learning with an unbounded action space", "author": ["Ji He", "Jianshu Chen", "Xiaodong He", "Jianfeng Gao", "Lihong Li", "Li Deng", "Mari Ostendorf."], "venue": "Proceedings of the Conference of the Association for Computational Linguistics.", "citeRegEx": "He et al\\.,? 2016", "shortCiteRegEx": "He et al\\.", "year": 2016}, {"title": "Teaching machines to read and comprehend", "author": ["Karl Moritz Hermann", "Tom\u00e1\u0161 Ko\u010disk\u00fd", "Edward Grefenstette", "Lasse Espeholt", "Will Kay", "Mustafa Suleyman", "Phil Blunsom."], "venue": "Advances in Neural Information Processing Systems.", "citeRegEx": "Hermann et al\\.,? 2015", "shortCiteRegEx": "Hermann et al\\.", "year": 2015}, {"title": "Wikireading: A novel large-scale language understanding task over wikipedia", "author": ["Daniel Hewlett", "Alexandre Lacoste", "Llion Jones", "Illia Polosukhin", "Andrew Fandrianto", "Jay Han", "Matthew Kelcey", "David Berthelot."], "venue": "Proceedings of the Conference of the", "citeRegEx": "Hewlett et al\\.,? 2016", "shortCiteRegEx": "Hewlett et al\\.", "year": 2016}, {"title": "The goldilocks principle: Reading children\u2019s books with explicit memory representations", "author": ["Felix Hill", "Antoine Bordes", "Sumit Chopra", "Jason Weston."], "venue": "The International Conference on Learning Representations.", "citeRegEx": "Hill et al\\.,? 2015", "shortCiteRegEx": "Hill et al\\.", "year": 2015}, {"title": "Knowledgebased weak supervision for information extraction of overlapping relations", "author": ["Raphael Hoffmann", "Congle Zhang", "Xiao Ling", "Luke Zettlemoyer", "Daniel S Weld."], "venue": "Proceedings of the 49th Annual Meeting of the Association for Computational", "citeRegEx": "Hoffmann et al\\.,? 2011", "shortCiteRegEx": "Hoffmann et al\\.", "year": 2011}, {"title": "SelQA: A New Benchmark for Selection-based Question Answering", "author": ["Tomasz Jurczyk", "Michael Zhai", "Jinho D. Choi."], "venue": "Proceedings of the 28th International Conference on Tools with Artificial Intelligence, ICTAI\u201916, San Jose, CA.", "citeRegEx": "Jurczyk et al\\.,? 2016", "shortCiteRegEx": "Jurczyk et al\\.", "year": 2016}, {"title": "Text understanding with the attention sum reader network", "author": ["Rudolf Kadlec", "Martin Schmid", "Ond\u0159ej Bajgar", "Jan Kleindienst."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 908\u2013918,", "citeRegEx": "Kadlec et al\\.,? 2016", "shortCiteRegEx": "Kadlec et al\\.", "year": 2016}, {"title": "A convolutional neural network for modelling sentences", "author": ["Nal Kalchbrenner", "Edward Grefenstette", "Phil Blunsom."], "venue": "Proceedings of the Annual Meeting of the Association for Computational Linguistics.", "citeRegEx": "Kalchbrenner et al\\.,? 2014", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2014}, {"title": "Convolutional neural networks for sentence classification", "author": ["Yoon Kim."], "venue": "Proceedings of the Conference of the Empirical Methods in Natural Language Processing.", "citeRegEx": "Kim.,? 2014", "shortCiteRegEx": "Kim.", "year": 2014}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba."], "venue": "The International Conference on Learning Representations.", "citeRegEx": "Kingma and Ba.,? 2015", "shortCiteRegEx": "Kingma and Ba.", "year": 2015}, {"title": "Ask me anything: Dynamic memory networks for natural language processing", "author": ["Ankit Kumar", "Ozan Irsoy", "Peter Ondruska", "Mohit Iyyer", "James Bradbury", "Ishaan Gulrajani", "Victor Zhong", "Romain Paulus", "Richard Socher."], "venue": "Proceedings of the International", "citeRegEx": "Kumar et al\\.,? 2016", "shortCiteRegEx": "Kumar et al\\.", "year": 2016}, {"title": "Rationalizing neural predictions", "author": ["Tao Lei", "Regina Barzilay", "Tommi S. Jaakkola."], "venue": "Proceedings of the Conference of the Empirical Methods in Natural Language Processing.", "citeRegEx": "Lei et al\\.,? 2016", "shortCiteRegEx": "Lei et al\\.", "year": 2016}, {"title": "Hierarchical question-image coattention for visual question answering", "author": ["Jiasen Lu", "Jianwei Yang", "Dhruv Batra", "Devi Parikh."], "venue": "arXiv preprint arXiv:1606.00061.", "citeRegEx": "Lu et al\\.,? 2016", "shortCiteRegEx": "Lu et al\\.", "year": 2016}, {"title": "Effective approaches to attentionbased neural machine translation", "author": ["Minh-Thang Luong", "Hieu Pham", "Christopher D. Manning."], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1412\u20131421, Lisbon, Por-", "citeRegEx": "Luong et al\\.,? 2015", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Conceptual processing of text during skimming and rapid sequential reading", "author": ["Michael EJ Masson."], "venue": "Memory & Cognition, 11(3):262\u2013274.", "citeRegEx": "Masson.,? 1983", "shortCiteRegEx": "Masson.", "year": 1983}, {"title": "Key-value memory networks for directly reading documents", "author": ["Alexander Miller", "Adam Fisch", "Jesse Dodge", "AmirHossein Karimi", "Antoine Bordes", "Jason Weston."], "venue": "Proceedings of the Conference of the Empirical Methods in Natural Language Processing.", "citeRegEx": "Miller et al\\.,? 2016", "shortCiteRegEx": "Miller et al\\.", "year": 2016}, {"title": "Distant supervision for relation extraction without labeled data", "author": ["Mike Mintz", "Steven Bills", "Rion Snow", "Dan Jurafsky."], "venue": "Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language", "citeRegEx": "Mintz et al\\.,? 2009", "shortCiteRegEx": "Mintz et al\\.", "year": 2009}, {"title": "Language understanding for text-based games using deep reinforcement learning", "author": ["Karthik Narasimhan", "Tejas Kulkarni", "Regina Barzilay."], "venue": "Proceedings of the Conference of the Empirical Methods in Natural Language Processing.", "citeRegEx": "Narasimhan et al\\.,? 2015", "shortCiteRegEx": "Narasimhan et al\\.", "year": 2015}, {"title": "Improving information extraction by acquiring external evidence with reinforcement learning", "author": ["Karthik Narasimhan", "Adam Yala", "Regina Barzilay."], "venue": "Proceedings of the Conference of the Empirical Methods in Natural Language Processing.", "citeRegEx": "Narasimhan et al\\.,? 2016", "shortCiteRegEx": "Narasimhan et al\\.", "year": 2016}, {"title": "Who did what: A largescale person-centered cloze dataset", "author": ["Takeshi Onishi", "Hai Wang", "Mohit Bansal", "Kevin Gimpel", "David McAllester."], "venue": "Proceedings of Empirical Methods in Natural Language Processing.", "citeRegEx": "Onishi et al\\.,? 2016", "shortCiteRegEx": "Onishi et al\\.", "year": 2016}, {"title": "Squad: 100,000+ questions for machine comprehension of text", "author": ["P. Rajpurkar", "J. Zhang", "K. Lopyrev", "P. Liang."], "venue": "Proceedings of the Conference of the Empirical Methods in Natural Language Processing.", "citeRegEx": "Rajpurkar et al\\.,? 2016", "shortCiteRegEx": "Rajpurkar et al\\.", "year": 2016}, {"title": "Mctest: A challenge dataset for the open-domain machine comprehension of text", "author": ["Matthew Richardson", "Christopher JC Burges", "Erin Renshaw."], "venue": "Proceedings of the Conference of the Empirical Methods in Natural Language Processing.", "citeRegEx": "Richardson et al\\.,? 2013", "shortCiteRegEx": "Richardson et al\\.", "year": 2013}, {"title": "A reduction of imitation learning and structured prediction to no-regret online learning", "author": ["St\u00e9phane Ross", "Geoffrey J Gordon", "Drew Bagnell."], "venue": "International Conference on Artificial Intelligence and Statistics.", "citeRegEx": "Ross et al\\.,? 2011", "shortCiteRegEx": "Ross et al\\.", "year": 2011}, {"title": "Easy questions first? a case study on curriculum learning for question answering", "author": ["Mrinmaya Sachan", "Eric P Xing."], "venue": "Proceedings of the Annual Meeting of the Association for Computational Linguistics.", "citeRegEx": "Sachan and Xing.,? 2016", "shortCiteRegEx": "Sachan and Xing.", "year": 2016}, {"title": "Automatic feature engineering for answer selection and extraction", "author": ["Aliaksei Severyn", "Alessandro Moschitti."], "venue": "Proceedings of the Conference of the Empirical Methods in Natural Language Processing.", "citeRegEx": "Severyn and Moschitti.,? 2013", "shortCiteRegEx": "Severyn and Moschitti.", "year": 2013}, {"title": "Learning to rank short text pairs with convolutional deep neural networks", "author": ["Aliaksei Severyn", "Alessandro Moschitti."], "venue": "Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 373\u2013", "citeRegEx": "Severyn and Moschitti.,? 2015", "shortCiteRegEx": "Severyn and Moschitti.", "year": 2015}, {"title": "Dropout: a simple way to prevent neural networks from overfitting", "author": ["Nitish Srivastava", "Geoffrey E Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov."], "venue": "Journal of Machine Learning Research, 15(1):1929\u20131958.", "citeRegEx": "Srivastava et al\\.,? 2014", "shortCiteRegEx": "Srivastava et al\\.", "year": 2014}, {"title": "A joint model for answer sentence ranking and answer extraction", "author": ["Md. Arafat Sultan", "Vittorio Castelli", "Radu Florian."], "venue": "Transactions of the Association for Computational Linguistics, 4:113\u2013125.", "citeRegEx": "Sultan et al\\.,? 2016", "shortCiteRegEx": "Sultan et al\\.", "year": 2016}, {"title": "Building a question answering test collection", "author": ["Ellen M Voorhees", "Dawn M Tice."], "venue": "Proceedings of the 23rd annual international ACM SIGIR conference on Research and development in information retrieval, pages 200\u2013207. ACM.", "citeRegEx": "Voorhees and Tice.,? 2000", "shortCiteRegEx": "Voorhees and Tice.", "year": 2000}, {"title": "A long short-term memory model for answer sentence selection in question answering", "author": ["Di Wang", "Eric Nyberg."], "venue": "Proceedings of the Annual Meeting of the Association for Computational Linguistics.", "citeRegEx": "Wang and Nyberg.,? 2015", "shortCiteRegEx": "Wang and Nyberg.", "year": 2015}, {"title": "Towards ai-complete question answering: A set of prerequisite toy tasks", "author": ["Jason Weston", "Antoine Bordes", "Sumit Chopra", "Alexander M Rush", "Bart van Merri\u00ebnboer", "Armand Joulin", "Tomas Mikolov."], "venue": "arXiv preprint arXiv:1502.05698.", "citeRegEx": "Weston et al\\.,? 2015", "shortCiteRegEx": "Weston et al\\.", "year": 2015}, {"title": "Simple statistical gradientfollowing algorithms for connectionist reinforcement learning", "author": ["Ronald J Williams."], "venue": "Machine learning, 8(3-4):229\u2013256.", "citeRegEx": "Williams.,? 1992", "shortCiteRegEx": "Williams.", "year": 1992}, {"title": "Show, attend and tell: Neural image caption generation with visual attention", "author": ["Kelvin Xu", "Jimmy Ba", "Ryan Kiros", "Kyunghyun Cho", "Aaron Courville", "Ruslan Salakhutdinov", "Richard Zemel", "Yoshua Bengio."], "venue": "Proceedings of the International Conference", "citeRegEx": "Xu et al\\.,? 2015", "shortCiteRegEx": "Xu et al\\.", "year": 2015}, {"title": "Wikiqa: A challenge dataset for open-domain question answering", "author": ["Yi Yang", "Wen-tau Yih", "Christopher Meek."], "venue": "Proceedings of the Conference of the Empirical Methods in Natural Language Processing.", "citeRegEx": "Yang et al\\.,? 2016a", "shortCiteRegEx": "Yang et al\\.", "year": 2016}, {"title": "Deep Learning for Answer Sentence Selection", "author": ["Lei Yu", "Karl Moritz Hermann", "Phil Blunsom", "Stephen Pulman."], "venue": "NIPS Deep Learning Workshop, December.", "citeRegEx": "Yu et al\\.,? 2014", "shortCiteRegEx": "Yu et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 10, "context": "Experiments demonstrate state-of-the-art performance on a challenging subset of the WIKIREADING dataset (Hewlett et al., 2016) and on a newly-gathered dataset, while reducing the number of sequential RNN steps by 88% against a standard sequence to sequence model.", "startOffset": 104, "endOffset": 126}, {"referenceID": 9, "context": "Recently, interest in question answering (QA) from unstructured documents has rocketed along with the availability of large scale datasets for reading comprehension (Hermann et al., 2015; Hill et al., 2015; Rajpurkar et al., 2016; Onishi et al., 2016).", "startOffset": 165, "endOffset": 251}, {"referenceID": 11, "context": "Recently, interest in question answering (QA) from unstructured documents has rocketed along with the availability of large scale datasets for reading comprehension (Hermann et al., 2015; Hill et al., 2015; Rajpurkar et al., 2016; Onishi et al., 2016).", "startOffset": 165, "endOffset": 251}, {"referenceID": 28, "context": "Recently, interest in question answering (QA) from unstructured documents has rocketed along with the availability of large scale datasets for reading comprehension (Hermann et al., 2015; Hill et al., 2015; Rajpurkar et al., 2016; Onishi et al., 2016).", "startOffset": 165, "endOffset": 251}, {"referenceID": 27, "context": "Recently, interest in question answering (QA) from unstructured documents has rocketed along with the availability of large scale datasets for reading comprehension (Hermann et al., 2015; Hill et al., 2015; Rajpurkar et al., 2016; Onishi et al., 2016).", "startOffset": 165, "endOffset": 251}, {"referenceID": 9, "context": "(RNNs) that encode the document and the question to determine the answer (Hermann et al., 2015; Chen et al., 2016; Kumar et al., 2016; Kadlec et al., 2016).", "startOffset": 73, "endOffset": 155}, {"referenceID": 4, "context": "(RNNs) that encode the document and the question to determine the answer (Hermann et al., 2015; Chen et al., 2016; Kumar et al., 2016; Kadlec et al., 2016).", "startOffset": 73, "endOffset": 155}, {"referenceID": 18, "context": "(RNNs) that encode the document and the question to determine the answer (Hermann et al., 2015; Chen et al., 2016; Kumar et al., 2016; Kadlec et al., 2016).", "startOffset": 73, "endOffset": 155}, {"referenceID": 14, "context": "(RNNs) that encode the document and the question to determine the answer (Hermann et al., 2015; Chen et al., 2016; Kumar et al., 2016; Kadlec et al., 2016).", "startOffset": 73, "endOffset": 155}, {"referenceID": 23, "context": "number of tokens (Miller et al., 2016; Hewlett et al., 2016).", "startOffset": 17, "endOffset": 60}, {"referenceID": 10, "context": "number of tokens (Miller et al., 2016; Hewlett et al., 2016).", "startOffset": 17, "endOffset": 60}, {"referenceID": 22, "context": "Inspired by studies (Masson, 1983) on how people answer questions by first skimming the docuar X iv :1 61 1.", "startOffset": 20, "endOffset": 34}, {"referenceID": 42, "context": "First, a fast model is used for sentence selection (Yu et al., 2014; Yang et al., 2016a), that is, to select a few sentences from the document that are relevant for answering the question.", "startOffset": 51, "endOffset": 88}, {"referenceID": 41, "context": "First, a fast model is used for sentence selection (Yu et al., 2014; Yang et al., 2016a), that is, to select a few sentences from the document that are relevant for answering the question.", "startOffset": 51, "endOffset": 88}, {"referenceID": 39, "context": "We explore both a hard attention sentence selection model, trained using REINFORCE (Williams, 1992), as well as a fully differentiable soft attention sentence selection model that is trained end-to-end.", "startOffset": 83, "endOffset": 99}, {"referenceID": 10, "context": "We evaluate our model on a subset of the recently published WIKIREADING dataset (Hewlett et al., 2016), focusing on examples where the input document is lengthy and sentence selection is challenging.", "startOffset": 80, "endOffset": 102}, {"referenceID": 10, "context": "WIKIREADING (Hewlett et al., 2016) is a recently released dataset that was automatically generated from Wikipedia and Wikidata, where the goal is as follows: given a Wikipedia page about an entity and a Wikidata property, such as PROFESSION, or GENDER, to infer the value based on the document.", "startOffset": 12, "endOffset": 34}, {"referenceID": 10, "context": "data properties for which Hewlett et al. (2016)\u2019s best model obtains an accuracy that is lower than 60%.", "startOffset": 26, "endOffset": 48}, {"referenceID": 42, "context": "Following recent work on sentence selection for QA (Yu et al., 2014; Yang et al., 2016b), we build a feedforward neural network model to define a probability distribution (attention) over the list of sentences", "startOffset": 51, "endOffset": 88}, {"referenceID": 21, "context": "We also consider a dot product model without a non-linear interaction between the sentence and query, inspired by its strong performance in MT setting (Luong et al., 2015).", "startOffset": 151, "endOffset": 171}, {"referenceID": 23, "context": "Chunked BoW Model To get a more fine-grained granularity, one can split sentences into fixed size smaller chunks (seven tokens per chunk) and score each chunk separately (Miller et al., 2016).", "startOffset": 170, "endOffset": 191}, {"referenceID": 16, "context": "vious work (Kim, 2014; Kalchbrenner et al., 2014) and add a single convolutional layer over the encoding enc(x, sl) with 100 filters of width 5.", "startOffset": 11, "endOffset": 49}, {"referenceID": 15, "context": "vious work (Kim, 2014; Kalchbrenner et al., 2014) and add a single convolutional layer over the encoding enc(x, sl) with 100 filters of width 5.", "startOffset": 11, "endOffset": 49}, {"referenceID": 2, "context": "Soft Attention The soft attention model (Bahdanau et al., 2015) generates the summary by computing a weighted average of sentences word by word according to p(s | x, d).", "startOffset": 40, "endOffset": 63}, {"referenceID": 4, "context": "State-of-the-art question answering models (Chen et al., 2016) use sequence-to-sequence models to encode the document and question and generate the", "startOffset": 43, "endOffset": 62}, {"referenceID": 10, "context": "Longer sentences are truncated and shorter ones are padded Hewlett et al. (2016), and review it shortly here for completeness.", "startOffset": 59, "endOffset": 81}, {"referenceID": 6, "context": "document (or in out case, document summary) tokens as input and encodes them with a Gated Recurrent Unit (GRU) (Cho et al., 2014).", "startOffset": 111, "endOffset": 129}, {"referenceID": 39, "context": "The hard attention model is nondifferentiable and is optimized with the REINFORCE algorithm (Williams, 1992).", "startOffset": 92, "endOffset": 108}, {"referenceID": 17, "context": "We use stochastic gradient descent with Adam (Kingma and Ba, 2015).", "startOffset": 45, "endOffset": 66}, {"referenceID": 39, "context": "Following REINFORCE (Williams, 1992), we approximate the gradient of the objective with a sample, \u015d \u223c p\u03b8(s | x, d):", "startOffset": 20, "endOffset": 36}, {"referenceID": 34, "context": "We employed dropout (Srivastava et al., 2014) in the smaller WIKIREADING LONG dataset to avoid overfitting.", "startOffset": 20, "endOffset": 45}, {"referenceID": 10, "context": "FULL is an implementation of the best model by Hewlett et al. (2016). A word-level sequence-to-sequence model, which consumes the first 300 tokens of the document.", "startOffset": 47, "endOffset": 69}, {"referenceID": 40, "context": "6 Comparing hard attention to soft attention, we observe that in general hard attention models were more effective, a pattern similar to results from caption generation (Xu et al., 2015).", "startOffset": 169, "endOffset": 186}, {"referenceID": 29, "context": "The MCTest (Richardson et al., 2013) and BioProcess bank (Berant et al.", "startOffset": 11, "endOffset": 36}, {"referenceID": 3, "context": ", 2013) and BioProcess bank (Berant et al., 2014) are smallerscale datasets focusing on common sense reasoning", "startOffset": 28, "endOffset": 49}, {"referenceID": 38, "context": "bAbi (Weston et al., 2015) is a synthetic dataset with simulated questions to capture various aspects of reasoning.", "startOffset": 5, "endOffset": 26}, {"referenceID": 38, "context": "bAbi (Weston et al., 2015) is a synthetic dataset with simulated questions to capture various aspects of reasoning. WikiQA is a well-curated, smaller-scale answer selection dataset. Jurczyk et al (2016) presented a crowd-", "startOffset": 6, "endOffset": 203}, {"referenceID": 28, "context": "sourced dataset on Wikipedia which involves sentence selection and the SQuAD (Rajpurkar et al., 2016) dataset proposes answer selection, but covers shorter documents.", "startOffset": 77, "endOffset": 101}, {"referenceID": 9, "context": "Cloze-style question answering datasets (CNN (Hermann et al., 2015), Who did", "startOffset": 45, "endOffset": 67}, {"referenceID": 27, "context": "What (Onishi et al., 2016), and CBT (Hill et al.", "startOffset": 5, "endOffset": 26}, {"referenceID": 11, "context": ", 2016), and CBT (Hill et al., 2015)) are designed to assess machine comprehension but do not form valid questions.", "startOffset": 17, "endOffset": 36}, {"referenceID": 36, "context": "dataset (Voorhees and Tice, 2000).", "startOffset": 8, "endOffset": 33}, {"referenceID": 37, "context": "Recently, neural network architectures (Wang and Nyberg, 2015; Severyn and Moschitti, 2015; dos Santos et al., 2016) achieved improvements over earlier feature-", "startOffset": 39, "endOffset": 116}, {"referenceID": 33, "context": "Recently, neural network architectures (Wang and Nyberg, 2015; Severyn and Moschitti, 2015; dos Santos et al., 2016) achieved improvements over earlier feature-", "startOffset": 39, "endOffset": 116}, {"referenceID": 32, "context": "based models (Severyn and Moschitti, 2013).", "startOffset": 13, "endOffset": 42}, {"referenceID": 35, "context": "Recent work (Sultan et al., 2016) models answer sentence extraction and answer extraction jointly.", "startOffset": 12, "endOffset": 33}, {"referenceID": 5, "context": ", 2016b), extractive summarization (Cheng and Lapata, 2016), machine translation (Ba et al.", "startOffset": 35, "endOffset": 59}, {"referenceID": 1, "context": ", 2016b), extractive summarization (Cheng and Lapata, 2016), machine translation (Ba et al., 2014),", "startOffset": 81, "endOffset": 98}, {"referenceID": 19, "context": "aspect sentiment classification (Lei et al., 2016), and vision (Lu et al.", "startOffset": 32, "endOffset": 50}, {"referenceID": 20, "context": ", 2016), and vision (Lu et al., 2016; Xu et al., 2015).", "startOffset": 20, "endOffset": 54}, {"referenceID": 40, "context": ", 2016), and vision (Lu et al., 2016; Xu et al., 2015).", "startOffset": 20, "endOffset": 54}, {"referenceID": 24, "context": "Finally, our work is related to reinforcement learning literature and learning from noisy signals (Mintz et al., 2009; Hoffmann et al., 2011).", "startOffset": 98, "endOffset": 141}, {"referenceID": 12, "context": "Finally, our work is related to reinforcement learning literature and learning from noisy signals (Mintz et al., 2009; Hoffmann et al., 2011).", "startOffset": 98, "endOffset": 141}, {"referenceID": 31, "context": "Curriculum learning for question answering was investigated in Sachan and Xing (2016), but they focused on the ordering of training examples while we interpolate two supervision signals.", "startOffset": 63, "endOffset": 86}, {"referenceID": 7, "context": "in natural language processing tasks such as coreference resolution (Clark and Manning, 2016), information extraction (Narasimhan et al.", "startOffset": 68, "endOffset": 93}, {"referenceID": 26, "context": "in natural language processing tasks such as coreference resolution (Clark and Manning, 2016), information extraction (Narasimhan et al., 2016), semantic parsing (Andreas et al.", "startOffset": 118, "endOffset": 143}, {"referenceID": 0, "context": ", 2016), semantic parsing (Andreas et al., 2016) and textual games (Narasimhan et al.", "startOffset": 26, "endOffset": 48}, {"referenceID": 25, "context": ", 2016) and textual games (Narasimhan et al., 2015; He et al., 2016).", "startOffset": 26, "endOffset": 68}, {"referenceID": 8, "context": ", 2016) and textual games (Narasimhan et al., 2015; He et al., 2016).", "startOffset": 26, "endOffset": 68}], "year": 2016, "abstractText": "Reading an article and answering questions about its content is a fundamental task for natural language understanding. While most successful neural approaches to this problem rely on recurrent neural networks (RNNs), training RNNs over long documents can be prohibitively slow. We present a novel framework for question answering that can efficiently scale to longer documents while maintaining or even improving performance. Our approach combines a coarse, inexpensive model for selecting one or more relevant sentences and a more expensive RNN that produces the answer from those sentences. A central challenge is the lack of intermediate supervision for the coarse model, which we address using reinforcement learning. Experiments demonstrate state-of-the-art performance on a challenging subset of the WIKIREADING dataset (Hewlett et al., 2016) and on a newly-gathered dataset, while reducing the number of sequential RNN steps by 88% against a standard sequence to sequence model.", "creator": "LaTeX with hyperref package"}}}