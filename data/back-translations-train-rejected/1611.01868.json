{"id": "1611.01868", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Nov-2016", "title": "Truth Discovery with Memory Network", "abstract": "Truth discovery is to resolve conflicts and find the truth from multiple-source statements. Conventional methods mostly research based on the mutual effect between the reliability of sources and the credibility of statements, however, pay no attention to the mutual effect among the credibility of statements about the same object. We propose memory network based models to incorporate these two ideas to do the truth discovery. We use feedforward memory network and feedback memory network to learn the representation of the credibility of statements which are about the same object. Specially, we adopt memory mechanism to learn source reliability and use it through truth prediction. During learning models, we use multiple types of data (categorical data and continuous data) by assigning different weights automatically in the loss function based on their own effect on truth discovery prediction. The experiment results show that the memory network based models much outperform the state-of-the-art method and other baseline methods.", "histories": [["v1", "Mon, 7 Nov 2016 01:08:11 GMT  (1256kb,D)", "http://arxiv.org/abs/1611.01868v1", "10 pages, 2 figures"]], "COMMENTS": "10 pages, 2 figures", "reviews": [], "SUBJECTS": "cs.CL cs.DB", "authors": ["luyang li", "bing qin", "wenjing ren", "ting liu"], "accepted": false, "id": "1611.01868"}, "pdf": {"name": "1611.01868.pdf", "metadata": {"source": "CRF", "title": "Truth Discovery with Memory Network", "authors": ["Luyang Li", "Bing Qin", "Wenjing Ren", "Ting Liu"], "emails": ["tliu}@ir.hit.edu.cn"], "sections": [{"heading": "1 Introduction", "text": "In the age of information overload, conflicts often exist in multiple-source statements about the same object. For example, different booking sites offer different boarding times on the same flight on the same day. The phenomenon has a serious impact on people's lives. Truth-finding task is to find the most credible statement to resolve the conflict (Yin et al., 2008; Dong et al., 2009; Galland et al., 2010). The development of research benefits from other natural language-processing tasks such as knowledge management (Poston and Speier, 2005), answering questions (Banerjee et al., 2009), retrieving information (Olteanu et al., 2013), and so on. Previous methods usually take elective mechanisms by predicting the truth (Yin et al., 2008; Dong et al., 2009; Galland et al., 2010; Dong et al., 2012; Wang et al., 2012; Li et al."}, {"heading": "2 Methodology", "text": "First we formulate the problem, then we present the framework and the central functions."}, {"heading": "2.1 Problem Formulation", "text": "In the real world, the statement is a naturally existing unit of information, which is the description of a thing of an event. In the real world, we can always draw an object and a property from the statement, which form an entry (Li et al., 2014b). The observation is the value of the entry. According to the same entry, observations from different sources may contain contradictory values. \"The aim of the predictive model is to find the correct observation of the entry.\" The departure time of flight AA-2446-LAX-DFW will start at 13: 19 \"is a statement.\" Flight AA-2446-LAX-DFW \"is an object and the\" departure time \"is a property.\" The departure time of flight AA-2446-LAX-DFW \"is an entry. The observation of the arrival time\" the departure time of aircraft AA-2446-LAX-DFW \"is a property of the aircraft.\""}, {"heading": "2.2 Basic Framework", "text": "The CRH method (Li \u03b2 al., 2014b) uses an unattended framework to solve the problem. Li et al. consider reliable sources reliable observations that must be close to the truth. We think cautiously that more trustworthy observations should be closer to the truth. In the face of an entry ei is D the function for calculating the distance between the truth ti and the observation value vik. The task of finding the truth is treated as an optimization problem. The optimization goal is to minimize the following loss function. The rik stands for the credibility of the observation provided by the source Sk. We calculate the credibility of the observation using a storage network-based model that flowed in Section 3.flowed = N \u2211 i = 1 K \u2211 k = 1 rik k = 1 rik D (ti, vik) (1) The removal functions of categorical data and continuous data are different."}, {"heading": "3 Memory Network based Model for Truth Discovery", "text": "We would like to briefly introduce the Storage Network. Storage Network is a framework with a long-term memory as a feedback component (Weston et al., 2014; Sukhbaatar et al., 2015; Tang et al., 2016). It consists of a memory with four components I, G, O and R. The memory can be read and written to store the long-term information useful in the prediction. Our problem is the reliability of the source long-term information used by predicting the truth. Source reliability should be updated during learning a model by entering each sample and combined with input to generate a new output of the data. We use memory M = {m1, m2,..., mK} to store the reliability of the sources."}, {"heading": "3.1 Feedforward Memory Network", "text": "Feedforward neural network is like a directed acyclic graph that has no feedback from the network. Feedforward storage network is a feedforward neural network with storage network mechanism. The architecture of FFMN is shown in Figure 1. Entering {x1, x2,..., xj,..., xK} of each iteration in component I are the vectors of the observations according to the same entry. The observation vector is the concatenation of the vectors of object data, property data and value data. M stores memory vectors representing the reliability of sources. Memories and inputs are calculated by elementary multiplication."}, {"heading": "3.2 Feedback Memory Network", "text": "The neural feedback network is a type of network whose neurons are returned to other neurons as input after a time step. Long-term memory (LSTM) is a typical neural feedback network. LSTM uses Hidden LayerH to store \"short-term memory\" and uses cell unit C to store \"long-term memory.\" It takes Input-Gate xt, forget-Gate ft to control the update of \"long-term memory,\" and takes Output-Gate ot to calculate hidden vectors at the current time. Our Feedback Memory Network (FBMN) is shown in Figure 2. We insert a memory component M into LSTM to store the reliability of the sources. The memory component M plays a different role and also has a different operation, comparing it to cell C in LSTM. Entering the model is a set of updated vectors from different sources according to a same input sequence of input vectors for a time step 1, which is the last vector of the series."}, {"heading": "4 Experiment and Analysis", "text": "We present the test results to demonstrate the effectiveness of our storage network models. First, we present the data set and evaluation criteria, present the results and perform an analysis, and discuss the influence of parameters on the results."}, {"heading": "4.1 Datasets", "text": "We use two public datasets (Li et al., 2012) 1 to demonstrate the effectiveness of the proposed method. Statistics of the datasets are in Table 32. Considering the redundancy of the datasets, which specifically causes several different values according to an entry in the soil truth set, we eliminate redundancy by data pre-processing. The entries contained in soil truths are part of whole entries in the dataset. The soil truths contain only NASDAQ100 shares and other 100 randomly selected shares. This inventory data is acquired by taking the majority of the values provided by five sources, nasdaq.com, yahoo finance, google finance, bloomberg and MSN finance."}, {"heading": "4.2 Evaluation Criteria", "text": "In order to evaluate our methods, we use two evaluation criteria, namely error rate and average normalized absolute distance (MNAD) (Li et al., 2014b). The lower these two criteria, the better the results. Error rate: Error rate is the calculation of the percentage of false prediction of categorical data. If the output of the method deviates from the basic truth, it is a false prediction. This evaluation criterion reflects the ability of methods to predict categorical data. MNAD: MNAD is the assessment of the proximity between the forecast output and the basic truths on the continuous data. Since the values of different entries have different scales, the absolute distance between the forecast output oi and the basic truth ti is normalized by the mean square error of the input. MNAD = 1M \u2212 oi = 1 | ti \u2212 oi (2 \u2212 vivi) (2 \u2212 viviviv) (2 \u2212 v) v)"}, {"heading": "4.3 Baseline Methods", "text": "We use the following baseline methods to make a comparison with our methods. Mean: Mean method measures all values of the same entry as the prediction that is used on continuous data. Median:: Median method finds the median value of all values of the same entry as the prediction that is used on continuous data. GTM (Zhao and Han, 2012): Gauss Truth Model (GTM) is a Bayesian probabilistic method that works only on continuous data. Note that this method only uses continuous data to learn the model and make a prediction, and insufficient data can lead to lower performance in the GTM, compared with others. Voting: The method takes the value with the highest occurrence as the predicted value. Investments (Pasternack and Roth, 2011): In this approach, a source invests its reliability on the observations that it provides, and collects credits back from the credibility of these observations."}, {"heading": "4.4 Truth Discovery Experiment", "text": "We compare our FFMN and FBMN models with basic methods. Experimental results in Table 2 confirm the effectiveness of our models. Results show that our storage network models outperform previous methods. FFMN has the best predictive capacity in inventory data for categorical data with the lowest error rate, and LSTM-based models, which are Bi-LSTM and FBMN, perform better for continuous data with the lowest MNAD. FFMN performs best in flight data for both categorical and continuous data. Under previous methods, CRH has the same framework as our methods. Results between the two methods confirm that the effectiveness of using neural network models to solve the truth-finding problem is best. We compare neural network methods and find storage network models to the best results. FBMN has good MNAD results for simple network models, but particularly impresses with other complex MFFN models."}, {"heading": "4.5 Parameter Setting", "text": "We have conducted a series of experiments to analyze the effects of parameters on the results of truth finding. We do an analysis of the following parameters, embedding learned by different algorithms, different embedding lengths, different learning rates. Embedding: We use word embedding algorithms to compare with Word2Vec (Mikolov et al., 2013) in the Truth Finding Experiment. Embedding we can learn from information about network structures that maintain both the proximity of the first order and the proximity of the second order. We use embedding in FFMN models and conduct experiments with flight data. The results show that the embedding learned from Word2Vec is better suited to our problem. We can also see that the second order of the line gains better than the first order. Embedding the length: We try to perform multiple numbers of dimensionality from 50 to 300 dimensionalitys.We find that the second order almost does not have an impact on the order."}, {"heading": "5 Related Work", "text": "Most methods also take into account some important characteristics, such as the reliability of the sources, number of sources posting the same statements, difficulty of the statement, uncertainty in information extraction (Pasternack and Roth, 2011), similarity between statements and copying relationships, etc. Li et al. categorize the previous methods (Li et al., 2012). The basic method is the use of election strategy. Web link-based methods share the similar strategy that the credibility of the statements is based on the links between statements and sources. Appropriate methods are HUB (Kleinberg, 1999), AvgLog (Pasternack and Dan, 2010), Investments (Pasternack and Dan, 2010) and PoolInvestment (Pasternack and Dan, 2010)."}, {"heading": "6 Conclusion", "text": "Finding the truth is a fundamental research problem in natural language processing and data mining. Previous approaches have mostly looked at the reciprocal effect between the reliability of sources and the credibility of observations. The reciprocal effect between the credibility of observations on the same entry is also important, but has not yet been appreciated. We use memory network-based models to learn the latent relationship between observations on the same entry. Specifically, the proposed model uses storage mechanisms to learn the reliability of sources and include them in the representation of observational credibility. We use several types of data and take into account their different contributions to finding the truth by automatically assigning weights in the loss function. Experiments show that our methods significantly exceed the state-of-the-art method on two public gold standard datasets."}], "references": [{"title": "Answer credibility: A language modeling approach to answer validation", "author": ["Banerjee", "Han2009] Protima Banerjee", "Hyoil Han"], "venue": "In Human Language Technologies: Conference of the North American Chapter of the Association of Computational Linguistics, Proceedings, May 31 - June", "citeRegEx": "Banerjee et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Banerjee et al\\.", "year": 2009}, {"title": "Integrating conflicting data: The role of source dependence", "author": ["Dong et al.2009] Xin Luna Dong", "Laure Berti-Equille", "Divesh Srivastava"], "venue": "Proceedings of the Vldb Endowment,", "citeRegEx": "Dong et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Dong et al\\.", "year": 2009}, {"title": "Less is more: selecting sources wisely for integration", "author": ["Dong et al.2012] Xin Luna Dong", "Barna Saha", "Divesh Srivastava"], "venue": "Proceedings of the VLDB Endowment,", "citeRegEx": "Dong et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Dong et al\\.", "year": 2012}, {"title": "Corroborating information from disagreeing views", "author": ["Serge Abiteboul", "Am Marian", "Pierre Senellart"], "venue": "In ACM International Conference on Web Search & Data Mining,", "citeRegEx": "Galland et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Galland et al\\.", "year": 2010}, {"title": "Authoritative sources in a hyperlinked environment", "author": ["Jon M. Kleinberg"], "venue": "Journal of the Acm,", "citeRegEx": "Kleinberg.,? \\Q1999\\E", "shortCiteRegEx": "Kleinberg.", "year": 1999}, {"title": "Truth finding on the deep web: is the problem solved", "author": ["Li et al.2012] Xian Li", "Xin Luna Dong", "Kenneth Lyons", "Weiyi Meng", "Divesh Srivastava"], "venue": "Proceedings of the Vldb Endowment,", "citeRegEx": "Li et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Li et al\\.", "year": 2012}, {"title": "A confidence-aware approach for truth discovery on long-tail data", "author": ["Li et al.2014a] Qi Li", "Yaliang Li", "Jing Gao", "Lu Su", "Bo Zhao", "Murat Demirbas", "Wei Fan", "Jiawei Han"], "venue": "Proceedings of the Vldb Endowment,", "citeRegEx": "Li et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Li et al\\.", "year": 2014}, {"title": "Resolving conflicts in heterogeneous data by truth discovery and source reliability estimation", "author": ["Li et al.2014b] Qi Li", "Yaliang Li", "Jing Gao", "Bo Zhao", "Wei Fan", "Jiawei Han"], "venue": "In Proceedings of the 2014 ACM SIGMOD International Conference on Management of Data,", "citeRegEx": "Li et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Li et al\\.", "year": 2014}, {"title": "A survey on truth discovery", "author": ["Li et al.2016] Yaliang Li", "Jing Gao", "Chuishi Meng", "Qi Li", "Lu Su", "Bo Zhao", "Wei Fan", "Jiawei Han"], "venue": "Acm Sigkdd Explorations Newsletter,", "citeRegEx": "Li et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Li et al\\.", "year": 2016}, {"title": "Efficient estimation of word representations in vector space", "author": ["Kai Chen", "Greg Corrado", "Jeffrey Dean"], "venue": "Computer Science", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Web credibility: Features exploration and credibility prediction", "author": ["Stanislav Peshterliev", "Xin Liu", "Karl Aberer"], "venue": "In European Conference on Advances in Information Retrieval,", "citeRegEx": "Olteanu et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Olteanu et al\\.", "year": 2013}, {"title": "Knowing what to believe(when you already know something)", "author": ["Pasternack", "Dan2010] Jeff Pasternack", "Roth Dan"], "venue": "In COLING 2010, International Conference on Computational Linguistics, Proceedings of the Conference,", "citeRegEx": "Pasternack et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Pasternack et al\\.", "year": 2010}, {"title": "Latent credibility analysis", "author": ["Pasternack", "Dan2013] Jeff Pasternack", "Roth Dan"], "venue": "In International Conference on World Wide Web,", "citeRegEx": "Pasternack et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Pasternack et al\\.", "year": 2013}, {"title": "Making better informed trust decisions with generalized fact-finding", "author": ["Pasternack", "Roth2011] Jeff Pasternack", "Dan Roth"], "venue": "In International Joint Conference on Artificial Intelligence", "citeRegEx": "Pasternack et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Pasternack et al\\.", "year": 2011}, {"title": "Effective use of knowledge management systems: a process model of content ratings and credibility indicators", "author": ["Poston", "Speier2005] Robin S. Poston", "Cheri Speier"], "venue": null, "citeRegEx": "Poston et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Poston et al\\.", "year": 2005}, {"title": "Cheap and fast\u2014but is it good?: evaluating non-expert annotations for natural language tasks", "author": ["Snow et al.2008] Rion Snow", "Brendan O\u2019Connor", "Daniel Jurafsky", "Andrew Y. Ng"], "venue": null, "citeRegEx": "Snow et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Snow et al\\.", "year": 2008}, {"title": "Line: Large-scale information network embedding", "author": ["Tang et al.2015] Jian Tang", "Meng Qu", "Mingzhe Wang", "Ming Zhang", "Jun Yan", "Qiaozhu Mei"], "venue": "In International Conference on World Wide Web", "citeRegEx": "Tang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Tang et al\\.", "year": 2015}, {"title": "Aspect level sentiment classification with deep memory", "author": ["Tang et al.2016] Duyu Tang", "Bing Qin", "Ting Liu"], "venue": null, "citeRegEx": "Tang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Tang et al\\.", "year": 2016}, {"title": "On truth discovery in social sensing: A maximum likelihood estimation approach", "author": ["Wang et al.2012] Dong Wang", "Lance Kaplan", "Hieu Le", "Tarek Abdelzaher"], "venue": "In ACM/IEEE International Conference on Information Processing in Sensor Networks,", "citeRegEx": "Wang et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2012}, {"title": "Truth discovery with multiple conflicting information providers on the web", "author": ["Yin et al.2008] Xiaoxin Yin", "Jiawei Han", "Philip S. Yu"], "venue": "IEEE Transactions on Knowledge & Data Engineering,", "citeRegEx": "Yin et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Yin et al\\.", "year": 2008}, {"title": "A probabilistic model for estimating real-valued truth from conflicting sources", "author": ["Zhao", "Han2012] Bo Zhao", "Jiawei Han"], "venue": "Proc.of Intl.workshop on Quality in Databases", "citeRegEx": "Zhao et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Zhao et al\\.", "year": 2012}, {"title": "A bayesian approach to discovering truth from conflicting sources for data integration", "author": ["Zhao et al.2012] Bo Zhao", "Benjamin I.P. Rubinstein", "Jim Gemmell", "Jiawei Han"], "venue": "Proceedings of the Vldb Endowment,", "citeRegEx": "Zhao et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Zhao et al\\.", "year": 2012}], "referenceMentions": [{"referenceID": 19, "context": "Truth discovery task is to find the most credible statement to resolve the confliction (Yin et al., 2008; Dong et al., 2009; Galland et al., 2010).", "startOffset": 87, "endOffset": 146}, {"referenceID": 1, "context": "Truth discovery task is to find the most credible statement to resolve the confliction (Yin et al., 2008; Dong et al., 2009; Galland et al., 2010).", "startOffset": 87, "endOffset": 146}, {"referenceID": 3, "context": "Truth discovery task is to find the most credible statement to resolve the confliction (Yin et al., 2008; Dong et al., 2009; Galland et al., 2010).", "startOffset": 87, "endOffset": 146}, {"referenceID": 10, "context": "The development of the research benefit other natural language processing task like knowledge management (Poston and Speier, 2005), question answering (Banerjee and Han, 2009), information retrieval (Olteanu et al., 2013) and so on.", "startOffset": 199, "endOffset": 221}, {"referenceID": 19, "context": "Previous methods mostly take voting mechanism through predicting the truth (Yin et al., 2008; Snow et al., 2008; Dong et al., 2009; Galland et al., 2010; Dong et al., 2012; Wang et al., 2012; Zhao et al., 2012; Li et al., 2014a; Li et al., 2016; Pasternack and Dan, 2010).", "startOffset": 75, "endOffset": 271}, {"referenceID": 15, "context": "Previous methods mostly take voting mechanism through predicting the truth (Yin et al., 2008; Snow et al., 2008; Dong et al., 2009; Galland et al., 2010; Dong et al., 2012; Wang et al., 2012; Zhao et al., 2012; Li et al., 2014a; Li et al., 2016; Pasternack and Dan, 2010).", "startOffset": 75, "endOffset": 271}, {"referenceID": 1, "context": "Previous methods mostly take voting mechanism through predicting the truth (Yin et al., 2008; Snow et al., 2008; Dong et al., 2009; Galland et al., 2010; Dong et al., 2012; Wang et al., 2012; Zhao et al., 2012; Li et al., 2014a; Li et al., 2016; Pasternack and Dan, 2010).", "startOffset": 75, "endOffset": 271}, {"referenceID": 3, "context": "Previous methods mostly take voting mechanism through predicting the truth (Yin et al., 2008; Snow et al., 2008; Dong et al., 2009; Galland et al., 2010; Dong et al., 2012; Wang et al., 2012; Zhao et al., 2012; Li et al., 2014a; Li et al., 2016; Pasternack and Dan, 2010).", "startOffset": 75, "endOffset": 271}, {"referenceID": 2, "context": "Previous methods mostly take voting mechanism through predicting the truth (Yin et al., 2008; Snow et al., 2008; Dong et al., 2009; Galland et al., 2010; Dong et al., 2012; Wang et al., 2012; Zhao et al., 2012; Li et al., 2014a; Li et al., 2016; Pasternack and Dan, 2010).", "startOffset": 75, "endOffset": 271}, {"referenceID": 18, "context": "Previous methods mostly take voting mechanism through predicting the truth (Yin et al., 2008; Snow et al., 2008; Dong et al., 2009; Galland et al., 2010; Dong et al., 2012; Wang et al., 2012; Zhao et al., 2012; Li et al., 2014a; Li et al., 2016; Pasternack and Dan, 2010).", "startOffset": 75, "endOffset": 271}, {"referenceID": 20, "context": "Previous methods mostly take voting mechanism through predicting the truth (Yin et al., 2008; Snow et al., 2008; Dong et al., 2009; Galland et al., 2010; Dong et al., 2012; Wang et al., 2012; Zhao et al., 2012; Li et al., 2014a; Li et al., 2016; Pasternack and Dan, 2010).", "startOffset": 75, "endOffset": 271}, {"referenceID": 8, "context": "Previous methods mostly take voting mechanism through predicting the truth (Yin et al., 2008; Snow et al., 2008; Dong et al., 2009; Galland et al., 2010; Dong et al., 2012; Wang et al., 2012; Zhao et al., 2012; Li et al., 2014a; Li et al., 2016; Pasternack and Dan, 2010).", "startOffset": 75, "endOffset": 271}, {"referenceID": 17, "context": "Memory network is a framework with a longterm memory as inference component (Weston et al., 2014; Sukhbaatar et al., 2015; Tang et al., 2016).", "startOffset": 76, "endOffset": 141}, {"referenceID": 5, "context": "1 Datasets We use two public datasets (Li et al., 2012) 1 to demonstrate the effectiveness of the proposed method.", "startOffset": 38, "endOffset": 55}, {"referenceID": 5, "context": "(Li et al., 2012) collected stock data from 55 sources on every work data in July 2011.", "startOffset": 0, "endOffset": 17}, {"referenceID": 3, "context": "2-Estimates (Galland et al., 2010): This method is proposed based on the assumption that there is one and only one true value for each entry\u201d.", "startOffset": 12, "endOffset": 34}, {"referenceID": 3, "context": "3-Estimates (Galland et al., 2010): 3-Estimates improves 2-Estimates by considering the difficulty of getting the truth for each entry, the estimation of which will affect the sources weight.", "startOffset": 12, "endOffset": 34}, {"referenceID": 19, "context": "TruthFinder (Yin et al., 2008): TruthFinder adopts Bayesian analysis, in which for each observation, its confidence is calculated as the product of its providers reliability degrees.", "startOffset": 12, "endOffset": 30}, {"referenceID": 1, "context": "AccuSim (Dong et al., 2009): AccuSim also applies Bayesian analysis and it also adopts the usage of the similarity function.", "startOffset": 8, "endOffset": 27}, {"referenceID": 16, "context": "Embedding: We adopt LINE (Tang et al., 2015) word embedding learning algorithm to compare with Word2Vec (Mikolov et al.", "startOffset": 25, "endOffset": 44}, {"referenceID": 9, "context": ", 2015) word embedding learning algorithm to compare with Word2Vec (Mikolov et al., 2013) in truth discovery experiment.", "startOffset": 67, "endOffset": 89}, {"referenceID": 5, "context": "categorize the previous methods (Li et al., 2012).", "startOffset": 32, "endOffset": 49}, {"referenceID": 4, "context": "Corresponding methods are HUB (Kleinberg, 1999), AvgLog (Pasternack and Dan, 2010), Investment (Pasternack and Dan, 2010) and PoolInvestment (Pasternack and Dan, 2010).", "startOffset": 30, "endOffset": 47}, {"referenceID": 3, "context": "Corresponding methods are Cosine (Galland et al., 2010), 2-estimates (Galland et al.", "startOffset": 33, "endOffset": 55}, {"referenceID": 3, "context": ", 2010), 2-estimates (Galland et al., 2010) and 3-estimates (Galland et al.", "startOffset": 21, "endOffset": 43}, {"referenceID": 3, "context": ", 2010) and 3-estimates (Galland et al., 2010).", "startOffset": 24, "endOffset": 46}, {"referenceID": 19, "context": "The corresponding methods include TruthFinder (Yin et al., 2008), AccuPr (Dong et al.", "startOffset": 46, "endOffset": 64}, {"referenceID": 1, "context": ", 2008), AccuPr (Dong et al., 2009), AccuSim (Dong et al.", "startOffset": 16, "endOffset": 35}, {"referenceID": 1, "context": ", 2009), AccuSim (Dong et al., 2009), AccuFormat (Dong et al.", "startOffset": 17, "endOffset": 36}, {"referenceID": 1, "context": ", 2009), AccuFormat (Dong et al., 2009), LCA (Pasternack and Dan, 2013) and CRH (Li et al.", "startOffset": 20, "endOffset": 39}, {"referenceID": 1, "context": "The copying affected method discounts votes from copied observations through computing credibility, such as AccuCopy (Dong et al., 2009).", "startOffset": 117, "endOffset": 136}], "year": 2016, "abstractText": "Truth discovery is to resolve conflicts and find the truth from multiple-source statements. Conventional methods mostly research based on the mutual effect between the reliability of sources and the credibility of statements, however, pay no attention to the mutual effect among the credibility of statements about the same object. We propose memory network based models to incorporate these two ideas to do the truth discovery. We use feedforward memory network and feedback memory network to learn the representation of the credibility of statements which are about the same object. Specially, we adopt memory mechanism to learn source reliability and use it through truth prediction. During learning models, we use multiple types of data (categorical data and continuous data) by assigning different weights automatically in the loss function based on their own effect on truth discovery prediction. The experiment results show that the memory network based models much outperform the state-of-the-art method and other baseline methods.", "creator": "LaTeX with hyperref package"}}}