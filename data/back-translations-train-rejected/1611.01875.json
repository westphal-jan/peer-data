{"id": "1611.01875", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Nov-2016", "title": "Challenges of Feature Selection for Big Data Analytics", "abstract": "We are surrounded by huge amounts of large-scale high dimensional data. It is desirable to reduce the dimensionality of data for many learning tasks due to the curse of dimensionality. Feature selection has shown its effectiveness in many applications by building simpler and more comprehensive model, improving learning performance, and preparing clean, understandable data. Recently, some unique characteristics of big data such as data velocity and data variety present challenges to the feature selection problem. In this paper, we envision these challenges of feature selection for big data analytics. In particular, we first give a brief introduction about feature selection and then detail the challenges of feature selection for structured, heterogeneous and streaming data as well as its scalability and stability issues. At last, to facilitate and promote the feature selection research, we present an open-source feature selection repository (scikit-feature), which consists of most of current popular feature selection algorithms.", "histories": [["v1", "Mon, 7 Nov 2016 02:17:43 GMT  (1127kb)", "http://arxiv.org/abs/1611.01875v1", "Special Issue on Big Data, IEEE Intelligent Systems, 2016. arXiv admin note: text overlap witharXiv:1601.07996"]], "COMMENTS": "Special Issue on Big Data, IEEE Intelligent Systems, 2016. arXiv admin note: text overlap witharXiv:1601.07996", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["jundong li", "huan liu"], "accepted": false, "id": "1611.01875"}, "pdf": {"name": "1611.01875.pdf", "metadata": {"source": "CRF", "title": "Challenges of Feature Selection for Big Data Analytics", "authors": ["Jundong Li"], "emails": ["jundongl@asu.edu", "huan.liu@asu.edu"], "sections": [{"heading": null, "text": "ar Xiv: 161 1.01 875v 1 [cs.L GKeywords. Feature Selection; Big Data; Repository"}, {"heading": "1 A Brief Introduction of Feature Selection", "text": "This year, the time has come for an agreement to be reached, and it will only take a few days."}, {"heading": "2 Challenges of Feature Selection", "text": "Lately, the popularity of big data has presented some challenges to the traditional task of feature selection. In the meantime, some unique features of big data also bring new possibilities for feature selection research. In the next subsections, we will present these challenges of feature selection for big data analysis from the following six aspects: In particular, the challenges of structured features, linked data, multi-source data and multi-view data, streaming data and features from the perspective of data; while the last two challenges are viewed scalability and stability from the perspective of performance."}, {"heading": "2.1 Structured Features", "text": "Most existing feature selection algorithms are designed for generic data, and they are based on the strong assumption that features do not have explicit correlations. In other words, they completely ignore the intrinsic structures between features. For example, these feature selection methods can select the same subset of features even though the features are reshuffled [18]. In many real-world applications, features have different types of structures, such as spatial or temporal smoothness, fragmented groups, overlapping groups, trees, and diagrams. When applying feature selection algorithms to data sets with structured features, it is beneficial to explicitly include this knowledge, which can improve post-learning tasks such as classification and clustering. Next, we will focus on the three most common feature structures, i.e. group structure, tree structure, and diagram structure, we can have different face structures that can have group structures."}, {"heading": "2.2 Linked Data", "text": "Linked data is ubiquitous on many platforms such as Twitter1 (tweets linked by hyperlinks), Facebook social networks (users connected by friendships), and biological networks (protein interactions). Linked data is different from traditional attribute value data because it is correlated with different types of links. Figure 6 is a clear example of linked data and its two representations. Figure 6 (a) shows 8 linked instances (u1 to u8), while Figure 6 (b) is a conventional representation of attribute value data, so that each row corresponds to an instance and each column corresponds to a characteristic. As mentioned above, Linked data is an additional source of information in the form of links that can be represented by an adjacence matrix, as illustrated in Figure 6 (c)."}, {"heading": "2.3 Multi-Source Data and Multi-View Data", "text": "In many data mining and machine learning tasks, we have multiple data sources for the same set of data instances. For example, recent advances in bioinformatics show the existence of some non-coding types of RNA in addition to the widely used messenger RNA. These non-coding types of RNA work across a variety of biological processes, and the availability of multiple data sources allows us to address some issues that would otherwise not be solved with a single source, as the layered representations of data can help reveal some inherent patterns hidden in a single data source. When selecting multiple data sources, 1https: / / twitter.com / 2https: / / www.facebook.com / we usually have one target source, and other sources complement the selection of characteristics at the target source. [19] Multi-view sources represent different facets of data instances across different feature ranges. These feature spaces are, of course, multidimensional and also highly clustered, which indicates that data selection is necessary for these characteristics such as view selection."}, {"heading": "2.4 Streaming Data and Features", "text": "In many scenarios, we are faced with a considerable amount of data that needs to be processed in real time to gain insights. In the worst-case scenario, the size of the data or features is unknown or even infinite, so it is not practical to wait until all data instances or features are available to perform a feature selection. For streaming data, a motivating example is that in online spam email detection new emails are constantly arriving, it is not easy to apply batch mode feature selection methods to select relevant features in a timely manner. Therefore, some feature selection algorithms are suggested to maintain and update a feature subset when new data streams constantly arrive. The process of feature selection in data streams is illustrated in Figure 8. In some settings where the streaming data cannot be loaded into memory, a run of the data is required. We can only make a run of the data in advance if the second pass is either unavailable or expensive."}, {"heading": "2.5 Scalability", "text": "In many scientific and business applications, data is usually measured in terabytes (1TB = 1012 bytes). Normally, data sets in terabyte scale cannot be loaded directly into memory and therefore restrict the usability of most feature selection algorithms. Currently, there are some attempts to use distributed programming frames such as MapReduce and MPI to perform parallel feature selection for very large data sets [14]. Recently, large amounts of ultra-high dimensionality data have emerged in many real-world applications, such as text mining and information gathering. Most feature selection algorithms do not scale well on ultra-high-dimensional data, their efficiency deteriorates rapidly or is even mathematically impossible. In this case, well-designed feature selection algorithms in linear or sublinear runtime are preferred."}, {"heading": "2.6 Stability", "text": "The stability of these algorithms is also an important consideration in the development of new feature selection algorithms [4]. A motivating example from the field of bioinformatics shows that domain experts would like to see the same set or similar sets of genes (characteristics) selected each time they receive new samples in the small amount of perturbations. Otherwise, domain experts would not trust these algorithms if they received very different characteristics with little data perturbation. it is also noted that the underlying characteristics of data can strongly influence the stability of feature selection algorithms and the stability problem can also be data dependent. These factors include the dimensionality of the characteristics, the number of data instances, etc. In contrast to supervised feature selection, the stability of unguarded feature selection algorithms is not yet well studied. Studying stability for uncontrolled feature selection is much more difficult than that of the monitored methods."}, {"heading": "3 Feature Selection Repository", "text": "To address the challenges of feature selection for big data analytics and promote feature selection research in this community, we present an open source feature selection repository - Scikit feature (http: / / featureselection.asu.edu /). The purpose of this feature selection repository is to collect some widely used feature selection algorithms developed in feature selection research to facilitate their application, comparison and joint study. The feature selection repository also effectively helps researchers achieve a more reliable evaluation in the process of developing new feature selection algorithms. Currently, the scikit feature consists of popular feature selection algorithms in the following categories: \u2022 similarity-based feature selection \u2022 information-theoretical feature selection \u2022 sparse-based feature selection algorithms \u2022 also comply with these feature selection methods \u2022 feature-based structural categories."}, {"heading": "Acknowledgements", "text": "This material is partially funded by the National Science Foundation (NSF) under grant number IIS-1217466."}], "references": [{"title": "Featureminer: A tool for interactive feature selection", "author": ["Kewei Cheng", "Jundong Li", "Huan Liu"], "venue": "In CIKM,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2016}, {"title": "Stable feature selection for biomarker discovery", "author": ["Zengyou He", "Weichuan Yu"], "venue": "Computational biology and chemistry,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2010}, {"title": "Unsupervised feature selection on data streams", "author": ["Hao Huang", "Shinjae Yoo", "S Kasiviswanathan"], "venue": "In CIKM, pages 1031\u20131040,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2015}, {"title": "Structured variable selection with sparsity-inducing norms", "author": ["Rodolphe Jenatton", "Jean-Yves Audibert", "Francis Bach"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2011}, {"title": "Feature selection: A data perspective", "author": ["Jundong Li", "Kewei Cheng", "Suhang Wang", "Fred Morstatter", "Robert P Trevino", "Jiliang Tang", "Huan Liu"], "venue": "arXiv preprint arXiv:1601.07996,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2016}, {"title": "Toward time-evolving feature selection on dynamic networks", "author": ["Jundong Li", "Xia Hu", "Ling Jian", "Huan Liu"], "venue": "In ICDM,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2016}, {"title": "Unsupervised streaming feature selection in social media", "author": ["Jundong Li", "Xia Hu", "Jiliang Tang", "Huan Liu"], "venue": "In CIKM,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2015}, {"title": "Robust unsupervised feature selection on networked data", "author": ["Jundong Li", "Xia Hu", "LiangWu", "Huan Liu"], "venue": "In SDM,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2016}, {"title": "Computational methods of feature selection", "author": ["Huan Liu", "Hiroshi Motoda"], "venue": "CRC Press,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2007}, {"title": "Supervised group lasso with applications to microarray data analysis", "author": ["Shuangge Ma", "Xiao Song", "Jian Huang"], "venue": "BMC bioinformatics,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2007}, {"title": "Subband correlation and robust speech recognition", "author": ["James McAuley", "Ji Ming", "Darryl Stewart", "Philip Hanna"], "venue": "IEEE Transactions on Speech and Audio Processing,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2005}, {"title": "Parallel large scale feature selection for logistic regression", "author": ["Sameer Singh", "Jeremy Kubica", "Scott Larsen", "Daria Sorokina"], "venue": "In SDM,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2009}, {"title": "Unsupervised feature selection for multi-view data in social media", "author": ["Jiliang Tang", "Xia Hu", "Huiji Gao", "Huan Liu"], "venue": "In SDM,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2013}, {"title": "Unsupervised feature selection for linked social media data", "author": ["Jiliang Tang", "Huan Liu"], "venue": "In KDD,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2012}, {"title": "Multi-view clustering and feature learning via structured sparsity", "author": ["Hua Wang", "Feiping Nie", "Heng Huang"], "venue": "In ICML,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2013}, {"title": "Sparse methods for biomedical data", "author": ["Jieping Ye", "Jun Liu"], "venue": "ACM SIGKDD Explorations Newsletter,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2012}, {"title": "Spectral feature selection for data mining", "author": ["Zheng Alan Zhao", "Huan Liu"], "venue": "CRC Press,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2011}], "referenceMentions": [{"referenceID": 8, "context": "Feature selection, as a type of dimension reduction technique, has been proven to be effective and efficient in handling high dimensional data [11, 7].", "startOffset": 143, "endOffset": 150}, {"referenceID": 4, "context": "Feature selection, as a type of dimension reduction technique, has been proven to be effective and efficient in handling high dimensional data [11, 7].", "startOffset": 143, "endOffset": 150}, {"referenceID": 15, "context": "For example, these feature selection methods may select the same subset of features even though the features are reshuffled [18].", "startOffset": 124, "endOffset": 128}, {"referenceID": 10, "context": "Examples of group structured features include different frequency bands represented as groups in signal processing [13] and genes with similar functionalities acting as groups in bioinformatics [12].", "startOffset": 115, "endOffset": 119}, {"referenceID": 9, "context": "Examples of group structured features include different frequency bands represented as groups in signal processing [13] and genes with similar functionalities acting as groups in bioinformatics [12].", "startOffset": 194, "endOffset": 198}, {"referenceID": 3, "context": "Another motivating example is that genes/proteins may form certain hierarchical tree structures [6].", "startOffset": 96, "endOffset": 99}, {"referenceID": 5, "context": "The challenges of feature selection for linked data [8, 10, 16] lie in the following three aspects: (1) how to exploit relations among data instances; (2) how to take advantage of these relations for feature selection; and (3) linked data are often unlabeled, how to evaluate the relevance of features without the guide of label information.", "startOffset": 52, "endOffset": 63}, {"referenceID": 7, "context": "The challenges of feature selection for linked data [8, 10, 16] lie in the following three aspects: (1) how to exploit relations among data instances; (2) how to take advantage of these relations for feature selection; and (3) linked data are often unlabeled, how to evaluate the relevance of features without the guide of label information.", "startOffset": 52, "endOffset": 63}, {"referenceID": 13, "context": "The challenges of feature selection for linked data [8, 10, 16] lie in the following three aspects: (1) how to exploit relations among data instances; (2) how to take advantage of these relations for feature selection; and (3) linked data are often unlabeled, how to evaluate the relevance of features without the guide of label information.", "startOffset": 52, "endOffset": 63}, {"referenceID": 16, "context": "we usually have a target source and other sources complement the selection of features on the target source [19].", "startOffset": 108, "endOffset": 112}, {"referenceID": 12, "context": "A task of multi-view feature selection thus arises, which aims to select features from different feature spaces simultaneously by using their relations [15, 17].", "startOffset": 152, "endOffset": 160}, {"referenceID": 14, "context": "A task of multi-view feature selection thus arises, which aims to select features from different feature spaces simultaneously by using their relations [15, 17].", "startOffset": 152, "endOffset": 160}, {"referenceID": 2, "context": "How to select relevant features timely by one pass of data [5] is still a challenging problem.", "startOffset": 59, "endOffset": 62}, {"referenceID": 6, "context": "Therefore, it is more preferable to perform streaming feature selection to rapidly adapt to the changes [9].", "startOffset": 104, "endOffset": 107}, {"referenceID": 11, "context": "Currently, there are some attempts to use distributed programming frameworks such as MapReduce and MPI to perform parallel feature selection for very largescale datasets [14].", "startOffset": 170, "endOffset": 174}, {"referenceID": 1, "context": "6 Stability The stability of these algorithms is also an important consideration when developing new feature selection algorithms [4].", "startOffset": 130, "endOffset": 133}, {"referenceID": 0, "context": "We also provide an interactive tool FeatureMiner [2] to ease the usage of these feature selection algorithms based on the repository.", "startOffset": 49, "endOffset": 52}], "year": 2016, "abstractText": "We are surrounded by huge amounts of large-scale high dimensional data. It is desirable to reduce the dimensionality of data for many learning tasks due to the curse of dimensionality. Feature selection has shown its effectiveness in many applications by building simpler and more comprehensive model, improving learning performance, and preparing clean, understandable data. Recently, some unique characteristics of big data such as data velocity and data variety present challenges to the feature selection problem. In this paper, we envision these challenges of feature selection for big data analytics. In particular, we first give a brief introduction about feature selection and then detail the challenges of feature selection for structured, heterogeneous and streaming data as well as its scalability and stability issues. At last, to facilitate and promote the feature selection research, we present an open-source feature selection repository (scikit-feature), which consists of most of current popular feature selection algorithms.", "creator": "LaTeX with hyperref package"}}}