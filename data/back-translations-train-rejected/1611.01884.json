{"id": "1611.01884", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Nov-2016", "title": "AC-BLSTM: Asymmetric Convolutional Bidirectional LSTM Networks for Text Classification", "abstract": "Recently deeplearning models have been shown to be capable of making remarkable performance in sentences and documents classification tasks. In this work, we propose a novel framework called AC-BLSTM for modeling setences and documents, which combines the asymmetric convolution neural network (ACNN) with the Bidirectional Long Short-Term Memory network (BLSTM). Experiment results demonstrate that our model achieves state-of-the-art results on all six tasks, including sentiment analysis, question type classification, and subjectivity classification.", "histories": [["v1", "Mon, 7 Nov 2016 03:39:52 GMT  (112kb,D)", "https://arxiv.org/abs/1611.01884v1", "7 pages"], ["v2", "Thu, 15 Dec 2016 03:22:12 GMT  (112kb,D)", "http://arxiv.org/abs/1611.01884v2", "7 pages"], ["v3", "Mon, 5 Jun 2017 03:47:15 GMT  (114kb,D)", "http://arxiv.org/abs/1611.01884v3", "9 pages"]], "COMMENTS": "7 pages", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["depeng liang", "yongdong zhang"], "accepted": false, "id": "1611.01884"}, "pdf": {"name": "1611.01884.pdf", "metadata": {"source": "CRF", "title": "AC-BLSTM: Asymmetric Convolutional Bidirectional LSTM Networks for Text Classification", "authors": ["Depeng Liang", "Yongdong Zhang", "Guang Zhou"], "emails": ["liangdp@mail2.sysu.edu.cn", "lnszyd@mail.sysu.edu.cn", "lnszyd@mail.sysu.edu.cn."], "sections": [{"heading": null, "text": "In this paper, we propose a novel framework called AC-BLSTM for modeling sentences and documents that combines the asymmetric folding neural network (ACNN) with the bidirectional Long ShortTerm Memory Network (BLSTM). Experimental results show that our model achieves current results for five tasks, including mood analysis, question type classification, and subjectivity classification. To further improve the performance of AC-BLSTM, we propose a semi-supervised learning framework called G-AC-BLSTM for text classification by combining the generative model with AC-BLSTM."}, {"heading": "1 Introduction", "text": "In recent years, we have seen remarkable results in the areas of computer vision (Krizhevsky et al., 2012; Szegedy et al., 2015a; Simonyan and Zisserman, 2014; He and his colleagues, 2015), and a number of NLP tasks (Kim, 2014; Zhou et al., 2015), and issues (Sukhbaatar et al., 2015) that address the question of how to behave."}, {"heading": "2 Related Work", "text": "Recently, we have made remarkable progress on various NLP tasks, such as word embedding (Mikolov et al., 2013; Pennington et al., 2014), questions (Sukhbaatar et al., 2015), emotion analysis (Tang et al., 2015; Chen et al., 2016a, b), machine translation (Sutskever et al., 2014), and so on. CNNs and RNNNs are two wildly used architectures among these models. The success of deep learning models for NLP mostly relates to the progress in learning distributed word representations (Mikolov et al., 2013; Pennington et al., 2014). In these models, instead of uniform vectors, words are represented as low-dimensional and dense vectors encoding both semantic and syntactic information from words by indexing words into a vocabulary."}, {"heading": "3 AC-BLSTM Model", "text": "In this section, we present our AC-BLSTM architecture in detail. First, we describe the ACNN, which uses the word vector matrix of the sentence as input and produces an overarching representation of word characteristics. Then, we introduce the BLSTM, which can include context on both sides of any position in the input sequence. Finally, we present the techniques to avoid overfitting in our model. A general illustration of our architecture is shown in Figure 1."}, {"heading": "3.1 Asymmetric Convolution", "text": "In fact, it is as if the EU Commission's allegations against the EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU, the EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU Commission, the Commission, the Commission, the Commission, the Commission, the EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU, the EU Commission, the EU, the EU, the EU, the EU, the EU Commission, the EU, the EU, the EU, the EU, the EU, the EU, the EU, the EU, the EU, the EU, the EU, the EU, the EU, the EU, the EU, the EU, the EU, the EU, the EU, the EU, the EU, the EU, the EU, the EU, the EU, the EU, the EU, the EU, the EU, the EU, the EU, the EU, the EU, the EU, the EU, the EU, the EU, the EU, the EU, the EU, the EU, the EU, the EU, the EU, the EU, the EU, the EU, in, the EU, the EU, the EU, the EU, the EU, the EU, the EU, in, the EU, the EU, the EU, the EU, the EU, the EU, the EU, the EU, the EU, the EU, the EU, the EU, in, the EU, the EU, the EU, the EU, the EU, the EU, the EU, the EU, the EU, the EU, in, the EU, the EU, the EU, the EU, the EU,"}, {"heading": "3.2 Bidirectional Long Short-Term Memory Network", "text": "Initially introduced in (Hochreiter and Schmidhuber, 1997) and recently demonstrated as a successful model, LSTM is an RNN architecture specifically designed to bridge long time delays between relevant input and target events, making it suitable for problems requiring long-term context, such as handwriting recognition, machine translation, etc. For many sequence processing tasks, it is useful to analyze both the future and past of a particular point in the series. While standard RNNNNs only use the previous context, BLSTM (Graves et al., 2005) is explicitly designed to learn long-term dependencies of a particular point on both sides, which has also proven to be more powerful than other neural network architectures within the framework of phoneme recognition (Graves and Schmidhuber, 2005). Therefore, we select BLSTM on the top of the ACNN in order to learn such dependencies in view of the overall STM STM end sequence and STM end sequence without any problems."}, {"heading": "3.3 Semi-supervised Framework", "text": "Our semi-supervised text classification framework is based on work (Odena, 2016; Salimans et al., 2016). We assume that the original classifier divides a sample into one of the possible K classes. Thus, we can perform semi-supervised learning simply by adding samples from a generative network G to our data set and labeling them as an additional class y = K + 1. And accordingly, the dimension of our classification performance increases from K to K + 1. The configuration of our generator network G is based on the architecture proposed in Radford et al., 2015. And we modify the architecture to make it suitable for text classification tasks. Table 1 shows the configuration of each layer in generator G. Lets assumes that the training batch size m and the percentage of samples generated within a stack is training samples pg. At each iteration of the training process, we first generate m \u2212 pg samples from the generator, then we retrieve the sample G \u00b7 pm \u2212 sampling."}, {"heading": "3.4 Regularization", "text": "In our model, we apply dropouts to the input function of the BLSTM and the output of the BLSTM before the Softmax layer. And we apply batch normalization to the results of each folding operation just before the relay activation. During the training, after we get the gradients of the ACBLSTM network, we first compute the L2 standard of all gradients and add them together to get the sum norm. Then, we compare the sum norm to 0.5. If the sum norm is greater than 0.5, we multiply all gradients by 0.5 / sum norm, otherwise we simply use the original gradients to update the weights."}, {"heading": "4 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Datasets", "text": "We evaluate our model using various benchmarks. Stanford Sentiment Treebank (SST-71) is a popular sentiment classification data set introduced by (Socher et al., 2013), with fine-grained labeling (SST-1): very negative, negative, neutral, positive, very positive. The data set has been divided into 8,544 training sessions, 1,101 validation and 2,210 test sets. By removing the neutral sentences, SST can also be used for binary classification (SST2), which has been divided into 6,920 training sessions, 872 validation and 1,821 tests. As the data is provided in subset format, we train the model on both phrases and sentences, but only test on the sentences as in several previous work (Socher et al., 2013; Kalchbrenner et al., 2014).Movie Review Data (MR) is a data set that (Pang and Lee 2005, Movie Treebank) is a data set of 5,000 for sensitization."}, {"heading": "4.2 Training and Implementation Details", "text": "We implement our model based on Mxnet (Chen et al., 2015) - a C + + library that provides a deep learning framework designed for both efficiency and flexibility. To benefit from the efficiency of the parallel calculation of the tensors, we train our model on an Nvidia GTX 1070 GPU. Training is done by stochastic gradient descent via mixed mini-batches with the optimizer RMSprop (Tieleman and Hinton, 2012). For all experiments, we apply three asymmetric folding operations with the second filter length ki of 2, 3, 4 to the input, set the drop-out rate to 0.5 before feeding the feature into BLSTM, and set the initial learning rate to 0.0001. But there are some hyperparameters that are not the same for all datasets listed in Table 2."}, {"heading": "4.3 Word Vector Initialization", "text": "We use publicly available word2vec vectors trained on 100 billion words of Google News. The vectors have a dimensionality of 300 and were trained using the continuous sack-of-word architecture (Mikolov et al., 2013). Words that do not appear in the set of pre-trained words are initialized from the uniform distribution [-0.25, 0.25]. We fix the word vectors and learn only the other parameters of the model during the training."}, {"heading": "4.4 Results and Discussion", "text": "The results of our models against other methods are listed in Table 4. To the best of our knowledge, AC-BLSTM achieves the best results for five tasks. By simply using word2vec vectors, our model can achieve better results than (Zhang et al., 2016b), which combines several word embedding methods such as word2vec (Mikolov et al., 2013), glove (Pennington et al., 2014) and syntactic embedding of dimensions. And AC-BLSTM performs better when trained with the semi-monitored frame, which proves the success of combining the result with the hydration rate."}, {"heading": "5 Conclusions", "text": "In this paper, we proposed AC-BLSTM: a novel framework that combines asymmetric Convolutionary Neural Networks with bi-directional Long-Term Short-Term Memory Networks. Asymmetric Convolutionary Layers are able to learn features at the phrase level, and then output sequences of such higher representations are fed into BLSTM to learn long-term dependencies from a given point on both sides. To our knowledge, the AC-BLSTM model achieves excellence in standard sentiment classification, question classification, and document categorization tasks, and then we proposed a semi-supervised text classification framework that further improves AC-BLSTM's performance. In future work, we plan to examine the combination of multiple word embeddings described in (Zhang et al., 2016b)."}], "references": [{"title": "Neural sentiment classification with user and product attention", "author": ["Huimin Chen", "Maosong Sun", "Cunchao Tu", "Yankai Lin", "Zhiyuan Liu"], "venue": "In EMNLP,", "citeRegEx": "Chen et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2016}, {"title": "Learning user and product distributed representations using a sequence model for sentiment analysis", "author": ["Tao Chen", "Ruifeng Xu", "Yulan He", "Yunqing Xia", "Xuan Wang"], "venue": "IEEE Computational Intelligence Magazine,", "citeRegEx": "Chen et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2016}, {"title": "Framewise phoneme classification with bidirectional LSTM and other neural network architectures", "author": ["Alex Graves", "J\u00fcrgen Schmidhuber"], "venue": "Neural Networks,", "citeRegEx": "Graves and Schmidhuber.,? \\Q2005\\E", "shortCiteRegEx": "Graves and Schmidhuber.", "year": 2005}, {"title": "Bidirectional LSTM networks for improved phoneme classification and recognition", "author": ["Alex Graves", "Santiago Fern\u00e1ndez", "J\u00fcrgen Schmidhuber"], "venue": "In ICANN,", "citeRegEx": "Graves et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2005}, {"title": "Deep residual learning for image recognition", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"], "venue": "In arXiv prepring arXiv:1506.01497,", "citeRegEx": "He et al\\.,? \\Q2015\\E", "shortCiteRegEx": "He et al\\.", "year": 2015}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural Computation,", "citeRegEx": "Hochreiter and Schmidhuber.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["Sergey Ioffe", "Christian Szegedy"], "venue": "In ICML,", "citeRegEx": "Ioffe and Szegedy.,? \\Q2015\\E", "shortCiteRegEx": "Ioffe and Szegedy.", "year": 2015}, {"title": "A convolutional neural network for modelling sentences", "author": ["Nal Kalchbrenner", "Edward Grefenstette", "Phil Blunsom"], "venue": "In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "Kalchbrenner et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2014}, {"title": "Convolutional neural networks for sentence classification", "author": ["Yoon Kim"], "venue": "In EMNLP, pages 1746\u20131751,", "citeRegEx": "Kim.,? \\Q2014\\E", "shortCiteRegEx": "Kim.", "year": 2014}, {"title": "Structural attention neural networks for improved sentiment analysis", "author": ["Filippos Kokkinos", "Alexandros Potamianos"], "venue": null, "citeRegEx": "Kokkinos and Potamianos.,? \\Q2017\\E", "shortCiteRegEx": "Kokkinos and Potamianos.", "year": 2017}, {"title": "Molding cnns for text: non-linear, non-consecutive convolutions", "author": ["Tao Lei", "Regina Barzilay", "Tommi Jaakkola"], "venue": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Lei et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lei et al\\.", "year": 2015}, {"title": "Learning question classifiers", "author": ["Xin Li", "Dan Roth"], "venue": "In Proceedings of the 19th international conference on Computational linguistics-Volume", "citeRegEx": "Li and Roth.,? \\Q2002\\E", "shortCiteRegEx": "Li and Roth.", "year": 2002}, {"title": "Rectified linear units improve restricted boltzmann machines", "author": ["Vinod Nair", "Geoffrey E. Hinton"], "venue": "In ICML,", "citeRegEx": "Nair and Hinton.,? \\Q2010\\E", "shortCiteRegEx": "Nair and Hinton.", "year": 2010}, {"title": "Semi-supervised learning with generative adversarial networks", "author": ["Augustus Odena"], "venue": "arXiv preprint arXiv:1606.01583,", "citeRegEx": "Odena.,? \\Q2016\\E", "shortCiteRegEx": "Odena.", "year": 2016}, {"title": "A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts", "author": ["Bo Pang", "Lillian Lee"], "venue": "In Proceedings of the 42nd annual meeting on Association for Computational Linguistics,", "citeRegEx": "Pang and Lee.,? \\Q2004\\E", "shortCiteRegEx": "Pang and Lee.", "year": 2004}, {"title": "Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales", "author": ["Bo Pang", "Lillian Lee"], "venue": "In ACL,", "citeRegEx": "Pang and Lee.,? \\Q2005\\E", "shortCiteRegEx": "Pang and Lee.", "year": 2005}, {"title": "Glove: Global vectors for word representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher D. Manning"], "venue": "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Pennington et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "A meeting of SIGDAT, a Special Interest Group of the ACL, pages 1532\u20131543", "author": ["Doha", "Qatar"], "venue": null, "citeRegEx": "Doha and Qatar,? \\Q2014\\E", "shortCiteRegEx": "Doha and Qatar", "year": 2014}, {"title": "A bidirectional recurrent neural language model for machine translation", "author": ["Alvaro Peris", "Francisco Casacuberta"], "venue": "Procesamiento del Lenguaje Natural,", "citeRegEx": "Peris and Casacuberta.,? \\Q2015\\E", "shortCiteRegEx": "Peris and Casacuberta.", "year": 2015}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["Karen Simonyan", "Andrew Zisserman"], "venue": "CoRR, abs/1409.1556,", "citeRegEx": "Simonyan and Zisserman.,? \\Q2014\\E", "shortCiteRegEx": "Simonyan and Zisserman.", "year": 2014}, {"title": "Recursive deep models for semantic compositionality over a sentiment treebank", "author": ["Richard Socher", "Alex Perelygin", "Jean Y Wu", "Jason Chuang", "Christopher D Manning", "Andrew Y Ng", "Christopher Potts"], "venue": "In EMNLP,", "citeRegEx": "Socher et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Dropout: a simple way to prevent neural networks from overfitting", "author": ["Nitish Srivastava", "Geoffrey E. Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Srivastava et al\\.,? \\Q1929\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 1929}, {"title": "End-to-end memory networks", "author": ["Sainbayar Sukhbaatar", "Arthur Szlam", "Jason Weston", "Rob Fergus"], "venue": "In Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems", "citeRegEx": "Sukhbaatar et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sukhbaatar et al\\.", "year": 2015}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V. Le"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Rethinking the inception architecture for computer", "author": ["Christian Szegedy", "Vincent Vanhoucke", "Sergey Ioffe", "Jonathon Shlens", "Zbigniew Wojna"], "venue": "vision. CoRR,", "citeRegEx": "Szegedy et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Szegedy et al\\.", "year": 2015}, {"title": "Learning semantic representations of users and products for document level sentiment classification", "author": ["Duyu Tang", "Bing Qin", "Ting Liu"], "venue": "In ACL,", "citeRegEx": "Tang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Tang et al\\.", "year": 2015}, {"title": "Dependency sensitive convolutional neural networks for modeling sentences and documents", "author": ["Rui Zhang", "Honglak Lee", "Dragomir Radev"], "venue": "In Proceedings of NAACL-HLT,", "citeRegEx": "Zhang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2016}, {"title": "Mgnc-cnn: A simple approach to exploiting multiple word embeddings for sentence classification", "author": ["Ye Zhang", "Stephen Roller", "Byron C. Wallace"], "venue": "In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computa-", "citeRegEx": "Zhang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2016}, {"title": "Topic-aware deep compositional models for sentence classification", "author": ["Rui Zhao", "Kezhi Mao"], "venue": "IEEE/ACM Trans. Audio, Speech & Language Processing,", "citeRegEx": "Zhao and Mao.,? \\Q2017\\E", "shortCiteRegEx": "Zhao and Mao.", "year": 2017}, {"title": "A C-LSTM neural network for text classification", "author": ["Chunting Zhou", "Chonglin Sun", "Zhiyuan Liu", "Francis C.M. Lau"], "venue": "CoRR, abs/1511.08630,", "citeRegEx": "Zhou et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zhou et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 19, "context": "Deep neural models recently have achieved remarkable results in computer vision (Krizhevsky et al., 2012; Szegedy et al., 2015a; Simonyan and Zisserman, 2014; He et al., 2015), and a range of NLP tasks such as sentiment classification (Kim, 2014; Zhou et al.", "startOffset": 80, "endOffset": 175}, {"referenceID": 4, "context": "Deep neural models recently have achieved remarkable results in computer vision (Krizhevsky et al., 2012; Szegedy et al., 2015a; Simonyan and Zisserman, 2014; He et al., 2015), and a range of NLP tasks such as sentiment classification (Kim, 2014; Zhou et al.", "startOffset": 80, "endOffset": 175}, {"referenceID": 8, "context": ", 2015), and a range of NLP tasks such as sentiment classification (Kim, 2014; Zhou et al., 2015; Kalchbrenner et al., 2014), and questionanswering (Sukhbaatar et al.", "startOffset": 67, "endOffset": 124}, {"referenceID": 29, "context": ", 2015), and a range of NLP tasks such as sentiment classification (Kim, 2014; Zhou et al., 2015; Kalchbrenner et al., 2014), and questionanswering (Sukhbaatar et al.", "startOffset": 67, "endOffset": 124}, {"referenceID": 7, "context": ", 2015), and a range of NLP tasks such as sentiment classification (Kim, 2014; Zhou et al., 2015; Kalchbrenner et al., 2014), and questionanswering (Sukhbaatar et al.", "startOffset": 67, "endOffset": 124}, {"referenceID": 22, "context": ", 2014), and questionanswering (Sukhbaatar et al., 2015).", "startOffset": 31, "endOffset": 56}, {"referenceID": 6, "context": "2015b), Batchnorm (Ioffe and Szegedy, 2015) and Residual Network (He et al.", "startOffset": 18, "endOffset": 43}, {"referenceID": 4, "context": "2015b), Batchnorm (Ioffe and Szegedy, 2015) and Residual Network (He et al., 2015) have also made the performance even better.", "startOffset": 65, "endOffset": 82}, {"referenceID": 5, "context": "LSTMs (Hochreiter and Schmidhuber, 1997) were designed for better remembering and memory accesses, which can also avoid the problem of gradient exploding or vanishing in the standard RNN.", "startOffset": 6, "endOffset": 40}, {"referenceID": 3, "context": "Be capable of incorporating context on both sides of every position in the input sequence, BLSTMs introduced in (Graves et al., 2005; Graves and Schmidhuber, 2005) have reported to achieve great performance in Handwriting Recognition (Liwicki et al.", "startOffset": 112, "endOffset": 163}, {"referenceID": 2, "context": "Be capable of incorporating context on both sides of every position in the input sequence, BLSTMs introduced in (Graves et al., 2005; Graves and Schmidhuber, 2005) have reported to achieve great performance in Handwriting Recognition (Liwicki et al.", "startOffset": 112, "endOffset": 163}, {"referenceID": 18, "context": ", 2007), and Machine Translation (Peris and Casacuberta, 2015) tasks.", "startOffset": 33, "endOffset": 62}, {"referenceID": 13, "context": "Several recent papers have also extended GANs to the semi-supervised context (Odena, 2016; Salimans et al., 2016) by simply increasing the dimension of the classifier output from K to K + 1, which the samples of the extra class are generated by G.", "startOffset": 77, "endOffset": 113}, {"referenceID": 13, "context": "Further more, inspired by the ideas of extending GANs to the semi-supervised learning context by (Odena, 2016; Salimans et al., 2016), we propose a semi-supervised learning framework for text classification which further improve the performance of AC-BLSTM.", "startOffset": 97, "endOffset": 133}, {"referenceID": 16, "context": "For example, word embeddings (Mikolov et al., 2013; Pennington et al., 2014), question answearing (Sukhbaatar et al.", "startOffset": 29, "endOffset": 76}, {"referenceID": 22, "context": ", 2014), question answearing (Sukhbaatar et al., 2015), sentiment analysis (Tang et al.", "startOffset": 29, "endOffset": 54}, {"referenceID": 23, "context": ", 2016a,b), machine translation (Sutskever et al., 2014) and so on.", "startOffset": 32, "endOffset": 56}, {"referenceID": 16, "context": "The success of deep learning models for NLP mostly relates to the progress in learning distributed word representations (Mikolov et al., 2013; Pennington et al., 2014).", "startOffset": 120, "endOffset": 167}, {"referenceID": 8, "context": "Our model mostly relates to (Kim, 2014) which combines CNNs of different filter lengths and either static or fine-tuned word vectors, and (Zhou et al.", "startOffset": 28, "endOffset": 39}, {"referenceID": 29, "context": "Our model mostly relates to (Kim, 2014) which combines CNNs of different filter lengths and either static or fine-tuned word vectors, and (Zhou et al., 2015) which stacks CNN and LSTM in a unified architecture with static word vectors.", "startOffset": 138, "endOffset": 157}, {"referenceID": 8, "context": "Then instead of employing the ki \u00d7 d convolution operation described in (Kim, 2014; Zhou et al., 2015), we apply the asymmetric convolution operation inspired by (Szegedy et al.", "startOffset": 72, "endOffset": 102}, {"referenceID": 29, "context": "Then instead of employing the ki \u00d7 d convolution operation described in (Kim, 2014; Zhou et al., 2015), we apply the asymmetric convolution operation inspired by (Szegedy et al.", "startOffset": 72, "endOffset": 102}, {"referenceID": 12, "context": "In our case, we choose ReLU (Nair and Hinton, 2010) as the nonlinear function.", "startOffset": 28, "endOffset": 51}, {"referenceID": 5, "context": "First introduced in (Hochreiter and Schmidhuber, 1997) and shown as a successful model recently, LSTM is a RNN architecture specifically designed to bridge long time delays between relevant input and target events, making it suitable for problems where long range context is required, such as handwriting recognition, machine translation and so on.", "startOffset": 20, "endOffset": 54}, {"referenceID": 3, "context": "Whereas standard RNNs make use of previous context only, BLSTM (Graves et al., 2005) is explicitly designed for learning long-term dependencies of a given point on both side, which has also been shown to outperform other neural network architectures in framewise phoneme recognition (Graves and Schmidhuber, 2005).", "startOffset": 63, "endOffset": 84}, {"referenceID": 2, "context": ", 2005) is explicitly designed for learning long-term dependencies of a given point on both side, which has also been shown to outperform other neural network architectures in framewise phoneme recognition (Graves and Schmidhuber, 2005).", "startOffset": 206, "endOffset": 236}, {"referenceID": 13, "context": "Our semi-supervised text classification framewrok is inspired by works (Odena, 2016; Salimans et al., 2016).", "startOffset": 71, "endOffset": 107}, {"referenceID": 6, "context": ", 2014) and batch normalization (Ioffe and Szegedy, 2015).", "startOffset": 32, "endOffset": 57}, {"referenceID": 20, "context": "Stanford Sentiment Treebank (SST) is a popular sentiment classification dataset introduced by (Socher et al., 2013).", "startOffset": 94, "endOffset": 115}, {"referenceID": 20, "context": "Since the data is provided in the format of sub-sentences, we train the model on both phrases and sentences but only test on the sentences as in several previous works (Socher et al., 2013; Kalchbrenner et al., 2014).", "startOffset": 168, "endOffset": 216}, {"referenceID": 7, "context": "Since the data is provided in the format of sub-sentences, we train the model on both phrases and sentences but only test on the sentences as in several previous works (Socher et al., 2013; Kalchbrenner et al., 2014).", "startOffset": 168, "endOffset": 216}, {"referenceID": 15, "context": "Movie Review Data (MR) proposed by (Pang and Lee, 2005) is another dataset for sentiment analysis of movie reviews.", "startOffset": 35, "endOffset": 55}, {"referenceID": 14, "context": "Furthermore, we apply AC-BLSTM on the subjectivity classification dataset (SUBJ) released by (Pang and Lee, 2004).", "startOffset": 93, "endOffset": 113}, {"referenceID": 11, "context": "We also benchmark our system on question type classification task (TREC) (Li and Roth, 2002), where sentences are questions in the following 6 classes: abbreviation, human, entity, description, location, numeric.", "startOffset": 73, "endOffset": 92}, {"referenceID": 25, "context": "For document-level dataset, we use the sentiment classification dataset Yelp 2013 (YELP13) with user and product information, which is built by (Tang et al., 2015).", "startOffset": 144, "endOffset": 163}, {"referenceID": 8, "context": "Compared to methods (Kim, 2014) and (Zhou et al.", "startOffset": 20, "endOffset": 31}, {"referenceID": 29, "context": "Compared to methods (Kim, 2014) and (Zhou et al., 2015), which inspired our model mostly, ACBLSTM can achieve better performance which show that deeper model actually has better performance.", "startOffset": 36, "endOffset": 55}, {"referenceID": 16, "context": ", 2013), glove (Pennington et al., 2014) and Syntactic embedding.", "startOffset": 15, "endOffset": 40}], "year": 2017, "abstractText": "Recently deeplearning models have been shown to be capable of making remarkable performance in sentences and documents classification tasks. In this work, we propose a novel framework called AC-BLSTM for modeling sentences and documents, which combines the asymmetric convolution neural network (ACNN) with the Bidirectional Long ShortTerm Memory network (BLSTM). Experiment results demonstrate that our model achieves state-ofthe-art results on five tasks, including sentiment analysis, question type classification, and subjectivity classification. In order to further improve the performance of AC-BLSTM, we propose a semi-supervised learning framework called G-AC-BLSTM for text classification by combining the generative model with AC-BLSTM.", "creator": "LaTeX with hyperref package"}}}