{"id": "1611.01964", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Nov-2016", "title": "Log-time and Log-space Extreme Classification", "abstract": "We present LTLS, a technique for multiclass and multilabel prediction that can perform training and inference in logarithmic time and space. LTLS embeds large classification problems into simple structured prediction problems and relies on efficient dynamic programming algorithms for inference. We train LTLS with stochastic gradient descent on a number of multiclass and multilabel datasets and show that despite its small memory footprint it is often competitive with existing approaches.", "histories": [["v1", "Mon, 7 Nov 2016 10:10:43 GMT  (13kb)", "http://arxiv.org/abs/1611.01964v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["kalina jasinska", "nikos karampatziakis"], "accepted": false, "id": "1611.01964"}, "pdf": {"name": "1611.01964.pdf", "metadata": {"source": "CRF", "title": "Log-time and Log-space Extreme Classification", "authors": ["Kalina Jasinska", "Nikos Karampatziakis"], "emails": ["kjasinska@cs.put.poznan.pl", "nikosk@microsoft.com"], "sections": [{"heading": null, "text": "ar Xiv: 161 1,01 964v 1 [cs.L G] 7N ov2 01"}, {"heading": "1 Introduction", "text": "This problem appears in many application areas of machine learning, such as recommendation, ranking and language modeling. The extreme setting brings with it a lot of challenges, such as, among others, the temporal and spatial complexity of training and prediction, the long tail of labels, missing labels and very few training examples per label. In this paper, we undertake the first problem - complexity, and propose the first to apply our best knowledge, really log time and log space training and prediction algorithms that can produce their top k predictions in time O (k log (k) log (C))) for a production space of size C. To this end, we adapt ideas from structured predictions to impose an efficient search structure on each multi-chain and multi-label problem, allowing us to (a) characterize when we expect our technique to work as well as a one-vs-all (OVA) classification and fibel (multiple) models that can be trained with any technique."}, {"heading": "2 Problem Setting", "text": "We use (x, y) to designate an instance of a multi-level or multi-level training set. Let x be a feature vector, x-RD and y be a label vector of dimension C, y = 1 if relevant for x. In the multi-level case y is an indicator vector, in the multi-level case y {0, 1} C."}, {"heading": "3 Proposed Approach", "text": "eDi eeisrcehnlrc\u00fceBnhei nvo of eeisrcnlhsrteeaeVnlhsrtee\u00fccnlhsrtee\u00fccnlhsrtee\u00fccnlhsrf\u00fc ide eeisrmtlrteeaeaeVnlrlrrrteeeeeeeeeetnlrrVnlrteeeeeeeeeegnln rf\u00fc ide eeisrrlrteeeeeeeeVnlrlrrrrteeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeee"}, {"heading": "4 LTLS model", "text": "A path s is a vector of length E, where se = 1, if the edge e is a part of that path, and se = 0 otherwise, is such that you can get from the starting point to the auxiliary point in the diagram G. Using all and only edges in s, we say that the edge e, if and only if se = 1. There are exactly C paths in diagram G. All paths in a diagram G, stacked horizontally, form a matrix MG of dimensionality C \u00b7 E. Each label in L = (1, 2,.,.,., C) is assigned exclusively to a path s. In view of a feature vector x of dimension D and the model weights w, each edge e receives a value he (w, x). Values for all edges form an E-dimensional vector h (w, x).The label score of the model is the value for the corresponding path (s) (f, x) of a fast path (Vw)."}, {"heading": "4.1 Underlying models", "text": "LTLS can use various learnable functions to estimate edge values h (w, x). The most basic model can use a linear model to predict each edge weight. Then, the weights become W-RE-D, and the lower-graded models get a form f = MGWx.While we can show that if a powerful linear OVA model V-RC-D can be approximated by MGW, our approach will work well (details omitted), this assumption is not always practicable. Fortunately, our approach can also be used as the output layer of a deep network, where the values of edges h (w, x) are given through a deep network, while still being able to draw conclusions and propagate through that layer into O (log (C) backwards."}, {"heading": "5 Optimization", "text": "In this case, it is what we use when the underlying model is a deep mesh. In this case, we limit ourselves to linear predictors and use a loss that leads to a dual split. In this case, we use the separation ranking for all parameters. In this case, we will use the positive labels P (y) for a deep mesh. In this case, we will limit ourselves to linear predictors and use a loss that leads to a dual split."}, {"heading": "5.1 Label-path assignment policy", "text": "Since the decompression matrix MG structure is fixed to allow quick conclusions, bidirectional matching between labels L and paths s S becomes an important topic. To keep the training online and fast, we could not propose a very complex method to find a good path for each class. A simple approach is that once an instance (x, y) is encountered with an invisible label, we find the uppermost m paths for x and assign them to the highest free path. If there is no free path, we assign a random path. We limit the size of the ranking so that m O (log (C) is to keep the training fast. While this increases our memory requirements to O (C) (to know which paths are free), this memory is not for model parameters and therefore remains constant as the input size (or model size in the case of deep networks) increases. Training time also extends to log (O) (C), but does not make a difference in our log (C)."}, {"heading": "6 Experiments", "text": "This section presents an experimental evaluation of LTLS 2. First, we report on the results of LTLS with a simple linear model on each edge and separation ranking loss. We have run LTLS on the data sets used in [4], where one can find a comparison of a set of multi-class and multi-label algorithms in terms of precision @ 1, training and prediction time, and model size. In the reported training times, we take into account that the LTLS implementation is currently implemented in Python, while other algorithms are implemented in compiled languages. In Tables 1 and 2, we compare LTLS with LOMtree, FastXML, and LEML, for which we report the results from [4], multi-class problems on all but one dataset LTLS gets results comparable to the LOMtree, while we build a smaller model and deliver predictions faster (although LLS is currently Python)."}, {"heading": "7 Conclusions", "text": "We introduced LTLS, the first extreme classification log-time and logspace technology. By embedding extreme problems in structured predictions, we are able to address both temporal and spatial complexities, while making clear connections to low-level models and offering opportunities to incorporate in-depth learning into extreme classification. Many of our design decisions are motivated purely out of convenience and leave many interesting questions about the impact of these decisions open as open questions for future work."}, {"heading": "Acknowledgments", "text": "Kalina Jasinska is also supported by the Polish National Science Center under grant number 2013 / 09 / D / ST6 / 03917. Some experiments were conducted at the Supercomputing and Networking Center Poznan under grant number 243."}], "references": [{"title": "Sparse local embeddings for extreme multilabel classification", "author": ["K. Bhatia", "H. Jain", "P. Kar", "M. Varma", "Jain P"], "venue": "In NIPS,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2015}, {"title": "Logarithmic time online multiclass prediction", "author": ["A. Choromanska", "J. Langford"], "venue": "In NIPS,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2015}, {"title": "A family of additive online algorithms for category ranking", "author": ["Koby Crammer", "Yoram Singer"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2003}, {"title": "Pd-sparse : A primal and dual sparse approach to extreme multiclass and multilabel classification", "author": ["I. En-Hsu Yen", "X. Huang", "P. Ravikumar", "K. Zhong", "I. Dhillon"], "venue": "Proceedings of The 33rd International Conference on Machine Learning,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2016}, {"title": "Extreme f-measure maximization using sparse probability estimates", "author": ["K. Jasinska", "K. Dembczynski", "R. Busa-Fekete", "K. Pfannschmidt", "T. Klerx", "E. Hullermeier"], "venue": "In Proceedings of The 33rd International Conference on Machine Learning,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2016}, {"title": "Fast label embeddings via randomized linear algebra", "author": ["P. Mineiro", "N. Karampatziakis"], "venue": "In ECML,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2015}, {"title": "FastXML: A fast, accurate and stable tree-classifier for extreme multilabel learning", "author": ["Y. Prabhu", "M. Varma"], "venue": "In KDD,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2014}, {"title": "Wsabie: Scaling up to large vocabulary image annotation", "author": ["J. Weston", "S. Bengio", "N. Usunier"], "venue": "In IJCAI,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2011}, {"title": "Large-scale multi-label learning with missing labels", "author": ["H. Yu", "P. Jain", "P. Kar", "I. Dhillon"], "venue": "In Proceedings of The 31st International Conference on Machine Learning,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2015}], "referenceMentions": [{"referenceID": 3, "context": "An example of such an approach is PD-Sparse [4], where the authors show that it is possible to get accurate sparse models in high dimensional datasets.", "startOffset": 44, "endOffset": 47}, {"referenceID": 0, "context": "Examples of such methods are SLEEC [1], LEML [9], WSABIE [8] or Rembrandt [6].", "startOffset": 35, "endOffset": 38}, {"referenceID": 8, "context": "Examples of such methods are SLEEC [1], LEML [9], WSABIE [8] or Rembrandt [6].", "startOffset": 45, "endOffset": 48}, {"referenceID": 7, "context": "Examples of such methods are SLEEC [1], LEML [9], WSABIE [8] or Rembrandt [6].", "startOffset": 57, "endOffset": 60}, {"referenceID": 5, "context": "Examples of such methods are SLEEC [1], LEML [9], WSABIE [8] or Rembrandt [6].", "startOffset": 74, "endOffset": 77}, {"referenceID": 6, "context": "For example, a multi-label decision tree based method, FastXML [7] builds a tree of depth logarithmic in the number of training examples.", "startOffset": 63, "endOffset": 66}, {"referenceID": 4, "context": "Label tree based method, PLT [5], has a O(log(C)) training time, since an update with one training instance is applied to O(log(C)) models.", "startOffset": 29, "endOffset": 32}, {"referenceID": 1, "context": "Multi-class logarithmic time prediction is addressed by LOMtree [2], but space used by this algorithm is O(C).", "startOffset": 64, "endOffset": 67}, {"referenceID": 2, "context": "We use the separation ranking loss [3, 4], being zero if all the positive labels P(y) for given instance (x, y) have higher scores than all the negative labels N (y) plus a margin, and being the difference between highest scoring negative label ln score F (\u00b7, s(ln), w) plus a margin and lowest scoring positive label lp score F (\u00b7, s(lp), w).", "startOffset": 35, "endOffset": 41}, {"referenceID": 3, "context": "We use the separation ranking loss [3, 4], being zero if all the positive labels P(y) for given instance (x, y) have higher scores than all the negative labels N (y) plus a margin, and being the difference between highest scoring negative label ln score F (\u00b7, s(ln), w) plus a margin and lowest scoring positive label lp score F (\u00b7, s(lp), w).", "startOffset": 35, "endOffset": 41}, {"referenceID": 3, "context": "We have run LTLS on the datasets used in [4], where one can find a comparison of a set of multi-class and multi-label algorithms in terms of precision@1, training and prediction time, and model size.", "startOffset": 41, "endOffset": 44}, {"referenceID": 3, "context": "In Tables 1 and 2 we compare LTLS with LOMtree, FastXML, and LEML, for which we report the results from [4].", "startOffset": 104, "endOffset": 107}], "year": 2016, "abstractText": "We present LTLS, a technique for multiclass and multilabel prediction that can perform training and inference in logarithmic time and space. LTLS embeds large classification problems into simple structured prediction problems and relies on efficient dynamic programming algorithms for inference. We train LTLS with stochastic gradient descent on a number of multiclass and multilabel datasets and show that despite its small memory footprint it is often competitive with existing approaches.", "creator": "LaTeX with hyperref package"}}}