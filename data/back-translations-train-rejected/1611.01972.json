{"id": "1611.01972", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Nov-2016", "title": "Fixed-point Factorized Networks", "abstract": "In recent years, Deep Neural Networks (DNNs) based methods have achieved remarkable performance in a wide range of tasks and have been among the most powerful and widely used techniques in computer vision, speech recognition and Natural Language Processing. However, DNN-based methods are both computational-intensive and resource-consuming, which hinders the application of these methods on embedded systems like smart phones. To alleviate this problem, we introduce a novel Fixed-point Factorized Networks (FFN) on pre-trained models to reduce the computational complexity as well as the storage requirement of networks. Extensive experiments on large-scale ImageNet classification task show the effectiveness of our proposed method.", "histories": [["v1", "Mon, 7 Nov 2016 10:26:41 GMT  (22kb)", "http://arxiv.org/abs/1611.01972v1", null], ["v2", "Tue, 29 Aug 2017 09:46:41 GMT  (122kb)", "http://arxiv.org/abs/1611.01972v2", null]], "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["peisong wang", "jian cheng"], "accepted": false, "id": "1611.01972"}, "pdf": {"name": "1611.01972.pdf", "metadata": {"source": "CRF", "title": "Fixed-point Factorized Networks", "authors": ["Peisong Wang", "Jian Cheng"], "emails": ["peisong.wang@nlpr.ia.ac.cn", "jcheng@nlpr.ia.ac.cn"], "sections": [{"heading": null, "text": "ar Xiv: 161 1.01 972v 1 [cs.C V"}, {"heading": "1. Introduction", "text": "This year, it has reached the stage where it will be able to take the lead."}, {"heading": "2. Related Work", "text": "We mainly have work that is closely related to our networks, i.e., the low-rank based methods and the fixed-point quantification are based on methods.Deep neural networks are usually over-parameterized and redundancy can be largely removed, as in the work of [3], which achieves an acceleration of the individual layers using biclustering and low-rank approximation of the filter matrix. Since then, many low-rank based methods have been proposed to use low-rank and reconstruction filters to reduce the approximation error and achieve 2.5-rank approximation of scene character recognition without loss of accuracy. Zhang et al proposes a novel non-linear data reconstruction method to prevent asymmetrical reconstruction of errors, which results in errors crossing multiple layers."}, {"heading": "3. Approaches", "text": "Our method uses the method of weight-matrix approximation for the acceleration and compression of deep neural networks. Unlike many other low-rank matrix decomposition methods that use floating-point values for factorized submatrices, our method focuses directly on fixed-point factorization. In order to make more efficient use of pre-trained weights, in addition to the fixed-point approximation, we also introduce our novel psudo method for full-precision weight matrix restoration. Therefore, the information from the pre-trained models is divided into two parts: the first part consists of fixed-point factorized submatrices and the second part is located in the psudo full-precision weight matrices, which are transformed into the fixed-point weight matrices of the finetuning stage. In addition, our Weight Balancing technique makes fine-tuning more efficient and plays an important role in our entire framework. We will carefully discuss these three parts of our proposed framework in the following paragraphs."}, {"heading": "3.1. Fixed-point Factorization of Weight Matrices", "text": "& & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & & &"}, {"heading": "3.2. Full-precision Weights Recovery", "text": "\"We're not going to make a decision on whether or not we're going to make a decision on whether or not we're going to make a decision on whether or not we're going to make a decision on whether or not we're going to make a decision on whether or not we're going to make a decision on whether or not we're going to make a decision on whether or not we're going to make a decision on whether or not we're going to make a decision on whether or not we're going to make a decision on whether or not to make a decision on whether or not we're going to make a decision on whether or not to make a decision,\" he said."}, {"heading": "3.3. Weight Balancing", "text": "So far, we have Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q-Q"}, {"heading": "4. Experiments", "text": "In this section, we fully evaluate our method based on the ILSVRC-12 image classification benchmark with 1.2 million training examples and 50k validation examples. We report on the results of the precision grain analysis with top-1 and top-5 accuracy. Experiments are performed with two most commonly used CNN models, AlexNet and VGG-16. All of these models are downloaded without modification from the Caffe Model Zoo in Berkeley and serve as a basis for comparison."}, {"heading": "4.1. AlexNet", "text": "Alexnet was the first CNN architecture to be successful in the ImageNet classification task. This network has 61M parameters and more than 59% of them are located in the fully connected layers. Therefore, we choose relatively smaller k's for fully connected layers. Specifically, for the revolutionary layers with 4-D size weights w \u00b7 h \u00b7 c \u00b7 n, we choose k = min (w \u0445 h \u0445 c, n). We also compare our method with the following approaches, the results of which are available to us on ImageNet datasets. The resulting architecture has 60M parameters. In the finetuning phase, the images are reduced to 256 x x by 256 pixel size like the original Alexnet.We compare our method with the following approaches, the results of which are available to us on ImageNet datasets. The BWN method reports only its results on AlexNet using batch normalization [7], i.e., to compare binix results, we report our results using weights BN \u2022 11 using the same weights as the Wch factors:"}, {"heading": "4.2. VGG-16", "text": "VGG-16 [16] uses a much wider and deeper structure than AlexNet, with 13 corrugated layers and 3 fully joined layers. VGG-16 won second place in ILSVRC 2014. We use the same rules to select the k's, and we also use k = 3138, 3072, 1000 for the last three fully joined layers, resulting in the same number of parameters as the original VGG-16 model. In fine tuning, we reduce the images to 256 pixels in their smaller format. Table 4.2 shows that our method of acceleration and compression even exceeds the original VGG-16 model by 0.1%."}, {"heading": "5. Conclusion", "text": "We present a novel factorized fixed-point method for the acceleration and compression of deep neural networks. To fully exploit the pre-trained models, we propose a novel method for full-precision weight recovery that makes fine adjustment more efficient and effective. In addition, we present a weight compensation technique that facilitates fine adjustment by making the training phase more stable. Extensive experiments on AlexNet and VGG-16 demonstrate the effectiveness of our method."}], "references": [{"title": "Training binary multilayer neural networks for image classification using expectation backpropagation", "author": ["Z. Cheng", "D. Soudry", "Z. Mao", "Z. Lan"], "venue": "arXiv preprint arXiv:1503.03562", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2015}, {"title": "Binaryconnect: Training deep neural networks with binary weights during propagations", "author": ["M. Courbariaux", "Y. Bengio", "J.-P. David"], "venue": "Advances in Neural Information Processing Systems, pages 3123\u20133131", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2015}, {"title": "N", "author": ["M. Denil", "B. Shakibi", "L. Dinh"], "venue": "de Freitas, et al. Predicting parameters in deep learning. In Advances in Neural Information Processing Systems, pages 2148\u20132156", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2013}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["X. Glorot", "Y. Bengio"], "venue": "Journal of Machine Learning Research, 9:249\u2013256", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2010}, {"title": "Learning both weights and connections for efficient neural network", "author": ["S. Han", "J. Pool", "J. Tran", "W. Dally"], "venue": "Advances in Neural Information Processing Systems, pages 1135\u20131143", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2015}, {"title": "Fixed-point feedforward deep neural network design using weights+ 1", "author": ["K. Hwang", "W. Sung"], "venue": "0, and- 1. In 2014 IEEE Workshop on Signal Processing Systems (SiPS), pages 1\u20136. IEEE", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2014}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["S. Ioffe", "C. Szegedy"], "venue": "arXiv preprint arXiv:1502.03167", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2015}, {"title": "Speeding up convolutional neural networks with low rank expansions", "author": ["M. Jaderberg", "A. Vedaldi", "A. Zisserman"], "venue": "arXiv preprint arXiv:1405.3866", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2014}, {"title": "Compression of deep convolutional neural networks for fast and low power mobile applications", "author": ["Y.-D. Kim", "E. Park", "S. Yoo", "T. Choi", "L. Yang", "D. Shin"], "venue": "arXiv preprint arXiv:1511.06530", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2015}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "Advances in neural information processing systems, pages 1097\u20131105", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2012}, {"title": "Speeding-up convolutional neural networks using fine-tuned cp-decomposition", "author": ["V. Lebedev", "Y. Ganin", "M. Rakhuba", "I. Oseledets", "V. Lempitsky"], "venue": "arXiv preprint arXiv:1412.6553", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2014}, {"title": "Fast training of convolutional networks through ffts", "author": ["M. Mathieu", "M. Henaff", "Y. LeCun"], "venue": "arXiv preprint arXiv:1312.5851", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2013}, {"title": "Convolutional neural networks using logarithmic data representation", "author": ["D. Miyashita", "E.H. Lee", "B. Murmann"], "venue": "arXiv preprint arXiv:1603.01025", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2016}, {"title": "Fitnets: Hints for thin deep nets", "author": ["A. Romero", "N. Ballas", "S.E. Kahou", "A. Chassang", "C. Gatta", "Y. Bengio"], "venue": "arXiv preprint arXiv:1412.6550", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2014}, {"title": "et al", "author": ["O. Russakovsky", "J. Deng", "H. Su", "J. Krause", "S. Satheesh", "S. Ma", "Z. Huang", "A. Karpathy", "A. Khosla", "M. Bernstein"], "venue": "Imagenet large scale visual recognition challenge. International Journal of Computer Vision, 115(3):211\u2013252", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2015}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "arXiv preprint arXiv:1409.1556", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2014}, {"title": "Accelerating convolutional neural networks for mobile applications", "author": ["P. Wang", "J. Cheng"], "venue": "Proceedings of the 2016 ACM on Multimedia Conference, pages 541\u2013545. ACM", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2016}, {"title": "Quantized convolutional neural networks for mobile devices", "author": ["J. Wu", "C. Leng", "Y. Wang", "Q. Hu", "J. Cheng"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR)", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2016}, {"title": "Accelerating very deep convolutional networks for classification and detection", "author": ["X. Zhang", "J. Zou", "K. He", "J. Sun"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 9, "context": "For example, the Alexnet [10] involves 61M float-point parameters and 729M high precision multiply-accumulate operations (MACs).", "startOffset": 25, "endOffset": 29}, {"referenceID": 2, "context": "Deep neural networks are usually over-parameterized and the redundancy can be removed as shown in the work of [3], which achieves 1.", "startOffset": 110, "endOffset": 113}, {"referenceID": 7, "context": "Jaderberg [8] propose to use filter low-rank approximation and data reconstruction to lower the approximation error and achieve 2.", "startOffset": 10, "endOffset": 13}, {"referenceID": 18, "context": "[19] propose a novel nonlinear data reconstruction method, which allows asymmetric reconstruction to prevent error accumulation across layers.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "3% increase of top-5 error in ImageNet [15] classification.", "startOffset": 39, "endOffset": 43}, {"referenceID": 10, "context": "[11] propose to use CP-decomposition on 4D convolutional kernels using multiple rank-one tensors, which achieves 8.", "startOffset": 0, "endOffset": 4}, {"referenceID": 8, "context": "[9] propose to use Tucker decomposition to speed up the execution of CNN models.", "startOffset": 0, "endOffset": 3}, {"referenceID": 16, "context": "[17] propose to accelerate the test-phase computation of CNNs based on low-rank and group sparse tensor decomposition.", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "develop the Expectation Backpropagation (EBP) [1] method, which is a variational Bayes method to binarize both weights and neurons and achieves good results for fully connected networks on MNIST dataset.", "startOffset": 46, "endOffset": 49}, {"referenceID": 1, "context": "In the work of BinaryConnect (BC) [2], the authors propose to use binary weights for forward and backward computation while keep a full-precision version of weights for gradients accumulation.", "startOffset": 34, "endOffset": 37}, {"referenceID": 10, "context": "Binary-Weight-Network (BWN) [11] is proposed in a more recent work, which is among the first ones to evaluate the performance of binarization on large-scale datasets like ImageNet [15] and yields good results.", "startOffset": 28, "endOffset": 32}, {"referenceID": 14, "context": "Binary-Weight-Network (BWN) [11] is proposed in a more recent work, which is among the first ones to evaluate the performance of binarization on large-scale datasets like ImageNet [15] and yields good results.", "startOffset": 180, "endOffset": 184}, {"referenceID": 5, "context": "[6] find a way by first quantize pretrained weights using a small number of weights, followed by retraining.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[5] use network pruning to remove low-saliency parametera and small-weight connections to dramatically reduce parameter size.", "startOffset": 0, "endOffset": 3}, {"referenceID": 17, "context": "[18] propose to use product quantization for CNN compression and acceleration at the same time.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "FFT-based methods [12] are also investigated.", "startOffset": 18, "endOffset": 22}, {"referenceID": 13, "context": "Many works [14] utilize a smaller network but achieve comparable performance, which is another widely studied stream for DNN acceleration.", "startOffset": 11, "endOffset": 15}, {"referenceID": 3, "context": "To balance the weights into their appropriate scales, and inspired by the normalized weight initialization methhod proposed in [4], we develop the following weight balancing approaches: First, we want to find the scale factor \u03bbX and \u03bbY for X\u0302 and \u0176 , which are proportional to the square root of the number of their rows and colums.", "startOffset": 127, "endOffset": 130}, {"referenceID": 6, "context": "The BWN method only report their results on AlexNet using batch normalization [7], so in order to compare with their results, we also report our results using batch normalization with the same settings as in BWN.", "startOffset": 78, "endOffset": 81}, {"referenceID": 10, "context": "\u2022 BWN: [11]: Binary-weight-network, using binary weights and float-point scaling factors;", "startOffset": 7, "endOffset": 11}, {"referenceID": 1, "context": "\u2022 BC: [2]: BinaryConnec, using binary weights, reproduced by [11];", "startOffset": 6, "endOffset": 9}, {"referenceID": 10, "context": "\u2022 BC: [2]: BinaryConnec, using binary weights, reproduced by [11];", "startOffset": 61, "endOffset": 65}, {"referenceID": 12, "context": "\u2022 LDR [13]: Logarithmic Data Representation, 4-bit logarithmic activation and 5-bit logarithmic weights.", "startOffset": 6, "endOffset": 10}, {"referenceID": 6, "context": "The suffix BN indicates that this method use batch normalization [7].", "startOffset": 65, "endOffset": 68}, {"referenceID": 9, "context": "AlexNet [10] 57.", "startOffset": 8, "endOffset": 12}, {"referenceID": 1, "context": "BC-BN [2] 35.", "startOffset": 6, "endOffset": 9}, {"referenceID": 10, "context": "BWN-BN [11] 56.", "startOffset": 7, "endOffset": 11}, {"referenceID": 12, "context": "4 LDR [13] 75.", "startOffset": 6, "endOffset": 10}, {"referenceID": 15, "context": "VGG-16 [16] use much wider and deeper structure than AlexNet, having 13 convolutional layers and 3 fullyconnected layers.", "startOffset": 7, "endOffset": 11}, {"referenceID": 15, "context": "VGG-16 [16] 71.", "startOffset": 7, "endOffset": 11}, {"referenceID": 12, "context": "LDR [13] 89.", "startOffset": 4, "endOffset": 8}], "year": 2016, "abstractText": "In recent years, Deep Neural Networks (DNN) based methods have achieved remarkable performance in a wide range of tasks and have been among the most powerful and widely used techniques in computer vision, speech recognition and Natural Language Processing. However, DNN-based methods are both computational-intensive and resource-consuming, which hinders the application of these methods on embedded systems like smart phones. To alleviate this problem, we introduce a novel Fixed-point Factorized Networks (FFN) on pre-trained models to reduce the computational complexity as well as the storage requirement of networks. Extensive experiments on large-scale ImageNet classification task show the effectiveness of our proposed method.", "creator": "LaTeX with hyperref package"}}}