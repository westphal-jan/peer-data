{"id": "1611.02019", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Nov-2016", "title": "Multi-view Generative Adversarial Networks", "abstract": "Learning over multi-view data is a challenging problem with strong practical applications. Most related studies focus on the classification point of view and assume that all the views are available at any time. We consider an extension of this framework in two directions. First, based on the BiGAN model, the Multi-view BiGAN (MV-BiGAN) is able to perform density estimation from multi-view inputs. Second, it can deal with missing views and is able to update its prediction when additional views are provided. We illustrate these properties on a set of experiments over different datasets.", "histories": [["v1", "Mon, 7 Nov 2016 12:29:19 GMT  (3694kb,D)", "http://arxiv.org/abs/1611.02019v1", "Submitted at ICLR 2017"]], "COMMENTS": "Submitted at ICLR 2017", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["micka\\\"el chen", "ludovic denoyer"], "accepted": false, "id": "1611.02019"}, "pdf": {"name": "1611.02019.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Micka\u00ebl Chen", "Ludovic Denoyer"], "emails": ["mickael.chen@lip6.fr", "ludovic.denoyer@lip6.fr"], "sections": [{"heading": null, "text": "Most related studies focus on the standpoint of classification and assume that all views are always available. We are considering expanding this framework in two directions. First, based on the BiGAN model, the Multi-view BiGAN (MV-BiGAN) is able to perform density estimates from multiple views. Second, it can deal with missing views and update its prediction when additional views are provided. We illustrate these properties using a series of experiments across different datasets."}, {"heading": "1 INTRODUCTION", "text": "Many specific applications involve multiple sources of information that generate different views of the same object (Cesa-Bianchi et al., 2010). Considering, for example, human activity, GPS readings from a mobile phone, Internet navigation traces, or even photos posted on social networks, different views of a particular user are possible. In multimedia applications, views can correspond to different modalities (Atrey et al., 2010), such as sounds, images, videos, sequences of previous frames, etc... The problem of multi-view machine learning has been extensively investigated in the last decade, mainly from the point of view of classification. In this case, one wants to predict an output based on multiple views acquired on an unknown object x. Different strategies have been explored, but a common general idea is based on the (early or late) merging of different views at a particular level of a deep architecture (Wang et al., 2015; Ngiam et al., 2011; Srivastastaakdinov & Salakdinov, 2012)."}, {"heading": "2 BACKGROUND AND GENERAL IDEA", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 NOTATIONS AND TASK", "text": "A classic machine learning problem is the estimation of P (y | x) on the basis of the education theorem. But we will instead consider a multiple view problem where different views of x are available, since x is unknown. we will give V the number of possible views and x \u0433k the k-th view of x. The description space for the view k is Rnk, where nk is the size of that space. Furthermore, we will consider that some of the V views may be missing. The subset of available views for input xi is represented by an index vector si \u0441S = {0, 1} V, so that sik = 1 is when the view k = 0 is available elsewhere. Note that all V views may not be available for each input x, and the prediction model must be able to predict an output, since each subset of views si = {0; 1} V. In this configuration, our goal is not defined by the estimate of views v, v, v (v = 1), but by the estimate v (v), v (v = 1)."}, {"heading": "2.2 BIDIRECTIONAL GENERATIVE ADVERSARIAL NETS (BIGAN)", "text": "We quickly remember the principle of BiGANs, as our model is an extension of this technique. Generative Adversarial Networks (GAN) were introduced by Goodfellow et al. (2014) and demonstrated their ability to model complex distributions, and were used to generate compelling natural images from a simple latent distribution (Radford et al., 2015; Denton et al., 2015). Research into latent space has revealed interesting, significant patterns in the resulting results. However, GANs lack the ability to retrieve a latent representation from an output, thus wasting an opportunity to use the learned manifolds. Bidirectional Generative Adversarial Networks (BiGANs) were proposed by Donahue et al al al al. (2016) and Dumoulin et al al al al al al al al. (2016), given a latent representation to fill a gap."}, {"heading": "2.3 GENERAL IDEA", "text": "We propose a model based on the paradigm Generative Adversarial Networks, which is adapted to the multiview prediction problem. Our model is based on two different principles: Conditional Views BiGAN (CV-BiGAN): Firstly, since one wants to model an output distribution based on observations, our first contribution is to propose an adaptation of BiGAN to the conditional probability model, which leads to a model that is able to learn P (y | x), where x can be either a single view or an aggregation of multiple views. If conditional GANs have already been proposed in the literature (see Section 6), they are not adapted to our problem, which requires explicit mappings between input space and latent space to output the output space. Multi-view BiGANs (MV-BiGAN): On top of the CV-BiGAN model, we form a multiview based on the possible location of a subset of a model that is estimated."}, {"heading": "3 THE CONDITIONAL BIGAN MODEL (CV-BIGAN)", "text": "Our first goal is to expand the BiGAN formalism to handle an input space (e.g. a single observed view) in addition to the output space Rn. We will predict x-x the observation and y the output. In other words, we want to capture the conditional probability P (y | x) from a given learning dataset. The CV-BiGAN model provides a bidirectional mapping between the input space and an associated representation space, i.e., the goal is to map a value x to the latent space RZ. The CV-BiGAN model holds a bidirectional mapping between the encoder E and the generator G that was previously defined, but also an additional encoder function called H."}, {"heading": "4 MULTI-VIEW BIGAN", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 AGGREGATING MULTI-VIEWS FOR CV-BIGAN", "text": "In this case, we can use the CV-BiGAN model (or other conditional approaches) together with a model that is able to aggregate the different views, where A is the size of the aggregation space. Instead of looking at the input x, we define an aggregation model whose representation will be the representation of the aggregation of all available views x-k1: BA (v (s, x) = V-k = 1-k-k (x-k) (6), where \u03c6k is a function that will be learned that maps a particular view in Rnk to the aggregation space RA. By replacing x in Equation 5, we can then simultaneously learn the functions pack and the distributions PH, PE, and PD, resulting in a multiview model that is able to handle any subset of views."}, {"heading": "4.2 UNCERTAINTY REDUCTION ASSUMPTION", "text": "However, the previous idea suffers from a very high level of instability in learning, as is commonly found in complex GAN architectures (see Section 5). To stabilize our model, we propose to add a regulation based on the idea that adding new views to an existing subset of views should reduce uncertainty about the results (see Section 5). Indeed, assuming that views are consistent, 1 hint that other aggregation schemes such as recursive neural networks can be used for examples. Adding a new perspective should make it possible to disprove the predictions and reduce the variance of the distribution of outcomes. Let's look at an object x and two index vectors s and s so that v (x, s) and s can be used in a way that v (x, s)."}, {"heading": "4.3 LEARNING THE MV-BIGAN", "text": "The various functions E, G, H, D1 and D2 are implemented as parametric neural networks and trained by stochastic gradient departure in the minibatch (see appendix for more details on the architectures). We first update the discriminator networks D1 and D2, then we update the generator and the encoders G, E and H with gradient steps in the opposite direction. As with most other implementations of GAN-based models, we find that using an alternative target proposed by Goodfellow et al. (2014) for E, G and H instead leads to more stable training. The new goal is to replace the labels for the discriminators rather than reversing the gradient. We also find that we can update all modules in one pass, instead of performing alternate gradient steps for E, G and H. Note that the MV-GAN model is designed to form all of the possible subsets of data available in order to generate all of the examples of each of which are trained."}, {"heading": "5 EXPERIMENTS", "text": "The first dataset we are experimenting with is the MNIST dataset with handwritten digits; the second dataset is the CelebA dataset (Liu et al., 2015), which is composed of images of faces as well as corresponding attributes; and the MNIST dataset is used to illustrate the ability of the MV-BiGAN to deal with different (heterogeneous) views and to update its prediction when integrating new views; and the CelebA dataset is used to demonstrate the ability of MV-BiGAN to deal with different types of views."}, {"heading": "5.1 MNIST, 4 VIEWS", "text": "We consider the problem that four different views can be available, with each view corresponding to a certain quarter of the final image to be predicted - each view is a vector of R (14 x 14). The MV-BiGAN is used here to restore the original image; the model is trained on the MNIST training digits, and the results are provided on the MNIST test dataset. Figure 3 illustrates the results obtained for some digits. In this figure, the first column shows the input (the subset of views), while the other columns show predicted results sampled by the MV-BiGAN. An additional view is added between each line. This experiment shows that when new views are added, the variety of the predicted views decreases due to the KL contract introduced in the model, which is the desired behavior, i.e. more information implies less deviation."}, {"heading": "5.2 MNIST, SEQUENCE OF INCOMING VIEWS", "text": "We conducted another series of experiments where the views correspond to images with missing values (missing values are replaced by 0.5), which can be considered a data imputation problem - Figure 5. Again, the behavior of the MV-BiGAN has interesting properties: The model is able to predict the desired performance as long as enough information is provided. If only non-informative views are provided, the model produces high-diversity digits, with diversity diminishing as new information is added."}, {"heading": "5.3 CELEBA, INTEGRATING HETEROGENEOUS INFORMATION", "text": "Finally, the third experiment aims to measure the ability of MV-BiGAN to process heterogeneous input. We look at two views: (i) the attribute vector, which contains information about the person in the image (hair color, gender,...), and (ii) an incomplete face. Figure 9 illustrates the results obtained on two faces: the first line corresponds to the faces generated on the basis of the attribute vector. It can be seen that the attribute information was captured by the model: for example, the gender of the generated face is constant (only women), which shows that MV-BiGan captured this information from the attribute vector. The second line corresponds to the faces generated as input when using the incomplete face. It can also be seen that the results generated are \"compatible\" with the incomplete information provided to the model. But the attribute is not taken into account (for example, men and women)."}, {"heading": "6 RELATED WORK", "text": "For example, the use of a number of views is common (Atrey et al., 2010): text, audio, images (different framings of videos) are the starting points for these views. Furthermore, multimedia learning exercises from multiple views have led to a large amount of different approaches and experimental outcomes. The success of multimedia learning approaches in the multimedia community seems to be based on the ability of systems to deal with the complementarity of the information carried by each modality. Comparable studies are important in many domains, such as bioinformatics. Sokolov & Ben-Hur, 2011), speech recognition (Ko\u00e7o et al al al al al., 2012), signal-based multimodal integration (Wu et al., 1999), gesture recognition."}, {"heading": "7 CONCLUSION AND PERSPECTIVES", "text": "We have proposed the CV-BiGAN model for estimating conditional densities, and its extension MV-BiGAN for processing multiview inputs. The MV-BiGAN model is capable of processing both subsets of views, as well as updating its prediction when new views are added. It is based on the idea that the uncertainty of the prediction must diminish as additional information is provided, with this idea handled by a KL constraint in latent space. This work opens up several lines of research: the first concerns the architecture of the model itself, as the convergence of MV-BiGAN is still difficult to maintain and incurs particularly high training costs. Another direction would be to see if this model family could be used for any prediction on data streams."}, {"heading": "ACKNOWLEDGMENTS", "text": "This work was supported by the French project LIVES ANR-15-CE23-0026-03."}], "references": [{"title": "Kernel cca for multi-view acoustic feature learning using articulatory measurements", "author": ["R. Arora", "K. Livescu"], "venue": "In MLSP,", "citeRegEx": "Arora and Livescu.,? \\Q2012\\E", "shortCiteRegEx": "Arora and Livescu.", "year": 2012}, {"title": "Multimodal fusion for multimedia analysis: A survey", "author": ["Pradeep K. Atrey", "M. Anwar Hossain", "Abdulmotaleb El Saddik", "Mohan S. Kankanhalli"], "venue": "Multimedia Syst.,", "citeRegEx": "Atrey et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Atrey et al\\.", "year": 2010}, {"title": "Generalized denoising auto-encoders as generative models", "author": ["Yoshua Bengio", "Li Yao", "Guillaume Alain", "Pascal Vincent"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Bengio et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2013}, {"title": "Guest editorial: Learning from multiple sources", "author": ["Nicol\u00f2 Cesa-Bianchi", "David R. Hardoon", "Gayle Leen"], "venue": "Machine Learning,", "citeRegEx": "Cesa.Bianchi et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Cesa.Bianchi et al\\.", "year": 2010}, {"title": "Semi-supervised Learning", "author": ["O. Chapelle", "B. Sch\u00f6lkopf", "A. Zien"], "venue": null, "citeRegEx": "Chapelle et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Chapelle et al\\.", "year": 2006}, {"title": "Deep generative image models using a laplacian pyramid of adversarial networks", "author": ["Emily L Denton", "Soumith Chintala", "Rob Fergus"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Denton et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Denton et al\\.", "year": 2015}, {"title": "Adversarial feature learning", "author": ["Jeff Donahue", "Philipp Kr\u00e4henb\u00fchl", "Trevor Darrell"], "venue": "arXiv preprint arXiv:1605.09782,", "citeRegEx": "Donahue et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Donahue et al\\.", "year": 2016}, {"title": "Adversarially learned inference", "author": ["Vincent Dumoulin", "Ishmael Belghazi", "Ben Poole", "Alex Lamb", "Martin Arjovsky", "Olivier Mastropietro", "Aaron Courville"], "venue": "arXiv preprint arXiv:1606.00704,", "citeRegEx": "Dumoulin et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Dumoulin et al\\.", "year": 2016}, {"title": "Conditional generative adversarial nets for convolutional face generation. Class Project for Stanford CS231N: Convolutional Neural Networks for Visual Recognition, Winter semester", "author": ["Jon Gauthier"], "venue": null, "citeRegEx": "Gauthier.,? \\Q2014\\E", "shortCiteRegEx": "Gauthier.", "year": 2014}, {"title": "Generative adversarial nets", "author": ["Ian Goodfellow", "Jean Pouget-Abadie", "Mehdi Mirza", "Bing Xu", "David Warde-Farley", "Sherjil Ozair", "Aaron Courville", "Yoshua Bengio"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Goodfellow et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2014}, {"title": "Semi-supervised learning with multi-view embedding: theory and application with convolutional neural networks", "author": ["R. Johnson", "T. Zhang"], "venue": "CoRR, abs/1504.012555v1,", "citeRegEx": "Johnson and Zhang.,? \\Q2015\\E", "shortCiteRegEx": "Johnson and Zhang.", "year": 2015}, {"title": "Auto-encoding variational bayes", "author": ["Diederik P Kingma", "Max Welling"], "venue": "In Proceedings of the 2nd International Conference on Learning Representations (ICLR),", "citeRegEx": "Kingma and Welling.,? \\Q2014\\E", "shortCiteRegEx": "Kingma and Welling.", "year": 2014}, {"title": "Applying multiview learning algorithms to human-human conversation classification", "author": ["Sokol Ko\u00e7o", "C\u00e9cile Capponi", "Fr\u00e9d\u00e9ric B\u00e9chet"], "venue": "In INTERSPEECH,", "citeRegEx": "Ko\u00e7o et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Ko\u00e7o et al\\.", "year": 2012}, {"title": "Deep learning face attributes in the wild", "author": ["Ziwei Liu", "Ping Luo", "Xiaogang Wang", "Xiaoou Tang"], "venue": "In Proceedings of International Conference on Computer Vision (ICCV),", "citeRegEx": "Liu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2015}, {"title": "Deep multi-scale video prediction beyond mean square error", "author": ["Michael Mathieu", "Camille Couprie", "Yann LeCun"], "venue": "arXiv preprint arXiv:1511.05440,", "citeRegEx": "Mathieu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mathieu et al\\.", "year": 2015}, {"title": "Conditional generative adversarial nets", "author": ["Mehdi Mirza", "Simon Osindero"], "venue": "arXiv preprint arXiv:1411.1784,", "citeRegEx": "Mirza and Osindero.,? \\Q2014\\E", "shortCiteRegEx": "Mirza and Osindero.", "year": 2014}, {"title": "Multimodal deep learning", "author": ["Jiquan Ngiam", "Aditya Khosla", "Mingyu Kim", "Juhan Nam", "Honglak Lee", "Andrew Y Ng"], "venue": "In Proceedings of the 28th International Conference on Machine Learning (ICML),", "citeRegEx": "Ngiam et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Ngiam et al\\.", "year": 2011}, {"title": "Context encoders: Feature learning by inpainting", "author": ["Deepak Pathak", "Philipp Krahenbuhl", "Jeff Donahue", "Trevor Darrell", "Alexei A Efros"], "venue": "arXiv preprint arXiv:1604.07379,", "citeRegEx": "Pathak et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Pathak et al\\.", "year": 2016}, {"title": "Unsupervised representation learning with deep convolutional generative adversarial networks", "author": ["Alec Radford", "Luke Metz", "Soumith Chintala"], "venue": "arXiv preprint arXiv:1511.06434,", "citeRegEx": "Radford et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Radford et al\\.", "year": 2015}, {"title": "Ensemble-based classifiers", "author": ["L. Rokach"], "venue": "Artif. Intell. Rev.,", "citeRegEx": "Rokach.,? \\Q2010\\E", "shortCiteRegEx": "Rokach.", "year": 2010}, {"title": "Deep boltzmann machines", "author": ["Ruslan Salakhutdinov", "Geoffrey E Hinton"], "venue": "In AISTATS,", "citeRegEx": "Salakhutdinov and Hinton.,? \\Q2009\\E", "shortCiteRegEx": "Salakhutdinov and Hinton.", "year": 2009}, {"title": "Improved multimodal deep learning with variation of information", "author": ["Kihyuk Sohn", "Wenling Shang", "Honglak Lee"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Sohn et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sohn et al\\.", "year": 2014}, {"title": "Multi-view prediction of protein function", "author": ["Artem Sokolov", "Asa Ben-Hur"], "venue": "In ACM-BCB,", "citeRegEx": "Sokolov and Ben.Hur.,? \\Q2011\\E", "shortCiteRegEx": "Sokolov and Ben.Hur.", "year": 2011}, {"title": "Multimodal learning with deep boltzmann machines", "author": ["Nitish Srivastava", "Ruslan R Salakhutdinov"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Srivastava and Salakhutdinov.,? \\Q2012\\E", "shortCiteRegEx": "Srivastava and Salakhutdinov.", "year": 2012}, {"title": "A survey of multi-view machine learning", "author": ["Shiliang Sun"], "venue": "Neural Comput. Appl.,", "citeRegEx": "Sun.,? \\Q2013\\E", "shortCiteRegEx": "Sun.", "year": 2013}, {"title": "PAC-Bayes analysis of multi-view learning", "author": ["Shiliang Sun", "John-Shawe Taylor"], "venue": "CoRR, abs/1406.5614,", "citeRegEx": "Sun and Taylor.,? \\Q2014\\E", "shortCiteRegEx": "Sun and Taylor.", "year": 2014}, {"title": "Deep generative stochastic networks trainable by backprop", "author": ["Eric Thibodeau-Laufer", "Guillaume Alain", "Jason Yosinski"], "venue": null, "citeRegEx": "Thibodeau.Laufer et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Thibodeau.Laufer et al\\.", "year": 2014}, {"title": "Extracting and composing robust features with denoising autoencoders", "author": ["Pascal Vincent", "Hugo Larochelle", "Yoshua Bengio", "Pierre-Antoine Manzagol"], "venue": "In Proceedings of the 25th international conference on Machine learning,", "citeRegEx": "Vincent et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Vincent et al\\.", "year": 2008}, {"title": "On deep multi-view representation learning", "author": ["Weiran Wang", "Raman Arora", "Karen Livescu", "Jeff Bilmes"], "venue": "In Proc. of the 32st Int. Conf. Machine Learning (ICML", "citeRegEx": "Wang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2015}, {"title": "Fusing multi-modal features for gesture recognition", "author": ["Jiaxiang Wu", "Jian Cheng", "Chaoyang Zhao", "Hanqing Lu"], "venue": "In ICMI, pp", "citeRegEx": "Wu et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Wu et al\\.", "year": 2013}, {"title": "Multimodal integration: a statistical view", "author": ["L. Wu", "S.L. Oviatt", "P.R. Cohen"], "venue": "MM, 1(4):334\u2013341,", "citeRegEx": "Wu et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Wu et al\\.", "year": 1999}, {"title": "A novel ensemble construction method for multi-view data using random cross-view correlation between within-class examples", "author": ["Jianchun Zhang", "Daoqiang Zhang"], "venue": "Pattern Recogn.,", "citeRegEx": "Zhang and Zhang.,? \\Q2011\\E", "shortCiteRegEx": "Zhang and Zhang.", "year": 2011}, {"title": "Multi-view perceptron: a deep model for learning face identity and view representations", "author": ["Zhenyao Zhu", "Ping Luo", "Xiaogang Wang", "Xiaoou Tang"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Zhu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zhu et al\\.", "year": 2014}, {"title": "\u03bb is set to 1 \u00b7 10\u22123, and minibatch size is 16", "author": ["Aggregation space is of size"], "venue": "The model has been", "citeRegEx": "size,? 1000", "shortCiteRegEx": "size", "year": 1000}], "referenceMentions": [{"referenceID": 3, "context": "Many concrete applications involve multiple sources of information generating different views on the same object (Cesa-Bianchi et al., 2010).", "startOffset": 113, "endOffset": 140}, {"referenceID": 1, "context": "In multimedia applications, views can correspond to different modalities (Atrey et al., 2010) such as sounds, images, videos, sequences of previous frames, etc.", "startOffset": 73, "endOffset": 93}, {"referenceID": 28, "context": "Different strategies have been explored but a general common idea is based on the (early or late) fusion of the different views at a particular level of a deep architecture (Wang et al., 2015; Ngiam et al., 2011; Srivastava & Salakhutdinov, 2012).", "startOffset": 173, "endOffset": 246}, {"referenceID": 16, "context": "Different strategies have been explored but a general common idea is based on the (early or late) fusion of the different views at a particular level of a deep architecture (Wang et al., 2015; Ngiam et al., 2011; Srivastava & Salakhutdinov, 2012).", "startOffset": 173, "endOffset": 246}, {"referenceID": 1, "context": "In multimedia applications, views can correspond to different modalities (Atrey et al., 2010) such as sounds, images, videos, sequences of previous frames, etc... The problem of multi-view machine learning has been extensively studied during the last decade, mainly from the classification point of view. In that case, one wants to predict an output y based on multiple views acquired on an unknown object x. Different strategies have been explored but a general common idea is based on the (early or late) fusion of the different views at a particular level of a deep architecture (Wang et al., 2015; Ngiam et al., 2011; Srivastava & Salakhutdinov, 2012). The existing literature mainly explores problems where outputs are chosen in a discrete set (e.g categorization), and where all the views are available. An extension of this problem is to consider the density estimation problem where one wants to estimate the conditional probabilities of the outputs given the available views. As noted by Mathieu et al. (2015), minimizing classical prediction losses (e.", "startOffset": 74, "endOffset": 1019}, {"referenceID": 18, "context": "They have been used to produce compelling natural images from a simple latent distribution (Radford et al., 2015; Denton et al., 2015).", "startOffset": 91, "endOffset": 134}, {"referenceID": 5, "context": "They have been used to produce compelling natural images from a simple latent distribution (Radford et al., 2015; Denton et al., 2015).", "startOffset": 91, "endOffset": 134}, {"referenceID": 6, "context": "Generative Adversarial Networks (GAN) have been introduced by Goodfellow et al. (2014) and have demonstrated their ability to model complex distributions.", "startOffset": 62, "endOffset": 87}, {"referenceID": 5, "context": ", 2015; Denton et al., 2015). Exploring the latent space has uncovered interesting, meaningful patterns in the resulting outputs. However, GANs lack the ability to retrieve a latent representation given an output, missing out an opportunity to exploit the learned manifold. Bidirectional Generative Adversarial Networks (BiGANs) have been proposed by Donahue et al. (2016) and Dumoulin et al.", "startOffset": 8, "endOffset": 373}, {"referenceID": 5, "context": ", 2015; Denton et al., 2015). Exploring the latent space has uncovered interesting, meaningful patterns in the resulting outputs. However, GANs lack the ability to retrieve a latent representation given an output, missing out an opportunity to exploit the learned manifold. Bidirectional Generative Adversarial Networks (BiGANs) have been proposed by Donahue et al. (2016) and Dumoulin et al. (2016), independently, to fill that gap.", "startOffset": 8, "endOffset": 400}, {"referenceID": 8, "context": "It can be shown, by following the same steps as in Goodfellow et al. (2014), that the optimization problem described in Equation 2 minimizes the Jensen-Shanon divergence between PE(y, z) and PG(y, z), allowing the model to learn both a decoder and a generator over a training set that will model the joint distribution of (y, z) pairs.", "startOffset": 51, "endOffset": 76}, {"referenceID": 7, "context": "As proposed by Dumoulin et al. (2016), we consider in the following that PG(y|z) is modeled by a deterministic non-linear model G so that G(z) = y, and PE as a diagonal Gaussian distribution E(z) = (\u03bc(y), \u03c3(y)).", "startOffset": 15, "endOffset": 38}, {"referenceID": 9, "context": "As with most other implementation of GAN-based models, we find that using an alternative objective proposed by Goodfellow et al. (2014) for E, G and H instead leads to more stable training.", "startOffset": 111, "endOffset": 136}, {"referenceID": 13, "context": "The second dataset is the CelebA (Liu et al., 2015) dataset composed of both images of faces and corresponding attributes.", "startOffset": 33, "endOffset": 51}, {"referenceID": 1, "context": "For example, in the multimedia domain, dealing with a bunch of views is usual (Atrey et al., 2010): text, audio, images (different framings from videos) are starting points of these views.", "startOffset": 78, "endOffset": 98}, {"referenceID": 12, "context": "Comparable studies are of importance in many domains, such as bioinformatics (Sokolov & Ben-Hur, 2011), speech recognition (Arora & Livescu, 2012; Ko\u00e7o et al., 2012), signal-based multimodal integration (Wu et al.", "startOffset": 123, "endOffset": 165}, {"referenceID": 30, "context": ", 2012), signal-based multimodal integration (Wu et al., 1999), gesture recognition (Wu et al.", "startOffset": 45, "endOffset": 62}, {"referenceID": 29, "context": ", 1999), gesture recognition (Wu et al., 2013), etc.", "startOffset": 29, "endOffset": 46}, {"referenceID": 4, "context": "Moreover, multi-view learning has been theoretically studied mainly under the semi-supervised setting, but only with two facing views (Chapelle et al., 2006; Sun, 2013; Sun & Taylor, 2014; Johnson & Zhang, 2015).", "startOffset": 134, "endOffset": 211}, {"referenceID": 24, "context": "Moreover, multi-view learning has been theoretically studied mainly under the semi-supervised setting, but only with two facing views (Chapelle et al., 2006; Sun, 2013; Sun & Taylor, 2014; Johnson & Zhang, 2015).", "startOffset": 134, "endOffset": 211}, {"referenceID": 19, "context": "In parallel, ensemble-based learning approaches have been theoretically studied, in the supervised setting: many interesting results should concern multi-view learning, as long as the ensemble is built upon many views (Rokach, 2010; Zhang & Zhang, 2011).", "startOffset": 218, "endOffset": 253}, {"referenceID": 16, "context": "From the representation learning point of view, recent models are based on the incorporation of some \u201dfusion\u201d layers in the deep neural network architecture as in (Ngiam et al., 2011) or (Srivastava & Salakhutdinov, 2012) for example.", "startOffset": 163, "endOffset": 183}, {"referenceID": 32, "context": "Some other interesting models include the multiview perceptron(Zhu et al., 2014).", "startOffset": 62, "endOffset": 80}, {"referenceID": 21, "context": "In a multi-view setting, they are able to deal with missing views and have been used to capture the joint distribution in bi-modal text and image data (Srivastava & Salakhutdinov, 2012; Sohn et al., 2014).", "startOffset": 151, "endOffset": 204}, {"referenceID": 27, "context": "Another trend started with denoising autoencoder (Vincent et al., 2008), which aims to reconstruct a data from a noisy input have been proved to possess some desirable properties for data generation (Bengio et al.", "startOffset": 49, "endOffset": 71}, {"referenceID": 2, "context": ", 2008), which aims to reconstruct a data from a noisy input have been proved to possess some desirable properties for data generation (Bengio et al., 2013).", "startOffset": 135, "endOffset": 156}, {"referenceID": 26, "context": "The model have been generalized under the name Generative Stochastic Networks by replacing the noise function C with a mapping to a latent space (Thibodeau-Laufer et al., 2014).", "startOffset": 145, "endOffset": 176}, {"referenceID": 6, "context": "The BiGAN model (Donahue et al., 2016; Dumoulin et al., 2016) that serves as a basis for our work is an extension of the Generative Adversarial Nets (Goodfellow et al.", "startOffset": 16, "endOffset": 61}, {"referenceID": 7, "context": "The BiGAN model (Donahue et al., 2016; Dumoulin et al., 2016) that serves as a basis for our work is an extension of the Generative Adversarial Nets (Goodfellow et al.", "startOffset": 16, "endOffset": 61}, {"referenceID": 9, "context": ", 2016) that serves as a basis for our work is an extension of the Generative Adversarial Nets (Goodfellow et al., 2014).", "startOffset": 95, "endOffset": 120}, {"referenceID": 14, "context": "However, as noted by (Mathieu et al., 2015) and (Pathak et al.", "startOffset": 21, "endOffset": 43}, {"referenceID": 17, "context": ", 2015) and (Pathak et al., 2016), they display very unstable behavior.", "startOffset": 12, "endOffset": 33}, {"referenceID": 8, "context": "More specifically, CGAN have been able to generate image of faces conditioned on an attribute vector (Gauthier, 2014), but fail to model image distribution conditioned on a part of the image or on previous frames.", "startOffset": 101, "endOffset": 117}], "year": 2016, "abstractText": "Learning over multi-view data is a challenging problem with strong practical applications. Most related studies focus on the classification point of view and assume that all the views are available at any time. We consider an extension of this framework in two directions. First, based on the BiGAN model, the Multi-view BiGAN (MV-BiGAN) is able to perform density estimation from multi-view inputs. Second, it can deal with missing views and is able to update its prediction when additional views are provided. We illustrate these properties on a set of experiments over different datasets.", "creator": "LaTeX with hyperref package"}}}