{"id": "1611.02025", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Nov-2016", "title": "Presenting a New Dataset for the Timeline Generation Problem", "abstract": "The timeline generation task summarises an entity's biography by selecting stories representing key events from a large pool of relevant documents. This paper addresses the lack of a standard dataset and evaluative methodology for the problem. We present and make publicly available a new dataset of 18,793 news articles covering 39 entities. For each entity, we provide a gold standard timeline and a set of entity-related articles. We propose ROUGE as an evaluation metric and validate our dataset by showing that top Google results outperform straw-man baselines.", "histories": [["v1", "Mon, 7 Nov 2016 12:47:25 GMT  (744kb,D)", "http://arxiv.org/abs/1611.02025v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["xavier holt", "will radford", "ben hachey"], "accepted": false, "id": "1611.02025"}, "pdf": {"name": "1611.02025.pdf", "metadata": {"source": "CRF", "title": "Presenting a New Dataset for the Timeline Generation Problem", "authors": ["Xavier Holt", "Will Radford", "Ben Hachey"], "emails": [], "sections": [{"heading": null, "text": "We present and publish a new dataset of 18,793 news articles on 39 companies. For each company, we provide a gold standard timeline and a series of business-related articles. We propose ROUGE as a benchmark and validate our dataset by showing that Google top scores outperform Strawman baselines."}, {"heading": "1 Introduction", "text": "Information is more readily available in greater quantities than ever before. Timeline generation is a newer method of summarizing data - taking as input a large pool of business-related documents and selecting a small sentence that best describes the most important events in the life of the company. There are several challenges in evaluating: (1) finding gold standard timelines, (2) finding companies from which to draw documents to build timelines, and (3) evaluating system timelines produced by news agencies (Chieu and Lee, 2004; Yan et al., 2011a), but these are limited by the narrow editorial focus on prominent companies and depend on well-funded news agencies. Another approach is to comment on new timelines from the web for domains of choice. Wang (2013) does so, but does not make their data available for direct comparison."}, {"heading": "2 Data Collection", "text": "We have taken care to design a generic experimental protocol that can be used to generate entities from a range of domains. 1https: / / news.google.com 2https: / / github.com / xavi-ai / tlg-datasetar Xiv: 161 1.02 025v 1 [cs.C L] 7N ov2 016We start by selecting one domain (politics) and two regions (the US / Australia). We motivate this choice of domain by highlighting its huge media interest, polarizing entities and diverse topics. We select several entities from each region a priori - 39 in total. The rest of our entity is then generated through a process."}, {"heading": "3 Data Annotation and Gold-Standards", "text": "We present a general framework for formulating the gold standard timeline generation as an annotation task. This includes two components - using Wikipedia to generate a minimum number of sufficient links, and formulating the problem as an annotation task."}, {"heading": "3.1 Article Selection", "text": "Therefore, attempting to comment on the entire corpus of over 15,000 articles is unfeasible. We propose a method to reduce the size of our task while maintaining the quality of the underlying timeline. In our article selection process, we must meet the following criteria: \u2022 Clarity: Our article set should have good coverage. Timelines should cover a wide range of periods and events. As such, the record from which we derive our reference timelines must also have this property. \u2022 Clarity: Each article pair will be subject to a series of crowd assessments. Therefore, it is important to align coverage with the overall data size. \u2022 Informativeness: Ideally, we want the articles to be of high quality. To meet these criteria, we scrape off the external (non-wiki) links from an entity's Wikipedia page. We motivate this decision by first highlighting the Wikipedia verifiability guidelines."}, {"heading": "3.2 Crowd-task Formulation", "text": "We formulate the timeline creation as an annotation task by reducing it to a simple classification problem. A single judgement is made at the level of an entity-article pair. An annotator receives the first paragraph and a link to the entity's Wikipedia page. Then, they follow a predefined link and perform a two-step classification task. First, the annotators determine whether a link is valid. A valid article is one that includes a single 3https: / / en.wikipedia.org / wiki / Wikipedia: verifiability in the life of the target subject. Then, they indicate the importance of an article when it comes to the history of the entity. There were a selection of three terms: \u2022 Very important: key events that would be included in a one-page summary or short summary of the entity. \u2022 Something important: newsworthy events that could make it a broader biography, but not of critical relevance, unimportant events or mundane events."}, {"heading": "4 Analysis of Gold-Standards", "text": "We see that particularly prominent companies are responsible for a large proportion of the articles. \"Barack Obama\" and \"Donald Trump\" each have over four hundred articles. In fact, the six most prominent companies account for more than half of all articles (Figure 1). Very important articles The \"very important\" articles make up our gold standard timelines. The mean and average number of articles per company is 5.56 and 2, respectively. It goes without saying that certain companies will be involved in more newsworthy events than others. However, \"Barack Obama\" and \"Donald Trump\" each have roughly the same number of articles. The former have 14.6% articles that are considered \"very important\" - the latter only 1.5% (Figure 2). It is self-evident that certain companies will be involved in more newsworthy events than others. However, having such a discrepancy 5 - considering that all articles were deemed necessary to refer to a company's wiki - is the number of articles \"curious\" worth looking at, \"do we believe that crowd4or's very proportionate?"}, {"heading": "5 Evaluation", "text": "For our evaluation pipeline, we are following the approach of a number of papers in this area (Wang, 2013; Yan et al., 2011a; Yan et al., 2011b) in using the ROUGE metric (Lin, 2004). ROUGE was first used in automatic summary evaluation and is similar to BLEU machine translation measurement (Papineni et al., 2002). In terms of timeline evaluation, quality is measured by the number of overlapping units (e.g. word-n-grams) between articles in a system timeline and articles in a reference timeline. For details on calculating ROUGE values, refer to the original document (Lin, 2004). For our purposes, articles that are commented as \"valid\" and \"very important\" are considered as components of a company's reference timeline. We use ROUGE-F measurement via unigrams and bi-grams (n = 2)."}, {"heading": "6 Benchmarks and System Validation", "text": "In this section, we use our supplementary data set of articles generated by Google News to validate and name the task. ROUGE vs. Search Rank For a given news query, the rank of an article is a signal of its importance and centrality. Therefore, the better the search rank of an article, the more likely it appears in a company timeline. This seems to be the case. For both the ROUGE 1 and 2 metrics, there is a clear negative correlation between the average score and index of an article (Figure 5). Benchmarks For a given timeline, we include the following three benchmarks - Random (R): 15 articles are sampled from the entire corpus. Random + Linked (RL): 15 articles associated with the company are sampled. Ordered + Linked (OL): The 15 highest-rated articles for a company are selected."}, {"heading": "7 Conclusion and Future Work", "text": "In this paper, we have developed, analyzed and justified a new dataset for the problem of timeline generation. There are several interesting avenues for future work. The most obvious is the development of new timeline generation systems using this dataset. There are also still problems that need to be solved with the process of evaluating timeline models, but we hope that the framework described above will allow researchers to easily create evaluation datasets for timeline creation."}], "references": [{"title": "Timeline: A Dynamic Hierarchical Dirichlet Process Model for Recovering Birth/Death and Evolution of Topics in Text Stream", "author": ["Ahmed", "Xing2012] Amr Ahmed", "Eric P Xing"], "venue": "CoRR abs/1203.3463", "citeRegEx": "Ahmed et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Ahmed et al\\.", "year": 2012}, {"title": "Query based event extraction along a timeline", "author": ["Chieu", "Lee2004] Hai Leong Chieu", "Yoong Keok Lee"], "venue": null, "citeRegEx": "Chieu et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Chieu et al\\.", "year": 2004}, {"title": "Rouge: A package for automatic evaluation of summaries", "author": ["Chin-Yew Lin"], "venue": "Text Summarization Branches Out: Proceedings of the ACL04 Workshop,", "citeRegEx": "Lin.,? \\Q2004\\E", "shortCiteRegEx": "Lin.", "year": 2004}, {"title": "Bleu: a method for automatic evaluation of machine translation", "author": ["Salim Roukos", "Todd Ward", "Wei-Jing Zhu"], "venue": "In Proceedings of 40th Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "Papineni et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "Time-dependent Hierarchical Dirichlet Model for Timeline Generation", "author": ["Tao Wang"], "venue": "arXiv preprint arXiv:1312.2244", "citeRegEx": "Wang.,? \\Q2013\\E", "shortCiteRegEx": "Wang.", "year": 2013}, {"title": "Timeline Generation through Evolutionary Trans-Temporal Summarization", "author": ["Yan et al.2011a] Rui Yan", "Liang Kong", "Congrui Huang", "Xiaojun Wan", "Xiaoming Li", "Yan Zhang"], "venue": null, "citeRegEx": "Yan et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Yan et al\\.", "year": 2011}, {"title": "Evolutionary timeline summarization", "author": ["Yan et al.2011b] Rui Yan", "Xiaojun Wan", "Jahna Otterbacher", "Liang Kong", "Xiaoming Li", "Yan Zhang"], "venue": "In the 34th international ACM SIGIR conference,", "citeRegEx": "Yan et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Yan et al\\.", "year": 2011}], "referenceMentions": [{"referenceID": 2, "context": "Previous approaches have used drawn on working in document summarisation, using ROUGE (Lin, 2004) to evaluate timeline generation (Chieu and Lee, 2004; Yan et al.", "startOffset": 86, "endOffset": 97}, {"referenceID": 4, "context": "Previous approaches have used drawn on working in document summarisation, using ROUGE (Lin, 2004) to evaluate timeline generation (Chieu and Lee, 2004; Yan et al., 2011a; Yan et al., 2011b; Ahmed and Xing, 2012; Wang, 2013).", "startOffset": 130, "endOffset": 223}, {"referenceID": 3, "context": "Wang (2013) do this, but do not make their data available for direct comparison.", "startOffset": 0, "endOffset": 12}, {"referenceID": 4, "context": "For our evaluation pipeline, we adopt the approach of a number of papers in the field (Wang, 2013; Yan et al., 2011a; Yan et al., 2011b) in using the ROUGE metric (Lin, 2004).", "startOffset": 86, "endOffset": 136}, {"referenceID": 2, "context": ", 2011b) in using the ROUGE metric (Lin, 2004).", "startOffset": 35, "endOffset": 46}, {"referenceID": 3, "context": "It is similar to the BLEU measure for machine translation (Papineni et al., 2002).", "startOffset": 58, "endOffset": 81}, {"referenceID": 2, "context": "For details on how ROUGE scores are calculated, please refer to the original paper (Lin, 2004).", "startOffset": 83, "endOffset": 94}], "year": 2016, "abstractText": "The timeline generation task summarises an entity\u2019s biography by selecting stories representing key events from a large pool of relevant documents. This paper addresses the lack of a standard dataset and evaluative methodology for the problem. We present and make publicly available a new dataset of 18,793 news articles covering 39 entities. For each entity, we provide a gold standard timeline and a set of entityrelated articles. We propose ROUGE as an evaluation metric and validate our dataset by showing that top Google results outperform straw-man baselines.", "creator": "LaTeX with hyperref package"}}}