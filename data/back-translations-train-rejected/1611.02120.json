{"id": "1611.02120", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Nov-2016", "title": "Neural Networks Designing Neural Networks: Multi-Objective Hyper-Parameter Optimization", "abstract": "Artificial neural networks have gone through a recent rise in popularity, achieving state-of-the-art results in various fields, including image classification, speech recognition, and automated control. Both the performance and computational complexity of such models are heavily dependant on the design of characteristic hyper-parameters (e.g., number of hidden layers, nodes per layer, or choice of activation functions), which have traditionally been optimized manually. With machine learning penetrating low-power mobile and embedded areas, the need to optimize not only for performance (accuracy), but also for implementation complexity, becomes paramount. In this work, we present a multi-objective design space exploration method that reduces the number of solution networks trained and evaluated through response surface modelling. Given spaces which can easily exceed 1020 solutions, manually designing a near-optimal architecture is unlikely as opportunities to reduce network complexity, while maintaining performance, may be overlooked. This problem is exacerbated by the fact that hyper-parameters which perform well on specific datasets may yield sub-par results on others, and must therefore be designed on a per-application basis. In our work, machine learning is leveraged by training an artificial neural network to predict the performance of future candidate networks. The method is evaluated on the MNIST and CIFAR-10 image datasets, optimizing for both recognition accuracy and computational complexity. Experimental results demonstrate that the proposed method can closely approximate the Pareto-optimal front, while only exploring a small fraction of the design space.", "histories": [["v1", "Mon, 7 Nov 2016 15:38:39 GMT  (635kb,D)", "http://arxiv.org/abs/1611.02120v1", "To appear in ICCAD'16. The authoritative version will appear in the ACM Digital Library"]], "COMMENTS": "To appear in ICCAD'16. The authoritative version will appear in the ACM Digital Library", "reviews": [], "SUBJECTS": "cs.NE cs.LG", "authors": ["sean c smithson", "guang yang", "warren j gross", "brett h meyer"], "accepted": false, "id": "1611.02120"}, "pdf": {"name": "1611.02120.pdf", "metadata": {"source": "CRF", "title": "Neural Networks Designing Neural Networks: Multi-Objective Hyper-Parameter Optimization", "authors": ["Sean C. Smithson", "Guang Yang", "Warren J. Gross", "Brett H. Meyer"], "emails": ["guang.yang3}@mail.mcgill.ca,", "brett.meyer}@mcgill.ca", "permissions@acm.org."], "sections": [{"heading": "1. INTRODUCTION", "text": "This is not the first time that a country in which it is not a country in which it is not a country but a country in which it is not a country, a country in which it is not a country but a country in which it is not a country, a country in which it is not a country, a country in which it is not a country in which it is a country, a country in which it is not a country, a country, a country, a country, a country, a nation, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country,"}, {"heading": "1.1 Motivational Example", "text": "In fact, most people who fight for the rights of women and men struggle to enforce their rights; most people who fight for the rights of men and women struggle to exercise their rights and duties; most of them struggle to enforce their rights; most of them are women who fight for the rights of men and women; most of them are women who fight for the rights of men and women; most of them are women who fight for the rights of men and women; most of them are women who fight for the rights of men and women; and most of them are women who fight for the rights of men and women."}, {"heading": "1.2 Summary of Contributions", "text": "The main contributions of this paper are presented as follows: \u2022 We introduce a DSE method that uses RSM techniques to predict classification accuracy to automate the design of ANN hyperparameters, which is then validated with MLP and CNN designs that target the CIFAR-10 and MNIST image recognition datasets [14, 17]. \u2022 We show that multi-objective hyperparameter optimization can be successfully used as a method to reduce ANN implementation costs (computational complexity)."}, {"heading": "2.1 Hyper-Parameter Optimization", "text": "The design of neural network hyperparameters has long been considered cumbersome, intuitive and, consequently, ideal for automated hyperparameter optimization techniques such as those in [3], [4], [7] and [22]. However, this work was designed with the sole purpose of optimizing performance, regardless of the resulting computational resource requirements. In [4], two types of sequential hyperparameter optimization algorithms were presented; in both cases, the experimental results were positive compared to human-designed alternatives, and these results were also reflected in [3], where a random search for a large design space containing areas that may be considered less promising can sometimes exceed the performance of manually matched hyperparameters. Positive results were also presented in [7], where similar algorithms using a method of extrapolating the shape of learning curves during training are so suitable for large-scale network adjustments, while the parameters are less suitable for large-scale network adjustments."}, {"heading": "2.2 Weight Quantization and Pruning", "text": "The research in [6] and [9] are examples of two methods of building networks that reduce the need for multiplication or modify already-formed networks to minimize the number of non-zero weights. In [6], the authors attempted to reduce the computational resources required for the evaluation of fully interconnected as well as non-interconnected networks by using all weights as binary values of + 1 or \u2212 1. This reduces the number of multiplication operations that are performed in each run-up; however, the requirement to store floating point weights during training remains in place. [6] also compared trained binary weighted networks with traditional equivalents with the same layer dimensions and similar performance. However, [6] only very large network dimensions were taken into account; other benefits can be achieved through smaller optimized architectures. Instead of limiting to specific values, which in most cases cannot be represented with a weight reduction of 9, a very small number of network operations has been demonstrated."}, {"heading": "3. ANN SELF-DESIGN METHODOLOGY", "text": "This paper represents a DSE approach that looks for pareto-optimal hyperparameter configurations and has been applied to both MLP and CNN topologies. Design space is limited to: the number of Fully-Connected (FC) and Convolutionary layers, the number of nodes or filters in each layer, the size of the folding cores, the maximum pooling size, the type of activation function and the training rate of the network. These degrees of freedom represent huge design spaces and all strongly influence the cost and performance of the resulting networks. For design spaces of this size, conducting an exhaustive search is insoluble (designs with more than 1010 to 1020 possible solutions are not uncommon), so we propose to model the response area using an ANN for regression, where the amount of investigated solution points is used as a training set. The presented meta-heuristic ANN is then used to predict the performance of the candidate networks, not just points that are expected to be dominated by them."}, {"heading": "3.1 Main DSE Algorithm Overview", "text": "A flowchart describing the DSE implementation is shown in Figure 3. The general steps performed during the design space exploration can be divided as follows: 1. Sample of the next candidate point from a Gaussian distribution revolving around the previously examined solution (or sample of a random point for the first iteration).2. Predict the performance of the candidate solution using the neural RSM network and calculate the cost as: Costs = (number of weights) \u00b7 (weight units) + (number of multiplications) \u00b7 (multiplication units) 3. Compare the predicted results with the current paretooptimal front. If the candidate is predicted to be paretodominated, it is accepted with probability \u03b1, otherwise it is rejected with probability 1 \u2212 \u03b1.4., the previously investigated solution is rolled back and the algorithm reverts to step 1.5. If the candidate model is adopted, the maximum number of training steps is tested and the M is subsequently evaluated (the number of training steps)."}, {"heading": "3.2 Candidate Solution Sampling", "text": "The proposed sampling strategy is an adaptation of the Metropolis-Hastings algorithm [18]. In each iteration, a new candidate is selected from a Gaussian distribution revolving around the previously investigated solution point. Carrying out this random walk limits the number of samples selected from areas of the design space that are known to contain unsuitable solutions, thus reducing the exploration effort wasted. However, the exploration of an inferior solution may ultimately lead to those with superior performance, so the probability of accepting such a solution (\u03b1) must remain greater than zero; this also ensures that the training set for the RSM ANN remains variable. All experimental results in section 5 were obtained with \u03b1 = 10 \u2212 4."}, {"heading": "3.3 Predictive Neural Network Design", "text": "This RSM ANN consists of two hidden ReLU layers and a linear output layer. Experimental results showed that the size of the hidden layers provided the best performance with 25 x to 30 x the number of input nodes. RSM network inputs are formed as arrays characterizing all the dimensions explored. Integer input parameters (such as the number of nodes in a hidden layer or the size of the convolutionary cores) are scaled by the highest possible value of the respective parameter, resulting in normalized variables ranging from 0 to 1. For each parameter that represents a choice where options have no numerical relationship to each other (such as the number of nodes in a hidden layer or the size of the convolutionary cores), an input mode is added and the node representing the selected option is given an input value of 0.1, with the options having a hidden rate of 0.1 learned, two additional functions each (1 and 1 respectively)."}, {"heading": "4. EXPERIMENTAL SETUP", "text": "We evaluated our DSE strategy on ANN applications targeting the standard MNIST and CIFAR-10 image recognition data sets. In the case of the development of MLP models targeting the simpler MNIST problem, the results generated by the DSE algorithm were compared with the true Pareto-optimal fronts obtained from an exhaustive search in a limited solution space. Such a limitation of design space was required to make the full search tractable. In all cases, only a few iterations were required to compare to approximate Pareto-optimal fronts for both MLP and CNN models targeting MNIST, as well as an even larger one for the design of CNNs targeting CIFAR-10. In addition, the DSE algorithm was evaluated on much larger design spaces for both MLP and CNN models targeting MNIST, as on an even larger one for the design of CNNs targeting CIFAR-10."}, {"heading": "5. RESULTS", "text": "In addition, several parameters were kept constant for all experiments: The output layers consisted of Softmax units (where the jth output was defined as one j \u00f7 \u2211 Kk = 1 e-ink for a layer with K-nodes) and all network trainings were performed with categorical cross entropy (\u2212 \u2211 [targets \u00b7 log (predictions)] as a loss function [8, 12]. Finally, for all but the reduced MNIST design problem, batch normalization was recorded after each network layer in order to smooth the response areas [10]."}, {"heading": "5.1 Exhaustive Search Comparison", "text": "In order to evaluate the efficiency with which the method presents the true Pareto optimum options, we first compare the experimental results with those of a complete search for the design of MLP models for the MNIST dataset. To enable a complete search, we limit the design space to only the values outlined in the first section of the table, resulting in a moderate design space of 104 solutions, all of which have been trained and tested. Results of the execution of the DSE algorithm for 200 iterations (each iteration represents a design training, evaluation and model update) are plotted alongside those of the complete search in Figure 4. These results show that the true Pareto optimum optics is very closely approximated before the presented method, while requiring very few solutions to be evaluated."}, {"heading": "5.2 Evaluation on Expanded Design Spaces", "text": "To evaluate the performance of the heuristic method for much larger designs, the algorithm was executed on the remaining areas described in Table 2 for the design examples MLP and CNN. Total execution times (on an Intel Xeon E51603 CPU with 8GB RAM and an Nvidia Tesla K20c GPU) for the design examples are listed in Table 4 and the corresponding Pareto-optimal results (plotted as functions of the number of completed iterations) are in Figure 5. In these diagrams, the color scheme shows solutions with low errors in blue and low costs in red. Although the DSE results cannot be directly compared with the true results of an exhaustive search (the CIFAR-10 sample design space exceeds 1010 solutions), the trends discussed in Section 5.1 are mirrored by all the diagrams in Figure 5.1, which are not predicted in Figure 5.Due to the intractable nature of such exhaustive search results, the DSE results cannot always be predicted."}, {"heading": "5.3 RSM Prediction Accuracy", "text": "To confirm the assumption that a neural network can be trained by regression to model the response surface with sufficient accuracy, the RSM ANN prediction error is shown in Figure 6. This graph shows the absolute value of the error (percentage difference between the predicted and evaluated performance of each solution investigated) for each of the 500 DSE algorithm iterations performed during the MNIST-CNN design example (with results in Figure 5 (c)). The expected narrow error peaks occur at points where the DSE algorithm meets previously unexplored areas. If these solutions are added to the training set, the prediction error decreases when the response surface model is updated and peaks occur less frequently. Outside of these sparse peaks, the prediction accuracy is exceptionally high; the mean error over the last 95 iterations (all 35 points after the last peaks) is just 0.0."}, {"heading": "6. CONCLUSION", "text": "A DSE method for automating the multi-objective optimization of neural network hyperparameters that reduces both algorithm errors and computational complexity was presented. Compared to the results of an extensive search on a limited design space aimed at the MNIST dataset, it was found that the presented method drastically reduces the computing time required for convergence to a Pareto-optimal solution set. A low ADRS of 5% was achieved even after only 100 iterations; in practice, fewer solution evaluations may be required, with the corresponding execution times shorter than in Table 4. In addition, the scalability of the method was demonstrated in larger design spaces for both MLP and CNN models, which also target the CIFAR-10 dataset. Even if the presented DSE method is evaluated in vast design spaces, it was found to still efficiently approximate to a diverse Pareto-optimal front."}, {"heading": "7. ACKNOWLEDGEMENTS", "text": "This work was supported in part by a Postgraduate Scholarship - Doctoral (PGS-D) scholarship from the Natural Sciences and Engineering Research Council of Canada (NSERC) and equipment donations from Nvidia Corporation."}, {"heading": "8. REFERENCES", "text": "[1] M. Assefi, M. Wittie and A. Knight. Impact ofnetwork performance on cloud speech recognition. In Computer Communication and Networks (ICCCN), 2015 24th Int. Conf., pp. 1-6, Aug. 2015. [2] F. Bastien et al.: new features and speed improvements."}], "references": [{"title": "Impact of network performance on cloud speech recognition", "author": ["M. Assefi", "M. Wittie", "A. Knight"], "venue": "Computer Communication and Networks (ICCCN), 2015 24th Int. Conf., pages 1\u20136, Aug", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2015}, {"title": "Theano: new features and speed improvements", "author": ["F. Bastien"], "venue": "Deep Learning and Unsupervised Feature Learning NIPS 2012 Workshop,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2012}, {"title": "Random search for hyper-parameter optimization", "author": ["J. Bergstra", "Y. Bengio"], "venue": "J. of Machine Learning Research, 13(1):281\u2013305, Feb", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2012}, {"title": "Algorithms for hyper-parameter optimization", "author": ["J.S. Bergstra"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2011}, {"title": "Links between perceptrons, MLPs and SVMs", "author": ["R. Collobert", "S. Bengio"], "venue": "Proc. of the Twenty-first Int. Conf. on Machine Learning, ICML \u201904, pages 23\u201330, New York, NY, USA,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2004}, {"title": "Binaryconnect: Training deep neural networks with binary weights during propagations", "author": ["M. Courbariaux", "Y. Bengio", "J.-P. David"], "venue": "Advances in Neural Information Processing Systems 28, pages 3123\u20133131. Curran Associates, Inc.,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2015}, {"title": "Speeding up automatic hyperparameter optimization of deep neural networks by extrapolation of learning curves", "author": ["T. Domhan", "J.T. Springenberg", "F. Hutter"], "venue": "Proc. of the Twenty-Fourth Int. Joint Conf. on Artificial Intelligence, IJCAI, pages 3460\u20133468, July", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2015}, {"title": "Speech recognition with deep recurrent neural networks", "author": ["A. Graves", "A.R. Mohamed", "G. Hinton"], "venue": "2013 IEEE Int. Conf. on Acoustics, Speech and Signal Processing, pages 6645\u20136649, May", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2013}, {"title": "Learning both weights and connections for efficient neural network", "author": ["S. Han"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2015}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["S. Ioffe", "C. Szegedy"], "venue": "CoRR, abs/1502.03167,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "Statistical localization exploiting convolutional neural network for an autonomous vehicle", "author": ["S. Ishibushi"], "venue": "In Industrial Electronics Society, IECON 2015 - 41st Annual Conf. of the IEEE,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2015}, {"title": "What is the best multi-stage architecture for object recognition", "author": ["K. Jarrett"], "venue": "In Computer Vision,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2009}, {"title": "Adam: A method for stochastic optimization", "author": ["D.P. Kingma", "J. Ba"], "venue": "CoRR, abs/1412.6980,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning multiple layers of features from tiny images", "author": ["A. Krizhevsky"], "venue": "Technical report,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2009}, {"title": "ImageNet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "Advances in Neural Information Processing Systems 25, pages 1097\u20131105. Curran Associates, Inc.,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2012}, {"title": "Deep learning", "author": ["Y. LeCun", "Y. Bengio", "G. Hinton"], "venue": "Nature, 521(7553):436\u2013444, May", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. LeCun"], "venue": "Proc. of the IEEE,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1998}, {"title": "Equation of state calculations by fast computing machines", "author": ["N. Metropolis", "A.W. Rosenbluth", "M.N. Rosenbluth", "A.H. Teller", "E. Teller"], "venue": "Journal of Chemical Physics, 21:1087\u20131092, Jun", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1953}, {"title": "A critical survey of performance indices for multi-objective optimisation", "author": ["T. Okabe", "Y. Jin", "B. Sendhoff"], "venue": "Evolutionary Computation, 2003. CEC \u201903. The 2003 Congress on, volume 2, pages 878\u2013885, Dec", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2003}, {"title": "ReSPIR: A response surface-based Pareto iterative refinement for application-specific design space exploration", "author": ["G. Palermo", "C. Silvano", "V. Zaccaria"], "venue": "Computer-Aided Design of Integrated Circuits and Systems, IEEE Trans., 28(12):1816\u20131829, Dec", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2009}, {"title": "Big/little deep neural network for ultra low power inference", "author": ["E. Park"], "venue": "In Hardware/Software Codesign and System Synthesis (CODES+ISSS),", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2015}, {"title": "Optimizing deep learning hyper-parameters through an evolutionary algorithm", "author": ["S.R. Young"], "venue": "In Proc. of the Workshop on Machine Learning in High-Performance Computing Environments,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2015}], "referenceMentions": [{"referenceID": 15, "context": "Artificial Neural Network (ANN) models have become widely adopted as means to implement many machine learning algorithms and represent the state-of-the-art for many image and speech recognition applications [16].", "startOffset": 207, "endOffset": 211}, {"referenceID": 0, "context": "Instead, such applications are processed remotely and, depending on network conditions, are subject to variations in performance and delay [1].", "startOffset": 139, "endOffset": 142}, {"referenceID": 10, "context": "ANNs are also finding application in other emerging areas, such as autonomous vehicle localization and control, where meeting power and cost requirements is paramount [11].", "startOffset": 167, "endOffset": 171}, {"referenceID": 15, "context": "These adjustments often involve a variation of the Stochastic Gradient Descent (SGD) method [16].", "startOffset": 92, "endOffset": 96}, {"referenceID": 6, "context": "This relies on human intuition and expert knowledge of the target application in conjunction with extensive trial and error [7, 22].", "startOffset": 124, "endOffset": 131}, {"referenceID": 21, "context": "This relies on human intuition and expert knowledge of the target application in conjunction with extensive trial and error [7, 22].", "startOffset": 124, "endOffset": 131}, {"referenceID": 8, "context": "With the proliferation of machine learning on embedded and mobile devices, ANN application designers must now deal with stringent power and cost requirements [9, 16, 21].", "startOffset": 158, "endOffset": 169}, {"referenceID": 15, "context": "With the proliferation of machine learning on embedded and mobile devices, ANN application designers must now deal with stringent power and cost requirements [9, 16, 21].", "startOffset": 158, "endOffset": 169}, {"referenceID": 20, "context": "With the proliferation of machine learning on embedded and mobile devices, ANN application designers must now deal with stringent power and cost requirements [9, 16, 21].", "startOffset": 158, "endOffset": 169}, {"referenceID": 18, "context": "Formally, a solution vector (a1, b1) (where a and b are two objectives for optimization) is said to dominate another point (a2, b2) if a1 < a2 and b1 \u2264 b2, or b1 < b2 and a1 \u2264 a2; the set of points which are not dominated by any other solution constitutes the Pareto-optimal front [19].", "startOffset": 281, "endOffset": 285}, {"referenceID": 4, "context": "1 Motivational Example While there are many different ANN models, the MultiLayer Perceptron (MLP) is a well-known form, which rose in popularity with the advent of the back-propagation training algorithm [5].", "startOffset": 204, "endOffset": 207}, {"referenceID": 8, "context": "Therefore, multiply-accumulate operations and memory accesses remain the dominant tasks in terms of cost [9, 12].", "startOffset": 105, "endOffset": 112}, {"referenceID": 11, "context": "Therefore, multiply-accumulate operations and memory accesses remain the dominant tasks in terms of cost [9, 12].", "startOffset": 105, "endOffset": 112}, {"referenceID": 14, "context": "Even more complex structures are those of Convolutional Neural Network (CNN), which have demonstrated state-ofthe-art results in image recognition [15].", "startOffset": 147, "endOffset": 151}, {"referenceID": 14, "context": "An example of a possible efficient down-sampling method is the inclusion of max-pooling (or mean-pooling) after convolutional layers [15].", "startOffset": 133, "endOffset": 137}, {"referenceID": 13, "context": "This method is then validated with MLP and CNN designs targeting the CIFAR-10 and MNIST image recognition datasets [14, 17].", "startOffset": 115, "endOffset": 123}, {"referenceID": 16, "context": "This method is then validated with MLP and CNN designs targeting the CIFAR-10 and MNIST image recognition datasets [14, 17].", "startOffset": 115, "endOffset": 123}, {"referenceID": 2, "context": "1 Hyper-Parameter Optimization The design of neural network hyper-parameters has long been considered to be unwieldy, unintuitive, and as a consequence, ideal for automated hyper-parameter optimization techniques such as in [3], [4], [7], and [22].", "startOffset": 224, "endOffset": 227}, {"referenceID": 3, "context": "1 Hyper-Parameter Optimization The design of neural network hyper-parameters has long been considered to be unwieldy, unintuitive, and as a consequence, ideal for automated hyper-parameter optimization techniques such as in [3], [4], [7], and [22].", "startOffset": 229, "endOffset": 232}, {"referenceID": 6, "context": "1 Hyper-Parameter Optimization The design of neural network hyper-parameters has long been considered to be unwieldy, unintuitive, and as a consequence, ideal for automated hyper-parameter optimization techniques such as in [3], [4], [7], and [22].", "startOffset": 234, "endOffset": 237}, {"referenceID": 21, "context": "1 Hyper-Parameter Optimization The design of neural network hyper-parameters has long been considered to be unwieldy, unintuitive, and as a consequence, ideal for automated hyper-parameter optimization techniques such as in [3], [4], [7], and [22].", "startOffset": 243, "endOffset": 247}, {"referenceID": 3, "context": "Two types of sequential hyper-parameter optimization algorithms were presented in [4]; in both cases experimental results compared positively to human designed alternatives.", "startOffset": 82, "endOffset": 85}, {"referenceID": 2, "context": "These findings were echoed in [3], where it was demonstrated that a random search of a large design space, which contains areas that may be considered less promising, can at times exceed the performance of manually tuned hyper-parameters.", "startOffset": 30, "endOffset": 33}, {"referenceID": 6, "context": "Positive results were also presented in [7] where similar algorithms were extended using a method to extrapolate the shape of learning curves during training, so as to evaluate fewer epochs for unfit solutions and reduce design time.", "startOffset": 40, "endOffset": 43}, {"referenceID": 2, "context": "Since parameters which perform favourably for a large network may not be optimal for a smaller alternative, and the most important hyper-parameters (with the greatest impact on resulting performance) may vary for different data sets, the need for multi-objective optimization (especially where low power platforms are targeted) is clear [3, 22].", "startOffset": 337, "endOffset": 344}, {"referenceID": 21, "context": "Since parameters which perform favourably for a large network may not be optimal for a smaller alternative, and the most important hyper-parameters (with the greatest impact on resulting performance) may vary for different data sets, the need for multi-objective optimization (especially where low power platforms are targeted) is clear [3, 22].", "startOffset": 337, "endOffset": 344}, {"referenceID": 19, "context": "Additionally, in [20] an automated DSE method was applied to the multi-objective optimization problem of applicationspecific MPSoC design, a field which also consists of highdimensionality solution spaces.", "startOffset": 17, "endOffset": 21}, {"referenceID": 5, "context": "The research in [6] and [9] are examples of two methods that construct networks which reduce the need for multiplications, or modify already trained networks in order to minimize the number of non-zero weights.", "startOffset": 16, "endOffset": 19}, {"referenceID": 8, "context": "The research in [6] and [9] are examples of two methods that construct networks which reduce the need for multiplications, or modify already trained networks in order to minimize the number of non-zero weights.", "startOffset": 24, "endOffset": 27}, {"referenceID": 5, "context": "In [6], the authors attempted to reduce the computational resources required when evaluating fully-connected, as well as convolutional, networks through representing all weights as binary values of +1 or\u22121.", "startOffset": 3, "endOffset": 6}, {"referenceID": 5, "context": "The work in [6] also compared trained binary weighted networks to traditional equivalents with equal layer dimensions and similar performance was demonstrated.", "startOffset": 12, "endOffset": 15}, {"referenceID": 5, "context": "However, [6] only considered very large network dimensions; further benefits may be obtained from smaller optimized architectures.", "startOffset": 9, "endOffset": 12}, {"referenceID": 8, "context": "Instead of restricting weights to specific values, in [9] a pruning method was presented in which a network can be trained while reducing the number of non-zero weights.", "startOffset": 54, "endOffset": 57}, {"referenceID": 8, "context": "The results in [9] demonstrated up to 70% reductions in the numbers of floating-point operations required for various networks, with little to no reduction in performance.", "startOffset": 15, "endOffset": 18}, {"referenceID": 17, "context": "2 Candidate Solution Sampling The sampling strategy proposed is an adaptation of the Metropolis-Hastings algorithm [18].", "startOffset": 115, "endOffset": 119}, {"referenceID": 1, "context": "In order to perform the DSE, all ANN models were trained and tested using the Theano framework in order to take advantage of GPGPU optimizations [2].", "startOffset": 145, "endOffset": 148}, {"referenceID": 8, "context": "Operation Energy Cost from [9] Normalized Cost", "startOffset": 27, "endOffset": 30}, {"referenceID": 8, "context": "Normalized costs assumed for all the experimental results are shown in Table 1, which are based on the energy costs for 32-bit floating-point operations from [9].", "startOffset": 158, "endOffset": 161}, {"referenceID": 7, "context": "In addition, several parameters were kept constant for all experiments: output layers were composed of softmax units (with jth output defined as ej \u00f7 \u2211K k=1 e ink for a layer with K nodes) and all network training was performed with categorical cross-entropy (\u2212 \u2211 [targets\u00d7 log (predictions)]) as loss function [8, 12].", "startOffset": 311, "endOffset": 318}, {"referenceID": 11, "context": "In addition, several parameters were kept constant for all experiments: output layers were composed of softmax units (with jth output defined as ej \u00f7 \u2211K k=1 e ink for a layer with K nodes) and all network training was performed with categorical cross-entropy (\u2212 \u2211 [targets\u00d7 log (predictions)]) as loss function [8, 12].", "startOffset": 311, "endOffset": 318}, {"referenceID": 9, "context": "Finally, for all except the reduced MNIST design problem, batch normalization was included after each network layer in order to smooth the response surfaces [10].", "startOffset": 157, "endOffset": 161}, {"referenceID": 12, "context": "1 16 Log Filter kernel size 3x3 to 9x9 4 Linear Max-pool size 2x2 to 3x3 2 Linear Activation function ReLU 1 N/A Training algorithm Adam [13] 1 N/A Batch sizes 200 1 N/A Training epochs 20 1 N/A", "startOffset": 137, "endOffset": 141}, {"referenceID": 19, "context": "known for this restricted case, we use Average Distance to Reference Set (ADRS) as the metric of evaluation; quantifying how closely the approximated set differs from the exact [20].", "startOffset": 177, "endOffset": 181}, {"referenceID": 11, "context": "In comparison with manually designed architectures in literature, the Pareto-optimal results in Figure 5(c) include points that offer equivalent performance to the CNN designs in [12] and [17], with implementation costs as low as 25% of their manually designed counterparts (when weighted with the same cost model detailed in Table 1).", "startOffset": 179, "endOffset": 183}, {"referenceID": 16, "context": "In comparison with manually designed architectures in literature, the Pareto-optimal results in Figure 5(c) include points that offer equivalent performance to the CNN designs in [12] and [17], with implementation costs as low as 25% of their manually designed counterparts (when weighted with the same cost model detailed in Table 1).", "startOffset": 188, "endOffset": 192}], "year": 2016, "abstractText": "Artificial neural networks have gone through a recent rise in popularity, achieving state-of-the-art results in various fields, including image classification, speech recognition, and automated control. Both the performance and computational complexity of such models are heavily dependant on the design of characteristic hyper-parameters (e.g., number of hidden layers, nodes per layer, or choice of activation functions), which have traditionally been optimized manually. With machine learning penetrating low-power mobile and embedded areas, the need to optimize not only for performance (accuracy), but also for implementation complexity, becomes paramount. In this work, we present a multi-objective design space exploration method that reduces the number of solution networks trained and evaluated through response surface modelling. Given spaces which can easily exceed 10 solutions, manually designing a near-optimal architecture is unlikely as opportunities to reduce network complexity, while maintaining performance, may be overlooked. This problem is exacerbated by the fact that hyper-parameters which perform well on specific datasets may yield sub-par results on others, and must therefore be designed on a per-application basis. In our work, machine learning is leveraged by training an artificial neural network to predict the performance of future candidate networks. The method is evaluated on the MNIST and CIFAR-10 image datasets, optimizing for both recognition accuracy and computational complexity. Experimental results demonstrate that the proposed method can closely approximate the Pareto-optimal front, while only exploring a small fraction of the design space.", "creator": "LaTeX with hyperref package"}}}