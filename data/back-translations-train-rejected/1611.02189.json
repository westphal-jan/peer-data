{"id": "1611.02189", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Nov-2016", "title": "CoCoA: A General Framework for Communication-Efficient Distributed Optimization", "abstract": "The scale of modern datasets necessitates the development of efficient distributed optimization methods for machine learning. We present a general-purpose framework for the distributed environment, CoCoA, that has an efficient communication scheme and is applicable to a wide variety of problems in machine learning and signal processing. We extend the framework to cover general non-strongly convex regularizers, including L1-regularized problems like lasso, sparse logistic regression, and elastic net regularization, and show how earlier work can be derived as a special case. We provide convergence guarantees for the class of convex regularized loss minimization objectives, leveraging a novel approach in handling non-strongly convex regularizers and non-smooth loss functions. The resulting framework has markedly improved performance over state-of-the-art methods, as we illustrate with an extensive set of experiments on real distributed datasets.", "histories": [["v1", "Mon, 7 Nov 2016 17:49:49 GMT  (406kb,D)", "http://arxiv.org/abs/1611.02189v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["virginia smith", "simone forte", "chenxin ma", "martin takac", "michael i jordan", "martin jaggi"], "accepted": false, "id": "1611.02189"}, "pdf": {"name": "1611.02189.pdf", "metadata": {"source": "CRF", "title": "CoCoA: A General Framework for Communication-Efficient Distributed Optimization", "authors": ["Virginia Smith", "Simone Forte", "Chenxin Ma"], "emails": ["vsmith@berkeley.edu", "simone.forte@gess.ethz.ch", "chm514@lehigh.edu", "Takac.MT@gmail.com", "jordan@cs.berkeley.edu", "martin.jaggi@epfl.ch"], "sections": [{"heading": null, "text": "The scale of modern data sets requires the development of efficient distributed optimisation methods for machine learning. We present a universal framework for the distributed environment, CoCoA, which has an efficient communication scheme and is applicable to a variety of machine learning and signal processing problems. We extend the framework to include general non-convex regulators, including L1-regulated problems such as lasso, sparse logistic regression and elastic network regulation, and show how earlier work can be derived as a special case. We offer convergence guarantees for the class of convex regulated loss minimisation targets, using a novel approach to dealing with non-convex regulators and non-frictionless loss functions. The resulting framework has significantly improved performance compared to modern machine learning methods, as we illustrate with an extensive series of experiments on real distributed data sets."}, {"heading": "1. Introduction", "text": "In fact, most of them are able to play by the rules they have imposed on themselves."}, {"heading": "1.1 Contributions", "text": "Unlike previous work by Yang (2013); Jaggi et al. (2014); Ma et al. (2015a); and Ma et al. (2015b), our generalized, coherent framework can: (1) specifically include difficult cases of L1 regulation and other non-strongly convex regulators; (2) allows the flexibility of data distribution either by feature or training point; and (3) can be executed either on a primary or dual formulation that we show to have significant theoretical and practical implications. Flexible communication and local solutions. Two key benefits of the proposed framework are its communication efficiency and the ability to deploy off-the-shelf machine solvers."}, {"heading": "2. Background and Setup", "text": "In this paper, we develop a general framework for minimizing problems of the following form: \"(u) + r (u), (I) for convex functions\" and \"r. Often, the first term is an empirical loss of data taking the form \u2211 i'i (u), and the second term r is a regularizer, e.g. r (u) = \u03bb \u0430\u0441u \u0121p. This formulation includes many popular methods of machine learning and signal processing, such as support vector machines, linear and logistic regression, lasso and sparse logistic regression, and many others."}, {"heading": "2.1 Definitions", "text": "The following standard definitions are used throughout the paper: Definition 1 (L-Lipschitz continuity); A function h: Rm \u2192 R is L-Lipschitz continuous if we have L-limited support if its effective domain is limited by L, i.e., h (u) < +; Definition 2 (L-Bounded Support); A function h: Rm \u2192 R-smoothness); A function h: Rm \u2192 R is (1 / \u00b5) -smooth if it is differentiable and its derivative (1 / \u00b5) -Lipschitz continuous or equivalent (u) -h (v) + < A function h: Rm \u2192 R is (1 / \u00b5) -smooth if it is differentiable and its derivative (1 / \u00b5) -Lipschitz continuous, or equivalent (u) -h (v), u v > -2 (4-\u00b5) (\u00b5)."}, {"heading": "2.2 Primal-Dual Setting", "text": "Numerous methods have been proposed to solve (I), and these methods generally fall into two categories: primary methods that are directly based on the primary target, and dual methods that instead run on the dual formulation of the primary target. In developing our framework, we present an abstraction that makes it possible to run either a dual problem or a dual variant of our framework. In particular, to solve the input problem (I), we look at the mapping of the problem on one of the following two general problems: min \u03b1, Rn [OA (\u03b1): = f (A\u03b1) + g (\u03b1)] (A) min w (OB): = f) + g (A > w)] (B): Here are the differences between the parameters vectors, A: = [x1;.; xn] [Rd \u00d7 n], a data matrix is a data matrix with column vectors xi [1].n}, n}, and we have the parameters A; A."}, {"heading": "2.3 Assumptions and Problem Cases", "text": "Our main assumptions with regard to the problem (A) are that f (1 / \u03c4) -smooth and the function g is separable, i.e., g (\u03b1) = \u2211 i gi (\u03b1i), with each gi having an L-limited support. Given the duality between the problems (A) and (B), this can be tantamount to stating that in problem (B) f * is strongly convex and the function g * (\u2212 A > w) = \u2211 i (\u2212 x > i w) is separable, where each g * i is L-Lipschitz. To clarify, we base our assumptions in Table 1 on the targets (A) and (B) on the general input problem (I). Suppose, as in Equation (I), we want to find a minimizer of the general object i, which is L-Lipschitz."}, {"heading": "2.4 Running Examples", "text": "To illustrate the three cases in Table 1, we will consider several examples below. These applications will serve as examples for the whole work and we will look at them again in our experiments (Section 6). Further applications and details can be found in Section 5.1. Elastic Net Regression (Case I: Map to either (A) or (B). We can map the elastic-net regulated smallest squares regression, min-u-Rp1 2 - Implications. \u2212 b-2 + Implications. (Case I: Map to either (A) or (B), (7) to each target (A) or (B). To make the map to target (A), we will allow: f (A\u03b1) = 12 - b-2 + Implications."}, {"heading": "2.5 Data Partitioning", "text": "To view our setup in the distributed environment, we assume that the dataset A is distributed on K machines according to a partition {Pk} Kk = 1 of the columns of A-Rd \u00b7 n. We define the size of the partition on machine k by nk = | Pk |. For machine k {1,..., K} and the weight vector \u03b1-Rn, we define \u03b1 [k] and Rn as n-vectors with elements (\u03b1 [k]) i: = \u03b1i if i-Pk and (\u03b1 [k]) i: = 0 otherwise. Similarly, we write A [k] for the corresponding group of columns of A and zeros elsewhere (note that columns can either correspond to training examples or characteristics depending on the application)."}, {"heading": "3. CoCoA", "text": "We will first describe the proposed framework, CoCoCoA, at a high level, and then discuss two approaches to the practical application of the framework: CoCoA at the origin, where we consider (A) the primary goal and apply the framework directly to this problem, and CoCoCoA at the dual, where we instead consider (B) the primary goal, and then the framework in the dual (A). Note that in both approaches the goal is to calculate a minimization of the problem (A) in a distributed way; the main difference will be whether we consider (A) the primary goal or the dual goal."}, {"heading": "3.1 The Generalized Framework", "text": "The goal of our framework is to find a global lens minimization problem (A) for each problem, while the distribution of the calculation is based on the partitioning of the dataset A across machines (Section 2.5). As a first step, it should be noted that the distribution of the update to the function g in the lens (A) is simple, since we have requested that this term be divisible according to the partitioning of our data, i.e. we propose to minimize a square approximation of the function that splits the minimization between different machines. We make this approximation precise in the following subsection.Data-local quadratic subproblems. In the general CoCoA framework (algorithm 1), we distribute the calculation by defining a data-local problem of the optimization problem."}, {"heading": "3.2 Primal Distributed Optimization", "text": "In the primary distributed version of the framework (algorithm 2), the framework is executed by mapping the initial problem (I) directly to the target (A) and then applying the generalized CoCoA framework described in algorithm 1. In other words, we view problem (A) as the primary target and solve this problem directly. Theoretically, considering (A) as the primacy will allow us to consider non-strongly convex regulators, since we allow the terms gi to be not strongly convex. This setting has not been taken into account in previous work by Yang (2013), Jaggi et al. (2014), Ma et al. (2015b) and Ma et al. (2015a), and we discuss it in detail in section 4, as additional machines need to be introduced to develop primary dual rates for this setting."}, {"heading": "3.3 Dual Distributed Optimization", "text": "In the dual distributed version of the framework (algorithm 3), we execute the framework by mapping the original problem (I) to the target (B) and then solving the problem by executing algorithm 1 to the target (A). In other words, we consider problem (B) to be the origin and solve this problem via the target (A).This version of the framework will allow us to take into account non-smooth losses, such as loss of hinges or absolute deviation, since the terms g \u043a i cannot be smooth. From a practical point of view, this version of the framework typically implies that the data is distributed according to training points and that a vector O (number of characteristics) must be communicated with each external iteration. Therefore, this variant may be preferable if the number of training points exceeds the number of characteristics. Algorithm 3 CoCoA-Dual (mapping Problem (I) to (B)) 1: Input map: problem from target (I) to data scale (B): 1."}, {"heading": "3.4 Primal vs. Dual", "text": "In Table 2, we look at the three cases in Section 2 that show how the primary and dual variants of CoCoA can be applied to different input problems (u) + r (u) depending on the properties of the functions \"and r. In particular, in the setting where\" it is smooth and the user is strongly convex, the user can choose whether to execute the framework in the primary (Algorithm 2) or in the dual (Algorithm 3) method. Intuitively, Algorithm 2 will be preferable as it loses high precision and Algorithm 3 is preferred as \"it loses smoothness.\" However, there are also system-related aspects that need to be taken into account. In Algorithm 2, we typically distribute the data by feature, and in Algorithm 3 by training point (this distribution depends on how the terms n and d are defined in our mapping, see Section 5. Depending on whether the number of features or the number of training points is the dominant algorithm, we can opt for algorithm 2 or 3."}, {"heading": "3.6 Comparison to ADMM", "text": "Finally, in this sub-section, we offer a direct comparison between CoCoA and ADMM (Boyd et al., 2010). ADMM is a well-established framework for distributed optimisation. Similar to CoCoA, ADMM differs from the methods discussed in the previous section by defining a partial problem for each problem in parallel, instead of paralleling a global charge or mini-batch update. It also uses duality structure, similar to that discussed in Section 2.For the ADMM consensus, the target W W W (B) is dismantled with a re-parameterisation. \u2212 W \u2212 W \u2212 W \u2212 W \u2212 W \u2212 W \u2212 W \u2212 W \u2212 W \u2212 W \u2212 W \u2212 W \u2212 W \u2212 W \u2212 W \u2212 W \u2212 W \u2212 W W W W W W W \u2212 W W W \u2212 W \u2212 W \u2212 W \u2212 W \u2212 W \u2212 W \u2212 W \u2212 W \u2212 W \u2212 W \u2212 W \u2212 W \u2212 W \u2212 W \u2212 W \u2212 W \u2212 W \u2212 K \u2212 K \u2212 K \u2212 K \u2212 K \u2212 W \u2212 W \u2212 W \u2212 W \u2212 W \u2212 W \u2212 W \u2212 W \u2212 W \u2212 W \u2212 W \u2212 W \u2212 W \u2212 W \u2212 W \u2212 W \u2212 W \u2212 W \u2212 W \u2212 W \u2212 W \u2212 W \u2212 W \u2212 W \u2212 W \u2212 W \u2212 W \u2212 W \u2212 W \u2212 W \u2212 W \u2212 W \u2212 W \u2212 W \u2212 W \u2212 W \u2212 W \u2212 W \u2212 W \u2212 W \u2212 W \u2212 W \u2212 W \u2212 W \u2212 W \u2212 W \u2212 W \u2212 W \u2212 W \u2212 W \u2212 W \u2212 W \u2212 W \u2212 W \u2212 W \u2212 W \u2212 W \u2212 W \u2212 W \u2212 W \u2212 W \u2212 W \u2212 W \u2212 W \u2212 W \u2212 W \u2212 W \u2212 W \u2212 W \u2212 W \u2212 W \u2212 W \u2212 W \u2212 W \u2212 W \u2212 W \u2212 W \u2212 W W \u2212 W \u2212 W \u2212 W \u2212 W \u2212 W \u2212 W \u2212 W \u2212 W \u2212 W \u2212 W \u2212 W \u2212 W \u2212 W \u2212 W \u2212 W \u2212 W \u2212 W \u2212 W \u2212 W \u2212 W \u2212 W \u2212 W \u2212 W \u2212 W \u2212 W \u2212 W \u2212 W \u2212 W \u2212 W \u2212 W \u2212 W \u2212 W \u2212 W \u2212 W \u2212 W \u2212 W \u2212 W \u2212 W \u2212 W \u2212 W \u2212 W \u2212 W \u2212 W \u2212 W \u2212 W \u2212 W \u2212 W \u2212 W \u2212 W \u2212 W \u2212 W \u2212 W \u2212 W \u2212 W \u2212 W \u2212 W \u2212 W \u2212 W \u2212 W \u2212 W \u2212 W \u2212 W \u2212 W"}, {"heading": "4. Convergence Analysis", "text": "In this section, we provide convergence rates for the proposed framework and introduce an important theoretical technique for the analysis of non-strongly convex terms in the primary-dual environment. To simplify the representation, we assume in the analysis that the data distribution is balanced; i.e. nk = n / K for all k. Furthermore, we assume that the columns of A meet the requirements for all i [n]. We present rates in the case where \u03b3: = 1 is in algorithm 1 and in which the partial problems (10) are defined using the corresponding safe bound \u03c3: = K. This case guarantees convergence and delivers our fastest rates in the distributed environment, which will not deteriorate in particular if the number of machines K grows and n remains unchanged."}, {"heading": "4.1 Proof Strategy: Relating Subproblem Approximation to Global Progress", "text": "In order to ensure convergence, it is crucial to show how the progress on the local sub-problems (10) relates to the global objective OA. Our first problem provides precisely this information. In particular, we see that if the aggregation and sub-problem parameters are selected according to definition 5, the sum of the sub-problem targets, \u2211 K = 1 G\u03c3 \u2032 k, will form a blockable upper limit on the global objective OA. Lemma 1. For any weight vector \u03b1 Rn, v = v (\u03b1): = A\u03b1, and real values \u03b3 \u00b2 satisfactorily (11), it applies that OA (\u03b1 + \u03b3 K \u00b2 k = 1 \u2206 \u03b1 \u2212 k]) \u2264 (1 \u2212 \u03b3) OA (\u03b1 \u00b2 K \u00b2 k (\u03b1 \u00b2 k), and real values \u03b3 \u00b2 (\u03b1 \u00b2), \u03c3 \u00b2 (2) that the OA (\u03b1 \u00b2) upper limit is. (16) Proof Lemma 1 is provided in Appendix D. \u2212 We use this main problem \u2212 in our assumption about the combination of the objective."}, {"heading": "4.2.1 Bounded support modification", "text": "As already mentioned, additional work is needed if the original problem is to be applied at this point, while these assumptions are necessary to make the necessary adjustments, such as the L1 standard, which has no L-bounded support for each individual problem, and therefore violates the assumptions of the theory. Note: The conjugate function of gi = = = the indicator function of an interval is not even defined at all points. To solve this problem, existing approaches will typically apply a simple smoothing technique (as in Nesterov, 2005): by adding a small amount of L2 to the objective, the functions become strongly convex."}, {"heading": "4.4 Convergence Cases", "text": "If we look at Table 1 from Section 2, we summarize our convergence guarantees for the three cases of input problems (I) in the following table. In particular, we see that for cases II and III we get a sublinear convergence rate, while for case I we can get a faster linear rate as provided in Theorem 3."}, {"heading": "4.5 Recovering Earlier Work as a Special Case", "text": "As a special case, the proposed framework and rates apply directly to L2-regulated loss minimization problems, including those presented in the earlier work of Jaggi et al. (2014) and Ma et al. (2015b). Note 3. If we apply algorithm 3 (mapping (I) to (B)), we limit f-Lipschitz \"i losses (see Ma et al., 2015b, episode 9) and leave g-v1 (Jaggi et al., 2014) as a special case, the CoCoA + rates apply to general L-Lipschitz'i losses (see Ma et al., 2015b, episode 9). Previous work of CoCoA-v1 (Jaggi et al., 2014) did not provide rates for L-Lipschitz's i losses."}, {"heading": "5. Applications", "text": "In this section, we will discuss in detail sample applications that can be poured within the general CoCoA framework. For each example, we will describe the primary dual setup and algorithmic details, discuss the convergence characteristics of our framework for application, and include practical aspects such as information on modern local solvers. We will discuss examples based on the three cases defined in Table 1 of Section 2 to find a minimization of the overall goal \"(u) + r (u), and give a summary of these common examples in Table 4."}, {"heading": "5.1 Case I: Smooth `, Strongly convex r", "text": "To illustrate the role of f as a smooth loss function and g as a strongly convex regularizer in objective (A), in contrast to their traditional roles in the previous work (Yang, 2013; Jaggi et al., 2014; Ma et al., 2015b, a), we can consider the following examples. Note: The assignment to objective (B) trivially follows the assumption that the loss is separable across training points (see Table 4).For the examples in this subsection, we use non-standardized definitions of the number of training points as d and the number of features as n."}, {"heading": "5.2 Case II: Smooth `, Non-Strongly Convex Separable r", "text": "In case II, we consider the mapping of the input problem (I) to the objective problem (A), where \"it is assumed that it is smooth and not strongly convex and separable. For smooth losses in (A), we can consider as examples provided in subsection 5.1, for example, the least square loss or logistical loss. For an example of a non-strongly convex convex regulator, we consider the important case of L1 regulation below. Here, too, we note that this application cannot be realized by objective (B), assuming that the regularization term f * is strongly convex. Not strongly convex r: L1 regulator. L1 regulation is achieved in an objective manner (A) by making an additional modification to obtain a primaldual convergence and certificates for this setting."}, {"heading": "5.3 Case III: Non-Smooth Separable `, Strongly Convex r", "text": "Finally, in case III, we look at the mapping of the input problem (I) to the target (B), where \"it is assumed that it is not smooth and separable and r is strongly convex. We discuss two common cases of general non-smooth losses,\" including loss of hinges for classification and absolute loss of deviation for regression. We note that these losses cannot be realized directly by objective (A), assuming that the data pass term f is smooth: loss of hinges. For classification problems, we can consider a hinge loss support vector machine model, on n training points in Rd given with the loss: g (\u2212 A > w) = n g convex: i = 1 g convex."}, {"heading": "5.4 Local Solvers", "text": "As discussed in Section 3, the partial problems solved on each machine in the CoCoA framework are attractive in that their structure is very similar to the global problem (A), the main difference being that they are defined on a smaller (local) subset of data and a simpler dependence on the term f. Therefore, solutions that have already proven their value in the single machine or in the multicore setting can easily be used within the framework. We discuss some specific examples of local solutions below and refer the reader to Ma et al. (2015a) for empirical research of these selection criteria. In the primary environment (algorithm 2), the local partial problem (10) becomes a simple square problem on the local data, applying the regulation only to local variables. For the L1 examples discussed, there are fast L1 solvers for the individual machine case, such as Glmnet variants (Friedman., 2010) or Flash (problem and Johnson) that can be applied directly to the local ones."}, {"heading": "6. Experiments", "text": "In this section we will show the empirical performance of CoCoCoA in the distributed environment. We will first compare CoCoA to competing methods for two common machine learning applications: lasso regression (Section 6.1) and supportive methods (Section 6.2). We will then explore the performance of CoCoA in the primary constellation by applying an elastic mesh regression model with both variants (Section 6.3). Finally, we will illustrate general properties of the CoCoCoA method empirically in Section 6.4.4.Experiment CoCoA to compare numerous states of art. \u2022 We will compare CoCoCoA to numerous states of art that are generally optimized, including: mini-batch stochastic gradient. \u2022 We will compare ourselves with lasso, we will compare ourselves against Mb-SGD with an L1-prox. \u2022 GD: Full Gravenz."}, {"heading": "6.3 Primal vs. Dual", "text": "To understand the effect of the primary versus dual optimization for CoCoA, we compare the performance of both variants by adjusting an elastic network regression model (7) to two sets of data. To compare the methods, we use the closed-loop coordinate descent in both variants as a local solver. Results in Figure 4 show that CoCoA tends to perform better in the dual model in datasets with a large number of training points (relative to the number of features) and that performance deteriorates as expected when the strong convexity of the problem disappears. In contrast, CoCoA performs well in the primary for datasets with a large number of features relative to training points and is robust to changes in strong convexity. These performance changes are to be expected as we have already discussed that CoCoA is better suited for non-strongly convexed regulators in the primary (Section 6.1), and that the function of CoCoA compares codes to the dual point (relative to the dual point)."}, {"heading": "6.4 General Properties: Effect of Communication", "text": "Finally, we note that, unlike the methods compared in Sections 6.1 and 6.2, CoCoA has only one parameter for tuning: the approximation quality of the partial problem, which we control in our experiments based on the number of local iterations of partial problem H using the example of local coordinate origin. We continue to examine the effect of this parameter in Figure 5 and provide a general guide for choosing this parameter in practice (see Note 1). In particular, we see that increasing H always leads to better performance in terms of the number of communication rounds, but smaller or greater values of H can lead to better performance in terms of the wall clock time, depending on the cost of communication and calculation. One of the reasons for the significant performance gains of CoCoA is the flexibility to fine-tune H."}, {"heading": "6.5 Experiment Details", "text": "In this area, we are able to position ourselves in a different way, as we have done in the past. (...) We have to get involved in another world, another world, another world, another world, another world, another world, another world, another world, another world, another world, another world, another world, another world, another world, another world, another world, another world, another world, another world, another world, another world, another world, another world, another world, another world, another world, another world, another world, another world, another world, another world. \"(...)"}, {"heading": "7. Related Work", "text": "For strongly convex regulators, however, the current state of the technology for empirical loss minimization has been considered two-dimensional since 2011. (e.g., ShalevShwartz and Zhang, 2014) In contrast to primary stochastic gradient lineage methods (SGD), the SDCA family is often preferred because it is free of learning rate parameters and guarantees faster (geometric) convergence. Interestingly, a similar trend in coordinate communication between solvers has been observed, but with the roles of primary and dual inversion. For these problems, coordinated lineage methods at the primary level have become state-of-the-of-art, as in glmnet (Friedman et al, 2010) and Extensions. (Yuan al, 2012); see e.g., the overview in Yurik."}, {"heading": "8. Discussion", "text": "To facilitate comprehensive machine learning, we have developed, analyzed and evaluated a universal framework for communication-efficient primary-dual optimization in the distributed environment. Our framework, CoCoA, takes a unique approach, using duality to derive sub-problems that need to be solved in parallel for each machine. These sub-problems are closely aligned with the global problem of interest that allows easy reuse of state-of-the-art stand-alone solutions in the distributed environment. Furthermore, we reduce and adjust the need for communication to the existing system, which helps manage the communication bottleneck in the distributed environment. We analyzed the impact of local solution solutions on their local parameters and determined global primary-dual convergence rates for our framework, which are agnostically aligned to the specifications of local distribution technology."}, {"heading": "Acknowledgments", "text": "We thank Michael P. Friedlander, Jakub Kone\u010dn\u00fd and Peter Richt\u00e1rik for their help and fruitful discussions."}, {"heading": "Appendix A. Convex Conjugates", "text": "The convex conjugate of a function f: Rd \u2192 R is defined as asf * (v): = max u * Rd > u \u2212 f (u). (28) Below we list several useful properties of conjugates (see e.g. Boyd and Vandenberghe, 2004, Section 3.3.2): \u2022 Double conjugate: (f *) \u0445 = f if f closed and convex. \u2022 Value scaling: (for \u03b1 > 0) f (v) = \u03b1g (v) \u21d2 f * (w) = \u03b1g * (w / \u03b1). \u2022 Argumentation Scaling: (for \u03b1 6 = 0) f (v) = g (\u03b1v). \u2022 Conjugate of a divisible sum: f (v) \u00b2 i \u00b2 (vi) \u00b2 f \u00b2 (w) = i \u00b2 (w) f (v) = g (w)."}, {"heading": "Appendix B. Proofs of Primal-Dual Relationships", "text": "In the following subsections we present derivations of the primary-dual ratio of the general objectives (A) and (B) (A) and then show how to derive the conjugation of the modified L1 standard (B) as an example of the bounded-support modification (A) introduced in Section 4.B.1. The relationship between our original formula (A) and its dual formulation (B) is standard in convex analysis and is a specific case of the concept of fennel duality. If we use the combination with the linear map A, as in our case, the relationship between fennel and rockafellar duality is called fennel-rockafellar duality, see e.g. Borwein and Zhu (2005, Theorem 4.4.2) or Bauschke and Combettes (2011, Proposition 15.18). To complete this correspondence, we illustrate a self-contained derivative of duality.Beginning with the original formulation (A), we present an auxilivector problem (RVector)."}, {"heading": "Appendix C. Comparison to ADMM", "text": "Here we derive the comparison of ADMM and CoCoCoA, discussed in Section 3.6, following the line of reasoning in Yang (2013). For the ADMM consensus, the target (B) is dissected using the following parameters. \u2212 To solve this problem, we construct the augmented Lagrangian: Lurch (w1,., wk, u1,., uk, w): K \u00b2 k = 1 x > Pk g \u00b2 (w) + f \u00b2 (w)., we construct the augmented Lagrangian: L \u00b2 (w1,., wk \u00b2, u1,., uk, w): K \u00b2 k = 1 x \u00b2 Pk \u00b2 (w)."}, {"heading": "Appendix D. Convergence Proofs", "text": "In this section, we provide evidence for our most important convergence results. The arguments follow the reasoning in Ma et al. (2015b, a), but where we have generalized them to be directly applicable to (A). We provide full details on Lemma 1 as proof of the concept, but omit details in later evidence where the theory can be deduced () on the basis of the arguments in Ma et al. (\u2212 v) or earlier work by Shalev-Shwartz and Zhang (2013a), and instead outline the sections of evidence where the theory differs. D.1 Approximation of OA () by the Local Subproblems. \u2212 k (\u2212) Our first problem in the Overall Evidence of Convergence helps progress towards the global objective OA (\u00b7). Lemma '1. For all dual variables."}], "references": [{"title": "Scalable training of L1-regularized log-linear models", "author": ["G. Andrew", "J. Gao"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Andrew and Gao.,? \\Q2007\\E", "shortCiteRegEx": "Andrew and Gao.", "year": 2007}, {"title": "Communication complexity of distributed convex learning and optimization", "author": ["Y. Arjevani", "O. Shamir"], "venue": "In Neural Information Processing Systems,", "citeRegEx": "Arjevani and Shamir.,? \\Q2015\\E", "shortCiteRegEx": "Arjevani and Shamir.", "year": 2015}, {"title": "Distributed learning, communication complexity and privacy", "author": ["M.-F. Balcan", "A. Blum", "S. Fine", "Y. Mansour"], "venue": "In Conference on Learning Theory,", "citeRegEx": "Balcan et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Balcan et al\\.", "year": 2012}, {"title": "Convex Analysis and Monotone Operator Theory in Hilbert Spaces", "author": ["H.H. Bauschke", "P.L. Combettes"], "venue": "Springer Science & Business Media,", "citeRegEx": "Bauschke and Combettes.,? \\Q2011\\E", "shortCiteRegEx": "Bauschke and Combettes.", "year": 2011}, {"title": "Parallel and Distributed Computation: Numerical Methods", "author": ["D.P. Bersekas", "J.N. Tsitsiklis"], "venue": null, "citeRegEx": "Bersekas and Tsitsiklis.,? \\Q1989\\E", "shortCiteRegEx": "Bersekas and Tsitsiklis.", "year": 1989}, {"title": "Parallel coordinate descent Newton method for efficient `1-regularized minimization", "author": ["Y. Bian", "X. Li", "Y. Liu", "M.-H. Yang"], "venue": "arXiv.org,", "citeRegEx": "Bian et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bian et al\\.", "year": 2013}, {"title": "Convex Optimization", "author": ["S. Boyd", "L. Vandenberghe"], "venue": null, "citeRegEx": "Boyd and Vandenberghe.,? \\Q2004\\E", "shortCiteRegEx": "Boyd and Vandenberghe.", "year": 2004}, {"title": "Distributed optimization and statistical learning via the alternating direction method of multipliers", "author": ["S. Boyd", "N. Parikh", "E. Chu", "B. Peleato", "J. Eckstein"], "venue": "Foundations and Trends in Machine Learning,", "citeRegEx": "Boyd et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Boyd et al\\.", "year": 2010}, {"title": "Parallel coordinate descent for l1-regularized loss minimization", "author": ["J.K. Bradley", "A. Kyrola", "D. Bickson", "C. Guestrin"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Bradley et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Bradley et al\\.", "year": 2011}, {"title": "Estimation, optimization, and parallelism when data is sparse", "author": ["J. Duchi", "M.I. Jordan", "B. McMahan"], "venue": "Neural Information Processing Systems,", "citeRegEx": "Duchi et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Duchi et al\\.", "year": 2013}, {"title": "Primal-dual rates and certificates", "author": ["C. D\u00fcnner", "S. Forte", "M. Tak\u00e1\u010d", "M. Jaggi"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "D\u00fcnner et al\\.,? \\Q2016\\E", "shortCiteRegEx": "D\u00fcnner et al\\.", "year": 2016}, {"title": "LIBLINEAR: A library for large linear classification", "author": ["R.-E. Fan", "K.-W. Chang", "C.-J. Hsieh", "X.-R. Wang", "C.-J. Lin"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Fan et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Fan et al\\.", "year": 2008}, {"title": "Accelerated, parallel, and proximal coordinate descent", "author": ["O. Fercoq", "P. Richt\u00e1rik"], "venue": "SIAM Journal on Optimization,", "citeRegEx": "Fercoq and Richt\u00e1rik.,? \\Q2015\\E", "shortCiteRegEx": "Fercoq and Richt\u00e1rik.", "year": 2015}, {"title": "Distributed Optimization for Non-Strongly Convex Regularizers", "author": ["S. Forte"], "venue": "Master\u2019s thesis, ETH Zu\u0308rich,", "citeRegEx": "Forte.,? \\Q2015\\E", "shortCiteRegEx": "Forte.", "year": 2015}, {"title": "Regularization paths for generalized linear models via coordinate descent", "author": ["J. Friedman", "T. Hastie", "R. Tibshirani"], "venue": "Journal of Statistical Software,", "citeRegEx": "Friedman et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Friedman et al\\.", "year": 2010}, {"title": "DUAL-LOCO: Distributing statistical estimation using random projections", "author": ["C. Heinze", "B. McWilliams", "N. Meinshausen"], "venue": "In International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "Heinze et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Heinze et al\\.", "year": 2016}, {"title": "Fundamentals of convex analysis", "author": ["J.-B. Hiriart-Urruty", "C. Lemar\u00e9chal"], "venue": "Springer\u2013Verlag, Berlin,", "citeRegEx": "Hiriart.Urruty and Lemar\u00e9chal.,? \\Q2001\\E", "shortCiteRegEx": "Hiriart.Urruty and Lemar\u00e9chal.", "year": 2001}, {"title": "Communication-efficient distributed dual coordinate ascent", "author": ["M. Jaggi", "V. Smith", "M. Tak\u00e1\u010d", "J. Terhorst", "S. Krishnan", "T. Hofmann", "M.I. Jordan"], "venue": "In Neural Information Processing Systems,", "citeRegEx": "Jaggi et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Jaggi et al\\.", "year": 2014}, {"title": "Blitz: A principled meta-algorithm for scaling sparse optimization", "author": ["T. Johnson", "C. Guestrin"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Johnson and Guestrin.,? \\Q2015\\E", "shortCiteRegEx": "Johnson and Guestrin.", "year": 2015}, {"title": "Linear convergence of gradient and proximal-gradient methods under the Polyak-\u0142ojasiewicz condition", "author": ["H. Karimi", "J. Nutini", "M. Schmidt"], "venue": "In European Conference on Machine Learning,", "citeRegEx": "Karimi et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Karimi et al\\.", "year": 2016}, {"title": "Distributed box-constrained quadratic optimization for dual linear SVM", "author": ["C.-P. Lee", "D. Roth"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Lee and Roth.,? \\Q2015\\E", "shortCiteRegEx": "Lee and Roth.", "year": 2015}, {"title": "On the complexity analysis of randomized block-coordinate descent methods", "author": ["Z. Lu", "L. Xiao"], "venue": null, "citeRegEx": "Lu and Xiao.,? \\Q2013\\E", "shortCiteRegEx": "Lu and Xiao.", "year": 2013}, {"title": "Distributed optimization with arbitrary local solvers. arXiv.org, 2015a", "author": ["C. Ma", "J. Kone\u010dn\u00fd", "M. Jaggi", "V. Smith", "M. Jordan", "P. Richt\u00e1rik", "M. Tak\u00e1\u010d"], "venue": null, "citeRegEx": "Ma et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ma et al\\.", "year": 2015}, {"title": "Adding vs. averaging in distributed primal-dual optimization", "author": ["C. Ma", "V. Smith", "M. Jaggi", "M.I. Jordan", "P. Richt\u00e1rik", "M. Tak\u00e1\u010d"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Ma et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ma et al\\.", "year": 2015}, {"title": "Linear convergence of the randomized feasible descent method under the weak strong convexity assumption. arXiv.org, 2015c", "author": ["C. Ma", "R. Tappenden", "M. Tak\u00e1\u010d"], "venue": null, "citeRegEx": "Ma et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ma et al\\.", "year": 2015}, {"title": "A distributed block coordinate descent method for training l1 regularized linear classifiers", "author": ["D. Mahajan", "S.S. Keerthi", "S. Sundararajan"], "venue": "arXiv.org,", "citeRegEx": "Mahajan et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Mahajan et al\\.", "year": 2014}, {"title": "Distributed Block Coordinate Descent for Minimizing Partially Separable Functions, volume", "author": ["J. Marecek", "P. Richt\u00e1rik", "M. Tak\u00e1\u010d"], "venue": "Proceedings in Mathematics & Statistics. Springer International Publishing,", "citeRegEx": "Marecek et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Marecek et al\\.", "year": 2015}, {"title": "LOCO: Distributing ridge regression with random projections", "author": ["B. McWilliams", "C. Heinze", "N. Meinshausen", "G. Krummenacher", "H.P. Vanchinathan"], "venue": null, "citeRegEx": "McWilliams et al\\.,? \\Q2014\\E", "shortCiteRegEx": "McWilliams et al\\.", "year": 2014}, {"title": "MLlib: Machine learning in apache spark", "author": ["X. Meng", "J. Bradley", "B. Yavuz", "E. Sparks", "S. Venkataraman", "D. Liu", "J. Freeman", "D. Tsai", "M. Amde", "S. Owen", "D. Xin", "R. Xin", "M.J. Franklin", "R. Zadeh", "M. Zaharia", "A. Talwalkar"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Meng et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Meng et al\\.", "year": 2016}, {"title": "Linear convergence of first order methods under weak nondegeneracy assumptions for convex programming", "author": ["I. Necoara"], "venue": null, "citeRegEx": "Necoara.,? \\Q2015\\E", "shortCiteRegEx": "Necoara.", "year": 2015}, {"title": "Distributed dual gradient methods and error bound conditions", "author": ["I. Necoara", "V. Nedelcu"], "venue": "arXiv.org,", "citeRegEx": "Necoara and Nedelcu.,? \\Q2014\\E", "shortCiteRegEx": "Necoara and Nedelcu.", "year": 2014}, {"title": "Smooth minimization of non-smooth functions", "author": ["Y. Nesterov"], "venue": "Mathematical Programming,", "citeRegEx": "Nesterov.,? \\Q2005\\E", "shortCiteRegEx": "Nesterov.", "year": 2005}, {"title": "Hogwild!: A lock-free approach to parallelizing stochastic gradient descent", "author": ["F. Niu", "B. Recht", "C. R\u00e9", "S.J. Wright"], "venue": "In Neural Information Processing Systems,", "citeRegEx": "Niu et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Niu et al\\.", "year": 2011}, {"title": "Solving large scale linear SVM with distributed block minimization", "author": ["D. Pechyony", "L. Shen", "R. Jones"], "venue": "In International Conference on Information and Knowledge Management,", "citeRegEx": "Pechyony et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Pechyony et al\\.", "year": 2011}, {"title": "Quartz: Randomized dual coordinate ascent with arbitrary sampling", "author": ["Z. Qu", "P. Richt\u00e1rik", "T. Zhang"], "venue": "In Neural Information Processing Systems,", "citeRegEx": "Qu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Qu et al\\.", "year": 2015}, {"title": "SDNA: Stochastic dual Newton ascent for empirical risk minimization", "author": ["Z. Qu", "P. Richt\u00e1rik", "M. Tak\u00e1\u010d", "O. Fercoq"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Qu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Qu et al\\.", "year": 2016}, {"title": "Distributed coordinate descent method for learning with big data", "author": ["P. Richt\u00e1rik", "M. Tak\u00e1\u010d"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Richt\u00e1rik and Tak\u00e1\u010d.,? \\Q2016\\E", "shortCiteRegEx": "Richt\u00e1rik and Tak\u00e1\u010d.", "year": 2016}, {"title": "Convex Analysis", "author": ["R.T. Rockafellar"], "venue": null, "citeRegEx": "Rockafellar.,? \\Q1997\\E", "shortCiteRegEx": "Rockafellar.", "year": 1997}, {"title": "Stochastic methods for l1-regularized loss minimization", "author": ["S. Shalev-Shwartz", "A. Tewari"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Shalev.Shwartz and Tewari.,? \\Q2011\\E", "shortCiteRegEx": "Shalev.Shwartz and Tewari.", "year": 2011}, {"title": "Stochastic dual coordinate ascent methods for regularized loss minimization", "author": ["S. Shalev-Shwartz", "T. Zhang"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Shalev.Shwartz and Zhang.,? \\Q2013\\E", "shortCiteRegEx": "Shalev.Shwartz and Zhang.", "year": 2013}, {"title": "Accelerated mini-batch stochastic dual coordinate ascent", "author": ["S. Shalev-Shwartz", "T. Zhang"], "venue": "In Neural Information Processing Systems,", "citeRegEx": "Shalev.Shwartz and Zhang.,? \\Q2013\\E", "shortCiteRegEx": "Shalev.Shwartz and Zhang.", "year": 2013}, {"title": "Accelerated proximal stochastic dual coordinate ascent for regularized loss minimization", "author": ["S. Shalev-Shwartz", "T. Zhang"], "venue": "Mathematical Programming, Series", "citeRegEx": "Shalev.Shwartz and Zhang.,? \\Q2014\\E", "shortCiteRegEx": "Shalev.Shwartz and Zhang.", "year": 2014}, {"title": "Communication-efficient distributed optimization using an approximate newton-type method", "author": ["O. Shamir", "N. Srebro", "T. Zhang"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Shamir et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Shamir et al\\.", "year": 2014}, {"title": "L1-Regularized Distributed Optimization: A Communication-Efficient Primal-Dual Framework", "author": ["V. Smith", "S. Forte", "M.I. Jordan", "M. Jaggi"], "venue": null, "citeRegEx": "Smith et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Smith et al\\.", "year": 2015}, {"title": "Mini-batch primal and dual methods for SVMs", "author": ["M. Tak\u00e1\u010d", "A. Bijral", "P. Richt\u00e1rik", "N. Srebro"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Tak\u00e1\u010d et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Tak\u00e1\u010d et al\\.", "year": 2013}, {"title": "Distributed mini-batch SDCA", "author": ["M. Tak\u00e1\u010d", "P. Richt\u00e1rik", "N. Srebro"], "venue": "arXiv.org,", "citeRegEx": "Tak\u00e1\u010d et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Tak\u00e1\u010d et al\\.", "year": 2015}, {"title": "On the complexity of parallel coordinate descent", "author": ["R. Tappenden", "M. Tak\u00e1\u010d", "P. Richt\u00e1rik"], "venue": null, "citeRegEx": "Tappenden et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Tappenden et al\\.", "year": 2015}, {"title": "Distributed coordinate descent for l1-regularized logistic regression", "author": ["I. Trofimov", "A. Genkin"], "venue": "arXiv.org,", "citeRegEx": "Trofimov and Genkin.,? \\Q2014\\E", "shortCiteRegEx": "Trofimov and Genkin.", "year": 2014}, {"title": "Iteration complexity of feasible descent methods for convex optimization", "author": ["P.-W. Wang", "C.-J. Lin"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Wang and Lin.,? \\Q2014\\E", "shortCiteRegEx": "Wang and Lin.", "year": 2014}, {"title": "Coordinate descent algorithms", "author": ["S.J. Wright"], "venue": "Mathematical Programming,", "citeRegEx": "Wright.,? \\Q2015\\E", "shortCiteRegEx": "Wright.", "year": 2015}, {"title": "Trading computation for communication: Distributed stochastic dual coordinate ascent", "author": ["T. Yang"], "venue": "In Neural Information Processing Systems,", "citeRegEx": "Yang.,? \\Q2013\\E", "shortCiteRegEx": "Yang.", "year": 2013}, {"title": "On theoretical analysis of distributed stochastic dual coordinate ascent", "author": ["T. Yang", "S. Zhu", "R. Jin", "Y. Lin"], "venue": null, "citeRegEx": "Yang et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2013}, {"title": "A dual augmented block minimization framework for learning with limited memory", "author": ["I.E.-H. Yen", "S.-W. Lin", "S.-D. Lin"], "venue": "In Neural Information Processing Systems,", "citeRegEx": "Yen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Yen et al\\.", "year": 2015}, {"title": "Large linear classification when data cannot fit in memory", "author": ["H.-F. Yu", "C.-J. Hsieh", "K.-W. Chang", "C.-J. Lin"], "venue": "ACM Transactions on Knowledge Discovery from Data,", "citeRegEx": "Yu et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Yu et al\\.", "year": 2012}, {"title": "A quasi-Newton approach to nonsmooth convex optimization problems in machine learning", "author": ["J. Yu", "S. Vishwanathan", "S. G\u00fcnter", "N.N. Schraudolph"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Yu et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Yu et al\\.", "year": 2010}, {"title": "A comparison of optimization methods and software for large-scale l1-regularized linear classification", "author": ["G.-X. Yuan", "K.-W. Chang", "C.-J. Hsieh", "C.-J. Lin"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Yuan et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Yuan et al\\.", "year": 2010}, {"title": "An improved GLMNET for L1-regularized logistic regression", "author": ["G.-X. Yuan", "C.-H. Ho", "C.-J. Lin"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Yuan et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Yuan et al\\.", "year": 2012}, {"title": "Efficient distributed linear classification algorithms via the alternating direction method of multipliers", "author": ["C. Zhang", "H. Lee", "K.G. Shin"], "venue": "In Artificial Intelligence and Statistics Conference,", "citeRegEx": "Zhang et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2012}, {"title": "Stochastic primal-dual coordinate method for regularized empirical risk minimization", "author": ["Y. Zhang", "X. Lin"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Zhang and Lin.,? \\Q2015\\E", "shortCiteRegEx": "Zhang and Lin.", "year": 2015}, {"title": "Communication-efficient algorithms for statistical optimization", "author": ["Y. Zhang", "J.C. Duchi", "M.J. Wainwright"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Zhang et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2013}, {"title": "Parallelized stochastic gradient descent", "author": ["M.A. Zinkevich", "M. Weimer", "A.J. Smola", "L. Li"], "venue": "Neural Information Processing Systems,", "citeRegEx": "Zinkevich et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Zinkevich et al\\.", "year": 2010}], "referenceMentions": [{"referenceID": 34, "context": "Although numerous distributed optimization methods have been proposed, the minibatch optimization approach has emerged as one of the most popular paradigms for tackling this communication-computation tradeoff (see, e.g., Tak\u00e1\u010d et al., 2013; Shalev-Shwartz and Zhang, 2013b; Qu et al., 2015; Richt\u00e1rik and Tak\u00e1\u010d, 2016).", "startOffset": 209, "endOffset": 317}, {"referenceID": 36, "context": "Although numerous distributed optimization methods have been proposed, the minibatch optimization approach has emerged as one of the most popular paradigms for tackling this communication-computation tradeoff (see, e.g., Tak\u00e1\u010d et al., 2013; Shalev-Shwartz and Zhang, 2013b; Qu et al., 2015; Richt\u00e1rik and Tak\u00e1\u010d, 2016).", "startOffset": 209, "endOffset": 317}, {"referenceID": 17, "context": "CoCoA-v1 (Jaggi et al., 2014) and CoCoA+ (Ma et al.", "startOffset": 9, "endOffset": 29}, {"referenceID": 13, "context": "Portions of this newer work appear in SF\u2019s Master\u2019s Thesis (Forte, 2015) and Smith et al.", "startOffset": 59, "endOffset": 72}, {"referenceID": 13, "context": "Portions of this newer work appear in SF\u2019s Master\u2019s Thesis (Forte, 2015) and Smith et al. (2015).", "startOffset": 60, "endOffset": 97}, {"referenceID": 42, "context": "Notably, in contrast to earlier work of Yang (2013); Jaggi et al.", "startOffset": 40, "endOffset": 52}, {"referenceID": 17, "context": "Notably, in contrast to earlier work of Yang (2013); Jaggi et al. (2014); Ma et al.", "startOffset": 53, "endOffset": 73}, {"referenceID": 17, "context": "Notably, in contrast to earlier work of Yang (2013); Jaggi et al. (2014); Ma et al. (2015a); and Ma et al.", "startOffset": 53, "endOffset": 92}, {"referenceID": 17, "context": "Notably, in contrast to earlier work of Yang (2013); Jaggi et al. (2014); Ma et al. (2015a); and Ma et al. (2015b), our generalized, cohesive framework: (1) specifically incorporates difficult cases of L1 regularization and other non-strongly convex regularizers; (2) allows for the flexibility of distributing the data by either feature or training point; and (3) can be run on either a primal or dual formulation, which we show to have significant theoretical and practical implications.", "startOffset": 53, "endOffset": 115}, {"referenceID": 17, "context": "Notably, in contrast to earlier work of Yang (2013); Jaggi et al. (2014); Ma et al. (2015a); and Ma et al. (2015b), our generalized, cohesive framework: (1) specifically incorporates difficult cases of L1 regularization and other non-strongly convex regularizers; (2) allows for the flexibility of distributing the data by either feature or training point; and (3) can be run on either a primal or dual formulation, which we show to have significant theoretical and practical implications. Flexible communication and local solvers. Two key advantages of the proposed framework are its communication efficiency and ability to employ off-the-shelf single-machine solvers internally. On real-world systems, the cost of communication versus computation can vary widely, and it is thus advantageous to permit a flexible amount of communication depending on the setting at hand. Our framework provides exactly such control. Moreover, we allow arbitrary solvers to be used on each machine, which permits the reuse of existing code and the benefits from multi-core or other optimizations therein. Primal-dual rates. We derive convergence rates for our framework, leveraging a novel approach in the analysis of primal-dual rates for non-strongly convex regularizers. The proposed technique is a significant improvement over simple smoothing techniques used in, e.g., Nesterov (2005); Shalev-Shwartz and Zhang (2014); and Zhang and Lin (2015) that enforce strong convexity by adding a small L2 term to the objective.", "startOffset": 53, "endOffset": 1374}, {"referenceID": 17, "context": "Notably, in contrast to earlier work of Yang (2013); Jaggi et al. (2014); Ma et al. (2015a); and Ma et al. (2015b), our generalized, cohesive framework: (1) specifically incorporates difficult cases of L1 regularization and other non-strongly convex regularizers; (2) allows for the flexibility of distributing the data by either feature or training point; and (3) can be run on either a primal or dual formulation, which we show to have significant theoretical and practical implications. Flexible communication and local solvers. Two key advantages of the proposed framework are its communication efficiency and ability to employ off-the-shelf single-machine solvers internally. On real-world systems, the cost of communication versus computation can vary widely, and it is thus advantageous to permit a flexible amount of communication depending on the setting at hand. Our framework provides exactly such control. Moreover, we allow arbitrary solvers to be used on each machine, which permits the reuse of existing code and the benefits from multi-core or other optimizations therein. Primal-dual rates. We derive convergence rates for our framework, leveraging a novel approach in the analysis of primal-dual rates for non-strongly convex regularizers. The proposed technique is a significant improvement over simple smoothing techniques used in, e.g., Nesterov (2005); Shalev-Shwartz and Zhang (2014); and Zhang and Lin (2015) that enforce strong convexity by adding a small L2 term to the objective.", "startOffset": 53, "endOffset": 1407}, {"referenceID": 17, "context": "Notably, in contrast to earlier work of Yang (2013); Jaggi et al. (2014); Ma et al. (2015a); and Ma et al. (2015b), our generalized, cohesive framework: (1) specifically incorporates difficult cases of L1 regularization and other non-strongly convex regularizers; (2) allows for the flexibility of distributing the data by either feature or training point; and (3) can be run on either a primal or dual formulation, which we show to have significant theoretical and practical implications. Flexible communication and local solvers. Two key advantages of the proposed framework are its communication efficiency and ability to employ off-the-shelf single-machine solvers internally. On real-world systems, the cost of communication versus computation can vary widely, and it is thus advantageous to permit a flexible amount of communication depending on the setting at hand. Our framework provides exactly such control. Moreover, we allow arbitrary solvers to be used on each machine, which permits the reuse of existing code and the benefits from multi-core or other optimizations therein. Primal-dual rates. We derive convergence rates for our framework, leveraging a novel approach in the analysis of primal-dual rates for non-strongly convex regularizers. The proposed technique is a significant improvement over simple smoothing techniques used in, e.g., Nesterov (2005); Shalev-Shwartz and Zhang (2014); and Zhang and Lin (2015) that enforce strong convexity by adding a small L2 term to the objective.", "startOffset": 53, "endOffset": 1433}, {"referenceID": 46, "context": "It is also useful as an analysis tool, helping us to present a cohesive framework and relate this work to the prior work of Yang (2013); Jaggi et al.", "startOffset": 124, "endOffset": 136}, {"referenceID": 17, "context": "It is also useful as an analysis tool, helping us to present a cohesive framework and relate this work to the prior work of Yang (2013); Jaggi et al. (2014); and Ma et al.", "startOffset": 137, "endOffset": 157}, {"referenceID": 46, "context": "This setting was not covered in earlier work of Yang (2013); Jaggi et al.", "startOffset": 48, "endOffset": 60}, {"referenceID": 17, "context": "This setting was not covered in earlier work of Yang (2013); Jaggi et al. (2014); Ma et al.", "startOffset": 61, "endOffset": 81}, {"referenceID": 17, "context": "This setting was not covered in earlier work of Yang (2013); Jaggi et al. (2014); Ma et al. (2015b); and Ma et al.", "startOffset": 61, "endOffset": 100}, {"referenceID": 17, "context": "This setting was not covered in earlier work of Yang (2013); Jaggi et al. (2014); Ma et al. (2015b); and Ma et al. (2015a), and we discuss it in detail in Section 4, as additional machinery must be introduced to develop primal-dual rates for this setting.", "startOffset": 61, "endOffset": 123}, {"referenceID": 4, "context": "From a coordinate-wise perspective, two approaches for updating the parameter vector \u03b1 in an iterative fashion include the Jacobi method, in which updates made to coordinates of \u03b1 do not take into account the most recent updates to the other coordinates, and Gauss-Seidel, in which the most recent information is used (Bersekas and Tsitsiklis, 1989).", "startOffset": 318, "endOffset": 349}, {"referenceID": 44, "context": "As the size of the mini-batch grows, this can slow them down in terms of overall runtime, and can even lead to divergence in practice (Tak\u00e1\u010d et al., 2013; Tak\u00e1\u010d et al., 2015; Richt\u00e1rik and Tak\u00e1\u010d, 2016; Marecek et al., 2015).", "startOffset": 134, "endOffset": 223}, {"referenceID": 45, "context": "As the size of the mini-batch grows, this can slow them down in terms of overall runtime, and can even lead to divergence in practice (Tak\u00e1\u010d et al., 2013; Tak\u00e1\u010d et al., 2015; Richt\u00e1rik and Tak\u00e1\u010d, 2016; Marecek et al., 2015).", "startOffset": 134, "endOffset": 223}, {"referenceID": 36, "context": "As the size of the mini-batch grows, this can slow them down in terms of overall runtime, and can even lead to divergence in practice (Tak\u00e1\u010d et al., 2013; Tak\u00e1\u010d et al., 2015; Richt\u00e1rik and Tak\u00e1\u010d, 2016; Marecek et al., 2015).", "startOffset": 134, "endOffset": 223}, {"referenceID": 26, "context": "As the size of the mini-batch grows, this can slow them down in terms of overall runtime, and can even lead to divergence in practice (Tak\u00e1\u010d et al., 2013; Tak\u00e1\u010d et al., 2015; Richt\u00e1rik and Tak\u00e1\u010d, 2016; Marecek et al., 2015).", "startOffset": 134, "endOffset": 223}, {"referenceID": 7, "context": "6 Comparison to ADMM Finally, in this subsection we provide a direct comparison between CoCoA and ADMM (Boyd et al., 2010).", "startOffset": 103, "endOffset": 122}, {"referenceID": 41, "context": "To address this problem, existing approaches typically use a simple smoothing technique (as in Nesterov, 2005; Shalev-Shwartz and Zhang, 2014): by adding a small amount of L2 to the objective gi, the functions gi become strongly convex.", "startOffset": 88, "endOffset": 142}, {"referenceID": 10, "context": "See also D\u00fcnner et al. (2016) for a follow-up discussion of this technique in the non-distributed case.", "startOffset": 9, "endOffset": 30}, {"referenceID": 17, "context": "The earlier work of CoCoA-v1 (Jaggi et al., 2014) did not provide rates for L-Lipschitz `i losses.", "startOffset": 29, "endOffset": 49}, {"referenceID": 17, "context": "5 Recovering Earlier Work as a Special Case As a special case, the proposed framework and rates directly apply to L2-regularized lossminimization problems, including those presented in the earlier work of Jaggi et al. (2014) and Ma et al.", "startOffset": 205, "endOffset": 225}, {"referenceID": 17, "context": "5 Recovering Earlier Work as a Special Case As a special case, the proposed framework and rates directly apply to L2-regularized lossminimization problems, including those presented in the earlier work of Jaggi et al. (2014) and Ma et al. (2015b). Remark 3.", "startOffset": 205, "endOffset": 247}, {"referenceID": 9, "context": ", D\u00fcnner et al. (2016) or Boyd and Vandenberghe (2004, Example 3.", "startOffset": 2, "endOffset": 23}, {"referenceID": 14, "context": "For the L1 examples discussed, existing fast L1-solvers for the single-machine case, such as glmnet variants (Friedman et al., 2010) or blitz (Johnson and Guestrin, 2015) can be directly applied to each local subproblem G\u03c3 k ( \u00b7 ;v,\u03b1[k]) within Algorithm 1.", "startOffset": 109, "endOffset": 132}, {"referenceID": 18, "context": ", 2010) or blitz (Johnson and Guestrin, 2015) can be directly applied to each local subproblem G\u03c3 k ( \u00b7 ;v,\u03b1[k]) within Algorithm 1.", "startOffset": 17, "endOffset": 45}, {"referenceID": 20, "context": "We discuss some specific examples of local solvers below, and point the reader to Ma et al. (2015a) for an empirical exploration of these choices.", "startOffset": 82, "endOffset": 100}, {"referenceID": 49, "context": "This algorithm and its variants are increasingly used in practice (Wright, 2015), and extensions such as accelerated and parallel versions can directly be applied (Shalev-Shwartz and Zhang, 2014; Fan et al.", "startOffset": 66, "endOffset": 80}, {"referenceID": 41, "context": "This algorithm and its variants are increasingly used in practice (Wright, 2015), and extensions such as accelerated and parallel versions can directly be applied (Shalev-Shwartz and Zhang, 2014; Fan et al., 2008) in our framework.", "startOffset": 163, "endOffset": 213}, {"referenceID": 11, "context": "This algorithm and its variants are increasingly used in practice (Wright, 2015), and extensions such as accelerated and parallel versions can directly be applied (Shalev-Shwartz and Zhang, 2014; Fan et al., 2008) in our framework.", "startOffset": 163, "endOffset": 213}, {"referenceID": 30, "context": ", by using error bound conditions (Necoara and Nedelcu, 2014; Wang and Lin, 2014), weak strong convexity conditions (Ma et al.", "startOffset": 34, "endOffset": 81}, {"referenceID": 48, "context": ", by using error bound conditions (Necoara and Nedelcu, 2014; Wang and Lin, 2014), weak strong convexity conditions (Ma et al.", "startOffset": 34, "endOffset": 81}, {"referenceID": 29, "context": ", by using error bound conditions (Necoara and Nedelcu, 2014; Wang and Lin, 2014), weak strong convexity conditions (Ma et al., 2015c; Necoara, 2015) or by considering Polyak-\u0141ojasiewicz conditions (Karimi et al.", "startOffset": 116, "endOffset": 149}, {"referenceID": 19, "context": ", 2015c; Necoara, 2015) or by considering Polyak-\u0141ojasiewicz conditions (Karimi et al., 2016).", "startOffset": 72, "endOffset": 93}, {"referenceID": 19, "context": "example, for randomized coordinate descent (as part of glmnet), Lu and Xiao (2013, Theorem 1) gives a O(1/t) approximation quality for any separable regularizer, including L1 and elastic net; see also Tappenden et al. (2015) and Shalev-Shwartz and Tewari (2011).", "startOffset": 64, "endOffset": 225}, {"referenceID": 19, "context": "example, for randomized coordinate descent (as part of glmnet), Lu and Xiao (2013, Theorem 1) gives a O(1/t) approximation quality for any separable regularizer, including L1 and elastic net; see also Tappenden et al. (2015) and Shalev-Shwartz and Tewari (2011). In the dual setting (Algorithm 3) for the discussed examples, the losses are applied only to local variables \u03b1[k], and the regularizer is approximated via a quadratic term.", "startOffset": 64, "endOffset": 262}, {"referenceID": 11, "context": "This algorithm and its variants are increasingly used in practice (Wright, 2015), and extensions such as accelerated and parallel versions can directly be applied (Shalev-Shwartz and Zhang, 2014; Fan et al., 2008) in our framework. For non-smooth losses such as SVMs, the analysis of Shalev-Shwartz and Zhang (2013a) provides a O(1/t) rate, and for smooth losses, a faster linear rate.", "startOffset": 196, "endOffset": 317}, {"referenceID": 28, "context": "0) (Meng et al., 2016).", "startOffset": 3, "endOffset": 22}, {"referenceID": 8, "context": "A comparison with Shotgun (Bradley et al., 2011), a popular method for solving L1-regularized problems in the multicore environment, is provided as an extreme case to highlight the detrimental effects of frequent communication in the distributed environment.", "startOffset": 26, "endOffset": 48}, {"referenceID": 31, "context": "To get around this requirement, previous work has suggested implementing the Nesterov smoothing technique used in, e.g., Shalev-Shwartz and Zhang (2014); Zhang and Lin (2015) \u2014 adding a small amount of strong convexity \u03b4\u2016\u03b1\u20162 to the objective for lasso regression.", "startOffset": 77, "endOffset": 153}, {"referenceID": 31, "context": "To get around this requirement, previous work has suggested implementing the Nesterov smoothing technique used in, e.g., Shalev-Shwartz and Zhang (2014); Zhang and Lin (2015) \u2014 adding a small amount of strong convexity \u03b4\u2016\u03b1\u20162 to the objective for lasso regression.", "startOffset": 77, "endOffset": 175}, {"referenceID": 7, "context": "Alternating Direction Method of Multipliers (ADMM) (Boyd et al., 2010) is a popular method that lends itself naturally to the distributed environment.", "startOffset": 51, "endOffset": 70}, {"referenceID": 28, "context": "0 (Meng et al., 2016).", "startOffset": 2, "endOffset": 21}, {"referenceID": 7, "context": "Alternating Direction Method of Multipliers (ADMM) (Boyd et al., 2010) is a popular method that lends itself naturally to the distributed environment. For lasso regression, implementing ADMM for the problems of interest requires solving a large linear system Cx = d on each machine, where C \u2208 Rn\u00d7n with n scaling beyond 107 for the datasets in Table 5, and with C being possibly dense. It is prohibitively slow to solve this directly on each machine, and we therefore employ the iterative method of conjugate gradient with early stopping (see, e.g., Boyd et al., 2010, Section 4.3). For SVM classification, we use stochastic dual coordinate ascent as an internal optimizer, which is shown in Zhang et al. (2012) to have superior performance.", "startOffset": 52, "endOffset": 712}, {"referenceID": 38, "context": "Mini-batch CD (for lasso) and SDCA (for SVM) aim to improve mini-batch SGD by employing coordinate descent, which has theoretical and practical justifications (Shalev-Shwartz and Tewari, 2011; Tak\u00e1\u010d et al., 2015; Fercoq and Richt\u00e1rik, 2015; Tappenden et al., 2015; Tak\u00e1\u010d et al., 2013).", "startOffset": 159, "endOffset": 284}, {"referenceID": 45, "context": "Mini-batch CD (for lasso) and SDCA (for SVM) aim to improve mini-batch SGD by employing coordinate descent, which has theoretical and practical justifications (Shalev-Shwartz and Tewari, 2011; Tak\u00e1\u010d et al., 2015; Fercoq and Richt\u00e1rik, 2015; Tappenden et al., 2015; Tak\u00e1\u010d et al., 2013).", "startOffset": 159, "endOffset": 284}, {"referenceID": 12, "context": "Mini-batch CD (for lasso) and SDCA (for SVM) aim to improve mini-batch SGD by employing coordinate descent, which has theoretical and practical justifications (Shalev-Shwartz and Tewari, 2011; Tak\u00e1\u010d et al., 2015; Fercoq and Richt\u00e1rik, 2015; Tappenden et al., 2015; Tak\u00e1\u010d et al., 2013).", "startOffset": 159, "endOffset": 284}, {"referenceID": 46, "context": "Mini-batch CD (for lasso) and SDCA (for SVM) aim to improve mini-batch SGD by employing coordinate descent, which has theoretical and practical justifications (Shalev-Shwartz and Tewari, 2011; Tak\u00e1\u010d et al., 2015; Fercoq and Richt\u00e1rik, 2015; Tappenden et al., 2015; Tak\u00e1\u010d et al., 2013).", "startOffset": 159, "endOffset": 284}, {"referenceID": 44, "context": "Mini-batch CD (for lasso) and SDCA (for SVM) aim to improve mini-batch SGD by employing coordinate descent, which has theoretical and practical justifications (Shalev-Shwartz and Tewari, 2011; Tak\u00e1\u010d et al., 2015; Fercoq and Richt\u00e1rik, 2015; Tappenden et al., 2015; Tak\u00e1\u010d et al., 2013).", "startOffset": 159, "endOffset": 284}, {"referenceID": 8, "context": "For the case of lasso regression, we implement Shotgun (Bradley et al., 2011), which is a popular method for parallel optimization.", "startOffset": 55, "endOffset": 77}, {"referenceID": 54, "context": "OWN-QN (Yu et al., 2010) is a quasi-Newton method optimized in Spark\u2019s spark.", "startOffset": 7, "endOffset": 24}, {"referenceID": 28, "context": "ml package (Meng et al., 2016).", "startOffset": 11, "endOffset": 30}, {"referenceID": 14, "context": ", by using existing fast L1-solvers for the singlemachine case, such as glmnet variants (Friedman et al., 2010) or blitz (Johnson and Guestrin, 2015) or SVM solvers like liblinear (Fan et al.", "startOffset": 88, "endOffset": 111}, {"referenceID": 18, "context": ", 2010) or blitz (Johnson and Guestrin, 2015) or SVM solvers like liblinear (Fan et al.", "startOffset": 17, "endOffset": 45}, {"referenceID": 11, "context": ", 2010) or blitz (Johnson and Guestrin, 2015) or SVM solvers like liblinear (Fan et al., 2008).", "startOffset": 76, "endOffset": 94}, {"referenceID": 14, "context": "of-the-art, as in glmnet (Friedman et al., 2010) and extensions (Yuan et al.", "startOffset": 25, "endOffset": 48}, {"referenceID": 56, "context": ", 2010) and extensions (Yuan et al., 2012); see, e.", "startOffset": 23, "endOffset": 42}, {"referenceID": 41, "context": "However, primal-dual convergence rates for unmodified coordinate algorithms have to our knowledge only been obtained for strongly convex regularizers to date (Shalev-Shwartz and Zhang, 2014; Zhang and Lin, 2015).", "startOffset": 158, "endOffset": 211}, {"referenceID": 58, "context": "However, primal-dual convergence rates for unmodified coordinate algorithms have to our knowledge only been obtained for strongly convex regularizers to date (Shalev-Shwartz and Zhang, 2014; Zhang and Lin, 2015).", "startOffset": 158, "endOffset": 211}, {"referenceID": 14, "context": "In the single-coordinate update case, this is at the core of glmnet (Friedman et al., 2010; Yuan et al., 2010), and widely used in, e.", "startOffset": 68, "endOffset": 110}, {"referenceID": 55, "context": "In the single-coordinate update case, this is at the core of glmnet (Friedman et al., 2010; Yuan et al., 2010), and widely used in, e.", "startOffset": 68, "endOffset": 110}, {"referenceID": 38, "context": ", solvers based on the primal formulation of L1-regularized objectives (Shalev-Shwartz and Tewari, 2011; Yuan et al., 2012; Bian et al., 2013; Fercoq and Richt\u00e1rik, 2015; Tappenden et al., 2015).", "startOffset": 71, "endOffset": 194}, {"referenceID": 56, "context": ", solvers based on the primal formulation of L1-regularized objectives (Shalev-Shwartz and Tewari, 2011; Yuan et al., 2012; Bian et al., 2013; Fercoq and Richt\u00e1rik, 2015; Tappenden et al., 2015).", "startOffset": 71, "endOffset": 194}, {"referenceID": 5, "context": ", solvers based on the primal formulation of L1-regularized objectives (Shalev-Shwartz and Tewari, 2011; Yuan et al., 2012; Bian et al., 2013; Fercoq and Richt\u00e1rik, 2015; Tappenden et al., 2015).", "startOffset": 71, "endOffset": 194}, {"referenceID": 12, "context": ", solvers based on the primal formulation of L1-regularized objectives (Shalev-Shwartz and Tewari, 2011; Yuan et al., 2012; Bian et al., 2013; Fercoq and Richt\u00e1rik, 2015; Tappenden et al., 2015).", "startOffset": 71, "endOffset": 194}, {"referenceID": 46, "context": ", solvers based on the primal formulation of L1-regularized objectives (Shalev-Shwartz and Tewari, 2011; Yuan et al., 2012; Bian et al., 2013; Fercoq and Richt\u00e1rik, 2015; Tappenden et al., 2015).", "startOffset": 71, "endOffset": 194}, {"referenceID": 12, "context": "of-the-art, as in glmnet (Friedman et al., 2010) and extensions (Yuan et al., 2012); see, e.g., the overview in Yuan et al. (2010). However, primal-dual convergence rates for unmodified coordinate algorithms have to our knowledge only been obtained for strongly convex regularizers to date (Shalev-Shwartz and Zhang, 2014; Zhang and Lin, 2015).", "startOffset": 26, "endOffset": 131}, {"referenceID": 32, "context": "Several variants of SGD have been proposed for parallel computing, many of which build on the idea of asynchronous communication (Niu et al., 2011; Duchi et al., 2013).", "startOffset": 129, "endOffset": 167}, {"referenceID": 9, "context": "Several variants of SGD have been proposed for parallel computing, many of which build on the idea of asynchronous communication (Niu et al., 2011; Duchi et al., 2013).", "startOffset": 129, "endOffset": 167}, {"referenceID": 7, "context": "For the specific case of L1-regularized objectives, parallel coordinate descent (with and without using mini-batches) was proposed in Bradley et al. (2011) (Shotgun) and generalized in Bian et al.", "startOffset": 134, "endOffset": 156}, {"referenceID": 5, "context": "(2011) (Shotgun) and generalized in Bian et al. (2013), and is among the best performing solvers in the parallel setting.", "startOffset": 36, "endOffset": 55}, {"referenceID": 55, "context": "(2009); Zinkevich et al. (2010); Zhang et al.", "startOffset": 8, "endOffset": 32}, {"referenceID": 54, "context": "(2010); Zhang et al. (2013); McWilliams et al.", "startOffset": 8, "endOffset": 28}, {"referenceID": 26, "context": "(2013); McWilliams et al. (2014); and Heinze et al.", "startOffset": 8, "endOffset": 33}, {"referenceID": 15, "context": "(2014); and Heinze et al. (2016). These methods require additional assumptions on the partitioning of the data, which are usually not satisfied in practice if the data are distributed \u201cas is\u201d, i.", "startOffset": 12, "endOffset": 33}, {"referenceID": 15, "context": "(2014); and Heinze et al. (2016). These methods require additional assumptions on the partitioning of the data, which are usually not satisfied in practice if the data are distributed \u201cas is\u201d, i.e., if we do not have the opportunity to distribute the data in a specific way beforehand. Furthermore, some cannot guarantee convergence rates beyond what could be achieved if we ignored data residing on all but a single computer, as shown in Shamir et al. (2014). Additional relevant lower bounds on", "startOffset": 12, "endOffset": 460}, {"referenceID": 34, "context": "However, mini-batch versions of both SGD and coordinate descent (CD) (e.g., Tak\u00e1\u010d et al., 2013; Shalev-Shwartz and Zhang, 2013b; Qu et al., 2015; Richt\u00e1rik and Tak\u00e1\u010d, 2016) suffer from their convergence rate degrading towards the rate of batch gradient descent as the size of the mini-batch is increased.", "startOffset": 69, "endOffset": 172}, {"referenceID": 36, "context": "However, mini-batch versions of both SGD and coordinate descent (CD) (e.g., Tak\u00e1\u010d et al., 2013; Shalev-Shwartz and Zhang, 2013b; Qu et al., 2015; Richt\u00e1rik and Tak\u00e1\u010d, 2016) suffer from their convergence rate degrading towards the rate of batch gradient descent as the size of the mini-batch is increased.", "startOffset": 69, "endOffset": 172}, {"referenceID": 7, "context": "ADMM (Boyd et al., 2010), gradient descent, and quasi-Newton methods such as L-BFGS and are also often used in distributed environments because of their relatively low communication requirements.", "startOffset": 5, "endOffset": 24}, {"referenceID": 0, "context": "In Section 6, we include experimental comparisons with ADMM, gradient descent, and L-BFGS variants, including orthant-wise limited memory quasi-Newton (OWL-QN) for the L1 setting (Andrew and Gao, 2007).", "startOffset": 179, "endOffset": 201}, {"referenceID": 50, "context": "The practical variant of the DisDCA (Yang, 2013), called DisDCA-p, allows for additive updates in a similar manner to CoCoA, but is restricted to coordinate decent (CD) being the local solver, and was initially proposed without convergence guarantees.", "startOffset": 36, "endOffset": 48}, {"referenceID": 0, "context": "the minimum number of communication rounds necessary for a given approximation quality are presented in Balcan et al. (2012) and Arjevani and Shamir (2015).", "startOffset": 104, "endOffset": 125}, {"referenceID": 0, "context": "(2012) and Arjevani and Shamir (2015). Mini-batch methods.", "startOffset": 11, "endOffset": 38}, {"referenceID": 0, "context": "In Section 6, we include experimental comparisons with ADMM, gradient descent, and L-BFGS variants, including orthant-wise limited memory quasi-Newton (OWL-QN) for the L1 setting (Andrew and Gao, 2007). Finally, we note that while the convergence rates provided for CoCoA mirror the convergence class of classical batch gradient methods in terms of the number of outer rounds, existing batch gradient methods come with a weaker theory, as they do not allow general inexactness \u0398 for the local subproblem (10). In contrast, our convergence rates incorporate this approximation directly, and, moreover, hold for arbitrary local solvers of much cheaper cost than batch methods (where in each round, every machine has to process exactly a full pass through the local data). This makes CoCoA more flexible in the distributed setting, as it can adapt to varied communication costs on real systems. We have seen in Section 6 that this flexibility results in significant performance gains over the competing methods. Distributed solvers. By making use of the primal-dual structure in the line of work of Yu et al. (2012); Pechyony et al.", "startOffset": 180, "endOffset": 1113}, {"referenceID": 0, "context": "In Section 6, we include experimental comparisons with ADMM, gradient descent, and L-BFGS variants, including orthant-wise limited memory quasi-Newton (OWL-QN) for the L1 setting (Andrew and Gao, 2007). Finally, we note that while the convergence rates provided for CoCoA mirror the convergence class of classical batch gradient methods in terms of the number of outer rounds, existing batch gradient methods come with a weaker theory, as they do not allow general inexactness \u0398 for the local subproblem (10). In contrast, our convergence rates incorporate this approximation directly, and, moreover, hold for arbitrary local solvers of much cheaper cost than batch methods (where in each round, every machine has to process exactly a full pass through the local data). This makes CoCoA more flexible in the distributed setting, as it can adapt to varied communication costs on real systems. We have seen in Section 6 that this flexibility results in significant performance gains over the competing methods. Distributed solvers. By making use of the primal-dual structure in the line of work of Yu et al. (2012); Pechyony et al. (2011); Yang (2013); Yang et al.", "startOffset": 180, "endOffset": 1137}, {"referenceID": 0, "context": "In Section 6, we include experimental comparisons with ADMM, gradient descent, and L-BFGS variants, including orthant-wise limited memory quasi-Newton (OWL-QN) for the L1 setting (Andrew and Gao, 2007). Finally, we note that while the convergence rates provided for CoCoA mirror the convergence class of classical batch gradient methods in terms of the number of outer rounds, existing batch gradient methods come with a weaker theory, as they do not allow general inexactness \u0398 for the local subproblem (10). In contrast, our convergence rates incorporate this approximation directly, and, moreover, hold for arbitrary local solvers of much cheaper cost than batch methods (where in each round, every machine has to process exactly a full pass through the local data). This makes CoCoA more flexible in the distributed setting, as it can adapt to varied communication costs on real systems. We have seen in Section 6 that this flexibility results in significant performance gains over the competing methods. Distributed solvers. By making use of the primal-dual structure in the line of work of Yu et al. (2012); Pechyony et al. (2011); Yang (2013); Yang et al.", "startOffset": 180, "endOffset": 1150}, {"referenceID": 0, "context": "In Section 6, we include experimental comparisons with ADMM, gradient descent, and L-BFGS variants, including orthant-wise limited memory quasi-Newton (OWL-QN) for the L1 setting (Andrew and Gao, 2007). Finally, we note that while the convergence rates provided for CoCoA mirror the convergence class of classical batch gradient methods in terms of the number of outer rounds, existing batch gradient methods come with a weaker theory, as they do not allow general inexactness \u0398 for the local subproblem (10). In contrast, our convergence rates incorporate this approximation directly, and, moreover, hold for arbitrary local solvers of much cheaper cost than batch methods (where in each round, every machine has to process exactly a full pass through the local data). This makes CoCoA more flexible in the distributed setting, as it can adapt to varied communication costs on real systems. We have seen in Section 6 that this flexibility results in significant performance gains over the competing methods. Distributed solvers. By making use of the primal-dual structure in the line of work of Yu et al. (2012); Pechyony et al. (2011); Yang (2013); Yang et al. (2013) and Lee and Roth (2015), the CoCoA-v1 and CoCoA+ frameworks (which are special cases of the presented framework, CoCoA) are the first to allow the use of any local solver\u2014of weak local approximation quality\u2014in each round in the distributed setting.", "startOffset": 180, "endOffset": 1170}, {"referenceID": 0, "context": "In Section 6, we include experimental comparisons with ADMM, gradient descent, and L-BFGS variants, including orthant-wise limited memory quasi-Newton (OWL-QN) for the L1 setting (Andrew and Gao, 2007). Finally, we note that while the convergence rates provided for CoCoA mirror the convergence class of classical batch gradient methods in terms of the number of outer rounds, existing batch gradient methods come with a weaker theory, as they do not allow general inexactness \u0398 for the local subproblem (10). In contrast, our convergence rates incorporate this approximation directly, and, moreover, hold for arbitrary local solvers of much cheaper cost than batch methods (where in each round, every machine has to process exactly a full pass through the local data). This makes CoCoA more flexible in the distributed setting, as it can adapt to varied communication costs on real systems. We have seen in Section 6 that this flexibility results in significant performance gains over the competing methods. Distributed solvers. By making use of the primal-dual structure in the line of work of Yu et al. (2012); Pechyony et al. (2011); Yang (2013); Yang et al. (2013) and Lee and Roth (2015), the CoCoA-v1 and CoCoA+ frameworks (which are special cases of the presented framework, CoCoA) are the first to allow the use of any local solver\u2014of weak local approximation quality\u2014in each round in the distributed setting.", "startOffset": 180, "endOffset": 1194}, {"referenceID": 0, "context": "In Section 6, we include experimental comparisons with ADMM, gradient descent, and L-BFGS variants, including orthant-wise limited memory quasi-Newton (OWL-QN) for the L1 setting (Andrew and Gao, 2007). Finally, we note that while the convergence rates provided for CoCoA mirror the convergence class of classical batch gradient methods in terms of the number of outer rounds, existing batch gradient methods come with a weaker theory, as they do not allow general inexactness \u0398 for the local subproblem (10). In contrast, our convergence rates incorporate this approximation directly, and, moreover, hold for arbitrary local solvers of much cheaper cost than batch methods (where in each round, every machine has to process exactly a full pass through the local data). This makes CoCoA more flexible in the distributed setting, as it can adapt to varied communication costs on real systems. We have seen in Section 6 that this flexibility results in significant performance gains over the competing methods. Distributed solvers. By making use of the primal-dual structure in the line of work of Yu et al. (2012); Pechyony et al. (2011); Yang (2013); Yang et al. (2013) and Lee and Roth (2015), the CoCoA-v1 and CoCoA+ frameworks (which are special cases of the presented framework, CoCoA) are the first to allow the use of any local solver\u2014of weak local approximation quality\u2014in each round in the distributed setting. The practical variant of the DisDCA (Yang, 2013), called DisDCA-p, allows for additive updates in a similar manner to CoCoA, but is restricted to coordinate decent (CD) being the local solver, and was initially proposed without convergence guarantees. DisDCA-p, CoCoA-v1, and CoCoA+ are all limited to strongly convex regularizers, and therefore are not as general as the CoCoA framework discussed in this work. In the L1-regularized setting, an approach related to our framework includes distributed variants of glmnet as in Mahajan et al. (2014). Inspired by glmnet and Yuan et al.", "startOffset": 180, "endOffset": 1967}, {"referenceID": 25, "context": "If hypothetically each of our quadratic subproblems G\u03c3 k (\u2206\u03b1[k]) as defined in (10) were to be minimized exactly, the resulting steps could be interpreted as block-wise Newton-type steps on each coordinate block k, where the Newton-subproblem is modified to also contain the L1-regularizer (Mahajan et al., 2014; Yuan et al., 2012; Qu et al., 2016).", "startOffset": 290, "endOffset": 348}, {"referenceID": 56, "context": "If hypothetically each of our quadratic subproblems G\u03c3 k (\u2206\u03b1[k]) as defined in (10) were to be minimized exactly, the resulting steps could be interpreted as block-wise Newton-type steps on each coordinate block k, where the Newton-subproblem is modified to also contain the L1-regularizer (Mahajan et al., 2014; Yuan et al., 2012; Qu et al., 2016).", "startOffset": 290, "endOffset": 348}, {"referenceID": 35, "context": "If hypothetically each of our quadratic subproblems G\u03c3 k (\u2206\u03b1[k]) as defined in (10) were to be minimized exactly, the resulting steps could be interpreted as block-wise Newton-type steps on each coordinate block k, where the Newton-subproblem is modified to also contain the L1-regularizer (Mahajan et al., 2014; Yuan et al., 2012; Qu et al., 2016).", "startOffset": 290, "endOffset": 348}, {"referenceID": 5, "context": "(2012), the works of Bian et al. (2013) and Mahajan et al.", "startOffset": 21, "endOffset": 40}, {"referenceID": 5, "context": "(2012), the works of Bian et al. (2013) and Mahajan et al. (2014) introduced the idea of a block-diagonal Hessian upper approximation in the distributed L1 context.", "startOffset": 21, "endOffset": 66}, {"referenceID": 5, "context": "(2012), the works of Bian et al. (2013) and Mahajan et al. (2014) introduced the idea of a block-diagonal Hessian upper approximation in the distributed L1 context. The later work of Trofimov and Genkin (2014) specialized this approach to sparse logistic regression.", "startOffset": 21, "endOffset": 210}, {"referenceID": 5, "context": "(2012), the works of Bian et al. (2013) and Mahajan et al. (2014) introduced the idea of a block-diagonal Hessian upper approximation in the distributed L1 context. The later work of Trofimov and Genkin (2014) specialized this approach to sparse logistic regression. If hypothetically each of our quadratic subproblems G\u03c3 k (\u2206\u03b1[k]) as defined in (10) were to be minimized exactly, the resulting steps could be interpreted as block-wise Newton-type steps on each coordinate block k, where the Newton-subproblem is modified to also contain the L1-regularizer (Mahajan et al., 2014; Yuan et al., 2012; Qu et al., 2016). While Mahajan et al. (2014) allows a fixed accuracy for these subproblems, but not arbitrary approximation quality \u0398 as in our framework, the works of Trofimov and Genkin (2014); Yuan et al.", "startOffset": 21, "endOffset": 645}, {"referenceID": 5, "context": "(2012), the works of Bian et al. (2013) and Mahajan et al. (2014) introduced the idea of a block-diagonal Hessian upper approximation in the distributed L1 context. The later work of Trofimov and Genkin (2014) specialized this approach to sparse logistic regression. If hypothetically each of our quadratic subproblems G\u03c3 k (\u2206\u03b1[k]) as defined in (10) were to be minimized exactly, the resulting steps could be interpreted as block-wise Newton-type steps on each coordinate block k, where the Newton-subproblem is modified to also contain the L1-regularizer (Mahajan et al., 2014; Yuan et al., 2012; Qu et al., 2016). While Mahajan et al. (2014) allows a fixed accuracy for these subproblems, but not arbitrary approximation quality \u0398 as in our framework, the works of Trofimov and Genkin (2014); Yuan et al.", "startOffset": 21, "endOffset": 795}, {"referenceID": 5, "context": "(2012), the works of Bian et al. (2013) and Mahajan et al. (2014) introduced the idea of a block-diagonal Hessian upper approximation in the distributed L1 context. The later work of Trofimov and Genkin (2014) specialized this approach to sparse logistic regression. If hypothetically each of our quadratic subproblems G\u03c3 k (\u2206\u03b1[k]) as defined in (10) were to be minimized exactly, the resulting steps could be interpreted as block-wise Newton-type steps on each coordinate block k, where the Newton-subproblem is modified to also contain the L1-regularizer (Mahajan et al., 2014; Yuan et al., 2012; Qu et al., 2016). While Mahajan et al. (2014) allows a fixed accuracy for these subproblems, but not arbitrary approximation quality \u0398 as in our framework, the works of Trofimov and Genkin (2014); Yuan et al. (2012); and Yen et al.", "startOffset": 21, "endOffset": 815}, {"referenceID": 5, "context": "(2012), the works of Bian et al. (2013) and Mahajan et al. (2014) introduced the idea of a block-diagonal Hessian upper approximation in the distributed L1 context. The later work of Trofimov and Genkin (2014) specialized this approach to sparse logistic regression. If hypothetically each of our quadratic subproblems G\u03c3 k (\u2206\u03b1[k]) as defined in (10) were to be minimized exactly, the resulting steps could be interpreted as block-wise Newton-type steps on each coordinate block k, where the Newton-subproblem is modified to also contain the L1-regularizer (Mahajan et al., 2014; Yuan et al., 2012; Qu et al., 2016). While Mahajan et al. (2014) allows a fixed accuracy for these subproblems, but not arbitrary approximation quality \u0398 as in our framework, the works of Trofimov and Genkin (2014); Yuan et al. (2012); and Yen et al. (2015) assume that the quadratic subproblems are solved exactly.", "startOffset": 21, "endOffset": 838}, {"referenceID": 5, "context": "(2012), the works of Bian et al. (2013) and Mahajan et al. (2014) introduced the idea of a block-diagonal Hessian upper approximation in the distributed L1 context. The later work of Trofimov and Genkin (2014) specialized this approach to sparse logistic regression. If hypothetically each of our quadratic subproblems G\u03c3 k (\u2206\u03b1[k]) as defined in (10) were to be minimized exactly, the resulting steps could be interpreted as block-wise Newton-type steps on each coordinate block k, where the Newton-subproblem is modified to also contain the L1-regularizer (Mahajan et al., 2014; Yuan et al., 2012; Qu et al., 2016). While Mahajan et al. (2014) allows a fixed accuracy for these subproblems, but not arbitrary approximation quality \u0398 as in our framework, the works of Trofimov and Genkin (2014); Yuan et al. (2012); and Yen et al. (2015) assume that the quadratic subproblems are solved exactly. Therefore, these methods are not able to freely trade off communication and computation. Also, they do not allow the re-use of arbitrary local solvers. On the theoretical side, the convergence rate results provided by Mahajan et al. (2014); Trofimov and Genkin (2014); and Yuan et al.", "startOffset": 21, "endOffset": 1136}, {"referenceID": 5, "context": "(2012), the works of Bian et al. (2013) and Mahajan et al. (2014) introduced the idea of a block-diagonal Hessian upper approximation in the distributed L1 context. The later work of Trofimov and Genkin (2014) specialized this approach to sparse logistic regression. If hypothetically each of our quadratic subproblems G\u03c3 k (\u2206\u03b1[k]) as defined in (10) were to be minimized exactly, the resulting steps could be interpreted as block-wise Newton-type steps on each coordinate block k, where the Newton-subproblem is modified to also contain the L1-regularizer (Mahajan et al., 2014; Yuan et al., 2012; Qu et al., 2016). While Mahajan et al. (2014) allows a fixed accuracy for these subproblems, but not arbitrary approximation quality \u0398 as in our framework, the works of Trofimov and Genkin (2014); Yuan et al. (2012); and Yen et al. (2015) assume that the quadratic subproblems are solved exactly. Therefore, these methods are not able to freely trade off communication and computation. Also, they do not allow the re-use of arbitrary local solvers. On the theoretical side, the convergence rate results provided by Mahajan et al. (2014); Trofimov and Genkin (2014); and Yuan et al.", "startOffset": 21, "endOffset": 1164}, {"referenceID": 5, "context": "(2012), the works of Bian et al. (2013) and Mahajan et al. (2014) introduced the idea of a block-diagonal Hessian upper approximation in the distributed L1 context. The later work of Trofimov and Genkin (2014) specialized this approach to sparse logistic regression. If hypothetically each of our quadratic subproblems G\u03c3 k (\u2206\u03b1[k]) as defined in (10) were to be minimized exactly, the resulting steps could be interpreted as block-wise Newton-type steps on each coordinate block k, where the Newton-subproblem is modified to also contain the L1-regularizer (Mahajan et al., 2014; Yuan et al., 2012; Qu et al., 2016). While Mahajan et al. (2014) allows a fixed accuracy for these subproblems, but not arbitrary approximation quality \u0398 as in our framework, the works of Trofimov and Genkin (2014); Yuan et al. (2012); and Yen et al. (2015) assume that the quadratic subproblems are solved exactly. Therefore, these methods are not able to freely trade off communication and computation. Also, they do not allow the re-use of arbitrary local solvers. On the theoretical side, the convergence rate results provided by Mahajan et al. (2014); Trofimov and Genkin (2014); and Yuan et al. (2012) are not explicit convergence rates but only asymptotic, as the quadratic upper bounds are not explicitly controlled for safety as with our \u03c3\u2032.", "startOffset": 21, "endOffset": 1188}, {"referenceID": 50, "context": "6, following the line of reasoning in Yang (2013). For consensus ADMM, the objective (B) is decomposed using the following re-parameterization:", "startOffset": 38, "endOffset": 50}, {"referenceID": 22, "context": "The arguments follow the reasoning in Ma et al. (2015b,a), but where we have generalized them to be applicable directly to (A). We provide full details of Lemma 1 as a proof of concept, but omit details in later proofs that can be derived using the arguments in Ma et al. (2015b) or earlier work of Shalev-Shwartz and Zhang (2013a), and instead outline the proof strategy and highlight sections where the theory deviates.", "startOffset": 38, "endOffset": 280}, {"referenceID": 22, "context": "The arguments follow the reasoning in Ma et al. (2015b,a), but where we have generalized them to be applicable directly to (A). We provide full details of Lemma 1 as a proof of concept, but omit details in later proofs that can be derived using the arguments in Ma et al. (2015b) or earlier work of Shalev-Shwartz and Zhang (2013a), and instead outline the proof strategy and highlight sections where the theory deviates.", "startOffset": 38, "endOffset": 332}, {"referenceID": 36, "context": "3 Proof of Convergence Result for Strongly Convex gi Our second main theorem follows reasoning in Shalev-Shwartz and Zhang (2013a) and is a generalization of Ma et al.", "startOffset": 98, "endOffset": 131}], "year": 2016, "abstractText": "The scale of modern datasets necessitates the development of efficient distributed optimization methods for machine learning. We present a general-purpose framework for the distributed environment, CoCoA, that has an efficient communication scheme and is applicable to a wide variety of problems in machine learning and signal processing. We extend the framework to cover general non-strongly convex regularizers, including L1-regularized problems like lasso, sparse logistic regression, and elastic net regularization, and show how earlier work can be derived as a special case. We provide convergence guarantees for the class of convex regularized loss minimization objectives, leveraging a novel approach in handling non-strongly convex regularizers and non-smooth loss functions. The resulting framework has markedly improved performance over state-of-the-art methods, as we illustrate with an extensive set of experiments on real distributed datasets.", "creator": "LaTeX with hyperref package"}}}