{"id": "1611.02205", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Nov-2016", "title": "Playing SNES in the Retro Learning Environment", "abstract": "Mastering a video game requires skill, tactics and strategy. While these attributes may be acquired naturally by human players, teaching them to a computer program is a far more challenging task. In recent years, extensive research was carried out in the field of reinforcement learning and numerous algorithms were introduced, aiming to learn how to perform human tasks such as playing video games. As a results, the Arcade Learning Environment (ALE) has become a commonly used benchmark environment allowing algorithms to train on various Atari 2600 games. Most Atari games no longer pose a challenge to state-of-the-art algorithms. In this paper we introduce a new learning environment, the Retro Learning Environment --- RLE, based on the Super Nintendo Entertainment System (SNES). The environment is expandable, allowing for more video games and consoles to be easily added to the environment, while maintaining the same interface as ALE. Moreover, RLE is compatible with Python and Torch. SNES games pose a significant challenge to current algorithms due to their higher level of complexity and versatility. To overcome these challenges, we introduce a novel training method based on training two agents against each other.", "histories": [["v1", "Mon, 7 Nov 2016 18:33:38 GMT  (1316kb,D)", "http://arxiv.org/abs/1611.02205v1", null], ["v2", "Tue, 7 Feb 2017 18:50:50 GMT  (1308kb,D)", "http://arxiv.org/abs/1611.02205v2", null]], "reviews": [], "SUBJECTS": "cs.LG cs.AI", "authors": ["nadav bhonker", "shai rozenberg", "itay hubara"], "accepted": false, "id": "1611.02205"}, "pdf": {"name": "1611.02205.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["PLAYING SNES", "IN THE", "RETRO LEARNING ENVIRONMENT", "Nadav Bhonker", "Shai Rozenberg"], "emails": ["nadavbh@tx.technion.ac.il", "shairoz@tx.technion.ac.il", "itayhubara@gmail.com"], "sections": [{"heading": "1 INTRODUCTION", "text": "In fact, it is not as if it is a real game in which the real world is far too complex for the agent to perceive it. In practice, the interaction with the virtual world is simulated by the algorithms made by the algorithms. (Shalev-Shwartz et al., 2016) Therefore, the interaction with the virtual world that receives feedback on an algorithm's decision is simulated. Traditionally, games are used as an RL environment that dates back to Chess., 2002), Checkers et al., 1992, backgammon."}, {"heading": "2 RELATED WORK", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 ARCADE LEARNING ENVIRONMENT", "text": "The interface provided by ALE allows the algorithms to select an action and receive the Atari screen as well as a reward at each step. The action corresponds to the joystick key combination of a human and the reward is the difference between the results in timestamp t and t \u2212 1. The gameplay for Atari provides a solid benchmark, as different games have clearly different goals. Atari 2600 has 500 games, of which more than 70 are currently implemented in ALE and are frequently used for algorithm comparison. Current state-of-the-art algorithms (((Mnih et al., 2015) (Mnih et al., 2016) are able to exceed the human expert performance in most games and thus reduce the attractiveness of ALE for future research."}, {"heading": "2.2 INFINITE MARIO", "text": "Infinite Mario (Togelius et al., 2009) is a remake of the classic Super Mario game, in which levels are randomly generated on which the Mario AI competition took place. During the competition, several algorithms were trained on Infinite Mario and their performance measured by the number of levels completed. Unlike ALE, the training is not based on the raw data from the screen, but on indicating the location and objects in its environment. This environment no longer poses a challenge to state-of-the-art algorithms, but still serves as a benchmark platform. Its main shortcoming lies in the fact that it offers only one game to learn. In addition, the learning process is based on manual features extracted directly from the simulator."}, {"heading": "2.3 OPENAI GYM", "text": "OpenAI Gym (Brockman et al., 2016) is an open source platform with the goal of creating an interface between RL environments and algorithms for evaluation and comparison purposes. OpenAI Gym is currently enjoying great popularity due to the large number of supported environments. For example, ALE, Go, MouintainCar and VizDoom (Zhu et al., 2016), an environment for learning the 3D first-person shooter game \"Doom.\" The recent appearance and widespread use of OpenAI Gym indicate the growing interest and research in the field of RL."}, {"heading": "2.4 DEEP Q-LEARNING", "text": "In our thesis, we used the Deep Q Network Algorithm (DQN) (Mnih et al., 2013), an RL algorithm whose goal is to find an optimal policy (i.e., taking into account the current state, which action should be selected to achieve the highest score).The game status is simply the screen of the game, and the action is a combination of joystick buttons to which the game responds (i.e., move, jump).DQN learns by trial and error while trying to estimate a function called \"Q function,\" which is used to predict the score at the end of the game taking into account the current state and the action chosen.The Q function is represented by a foldable neural network that receives the screen as input and predicts the best possible action at the output.The Q function weights are updated according to: The Q function weights are based on: D + 1 (st), function + 4 = + 3 (Qt) and Qt (+ 3)."}, {"heading": "3 THE RETRO LEARNING ENVIRONMENT", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 SUPER NINTENDO ENTERTAINMENT SYSTEM", "text": "The Super Nintendo Entertainment System (SNES) is a home video game console developed by Nintendo and released in 1990. A total of 783 games were released, including the cult games Super Mario World, Donkey Kong Country and The Legend of Zelda. Table 1 presents a comparison between the Atari 2600 and SNES game consoles, which clearly shows that SNES games are far more complicated."}, {"heading": "3.2 IMPLEMENTATION", "text": "The environment is based on the ALE, with the aim of retaining as much of its user interface as possible to allow easier integration with current platforms and algorithms. While the ALE is strongly coupled with the Atari emulator, Stella 1, RLE takes a different approach and separates the learning environment from the emulator. This has been achieved by integrating an interface called LibRetro (lib), which allows communication between front-end programs to game console emulators. Currently, LibRetro supports over 15 game consoles, each containing hundreds of games, with a total estimated of over 7,000 games that can potentially be supported through this interface. Examples of supported game consoles are Nintendo Entertainment System, Game Boy, N64, Sega Genesis, Saturn, Dreamcast and Sony PlayStation. We decided to focus on the SNES game console, which was implemented using the snes9x2, as its gaming options are plausible, but also allow for different levels of difficulty, such as LE."}, {"heading": "3.3 SOURCE CODE", "text": "RLE is fully available as open source software, which can be used under the GNU General Public License on the environment website. It is implemented in C + + and has an interface to algorithms in C + +, Python and Lua. Adding a new game to the environment is a relatively simple process."}, {"heading": "3.4 INTERFACE COMPARISON", "text": "The interface of RLE is identical to that of ALE, with four exceptions: \u2022 RLE requires an emulator and a computer version of the console game (ROM file) at initialization, not just a ROM file. The emulators are equipped with RLE and are updated in their repository3 as more emulators are supported. \u2022 Unlike ALE, where each pixel is represented by an 8-bit RGB color model, RLE uses a 32-bit model. \u2022 In ALE, each action is represented by a unique integer from 0 to 35. In RLE actions, there is a bit-by-bit representation where each controller key is represented by a single RGB vector."}, {"heading": "3.5 ENVIRONMENT CHALLENGES", "text": "The integration of SNES with RLE presents new challenges for the realm of RL, since visual information in the form of an image is the only state available to the agent. Firstly, SNES games are significantly more complex and unpredictable than Atari games. For example, while in NBA games the player (agent) controls an individual player, the behavior of all other nine players is determined by pre-programmed agents, each of whom exhibits random behavior. Secondly, many SNES games exhibit delayed rewards during the course of their game (i.e. a reward for the player's actions is obtained many steps after execution).An analysis of such a game is presented in Section 4.2. Furthermore, unlike Atari, which consists of eight directions and one action button, SNES has eight direction buttons and six action buttons. As combinations of buttons are allowed and sometimes required, the actual action space for NES can be larger than 700 compared to the maximum of 18 actions in Atari."}, {"heading": "4 EXPERIMENTS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 EVALUATION METHODOLOGY", "text": "The evaluation method we used for benchmarking the different algorithms is therefore the popular method proposed by (Mnih et al., 2013). Each algorithm studied is trained until it reaches either convergence or 100 epochs (each epoch corresponds to 50,000 actions), after which it is evaluated by performing 30 episodes of each game. Each episode ends either by reaching a terminal state or after 5 minutes. Results are averaged per game and compared with the average score of a human player. For each game, the human player was given two hours to train and his performances were evaluated over 20 episodes. As the different algorithms do not use the sound of the game in the learning process, the sound was muted for both the agent and the human. From both scores and agents, the score of a random agent (an agent who randomly performs actions) is subtracted to ensure that the learning actually took place."}, {"heading": "4.1.1 RESULTS", "text": "A thorough comparison of the performance of the four different agents in SNES games is shown in Figure 2. The full results can be found in Table 2. Only in the Mortal Kombat game was a trained agent able to outperform the performance of an experienced human player, as opposed to Atari games, where the same algorithms have overtaken a human player in most games. Wolfenstein, a 3D first-person shooter game, is about 3D vision, maze navigation and object recognition for the agent. As shown in Figure 2, all agents deliver poor results indicating a lack of the required characteristics. An interesting case is Gradius III, a side-scrolling, flightshooter game. While the trained agent was able to master the technical aspects of the game, which include shooting at incoming enemies and dodging their projectiles, its end result is still far from that of a human."}, {"heading": "4.2 REWARD SHAPING", "text": "As part of the environment and the algorithm evaluation process, we examined two case studies. First, a game in which DQN failed to achieve a better than random score, and second, a game in which the training duration was significantly longer than in other games. However, in the first case study, we used a 2D backview racing game \"F-Zero.\" In this game, it is necessary to complete four laps on the track while avoiding other racing cars.The reward as defined by the score of the game is not obtained until after the completion of a lap. This is an extreme case of reward delay. A lap can last up to 30 seconds, spanning over 450 states, before the reward is received, i.e. exploring DQN is a simple -greedy approach that has not been able to produce a useful strategy."}, {"heading": "4.3 RIVALRY TRAINING", "text": "The RLE presents a novel setup in which two different agents can compete against each other. We have investigated two uses of this setup: the first was to train two different agents against the AI in the game, as happened in the previous sections, and to evaluate the two agents against each other; the second was to train two agents against the AI in the game and continue the training while competing with each other and evaluating the AI separately in the game."}, {"heading": "4.3.1 RESULTS", "text": "We chose Mortal Kombat, a combat game with two characters (a screenshot of the game can be seen in Figure 1, as a test environment for the above, as it has advantageous features: both players share the same screen, the optimal strategy of the agent depends heavily on the behavior of the rival, as opposed to racing games, for example. To evaluate two agents fairly, both were trained with the same characters, which preserve the identity of the rival and the agent. Furthermore, in order to eliminate the effects of the starting positions of both agents on their performance, the starting positions were randomly initialized. In the first experiment, we evaluated all combinations of DQN against D-DQN and Dueling D-DQN. Each agent was trained against the in-game AI until convergence. Then, 50 games were played between two agents. DQN lost 28 out of 50 games to Dueling D-DQN and 33 to D-DQN.D-DQN lost 26 times to Dueling-QD-DQN."}, {"heading": "4.4 FEATURED CHALLENGES", "text": "As shown, RLE presents numerous challenges that have yet to be addressed. In addition to the ability to learn all available games, the task of learning games with extreme reward lag, such as F-Zero without reward shaping, remains an unsolved challenge. In addition, some games, such as Super Mario, have several levels that differ in the background; the task of transferring learning between levels, learning on one level and being tested on the other is another unexplored challenge. The most important challenge remains to surpass human performance in available games - a task that current state-of-the-art algorithms can only manage with difficulty."}, {"heading": "5 CONCLUSION", "text": "The modular implementation we have chosen makes it easy to expand the environment with new consoles and games, ensuring the environment's relevance to RL algorithms for years to come. Unlike its predecessor ALE, the challenges presented in the RLE consist of: 3D interpretation, delayed reward, noisy background, stochastic AI behavior and much more. Although some algorithms have been able to successfully play some of the games to fully meet these challenges, an agent needs to integrate both technology and strategy."}, {"heading": "6 ACKNOWLEDGMENTS", "text": "The authors thank the Signal and Image Processing Lab (SIPL) staff for their support, Alfred Agrell and the LibRetro community for their support, and Marc G. Bellemare for his valuable contributions."}], "references": [{"title": "The arcade learning environment: An evaluation platform for general agents", "author": ["M.G. Bellemare", "Y. Naddaf", "J. Veness", "M. Bowling"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Bellemare et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bellemare et al\\.", "year": 2013}, {"title": "Hierarchical reinforcement learning for robot navigation", "author": ["B. Bischoff", "D. Nguyen-Tuong", "I.-H. Lee", "F. Streichert", "A. Knoll"], "venue": "In ESANN,", "citeRegEx": "Bischoff et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bischoff et al\\.", "year": 2013}, {"title": "Playing atari with deep reinforcement learning", "author": ["V. Mnih", "K. Kavukcuoglu", "D. Silver", "A. Graves", "I. Antonoglou", "D. Wierstra", "M. Riedmiller"], "venue": "arXiv preprint arXiv:1312.5602,", "citeRegEx": "Mnih et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2013}, {"title": "Human-level control through deep reinforcement learning", "author": ["V. Mnih", "K. Kavukcuoglu", "D. Silver", "A.A. Rusu", "J. Veness", "M.G. Bellemare", "A. Graves", "M. Riedmiller", "A.K. Fidjeland", "G. Ostrovski"], "venue": "Nature, 518(7540):529\u2013533,", "citeRegEx": "Mnih et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2015}, {"title": "Asynchronous methods for deep reinforcement learning", "author": ["V. Mnih", "A.P. Badia", "M. Mirza", "A. Graves", "T.P. Lillicrap", "T. Harley", "D. Silver", "K. Kavukcuoglu"], "venue": "arXiv preprint arXiv:1602.01783,", "citeRegEx": "Mnih et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2016}, {"title": "Massively parallel methods for deep reinforcement learning", "author": ["A. Nair", "P. Srinivasan", "S. Blackwell", "C. Alcicek", "R. Fearon", "A. De Maria", "V. Panneershelvam", "M. Suleyman", "C. Beattie", "S. Petersen"], "venue": "arXiv preprint arXiv:1507.04296,", "citeRegEx": "Nair et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Nair et al\\.", "year": 2015}, {"title": "A world championship caliber checkers program", "author": ["J. Schaeffer", "J. Culberson", "N. Treloar", "B. Knight", "P. Lu", "D. Szafron"], "venue": "Artificial Intelligence,", "citeRegEx": "Schaeffer et al\\.,? \\Q1992\\E", "shortCiteRegEx": "Schaeffer et al\\.", "year": 1992}, {"title": "Long-term planning by short-term prediction", "author": ["S. Shalev-Shwartz", "N. Ben-Zrihem", "A. Cohen", "A. Shashua"], "venue": "arXiv preprint arXiv:1602.01580,", "citeRegEx": "Shalev.Shwartz et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Shalev.Shwartz et al\\.", "year": 2016}, {"title": "Mastering the game of go with deep neural networks and tree", "author": ["D. Silver", "A. Huang", "C.J. Maddison", "A. Guez", "L. Sifre", "G. Van Den Driessche", "J. Schrittwieser", "I. Antonoglou", "V. Panneershelvam", "M. Lanctot"], "venue": "search. Nature,", "citeRegEx": "Silver et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Silver et al\\.", "year": 2016}, {"title": "Temporal difference learning and td-gammon", "author": ["G. Tesauro"], "venue": "Communications of the ACM,", "citeRegEx": "Tesauro.,? \\Q1995\\E", "shortCiteRegEx": "Tesauro.", "year": 1995}, {"title": "Super mario evolution", "author": ["J. Togelius", "S. Karakovskiy", "J. Koutn\u0131\u0301k", "J. Schmidhuber"], "venue": "IEEE Symposium on Computational Intelligence and Games,", "citeRegEx": "Togelius et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Togelius et al\\.", "year": 2009}, {"title": "Deep reinforcement learning with double q-learning", "author": ["H. Van Hasselt", "A. Guez", "D. Silver"], "venue": "CoRR, abs/1509.06461,", "citeRegEx": "Hasselt et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hasselt et al\\.", "year": 2015}, {"title": "Dueling network architectures for deep reinforcement learning", "author": ["Z. Wang", "N. de Freitas", "M. Lanctot"], "venue": "arXiv preprint arXiv:1511.06581,", "citeRegEx": "Wang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2015}, {"title": "Target-driven visual navigation in indoor scenes using deep reinforcement learning", "author": ["Y. Zhu", "R. Mottaghi", "E. Kolve", "J.J. Lim", "A. Gupta", "L. Fei-Fei", "A. Farhadi"], "venue": "arXiv preprint arXiv:1609.05143,", "citeRegEx": "Zhu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Zhu et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 0, "context": "As a results, the Arcade Learning Environment (ALE) (Bellemare et al., 2013) has become a commonly used benchmark environment allowing algorithms to train on various Atari 2600 games.", "startOffset": 52, "endOffset": 76}, {"referenceID": 7, "context": "Recent breakthroughs in the field allow its utilization in real-world applications such as autonomous driving (Shalev-Shwartz et al., 2016), navigation (Bischoff et al.", "startOffset": 110, "endOffset": 139}, {"referenceID": 1, "context": ", 2016), navigation (Bischoff et al., 2013), financial predictions (Du et al.", "startOffset": 20, "endOffset": 43}, {"referenceID": 6, "context": ", 2002), Checkers (Schaeffer et al., 1992), backgammon (Tesauro, 1995) and the more recent Go (Silver et al.", "startOffset": 18, "endOffset": 42}, {"referenceID": 9, "context": ", 1992), backgammon (Tesauro, 1995) and the more recent Go (Silver et al.", "startOffset": 20, "endOffset": 35}, {"referenceID": 8, "context": ", 1992), backgammon (Tesauro, 1995) and the more recent Go (Silver et al., 2016).", "startOffset": 59, "endOffset": 80}, {"referenceID": 0, "context": "For high-dimensional input, the leading benchmark is the Arcade Learning Environment (ALE) (Bellemare et al., 2013) which provides a common interface to dozens of Atari 2600 games, each presenting a different challenge.", "startOffset": 91, "endOffset": 115}, {"referenceID": 3, "context": "A key work to tackle this problem is the Deep Q-Networks algorithm (Mnih et al., 2015), which made a breakthrough in the field of Deep Reinforcement Learning by achieving human level performance on 29 out of 49 games.", "startOffset": 67, "endOffset": 86}, {"referenceID": 5, "context": "Subsequent algorithms such as (Nair et al., 2015) and (Mnih et al.", "startOffset": 30, "endOffset": 49}, {"referenceID": 4, "context": ", 2015) and (Mnih et al., 2016) achieved above expert human-level scores on 38 and 42 out of 57 games respectively.", "startOffset": 12, "endOffset": 31}, {"referenceID": 3, "context": "Current state-of-the-art algorithms ( (Mnih et al., 2015), (Mnih et al.", "startOffset": 38, "endOffset": 57}, {"referenceID": 4, "context": ", 2015), (Mnih et al., 2016)) are able to surpass expert human performance on most games, thus detracting from ALE attractiveness for future research.", "startOffset": 9, "endOffset": 28}, {"referenceID": 10, "context": "Infinite Mario (Togelius et al., 2009) is a remake of the classic Super Mario game in which levels are randomly generated, on which the Mario AI Competition was held.", "startOffset": 15, "endOffset": 38}, {"referenceID": 13, "context": "For example, ALE , Go , MouintainCar and VizDoom (Zhu et al., 2016), an environment for the learning of the 3D first-person-shooter game \u201dDoom\u201d.", "startOffset": 49, "endOffset": 67}, {"referenceID": 2, "context": "In our work, we used the Deep Q-Network algorithm (DQN) (Mnih et al., 2013), an RL algorithm whose goal is to find an optimal policy (i.", "startOffset": 56, "endOffset": 75}, {"referenceID": 12, "context": "Dueling Double DQN (Wang et al., 2015), a modification of D-DQN\u2019s architecture in which the Q-function is modeled using a state (screen) dependent estimator and an action dependent estimator.", "startOffset": 19, "endOffset": 38}, {"referenceID": 2, "context": "The evaluation methodology which we used for benchmarking the different algorithms is the popular method proposed by (Mnih et al., 2013).", "startOffset": 117, "endOffset": 136}], "year": 2016, "abstractText": "Mastering a video game requires skill, tactics and strategy. While these attributes may be acquired naturally by human players, teaching them to a computer program is a far more challenging task. In recent years, extensive research was carried out in the field of reinforcement learning and numerous algorithms were introduced, aiming to learn how to perform human tasks such as playing video games. As a results, the Arcade Learning Environment (ALE) (Bellemare et al., 2013) has become a commonly used benchmark environment allowing algorithms to train on various Atari 2600 games. Most Atari games no longer pose a challenge to stateof-the-art algorithms. In this paper we introduce a new learning environment, the Retro Learning Environment \u2014 RLE, based on the Super Nintendo Entertainment System (SNES). The environment is expandable, allowing for more video games and consoles to be easily added to the environment, while maintaining the same interface as ALE. Moreover, RLE is compatible with Python and Torch. SNES games pose a significant challenge to current algorithms due to their higher level of complexity and versatility. To overcome these challenges, we introduce a novel training method based on training two agents against each other.", "creator": "LaTeX with hyperref package"}}}