{"id": "1611.02360", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Nov-2016", "title": "Cruciform: Solving Crosswords with Natural Language Processing", "abstract": "Crossword puzzles are popular word games that require not only a large vocabulary, but also a broad knowledge of topics. Answering each clue is a natural language task on its own as many clues contain nuances, puns, or counter-intuitive word definitions. Additionally, it can be extremely difficult to ascertain definitive answers without the constraints of the crossword grid itself. This task is challenging for both humans and computers. We describe here a new crossword solving system, Cruciform. We employ a group of natural language components, each of which returns a list of candidate words with scores when given a clue. These lists are used in conjunction with the fill intersections in the puzzle grid to formulate a constraint satisfaction problem, in a manner similar to the one used in the Dr. Fill system. We describe the results of several of our experiments with the system.", "histories": [["v1", "Tue, 8 Nov 2016 01:47:41 GMT  (847kb,D)", "http://arxiv.org/abs/1611.02360v1", null], ["v2", "Wed, 23 Nov 2016 16:14:23 GMT  (0kb,I)", "http://arxiv.org/abs/1611.02360v2", "based on feedback, we have determined that the paper needs more work"]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["dragomir radev", "rui zhang", "steve wilson", "derek van assche", "henrique spyra gubert", "alisa krivokapic", "meixing dong", "chongruo wu", "spruce bondera", "luke brandl", "jeremy dohmann"], "accepted": false, "id": "1611.02360"}, "pdf": {"name": "1611.02360.pdf", "metadata": {"source": "CRF", "title": "Cruciform: Solving Crosswords with Natural Language Processing", "authors": ["Dragomir Radev", "Rui Zhang", "Steve Wilson", "Derek Van Assche", "Henrique Spyra Gubert", "Alisa Krivokapic", "MeiXing Dong", "Chongruo Wu", "Spruce Bondera", "Luke Brandl", "Jeremy Dohmann"], "emails": ["(radev@umich.edu,", "ryanzh@umich.edu,", "steverw@umich.edu,", "dvanassc@umich.edu", "hs2807@columbia.edu,", "ak3533@columbia.edu,", "meixingd@umich.edu,", "chongruo@umich.edu,", "spruceb@umich.edu,", "brandl@umich.edu,", "dohmann@college.harvard.edu)"], "sections": [{"heading": "1 Introduction", "text": "The answer to this question is a natural task in itself, as many of the more difficult crossword puzzles can be a formidable task for many people. Working in a natural language domain makes the task similarly difficult for computers. An early system for solving crossword puzzles is Proverb [6], which uses a variety of modules aimed at different types of clues, and combines them into a probability list to integrate them into the puzzle."}, {"heading": "2 New York Times Crossword Puzzles", "text": "The New York Times (NYT) releases a new crossword puzzle daily1. Puzzles are submitted by individual composers or crosswords to the editor of the daily puzzle, and as the week progresses, the puzzles become more difficult, with Monday puzzles being the simplest and Saturday puzzles the most difficult, each of which has a grid size of 15x15. Sunday puzzles are about as difficult as Thursday puzzles, but are larger, typically at 21x21. Each abundance or answer is at least three letters long, and all squares appear in exactly two words. Typically, the maximum number of words for a Monday-Thursday puzzle is 78 words, 72 words for a Sunday puzzle, and 140 words for a Sunday puzzle. NYT puzzles follow many additional sentence conventions."}, {"heading": "2.1 Example", "text": "An example of a puzzle by Peter Gordon (edited by Will Shortz) is shown in Figure 1 \u2022 A single word can have clues that refer to many different senses and the use of a word \u2022 Even for a literal sense, different clues can link this sense to very different contexts. For example, \"ear\" can be described as \"a musician's asset,\" \"Van Gogh had one later in life\" or \"corn unit.\" Examples of clusions to \"tree,\" \"NOAH,\" \"ASTRO\" and \"EAR.\" TREE1http: / / www.nytimes.com / Crossword puzzles / \u2022 It has bark but no bite \u2022 Its bark is quiet \u2022 Location of a branch office \u2022 House for children \u2022 Lineage display \u2022 Every family has a \u2022 Family chart \u2022 Cobbler's need \u2022 Shoe stretcher \u2022 It leaves in the spring \u2022 Where is there data \u2022 Location for many cat rescuers \u2022 Dendrophobe's fear \u2022 Forbidden fruit \u2022 Ring for two pairs \u2022 One elephant \u2022 One pair of meaning \u2022 One pair of different NOH \u2022 One elephant \u2022 One pair of different clues \u2022 One elephant in a puzzle \u2022 One elephant in many different sense \u2022 One elephant in a puzzle."}, {"heading": "3 Related Work", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 The Dr. Fill System", "text": "The Dr. Fill system [4] begins by creating candidate answers for each clue. Candidate answers are extracted from various resources such as crossword databases, dictionaries, WordNet and Wikipedia: \u2022 A crossword database from different publishers. \u2022 A small, basic dictionary of common words. \u2022 An extensive dictionary with manually generated scores in which words with a higher score are considered good for crossword puzzles. \u2022 Information such as page titles Wikipedia. Each word of the appropriate length is evaluated by the following five criteria, uses unusual letters and has positive connotations.Powered by TCPDF (www.tcpdf.org) \u2022 Word roots and synonyms from WordNet. \u2022 Each word of the appropriate length is evaluated by the following five criteria: 1) Match for clue 2) Part of language analysis 3) Crossword puzzles Merit 4) Breviation 5) Break off the blank."}, {"heading": "3.1.1 CSP Searching Heuristics", "text": "After generating the candidate answers, Dr. Fill uses the following heuristics to solve the underlying CSP. \u2022 Value Selection: Prefer decisions that not only work well for the word slot in question, but also minimally increase the cost of the intersection clues. \u2022 It is important to note that if Dr.Fill simply selects the best candidate, performance drops significantly. \u2022 Variable Selection: Select the variable that maximizes the difference between the best candidate and the second-best candidate. \u2022 Limited Discrepancy Search (LDS): The search keeps a list of rejected value selections. Each element of the list is a pair of (v, x) that does not specify the value v for x. Discarded elements are not taken into account in the value and variable selection heuristics and are discarded in the rest of the search."}, {"heading": "3.2 Proverb", "text": "Proverb [5, 6] uses thirty expert modules. Each module provides a list of answers from candidates of any length and numerical score, representing the confidence level for each word. It also contains several modules to deal with unusual answers that do not appear in the crossword database, such as multiword answers. Our approach is similar, as we also use a combination of expert modules."}, {"heading": "3.2.1 Merging Lists and Grid Filling", "text": "The merging process is controlled by three parameters: scale (m), length scale (m) and spread (m), with m referring to an expert module. For each module, the weight of each candidate is adjusted by increasing its spreading force (m) and then adjusted, and the safety of each module is multiplied by scale (m) and length scale (m) target length. Fusion then combines the candidates of all modules by adding up their probabilities, weighted by the adjusted confidence, and normalizing the sum to one. These parameters are optimized by mountain running, with the objective function being the average log probability assigned to the correct targets."}, {"heading": "3.3 WebCrow and SACRY", "text": "Webcrow [2] extracts candidate words from web documents and previously solved crossword puzzles. Candidate words are assigned trust values by various modules and combined to a solution using a CSP version of the weighted A * search."}, {"heading": "3.3.1 Candidate Answers Generation", "text": "The WebCrow system draws its candidates primarily from web documents and previously solved crossword puzzles. By means of a novel Web Search Module (WSM), the system extracts potential words from web documents and classifies them using various filters. After parsing the document and identifying an unweighted list of candidate words, they pass through both a statistical filter and a morphological filter. The statistical filter establishes a score based on the distance between the candidate words and the search query. The morphological filter consists of two parts, one of which attempts to determine the morphological class of each word in the document, while the other component identifies the most likely morphological classes of the answer of the clue. Furthermore, the words are sorted according to a fairly standard crossword database module, a hard-coded rules-based module, a dictionary and other similar modules. These scores are then combined additively by word and weighted according to the trust of each crossword database module, the CSR system is then divided by the CSR system *."}, {"heading": "3.4 SACRY", "text": "SACRY is another crossword solving system introduced in [1] and introduces a new re-ranking module into the WebCrow system."}, {"heading": "3.4.1 Reranking", "text": "SACRY uses cores to attempt to relate various clues to each other, in the hope that some higher features of the clues may help to relate similar clues to a similar group of answers. Each answer is modelled by the set of clues associated with it, and the paper describes several features that summarize this information. One of these features is based on basic statistics, the characteristic values of all such clues determined by the reranking modules, as well as the frequency of the answer in the crossword database. The occurrence of each instance of answer is modelled by a vector in the size of the clue list, with the values corresponding to the position in the candidate list of each clue, or zero if the word does not exist. Furthermore, the system attempts to determine the similarity between the answer candidates and clues inserted using features based on word embeddings. These features are determined by (i) the similarity between the clues, the candidate (ii) and the solder point (ii), and the solder point."}, {"heading": "4 Methodology used in the Cruciform system", "text": "Faced with a crossword puzzle to be solved, our system first uses different components (Section 4.1) to generate candidate answers for each clue. After merging the answers from different components, we compile a list of candidate answers together with confidence values. These answers are then used to formulate a CSP that is solved by the algorithm similar to Dr.Fill (Section 4.2.2). Specifically, the CSP algorithm uses a variable selection heuristics by selecting the variable with the maximum difference of the confidence point between the best candidate and the second-best candidate and then selecting the best candidate of that variable heuristically after selecting the values. To introduce some variations, the CSP algorithm also searches for non-optimal values with a predefined depth limit. The CSP search method generates a number of solutions (typically about 20) for the entire puzzle, and we then refine each solution with reworking on the probability steps (including filling in the 4.3)."}, {"heading": "4.1 Component Mechanism", "text": "Since many clues follow a given pattern, it can be advantageous to prepare specialized programs that are created for the purpose of providing answers to specific types of clues. By modular setup of the system, additional components can be added at any time, thus increasing the overall performance of the system continuously. The task of each component is to identify a specific clue type, understand the clue in some way, and provide a (optional) list of candidate answers that have the correct length to be inserted into the puzzle. Input for each component consists of a list of all clues in a puzzle together with the clue ID (e.g. 1A, 2D, 3A...) and the length of the correct answer derived from the puzzle grid layout. Output for each component is a list of candidate answers for each clue that is provided with the appropriate clue ID and a score indicating the quality of the answer generated."}, {"heading": "4.1.1 Lucene Components", "text": "The Lucene components are used as basic components that can handle each clue pattern in a generic manner by providing an interface to the Lucene indexing system.2 Clues are converted into queries that search records of known clues as well as Wikipedia titles.The top results from each data source are compiled, and the incorrect length entries are removed from the list of candidate replies. Lucene automatically generates a score for each result, which is returned along with the list of candidate replies. The three components we are looking at are CWG, OTSYS, and WIKI, which search the text of the CWG, OTSYS, and Wikipedia records described in Section 5."}, {"heading": "4.1.2 Specialized NLP Components", "text": "We first integrated components focusing on very specific tasks such as identifying missing surnames (ex. Clue: Titanic actor Billy (4) Fill: ZANE), missing first names (ex. Clue: \"Wrecking Ball\" singer Cyrus (5) Fill: MILEY \u2022 World Capitals, acronyms and other tasks. Unfortunately, these components caused significant time increases for modest performance improvements. For example, by focusing components on first names, last names, capitals and country capitals, less than 1% of the average accuracy for June 2015 puzzles, at the expense of increasing the time from 82.4 seconds to 494 seconds per puzzle. As such, we decided to exclude them from our final system. Different NLP components with examples: \u2022 Missing last name: Titanic actor Billy (4) ZANE, James who wrote \"A Death in the Family\" (4), AGEE \u2022 Missing first name: \"Wrecking Ball singer Cyrus (MILEY) (4), Knee-VY (4)."}, {"heading": "4.2 Solution Generation", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.2.1 CSP", "text": "A CSP instance can be formally defined by the triple (X, D, C) [8]. Here, X is the set of variables {X1, X2,..., Xn}, D is the set of domains for each variable, {D1, D2,..., Dn}, in which each Di = {v1, v2,..., vn} specifies valid values for the corresponding variable Xi, and C is the set of constraints between variables. Each constraint is a tuple (circumference, rel) in which the scope contains a list of variables affected by the constraints, and rel describes the constraints imposed by the constraints. In the case of a crossword puzzle, X is the set of entries in the puzzle that must be filled in, with each entry Xi having exactly one clue and one correct answer. Both the length of the correct answer '(Xi) and the constraints formed between the two pieces of the puzzle cannot be linked by two sections. \""}, {"heading": "4.2.2 Filling in the Grid", "text": "Instead of looking for a solution that only meets the limitations, we are looking for one that maximizes the likelihood of words. Each candidate word has a rating that reflects the likelihood of the given clue, which is provided by the NLP components. For the algorithm for selecting the next variable, we use the following variable selection heuristically: Select the variable that has the greatest difference between the confidence results of the best and second-best candidate. This difference shows how confident we are in filling this variable. In our value selection heuristically, we select the candidate with the highest score. However, the best candidate is not always the right answer, and always selecting the candidate with the highest score can lead to bad solutions. To work around this problem, we also implemented the Restricted Discrepancy Search, where \"discrepancy\" is defined as the number of times at which the heuristic word is the best solution."}, {"heading": "4.3 Post-processing", "text": "We apply the following two post-processing steps."}, {"heading": "4.3.1 Filling in the blanks", "text": "For each solution, we perform the following algorithm 2. Here, the function SORT (Entries) sorts the list of entries with missing letters by the number of missing letters. Entries with fewer missing letters are filled in first. MATCH (e) returns a word that corresponds to the length and existing letters of the entry, or None if none is found. Algorithm 2 Fill in the empty procedure FILLBLANKEntries \u2190 a list of incomplete entries SortedEntries \u2190 SortedEntries (Entries) for e in SortedEntries dow \u2190 MATCH (e) if w is not None thene \u2190 w."}, {"heading": "4.3.2 Bigram Probability Reranking", "text": "Finally, we perform a Bigram probability check of the possible solutions. First, we create a Bigram language model based on the reference set described in Section 5.1. Afterwards, we calculate the Bigram probability of each solution after filling in the space. The value of the solution is then recalculated by the Bigram probability. We select the solution with the highest score."}, {"heading": "5 Datasets", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 Crossword Clues", "text": "We use Crossword Giant (CWG) puzzles, 3 an online crossword resource with clues and answers, and a clue data set collected by Matthew Ginsberg (OTSYS).4 CWG and OTSYS were downloaded around May 2013 and March 2014, respectively. Statistics including the number of words and clues for each record are summarized in Table 1. The word with the most clues is \"EAR\" with 1,808 different accompanying clues, followed by \"SEA\" (1,707), \"TREE\" (1,578), \"ERA\" (1,578) and \"ARIA\" (1,428)."}, {"heading": "5.2 Wikipedia", "text": "We collect the title and the first set of Wikipedia pages from 2013."}, {"heading": "5.3 Dataset Coverage", "text": "Our main training data set used by the Lucene components consists of clues and answers in CWG and OTSYS. We examine the intersection of clues and answers in the database and NYT clues and answers. If a clue appears in a puzzle that is evaluated and also included in the database, it is included in the average percentage of clues and answers in the database; this also applies to answers. If a puzzle and its associated answer are both included in the database, then it is included in the average percentage of clue / answer pairs in the database. The percentage of clues and answers from the puzzles of the given month found in the database tells us how much of the puzzle we have seen before. In Table 2, we randomly select five months of New York time frames (April 2008, May 2009, June 2015, July 2015, January 2016) and calculate intersections of puzzles for each month. As we can see, the percentage increases from the previously seen clues over time."}, {"heading": "6 Experimental Results", "text": "Our test kit consists of the 61 crossword puzzles from June and July 2015. In this section we describe the basic system based on Dr.Fill [4] and show that our post-processing (filling in blank and bigram rankings) and additional components based on Wikipedia continuously improve the accuracy of the solver."}, {"heading": "6.1 Baseline", "text": "We use two Lucene components based on our crossword datasets, CWG Lucene Component and OTSYS Lucene Component, to retrieve the candidates \"answers. We replicate Dr.Fill's CSP algorithm with an LDS depth limit of 4."}, {"heading": "6.2 System Evaluation", "text": "Finally, in order to evaluate the performance of the entire system, we use the percentage of correct squares filled in by the system, compared to gold standards."}, {"heading": "6.2.1 Post-processing", "text": "We evaluate our post-processing steps by measuring the performance of our base system with and without the two post-processing steps. Results are shown in Table 4. Our base system uses only the CWG and OTSYS-Lucene components. Fill and Rerank refer to the steps Fill-in-the-Blank and Bigram-Reranking respectively. We see that the use of the fill-in-the-blank step does not consistently improve on any post-processing step for all days."}, {"heading": "6.2.2 Additional Lucene Component", "text": "We are adding the Wikipedia Lucene Component (WIKI) to the base system, which only uses the CWG and OTSYS Lucene components. As we can see from the results in Table 4, the Wikipedia Lucene Component offers big improvements for the Wednesday, Saturday and Sunday puzzles. However, it reduces the performance for Thursday and Friday puzzles. The best accuracy is achieved for both June 2015 and July 2015 by combining two post-processing steps with WIKI."}, {"heading": "7 Analysis and Discussion", "text": "Evaluating the results of a crossword solver is simple: we count how many words or squares are correctly filled as specified by the solution. However, crossword solvers often have multiple phases, and a single final accuracy cannot quantify the contribution of each phase. Therefore, intermediate evaluation metrics are critical to understanding different parts of the system and showing meaningful directions for future improvements. In this section, we propose multiple evaluation metrics for individual components, CSP algorithms, and the entire system."}, {"heading": "7.1 Individual Component Evaluation", "text": "All components are executed and evaluated individually on the basis of the following indicators. \u2022 The mean reciprocal response rank (MRAR). \u2022 MRAR = 1 | C \u2032 | | | fic i \"C \u2032 1 riwhere C\" is a series of queries for which the component gives the correct answers, and ri is the rank of the i-th answer. \u2022 Average precision (AP). \u2022 Experimental rate (AR) AR = 1N N N \u00b2 i {1 at answer 0 otherwise A good component should be able to achieve a relatively high MRAR and AP. A high MRAR implies that the component provides helpful results and does not mislead the solver by giving low values to valuable answers. High precision means that there is a good chance that the solver will actually list the solution in the three solution components mentioned above, while the LucR solution is correct in most cases."}, {"heading": "7.2 CSP Evaluation", "text": "To examine the CSP system part more closely, we calculate the following performance statistics: \u2022 Of Components - This is the percentage of clues where the right word is included in at least one of the component candidate lists. \u2022 AverageRank - This is the average rank of the right word in the final candidate list for CSP, if it was used as a clue. \u2022 IntoCSP - This is the percentage of correct words presented by the components that are included in the final candidate list for CSP. \u2022 IntoSolution- This is the percentage of correct words in the candidate list for CSP, provided by CSP. Here, FromComponents provides an overview of the quality of candidate responses generated by different components. AverageRank and IntoCSP quantify the input quality of CSP, while IntoSolution evaluates the quality of the CSP algorithm itself. A good CSP algorithm searches for the correct answers from the input gap between the CSP and the input algorithms leading to a small CSP."}, {"heading": "8 Web Application", "text": "As shown in Figure 3, it is possible to select a puzzle to be solved from all NYT puzzles between 1999 and 2015. It is also possible to select which components of the candidate generation are to be used by the solver. When a puzzle is loaded, both empty and correctly filled grids are displayed, together with all clues. The empty grid is filled with the generated solution after the puzzle has been solved. Results such as square and word accuracy are also displayed. Figure 4 is an output example for our interface.5http: / / clair.si.umich.edu / crossword2015 /"}, {"heading": "9 Conclusion", "text": "In this paper, we propose Cruciform, a new crossword solving system. We use a group of natural language components, each of which provides a list of candidate words with probability values when they receive a clue. Subsequently, these lists are used in conjunction with the filling interfaces in the puzzle grid to formulate a constraint satisfaction problem (CSP). Additional steps, such as filling in the gaps using Bigram probabilities, complete the components of the system."}, {"heading": "Acknowledgments", "text": "We thank Di Chen, Yanni Gu, Malcolm MacLachlan, Benjamin Englard, Cody Hansen, Anthony Vito, Seunbum Park, Jonathan Juett, Yue Xu, Jonathan Kummerfeld for helpful discussions and feedback."}], "references": [{"title": "Sacry: Syntax-based automatic crossword puzzle resolution system", "author": ["Gianni Barlacchi", "Massimo Nicosia", "Alessandro Moschitti"], "venue": "ACL-IJCNLP", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2015}, {"title": "Webcrow: A web-based system for crossword solving", "author": ["Marco Ernandes", "Giovanni Angelini", "Marco Gori"], "venue": "In AAAI,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2005}, {"title": "Ibm\u2019s watson/deepqa", "author": ["David A Ferrucci"], "venue": "In ACM SIGARCH Computer Architecture News,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2011}, {"title": "Dr. fill: Crosswords and an implemented solver for singly weighted csps", "author": ["Matthew L Ginsberg"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2011}, {"title": "Proverb: The probabilistic cruciverbalist", "author": ["Greg A Keim", "Noam M Shazeer", "Michael L Littman", "Sushant Agarwal", "Catherine M Cheves", "Joseph Fitzgerald", "Jason Grosland", "Fan Jiang", "Shannon Pollard", "Karl Weinmeister"], "venue": "New York Times (NYT),", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1999}, {"title": "A probabilistic approach to solving crossword puzzles", "author": ["Michael L Littman", "Greg A Keim", "Noam Shazeer"], "venue": "Artificial Intelligence,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2002}, {"title": "AI* IA 2005: Advances in Artificial Intelligence: 9th Congress of the Italian Association for Artificial Intelligence Milan", "author": ["Sara Manzoni"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2005}, {"title": "Artificial intelligence: a modern approach, volume 2. Prentice hall", "author": ["Stuart Jonathan Russell", "Peter Norvig", "John F Canny", "Jitendra M Malik", "Douglas D Edwards"], "venue": "Upper Saddle River,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2003}], "referenceMentions": [{"referenceID": 3, "context": "Fill system[4].", "startOffset": 11, "endOffset": 14}, {"referenceID": 5, "context": "One early system designed to solve crossword puzzles, Proverb [6], uses a variety of modules that targeted different types of clues and combined them into a probabilistic list in order to place them into the puzzle.", "startOffset": 62, "endOffset": 65}, {"referenceID": 3, "context": "Fill [4] treats the crossword problem as a probabilistic constraint satisfaction problem and uses various lexical features to rank the candidate answers.", "startOffset": 5, "endOffset": 8}, {"referenceID": 2, "context": "IBM\u2019s Watson [3] uses an enormous amount of computation power and engineering talent to successfully take on another difficult natural language game: Jeopardy!.", "startOffset": 13, "endOffset": 16}, {"referenceID": 6, "context": "Compared with other Question Answering systems, crossword solving systems are generally more complex, because of the following properties of crossword puzzles: [7]:", "startOffset": 160, "endOffset": 163}, {"referenceID": 3, "context": "Fill system [4] starts with producing candidate answers for each clue.", "startOffset": 12, "endOffset": 15}, {"referenceID": 3, "context": "For additional details, see [4].", "startOffset": 28, "endOffset": 31}, {"referenceID": 4, "context": "Proverb [5, 6] uses thirty expert modules.", "startOffset": 8, "endOffset": 14}, {"referenceID": 5, "context": "Proverb [5, 6] uses thirty expert modules.", "startOffset": 8, "endOffset": 14}, {"referenceID": 1, "context": "Webcrow [2] draws candidate words from web documents and previously solved crosswords.", "startOffset": 8, "endOffset": 11}, {"referenceID": 0, "context": "SACRY is another crossword solving system introduced in [1] that introduces a new reranking module into the WebCrow system.", "startOffset": 56, "endOffset": 59}, {"referenceID": 7, "context": "A CSP instance can formally be defined with the triple (X,D,C) [8].", "startOffset": 63, "endOffset": 66}, {"referenceID": 3, "context": "Fill [4], and show that our post-processing (fill in blank and bigram reranking) and additional component based on Wikipedia consistently improve the accuracy of the solver.", "startOffset": 5, "endOffset": 8}, {"referenceID": 3, "context": "Fill [4] as much as possible.", "startOffset": 5, "endOffset": 8}], "year": 2016, "abstractText": "Crossword puzzles are popular word games that require not only a large vocabulary, but also a broad knowledge of topics. Answering each clue is a natural language task on its own as many clues contain nuances, puns, or counter-intuitive word definitions. Additionally, it can be extremely difficult to ascertain definitive answers without the constraints of the crossword grid itself. This task is challenging for both humans and computers. We describe here a new crossword solving system, Cruciform. We employ a group of natural language components, each of which returns a list of candidate words with scores when given a clue. These lists are used in conjunction with the fill intersections in the puzzle grid to formulate a constraint satisfaction problem, in a manner similar to the one used in the Dr. Fill system[4]. We describe the results of several of our experiments with the system.", "creator": "LaTeX with hyperref package"}}}