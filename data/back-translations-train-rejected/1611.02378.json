{"id": "1611.02378", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Nov-2016", "title": "A Surrogate-based Generic Classifier for Chinese TV Series Reviews", "abstract": "With the emerging of various online video platforms like Youtube, Youku and LeTV, online movie reviews become more and more important both for movie viewers and producers. As a result, automatically classifying reviews according to different requirements evolves as a popular research topic and is very essential in our daily life. In this paper, we focused on reviews of hot TV series in China and successfully trained generic classifiers based on 8 predefined categories. The experimental results showed promising performance and effectiveness of its generalization to different TV series.", "histories": [["v1", "Tue, 8 Nov 2016 03:33:46 GMT  (180kb,D)", "http://arxiv.org/abs/1611.02378v1", null], ["v2", "Mon, 21 Nov 2016 14:55:40 GMT  (225kb,D)", "http://arxiv.org/abs/1611.02378v2", "submitted to IDD"]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["yufeng ma", "long xia", "wenqi shen", "mi zhou", "weiguo fan"], "accepted": false, "id": "1611.02378"}, "pdf": {"name": "1611.02378.pdf", "metadata": {"source": "CRF", "title": "A Surrogate-based Generic Classifier for Chinese Movie Reviews", "authors": ["Yufeng Ma"], "emails": ["yufengma@vt.edu", "longxia1@vt.edu", "shenw@vt.edu", "wfan@vt.edu"], "sections": [{"heading": "1 Introduction", "text": "In fact, most people who are able to survive themselves, to survive themselves and to survive, are not able to survive themselves, \"he said in an interview with the\" New York Times, \"in which he seems to side with the\" New York Times. \""}, {"heading": "2 Related Work", "text": "Since we perform supervised learning tasks with text input, this is related to the work of useful techniques such as feature selection and supervised classifiers. Also, there are currently only public movie rating records in English, which differs from our language requirements. Below, we will first present some existing feature selection techniques and supervised classifiers that we have used in our approach, and then we will present some relevant records that are normally used in the movie rating domain."}, {"heading": "2.1 Feature selection", "text": "The selection of characteristics or variable selection method is a very common machine learning strategy that attempts to select a subset of relevant characteristics from the entire set, with three main purposes. Smaller characteristics or features with a smaller dimension can help researchers better understand or interpret the model they are designing. With fewer characteristics, we can also improve the generalization of our model by preventing overadjustment and reducing overall training time.Document Relevance Correlation (DRC), proposed by W. Fan et al 2005 [4], is a fairly useful selection technique. The authors apply this approach to profiling in the digital library service and message monitoring. They have compared DRC with other well-known methods such as Robertson's selection value [5] and machine learning-based methods such as gaining information [6]. Promising experimental results have been shown to demonstrate the effectiveness of DRC testing as a selection method in the text field [7]."}, {"heading": "2.2 Supervised Classifier", "text": "What we need is a classification of each review into several generic categories that might be attractive to readers, so selecting the classifier is also very important for our problem. Supervised learning takes labeled training pairs and tries to learn a derivative function that can be used to predict new samples. Here, in this paper, our selection is based on two types of learning, i.e. discriminatory and generative learning algorithms, and we choose three typical algorithms to compare. Na\u00efve Bayes [8], who is the representative of generative learning, gives the class with the highest probability generated by the Bayes rule. While for discriminatory classifiers such as logistic regression [9] or support vector machine [10], the final decisions are based on the output score of the classifier, which is compared with a certain threshold to distinguish between different classes."}, {"heading": "2.3 Movie Review Dataset", "text": "The dataset is another important factor influencing the performance of our classifiers. Most publicly available movie rating data is in English, like the IMDB dataset collected by Pang / Lee in 2004 [11]. Although it covers all types of movies on the IMDB website, it only has labels related to mood. Its original goal was sentimental analysis. Another intact dataset for movie ratings is SNAP [12], which consists of reviews from Amazon but contains only ratings. However, what we need are the content or aspect marks discussed in each review. In addition, our review text requirement for Chinese is, so we need to build the dataset ourselves and place it into general categories that can be considered one of our contributions."}, {"heading": "3 Chinese Movie Review Classification", "text": "Let R = r1, r2,.., rn be a series of Chinese film reviews without categorical information. The ultimate task of classifying film reviews is to classify them into various predefined categories such as c1, c2,.., cm. Starting from the sketch, we must collect such review sets R from a web page and then manually place them into general categories {ci}. Based on the data set collected, we can use natural language processing techniques to obtain raw text characteristics and further learn the classifiers. In the following subsections, we will go through and elaborate on all the subtasks shown in Figure 1."}, {"heading": "3.1 Building Dataset", "text": "What interests us are reviews of the hottest or currently aired TV series, so we choose one of the most influential Web 2.0 sites to share movie / TV series in China, called Douban. For each movie or TV series, you will find a corresponding section in Douban. For popularity reasons, we choose \"The Journey of the Flower,\" \"Nirvana in Fire\" and \"Good Time\" as parts of our record for movie reviews, which are the hottest TV series from summer to fall 2015. Ratings for each episode are to be collected to make the dataset more comprehensive. Then, we built the crawler written in Python using Scrapy. Scrapy generates multiple threads to search through information we need simultaneously, saving us a lot of time. For each episode, both the short descriptions of this episode and all the reviews are collected under this post. Statistics from our movie review dataset are presented in Table 2."}, {"heading": "3.2 Basic Text Processing", "text": "Based on the reviews collected, we are almost ready to build a rough classifier, but before we feed it into a classifier, we need to perform some basic pre-processing processes. Two basic procedures: tokenization and stop-word removal are applied to all ratings. Let's take a detour here. We've done some tricks to make our ratings more general before that. We've replaced the names of the roles and actors / actresses in the ratings with some common symbols, such as role i, actor j, where i and j are determined by their meaning in this TV series. Therefore, we have the following inferencing < j = \u21d2 IM (i) > IM (j) (1), where the sequence of words of a role or actor is represented in their meaning, but how can we successfully conclude this? Fortunately, we have Baidu Encyclopedia, which is the Chinese version of Wikipedia."}, {"heading": "3.3 Topic Modelling and Labeling", "text": "Since it is difficult for us to define generic categories without looking at them individually, it is necessary to perform some unattended models to get an overview of what is spoken throughout the corpus. At this point, we applied Latent Dirichlet Allocation [14, 15] to discover the main themes associated with the films and actors. In short, the LDA model assumes that there is a hidden structure consisting of the themes that appear throughout the text corpus. The LDA algorithm uses the simultaneous occurrence of observed words to learn this hidden structure. Mathematically, the model calculates the posterior distribution of the unobserved variables. In the face of a series of training documents, LDA provides two main results. The first is the list of themes represented as a set of words that are likely to contribute to this topic in terms of their weights. The second output is a list of documents with a vector of weight values that show the probability of a document."}, {"heading": "3.4 Feature Selection", "text": "One problem is that the vocabulary size of our corpus will be quite large, which will most likely result in a match with the training data. As the dimension of the feature increases, so will the complexity of our model. Let's first define a contingency table for each word j, as in Table 4. If j = 1, this means the appearance of the word j.Recall, that in classical statistics [2] a method has been developed to measure the independence between two variables or events, which in our case is the word j and its relevance to class i."}, {"heading": "3.5 Learning Classifiers", "text": "As we have already said, there are two types of learning algorithms, i.e., there are two types of learning algorithms, i.e., there are two types of learning algorithms, i.e., there are two types of learning algorithms, i.e., there are two types of learning algorithms, i.e., there are two types of learning algorithms, i.e., there are two types of learning algorithms, i.e., there are two types of learning algorithms, i.e., there are two types of learning algorithms, both types of learning algorithms, both types of learning algorithms, i.e., we can only apply the above Bayes rule to y predict that we assume that each trait is conditionally independent with each other, so that we havep (x) = p (x2 | y) \u00b7 p."}, {"heading": "4 Experimental Results and Discussion", "text": "Since our ultimate goal is to learn a generic classifier that is agnostic for television series but can reasonably predict the category of review, we conducted experiments that followed our procedures for constructing the classifier as described in Section 1."}, {"heading": "4.1 Category Determining by LDA", "text": "Before defining the categories of film reviews, we should first perform some topic modeling methods. Here, we define the categories with the help of LDA. Since we only rely on LDA as a guide for our category definition, we have not applied it to other TV series. Here, the results are in Figure 2. Note that the input data here has not been replaced by the generic keyword like role i or actor j, because we want to know the specifics spoken by the critics. Here, we present them in the form of thermal images. If lines are lighter in color, the corresponding topic is discussed more than others on the same level for each review. As the original texts are in Chinese, the editions of LDA are also presented in Chines.We can see that most of the reviews focus on discussing the roles and analyzing the actions in the movie, i.e., the sixth and seventh parts of this topic are presented in the following Figure 5th in the following Figure 2 while the following data fairly appear in the following categories."}, {"heading": "4.2 Feature Size Comparison", "text": "Based on the methodology discussed in Section 3.4, we can sort the meaning of each word term. We can use different feature sizes to train the 8 generic classifiers and determine their performance both on the training bench and on the test bench. Here, we use SVM as a classifier to compare the influence of the feature size because we find that it performs best among the three. Results are in Figure 3. The red squirrels represent training accuracy, while the blue triangles test accuracy. If you look at Figure 3, it is easy for us to determine the feature size for each classifier. It is also obvious that the test accuracy of the action, actor / actress, analysis and thumbs up or down classifiers has not increased much with the addition of more words. Therefore, the top 1000 words are set as definitive feature words with respect to these classes. While for the rest of the classifiers peak performance is achieved at the size of about 4000, we have used fictitious characteristics in our final classifiers."}, {"heading": "4.3 Generalization of Classifiers", "text": "To prove the generalization of our classifiers, we use two of the TV series as training data and the rest as a test set. And we compare them with classifiers trained without substituting generic tags such as role i or actor j. Thus, 3 sets of experiments are carried out, and each of them is trained on top of Na\u00efve Bayes, Logistic Regression and SVM. Average accuracies between them are given as a measure of performance for the sake of the space boundary. The results are presented in Table 5. \"1,\" \"2\" and \"3\" represent the television series \"The Journey of the Flower,\" \"Nirvana in Fire\" and \"Good Time.\" In each cell, the left value represents the accuracy of the classifier without substituting generic tags and the winners are bold.From the table above, by replacing generic tags in film reviews, we can see that the top 5 classifiers have experienced an increase in performance, proving the effectiveness of our method."}, {"heading": "5 Conclusion", "text": "Based on the results of the topic modeling, we define 8 generic categories and manually label the collected film reviews. Then, using the Baidu Encyclopaedia, the specific information of TV series such as roles and actor names is replaced by common tags within the TV series domain. Our experimental results showed that such a strategy, combined with the selection of feature films, improved the performance of the classifications. In this way, we can easily build classifiers on already collected film reviews and then successfully classify them from new TV series."}, {"heading": "6 Acknowledgement", "text": "It is not the first time that the EU Commission has taken such a step."}], "references": [{"title": "The world is your library, or the state of international interlibrary loan in 2015", "author": ["K. Munson", "H.H. Thompson", "J. Cabaniss", "H. Nance", "P. Erlandsen", "M. McGrath", "M. Mc- Grath"], "venue": "Interlending & Document Supply, vol. 44, no. 2, 2016.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2016}, {"title": "Resource sharing in a cloud computing age", "author": ["M. Goldner", "K. Birch"], "venue": "Interlending & Document Supply, vol. 40, no. 1, pp. 4\u201311, 2012.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2012}, {"title": "Chinese movie big data analysis report.", "author": ["C. Film"], "venue": "http://sanwen8.cn/p/197jW1H. html,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2016}, {"title": "Effective profiling of consumer information retrieval needs: a unified framework and empirical comparison", "author": ["W. Fan", "M.D. Gordon", "P. Pathak"], "venue": "Decision Support Systems, vol. 40, no. 2, pp. 213\u2013233, 2005.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2005}, {"title": "On relevance weight estimation and query expansion", "author": ["S.E. Robertson"], "venue": "Journal of Documentation, vol. 42, no. 3, pp. 182\u2013188, 1986.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1986}, {"title": "Hierarchical clustering based on mutual information", "author": ["A. Kraskov", "H. St\u00f6gbauer", "R.G. Andrzejak", "P. Grassberger"], "venue": "arXiv preprint q-bio/0311039, 2003.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2003}, {"title": "Contingency tables involving small numbers and the \u03c7 2 test", "author": ["F. Yates"], "venue": "Supplement to the Journal of the Royal Statistical Society, vol. 1, no. 2, pp. 217\u2013235, 1934.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1934}, {"title": "Tackling the poor assumptions of naive bayes text classifiers", "author": ["J.D. Rennie", "L. Shih", "J. Teevan", "D.R. Karger"], "venue": "ICML, vol. 3, pp. 616\u2013623, Washington DC, 2003.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2003}, {"title": "Estimation of the probability of an event as a function of several independent variables", "author": ["S.H. Walker", "D.B. Duncan"], "venue": "Biometrika, vol. 54, no. 1-2, pp. 167\u2013179, 1967.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1967}, {"title": "Support-vector networks", "author": ["C. Cortes", "V. Vapnik"], "venue": "Machine learning, vol. 20, no. 3, pp. 273\u2013 297, 1995.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1995}, {"title": "A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts", "author": ["B. Pang", "L. Lee"], "venue": "Proceedings of the 42nd annual meeting on Association for Computational Linguistics, p. 271, Association for Computational Linguistics, 2004.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2004}, {"title": "From amateurs to connoisseurs: modeling the evolution of user expertise through online reviews", "author": ["J.J. McAuley", "J. Leskovec"], "venue": "Proceedings of the 22nd international conference on World Wide Web, pp. 897\u2013908, ACM, 2013.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2013}, {"title": "Nlpir: A theoretical framework for applying natural language processing to information retrieval", "author": ["L. Zhou", "D. Zhang"], "venue": "Journal of the American Society for Information Science and Technology, vol. 54, no. 2, pp. 115\u2013123, 2003.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2003}, {"title": "Latent dirichlet allocation", "author": ["D.M. Blei", "A.Y. Ng", "M.I. Jordan"], "venue": "Journal of machine Learning research, vol. 3, no. Jan, pp. 993\u20131022, 2003.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2003}, {"title": "On the design of lda models for aspect-based opinion mining", "author": ["S. Moghaddam", "M. Ester"], "venue": "Proceedings of the 21st ACM international conference on Information and knowledge management, pp. 803\u2013812, ACM, 2012. 11", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2012}], "referenceMentions": [{"referenceID": 0, "context": "0\u2019s development, more and more commercial websites, such as Amazon, Youtube and Youku, encourage people to post reviews on their platforms for what they are interested in or complain about [1, 2].", "startOffset": 189, "endOffset": 195}, {"referenceID": 1, "context": "0\u2019s development, more and more commercial websites, such as Amazon, Youtube and Youku, encourage people to post reviews on their platforms for what they are interested in or complain about [1, 2].", "startOffset": 189, "endOffset": 195}, {"referenceID": 2, "context": "First, let\u2019s look at the Chinese movies\u2019 development [3] in recent years as shown in Table 1.", "startOffset": 53, "endOffset": 56}, {"referenceID": 3, "context": "Fan et al 2005 [4], is a quite useful feature selection technique.", "startOffset": 15, "endOffset": 18}, {"referenceID": 4, "context": "They\u2019ve compared DRC with other well-know methods like Robertson\u2019s Selection Value [5], and machine learning based ones like information gain [6].", "startOffset": 83, "endOffset": 86}, {"referenceID": 5, "context": "They\u2019ve compared DRC with other well-know methods like Robertson\u2019s Selection Value [5], and machine learning based ones like information gain [6].", "startOffset": 142, "endOffset": 145}, {"referenceID": 6, "context": "Another popular feature selection method is called \u03c7 [7], which is a variant of \u03c7 test in statistics that tries to test the independence between two events.", "startOffset": 53, "endOffset": 56}, {"referenceID": 7, "context": "Na\u0131\u0308ve Bayes [8], which is the representative of generative learning, will output the class with the highest probability that is generated through the bayes\u2019 rule.", "startOffset": 13, "endOffset": 16}, {"referenceID": 8, "context": "While for the discriminative classifiers like logistic regression [9] or Support Vector Machine [10], final decisions are based on the classifier\u2019s output score, which is compared with some threshold to distinguish between different classes.", "startOffset": 66, "endOffset": 69}, {"referenceID": 9, "context": "While for the discriminative classifiers like logistic regression [9] or Support Vector Machine [10], final decisions are based on the classifier\u2019s output score, which is compared with some threshold to distinguish between different classes.", "startOffset": 96, "endOffset": 100}, {"referenceID": 10, "context": "Most of the public available movie review data is in English, like the IMDB dataset collected by Pang/Lee 2004 [11].", "startOffset": 111, "endOffset": 115}, {"referenceID": 11, "context": "Another intact movie review dataset is SNAP [12], which consists of reviews from Amazon but only bearing rating scores.", "startOffset": 44, "endOffset": 48}, {"referenceID": 12, "context": "Each sequence is broken up into a vector of unigram-based tokens using NLPIR [13], which is a very powerful tool supporting sentence segmentation in Chinese.", "startOffset": 77, "endOffset": 81}, {"referenceID": 13, "context": "Here we applied Latent Dirichlet Allocation [14, 15] to discover the main topics related to the movies and actors.", "startOffset": 44, "endOffset": 52}, {"referenceID": 14, "context": "Here we applied Latent Dirichlet Allocation [14, 15] to discover the main topics related to the movies and actors.", "startOffset": 44, "endOffset": 52}, {"referenceID": 6, "context": "Therefore, based on the definition of \u03c7 in [7] and the above Table 4, we can represent the \u03c7 value as below: \u03c7 = N \u00d7 (AD \u2212 CB) (A+ C)\u00d7 (B +D)\u00d7 (A+B)\u00d7 (C +D) (2)", "startOffset": 43, "endOffset": 46}], "year": 2017, "abstractText": "With the emerging of various online video platforms like Youtube, Youku and LeTV, online movie reviews become more and more important both for movie viewers and producers. As a result, automatically classifying reviews according to different requirements evolves as a popular research topic and is very essential in our daily life. In this paper, we focused on reviews of hot TV series in China and successfully trained generic classifiers based on 8 predefined categories. The experimental results showed promising performance and effectiveness of its generalization to different TV series.", "creator": "LaTeX with hyperref package"}}}