{"id": "1611.02416", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Nov-2016", "title": "An Efficient Approach to Boosting Performance of Deep Spiking Network Training", "abstract": "Nowadays deep learning is dominating the field of machine learning with state-of-the-art performance in various application areas. Recently, spiking neural networks (SNNs) have been attracting a great deal of attention, notably owning to their power efficiency, which can potentially allow us to implement a low-power deep learning engine suitable for real-time/mobile applications. However, implementing SNN-based deep learning remains challenging, especially gradient-based training of SNNs by error backpropagation. We cannot simply propagate errors through SNNs in conventional way because of the property of SNNs that process discrete data in the form of a series. Consequently, most of the previous studies employ a workaround technique, which first trains a conventional weighted-sum deep neural network and then maps the learning weights to the SNN under training, instead of training SNN parameters directly. In order to eliminate this workaround, recently proposed is a new class of SNN named deep spiking networks (DSNs), which can be trained directly (without a mapping from conventional deep networks) by error backpropagation with stochastic gradient descent. In this paper, we show that the initialization of the membrane potential on the backward path is an important step in DSN training, through diverse experiments performed under various conditions. Furthermore, we propose a simple and efficient method that can improve DSN training by controlling the initial membrane potential on the backward path. In our experiments, adopting the proposed approach allowed us to boost the performance of DSN training in terms of converging time and accuracy.", "histories": [["v1", "Tue, 8 Nov 2016 07:41:54 GMT  (1341kb,D)", "http://arxiv.org/abs/1611.02416v1", "9 pages, 5 figures"]], "COMMENTS": "9 pages, 5 figures", "reviews": [], "SUBJECTS": "cs.LG cs.NE", "authors": ["seongsik park", "sang-gil lee", "hyunha nam", "sungroh yoon"], "accepted": false, "id": "1611.02416"}, "pdf": {"name": "1611.02416.pdf", "metadata": {"source": "CRF", "title": "An Efficient Approach to Boosting Performance of Deep Spiking Network Training", "authors": ["Seongsik Park", "Sang-gil Lee", "Hyunha Nam", "Sungroh Yoon"], "emails": ["sryoon@snu.ac.kr"], "sections": [{"heading": "1 Introduction", "text": "This year it is so far that it will only take a few days to achieve an outcome that everyone else can participate in."}, {"heading": "2 Methods and Experiments", "text": "As a basic training algorithm, we present the original version of the training methodology for DSNs [22] in Algorithm 1. As outlined in this algorithm, the training methodology of DSNs differs from that of conventional DNNs. For each training example, we extract a series of spike trains (spikes) t-times and use each spike train to train a DSN. In this essay, we denote the amount t the time period for training (in other essays, the term \"time steps\" is also commonly used), the spikes in a spike train are propagated forwards and backwards through the network, while the gradient information for weight updates is accumulated. After each period of time, the parameters in the DSN are updated using the accumulated gradient information. The properties of information coding and propagation in a spike train make the period for training data of DSNNs more likely to be obtained as the SNN may ultimately play a decisive role in increasing the time span SNN."}, {"heading": "2.1 Effects of Time Periods and Initial Potential on SNN Training", "text": "In order to verify the previous work and confirm the effectiveness of the proposed approach, we have conducted a series of experiments that delimit the code from the original work under the same set of hyperparameters and conditions. (...) It is not that the time in which we see ourselves able to capture the results of the training. (...) It is not that the time in which we are able to recognize and understand the results of the work. (...) It is not that the time in which we are able to significantly influence the results of the training. (...) It is not that the time in which we are able to process the results of the work. (...) It is not that the time in which we do it. (...) It is that the time in which we are able to do the work. (...) It is not that we are able to do it. \""}, {"heading": "2.2 Our Proposal: Step-Decay Precharge Method", "text": "This year, it has reached the stage where it will be able to take the lead."}, {"heading": "3 Discussion and Conclusion", "text": "In this paper, we have proposed a new approach to increase the effectiveness of DSN training with SGD and back propagation. Our proposal is based on the fact that the initialization of membranes on the way back of a DSN significantly influences training accuracy as it was previously discovered. However, the main idea of the proposed method is that reducing the effects of initialization potentials can improve training accuracy as the training progresses. We have confirmed the effectiveness of our method through a variety of experiments under different conditions. As long as we train an SNN with SGD and back propagation (both are widely used in DNNs), the preload method is expected to be an inevitable prerequisite for increasing SNN training effectiveness. In this context, we believe that our approach makes a meaningful contribution to the field by providing a simple and effective means of improving SNN training quality."}, {"heading": "Acknowledgments", "text": "This work was supported in 2016 by the BK21 Plus Project (Electrical and Computer Engineering, Seoul National University), partly with a scholarship from Samsung Electronics and partly with a scholarship from Naver."}], "references": [{"title": "Neural Machine Translation by Jointly Learning to Align and Translate", "author": ["D. Bahdanau", "K. Cho", "Y. Bengio"], "venue": "arXiv.org, page arXiv:1409.0473, Sept.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep autoencoder neural networks for gene ontology annotation predictions", "author": ["D. Chicco", "P.J. Sadowski", "P. Baldi"], "venue": "BCB, pages 533\u2013540,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation", "author": ["K. Cho", "B. van Merrienboer", "C. Gulcehre", "D. Bahdanau", "F. Bougares", "H. Schwenk", "Y. Bengio"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2014}, {"title": "Plasticity in the human central nervous system", "author": ["S. Cooke", "T. Bliss"], "venue": "Brain, 129(7):1659\u20131673,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2006}, {"title": "Fast-classifying, highaccuracy spiking deep networks through weight and threshold balancing", "author": ["P.U. Diehl", "D. Neil", "J. Binas", "M. Cook", "S.-C. Liu", "M. Pfeiffer"], "venue": "2015 International Joint Conference on Neural Networks (IJCNN), pages 1\u20138. IEEE,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2015}, {"title": "Functional requirements for reward-modulated spike-timing-dependent plasticity", "author": ["N. Fr\u00e9maux", "H. Sprekeler", "W. Gerstner"], "venue": "The Journal of Neuroscience, 30(40):13326\u201313337,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2010}, {"title": "Spiking neuron models: Single neurons, populations, plasticity", "author": ["W. Gerstner", "W.M. Kistler"], "venue": "Cambridge university press,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2002}, {"title": "The tempotron: a neuron that learns spike timing\u2013based decisions", "author": ["R. G\u00fctig", "H. Sompolinsky"], "venue": "Nature neuroscience, 9(3):420\u2013428,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2006}, {"title": "Memory maintenance in synapses with calcium-based plasticity in the presence of background activity", "author": ["D. Higgins", "M. Graupner", "N. Brunel"], "venue": "PLOS Comput Biol, 10(10):e1003834,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2014}, {"title": "A quantitative description of membrane current and its application to conduction and excitation in nerve", "author": ["A.L. Hodgkin", "A.F. Huxley"], "venue": "The Journal of physiology, 117(4):500,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1952}, {"title": "Polychronization: computation with spikes", "author": ["E.M. Izhikevich"], "venue": "Neural computation, 18(2): 245\u2013282,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2006}, {"title": "Modeling synaptic plasticity in conjunction with the timing of pre-and postsynaptic action potentials", "author": ["W.M. Kistler", "J.L. Van Hemmen"], "venue": "Neural Computation, 12(2):385\u2013405,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2000}, {"title": "ImageNet Classification with Deep Convolutional Neural Networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "NIPS,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2012}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. Lecun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the Ieee, 86(11):2278\u20132324, Nov.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1998}, {"title": "The mnist database of handwritten digits", "author": ["Y. LeCun", "C. Cortes", "C.J. Burges"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1998}, {"title": "Deep learning", "author": ["Y. Lecun", "Y. Bengio", "G. Hinton"], "venue": "Nature, 521(7553):436\u2013444, May", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}, {"title": "deeptarget: End-to-end learning framework for microrna target prediction using deep recurrent neural networks", "author": ["B. Lee", "J. Baek", "S. Park", "S. Yoon"], "venue": "BCB, pages 434\u2013442,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2016}, {"title": "Training Deep Spiking Neural Networks using Backpropagation", "author": ["J. Lee", "T. Delbruck", "M. Pfeiffer"], "venue": "arXiv.org, page arXiv:1608.08782, Aug.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2016}, {"title": "Networks of spiking neurons - The third generation of neural network models", "author": ["W. Maass"], "venue": "Neural Networks, 10(9):1659\u20131671,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1997}, {"title": "A million spiking-neuron integrated circuit with a scalable communication network and interface", "author": ["P.A. Merolla", "J.V. Arthur", "R. Alvarez-Icaza", "A.S. Cassidy", "J. Sawada", "F. Akopyan", "B.L. Jackson", "N. Imam", "C. Guo", "Y. Nakamura"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2014}, {"title": "Deep learning in bioinformatics", "author": ["S. Min", "B. Lee", "S. Yoon"], "venue": "Briefings in Bioinformatics,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2016}, {"title": "Real-time classification and sensor fusion with a spiking deep belief network", "author": ["P. O\u2019Connor", "D. Neil", "S.-C. Liu", "T. Delbruck", "M. Pfeiffer"], "venue": "Frontiers in Neuroscience,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2013}, {"title": "deepmirgene: Deep neural network based precursor microrna prediction", "author": ["S. Park", "S. Min", "H. Choi", "S. Yoon"], "venue": "arXiv preprint arXiv:1605.00017,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2016}, {"title": "Sequence to Sequence Learning with Neural Networks", "author": ["I. Sutskever", "O. Vinyals", "Q.V. Le"], "venue": "Advances in neural information processing systems, pages 3104\u20133112,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2014}, {"title": "Fast and Efficient Asynchronous Neural Computation with Adapting Spiking Neural Networks", "author": ["D. Zambrano", "S.M. Bohte"], "venue": "arXiv.org, Sept.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 15, "context": "Recently, a breakthrough in deep learning [16, 14] has shown that, with carefully selected learning rules and regularization methods, multiple non-linear representations from deep artificial neural networks can achieve state-of-the-art performance in various tasks including image recognition [13], natural language processing [3, 25, 1], and bioinformatics [2, 17, 24, 21].", "startOffset": 42, "endOffset": 50}, {"referenceID": 13, "context": "Recently, a breakthrough in deep learning [16, 14] has shown that, with carefully selected learning rules and regularization methods, multiple non-linear representations from deep artificial neural networks can achieve state-of-the-art performance in various tasks including image recognition [13], natural language processing [3, 25, 1], and bioinformatics [2, 17, 24, 21].", "startOffset": 42, "endOffset": 50}, {"referenceID": 12, "context": "Recently, a breakthrough in deep learning [16, 14] has shown that, with carefully selected learning rules and regularization methods, multiple non-linear representations from deep artificial neural networks can achieve state-of-the-art performance in various tasks including image recognition [13], natural language processing [3, 25, 1], and bioinformatics [2, 17, 24, 21].", "startOffset": 293, "endOffset": 297}, {"referenceID": 2, "context": "Recently, a breakthrough in deep learning [16, 14] has shown that, with carefully selected learning rules and regularization methods, multiple non-linear representations from deep artificial neural networks can achieve state-of-the-art performance in various tasks including image recognition [13], natural language processing [3, 25, 1], and bioinformatics [2, 17, 24, 21].", "startOffset": 327, "endOffset": 337}, {"referenceID": 23, "context": "Recently, a breakthrough in deep learning [16, 14] has shown that, with carefully selected learning rules and regularization methods, multiple non-linear representations from deep artificial neural networks can achieve state-of-the-art performance in various tasks including image recognition [13], natural language processing [3, 25, 1], and bioinformatics [2, 17, 24, 21].", "startOffset": 327, "endOffset": 337}, {"referenceID": 0, "context": "Recently, a breakthrough in deep learning [16, 14] has shown that, with carefully selected learning rules and regularization methods, multiple non-linear representations from deep artificial neural networks can achieve state-of-the-art performance in various tasks including image recognition [13], natural language processing [3, 25, 1], and bioinformatics [2, 17, 24, 21].", "startOffset": 327, "endOffset": 337}, {"referenceID": 1, "context": "Recently, a breakthrough in deep learning [16, 14] has shown that, with carefully selected learning rules and regularization methods, multiple non-linear representations from deep artificial neural networks can achieve state-of-the-art performance in various tasks including image recognition [13], natural language processing [3, 25, 1], and bioinformatics [2, 17, 24, 21].", "startOffset": 358, "endOffset": 373}, {"referenceID": 16, "context": "Recently, a breakthrough in deep learning [16, 14] has shown that, with carefully selected learning rules and regularization methods, multiple non-linear representations from deep artificial neural networks can achieve state-of-the-art performance in various tasks including image recognition [13], natural language processing [3, 25, 1], and bioinformatics [2, 17, 24, 21].", "startOffset": 358, "endOffset": 373}, {"referenceID": 22, "context": "Recently, a breakthrough in deep learning [16, 14] has shown that, with carefully selected learning rules and regularization methods, multiple non-linear representations from deep artificial neural networks can achieve state-of-the-art performance in various tasks including image recognition [13], natural language processing [3, 25, 1], and bioinformatics [2, 17, 24, 21].", "startOffset": 358, "endOffset": 373}, {"referenceID": 20, "context": "Recently, a breakthrough in deep learning [16, 14] has shown that, with carefully selected learning rules and regularization methods, multiple non-linear representations from deep artificial neural networks can achieve state-of-the-art performance in various tasks including image recognition [13], natural language processing [3, 25, 1], and bioinformatics [2, 17, 24, 21].", "startOffset": 358, "endOffset": 373}, {"referenceID": 19, "context": "In this regard, spiking neural networks (SNNs) can provide a much more power-efficient way of implementing artificial neurons than conventional DNNs [20, 19] leveraged by their compact and sparse activity profiles.", "startOffset": 149, "endOffset": 157}, {"referenceID": 18, "context": "In this regard, spiking neural networks (SNNs) can provide a much more power-efficient way of implementing artificial neurons than conventional DNNs [20, 19] leveraged by their compact and sparse activity profiles.", "startOffset": 149, "endOffset": 157}, {"referenceID": 9, "context": "Triggered by the recent success of DNNs, the research on SNNs is gaining momentum with renewed interest: Various neuronal models [10, 7], spike encoding methods [8, 11], and learning rules [12, 6] have been proposed.", "startOffset": 129, "endOffset": 136}, {"referenceID": 6, "context": "Triggered by the recent success of DNNs, the research on SNNs is gaining momentum with renewed interest: Various neuronal models [10, 7], spike encoding methods [8, 11], and learning rules [12, 6] have been proposed.", "startOffset": 129, "endOffset": 136}, {"referenceID": 7, "context": "Triggered by the recent success of DNNs, the research on SNNs is gaining momentum with renewed interest: Various neuronal models [10, 7], spike encoding methods [8, 11], and learning rules [12, 6] have been proposed.", "startOffset": 161, "endOffset": 168}, {"referenceID": 10, "context": "Triggered by the recent success of DNNs, the research on SNNs is gaining momentum with renewed interest: Various neuronal models [10, 7], spike encoding methods [8, 11], and learning rules [12, 6] have been proposed.", "startOffset": 161, "endOffset": 168}, {"referenceID": 11, "context": "Triggered by the recent success of DNNs, the research on SNNs is gaining momentum with renewed interest: Various neuronal models [10, 7], spike encoding methods [8, 11], and learning rules [12, 6] have been proposed.", "startOffset": 189, "endOffset": 196}, {"referenceID": 5, "context": "Triggered by the recent success of DNNs, the research on SNNs is gaining momentum with renewed interest: Various neuronal models [10, 7], spike encoding methods [8, 11], and learning rules [12, 6] have been proposed.", "startOffset": 189, "endOffset": 196}, {"referenceID": 21, "context": "[23] mapped a trained deep belief network to an SNN by using the Siegert mean-firing-rate approximation model for integrate-and-fire spiking neurons.", "startOffset": 0, "endOffset": 4}, {"referenceID": 4, "context": "[5] trained a DNN with the stochastic gradient descent (SGD) and then converted it to an SNN based on parameter optimization.", "startOffset": 0, "endOffset": 3}, {"referenceID": 24, "context": "Zambrano and Bohte [26] used the asynchronous pulsed sigma-delta coding for spike trains with adaptive dynamic range in the membrane potential.", "startOffset": 19, "endOffset": 23}, {"referenceID": 17, "context": "[18] reported that they could train SNNs with a differentiable activation function, their work incurred additional computation overhead for handling activation and gradients, thus discouraging the deployment of their method for neuromorphic hardware with limited resources.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "Figure 1a shows the training results (error rates versus epochs) obtained from training an SNN implemented as a DSN using SGD with the MNIST data [15] for three different time periods.", "startOffset": 146, "endOffset": 150}, {"referenceID": 3, "context": "From a neuroscience point of view, these phenomena correspond to the long-term potentiation where a persistent strengthening of synapses occurs between neurons [4], resulting in higher spikes.", "startOffset": 160, "endOffset": 163}, {"referenceID": 8, "context": "In addition, if we interpret the initialized values as a trace of background activities of the network, they interfere with memory maintenance in synapses by leading a synaptic efficacy to faster decay [9], inhibiting the long-term potentiation, i.", "startOffset": 202, "endOffset": 205}], "year": 2016, "abstractText": "Nowadays deep learning is dominating the field of machine learning with state-ofthe-art performance in various application areas. Recently, spiking neural networks (SNNs) have been attracting a great deal of attention, notably owning to their power efficiency, which can potentially allow us to implement a low-power deep learning engine suitable for real-time/mobile applications. However, implementing SNN-based deep learning remains challenging, especially gradient-based training of SNNs by error backpropagation. We cannot simply propagate errors through SNNs in conventional way because of the property of SNNs that process discrete data in the form of a series. Consequently, most of the previous studies employ a workaround technique, which first trains a conventional weighted-sum deep neural network and then maps the learning weights to the SNN under training, instead of training SNN parameters directly. In order to eliminate this workaround, recently proposed is a new class of SNN named deep spiking networks (DSNs), which can be trained directly (without a mapping from conventional deep networks) by error backpropagation with stochastic gradient descent. In this paper, we show that the initialization of the membrane potential on the backward path is an important step in DSN training, through diverse experiments performed under various conditions. Furthermore, we propose a simple and efficient method that can improve DSN training by controlling the initial membrane potential on the backward path. In our experiments, adopting the proposed approach allowed us to boost the performance of DSN training in terms of converging time and accuracy.", "creator": "LaTeX with hyperref package"}}}