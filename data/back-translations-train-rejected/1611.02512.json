{"id": "1611.02512", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Nov-2016", "title": "Cognitive Discriminative Mappings for Rapid Learning", "abstract": "Humans can learn concepts or recognize items from just a handful of examples, while machines require many more samples to perform the same task. In this paper, we build a computational model to investigate the possibility of this kind of rapid learning. The proposed method aims to improve the learning task of input from sensory memory by leveraging the information retrieved from long-term memory. We present a simple and intuitive technique called cognitive discriminative mappings (CDM) to explore the cognitive problem. First, CDM separates and clusters the data instances retrieved from long-term memory into distinct classes with a discrimination method in working memory when a sensory input triggers the algorithm. CDM then maps each sensory data instance to be as close as possible to the median point of the data group with the same class. The experimental results demonstrate that the CDM approach is effective for learning the discriminative features of supervised classifications with few training sensory input instances.", "histories": [["v1", "Tue, 8 Nov 2016 13:26:32 GMT  (292kb,D)", "http://arxiv.org/abs/1611.02512v1", null]], "reviews": [], "SUBJECTS": "cs.AI cs.LG cs.NE", "authors": ["wen-chieh fang", "yi-ting chiang"], "accepted": false, "id": "1611.02512"}, "pdf": {"name": "1611.02512.pdf", "metadata": {"source": "CRF", "title": "Cognitive Discriminative Mappings for Rapid Learning", "authors": ["Wen-Chieh Fang", "Yi-ting Chiang"], "emails": [], "sections": [{"heading": null, "text": "The proposed method aims to improve the learning task of sensory memory input by using the information gained from long-term memory. We present a simple and intuitive technique called cognitive discriminative mapping (CDM) to investigate the cognitive problem. First, CDM separates and clusters the data instances retrieved from long-term memory into different classes with a discriminatory method in working memory when a sensory input triggers the algorithm. CDM then maps each sensory data input as close as possible to the median point of the same class. Experimental results show that the CDM approach is effective in learning the discriminatory features of monitored classifications with a few training-related sensory input examples."}, {"heading": "1 Introduction", "text": "We are interested in understanding the relationships between all levels that describe what a brain does. Adolphs have the top 23 unsolved problems in neuroscience (including three \"meta\" problems) [1]. Two important questions, \"How does sensory transformation work?\" and \"How can learning and memory work?,\" are closely related [1]. Without sensory transduction, which transforms a sensory stimulus from one form to another, the brain cannot integrate and process sensory input information. Humans can learn a new concept or recognize an object from just a handful of examples, while the statue of machine learning typically requires dozens or hundreds of examples to achieve similar accuracy."}, {"heading": "2 Problem Definition", "text": "Assuming that we have a labeled dataset of NDL points: {(xi, li)} NLi = 1. The class li of each data instance xi-Rm is in class set CL. We refer to this dataset as LTM dataset DL. Let there be another dataset of interest. We refer to it as SM dataset DS, which has few labeled data instances {(yi, li)} NSi = 1. Each instance yi-Rn. The class li is in class set CS. The class set CS is assumed to be the same CL. The LTM and SM datasets are in different attribute spaces. In most cases, the number of dimensions of the two input datasets is also different. Our goal is to build a classifier by using the small data {(yi, li)} NSi = 1 of SM and the relatively large data sets (xi, li) NLi = 1 of DL."}, {"heading": "2.1 Lemma", "text": "Lemma 1. A group of clusters C should be fragmented in pairs if and only if for each CL, CS, CS, C, rCL and rCS are the radii of CL and CS respectively; thend (CL, CS) > rCL + rCS, (1) where d (CL, CS) is the distance between the median points of the two clusters CL and CS."}, {"heading": "2.2 Hypothesis", "text": "We present the following main theoretical hypothesis: We believe that the proposed hypothesis provides a promising theoretical basis for the development of the algorithm. Hypothesis 1. Since there is an SM domain DS, a sample DS comes from DS. If we can find an example set DL in the LTM domain DL, and two mapping functions f and g are such that the following conditions are met: 1. Each data instance xi-DL with class l and each data instance yj-DS with the same class l are mapped into a common discriminatory cluster corresponding to the same class l in a new space. 2. These discriminatory clusters are laboriously incompatible.Then at least the hypothesis h-H (H is a family of hypotheses in DS) exists that the empirical error rate \u0445U-V (h) \u2264 \u0432-V (h) (2) where U is the projected LTM sample in the new space, and the jempirical sample helps the new sampling space with irical sampling."}, {"heading": "3 Proposed Solution", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Main Idea", "text": "Since the data of LTM and SM are located in different attribute spaces, it is desirable to find a common invariant attribute space in which all data can be directly compared. Inspired by the min-max principle [5] and class-based constraints [8], we first define two variables to map the LTM and SM data in the order in which class constraints between the transformed points can be met. To learn two transformation functions f and g for SM data classification, we first define two variables named \"S\" and \"D\" as follows: \"S\" (f, g) = \"i,\" \"i,\" i, \"i,\" i \"SM,\" li, \"i,\" i, \"i,\" i, \"\" LTM, \"li\" and \"L.\""}, {"heading": "3.2 Our Solution", "text": "We present f and g as two linear transformation functions, namely P and Q. That is, f (xi) = Pxi and g (yj) = Qyj. We present a computerized mapping model to use both LTM and SM data. In the new memory, this transformed data is divided into clusters according to their classes. Fig. 1 illustrates the main idea of the proposed model. We can estimate the two linear transformations P and Q by first deciding on a transformation. Most of the time, there are many more contiguous samples of LTM than there are sensory cases in SM. Consequently, we use the mapping function P: X \u2192 U to project the LTM data X."}, {"heading": "3.3 Feature Augmentation", "text": "Daum\u00e9 III provides a method of feature augmentation to integrate more information [3]. For each instance, we use the feature augmentation method to add the original features to the new features in order to render the instance [3]. We define two feature mapping functions \u03c6L (x) = [(HPx) >, 0 > n] > and \u03c6S (y) = [(HQy) >, y >] > for the data from LTM and SM respectively. Here, 0n stands for zero column vectors of dimensions n. Including zeroes in the feature representations ensures that the dimensions of the instances from LTM and SM are the same. Also, the entire data set from LTM has no information about the features from SM. Therefore, it makes sense to set zero values (0 > n) in the later n feature dimensions."}, {"heading": "3.4 Proposed Algorithmic Procedure", "text": "In summary, we specify the algorithm of the proposed method: 1. Use a projection approach to learn a mapping P to project data retrieved from LTM into a new d-dimensional space in memory. All projected data in the new room will be grouped into several class-appropriate clusters.2. Calculate the generalized geometric median of all clusters.3. Calculate the mapping function Q for data in SM using these generic geometric medians. 4. Make Q to map the SM data for training and new incoming SM data to the new d-dimensional space. 5. Use a discrimination method again to construct a transformation matrix H to separate all instances into different classes. 6. Complete the learned features with the original SM characteristics to represent the instance. 7. Using the projected data with known classes from LTM, the new learning data from the machine and SM can be used as a conventional training injection."}, {"heading": "4 Experimental Results", "text": "In this section, we perform several experiments on two benchmark datasets. We assume that there is only one LTM dataset and one SM dataset. Since the upper limit of the number of dimensions obtained in the LDA is c \u2212 1 (c is the number of classes), we do not apply the dimension of the new space in the first projection for learning P and the final projection on c \u2212 1. Cross-validation is not applicable for selecting the weight of the regularization term \u03b7 due to the small number of training instances from the SM domain. Therefore, we adjust the parameters to a predefined range and specify the optimal parameter value. In this essay, we report the results when \u03b7 = 1. We apply a k nearest classifier (kNN, k = 5) and a Support Vector Machine with a Radial Basis Function Kernel (RBF SVM) to train the final classifiers on the data sets as well as the M training instances."}, {"heading": "4.1 Data Sets", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1.1 Object Recognition Data Set", "text": "The first dataset contains 4,652 images from 31 category2, which come from the following three areas: Amazon (images downloaded from an online retail website), dslr (high-resolution images taken by a DLR digital camera) and webcam (low-resolution images taken by a web camera) [8]. SURF features are extracted for all images. Amazon and webcam images are used as data from LTM. We randomly select 28 training images per category for the Amazon or webcam data sets. We then randomly select k training images per category for the dslr data set as SM data, using k = 3, 4, 5 and 6. The remaining images for the test. Table 1 shows a summary of the data set."}, {"heading": "4.1.2 Text Categorization Data Set", "text": "The second data set is a subset of the Reuters RCV1 / RCV2 collections [2]. It contains newswire articles written in English, French, German, Italian and Spanish. There are six classes for these articles: C15, CCAT, E21, ECAT, GCAT and M11. We take Spanish articles as SM data and articles written in the other four languages as individual data sets retrieved from LTM. For each class, a random sample of one hundred training instances from the LTM and k training instances data is used as test instances, with k = 5, 7, 10, 15 and 20. We also perform a Principal Components Analysis (PCA), in which 60% of the energy is retained on the TF-IDF characteristics. The remaining instances of SM data are used as test instances. Table 2 shows a summary of the data set, and Table 3 shows the distribution of classes within the data set."}, {"heading": "4.2 Comparative Studies", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.2.1 Object Recognition Data Set", "text": "Table 4 shows the performance of the basic approach and the CDM approach in different settings on the object recognition dataset. The results show that the CDM approach, using a kNN classifier, performs excellently in the object recognition dataset, both with and without augmentation. In the case of the RBF SVM classifier, although the functions learned are not good enough to train a predictive model, supplementing the learned functions with the original functions of SM improves performance.1Visit http: / / vision.cs.uml.edu / adaptation.html for more details. 2These 31 categories are: backpack, bicycle, bicycle helmet, bookcase, bottle, calculator, computer, desk chair, desk lamp, filing cabinet, headphones, keyboard, laptop, letterbox, mobile phone, monitor, mouse, mug, notebook, pen, printer, ring binder, speaker, folder, speaker, folder, and folder."}, {"heading": "4.2.2 Text Categorization Data Set", "text": "Table 5 shows the performance of both the base and CDM approach in different settings of the text categorization dataset. Both the KNN and RBF-SVM classifiers with and without augmentation perform better than the base approach."}, {"heading": "4.3 Influence of the Number of Training Samples per Class from SM domain", "text": "Figure 2 shows the accuracy of the baseline and the proposed CDM approach in terms of the number of training samples per class for each data set from the SM. For both the CDM and baseline approach, we use the RBF SVM as classifier. As shown in Figure 2a and Figure 2b, the accuracy of the baseline approach and CDM increases when we use a larger k."}, {"heading": "5 Conclusions", "text": "CDM has the following outstanding features: a) CDM is elegant, intuitive, and efficient; and b) CDM has a complete theoretical architecture; the experimental results show that we can find at least one hypothesis or classifier that meets our fast learning hypothesis; one-shot learning or fast learning is still a work in progress; and based on CDM experience, we believe that cognitive scientists should be obliged to pay attention to the role of the information gained from LTM in fast learning; from the experimental results, we have found that performance enhancement has not always been a guarantee; it is worth exploring this problem in the future. In CDM, since we want to create a common metric space for LTM and SM, we propose global learning in working memory; unfortunately, this results in a CDM constraint. CDM is trying to find mapping matrices that minimize the sum of all irrational problem areas, when the data gaps between the data classes differ depending on the extent of the data classes."}], "references": [{"title": "The unsolved problems of neuroscience", "author": ["Ralph Adolphs"], "venue": "Trends in Cognitive Sciences,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2015}, {"title": "Learning from multiple partially observed views - an application to multilingual text categorization", "author": ["Massih-Reza Amini", "Nicolas Usunier", "Cyril Goutte"], "venue": "In Proceedings of the 23rd Annual Conference on Advances in Neural Information Processing Systems", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2009}, {"title": "Frustratingly easy domain adaptation", "author": ["Halneigh Daum\u00e9 III"], "venue": "In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2007}, {"title": "Introduction to statistical pattern recognition", "author": ["Keinosuke Fukunaga"], "venue": "Academic Press Professional,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1990}, {"title": "Semi-supervised distance metric learning for collaborative image retrieval", "author": ["Steven C.H. Hoi", "Wei Liu", "Shih-Fu Chang"], "venue": "In Proceedings of the 21st IEEE Conference on Computer Vision and Pattern Recognition", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2008}, {"title": "Human-level concept learning through probabilistic program induction", "author": ["Brenden Lake", "Ruslan Salakhutdinov", "Joshua Tenenbaum"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2015}, {"title": "A survey on transfer learning", "author": ["Sinno Jialin Pan", "Qiang Yang"], "venue": "IEEE Transactions on Knowledge and Data Engineering,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2010}, {"title": "Adapting visual category models to new domains", "author": ["Kate Saenko", "Brian Kulis", "Mario Fritz", "Trevor Darrell"], "venue": "In Proceedings of the 11th European Conference on Computer Vision (ECCV\u201910),", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2010}, {"title": "Cognitive Psychology", "author": ["Robert J. Sternberg", "Karin Sternberg"], "venue": "Wadsworth Publishing, sixth edition,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2011}, {"title": "Features and objects in visual processing", "author": ["Anne Treisman"], "venue": "Scientific American,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1986}, {"title": "Solutions to the binding problem: progress through controversy", "author": ["Anne Treisman"], "venue": "and convergence. Neuron,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1999}, {"title": "How one-shot learning unfolds in the brain", "author": ["Janelle Weaver"], "venue": "PLOS Biology,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2015}, {"title": "Graph embedding and extensions: A general framework for dimensionality reduction", "author": ["Shuicheng Yan", "Dong Xu", "Benyu Zhang", "Hong-Jiang Zhang", "Qiang Yang", "Stephen Lin"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2007}], "referenceMentions": [{"referenceID": 0, "context": "Adolphs listed the top 23 unsolved problems in neuroscience (including three \"meta\" issues) [1].", "startOffset": 92, "endOffset": 95}, {"referenceID": 0, "context": "Two important questions, \"How does sensory transduction work?\" and \"How does learning and memory work?\", are closely connected [1].", "startOffset": 127, "endOffset": 130}, {"referenceID": 5, "context": "People can learn a new concept or recognize an item from just a handful of examples, while stateof-the-art machine learning algorithms typically require tens or hundreds of examples to perform with similar accuracy [6].", "startOffset": 215, "endOffset": 218}, {"referenceID": 11, "context": "Scientists have long suspected that this type of \"one-shot learning,\" or rapid learning, involves a different mechanism in the brain than gradual learning [12].", "startOffset": 155, "endOffset": 159}, {"referenceID": 8, "context": "Humans have five main senses: sight, hearing, taste, smell, and touch [9].", "startOffset": 70, "endOffset": 73}, {"referenceID": 9, "context": "Feature integration theory tackles the question of how humans perceive individual features as part of the same object by proposing a two-stage process: preattentive processing and focused attention processing [10] [11].", "startOffset": 209, "endOffset": 213}, {"referenceID": 10, "context": "Feature integration theory tackles the question of how humans perceive individual features as part of the same object by proposing a two-stage process: preattentive processing and focused attention processing [10] [11].", "startOffset": 214, "endOffset": 218}, {"referenceID": 6, "context": "According to the definition in [7], given two domains, LTM domain DL = {X , P (X)} and SM domain DS = {Y, P (Y )}, where X , Y are feature spaces and P (X), P (Y ) are marginal probability distributions, X = {xi} i=1 \u2208 X and Y = {yi} NS i=1 \u2208 Y .", "startOffset": 31, "endOffset": 34}, {"referenceID": 4, "context": "Inspired by the min-max principle [5] and class-based constraints [8], we apply two transformations to map the LTM and SM data in order, to satisfy class constraints between the transformed points.", "startOffset": 34, "endOffset": 37}, {"referenceID": 7, "context": "Inspired by the min-max principle [5] and class-based constraints [8], we apply two transformations to map the LTM and SM data in order, to satisfy class constraints between the transformed points.", "startOffset": 66, "endOffset": 69}, {"referenceID": 3, "context": "In the second approach, we apply Linear discriminant analysis (LDA) [4] to characterize or separate the classes of instances.", "startOffset": 68, "endOffset": 71}, {"referenceID": 12, "context": "In the third approach, a linear transformation P is derived by minimizing a cost function similar to Graph embedding method (GE) [13] as follows:", "startOffset": 129, "endOffset": 133}, {"referenceID": 2, "context": "Daum\u00e9 III provides a feature augmentation method to integrate more information [3].", "startOffset": 79, "endOffset": 82}, {"referenceID": 2, "context": "For each instance, we utilize the feature augmentation method to augment the new features with the original features to represent the instance [3].", "startOffset": 143, "endOffset": 146}, {"referenceID": 7, "context": "1 Object Recognition Data Set The first data set1 contains 4, 652 images from 31 categories2 originating from the following three domains: Amazon (images downloaded from an online retail website), dslr (high-resolution images taken from a digital DLR camera) and webcam (low-resolution images taken from a web camera) [8].", "startOffset": 318, "endOffset": 321}, {"referenceID": 1, "context": "2 Text Categorization Data Set The second data set is a subset of the Reuters RCV1/RCV2 collections [2].", "startOffset": 100, "endOffset": 103}], "year": 2016, "abstractText": "Humans can learn concepts or recognize items from just a handful of examples, while machines require many more samples to perform the same task. In this paper, we build a computational model to investigate the possibility of this kind of rapid learning. The proposed method aims to improve the learning task of input from sensory memory by leveraging the information retrieved from long-term memory. We present a simple and intuitive technique called cognitive discriminative mappings (CDM) to explore the cognitive problem. First, CDM separates and clusters the data instances retrieved from long-term memory into distinct classes with a discrimination method in working memory when a sensory input triggers the algorithm. CDM then maps each sensory data instance to be as close as possible to the median point of the data group with the same class. The experimental results demonstrate that the CDM approach is effective for learning the discriminative features of supervised classifications with few training sensory input instances.", "creator": "LaTeX with hyperref package"}}}