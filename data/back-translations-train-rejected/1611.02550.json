{"id": "1611.02550", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Nov-2016", "title": "Discriminative Acoustic Word Embeddings: Recurrent Neural Network-Based Approaches", "abstract": "Acoustic word embeddings --- fixed-dimensional vector representations of variable-length spoken word segments --- have begun to be considered for tasks such as speech recognition and query-by-example search. Such embeddings can be learned discriminatively so that they are similar for speech segments corresponding to the same word, while being dissimilar for segments corresponding to different words. Recent work has found that acoustic word embeddings can outperform dynamic time warping on query-by-example search and related word discrimination tasks. However, the space of embedding models and training approaches is still relatively unexplored. In this paper we present new discriminative embedding models based on recurrent neural networks (RNNs). We consider training losses that have been successful in prior work, in particular a cross entropy loss for word classification and a contrastive loss that explicitly aims to separate same-word and different-word pairs in a \"Siamese network\" training setting. We find that both classifier-based and Siamese RNN embeddings improve over previously reported results on a word discrimination task, with Siamese RNNs outperforming classification models. In addition, we present analyses of the learned embeddings and the effects of variables such as dimensionality and network structure.", "histories": [["v1", "Tue, 8 Nov 2016 15:13:19 GMT  (328kb,D)", "http://arxiv.org/abs/1611.02550v1", "To appear at SLT 2016"]], "COMMENTS": "To appear at SLT 2016", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["shane settle", "karen livescu"], "accepted": false, "id": "1611.02550"}, "pdf": {"name": "1611.02550.pdf", "metadata": {"source": "CRF", "title": "DISCRIMINATIVE ACOUSTIC WORD EMBEDDINGS: RECURRENT NEURAL NETWORK-BASED APPROACHES", "authors": ["Shane Settle", "Karen Livescu"], "emails": ["klivescu}@ttic.edu"], "sections": [{"heading": null, "text": "In most cases, words are divided into subword units such as phones and models. An alternative, which has been considered by some researchers, is to consider each individual word segment as a unit, without assigning parts of it to sub-word units. A motivation for using full-word models is that they are supported by a faculty of research and development. Opinions expressed in this work are those of the authors, which do not necessarily reflect the views of the funding agencies, which is helpful because, despite decades of work, they are based on subword models."}, {"heading": "3.1. Training", "text": "In this case, the last level of G (X) is a logbook level where you can move to a different level from another, where you limit the subset of training that has a sufficient number of segments per word to form an approximately equal number of words, and where the output dimensions are the same as the number of words (but see also: the study of dimensions in such a model is limited).This model is trained and optimized by us end to end. Although the marked data is necessarily limited, the hope is that the learned models will also be applied to spoken words."}, {"heading": "4.1. Classification network details", "text": "Our classifier-based embedding uses LSTM or GRU networks with 2-4 stacked layers and 1-3 fully connected layers. The final embedding of the dimensionality corresponds to the number of unique word labels in the training set, which is 1061. The recurring hidden dimensionality of the state is fixed at 512 and the dropout [33] between the stacked recurring layers is most likely p = 0.3. The fully connected hidden layer dimensionality is fixed at 1024. Reflected linear units (ReLU) Nonlinearity and dropout with p = 0.5 are used between fully connected layers. However, between the last recurrent hidden layer and the first fully connected layer, no nonlinearity or dropout is applied. These settings were made by experiments in the development set.The classifier network is trained with a cross-entropy loss and is optimized with this learning factor of 34%."}, {"heading": "4.2. Siamese network details", "text": "For experiments with Siamese networks, we compared the nets with the concerted classification network that we all provided earlier to remove a uniform log softmax layer and replace it with a linear layer of size corresponding to the desired embedding of dimensionality. We examined embedding with dimensionalities between dimensionalities. We thank Herman Kamper for supporting with the data and evaluations. We use a margin of 0.4 in the same class (a pair of the same values). In the formation of Siamese networks, each training minibatch consists of 2B trebles. B trebles are of the form (xa, xs, xs, xs) where xa and xs are examples of the same class (a pair of the 100k same word pairs) and xd is a randomly sampled example of another class. Then, for each of these B trebles (xs, xs, additional group, xd, xd, xplet) we are a mini (xd, xd)."}, {"heading": "5.1. Effect of model structure", "text": "However, the best performance in this experiment is achieved by the LSTM network with S = F = 3 to enable more experiments and analysis within a reasonable time. 2Yuan et al. [40] have recently managed to further improve performance by adding even more layers of both types by setting the model to S = F = 3 to allow more experiments and analysis within a reasonable time. 2Yuan et al. [40] were able to further improve AP on this test set with CNN embedding by using a large set of additional (cross-lingual) training data. We do not consider these results comparable due to their dependence on additional data. Table 2 shows an interesting trend. If only one fully bonded layer is used, the GRU networks outperform the LSTMs with a sufficient number of stacked layers. On the other hand, we add tightly bonded layers 4 layers bonded to the fully bonded layers."}, {"heading": "5.2. Effect of embedding dimensionality", "text": "For the Siamese networks, we varied the output embedding of dimensionality, as shown in Fig. 2. This analysis shows that the embedding learned from the Siamese RNN network is relatively robust to reduced, exceeds the classification model for all dimensions 32 or higher, and exceeds the previously reported developer performance with CNN-based embedding [14] for all dimensions \u2265 16."}, {"heading": "5.3. Effect of training vocabulary", "text": "One might expect that the learned embeddings in words seen in training are more accurate than in words that are not. Fig. 2 measures this effect by showing the performance depending on the number of occurrences of dev words in the training set. In fact, both model types are much more successful with words that occur in vocabulary, and their performance improves the training frequency of the words. However, the performance for the Siamese network increases faster than for the classifier with increasing training frequency. This could be due to the fact that a word type that occurs at least k times in the classifier training set occurs at least 2 x (k 2) times in the paired training data of Siam."}, {"heading": "5.4. Visualization of embeddings", "text": "In order to achieve a better qualitative understanding of the differences between individual countries, it is necessary that individual countries agree on a common language. \"We must agree on a common language,\" he said. \"We must agree on a common language.\" \"We must agree on a common language.\" \"We must agree on a common language.\" \"We must agree on a common language.\" \"We must agree on a common language.\" \"We must agree on a common language.\" \"We must agree on a common language.\" \"We must agree on a common language.\" \"We must agree on a common language.\" \"We must agree on a common language.\" \"We must agree on a common language.\" \"We must agree on a common language.\" \"We must agree on a common language.\" \"We must understand on a language.\" We must understand on a language. \"We must understand on a language.\""}], "references": [{"title": "Moving Beyond the \u2019Beads-on-a-String\u2019 Model of Speech", "author": ["M. Ostendorf"], "venue": "IEEE Automatic Speech Recognition & Understanding (ASRU), 1999.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1999}, {"title": "Subword modeling for automatic speech recognition: Past, present, and emerging approaches", "author": ["Karen Livescu", "Eric Fosler-Lussier", "Florian Metze"], "venue": "IEEE Signal Processing Magazine, vol. 29, no. 6, pp. 44\u201357, 2012.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2012}, {"title": "Template-based continuous speech recognition", "author": ["Mathias De Wachter", "Mike Matton", "Kris Demuynck", "Patrick Wambacq", "Ronald Cools", "Dirk Van Compernolle"], "venue": "IEEE Trans. Audio, Speech, Language Process., vol. 15, no. 4, pp. 1377\u20131390, 2007.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2007}, {"title": "Investigations on exemplarbased features for speech recognition towards thousands of hours of unsupervised, noisy data", "author": ["Georg Heigold", "Patrick Nguyen", "Mitchel Weintraub", "Vincent Vanhoucke"], "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2012, pp. 4437\u20134440.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2012}, {"title": "The spoken web search task at MediaEval 2012", "author": ["Florian Metze", "Xavier Anguera", "Etienne Barnard", "Marelie Davel", "Guillaume Gravier"], "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2013.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2013}, {"title": "Speaker independent discriminant feature extraction for acoustic pattern-matching", "author": ["Xavier Anguera"], "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2012.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2012}, {"title": "Fast spoken query detection using lower-bound dynamic time warping on graphical processing units", "author": ["Yaodong Zhang", "Kiarash Adl", "James Glass"], "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2012, pp. 5173\u20135176.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2012}, {"title": "Coping with channel mismatch in query-by-example - BUT QUESST 2014", "author": ["Igor Sz\u00f6ke", "Miroslav Sk\u00e1cel", "Luk\u00e1s\u0306 Burget", "Jan \u201cHonza\u201d C\u0306ernock\u00fd"], "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2015.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2015}, {"title": "Word-level acoustic modeling with convolutional vector regression", "author": ["Andrew L Maas", "Stephen D Miller", "Tyler M O\u2019neil", "Andrew Y Ng", "Patrick Nguyen"], "venue": "International Conference on Machine Learning (ICML), Representation Learning Workshop, 2012.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2012}, {"title": "Word embeddings for speech recognition", "author": ["Samy Bengio", "Georg Heigold"], "venue": "Interspeech, 2014.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2014}, {"title": "Fixed-dimensional acoustic embeddings of variable-length segments in low-resource settings", "author": ["Keith Levin", "Katharine Henry", "Aren Jansen", "Karen Livescu"], "venue": "IEEE Automatic Speech Recognition & Understanding (ASRU), 2013.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2013}, {"title": "Segmental acoustic indexing for zero resource keyword search", "author": ["Keith Levin", "Aren Jansen", "Benjamin Van Durme"], "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2015.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep convolutional acoustic word embeddings using word-pair side information", "author": ["Herman Kamper", "Weiran Wang", "Karen Livescu"], "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2016, pp. 4950\u20134954.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2016}, {"title": "Unsupervised learning of audio segment representations using sequence-to-sequence recurrent neural networks", "author": ["Yu-An Chung", "Chao-Chung Wu", "Chia-Hao Shen", "Hung-Yi Lee"], "venue": "Interspeech, 2016.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2016}, {"title": "Word-level invariant representations from acoustic waveforms", "author": ["Stephen Voinea", "Chiyuan Zhang", "Georgios Evangelopoulos", "Lorenzo Rosasco", "Tomaso Poggio"], "venue": "Interspeech, 2014, pp. 2385\u20132389.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2014}, {"title": "Query-by-example keyword spotting using long shortterm memory networks", "author": ["Guoguo Chen", "Carolina Parada", "Tara N Sainath"], "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2015.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2015}, {"title": "A segmental CRF approach to large vocabulary continuous speech recognition", "author": ["Geoffrey Zweig", "Patrick Nguyen"], "venue": "IEEE Automatic Speech Recognition & Understanding (ASRU), 2009, pp. 152\u2013157.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2009}, {"title": "Unsupervised lexical clustering of speech segments using fixed-dimensional acoustic embeddings", "author": ["Herman Kamper", "Aren Jansen", "Simon King", "Sharon Goldwater"], "venue": "IEEE Spoken Language Technology Workshop (SLT), 2014, pp. 100\u2013105.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2014}, {"title": "Fully unsupervised small-vocabulary speech recognition using a segmental bayesian model", "author": ["Herman Kamper", "Aren Jansen", "Sharon Goldwater"], "venue": "Interspeech, 2015.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2015}, {"title": "Rapid evaluation of speech representations for spoken term discovery", "author": ["Michael A Carlin", "Samuel Thomas", "Aren Jansen", "Hynek Hermansky"], "venue": "Interspeech, 2011.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2011}, {"title": "Unsupervised neural network based feature extraction using weak top-down constraints", "author": ["H. Kamper", "M. Elsner", "A. Jansen", "S.J. Goldwater"], "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2015.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2015}, {"title": "Weak top-down constraints for unsupervised acoustic model training", "author": ["Aren Jansen", "Samuel Thomas", "Hynek Hermansky"], "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2013.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2013}, {"title": "Long shortterm memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation, vol. 9, no. 8, pp. 1735\u20131780, 1997.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 1997}, {"title": "Empirical evaluation of gated recurrent neural networks on sequence modeling", "author": ["Junyoung Chung", "Caglar Gulcehre", "KyungHyun Cho", "Yoshua Bengio"], "venue": "Neural Information Processing Systems (NIPS), 2014.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2014}, {"title": "Speech recognition with deep recurrent neural networks", "author": ["Alex Graves", "Abdel-rahman Mohamed", "Geoffrey Hinton"], "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2013, pp. 6645\u20136649.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2013}, {"title": "Long short-term memory based recurrent neural network architectures for large vocabulary speech recognition", "author": ["Ha\u015fim Sak", "Andrew Senior", "Fran\u00e7oise Beaufays"], "venue": "arXiv preprint arXiv:1402.1128, 2014.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2014}, {"title": "Attention-based models for speech recognition", "author": ["Jan K Chorowski", "Dzmitry Bahdanau", "Dmitriy Serdyuk", "Kyunghyun Cho", "Yoshua Bengio"], "venue": "Neural Information Processing Systems (NIPS), 2015, pp. 577\u2013585.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2015}, {"title": "A study of the recurrent neural network encoder-decoder for large vocabulary speech recognition", "author": ["Liang Lu", "Xingxing Zhang", "Kyunghyun Cho", "Steve Renals"], "venue": "Interspeech, 2015.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2015}, {"title": "Rectified linear units improve restricted boltzmann machines", "author": ["Vinod Nair", "Geoffrey E Hinton"], "venue": "International Conference on Machine Learning (ICML), 2010, pp. 807\u2013814.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2010}, {"title": "Signature verification using a \u2018Siamese\u2019 time delay neural network", "author": ["Jane Bromley", "James W Bentz", "L\u00e9on Bottou", "Isabelle Guyon", "Yann LeCun", "Cliff Moore", "Eduard S\u00e4ckinger", "Roopak Shah"], "venue": "Int. J. Pattern Rec., vol. 7, no. 4, pp. 669\u2013688, 1993.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 1993}, {"title": "Switchboard: Telephone speech corpus for research and development", "author": ["John J Godfrey", "Edward C Holliman", "Jane Mc- Daniel"], "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 1992, vol. 1, pp. 517\u2013520.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 1992}, {"title": "Dropout: a simple way to prevent neural networks from overfitting", "author": ["Nitish Srivastava", "Geoffrey E Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov"], "venue": "Journal of Machine Learning Research, vol. 15, no. 1, pp. 1929\u20131958, 2014.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 1929}, {"title": "A method of solving a convex programming problem with convergence rate O(1/k2)", "author": ["Yurii Nesterov"], "venue": ".", "citeRegEx": "34", "shortCiteRegEx": null, "year": 0}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["John Duchi", "Elad Hazan", "Yoram Singer"], "venue": "Tech. Rep. UCB/EECS-2010-24, EECS Department, University of California, Berkeley, Mar 2010.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2010}, {"title": "ADADELTA: an adaptive learning rate method", "author": ["Matthew D. Zeiler"], "venue": "CoRR, vol. abs/1212.5701, 2012.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2012}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik P. Kingma", "Jimmy Ba"], "venue": "CoRR, vol. abs/1412.6980, 2014.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2014}, {"title": "Torch7: A matlab-like environment for machine learning", "author": ["Ronan Collobert", "Koray Kavukcuoglu", "Cl\u00e9ment Farabet"], "venue": "BigLearn, Neural Information Processing (NIPS) Workshop, 2011, number EPFL-CONF- 192376.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2011}, {"title": "rnn : Recurrent library for torch", "author": ["Nicholas L\u00e9onard", "Sagar Waghmare", "Yang Wang", "Jin-Hwa Kim"], "venue": "CoRR, vol. abs/1511.07889, 2015.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning neural network representations using cross-lingual bottleneck features with word-pair information", "author": ["Yougen Yuan", "Cheung-Chi Leung", "Lei Xie", "Bin Ma", "Haizhou Li"], "venue": "Interspeech, 2016.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2016}, {"title": "Visualizing data using t-sne", "author": ["Laurens van der Maaten", "Geoffrey Hinton"], "venue": "Journal of Machine Learning Research, vol. 9, no. Nov, pp. 2579\u20132605, 2008.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2008}], "referenceMentions": [{"referenceID": 0, "context": "This is helpful since, despite decades of work on sub-word modeling [1, 2], it still poses significant challenges.", "startOffset": 68, "endOffset": 74}, {"referenceID": 1, "context": "For example, speech processing systems are still hampered by differences in conversational pronunciations [3].", "startOffset": 106, "endOffset": 109}, {"referenceID": 2, "context": "For example, in template-based speech recognition [4, 5], word scores are computed from dynamic time warping (DTW) distances between an observed segment and training segments of the hypothesized word.", "startOffset": 50, "endOffset": 56}, {"referenceID": 3, "context": "For example, in template-based speech recognition [4, 5], word scores are computed from dynamic time warping (DTW) distances between an observed segment and training segments of the hypothesized word.", "startOffset": 50, "endOffset": 56}, {"referenceID": 4, "context": "In queryby-example search, putative matches are typically found by measuring the DTW distance between the query and segments of the search database [6, 7, 8, 9].", "startOffset": 148, "endOffset": 160}, {"referenceID": 5, "context": "In queryby-example search, putative matches are typically found by measuring the DTW distance between the query and segments of the search database [6, 7, 8, 9].", "startOffset": 148, "endOffset": 160}, {"referenceID": 6, "context": "In queryby-example search, putative matches are typically found by measuring the DTW distance between the query and segments of the search database [6, 7, 8, 9].", "startOffset": 148, "endOffset": 160}, {"referenceID": 7, "context": "In queryby-example search, putative matches are typically found by measuring the DTW distance between the query and segments of the search database [6, 7, 8, 9].", "startOffset": 148, "endOffset": 160}, {"referenceID": 8, "context": "There has been some, thus far limited, work on acoustic word embeddings, focused on a number of embedding models, training approaches, and tasks [10, 11, 12, 13, 14, 15, 16, 17].", "startOffset": 145, "endOffset": 177}, {"referenceID": 9, "context": "There has been some, thus far limited, work on acoustic word embeddings, focused on a number of embedding models, training approaches, and tasks [10, 11, 12, 13, 14, 15, 16, 17].", "startOffset": 145, "endOffset": 177}, {"referenceID": 10, "context": "There has been some, thus far limited, work on acoustic word embeddings, focused on a number of embedding models, training approaches, and tasks [10, 11, 12, 13, 14, 15, 16, 17].", "startOffset": 145, "endOffset": 177}, {"referenceID": 11, "context": "There has been some, thus far limited, work on acoustic word embeddings, focused on a number of embedding models, training approaches, and tasks [10, 11, 12, 13, 14, 15, 16, 17].", "startOffset": 145, "endOffset": 177}, {"referenceID": 12, "context": "There has been some, thus far limited, work on acoustic word embeddings, focused on a number of embedding models, training approaches, and tasks [10, 11, 12, 13, 14, 15, 16, 17].", "startOffset": 145, "endOffset": 177}, {"referenceID": 13, "context": "There has been some, thus far limited, work on acoustic word embeddings, focused on a number of embedding models, training approaches, and tasks [10, 11, 12, 13, 14, 15, 16, 17].", "startOffset": 145, "endOffset": 177}, {"referenceID": 14, "context": "There has been some, thus far limited, work on acoustic word embeddings, focused on a number of embedding models, training approaches, and tasks [10, 11, 12, 13, 14, 15, 16, 17].", "startOffset": 145, "endOffset": 177}, {"referenceID": 15, "context": "There has been some, thus far limited, work on acoustic word embeddings, focused on a number of embedding models, training approaches, and tasks [10, 11, 12, 13, 14, 15, 16, 17].", "startOffset": 145, "endOffset": 177}, {"referenceID": 8, "context": "[10] and Bengio and Heigold [11] used acoustic word embeddings, based on convolutional neural networks (CNNs), to generate scores for word segments in automatic speech recognition.", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "[10] and Bengio and Heigold [11] used acoustic word embeddings, based on convolutional neural networks (CNNs), to generate scores for word segments in automatic speech recognition.", "startOffset": 28, "endOffset": 32}, {"referenceID": 16, "context": "trained CNNs to predict (continuous-valued) embeddings of the word labels, and used the resulting embeddings to define feature functions in a segmental conditional random field [18] rescoring system.", "startOffset": 177, "endOffset": 181}, {"referenceID": 10, "context": "[12] developed unsupervised embeddings based on representing each word as a vector of DTW distances to a collection of reference word segments.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "This representation was subsequently used in several applications: a segmental approach for query-by-example search [13], lexical clustering [19], and unsupervised speech recognition [20].", "startOffset": 116, "endOffset": 120}, {"referenceID": 17, "context": "This representation was subsequently used in several applications: a segmental approach for query-by-example search [13], lexical clustering [19], and unsupervised speech recognition [20].", "startOffset": 141, "endOffset": 145}, {"referenceID": 18, "context": "This representation was subsequently used in several applications: a segmental approach for query-by-example search [13], lexical clustering [19], and unsupervised speech recognition [20].", "startOffset": 183, "endOffset": 187}, {"referenceID": 14, "context": "[16] developed a representation also based on templates, in their case phone templates, designed to be invariant to specific transformations, and showed their robustness on digit classification.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[14] compared several types of acoustic word embeddings for a word discrimination task related to query-by-example search, finding that embeddings based on convolutional neural networks (CNNs) trained with a contrastive loss outperformed the reference vector approach of Levin et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[12] as well as several other CNN and DNN embeddings and DTW using several feature types.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "There have now been a number of approaches compared on this same task and data [12, 21, 22, 23].", "startOffset": 79, "endOffset": 95}, {"referenceID": 19, "context": "There have now been a number of approaches compared on this same task and data [12, 21, 22, 23].", "startOffset": 79, "endOffset": 95}, {"referenceID": 20, "context": "There have now been a number of approaches compared on this same task and data [12, 21, 22, 23].", "startOffset": 79, "endOffset": 95}, {"referenceID": 21, "context": "There have now been a number of approaches compared on this same task and data [12, 21, 22, 23].", "startOffset": 79, "endOffset": 95}, {"referenceID": 15, "context": "[17] and Chung et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[15].", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[15] worked in an unsupervised setting and trained single-layer RNN autoencoders to produce embeddings for a word discrimination task.", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "Within this class of embedding models, we focus on Long Short-Term Memory (LSTM) networks [24] and Gated Recurrent Unit (GRU) networks [25].", "startOffset": 90, "endOffset": 94}, {"referenceID": 23, "context": "Within this class of embedding models, we focus on Long Short-Term Memory (LSTM) networks [24] and Gated Recurrent Unit (GRU) networks [25].", "startOffset": 135, "endOffset": 139}, {"referenceID": 24, "context": "Both of these RNN variants have been used successfully in speech recognition [26, 27, 28, 29].", "startOffset": 77, "endOffset": 93}, {"referenceID": 25, "context": "Both of these RNN variants have been used successfully in speech recognition [26, 27, 28, 29].", "startOffset": 77, "endOffset": 93}, {"referenceID": 26, "context": "Both of these RNN variants have been used successfully in speech recognition [26, 27, 28, 29].", "startOffset": 77, "endOffset": 93}, {"referenceID": 27, "context": "Both of these RNN variants have been used successfully in speech recognition [26, 27, 28, 29].", "startOffset": 77, "endOffset": 93}, {"referenceID": 28, "context": "For the fully connected layers, we use rectified linear unit (ReLU) [30] activation, except for the final layer which depends on the form of supervision and loss used in training.", "startOffset": 68, "endOffset": 72}, {"referenceID": 12, "context": "As in [14, 11], our first approach is to use the word labels of the training segments and train the networks to classify the word.", "startOffset": 6, "endOffset": 14}, {"referenceID": 9, "context": "As in [14, 11], our first approach is to use the word labels of the training segments and train the networks to classify the word.", "startOffset": 6, "endOffset": 14}, {"referenceID": 12, "context": "Here we are limited to the subset of the training set that has a sufficient number of segments per word to train a good classifier, and the output dimensionality is equal to the number of words (but see [14] for a study of varying the dimensionality in such a classifier-based embedding model by introducing a bottleneck layer).", "startOffset": 203, "endOffset": 207}, {"referenceID": 12, "context": "[14], is to train \u201dSiamese\u201d networks [31].", "startOffset": 0, "endOffset": 4}, {"referenceID": 29, "context": "[14], is to train \u201dSiamese\u201d networks [31].", "startOffset": 37, "endOffset": 41}, {"referenceID": 19, "context": "[21], which is similar to a query-by-example task where the word segmentations are known.", "startOffset": 0, "endOffset": 4}, {"referenceID": 30, "context": "The data used for this task is drawn from the Switchboard conversational English corpus [32].", "startOffset": 88, "endOffset": 92}, {"referenceID": 12, "context": "We use the same train, development, and test partitions as in prior work [14, 12], and the same acoustic features as in [14], for as direct a comparison as possible.", "startOffset": 73, "endOffset": 81}, {"referenceID": 10, "context": "We use the same train, development, and test partitions as in prior work [14, 12], and the same acoustic features as in [14], for as direct a comparison as possible.", "startOffset": 73, "endOffset": 81}, {"referenceID": 12, "context": "We use the same train, development, and test partitions as in prior work [14, 12], and the same acoustic features as in [14], for as direct a comparison as possible.", "startOffset": 120, "endOffset": 124}, {"referenceID": 12, "context": "As in [14], when training the classificationbased embeddings, we use a subset of the training set containing all word types with a minimum of 3 occurrences, reducing the training set size to approximately 9k segments.", "startOffset": 6, "endOffset": 10}, {"referenceID": 31, "context": "The recurrent hidden state dimensionality is fixed at 512 and dropout [33] between stacked recurrent layers is used with probability p = 0.", "startOffset": 70, "endOffset": 74}, {"referenceID": 32, "context": "The classifier network is trained with a cross entropy loss and optimized using stochastic gradient descent (SGD) with Nesterov momentum [34].", "startOffset": 137, "endOffset": 141}, {"referenceID": 33, "context": "Several other optimizers\u2014 Adagrad [35], Adadelta [36], and Adam [37]\u2014were explored in initial experiments on the dev set, but all reported results were obtained using SGD with Nesterov momentum.", "startOffset": 34, "endOffset": 38}, {"referenceID": 34, "context": "Several other optimizers\u2014 Adagrad [35], Adadelta [36], and Adam [37]\u2014were explored in initial experiments on the dev set, but all reported results were obtained using SGD with Nesterov momentum.", "startOffset": 49, "endOffset": 53}, {"referenceID": 35, "context": "Several other optimizers\u2014 Adagrad [35], Adadelta [36], and Adam [37]\u2014were explored in initial experiments on the dev set, but all reported results were obtained using SGD with Nesterov momentum.", "startOffset": 64, "endOffset": 68}, {"referenceID": 12, "context": "This is a slight departure from earlier work [14], which we found to improve stability in training and performance on the development set.", "startOffset": 45, "endOffset": 49}, {"referenceID": 36, "context": "All models were implemented in Torch [38] and used the rnn library of [39].", "startOffset": 37, "endOffset": 41}, {"referenceID": 37, "context": "All models were implemented in Torch [38] and used the rnn library of [39].", "startOffset": 70, "endOffset": 74}, {"referenceID": 12, "context": "MFCCs + DTW [14] 39\u2217 0.", "startOffset": 12, "endOffset": 16}, {"referenceID": 20, "context": "autoencoder + DTW [22] 100\u2217 0.", "startOffset": 18, "endOffset": 22}, {"referenceID": 12, "context": "Classifier CNN [14] 1061 0.", "startOffset": 15, "endOffset": 19}, {"referenceID": 12, "context": "014 Siamese CNN [14] 1024 0.", "startOffset": 16, "endOffset": 20}, {"referenceID": 12, "context": "We include a comparison with the best prior results on this task from [14], as well as the result of using standard DTW on the input MFCCs (reproduced from [14]) and the best prior result using DTW, obtained with frame features learned with correlated autoencoders [22].", "startOffset": 70, "endOffset": 74}, {"referenceID": 12, "context": "We include a comparison with the best prior results on this task from [14], as well as the result of using standard DTW on the input MFCCs (reproduced from [14]) and the best prior result using DTW, obtained with frame features learned with correlated autoencoders [22].", "startOffset": 156, "endOffset": 160}, {"referenceID": 20, "context": "We include a comparison with the best prior results on this task from [14], as well as the result of using standard DTW on the input MFCCs (reproduced from [14]) and the best prior result using DTW, obtained with frame features learned with correlated autoencoders [22].", "startOffset": 265, "endOffset": 269}, {"referenceID": 38, "context": "[40] have recently been able to improve AP on this test set even further with CNN embeddings, by using a large set of additional (crosslingual) training data.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "There is clear utility in stacking additional layers; however, even with 4 stacked layers the RNNs still underperform the CNN-based embeddings of [14] until we begin adding fully connected layers.", "startOffset": 146, "endOffset": 150}, {"referenceID": 12, "context": "This may raise the question of whether some simple fully connected model may be all that is needed; however, previous work has shown that this approach is not competitive [14], and convolutional or recurrent layers are needed to summarize arbitrarylength segments into a fixed-dimensional representation.", "startOffset": 171, "endOffset": 175}, {"referenceID": 12, "context": "This analysis shows that the embeddings learned by the Siamese RNN network are quite robust to reduced dimensionality, outperforming the classifier model for all dimensionalities 32 or higher and outperforming previously reported dev set performance with CNN-based embeddings [14] for all dimensionalities \u2265 16.", "startOffset": 276, "endOffset": 280}, {"referenceID": 39, "context": "In order to gain a better qualitative understanding of the differences between clasiffier and Siamese-based embeddings, and of the learned embedding space more generally, we plot a two-dimensional visualization of some of our learned embeddings via t-SNE [41] in Fig.", "startOffset": 255, "endOffset": 259}], "year": 2016, "abstractText": "Acoustic word embeddings \u2014 fixed-dimensional vector representations of variable-length spoken word segments \u2014 have begun to be considered for tasks such as speech recognition and query-by-example search. Such embeddings can be learned discriminatively so that they are similar for speech segments corresponding to the same word, while being dissimilar for segments corresponding to different words. Recent work has found that acoustic word embeddings can outperform dynamic time warping on query-by-example search and related word discrimination tasks. However, the space of embedding models and training approaches is still relatively unexplored. In this paper we present new discriminative embedding models based on recurrent neural networks (RNNs). We consider training losses that have been successful in prior work, in particular a cross entropy loss for word classification and a contrastive loss that explicitly aims to separate same-word and different-word pairs in a \u201dSiamese network\u201d training setting. We find that both classifier-based and Siamese RNN embeddings improve over previously reported results on a word discrimination task, with Siamese RNNs outperforming classification models. In addition, we present analyses of the learned embeddings and the effects of variables such as dimensionality and network structure.", "creator": "LaTeX with hyperref package"}}}