{"id": "1611.02588", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Nov-2016", "title": "Contradiction Detection for Rumorous Claims", "abstract": "The utilization of social media material in journalistic workflows is increasing, demanding automated methods for the identification of mis- and disinformation. Since textual contradiction across social media posts can be a signal of rumorousness, we seek to model how claims in Twitter posts are being textually contradicted. We identify two different contexts in which contradiction emerges: its broader form can be observed across independently posted tweets and its more specific form in threaded conversations. We define how the two scenarios differ in terms of central elements of argumentation: claims and conversation structure. We design and evaluate models for the two scenarios uniformly as 3-way Recognizing Textual Entailment tasks in order to represent claims and conversation structure implicitly in a generic inference model, while previous studies used explicit or no representation of these properties. To address noisy text, our classifiers use simple similarity features derived from the string and part-of-speech level. Corpus statistics reveal distribution differences for these features in contradictory as opposed to non-contradictory tweet relations, and the classifiers yield state of the art performance.", "histories": [["v1", "Tue, 8 Nov 2016 16:19:17 GMT  (410kb,D)", "https://arxiv.org/abs/1611.02588v1", "to appear in: Proc. Extra-Propositional Aspects of Meaning (ExProM) in Computational Linguistics, Osaka, Japan, 2016"], ["v2", "Fri, 11 Nov 2016 10:57:07 GMT  (517kb,D)", "http://arxiv.org/abs/1611.02588v2", "To appear in: Proceedings of Extra-Propositional Aspects of Meaning (ExProM) in Computational Linguistics, Osaka, Japan, 2016"]], "COMMENTS": "to appear in: Proc. Extra-Propositional Aspects of Meaning (ExProM) in Computational Linguistics, Osaka, Japan, 2016", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["piroska lendvai", "uwe d reichel"], "accepted": false, "id": "1611.02588"}, "pdf": {"name": "1611.02588.pdf", "metadata": {"source": "CRF", "title": "Contradiction Detection for Rumorous Claims", "authors": ["Piroska Lendvai", "Uwe D. Reichel"], "emails": ["piroska.r@gmail.com", "uwe.reichel@nytud.mta.hu"], "sections": [{"heading": "1 Introduction and Task Definition", "text": "The question is whether these two issues are a purely formal matter or a purely formal matter, which is a purely formal matter or a purely formal matter, the question is whether it is a purely formal matter or a purely formal matter, the question is whether it is a purely formal matter, which is not a purely formal matter, but a purely formal matter, which is a purely formal matter, the question is only whether it is a formal matter or a formal matter, whether it is a formal matter, a formal matter, or a formal matter, a formal matter, a formal matter or a formal matter."}, {"heading": "2 Related work and resources", "text": "The question of whether this is an improvement of the binary ensemble over the non-unbundling scenario has proved insufficient in recent years, but is still poorly studied (Ferreira and Vlachos, 2016; Lendvai et al., 2016a). The withdrawal relationship between two text snippets persists if the claim present in Snippet B can be completed by Snippet A. The contradiction relationship applies if the claim in B and the claim in B cannot be true at the same time. The unknown relationship applies if A and B do not contradict each other. RTE-3 Benchmark is the first resource to process text in terms 3 and 3."}, {"heading": "3 Data", "text": ", \"\", \"\", \"\", \",\", \"\", \"\", \"\", \"\", \",\", \",\", \"\", \",\" \",\" \",\", \"\", \",\" \",\" \",\" \",\" \",\" \",\" \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \",\", \",\", \",\", \",\" \",\", \"\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\" \",\", \",\", \",\", \",\", \",\", \"\", \",\", \"\", \",\", \",\", \",\" \",\", \",\", \",\", \",\", \",\", \",\" \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\","}, {"heading": "4 Text similarity features", "text": "Data pre-processing on both sets of data included removing screen names and hashtag characters, masking URLs, extracting vocabulary overlaps and local text alignment functions for each pair of tweets, and using the balloon toolkit (Reichel, 2012) (PENN tagset, (Marcus et al., 1999), partially tagging the tweets to the language, normalizing them to lowercase letters, and stamping them with a customized version of the Porter marker (Porter, 1980), defining content words to belong to the group of nouns, verbs, adjectives, adverbs, and numbers, and identifying them by their part of the markup, and removing all punctuation marks."}, {"heading": "4.1 Vocabulary overlap", "text": "The cosine similarity of two tweets is defined as C (X, Y) = | X-Y-Y-Y value, where X and Y value denote the sentences of the word with the vocabulary Y of another tweet (or vice versa). F1 value is defined as the harmonious mean of precision and retrieval. Precision and retrieval refer here to the coverage of the vocabulary X of a tweet by the vocabulary Y of another tweet (or vice versa). It is defined by F1 = 2 \u00b7 | X-Y | | X-Y | | X-Y | | X-Y | | Y-Y | | Y | |. Again, the vocabulary X and Y consist of vocabulary with the vocabulary Y of another tweet. Just like the Cosine index, the F1 value is a symmetrical similarity metric. These two metrics are applied additionally to the content word POS label within the tweet."}, {"heading": "4.2 Local alignment", "text": "The amount of overlap of inserted word marks was measured by applying local alignment pairs of symbol sequences using the Smith-Waterman algorithm (Smith and Waterman, 1981). We chose a score function that rewarded zero substitutions with + 1 and punished insertions, deletions, and substitutions with 0 reset each. After filling in the score matrix H, the alignment was applied iteratively in the following way: while max (H) \u2265 t - from the cell that contains this maximum, trace the path that leads to it until a zero cell is reached - add the substring collected this way to the set of aligned substrings - set all traversed cells to 0.The threshold defines the required minimum length of aligned substrings. It is set to 1 in this study, so it supports full alignment of any permutation of x."}, {"heading": "4.3 Corpus statistics", "text": "The diagrams show a general trend across all events and datasets: The similarity characteristics reach the highest values for the ENT class, followed by CON and UNK. Kruskal-Wallis tests, applied separately for all combinations of characteristics, events and datasets, confirmed these trends and showed significant differences for all triplets (p < 0.001 after correction for type 1 errors in this high number of comparisons using the false detection rate method (Benjamin Amini and Yekutieli, 2001). However, Dunnett post-hoc tests showed that for 16 of 72 comparisons (all POS similarity measurements) only UNK, but not ENT and CON, differ significantly from each other (Benjamini and Yekutieli, 2001)."}, {"heading": "5 RTE classification experiments for Contradiction and Disagreeing Reply detection", "text": "This year it is so far that it will be able to use the mentionlcihsrteeSe rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc-the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rfu the rfu the rfu the rf\u00fc the rfu the rfu the rfu the rfu the rfu the rfu the rfu the rfu the rfu the rfu the rfu the rfu the"}, {"heading": "6 Conclusions and Future Work", "text": "The detection of inconsistencies and inconsistencies in microposts provides important clues to factuality and veracity evaluation and is a central task in computer-aided journalism. We developed classifiers in a uniform, general follow-up framework that distinguishes two tasks based on the contextual proximity of the two posts to be evaluated, and when the claim goal may or may not be omitted in their content. We used simple text similarity metrics that prove to be a good basis for contradiction classification. Text similarity was measured in terms of vocabulary and symbol sequence. To derive the latter, local orientation proved to be a valuable tool: in contrast to the standard global alignments (Wagner and Fischer, 1974), it can be used for the crossing of dependencies and thus for the varying sequence of information structures in sequence of text pairs, e.g. in \"the cat the mouse of the mouse of the mouse\" and the mouse of the comment \"we expect the cat to be different from the comment and the comment of the mouse of the cat\")."}, {"heading": "7 Acknowledgments", "text": "P. Lendvai was supported by the PHEME FP7 project (grant no. 611233), U. D. Reichel by a scholarship of the Alexander von Humboldt Society. We thank anonymous experts for their contribution."}], "references": [{"title": "Semeval-2016 task 2: Interpretable semantic textual similarity", "author": ["Agirre et al.2016] Eneko Agirre", "Aitor Gonzalez-Agirre", "Inigo Lopez-Gazpio", "Montse Maritxalar", "German Rigau", "Larraitz Uria"], "venue": "Proceedings of SemEval,", "citeRegEx": "Agirre et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Agirre et al\\.", "year": 2016}, {"title": "USFD: Any-Target Stance Detection on Twitter with Autoencoders", "author": ["Andreas Vlachos", "Kalina Bontcheva"], "venue": "Proceedings of the International Workshop on Semantic Evaluation,", "citeRegEx": "Augenstein et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Augenstein et al\\.", "year": 2016}, {"title": "UKP: Computing semantic textual similarity by combining multiple content similarity measures", "author": ["B\u00e4r et al.2012] Daniel B\u00e4r", "Chris Biemann", "Iryna Gurevych", "Torsten Zesch"], "venue": "In Proceedings of the First Joint Conference on Lexical and Computational Semantics-Volume 1: Proceedings of the main conference and the shared task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation,", "citeRegEx": "B\u00e4r et al\\.,? \\Q2012\\E", "shortCiteRegEx": "B\u00e4r et al\\.", "year": 2012}, {"title": "The control of the false discovery rate in multiple testing under dependency", "author": ["Benjamini", "Yekutieli2001] Yoav Benjamini", "Daniel Yekutieli"], "venue": "Annals of Statistics,", "citeRegEx": "Benjamini et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Benjamini et al\\.", "year": 2001}, {"title": "A large annotated corpus for learning natural language inference", "author": ["Gabor Angeli", "Christopher Potts", "Christopher D. Manning"], "venue": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Bowman et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bowman et al\\.", "year": 2015}, {"title": "The PASCAL recognising textual entailment challenge. In Machine learning challenges. evaluating predictive uncertainty, visual object classification, and recognising tectual entailment", "author": ["Dagan et al.2006] Ido Dagan", "Oren Glickman", "Bernardo Magnini"], "venue": null, "citeRegEx": "Dagan et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Dagan et al\\.", "year": 2006}, {"title": "Finding contradictions in text", "author": ["Anna N Rafferty", "Christopher D Manning"], "venue": "In Proc. of ACL,", "citeRegEx": "Marneffe et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Marneffe et al\\.", "year": 2008}, {"title": "Did it happen? the pragmatic complexity of veridicality assessment", "author": ["Christopher D Manning", "Christopher Potts"], "venue": null, "citeRegEx": "Marneffe et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Marneffe et al\\.", "year": 2012}, {"title": "Emergent: a novel data-set for stance classification", "author": ["Ferreira", "Vlachos2016] William Ferreira", "Andreas Vlachos"], "venue": "In Proceedings of NAACL", "citeRegEx": "Ferreira et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Ferreira et al\\.", "year": 2016}, {"title": "Notes on transitivity and theme in English, part II", "author": [], "venue": "Journal of Linguistics,", "citeRegEx": "Halliday.,? \\Q1967\\E", "shortCiteRegEx": "Halliday.", "year": 1967}, {"title": "Distributed representations of sentences and documents", "author": ["Le", "Mikolov2014] Quoc V Le", "Tomas Mikolov"], "venue": "In ICML,", "citeRegEx": "Le et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Le et al\\.", "year": 2014}, {"title": "2016a. Monolingual social media datasets for detecting contradiction and entailment", "author": ["Isabelle Augenstein", "Kalina Bontcheva", "Thierry Declerck"], "venue": "In Proc. of LREC-2016", "citeRegEx": "Lendvai et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Lendvai et al\\.", "year": 2016}, {"title": "2016b. Algorithms for Detecting Disputed Information. Deliverable D4.2.2 for FP7-ICT Collaborative Project ICT-2013-611233 PHEME. https://www.pheme.eu/wp-content/uploads/2016/ 06/D422_final.pdf", "author": ["Isabelle Augenstein", "Dominic Rout", "Kalina Bontcheva", "Thierry Declerck"], "venue": null, "citeRegEx": "Lendvai et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Lendvai et al\\.", "year": 2016}, {"title": "Classification and regression by randomForest", "author": ["Liaw", "Wiener2002] Andy Liaw", "Matthew Wiener"], "venue": "R News,", "citeRegEx": "Liaw et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Liaw et al\\.", "year": 2002}, {"title": "2016. Hawkes Processes for Continuous Time Sequence Classification: An Application to Rumour Stance Classification in Twitter", "author": ["P.K. Srijith", "Duy Vu", "Kalina Bontcheva", "Arkaitz Zubiaga", "Trevor Cohn"], "venue": "Proceedings of ACL-16", "citeRegEx": "Lukasik et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Lukasik et al\\.", "year": 2016}, {"title": "Twitter Under Crisis: Can We Trust What We RT", "author": ["Barbara Poblete", "Carlos Castillo"], "venue": "In Proceedings of the First Workshop on Social Media Analytics", "citeRegEx": "Mendoza et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Mendoza et al\\.", "year": 2010}, {"title": "SemEval-2016 Task 6: Detecting stance in tweets", "author": ["Svetlana Kiritchenko", "Parinaz Sobhani", "Xiaodan Zhu", "Colin Cherry"], "venue": "In Proceedings of the International Workshop on Semantic Evaluation,", "citeRegEx": "Mohammad et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Mohammad et al\\.", "year": 2016}, {"title": "SEM 2012 shared task: Resolving the scope and focus of negation", "author": ["Morante", "Blanco2012] Roser Morante", "Eduardo Blanco"], "venue": "In Proceedings of the First Joint Conference on Lexical and Computational Semantics", "citeRegEx": "Morante et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Morante et al\\.", "year": 2012}, {"title": "Multi-level alignments as an extensible representation basis for textual entailment algorithms", "author": ["Noh et al.2015] Tae-Gil Noh", "Sebastian Pad\u00f3", "Vered Shwartz", "Ido Dagan", "Vivi Nastase", "Kathrin Eichler", "Lili Kotlerman", "Meni Adler"], "venue": "Lexical and Computational Semantics", "citeRegEx": "Noh et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Noh et al\\.", "year": 2015}, {"title": "Design and Realization of a Modular Architecture for Textual Entailment", "author": ["Pad\u00f3 et al.2015] Sebastian Pad\u00f3", "Tae-Gil Noh", "Asher Stern", "Rui Wang", "Roberto Zanoli"], "venue": "Natural Language Engineering,", "citeRegEx": "Pad\u00f3 et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Pad\u00f3 et al\\.", "year": 2015}, {"title": "An algorithm for suffix stripping", "author": ["Martin F. Porter"], "venue": null, "citeRegEx": "Porter.,? \\Q1980\\E", "shortCiteRegEx": "Porter.", "year": 1980}, {"title": "Rumor has it: Identifying misinformation in microblogs", "author": ["Emily Rosengren", "Dragomir R. Radev", "Qiaozhu Mei"], "venue": "In Proceedings of the Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Qazvinian et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Qazvinian et al\\.", "year": 2011}, {"title": "PermA and Balloon: Tools for string alignment and text processing", "author": ["Uwe D. Reichel"], "venue": "In Proc. Interspeech, page paper no. 346,", "citeRegEx": "Reichel.,? \\Q2012\\E", "shortCiteRegEx": "Reichel.", "year": 2012}, {"title": "Identification of common molecular subsequences", "author": ["Smith", "Waterman1981] Temple F. Smith", "Michael S. Waterman"], "venue": "Journal of Molecular Biology,", "citeRegEx": "Smith et al\\.,? \\Q1981\\E", "shortCiteRegEx": "Smith et al\\.", "year": 1981}, {"title": "Class prediction by nearest shrunken centroids,with applications to DNA microarrays", "author": ["Trevor Hastie", "Balasubramanian Narasimhan", "Gilbert Chu"], "venue": null, "citeRegEx": "Tibshirani et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Tibshirani et al\\.", "year": 2003}, {"title": "An analysis of event-agnostic features for rumour classification in twitter", "author": ["Tolosi et al.2016] Laura Tolosi", "Andrey Tagarev", "Georgi Georgiev"], "venue": "In Proc. of Social Media in the Newsroom Workshop", "citeRegEx": "Tolosi et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Tolosi et al\\.", "year": 2016}, {"title": "GRaSP: A Multilayered Annotation Scheme for Perspectives", "author": ["Tommaso Caselli", "Antske Fokkens", "Isa Maks", "Roser Morante", "Lora Aroyo", "Piek Vossen"], "venue": "In Proceedings of the 10th Edition of the Language Resources and Evaluation Conference (LREC)", "citeRegEx": "Son et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Son et al\\.", "year": 2016}, {"title": "The string-to-string correction problem", "author": ["Wagner", "Fischer1974] Robert A. Wagner", "Michael J. Fischer"], "venue": "Journal of the Association for Computing Machinery,", "citeRegEx": "Wagner et al\\.,? \\Q1974\\E", "shortCiteRegEx": "Wagner et al\\.", "year": 1974}, {"title": "Recognizing textual entailment using a subsequence kernel method", "author": ["Wang", "Neumann2007] Rui Wang", "G\u00fcnter Neumann"], "venue": "In AAAI,", "citeRegEx": "Wang et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2007}, {"title": "SemEval-2015 Task 1: Paraphrase and semantic similarity in Twitter (PIT)", "author": ["Xu et al.2015] Wei Xu", "Chris Callison-Burch", "William B Dolan"], "venue": "Proceedings of SemEval", "citeRegEx": "Xu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2015}, {"title": "Towards Detecting Rumours in Social Media", "author": ["Maria Liakata", "Rob Procter", "Kalina Bontcheva", "Peter Tolmie"], "venue": null, "citeRegEx": "Zubiaga et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zubiaga et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 15, "context": "Until resolution, a claim circulating on social media platforms is regarded as a rumor (Mendoza et al., 2010).", "startOffset": 87, "endOffset": 109}, {"referenceID": 16, "context": "Language technology has not yet solved the processing of contradiction-powering phenomena, such as negation (Morante and Blanco, 2012) and stance detection (Mohammad et al., 2016), where stance is defined to express speaker favorability towards an evaluation target, usually an entity or concept.", "startOffset": 156, "endOffset": 179}, {"referenceID": 14, "context": "Contradiction and stance detection have so far only marginally been addressed in the veracity context (de Marneffe et al., 2012; Ferreira and Vlachos, 2016; Lukasik et al., 2016).", "startOffset": 102, "endOffset": 178}, {"referenceID": 29, "context": "Our goals are manifold: (a) to offer richer context in contradiction modeling than what would be available on the level of individual tweets, the typical unit of analysis in previous studies; (b) to train and test supervised classifiers for contradiction detection in the RTE inference framework; (c) to address contradiction detection at the level of text similarity only, as opposed to semantic similarity (Xu et al., 2015); (d) to distinguish and focus on two different contradiction relationship types, each involving specific combinations of claim target mention, polarity, and contextual proximity, in particular: ar X iv :1 61 1.", "startOffset": 408, "endOffset": 425}, {"referenceID": 0, "context": "Texts paired in this type of contradiction resemble those of the recent Interpretable Semantic Similarity shared task (Agirre et al., 2016) that calls to identify five chunk level semantic relation types (equivalence, opposition, specificity, similarity or relatedness) between two texts that originate from headlines or captions.", "startOffset": 118, "endOffset": 139}, {"referenceID": 5, "context": "Inference has been suggested to be conveniently formalized in the generic framework of RTE1 (Dagan et al., 2006).", "startOffset": 92, "endOffset": 112}, {"referenceID": 4, "context": "Similarly, the new large annotated corpus used for deep models for entailment (Bowman et al., 2015) labeled text pairs as Contradiction are too broadly defined, i.", "startOffset": 78, "endOffset": 99}, {"referenceID": 19, "context": "A specific RTE application, the Excitement Open Platform2 (Pad\u00f3 et al., 2015) has been developed to provide a generic platform for applied RTE.", "startOffset": 58, "endOffset": 77}, {"referenceID": 21, "context": "Stance classification for rumors was introduced by (Qazvinian et al., 2011) where the goal was to generate a binary (for or against) stance judgment.", "startOffset": 51, "endOffset": 75}, {"referenceID": 16, "context": "A recent shared task on social media data defined separate challenges depending on whether target-specific training data is included in the task or not (Mohammad et al., 2016); the latter requires additional effort to encode information about the stance target, cf.", "startOffset": 152, "endOffset": 175}, {"referenceID": 1, "context": "(Augenstein et al., 2016).", "startOffset": 0, "endOffset": 25}, {"referenceID": 30, "context": "The PHEME project released a new stance-labeled social media dataset (Zubiaga et al., 2015) that we also utilize as described next.", "startOffset": 69, "endOffset": 91}, {"referenceID": 30, "context": "the corpus was pre-annotated as explained in (Zubiaga et al., 2015) for several rumorous claims9 \u2013 officially not yet confirmed statements lexicalized by a concise proposition, e.", "startOffset": 45, "endOffset": 67}, {"referenceID": 30, "context": "Tweets are organized into threaded conversations in the corpus and are marked up with respect to stance, certainty, evidentiality, and other veracity-related properties; for full details on released data we refer to (Zubiaga et al., 2015).", "startOffset": 216, "endOffset": 238}, {"referenceID": 30, "context": "We created the Threads RTE dataset drawing on manually pre-assigned Response Type labels by (Zubiaga et al., 2015) that were meant to characterize source tweet \u2013 replying tweet relations in terms of four categories.", "startOffset": 92, "endOffset": 114}, {"referenceID": 22, "context": "The tweets were part-of-speech-tagged using the Balloon toolkit (Reichel, 2012) (PENN tagset, (Marcus et al.", "startOffset": 64, "endOffset": 79}, {"referenceID": 20, "context": ", 1999)), normalized to lowercase and stemmed using an adapted version of the Porter stemmer (Porter, 1980).", "startOffset": 93, "endOffset": 107}, {"referenceID": 24, "context": "In order to predict the RTE classes based on the features introduced above, we trained two classifiers: Nearest (shrunken) centroids (NC) (Tibshirani et al., 2003) and Random forest (RF) (Breiman, 2001; Liaw and Wiener, 2002), using the R wrapper package Caret (Kuhn, 2016) with the methods pam and rf, respectively.", "startOffset": 138, "endOffset": 163}, {"referenceID": 14, "context": "On a different subset of the Threads data in terms of events, size of evidence, 4 stance classes and no resampling, (Lukasik et al., 2016) report .", "startOffset": 116, "endOffset": 138}, {"referenceID": 9, "context": "in \u201dthe cat chased the mouse\u201d and \u201dthe mouse was chased by the cat\u201d, which are differently structured into topic and comment (Halliday, 1967).", "startOffset": 125, "endOffset": 141}, {"referenceID": 2, "context": "Possible extensions to our approach include incorporating more informed text similarity metrics (B\u00e4r et al., 2012), formatting phenomena (Tolosi et al.", "startOffset": 96, "endOffset": 114}, {"referenceID": 25, "context": ", 2012), formatting phenomena (Tolosi et al., 2016), and distributed contextual representations (Le and Mikolov, 2014), the utilization of knowledge-intensive resources (Pad\u00f3 et al.", "startOffset": 30, "endOffset": 51}, {"referenceID": 19, "context": ", 2016), and distributed contextual representations (Le and Mikolov, 2014), the utilization of knowledge-intensive resources (Pad\u00f3 et al., 2015), representation of alignment on various content levels (Noh et al.", "startOffset": 125, "endOffset": 144}, {"referenceID": 18, "context": ", 2015), representation of alignment on various content levels (Noh et al., 2015), and formalization of contradiction scenarios in terms of additional layers of perspective (van Son et al.", "startOffset": 63, "endOffset": 81}], "year": 2016, "abstractText": "The utilization of social media material in journalistic workflows is increasing, demanding automated methods for the identification of misand disinformation. Since textual contradiction across social media posts can be a signal of rumorousness, we seek to model how claims in Twitter posts are being textually contradicted. We identify two different contexts in which contradiction emerges: its broader form can be observed across independently posted tweets and its more specific form in threaded conversations. We define how the two scenarios differ in terms of central elements of argumentation: claims and conversation structure. We design and evaluate models for the two scenarios uniformly as 3-way Recognizing Textual Entailment tasks in order to represent claims and conversation structure implicitly in a generic inference model, while previous studies used explicit or no representation of these properties. To address noisy text, our classifiers use simple similarity features derived from the string and part-of-speech level. Corpus statistics reveal distribution differences for these features in contradictory as opposed to non-contradictory tweet relations, and the classifiers yield state of the art performance.", "creator": "LaTeX with hyperref package"}}}