{"id": "1611.02654", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Nov-2016", "title": "Sentence Ordering using Recurrent Neural Networks", "abstract": "Modeling the structure of coherent texts is a task of great importance in NLP. The task of organizing a given set of sentences into a coherent order has been commonly used to build and evaluate models that understand such structure. In this work we propose an end-to-end neural approach based on the recently proposed set to sequence mapping framework to address the sentence ordering problem. Our model achieves state-of-the-art performance in the order discrimination task on two datasets widely used in the literature. We also consider a new interesting task of ordering abstracts from conference papers and research proposals and demonstrate strong performance against recent methods. Visualizing the sentence representations learned by the model shows that the model has captured high level logical structure in these paragraphs. The model also learns rich semantic sentence representations by learning to order texts, performing comparably to recent unsupervised representation learning methods in the sentence similarity and paraphrase detection tasks.", "histories": [["v1", "Tue, 8 Nov 2016 19:04:09 GMT  (1389kb,D)", "http://arxiv.org/abs/1611.02654v1", null]], "reviews": [], "SUBJECTS": "cs.CL cs.AI cs.LG", "authors": ["lajanugen logeswaran", "honglak lee", "dragomir radev"], "accepted": false, "id": "1611.02654"}, "pdf": {"name": "1611.02654.pdf", "metadata": {"source": "CRF", "title": "RECURRENT NEURAL NETWORKS", "authors": ["Lajanugen Logeswaran", "Honglak Lee", "Dragomir Radev"], "emails": ["llajan@umich.edu", "honglak@umich.edu", "radev@umich.edu"], "sections": [{"heading": "1 INTRODUCTION", "text": "This year, it is only a matter of time before an agreement is reached."}, {"heading": "2 RELATED WORK", "text": "This year, the time has come for us to be able to find a solution that is capable of finding a solution, that is capable of finding a solution, that is capable of finding a solution, and that is able to find a solution that is capable of finding a solution, that is capable of finding a solution, that is capable of finding a solution, that is capable of finding a solution, that is capable of finding a solution, that is capable of finding a solution, that is capable of finding a solution."}, {"heading": "3 APPROACH", "text": "rf\u00fc ide rf\u00fc ide rf\u00fc ide rf\u00fc ide rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rfu the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rfu the rfu the rfu the rf\u00fc the rfu the rfu the rfu the rfu the rfu the rfu the rfu the rfu the rfu the rfu the rfu the rfu the rfu the rrfu the rfu the rfu the rfu the rfu the r"}, {"heading": "3.1 SCORING FUNCTION", "text": "In our experiments, we consider two possibilities for the scoring functions f: The first is a single-layer feed net that uses s, h as the inputs and outputs of a score f (s, h) = W'tanh (W [s h] + b) + b, \"where W, b, W,\" b \"are learnable parameters. This scoring function takes a discriminatory approach to classifying the next sentence. Note that the structure of this scoring function is similar to the window network in Li and Hovy (2014). While they used a local window of sentences to capture the context, this scoring function uses the RNN hidden state to evaluate sentence candidates. We also consider a two-dimensional scoring function f (s, h) = sT (Wh + b). Compared to the previous scoring function, this takes a generative approach by trying to undo the next sentence in view of the current hidden state (Wh + b) and is most similar to the one that is corrected."}, {"heading": "3.2 TRAINING OBJECTIVE", "text": "The model is trained with the highest possible probability target: max \u2211 x \u0445 D | x | \u2211 t = 1 log p (xt | x1,..., xt \u2212 1) (9), where D denotes the training set and each training instance is given by an ordered sentence document x = (x1,..., x | x | |). We have also considered an alternative structured margin loss, which provides for a lower penalty for assigning high scores to penalty candidates who come close to the correct sentence in the source document, rather than punishing all false penalty candidates uniformly. However, Softmax output consistently performed better with cross entropy loss."}, {"heading": "3.3 COHERENCE MODELING", "text": "We define the coherence value of an arbitrary partial / complete assignment (sp1,..., spk) to the first k sentence positions, where S1,.., Sk are random variables representing the sentence assignment to positions 1 to k.The conditional probabilities originate from the network. This is our measure for comparing the coherence of different representations of a document. It is also used as heuristics during decoding."}, {"heading": "4 EXPERIMENTAL RESULTS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 MODEL TRAINING", "text": "For the two tasks discussed in the next sections, we train the model with the same goal (Eq.9) on the training data relevant to the task. The models are trained end-to-end. Model parameters. We use pre-formed 300-dimensional GloVe word embedding (Pennington et al., 2014). All LSTMs use a hidden layer size of 1000 and the MLP in Section 3.1 has a hidden layer size of 500. The number of read cycles in the encoder is set to 10. The same model architecture is used in all experiments.1 A subtle difference is that the last hidden state of the encoder hNenc has more dimensions than h0dec and only the first part of the vocabulary is copied (the reading is ignored for this time step). Pre-processing. The nltk sentence tokenizer was used for word tokenization. The GloVocabulator was used as a reference vocabulary."}, {"heading": "4.2 ORDER DISCRIMINATION", "text": "Finding the optimal order is a difficult problem when a large number of sentences have to be reordered or when there is ambiguity in the order of the sentences. Therefore, the order problem is commonly formulated as the following binary classification task. In the face of a reference paragraph and a permutated version of it, it is important to be organized all the more coherently (Barzilay and Lapata, 2008)."}, {"heading": "4.2.1 DATA", "text": "The ACCIDENTS data (also known as AIRPLANE data) is a series of air accident reports from the National Transportation Safety Board database. The EARTHQUAKES data includes newspaper articles from the North American News Text Corpus. In each of the above data sets, the training and test sets contain 100 articles and approximately 20 permutations of each article. Additional statistics on the data are shown in Table 1."}, {"heading": "4.2.2 RESULTS", "text": "Table 2 compares the performance of our model with previous approaches. We compare the results with traditional approaches in the literature as well as with some newer data-driven approaches (see section 2 for more details).The entity grid model provides a strong foundation on the ACCIDENTS dataset, which is surpassed only by our model and Li and Jurafsky (2016).For EARTHQUAKE data, the window approach of Li and Hovy (2014) and Li and Jurafsky (2016) outperforms strongly. Our approach outperforms previous models on both datasets and achieves near-perfect performance on the Earthquakes datasets. Although these datasets are widely used in the literature, they are inherently formulaic and no longer challenging."}, {"heading": "4.3 SENTENCE ORDERING", "text": "The difficulty of the ordering problem does not depend on the nature of the text and the length of the paragraphs considered. Evaluating text from any text source makes it difficult to interpret the results, as it may not be clear whether the observed performance is due to a deficient model or ambiguity in the selection of the next sentence due to many plausible explanations. Text summaries are a suitable data source for this task. They often have a clear flow of ideas and minimal redundancy. Specifically, we look at summaries of conference papers and NSF research proposals. These data have several beneficial properties. Abstracts usually have a particularly large format - they start with a brief introduction, a description of the problem addressed, and conclude with performance comments. This would allow us to determine whether the model is able to capture this logical structure at a higher level and have a larger amount of data available."}, {"heading": "4.3.1 DATA", "text": "We analyzed 3280 paper pdf abstracts and obtained 3259 abstracts after omitting faulty extracts; the data set was divided into 2005-2013 for training and 2014, 2015, and for validating and reviewing 2,ACL abstracts; a second source of abstracts we are considering are abstracts from the ACL Anthology Network (AAN) corpus (Radev et al., 2009) of ACL papers; at the time of retrieval, the corpus had publications up to 2013; and we extracted abstracts from the text parses using simple keyword matches for the strings \"Abstract\" and \"Introduction.\" Our extraction is successful for 12,157 articles; most of the errors occur with older papers because they have improper formatting and OCR datasets."}, {"heading": "4.3.2 METRICS", "text": "Accuracy measures how often the absolute position of a set has been correctly predicted. As it is too strict a measurement, it punishes correctly predicted subsequences that are shifted. Another widely used measurement in the literature is Kendall's tau (\u03c4), calculated as 1 \u2212 2 \u00d7 (number of inversions) / (n 2), where the number of inversions is the number of pairs in the predicted sequence in wrong relative order and n the length of the sequence. Lapata (2006) discusses that this measurement reliably correlates with human judgement. 2Experiments with a random split yielded a similar performance. We adopt this split so that future work can easily make comparisons with our results."}, {"heading": "4.3.3 BASELINES", "text": "Entity Grid. Our first baseline is the Entity Grid Model by Barzilay and Lapata (2008). We use the Stanford parser Klein and Manning (2003) to obtain constituency trees for all the records in our data sets. We conduct Entity Grid representations for the records analyzed using the Brown Coherence Toolkit.3 A ranking SVM is trained to rate correct orders higher than wrong orders as in the original work. We used 20 permutations per document as training data. Since the Entity Grid representation provides only one means of feature extraction, we evaluate the model in the order as follows. We select 1000 random permutations for each document, one of which is the correct order with maximum coherence. We experimented with transitions of the length most 3 in the entity extraction, which we consider as a sequence."}, {"heading": "4.3.4 RESULTS", "text": "In fact, it is that we are able to hide, and that we are able to hide, and that we are able to hide."}, {"heading": "4.4 LEARNED SENTENCE REPRESENTATIONS", "text": "One of the original motivations for this work is the question of whether we can learn high-quality sentence representations by learning text coherence. To answer this question, we trained our model on a large dataset of paragraphs. We chose the BookCorpus dataset (Kiros et al., 2015) for this purpose. We used the model with two keys trained on the abstracts. 1) In addition to the sentences that were considered, we added additional contrasting sentences from other paragraphs. 2) We used the bilinear scoring function. These techniques helped to obtain better representations for large amounts of data."}, {"heading": "5 CONCLUSION", "text": "In this paper, we considered the challenging problem of coherently organizing a given set of sentences. Our RNN-based model performs well compared to basic methods and previous work on sentence order and job discrimination. We also demonstrated that the model captures the structure of a document at a high level and learns useful sentence representations when trained on large amounts of data. Our approach to the ordering problem differs from most previous work that used handmade features. However, the use of linguistic features for the next sentence classification can potentially further improve the performance of the task. Entity distribution patterns can provide useful features about named entities that are treated like vocabulary words. The ordering problem can be further investigated at a higher level discourse units such as paragraphs, sections and chapters.4 We used the same hyperparameters used for abstract data to train our model."}, {"heading": "A WORD INFLUENCE", "text": "Some techniques for visualizing neural network models in the context of text applications are discussed in Li et al. (2015a). Inspired by this work, we use gradients of predictive patterns relating to the words of the correct sentence as a substitute for highlighting each word. Let's assume that the sentence mappings are correct for all previous time steps. Let's specify the current hidden state in this setting and s = (w1,..., wn) be the correct next sentence candidate, with the wi being his words. The score for this sentence is defined as e = f (s, h) (see Equation 7). The meaning of word wi in predicting s as the correct next sentence is interpreted as \"s.\" We assume h is fixed and only gradients through the sentence coder.Table 5 shows visualizations of some selected words set forth in the first abstraction. \""}, {"heading": "B PERFORMANCE ANALYSIS", "text": "Figure 3a shows the average value for the models on the NIPS Abstract Test for a specific paragraph length. Local approach performance is declining relatively quickly, as we can expect, and we face difficulties in handling long paragraphs. Our model tries to maintain consistent performance with a more gradual decrease in performance as paragraph size increases. Figure 3b compares the average predictive accuracy for a particular sentence position in the test sentence. It is interesting to note that all models perform well in predicting the first sentence. The greedy decryption process also contributes to the decline in performance as we move correctly. Our model remains more robust compared to the other two methods. Another trend to be observed is that as context size increases (2 for the next sentence generation, 3 for window networks, full sentence history for our model) the performance decline is more gradual."}, {"heading": "C MODEL DETAILS", "text": "The LSTM update in Eq.1 of the paper, ct = LSTM (ht \u2212 1, ct \u2212 1) (11) is as follows: it = \u03c3 (Wiht \u2212 1 + bi) (12) ft = \u03c3 (Wfht \u2212 1 + bf) (13) ot = \u03c3 (Living \u2212 1 + bo) (14) c-t = tanh (weight \u2212 1 + bc) (15) ct = ft ct \u2212 1 + it c-t (16) ht = ot tanh (ct) (17), where W {i, f, o, c}, b {i, f, o, c} are learnable parameters. The LSTM update in Eq.6ht, ct = LSTM (ht \u2212 1, ct \u2212 1, xt \u2212 1) (18) results from the following parameters."}], "references": [{"title": "Inferring strategies for sentence ordering in multidocument news summarization", "author": ["R. Barzilay", "N. Elhadad"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Barzilay and Elhadad.,? \\Q2002\\E", "shortCiteRegEx": "Barzilay and Elhadad.", "year": 2002}, {"title": "Modeling local coherence: An entity-based approach", "author": ["R. Barzilay", "M. Lapata"], "venue": "Computational Linguistics,", "citeRegEx": "Barzilay and Lapata.,? \\Q2008\\E", "shortCiteRegEx": "Barzilay and Lapata.", "year": 2008}, {"title": "Catching the drift: Probabilistic content models, with applications to generation and summarization", "author": ["R. Barzilay", "L. Lee"], "venue": "arXiv preprint cs/0405039,", "citeRegEx": "Barzilay and Lee.,? \\Q2004\\E", "shortCiteRegEx": "Barzilay and Lee.", "year": 2004}, {"title": "Using entity-based features to model coherence in student essays", "author": ["J. Burstein", "J. Tetreault", "S. Andreyev"], "venue": "Association for Computational Linguistics,", "citeRegEx": "Burstein et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Burstein et al\\.", "year": 2010}, {"title": "Neural sentence ordering", "author": ["X. Chen", "X. Qiu", "X. Huang"], "venue": "arXiv preprint arXiv:1607.06952,", "citeRegEx": "Chen et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2016}, {"title": "A unified local and global model for discourse coherence", "author": ["M. Elsner", "J.L. Austerweil", "E. Charniak"], "venue": "In HLT-NAACL,", "citeRegEx": "Elsner et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Elsner et al\\.", "year": 2007}, {"title": "The measurement of textual coherence with latent semantic analysis", "author": ["P.W. Foltz", "W. Kintsch", "T.K. Landauer"], "venue": "Discourse processes,", "citeRegEx": "Foltz et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Foltz et al\\.", "year": 1998}, {"title": "Centering: A framework for modeling the local coherence of discourse", "author": ["B.J. Grosz", "S. Weinstein", "A.K. Joshi"], "venue": "Computational linguistics,", "citeRegEx": "Grosz et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Grosz et al\\.", "year": 1995}, {"title": "Discriminative improvements to distributional sentence similarity", "author": ["Y. Ji", "J. Eisenstein"], "venue": "In EMNLP,", "citeRegEx": "Ji and Eisenstein.,? \\Q2013\\E", "shortCiteRegEx": "Ji and Eisenstein.", "year": 2013}, {"title": "Adam: A method for stochastic optimization", "author": ["D. Kingma", "J. Ba"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "Kingma and Ba.,? \\Q2014\\E", "shortCiteRegEx": "Kingma and Ba.", "year": 2014}, {"title": "Skip-thought vectors", "author": ["R. Kiros", "Y. Zhu", "R.R. Salakhutdinov", "R. Zemel", "R. Urtasun", "A. Torralba", "S. Fidler"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Kiros et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kiros et al\\.", "year": 2015}, {"title": "Accurate unlexicalized parsing", "author": ["D. Klein", "C.D. Manning"], "venue": "In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics-Volume", "citeRegEx": "Klein and Manning.,? \\Q2003\\E", "shortCiteRegEx": "Klein and Manning.", "year": 2003}, {"title": "Probabilistic text structuring: Experiments with sentence ordering", "author": ["M. Lapata"], "venue": "In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics-Volume", "citeRegEx": "Lapata.,? \\Q2003\\E", "shortCiteRegEx": "Lapata.", "year": 2003}, {"title": "Automatic evaluation of information ordering: Kendall\u2019s tau", "author": ["M. Lapata"], "venue": "Computational Linguistics,", "citeRegEx": "Lapata.,? \\Q2006\\E", "shortCiteRegEx": "Lapata.", "year": 2006}, {"title": "A model of coherence based on distributed sentence representation", "author": ["J. Li", "E.H. Hovy"], "venue": "In EMNLP, pages 2039\u20132048,", "citeRegEx": "Li and Hovy.,? \\Q2014\\E", "shortCiteRegEx": "Li and Hovy.", "year": 2014}, {"title": "Neural net models for open-domain discourse coherence", "author": ["J. Li", "D. Jurafsky"], "venue": "arXiv preprint arXiv:1606.01545,", "citeRegEx": "Li and Jurafsky.,? \\Q2016\\E", "shortCiteRegEx": "Li and Jurafsky.", "year": 2016}, {"title": "Visualizing and understanding neural models in nlp", "author": ["J. Li", "X. Chen", "E. Hovy", "D. Jurafsky"], "venue": "arXiv preprint arXiv:1506.01066,", "citeRegEx": "Li et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Li et al\\.", "year": 2015}, {"title": "A hierarchical neural autoencoder for paragraphs and documents", "author": ["J. Li", "M.-T. Luong", "D. Jurafsky"], "venue": "arXiv preprint arXiv:1506.01057,", "citeRegEx": "Li et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Li et al\\.", "year": 2015}, {"title": "Hierarchical recurrent neural network for document modeling", "author": ["R. Lin", "S. Liu", "M. Yang", "M. Li", "M. Zhou", "S. Li"], "venue": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Lin et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2015}, {"title": "A coherence model based on syntactic patterns", "author": ["A. Louis", "A. Nenkova"], "venue": "In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,", "citeRegEx": "Louis and Nenkova.,? \\Q2012\\E", "shortCiteRegEx": "Louis and Nenkova.", "year": 2012}, {"title": "Re-examining machine translation metrics for paraphrase identification", "author": ["N. Madnani", "J. Tetreault", "M. Chodorow"], "venue": "In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,", "citeRegEx": "Madnani et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Madnani et al\\.", "year": 2012}, {"title": "Evaluation of text coherence for electronic essay scoring systems", "author": ["E. Miltsakaki", "K. Kukich"], "venue": "Natural Language Engineering,", "citeRegEx": "Miltsakaki and Kukich.,? \\Q2004\\E", "shortCiteRegEx": "Miltsakaki and Kukich.", "year": 2004}, {"title": "Glove: Global vectors for word representation", "author": ["J. Pennington", "R. Socher", "C.D. Manning"], "venue": "In EMNLP,", "citeRegEx": "Pennington et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "A Bibliometric and Network Analysis of the field of Computational Linguistics", "author": ["D.R. Radev", "M.T. Joseph", "B. Gibson", "P. Muthukrishnan"], "venue": "Journal of the American Society for Information Science and Technology,", "citeRegEx": "Radev et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Radev et al\\.", "year": 2009}, {"title": "Dynamic pooling and unfolding recursive autoencoders for paraphrase detection", "author": ["R. Socher", "E.H. Huang", "J. Pennin", "C.D. Manning", "A.Y. Ng"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Socher et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2011}, {"title": "Discourse generation using utility-trained coherence models", "author": ["R. Soricut", "D. Marcu"], "venue": "In Proceedings of the COLING/ACL on Main conference poster sessions,", "citeRegEx": "Soricut and Marcu.,? \\Q2006\\E", "shortCiteRegEx": "Soricut and Marcu.", "year": 2006}, {"title": "Sequence to sequence learning with neural networks", "author": ["I. Sutskever", "O. Vinyals", "Q.V. Le"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Improved semantic representations from tree-structured long short-term memory networks", "author": ["K.S. Tai", "R. Socher", "C.D. Manning"], "venue": "arXiv preprint arXiv:1503.00075,", "citeRegEx": "Tai et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Tai et al\\.", "year": 2015}, {"title": "Order matters: Sequence to sequence for sets", "author": ["O. Vinyals", "S. Bengio", "M. Kudlur"], "venue": "arXiv preprint arXiv:1511.06391,", "citeRegEx": "Vinyals et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Vinyals et al\\.", "year": 2015}, {"title": "Drawing inspiration from this work, we use gradients of prediction decisions", "author": ["Li"], "venue": null, "citeRegEx": "Li,? \\Q2015\\E", "shortCiteRegEx": "Li", "year": 2015}, {"title": "We assume h to be fixed and only backpropagate gradients through the sentence encoder. Table 5 shows visualizations of a few selected abstracts. Words expressed in darker shades correspond to higher gradient norms. In the first example the model seems to be using the word clues \u2018first\u2019, \u2018second", "author": ["\u2202wi"], "venue": null, "citeRegEx": "..,? \\Q2016\\E", "shortCiteRegEx": "..", "year": 2016}], "referenceMentions": [{"referenceID": 0, "context": "Barzilay and Elhadad (2002) discuss the importance of an explicit ordering component in MDS systems.", "startOffset": 0, "endOffset": 28}, {"referenceID": 21, "context": "Automated essay scoring (Miltsakaki and Kukich, 2004; Burstein et al., 2010) is an application that can benefit from such a coherence model.", "startOffset": 24, "endOffset": 76}, {"referenceID": 3, "context": "Automated essay scoring (Miltsakaki and Kukich, 2004; Burstein et al., 2010) is an application that can benefit from such a coherence model.", "startOffset": 24, "endOffset": 76}, {"referenceID": 26, "context": "The Sequence to sequence mapping framework (Sutskever et al., 2014), as well as several of its variants have fuelled RNN based approaches to a wide variety of problems including language modeling, language generation, machine translation, question answering and many others.", "startOffset": 43, "endOffset": 67}, {"referenceID": 26, "context": "The Sequence to sequence mapping framework (Sutskever et al., 2014), as well as several of its variants have fuelled RNN based approaches to a wide variety of problems including language modeling, language generation, machine translation, question answering and many others. Vinyals et al. (2015a) recently showed that the order in which tokens of the input sequence are fed to seq2seq models has a significant impact on the performance of the model.", "startOffset": 44, "endOffset": 298}, {"referenceID": 6, "context": "Foltz et al. (1998) represent words using vectors of co-occurent counts and sentences as a mean of these word vectors.", "startOffset": 0, "endOffset": 20}, {"referenceID": 6, "context": "Foltz et al. (1998) represent words using vectors of co-occurent counts and sentences as a mean of these word vectors. Sentence similarity is defined as the cosine distance between sentence vectors and text coherence is modeled as a normalized sum of similarity scores of adjacent sentences. Lapata (2003) represents sentences by vectors of", "startOffset": 0, "endOffset": 306}, {"referenceID": 7, "context": "These two approaches find motivation from ideas in centering theory (Grosz et al., 1995) which state that nouns and entities in coherent discourses exhibit certain patterns.", "startOffset": 68, "endOffset": 88}, {"referenceID": 1, "context": "A popular model of coherence is the Entity-Grid model Barzilay and Lapata (2008) which captures local coherence by modeling patterns of entity distributions in the discourse.", "startOffset": 54, "endOffset": 81}, {"referenceID": 1, "context": "A popular model of coherence is the Entity-Grid model Barzilay and Lapata (2008) which captures local coherence by modeling patterns of entity distributions in the discourse. Sentences are represented by the syntactic roles of entities appearing in the document and entity transition frequencies in successive sentences are treated as features that are are used to train a ranking SVM. These two approaches find motivation from ideas in centering theory (Grosz et al., 1995) which state that nouns and entities in coherent discourses exhibit certain patterns. Global models of coherence typically use an HMM to model document structure. The content model proposed by Barzilay and Lee (2004) represents topics in a particular domain as states in an HMM.", "startOffset": 54, "endOffset": 691}, {"referenceID": 1, "context": "A popular model of coherence is the Entity-Grid model Barzilay and Lapata (2008) which captures local coherence by modeling patterns of entity distributions in the discourse. Sentences are represented by the syntactic roles of entities appearing in the document and entity transition frequencies in successive sentences are treated as features that are are used to train a ranking SVM. These two approaches find motivation from ideas in centering theory (Grosz et al., 1995) which state that nouns and entities in coherent discourses exhibit certain patterns. Global models of coherence typically use an HMM to model document structure. The content model proposed by Barzilay and Lee (2004) represents topics in a particular domain as states in an HMM. State transitions capture possible presentation orderings within the domain. Words of a sentence are modeled using a topic-specific language model. The content model has inspired several subsequent work to combine the strengths of local and global models. Elsner et al. (2007) combine the entity model and the content model using a non-parametric HMM.", "startOffset": 54, "endOffset": 1030}, {"referenceID": 1, "context": "A popular model of coherence is the Entity-Grid model Barzilay and Lapata (2008) which captures local coherence by modeling patterns of entity distributions in the discourse. Sentences are represented by the syntactic roles of entities appearing in the document and entity transition frequencies in successive sentences are treated as features that are are used to train a ranking SVM. These two approaches find motivation from ideas in centering theory (Grosz et al., 1995) which state that nouns and entities in coherent discourses exhibit certain patterns. Global models of coherence typically use an HMM to model document structure. The content model proposed by Barzilay and Lee (2004) represents topics in a particular domain as states in an HMM. State transitions capture possible presentation orderings within the domain. Words of a sentence are modeled using a topic-specific language model. The content model has inspired several subsequent work to combine the strengths of local and global models. Elsner et al. (2007) combine the entity model and the content model using a non-parametric HMM. Soricut and Marcu (2006) use several models as feature functions and define a log linear model to assign probability to a given text.", "startOffset": 54, "endOffset": 1130}, {"referenceID": 1, "context": "A popular model of coherence is the Entity-Grid model Barzilay and Lapata (2008) which captures local coherence by modeling patterns of entity distributions in the discourse. Sentences are represented by the syntactic roles of entities appearing in the document and entity transition frequencies in successive sentences are treated as features that are are used to train a ranking SVM. These two approaches find motivation from ideas in centering theory (Grosz et al., 1995) which state that nouns and entities in coherent discourses exhibit certain patterns. Global models of coherence typically use an HMM to model document structure. The content model proposed by Barzilay and Lee (2004) represents topics in a particular domain as states in an HMM. State transitions capture possible presentation orderings within the domain. Words of a sentence are modeled using a topic-specific language model. The content model has inspired several subsequent work to combine the strengths of local and global models. Elsner et al. (2007) combine the entity model and the content model using a non-parametric HMM. Soricut and Marcu (2006) use several models as feature functions and define a log linear model to assign probability to a given text. Louis and Nenkova (2012) attempt to capture the intentional structure in documents using syntax as a proxy for the communicative goal of a sentence.", "startOffset": 54, "endOffset": 1264}, {"referenceID": 1, "context": "A popular model of coherence is the Entity-Grid model Barzilay and Lapata (2008) which captures local coherence by modeling patterns of entity distributions in the discourse. Sentences are represented by the syntactic roles of entities appearing in the document and entity transition frequencies in successive sentences are treated as features that are are used to train a ranking SVM. These two approaches find motivation from ideas in centering theory (Grosz et al., 1995) which state that nouns and entities in coherent discourses exhibit certain patterns. Global models of coherence typically use an HMM to model document structure. The content model proposed by Barzilay and Lee (2004) represents topics in a particular domain as states in an HMM. State transitions capture possible presentation orderings within the domain. Words of a sentence are modeled using a topic-specific language model. The content model has inspired several subsequent work to combine the strengths of local and global models. Elsner et al. (2007) combine the entity model and the content model using a non-parametric HMM. Soricut and Marcu (2006) use several models as feature functions and define a log linear model to assign probability to a given text. Louis and Nenkova (2012) attempt to capture the intentional structure in documents using syntax as a proxy for the communicative goal of a sentence. Syntax features such as parse tree production rules and constituency tags at a particular tree depth were used. Unlike previous approaches, we do not employ any handcrafted features and adopt an embedding based approach. Local coherence is taken into account by having a next sentence prediction component in the model and global dependencies are naturally captured by an RNN. We demonstrate that our model is able to capture both logical and topical structure by evaluating its performance on different types of data. Data-driven approaches Neural approaches have gained attention more recently. Li and Hovy (2014) model sentences as embeddings derived from recurrent/recursive neural nets and train a feedforward neural network that takes an input window of sentence embeddings and outputs a probability which represents the coherence of the sentence window.", "startOffset": 54, "endOffset": 2004}, {"referenceID": 1, "context": "A popular model of coherence is the Entity-Grid model Barzilay and Lapata (2008) which captures local coherence by modeling patterns of entity distributions in the discourse. Sentences are represented by the syntactic roles of entities appearing in the document and entity transition frequencies in successive sentences are treated as features that are are used to train a ranking SVM. These two approaches find motivation from ideas in centering theory (Grosz et al., 1995) which state that nouns and entities in coherent discourses exhibit certain patterns. Global models of coherence typically use an HMM to model document structure. The content model proposed by Barzilay and Lee (2004) represents topics in a particular domain as states in an HMM. State transitions capture possible presentation orderings within the domain. Words of a sentence are modeled using a topic-specific language model. The content model has inspired several subsequent work to combine the strengths of local and global models. Elsner et al. (2007) combine the entity model and the content model using a non-parametric HMM. Soricut and Marcu (2006) use several models as feature functions and define a log linear model to assign probability to a given text. Louis and Nenkova (2012) attempt to capture the intentional structure in documents using syntax as a proxy for the communicative goal of a sentence. Syntax features such as parse tree production rules and constituency tags at a particular tree depth were used. Unlike previous approaches, we do not employ any handcrafted features and adopt an embedding based approach. Local coherence is taken into account by having a next sentence prediction component in the model and global dependencies are naturally captured by an RNN. We demonstrate that our model is able to capture both logical and topical structure by evaluating its performance on different types of data. Data-driven approaches Neural approaches have gained attention more recently. Li and Hovy (2014) model sentences as embeddings derived from recurrent/recursive neural nets and train a feedforward neural network that takes an input window of sentence embeddings and outputs a probability which represents the coherence of the sentence window. Coherence evaluation is performed by sliding the window over the text and aggregating the score. Li and Jurafsky (2016) study the same model in a larger scale task and also consider a sequence to sequence approach where the model is trained to generate the next sentence given the current sentence and vice versa.", "startOffset": 54, "endOffset": 2369}, {"referenceID": 1, "context": "A popular model of coherence is the Entity-Grid model Barzilay and Lapata (2008) which captures local coherence by modeling patterns of entity distributions in the discourse. Sentences are represented by the syntactic roles of entities appearing in the document and entity transition frequencies in successive sentences are treated as features that are are used to train a ranking SVM. These two approaches find motivation from ideas in centering theory (Grosz et al., 1995) which state that nouns and entities in coherent discourses exhibit certain patterns. Global models of coherence typically use an HMM to model document structure. The content model proposed by Barzilay and Lee (2004) represents topics in a particular domain as states in an HMM. State transitions capture possible presentation orderings within the domain. Words of a sentence are modeled using a topic-specific language model. The content model has inspired several subsequent work to combine the strengths of local and global models. Elsner et al. (2007) combine the entity model and the content model using a non-parametric HMM. Soricut and Marcu (2006) use several models as feature functions and define a log linear model to assign probability to a given text. Louis and Nenkova (2012) attempt to capture the intentional structure in documents using syntax as a proxy for the communicative goal of a sentence. Syntax features such as parse tree production rules and constituency tags at a particular tree depth were used. Unlike previous approaches, we do not employ any handcrafted features and adopt an embedding based approach. Local coherence is taken into account by having a next sentence prediction component in the model and global dependencies are naturally captured by an RNN. We demonstrate that our model is able to capture both logical and topical structure by evaluating its performance on different types of data. Data-driven approaches Neural approaches have gained attention more recently. Li and Hovy (2014) model sentences as embeddings derived from recurrent/recursive neural nets and train a feedforward neural network that takes an input window of sentence embeddings and outputs a probability which represents the coherence of the sentence window. Coherence evaluation is performed by sliding the window over the text and aggregating the score. Li and Jurafsky (2016) study the same model in a larger scale task and also consider a sequence to sequence approach where the model is trained to generate the next sentence given the current sentence and vice versa. Chen et al. (2016) also propose a sentence embedding based approach where they model the probability that one sentence should come before another and define coherence based on the likelihood of the relative order of every pair of sentences.", "startOffset": 54, "endOffset": 2582}, {"referenceID": 1, "context": "A popular model of coherence is the Entity-Grid model Barzilay and Lapata (2008) which captures local coherence by modeling patterns of entity distributions in the discourse. Sentences are represented by the syntactic roles of entities appearing in the document and entity transition frequencies in successive sentences are treated as features that are are used to train a ranking SVM. These two approaches find motivation from ideas in centering theory (Grosz et al., 1995) which state that nouns and entities in coherent discourses exhibit certain patterns. Global models of coherence typically use an HMM to model document structure. The content model proposed by Barzilay and Lee (2004) represents topics in a particular domain as states in an HMM. State transitions capture possible presentation orderings within the domain. Words of a sentence are modeled using a topic-specific language model. The content model has inspired several subsequent work to combine the strengths of local and global models. Elsner et al. (2007) combine the entity model and the content model using a non-parametric HMM. Soricut and Marcu (2006) use several models as feature functions and define a log linear model to assign probability to a given text. Louis and Nenkova (2012) attempt to capture the intentional structure in documents using syntax as a proxy for the communicative goal of a sentence. Syntax features such as parse tree production rules and constituency tags at a particular tree depth were used. Unlike previous approaches, we do not employ any handcrafted features and adopt an embedding based approach. Local coherence is taken into account by having a next sentence prediction component in the model and global dependencies are naturally captured by an RNN. We demonstrate that our model is able to capture both logical and topical structure by evaluating its performance on different types of data. Data-driven approaches Neural approaches have gained attention more recently. Li and Hovy (2014) model sentences as embeddings derived from recurrent/recursive neural nets and train a feedforward neural network that takes an input window of sentence embeddings and outputs a probability which represents the coherence of the sentence window. Coherence evaluation is performed by sliding the window over the text and aggregating the score. Li and Jurafsky (2016) study the same model in a larger scale task and also consider a sequence to sequence approach where the model is trained to generate the next sentence given the current sentence and vice versa. Chen et al. (2016) also propose a sentence embedding based approach where they model the probability that one sentence should come before another and define coherence based on the likelihood of the relative order of every pair of sentences. We believe these models are limited by the fact that they are local in nature and our experiments show that exploiting larger contexts can be very beneficial. Hierarchical RNNs for document modeling Word level and sentence level RNNs have been used in a hierarchical fashion for modeling documents in prior work. Li et al. (2015b) proposed a hierarchical document autoencoder which has potential to be used in generation and summarization applications.", "startOffset": 54, "endOffset": 3135}, {"referenceID": 1, "context": "A popular model of coherence is the Entity-Grid model Barzilay and Lapata (2008) which captures local coherence by modeling patterns of entity distributions in the discourse. Sentences are represented by the syntactic roles of entities appearing in the document and entity transition frequencies in successive sentences are treated as features that are are used to train a ranking SVM. These two approaches find motivation from ideas in centering theory (Grosz et al., 1995) which state that nouns and entities in coherent discourses exhibit certain patterns. Global models of coherence typically use an HMM to model document structure. The content model proposed by Barzilay and Lee (2004) represents topics in a particular domain as states in an HMM. State transitions capture possible presentation orderings within the domain. Words of a sentence are modeled using a topic-specific language model. The content model has inspired several subsequent work to combine the strengths of local and global models. Elsner et al. (2007) combine the entity model and the content model using a non-parametric HMM. Soricut and Marcu (2006) use several models as feature functions and define a log linear model to assign probability to a given text. Louis and Nenkova (2012) attempt to capture the intentional structure in documents using syntax as a proxy for the communicative goal of a sentence. Syntax features such as parse tree production rules and constituency tags at a particular tree depth were used. Unlike previous approaches, we do not employ any handcrafted features and adopt an embedding based approach. Local coherence is taken into account by having a next sentence prediction component in the model and global dependencies are naturally captured by an RNN. We demonstrate that our model is able to capture both logical and topical structure by evaluating its performance on different types of data. Data-driven approaches Neural approaches have gained attention more recently. Li and Hovy (2014) model sentences as embeddings derived from recurrent/recursive neural nets and train a feedforward neural network that takes an input window of sentence embeddings and outputs a probability which represents the coherence of the sentence window. Coherence evaluation is performed by sliding the window over the text and aggregating the score. Li and Jurafsky (2016) study the same model in a larger scale task and also consider a sequence to sequence approach where the model is trained to generate the next sentence given the current sentence and vice versa. Chen et al. (2016) also propose a sentence embedding based approach where they model the probability that one sentence should come before another and define coherence based on the likelihood of the relative order of every pair of sentences. We believe these models are limited by the fact that they are local in nature and our experiments show that exploiting larger contexts can be very beneficial. Hierarchical RNNs for document modeling Word level and sentence level RNNs have been used in a hierarchical fashion for modeling documents in prior work. Li et al. (2015b) proposed a hierarchical document autoencoder which has potential to be used in generation and summarization applications. More relevant to our work is a similar model (but without an encoder) considered by Lin et al. (2015). A sentence level RNN predicts the bag of words in the next sentence given the previous sentences and a word level RNN predicts the word sequence conditioned on the sentence level RNN hidden state.", "startOffset": 54, "endOffset": 3359}, {"referenceID": 1, "context": "A popular model of coherence is the Entity-Grid model Barzilay and Lapata (2008) which captures local coherence by modeling patterns of entity distributions in the discourse. Sentences are represented by the syntactic roles of entities appearing in the document and entity transition frequencies in successive sentences are treated as features that are are used to train a ranking SVM. These two approaches find motivation from ideas in centering theory (Grosz et al., 1995) which state that nouns and entities in coherent discourses exhibit certain patterns. Global models of coherence typically use an HMM to model document structure. The content model proposed by Barzilay and Lee (2004) represents topics in a particular domain as states in an HMM. State transitions capture possible presentation orderings within the domain. Words of a sentence are modeled using a topic-specific language model. The content model has inspired several subsequent work to combine the strengths of local and global models. Elsner et al. (2007) combine the entity model and the content model using a non-parametric HMM. Soricut and Marcu (2006) use several models as feature functions and define a log linear model to assign probability to a given text. Louis and Nenkova (2012) attempt to capture the intentional structure in documents using syntax as a proxy for the communicative goal of a sentence. Syntax features such as parse tree production rules and constituency tags at a particular tree depth were used. Unlike previous approaches, we do not employ any handcrafted features and adopt an embedding based approach. Local coherence is taken into account by having a next sentence prediction component in the model and global dependencies are naturally captured by an RNN. We demonstrate that our model is able to capture both logical and topical structure by evaluating its performance on different types of data. Data-driven approaches Neural approaches have gained attention more recently. Li and Hovy (2014) model sentences as embeddings derived from recurrent/recursive neural nets and train a feedforward neural network that takes an input window of sentence embeddings and outputs a probability which represents the coherence of the sentence window. Coherence evaluation is performed by sliding the window over the text and aggregating the score. Li and Jurafsky (2016) study the same model in a larger scale task and also consider a sequence to sequence approach where the model is trained to generate the next sentence given the current sentence and vice versa. Chen et al. (2016) also propose a sentence embedding based approach where they model the probability that one sentence should come before another and define coherence based on the likelihood of the relative order of every pair of sentences. We believe these models are limited by the fact that they are local in nature and our experiments show that exploiting larger contexts can be very beneficial. Hierarchical RNNs for document modeling Word level and sentence level RNNs have been used in a hierarchical fashion for modeling documents in prior work. Li et al. (2015b) proposed a hierarchical document autoencoder which has potential to be used in generation and summarization applications. More relevant to our work is a similar model (but without an encoder) considered by Lin et al. (2015). A sentence level RNN predicts the bag of words in the next sentence given the previous sentences and a word level RNN predicts the word sequence conditioned on the sentence level RNN hidden state. The model has a structure similar to the content model of Barzilay and Lee (2004) with RNNs playing the roles of the HMM and the bigram language model.", "startOffset": 54, "endOffset": 3639}, {"referenceID": 1, "context": "A popular model of coherence is the Entity-Grid model Barzilay and Lapata (2008) which captures local coherence by modeling patterns of entity distributions in the discourse. Sentences are represented by the syntactic roles of entities appearing in the document and entity transition frequencies in successive sentences are treated as features that are are used to train a ranking SVM. These two approaches find motivation from ideas in centering theory (Grosz et al., 1995) which state that nouns and entities in coherent discourses exhibit certain patterns. Global models of coherence typically use an HMM to model document structure. The content model proposed by Barzilay and Lee (2004) represents topics in a particular domain as states in an HMM. State transitions capture possible presentation orderings within the domain. Words of a sentence are modeled using a topic-specific language model. The content model has inspired several subsequent work to combine the strengths of local and global models. Elsner et al. (2007) combine the entity model and the content model using a non-parametric HMM. Soricut and Marcu (2006) use several models as feature functions and define a log linear model to assign probability to a given text. Louis and Nenkova (2012) attempt to capture the intentional structure in documents using syntax as a proxy for the communicative goal of a sentence. Syntax features such as parse tree production rules and constituency tags at a particular tree depth were used. Unlike previous approaches, we do not employ any handcrafted features and adopt an embedding based approach. Local coherence is taken into account by having a next sentence prediction component in the model and global dependencies are naturally captured by an RNN. We demonstrate that our model is able to capture both logical and topical structure by evaluating its performance on different types of data. Data-driven approaches Neural approaches have gained attention more recently. Li and Hovy (2014) model sentences as embeddings derived from recurrent/recursive neural nets and train a feedforward neural network that takes an input window of sentence embeddings and outputs a probability which represents the coherence of the sentence window. Coherence evaluation is performed by sliding the window over the text and aggregating the score. Li and Jurafsky (2016) study the same model in a larger scale task and also consider a sequence to sequence approach where the model is trained to generate the next sentence given the current sentence and vice versa. Chen et al. (2016) also propose a sentence embedding based approach where they model the probability that one sentence should come before another and define coherence based on the likelihood of the relative order of every pair of sentences. We believe these models are limited by the fact that they are local in nature and our experiments show that exploiting larger contexts can be very beneficial. Hierarchical RNNs for document modeling Word level and sentence level RNNs have been used in a hierarchical fashion for modeling documents in prior work. Li et al. (2015b) proposed a hierarchical document autoencoder which has potential to be used in generation and summarization applications. More relevant to our work is a similar model (but without an encoder) considered by Lin et al. (2015). A sentence level RNN predicts the bag of words in the next sentence given the previous sentences and a word level RNN predicts the word sequence conditioned on the sentence level RNN hidden state. The model has a structure similar to the content model of Barzilay and Lee (2004) with RNNs playing the roles of the HMM and the bigram language model. Our model has a hierarchical nature in that a sentence level RNN operates over words of a sentence and a document level RNN operates over sentence embeddings. Combinatorial optimization with RNNs Vinyals et al. (2015a) equip sequence to sequence models with the capability to handle input and output sets, and discuss experiments on sorting, language modeling and parsing.", "startOffset": 54, "endOffset": 3928}, {"referenceID": 1, "context": "A popular model of coherence is the Entity-Grid model Barzilay and Lapata (2008) which captures local coherence by modeling patterns of entity distributions in the discourse. Sentences are represented by the syntactic roles of entities appearing in the document and entity transition frequencies in successive sentences are treated as features that are are used to train a ranking SVM. These two approaches find motivation from ideas in centering theory (Grosz et al., 1995) which state that nouns and entities in coherent discourses exhibit certain patterns. Global models of coherence typically use an HMM to model document structure. The content model proposed by Barzilay and Lee (2004) represents topics in a particular domain as states in an HMM. State transitions capture possible presentation orderings within the domain. Words of a sentence are modeled using a topic-specific language model. The content model has inspired several subsequent work to combine the strengths of local and global models. Elsner et al. (2007) combine the entity model and the content model using a non-parametric HMM. Soricut and Marcu (2006) use several models as feature functions and define a log linear model to assign probability to a given text. Louis and Nenkova (2012) attempt to capture the intentional structure in documents using syntax as a proxy for the communicative goal of a sentence. Syntax features such as parse tree production rules and constituency tags at a particular tree depth were used. Unlike previous approaches, we do not employ any handcrafted features and adopt an embedding based approach. Local coherence is taken into account by having a next sentence prediction component in the model and global dependencies are naturally captured by an RNN. We demonstrate that our model is able to capture both logical and topical structure by evaluating its performance on different types of data. Data-driven approaches Neural approaches have gained attention more recently. Li and Hovy (2014) model sentences as embeddings derived from recurrent/recursive neural nets and train a feedforward neural network that takes an input window of sentence embeddings and outputs a probability which represents the coherence of the sentence window. Coherence evaluation is performed by sliding the window over the text and aggregating the score. Li and Jurafsky (2016) study the same model in a larger scale task and also consider a sequence to sequence approach where the model is trained to generate the next sentence given the current sentence and vice versa. Chen et al. (2016) also propose a sentence embedding based approach where they model the probability that one sentence should come before another and define coherence based on the likelihood of the relative order of every pair of sentences. We believe these models are limited by the fact that they are local in nature and our experiments show that exploiting larger contexts can be very beneficial. Hierarchical RNNs for document modeling Word level and sentence level RNNs have been used in a hierarchical fashion for modeling documents in prior work. Li et al. (2015b) proposed a hierarchical document autoencoder which has potential to be used in generation and summarization applications. More relevant to our work is a similar model (but without an encoder) considered by Lin et al. (2015). A sentence level RNN predicts the bag of words in the next sentence given the previous sentences and a word level RNN predicts the word sequence conditioned on the sentence level RNN hidden state. The model has a structure similar to the content model of Barzilay and Lee (2004) with RNNs playing the roles of the HMM and the bigram language model. Our model has a hierarchical nature in that a sentence level RNN operates over words of a sentence and a document level RNN operates over sentence embeddings. Combinatorial optimization with RNNs Vinyals et al. (2015a) equip sequence to sequence models with the capability to handle input and output sets, and discuss experiments on sorting, language modeling and parsing. Their goal is to show that input and output orderings can matter in these tasks, which is demonstrated using several small scale experiments. Our work exploits this framework to address the challenging problem of modeling logical and hierarchical structure in text. Vinyals et al. (2015b) proposed pointer-networks, aimed at combinatorial optimization problems where the output dictionary size depends on the number of input elements.", "startOffset": 54, "endOffset": 4371}, {"referenceID": 28, "context": "Our model is based on the read, process and write framework proposed by Vinyals et al. (2015a) briefly discussed in section 1.", "startOffset": 72, "endOffset": 95}, {"referenceID": 28, "context": "Our model is based on the read, process and write framework proposed by Vinyals et al. (2015a) briefly discussed in section 1. We use the encoder-decoder terminology that is more common in the literature in the following discussion. The model is comprised of a sentence encoder RNN, an encoder RNN and a decoder RNN (figure 1). An RNN sentence encoder takes as input the words of a sentence s sequentially and computes an embedding representation of the sentence (Figure 1a). Henceforth, we shall use s to refer to a sentence or its embedding interchangeably. The embeddings {s1, s2, ..., sn} of a given set of n sentences constitute the sentence memory, available to be accessed by subsequent components. The encoder is identical to the originally proposed process block and is defined by equations 1-5 (See Figure 1b). Following the regular LSTM hidden state (ht\u22121 enc , c t\u22121 enc ) update, the hidden state is concatenated with an attention readout vector satt, and this concatenated vector is treated as the hidden state for the next time step (Equation 5). Attention probabilities are computed by composing the hidden state with embeddings of the candidate sentences through a scoring function f and taking the softmax (Equations 2, 3). This process is iterated for a number of times, called the number of read cycles. As described in Vinyals et al. (2015a) the encoder has the desirable property of being invariant to the order in which the sentence embeddings reside in the memory.", "startOffset": 72, "endOffset": 1363}, {"referenceID": 14, "context": "Note that the structure of this scoring function is similar to the window network in Li and Hovy (2014). While they used a local window of sentences to capture context, this scoring function exploits the RNN hidden state to score sentence candidates.", "startOffset": 85, "endOffset": 104}, {"referenceID": 22, "context": "We use pre-trained 300 dimensional GloVe word embeddings (Pennington et al., 2014).", "startOffset": 57, "endOffset": 82}, {"referenceID": 9, "context": "We used a batch size of 10 and the Adam optimizer (Kingma and Ba, 2014) with a base learning rate of 5e-4 for all experiments.", "startOffset": 50, "endOffset": 71}, {"referenceID": 1, "context": "Given a reference paragraph and a permuted version of it, the more coherently organized one needs to be identified (Barzilay and Lapata, 2008).", "startOffset": 115, "endOffset": 142}, {"referenceID": 1, "context": "We consider data from two different domains that have been widely used for this task in previous work since Barzilay and Lee (2004); Barzilay and Lapata (2008).", "startOffset": 108, "endOffset": 132}, {"referenceID": 1, "context": "We consider data from two different domains that have been widely used for this task in previous work since Barzilay and Lee (2004); Barzilay and Lapata (2008). The ACCIDENTS data (aka AIRPLANE data) is a set of aviation accident reports from the National Transportation Safety Board\u2019s database.", "startOffset": 133, "endOffset": 160}, {"referenceID": 14, "context": "The entity grid model provides a strong baseline on the ACCIDENTS dataset, only outperformed by our model and Li and Jurafsky (2016) .", "startOffset": 110, "endOffset": 133}, {"referenceID": 14, "context": "On the EARTHQUAKE data the window approach of Li and Hovy (2014) and Li and Jurafsky (2016) perform strongly.", "startOffset": 46, "endOffset": 65}, {"referenceID": 14, "context": "On the EARTHQUAKE data the window approach of Li and Hovy (2014) and Li and Jurafsky (2016) perform strongly.", "startOffset": 46, "endOffset": 92}, {"referenceID": 1, "context": "Methods ACCIDENTS EARTHQUAKES Barzilay and Lapata (2008) 0.", "startOffset": 30, "endOffset": 57}, {"referenceID": 1, "context": "Methods ACCIDENTS EARTHQUAKES Barzilay and Lapata (2008) 0.904 0.872 Louis and Nenkova (2012) 0.", "startOffset": 30, "endOffset": 94}, {"referenceID": 1, "context": "Methods ACCIDENTS EARTHQUAKES Barzilay and Lapata (2008) 0.904 0.872 Louis and Nenkova (2012) 0.842 0.957 Guinaudeau and Strube (2013) 0.", "startOffset": 30, "endOffset": 135}, {"referenceID": 1, "context": "Methods ACCIDENTS EARTHQUAKES Barzilay and Lapata (2008) 0.904 0.872 Louis and Nenkova (2012) 0.842 0.957 Guinaudeau and Strube (2013) 0.846 0.635 Li and Hovy (2014) - Recurrent 0.", "startOffset": 30, "endOffset": 166}, {"referenceID": 1, "context": "Methods ACCIDENTS EARTHQUAKES Barzilay and Lapata (2008) 0.904 0.872 Louis and Nenkova (2012) 0.842 0.957 Guinaudeau and Strube (2013) 0.846 0.635 Li and Hovy (2014) - Recurrent 0.840 0.951 Li and Hovy (2014) - Recursive 0.", "startOffset": 30, "endOffset": 209}, {"referenceID": 1, "context": "Methods ACCIDENTS EARTHQUAKES Barzilay and Lapata (2008) 0.904 0.872 Louis and Nenkova (2012) 0.842 0.957 Guinaudeau and Strube (2013) 0.846 0.635 Li and Hovy (2014) - Recurrent 0.840 0.951 Li and Hovy (2014) - Recursive 0.864 0.976 Li and Jurafsky (2016) 0.", "startOffset": 30, "endOffset": 256}, {"referenceID": 23, "context": "A second source of abstracts we consider are papers from the ACL Anthology Network (AAN) corpus (Radev et al., 2009) of ACL papers.", "startOffset": 96, "endOffset": 116}, {"referenceID": 12, "context": "Lapata (2006) discusses that this metric reliably correlates with human judgements.", "startOffset": 0, "endOffset": 14}, {"referenceID": 1, "context": "46 0 Entity Grid (Barzilay and Lapata, 2008) 20.", "startOffset": 17, "endOffset": 44}, {"referenceID": 15, "context": "10 - Seq2seq (Uni) (Li and Jurafsky, 2016) 27.", "startOffset": 19, "endOffset": 42}, {"referenceID": 14, "context": "10 Window network (Li and Hovy, 2014) 41.", "startOffset": 18, "endOffset": 37}, {"referenceID": 1, "context": "Our first baseline is the Entity Grid model of Barzilay and Lapata (2008). We use the Stanford parser Klein and Manning (2003) to get constituency trees for all sentences in our datasets.", "startOffset": 47, "endOffset": 74}, {"referenceID": 1, "context": "Our first baseline is the Entity Grid model of Barzilay and Lapata (2008). We use the Stanford parser Klein and Manning (2003) to get constituency trees for all sentences in our datasets.", "startOffset": 47, "endOffset": 127}, {"referenceID": 1, "context": "Our first baseline is the Entity Grid model of Barzilay and Lapata (2008). We use the Stanford parser Klein and Manning (2003) to get constituency trees for all sentences in our datasets. We derive entity grid representations for the parsed sentences using the Brown Coherence Toolkit.3 A ranking SVM is trained to score correct orderings higher than incorrect orderings as in the original work. We used 20 permutations per document as training data. Since the entity grid representation only provides a means of feature extraction we evaluate the model in the ordering setting as follows. We choose 1000 random permutations for each document, one of them being the correct order, and pick the order with maximum coherence. We experimented with transitions of length at most 3 in the entity-grid. Sequence to sequence. The second baseline we consider is a sequence to sequence model which is trained to predict the next sentence given the current sentence. Li and Jurafsky (2016) consider similar methods and our model is same as the uni-directional model in their work.", "startOffset": 47, "endOffset": 980}, {"referenceID": 1, "context": "Our first baseline is the Entity Grid model of Barzilay and Lapata (2008). We use the Stanford parser Klein and Manning (2003) to get constituency trees for all sentences in our datasets. We derive entity grid representations for the parsed sentences using the Brown Coherence Toolkit.3 A ranking SVM is trained to score correct orderings higher than incorrect orderings as in the original work. We used 20 permutations per document as training data. Since the entity grid representation only provides a means of feature extraction we evaluate the model in the ordering setting as follows. We choose 1000 random permutations for each document, one of them being the correct order, and pick the order with maximum coherence. We experimented with transitions of length at most 3 in the entity-grid. Sequence to sequence. The second baseline we consider is a sequence to sequence model which is trained to predict the next sentence given the current sentence. Li and Jurafsky (2016) consider similar methods and our model is same as the uni-directional model in their work. These methods were shown to yield sentence embeddings that have competitive performance in several semantic tasks in Kiros et al. (2015). Window Network.", "startOffset": 47, "endOffset": 1208}, {"referenceID": 1, "context": "Our first baseline is the Entity Grid model of Barzilay and Lapata (2008). We use the Stanford parser Klein and Manning (2003) to get constituency trees for all sentences in our datasets. We derive entity grid representations for the parsed sentences using the Brown Coherence Toolkit.3 A ranking SVM is trained to score correct orderings higher than incorrect orderings as in the original work. We used 20 permutations per document as training data. Since the entity grid representation only provides a means of feature extraction we evaluate the model in the ordering setting as follows. We choose 1000 random permutations for each document, one of them being the correct order, and pick the order with maximum coherence. We experimented with transitions of length at most 3 in the entity-grid. Sequence to sequence. The second baseline we consider is a sequence to sequence model which is trained to predict the next sentence given the current sentence. Li and Jurafsky (2016) consider similar methods and our model is same as the uni-directional model in their work. These methods were shown to yield sentence embeddings that have competitive performance in several semantic tasks in Kiros et al. (2015). Window Network. We consider the window approach of Li and Hovy (2014) and Li and Jurafsky (2016) which demonstrated strong performance in the order discrimination task as our third baseline.", "startOffset": 47, "endOffset": 1279}, {"referenceID": 1, "context": "Our first baseline is the Entity Grid model of Barzilay and Lapata (2008). We use the Stanford parser Klein and Manning (2003) to get constituency trees for all sentences in our datasets. We derive entity grid representations for the parsed sentences using the Brown Coherence Toolkit.3 A ranking SVM is trained to score correct orderings higher than incorrect orderings as in the original work. We used 20 permutations per document as training data. Since the entity grid representation only provides a means of feature extraction we evaluate the model in the ordering setting as follows. We choose 1000 random permutations for each document, one of them being the correct order, and pick the order with maximum coherence. We experimented with transitions of length at most 3 in the entity-grid. Sequence to sequence. The second baseline we consider is a sequence to sequence model which is trained to predict the next sentence given the current sentence. Li and Jurafsky (2016) consider similar methods and our model is same as the uni-directional model in their work. These methods were shown to yield sentence embeddings that have competitive performance in several semantic tasks in Kiros et al. (2015). Window Network. We consider the window approach of Li and Hovy (2014) and Li and Jurafsky (2016) which demonstrated strong performance in the order discrimination task as our third baseline.", "startOffset": 47, "endOffset": 1306}, {"referenceID": 10, "context": "However, when the sentence encoder is initialized with a good set of parameters (such as pre-trained skip-thought parameters (Kiros et al., 2015)) this compensates for the lack of an encoder and leads to better performance.", "startOffset": 125, "endOffset": 145}, {"referenceID": 13, "context": "Interestingly, Li and Jurafsky (2016) observe that the seq2seq model outperforms the window network in an order discrimination task on wikipedia data.", "startOffset": 15, "endOffset": 38}, {"referenceID": 2, "context": "While approaches based on the content model of Barzilay and Lee (2004) attempt to explicitly capture topics by discovering clusters in sentences, we observe that the neural approach implicitly discovers such structure.", "startOffset": 47, "endOffset": 71}, {"referenceID": 10, "context": "We chose the BookCorpus dataset (Kiros et al., 2015) for this purpose.", "startOffset": 32, "endOffset": 52}, {"referenceID": 10, "context": "We chose the BookCorpus dataset (Kiros et al., 2015) for this purpose. We trained the model with two key changes from the models trained on the abstracts data - 1) In addition to the sentences in the paragraph being considered, we added more contrastive sentences from other paragraphs as well. 2) We use the bilinear scoring function. These techniques helped obtain better representations when training on large amounts of data. To evaluate the quality of the sentence embeddings derived from the model, we use the evaluation pipeline of Kiros et al. (2015) for tasks that involve understanding sentence semantics.", "startOffset": 33, "endOffset": 559}, {"referenceID": 27, "context": "Method r \u03c1 MSE Purely supervised methods DT-RNN (Tai et al., 2015) 0.", "startOffset": 48, "endOffset": 66}, {"referenceID": 27, "context": "382 LSTM (Tai et al., 2015) 0.", "startOffset": 9, "endOffset": 27}, {"referenceID": 27, "context": "283 DT-LSTM (Tai et al., 2015) 0.", "startOffset": 12, "endOffset": 30}, {"referenceID": 10, "context": "253 Classifier trained on sentence embeddings skip-bow (Kiros et al., 2015) 0.", "startOffset": 55, "endOffset": 75}, {"referenceID": 10, "context": "398 uni-skip (Kiros et al., 2015) 0.", "startOffset": 13, "endOffset": 33}, {"referenceID": 10, "context": "0 Classifier trained on sentence embeddings uni-skip-bow (Kiros et al., 2015) 67.", "startOffset": 57, "endOffset": 77}, {"referenceID": 10, "context": "3 uni-skip (Kiros et al., 2015) 73.", "startOffset": 11, "endOffset": 31}, {"referenceID": 21, "context": "Method Acc F1 Purely supervised methods Socher et al. (2011) 76.", "startOffset": 40, "endOffset": 61}, {"referenceID": 18, "context": "6 Madnani et al. (2012) 77.", "startOffset": 2, "endOffset": 24}, {"referenceID": 8, "context": "1 Ji and Eisenstein (2013) 80.", "startOffset": 2, "endOffset": 27}], "year": 2016, "abstractText": "Modeling the structure of coherent texts is a task of great importance in NLP. The task of organizing a given set of sentences into a coherent order has been commonly used to build and evaluate models that understand such structure. In this work we propose an end-to-end neural approach based on the recently proposed set to sequence mapping framework to address the sentence ordering problem. Our model achieves state-of-the-art performance in the order discrimination task on two datasets widely used in the literature. We also consider a new interesting task of ordering abstracts from conference papers and research proposals and demonstrate strong performance against recent methods. Visualizing the sentence representations learned by the model shows that the model has captured high level logical structure in these paragraphs. The model also learns rich semantic sentence representations by learning to order texts, performing comparably to recent unsupervised representation learning methods in the sentence similarity and paraphrase detection tasks.", "creator": "LaTeX with hyperref package"}}}