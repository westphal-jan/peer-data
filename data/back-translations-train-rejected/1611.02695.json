{"id": "1611.02695", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Nov-2016", "title": "Automatic recognition of child speech for robotic applications in noisy environments", "abstract": "Automatic speech recognition (ASR) allows a natural and intuitive interface for robotic educational applications for children. However there are a number of challenges to overcome to allow such an interface to operate robustly in realistic settings, including the intrinsic difficulties of recognising child speech and high levels of background noise often present in classrooms. As part of the EU EASEL project we have provided several contributions to address these challenges, implementing our own ASR module for use in robotics applications. We used the latest deep neural network algorithms which provide a leap in performance over the traditional GMM approach, and apply data augmentation methods to improve robustness to noise and speaker variation. We provide a close integration between the ASR module and the rest of the dialogue system, allowing the ASR to receive in real-time the language models relevant to the current section of the dialogue, greatly improving the accuracy. We integrated our ASR module into an interactive, multimodal system using a small humanoid robot to help children learn about exercise and energy. The system was installed at a public museum event as part of a research study where 320 children (aged 3 to 14) interacted with the robot, with our ASR achieving 90% accuracy for fluent and near-fluent speech.", "histories": [["v1", "Tue, 8 Nov 2016 09:50:30 GMT  (6892kb,D)", "http://arxiv.org/abs/1611.02695v1", "Submission to Computer Speech and Language, special issue on Interaction Technologies for Children"]], "COMMENTS": "Submission to Computer Speech and Language, special issue on Interaction Technologies for Children", "reviews": [], "SUBJECTS": "cs.CL cs.SD", "authors": ["samuel fernando", "roger k moore", "david cameron", "emily c collins", "abigail millings", "amanda j sharkey", "tony j prescott"], "accepted": false, "id": "1611.02695"}, "pdf": {"name": "1611.02695.pdf", "metadata": {"source": "CRF", "title": "Automatic recognition of child speech for robotic applications in noisy environments", "authors": ["Samuel Fernando", "Roger K. Moore", "David Cameron", "Emily C. Collins", "Abigail Millings", "Amanda J. Sharkey", "Tony J. Prescott"], "emails": [], "sections": [{"heading": null, "text": "Automatic Speech Recognition (ASR) provides a natural and intuitive user interface for robotic educational applications for children. However, there are a number of challenges that need to be overcome for such an interface to function robustly in realistic environments, including the intrinsic difficulties of recognizing children's language and the high level of background noise that is often present in classrooms. As part of the EU project EASEL, we have made several contributions to address these challenges and implemented our own ASR module for use in robotic applications. We have used the latest deep neural network algorithms, which offer a performance leap over the traditional GMM approach, and use data augmentation methods to improve robustness to noise and speaker variations. We offer close integration between the ASR module and the rest of the dialogue system, enabling the ASR to obtain the language models relevant to the current part of the dialogue in real time, which greatly improves the accuracy of the dialogue."}, {"heading": "1. Introduction", "text": "As part of the EU project EASEL (Expressive Agents for Symbiotic Education and Learning), we research the interactions of children and adolescents in social and educational contexts. For these interactions, the ability of automatic speech recognition (ASR) provides a natural and intuitive interface to traditional computer interfaces (Williams et al., 2000), including an effective and pleasant experience with the children. Furthermore, such an interface offers better learning outcomes (Mostow et al., 2008)."}, {"heading": "2. Related work", "text": "Apart from the difficulties of recognizing children's language and dealing with background noise, there are few high-quality voice recognition solutions that are readily available to researchers who are not ASR experts. Off-the-shelf commercial applications are usually designed for specialized applications such as dictation or search, and are not suitable for robotic dialog systems. There are open source toolkits such as Julius (Lee and Kawahara, 2009) or HTK (Young et al., 1997), but these can be difficult for laymen to configure. In addition, many of these toolkits have been stagnant or underdeveloped in recent years, meaning that they lack the powerful algorithms and techniques that have been developed in recent years. As a result of these challenges, there have been relatively few applications of ASR for children's language, despite the desirability of such a system for many scenarios. The main areas of application so far have been reading tutors (Mostow, 2013, this system is relatively useful for the ASR)."}, {"heading": "3. The EASEL project use-case", "text": "The work on ASR described in this article is part of the wider EU project EASEL. In this project, we are developing robot tutoring systems to help children familiarise themselves with scientific topics. As a consortium, we are investigating the social, psychological and educational aspects of interactions as well as technical skills. However, in this article, we focus on our particular Sheffield use case, the Healthy Living Scenario. This interaction involves a small humanoid robot that acts as a tutor to help children learn about healthy living, in particular about exercise and energy."}, {"heading": "3.1. Physical setup for the Healthy Living Interaction", "text": "In fact, it is the case that most of us are able to put ourselves in a different world, in which they are able to move, in which they move, in which they move, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they, in which they live, in which they live, in which they, in which they are able to put themselves into another world, in which they are able, in which they live, in which they live, in which they live in which they live, in which they live in which they live in which they live, in which they live in which they live, in which they live in which they live, in which they live in which they live in which they live, in which they live in which they live, in which they live in which they live, in which they live in which they live, in which they live in which they live, in which they live in which they live in which they live, in which they live, in which they live in which they live, in which they live in which they live, in which they live, in which they live, in which they live, in which they live, in which they live in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which"}, {"heading": "3.2. The Healthy-Living Interaction", "text": "Ae\u00fccnlllrrrrrrrrllrrrrrllrrrrlrrrrlrrrlrrrlrrrlrrrlrrrlrrrlrrrlrrrlrrrlrrrrrlrrrlrrrrlrrrrrrlrrrlrrrrrlrrrlrrrlrrrrrlrrlrrrrrlrrrrlrrrlrrrlrrrlrrlrrrlrrrrlrrrlrrrlrlrrrrlrrlrrrlrlrrrlrlrrlrrlrlrrrlrlrlrlrrrrlrlrlrrrrlrlrlrrlrrlrrrrlrlrlrrrlrrrrrlrrrrrrrlrrrrrlrrrlrrrrrrrrrlrrrrlrrrrrrrrlrrrrrrrrrrrrlrrrrrrrrrrrrrrrrrrlrrrrrrrrrrrrrrrrrrrlrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrlrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr"}, {"heading": "4. The SF-Kaldi-ASR setup", "text": "We used the Kaldi Open Source ASR Toolkit to train our acoustic models and developed the ASR module, which we integrated into the EASEL system described above. In this section we describe the data and methods used to create this module."}, {"heading": "4.1. Training data", "text": "We need a system that will be used in the classrooms and museums where native speakers sit. Therefore, we need corporations of the English language to develop our acoustic model. We need two corpora. The first is the English version of the English language (Russell, 2006), which we use as a whole to create a single acoustic model that is suitable for both adults and children."}, {"heading": "4.2. Noise data for augmentation", "text": "To enhance the robustness of the noise, we used background noise to supplement the training data described above. To this end, we used the CHiME corpus (Christensen et al., 2010), which contains various types of background noise that are recorded in real-world environments. As our primary relevant use case for the ASR is a public museum, we decided that the \"caf\u00e9\" background noise would be the most appropriate type of noise for our model. For each expression in the PF and WSJCAM corpus, a section of the noisy body of equal length was randomly selected and added to the sound."}, {"heading": "4.3. Kaldi acoustic modelling", "text": "We used the Kaldi toolkit to train the acoustic models for the ASR system. The toolkit has relatively standardized scripts (collectively known as recipes) designed to work with different sets of training data. We followed the Wall Street Journal's (WSJ) recipe. Training began with a standard monophone system that uses standard 13-dimensional MFCCs along with derivatives of first and second order. Cepstral means normalization is used everywhere to reduce the channel effect. A more refined system was then constructed with loudspeaker-independent alignments from the monophone system, and a linear discriminant analysis (LDA) was applied to select the most discriminatory dimensions from a large context (five frames left and right).Another refined system was then constructed by applying a maximum probability of linear transform (MLLT) onto the LDA model."}, {"heading": "4.4. Language models and pronunciation dictionary", "text": "For our purposes, we need two main types of language models. The first are large-language models that include the language found in the training data. To do this, we use the MIT n-gram language modelling tool (Hsu and Glass, 2008), which uses a text corpus as input and creates an ngram model that can then be inserted into Kaldi. We combine the text from the PF and WSJCAM corpora as input to this tool and then compile a bigram language model for use in Kaldi, following the Voxforge example in Kaldi. The second type of language model we need are small limited grammars that we use during Multiple Choice in Healthy Living Interaction."}, {"heading": "4.5. Online decoding", "text": "Most of them are able to move around in a different country than the one in which they live."}, {"heading": "4.6. Integration with other modules", "text": "The reason for this is that most people will be able to protect themselves without being able to move to safety, which means they will not be able to move to safety, and they will not be able to move to safety either."}, {"heading": "5. Experiments with existing corpora", "text": "We used a Bigram language model calculated using the MITLM toolkit, as described in Section 4.4, using all the text in the corpora to calculate the bigrams. We use the WSJCAM (adult) and the PF (child) described in Section 4.1. We add different levels to the test data to evaluate how well the algorithms work under these conditions. First, we report on the results obtained only with clean training data in Table 2.The DNN approach exceeds the level of clean adults (WSJ)."}, {"heading": "6. Live experiments", "text": "We installed the system at the Weston Park Museum in Sheffield, as described in Section 3.2. The event lasted 14 days during the Easter holidays. In total, we had 367 individuals 11 whom we recorded as participants in the interaction, of which we had 329 interactions in which parents / guardians gave their consent to participate in the study. Out of this subset, we had 326 individuals who participated in the interaction but were not recorded; some children wanted to try, but for whatever reason, wanted, or could not agree to participate in the study (e.g. because their parents were not present or because we did not have consent forms on paper on one occasion); the number also does not include some adults who participated in the interaction because our focus was on the child's speechlessness; consent to record audio during the interaction; and 321 who gave their consent to the video recording; of the 326 children for whom we had consent to the audio recording, we exclude a case in which we did not record the age, the adult's decision, and the child's decision in five cases, in which were detailed."}, {"heading": "6.1. Data transcription", "text": "The transcribers used the Xtrans software12, created by the Linguistic Data Consortium, to comment on the files. Transcribers were provided with the automatically generated transcriptions created during the experiment.12 They were asked to review and, if necessary, correct each of these transcriptions, as well as to search for other statements that might not be recognized12https: / / www.ldc.upenn.edu / language-resources / tools / xtransby the ASR. They were instructed that the segments of interest were a clear language of the child participant addressed to the robotic; they were asked to exclude sounds from the robot, speech or other people in the room, and to direct the child's speech to others in the room, especially if it was whispered or muttered. Since we wanted to test the effectiveness of the ASR in background noise, we asked the transcribers to leave such speech at the beginning and end of the speech."}, {"heading": "6.2. Fluent and in-vocabulary speech", "text": "From the transcriptions, we can easily categorize utterances into fluent and non-fluent utterances. Non-fluid or mispronounced utterances were marked with symbols to indicate mispronunciation and false starts. We marked all of these utterances as fluent and all others as fluent. We further divided the fluent utterances into expected and unexpected utterances based on whether or not the utterance appears in the recognition vocabulary. In total, there are only 24 sentences expected during the interaction (e.g. \"Hello Zeeno, I am ready to begin,\" \"Moved slowly for ten seconds,\" etc.) We also added the keyword \"! SIL\" as an expected utterance; this keyword denotes non-language / background noise. All other (fluent) utterances were marked as unexpected. In total, we had 4707 transcribed utterances. Of these 854 were marked as fluent, while 3853 were marked as fluent."}, {"heading": "6.3. Accuracy of recognition during live experiments", "text": "In this section, we evaluate the accuracy of the ASR module during the live experiments. To this end, we focus on the fluent, expected expressions described in the previous section. We ran with ASR for 266 of the 320 participants, but there was a system crash for one participant giving a set of 265. For these 265 participants, we received a total of 3596 expressions that we considered fluent, while the remaining 509 contained some deviations."}, {"heading": "6.4. Segmentation errors", "text": "This year, it has reached the point where it will be able to retaliate."}, {"heading": "7. Conclusions and Further Work", "text": "In this article, we presented our work on developing an ASR module for use in the EASEL project. Our module was integrated into a multimodal system using a humanoid robot and vision sensors. Using this system, we developed the Healthy Living Interaction, which helps children learn movement and energy in an interactive way. This system was successfully used in a public museum, with 325 children participating in the interaction for research purposes. We provided an initial analysis of the data collected in this article. We measured the number of fluent and expected expressions and how they varied by gender and age. We also evaluated the accuracy of the ASR module on these interactions, with results showing that over 90% of the fluent expressions contained in the vocabulary were correctly recognized. In future work, we plan to use this valuable data set to further improve our system. We will use the marked data as training and test data for thorough evaluations."}, {"heading": "Acknowledgements", "text": "This work is supported by the Seventh Framework Programme of the European Union (FP7-ICT-2013-10) under Funding Agreement No. 611971.We would like to thank Theo Botsford and Jenny Harding for their help in developing the interaction setups and hosting the event at the Museum, as well as Jon Barker and Saeid Mokaram for their technical advice in developing the ASR."}], "references": [{"title": "Openfst: A general and efficient weighted finite-state transducer library. In: Implementation and Application of Automata", "author": ["C. Allauzen", "M. Riley", "J. Schalkwyk", "W. Skut", "M. Mohri"], "venue": null, "citeRegEx": "Allauzen et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Allauzen et al\\.", "year": 2007}, {"title": "Multimodal child-robot interaction: Building social bonds", "author": ["T. Belpaeme", "P.E. Baxter", "R. Read", "R. Wood", "H. Cuay\u00e1huitl", "B. Kiefer", "S. Racioppa", "I. Kruijff-Korbayov\u00e1", "G. Athanasopoulos", "V Enescu"], "venue": "Journal of Human-Robot Interaction", "citeRegEx": "Belpaeme et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Belpaeme et al\\.", "year": 2012}, {"title": "Joint-sequence models for grapheme-to-phoneme conversion", "author": ["M. Bisani", "H. Ney"], "venue": "Speech Communication", "citeRegEx": "Bisani and Ney,? \\Q2008\\E", "shortCiteRegEx": "Bisani and Ney", "year": 2008}, {"title": "The chime corpus: a resource and a challenge for computational hearing in multisource environments", "author": ["H. Christensen", "J. Barker", "N. Ma", "P.D. Green"], "venue": null, "citeRegEx": "Christensen et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Christensen et al\\.", "year": 2010}, {"title": "The relationship between age and frequency of disfluency in preschool children", "author": ["D.A. DeJoy", "H.H. Gregory"], "venue": "Journal of Fluency Disorders", "citeRegEx": "DeJoy and Gregory,? \\Q1985\\E", "shortCiteRegEx": "DeJoy and Gregory", "year": 1985}, {"title": "Comparing open-source speech recognition toolkits", "author": ["C. Gaida", "P. Lange", "R. Petrick", "P. Proba", "A. Malatawy", "D. SuendermannOeft"], "venue": "Tech. rep., Baden-Wuerttemberg Ministry of Science and Arts as part of the research project OASIS", "citeRegEx": "Gaida et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Gaida et al\\.", "year": 2014}, {"title": "Acoustic variability and automatic recognition of childrens speech", "author": ["M. Gerosa", "D. Giuliani", "F. Brugnara"], "venue": "Speech Communication", "citeRegEx": "Gerosa et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Gerosa et al\\.", "year": 2007}, {"title": "Iterative language model estimation: efficient data structure & algorithms", "author": ["Hsu", "B.-J", "J. Glass"], "venue": "Proceedings of Interspeech", "citeRegEx": "Hsu et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Hsu et al\\.", "year": 2008}, {"title": "Recent development of open-source speech recognition engine julius", "author": ["A. Lee", "T. Kawahara"], "venue": "Processing Association,", "citeRegEx": "Lee and Kawahara,? \\Q2009\\E", "shortCiteRegEx": "Lee and Kawahara", "year": 2009}, {"title": "Yarp: yet another robot platform", "author": ["G. Metta", "P. Fitzpatrick", "L. Natale"], "venue": "International Journal on Advanced Robotics Systems", "citeRegEx": "Metta et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Metta et al\\.", "year": 2006}, {"title": "4-month evaluation of a learner-controlled reading tutor that listens", "author": ["J. Mostow", "G. Aist", "C. Huang", "B. Junker", "R. Kennedy", "H. Lan", "D.T. Latimer", "R. O\u2019Connor", "R. Tassone", "B. Tobin", "A. Wierman"], "venue": "The Path of Speech Technologies in Computer Assisted Language Learning: From Research Toward Practice. Routledge,", "citeRegEx": "Mostow et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Mostow et al\\.", "year": 2008}, {"title": "Computer-guided oral reading versus independent practice: Comparison of sustained silent reading", "author": ["J. Mostow", "J. Nelson-Taylor", "J.E. Beck"], "venue": null, "citeRegEx": "Mostow et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mostow et al\\.", "year": 2013}, {"title": "The kaldi speech recognition toolkit", "author": ["D. Povey", "A. Ghoshal", "G. Boulianne", "L. Burget", "O. Glembek", "N. Goel", "M. Hannemann", "P. Motlicek", "Y. Qian", "P. Schwarz", "J. Silovsky", "G. Stemmer", "K. Vesely", "Dec."], "venue": "In: IEEE 2011 Workshop on Automatic Speech Recognition and Understanding. IEEE Signal Processing Society, iEEE Catalog No.: CFP11SRW-USB.", "citeRegEx": "Povey et al\\.,? 2011", "shortCiteRegEx": "Povey et al\\.", "year": 2011}, {"title": "Wsjcamo: a british english speech corpus for large vocabulary continuous speech recognition", "author": ["T. Robinson", "J. Fransen", "D. Pye", "J. Foote", "S. Renals"], "venue": "In: Acoustics, Speech, and Signal Processing,", "citeRegEx": "Robinson et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Robinson et al\\.", "year": 1995}, {"title": "The pf-star british english childrens speech", "author": ["M. Russell"], "venue": null, "citeRegEx": "Russell,? \\Q2006\\E", "shortCiteRegEx": "Russell", "year": 2006}, {"title": "Challenges for computer recognition of children2s speech. In: SLaTE", "author": ["M.J. Russell", "S. D\u2019Arcy"], "venue": null, "citeRegEx": "Russell and D.Arcy,? \\Q2007\\E", "shortCiteRegEx": "Russell and D.Arcy", "year": 2007}, {"title": "Voice controlled child-robot interactions - development of ASR and TTS systems for the Nao robot within the ALIZ-E project", "author": ["G. Sommavilla", "G. Paci", "F. Tesser", "P. Cosi", "July"], "venue": "In: 13th International Conference on Intelligent Autonomous Systems (AIS 2014). Padova, Italy.", "citeRegEx": "Sommavilla et al\\.,? 2014", "shortCiteRegEx": "Sommavilla et al\\.", "year": 2014}, {"title": "Using speech recognition technology to enhance literacy instruction for emerging readers", "author": ["S.M. Williams", "D. Nix", "P. Fairweather"], "venue": "Fourth International Conference of the Learning Sciences", "citeRegEx": "Williams et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Williams et al\\.", "year": 2000}], "referenceMentions": [{"referenceID": 17, "context": "ASR provides benefits in educational applications (Williams et al., 2000) including a more engaging and enjoyable experience for children.", "startOffset": 50, "endOffset": 73}, {"referenceID": 10, "context": "Additionally, such an interface provides improved learning outcomes (Mostow et al., 2008) compared to traditional computer interfaces such as keyboards and mice.", "startOffset": 68, "endOffset": 89}, {"referenceID": 15, "context": "These include the intrinsic difficulty of recognising child speech due to physiological differences in the vocal tract (Russell and D\u2019Arcy, 2007), clarity in pronunciation (especially for younger children), the higher intra-speaker and inter-speaker variability in children\u2019s speech (Gerosa et al.", "startOffset": 119, "endOffset": 145}, {"referenceID": 6, "context": "These include the intrinsic difficulty of recognising child speech due to physiological differences in the vocal tract (Russell and D\u2019Arcy, 2007), clarity in pronunciation (especially for younger children), the higher intra-speaker and inter-speaker variability in children\u2019s speech (Gerosa et al., 2007), and the higher rate of disfluencies when compared to adult speech (DeJoy and Gregory, 1985).", "startOffset": 283, "endOffset": 304}, {"referenceID": 4, "context": ", 2007), and the higher rate of disfluencies when compared to adult speech (DeJoy and Gregory, 1985).", "startOffset": 75, "endOffset": 100}, {"referenceID": 12, "context": "We used the Kaldi (Povey et al., 2011) open source toolkit to develop our ASR module.", "startOffset": 18, "endOffset": 38}, {"referenceID": 8, "context": "There are open-source toolkits available such as Julius (Lee and Kawahara, 2009) or HTK (Young et al.", "startOffset": 56, "endOffset": 80}, {"referenceID": 11, "context": "The main applications so far have been in reading tutors (Mostow et al., 2013) which are a very useful application but are relatively unchallenging for ASR since the text that is going to be uttered is largely known beforehand.", "startOffset": 57, "endOffset": 78}, {"referenceID": 1, "context": "Recently the ALIZ-E social robotics project (Belpaeme et al., 2012) showed promising ASR results in offline experiments (Sommavilla et al.", "startOffset": 44, "endOffset": 67}, {"referenceID": 16, "context": ", 2012) showed promising ASR results in offline experiments (Sommavilla et al., 2014) but this system was not deployed in live interactions.", "startOffset": 60, "endOffset": 85}, {"referenceID": 1, "context": "Recently the ALIZ-E social robotics project (Belpaeme et al., 2012) showed promising ASR results in offline experiments (Sommavilla et al., 2014) but this system was not deployed in live interactions. A major step forward in speech recognition technology has arisen through the emergence of the Kaldi toolkit. Table 1 shows results from Gaida et al. (2014) comparing Kaldi against other major ASR toolkits, with the performance of Kaldi with the DNN approach having a third of the error rate in comparison to the other software.", "startOffset": 45, "endOffset": 357}, {"referenceID": 5, "context": "Table 1: Comparison of ASR toolkits (Gaida et al., 2014), WER results for the WSJ speech corpus.", "startOffset": 36, "endOffset": 56}, {"referenceID": 13, "context": "The first is the British English version of the Wall Street Journal corpus created at the University of Cambridge (Robinson et al., 1995), which we henceforth refer to as the WSJCAM corpus.", "startOffset": 114, "endOffset": 137}, {"referenceID": 14, "context": "The second is the PF-star corpus of British English child speech (Russell, 2006), henceforth referred to as the PF corpus.", "startOffset": 65, "endOffset": 80}, {"referenceID": 3, "context": "For this purpose we used the CHiME corpus (Christensen et al., 2010) which contains various kinds of background noise recorded in real-life environments.", "startOffset": 42, "endOffset": 68}, {"referenceID": 0, "context": "Next we use OpenFST tools (Allauzen et al., 2007) to compile the FSM into a finite-state transducer (FST).", "startOffset": 26, "endOffset": 49}, {"referenceID": 2, "context": "robot names, such as Zeno) we use the Sequitur tool (Bisani and Ney, 2008)", "startOffset": 52, "endOffset": 74}, {"referenceID": 9, "context": "In order to integrate the ASR with the other modules in the EASEL system, we used the YARP middleware (Metta et al., 2006) which is used throughout the EASEL project and has been widely used in many European robotics projects.", "startOffset": 102, "endOffset": 122}], "year": 2016, "abstractText": "Automatic speech recognition (ASR) allows a natural and intuitive interface for robotic educational applications for children. However there are a number of challenges to overcome to allow such an interface to operate robustly in realistic settings, including the intrinsic difficulties of recognising child speech and high levels of background noise often present in classrooms. As part of the EU EASEL project we have provided several contributions to address these challenges, implementing our own ASR module for use in robotics applications. We used the latest deep neural network algorithms which provide a leap in performance over the traditional GMM approach, and apply data augmentation methods to improve robustness to noise and speaker variation. We provide a close integration between the ASR module and the rest of the dialogue system, allowing the ASR to receive in real-time the language models relevant to the current section of the dialogue, greatly improving the accuracy. We integrated our ASR module into an interactive, multimodal system using a small humanoid robot to help children learn about exercise and energy. The system was installed at a public museum event as part of a research study where 320 children (aged 3 to 14) interacted with the robot, with our ASR achieving 90% accuracy for fluent and near-fluent speech.", "creator": "LaTeX with hyperref package"}}}