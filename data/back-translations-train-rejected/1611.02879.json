{"id": "1611.02879", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Nov-2016", "title": "Audio Visual Speech Recognition using Deep Recurrent Neural Networks", "abstract": "In this work, we propose a training algorithm for an audio-visual automatic speech recognition (AV-ASR) system using deep recurrent neural network (RNN).First, we train a deep RNN acoustic model with a Connectionist Temporal Classification (CTC) objective function. The frame labels obtained from the acoustic model are then used to perform a non-linear dimensionality reduction of the visual features using a deep bottleneck network. Audio and visual features are fused and used to train a fusion RNN. The use of bottleneck features for visual modality helps the model to converge properly during training. Our system is evaluated on GRID corpus. Our results show that presence of visual modality gives significant improvement in character error rate (CER) at various levels of noise even when the model is trained without noisy data. We also provide a comparison of two fusion methods: feature fusion and decision fusion.", "histories": [["v1", "Wed, 9 Nov 2016 10:24:52 GMT  (311kb,D)", "http://arxiv.org/abs/1611.02879v1", null]], "reviews": [], "SUBJECTS": "cs.CV cs.CL cs.LG", "authors": ["abhinav thanda", "shankar m venkatesan"], "accepted": false, "id": "1611.02879"}, "pdf": {"name": "1611.02879.pdf", "metadata": {"source": "CRF", "title": "Audio Visual Speech Recognition using Deep Recurrent Neural Networks", "authors": ["Abhinav Thanda", "Shankar M Venkatesan"], "emails": [], "sections": [{"heading": null, "text": "Keywords: audiovisual speech recognition, connectionist temporal classification, recurrent neural network"}, {"heading": "1 Introduction", "text": "Audio visualization of speech (AV-ASR) is a case of multimodal analysis in which two modalities (audio and visual) complement each other to recognize speech. To this end, several methods have been proposed that traditionally include variants of GMM / HMM models. [4] More recently, AV-ASR methods based on deep neural networks (DNN) have been developed. [5] [6]"}, {"heading": "2 Related Work", "text": "The differences between the different AV-ASR systems mainly lie in the methods used for visual feature extraction and audiovisual information fusion. Visual feature extraction methods can be of 3 types [21]: 1. Appearance based on features in which each pixel in the mouth region of the speaker (ROI) is considered informative. Normally, a transformation such as DCT or PCA is applied to the ROI to reduce dimensions. Additional feature processing such as middle normalization, intra-frame and inter-frame LDA can be applied [21]. Form-based features use geometric features such as the height, width and area of the lip region or build a static model of the lip contours whose parameters are used as features. 3. Combination of appearance and shape-based feature features can be broadly divided into two types."}, {"heading": "3 Sequence Labeling Using RNN", "text": "The following notations are taken over in this essay: For an expression u of the length Tu, O u a = (O u a, 1, O u a, 2,..., O u a, Tu) and O u v = (O u v, 1, O u v, 2,..., O u v, Tu) are the observation sequences of audio and picture frames in which Oa, t, Rda and Ov, t, Rdv. We assume the same frame rates for audio and image inputs, which are ensured in experiments by means of interpolation. Ouav = (O u av, 1, O u av, 2,..., O u av, Tu), in which O uav, t, O uv, t], Rdav, in which dav = da + dv denotes the associated features at present t for expression u. The associated label sequence is formed by l = (l1, l2,... lu, Su) on the basis of this text."}, {"heading": "3.1 Bi-directional RNN", "text": "RNNs are a class of neural networks used to map sequences to sequences, which is possible because of the feedback connections between hidden nodes. In a bidirectional RNN, the hidden layer has two components, each corresponding to forward (past) and backward (future) connections. For a given input sequence O = (O1, O2,..., OT), the output of the network is calculated as follows: Forward pass through a hidden layer at a given time t = g (W f hoOt + W f hhhh f \u2212 1 + b f h) (1), where Wfho is the input-to-hidden weights for the forward component, W f hh corresponds to hidden weights between forward components, and b fh is the forward component bias. g is a non-linear unit, depending on the choice of the hidden layer. Similarly, forward is not covered by the problem b and b hb is hidden."}, {"heading": "3.2 Connectionist Temporal Classification", "text": "DNNs used in ASR systems are frame-level classifiers, i.e. each frame of the input sequence requires a class name to train the DNN. Frame level labels are usually HMM states obtained by first training a GMM / HMM model and then by forcing the input sequences to align with the HMM states. The input sequence should be O = (O1, O2,..., OT) and a corresponding label sequence l = (l1, l2,..., lS) eliminates the need for such alignments as it allows the network to learn about all possible alignments. The RNN uses a soft-max output layer that contains a node for each element in L \u2032, where L \u2032 = L \u00b2 \u00b2 (= L \u00b2) is an equivalent output sequence l."}, {"heading": "4 Feature Extraction", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Audio Features", "text": "The sampling rate of the audio data is converted to 16kHz. For each frame of the speech signal of 25ms duration, filter bank features of 40 dimensions are extracted. Filter bank features are normalized as an average and \u2206 and \u2206 \u2206 \u2206 \u2206 attributes are appended. The last 120 dimensional attributes are used as audio attributes."}, {"heading": "4.2 Visual Features", "text": "In AV-ASR, the ROI for visual features is the region around the speaker's mouth. Each image is converted to grayscale and face recognition is performed using the Viola Jones algorithm. The 64x64 lip area is extracted by recognizing 68 boundaries [15] on the speaker's face and cutting out the ROI around the speaker's mouth and chin. 100-dimensional DCT features are extracted from the ROI. After several training experiments with DCT features, we found that RNN training either exploded or was poorly converted. To improve the distinctiveness of visual features, we perform a nonlinear dimension reduction of features using a deep bottleneck network. Bottleneck features are achieved by forming a neural network in which one of the hidden layers has relatively small visual dimensions."}, {"heading": "5 Fusion models", "text": "In this thesis, fusion models are character-based RNNs trained using the CTC lens function, i.e. L \u2032 is the set of the English alphabet including a blank label. The two fusion models are shown in Figure 2."}, {"heading": "5.1 Feature Fusion", "text": "In the feature fusion technique, a single RNNav is trained by concatenating the audiovisual and visual features using the CTC lens function. In the test phase, the concatenated features are propagated forward through the network at any time. In the CTC decoding step, the posterior probabilities obtained at the soft-max layer are converted into pseudo-log probabilities [23] aslogPav (Oav, t | k) = logPav (k | Oav, t) \u2212 logP (k) (6), where k-L \u2032 and P (k) are the previous probabilities of the class obtained from the training data [17]."}, {"heading": "5.2 Decision Fusion", "text": "In the decision technique, the audiovisual and visual modalities are modelled by separate networks, RNNa and RNNv. RNNv is a lip reading system.The networks are trained separately.In the test phase, for a given expression of the frame level, the pseudo-log probabilities of RNNa and RNNv aslogPav (Oa, t, Ov, t | k) = \u03b3 logPa (k | Oa, t) + (1 \u2212 \u03b3) logPv (k | Ov, t) \u2212 logP (7) are combined, whereby 0 \u2264 1 is a parameter that depends on the noise level and the reliability of each modality [4]. For example, if the noise level in the audio input is higher, a low value of \u03b3 (k | Ov, t) \u2212 logP (k) \u2212 logP (7) deviation from the deviation in the deviation in the deviation in the deviation in the deviation in the deviation in the deviation in the deviation in the deviation in the deviation in the deviation in the deviation in the deviation in the deviation in the Nlog.In this work, the deviation from the deviation from the deviation in the probability of the deviation in the deviation in the deviation in the deviation in the deviation in the NNNNNNz is determined."}, {"heading": "6 Experiments", "text": "GRID corpus is a collection of audio and video recordings of 34 speakers (18 male, 16 female), each pronouncing 1000 sentences, each with a fixed length of approximately 3 seconds, the total number of words in the vocabulary being 51. The syntactic structures of all sentences are similar to those shown below. < Command > < Color > < Preposition > < Letter > < Number > < Adverb > Ex. PLACE ROD AT M ZERO PLEASE"}, {"heading": "6.1 Training", "text": "In the reactionary world of reactionary and reactionary movements that have taken place in recent years, the reactionary and reactionary movement of reactionary forces has evolved into the reactionary, reactionary, reactionary, reactionary and reactionary movement of reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary and reactionary, reactionary and reactionary, reactionary and reactionary, reactionary, reactionary and reactionary, reactionary and reactionary, reactionary, reactionary and reactionary, reactionary and reactionary, reactionary, reactionary and reactionary, reactionary, reactionary and reactionary, reactionary and reactionary, reactionary and reactionary, reactionary, reactionary and reactionary, reactionary and reactionary, and reactionary, reactionary, reactionary and reactionary, reactionary and reactionary, reactionary and reactionary, and reactionary, reactionary, and reactionary, and reactionary, reactionary and reactionary, reactionary, and reactionary, reactionary and reactionary, and reactionary, reactionary, and reactionary, and reactionary, reactionary, and reactionary, reactionary, and reactionary, and reactionary, and reactionary, and reactionary in the reactionary, reactionary, reactionary, and reactionary, reactionary, and reactionary, and reactionary, and reactionary, reactionary, and reactionary, and reactionary, reactionary, and reactionary, and reactionary, and reactionary, and reactionary, reactionary, and reactionary, reactionary, and reactionary, and reactionary, reactionary, and reactionary, and reactionary, and reactionary, and reactionary, and reactionary, and reactionary, in the"}, {"heading": "6.2 Results", "text": "The audiovisual model is tested with three levels of Babbel Noise 0dB SNR, 10dB SNR and clean audio. Noise was added to artificially test the data by mixing Babbel Noise with clean.wav files. To show the importance of visual modality under noisy environments, the model is tested with either audio or video inputs turned off. A Token WFST [17] is used to map the paths to the corresponding edit sequences between them. Token WFST obtains this mapping by removing all spaces and repeating labels. Character Error Rate (CER) is determined from the decoded and expected edit sequences by calculating the distance between them. CER results are shown in Table 1.We find that with clean audio input only RNNa performs significantly better 5.45% (CER 2.45%) compared to Nav modules."}, {"heading": "7 Conclusions And Future Work", "text": "In this paper, we presented an audiovisual ASR system that uses deep RNNs trained with CTC objective function. We described a processing step for visual characteristics using a deep bottleneck layer and demonstrated that it contributes to faster convergence of the RNN model during training. We presented a training protocol in which one of the two modalities is turned off during training to avoid dependence on a single modality. Our results suggest that the trained model is noise-proof. Furthermore, we compared fusion strategies at the feature level and at the decision level. While the use of bottleneck characteristics for visual modality in training helps, it requires frame-level labels that include an additional step of training of audio RNN. Therefore, our system is not yet end-to-end. Our experiments in visual feature engineering with unsupervised methods such as multi-modal encoding [19] have not yielded noticeable results."}], "references": [{"title": "Multimodal speech processing using asynchronous hidden markov models", "author": ["S. Bengio"], "venue": "Information Fusion 5(2), 81\u201389", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2004}, {"title": "Coupled hidden markov models for complex action recognition", "author": ["M. Brand", "N. Oliver", "A. Pentland"], "venue": "Computer vision and pattern recognition, 1997. proceedings., 1997 ieee computer society conference on. pp. 994\u2013999. IEEE", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1997}, {"title": "An audio-visual corpus for speech perception and automatic speech recognition", "author": ["M. Cooke", "J. Barker", "S. Cunningham", "X. Shao"], "venue": "The Journal of the Acoustical Society of America 120(5), 2421\u20132424", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2006}, {"title": "Audio-visual speech modeling for continuous speech recognition", "author": ["S. Dupont", "J. Luettin"], "venue": "IEEE transactions on multimedia 2(3), 141\u2013151", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2000}, {"title": "Extracting deep bottleneck features using stacked auto-encoders", "author": ["J. Gehring", "Y. Miao", "F. Metze", "A. Waibel"], "venue": "2013 IEEE International Conference on Acoustics, Speech and Signal Processing. pp. 3377\u20133381. IEEE", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2013}, {"title": "Neural networks", "author": ["A. Graves"], "venue": "Supervised Sequence Labelling with Recurrent Neural Networks, pp. 15\u201335. Springer", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2012}, {"title": "Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks", "author": ["A. Graves", "S. Fern\u00e1ndez", "F. Gomez", "J. Schmidhuber"], "venue": "Proceedings of the 23rd international conference on Machine learning. pp. 369\u2013 376. ACM", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2006}, {"title": "Towards end-to-end speech recognition with recurrent neural networks", "author": ["A. Graves", "N. Jaitly"], "venue": "ICML. vol. 14, pp. 1764\u20131772", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep speech: Scaling up end-toend speech recognition", "author": ["A. Hannun", "C. Case", "J. Casper", "B. Catanzaro", "G. Diamos", "E. Elsen", "R. Prenger", "S. Satheesh", "S. Sengupta", "A Coates"], "venue": "arXiv preprint arXiv:1412.5567", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2014}, {"title": "Tandem connectionist feature extraction for conventional hmm systems", "author": ["H. Hermansky", "D.P. Ellis", "S. Sharma"], "venue": "Acoustics, Speech, and Signal Processing, 2000. ICASSP\u201900. Proceedings. 2000 IEEE International Conference on. vol. 3, pp. 1635\u2013 1638. IEEE", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2000}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural computation 9(8), 1735\u20131780", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1997}, {"title": "Audio-visual deep learning for noise robust speech recognition", "author": ["J. Huang", "B. Kingsbury"], "venue": "2013 IEEE International Conference on Acoustics, Speech and Signal Processing. pp. 7596\u20137599. IEEE", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2013}, {"title": "Improving audio-visual speech recognition with an infrared headset", "author": ["J. Huang", "G. Potamianos", "C. Neti"], "venue": "AVSP 2003-International Conference on Audio-Visual Speech Processing", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2003}, {"title": "Audiovisual fusion: Challenges and new approaches", "author": ["A.K. Katsaggelos", "S. Bahaadini", "R. Molina"], "venue": "Proceedings of the IEEE 103(9), 1635\u20131653", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2015}, {"title": "One millisecond face alignment with an ensemble of regression trees", "author": ["V. Kazemi", "J. Sullivan"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 1867\u20131874", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2014}, {"title": "Kaldi+ pdnn: building dnn-based asr systems with kaldi and pdnn", "author": ["Y. Miao"], "venue": "arXiv preprint arXiv:1401.6984", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2014}, {"title": "Eesen: End-to-end speech recognition using deep rnn models and wfst-based decoding", "author": ["Y. Miao", "M. Gowayyed", "F. Metze"], "venue": "2015 IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU). pp. 167\u2013174. IEEE", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep multimodal learning for audio-visual speech recognition", "author": ["Y. Mroueh", "E. Marcheret", "V. Goel"], "venue": "2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). pp. 2130\u20132134. IEEE", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2015}, {"title": "Multimodal deep learning", "author": ["J. Ngiam", "A. Khosla", "M. Kim", "J. Nam", "H. Lee", "A.Y. Ng"], "venue": "Proceedings of the 28th international conference on machine learning (ICML-11). pp. 689\u2013696", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2011}, {"title": "Audio-visual speech recognition using deep learning", "author": ["K. Noda", "Y. Yamaguchi", "K. Nakadai", "H.G. Okuno", "T. Ogata"], "venue": "Applied Intelligence 42(4), 722\u2013737", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2015}, {"title": "Recent advances in the automatic recognition of audiovisual speech", "author": ["G. Potamianos", "C. Neti", "G. Gravier", "A. Garg", "A.W. Senior"], "venue": "Proceedings of the IEEE 91(9), 1306\u20131326", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2003}, {"title": "The kaldi speech recognition toolkit", "author": ["D. Povey", "A. Ghoshal", "G. Boulianne", "L. Burget", "O. Glembek", "N. Goel", "M. Hannemann", "P. Motlicek", "Y. Qian", "P Schwarz"], "venue": "IEEE 2011 workshop on automatic speech recognition and understanding. No. EPFL-CONF-192584, IEEE Signal Processing Society", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2011}, {"title": "Sequence-discriminative training of deep neural networks", "author": ["K. Vesel\u1ef3", "A. Ghoshal", "L. Burget", "D. Povey"], "venue": "INTERSPEECH. pp. 2345\u20132349", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2013}, {"title": "Improved bottleneck features using pretrained deep neural networks", "author": ["D. Yu", "M.L. Seltzer"], "venue": "Interspeech. vol. 237, p. 240", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2011}], "referenceMentions": [{"referenceID": 3, "context": "To this end several methods have been proposed which traditionally include variants of GMM/HMM models[4][2].", "startOffset": 101, "endOffset": 104}, {"referenceID": 1, "context": "To this end several methods have been proposed which traditionally include variants of GMM/HMM models[4][2].", "startOffset": 104, "endOffset": 107}, {"referenceID": 11, "context": "More recently AV-ASR methods based on deep neural networks (DNN) models[12][18][20] have been proposed.", "startOffset": 71, "endOffset": 75}, {"referenceID": 17, "context": "More recently AV-ASR methods based on deep neural networks (DNN) models[12][18][20] have been proposed.", "startOffset": 75, "endOffset": 79}, {"referenceID": 19, "context": "More recently AV-ASR methods based on deep neural networks (DNN) models[12][18][20] have been proposed.", "startOffset": 79, "endOffset": 83}, {"referenceID": 7, "context": "End-to-end speech recognition methods based on RNNs trained with CTC objective function[8][17][9] have come to the fore recently and have been shown to give performances comparable to that of DNN/HMM.", "startOffset": 87, "endOffset": 90}, {"referenceID": 16, "context": "End-to-end speech recognition methods based on RNNs trained with CTC objective function[8][17][9] have come to the fore recently and have been shown to give performances comparable to that of DNN/HMM.", "startOffset": 90, "endOffset": 94}, {"referenceID": 8, "context": "End-to-end speech recognition methods based on RNNs trained with CTC objective function[8][17][9] have come to the fore recently and have been shown to give performances comparable to that of DNN/HMM.", "startOffset": 94, "endOffset": 97}, {"referenceID": 4, "context": "Bottleneck features used in tandem with audio features are known to improve ASR performance [5][10][24].", "startOffset": 92, "endOffset": 95}, {"referenceID": 9, "context": "Bottleneck features used in tandem with audio features are known to improve ASR performance [5][10][24].", "startOffset": 95, "endOffset": 99}, {"referenceID": 23, "context": "Bottleneck features used in tandem with audio features are known to improve ASR performance [5][10][24].", "startOffset": 99, "endOffset": 103}, {"referenceID": 20, "context": "Visual feature extraction methods can be of 3 types[21] : 1.", "startOffset": 51, "endOffset": 55}, {"referenceID": 12, "context": "Additional feature processing such as mean normalization, intra-frame and inter-frame LDA may be applied [13][21].", "startOffset": 105, "endOffset": 109}, {"referenceID": 20, "context": "Additional feature processing such as mean normalization, intra-frame and inter-frame LDA may be applied [13][21].", "startOffset": 109, "endOffset": 113}, {"referenceID": 20, "context": "Fusion methods can be broadly divided into two types[21][14]: 1.", "startOffset": 52, "endOffset": 56}, {"referenceID": 13, "context": "Fusion methods can be broadly divided into two types[21][14]: 1.", "startOffset": 56, "endOffset": 60}, {"referenceID": 3, "context": "Various decision fusion methods based on variants of HMMs have been proposed[4][2].", "startOffset": 76, "endOffset": 79}, {"referenceID": 1, "context": "Various decision fusion methods based on variants of HMMs have been proposed[4][2].", "startOffset": 79, "endOffset": 82}, {"referenceID": 0, "context": "While multistream HMM assumes state level synchrony between the two streams, some methods[1][2] such as coupled HMM[2] allow for asynchrony between two streams.", "startOffset": 89, "endOffset": 92}, {"referenceID": 1, "context": "While multistream HMM assumes state level synchrony between the two streams, some methods[1][2] such as coupled HMM[2] allow for asynchrony between two streams.", "startOffset": 92, "endOffset": 95}, {"referenceID": 1, "context": "While multistream HMM assumes state level synchrony between the two streams, some methods[1][2] such as coupled HMM[2] allow for asynchrony between two streams.", "startOffset": 115, "endOffset": 118}, {"referenceID": 20, "context": "For a detailed survey on HMM based AV-ASR systems we refer the readers to [21][14] Application of deep learning to multi-modal analyses was presented in [19] which describes multi-modal, cross-modal and shared representation learning and their applications to AV-ASR.", "startOffset": 74, "endOffset": 78}, {"referenceID": 13, "context": "For a detailed survey on HMM based AV-ASR systems we refer the readers to [21][14] Application of deep learning to multi-modal analyses was presented in [19] which describes multi-modal, cross-modal and shared representation learning and their applications to AV-ASR.", "startOffset": 78, "endOffset": 82}, {"referenceID": 18, "context": "For a detailed survey on HMM based AV-ASR systems we refer the readers to [21][14] Application of deep learning to multi-modal analyses was presented in [19] which describes multi-modal, cross-modal and shared representation learning and their applications to AV-ASR.", "startOffset": 153, "endOffset": 157}, {"referenceID": 11, "context": "In [12], Deep Belief Networks(DBN) are explored.", "startOffset": 3, "endOffset": 7}, {"referenceID": 17, "context": "In [18] the authors train separate networks for audio and visual inputs and fuse the final layers of two networks, and then build a third DNN with the fused features.", "startOffset": 3, "endOffset": 7}, {"referenceID": 17, "context": "In addition, [18] presents a new DNN architecture with a bilinear soft-max layer which further improves the performance.", "startOffset": 13, "endOffset": 17}, {"referenceID": 19, "context": "In [20] a deep de-noising auto-encoder is used to learn noise robust speech features.", "startOffset": 3, "endOffset": 7}, {"referenceID": 10, "context": "The training algorithm suffers from vanishing gradients problem which is overcome by using a special unit in hidden layer called the Long Short Term Memory(LSTM)[11][6].", "startOffset": 161, "endOffset": 165}, {"referenceID": 5, "context": "The training algorithm suffers from vanishing gradients problem which is overcome by using a special unit in hidden layer called the Long Short Term Memory(LSTM)[11][6].", "startOffset": 165, "endOffset": 168}, {"referenceID": 6, "context": "CTC objective function[7][8] obviates the need for such alignments as it enables the network to learn over all possible alignments.", "startOffset": 22, "endOffset": 25}, {"referenceID": 7, "context": "CTC objective function[7][8] obviates the need for such alignments as it enables the network to learn over all possible alignments.", "startOffset": 25, "endOffset": 28}, {"referenceID": 14, "context": "The 64x64 lip region is extracted by detecting 68 landmark points[15] on the speakers face, and cropping the ROI surrounding speakers mouth and chin.", "startOffset": 65, "endOffset": 69}, {"referenceID": 22, "context": "In the CTC decoding step, the posterior probabilities obtained at the soft-max layer are converted to pseudo log-likelihoods[23] as", "startOffset": 124, "endOffset": 128}, {"referenceID": 16, "context": "where k \u2208 L\u2032 and P (k) is the prior probability of class k obtained from the training data [17].", "startOffset": 91, "endOffset": 95}, {"referenceID": 3, "context": "where 0 \u2264 \u03b3 \u2264 1 is a parameter dependent on the noise level and the reliability of each modality[4].", "startOffset": 96, "endOffset": 99}, {"referenceID": 0, "context": "The KL-divergence is scaled to a value in [0, 1] using logistic sigmoid.", "startOffset": 42, "endOffset": 48}, {"referenceID": 2, "context": "The system was trained and tested on GRID audio-visual corpus[3].", "startOffset": 61, "endOffset": 64}, {"referenceID": 21, "context": "Models were trained and tested using Kaldi speech recognition tool kit[22], Kaldi+PDNN[16] and EESEN framework[17].", "startOffset": 70, "endOffset": 74}, {"referenceID": 15, "context": "Models were trained and tested using Kaldi speech recognition tool kit[22], Kaldi+PDNN[16] and EESEN framework[17].", "startOffset": 86, "endOffset": 90}, {"referenceID": 16, "context": "Models were trained and tested using Kaldi speech recognition tool kit[22], Kaldi+PDNN[16] and EESEN framework[17].", "startOffset": 110, "endOffset": 114}, {"referenceID": 22, "context": "Deep Bottleneck Network The training protocol similar to [23] was followed to train the bottleneck network.", "startOffset": 57, "endOffset": 61}, {"referenceID": 16, "context": "A token WFST[17] is used to map the paths to their corresponding label sequences.", "startOffset": 12, "endOffset": 16}, {"referenceID": 18, "context": "Our experiments in visual feature engineering with unsupervised methods like multi-modal auto-encoder[19] did not produce remarkable results.", "startOffset": 101, "endOffset": 105}], "year": 2016, "abstractText": "In this work, we propose a training algorithm for an audiovisual automatic speech recognition (AV-ASR) system using deep recurrent neural network (RNN).First, we train a deep RNN acoustic model with a Connectionist Temporal Classification (CTC) objective function. The frame labels obtained from the acoustic model are then used to perform a non-linear dimensionality reduction of the visual features using a deep bottleneck network. Audio and visual features are fused and used to train a fusion RNN. The use of bottleneck features for visual modality helps the model to converge properly during training. Our system is evaluated on GRID corpus. Our results show that presence of visual modality gives significant improvement in character error rate (CER) at various levels of noise even when the model is trained without noisy data. We also provide a comparison of two fusion methods: feature fusion and decision fusion.", "creator": "LaTeX with hyperref package"}}}