{"id": "1611.02956", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Nov-2016", "title": "A Comparison of Word Embeddings for English and Cross-Lingual Chinese Word Sense Disambiguation", "abstract": "Word embeddings are now ubiquitous forms of word representation in natural language processing. There have been applications of word embeddings for monolingual word sense disambiguation (WSD) in English, but few comparisons have been done. This paper attempts to bridge that gap by examining popular embeddings for the task of monolingual English WSD. Our simplified method leads to comparable state-of-the-art performance without expensive retraining. Cross-Lingual WSD - where the word senses of a word in a source language e come from a separate target translation language f - can also assist in language learning; for example, when providing translations of target vocabulary for learners. Thus we have also applied word embeddings to the novel task of cross-lingual WSD for Chinese and provide a public dataset for further benchmarking. We have also experimented with using word embeddings for LSTM networks and found surprisingly that a basic LSTM network does not work well. We discuss the ramifications of this outcome.", "histories": [["v1", "Wed, 9 Nov 2016 14:50:01 GMT  (25kb)", "https://arxiv.org/abs/1611.02956v1", "10 pages"], ["v2", "Fri, 11 Nov 2016 15:30:36 GMT  (25kb)", "http://arxiv.org/abs/1611.02956v2", "10 pages. Will appear in the Proceedings of The 3rd Workshop on Natural Language Processing Techniques for Educational Applications (NLPTEA 2016)"], ["v3", "Sun, 9 Apr 2017 11:54:01 GMT  (25kb)", "http://arxiv.org/abs/1611.02956v3", "10 pages. Appears in the Proceedings of The 3rd Workshop on Natural Language Processing Techniques for Educational Applications (NLPTEA 2016)"]], "COMMENTS": "10 pages", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["hong jin kang", "tao chen", "muthu kumar chandrasekaran", "min-yen kan"], "accepted": false, "id": "1611.02956"}, "pdf": {"name": "1611.02956.pdf", "metadata": {"source": "CRF", "title": "A Comparison of Word Embeddings for English and Cross-Lingual Chinese Word Sense Disambiguation", "authors": ["Hong Jin Kang", "Tao Chen", "Muthu Kumar Chandrasekaran", "Min-Yen Kan"], "emails": ["kanghongjin@gmail.com", "taochen@comp.nus.edu.sg", "muthu.chandra@comp.nus.edu.sg", "kanmy@comp.nus.edu.sg"], "sections": [{"heading": null, "text": "ar Xiv: 161 1.02 956v 3 [cs.C L] 9A pr2 017ing. There have been applications of word embedding for monolingual word decoding (WSD) in English, but few comparisons have been made. This paper attempts to bridge this gap by examining popular embedding for the task of monolingual English WSD. Our simplified method results in comparable state-of-the-art performance without costly re-education.Cross-Lingual WSD - where the meaning of a word in a source language e comes from a separate target translation language f - can also help with language acquisition; for example, providing translations of the target vocabulary for learners. Thus, we also applied word embedding to the novel task of translingual WSD for Chinese and provided a public dataset for further benchmarks. We also experimented with word embedding for LM networks, and found that a basic STM network experiment did not work well."}, {"heading": "1 Introduction", "text": "A word assumes different meanings, which largely depend on the context in which it is used. For example, the word \"bank\" could mean \"slope next to a body of water\" or a \"depository financial institution.\" Word Sense Disambiguation (WSD) is the task of determining the contextually appropriate meaning of the word. WSD is often considered a classification task in which the classifier predicts the meaning from a possible set of senses, known as an inventory of meaning, given the target word and the contextual information of the target word. Existing WSD systems can be categorized either into data-driven monitored or knowledgeable approaches. Both approaches are considered to be complementary. Word embedding has become a popular word representation formalism, and many tasks can be accomplished by word embedding. The effectiveness of the use of word embedding has been demonstrated in several NLP tasks (Turian et al. 2001). The goal of our Word is to apply the WD exclusively in terms of our work."}, {"heading": "2 Related Work", "text": "In fact, most of them will be able to play by the rules they have set themselves, and they will be able to play by the rules they have set themselves."}, {"heading": "2.1 Cross-Lingual Word Sense Disambiguation", "text": "CrossLingual WSD was partly conceived as a further attempt to solve this problem. In CrossLingual WSD, the specificity of a sense is determined by its correct translation into another language. The meaning inventory is the possible translation of each word into a different language. Two examples are intended to have the same meaning when assigned to the same translation in that language. SemEval2010 (Lefever and Hoste, 2010) 2 and SemEval-2013 (Lefever and Hoste, 2013) 3 contained iterations of this task. These tasks included English nouns as starting words and word senses as translations into Dutch, French, Italian, Spanish and Germany.Traditional WSD approaches are used in Cross-Lingual WSD, although some approaches use statistical machine translation methods and features from the translation. Cross-Lingual WSD includes training through the use of parallel or multilingual companies."}, {"heading": "2.2 WSD with Word Embeddings", "text": "In NLP, words are represented with a distributed representation, such as word embedding that integrates words into a low-dimensional space. In word embedding, information about a word is distributed over multiple dimensions, and similar words are expected to be close to each other in the vector space. Examples of word embedding, however, are ongoing word embedding methods (Mikolov et al., 2013), Collobert & Weston's embedding methods (Collobert and Weston, 2008), and GLoVe (Pennington et al., 2014). We implemented and evaluated the use of word embedding features using these three embedding features in IMS. A non-superimposed approach of word embedding for WSD is described by Chen (2014). This approach finds the representation of words rather than words, and calculates a context vector that is used during the disambiguation.Another approach to work on the extension of existing WSD systems is used."}, {"heading": "3 Methods", "text": "As Navigli (2009), we noted that the supervised approaches have performed best in WSD, we focus on the integration of word embedding into supervised approaches; in particular, we examine the use of word embedding within the IMS framework. We focus our work on Continuous Bag of Words (CBOW) by Word2Vec, Global Vectors for Word Representation (GloVe) and Collobert & Weston's Embeddings (C & W). CBOW embedding was trained via Wikipedia, while the publicly available vectors of GloVe and C & W. Word2Vec offers 2 architectures for learning word embedding, Skip-gram and CBOW. Unlike Iacobacci (2016), which focused on Skip-gram, we focused on our work on CBOW. In our first series of evaluations, we used tasks from Herseval-2 (Senseval-SE) and Senseval-SE (Senseval-SE) after SE."}, {"heading": "3.1 LSTM Network", "text": "A Long Short Term Memory (LSTM) network is a kind of recurrent neural network that has recently proven to perform many NLP classification tasks. A potential advantage of an approach that uses LSTM over our existing approach in IMS is that an LSTM network is able to use more information about the order of words. For the lexical sample tasks, we train the model based on the training data provided for the task. For the All Words task, we trained the model based on the One Million Sense Tagged dataset. For each task, we train a model for each word, similar to IMS, using GloVe word embedding as an input layer. The performance of na\u00efve LSTM is poor in both types of tasks, as examples of words hidden in Table 4 are used."}, {"heading": "4 English-Chinese Cross-Lingual Word Sense Disambiguation", "text": "A key application of such a task is the facilitation of language learning systems. For example, MindTheWord4 and WordNews (Chen et al., 2015) are two applications that allow users to learn vocabulary of a second language in context, in the form of word translations in an online article. In this thesis, we model this problem of locating word translations as a variant of WSD, Cross-Lingual Word Sense Disambiguation as formalized in (Chen et al., 2015). In the previous section, we validated and compared improvements to the IMS using word embeddings. These results are comparable and in some cases better than state-of-the-art performance in the monolingual WSD tasks. We continue to evaluate our approach to using the Cross-Lingual Word Sense Disambiguation for performing English appropriate translations of this monolingual WSD system."}, {"heading": "4.1 Dataset", "text": "This year, it has reached the stage where it will be able to take the lead."}, {"heading": "4.2 Experiments", "text": "As already described, IMS is a monitored system that requires training data prior to use. We constructed data by processing a parallel corpus, the message area of the UM corpus (Tian et al., 2014), and performing word alignment. As a sense inventory, we used the dictionary of (Chen et al., 2015), which we expanded further by using translations from Bing Translator and Google Translate. To evaluate our system, we compare the results of the method described in (Chen et al., 2015), which uses Bing Translator and word alignment as training labels for each English target word to obtain translations. We use the configuration in which each comment is considered correct for our main evaluation, as it is closer to a coarse-toned word."}, {"heading": "4.3 Bing Translator results", "text": "This could be because Bing Translator performs phrase-level translations. Therefore, many of the target words are not translated individually and are only translated as part of a larger unit, making them less suitable for the use case in WordNews where only the translation of individual words plays a role. For example, when the word \"little\" is translated into \"These are serious problems and topics, and sometimes young children are not willing to process and understand these ideas.\" Bing Translator provides a translation from \"little\" into \"These are serious problems and topics, and sometimes young children are not willing to process and understand these ideas.\" Instead, Bing Translator provides an alignment for the entire multiword unit \"little children.\" As such, the translation would not correspond to any of the comments provided by our commentators. This is appropriate treatment as a user should not see the word specifically for the translation of a single app."}, {"heading": "5 Conclusion", "text": "After evaluating the performance of the systems on this cross-lingual WSD dataset, we integrate the powerful system with word embedding and the trained models into a fork of the WordNews system. We experimented and implemented various methods of using word embedding for the monitored WSD. We tried two approaches by improving an existing WSD system, and by trying a neural approach with a simple LSTM. We evaluated our system as well as various methods in WSD, adding the first reviews of the existing test datasets from Senseval-2, SemEval2007, SemEvalor-7."}], "references": [{"title": "A unifiedmodel for word sense representation and disambiguation", "author": ["Chen et al.2014] Xinxiong Chen", "Zhiyuan Liu", "andMaosong Sun"], "venue": "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Chen et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2014}, {"title": "Interactive second language learning from news websites", "author": ["Chen et al.2015] Tao Chen", "Naijia Zheng", "Yue Zhao", "Muthu Kumar Chandrasekaran", "Min-Yen Kan"], "venue": "In Proceedings of ACL Workshop on Natural Language Processing Techniques for Educational Applications (NLP-TEA\u201915),", "citeRegEx": "Chen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2015}, {"title": "A unified architecture for natural language processing: Deep neural networks with multitask learning", "author": ["Collobert", "Weston2008] Ronan Collobert", "Jason Weston"], "venue": "In Proceedings of the 25th international conference on Machine learning,", "citeRegEx": "Collobert et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2008}, {"title": "Introduction to the special issue on evaluating word sense disambiguation systems", "author": ["Edmonds", "Kilgarriff2002] Philip Edmonds", "Adam Kilgarriff"], "venue": "Natural Language Engineering,", "citeRegEx": "Edmonds et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Edmonds et al\\.", "year": 2002}, {"title": "Embeddings for word sense disambiguation: An evaluation study", "author": ["Taher Mohammad Pilehvar", "Roberto Navigli"], "venue": "In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),", "citeRegEx": "Iacobacci et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Iacobacci et al\\.", "year": 2016}, {"title": "Word sense disambiguation using a bidirectional lstm", "author": ["K\u00e5geb\u00e4ck", "Salomonsson2016] Mikael K\u00e5geb\u00e4ck", "Hans Salomonsson"], "venue": "arXiv preprint arXiv:1606.03568", "citeRegEx": "K\u00e5geb\u00e4ck et al\\.,? \\Q2016\\E", "shortCiteRegEx": "K\u00e5geb\u00e4ck et al\\.", "year": 2016}, {"title": "English lexical sample task description", "author": ["Adam Kilgarriff"], "venue": "In The Proceedings of the Second International Workshop on Evaluating Word Sense Disambiguation Systems,", "citeRegEx": "Kilgarriff.,? \\Q2001\\E", "shortCiteRegEx": "Kilgarriff.", "year": 2001}, {"title": "Semeval-2010 task 3: Cross-lingual word sense disambiguation", "author": ["Lefever", "Hoste2010] Els Lefever", "V\u00e9ronique Hoste"], "venue": "In Proceedings of the 5th International Workshop on Semantic Evaluation,", "citeRegEx": "Lefever et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Lefever et al\\.", "year": 2010}, {"title": "Semeval-2013 task 10: Cross-lingual word sense disambiguation", "author": ["Lefever", "Hoste2013] Els Lefever", "V\u00e9ronique Hoste"], "venue": "In Second Joint Conference on Lexical and Computational Semantics (*SEM),", "citeRegEx": "Lefever et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Lefever et al\\.", "year": 2013}, {"title": "The Senseval3 English lexical sample task. In Senseval-3: Third International Workshop on the Evaluation of Systems for the Semantic Analysis of Text. Association for Computational Linguistics", "author": ["Timothy Anatolievich Chklovski", "AdamKilgarriff"], "venue": null, "citeRegEx": "Mihalcea et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Mihalcea et al\\.", "year": 2004}, {"title": "Efficient estimation of word representations in vector space. CoRR, abs/1301.3781", "author": ["Kai Chen", "Greg Corrado", "Jeffrey Dean"], "venue": null, "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Wordnet: A lexical database for english", "author": ["George A. Miller"], "venue": "Commun. ACM,", "citeRegEx": "Miller.,? \\Q1995\\E", "shortCiteRegEx": "Miller.", "year": 1995}, {"title": "Semeval-2007 task 07: Coarse-grained english all-words task", "author": ["Kenneth C Litkowski", "Orin Hargraves"], "venue": "In Proceedings of the 4th International Workshop on Semantic Evaluations,", "citeRegEx": "Navigli et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Navigli et al\\.", "year": 2007}, {"title": "Word sense disambiguation: a survey", "author": ["Roberto Navigli"], "venue": "ACM COMPUTING SURVEYS,", "citeRegEx": "Navigli.,? \\Q2009\\E", "shortCiteRegEx": "Navigli.", "year": 2009}, {"title": "A systematic comparison of various statistical alignment models", "author": ["Och", "Ney2003] Franz Josef Och", "Hermann Ney"], "venue": "Computational Linguistics,", "citeRegEx": "Och et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Och et al\\.", "year": 2003}, {"title": "English tasks: All-words and verb lexical sample", "author": ["Palmer et al.2001] Martha Palmer", "Christiane Fellbaum", "Scott Cotton", "Lauren Delfs", "Hoa Trang Dang"], "venue": "In The Proceedings of the Second International Workshop on Evaluating Word Sense Disambiguation Systems,", "citeRegEx": "Palmer et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Palmer et al\\.", "year": 2001}, {"title": "Glove: Global vectors for word representation", "author": ["Richard Socher", "Christopher D Manning"], "venue": "In Proc. of the Empiricial Methods in Natural Language Processing (EMNLP 2014),", "citeRegEx": "Pennington et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "SemEval2007 task 17: English lexical sample, SRL and all words", "author": ["Edward Loper", "Dmitriy Dligach", "Martha Palmer"], "venue": "In Proceedings of the 4th International Workshop on Semantic Evaluations,", "citeRegEx": "Pradhan et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Pradhan et al\\.", "year": 2007}, {"title": "Autoextend: Extending word embeddings to embeddings for synsets and lexemes", "author": ["Rothe", "Sch\u00fctze2015] Sascha Rothe", "Hinrich Sch\u00fctze"], "venue": "In Proceedings of the ACL-IJCNLP,", "citeRegEx": "Rothe et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Rothe et al\\.", "year": 2015}, {"title": "Evaluation methods for unsupervised word embeddings", "author": ["Igor Labutov", "David Mimno", "Thorsten Joachims"], "venue": "In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Schnabel et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Schnabel et al\\.", "year": 2015}, {"title": "The English all-words task", "author": ["Snyder", "Palmer2004] Benjamin Snyder", "Martha Palmer"], "venue": "In Senseval-3: Third International Workshop on the Evaluation of Systems for the Semantic Analysis of Text,", "citeRegEx": "Snyder et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Snyder et al\\.", "year": 2004}, {"title": "2015a. One million sense-tagged instances for word sense disambiguation and induction", "author": ["Taghipour", "Ng2015a] Kaveh Taghipour", "Hwee Tou Ng"], "venue": null, "citeRegEx": "Taghipour et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Taghipour et al\\.", "year": 2015}, {"title": "Semi-supervised word sense disambiguation using word embeddings in general and specific domains", "author": ["Taghipour", "Ng2015b] Kaveh Taghipour", "Hwee Tou Ng"], "venue": "Proceedings of HLT-NAACL,", "citeRegEx": "Taghipour et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Taghipour et al\\.", "year": 2015}, {"title": "UMCorpus: A Large English-Chinese Parallel Corpus for Statistical Machine Translation", "author": ["Tian et al.2014] Liang Tian", "Derek F Wong", "Lidia S Chao", "Paulo Quaresma", "Francisco Oliveira"], "venue": "In Proceeding of the LREC,", "citeRegEx": "Tian et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Tian et al\\.", "year": 2014}, {"title": "Word representations: a simple and general method for semi-supervised learning. In Proceedings of the 48th annual meeting of the association for computational linguistics, pages 384\u2013394", "author": ["Turian et al.2010] Joseph Turian", "Lev Ratinov", "Yoshua Bengio"], "venue": null, "citeRegEx": "Turian et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Turian et al\\.", "year": 2010}, {"title": "It makes sense: A wide-coverage word sense disambiguation system for free text", "author": ["Zhong", "Ng2010] Zhi Zhong", "Hwee Tou Ng"], "venue": "In Proceedings of the ACL 2010 System Demonstrations,", "citeRegEx": "Zhong et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Zhong et al\\.", "year": 2010}], "referenceMentions": [{"referenceID": 24, "context": "The effectiveness of using word embeddings has been shown in several NLP tasks (Turian et al., 2010).", "startOffset": 79, "endOffset": 100}, {"referenceID": 11, "context": "Unsupervised techniques are knowledge-rich, and rely heavily on knowledge bases and thesaurus, such as WordNet (Miller, 1995).", "startOffset": 111, "endOffset": 125}, {"referenceID": 10, "context": "Unsupervised techniques are knowledge-rich, and rely heavily on knowledge bases and thesaurus, such as WordNet (Miller, 1995). It is noted by Navigli (2009) that supervised approaches using memorybased learning and SVM approaches have worked best.", "startOffset": 112, "endOffset": 157}, {"referenceID": 10, "context": "Unsupervised techniques are knowledge-rich, and rely heavily on knowledge bases and thesaurus, such as WordNet (Miller, 1995). It is noted by Navigli (2009) that supervised approaches using memorybased learning and SVM approaches have worked best. Supervised approaches involve the extraction of features and then classification using machine learning. Zhong and Ng (2010) developed an open-source WSD system, ItMakesSense (hereafter, IMS), which was considered the state-of-the-art at the time it was developed.", "startOffset": 112, "endOffset": 373}, {"referenceID": 10, "context": "Examples of word embeddings are Continuous Bag of Words (Mikolov et al., 2013), Collobert &Weston\u2019s Embeddings (Collobert and Weston, 2008), and GLoVe (Pennington et al.", "startOffset": 56, "endOffset": 78}, {"referenceID": 16, "context": ", 2013), Collobert &Weston\u2019s Embeddings (Collobert and Weston, 2008), and GLoVe (Pennington et al., 2014).", "startOffset": 80, "endOffset": 105}, {"referenceID": 10, "context": "Examples of word embeddings are Continuous Bag of Words (Mikolov et al., 2013), Collobert &Weston\u2019s Embeddings (Collobert and Weston, 2008), and GLoVe (Pennington et al., 2014). We implemented and evaluated the use of word embedding features using these three embeddings in IMS. An unsupervised approach using word embeddings for WSD is described by Chen (2014). This approach finds representation of senses, instead of words, and computes a context vector which is used during disambiguation.", "startOffset": 57, "endOffset": 362}, {"referenceID": 10, "context": "Examples of word embeddings are Continuous Bag of Words (Mikolov et al., 2013), Collobert &Weston\u2019s Embeddings (Collobert and Weston, 2008), and GLoVe (Pennington et al., 2014). We implemented and evaluated the use of word embedding features using these three embeddings in IMS. An unsupervised approach using word embeddings for WSD is described by Chen (2014). This approach finds representation of senses, instead of words, and computes a context vector which is used during disambiguation. A different approach is to work on extending existing WSD systems. Turian (2010) suggests that for any existing supervised NLP system, a general way of improving accuracy would be to use unsupervised", "startOffset": 57, "endOffset": 575}, {"referenceID": 13, "context": "As Navigli (2009) noted that supervised approaches have performed best in WSD, we focus on integrating word embeddings in supervised approaches; in specific, we explore the use of word embeddings within the IMS framework.", "startOffset": 3, "endOffset": 18}, {"referenceID": 13, "context": "As Navigli (2009) noted that supervised approaches have performed best in WSD, we focus on integrating word embeddings in supervised approaches; in specific, we explore the use of word embeddings within the IMS framework. We focus our work on Continuous Bag of Words (CBOW) from Word2Vec, Global Vectors for Word Representation (GloVe) and Collobert & Weston\u2019s Embeddings(C&W). The CBOW embeddings were trained over Wikipedia, while the publicly available vectors from GloVe and C&W were used. Word2Vec provides 2 architectures for learning word embeddings, Skip-gram and CBOW. In contrast to Iacobacci (2016) which focused on Skip-gram, we focused our work on CBOW.", "startOffset": 3, "endOffset": 610}, {"referenceID": 19, "context": "However, we did not just experiment using C&W embeddings, as different word embeddings are known to vary in quality when evaluated on different tasks (Schnabel et al., 2015).", "startOffset": 150, "endOffset": 173}, {"referenceID": 6, "context": "For the Lexical Sample (LS) tasks of SE-2 (Kilgarriff, 2001) and SE3 (Mihalcea et al.", "startOffset": 42, "endOffset": 60}, {"referenceID": 9, "context": "For the Lexical Sample (LS) tasks of SE-2 (Kilgarriff, 2001) and SE3 (Mihalcea et al., 2004), we evaluated our system using fine-grained scoring.", "startOffset": 69, "endOffset": 92}, {"referenceID": 15, "context": "For the All Words (AW) tasks, fine-grained scoring is done for SE-2 (Palmer et al., 2001) and SE-3 (Snyder and Palmer, 2004); both the fine (Pradhan et al.", "startOffset": 68, "endOffset": 89}, {"referenceID": 17, "context": ", 2001) and SE-3 (Snyder and Palmer, 2004); both the fine (Pradhan et al., 2007) and coarse-grained were used in (Navigli et al.", "startOffset": 58, "endOffset": 80}, {"referenceID": 12, "context": ", 2007) and coarse-grained were used in (Navigli et al., 2007) AW tasks in SE-2007.", "startOffset": 40, "endOffset": 62}, {"referenceID": 4, "context": "We see from the results that the combination of (Taghipour and Ng, 2015b)\u2019s scaling strategy and summation produced results better than the proposal in (Iacobacci et al., 2016) to concatenate and average (0.", "startOffset": 152, "endOffset": 176}, {"referenceID": 4, "context": "682 (Iacobacci et al., 2016) 0.", "startOffset": 4, "endOffset": 28}, {"referenceID": 0, "context": "591 (Chen et al., 2014) - - - - 0.", "startOffset": 4, "endOffset": 23}, {"referenceID": 1, "context": "For example, MindTheWord and WordNews (Chen et al., 2015) are two applications that allow users to learn vocabulary of a second language in context, in the form of providing translations of words in an online article.", "startOffset": 38, "endOffset": 57}, {"referenceID": 1, "context": "In this work, we model this problem of finding translations of words as a variant of WSD, Cross-Lingual Word Sense Disambiguation, as formalized in (Chen et al., 2015).", "startOffset": 148, "endOffset": 167}, {"referenceID": 1, "context": "For our sense inventory, we work with the existing dictionary in the open-source educational application, WordNews (Chen et al., 2015), which contains a dictionary of English words and their possible Chinese translations.", "startOffset": 115, "endOffset": 134}, {"referenceID": 23, "context": "We constructed data by processing a parallel corpus, the news section of the UM-Corpus (Tian et al., 2014), and performing word alignment.", "startOffset": 87, "endOffset": 106}, {"referenceID": 1, "context": "We used the dictionary provided by (Chen et al., 2015) as the sense inventory, which we further expanded using translations from Bing Translator and Google Translate.", "startOffset": 35, "endOffset": 54}, {"referenceID": 1, "context": "To evaluate our system, we compare the results of the method described in (Chen et al., 2015), which uses Bing Translator and word alignment to obtain translations.", "startOffset": 74, "endOffset": 93}, {"referenceID": 4, "context": "This supports (Iacobacci et al., 2016)\u2019s conclusion that concluded that existing supervised approaches can be augmented with word embeddings to give better results.", "startOffset": 14, "endOffset": 38}, {"referenceID": 4, "context": "This supports (Iacobacci et al., 2016)\u2019s conclusion that concluded that existing supervised approaches can be augmented with word embeddings to give better results. Our findings also validated Iacobacci et al. (2016)\u2019s findings that Word2Vec gave the best performance.", "startOffset": 15, "endOffset": 217}, {"referenceID": 4, "context": "This supports (Iacobacci et al., 2016)\u2019s conclusion that concluded that existing supervised approaches can be augmented with word embeddings to give better results. Our findings also validated Iacobacci et al. (2016)\u2019s findings that Word2Vec gave the best performance. However, we also note that, other than Word2Vec, other publicly available word embeddings, Collobert & Weston\u2019s embeddings and GLoVe also consistently enhanced the performance of IMS using the summation feature with little effort. Other than on the Lexical Sample tasks, where smaller word embeddings performed better, we also found that the number of dimensions did not affect results as much as the scaling parameter. Unlike Iacobacci et al. (2016), we also found that a simple composition method using summation already gave good improvements over the standard WSD features, provided that the scaling method described in (Taghipour and Ng, 2015b) was performed.", "startOffset": 15, "endOffset": 720}], "year": 2017, "abstractText": "Word embeddings are now ubiquitous forms of word representation in natural language processing. There have been applications of word embeddings for monolingual word sense disambiguation (WSD) in English, but few comparisons have been done. This paper attempts to bridge that gap by examining popular embeddings for the task of monolingual English WSD. Our simplified method leads to comparable state-of-the-art performance without expensive retraining. Cross-Lingual WSD \u2013 where the word senses of a word in a source language e come from a separate target translation language f \u2013 can also assist in language learning; for example, when providing translations of target vocabulary for learners. Thus we have also applied word embeddings to the novel task of cross-lingual WSD for Chinese and provide a public dataset for further benchmarking. We have also experimented with using word embeddings for LSTM networks and found surprisingly that a basic LSTM network does not work well. We discuss the ramifications of this outcome.", "creator": "LaTeX with hyperref package"}}}