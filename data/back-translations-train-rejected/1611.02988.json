{"id": "1611.02988", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Nov-2016", "title": "Distant supervision for emotion detection using Facebook reactions", "abstract": "We exploit the Facebook reaction feature in a distant supervised fashion to train a support vector machine classifier for emotion detection, using several feature combinations and combining different Facebook pages. We test our models on existing benchmarks for emotion detection and show that employing only information that is derived completely automatically, thus without relying on any handcrafted lexicon as it's usually done, we can achieve competitive results. The results also show that there is large room for improvement, especially by gearing the collection of Facebook pages, with a view to the target domain.", "histories": [["v1", "Wed, 9 Nov 2016 15:49:31 GMT  (180kb,D)", "http://arxiv.org/abs/1611.02988v1", "Proceedings of the Workshop on Computational Modeling of People's Opinions, Personality, and Emotions in Social Media (PEOPLES 2016), held in conjunction with COLING 2016, Osaka, Japan"]], "COMMENTS": "Proceedings of the Workshop on Computational Modeling of People's Opinions, Personality, and Emotions in Social Media (PEOPLES 2016), held in conjunction with COLING 2016, Osaka, Japan", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["chris pool", "malvina nissim"], "accepted": false, "id": "1611.02988"}, "pdf": {"name": "1611.02988.pdf", "metadata": {"source": "CRF", "title": "Distant supervision for emotion detection using Facebook reactions", "authors": ["Chris Pool", "Malvina Nissim"], "emails": ["c.pool@anchormen.nl", "m.nissim@rug.nl"], "sections": [{"heading": "1 Introduction", "text": "In fact, most of them are able to play by the rules that they have set themselves in order to play by the rules."}, {"heading": "2 Facebook reactions as labels", "text": "For years, on Facebook, people have been able to leave comments on posts, and also \"like\" them by using a thumbs-up function to explicitly express a generic, rather sub-specific, consent. So a \"like\" could mean \"I like what you have said,\" but also \"I like that you raise such a topic (although I find the content of the article you have linked annoying).\" We collected Facebook posts and the corresponding reactions from public sites that use the Facebook API that we call up via the Facebook Python library1. We chose different pages (and therefore domains and removals) that aim at a balanced and diverse dataset, but we did this mainly based on intuition (see section 4) and looking at the nature of the datasets that are available for review (see section 5). Choosing which pages to select from posts is a more interesting, as we believe it's a far more paragraph-specific aspect."}, {"heading": "3 Emotion datasets", "text": "Three sets of emotion annotated data are commonly used to develop and evaluate emotion recognition systems, namely the affective text dataset, the Fairy Tales dataset and the ISEAR dataset. To compare our performance with current results, we have also used them. In this section, in addition to a description of each dataset, we offer an overview of the emotions used, their distribution and how we mapped them with those we obtained from Facebook posts in Section 3.4. A summary is included in Table 1, which also shows the role of each dataset in our experiments in the bottom line: Apart from the development part of the affective text we used to develop our models (Section 4), all three have been used as benchmarks for our evaluation."}, {"heading": "3.1 Affective Text dataset", "text": "Job 14 at SemEval 2007 (Strapparava and Mihalcea, 2007) concerned the classification of emotion and value in news headlines. Headlines were collected from several news websites, including Google News, The New York Times, BBC News and CNN. The emotion labels used were anger, disgust, fear, joy, sadness, surprise, in accordance with the six basic emotions of the Ekman Standard Model (Ekman, 1992). Value was to be determined as positive or negative. Classification of emotion and value were treated as separate tasks. Emotion labels were not considered mutually exclusive, and each emotion was assigned a score of 0 to 100. Training / development data amounted to 250 commented headlines (Affective Development), while systems were evaluated on a further 1000 (Affective Test). The evaluation was performed using two different methods: a fine-grained rating with Pearson's R to measure the correlation between system values and the gold standard; and one year this evaluation was transformed into a preemotion (in 2010)."}, {"heading": "3.2 Fairy Tales dataset", "text": "This is a dataset collected by Alm (2008), where about 1,000 sentences from fairy tales (by B. Potter, H.C. Andersen, and Grimm) were commented on with the same six emotions of the Affective Text dataset, albeit with different names: Angry, disgusted, anxious, happy, sad, and surprised. In most works that use this dataset (Kim et al., 2010; Chaffar and Inkpen, 2011; Calvo and Mac Kim, 2013), only sentences are used where all commentators agreed, and the labels merged in anger and disgust."}, {"heading": "3.3 ISEAR", "text": "ISEAR (International Survey on Emotion Antecedents and Reactions (Scherer and Wallbott, 1994; Scherer, 1997) is a data set created as part of a psychology project in the 1990s by collecting questionnaires that were answered by people from different cultural backgrounds. The main objective of this project was to gain insights into intercultural aspects of emotional reactions. Students, both psychologists and non-psychologists, were asked to report situations in which they had experienced all seven major emotions (joy, fear, anger, sadness, disgust, shame and guilt). In each case, the questions included the way they had assessed a particular situation and how they reacted."}, {"heading": "3.4 Overview of datasets and emotions", "text": "We summarize the data sets and the distribution of emotions from two points of view: First, because there are different sets of emotions in the data sets and Facebook data, we need to provide an illustration and derive from it a subset of emotions that we will use for the experiments. This is shown in Table 1, where we report in the \"Mapped\" column about the last emotions we use in this essay: anger, joy, sadness, surprise. All labels in each data set are associated with these last emotions, which are therefore the labels we use for training and testing our models. Second, the distribution of emotions for each data set is different, as shown in Figure 3. For example, in Figure 4 we also see the distribution of emotions anger, joy, sadness, surprise per Facebook page, in terms of the number of posts (remember that we assign the label to a post that corresponds to the emotions associated with the majority, see Section 2). We can find that for example, pages about news tend to have more tendency to cookery pages and high levels of information about them, such as anger and high levels of information about them."}, {"heading": "4 Model", "text": "In developing our model, two main decisions need to be made: (i) which Facebook pages should be selected as training data, and (ii) which features should be used to train the model, which we will discuss below. Specifically, we will first use a subset of pages and then experiment with features. Further research into the interaction between page selection and feature selection will be left to future work and will be partially covered in Section 6. For development, we will use a small portion of the affective data set described in Section 3.1, which is the part that was published as a development set for SemEvals 2007 Task 14 (Strapparava and Mihalcea, 2007) containing 250 annotated sets (Affective Development, Section 3.1). All results reported in this section relate to this data set. The Task 14 test set and the two other data sets described in Section 3 will be used to evaluate the final models (Section 4)."}, {"heading": "4.1 Selecting Facebook pages", "text": "Although page selection is a critical part of this approach, which in our opinion requires further and deeper, targeted research, we chose a rather simple approach for the experiments described here. First, we selected the sites that would provide training data based on intuition and availability, then we selected different combinations based on the results of a basic model based on development data, and finally tested feature combinations that were still at the development stage. For simplicity and transparency, we first trained an SVM with a simple bag-of-word model and standard parameters based on Scikit Learn implementation (Pedregosa et al., 2011) on different page combinations. Based on the results of the attempted combinations and the distribution of emotions in the development data set (Figure 3), we chose a best model (B-M), namely the combined set of Time, The Guardian and Disney, which delivers the highest level of development data for Time, and helps The Guardian to perform well on most emotions."}, {"heading": "4.2 Features", "text": "When choosing suitable features, we relied mainly on previous work and intuition. We experimented with various bed combinations, and all tests were still performed on Affective Development, with the pages for the best model (B-M) described above as training data. Results are in Table 2. Future work will continue to explore the simultaneous selection of features and page combinations. Standard text features We use a set of basic text-based features to capture the emotion class. These include a tf-idf-of-words feature, word (2-5) ngrams, and features related to the pre-action of negative words, and to the use of character systems. Affect Lexicons This feature is used in all non-outdated models as a source of information, and we include it mainly to rate its contribution, but finally we do not use it in our final modelling. We used the NRC10 lexicon, because it is best used in the experiments of the 2012, Despair, Emotional, Emotional, Emotional, Emotional, Emotional, Emotional, Anger, Emotional, Emotional, Despair, Despair, Despair, Despair, Desperation, Desperation, Desperation, Desperation, and Functions related to the pre-action of negative words, and to the use of character systems."}, {"heading": "4.3 Results on development set", "text": "The average f-score is reported as a micro-average to better account for the distorted distribution of classes, and in accordance with what is normally reported for this task (Mohammad and Kiritchenko, 2015).From Table 2, we draw three key observations: First, a simple tf-idf word bag mode already works very well, so the other text- and lexicon-based functions do not seem to contribute to the general f-score (0.368), although there is a fairly significant variation in values per class. Second, Google embeddings perform much better than Facebook embeddings, and this is probably due to the size of the corpus used for training. Retrofitting does not seem to be helpful at all for Google embeddings, but it does increase Facebook embeddings, leading to the assumption that with few data more accurate task-related information will help, but the size of the corpus is most important. Third, in combination with embeddings, both functions are automatically generated based on lexicon results, with both of which work better."}, {"heading": "5 Results", "text": "In fact, most of them are able to survive on their own, without being able to survive on their own."}, {"heading": "6 Discussion, conclusions and future work", "text": "The evaluation on standard benchmarks shows that models that are trained as such, especially when improved with continuous vector representations, can achieve competitive results without relying on handmade resources. An interesting aspect of our approach is looking at emotional embeddings through the selection of Facebook pages that are used as training data. We believe that this approach has a lot of potential, and we see the following instructions for improvement. Featurewise, we would like to train emotion-conscious embeddings, in the sense of the work of Tang et al. (2014), and Iacobacci et al. (2015). Retrofitting FB embeddings that are trained on a larger body could also be successful, but would rely on an external lexicon.The greatest scope for achieving not only better results, but also interesting insights about extensions of this approach lies in the choice of training content, both in relation to Facebook pages, and in relation to posts that we can select as the only ones."}, {"heading": "Acknowledgements", "text": "In addition to the anonymous reviewers, we would like to thank Lucia Passaro and Barbara Plank for the insightful discussions and comments on the drafts of this essay."}], "references": [{"title": "Affect in text and speech", "author": ["Ebba Cecilia Ovesdotter Alm."], "venue": "ProQuest.", "citeRegEx": "Alm.,? 2008", "shortCiteRegEx": "Alm.", "year": 2008}, {"title": "Emotions in text: dimensional and categorical models", "author": ["Rafael A Calvo", "Sunghwan Mac Kim."], "venue": "Computational Intelligence, 29(3):527\u2013543.", "citeRegEx": "Calvo and Kim.,? 2013", "shortCiteRegEx": "Calvo and Kim.", "year": 2013}, {"title": "Using a heterogeneous dataset for emotion analysis in text", "author": ["Soumaya Chaffar", "Diana Inkpen."], "venue": "Advances in Artificial Intelligence, pages 62\u201367. Springer.", "citeRegEx": "Chaffar and Inkpen.,? 2011", "shortCiteRegEx": "Chaffar and Inkpen.", "year": 2011}, {"title": "Feeler: Emotion classification of text using vector space model", "author": ["Taner Danisman", "Adil Alpkocak."], "venue": "AISB 2008 Convention Communication, Interaction and Social Intelligence, volume 1, page 53.", "citeRegEx": "Danisman and Alpkocak.,? 2008", "shortCiteRegEx": "Danisman and Alpkocak.", "year": 2008}, {"title": "An argument for basic emotions", "author": ["Paul Ekman."], "venue": "Cognition & emotion, 6(3-4):169\u2013200.", "citeRegEx": "Ekman.,? 1992", "shortCiteRegEx": "Ekman.", "year": 1992}, {"title": "Retrofitting word vectors to semantic lexicons", "author": ["Manaal Faruqui", "Jesse Dodge", "Sujay Kumar Jauhar", "Chris Dyer", "Eduard Hovy", "Noah A. Smith."], "venue": "Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 1606\u20131615, Denver, Colorado, May\u2013June. Association for Computational Linguistics.", "citeRegEx": "Faruqui et al\\.,? 2015", "shortCiteRegEx": "Faruqui et al\\.", "year": 2015}, {"title": "WordNet", "author": ["Christiane Fellbaum."], "venue": "Wiley Online Library.", "citeRegEx": "Fellbaum.,? 1998", "shortCiteRegEx": "Fellbaum.", "year": 1998}, {"title": "Twitter sentiment classification using distant supervision", "author": ["Alec Go", "Richa Bhayani", "Lei Huang."], "venue": "CS224N Project Report, Stanford, 1:12.", "citeRegEx": "Go et al\\.,? 2009", "shortCiteRegEx": "Go et al\\.", "year": 2009}, {"title": "Multi-class sentiment classification on twitter using an emoji training heuristic", "author": ["Fredrik Hallsmar", "Jonas Palm."], "venue": "Technical report, KTH/Skolan f\u00f6r datavetenskap och kommunikation (CSC). University essay.", "citeRegEx": "Hallsmar and Palm.,? 2016", "shortCiteRegEx": "Hallsmar and Palm.", "year": 2016}, {"title": "Sensembed: learning sense embeddings for word and relational similarity", "author": ["Ignacio Iacobacci", "Mohammad Taher Pilehvar", "Roberto Navigli."], "venue": "Proceedings of ACL, pages 95\u2013105.", "citeRegEx": "Iacobacci et al\\.,? 2015", "shortCiteRegEx": "Iacobacci et al\\.", "year": 2015}, {"title": "Evaluation of unsupervised emotion models to textual affect recognition", "author": ["Sunghwan Mac Kim", "Alessandro Valitutti", "Rafael A Calvo."], "venue": "Proceedings of the NAACL HLT 2010 Workshop on Computational Approaches to Analysis and Generation of Emotion in Text, pages 62\u201370. Association for Computational Linguistics.", "citeRegEx": "Kim et al\\.,? 2010", "shortCiteRegEx": "Kim et al\\.", "year": 2010}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean."], "venue": "Advances in neural information processing systems, pages 3111\u20133119.", "citeRegEx": "Mikolov et al\\.,? 2013", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Distant supervision for relation extraction without labeled data", "author": ["Mike Mintz", "Steven Bills", "Rion Snow", "Dan Jurafsky."], "venue": "Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP: Volume 2 - Volume 2, ACL \u201909, pages 1003\u20131011, Stroudsburg, PA, USA. Association for Computational Linguistics.", "citeRegEx": "Mintz et al\\.,? 2009", "shortCiteRegEx": "Mintz et al\\.", "year": 2009}, {"title": "Using hashtags to capture fine emotion categories from tweets", "author": ["Saif M Mohammad", "Svetlana Kiritchenko."], "venue": "Computational Intelligence, 31(2):301\u2013326.", "citeRegEx": "Mohammad and Kiritchenko.,? 2015", "shortCiteRegEx": "Mohammad and Kiritchenko.", "year": 2015}, {"title": "A dataset for detecting stance in tweets", "author": ["Saif M. Mohammad", "Svetlana Kiritchenko", "Parinaz Sobhani", "Xiaodan Zhu", "Colin Cherry."], "venue": "Proceedings of 10th edition of the the Language Resources and Evaluation Conference (LREC), Portoro\u017e, Slovenia.", "citeRegEx": "Mohammad et al\\.,? 2016", "shortCiteRegEx": "Mohammad et al\\.", "year": 2016}, {"title": "Portable features for classifying emotional text", "author": ["Saif Mohammad."], "venue": "Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 587\u2013591, Montr\u00e9al, Canada, June. Association for Computational Linguistics.", "citeRegEx": "Mohammad.,? 2012", "shortCiteRegEx": "Mohammad.", "year": 2012}, {"title": "Sentiment analysis: Detecting valence, emotions, and other affectual states from text", "author": ["Saif M. Mohammad."], "venue": "Herb Meiselman, editor, Emotion Measurement. Elsevier.", "citeRegEx": "Mohammad.,? 2016", "shortCiteRegEx": "Mohammad.", "year": 2016}, {"title": "Scikitlearn: Machine learning in Python", "author": ["F. Pedregosa", "G. Varoquaux", "A. Gramfort", "V. Michel", "B. Thirion", "O. Grisel", "M. Blondel", "P. Prettenhofer", "R. Weiss", "V. Dubourg", "J. Vanderplas", "A. Passos", "D. Cournapeau", "M. Brucher", "M. Perrot", "E. Duchesnay."], "venue": "Journal of Machine Learning Research, 12:2825\u20132830.", "citeRegEx": "Pedregosa et al\\.,? 2011", "shortCiteRegEx": "Pedregosa et al\\.", "year": 2011}, {"title": "Adapting taggers to twitter with not-so-distant supervision", "author": ["Barbara Plank", "Dirk Hovy", "Ryan T McDonald", "Anders S\u00f8gaard."], "venue": "COLING, pages 1783\u20131792.", "citeRegEx": "Plank et al\\.,? 2014", "shortCiteRegEx": "Plank et al\\.", "year": 2014}, {"title": "Experimenting with distant supervision for emotion classification", "author": ["Matthew Purver", "Stuart Battersby."], "venue": "Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 482\u2013491. Association for Computational Linguistics.", "citeRegEx": "Purver and Battersby.,? 2012", "shortCiteRegEx": "Purver and Battersby.", "year": 2012}, {"title": "Software Framework for Topic Modelling with Large Corpora", "author": ["Radim \u0158eh\u016f\u0159ek", "Petr Sojka."], "venue": "Proceedings of the LREC 2010 Workshop on New Challenges for NLP Frameworks, pages 45\u201350, Valletta, Malta, May. ELRA. http://is.muni.cz/publication/884893/en.", "citeRegEx": "\u0158eh\u016f\u0159ek and Sojka.,? 2010", "shortCiteRegEx": "\u0158eh\u016f\u0159ek and Sojka.", "year": 2010}, {"title": "Evidence for universality and cultural variation of differential emotion response patterning", "author": ["Klaus R Scherer", "Harald G Wallbott."], "venue": "Journal of personality and social psychology, 66(2):310.", "citeRegEx": "Scherer and Wallbott.,? 1994", "shortCiteRegEx": "Scherer and Wallbott.", "year": 1994}, {"title": "The role of culture in emotion-antecedent appraisal", "author": ["Klaus R Scherer."], "venue": "Journal of personality and social psychology, 73(5):902.", "citeRegEx": "Scherer.,? 1997", "shortCiteRegEx": "Scherer.", "year": 1997}, {"title": "Facebook reactions, the totally redesigned like button, is here", "author": ["Liz Stinson."], "venue": "Wired. http://www. wired.com/2016/02/facebook-reactions-totally-redesigned-like-button/.", "citeRegEx": "Stinson.,? 2016", "shortCiteRegEx": "Stinson.", "year": 2016}, {"title": "Semeval-2007 task 14: Affective text", "author": ["Carlo Strapparava", "Rada Mihalcea."], "venue": "Proceedings of the 4th International Workshop on Semantic Evaluations, pages 70\u201374. Association for Computational Linguistics.", "citeRegEx": "Strapparava and Mihalcea.,? 2007", "shortCiteRegEx": "Strapparava and Mihalcea.", "year": 2007}, {"title": "Learning to identify emotions in text", "author": ["Carlo Strapparava", "Rada Mihalcea."], "venue": "Proceedings of the 2008 ACM symposium on Applied computing, pages 1556\u20131560. ACM.", "citeRegEx": "Strapparava and Mihalcea.,? 2008", "shortCiteRegEx": "Strapparava and Mihalcea.", "year": 2008}, {"title": "Wordnet affect: an affective extension of wordnet", "author": ["Carlo Strapparava", "Alessandro Valitutti"], "venue": "In LREC,", "citeRegEx": "Strapparava and Valitutti,? \\Q2004\\E", "shortCiteRegEx": "Strapparava and Valitutti", "year": 2004}, {"title": "Learning sentiment-specific word embedding for Twitter sentiment classification", "author": ["Duyu Tang", "Furu Wei", "Nan Yang", "Ming Zhou", "Ting Liu", "Bing Qin."], "venue": "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, volume 1, pages 1555\u20131565.", "citeRegEx": "Tang et al\\.,? 2014", "shortCiteRegEx": "Tang et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 23, "context": "It has been observed that this new feature helps Facebook to know much more about their users and exploit this information for targeted advertising (Stinson, 2016), but interest in people\u2019s opinions and how they feel isn\u2019t limited to commercial reasons, as it invests social monitoring, too, including health care and education (Mohammad, 2016).", "startOffset": 148, "endOffset": 163}, {"referenceID": 16, "context": "It has been observed that this new feature helps Facebook to know much more about their users and exploit this information for targeted advertising (Stinson, 2016), but interest in people\u2019s opinions and how they feel isn\u2019t limited to commercial reasons, as it invests social monitoring, too, including health care and education (Mohammad, 2016).", "startOffset": 328, "endOffset": 344}, {"referenceID": 25, "context": "Creating manually annotated datasets large enough to train supervised models is not only costly, but also\u2014especially in the case of opinions and emotions\u2014difficult, due to the intrinsic subjectivity of the task (Strapparava and Mihalcea, 2008; Kim et al., 2010).", "startOffset": 211, "endOffset": 261}, {"referenceID": 10, "context": "Creating manually annotated datasets large enough to train supervised models is not only costly, but also\u2014especially in the case of opinions and emotions\u2014difficult, due to the intrinsic subjectivity of the task (Strapparava and Mihalcea, 2008; Kim et al., 2010).", "startOffset": 211, "endOffset": 261}, {"referenceID": 10, "context": "Therefore, research has focused on unsupervised methods enriched with information derived from lexica, which are manually created (Kim et al., 2010; Chaffar and Inkpen, 2011).", "startOffset": 130, "endOffset": 174}, {"referenceID": 2, "context": "Therefore, research has focused on unsupervised methods enriched with information derived from lexica, which are manually created (Kim et al., 2010; Chaffar and Inkpen, 2011).", "startOffset": 130, "endOffset": 174}, {"referenceID": 12, "context": "using some reasonably safe signals as proxies for automatically labelling training data (Mintz et al., 2009), has been used also for emotion recognition, for example exploiting both emoticons and Twitter hashtags (Purver and Battersby, 2012), but mainly towards creating emotion lexica.", "startOffset": 88, "endOffset": 108}, {"referenceID": 19, "context": ", 2009), has been used also for emotion recognition, for example exploiting both emoticons and Twitter hashtags (Purver and Battersby, 2012), but mainly towards creating emotion lexica.", "startOffset": 112, "endOffset": 140}, {"referenceID": 1, "context": ", 2010; Chaffar and Inkpen, 2011). Since Go et al. (2009) have shown that happy and sad emoticons can be successfully used as signals for sentiment labels, distant supervision, i.", "startOffset": 8, "endOffset": 58}, {"referenceID": 1, "context": ", 2010; Chaffar and Inkpen, 2011). Since Go et al. (2009) have shown that happy and sad emoticons can be successfully used as signals for sentiment labels, distant supervision, i.e. using some reasonably safe signals as proxies for automatically labelling training data (Mintz et al., 2009), has been used also for emotion recognition, for example exploiting both emoticons and Twitter hashtags (Purver and Battersby, 2012), but mainly towards creating emotion lexica. Mohammad and Kiritchenko (2015) use hashtags, experimenting also with highly fine-grained emotion sets (up to almost 600 emotion labels), to create the large Hashtag Emotion Lexicon.", "startOffset": 8, "endOffset": 501}, {"referenceID": 0, "context": "Mohammad and Kiritchenko (2015) use hashtags, experimenting also with highly fine-grained emotion sets (up to almost 600 emotion labels), to create the large Hashtag Emotion Lexicon. Emoticons are used as proxies also by Hallsmar and Palm (2016), who use distributed vector representations to find which words are interchangeable with emoticons but also which emoticons are used in a similar context.", "startOffset": 110, "endOffset": 246}, {"referenceID": 24, "context": "Task 14 at SemEval 2007 (Strapparava and Mihalcea, 2007) was concerned with the classification of emotions and valence in news headlines.", "startOffset": 24, "endOffset": 56}, {"referenceID": 4, "context": "The used emotion labels were Anger, Disgust, Fear, Joy, Sadness, Surprise, in line with the six basic emotions of Ekman\u2019s standard model (Ekman, 1992).", "startOffset": 137, "endOffset": 150}, {"referenceID": 10, "context": "As it is done in most works that use this dataset (Kim et al., 2010; Chaffar and Inkpen, 2011; Calvo and Mac Kim, 2013), we also treat this as a classification problem (coarse-grained).", "startOffset": 50, "endOffset": 119}, {"referenceID": 2, "context": "As it is done in most works that use this dataset (Kim et al., 2010; Chaffar and Inkpen, 2011; Calvo and Mac Kim, 2013), we also treat this as a classification problem (coarse-grained).", "startOffset": 50, "endOffset": 119}, {"referenceID": 25, "context": "This dataset has been extensively used for the evaluation of various unsupervised methods (Strapparava and Mihalcea, 2008), but also for testing different supervised learning techniques and feature portability (Mohammad, 2012).", "startOffset": 90, "endOffset": 122}, {"referenceID": 15, "context": "This dataset has been extensively used for the evaluation of various unsupervised methods (Strapparava and Mihalcea, 2008), but also for testing different supervised learning techniques and feature portability (Mohammad, 2012).", "startOffset": 210, "endOffset": 226}, {"referenceID": 10, "context": "In most works that use this dataset (Kim et al., 2010; Chaffar and Inkpen, 2011; Calvo and Mac Kim, 2013), only sentences where all annotators agreed are used, and the labels angry and disgusted are merged.", "startOffset": 36, "endOffset": 105}, {"referenceID": 2, "context": "In most works that use this dataset (Kim et al., 2010; Chaffar and Inkpen, 2011; Calvo and Mac Kim, 2013), only sentences where all annotators agreed are used, and the labels angry and disgusted are merged.", "startOffset": 36, "endOffset": 105}, {"referenceID": 0, "context": "This is a dataset collected by Alm (2008), where about 1,000 sentences from fairy tales (by B.", "startOffset": 31, "endOffset": 42}, {"referenceID": 21, "context": "3 ISEAR The ISEAR (International Survey on Emotion Antecedents and Reactions (Scherer and Wallbott, 1994; Scherer, 1997)) is a dataset created in the context of a psychology project of the 1990s, by collecting questionnaires answered by people with different cultural backgrounds.", "startOffset": 77, "endOffset": 120}, {"referenceID": 22, "context": "3 ISEAR The ISEAR (International Survey on Emotion Antecedents and Reactions (Scherer and Wallbott, 1994; Scherer, 1997)) is a dataset created in the context of a psychology project of the 1990s, by collecting questionnaires answered by people with different cultural backgrounds.", "startOffset": 77, "endOffset": 120}, {"referenceID": 24, "context": "1, that is the portion that had been released as development set for SemEval\u2019s 2007 Task 14 (Strapparava and Mihalcea, 2007), which contains 250 annotated sentences (Affective development, Section 3.", "startOffset": 92, "endOffset": 124}, {"referenceID": 17, "context": "For the sake of simplicity and transparency, we first trained an SVM with a simple bag-of-words model and default parameters as per the Scikit-learn implementation (Pedregosa et al., 2011) on different combinations of pages.", "startOffset": 164, "endOffset": 188}, {"referenceID": 15, "context": "We used the NRC10 Lexicon because it performed best in the experiments by (Mohammad, 2012), which is built around the emotions anger, anticipation, disgust, fear, joy, sadness, and surprise, and the valence values positive and negative.", "startOffset": 74, "endOffset": 90}, {"referenceID": 11, "context": "\u2022 Google embeddings: pre-trained embeddings trained on Google News and obtained with the skipgram architecture described in (Mikolov et al., 2013).", "startOffset": 124, "endOffset": 146}, {"referenceID": 20, "context": "Using the gensim library (\u0158eh\u016f\u0159ek and Sojka, 2010), we trained the embeddings with the following parameters: window size of 5, learning rate of 0.", "startOffset": 25, "endOffset": 50}, {"referenceID": 5, "context": "\u2022 Retrofitted embeddings: Retrofitting (Faruqui et al., 2015) has been shown as a simple but efficient way of informing trained embeddings with additional information derived from some lexical resource, rather than including it directly at the training stage, as it\u2019s done for example to create senseaware (Iacobacci et al.", "startOffset": 39, "endOffset": 61}, {"referenceID": 9, "context": ", 2015) has been shown as a simple but efficient way of informing trained embeddings with additional information derived from some lexical resource, rather than including it directly at the training stage, as it\u2019s done for example to create senseaware (Iacobacci et al., 2015) or sentiment-aware (Tang et al.", "startOffset": 252, "endOffset": 276}, {"referenceID": 27, "context": ", 2015) or sentiment-aware (Tang et al., 2014) embeddings.", "startOffset": 27, "endOffset": 46}, {"referenceID": 13, "context": "The average f-score is reported as microaverage, to better account for the skewed distribution of the classes as well as in accordance to what is usually reported for this task (Mohammad and Kiritchenko, 2015).", "startOffset": 177, "endOffset": 209}, {"referenceID": 6, "context": "Strapparava and Mihalcea (2008) experiment with several models based on a core LSA model and, in their best performing model (LSA-all emotion words) whose results we report in Table 3, also use information from lexical resources both in their general (WordNet (Fellbaum, 1998)) and emotionaware (WordNet Affect (Strapparava et al.", "startOffset": 260, "endOffset": 276}, {"referenceID": 10, "context": "Overall, the unsupervised but heavily lexicon-based best model of (Kim et al., 2010) performs well on all emotions, excluding surprise, which they do not address (thus also making their classification task slightly easier).", "startOffset": 66, "endOffset": 84}, {"referenceID": 25, "context": "The highest recall for all emotions for this dataset is reported in (Strapparava and Mihalcea, 2008), together with extremely low precision.", "startOffset": 68, "endOffset": 100}, {"referenceID": 10, "context": "On the Fairy Tales dataset, (Kim et al., 2010) Chaffar and Inkpen (2011) also used the Fairy tales dataset to evaluate a supervised model using features like bag-of-words, N-grams and lexical emotion features, but report cross-validated results using accuracy only, and are therefore harder to compare.", "startOffset": 28, "endOffset": 46}, {"referenceID": 2, "context": ", 2010) Chaffar and Inkpen (2011) also used the Fairy tales dataset to evaluate a supervised model using features like bag-of-words, N-grams and lexical emotion features, but report cross-validated results using accuracy only, and are therefore harder to compare.", "startOffset": 8, "endOffset": 34}, {"referenceID": 25, "context": "11 (Strapparava and Mihalcea, 2008) 0.", "startOffset": 3, "endOffset": 35}, {"referenceID": 10, "context": "(Kim et al., 2010) 0.", "startOffset": 0, "endOffset": 18}, {"referenceID": 3, "context": "(Danisman and Alpkocak, 2008) 0.", "startOffset": 0, "endOffset": 29}, {"referenceID": 25, "context": "54 (Strapparava and Mihalcea, 2008) 0.", "startOffset": 3, "endOffset": 35}, {"referenceID": 10, "context": "(Kim et al., 2010) 0.", "startOffset": 0, "endOffset": 18}, {"referenceID": 3, "context": "(Danisman and Alpkocak, 2008) 0.", "startOffset": 0, "endOffset": 29}, {"referenceID": 25, "context": "44 (Strapparava and Mihalcea, 2008) 0.", "startOffset": 3, "endOffset": 35}, {"referenceID": 10, "context": "(Kim et al., 2010) 0.", "startOffset": 0, "endOffset": 18}, {"referenceID": 3, "context": "(Danisman and Alpkocak, 2008) 0.", "startOffset": 0, "endOffset": 29}, {"referenceID": 25, "context": "07 (Strapparava and Mihalcea, 2008) 0.", "startOffset": 3, "endOffset": 35}, {"referenceID": 10, "context": "(Kim et al., 2010)", "startOffset": 0, "endOffset": 18}, {"referenceID": 3, "context": "(Danisman and Alpkocak, 2008)", "startOffset": 0, "endOffset": 29}, {"referenceID": 18, "context": "For the latter, one could for example only select posts that have a certain length, ignore posts that are only quotes or captions to images, or expand posts by including content from linked html pages, which might provide larger and better contexts (Plank et al., 2014).", "startOffset": 249, "endOffset": 269}, {"referenceID": 14, "context": "For the former, namely the choice of Facebook pages, which we believe deserves the most investigation, one could explore several avenues, especially in relation to stance-based issues (Mohammad et al., 2016).", "startOffset": 184, "endOffset": 207}, {"referenceID": 22, "context": "Feature-wise, we want to train emotion-aware embeddings, in the vein of work by Tang et al. (2014), and Iacobacci et al.", "startOffset": 80, "endOffset": 99}, {"referenceID": 9, "context": "(2014), and Iacobacci et al. (2015). Retrofitting FB-embeddings trained on a larger corpus might also be successful, but would rely on an external lexicon.", "startOffset": 12, "endOffset": 36}], "year": 2016, "abstractText": "We exploit the Facebook reaction feature in a distant supervised fashion to train a support vector machine classifier for emotion detection, using several feature combinations and combining different Facebook pages. We test our models on existing benchmarks for emotion detection and show that employing only information that is derived completely automatically, thus without relying on any handcrafted lexicon as it\u2019s usually done, we can achieve competitive results. The results also show that there is large room for improvement, especially by gearing the collection of Facebook pages, with a view to the target domain.", "creator": "TeX"}}}