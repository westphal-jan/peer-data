{"id": "1611.03068", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Nov-2016", "title": "Incremental Sequence Learning", "abstract": "Deep learning research over the past years has shown that by increasing the scope or difficulty of the learning problem over time, increasingly complex learning problems can be addressed. We study incremental learning in the context of sequence learning, using generative RNNs in the form of multi-layer recurrent Mixture Density Networks. We introduce Incremental Sequence Learning, a simple incremental approach to sequence learning. Incremental Sequence Learning starts out by using only the first few steps of each sequence as training data. Each time a performance criterion has been reached, the length of the parts of the sequences used for training is increased. To evaluate Incremental Sequence Learning and comparison methods, we introduce and make available a novel sequence learning task and data set: predicting and classifying MNIST pen stroke sequences, where the familiar handwritten digit images have been transformed to pen stroke sequences representing the skeletons of the digits. We find that Incremental Sequence Learning greatly speeds up sequence learning and reaches the best test performance level of regular sequence learning 20 times faster, reduces the test error by 74%, and in general performs more robustly; it displays lower variance and achieves sustained progress after all three comparison method have stopped improving. A trained sequence prediction model is also used in transfer learning to the task of sequence classification, where it is found that transfer learning realizes improved classification performance compared to methods that learn to classify from scratch.", "histories": [["v1", "Wed, 9 Nov 2016 20:12:08 GMT  (5605kb,D)", "http://arxiv.org/abs/1611.03068v1", "18 pages"], ["v2", "Thu, 1 Dec 2016 21:33:19 GMT  (5706kb,D)", "http://arxiv.org/abs/1611.03068v2", "Updated version: Clarified the contribution (see abstract, intro, and conclusion); added figures to illustrate the architecture of the network and the difference between training and generation; different selection of experiments in Section 6.4; some textual edits"]], "COMMENTS": "18 pages", "reviews": [], "SUBJECTS": "cs.LG cs.NE", "authors": ["edwin d de jong"], "accepted": false, "id": "1611.03068"}, "pdf": {"name": "1611.03068.pdf", "metadata": {"source": "CRF", "title": "Incremental sequence learning", "authors": ["Edwin D. de Jong"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "1.1 Incremental Learning, Transfer Learning, and Representation Learning", "text": "This principle was described as incremental learning by Elman (1991) and has a long history. Worse and Granger (1986) described a pseudo-connecxionist approach that includes incremental learning. Elman (2000) defined incremental learning as an approach in which the training data is not presented at once, but incrementally. Giraud-Carrier (2000) defined incremental learning as follows: \"A learning task is incremental when the training examples are used to solve it over time, but incremental.\" Bengio et al. introduced the framework of curriculum learning. The central idea behind this approach is that a learning system is guided by and / or complex concepts."}, {"heading": "1.2 Sequence Learning", "text": "We study incremental learning in the context of sequence learning, and the goal of sequence learning is to predict what the next step will be based on one step of the sequence. By iteratively feeding the predicted output into the network as the next input, the network can be used to generate a complete sequence of variable length. For a discussion of variants of sequence learning problems, see Sun and Giles (2001); a recent treatment of recurrent neural networks such as here by Lipton (2015). An interesting challenge in sequence learning is that the next step in a sequence learning problem does not clearly follow from the previous step. If this were the case, i.e. if the underlying process that generates the sequences fulfills the Markov property, the learning problem would be reduced to learning one mapping from each step to the next. Instead, steps in the sequence could depend on some or all of the preceding steps in the sequence, i.e. if the sequences underlying the learning problem were to be reduced to the next one that the process generates."}, {"heading": "1.3 Incremental Sequence Learning", "text": "Dependence on the sequence obtained so far provides a special opportunity for incremental learning specific to sequence learning. While the examples in a supervised learning problem have no known relationship to each other, the steps in a sequence have a very specific relationship; later steps in the sequence can only be learned well if the network has learned to develop the corresponding internal state that summarizes the part of the sequence seen so far. This observation leads to the idea that sequence learning can be accelerated by learning to predict the first steps in each sequence first and, once reasonable performance has been achieved and (therefore) an appropriate internal representation of the initial part of the sequence has been developed, gradually increasing the length of the sub-sequences used for training. A prefix of a sequence is a consecutive sub-sequence (a sub-sequence) of the sequence starting from the first element; e.g. the prefix S3 of a sequence S consists of the sequence that is predicted to be a short sequence."}, {"heading": "1.4 Related Work", "text": "In the presentation of the framework of Curriculum Learning, Bengio et al. (2009) provide an example within the field of sequence learning, more precisely in terms of language modeling, where the vocabulary for the training of word sequences is gradually expanded, i.e. the subset of sequences used for the training is gradually increased. A further specialization of Curriculum Learning in the context of sequence learning, described by Bengio et al. (2015), deals with the discrepancy between the training, in which the true previous step is presented as input, and the conclusion, in which the previous output from the network is used as input; when systematically scanned, the likelihood of using the network output as input is gradually increased. Zaremba and Sutskever (2014) apply the curriculum to learning in a sequence-to-sequence context, in which a neural network learns to predict the result of the initial Python work by predicting the number of the first steps, while the other, the generic network learns to predict the result of the simple Python work."}, {"heading": "2 MNIST Handwritten Digits as Pen Stroke Sequences", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Motivation for representing digits as pen stroke sequences", "text": "The classification of MNIST digital images, see LeCun and Cortes (2010), is an example of a task where the success of deep learning was convincingly demonstrated; a test error rate of 0.23% was achieved by Ciresan et al. (2012) with multi-column neural networks. To obtain a sequence that generated the incremental sequences, we created a variant of the familiar MNIST digital data provided by LeCun and Cortes (2010), where each digital image is transformed into a sequence of strokes that produced the digital. (One motivation for displaying digits as strokes is the idea that man is hard to recognize digital or letters, it seems to trace the line in order to reconstruct the path."}, {"heading": "2.2 Construction of the pen stroke sequence data set", "text": "The MNIST handwritten digit dataset consists of 60,000 training images and 10,000 test images, each consisting of 28 x 28 bit map images of the written numerical numbers from 0 to 9. The numbers are therefore converted into one or more pen strokes, each consisting of a sequence of pen-offset pairs (dx, dy). To extract the pen-dash sequences, the following steps are performed: 1. Incremental Theses. Starting from the original MNIST degree scale image, the following properties are measured: \u2022 The number of dissimilar pixels \u2022 The number of connected components, both for the 4 connected and the 8 connected variants. Starting from a sequence level of zero, the threshold is exceeded until either (A) the number of 4-connected or 8-connected components changes, the number of connected components, the number of remaining pixels falls below 50% of the original number, or (C) the threshold of a pre-selected level (250)."}, {"heading": "3 Network Architecture", "text": "We adopt the generative neural network approach described by Graves (2013), which provides for the use of mixed density networks = immediately following steps introduced by Bishop (1994).One sequence corresponds to a complete sequence of a numeral skeleton, represented as a sequence of < dx, dy, eos, eod > tuples, and may contain one or more strokes; see the previous paragraph.The network has four input units corresponding to these four input variables \u2212 sequentially (EOD).To generate the input for the network, the (dx, dy) pairs are scaled to obtain two real input variables dx and dy. The variables indicating the end of the stroke (EOS) and the end of the number (EOD) are binary inputs. Two hidden LSTM layers, see High Rider and Schmidhuber (1997), are used from 200 units, each beginning with one input at a time."}, {"heading": "4 Incremental Sequence Learning and Comparison Methods", "text": "Sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequ"}, {"heading": "5 Experimental Settings", "text": "In this section we describe the experimental setup in detail. Therefore, the configuration of the basic method, regular sequence learning, is as follows: The number of mixing components M = 17, two hidden layers of size 200 are used. \u2022 In these first experiments, a batch size of 50 sequences per batch is used. The learning rate is \u03b1 = 0.0025, with a decay rate of 0.99995 per epoch. \u2022 The sequence of training sequences (not steps within the sequences) is randomized. \u2022 The weight of the regularization component \u03bb = 0.25. In these first experiments, a subset of 10,000 training sequences and 5000 test sequences is used. Error measurement in these numbers is the RMSE of the initial movements predicted by the network (unscaled) pin offsets predicted by the network. \u2022 The RMSE is calculated based on the difference between predicted and actual (dx, dy) pairs that are returned to their original bandwidth from pixels, which is a part of the pixel scale."}, {"heading": "6 Experimental results", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6.1 Sequence Prediction: Comparison of the Methods", "text": "This year it is so far that it will only be a matter of time before it is ready, until it is ready."}, {"heading": "6.2 Loss as a function of sequence position", "text": "In order to further analyze why the variation in sequence length has a particularly strong effect on sequence learning, we evaluate how the relative difficulty of learning a sequence step is related to the position within the sequence. To this end, we measure the average loss contribution of points or steps within a sequence depending on their position within the sequence, as achieved with a learning method that learns whole sequences (not incremental learning) averaged over the first hundred epochs of the training. Figure 7 shows the results. The first steps are fundamentally unpredictable, as the network cannot know what example it will receive next; accordingly, the error at the beginning of the sequence is high, as the method cannot know in advance what the shape or digit class of the new sequence will be. Once the first steps of the sequence have been taken and the context increasingly narrows the possibilities, the loss for predicting the next steps of the sequence is steep, but the position may increase as the loss of the sequence progresses and the uncertainty in the sequence increases."}, {"heading": "6.3 Results on the Full MNIST Pen Stroke Sequence Data Set", "text": "The results reported to date were based on a subset of 10,000 training sequences and 5,000 test sequences to complete a sufficient number of runs for each of the experiments within a reasonable period of time. In light of the positive results obtained with Incremental Sequence Learning, we are now training this method on the complete MNIST Pen Stroke Sequence Data Set, consisting of 60,000 training sequences and 10,000 test sequences (Experiment 4). In these experiments, a batch size of 500 sequences is used instead of 50. Figure 8 shows the results. Compared to the performance of the above experiments, a significant improvement is achieved by training these larger samples; while the best test error in the above results was slightly above 1.5, the test performance for this experiment falls below one; a test error of 0.972 is obtained on the complete test dataset. It is noteworthy that the test error is initially much greater than the traction error, but the test error continues to improve over a long period of time and is very closely related to the training error in other words; it is virtually not monitored in other words."}, {"heading": "6.4 Transfer Learning", "text": "The first problem discussed here was the prediction of step t + 1 of a sequence capable of capturing the individual sequences, and the first sequence of a sequence representing a sequence of two or three sequences to see whether it is capable of obtaining a sequence of three sequences requiring a sequence of four sequences, and the second sequence of four sequences representing a sequence of four sequences in the sequence of four sequences in the sequence of four sequences in the sequence of four sequences."}, {"heading": "7 Generative results", "text": "To get an idea of what the network has learned, we will report on examples of network performance in this section. 1This performance was achieved after training in 7 \u00b7 107 sequence steps, approximately twice as long as the run shown in the diagram."}, {"heading": "7.1 Development during training", "text": "During training, the network receives each sequence step by step, and after each step it expresses its expectation of moving the next point. In these numbers and films, we visualize the predictions of the network for a particular sequence at different stages of the training process. All results were obtained from a single run of incremental sequence learning.After 80 lots After 140 lots After 530 lots After 570 lots After 650 lots"}, {"heading": "7.2 Unguided output generation, a.k.a. neural network hallucination", "text": "The guidance available during the training in the form of receiving each next step of the sequence following a prediction is not available here. Instead, the output generated by the network is fed back into the network as the next input; Figure 11 shows sample results."}, {"heading": "7.3 Sequence classification", "text": "The third analysis of the behavior of the trained network is to see what happens during sequence classification \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf At each step of the sequence, we monitor the ten class outputs and visualize their output. As further steps of the sequence are received, the network receives more information and adjusts its expectation of which digit class represents the sequence. \u25cf \u25cf MNIST impact sequence test image 25 \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf The sequence sequence sequence sequence-conditioned sequence sequence classification output for a sequence that represents a 0."}, {"heading": "8 Conclusions", "text": "We investigated an approach to sequence learning where the training data is initially limited to the first few steps of each sequence. Gradually, as the network learns to predict the early parts of the sequences, the length of the part of the sequences used for the training is increased. We call this approach Incremental Sequence Learning and note that it greatly improves the learning performance of the sequences. A first observation was that with incremental sequence learning, the time required to achieve the best test performance level of regular sequence learning was much lower; on average, the method reached this level twenty times faster, achieving a significant acceleration and reduction in the computing costs of sequence learning. More importantly, incremental sequence learning was found to reduce the test error of regular sequence learning by 74%. Two other forms of curriculum sequence learning used for comparison showed no improvement compared to regular sequence learning."}, {"heading": "9 Resources", "text": "The Tensorflow implementation used to perform these experiments is available for download here: https: / / github.com / edwin-de-jong / incremental-stroke-sequence-learningThe MNIST stroke sequence-data is available here: https: / / github.com / edwin-de-jong / mnist-digits-stroke-sequence-data / wiki / MNIST-digits-as-stroke-sequence-dataThe code for transforming the MNIST digit data set to a pen stroke sequence data set is available: https: / / github.com / edwin-de-jong / mnist-digits-as-stroke-sequences / wiki / MNIST-digits-as-stroke-sequences- (code)"}, {"heading": "Acknowledgments", "text": "The author thanks Max Welling, Dick de Ridder and Michiel de Jong for valuable comments and suggestions on earlier versions."}], "references": [{"title": "Scheduled sampling for sequence prediction with recurrent neural networks", "author": ["S. Bengio", "O. Vinyals", "N. Jaitly", "N. Shazeer"], "venue": "Proceedings of the 28th International Conference on Neural Information Processing Systems, NIPS\u201915, pages 1171\u20131179, Cambridge, MA, USA. MIT Press.", "citeRegEx": "Bengio et al\\.,? 2015", "shortCiteRegEx": "Bengio et al\\.", "year": 2015}, {"title": "Representation learning: A review and new perspectives", "author": ["Y. Bengio", "A. Courville", "P. Vincent"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell., 35(8):1798\u20131828.", "citeRegEx": "Bengio et al\\.,? 2013", "shortCiteRegEx": "Bengio et al\\.", "year": 2013}, {"title": "Curriculum learning", "author": ["Y. Bengio", "J. Louradour", "R. Collobert", "J. Weston"], "venue": "Proceedings of the 26th Annual International Conference on Machine Learning, ICML \u201909, pages 41\u201348, New York, NY, USA. ACM.", "citeRegEx": "Bengio et al\\.,? 2009", "shortCiteRegEx": "Bengio et al\\.", "year": 2009}, {"title": "Mixture density networks", "author": ["C. Bishop"], "venue": "Technical Report NCRG/94/0041, Aston University.", "citeRegEx": "Bishop,? 1994", "shortCiteRegEx": "Bishop", "year": 1994}, {"title": "Multitask learning", "author": ["R. Caruana"], "venue": "Mach. Learn., 28(1):41\u201375.", "citeRegEx": "Caruana,? 1997", "shortCiteRegEx": "Caruana", "year": 1997}, {"title": "Multi-column deep neural networks for image classification", "author": ["D.C. Ciresan", "U. Meier", "J. Schmidhuber"], "venue": "CoRR, abs/1202.2745.", "citeRegEx": "Ciresan et al\\.,? 2012", "shortCiteRegEx": "Ciresan et al\\.", "year": 2012}, {"title": "A coevolutionary approach to representation development", "author": ["E.D. de Jong", "T. Oates"], "venue": "Proceedings of the ICML-2002 Workshop on Development of Representations,", "citeRegEx": "Jong and Oates,? \\Q2002\\E", "shortCiteRegEx": "Jong and Oates", "year": 2002}, {"title": "Incremental learning, or the importance of starting small", "author": ["J.L. Elman"], "venue": "crl technical report 9101. Technical report, University of California, San Diego.", "citeRegEx": "Elman,? 1991", "shortCiteRegEx": "Elman", "year": 1991}, {"title": "Learning and development in neural networks: The importance of starting small", "author": ["J.L. Elman"], "venue": "Cognition, 48:781\u201399.", "citeRegEx": "Elman,? 1993", "shortCiteRegEx": "Elman", "year": 1993}, {"title": "A note on the utility of incremental learning", "author": ["C. Giraud-Carrier"], "venue": "AI Commun., 13(4):215\u2013223.", "citeRegEx": "Giraud.Carrier,? 2000", "shortCiteRegEx": "Giraud.Carrier", "year": 2000}, {"title": "Generating sequences with recurrent neural networks", "author": ["A. Graves"], "venue": "CoRR, abs/1308.0850.", "citeRegEx": "Graves,? 2013", "shortCiteRegEx": "Graves", "year": 2013}, {"title": "Deep residual learning for image recognition", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "CoRR, abs/1512.03385.", "citeRegEx": "He et al\\.,? 2015", "shortCiteRegEx": "He et al\\.", "year": 2015}, {"title": "Inferring motor programs from images of handwritten digits", "author": ["G.E. Hinton", "V. Nair"], "venue": "Advances in Neural Information Processing Systems 18 [Neural Information Processing Systems, NIPS 2005, December 5-8, 2005, Vancouver, British Columbia, Canada], pages 515\u2013522.", "citeRegEx": "Hinton and Nair,? 2005", "shortCiteRegEx": "Hinton and Nair", "year": 2005}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural Computation, 9(8):1735\u2013 1780.", "citeRegEx": "Hochreiter and Schmidhuber,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber", "year": 1997}, {"title": "On large-batch training for deep learning: Generalization gap and sharp minima", "author": ["N.S. Keskar", "D. Mudigere", "J. Nocedal", "M. Smelyanskiy", "P.T.P. Tang"], "venue": "CoRR, abs/1609.04836.", "citeRegEx": "Keskar et al\\.,? 2016", "shortCiteRegEx": "Keskar et al\\.", "year": 2016}, {"title": "A critical review of recurrent neural networks for sequence learning", "author": ["Z.C. Lipton"], "venue": "CoRR, abs/1506.00019.", "citeRegEx": "Lipton,? 2015", "shortCiteRegEx": "Lipton", "year": 2015}, {"title": "Symbiotic Evolution Of Neural Networks In Sequential Decision Tasks", "author": ["D.E. Moriarty"], "venue": "PhD thesis, Department of Computer Sciences, The University of Texas at Austin. Technical Report UT-AI97-257.", "citeRegEx": "Moriarty,? 1997", "shortCiteRegEx": "Moriarty", "year": 1997}, {"title": "Actor-mimic: Deep multitask and transfer reinforcement learning", "author": ["E. Parisotto", "L.J. Ba", "R. Salakhutdinov"], "venue": "CoRR, abs/1511.06342.", "citeRegEx": "Parisotto et al\\.,? 2015", "shortCiteRegEx": "Parisotto et al\\.", "year": 2015}, {"title": "Discriminability-based transfer between neural networks", "author": ["L.Y. Pratt"], "venue": "Advances in Neural Information Processing Systems 5, [NIPS Conference], pages 204\u2013211, San Francisco, CA, USA. Morgan Kaufmann Publishers Inc.", "citeRegEx": "Pratt,? 1993", "shortCiteRegEx": "Pratt", "year": 1993}, {"title": "Sim-to-real robot learning from pixels with progressive nets", "author": ["A.A. Rusu", "M. Vecerik", "T. Roth\u00f6rl", "N. Heess", "R. Pascanu", "R. Hadsell"], "venue": "arxiv:1610.04286 [cs.ro]. Technical report, Deep Mind.", "citeRegEx": "Rusu et al\\.,? 2016", "shortCiteRegEx": "Rusu et al\\.", "year": 2016}, {"title": "Incremental learning from noisy data", "author": ["J.C. Schlimmer", "R.H. Granger"], "venue": "Machine Learning, 1(3):317\u2013354.", "citeRegEx": "Schlimmer and Granger,? 1986", "shortCiteRegEx": "Schlimmer and Granger", "year": 1986}, {"title": "Structure learning in random fields for heart motion abnormality detection", "author": ["M. Schmidt", "K. Murphy", "G. Fung", "R. Rosales"], "venue": "In CVPR.", "citeRegEx": "Schmidt et al\\.,? 2008", "shortCiteRegEx": "Schmidt et al\\.", "year": 2008}, {"title": "Sequence learning: from recognition and prediction to sequential decision making", "author": ["R. Sun", "C.L. Giles"], "venue": "IEEE Intelligent Systems, 16(4):67\u201370.", "citeRegEx": "Sun and Giles,? 2001", "shortCiteRegEx": "Sun and Giles", "year": 2001}, {"title": "Is learning the n-th thing any easier than learning the first", "author": ["S. Thrun"], "venue": "Advances in Neural Information Processing Systems, volume 8, pages 640\u2013646.", "citeRegEx": "Thrun,? 1996", "shortCiteRegEx": "Thrun", "year": 1996}, {"title": "Learning to execute", "author": ["W. Zaremba", "I. Sutskever"], "venue": "CoRR, abs/1410.4615.", "citeRegEx": "Zaremba and Sutskever,? 2014", "shortCiteRegEx": "Zaremba and Sutskever", "year": 2014}, {"title": "A fast parallel algorithm for thinning digital patterns", "author": ["T.Y. Zhang", "C.Y. Suen"], "venue": "Commun. ACM, 27(3):236\u2013239.", "citeRegEx": "Zhang and Suen,? 1984", "shortCiteRegEx": "Zhang and Suen", "year": 1984}], "referenceMentions": [{"referenceID": 4, "context": "This principle has been described as Incremental learning by Elman (1991), and has a long history.", "startOffset": 61, "endOffset": 74}, {"referenceID": 4, "context": "This principle has been described as Incremental learning by Elman (1991), and has a long history. Schlimmer and Granger (1986) described a pseudo-connectionist distributed concept learning approach involving incremental learning.", "startOffset": 61, "endOffset": 128}, {"referenceID": 4, "context": "This principle has been described as Incremental learning by Elman (1991), and has a long history. Schlimmer and Granger (1986) described a pseudo-connectionist distributed concept learning approach involving incremental learning. Elman (1991) defined Incremental Learning as an approach where the training data is not presented all at once, but incrementally; see also Elman (1993).", "startOffset": 61, "endOffset": 244}, {"referenceID": 4, "context": "This principle has been described as Incremental learning by Elman (1991), and has a long history. Schlimmer and Granger (1986) described a pseudo-connectionist distributed concept learning approach involving incremental learning. Elman (1991) defined Incremental Learning as an approach where the training data is not presented all at once, but incrementally; see also Elman (1993). Giraud-Carrier (2000) defines Incremental Learning as follows: \u201cA learning task is incremental if the training examples used to solve it become available over time, usually one at a time.", "startOffset": 61, "endOffset": 383}, {"referenceID": 4, "context": "This principle has been described as Incremental learning by Elman (1991), and has a long history. Schlimmer and Granger (1986) described a pseudo-connectionist distributed concept learning approach involving incremental learning. Elman (1991) defined Incremental Learning as an approach where the training data is not presented all at once, but incrementally; see also Elman (1993). Giraud-Carrier (2000) defines Incremental Learning as follows: \u201cA learning task is incremental if the training examples used to solve it become available over time, usually one at a time.", "startOffset": 61, "endOffset": 406}, {"referenceID": 0, "context": "\u201c Bengio et al. (2009) introduced the framework of Curriculum Learning.", "startOffset": 2, "endOffset": 23}, {"referenceID": 10, "context": "This approach, known as Transfer Learning or Inductive Transfer, was first described by Pratt (1993). Thrun (1996) reported improved generalization performance for lifelong learning and described representation learning, whereas Caruana (1997) considered a Multitask learning setup where tasks are learned in parallel while using a shared representation.", "startOffset": 88, "endOffset": 101}, {"referenceID": 10, "context": "This approach, known as Transfer Learning or Inductive Transfer, was first described by Pratt (1993). Thrun (1996) reported improved generalization performance for lifelong learning and described representation learning, whereas Caruana (1997) considered a Multitask learning setup where tasks are learned in parallel while using a shared representation.", "startOffset": 88, "endOffset": 115}, {"referenceID": 1, "context": "Thrun (1996) reported improved generalization performance for lifelong learning and described representation learning, whereas Caruana (1997) considered a Multitask learning setup where tasks are learned in parallel while using a shared representation.", "startOffset": 127, "endOffset": 142}, {"referenceID": 1, "context": "Thrun (1996) reported improved generalization performance for lifelong learning and described representation learning, whereas Caruana (1997) considered a Multitask learning setup where tasks are learned in parallel while using a shared representation. In coevolutionary algorithms, the coevolution of representations with solutions that employ them, see e.g. Moriarty (1997); de Jong and Oates (2002), provides another approach to representation learning.", "startOffset": 127, "endOffset": 376}, {"referenceID": 1, "context": "Thrun (1996) reported improved generalization performance for lifelong learning and described representation learning, whereas Caruana (1997) considered a Multitask learning setup where tasks are learned in parallel while using a shared representation. In coevolutionary algorithms, the coevolution of representations with solutions that employ them, see e.g. Moriarty (1997); de Jong and Oates (2002), provides another approach to representation learning.", "startOffset": 127, "endOffset": 402}, {"referenceID": 0, "context": "Bengio et al. (2013) provide a review and insightful discussion of representation learning.", "startOffset": 0, "endOffset": 21}, {"referenceID": 0, "context": "Bengio et al. (2013) provide a review and insightful discussion of representation learning. Parisotto et al. (2015) report experiments with transfer learning across Atari 2600 arcade games where up to 5 million frames of training time in each game are saved.", "startOffset": 0, "endOffset": 116}, {"referenceID": 0, "context": "Bengio et al. (2013) provide a review and insightful discussion of representation learning. Parisotto et al. (2015) report experiments with transfer learning across Atari 2600 arcade games where up to 5 million frames of training time in each game are saved. More recently, successful transfer of robot learning from the virtual to the real world was achieved using transfer learning, see Rusu et al. (2016). And at the annual ImageNet Large-Scale Visual Recognition Challenge (ILSVRC), the depth of networks has steadily increased over the years, so far leading up to a network of 152 layers for the winning entry in the ILSVRC 2015 classification task; see He et al.", "startOffset": 0, "endOffset": 408}, {"referenceID": 0, "context": "Bengio et al. (2013) provide a review and insightful discussion of representation learning. Parisotto et al. (2015) report experiments with transfer learning across Atari 2600 arcade games where up to 5 million frames of training time in each game are saved. More recently, successful transfer of robot learning from the virtual to the real world was achieved using transfer learning, see Rusu et al. (2016). And at the annual ImageNet Large-Scale Visual Recognition Challenge (ILSVRC), the depth of networks has steadily increased over the years, so far leading up to a network of 152 layers for the winning entry in the ILSVRC 2015 classification task; see He et al. (2015).", "startOffset": 0, "endOffset": 676}, {"referenceID": 21, "context": "For a discussion of variants of sequence learning problems, see Sun and Giles (2001); a more recent treatment covering recurrent neural networks as used here is provided by Lipton (2015).", "startOffset": 64, "endOffset": 85}, {"referenceID": 15, "context": "For a discussion of variants of sequence learning problems, see Sun and Giles (2001); a more recent treatment covering recurrent neural networks as used here is provided by Lipton (2015). An interesting challenge in sequence learning is that for most sequence learning problems of interest, the next step in a sequence does not follow unambiguously from the previous step.", "startOffset": 173, "endOffset": 187}, {"referenceID": 0, "context": "In presenting the framework of Curriculum Learning, Bengio et al. (2009) provide an example within the domain of sequence learning, more specifically concerning language modeling.", "startOffset": 52, "endOffset": 73}, {"referenceID": 0, "context": "In presenting the framework of Curriculum Learning, Bengio et al. (2009) provide an example within the domain of sequence learning, more specifically concerning language modeling. There, the vocabulary used for training on word sequences is gradually increased, i.e. the subset of sequences used for training is gradually increased. Another specialization of Curriculum Learning to the context of sequence learning described by Bengio et al. (2015) addresses the discrepancy between training, where the true previous step is presented as input, and inference, where the previous output from the network is used as input; with scheduled sampling, the probability of using the network output as input is adapted to gradually increase over time.", "startOffset": 52, "endOffset": 449}, {"referenceID": 0, "context": "In presenting the framework of Curriculum Learning, Bengio et al. (2009) provide an example within the domain of sequence learning, more specifically concerning language modeling. There, the vocabulary used for training on word sequences is gradually increased, i.e. the subset of sequences used for training is gradually increased. Another specialization of Curriculum Learning to the context of sequence learning described by Bengio et al. (2015) addresses the discrepancy between training, where the true previous step is presented as input, and inference, where the previous output from the network is used as input; with scheduled sampling, the probability of using the network output as input is adapted to gradually increase over time. Zaremba and Sutskever (2014) apply curriculum learning in a sequence-to-sequence learning context where a neural network learns to predict the outcome of Python programs.", "startOffset": 52, "endOffset": 772}, {"referenceID": 2, "context": "23% was obtained by Ciresan et al. (2012) using Multi-column Deep Neural Networks.", "startOffset": 20, "endOffset": 42}, {"referenceID": 2, "context": "23% was obtained by Ciresan et al. (2012) using Multi-column Deep Neural Networks. To obtain a sequence learning data set for evaluating Incremental Sequence Learning, we created a variant of the familiar MNIST handwritten digit data set provided by LeCun and Cortes (2010) where each digit image is transformed into a sequence of pen strokes that could have generated the digit.", "startOffset": 20, "endOffset": 274}, {"referenceID": 2, "context": "23% was obtained by Ciresan et al. (2012) using Multi-column Deep Neural Networks. To obtain a sequence learning data set for evaluating Incremental Sequence Learning, we created a variant of the familiar MNIST handwritten digit data set provided by LeCun and Cortes (2010) where each digit image is transformed into a sequence of pen strokes that could have generated the digit. One motivation for representing digits as strokes is the notion that when humans try to discern digits or letters that are difficult to read, it appears natural to trace the line so as to reconstruct what path the author\u2019s pen may have taken. Indeed, Hinton and Nair (2005) note that the idea that patterns can be recognized by figuring out how they were generated was already introduced in the 1950\u2019s, and describe a generative model for handwritten digits that uses two pairs of opposing springs whose stiffnesses are controlled by a motor program.", "startOffset": 20, "endOffset": 654}, {"referenceID": 0, "context": "Pen stroke sequences also form a natural and efficient representation for digits; handwriting constitutes a canonical manifestation of the manifold hypothesis, according to which \u201creal-world data presented in high dimensional spaces are expected to concentrate in the vicinity of a manifoldM of much lower dimensionality dM, embedded in high dimensional input space Rx\u201d; see Bengio et al. (2013). Specifically: (i) the vast majority of the pixels are white, (ii) almost all digit images consist of a single connected set of pixels, and (iii) the shapes mostly consist of smooth curved lines.", "startOffset": 375, "endOffset": 396}, {"referenceID": 25, "context": "A common method for image thinning, described by Zhang and Suen (1984), is applied.", "startOffset": 49, "endOffset": 71}, {"referenceID": 9, "context": "We adopt the approach to generative neural networks described by Graves (2013) which makes use of mixture density networks, introduced by Bishop (1994).", "startOffset": 65, "endOffset": 79}, {"referenceID": 3, "context": "We adopt the approach to generative neural networks described by Graves (2013) which makes use of mixture density networks, introduced by Bishop (1994). One sequence corresponds to one complete", "startOffset": 138, "endOffset": 152}, {"referenceID": 13, "context": "Two hidden LSTM layers, see Hochreiter and Schmidhuber (1997), of 200 units each are used.", "startOffset": 28, "endOffset": 62}, {"referenceID": 20, "context": "Schmidt et al. (2008). The definition of the sequence prediction loss LP follows Graves (2013), with the difference that terms for the eod and for the L-\u221e loss are included:", "startOffset": 0, "endOffset": 22}, {"referenceID": 10, "context": "The definition of the sequence prediction loss LP follows Graves (2013), with the difference that terms for the eod and for the L-\u221e loss are included:", "startOffset": 58, "endOffset": 72}, {"referenceID": 0, "context": "\u2022 Increasing training set size Bengio et al. (2009) describe an application of curriculum learning to sequence learning, where the task is to predict the best word which can follow a given context of words in a correct English sentence.", "startOffset": 31, "endOffset": 52}, {"referenceID": 14, "context": "HypothesisH1 therefore is that (A) the smaller batch size improves performance, see Keskar et al. (2016) for earlier findings in this direction, and/or (B) the adaptive batch size aspect has a positive effect on performance.", "startOffset": 84, "endOffset": 105}], "year": 2017, "abstractText": "Deep learning research over the past years has shown that by increasing the scope or difficulty of the learning problem over time, increasingly complex learning problems can be addressed. We study incremental learning in the context of sequence learning, using generative RNNs in the form of multi-layer recurrent Mixture Density Networks. We introduce Incremental Sequence Learning, a simple incremental approach to sequence learning. Incremental Sequence Learning starts out by using only the first few steps of each sequence as training data. Each time a performance criterion has been reached, the length of the parts of the sequences used for training is increased. To evaluate Incremental Sequence Learning and comparison methods, we introduce and make available a novel sequence learning task and data set: predicting and classifying MNIST pen stroke sequences, where the familiar handwritten digit images have been transformed to pen stroke sequences representing the skeletons of the digits. We find that Incremental Sequence Learning greatly speeds up sequence learning and reaches the best test performance level of regular sequence learning 20 times faster, reduces the test error by 74%, and in general performs more robustly; it displays lower variance and achieves sustained progress after all three comparison method have stopped improving. A trained sequence prediction model is also used in transfer learning to the task of sequence classification, where it is found that transfer learning realizes improved classification performance compared to methods that learn to classify from scratch.", "creator": "LaTeX with hyperref package"}}}