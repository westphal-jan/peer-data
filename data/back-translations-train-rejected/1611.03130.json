{"id": "1611.03130", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Nov-2016", "title": "Computationally Efficient Target Classification in Multispectral Image Data with Deep Neural Networks", "abstract": "Detecting and classifying targets in video streams from surveillance cameras is a cumbersome, error-prone and expensive task. Often, the incurred costs are prohibitive for real-time monitoring. This leads to data being stored locally or transmitted to a central storage site for post-incident examination. The required communication links and archiving of the video data are still expensive and this setup excludes preemptive actions to respond to imminent threats. An effective way to overcome these limitations is to build a smart camera that transmits alerts when relevant video sequences are detected. Deep neural networks (DNNs) have come to outperform humans in visual classifications tasks. The concept of DNNs and Convolutional Networks (ConvNets) can easily be extended to make use of higher-dimensional input data such as multispectral data. We explore this opportunity in terms of achievable accuracy and required computational effort. To analyze the precision of DNNs for scene labeling in an urban surveillance scenario we have created a dataset with 8 classes obtained in a field experiment. We combine an RGB camera with a 25-channel VIS-NIR snapshot sensor to assess the potential of multispectral image data for target classification. We evaluate several new DNNs, showing that the spectral information fused together with the RGB frames can be used to improve the accuracy of the system or to achieve similar accuracy with a 3x smaller computation effort. We achieve a very high per-pixel accuracy of 99.1%. Even for scarcely occurring, but particularly interesting classes, such as cars, 75% of the pixels are labeled correctly with errors occurring only around the border of the objects. This high accuracy was obtained with a training set of only 30 labeled images, paving the way for fast adaptation to various application scenarios.", "histories": [["v1", "Wed, 9 Nov 2016 23:13:18 GMT  (1847kb)", "http://arxiv.org/abs/1611.03130v1", "Presented at SPIE Security + Defence 2016 Proc. SPIE 9997, Target and Background Signatures II"]], "COMMENTS": "Presented at SPIE Security + Defence 2016 Proc. SPIE 9997, Target and Background Signatures II", "reviews": [], "SUBJECTS": "cs.CV cs.AI cs.NE", "authors": ["lukas cavigelli", "dominic bernath", "michele magno", "luca benini"], "accepted": false, "id": "1611.03130"}, "pdf": {"name": "1611.03130.pdf", "metadata": {"source": "META", "title": "Computationally Efficient Target Classification", "authors": ["Lukas Cavigelli", "Dominic Bernath", "Michele Magno", "Luca Benini"], "emails": ["cavigelli@iis.ee.ethz.ch"], "sections": [{"heading": null, "text": "Detecting and classifying targets in video streams from surveillance cameras is a cumbersome, error-prone, and expensive task. Often, the cost of real-time surveillance is prohibitive, resulting in data being stored locally or transferred to a central location for post-incident investigation; the required communications and archiving of video data is still expensive, and this setup precludes preventive measures to respond to looming threats. An effective way to overcome these limitations is to build a smart camera that analyzes the data on-site near the sensor and delivers warnings when relevant video sequences are detected; deep neural networks (DNNs) have led to outperform people in visual classification tasks and perform exceptionally well on other computer vision tasks; the concept of DNNs and Convolutional Networks (ConvNets) can be easily expanded to make higher-dimensional input data such as multi-spectractor data."}, {"heading": "1. INTRODUCTION", "text": "This year, it is as far as ever in the history of the city, where it is as far as never before in the history of the city."}, {"heading": "2. RELATED WORK", "text": "The ground-breaking performance of deep learning and Convolutional Neural Networks (ConvNets) in particular is undeniable today. ConvNets have demonstrated that they far exceed traditional computer vision approaches in many areas of application, and have even demonstrated their superhuman performance in visual tasks such as image classification. In this paper, we focus on the labeling of scenes sometimes referred to as semantic segmentation, for which ConvNets show similarly outstanding results 8,16-18, hyper- and multispectral data and images obtained with very specific spectral filters have been successfully used for industrial computer visions (quality control,...) and for remote sensing, which have been very expensive for some time, bulky, and often require a non-trivial synchronization system. With the recent emergence of single-chip multispectral filters, sensors from snovialisms that relate to the amount of data are being used."}, {"heading": "3. DATASET COLLECTION", "text": "In addition, the sensor types used focus heavily on the use of beam splitters and dedicated image sensors for each channel instead of a multispectral mosaic sensor20. Before we can perform evaluations to answer the question of whether multispectral data can improve the results of scene labeling or simplify the processing pipeline to achieve good results, we need to create a dataset. In this section, we will combine a multispectral 25-channel mosaic sensor with a high-resolution RGB camera that could be integrated with an embedded processor such as the Tegra K1 to build a smart camera capable of processing data on the ground. In this section, we will explain how we collected this dataset, identify the specific cameras, explain how the data from the two sensors were merged, and how the down-to-earth truth marking was created."}, {"heading": "3.1 Image sensors", "text": "We collected a dataset with two sensors, a high-resolution RGB sensor to accurately capture shapes, and a lower-resolution multispectral sensor to obtain additional information about the materials. For the RGB images, we used a high-resolution camera from Point Grey, the Flea3 FL3-U3-32S2C-CS, which was built around the Sony IMX036 1 / 2.8 \"CMOS sensor, which has 2080 x 1552 pixel images at 60 frames / s via USB 3. We equipped this camera with a Fujinon YV2.8x2.8SA-2 lens with a variable focal length of 2.8-8mm.The multispectral images were captured with the Ximea xiSpec MQ022HG-IM-SM5X5 NIR camera, which has a 2 / 3\" CMOS snapshot mosaic sensor of 2.8-8 mm.The multispectral images were captured with a 2.8 \"spectral lens of the X8\" spectral range, the SM5X5 \"NIR camera is equipped with a 2.8\" spectral spectral of the X8 \"spectral range."}, {"heading": "3.2 Image alignment", "text": "To create a corrected setup, we mounted both cameras on a mounting plate and adjusted the focal length of the RGB camera lens to best match the image field size of the multispectral camera. In a first step, the multispectral mosaic image is converted from its 2D data layout into a multispectral cube with 217 x 409 pixels and 25 channels. As we overlay the images, some distortion differences become visible. To correct this, we exclude from this transformation a geometric transformation using the Local Weighted Center Transformation (LWMT) with the 12 narrowest points used to achieve a 2nd degree polymatic transformation for each control point pair based on a total of 33 pairs of correspondence scattered throughout the image. We use this transformation to distort the multispectral image cube onto the RGB image using bionic interpolation, with the pixels of the image sources of the two channels so that they can be exactly matched to one another 28."}, {"heading": "3.3 Data labeling", "text": "We have collected 40 images from the same perspective of road surveillance. Based on the RGB image, we have labeled each pixel with one of 8 classes: car / truck, sky, building, road / gravel, tree / bushes, tram, water, remote background. For evaluation, we have randomly divided the data set into 30 training images and 10 test images. Some sample images are shown in Figure 3.To facilitate the creation of ground truth, we have developed a program that helps with the labeling of the data set shown in Figure 5. Instead of assigning each pixel to a class individually, we segment the image into superpixels using the SLIC algorithm 34 and label it. SLIC groups the pixels on a combination of photo removal and L2 distance in the image plane to create an oversegmentation of the image, the label is distributed on each pixel individually."}, {"heading": "4. NEURAL NETWORK ARCHITECTURES", "text": "In this section, we will present three types of neural networks, starting with a per-pixel classification with a normal multi-layer neural network, to explore what is possible with a relatively simple classifier, and then we will present our own proposed ConvNets and examine the improvements that can be made to the shape and texture of objects in the image. We will approach this by matching a well-known scene name called ConvNet to another, RGB-only dataset, and further exploring the use of ConvNets based on current state-of-the-art image recognition concepts and adapting it to our application."}, {"heading": "4.1 Per-pixel classification", "text": "This type of analysis is performed on a perpixel basis, regardless of adjacent values. We perform such a material-based classification using a 5-layer neural network, which is evaluated for each pixel individually to determine whether the additional multispectral channels can improve the segmentation results in this setting, which should provide a data point in the corner of fast and highly energy-efficient analysis with less accuracy than the more complex ConvNets.We classify each pixel individually by its 3 or 28 channels for the RGB-pure or multispectral + RGB image. We train a 5-layer neural network with 32, 128, 512, 64 and 10 output channels for each layer. As a non-linearity between layers, we use the ReLU activation function, which is prefixed to a stack normalization, to support rapid training. As explained in the next sections, the other neural networks contain two subdivisions, each of which we better use in two layers before each."}, {"heading": "4.2 Convolutional network targeting the Stanford backgrounds dataset", "text": "Just think of single pixels of an image that is not optimal if you want to achieve a high-quality semantic segmentation of the scene. On the basis of this input, it would also be enormously difficult to solve such a task. A key factor for high-quality segmentation is the recognition of the shape of objects, their texture and their contextual relation. With such an analysis, one can improve only little information about the texture, only the color of the pixel, and very little contextual information about the capture of objects and their distribution."}, {"heading": "4.3 ResNet-inspired convolutional networks", "text": "The network presented in the previous section has been optimized for a different dataset, and newer types of networks that have higher image recognition accuracy are now available. Deep Remaining Networks (ResNets) are currently the most advanced method with a sequence of very small filters (3 x 3) and every few layers of a bypass path to train deeper layers of the network18. We use this concept to build our own ConvNets and optimize them for our application. We have constructed several ConvNets of varying depth and number of feature sketches and analyze the effects of multi-scale features and multispectral data. We have retained the two most interesting results that are on the Pareto-optimal front in terms of error rate and computing effort."}, {"heading": "4.4 Training the convolutional networks", "text": "The trained parameters are optimized using the ADAM algorithm 37 and batch normalization layers before the activation functions and without dropouts 36. We apply a balanced multi-stage margin loss function, (,) = \u2211 max (0, 1 \u2212) with the target class index and x of the output of the ConvNet. Thus, we optimize the parameters of the network to maximize the removal of training samples in the output area of the ConvNet, similar to the linear SVM lens 38. All networks used here have a lower resolution due to the pooling layers. We train the network with an appropriately scanned soil truth marking."}, {"heading": "5. RESULTS & DISCUSSION", "text": "It has been shown that this is a miscalculation, that it is in a position, it is in a position, it is in a position, it is in a position, it is in a position, it is in a position, it is in a position, it is in a position, it is in a position, it is in a position, it is in a position, it is in a position, it is in a position, it is in a position, it is in a position, it is in a position, it is in a position, it is in a position."}, {"heading": "6. CONCLUSIONS", "text": "In this paper, we explored the benefits of combining an RGB camera with a multispectral camera in an embedded smart camera. We collected a dataset for scene labeling from an urban surveillance perspective, including a multispectral camera. We presented novel scene labeling ConvNets that use this additional data. We demonstrated that even with a very limited amount of labeled data, high-precision pleated networks can be trained, making them an interesting option for rapid deployment in new environments. We also report on how multispectral data can be used to improve accuracy or alternatively reduce computing effort by effectively increasing overall energy efficiency by three times and bringing real-time processing closer to what is possible on embedded processing platforms."}, {"heading": "ACKNOWLEDGEMENTS", "text": "This work was financed by armasuisse Science & Technology."}], "references": [{"title": "Assessment of target detection limits in hyperspectral data,", "author": ["W. Gross", "J. Boehler", "Schilling H", "W. Middelmann", "J. Weyermann", "P. Wellig", "C. Oechslin R", "M. Kneubuehler"], "venue": "Proc. SPIE Secur. + Def", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2015}, {"title": "Hyperspectral Remote Sensing Data Analysis and Future Challenges,", "author": ["J.M. Bioucas-dias", "A. Plaza", "G. Camps-valls", "P. Scheunders", "Nasrabadi", "N. M", "J. Chanussot"], "venue": "IEEE Geosci. Remote Sens. Mag.(June),", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2013}, {"title": "Estimation of floodplain aboveground biomass using multispectral remote sensing and nonparametric modeling,", "author": ["I. G\u00fcneralp", "Filippi", "A. M", "J. Randall"], "venue": "Int. J. Appl. Earth Obs. Geoinf. 33(1),", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2014}, {"title": "Hyperspectral and multispectral imaging for evaluating food safety and quality,", "author": ["J. Qin", "K. Chao", "M.S. Kim", "Lu", "T.F. Burks"], "venue": "J. Food Eng", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2013}, {"title": "Using Multispectral Imaging for Spoilage Detection of Pork Meat,", "author": ["B.S. Dissing", "O.S. Papadopoulou", "C. Tassou", "B.K. Ersboll", "J.M. Carstensen", "Panagou", "E. Z", "G.J. Nychas"], "venue": "Food Bioprocess Technol", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2013}, {"title": "Multi- and hyperspectral geologic remote sensing: A review,", "author": ["F.D. van der Meer", "H.M.A. van der Werff", "F.J.A. van Ruitenbeek", "C.A. Hecker", "W.H. Bakker", "M.F. Noomen", "M. van der Meijde", "E.J.M. Carranza", "de Smeth", "J. B"], "venue": "Int. J. Appl. Earth Obs. Geoinf", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2012}, {"title": "Efficient segmentation of hyperspectral images on commodity GPUs,", "author": ["P. Quesada-Barriuso", "Arg\u00fcello", "D.B. Heras"], "venue": "Front. Artif. Intell. Appl. 243,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2012}, {"title": "Accelerating Real-Time Embedded Scene Labeling with Convolutional Networks,", "author": ["L. Cavigelli", "Magno", "L. Benini"], "venue": "Proc. ACM/IEEE Des. Autom. Conf", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2015}, {"title": "YodaNN: An Ultra-Low Power Convolutional Neural Network Accelerator Based on Binary Weights,", "author": ["R. Andri", "L. Cavigelli", "Rossi", "L. Benini"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2016}, {"title": "XNOR-Net: ImageNet Classification", "author": ["M. Rastegari", "V. Ordonez", "Redmon", "A. Farhadi"], "venue": "Using Binary Convolutional Neural Networks,\u201d", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2016}, {"title": "ENet: A Deep Neural Network Architecture for Real-Time Semantic Segmentation,", "author": ["A. Paszke", "A. Chaurasia", "Kim", "E. Culurciello"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2016}, {"title": "Accelerating Deep Convolutional Neural Networks Using Specialized Hardware", "author": ["K. Ovtcharov", "O. Ruwase", "J. Kim", "J. Fowers", "Strauss", "E.S. Chung"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2015}, {"title": "DianNao: A Small-Footprint High- Throughput Accelerator for Ubiquitous Machine-Learning,", "author": ["T. Chen", "Z. Du", "N. Sun", "J. Wang", "C. Wu", "Chen", "O. Temam"], "venue": "Proc. ACM Int. Conf. Archit. Support Program. Lang. Oper. Syst.,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2014}, {"title": "NeuFlow: A Runtime Reconfigurable Dataflow Processor for Vision,", "author": ["C. Farabet", "B. Martini", "B. Corda", "P. Akselrod", "Culurciello", "Y. LeCun"], "venue": "Proc. IEEE Conf. Comput. Vis. Pattern Recognit. Work.,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2011}, {"title": "A Ultra-Low-Energy Convolution Engine for Fast Brain-Inspired Vision in Multicore Clusters,", "author": ["Conti", "L. Benini"], "venue": "Proc. IEEE Des. Autom. Test Eur. Conf", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2015}, {"title": "Scene Parsing with Multiscale Feature Learning, Purity Trees, and Optimal Covers,", "author": ["C. Farabet", "C. Couprie", "Najman", "Y. LeCun"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2012}, {"title": "Fully Convolutional Networks for Semantic Segmentation,", "author": ["J. Long", "Shelhamer", "T. Darrell"], "venue": "Proc. IEEE Conf. Comput. Vis. Pattern Recognit", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2015}, {"title": "Deep Residual Learning for Image Recognition,", "author": ["K. He", "X. Zhang", "Ren", "J. Sun"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2015}, {"title": "Scyllarus: From Research to Commercial Software,", "author": ["Habili", "J. Oorloff"], "venue": "Proc. 24th Australas. Softw. Eng. Conf", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2015}, {"title": "Multispectral pedestrian detection: Benchmark dataset and baseline,", "author": ["S. Hwang", "J. Park", "N. Kim", "Choi", "I.S. Kweon"], "venue": "Proc. IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit.,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2015}, {"title": "Improving Spatial Feature Representation from Aerial Scenes by Using Convolutional Networks,", "author": ["K. Nogueira", "Miranda", "W. O", "Santos", "J.A. Dos"], "venue": "Brazilian Symp. Comput. Graph. Image Process. 2015-Octob,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2015}, {"title": "Towards Better Exploiting Convolutional Neural Networks for Remote Sensing Scene Classification,", "author": ["K. Nogueira", "Penatti", "O.A. B", "Santos", "J.A. dos"], "venue": null, "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2016}, {"title": "Bag-of-visual-words and spatial extensions for land-use classification,", "author": ["Yang", "S. Newsam"], "venue": "Proc. 18th SIGSPATIAL Int. Conf. Adv. Geogr. Inf. Syst. - GIS \u201910,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2010}, {"title": "Do Deep Features Generalize from Everyday Objects to Remote Sensing and Aerial Scenes Domains ?,", "author": ["A.B. Penatti", "Nogueira", "J.A. Santos"], "venue": null, "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2015}, {"title": "Learning Hierarchical Features for Scene Labeling,", "author": ["C. Farabet", "C. Couprie", "Najman", "Y. LeCun"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2013}, {"title": "Towards Real-Time Image Understanding with Convolutional Networks,", "author": ["C. Farabet"], "venue": "Paris-Est", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2014}, {"title": "Scene Labeling with Contextual Hierarchical Models,", "author": ["Seyedhosseini", "T. Tasdizen"], "venue": null, "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2014}, {"title": "Efficiently selecting regions for scene understanding,", "author": ["Kumar", "D. Koller"], "venue": "Proc. IEEE Conf. Comput. Vis. Pattern Recognit.,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2010}, {"title": "Occlusion Detection and Motion Estimation with Convex Optimization,", "author": ["A. Ayvaci", "Raptis", "S. Soatto"], "venue": "Adv. Neural Inf. Process. Syst", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2010}, {"title": "Superparsing: scalable nonparametric image parsing with superpixels,", "author": ["Tighe", "S. Lazebnik"], "venue": "Proc. Eur. Conf. Comput. Vis", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2010}, {"title": "Origami: A Convolutional Network Accelerator,", "author": ["L. Cavigelli", "D. Gschwend", "C. Mayer", "S. Willi", "Muheim", "L. Benini"], "venue": "Proc. ACM Gt. Lakes Symp. VLSI,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2015}, {"title": "Origami: A 803 GOp/s/W Convolutional Network Accelerator,", "author": ["Cavigelli", "L. Benini"], "venue": "IEEE Trans. Circuits Syst. Video Technol", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2016}, {"title": "CCTV Surveillance: Video Practices and Technology, Butterworth-Heinemann", "author": ["H. Kruegle"], "venue": null, "citeRegEx": "33", "shortCiteRegEx": "33", "year": 1995}, {"title": "SLIC Superpixels Compared to State-of-the-Art Superpixel Methods,", "author": ["R. Achanta", "Shaji", "K. Smith"], "venue": "Pattern Anal. ... 34(11),", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2012}, {"title": "Rectified Linear Units Improve Restricted Boltzmann Machines,", "author": ["Nair", "G.E. Hinton"], "venue": "Proc. 27th Int. Conf. Mach. Learn.(3),", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2010}, {"title": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift,", "author": ["Ioffe", "C. Szegedy"], "venue": "Proc. Int. Conf. Mach. Learn.,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2015}, {"title": "Adam: A Method for Stochastic Optimization,", "author": ["Kingma", "J. Ba"], "venue": "Proc. Int. Conf. Learn. Represent", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2015}, {"title": "Are loss functions all the same?,", "author": ["L. Rosasco", "E. De Vito", "A. Caponnetto", "Piana", "A. Verri"], "venue": "Neural Comput", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2004}], "referenceMentions": [], "year": 2016, "abstractText": "Detecting and classifying targets in video streams from surveillance cameras is a cumbersome, error-prone and expensive task. Often, the incurred costs are prohibitive for real-time monitoring. This leads to data being stored locally or transmitted to a central storage site for post-incident examination. The required communication links and archiving of the video data are still expensive and this setup excludes preemptive actions to respond to imminent threats. An effective way to overcome these limitations is to build a smart camera that analyzes the data on-site, close to the sensor, and transmits alerts when relevant video sequences are detected. Deep neural networks (DNNs) have come to outperform humans in visual classifications tasks and are also performing exceptionally well on other computer vision tasks. The concept of DNNs and Convolutional Networks (ConvNets) can easily be extended to make use of higher-dimensional input data such as multispectral data. We explore this opportunity in terms of achievable accuracy and required computational effort. To analyze the precision of DNNs for scene labeling in an urban surveillance scenario we have created a dataset with 8 classes obtained in a field experiment. We combine an RGB camera with a 25-channel VIS-NIR snapshot sensor to assess the potential of multispectral image data for target classification. We evaluate several new DNNs, showing that the spectral information fused together with the RGB frames can be used to improve the accuracy of the system or to achieve similar accuracy with a 3x smaller computation effort. We achieve a very high per-pixel accuracy of 99.1%. Even for scarcely occurring, but particularly interesting classes, such as cars, 75% of the pixels are labeled correctly with errors occurring only around the border of the objects. This high accuracy was obtained with a training set of only 30 labeled images, paving the way for fast adaptation to various application scenarios.", "creator": "Microsoft\u00ae Word 2016"}}}