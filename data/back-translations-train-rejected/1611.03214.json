{"id": "1611.03214", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Nov-2016", "title": "Ultimate tensorization: compressing convolutional and FC layers alike", "abstract": "Convolutional neural networks excel in image recognition tasks, but this comes at the cost of high computational and memory complexity. To tackle this problem, [1] developed a tensor factorization framework to compress fully-connected layers. In this paper, we focus on compressing convolutional layers. We show that while the direct application of the tensor framework [1] to the 4-dimensional kernel of convolution does compress the layer, we can do better. We reshape the convolutional kernel into a tensor of higher order and factorize it. We combine the proposed approach with the previous work to compress both convolutional and fully-connected layers of a network and achieve 80x network compression rate with 1.1% accuracy drop on the CIFAR-10 dataset.", "histories": [["v1", "Thu, 10 Nov 2016 08:07:46 GMT  (22kb,D)", "http://arxiv.org/abs/1611.03214v1", "NIPS 2016 workshop: Learning with Tensors: Why Now and How?"]], "COMMENTS": "NIPS 2016 workshop: Learning with Tensors: Why Now and How?", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["timur garipov", "dmitry podoprikhin", "alexander novikov", "dmitry vetrov"], "accepted": false, "id": "1611.03214"}, "pdf": {"name": "1611.03214.pdf", "metadata": {"source": "CRF", "title": "Ultimate tensorization: compressing convolutional and FC layers alike", "authors": ["Timur Garipov", "Dmitry Podoprikhin", "Alexander Novikov", "Dmitry Vetrov"], "emails": ["timgaripov@gmail.com", "podoprikhin.dmitry@gmail.com", "novikov@bayesgroup.ru", "vetrovd@yandex.ru"], "sections": [{"heading": "1 Introduction", "text": "Convolutional Neural Networks (CNNs) demonstrate state-of-the-art performance to many problems in computer vision, natural language processing and other areas [2, 3]. At the same time, CNNs require millions of floating-point operations to process an image, and real-time applications therefore require powerful CPU or GPU devices. In addition, these networks contain millions of transferable parameters and consume hundreds of megabytes of memory and storage bandwidth [4]. As a result, CNNs are forced to use RAM instead of relying solely on the processor cache - orders of magnitude of energy-efficient memory [5] - further increasing energy consumption. These reasons inhibit the spread of CNNs on mobile devices. To address the storage and storage requirements of neural networks, we used tensor decompression techniques to compress fully connected layers."}, {"heading": "2 Convolutional Layer", "text": "The main component of such networks is a convolutionary layer that transforms three-dimensional input xxxx x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x - x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x"}, {"heading": "3 Tensor Train Decomposition", "text": "The TT decomposition (or TT representation) of a tensor A-Rn1 \u00b7... \u00b7 nd is the set of matrices Gk [jk]. \u00b7 \u00b7 rk, where jk = 1,.. \u00b7, nk, k = 1,.., d, and r0 = rd = 1, so that each of the tensor elements is called A (j1, j2,.., jd) = G1 [j2].... Gd [jd]. (2) The elements of the collection {rk} dk = 0 are called TT ranks. The collections of matrices {{Gk] nkjk = 1} d = 1 k = 1 are called TT cores [6]. The TT format requires that dk = 1 nk."}, {"heading": "4 TT-convolutional Layer", "text": "In this section, we propose two ways to represent a Convolutionary Kernel K in TT format. One possibility is to apply the TT decomposition directly to the tensor K. To see the drawbacks of this approach, consider a confusion that is a small, fully connected layer that is applied to the channels of the input image in each pixel location. However, for fully connected layers, the Matrix-TT format has proven to be more efficient than the Matrix-Low-Rank format [1], so we are looking for a decomposition that would match the matrix format on 1 x TT format. However, for fully connected layers, the Matrix-TT format has proven to be more efficient than the Matrix-Low-Rank format [1]."}, {"heading": "5 Related Work", "text": "Fully connected layers of neural networks have traditionally been considered a storage bottleneck, and much work focuses on the compression of these layers [10, 11, 1, 12]. However, several state-of-the-art neural networks are either blocked by revolutionary layers [13, 14], or their fully connected layers can be compressed to shift the bottleneck to the revolutionary layers [1], leading to a series of works focusing on the compression and acceleration of the revolutionary layers [5, 15, 16, 17, 19]. An approach to compressing a convolutionary layer is based either on cutting off less important weights from the revolutionary core or on limiting possible variations in weights (quantification) or on both [5, 15, 17]. Our approach is compatible with the quantization technique: one can quantify the elements of TT nuclei of decomposition. Some works also add Huffman-coding to the other (5) compression [5]."}, {"heading": "6 Experiments", "text": "In fact, it is a purely reactionary project, which is a purely reactionary project, which is a reactionary project, which is a reactionary project, which is a reactionary project."}, {"heading": "7 Conclusion", "text": "In this paper, we proposed a tensor decomposition approach for compressing the convolutionary layers of a neural network. By combing this revolutionary approach with work [1] on fully connected layers, we compressed a revolutionary network 80 times. These results are a step toward the age of embedding compressed models in smartphones so that they can constantly look and listen to their surroundings. In future work, we will experiment with the proposed approach of the ILSVRC 2012 dataset [22] on state-of-the-art neural architectures."}], "references": [{"title": "Tensorizing neural networks", "author": ["A. Novikov", "D. Podoprikhin", "A. Osokin", "D. Vetrov"], "venue": "Advances in Neural Information Processing Systems 28 (NIPS), pages 442\u2013450.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2015}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "Advances in neural information processing systems, pages 1097\u20131105,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2012}, {"title": "A convolutional neural network for modelling sentences", "author": ["N. Kalchbrenner", "E. Grefenstette", "P. Blunsom"], "venue": "arXiv preprint arXiv:1404.2188,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2014}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "International Conference on Learning Representations (ICLR),", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep compression: Compressing deep neural network with pruning, trained quantization and huffman coding", "author": ["S. Han", "H. Mao", "W.J. Dally"], "venue": "CoRR, abs/1510.00149, 2,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2015}, {"title": "Tensor-Train decomposition", "author": ["I.V. Oseledets"], "venue": "SIAM J. Scientific Computing, 33(5):2295\u20132317,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2011}, {"title": "Caffe: Convolutional architecture for fast feature embedding", "author": ["Y. Jia", "E. Shelhamer", "J. Donahue", "S. Karayev"], "venue": "In Proceedings of the 22nd ACM international conference on Multimedia,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2014}, {"title": "Matconvnet: Convolutional neural networks for matlab", "author": ["A. Vedaldi", "K. Lenc"], "venue": "Proceedings of the 23rd ACM international conference on Multimedia,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "TensorFlow: Large-scale machine learning", "author": ["M. Abadi", "A. Agarwal", "P. Barham", "E. Brevdo"], "venue": "on heterogeneous systems,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2015}, {"title": "Compressing neural networks with the hashing trick", "author": ["W. Chen", "J.T. Wilson", "S. Tyree", "K.Q. Weinberger", "Y. Chen"], "venue": "International Conference on Machine Learning (ICML), pages 2285\u20132294,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "Restructuring of deep neural network acoustic models with singular value decomposition", "author": ["J. Xue", "J. Li", "Y. Gong"], "venue": "Interspeech, pages 2365\u20132369,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2013}, {"title": "Low-rank matrix factorization for deep neural network training with high-dimensional output targets", "author": ["T.N. Sainath", "B. Kingsbury", "V. Sindhwani", "E. Arisoy", "B. Ramabhadran"], "venue": "International Conference of Acoustics, Speech, and Signal Processing (ICASSP), pages 6655\u20136659,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2013}, {"title": "Going deeper with convolutions", "author": ["C. Szegedy", "W. Liu", "Y. Jia", "P. Sermanet"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2015}, {"title": "Deep residual learning for image recognition", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "arXiv preprint arXiv:1512.03385,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning both weights and connections for efficient neural network", "author": ["S. Han", "J. Pool", "J. Tran", "W. Dally"], "venue": "Advances in Neural Information Processing Systems, pages 1135\u20131143,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2015}, {"title": "PerforatedCNNs: Acceleration through elimination of redundant convolutions", "author": ["M. Figurnov", "A. Ibraimova", "D. Vetrov", "P. Kohli"], "venue": "Advances in Neural Information Processing Systems 29 (NIPS).", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2016}, {"title": "Speeding-up convolutional neural networks using fine-tuned CP-decomposition", "author": ["V. Lebedev", "Y. Ganin", "M. Rakhuba", "I. Oseledets", "V. Lempitsky"], "venue": "Proceedings of the International Conference on Learning Representations (ICLR),", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2015}, {"title": "Factorized convolutional neural networks", "author": ["M. Wang", "B. Liu", "H. Foroosh"], "venue": "arXiv preprint arXiv:1608.04337,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2016}, {"title": "Learning multiple layers of features from tiny images", "author": ["Alex Krizhevsky", "Geoffrey Hinton"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2009}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["S. Ioffe", "C. Szegedy"], "venue": "arXiv preprint arXiv:1502.03167,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2015}, {"title": "Imagenet large scale visual recognition challenge", "author": ["O. Russakovsky", "J. Deng", "H. Su", "J. Krause"], "venue": "International Journal of Computer Vision (IJCV),", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "To tackle this problem, [1] developed a tensor factorization framework to compress fully-connected layers.", "startOffset": 24, "endOffset": 27}, {"referenceID": 0, "context": "We show that while the direct application of the tensor framework [1] to the 4-dimensional kernel of convolution does compress the layer, we can do better.", "startOffset": 66, "endOffset": 69}, {"referenceID": 1, "context": "Convolutional Neural Networks (CNNs) show state-of-the-art performance on many problems in computer vision, natural language processing and other fields [2, 3].", "startOffset": 153, "endOffset": 159}, {"referenceID": 2, "context": "Convolutional Neural Networks (CNNs) show state-of-the-art performance on many problems in computer vision, natural language processing and other fields [2, 3].", "startOffset": 153, "endOffset": 159}, {"referenceID": 3, "context": "Moreover, these networks contain millions of trainable parameters and consume hundreds of megabytes of storage and memory bandwidth [4].", "startOffset": 132, "endOffset": 135}, {"referenceID": 4, "context": "Thus, CNNs are forced to use RAM instead of solely relying on the processor cache \u2013 orders of magnitude more energy efficient memory device [5] \u2013 which increases the energy consumption even more.", "startOffset": 140, "endOffset": 143}, {"referenceID": 0, "context": "To address the storage and memory requirements of neural networks, [1] used tensor decomposition techniques to compress fully-connected layers.", "startOffset": 67, "endOffset": 70}, {"referenceID": 5, "context": "They represented the parameters of the layers in the Tensor Train format [6] and learned the network from scratch in this representation.", "startOffset": 73, "endOffset": 76}, {"referenceID": 4, "context": "This approach provided enough compression to move the storage bottleneck of VGG-16 [5] from the fully-connected layers to convolutional layers.", "startOffset": 83, "endOffset": 86}, {"referenceID": 0, "context": "\u2022 We experimentally show that applying the Tensor Train decomposition \u2013 the compression technique used in [1] \u2013 directly to the tensor of a convolution yields poor results (see Sec.", "startOffset": 106, "endOffset": 109}, {"referenceID": 0, "context": "\u2022 We combine the proposed approach with the fully-connected layers compression of [1].", "startOffset": 82, "endOffset": 85}, {"referenceID": 6, "context": "To improve the computational performance, many deep learning frameworks reduce the convolution (1) to a matrix-by-matrix multiplication [7, 8] (see Fig.", "startOffset": 136, "endOffset": 142}, {"referenceID": 7, "context": "To improve the computational performance, many deep learning frameworks reduce the convolution (1) to a matrix-by-matrix multiplication [7, 8] (see Fig.", "startOffset": 136, "endOffset": 142}, {"referenceID": 5, "context": "The collections of matrices {{Gk[jk]} jk=1} d k=1 are called TT-cores [6].", "startOffset": 70, "endOffset": 73}, {"referenceID": 5, "context": "without materializing the tensor itself [6].", "startOffset": 40, "endOffset": 43}, {"referenceID": 0, "context": "But for fully-connected layers, the matrix TT-format proved to be more efficient than the matrix low-rank format [1].", "startOffset": 113, "endOffset": 116}, {"referenceID": 0, "context": "The convolutional kernel collapses into a matrix {K(1, 1, c\u2032, s\u2032)} c\u2032=1,s\u2032=1 and the decomposition (4) for this matrix coincides with the Tensor Train format for the fully-connected layer proposed in [1].", "startOffset": 200, "endOffset": 203}, {"referenceID": 8, "context": "To compute the necessary gradients we use automatic differentiation implemented in TensorFlow [9].", "startOffset": 94, "endOffset": 97}, {"referenceID": 9, "context": "Fully-connected layers of neural networks are traditionally considered as the memory bottleneck and numerous works focused on compressing these layers [10, 11, 1, 12].", "startOffset": 151, "endOffset": 166}, {"referenceID": 10, "context": "Fully-connected layers of neural networks are traditionally considered as the memory bottleneck and numerous works focused on compressing these layers [10, 11, 1, 12].", "startOffset": 151, "endOffset": 166}, {"referenceID": 0, "context": "Fully-connected layers of neural networks are traditionally considered as the memory bottleneck and numerous works focused on compressing these layers [10, 11, 1, 12].", "startOffset": 151, "endOffset": 166}, {"referenceID": 11, "context": "Fully-connected layers of neural networks are traditionally considered as the memory bottleneck and numerous works focused on compressing these layers [10, 11, 1, 12].", "startOffset": 151, "endOffset": 166}, {"referenceID": 12, "context": "However, several state-of-theart neural networks are either bottlenecked by convolutional layers [13, 14], or their fully-connected layers can be compressed to move the bottleneck to the convolutional layers [1].", "startOffset": 97, "endOffset": 105}, {"referenceID": 13, "context": "However, several state-of-theart neural networks are either bottlenecked by convolutional layers [13, 14], or their fully-connected layers can be compressed to move the bottleneck to the convolutional layers [1].", "startOffset": 97, "endOffset": 105}, {"referenceID": 0, "context": "However, several state-of-theart neural networks are either bottlenecked by convolutional layers [13, 14], or their fully-connected layers can be compressed to move the bottleneck to the convolutional layers [1].", "startOffset": 208, "endOffset": 211}, {"referenceID": 4, "context": "This leads to a number of works focusing on compressing and speeding up the convolutional layers [5, 15, 16, 17, 18, 19].", "startOffset": 97, "endOffset": 120}, {"referenceID": 14, "context": "This leads to a number of works focusing on compressing and speeding up the convolutional layers [5, 15, 16, 17, 18, 19].", "startOffset": 97, "endOffset": 120}, {"referenceID": 15, "context": "This leads to a number of works focusing on compressing and speeding up the convolutional layers [5, 15, 16, 17, 18, 19].", "startOffset": 97, "endOffset": 120}, {"referenceID": 16, "context": "This leads to a number of works focusing on compressing and speeding up the convolutional layers [5, 15, 16, 17, 18, 19].", "startOffset": 97, "endOffset": 120}, {"referenceID": 17, "context": "This leads to a number of works focusing on compressing and speeding up the convolutional layers [5, 15, 16, 17, 18, 19].", "startOffset": 97, "endOffset": 120}, {"referenceID": 4, "context": "One approach to compressing a convolutional layer is based on either pruning less important weights from the convolutional kernel, or restricting possible variation of the weights (quantization), or both [5, 15, 17].", "startOffset": 204, "endOffset": 215}, {"referenceID": 14, "context": "One approach to compressing a convolutional layer is based on either pruning less important weights from the convolutional kernel, or restricting possible variation of the weights (quantization), or both [5, 15, 17].", "startOffset": 204, "endOffset": 215}, {"referenceID": 15, "context": "One approach to compressing a convolutional layer is based on either pruning less important weights from the convolutional kernel, or restricting possible variation of the weights (quantization), or both [5, 15, 17].", "startOffset": 204, "endOffset": 215}, {"referenceID": 4, "context": "Some works also add Huffman coding on top of other compression techniques [5], which is also compatible with the proposed method.", "startOffset": 74, "endOffset": 77}, {"referenceID": 16, "context": "CP-decomposition [18] and Kronecker product factorization [19] allow to speed up the inference time of convolutions and compress the network as a side effect.", "startOffset": 17, "endOffset": 21}, {"referenceID": 17, "context": "CP-decomposition [18] and Kronecker product factorization [19] allow to speed up the inference time of convolutions and compress the network as a side effect.", "startOffset": 58, "endOffset": 62}, {"referenceID": 18, "context": "We evaluated the compressing strength of the proposed approach on CIFAR-10 dataset [20], which has 50 000 train images and 10 000 test images.", "startOffset": 83, "endOffset": 87}, {"referenceID": 19, "context": "The first network has the following architecture: conv (64 output channels); BN; ReLU; conv (64 output channels); BN; ReLU; max-pool (3\u00d7 3 with stride 2); conv (128 output channels); BN; ReLU; conv (128 output channels); BN; ReLU; max-pool (3\u00d7 3 with stride 2); conv (128 output channels); BN; ReLU; conv (128 output channels); avg-pool (4\u00d7 4); fc (128\u00d7 10), where \u2019BN\u2019 stands for batch normalization [21] and all convolutional filters are of size 3 \u00d7 3.", "startOffset": 401, "endOffset": 405}, {"referenceID": 0, "context": "To compress the second network, we replace all layers excluding the first and the last one (they occupy less than 1% of parameters) with TT-conv and TT-fc [1] layers.", "startOffset": 155, "endOffset": 158}, {"referenceID": 0, "context": "To compare against [1] we include the results of compressing only fully-connected layers (Tbl.", "startOffset": 19, "endOffset": 22}, {"referenceID": 0, "context": "By combing this convolutional approach with the work [1] for fully-connected layers, we compressed a convolutional network 80\u00d7 times.", "startOffset": 53, "endOffset": 56}, {"referenceID": 20, "context": "In the future work, we will experiment with the proposed approach on the ILSVRC-2012 dataset [22] on state-of-the-art neural architectures.", "startOffset": 93, "endOffset": 97}], "year": 2016, "abstractText": "Convolutional neural networks excel in image recognition tasks, but this comes at the cost of high computational and memory complexity. To tackle this problem, [1] developed a tensor factorization framework to compress fully-connected layers. In this paper, we focus on compressing convolutional layers. We show that while the direct application of the tensor framework [1] to the 4-dimensional kernel of convolution does compress the layer, we can do better. We reshape the convolutional kernel into a tensor of higher order and factorize it. We combine the proposed approach with the previous work to compress both convolutional and fully-connected layers of a network and achieve 80\u00d7 network compression rate with 1.1% accuracy drop on the CIFAR-10 dataset.", "creator": "LaTeX with hyperref package"}}}