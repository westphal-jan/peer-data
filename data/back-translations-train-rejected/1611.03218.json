{"id": "1611.03218", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Nov-2016", "title": "Learning to Play Guess Who? and Inventing a Grounded Language as a Consequence", "abstract": "Learning your first language is an incredible feat and not easily duplicated. Doing this using nothing but a few pictureless books, a corpus, would likely be impossible even for humans. As an alternative we propose to use situated interactions between agents as a driving force for communication, and the framework of Deep Recurrent Q-Networks (DRQN) for learning a common language grounded in the provided environment. We task the agents with interactive image search in the form of the game Guess Who?. The images from the game provide a non trivial environment for the agents to discuss and a natural grounding for the concepts they decide to encode in their communication. Our experiments show that it is possible to learn this task using DRQN and even more importantly that the words the agents use correspond to physical attributes present in the images that make up the agents environment.", "histories": [["v1", "Thu, 10 Nov 2016 08:44:52 GMT  (2843kb,D)", "http://arxiv.org/abs/1611.03218v1", "8 pages with 1 page appendix. Accepted to Deep Reinforcement Learning Workshop at NIPS 2016"], ["v2", "Sun, 27 Nov 2016 14:50:50 GMT  (2819kb,D)", "http://arxiv.org/abs/1611.03218v2", "8 pages with 1 page appendix. Accepted to Deep Reinforcement Learning Workshop at NIPS 2016"], ["v3", "Tue, 3 Jan 2017 18:28:43 GMT  (2843kb,D)", "http://arxiv.org/abs/1611.03218v3", "8 pages with 1 page appendix. Accepted to Deep Reinforcement Learning Workshop at NIPS 2016"], ["v4", "Wed, 15 Mar 2017 11:24:49 GMT  (4109kb,D)", "http://arxiv.org/abs/1611.03218v4", "Previous version was accepted to Deep Reinforcement Learning Workshop at NIPS 2016"]], "COMMENTS": "8 pages with 1 page appendix. Accepted to Deep Reinforcement Learning Workshop at NIPS 2016", "reviews": [], "SUBJECTS": "cs.AI cs.CL cs.LG cs.MA", "authors": ["emilio jorge", "mikael k{\\aa}geb\\\"ack", "fredrik d johansson", "emil gustavsson"], "accepted": false, "id": "1611.03218"}, "pdf": {"name": "1611.03218.pdf", "metadata": {"source": "CRF", "title": "Learning to Play Guess Who? and Inventing a Grounded Language as a Consequence", "authors": ["Emilio Jorge", "Mikael K\u00e5geb\u00e4ck", "Emil Gustavsson"], "emails": ["firstname.lastname@fcc.chalmers.se", "kageback@chalmers.se"], "sections": [{"heading": null, "text": "Learning the first language is an incredible feat that is not so easy to copy, and doing it with nothing more than a few pictureless books, a corpus, would probably be impossible even for humans. Alternatively, we propose to use situated interactions between agents as a driving force for communication, and the framework of Deep Recurrent Q-Networks (DRQN) to learn a common language based on the provided environment. We entrust agents with interactive image search in the form of the game Guess Who? The images from the game provide agents with a non-trivial environment for discussion and a natural basis for the concepts they encode in their communication. Our experiments show that it is possible to learn this task with DRQN, and more importantly that the words used by the agents correspond with physical attributes contained in the images that make up the agents \"environment."}, {"heading": "1 Introduction", "text": "In this context, it should be noted that the case concerns a case in which a person has been injured."}, {"heading": "2 Background", "text": "The results in this paper are mainly based on the theory of reinforcement learning, deep Q networks and the concept of differentiated interactive learning. In this section, brief descriptions of these methods are presented."}, {"heading": "2.1 Reinforcement learning", "text": "In a traditional framework where a single agent learns amplification (RL), an agent observes the current state st-S at each step t, takes action ut-U according to any policy \u03c0, receives the reward rt and enters a new state st + 1 according to a certain probability distribution (depending on the current state and action). The objective of the agent is to maximize the expected discounted future reward sum Rt = p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p."}, {"heading": "2.2 Deep Q-Networks", "text": "The space of the state action pairs is so large in many applications that the storage and updating of the Q values for each state action pair is mathematically insoluble. A solution to this dimensionality problem is to apply the concept of the Deep Q networks (DQN) (see [4] for a more detailed description).The idea of DQN is to represent the Q function by using a neural network that approximates the value Q (s, u; \u03b8) for all state action pairs by adding the Q (s, u;).The network is optimized by minimizing the loss function Li (\u03b8i) = E [(yDQNi \u2212 Q (s, u; \u03b8i) 2], with the iteration i, with yDQNi = r + \u03b3maxu \u2032 Q (s \u2032, u \u2212 randomating), in which an approximate rent (s \u2212 2) is specified."}, {"heading": "2.3 Differentiable inter-agent learning", "text": "In previous work where communication is part of the RL setting, messages are seen as actions and are selected as -greedy via Q functions and action policies. In [2] Foerster et al. introduce the idea of centralized training but decentralized execution, i.e. agents are trained together but evaluated separately; they introduce the concept of differentiated inter-agent learning (DIAL), where messages may be continuous during training but must be discreet during evaluation, allowing gradients to spread between agents through the messages in training, giving agents more feedback and thus reducing the amount of learning required by trial and error. In order to reduce the discrediting error that may occur by discrediting messages during the evaluation phase, messages are processed by a digitalization / regulation unit (DRU)."}, {"heading": "3 Guess Who?", "text": "The answer to this question is: \"I don't think the world is in order.\" The answer to this question is: \"I don't think the world is in order.\" The answer to this question is: \"I don't think the world is in order.\" The answer to this question is: \"I don't think the world is in order.\" The answer to this question is: \"I don't think the world is in order.\" The answer to this question is: \"I don't think the world is in order.\" The answer to this question is: \"I don't think the world is in order.\" The answer to this question is the answer to the question to the question to the question to the question to the question to the question to the question to the answer to the question to the answer to the question to the answer to the question to the answer to the question to the answer to the question to the answer to the question to the question to the answer to the question to the question to the answer to the question to the answer to the question to the answer to the question to the answer to the question to the answer to the question to the question to the answer to the question to the question to the answer to the question to the question to the answer to the question to the answer to the question to the answer to the question to the question to the question to the answer to the question to the question to the question to the answer to the question to the answer to the question to the question to the answer to the question to the question to the answer to the question to the question to the question to the question to the answer to the question to the answer to the question to the question to the question to the question to the answer the question to the question to the question to the answer to the question to the question to the answer to the question to the question to the answer to the question to the question to the question to the question to the answer the question to the question to the question to the question to the answer the question to the question to the question to the question to the question to the answer the question to the question to the answer the question to the question to the question to the question to the question to the question to the answer to the question to the answer to the question to the question to the answer the question to the question to the question to the question to the question to the question to the question to the answer the question to the question to the answer the"}, {"heading": "4 Model", "text": "The architecture of the model is presented in this section and a schematic mapping of the model is then shown in Figure 2. Each agent (a = 1, 2) consists of a recursive neural network (RNN) which is unrolled for T-time steps with an internal state h, an input network and an output network. The input network takes the available information (oat, m \u00b2 a \u2032 t \u2212 1, u a t \u2212 1) and transforms it into a 256 embedding zat. The observation of an agent in a time step t is passed on by a two-layer multilayer perceptron (MLP) of the size used [# colors \u00b7 Figure |, 128, 256]. The regulated message m \u00b2 t from the other agent, a \u2032 m \u00b2, is passed on by a size MLP [M |, 256]. The previous action uat \u2212 1 for a size agent is passed on by a review."}, {"heading": "5 Increasing noise", "text": "Inspired by the syllabus described in [8], in which Bengio et al. show that gradually increasing difficulty of the tasks leads to improved learning, we introduce the use of increasing noise into DIAL. We found that the use of a fixed value for noise \u03c3 resulted in an undesirable trade-off. If \u03c3 is too large, there is a risk that the model will never learn anything. If it is too small, you get an excellent training error (where continuous messages are allowed), but a bad test error due to the model overcoding the information in the messages, leading to a large discrediting error. Our solution is to make the noise increase linearly over time. This allows the model to learn quickly at the beginning, but punish the overcoding of information with increasing progression of the training, which can be compared to human language, where a person who begins to learn a new language needs a near-silent conversation to understand a message, so that a speaker would not have a problem listening to a message in a more competent environment, such as a message during a message."}, {"heading": "6 Experiments", "text": "To validate our ideas, we perform a series of experiments on our version of Guess Who? to see how well it works, but also what the model learns. \u00b7 Further, we evaluate the effectiveness of increasing noise by comparing it with experiments with non-increasing noise.0 10k 20k 30k 30k, 40k 50kepisodes 0,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,00000,000,000,000,000,000,000,000,000000000,000000000000000,0000000000"}, {"heading": "7 Results", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "7.1 Performance", "text": "Figure 3a illustrates the average performance (over five runs) in the case where the questioner sees two images, where it becomes clear that more messages are crucial for better performance on two images; the value for two messages is close to the theoretical limit of 0.89 for two messages (as described in Section 3), but is exceeded when using four and eight messages; the performance for four messages is shown using median values; this is because one of the four message runs learns nothing and maintains the basic performance; in addition, there are three runs (two with four messages and one with eight messages) that learn a reasonably successful protocol before returning to the baseline; these outliers result in averages that do not represent a good representation of learning ability; the performance over twelve passes can be seen in Figure 3b. It is evident that the performance achieved on four images is quite similar, regardless of how many questions are available, especially when considering that there is some instability in training."}, {"heading": "7.2 Understanding the questions", "text": "To better understand the nature of the questions posed by the questioner, we will try out a few episodes from a model with two images and two available questions to see how the games play out, i.e. what the message log between the questioners depends on the images they see. The message log is shown in Table 1. In the table, you can see that the questioners \"questions depend on what images they have, what answers they receive from the questioners, and their interpretation of the answer (their guess).You can see that in some cases it gets the same answer, even if the questioners have different images, which results in the questioners sometimes making a guess about the wrong image and getting zero remuneration.The answers to each question for the different images are illustrated in Figure 4. This shows that the two images he has difficulty separating (see those that result in zero remuneration in Table 1), the same answers to both questions have no answers (question A and there is only light at the top of the object, there is only light at the top of the object)."}, {"heading": "7.3 Inreasing noise", "text": "To assess the effectiveness of increasing noise \u03c3 during training, we compare the results on Guess Who? with four images and eight possible messages and an increasing \u03c3 with a constant \u03c3 = 0.5 (used in [2]), but also with \u03c3 values such as 0, 0.1, 1}. Figure 5 clearly shows that there is a significant advantage in using variable noise over constant noise. The model learns both faster and achieves better performance compared to models trained with constant noise."}, {"heading": "8 Related works", "text": "In [9], the predator-prey environment is investigated, in which agents learn a communication protocol consisting of bits. It is shown that longer messages (i.e. greater bandwidth) may take longer, but generally produce better results. In [2, 3], the authors solve puzzles involving multiple agents with one-bit communication."}, {"heading": "9 Conclusions", "text": "In this work, we demonstrated that a DRQN can learn to play Guess Who?. Furthermore, by carefully analyzing the results, we were able to establish a fundamental link between the words used by the agents and the properties of objects in images from the game. Finally, we demonstrated that regulating the noise level in the communication channel could have a major impact on the training speed as well as the final performance of the system."}, {"heading": "Acknowledgments", "text": "The authors thank the project Towards a knowledge-based culturomics, which is supported by a Framework Fellowship of the Swedish Research Council (2012-2016; dnr 2012-5738), and Jakob Foerster and Yannis M. Assael for the insightful discussions."}, {"heading": "A Derivations for optimal scoring with two messages", "text": "If one does not attempt to correctly distinguish between two images from a pool of 24 images based on one of two available questions, the probability of guessing the correct image can be estimated as follows: Image A is the right image and B is the wrong image from the pool. P (Correct) = = P (Correct | Divisible) \u00b7 P (Divisible) + P (Correct | Non-Divisible) \u00b7 P (Non-Divisible) (1) Images are separable if Image A and B are not in the same subset of images that are divided by the questions. Since a subset that divides the facility evenly is optimal, this means that the 24 images are so divided that there are 6 images in each subset and P (Divisible) = 1 \u2212 5 / 23 = 18 / 23. If they are inseparable, the questioner must make a random guess, which results in Equation (1) = 1 \u00b7 18 / 23 + 2 \u00b7 5 / 23 / 23: 0.89."}], "references": [{"title": "Language learning with restricted input: case studies of two hearing children of deaf parents", "author": ["J. Sachs", "B. Bard", "M.L. Johnson"], "venue": "Applied Psycholinguistics, vol. 2, no. 01, pp. 33\u201354, 1981.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1981}, {"title": "Learning to communicate with deep multi-agent reinforcement learning", "author": ["J.N. Foerster", "Y.M. Assael", "N. de Freitas", "S. Whiteson"], "venue": "CoRR, vol. abs/1605.06676, 2016.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2016}, {"title": "Learning to communicate to solve riddles with deep distributed recurrent q-networks", "author": ["\u2014"], "venue": "CoRR, vol. abs/1602.02672, 2016.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2016}, {"title": "Human-level control through deep reinforcement learning", "author": ["V. Mnih", "K. Kavukcuoglu", "D. Silver", "A.A. Rusu", "J. Veness", "M.G. Bellemare", "A. Graves", "M. Riedmiller", "A.K. Fidjeland", "G. Ostrovski", "S. Petersen", "C. Beattie", "A. Sadik", "I. Antonoglou", "H. King", "D. Kumaran", "D. Wierstra", "S. Legg", "D. Hassabis"], "venue": "Nature, vol. 518, no. 7540, pp. 529\u2013533, Feb. 2015.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep recurrent q-learning for partially observable mdps", "author": ["M.J. Hausknecht", "P. Stone"], "venue": "CoRR, vol. abs/1507.06527, 2015.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning phrase representations using RNN encoder-decoder for statistical machine translation", "author": ["K. Cho", "B. van Merrienboer", "\u00c7. G\u00fcl\u00e7ehre", "F. Bougares", "H. Schwenk", "Y. Bengio"], "venue": "CoRR, vol. abs/1406.1078, 2014.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2014}, {"title": "Batch normalization: accelerating deep network training by reducing internal covariate shift", "author": ["S. Ioffe", "C. Szegedy"], "venue": "CoRR, vol. abs/1502.03167, 2015.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2015}, {"title": "Curriculum learning", "author": ["Y. Bengio", "J. Louradour", "R. Collobert", "J. Weston"], "venue": "Proceedings of the 26th Annual International Conference on Machine Learning, ser. ICML \u201909, Montreal, Quebec, Canada: ACM, 2009, pp. 41\u201348, ISBN: 978-1-60558-516-1. DOI: 10.1145/1553374. 1553380.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2009}], "referenceMentions": [{"referenceID": 0, "context": "in [1].", "startOffset": 3, "endOffset": 6}, {"referenceID": 1, "context": "in [2, 3] where agents learn to communicate using binary messages to solve riddles.", "startOffset": 3, "endOffset": 9}, {"referenceID": 2, "context": "in [2, 3] where agents learn to communicate using binary messages to solve riddles.", "startOffset": 3, "endOffset": 9}, {"referenceID": 1, "context": "The model we propose is similar to the Differentiable inter-agent learning (DIAL) model presented in [2], see Section 2.", "startOffset": 101, "endOffset": 104}, {"referenceID": 0, "context": "The objective of the agent is to maximize the expected discounted future sum of rewards Rt = \u2211\u221e \u03c4=t \u03b3 r\u03c4 , where \u03b3 \u2208 [0, 1] is a discount factor that trades-off the importance of immediate and future rewards.", "startOffset": 117, "endOffset": 123}, {"referenceID": 3, "context": "One solution for this dimensionality problem is to employ the concept of Deep-Q-Networks (DQN) (for a more thorough description, see [4]).", "startOffset": 133, "endOffset": 136}, {"referenceID": 4, "context": "When agents only have partial observability, Hausknecht and Stone ([5]) propose to use an approach called Deep Recurrent Q-Networks (DRQN) where, instead of approximating the Q-values with a feed-forward network, they approximate the Q-values with a recurrent neural network that can maintain an internal state which aggregates the observations over time.", "startOffset": 67, "endOffset": 70}, {"referenceID": 1, "context": "In [2], Foerster et al.", "startOffset": 3, "endOffset": 6}, {"referenceID": 5, "context": "The embedding z a t is then processed by a 2-layer RNN with gated reccurent units (GRU) [6] of size [256,256] to give the output embedding h2,t.", "startOffset": 88, "endOffset": 91}, {"referenceID": 6, "context": "Batch normalization (see, [7]) is performed in the MLP for the image embedding and on the incoming messages m \u2032 t\u22121.", "startOffset": 26, "endOffset": 29}, {"referenceID": 7, "context": "Inspired by curriculum learning described in [8], where Bengio et al.", "startOffset": 45, "endOffset": 48}, {"referenceID": 1, "context": "1 to 1; this keeps the average \u03c3 close to what was found to work on a task with similar tasks in [2], where they used \u03c3 = 0.", "startOffset": 97, "endOffset": 100}, {"referenceID": 1, "context": "5 (which was used in [2]) but also with \u03c3 \u2208 {0, 0.", "startOffset": 21, "endOffset": 24}, {"referenceID": 1, "context": "Previous work has been on communication in a multi-agent reinforcement learning [2, 3, 9].", "startOffset": 80, "endOffset": 89}, {"referenceID": 2, "context": "Previous work has been on communication in a multi-agent reinforcement learning [2, 3, 9].", "startOffset": 80, "endOffset": 89}, {"referenceID": 1, "context": "In [2, 3] the authors solve puzzles with multiple agents with one-bit communication.", "startOffset": 3, "endOffset": 9}, {"referenceID": 2, "context": "In [2, 3] the authors solve puzzles with multiple agents with one-bit communication.", "startOffset": 3, "endOffset": 9}], "year": 2016, "abstractText": "Learning your first language is an incredible feat and not easily duplicated. Doing this using nothing but a few pictureless books, a corpus, would likely be impossible even for humans. As an alternative we propose to use situated interactions between agents as a driving force for communication, and the framework of Deep Recurrent Q-Networks (DRQN) for learning a common language grounded in the provided environment. We task the agents with interactive image search in the form of the game Guess Who?. The images from the game provide a non trivial environment for the agents to discuss and a natural grounding for the concepts they decide to encode in their communication. Our experiments show that it is possible to learn this task using DRQN and even more importantly that the words the agents use correspond to physical attributes present in the images that make up the agents environment.", "creator": "LaTeX with hyperref package"}}}