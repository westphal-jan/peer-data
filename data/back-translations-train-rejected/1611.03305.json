{"id": "1611.03305", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Nov-2016", "title": "Getting Started with Neural Models for Semantic Matching in Web Search", "abstract": "The vocabulary mismatch problem is a long-standing problem in information retrieval. Semantic matching holds the promise of solving the problem. Recent advances in language technology have given rise to unsupervised neural models for learning representations of words as well as bigger textual units. Such representations enable powerful semantic matching methods. This survey is meant as an introduction to the use of neural models for semantic matching. To remain focused we limit ourselves to web search. We detail the required background and terminology, a taxonomy grouping the rapidly growing body of work in the area, and then survey work on neural models for semantic matching in the context of three tasks: query suggestion, ad retrieval, and document retrieval. We include a section on resources and best practices that we believe will help readers who are new to the area. We conclude with an assessment of the state-of-the-art and suggestions for future work.", "histories": [["v1", "Tue, 8 Nov 2016 14:28:40 GMT  (75kb)", "http://arxiv.org/abs/1611.03305v1", "under review for the Information Retrieval Journal"]], "COMMENTS": "under review for the Information Retrieval Journal", "reviews": [], "SUBJECTS": "cs.IR cs.CL", "authors": ["kezban dilek onal", "ismail sengor altingovde", "pinar karagoz", "maarten de rijke"], "accepted": false, "id": "1611.03305"}, "pdf": {"name": "1611.03305.pdf", "metadata": {"source": "CRF", "title": "Getting Started with Neural Models for Semantic Matching in Web Search", "authors": ["Kezban Dilek Onal", "Pinar Karagoz", "Maarten de Rijke"], "emails": ["dilek@ceng.metu.edu.tr,", "altingovde@ceng.metu.edu.tr,", "karagoz@ceng.metu.edu.tr", "k.d.onal@uva.nl,", "derijke@uva.nl"], "sections": [{"heading": null, "text": "ar Xiv: 161 1,03 305v 1 [cs.I R] 8N ovKeywords Distributed representations \u00b7 Semantic matching \u00b7 Web search"}, {"heading": "1 Introduction", "text": "This year, it has reached the point where it will be able to retaliate."}, {"heading": "2 Background and terminology", "text": "This section quantifies a number of relevant key terms that underlie the use of semantic matrices."}, {"heading": "3 Resources", "text": "This year, the time has come for the United States not to do as well as other countries in the world where it is not yet ready to establish itself as it is in the United States."}, {"heading": "4 Taxonomy", "text": "As part of this introduction to neural models for semantic matching in web search, we provide an overview of related work. To organize the material, we use a simple taxonomy. We classify related work based on the text unit (TTU), which requires distributed representations, how distributed representations are structured, and the intended use of the learned distributed representations. We summarize these features in Table 1."}, {"heading": "4.1 TTU", "text": "Distributed representations of queries are required for all web search tasks covered in this survey. Document and ad representations are also required for document and ad retrieval tasks. 11 https: / / github.com / ielab / adcs2015-NTLM, https: / / github.com / cvangysel / SERT, https: / / github.com / cvangysel / SERT12 See, e.g. http: / / colah.github.io /, http: / / sebastianruder.com / # open, http: / / www.wildml.com /, http: / / neuralnetworksanddeeplearning.com / to mention a few.13 http: / / stackoverflow.com / questions / tagged / deep-learning 14 https: / / / www.quora.com / topic / Deep-Learning4.2 UsaUsageThere are two types of TTU similarities that can be observed."}, {"heading": "4.3.1 Aggregate", "text": "The methods in this category are based on pre-trained word embeddings as external resources for distributed TTU representations. Existing work can be divided into two subcategories depending on the use of the embeddings: Explicit word embeddings are considered as building blocks for distributed TTU representations. The work following this pattern treats a TTU as a Bag of Embedded Words (BoEW) or a series of points in the word embedding.The most common aggregation method is the averaging or summing of the vectors of the terms in the TTU. Here, the vector similarity in the embeddingspace is implicitly used in language modeling frames, without explicit similarity calculation based on distributed representations for TTUs. Thus, Zuccon et al [93] calculate the translation probabilities of word pairs in a translation model that compares with cosmic similarities of skip-graph vectors."}, {"heading": "4.3.2 Learn", "text": "This category includes the work on neural semantic composition models for distributed representations. Three separate training objectives are observed in the reviewed work (13). On the basis of these objectives, we define the three sub-categories, namely neuronal learning to correspond, learn to predict contexts, and learn to generate contexts, all of which are detailed. Neural learning to match each other is the problem of learning a matching function f (x, y), which calculates the similarity of two objects x and y from two different spaces X and Y. Given the training data T consisting of triples (x, y, r), learning to match the optimization problem in Equation 12 is the problem of optimization 12. It names a loss function between the actual similarity values r and the score predicted by the f-function: argmin f-function F (x, r)."}, {"heading": "5 Document retrieval", "text": "In this section, we will examine work on neural models for semantic matching for retrieving documents. We will follow the how-to feature for organizing the material discussed in Section 4. Since query enhancement and query reweighting are aimed at improving the effectiveness of query analysis, this section also includes publications on these tasks. 5.1 AggregatesIn this section, we will present the work based on pre-trained word embedding for retrieving documents under implicit and explicit categories. In each subsection, we will mention query enhancement and query reweighting work separately."}, {"heading": "5.1.1 Explicit", "text": "In fact, most people are able to recognize themselves, understand and understand what they are doing."}, {"heading": "5.1.2 Implicit", "text": "In fact, the fact is that most of them will be able to move to another world in which they are able to find themselves."}, {"heading": "5.1.3 Reflections on evaluation", "text": "It is difficult to deduce conclusions about the comparative performance of the proposed approaches in terms of the diversity of experimental frameworks. In Table 4, we present an overview of the experimental frameworks used in these studies. NLM column identifies the methods against which the proposed method is compared. Only a recent study shows the comparison of embedding methods for word embedding. Retrievalcorpus column presents the document collection used in the Retrieval Corpus column in the Retrieval Corpus column. Finally, the last column indicates the methods against which the proposed method is compared."}, {"heading": "5.2.1 Learn to match", "text": "In fact, it is the case that you are able to play by the rules and that you are able to play by the rules that you have set yourself in order to play by them."}, {"heading": "5.2.2 Learn to predict", "text": "There are three shortcomings in the PV-DBOW model and an expanded sales vector model that is equipped with remedies for these shortcomings. Firstly, the PV-DBOW model is updated with respect to Inverse Corpus Frequencies (ICF) as inadequate for Inverse Document Frequency (IDF). Secondly, the PV-DBOW model is updated with implicit weights in relation to Inverse Corpus Frequencies (ICF) that have proven to be inferior to Inverse Document Frequency (IDF)."}, {"heading": "5.2.3 Learn to generate", "text": "Lioma et al. [53] ask if it is possible to generate relevant documents based on a query. A character-level LSTM network is optimized to generate a synthetic document, the network is fed with a sequence of words that are built around query terms in all relevant documents for the query by concatenating the query and context windows around the query terms. For each query, a separate model is generated and a synthetic document is generated for the same query with the learned model. Synthetic documents are evaluated in a crowdsourcing setting. Users are given four word clouds belonging to three known relevant documents and the synthetic document. Each word cloud is constructed by selecting the most common terms from the document. Users are asked to select the most relevant word cloud of the synthetic document. The author reports that the word cloud of the synthetic document takes second place in most queries with the first or second REC queries on the 5th experiments only."}, {"heading": "6 Query suggestion", "text": "Next, we turn to semantic matching for search queries. As explained in Section 4, the literature for the search query suggestion task has publications in the categories aggregate and learning (learn to match and generate contexts).6.1 AggregatesThe work of Cai and de Rijke [14] on the automatic completion of queries introduces semantic characteristics calculated using skip-gram embeddings to learn to classify candidates for the query for auto-completion. Query similarity, calculated by the sum and maximum of embedding similarity of terms from queries, is used as two separate characteristics. Maximum embedding similarity of terms turns out to be the most important feature in a diverse feature, including popular-related characteristics, lexical similarity, and another semantic similarity characteristic based on simultaneous learning of terms 6.2."}, {"heading": "6.2.1 Learn to match", "text": "In both studies, additional features are used in an existing learning process to ratify fragments; queries that begin with the user's last word are intercepted by the query logs; a query log is intercepted by the query logs; a query log is formed by the query results; a query log is formed by the query results; and the query results are intercepted by the query results."}, {"heading": "6.2.2 Learn to generate context", "text": "The Hieararchical Recurrent Encoder Decoder (HRED) model [81] is a neural model designed to generate the next query based on the session context vector. It is able to simultaneously learn representations for queries, words and sessions. It consists of two encoder RNNs and a decoder RNN. The first encoder RNN assigns each query to a distributed representation and the second encoder maps the sequence of query vectors into a session context vector. The decoder generates the most likely query to follow the session based on the distributed session representation. The HRED is evaluated in two settings. Firstly, the probability values of the model for a query to follow a session context are used as an additional feature in a learning frame to rank the query candidates to evaluate the query candidates that are collectively derived from a pre-existing query model."}, {"heading": "7 Ad retrieval", "text": "In fact, most of us are able to move to a different world, to move to a different world."}, {"heading": "8 Conclusion", "text": "This year, it has reached the point where it will be able to retaliate."}, {"heading": "A Acronyms used", "text": "AI Artificial Intelligence BoEW Bag of Embedded Words BWESG Bilingual Word Embedding NDM-NSC Model Distributed Embedding Skip Gram CBOW Continuous Bag of Words CLSM Convolutional Latent Semantic Model DESM Dual Embedding Space Model DFR Deviation from Randomness DSM Distribution Semantics Model DSSM Deep Structured Semantic Model EPV Extended Paragraph Vector GLM Generalized Language Model HAL Hyperspace Analog to Language HDV Hierarchical Document Vector HRED Hieararchical Recurrent Encoder Decoder IR Information Retrieeval IS Importance Sampling Sampling LDA Latent Dirichlet Allocation LM Language Model LSA Latent Semantic Memantic Analysis LSI Latent Semantic Indexing NCE Noise Estimation Estimation Neural Neural Neural PLM Neural Model Lurage Model VW NW Lurage Model"}], "references": [{"title": "Analysis of the paragraph vector model for information retrieval", "author": ["Q Ai", "L Yang", "J Guo", "WB Croft"], "venue": "Proceedings of the 2016 ACM International Conference on the Theory of Information Retrieval,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2016}, {"title": "Improving language estimation with the paragraph vector model for ad-hoc retrieval", "author": ["Q Ai", "L Yang", "J Guo", "WB Croft"], "venue": "Proceedings of the 39th International ACM SIGIR Conference on Research and Development in Information Retrieval,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2016}, {"title": "A comparison of deep learning based query expansion with pseudo-relevance feedback and mutual information", "author": ["M ALMasri", "C Berrut", "JP Chevallet"], "venue": "Advances in Information Retrieval: 38th European Conference on IR Research, ECIR 2016,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2016}, {"title": "Ads keyword rewriting using search engine results", "author": ["J Azimi", "A Alam", "R Zhang"], "venue": "Proceedings of the 24th International Conference on World Wide Web, International World Wide Web Conferences Steering Committee, Republic and Canton of Geneva, Switzerland, WWW \u201915 Companion,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2015}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["D Bahdanau", "K Cho", "Y Bengio"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2014}, {"title": "Don\u2019t count, predict! a systematic comparison of contextcounting vs. context-predicting semantic vectors. In: Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)", "author": ["M Baroni", "G Dinu", "G Kruszewski"], "venue": "Association for Computational Linguistics,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2014}, {"title": "Automatic differentiation in machine learning: a survey", "author": ["AG Baydin", "BA Pearlmutter", "AA Radul", "JM Siskind"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2015}, {"title": "A neural probabilistic language model", "author": ["Y Bengio", "R Ducharme", "P Vincent", "C Janvin"], "venue": "J Mach Learn Res 3:1137\u20131155,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2003}, {"title": "Quick training of probabilistic neural nets by importance sampling", "author": ["Y Bengio", "JS Sen\u00e9cal"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2003}, {"title": "Latent dirichlet allocation. Journal of machine Learning research", "author": ["DM Blei", "AY Ng", "MI Jordan"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2003}, {"title": "2016) A context-aware time model for web search", "author": ["A Borisov", "I Markov", "M de Rijke", "P Serdyukov"], "venue": "SIGIR 2016: 39th international ACM SIGIR conference on Research and development in information retrieval,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2016}, {"title": "2016) A neural click model for web search. In: WWW 2016", "author": ["A Borisov", "I Markov", "M de Rijke", "P Serdyukov"], "venue": "25th International World Wide Web Conference,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2016}, {"title": "Signature verification using a \u201csiamese\u201d time delay neural network. International Journal of Pattern Recognition and Artificial Intelligence", "author": ["J Bromley", "JW Bentz", "L Bottou", "I Guyon", "Y LeCun", "C Moore", "E S\u00e4ckinger", "R Shah"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1993}, {"title": "Learning from homologous queries and semantically related terms for query auto completion", "author": ["F Cai", "M de Rijke"], "venue": "Information Processing & Management", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2016}, {"title": "2016) A survey of query auto completion in information retrieval. Foundations and Trends in Information Retrieval 10(4):273\u2013363  Getting Started with Neural Models for Semantic Matching in Web Search", "author": ["F Cai", "M de Rijke"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2016}, {"title": "A survey of automatic query expansion in information retrieval", "author": ["C Carpineto", "G Romano"], "venue": "ACM Comput Surv 44(1):1:1\u20131:50,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2012}, {"title": "Strategies for training large vocabulary neural language models. In: Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), Association for Computational Linguistics, Berlin, Germany, pp 1975\u20131985", "author": ["W Chen", "D Grangier", "M Auli"], "venue": null, "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2016}, {"title": "Natural language understanding with distributed representation", "author": ["K Cho"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2015}, {"title": "Learning phrase representations using RNN encoder-decoder for statistical machine translation", "author": ["K Cho", "B van Merrienboer", "\u00c7 G\u00fcl\u00e7ehre", "F Bougares", "H Schwenk", "Y Bengio"], "venue": "CoRR abs/1406.1078,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2014}, {"title": "Click Models for Web Search. Synthesis Lectures on Information Concepts, Retrieval, and Services, Morgan & Claypool Publishers, DOI http://doi. org/10.2200/S00654ED1V01Y201507ICR043, URL http://clickmodels.weebly.com/ uploads/5/2/2/5/52257029/mc2015-clickmodels.pdf", "author": ["A Chuklin", "I Markov", "M de Rijke"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2015}, {"title": "Aggregating continuous word embeddings for information retrieval. In: Proceedings of the Workshop on Continuous Vector Space Models and their Compositionality, Association for Computational Linguistics, Sofia, Bulgaria, pp 100\u2013109", "author": ["S Clinchant", "F Perronnin"], "venue": "URL http: //www.aclweb.org/anthology/W13-3212", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2013}, {"title": "A Unified Architecture for Natural Language Processing: Deep Neural Networks with Multitask Learning", "author": ["R Collobert", "J Weston"], "venue": "Proceedings of the 25th International Conference on Machine Learning,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2008}, {"title": "Document embedding with paragraph vectors", "author": ["AM Dai", "C Olah", "QV Le", "GS Corrado"], "venue": "NIPS Deep Learning Workshop", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2014}, {"title": "Indexing by latent semantic analysis. Journal of the American society for information science", "author": ["S Deerwester", "ST Dumais", "GW Furnas", "TK Landauer", "R Harshman"], "venue": null, "citeRegEx": "24", "shortCiteRegEx": "24", "year": 1990}, {"title": "Deep learning: Methods and applications. Foundations and Trends in Signal Processing 7(3\u20134):197\u2013387, URL https://www.microsoft.com/en-us/research/ publication/deep-learning-methods-and-applications", "author": ["L Deng", "D Yu"], "venue": null, "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2013}, {"title": "Query expansion with locally-trained word embeddings. In: Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), Association for Computational Linguistics, Berlin, Germany, pp 367\u2013377", "author": ["F Diaz", "B Mitra", "N Craswell"], "venue": null, "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2016}, {"title": "Hierarchical neural language models for joint representation of streaming documents and their content", "author": ["N Djuric", "H Wu", "V Radosavljevic", "M Grbovic", "N Bhamidipati"], "venue": "Proceedings of the 24th International Conference on World Wide Web,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2015}, {"title": "The vocabulary problem in human-system communication", "author": ["GW Furnas", "TK Landauer", "LM Gomez", "ST Dumais"], "venue": "Commun ACM 30(11):964\u2013971,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 1987}, {"title": "Word embedding based generalized language model for information retrieval", "author": ["D Ganguly", "D Roy", "M Mitra", "GJ Jones"], "venue": "Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2015}, {"title": "A primer on neural network models for natural language processing", "author": ["Y Goldberg"], "venue": null, "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2015}, {"title": "A (2016) Deep learning, URL http://www. deeplearningbook.org, book in preparation", "author": ["I Goodfellow", "Y Bengio", "Courville"], "venue": null, "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2016}, {"title": "Neural networks. In: Supervised Sequence Labelling with Recurrent Neural Networks, Springer, pp", "author": ["A Graves"], "venue": null, "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2012}, {"title": "Search retargeting using directed query embeddings", "author": ["M Grbovic", "N Djuric", "V Radosavljevic", "N Bhamidipati"], "venue": "Proceedings of International World Wide Web Conference (WWW)", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2015}, {"title": "Context- and content-aware embeddings for query rewriting in sponsored search", "author": ["M Grbovic", "N Djuric", "V Radosavljevic", "F Silvestri", "N Bhamidipati"], "venue": "Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2015}, {"title": "2016) A deep relevance matching model for ad-hoc retrieval. In: CIKM 2016", "author": ["J Guo", "Y Fan", "Q Ai", "WB Croft"], "venue": "25th ACM Conference on Information and Knowledge Management,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2016}, {"title": "A (2012) Noise-contrastive estimation of unnormalized statistical models, with applications to natural image statistics", "author": ["Gutmann MU", "Hyv\u00e4rinen"], "venue": "J Mach Learn Res 13(1):307\u2013361,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2012}, {"title": "Overview of the reliable information access workshop", "author": ["D Harman", "C Buckley"], "venue": "Inf Retr 12(6):615\u2013641,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2009}, {"title": "Learning distributed representations of sentences from unlabelled data. In: Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Association for Computational Linguistics, San Diego, California, pp 1367\u20131377", "author": ["F Hill", "K Cho", "A Korhonen"], "venue": "URL http://www.aclweb.org/anthology/", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2016}, {"title": "Learning deep structured semantic models for web search using clickthrough data", "author": ["PS Huang", "X He", "J Gao", "L Deng", "A Acero", "L Heck"], "venue": "Proceedings of the 22Nd ACM International Conference on Conference on Information ", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2013}, {"title": "Learning dynamic classes of events using stacked multilayer perceptron networks. In: Neu-IR: The SIGIR 2016", "author": ["N Kanhabua", "H Ren", "TB Moeslund"], "venue": "Workshop on Neural Information Retrieval", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2016}, {"title": "Short text similarity with word embeddings", "author": ["T Kenter", "M de Rijke"], "venue": "Proceedings of the 24th ACM International on Conference on Information and Knowledge Management, ACM,", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2015}, {"title": "Ad hoc monitoring of vocabulary shifts over time", "author": ["T Kenter", "P Huijnen", "M Wevers", "M de Rijke"], "venue": "CIKM 2015: 24th ACM Conference on Information and Knowledge Management,", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2015}, {"title": "Relevance based language models", "author": ["V Lavrenko", "WB Croft"], "venue": "Proceedings of the 24th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval,", "citeRegEx": "46", "shortCiteRegEx": "46", "year": 2001}, {"title": "Distributed representations of sentences and documents", "author": ["QV Le", "T Mikolov"], "venue": "Proceedings of the 31th International Conference on Machine Learning,", "citeRegEx": "47", "shortCiteRegEx": "47", "year": 2014}, {"title": "Improving distributional similarity with lessons learned from word embeddings. Transactions of the Association for Computational Linguistics 3:211\u2013225", "author": ["O Levy", "Y Goldberg", "I Dagan"], "venue": null, "citeRegEx": "48", "shortCiteRegEx": "48", "year": 2015}, {"title": "Deep learning for information retrieval", "author": ["H Li", "Z Lu"], "venue": "Proceedings of the 39th International ACM SIGIR Conference on Research and Development in Information Retrieval,", "citeRegEx": "49", "shortCiteRegEx": "49", "year": 2016}, {"title": "Semantic matching in search. Foundations and Trends in Information Retrieval 7(5):343\u2013469", "author": ["H Li", "J Xu"], "venue": "DOI 10.1561/1500000035,", "citeRegEx": "50", "shortCiteRegEx": "50", "year": 2013}, {"title": "When are tree structures necessary for deep learning of representations", "author": ["J Li", "MT Luong", "D Jurafsky", "E Hovy"], "venue": null, "citeRegEx": "51", "shortCiteRegEx": "51", "year": 2015}, {"title": "Deep learning powered in-session contextual ranking using clickthrough data. In: Workshop on Personalization: Methods and Applications, at Neural Information Processing Systems (NIPS)  Getting Started with Neural Models for Semantic Matching in Web Search", "author": ["X Li", "C Guo", "W Chu", "YY Wang", "J Shavlik"], "venue": null, "citeRegEx": "52", "shortCiteRegEx": "52", "year": 2014}, {"title": "Deep learning relevance: Creating relevant information (as opposed to retrieving it)", "author": ["C Lioma", "B Larsen", "C Petersen", "JG Simonsen"], "venue": "CoRR abs/1606.07660,", "citeRegEx": "53", "shortCiteRegEx": "53", "year": 2016}, {"title": "Representation learning using multi-task deep neural networks for semantic classification and information retrieval", "author": ["X Liu", "J Gao", "X He", "L Deng", "K Duh", "YY Wang"], "venue": "Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,", "citeRegEx": "54", "shortCiteRegEx": "54", "year": 2015}, {"title": "Producing high-dimensional semantic spaces from lexical co-occurrence", "author": ["K Lund", "C Burgess"], "venue": "Behavior Research Methods, Instruments,", "citeRegEx": "55", "shortCiteRegEx": "55", "year": 1996}, {"title": "Visualizing data using t-SNE", "author": ["L van der Maaten", "G Hinton"], "venue": "Journal of Machine Learning Research", "citeRegEx": "56", "shortCiteRegEx": "56", "year": 2008}, {"title": "Parallel distributed processing. Explorations in the microstructure of cognition", "author": ["JL McClelland", "DE Rumelhart", "Group PDP Research"], "venue": null, "citeRegEx": "57", "shortCiteRegEx": "57", "year": 1986}, {"title": "Recurrent neural network based language model", "author": ["T Mikolov", "M Karafi\u00e1t", "L Burget", "J Cernock\u1ef3", "S Khudanpur"], "venue": null, "citeRegEx": "58", "shortCiteRegEx": "58", "year": 2010}, {"title": "Efficient estimation of word representations in vector space", "author": ["T Mikolov", "K Chen", "G Corrado", "J Dean"], "venue": "CoRR abs/1301.3781,", "citeRegEx": "59", "shortCiteRegEx": "59", "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["T Mikolov", "I Sutskever", "K Chen", "GS Corrado", "J Dean"], "venue": "KQ (eds) Advances in Neural Information Processing Systems", "citeRegEx": "60", "shortCiteRegEx": "60", "year": 2013}, {"title": "Linguistic regularities in continuous space word representations. In: Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Association", "author": ["T Mikolov", "Wt Yih", "G Zweig"], "venue": null, "citeRegEx": "61", "shortCiteRegEx": "61", "year": 2013}, {"title": "Composition in distributional models of semantics. Cognitive Science 34(8):1388\u20131429, DOI 10.1111/j.1551-6709.2010.01106.x, URL http://dx.doi.org/ 10.1111/j.1551-6709.2010.01106.x", "author": ["J Mitchell", "M Lapata"], "venue": null, "citeRegEx": "62", "shortCiteRegEx": "62", "year": 2010}, {"title": "Exploring session context using distributed representations of queries and reformulations", "author": ["B Mitra"], "venue": "Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval,", "citeRegEx": "63", "shortCiteRegEx": "63", "year": 2015}, {"title": "Query auto-completion for rare prefixes", "author": ["B Mitra", "N Craswell"], "venue": "Proceedings of the 24th ACM International on Conference on Information and Knowledge Management, ACM,", "citeRegEx": "64", "shortCiteRegEx": "64", "year": 2015}, {"title": "2016) A dual embedding space model for document ranking", "author": ["B Mitra", "E Nalisnick", "N Craswell", "R Caruana"], "venue": null, "citeRegEx": "65", "shortCiteRegEx": "65", "year": 2011}, {"title": "A fast and simple algorithm for training neural probabilistic language models", "author": ["A Mnih", "YW Teh"], "venue": null, "citeRegEx": "66", "shortCiteRegEx": "66", "year": 2012}, {"title": "Hierarchical probabilistic neural network language model", "author": ["F Morin", "Y Bengio"], "venue": "Proceedings of the Tenth International Workshop on Artificial Intelligence and Statistics,", "citeRegEx": "67", "shortCiteRegEx": "67", "year": 2005}, {"title": "Improving document ranking with dual word embeddings", "author": ["E Nalisnick", "B Mitra", "N Craswell", "R Caruana"], "venue": "Proceedings of the 25th International Conference Companion on World Wide Web, International World Wide Web Conferences Steering Committee, Republic and Canton of Geneva, Switzerland, WWW \u201916 Companion,", "citeRegEx": "68", "shortCiteRegEx": "68", "year": 2016}, {"title": "Query transformations for result merging", "author": ["S Palakodety", "J Callan"], "venue": "Proceedings of The Twenty-Third Text REtrieval Conference,", "citeRegEx": "69", "shortCiteRegEx": "69", "year": 2014}, {"title": "Semantic modelling with long-short-term memory for information retrieval. CoRR abs/1412.6629, URL http://arxiv", "author": ["H Palangi", "L Deng", "Y Shen", "J Gao", "X He", "J Chen", "X Song", "R Ward"], "venue": null, "citeRegEx": "70", "shortCiteRegEx": "70", "year": 2014}, {"title": "Deep sentence embedding using the long short term memory network: Analysis and application to information retrieval", "author": ["H Palangi", "L Deng", "Y Shen", "J Gao", "X He", "J Chen", "X Song", "R Ward"], "venue": "CoRR abs/1502.06922,", "citeRegEx": "71", "shortCiteRegEx": "71", "year": 2015}, {"title": "Deep sentence embedding using long short-term memory networks: Analysis and application to information retrieval", "author": ["H Palangi", "L Deng", "Y Shen", "J Gao", "X He", "J Chen", "X Song", "R Ward"], "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing 24(4):694\u2013707,", "citeRegEx": "72", "shortCiteRegEx": "72", "year": 2016}, {"title": "Glove: Global vectors for word representation", "author": ["J Pennington", "R Socher", "C Manning"], "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "73", "shortCiteRegEx": "73", "year": 2014}, {"title": "Query chains: Learning to rank from implicit feedback", "author": ["F Radlinski", "T Joachims"], "venue": "Proceedings of the Eleventh ACM SIGKDD International Conference on Knowledge Discovery in Data Mining, ACM,", "citeRegEx": "74", "shortCiteRegEx": "74", "year": 2005}, {"title": "Understanding inverse document frequency: on theoretical arguments for idf. Journal of documentation", "author": ["S Robertson"], "venue": null, "citeRegEx": "75", "shortCiteRegEx": "75", "year": 2004}, {"title": "Representing documents and queries as sets of word embedded vectors for information", "author": ["D Roy", "D Ganguly", "M Mitra", "GJ Jones"], "venue": null, "citeRegEx": "76", "shortCiteRegEx": "76", "year": 2016}, {"title": "Using word embeddings for automatic query expansion", "author": ["D Roy", "D Paul", "M Mitra", "U Garain"], "venue": null, "citeRegEx": "77", "shortCiteRegEx": "77", "year": 2016}, {"title": "A latent semantic model with convolutionalpooling structure for information retrieval", "author": ["Y Shen", "X He", "J Gao", "L Deng", "G Mesnil"], "venue": "Proceedings of the 23rd ACM International Conference on Conference on Information and Knowledge Management,", "citeRegEx": "78", "shortCiteRegEx": "78", "year": 2014}, {"title": "Learning semantic representations using convolutional neural networks for web search", "author": ["Y Shen", "X He", "J Gao", "L Deng", "G Mesnil"], "venue": "Proceedings of the 23rd International Conference on World Wide Web, International World Wide Web Conferences Steering Committee, Republic and Canton of Geneva,", "citeRegEx": "79", "shortCiteRegEx": "79", "year": 2014}, {"title": "Cnu system in ntcir-11 imine task", "author": ["W Song", "W Xu", "L Liu", "H Wang"], "venue": null, "citeRegEx": "80", "shortCiteRegEx": "80", "year": 2014}, {"title": "A hierarchical recurrent encoder-decoder for generative context-aware query suggestion", "author": ["A Sordoni", "Y Bengio", "H Vahabi", "C Lioma", "J Grue Simonsen", "JY Nie"], "venue": "Proceedings of the 24th ACM International on Conference on Information and Knowledge Management, ACM,", "citeRegEx": "81", "shortCiteRegEx": "81", "year": 2015}, {"title": "From frequency to meaning: Vector space models of semantics", "author": ["PD Turney", "P Pantel"], "venue": "J Artif Intell Res (JAIR) 37:141\u2013188,", "citeRegEx": "82", "shortCiteRegEx": "82", "year": 2010}, {"title": "Learning latent vector spaces for product search. In: CIKM 2016", "author": ["C Van Gysel", "M de Rijke", "E Kanoulas"], "venue": "25th ACM Conference on Information and Knowledge Management,", "citeRegEx": "83", "shortCiteRegEx": "83", "year": 2016}, {"title": "Unsupervised, efficient and semantic expertise retrieval. In: WWW 2016", "author": ["C Van Gysel", "M de Rijke", "M Worring"], "venue": "25th International World Wide Web Conference,", "citeRegEx": "84", "shortCiteRegEx": "84", "year": 2016}, {"title": "Monolingual and cross-lingual information retrieval models based on (bilingual) word embeddings", "author": ["I Vuli\u0107", "MF Moens"], "venue": "Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval,", "citeRegEx": "85", "shortCiteRegEx": "85", "year": 2015}, {"title": "Show, attend and tell: Neural image caption generation with visual attention", "author": ["K Xu", "J Ba", "R Kiros", "K Cho", "AC Courville", "R Salakhutdinov", "RS Zemel", "Y Bengio"], "venue": "CoRR abs/1502.03044,", "citeRegEx": "86", "shortCiteRegEx": "86", "year": 2015}, {"title": "Selective term proximity scoring via bp-ann", "author": ["J Yang", "J Tong", "RJ Stones", "Z Zhang", "B Ye", "G Wang", "X Liu"], "venue": "Neu-IR: The SIGIR 2016 Workshop on Neural Information Retrieval", "citeRegEx": "87", "shortCiteRegEx": "87", "year": 2016}, {"title": "Embedding-based query language models", "author": ["H Zamani", "WB Croft"], "venue": "Proceedings of the 2016 ACM International Conference on the Theory of Information Retrieval,", "citeRegEx": "88", "shortCiteRegEx": "88", "year": 2016}, {"title": "Estimating embedding vectors for queries", "author": ["H Zamani", "WB Croft"], "venue": "Proceedings of the 2016 ACM International Conference on the Theory of Information Retrieval,", "citeRegEx": "89", "shortCiteRegEx": "89", "year": 2016}, {"title": "Attention based recurrent neural networks for online advertising", "author": ["S Zhai", "Kh Chang", "R Zhang", "Z Zhang"], "venue": "Proceedings of the 25th International Conference Companion on World Wide Web, International World Wide Web Conferences Steering Committee, Republic and Canton of Geneva, Switzerland, WWW \u201916", "citeRegEx": "90", "shortCiteRegEx": "90", "year": 2016}, {"title": "Deepintent: Learning attentions for online advertising with recurrent neural networks", "author": ["S Zhai", "Kh Chang", "R Zhang", "ZM Zhang"], "venue": "Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, ACM,", "citeRegEx": "91", "shortCiteRegEx": "91", "year": 2016}, {"title": "Learning to reweight terms with distributed representations", "author": ["G Zheng", "J Callan"], "venue": "Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval,", "citeRegEx": "92", "shortCiteRegEx": "92", "year": 2015}, {"title": "Integrating and evaluating neural word embeddings in information retrieval", "author": ["G Zuccon", "B Koopman", "P Bruza", "L Azzopardi"], "venue": "Proceedings of the 20th Australasian Document Computing Symposium,", "citeRegEx": "93", "shortCiteRegEx": "93", "year": 2015}], "referenceMentions": [{"referenceID": 27, "context": "In web search, the vocabulary mismatch problem [28] necessitates effective semantic similarity functions for textual units of different types.", "startOffset": 47, "endOffset": 51}, {"referenceID": 46, "context": "According to Li and Xu [50], semantic matching is concerned with computing the relevance of a document for a query based on representations enriched by linguistic analysis that is meant to capture the semantics of the query and document.", "startOffset": 23, "endOffset": 27}, {"referenceID": 5, "context": "Recent advances in language understanding have given rise to neural network models for unsupervised learning of distributed representations of words [6, 8, 59, 73] and larger textual units [39, 47].", "startOffset": 149, "endOffset": 163}, {"referenceID": 7, "context": "Recent advances in language understanding have given rise to neural network models for unsupervised learning of distributed representations of words [6, 8, 59, 73] and larger textual units [39, 47].", "startOffset": 149, "endOffset": 163}, {"referenceID": 55, "context": "Recent advances in language understanding have given rise to neural network models for unsupervised learning of distributed representations of words [6, 8, 59, 73] and larger textual units [39, 47].", "startOffset": 149, "endOffset": 163}, {"referenceID": 69, "context": "Recent advances in language understanding have given rise to neural network models for unsupervised learning of distributed representations of words [6, 8, 59, 73] and larger textual units [39, 47].", "startOffset": 149, "endOffset": 163}, {"referenceID": 37, "context": "Recent advances in language understanding have given rise to neural network models for unsupervised learning of distributed representations of words [6, 8, 59, 73] and larger textual units [39, 47].", "startOffset": 189, "endOffset": 197}, {"referenceID": 43, "context": "Recent advances in language understanding have given rise to neural network models for unsupervised learning of distributed representations of words [6, 8, 59, 73] and larger textual units [39, 47].", "startOffset": 189, "endOffset": 197}, {"referenceID": 53, "context": "A distributed representation for a textual unit is a dense real-valued vector that somehow encodes the semantics of the textual unit [57].", "startOffset": 133, "endOffset": 137}, {"referenceID": 46, "context": "Distributed representations hold the promise of aiding semantic matching: by mapping words and other textual units to their representations, semantic matches can be computed in the representation space [50].", "startOffset": 202, "endOffset": 206}, {"referenceID": 78, "context": "The problem of mapping words to a representation that can capture their meanings is referred as distributional semantics and has been studied for a very long time; see [82] for an overview.", "startOffset": 168, "endOffset": 172}, {"referenceID": 51, "context": "Neural language models, which may be viewed as a particular flavor of distributional semantic models, so-called context-predicting distributional semantic models, have been shown to outperform so-called contextcounting models such as Hyperspace Analog to Language (HAL) [55], Latent Semantic Analysis (LSA) [24], on word analogy and semantic relatedness tasks [6].", "startOffset": 270, "endOffset": 274}, {"referenceID": 23, "context": "Neural language models, which may be viewed as a particular flavor of distributional semantic models, so-called context-predicting distributional semantic models, have been shown to outperform so-called contextcounting models such as Hyperspace Analog to Language (HAL) [55], Latent Semantic Analysis (LSA) [24], on word analogy and semantic relatedness tasks [6].", "startOffset": 307, "endOffset": 311}, {"referenceID": 5, "context": "Neural language models, which may be viewed as a particular flavor of distributional semantic models, so-called context-predicting distributional semantic models, have been shown to outperform so-called contextcounting models such as Hyperspace Analog to Language (HAL) [55], Latent Semantic Analysis (LSA) [24], on word analogy and semantic relatedness tasks [6].", "startOffset": 360, "endOffset": 363}, {"referenceID": 44, "context": "Moreover, Levy et al [48] improve context-counting models by adopting lessons from context-predicting models.", "startOffset": 21, "endOffset": 25}, {"referenceID": 7, "context": "Bengio et al [8] seem to have been the first ones to propose a neural language model; they introduce the idea of simultaneously learning a language model that predicts a word given its context and its representation, a so-called word embedding.", "startOffset": 13, "endOffset": 16}, {"referenceID": 55, "context": "The most well-known and most widely used context-predicting models, Word2Vec [59] and GloVe [73], have been used extensively in recent work on web search.", "startOffset": 77, "endOffset": 81}, {"referenceID": 69, "context": "The most well-known and most widely used context-predicting models, Word2Vec [59] and GloVe [73], have been used extensively in recent work on web search.", "startOffset": 92, "endOffset": 96}, {"referenceID": 37, "context": "The success of neural word embeddings has also given rise to work on computing context-predicting representations of larger textual units, including paragraphs and documents [39].", "startOffset": 174, "endOffset": 178}, {"referenceID": 24, "context": "In recent years, neural network-based models have given rise to significant performance improvements in computer vision, speech processing and machine translation [25].", "startOffset": 163, "endOffset": 167}, {"referenceID": 24, "context": "Training deep neural networks to learn hierarchies of representations became possible owing both to theoretical contributions of new machine learning algorithms and parallelization of training on GPUs [25].", "startOffset": 201, "endOffset": 205}, {"referenceID": 24, "context": "Deng and Yu [25] devote a chapter to applications of deep learning in information retrieval.", "startOffset": 12, "endOffset": 16}, {"referenceID": 45, "context": "The recent tutorial on deep learning for information retrieval by Li and Lu [49] sketches a range of potential applications of deep learning to information retrieval (IR) problems with a broader scope.", "startOffset": 76, "endOffset": 80}, {"referenceID": 80, "context": "Therefore, studies that adopt neural models for other information retrieval tasks, such as semantic expertise retrieval [84], product search [83], click and behavior models [11, 12], text similarity [42, 43], subtopic mining [80], are out of scope.", "startOffset": 120, "endOffset": 124}, {"referenceID": 79, "context": "Therefore, studies that adopt neural models for other information retrieval tasks, such as semantic expertise retrieval [84], product search [83], click and behavior models [11, 12], text similarity [42, 43], subtopic mining [80], are out of scope.", "startOffset": 141, "endOffset": 145}, {"referenceID": 10, "context": "Therefore, studies that adopt neural models for other information retrieval tasks, such as semantic expertise retrieval [84], product search [83], click and behavior models [11, 12], text similarity [42, 43], subtopic mining [80], are out of scope.", "startOffset": 173, "endOffset": 181}, {"referenceID": 11, "context": "Therefore, studies that adopt neural models for other information retrieval tasks, such as semantic expertise retrieval [84], product search [83], click and behavior models [11, 12], text similarity [42, 43], subtopic mining [80], are out of scope.", "startOffset": 173, "endOffset": 181}, {"referenceID": 40, "context": "Therefore, studies that adopt neural models for other information retrieval tasks, such as semantic expertise retrieval [84], product search [83], click and behavior models [11, 12], text similarity [42, 43], subtopic mining [80], are out of scope.", "startOffset": 199, "endOffset": 207}, {"referenceID": 41, "context": "Therefore, studies that adopt neural models for other information retrieval tasks, such as semantic expertise retrieval [84], product search [83], click and behavior models [11, 12], text similarity [42, 43], subtopic mining [80], are out of scope.", "startOffset": 199, "endOffset": 207}, {"referenceID": 76, "context": "Therefore, studies that adopt neural models for other information retrieval tasks, such as semantic expertise retrieval [84], product search [83], click and behavior models [11, 12], text similarity [42, 43], subtopic mining [80], are out of scope.", "startOffset": 225, "endOffset": 229}, {"referenceID": 30, "context": ", Part II of [31].", "startOffset": 13, "endOffset": 17}, {"referenceID": 5, "context": "Baroni et al [6] classify existing DSMs into two categories: context-counting and context-predicting.", "startOffset": 13, "endOffset": 16}, {"referenceID": 51, "context": "The context-counting category includes earlier DSMs such as Hyperspace Analog to Language (HAL) [55], Latent Semantic Analysis (LSA) [24].", "startOffset": 96, "endOffset": 100}, {"referenceID": 23, "context": "The context-counting category includes earlier DSMs such as Hyperspace Analog to Language (HAL) [55], Latent Semantic Analysis (LSA) [24].", "startOffset": 133, "endOffset": 137}, {"referenceID": 53, "context": "A distributed representation of a symbol is a vector of features that characterize the meaning of the symbol and are not mutually exclusive [57].", "startOffset": 140, "endOffset": 144}, {"referenceID": 7, "context": "However, it soon turned out that the embedding layer component, which addresses the curse of dimensionality caused by one-hot vectors [8], yields useful distributed word representations, so-called word embeddings.", "startOffset": 134, "endOffset": 137}, {"referenceID": 21, "context": "Collobert and Weston [22] are the first ones to show the benefit of word embeddings as features for NLP tasks.", "startOffset": 21, "endOffset": 25}, {"referenceID": 55, "context": "Subsequently, word embeddings became widespread after introduction of the shallow models Skip-Gram and CBOW in the Word2Vec framework by Mikolov et al [59, 60]; see Section 2.", "startOffset": 151, "endOffset": 159}, {"referenceID": 56, "context": "Subsequently, word embeddings became widespread after introduction of the shallow models Skip-Gram and CBOW in the Word2Vec framework by Mikolov et al [59, 60]; see Section 2.", "startOffset": 151, "endOffset": 159}, {"referenceID": 5, "context": "Baroni et al [6] report that context-predicting models outperform context-counting models on several tasks including question sets, semantic relatedness, synonym detection, concept categorization and word analogy.", "startOffset": 13, "endOffset": 16}, {"referenceID": 44, "context": "In contrast, Levy et al [48] point out that the success of the popular context-predicting models Word2Vec and GloVe does not originate from the neural network architecture and the training objective but from the choices of hyper-parameters for contexts.", "startOffset": 24, "endOffset": 28}, {"referenceID": 58, "context": "Compositional distributional semantics or semantic compositionality (SC) is the problem of formalizing how the meaning of larger textual units such as sentences, phrases, paragraphs and documents are built from the meanings of words [62].", "startOffset": 233, "endOffset": 237}, {"referenceID": 37, "context": "from unlabelled data is presented in [39].", "startOffset": 37, "endOffset": 41}, {"referenceID": 37, "context": "Besides the models reviewed by Hill et al [39], there exist sentence-level models that are trained using task-specific labelled data.", "startOffset": 42, "endOffset": 46}, {"referenceID": 47, "context": "For instance, a model can be trained to encode the sentiment of a sentence using a dataset of sentences annotated with sentiment class labels [51].", "startOffset": 142, "endOffset": 146}, {"referenceID": 7, "context": "The first neural language model published is the Neural Network Language Model (NNLM) [8].", "startOffset": 86, "endOffset": 89}, {"referenceID": 7, "context": "In NNLM [8] it is a non-linear neural network layer whereas in the Continuous Bag of Words (CBOW) model of Word2Vec [59], it is vector addition over word embeddings.", "startOffset": 8, "endOffset": 11}, {"referenceID": 55, "context": "In NNLM [8] it is a non-linear neural network layer whereas in the Continuous Bag of Words (CBOW) model of Word2Vec [59], it is vector addition over word embeddings.", "startOffset": 116, "endOffset": 120}, {"referenceID": 54, "context": "In the Recurrent Neural Network Language Model (RNNLM) [58] the hidden context representation is computed by a recurrent neural network.", "startOffset": 55, "endOffset": 59}, {"referenceID": 21, "context": "In [22] and in the CBOW model [59] context is defined by the words that surround a center word in a symmetric context.", "startOffset": 3, "endOffset": 7}, {"referenceID": 55, "context": "In [22] and in the CBOW model [59] context is defined by the words that surround a center word in a symmetric context.", "startOffset": 30, "endOffset": 34}, {"referenceID": 7, "context": "In the NNLM [8] and RNNLM [58] models, the context is defined by words that precede the target word.", "startOffset": 12, "endOffset": 15}, {"referenceID": 54, "context": "In the NNLM [8] and RNNLM [58] models, the context is defined by words that precede the target word.", "startOffset": 26, "endOffset": 30}, {"referenceID": 55, "context": "The Skip-Gram [59] takes a single word as input and predict words from a dynamically sized symmetric context window around the input word.", "startOffset": 14, "endOffset": 18}, {"referenceID": 16, "context": "Chen et al [17] present a comparison of training methods for the NNLM.", "startOffset": 11, "endOffset": 15}, {"referenceID": 63, "context": "Hierarchical Softmax [67] and differentiated softmax [17] propose adapted softmax layer architectures for efficient computation of the softmax function.", "startOffset": 21, "endOffset": 25}, {"referenceID": 16, "context": "Hierarchical Softmax [67] and differentiated softmax [17] propose adapted softmax layer architectures for efficient computation of the softmax function.", "startOffset": 53, "endOffset": 57}, {"referenceID": 8, "context": "Another solution, adopted by methods like Importance Sampling (IS) [9] and Noise Contrastive Estimation (NCE) [66], is to avoid the normalization by using modified loss functions to approximate the softmax.", "startOffset": 67, "endOffset": 70}, {"referenceID": 62, "context": "Another solution, adopted by methods like Importance Sampling (IS) [9] and Noise Contrastive Estimation (NCE) [66], is to avoid the normalization by using modified loss functions to approximate the softmax.", "startOffset": 110, "endOffset": 114}, {"referenceID": 21, "context": "Collobert and Weston [22] propose the cost function in Equation 5, which does not require normalization over the vocabulary.", "startOffset": 21, "endOffset": 25}, {"referenceID": 62, "context": "Mnih and Teh [66] apply NCE [36] to NLM training.", "startOffset": 13, "endOffset": 17}, {"referenceID": 35, "context": "Mnih and Teh [66] apply NCE [36] to NLM training.", "startOffset": 28, "endOffset": 32}, {"referenceID": 55, "context": "Mikolov et al [59] introduce the Skip-Gram and CBOW models that follow the NLM architecture with a linear layer for computing a distributed context representation.", "startOffset": 14, "endOffset": 18}, {"referenceID": 63, "context": "Efficient training of Skip-Gram and CBOW models is achieved by hiearchical softmax [67] with a Huffman tree Mikolov et al [59].", "startOffset": 83, "endOffset": 87}, {"referenceID": 55, "context": "Efficient training of Skip-Gram and CBOW models is achieved by hiearchical softmax [67] with a Huffman tree Mikolov et al [59].", "startOffset": 122, "endOffset": 126}, {"referenceID": 56, "context": "In follow-up work [60], Negative Sampling (NEG) is proposed for efficiently training the Skip-Gram model.", "startOffset": 18, "endOffset": 22}, {"referenceID": 56, "context": "Subsampling frequent words is another extension introduced in [60] for speeding up training and increasing the quality of embeddings.", "startOffset": 62, "endOffset": 66}, {"referenceID": 69, "context": "Global Vectors (GloVe) [73] combines global context and local context in the training objective for learning word embeddings.", "startOffset": 23, "endOffset": 27}, {"referenceID": 44, "context": "Levy et al [48] discuss that diluting frequent words before training enlarges the context window size in practice.", "startOffset": 11, "endOffset": 15}, {"referenceID": 44, "context": "Experiments show that the hyper-parameters about context-windows like dynamic size and subsampling frequent words have a notable impact on the performance of SGNS and GloVe [48].", "startOffset": 173, "endOffset": 177}, {"referenceID": 5, "context": "In contrast to the conclusions obtained in [6], the success of context-predicting models is attributed to choice of hyper-parameters, which can also be used for context-counting DSMs, rather than to the neural architecture or the training objective.", "startOffset": 43, "endOffset": 46}, {"referenceID": 43, "context": "The Paragraph Vector [47] extends Word2Vec in order to learn representations for socalled paragraph, textual units of any length.", "startOffset": 21, "endOffset": 25}, {"referenceID": 43, "context": "Le and Mikolov [47] assess vectors obtained by averaging PV-DM and PV-DBOW vectors on sentiment classification and snippet retrieval tasks.", "startOffset": 15, "endOffset": 19}, {"referenceID": 22, "context": "Dai et al [23] show that paragraph vectors outperform the vector representations obtained by Latent Dirichlet Allocation (LDA) [10], average of word embeddings and tf-idf weighted one-hot vector representations, on a set of document triplets constructed with the same strategy in [47], using Wikipedia and arXiv documents.", "startOffset": 10, "endOffset": 14}, {"referenceID": 9, "context": "Dai et al [23] show that paragraph vectors outperform the vector representations obtained by Latent Dirichlet Allocation (LDA) [10], average of word embeddings and tf-idf weighted one-hot vector representations, on a set of document triplets constructed with the same strategy in [47], using Wikipedia and arXiv documents.", "startOffset": 127, "endOffset": 131}, {"referenceID": 43, "context": "Dai et al [23] show that paragraph vectors outperform the vector representations obtained by Latent Dirichlet Allocation (LDA) [10], average of word embeddings and tf-idf weighted one-hot vector representations, on a set of document triplets constructed with the same strategy in [47], using Wikipedia and arXiv documents.", "startOffset": 280, "endOffset": 284}, {"referenceID": 29, "context": "Goldberg [30] and Cho [18] provide tutorials on getting started with neural networks from the natural language understanding perspective.", "startOffset": 9, "endOffset": 13}, {"referenceID": 17, "context": "Goldberg [30] and Cho [18] provide tutorials on getting started with neural networks from the natural language understanding perspective.", "startOffset": 22, "endOffset": 26}, {"referenceID": 29, "context": "Goldberg [30] covers details on training neural networks and a broader set of architectures including feed-forward networks, convolutional networks, recurrent networks and recursive networks.", "startOffset": 9, "endOffset": 13}, {"referenceID": 17, "context": "Cho [18] focuses on language modeling and machine translation, sketches a clear picture of the encoder-decoder architectures, recurrent networks and attention modules [5].", "startOffset": 4, "endOffset": 8}, {"referenceID": 4, "context": "Cho [18] focuses on language modeling and machine translation, sketches a clear picture of the encoder-decoder architectures, recurrent networks and attention modules [5].", "startOffset": 167, "endOffset": 170}, {"referenceID": 13, "context": "2 Several authors have learned embeddings from query logs [14, 41, 81].", "startOffset": 58, "endOffset": 70}, {"referenceID": 39, "context": "2 Several authors have learned embeddings from query logs [14, 41, 81].", "startOffset": 58, "endOffset": 70}, {"referenceID": 77, "context": "2 Several authors have learned embeddings from query logs [14, 41, 81].", "startOffset": 58, "endOffset": 70}, {"referenceID": 64, "context": "Pre-trained word embeddings It is possible to obtain pre-trained GloVe embeddings3 and CBOW embeddings learned from Bing query logs [68].", "startOffset": 132, "endOffset": 136}, {"referenceID": 69, "context": "Learning word embeddings The source code for GloVe [73]4 and the models introduced in [48]5 is publicly shared by the authors.", "startOffset": 51, "endOffset": 55}, {"referenceID": 44, "context": "Learning word embeddings The source code for GloVe [73]4 and the models introduced in [48]5 is publicly shared by the authors.", "startOffset": 86, "endOffset": 90}, {"referenceID": 52, "context": "Visualizing word embeddings The dimensionality reduction technique t-Distributed Stochastic Neighbor Embedding (t-SNE) [56] is commonly used for visualizing word embedding spaces and for trying to understand the structures learned by neural models.", "startOffset": 119, "endOffset": 123}, {"referenceID": 34, "context": "Specifically, for evaluating neural models for semantic matching in an end-to-end web search task, TREC collections such as ClueWeb [35], .", "startOffset": 132, "endOffset": 136}, {"referenceID": 89, "context": "GOV [93], GOV2 [87], WT10G [77], CLEF collections such as CLEF 2001\u20132003 Ad hoc [85] and CLEF 2003 English Ad hoc [3], as well as logs such as the AOL log [14, 81] and the MSN log [41] have been used.", "startOffset": 4, "endOffset": 8}, {"referenceID": 83, "context": "GOV [93], GOV2 [87], WT10G [77], CLEF collections such as CLEF 2001\u20132003 Ad hoc [85] and CLEF 2003 English Ad hoc [3], as well as logs such as the AOL log [14, 81] and the MSN log [41] have been used.", "startOffset": 15, "endOffset": 19}, {"referenceID": 73, "context": "GOV [93], GOV2 [87], WT10G [77], CLEF collections such as CLEF 2001\u20132003 Ad hoc [85] and CLEF 2003 English Ad hoc [3], as well as logs such as the AOL log [14, 81] and the MSN log [41] have been used.", "startOffset": 27, "endOffset": 31}, {"referenceID": 81, "context": "GOV [93], GOV2 [87], WT10G [77], CLEF collections such as CLEF 2001\u20132003 Ad hoc [85] and CLEF 2003 English Ad hoc [3], as well as logs such as the AOL log [14, 81] and the MSN log [41] have been used.", "startOffset": 80, "endOffset": 84}, {"referenceID": 2, "context": "GOV [93], GOV2 [87], WT10G [77], CLEF collections such as CLEF 2001\u20132003 Ad hoc [85] and CLEF 2003 English Ad hoc [3], as well as logs such as the AOL log [14, 81] and the MSN log [41] have been used.", "startOffset": 114, "endOffset": 117}, {"referenceID": 13, "context": "GOV [93], GOV2 [87], WT10G [77], CLEF collections such as CLEF 2001\u20132003 Ad hoc [85] and CLEF 2003 English Ad hoc [3], as well as logs such as the AOL log [14, 81] and the MSN log [41] have been used.", "startOffset": 155, "endOffset": 163}, {"referenceID": 77, "context": "GOV [93], GOV2 [87], WT10G [77], CLEF collections such as CLEF 2001\u20132003 Ad hoc [85] and CLEF 2003 English Ad hoc [3], as well as logs such as the AOL log [14, 81] and the MSN log [41] have been used.", "startOffset": 155, "endOffset": 163}, {"referenceID": 39, "context": "GOV [93], GOV2 [87], WT10G [77], CLEF collections such as CLEF 2001\u20132003 Ad hoc [85] and CLEF 2003 English Ad hoc [3], as well as logs such as the AOL log [14, 81] and the MSN log [41] have been used.", "startOffset": 180, "endOffset": 184}, {"referenceID": 6, "context": "GPU support and automatic differentiation [7] are crucial features required for training neural networks.", "startOffset": 42, "endOffset": 45}, {"referenceID": 77, "context": ", [81, 83, 84, 93].", "startOffset": 2, "endOffset": 18}, {"referenceID": 79, "context": ", [81, 83, 84, 93].", "startOffset": 2, "endOffset": 18}, {"referenceID": 80, "context": ", [81, 83, 84, 93].", "startOffset": 2, "endOffset": 18}, {"referenceID": 89, "context": ", [81, 83, 84, 93].", "startOffset": 2, "endOffset": 18}, {"referenceID": 89, "context": "For instance, Zuccon et al [93] compute translation probabilities of word pairs in a translation language model retrieval framework with cosine similarity of Skip-Gram vectors.", "startOffset": 27, "endOffset": 31}, {"referenceID": 46, "context": "Neural learning to match Learning to match [50] is the problem of learning a matching function f(x, y) that computes the similarity degree of two objects x and y from two different spaces X and Y .", "startOffset": 43, "endOffset": 47}, {"referenceID": 12, "context": "They are based on siamese neural networks [13], as illustrated in Figure 4, which consist of two identical sub-networks joined at their outputs in order to compute the similarity of the inputs.", "startOffset": 42, "endOffset": 46}, {"referenceID": 24, "context": "5: Architecture of the neural learn to match models adapted from [25].", "startOffset": 65, "endOffset": 69}, {"referenceID": 37, "context": "Learn to predict context The success of word-based neural language models has motivated models for learning representations of larger textual units [39, 47] from unlabelled data.", "startOffset": 148, "endOffset": 156}, {"referenceID": 43, "context": "Learn to predict context The success of word-based neural language models has motivated models for learning representations of larger textual units [39, 47] from unlabelled data.", "startOffset": 148, "endOffset": 156}, {"referenceID": 37, "context": "Among the models reviewed in [39], Skip-thought Vector [44] and Paragraph Vector (PV) [47] are successful representatives of the learn to predict context idea.", "startOffset": 29, "endOffset": 33}, {"referenceID": 43, "context": "Among the models reviewed in [39], Skip-thought Vector [44] and Paragraph Vector (PV) [47] are successful representatives of the learn to predict context idea.", "startOffset": 86, "endOffset": 90}, {"referenceID": 33, "context": "For instance, the context of the query can be defined by the other queries in the session [34].", "startOffset": 90, "endOffset": 94}, {"referenceID": 26, "context": "Finally, context of a document is defined by joining its content and the neighboring documents in a document stream in [27].", "startOffset": 119, "endOffset": 123}, {"referenceID": 31, "context": "Studies in this category are motivated by successful applications of RNNs, LSTM networks, and encoder-decoder architectures, illustrated in Figure 6, to sequence-to-sequence learning [32] tasks such as machine translation [19] and image captioning [86].", "startOffset": 183, "endOffset": 187}, {"referenceID": 18, "context": "Studies in this category are motivated by successful applications of RNNs, LSTM networks, and encoder-decoder architectures, illustrated in Figure 6, to sequence-to-sequence learning [32] tasks such as machine translation [19] and image captioning [86].", "startOffset": 222, "endOffset": 226}, {"referenceID": 82, "context": "Studies in this category are motivated by successful applications of RNNs, LSTM networks, and encoder-decoder architectures, illustrated in Figure 6, to sequence-to-sequence learning [32] tasks such as machine translation [19] and image captioning [86].", "startOffset": 248, "endOffset": 252}, {"referenceID": 49, "context": "Besides, in [53], which falls into learn to generate context category, the generated document is not used in a quantitative experimental framework.", "startOffset": 12, "endOffset": 16}, {"referenceID": 20, "context": "Query-Doc similarity aggregate explicit [21, 68, 85] 5.", "startOffset": 40, "endOffset": 52}, {"referenceID": 64, "context": "Query-Doc similarity aggregate explicit [21, 68, 85] 5.", "startOffset": 40, "endOffset": 52}, {"referenceID": 81, "context": "Query-Doc similarity aggregate explicit [21, 68, 85] 5.", "startOffset": 40, "endOffset": 52}, {"referenceID": 28, "context": "1 implicit [29, 93] 5.", "startOffset": 11, "endOffset": 19}, {"referenceID": 89, "context": "1 implicit [29, 93] 5.", "startOffset": 11, "endOffset": 19}, {"referenceID": 2, "context": "Query similarity aggregate explicit [3, 92] 5.", "startOffset": 36, "endOffset": 43}, {"referenceID": 88, "context": "Query similarity aggregate explicit [3, 92] 5.", "startOffset": 36, "endOffset": 43}, {"referenceID": 25, "context": "1 \u2013 implicit [26, 88] 5.", "startOffset": 13, "endOffset": 21}, {"referenceID": 84, "context": "1 \u2013 implicit [26, 88] 5.", "startOffset": 13, "endOffset": 21}, {"referenceID": 38, "context": "Query-Doc similarity learn learn-to-match [40, 71, 79] 5.", "startOffset": 42, "endOffset": 54}, {"referenceID": 67, "context": "Query-Doc similarity learn learn-to-match [40, 71, 79] 5.", "startOffset": 42, "endOffset": 54}, {"referenceID": 75, "context": "Query-Doc similarity learn learn-to-match [40, 71, 79] 5.", "startOffset": 42, "endOffset": 54}, {"referenceID": 0, "context": "1 learn-to-predict [1, 27] 5.", "startOffset": 19, "endOffset": 26}, {"referenceID": 26, "context": "1 learn-to-predict [1, 27] 5.", "startOffset": 19, "endOffset": 26}, {"referenceID": 49, "context": "Query \u2013 learn learn-to-generate [53] 5.", "startOffset": 32, "endOffset": 36}, {"referenceID": 13, "context": "Query \u2013 aggregate implicit [14] 6.", "startOffset": 27, "endOffset": 31}, {"referenceID": 59, "context": "Query feature learn learn-to-match [63, 64] 6.", "startOffset": 35, "endOffset": 43}, {"referenceID": 60, "context": "Query feature learn learn-to-match [63, 64] 6.", "startOffset": 35, "endOffset": 43}, {"referenceID": 77, "context": "1 learn learn-to-generate [81] 6.", "startOffset": 26, "endOffset": 30}, {"referenceID": 3, "context": "Query-Ad similarity learn learn-to-match [4, 90, 91] 7.", "startOffset": 41, "endOffset": 52}, {"referenceID": 86, "context": "Query-Ad similarity learn learn-to-match [4, 90, 91] 7.", "startOffset": 41, "endOffset": 52}, {"referenceID": 87, "context": "Query-Ad similarity learn learn-to-match [4, 90, 91] 7.", "startOffset": 41, "endOffset": 52}, {"referenceID": 32, "context": "Query similarity learn learn-to-predict [33, 34] 7.", "startOffset": 40, "endOffset": 48}, {"referenceID": 33, "context": "Query similarity learn learn-to-predict [33, 34] 7.", "startOffset": 40, "endOffset": 48}, {"referenceID": 81, "context": "[85] Skipgram Sum of BoEW-IN Sum over BoEW-IN Cosine similarity", "startOffset": 0, "endOffset": 4}, {"referenceID": 64, "context": "[68] CBOW BoEW-IN BoEW-OUT Aggregation of cosine similarities of accross all querydocument pairs", "startOffset": 0, "endOffset": 4}, {"referenceID": 72, "context": "[76] CBOW BoEW-IN Set of clusters in IN Average inter-similarity of query to the set of centroids in the document", "startOffset": 0, "endOffset": 4}, {"referenceID": 65, "context": "[69] CBOW Mean of BoEW-IN N/A Cosine similarity", "startOffset": 0, "endOffset": 4}, {"referenceID": 73, "context": "[77] Word2Vec Mean of BoEW-IN N/A The mean cosine similarity between expansion term and all the terms in Q", "startOffset": 0, "endOffset": 4}, {"referenceID": 64, "context": "The E matrix is referred as IN embeddings whereas the C matrix as OUT embeddings, in accordance with the naming in [68].", "startOffset": 115, "endOffset": 119}, {"referenceID": 64, "context": "Nalisnick et al [68] point out an important feature of CBOW embeddings: the neighbors of a word represented with its IN embedding vector in the OUT space are topically similar words.", "startOffset": 16, "endOffset": 20}, {"referenceID": 64, "context": "Motivated by this observation, Nalisnick et al [68] propose the Dual Embedding Space Model (DESM).", "startOffset": 47, "endOffset": 51}, {"referenceID": 61, "context": "Query-document similarity is computed by aggregating cosine similarities across all the query-document word pairs [65, 68].", "startOffset": 114, "endOffset": 122}, {"referenceID": 64, "context": "Query-document similarity is computed by aggregating cosine similarities across all the query-document word pairs [65, 68].", "startOffset": 114, "endOffset": 122}, {"referenceID": 61, "context": "DESM is evaluated in a document ranking setting with both explicitly judged data sets and implicit feedback based data sets in [65].", "startOffset": 127, "endOffset": 131}, {"referenceID": 81, "context": "In the explicit category, Vuli\u0107 and Moens [85] propose to construct query and document representations as a sum of word embeddings learned from a pseudobilingual document collection with a Skip-Gram model.", "startOffset": 42, "endOffset": 46}, {"referenceID": 72, "context": "In [76], documents are modelled as a mixture distribution that generates the observed terms in the document.", "startOffset": 3, "endOffset": 7}, {"referenceID": 20, "context": "A different explicit aggregation method based on BoEW representations is the Fisher Kernel proposed in [21].", "startOffset": 103, "endOffset": 107}, {"referenceID": 2, "context": "Query expansion and re-weighting ALMasri et al [3] use Skip-Gram and CBOW embeddings for extracting most similar terms for a given query term.", "startOffset": 47, "endOffset": 50}, {"referenceID": 65, "context": "The work on query expansion [69] based on word embeddings, although proposed for result merging in federated web search, can also be noted here since it introduces the idea of an importance vector in the query re-weighting method in [92].", "startOffset": 28, "endOffset": 32}, {"referenceID": 88, "context": "The work on query expansion [69] based on word embeddings, although proposed for result merging in federated web search, can also be noted here since it introduces the idea of an importance vector in the query re-weighting method in [92].", "startOffset": 233, "endOffset": 237}, {"referenceID": 65, "context": "In [69], a query is represented by the mean vector of query term embeddings and k-nearest neighbors of the query vector are selected to expand the query.", "startOffset": 3, "endOffset": 7}, {"referenceID": 88, "context": "Query re-weighting is modelled as a linear regression problem from importance vectors to term weights in [92].", "startOffset": 105, "endOffset": 109}, {"referenceID": 73, "context": "Roy et al [77] propose a set of query expansion methods, based on selecting k nearest neighbors of the query terms in the word embedding space and ranking these terms with respect to their similarity to the whole query.", "startOffset": 10, "endOffset": 14}, {"referenceID": 89, "context": "Zuccon et al [93] compute translation probabilities of word pairs using the cosine similarity of Word2Vec vectors in a translation language model for retrieval.", "startOffset": 13, "endOffset": 17}, {"referenceID": 28, "context": "Cosine similarity of Word2Vec embeddings is used in a similar way in the Generalized Language Model (GLM) [29].", "startOffset": 106, "endOffset": 110}, {"referenceID": 25, "context": "Query expansion In [26, 88], word embeddings are used for defining a new query language model (QLM).", "startOffset": 19, "endOffset": 27}, {"referenceID": 84, "context": "Query expansion In [26, 88], word embeddings are used for defining a new query language model (QLM).", "startOffset": 19, "endOffset": 27}, {"referenceID": 15, "context": "In query expansion with language modeling, the top m terms w that have the highest p(w | q) value are selected as expansion terms [16].", "startOffset": 130, "endOffset": 134}, {"referenceID": 25, "context": "Diaz et al [26], propose a query expansion language model based on word embeddings learned from topic-constrained corpora.", "startOffset": 11, "endOffset": 15}, {"referenceID": 84, "context": "Zamani and Croft [88] propose two QLMs and an extended relevance model [46] based on word embeddings.", "startOffset": 17, "endOffset": 21}, {"referenceID": 42, "context": "Zamani and Croft [88] propose two QLMs and an extended relevance model [46] based on word embeddings.", "startOffset": 71, "endOffset": 75}, {"referenceID": 42, "context": "Besides the QLMs, a relevance model [46], which computes a feedback query language model using embedding similarities in addition to term matching, is introduced.", "startOffset": 36, "endOffset": 40}, {"referenceID": 2, "context": "The proposed query language models are compared to Maximum Likelihood Estimation (MLE), GLM [3, 29] on AP, Robust and GOV2 collections from TREC.", "startOffset": 92, "endOffset": 99}, {"referenceID": 28, "context": "The proposed query language models are compared to Maximum Likelihood Estimation (MLE), GLM [3, 29] on AP, Robust and GOV2 collections from TREC.", "startOffset": 92, "endOffset": 99}, {"referenceID": 81, "context": "[85] SkipGram EuroParl corpus, Aligned EnglishDutch Wikipedia Articles CLEF En-Dutch Collection Unigram LM Unigram LM+LDA Google Translate + Unigram LM +LDA", "startOffset": 0, "endOffset": 4}, {"referenceID": 89, "context": "[93] Skipgram, CBOW AP88-89, WSJ87-92, Wikipedia TREC 6-7-8, DOTGOV, TREC Medical Records Track 20112012 Dirichlet LM, Translation Language Model with Mutual Information", "startOffset": 0, "endOffset": 4}, {"referenceID": 28, "context": "[29] CBOW TREC Document Collection TREC 6-7-8 LM, LDA smoothed LM", "startOffset": 0, "endOffset": 4}, {"referenceID": 61, "context": "[65] CBOW Bing logs Document collection from Bing logs LSA, BM25", "startOffset": 0, "endOffset": 4}, {"referenceID": 84, "context": "[88] Glove Wikipedia 2014 + Gigawords AP, Robust and GOV2 from TREC [3, 29]", "startOffset": 0, "endOffset": 4}, {"referenceID": 2, "context": "[88] Glove Wikipedia 2014 + Gigawords AP, Robust and GOV2 from TREC [3, 29]", "startOffset": 68, "endOffset": 75}, {"referenceID": 28, "context": "[88] Glove Wikipedia 2014 + Gigawords AP, Robust and GOV2 from TREC [3, 29]", "startOffset": 68, "endOffset": 75}, {"referenceID": 88, "context": "[92] CBOW Google News / ClueWeb09B / Retrieval Corpus ROBUST04 WT10g GOV2 ClueWeb09B Sequential dependency Model, Unweighted Queries", "startOffset": 0, "endOffset": 4}, {"referenceID": 2, "context": "[3] CBOW Image2009, Case2011 and Case2012 from CLEF Image2011, Image 2012, Image 2012 and Case2011 from CLEF Expansion with PRF and Mutual Information", "startOffset": 0, "endOffset": 3}, {"referenceID": 25, "context": "[26] CBOW Wikipedia / Gigawords / Retrieval Corpus TREC12, Robust and ClueWeb 2009 Category B QL", "startOffset": 0, "endOffset": 4}, {"referenceID": 72, "context": "[76] CBOW Retrieval Corpus TREC 6-7-8 and Robust LM with Jelinek Mercer Smoothing", "startOffset": 0, "endOffset": 4}, {"referenceID": 84, "context": "Only a recent study [88] presents comparison of the embedding based query language model [88] to the GLM [29] and NLTM [93].", "startOffset": 20, "endOffset": 24}, {"referenceID": 84, "context": "Only a recent study [88] presents comparison of the embedding based query language model [88] to the GLM [29] and NLTM [93].", "startOffset": 89, "endOffset": 93}, {"referenceID": 28, "context": "Only a recent study [88] presents comparison of the embedding based query language model [88] to the GLM [29] and NLTM [93].", "startOffset": 105, "endOffset": 109}, {"referenceID": 89, "context": "Only a recent study [88] presents comparison of the embedding based query language model [88] to the GLM [29] and NLTM [93].", "startOffset": 119, "endOffset": 123}, {"referenceID": 28, "context": "Among the models that fall within the aggregate category, directly using word embeddings provides consistent gains in [29] but not in [26, 68, 88, 89, 93].", "startOffset": 118, "endOffset": 122}, {"referenceID": 25, "context": "Among the models that fall within the aggregate category, directly using word embeddings provides consistent gains in [29] but not in [26, 68, 88, 89, 93].", "startOffset": 134, "endOffset": 154}, {"referenceID": 64, "context": "Among the models that fall within the aggregate category, directly using word embeddings provides consistent gains in [29] but not in [26, 68, 88, 89, 93].", "startOffset": 134, "endOffset": 154}, {"referenceID": 84, "context": "Among the models that fall within the aggregate category, directly using word embeddings provides consistent gains in [29] but not in [26, 68, 88, 89, 93].", "startOffset": 134, "endOffset": 154}, {"referenceID": 85, "context": "Among the models that fall within the aggregate category, directly using word embeddings provides consistent gains in [29] but not in [26, 68, 88, 89, 93].", "startOffset": 134, "endOffset": 154}, {"referenceID": 89, "context": "Among the models that fall within the aggregate category, directly using word embeddings provides consistent gains in [29] but not in [26, 68, 88, 89, 93].", "startOffset": 134, "endOffset": 154}, {"referenceID": 89, "context": "In [93], word embedding similarity achieves comparable effectiveness to mutual informationbased term similarity.", "startOffset": 3, "endOffset": 7}, {"referenceID": 64, "context": "For query-document similarity, Nalisnick et al [68] point out that utilising relations between the IN and OUT embedding spaces learned by CBOW yields a more effective similarity function for query-document pairs.", "startOffset": 47, "endOffset": 51}, {"referenceID": 25, "context": "Diaz et al [26] propose to learn word embeddings from a topically constrained corpora since the word embeddings learned from an unconstrained corpus are found to be too general.", "startOffset": 11, "endOffset": 15}, {"referenceID": 84, "context": "Zamani and Croft [88, 89] apply a sigmoid function on the cosine similarity scores in order to increase the discriminative power.", "startOffset": 17, "endOffset": 25}, {"referenceID": 85, "context": "Zamani and Croft [88, 89] apply a sigmoid function on the cosine similarity scores in order to increase the discriminative power.", "startOffset": 17, "endOffset": 25}, {"referenceID": 89, "context": "The effect of the model choice is only investigated in [93].", "startOffset": 55, "endOffset": 59}, {"referenceID": 88, "context": "Corpus Embeddings learned from a general-purpose corpus like Wikipedia (general purpose embeddings) and embeddings learned from the retrieval corpus itself (corpus-specific word embeddings) are compared in [92, 93].", "startOffset": 206, "endOffset": 214}, {"referenceID": 89, "context": "Corpus Embeddings learned from a general-purpose corpus like Wikipedia (general purpose embeddings) and embeddings learned from the retrieval corpus itself (corpus-specific word embeddings) are compared in [92, 93].", "startOffset": 206, "endOffset": 214}, {"referenceID": 88, "context": "A notable difference is not observed between corpus-specific embeddings and general-purpose word embeddings when used for query-reweighting [92] or in a translation language model for computing term similarities [93].", "startOffset": 140, "endOffset": 144}, {"referenceID": 89, "context": "A notable difference is not observed between corpus-specific embeddings and general-purpose word embeddings when used for query-reweighting [92] or in a translation language model for computing term similarities [93].", "startOffset": 212, "endOffset": 216}, {"referenceID": 25, "context": "However, Diaz et al [26] highlight that query expansion with word embeddings learned from topic-constrained collection of documents, yield higher effectiveness scores compared to embeddings learned from a general-purpose corpora.", "startOffset": 20, "endOffset": 24}, {"referenceID": 89, "context": "Embedding dimension and context-window size Zuccon et al [93] report that embeddings learned using Skip-Gram and CBOW models are robust to different choices of embedding dimensionality and context window size.", "startOffset": 57, "endOffset": 61}, {"referenceID": 81, "context": "A similar observation about effect of embedding size and context-size on retrieval effectiveness is shared by Vuli\u0107 and Moens [85].", "startOffset": 126, "endOffset": 130}, {"referenceID": 81, "context": "The retrieval effectiveness is found to be stable with embedding dimensions greater than 300 and context window size greater than 30 in [85].", "startOffset": 136, "endOffset": 140}, {"referenceID": 38, "context": "The Deep Structured Semantic Model (DSSM) [40] is the earliest neural learn to match model.", "startOffset": 42, "endOffset": 46}, {"referenceID": 38, "context": "DSSM [40] Query-Document Feed Forward Deep Neural Network (DNN)", "startOffset": 5, "endOffset": 9}, {"referenceID": 74, "context": "CLSM [78, 79] Query-Document (Title) Convolutional Neural Network (CNN)", "startOffset": 5, "endOffset": 13}, {"referenceID": 75, "context": "CLSM [78, 79] Query-Document (Title) Convolutional Neural Network (CNN)", "startOffset": 5, "endOffset": 13}, {"referenceID": 67, "context": "LSTMDSSM[71] Query-Document Title Long Short Term Memory (LSTM) Network", "startOffset": 8, "endOffset": 12}, {"referenceID": 74, "context": "The Convolutional Latent Semantic Model (CLSM) [78, 79] and LSTM-DSSM [70, 72] differ from DSSM in the input representations and architecture of the SCN component.", "startOffset": 47, "endOffset": 55}, {"referenceID": 75, "context": "The Convolutional Latent Semantic Model (CLSM) [78, 79] and LSTM-DSSM [70, 72] differ from DSSM in the input representations and architecture of the SCN component.", "startOffset": 47, "endOffset": 55}, {"referenceID": 66, "context": "The Convolutional Latent Semantic Model (CLSM) [78, 79] and LSTM-DSSM [70, 72] differ from DSSM in the input representations and architecture of the SCN component.", "startOffset": 70, "endOffset": 78}, {"referenceID": 68, "context": "The Convolutional Latent Semantic Model (CLSM) [78, 79] and LSTM-DSSM [70, 72] differ from DSSM in the input representations and architecture of the SCN component.", "startOffset": 70, "endOffset": 78}, {"referenceID": 38, "context": "DSSM, CLSM and LSTM-DSSM are evaluated on large-scale real data sets from Bing in [40, 78, 79].", "startOffset": 82, "endOffset": 94}, {"referenceID": 74, "context": "DSSM, CLSM and LSTM-DSSM are evaluated on large-scale real data sets from Bing in [40, 78, 79].", "startOffset": 82, "endOffset": 94}, {"referenceID": 75, "context": "DSSM, CLSM and LSTM-DSSM are evaluated on large-scale real data sets from Bing in [40, 78, 79].", "startOffset": 82, "endOffset": 94}, {"referenceID": 38, "context": "In [40], DSSM is shown to outperform the Word Translation Model, BM25, TF-IDF and Bilingual Topic Models with posterior regularization in terms of NDCG at cutoff values 1, 3 and 10.", "startOffset": 3, "endOffset": 7}, {"referenceID": 75, "context": "CLSM is shown to outperform DSSM in [79].", "startOffset": 36, "endOffset": 40}, {"referenceID": 68, "context": "Finally, LSTM-DSSM outperforms CLSM in [72] when document titles are used instead of full document content.", "startOffset": 39, "endOffset": 43}, {"referenceID": 75, "context": "When document titles are used instead of the full document content, higher NDCG scores are achieved by[79].", "startOffset": 102, "endOffset": 106}, {"referenceID": 68, "context": "For computational reasons, LSTM-DSSM is evaluated only with document titles [72].", "startOffset": 76, "endOffset": 80}, {"referenceID": 50, "context": "Another interesting publication that follows DSSM is by Liu et al [54] who propose a neural model with multi-task objectives.", "startOffset": 66, "endOffset": 70}, {"referenceID": 48, "context": "Li et al [52] utilize distributed representations produced by DSSM and CLSM in order to re-rank documents based on in-session contextual information.", "startOffset": 9, "endOffset": 13}, {"referenceID": 0, "context": "Ai et al [1, 2] investigate the use of the PV-DBOW model as a document language model for retrieval.", "startOffset": 9, "endOffset": 15}, {"referenceID": 1, "context": "Ai et al [1, 2] investigate the use of the PV-DBOW model as a document language model for retrieval.", "startOffset": 9, "endOffset": 15}, {"referenceID": 71, "context": "Secondly, the PV-DBOW model trained with NEG implicitly weights terms with respect to Inverse Corpus Frequencies (ICF) which has been shown to be inferior to Inverse Document Frequency (IDF) in [75].", "startOffset": 194, "endOffset": 198}, {"referenceID": 26, "context": "The Hierarchical Document Vector (HDV) [27] model extends the PV-DM model to predict not only words in a document but also its temporal neighbors in a document stream.", "startOffset": 39, "endOffset": 43}, {"referenceID": 26, "context": "Djuric et al [27] point out that words and documents are embedded in the same space and this makes the model useful for both recommendation and retrieval tasks including document retrieval, document recommendation, document tag recommendation and keyword suggestion.", "startOffset": 13, "endOffset": 17}, {"referenceID": 49, "context": "Lioma et al [53] ask whether it is possible to generate relevant documents given a query.", "startOffset": 12, "endOffset": 16}, {"referenceID": 0, "context": "(a) Two Layer PV-DBOW [1] w1 w2 w4 w5 w3 p1 p2 p4 p5", "startOffset": 22, "endOffset": 25}, {"referenceID": 26, "context": "(b) HDV [27] and context-content2vec [34]", "startOffset": 8, "endOffset": 12}, {"referenceID": 33, "context": "(b) HDV [27] and context-content2vec [34]", "startOffset": 37, "endOffset": 41}, {"referenceID": 38, "context": "Moreover, the models from [40, 71, 79] in the neural learn to match category are evaluated on data sets derived from a commercial search engine and no quantitative evaluation is provided for the learn to predict model HDV [27].", "startOffset": 26, "endOffset": 38}, {"referenceID": 67, "context": "Moreover, the models from [40, 71, 79] in the neural learn to match category are evaluated on data sets derived from a commercial search engine and no quantitative evaluation is provided for the learn to predict model HDV [27].", "startOffset": 26, "endOffset": 38}, {"referenceID": 75, "context": "Moreover, the models from [40, 71, 79] in the neural learn to match category are evaluated on data sets derived from a commercial search engine and no quantitative evaluation is provided for the learn to predict model HDV [27].", "startOffset": 26, "endOffset": 38}, {"referenceID": 26, "context": "Moreover, the models from [40, 71, 79] in the neural learn to match category are evaluated on data sets derived from a commercial search engine and no quantitative evaluation is provided for the learn to predict model HDV [27].", "startOffset": 222, "endOffset": 226}, {"referenceID": 13, "context": "The work by Cai and de Rijke [14] on query auto-completion, introduces semantic features computed using Skip-Gram embeddings, for learning to rank query autocompletion candidates.", "startOffset": 29, "endOffset": 33}, {"referenceID": 75, "context": "The CLSM [79] has been used to learn distributed representations of queries [64] and query reformulations [63].", "startOffset": 9, "endOffset": 13}, {"referenceID": 60, "context": "The CLSM [79] has been used to learn distributed representations of queries [64] and query reformulations [63].", "startOffset": 76, "endOffset": 80}, {"referenceID": 59, "context": "The CLSM [79] has been used to learn distributed representations of queries [64] and query reformulations [63].", "startOffset": 106, "endOffset": 110}, {"referenceID": 60, "context": "Mitra and Craswell [64] train a CLSM model on query prefix-suffix pairs extracted from query logs by segmenting each query at every word boundary.", "startOffset": 19, "endOffset": 23}, {"referenceID": 59, "context": "In [63], a CLSM model is trained on query pairs that are observed in succession in search logs.", "startOffset": 3, "endOffset": 7}, {"referenceID": 57, "context": "This work provides an analysis of CLSM vectors for queries similar to the word embedding space analysis in [61].", "startOffset": 107, "endOffset": 111}, {"referenceID": 59, "context": "Mitra [63] found out that offsets between CLSM query vectors can represent intent transition patterns.", "startOffset": 6, "endOffset": 10}, {"referenceID": 59, "context": "Motivated by this feature of the CLSM vectors, query reformulations are represented as the offset vector from the source query to target query [63].", "startOffset": 143, "endOffset": 147}, {"referenceID": 59, "context": "Mitra [63] uses CLSM vectors to define features to represent the session context and rank suggestion candidates against a prefix in personalized query autocompletion.", "startOffset": 6, "endOffset": 10}, {"referenceID": 77, "context": "The Hieararchical Recurrent Encoder Decoder (HRED) model [81] is a neural model designed for generating the next query based on the session context vector.", "startOffset": 57, "endOffset": 61}, {"referenceID": 3, "context": "Azimi et al [4] use a DSSM model for ad keyword re-writing.", "startOffset": 12, "endOffset": 15}, {"referenceID": 86, "context": "Deep intent [90, 91] Query-Ad Bidirectional RNN/LSTM + Attention Module", "startOffset": 12, "endOffset": 20}, {"referenceID": 87, "context": "Deep intent [90, 91] Query-Ad Bidirectional RNN/LSTM + Attention Module", "startOffset": 12, "endOffset": 20}, {"referenceID": 3, "context": "[4] Ads Keyword Pair Utilizes DSSM", "startOffset": 0, "endOffset": 3}, {"referenceID": 86, "context": "The deep-intent model proposed in Zhai et al [90, 91] comprises a Bidirectional Recurrent Neural Network (BRNN) combined with an attention module as the SCN.", "startOffset": 45, "endOffset": 53}, {"referenceID": 87, "context": "The deep-intent model proposed in Zhai et al [90, 91] comprises a Bidirectional Recurrent Neural Network (BRNN) combined with an attention module as the SCN.", "startOffset": 45, "endOffset": 53}, {"referenceID": 4, "context": "The attention module, first introduced in [5] for neural machine translation, is referred to as attention pooling layer.", "startOffset": 42, "endOffset": 45}, {"referenceID": 33, "context": "context2vec [34] Query A query Other queries (and clicked ads) in the session", "startOffset": 12, "endOffset": 16}, {"referenceID": 33, "context": "contextcontent2vec [34] Query (1) A query (2) The query prefix (1) Other queries (and clicked ads) in the session (2) The last word of the query", "startOffset": 19, "endOffset": 23}, {"referenceID": 33, "context": "8: The context2vec model, after [34].", "startOffset": 32, "endOffset": 36}, {"referenceID": 33, "context": "The context-content2vec model [34], which has the same architecture as the HDV model depicted in Figure 7b, is aimed at learning query embeddings.", "startOffset": 30, "endOffset": 34}, {"referenceID": 33, "context": "Besides the context-content2vec model, the context2vec model illustrated in Figure 8, which predicts only temporal context, is also introduced in [34].", "startOffset": 146, "endOffset": 150}, {"referenceID": 4, "context": "Another future direction would be applications of the attention mechanism [5] for designing models that can predict where a user would attend in document, given a query.", "startOffset": 74, "endOffset": 77}, {"referenceID": 33, "context": "For instance, the context-content2vec model of [34] was evaluated only on matching ads to queries yet the distributed query representations could also be evaluated for query suggestion or query auto completion [15].", "startOffset": 47, "endOffset": 51}, {"referenceID": 14, "context": "For instance, the context-content2vec model of [34] was evaluated only on matching ads to queries yet the distributed query representations could also be evaluated for query suggestion or query auto completion [15].", "startOffset": 210, "endOffset": 214}, {"referenceID": 36, "context": "More broadly, there is a need for systematic and broad task-based experimental surveys that focus on comparative evaluations of models from different categories, but for the same tasks and under the same experimental conditions, very much like the reliable information access (RIA) workshop that was run in the early 2000s to gain a deeper understanding of query expansion and pseudo relevance feedback [37].", "startOffset": 403, "endOffset": 407}, {"referenceID": 49, "context": "The generated textual units have been evaluated through user studies [53, 81].", "startOffset": 69, "endOffset": 77}, {"referenceID": 77, "context": "The generated textual units have been evaluated through user studies [53, 81].", "startOffset": 69, "endOffset": 77}, {"referenceID": 70, "context": ", click-based rankers [74] or, more abstractly, by using models that capture behavioral notions such as examination probability and attractiveness of search results through click models [20].", "startOffset": 22, "endOffset": 26}, {"referenceID": 19, "context": ", click-based rankers [74] or, more abstractly, by using models that capture behavioral notions such as examination probability and attractiveness of search results through click models [20].", "startOffset": 186, "endOffset": 190}], "year": 2016, "abstractText": "The vocabulary mismatch problem is a long-standing problem in information retrieval. Semantic matching holds the promise of solving the problem. Recent advances in language technology have given rise to unsupervised neural models for learning representations of words as well as bigger textual units. Such representations enable powerful semantic matching methods. This survey is meant as an introduction to the use of neural models for semantic matching. To remain focused we limit ourselves to web search. We detail the required background and terminology, a taxonomy grouping the rapidly growing body of work in the area, and then survey work on neural models for semantic matching in the context of three tasks: query suggestion, ad retrieval, and document retrieval. We include a section on resources and best practices that we believe will help readers who are new to the area. We conclude with an assessment of the state-of-the-art and suggestions for future work.", "creator": null}}}