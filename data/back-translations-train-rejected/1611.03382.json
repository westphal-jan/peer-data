{"id": "1611.03382", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Nov-2016", "title": "Efficient Summarization with Read-Again and Copy Mechanism", "abstract": "Encoder-decoder models have been widely used to solve sequence to sequence prediction tasks. However current approaches suffer from two shortcomings. First, the encoders compute a representation of each word taking into account only the history of the words it has read so far, yielding suboptimal representations. Second, current decoders utilize large vocabularies in order to minimize the problem of unknown words, resulting in slow decoding times. In this paper we address both shortcomings. Towards this goal, we first introduce a simple mechanism that first reads the input sequence before committing to a representation of each word. Furthermore, we propose a simple copy mechanism that is able to exploit very small vocabularies and handle out-of-vocabulary words. We demonstrate the effectiveness of our approach on the Gigaword dataset and DUC competition outperforming the state-of-the-art.", "histories": [["v1", "Thu, 10 Nov 2016 16:23:04 GMT  (1087kb,D)", "http://arxiv.org/abs/1611.03382v1", "11 pages, 4 figures, 5 tables"]], "COMMENTS": "11 pages, 4 figures, 5 tables", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["wenyuan zeng", "wenjie luo", "sanja fidler", "raquel urtasun"], "accepted": false, "id": "1611.03382"}, "pdf": {"name": "1611.03382.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Wenyuan Zeng", "Wenjie Luo", "Sanja Fidler", "Raquel Urtasun"], "emails": ["cengwy13@mails.tsinghua.edu.cn", "urtasun}@cs.toronto.edu"], "sections": [{"heading": "1 INTRODUCTION", "text": "This year it is so far that it will be able to erenie.n the aforementioned lcihsrcehnlrc\u00fceF"}, {"heading": "2 RELATED WORK", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 SUMMARIZATION", "text": "Over the past few years, there has been a lot of work in extractive summaries, where a summary is created by composing words or sentences from the source text. Notable examples include Neto et al. (2002), Erkan & Radev (2008), Filippova & Altun (2013) and Colmenares et al. Although there is much less work in extractive nature, the summary is limited to words (sentences) in the source text. Abstractive summaries are aimed at generating consistent summaries aimed at understanding the input text."}, {"heading": "2.2 NEURAL MACHINE TRANSLATION", "text": "Our work is also closely related to recent work on neural machine translation, where neural encoder models have shown promising results (Kalchbrenner & Blunsom (2013); Cho et al. (2014); Sutskever et al. (2014)). Bahdanau et al. (2014) further developed an attention mechanism in the decoder to pay attention to a specific part of the input at each generating time step."}, {"heading": "2.3 OUT-OF-VOCABULARY AND COPY MECHANISM", "text": "Dealing with words outside the vocabulary (OOVs) is an important topic in the sequence of sequence approaches, as we cannot list all possible words and learn how to embed them, as they may not be part of our training set. Luong et al. (2014) address this problem by annotating words at source and aligning OOOVs at the target with these source words. Recently, Vinyals et al. (2015) developed a neural extractive summary model that predicts the targets from the input sequences. Gulcehre et al. (2016); Nallapati et al. (2016) add a hard gate so that the model can decide whether to generate a target word from the fixed-size dictionary or from the input sequence."}, {"heading": "3 THE READ AGAIN MODEL", "text": "The text summary can be formulated as a sequence for predicting the sequence, where the input is a longer text and the output is a summary of that text. In this essay, we develop an encoder decoder approach to the summary. The encoder is used to represent the input text with a set of continuous vectors, and the decoder is used to generate a summary word by word. In the following, we first present our \"read again\" model for encoding sentences. The idea behind our approach is very intuitive and inspired by the way people do the job. When we create summaries, we first read the text, and then a second reading, paying special attention to the words that are relevant to creating the summary. Our \"read again\" model implements this idea by reading the input text twice and using the information from the first reading to distort the second reading. This idea can be seamlessly inserted into LMU and GRU models."}, {"heading": "3.1 ENCODER", "text": "We first check the typical encoder used in machine translation (e.g. Sutskever et al. (2014); Bahdanau et al. (2014). Let's let x = {x1, x2, \u00b7 \u00b7 \u00b7, xn} be the input sequence of words. An encoder reads each word sequentially and generates the hidden representation hi by showing a recursive neural network (RNN) hi = RNN (xi, hi \u2212 1), (1) where xi is the word embedding xi. The hidden vectors h = {h1, h2, \u00b7, hn} are then treated as feature representations for the entire input set and can be used by another RNN to decipher and generate a target sentence. Although RNNNs have proved useful in modeling sequences, one of the main drawbacks is that hi depends only on previous information, i.e. {1, we have to read a single word, xxi, without even having to use it."}, {"heading": "3.1.1 GRU READ-AGAIN", "text": "We read the input sentence {x1, x2, \u00b7 \u00b7, xn} for the first time using a standard GRU h1i = GRU 1 (xi, h 1 i \u2212 1), (2) where the function GRU1 is as, zi = \u03c3 (Wz [xi, h 1 i \u2212 1]) (3) ri = \u03c3 (Wr [xi, h 1 i \u2212 1]) h - 1i = tanh (Wh [xi, ri h1i \u2212 1]) h1i = (1 \u2212 zi) h1i \u2212 1 + zi h - 1iIt consists of two gadgets zi, ri, which control whether the current hidden state h1i \u2212 1 should be copied directly from h1i \u2212 1 or should pass through a more complex path h1i i i. Given the sentence characteristic vector h1n, we then calculate a meaning vector \u03b1i of each word for the second reading."}, {"heading": "3.1.2 LSTM READ-AGAIN", "text": "We now apply the \"Read Again\" idea to the LSTM architecture as shown in Fig. 2 (b). Our first reading is represented by an LSTM1 definition asfi = \u03c3 (Wf [xi, hi \u2212 1]) (7) ii = \u03c3 (Wi [xi, hi \u2212 1]) oi = \u03c3 (Wo [xi, hi \u2212 1]) C-i = tanh (WC [xi, hi \u2212 1]) Ci = ft Ci \u2212 1 + ii C-i hi = oi tanh (Ci). Unlike the GRU architecture, LSTM calculates the hidden state by applying a nonlinear activation function to the cell state Ci, instead of a linear combination of two paths used in the GRU."}, {"heading": "3.1.3 READING MULTIPLE SENTENCES", "text": "In this section, we extend our \"read-again\" model to include the case that the input sequence has more than one sentence. To achieve this, we propose to use a hierarchical representation in which each sentence has its own characteristic vector from the first read. We then combine it into a single vector to distort the second read. We illustrate this in the context of two input sentences, but it is easy to generalize to further sentences. Let {x1, x2, \u00b7, xn} and {x \u2032 1, \u00b7, x \u2032 m} be the two input sentences. The first RNN reads these two sentences independently to obtain two sentence function vectors h1n and h \u2032 1 m. Here, we examine two different ways to deal with multiple sentences. Our first option is to link the two feature vectors globally to distort our second read: h2i = RNh = 1 h, \u00b2 h, h \u00b2 h, h \u00b2 h, h \u00b2 h, h \u00b2 i, h \u00b2 i, h \u00b2, h \u00b2 i, h \u00b2, h \u00b2, h \u00b2 i, h \u00b2, h \u00b2 i, h \u00b2, h \u00b2 i, h \u00b2, h \u00b2 i, h \u00b2, h \u00b2 i, h \u00b2 i, 1, h \u00b2, h \u00b2 i, h \u00b2 i, h \u00b2, h \u00b2."}, {"heading": "3.2 DECODER WITH COPY MECHANISM", "text": "In this paper, only a small number of words are needed to generate a summary that exists in the source text. We can consider this a hybrid approach that combines extractive and abstract summaries."}, {"heading": "3.3 LEARNING", "text": "We jointly learn our encoder and decoder by maximizing the probability of decoding the correct word at each step. For more details, please refer the reader to the experimental evaluation."}, {"heading": "4 EXPERIMENTAL EVALALUATION", "text": "In this section, we present the results of the abstract summary on Gigaword (Graff & Cieri (2003); Napoles et al. (2012)) and DUC2004 (Over et al. (2007)) datasets. Our model can learn a meaningful re-reading weight distribution for each word in the input text by placing more emphasis on important verbs and nous while ignoring common words such as prepositions. As for the decoder, we show that our copying mechanism can successfully reduce the typical word size by a factor of 5 while achieving much better performance than the state of the art, and by a factor of 30 while maintaining the same level of performance. In addition, we offer an analysis and examples of which words are copied during decoding. Dataset and Evaluation Metric: We use the Gigaword corpus to train and evaluate our models. Gigaword is a news corpus in which the title is used as a proxy for the summary of the article."}, {"heading": "4.1 QUANTITATIVE EVALUATION", "text": "Results on Gigaword: We compare the performance of the different architectures and report the results in Tab. < / p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p # p > p # p # p > p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p # p We also observe that adding the copy mechanisms further helps to improve performance: Even if the decoder vocabulary of our copying approach (15K) is much smaller than ABS (69K) and GRU (69U), our ROK score also achieves a higher score."}, {"heading": "4.2 DECODER VOCABULARY SIZE", "text": "Table 3 shows the impact on our model of reducing the decoder vocabulary size from 0.08 seconds per table to 0.08 seconds. We can see that when using the copy mechanism, we can reduce the decoder vocabulary size from 69K to 2K, with only 2-3 points falling on the ROUGE score. This is in contrast to models that do not use the copy mechanism. This may have two reasons: First, our model, when faced with OOVs during the decoding time, can extract their meanings from the input text. Second, our model, equipped with a copy mechanism, can generate OOVs as summary words, maintaining its expressiveness even with a small decoder vocabulary size. Tab. 4 shows the decoding time as a function of the vocabulary size. Since the calculation of Soft-Max is usually the bottleneck for decoding words, the vocabulary model reduces the decoding time per table to 0.08 seconds per table of decoding time, which we can use 0.08 examples of the copy mechanism."}, {"heading": "5 CONCLUSION", "text": "In this paper, we have proposed two simple mechanisms to mitigate the problems of current encoder decoder models: our first contribution is a \"read again\" model that does not represent the input word until the whole sentence is read; our second contribution is a copying mechanism that can handle words from the vocabulary in principle, which allows us to reduce the size of the decoder vocabulary and significantly accelerate inference; we have demonstrated the effectiveness of our approach in the context of the summary and demonstrated the state of the art; in the future, we plan to tackle summary problems with large input text; and we also plan to use our findings in other tasks such as machine translation."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1409.0473,", "citeRegEx": "Bahdanau et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Headline generation based on statistical translation", "author": ["Michele Banko", "Vibhu O Mittal", "Michael J Witbrock"], "venue": "In Proceedings of the 38th Annual Meeting on Association for Computational Linguistics,", "citeRegEx": "Banko et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Banko et al\\.", "year": 2000}, {"title": "Neural summarization by extracting sentences and words", "author": ["Jianpeng Cheng", "Mirella Lapata"], "venue": "arXiv preprint arXiv:1603.07252,", "citeRegEx": "Cheng and Lapata.,? \\Q2016\\E", "shortCiteRegEx": "Cheng and Lapata.", "year": 2016}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "author": ["Kyunghyun Cho", "Bart Van Merri\u00ebnboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1406.1078,", "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Abstractive sentence summarization with attentive recurrent neural networks", "author": ["Sumit Chopra", "Michael Auli", "Alexander M Rush", "SEAS Harvard"], "venue": "arXiv preprint arXiv:1602.06023,", "citeRegEx": "Chopra et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Chopra et al\\.", "year": 2016}, {"title": "Sentence compression beyond word deletion", "author": ["Trevor Cohn", "Mirella Lapata"], "venue": "In Proceedings of the 22nd International Conference on Computational Linguistics-Volume", "citeRegEx": "Cohn and Lapata.,? \\Q2008\\E", "shortCiteRegEx": "Cohn and Lapata.", "year": 2008}, {"title": "Natural language processing (almost) from scratch", "author": ["Ronan Collobert", "Jason Weston", "L\u00e9on Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Collobert et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Heads: Headline generation as sequence prediction using an abstract feature-rich", "author": ["Carlos A Colmenares", "Marina Litvak", "Amin Mantrach", "Fabrizio Silvestri"], "venue": null, "citeRegEx": "Colmenares et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Colmenares et al\\.", "year": 2015}, {"title": "Lexrank: Graph-based lexical centrality as salience in text summarization", "author": ["G\u00fcnes Erkan", "Dragomir R Radev"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Erkan and Radev.,? \\Q2004\\E", "shortCiteRegEx": "Erkan and Radev.", "year": 2004}, {"title": "Overcoming the lack of parallel data in sentence compression", "author": ["Katja Filippova", "Yasemin Altun"], "venue": "In EMNLP,", "citeRegEx": "Filippova and Altun.,? \\Q2013\\E", "shortCiteRegEx": "Filippova and Altun.", "year": 2013}, {"title": "Incorporating copying mechanism in sequence-to-sequence learning", "author": ["Jiatao Gu", "Zhengdong Lu", "Hang Li", "Victor OK Li"], "venue": "arXiv preprint arXiv:1603.06393,", "citeRegEx": "Gu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Gu et al\\.", "year": 2016}, {"title": "Pointing the unknown words", "author": ["Caglar Gulcehre", "Sungjin Ahn", "Ramesh Nallapati", "Bowen Zhou", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1603.08148,", "citeRegEx": "Gulcehre et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Gulcehre et al\\.", "year": 2016}, {"title": "Lcsts: A large scale chinese short text summarization dataset", "author": ["Baotian Hu", "Qingcai Chen", "Fangze Zhu"], "venue": "arXiv preprint arXiv:1506.05865,", "citeRegEx": "Hu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hu et al\\.", "year": 2015}, {"title": "Recurrent continuous translation models", "author": ["Nal Kalchbrenner", "Phil Blunsom"], "venue": "In EMNLP,", "citeRegEx": "Kalchbrenner and Blunsom.,? \\Q2013\\E", "shortCiteRegEx": "Kalchbrenner and Blunsom.", "year": 2013}, {"title": "Addressing the rare word problem in neural machine translation", "author": ["Minh-Thang Luong", "Ilya Sutskever", "Quoc V Le", "Oriol Vinyals", "Wojciech Zaremba"], "venue": "arXiv preprint arXiv:1410.8206,", "citeRegEx": "Luong et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Luong et al\\.", "year": 2014}, {"title": "Abstractive text summarization using sequence-to-sequence rnns and beyond", "author": ["Ramesh Nallapati", "Bowen Zhou", "\u00c7a glar Gul\u00e7ehre", "Bing Xiang"], "venue": null, "citeRegEx": "Nallapati et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Nallapati et al\\.", "year": 2016}, {"title": "Annotated gigaword. In Proceedings of the Joint Workshop on Automatic Knowledge Base Construction and Web-scale Knowledge Extraction, pp. 95\u2013100", "author": ["Courtney Napoles", "Matthew Gormley", "Benjamin Van Durme"], "venue": "Association for Computational Linguistics,", "citeRegEx": "Napoles et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Napoles et al\\.", "year": 2012}, {"title": "Automatic text summarization using a machine learning approach", "author": ["Joel Larocca Neto", "Alex A Freitas", "Celso AA Kaestner"], "venue": "In Brazilian Symposium on Artificial Intelligence,", "citeRegEx": "Neto et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Neto et al\\.", "year": 2002}, {"title": "A neural attention model for abstractive sentence summarization", "author": ["Alexander M Rush", "Sumit Chopra", "Jason Weston"], "venue": "arXiv preprint arXiv:1509.00685,", "citeRegEx": "Rush et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Rush et al\\.", "year": 2015}, {"title": "Bidirectional recurrent neural networks", "author": ["Mike Schuster", "Kuldip K Paliwal"], "venue": "IEEE Transactions on Signal Processing,", "citeRegEx": "Schuster and Paliwal.,? \\Q1997\\E", "shortCiteRegEx": "Schuster and Paliwal.", "year": 1997}, {"title": "Random walk initialization for training very deep feedforward networks", "author": ["David Sussillo", "LF Abbott"], "venue": "arXiv preprint arXiv:1412.6558,", "citeRegEx": "Sussillo and Abbott.,? \\Q2014\\E", "shortCiteRegEx": "Sussillo and Abbott.", "year": 2014}, {"title": "Sequence to sequence learning with neural networks. In Advances in neural information processing", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V Le"], "venue": null, "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Extractive summarization using supervised and semisupervised learning", "author": ["Kam-Fai Wong", "Mingli Wu", "Wenjie Li"], "venue": "In Proceedings of the 22nd International Conference on Computational Linguistics-Volume", "citeRegEx": "Wong et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Wong et al\\.", "year": 2008}, {"title": "Generation with quasi-synchronous grammar", "author": ["Kristian Woodsend", "Yansong Feng", "Mirella Lapata"], "venue": "In Proceedings of the 2010 conference on empirical methods in natural language processing,", "citeRegEx": "Woodsend et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Woodsend et al\\.", "year": 2010}, {"title": "Bbn/umd at duc-2004: Topiary", "author": ["David Zajic", "Bonnie Dorr", "Richard Schwartz"], "venue": "In Proceedings of the HLT-NAACL", "citeRegEx": "Zajic et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Zajic et al\\.", "year": 2004}], "referenceMentions": [{"referenceID": 2, "context": "Encoder-decoder models have been widely used in sequence to sequence tasks such as machine translation (Cho et al. (2014); Sutskever et al.", "startOffset": 104, "endOffset": 122}, {"referenceID": 2, "context": "Encoder-decoder models have been widely used in sequence to sequence tasks such as machine translation (Cho et al. (2014); Sutskever et al. (2014)).", "startOffset": 104, "endOffset": 147}, {"referenceID": 2, "context": "Encoder-decoder models have been widely used in sequence to sequence tasks such as machine translation (Cho et al. (2014); Sutskever et al. (2014)). They consist of an encoder which represents the whole input sequence with a single feature vector. The decoder then takes this representation and generates the desired output sequence. The most successful models are LSTM and GRU as they are much easier to train than vanilla RNNs. In this paper we are interested in summarization where the input sequence is a sentence/paragraph and the output is a summary of the text. Several encoding-decoding approaches have been proposed (Rush et al. (2015); Hu et al.", "startOffset": 104, "endOffset": 645}, {"referenceID": 2, "context": "Encoder-decoder models have been widely used in sequence to sequence tasks such as machine translation (Cho et al. (2014); Sutskever et al. (2014)). They consist of an encoder which represents the whole input sequence with a single feature vector. The decoder then takes this representation and generates the desired output sequence. The most successful models are LSTM and GRU as they are much easier to train than vanilla RNNs. In this paper we are interested in summarization where the input sequence is a sentence/paragraph and the output is a summary of the text. Several encoding-decoding approaches have been proposed (Rush et al. (2015); Hu et al. (2015); Chopra et al.", "startOffset": 104, "endOffset": 663}, {"referenceID": 2, "context": "Encoder-decoder models have been widely used in sequence to sequence tasks such as machine translation (Cho et al. (2014); Sutskever et al. (2014)). They consist of an encoder which represents the whole input sequence with a single feature vector. The decoder then takes this representation and generates the desired output sequence. The most successful models are LSTM and GRU as they are much easier to train than vanilla RNNs. In this paper we are interested in summarization where the input sequence is a sentence/paragraph and the output is a summary of the text. Several encoding-decoding approaches have been proposed (Rush et al. (2015); Hu et al. (2015); Chopra et al. (2016)).", "startOffset": 104, "endOffset": 685}, {"referenceID": 2, "context": "Encoder-decoder models have been widely used in sequence to sequence tasks such as machine translation (Cho et al. (2014); Sutskever et al. (2014)). They consist of an encoder which represents the whole input sequence with a single feature vector. The decoder then takes this representation and generates the desired output sequence. The most successful models are LSTM and GRU as they are much easier to train than vanilla RNNs. In this paper we are interested in summarization where the input sequence is a sentence/paragraph and the output is a summary of the text. Several encoding-decoding approaches have been proposed (Rush et al. (2015); Hu et al. (2015); Chopra et al. (2016)). Despite their success, it is commonly believed that the intermediate feature vectors are limited as they are created by only looking at previous words. This is particularly detrimental when dealing with large input sequences. Bi-directorial RNNs (Schuster & Paliwal (1997); Bahdanau et al.", "startOffset": 104, "endOffset": 960}, {"referenceID": 0, "context": "Bi-directorial RNNs (Schuster & Paliwal (1997); Bahdanau et al. (2014)) try to address this problem by computing two different representations resulting of reading the input sequence left-to-right and right-to-left.", "startOffset": 48, "endOffset": 71}, {"referenceID": 0, "context": "Bi-directorial RNNs (Schuster & Paliwal (1997); Bahdanau et al. (2014)) try to address this problem by computing two different representations resulting of reading the input sequence left-to-right and right-to-left. The final vectors are computed by concatenating the two representations. However, the word representations are computed with limited scope. The decoder employed in all these methods outputs at each time step a distribution over a fixed vocabulary. In practice, this introduces problems with rare words (e.g., proper nouns) which are out of vocabulary. To alleviate this problem, one could potentially increase the size of the decoder vocabulary, but decoding becomes computationally much harder, as one has to compute the soft-max over all possible words. Gulcehre et al. (2016), Nallapati et al.", "startOffset": 48, "endOffset": 795}, {"referenceID": 0, "context": "Bi-directorial RNNs (Schuster & Paliwal (1997); Bahdanau et al. (2014)) try to address this problem by computing two different representations resulting of reading the input sequence left-to-right and right-to-left. The final vectors are computed by concatenating the two representations. However, the word representations are computed with limited scope. The decoder employed in all these methods outputs at each time step a distribution over a fixed vocabulary. In practice, this introduces problems with rare words (e.g., proper nouns) which are out of vocabulary. To alleviate this problem, one could potentially increase the size of the decoder vocabulary, but decoding becomes computationally much harder, as one has to compute the soft-max over all possible words. Gulcehre et al. (2016), Nallapati et al. (2016) and Gu et al.", "startOffset": 48, "endOffset": 820}, {"referenceID": 0, "context": "Bi-directorial RNNs (Schuster & Paliwal (1997); Bahdanau et al. (2014)) try to address this problem by computing two different representations resulting of reading the input sequence left-to-right and right-to-left. The final vectors are computed by concatenating the two representations. However, the word representations are computed with limited scope. The decoder employed in all these methods outputs at each time step a distribution over a fixed vocabulary. In practice, this introduces problems with rare words (e.g., proper nouns) which are out of vocabulary. To alleviate this problem, one could potentially increase the size of the decoder vocabulary, but decoding becomes computationally much harder, as one has to compute the soft-max over all possible words. Gulcehre et al. (2016), Nallapati et al. (2016) and Gu et al. (2016) proposed to use a copy mechanism that dynamically copy the words from the input sequence while decoding.", "startOffset": 48, "endOffset": 841}, {"referenceID": 0, "context": "Bi-directorial RNNs (Schuster & Paliwal (1997); Bahdanau et al. (2014)) try to address this problem by computing two different representations resulting of reading the input sequence left-to-right and right-to-left. The final vectors are computed by concatenating the two representations. However, the word representations are computed with limited scope. The decoder employed in all these methods outputs at each time step a distribution over a fixed vocabulary. In practice, this introduces problems with rare words (e.g., proper nouns) which are out of vocabulary. To alleviate this problem, one could potentially increase the size of the decoder vocabulary, but decoding becomes computationally much harder, as one has to compute the soft-max over all possible words. Gulcehre et al. (2016), Nallapati et al. (2016) and Gu et al. (2016) proposed to use a copy mechanism that dynamically copy the words from the input sequence while decoding. However, they lack the ability to extract proper embeddings of out-of-vocabulary words from the input context. Bahdanau et al. (2014) proposed to use an attention mechanism to emphasize specific parts of the input sentence when generating each word.", "startOffset": 48, "endOffset": 1080}, {"referenceID": 11, "context": "Notable examples are Neto et al. (2002), Erkan & Radev (2004), Wong et al.", "startOffset": 21, "endOffset": 40}, {"referenceID": 11, "context": "Notable examples are Neto et al. (2002), Erkan & Radev (2004), Wong et al.", "startOffset": 21, "endOffset": 62}, {"referenceID": 11, "context": "Notable examples are Neto et al. (2002), Erkan & Radev (2004), Wong et al. (2008), Filippova & Altun (2013) and Colmenares et al.", "startOffset": 21, "endOffset": 82}, {"referenceID": 11, "context": "Notable examples are Neto et al. (2002), Erkan & Radev (2004), Wong et al. (2008), Filippova & Altun (2013) and Colmenares et al.", "startOffset": 21, "endOffset": 108}, {"referenceID": 4, "context": "(2008), Filippova & Altun (2013) and Colmenares et al. (2015). As a consequence of their extractive nature the summary is restricted to words (sentences) in the source text.", "startOffset": 37, "endOffset": 62}, {"referenceID": 4, "context": "(2008), Filippova & Altun (2013) and Colmenares et al. (2015). As a consequence of their extractive nature the summary is restricted to words (sentences) in the source text. Abstractive summarization, on the contrary, aims at generating consistent summaries based on understanding the input text. Although there has been much less work on abstractive methods, they can in principle produce much richer summaries. Abstractive summarization is standardized by the DUC2003 and DUC2004 competitions (Over et al. (2007)).", "startOffset": 37, "endOffset": 515}, {"referenceID": 1, "context": "Some of the prominent approaches on this task includes Banko et al. (2000), Zajic et al.", "startOffset": 55, "endOffset": 75}, {"referenceID": 1, "context": "Some of the prominent approaches on this task includes Banko et al. (2000), Zajic et al. (2004), Cohn & Lapata (2008) and Woodsend et al.", "startOffset": 55, "endOffset": 96}, {"referenceID": 1, "context": "Some of the prominent approaches on this task includes Banko et al. (2000), Zajic et al. (2004), Cohn & Lapata (2008) and Woodsend et al.", "startOffset": 55, "endOffset": 118}, {"referenceID": 1, "context": "Some of the prominent approaches on this task includes Banko et al. (2000), Zajic et al. (2004), Cohn & Lapata (2008) and Woodsend et al. (2010). Among them, the TOPIARY system (Zajic et al.", "startOffset": 55, "endOffset": 145}, {"referenceID": 1, "context": "Some of the prominent approaches on this task includes Banko et al. (2000), Zajic et al. (2004), Cohn & Lapata (2008) and Woodsend et al. (2010). Among them, the TOPIARY system (Zajic et al. (2004)) performs the best in the competitions amongst non neural net based methods.", "startOffset": 55, "endOffset": 198}, {"referenceID": 1, "context": "Some of the prominent approaches on this task includes Banko et al. (2000), Zajic et al. (2004), Cohn & Lapata (2008) and Woodsend et al. (2010). Among them, the TOPIARY system (Zajic et al. (2004)) performs the best in the competitions amongst non neural net based methods. Very recently, the success of deep neural networks in many natural language processing tasks (Collobert et al. (2011)) has inspired new work in abstractive summarization .", "startOffset": 55, "endOffset": 393}, {"referenceID": 1, "context": "Some of the prominent approaches on this task includes Banko et al. (2000), Zajic et al. (2004), Cohn & Lapata (2008) and Woodsend et al. (2010). Among them, the TOPIARY system (Zajic et al. (2004)) performs the best in the competitions amongst non neural net based methods. Very recently, the success of deep neural networks in many natural language processing tasks (Collobert et al. (2011)) has inspired new work in abstractive summarization . Rush et al. (2015) propose a neural attention model with a convolutional encoder to solve this task.", "startOffset": 55, "endOffset": 466}, {"referenceID": 1, "context": "Some of the prominent approaches on this task includes Banko et al. (2000), Zajic et al. (2004), Cohn & Lapata (2008) and Woodsend et al. (2010). Among them, the TOPIARY system (Zajic et al. (2004)) performs the best in the competitions amongst non neural net based methods. Very recently, the success of deep neural networks in many natural language processing tasks (Collobert et al. (2011)) has inspired new work in abstractive summarization . Rush et al. (2015) propose a neural attention model with a convolutional encoder to solve this task. Hu et al. (2015) build a large dataset for Chinese text summarization and propose to feed all hidden states from the encoder into the decoder.", "startOffset": 55, "endOffset": 565}, {"referenceID": 1, "context": "Some of the prominent approaches on this task includes Banko et al. (2000), Zajic et al. (2004), Cohn & Lapata (2008) and Woodsend et al. (2010). Among them, the TOPIARY system (Zajic et al. (2004)) performs the best in the competitions amongst non neural net based methods. Very recently, the success of deep neural networks in many natural language processing tasks (Collobert et al. (2011)) has inspired new work in abstractive summarization . Rush et al. (2015) propose a neural attention model with a convolutional encoder to solve this task. Hu et al. (2015) build a large dataset for Chinese text summarization and propose to feed all hidden states from the encoder into the decoder. More recently, Chopra et al. (2016) extended Rush et al.", "startOffset": 55, "endOffset": 727}, {"referenceID": 1, "context": "Some of the prominent approaches on this task includes Banko et al. (2000), Zajic et al. (2004), Cohn & Lapata (2008) and Woodsend et al. (2010). Among them, the TOPIARY system (Zajic et al. (2004)) performs the best in the competitions amongst non neural net based methods. Very recently, the success of deep neural networks in many natural language processing tasks (Collobert et al. (2011)) has inspired new work in abstractive summarization . Rush et al. (2015) propose a neural attention model with a convolutional encoder to solve this task. Hu et al. (2015) build a large dataset for Chinese text summarization and propose to feed all hidden states from the encoder into the decoder. More recently, Chopra et al. (2016) extended Rush et al. (2015)\u2019s work with an RNN decoder, and Nallapati et al.", "startOffset": 55, "endOffset": 755}, {"referenceID": 1, "context": "Some of the prominent approaches on this task includes Banko et al. (2000), Zajic et al. (2004), Cohn & Lapata (2008) and Woodsend et al. (2010). Among them, the TOPIARY system (Zajic et al. (2004)) performs the best in the competitions amongst non neural net based methods. Very recently, the success of deep neural networks in many natural language processing tasks (Collobert et al. (2011)) has inspired new work in abstractive summarization . Rush et al. (2015) propose a neural attention model with a convolutional encoder to solve this task. Hu et al. (2015) build a large dataset for Chinese text summarization and propose to feed all hidden states from the encoder into the decoder. More recently, Chopra et al. (2016) extended Rush et al. (2015)\u2019s work with an RNN decoder, and Nallapati et al. (2016) proposed an RNN encoder-decoder architecture for summarization.", "startOffset": 55, "endOffset": 811}, {"referenceID": 2, "context": "Our work is also closely related to recent work on neural machine translation, where neural encoderdecoder models have shown promising results (Kalchbrenner & Blunsom (2013); Cho et al. (2014); Sutskever et al.", "startOffset": 175, "endOffset": 193}, {"referenceID": 2, "context": "Our work is also closely related to recent work on neural machine translation, where neural encoderdecoder models have shown promising results (Kalchbrenner & Blunsom (2013); Cho et al. (2014); Sutskever et al. (2014)).", "startOffset": 175, "endOffset": 218}, {"referenceID": 0, "context": "Bahdanau et al. (2014) further developed an attention mechanism in the decoder in order to pay attention to a specific part of the input at every generating time-step.", "startOffset": 0, "endOffset": 23}, {"referenceID": 14, "context": "Luong et al. (2014) address this issue by annotating words on the source, and aligning OOVs in the target with those source words.", "startOffset": 0, "endOffset": 20}, {"referenceID": 14, "context": "Luong et al. (2014) address this issue by annotating words on the source, and aligning OOVs in the target with those source words. Recently, Vinyals et al. (2015) propose Pointer Networks, which calculate a probability distribution over the input sequence instead", "startOffset": 0, "endOffset": 163}, {"referenceID": 10, "context": "Gulcehre et al. (2016); Nallapati et al.", "startOffset": 0, "endOffset": 23}, {"referenceID": 10, "context": "Gulcehre et al. (2016); Nallapati et al. (2016) add a hard gate to allow the model to decide wether to generate a target word from the fixed-size dictionary or from the input sequence.", "startOffset": 0, "endOffset": 48}, {"referenceID": 10, "context": "Gu et al. (2016) use a softmax operation instead of the hard gating.", "startOffset": 0, "endOffset": 17}, {"referenceID": 20, "context": ", Sutskever et al. (2014); Bahdanau et al.", "startOffset": 2, "endOffset": 26}, {"referenceID": 0, "context": "(2014); Bahdanau et al. (2014)).", "startOffset": 8, "endOffset": 31}, {"referenceID": 16, "context": "In this section, we show results of abstractive summarization on Gigaword (Graff & Cieri (2003); Napoles et al. (2012)) and DUC2004 (Over et al.", "startOffset": 97, "endOffset": 119}, {"referenceID": 16, "context": "In this section, we show results of abstractive summarization on Gigaword (Graff & Cieri (2003); Napoles et al. (2012)) and DUC2004 (Over et al. (2007)) datasets.", "startOffset": 97, "endOffset": 152}, {"referenceID": 16, "context": "We follow the same pre-processing steps of Rush et al. (2015), which include filtering, PTB tokenization, lower-casing, replacing digit characters with #, replacing low-frequency words with UNK and extracting the first sentence in each article.", "startOffset": 43, "endOffset": 62}, {"referenceID": 4, "context": "As for evaluation metric, we use full-length F1 score on Rouge-1, Rouge-2 and Rouge-L, following Chopra et al. (2016) and Nallapati et al.", "startOffset": 97, "endOffset": 118}, {"referenceID": 4, "context": "As for evaluation metric, we use full-length F1 score on Rouge-1, Rouge-2 and Rouge-L, following Chopra et al. (2016) and Nallapati et al. (2016), since these metrics are less bias to the outputs\u2019 length than full-length recall scores.", "startOffset": 97, "endOffset": 146}, {"referenceID": 18, "context": "Our baselines include the ABS model of Rush et al. (2015) with its proposed", "startOffset": 39, "endOffset": 58}, {"referenceID": 21, "context": "Models Size Rouge-1 Rouge-2 Rouge-L ZOPIARY (Zajic et al. (2004)) - 25.", "startOffset": 45, "endOffset": 65}, {"referenceID": 16, "context": "12 ABS (Rush et al. (2015)) 69K 26.", "startOffset": 8, "endOffset": 27}, {"referenceID": 16, "context": "12 ABS (Rush et al. (2015)) 69K 26.55 7.06 23.49 ABS+ (Rush et al. (2015)) 69K 28.", "startOffset": 8, "endOffset": 74}, {"referenceID": 4, "context": "81 RAS-LSTM (Chopra et al. (2016)) 69K 27.", "startOffset": 13, "endOffset": 34}, {"referenceID": 4, "context": "81 RAS-LSTM (Chopra et al. (2016)) 69K 27.41 7.69 23.06 RAS-Elman (Chopra et al. (2016)) 69K 28.", "startOffset": 13, "endOffset": 88}, {"referenceID": 4, "context": "81 RAS-LSTM (Chopra et al. (2016)) 69K 27.41 7.69 23.06 RAS-Elman (Chopra et al. (2016)) 69K 28.97 8.26 24.06 big-words-lvt2k-1sent (Nallapati et al. (2016)) 69K 28.", "startOffset": 13, "endOffset": 157}, {"referenceID": 4, "context": "81 RAS-LSTM (Chopra et al. (2016)) 69K 27.41 7.69 23.06 RAS-Elman (Chopra et al. (2016)) 69K 28.97 8.26 24.06 big-words-lvt2k-1sent (Nallapati et al. (2016)) 69K 28.35 9.46 24.59 big-words-lvt5k-1sent (Nallapati et al. (2016)) 200K 28.", "startOffset": 13, "endOffset": 226}, {"referenceID": 18, "context": "Similar to Rush et al. (2015), we train our neural model on the Gigaword training set, and show the models\u2019 performances on DUC2004.", "startOffset": 11, "endOffset": 30}], "year": 2016, "abstractText": "Encoder-decoder models have been widely used to solve sequence to sequence prediction tasks. However current approaches suffer from two shortcomings. First, the encoders compute a representation of each word taking into account only the history of the words it has read so far, yielding suboptimal representations. Second, current decoders utilize large vocabularies in order to minimize the problem of unknown words, resulting in slow decoding times. In this paper we address both shortcomings. Towards this goal, we first introduce a simple mechanism that first reads the input sequence before committing to a representation of each word. Furthermore, we propose a simple copy mechanism that is able to exploit very small vocabularies and handle out-of-vocabulary words. We demonstrate the effectiveness of our approach on the Gigaword dataset and DUC competition outperforming the state-of-the-art.", "creator": "LaTeX with hyperref package"}}}