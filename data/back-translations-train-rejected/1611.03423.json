{"id": "1611.03423", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Nov-2016", "title": "DiffSharp: An AD Library for .NET Languages", "abstract": "DiffSharp is an algorithmic differentiation or automatic differentiation (AD) library for the .NET ecosystem, which is targeted by the C# and F# languages, among others. The library has been designed with machine learning applications in mind, allowing very succinct implementations of models and optimization routines. DiffSharp is implemented in F# and exposes forward and reverse AD operators as general nestable higher-order functions, usable by any .NET language. It provides high-performance linear algebra primitives---scalars, vectors, and matrices, with a generalization to tensors underway---that are fully supported by all the AD operators, and which use a BLAS/LAPACK backend via the highly optimized OpenBLAS library. DiffSharp currently uses operator overloading, but we are developing a transformation-based version of the library using F#'s \"code quotation\" metaprogramming facility. Work on a CUDA-based GPU backend is also underway.", "histories": [["v1", "Thu, 10 Nov 2016 17:50:06 GMT  (10kb)", "http://arxiv.org/abs/1611.03423v1", "Extended abstract presented at the AD 2016 Conference, Sep 2016, Oxford UK"]], "COMMENTS": "Extended abstract presented at the AD 2016 Conference, Sep 2016, Oxford UK", "reviews": [], "SUBJECTS": "cs.MS cs.LG", "authors": ["at{\\i}l{\\i}m g\\\"une\\c{s} baydin", "barak a pearlmutter", "jeffrey mark siskind"], "accepted": false, "id": "1611.03423"}, "pdf": {"name": "1611.03423.pdf", "metadata": {"source": "CRF", "title": "DiffSharp: An AD Library for .NET Languages", "authors": ["At\u0131l\u0131m G\u00fcne\u015f Baydin", "Barak A. Pearlmutter", "Jeffrey Mark Siskind"], "emails": [], "sections": [{"heading": null, "text": "ar Xiv: 161 1,03 423v 1 [cs.M S] 1 0N ov2 016DiffSharp: An AD Library for.NET Languages: At\u0131l\u0131m G\u00c3 \u00bc nes \"Baydin\" Barak A. Pearlmutter \"Jeffrey Mark Siskind \u00a7 April 2016"}, {"heading": "Introduction", "text": "DiffSharp1 is an algorithmic differentiation (AD) library for the.NET ecosystem that targets, among other languages, C # and F #. Developed with machine learning applications in mind [1], the library enables very concise implementations of models and optimization routines. DiffSharp is implemented in F # and presents AD operators as a generally higher-order nestable that can be used by anyone. It provides high-performance linear algebra primitives - scalars, vectors, and matrices, with generalization to tensors in progress - fully supported by all AD operators that use a BLAS / LAPACK backend via the highly optimized OpenBLAS library."}, {"heading": "The .NET platform and F#", "text": "/ / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / /"}, {"heading": "Key features and contributions", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Higher-order functional AD API", "text": "The basic elements of the DiffSharp API are the operations of jacobianv 'and jacobianTv, which correspond to the Jacobian vector product (forward mode) and the Jacobian transpose vector product (reverse mode), respectively. The library provides the differentiation functionality through a superior functional API (Table 1), where the operators accept functions as arguments and return derived functions. For example, for a function f: Rn \u2192 Rm, the function jacobianTv \"is evaluated with type (Rn \u2192 Rm) \u2192 Rn (Rm \u00b7 (Rm \u2192 Rn) and the function value is returned together with another function that can be called repeatedly to calculate the adjoints of the input using reverse mode AD. The API also contains specialized operations (e.g. Hessen vector product) to cover common applications and mutate modular code."}, {"heading": "Nesting", "text": "All AD operators can be curved or nested. For example, the internal implementation of the Hessian operator in DiffSharp using flow can simply be guided by inline Hessian f x = jacobian (grad f) x, which results in a predictive AD evaluation of a function at a point.In another example, we can write z = ddx (x (d (x + y), x = 1), x = 1 in F # aslet z = diff (fun x - > x * (diff (fun y - > x + y) (D 1.)))) (D 1.) This can be written using DiffSharp.Interop, asvar z = AD.Diff (x = > x * AD.Diff (y = > x + y, 1), 1)."}, {"heading": "Linear algebra primitives", "text": "One can automatically handle the derivatives of linear algebra primitives using an \"array-of-structure\" approach, where arrays of AD-enabled scalars would yield correct results for derivatives, albeit with poor performance and high memory usage. This approach was used in DiffSharp up to version 0.7. At this point, the library was rewritten using a \"structure-of-arrays\" approach, where vector and matrix types were internally separated arrays for their primal11http: / / diffsharp.io / DiffSharp / csharp.html 12http: / / diffsharp.github.io / DiffSharp / api-overview.html 13Currently, when using reverse mode, closed variables in the functional argument against the fixed-point operator should be subjected to manual closure conversion. We hope that this constraint will set out multilinear and derivative values, such as the AD-11 library recognizes algebra and AD-derivatives."}, {"heading": "Benchmarks", "text": "We provide benchmarks that measure the AD runtime of the differentiation operations in API.15. The code for the benchmarks is available in the GitHub repository, and we distribute a command-line benchmarking tool with each release. 16 We intend to add memory usage figures to these benchmarks in the upcoming release."}, {"heading": "Current work", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Generalization to tensors", "text": "DiffSharp currently offers scalar (D), vector (DV) and matrix (DM) types. We are working to generalize them to an n-dimensional array type, with capabilities similar to those of the Torch Tensor class17 or the NumPy ndarray."}, {"heading": "Source transformation", "text": "One of the reasons why F # is an interesting language for AD is the advanced metaprogramming functions. The function \"Code Quotes\" [2] allows programmatically 14http: / / diffsharp.github.io / DiffSharp / examples-neuralnetworks.html 15http: / / diffsharp.github.io / DiffSharp / benchmarks.html 16http: / / github.com / DiffSharp / DiffSharp / releases 17http: / / torch7.readthedocs.org / en / rtd / tensor / index.html 18http: / / docs.scipy.org / doc / numpy-1.10.0 / reference / arrays.ndarray.htmlread and abstract syntax trees of functions passed as arguments to generate. The symbolic differentiation module in DiffSharp already uses code quotes. We are developing a source-based transformation function that should be implemented in both API and API."}, {"heading": "GPU backend", "text": "The backend interface we defined during DiffSharp vectorization allows us to connect other computational backends that the user can select to execute his AD code. Our current work on DiffSharp includes implementing a CUDA-based backend that uses cuBLAS for BLAS operations, custom CUDA kernels for non-BLAS operations such as elementary functional applications, and cuDNN for convolution operations."}, {"heading": "The Hype library", "text": "DiffSharp is maintained as a basic library that provides an AD infrastructure for.NET languages regardless of the application domain. Besides building this infrastructure, we are interested in using generalized nested AD to implement machine learning models and algorithms. To this end, we have started to develop the Hype Library 19 that DiffSharp uses. Hype is in the early stages of development and is currently being used as a proof-of-concept for the use of general AD in machine learning. It shows how the combination of nested AD and functional programming enables concise implementations of optimization routines 20 (e.g. stochastic gradient descent, AdaGrad, RMSProp) and forward and recurring neural networks. Upcoming GPU and tensor support in DiffSharp are particularly relevant in this area of application, as they are indispensable for modern deep learning models."}, {"heading": "Conclusions", "text": "Although DiffSharp began as a vehicle for conducting research at the interface of AD and machine learning, it has evolved into an industry-leading AD solution for F # in particular and the cross-platform.NET platform in general. Its functional API, combined with the ability to freely nest constructs, enables the convenient implementation of highly modular AD-based software, as seen in the Hype Library. Our goal is to complete our work on the GPU backend and tensors before September 2016."}, {"heading": "Acknowledgments", "text": "This work has been partially supported by Science Foundation Ireland grant 09 / IN.1 / I2637 and NSF grant 1522954-IIS. Any opinions, findings, conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the sponsors."}], "references": [{"title": "Diffsharp: Automatic differentiation", "author": ["At\u0131l\u0131m G\u00fcne\u015f Baydin", "Barak A. Pearlmutter", "Jeffrey Mark Siskind"], "venue": "library. arXiv:1511.07727,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2015}, {"title": "Leveraging .NET meta-programming components from F#: Integrated queries and interoperable heterogeneous execution", "author": ["Don Syme"], "venue": "Workshop on ML,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2006}, {"title": "Nesting forward-mode AD in a functional framework", "author": ["Jeffrey Mark Siskind", "Barak A. Pearlmutter"], "venue": "Higher-Order and Symbolic Computation,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2008}, {"title": "Reverse-mode AD in a functional framework: Lambda the ultimate backpropagator", "author": ["Barak A. Pearlmutter", "Jeffrey Mark Siskind"], "venue": "ACM Transactions on Programming Languages and Systems (TOPLAS),", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2008}, {"title": "Differentiating fixed point iterations with ADOL-C", "author": ["Sebastian Schlenkirch", "Andrea Walther"], "venue": "Presentation at the 2nd European Workshop on Automatic Differentiation,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2005}, {"title": "Reverse accumulation and attractive fixed points", "author": ["Bruce Christianson"], "venue": "Optimization Methods and Software,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1994}, {"title": "Nesting forward-mode AD in a functional framework", "author": ["Jeffrey Mark Siskind", "Barak A. Pearlmutter"], "venue": "Higher-Order and Symbolic Computation,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2008}, {"title": "Method for computing all occurrences of a compound event from occurrences of primitive events", "author": ["Jeffrey Mark Siskind"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2005}, {"title": "Collected matrix derivative results for forward and reverse mode algorithmic differentiation", "author": ["Mike B. Giles"], "venue": "In Advances in Automatic Differentiation,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2008}, {"title": "GPU-accelerated adjoint algorithmic differentiation", "author": ["Felix Gremse", "Andreas H\u00f6fter", "Lukas Razik", "Fabian Kiessling", "Uwe Naumann"], "venue": "Computer Physics Communications,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2016}], "referenceMentions": [{"referenceID": 0, "context": "The library has been designed with machine learning applications in mind [1], allowing very succinct implementations of models and optimization routines.", "startOffset": 73, "endOffset": 76}, {"referenceID": 1, "context": "DiffSharp currently uses operator overloading, but we are developing a transformationbased version of the library using F#\u2019s \u201ccode quotation\u201d metaprogramming facility [2].", "startOffset": 167, "endOffset": 170}, {"referenceID": 1, "context": "NET and maintains a degree of compatibility with OCaml [2].", "startOffset": 55, "endOffset": 58}, {"referenceID": 2, "context": "com/Functional-AutoDiff/dysvunctional-language [3, 4].", "startOffset": 47, "endOffset": 53}, {"referenceID": 3, "context": "com/Functional-AutoDiff/dysvunctional-language [3, 4].", "startOffset": 47, "endOffset": 53}, {"referenceID": 4, "context": "DiffSharp provides a fixed-point-iteration operator, with appropriate forward and reverse AD rules [5].", "startOffset": 99, "endOffset": 102}, {"referenceID": 5, "context": "The forward mode is handled by iterating until convergence of both the primal and the tangent values, while reverse mode uses the \u201ctwo-phases\u201d strategy [6].", "startOffset": 152, "endOffset": 155}, {"referenceID": 6, "context": "Correctness of AD in the presence of nesting requires avoiding perturbation confusion [7].", "startOffset": 86, "endOffset": 89}, {"referenceID": 7, "context": "See [8, 3, 4, 9] for further discussion.", "startOffset": 4, "endOffset": 16}, {"referenceID": 2, "context": "See [8, 3, 4, 9] for further discussion.", "startOffset": 4, "endOffset": 16}, {"referenceID": 3, "context": "See [8, 3, 4, 9] for further discussion.", "startOffset": 4, "endOffset": 16}, {"referenceID": 8, "context": "and derivative values, and the library recognizes linear algebra operations such as matrix multiplication as intrinsic functions [10].", "startOffset": 129, "endOffset": 133}, {"referenceID": 9, "context": "[11].", "startOffset": 0, "endOffset": 4}, {"referenceID": 1, "context": "The \u201ccode quotations\u201d feature [2] allows one to programmatically", "startOffset": 30, "endOffset": 33}], "year": 2016, "abstractText": "DiffSharp is an algorithmic differentiation (AD) library for the .NET ecosystem, which is targeted by the C# and F# languages, among others. The library has been designed with machine learning applications in mind [1], allowing very succinct implementations of models and optimization routines. DiffSharp is implemented in F# and exposes forward and reverse AD operators as general nestable higher-order functions, usable by any .NET language. It provides high-performance linear algebra primitives\u2014scalars, vectors, and matrices, with a generalization to tensors underway\u2014that are fully supported by all the AD operators, and which use a BLAS/LAPACK backend via the highly optimized OpenBLAS library. DiffSharp currently uses operator overloading, but we are developing a transformationbased version of the library using F#\u2019s \u201ccode quotation\u201d metaprogramming facility [2]. Work on a CUDA-based GPU backend is also underway.", "creator": "LaTeX with hyperref package"}}}