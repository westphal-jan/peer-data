{"id": "1611.03477", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Nov-2016", "title": "Song From PI: A Musically Plausible Network for Pop Music Generation", "abstract": "We present a novel framework for generating pop music. Our model is a hierarchical Recurrent Neural Network, where the layers and the structure of the hierarchy encode our prior knowledge about how pop music is composed. In particular, the bottom layers generate the melody, while the higher levels produce the drums and chords. We conduct several human studies that show strong preference of our generated music over that produced by the recent method by Google. We additionally show two applications of our framework: neural dancing and karaoke, as well as neural story singing.", "histories": [["v1", "Thu, 10 Nov 2016 20:35:47 GMT  (1278kb,D)", "http://arxiv.org/abs/1611.03477v1", "under review at ICLR 2017"]], "COMMENTS": "under review at ICLR 2017", "reviews": [], "SUBJECTS": "cs.SD cs.AI", "authors": ["hang chu", "raquel urtasun", "sanja fidler"], "accepted": false, "id": "1611.03477"}, "pdf": {"name": "1611.03477.pdf", "metadata": {"source": "CRF", "title": "SONG FROM PI: A MUSICALLY PLAUSIBLE NETWORK FOR POP MUSIC GENERATION", "authors": ["Hang Chu", "Raquel Urtasun", "Sanja Fidler"], "emails": ["chuhang1122@cs.toronto.edu", "urtasun@cs.toronto.edu", "fidler@cs.toronto.edu"], "sections": [{"heading": "1 INTRODUCTION", "text": "Neural networks have revolutionized many areas, not only proving to be powerful in performing perceptual tasks such as image classification and language understanding, but have also proved to be surprisingly good \"artists.\" In Gatys et al. (2015), photos were turned into paintings by producing certain styles of signs such as Van Goghs, Kiros et al. (2015) stories about images that are biased by writing style (e.g., love songs), Karpathy et al. (2016) wrote Shakespeare-inspired novels, and Simo-Serra et al. (2015) gave fashion advice. Music composition is another artistic domain in which neural approaches were proposed; early approaches that exploit neural networks (Bharucha & Todd 1989); Mozer & Miikkulainen (2001); Eck & Schmidhuber (2002). The most important variations between the different models are the representation of notes and the outputs they produce."}, {"heading": "2 RELATED WORK", "text": "Music generation has been an active field of research for decades, bringing together machines that researchers learn to grasp the complex structure of music (Eck & Schmidhuber (2002); Boulanger-lewandowski et al. (2012), as well as music professionals (Chan et al. (2006)) and enthusiasts (Johnson; Sun) who want to see how far a computer is capable of becoming a real composer. Real-time music generations are also being explored for gaming (Engels et al. (2015)). Early approaches have mostly brought knowledge from music theory to the generation by using rules on how music segments can be sewn together in a plausible way, e.g. Chan et al. (2006)."}, {"heading": "3 CONCEPTS FROM MUSIC THEORY", "text": "We begin with the introduction of the basic notation and definitions from music theory. A note defines the basic unit that music consists of. Music follows the 12-tone staff, i.e., 12 is the cycle length of all notes. The 12 notes are: C, C] / D [, D, D] / E [, E, F, F] / G [, G, G] / A [, A, A] / B [, B. A bar is a short time segment that corresponds to a certain number of bars (notes). The borders of the bar are indicated by vertical bar lines. The scale is a subset of notes. There are four types of scales that are most commonly used: Major (Minor), Harmonic Minor, Melodic Minor Blues. Each scale type specifies a sequence of relative intervals (or shifts) that act relative to the initial note."}, {"heading": "4 HIERARCHICAL RECURRENT NETWORKS FOR POP MUSIC GENERATION", "text": "Specifically, we create music using a hierarchical Recurrent Neural Network, in which the levels and structure of the hierarchy encode our previous knowledge of how pop music is composed. First, we sketch the model and describe the details and rationale for our decisions in the following subsections. We condition our generation to the scale type, as this helps the model pick up the regularities in pop songs. We encode melody with two random variables in each time step, which represent which key is played (the key key) and the duration in which the key is pressed (the pressure key). The melody is created on the scale that is not typically different from the song as in pop music. We assume that the drums and chords are independent of the melody. Thus, conditioned to the melody melody, we create the chord (the chord register) and the drums (the final layer) of the output in all the drums."}, {"heading": "4.1 THE ROLE OF SCALE", "text": "It is well known from music theory that while in principle each song has 12 notes to choose from, most notes actually only use the six (for blues) or seven (for other scales) tone subsets specified by the scale rule. We found that by conditioning the music generator on scale, it more easily captures these regularities. As the scale is defined relative to a starting note, we do not initially attempt to determine its influence and normalize all songs to have identical starting notes. To identify the scale of a song, we calculate the histogram of the 12 notes and match it with the 48 tone subsets of 4 scale types with 12 different starting notes. We then normalize all songs to have the initial note C by applying a constant shift of all notes. This allows us to categorize each song into 4 keys and associate it with the 48 tone subsets of 4 scale types of 4 scales with 12 different starting notes."}, {"heading": "4.2 TWO-LAYER RNN FOR MELODY GENERATION", "text": "In fact, it is the case that it is a matter of a way in which people move in the most diverse regions of the world. (...) In fact, it is the case that people live in the different regions of the world in which they live, in the most diverse regions of the world. (...) It is as if people live in the most diverse regions of the world. (...) It is as if people live in the world in which they live. (...) It is as if they live in the world in which they live, in which they live, in which they live, in which they live, live. (...) It is as if they live in the world in which they live. (...) It is as if they live in the world, as if they live in the world in which they live. (...) It is as if they live in the world, if they live in the world. (...) It is as if they live in the world in which they live. (...) It is as if they live in the world in which they live."}, {"heading": "4.3 CHORD AND DRUM RNN LAYERS", "text": "We examined all existing chords in our 100 hours of pop music. Although a chord can in principle be any combination of several notes, we observed that in the actual music data, 99.19% of the chords belong to one of 72 chord classes (6 types x 12 initial notes). Fig. 3 shows the correlation between the tone of the melody and the initial note of the simultaneously playing chord. It is evident that the chord is strongly correlated with the melody. These two insights inspire our design. Therefore, we present chord ytchd as a uniform encoding of 72 classes and predict it using a two-layered LSTM with a 512-dimensional hidden state. We create a chord at each step. The input is yt \u2212 4chd linked to yt \u2212 3: tkey. We look at our music data set and find all unique drum patterns with a duration of half-measure \u2212. We then calculate the histogram of all two-dimensional patterns."}, {"heading": "4.4 LEARNING", "text": "We follow the typical training strategy, where we make predictions in each shift and time step, but feed information about the ground truth into the next. This effectively decomposes the training and makes it possible to train all shifts in parallel. We use the Adam Optimizer, a learning rate of 2e-3 and a learning rate of 0.99 after each epoch for 10 epochs."}, {"heading": "4.5 MUSIC SYNTHESIS: PUTTING ALL THE OUTPUTS TOGETHER", "text": "To synthesize music, we first randomly select a scale and profile xprf. To generate xprf, we randomly select a cluster ID with a random duration and repeat it until we get the desired total length of the music sequence. We then perform conclusions in our model, which is conditioned on the selected scale, and use xprf as input for our key level. At each step, we try out a key key key by P (ytkey). We encode it as a uniform vector and go to the press, chord and drum layers. We try the press, chords and drums in a similar way at each step. Before we merge the outputs across layers, we further adjust the generated sequences at the bar level. In the melody, we first check at each bar whether the first step is a continuation of a previous note or silence. If it is the last, we find the first newly pressed note within the bar and move it to the beginning of the bar."}, {"heading": "5 EXPERIMENTS", "text": "In fact, most people who are able, are able, are able to determine for themselves what they want to do and what they want to do."}, {"heading": "6 APPLICATIONS", "text": "In this section we show two new applications of our pop music generation framework: For the music videos, we refer the reader to http: / / www.cs.toronto.edu / songfrompi /."}, {"heading": "6.1 NEURAL DANCING AND KARAOKE", "text": "In our first application, we try to create both music and a stickman who dances to it, as well as a sequence of karaoke-like text that people can sing along with. To learn the relationship between music and dance, we download 1 hour of video from the game Just Dance, as well as the MIDI files for songs included in the video from various sources. We use the method in Newell et al. (2016) to track the 2D pose sequence in 3D. We observe that our processing pipeline is able to extract reasonable human poses over time, and then use the method from Zhou et al. (2016) to convert the 2D pose sequence to 3D. Exemplary results will be shown in Fig. 5. We observe that our processing pipeline is able to extract most of the time of human poses. However, the quality is not perfect because we define the failure or the video effects as clusters of all joints."}, {"heading": "6.2 NEURAL STORY SINGING", "text": "In this application, our goal is to sing a song over a photo. We first create a story about the photo with the neural narrator Kiros et al. (2015) and try to accompany the generated text with music. We use the same 1-hour dataset with temporally oriented texts and music. We also add the phoneme list to our 3390 vocabulary because we also want to sing the story. Based on the text produced by the neural narrator, we arrange it into a temporal sequence with 1 bar per word and a short pause at the end of the sentence, where the pause length is specified in such a way that the next sentence assumes a new bar. Since our dataset is relatively small, we create the profile conditioned on the text, which has fewer dimensions compared to the key. This is done by a 2-layer LSTM, which takes the generated profile as input the last time, we associate the generated profile with a uniform profile of the given vector of the current word pitch, and then we specify the actual pitch of the current song, whereby we then specify the actual pitch of the song."}, {"heading": "7 CONCLUSION AND FUTURE WORK", "text": "We have presented a hierarchical approach to generating pop songs that uses music theory in model design; unlike previous work, our approach is capable of generating multi-track music; our human studies show the strength of our framework versus an existing strong baseline; we have also proposed two new applications: neural dancing & karaoke and neural storytelling; next, we will discuss the limitations and possibilities of future work; since most existing approaches are the goal of our method of learning music at the note level, this may be unsuitable for music because music is flexible and deliberately designed to be unpredictable when composing; this requires a deeper study of music theory, since in this essay we are only scratching the surface."}], "references": [{"title": "Modeling the perception of tonal structure with neural nets", "author": ["Jamshed J. Bharucha", "Peter M. Todd"], "venue": "Computer Music Journal,", "citeRegEx": "Bharucha and Todd.,? \\Q1989\\E", "shortCiteRegEx": "Bharucha and Todd.", "year": 1989}, {"title": "Modeling temporal dependencies in high-dimensional sequences: Application to polyphonic music generation and transcription", "author": ["Nicolas Boulanger-lewandowski", "Yoshua Bengio", "Pascal Vincent"], "venue": "In ICML,", "citeRegEx": "Boulanger.lewandowski et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Boulanger.lewandowski et al\\.", "year": 2012}, {"title": "Improving algorithmic music composition with machine learning", "author": ["Michael Chan", "John Potter", "Emery Schubert"], "venue": "In 9th International Conference on Music Perception and Cognition,", "citeRegEx": "Chan et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Chan et al\\.", "year": 2006}, {"title": "Creating melodies with evolving recurrent neural networks", "author": ["Chun-Chi J. Chen", "Risto Miikkulainen"], "venue": "In International Joint Conference on Neural Networks,", "citeRegEx": "Chen and Miikkulainen.,? \\Q2001\\E", "shortCiteRegEx": "Chen and Miikkulainen.", "year": 2001}, {"title": "A first look at music composition using lstm recurrent neural networks", "author": ["Douglas Eck", "Juergen Schmidhuber"], "venue": null, "citeRegEx": "Eck and Schmidhuber.,? \\Q2002\\E", "shortCiteRegEx": "Eck and Schmidhuber.", "year": 2002}, {"title": "Automatic real-time music generation for games", "author": ["Steve Engels", "Fabian Chan", "Tiffany Tong"], "venue": "In AIIDE Workshop,", "citeRegEx": "Engels et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Engels et al\\.", "year": 2015}, {"title": "Clustering by passing messages between data points", "author": ["Brendan J Frey", "Delbert Dueck"], "venue": null, "citeRegEx": "Frey and Dueck.,? \\Q2007\\E", "shortCiteRegEx": "Frey and Dueck.", "year": 2007}, {"title": "A neural algorithm of artistic style", "author": ["Leon A. Gatys", "Alexander S. Ecker", "Matthias Bethge"], "venue": "In arXiv:1508.06576,", "citeRegEx": "Gatys et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gatys et al\\.", "year": 2015}, {"title": "Deep learning for music", "author": ["Allen Huang", "Raymond Wu"], "venue": "arXiv preprint arXiv:1606.04930,", "citeRegEx": "Huang and Wu.,? \\Q2016\\E", "shortCiteRegEx": "Huang and Wu.", "year": 2016}, {"title": "Automatic Music Generation and Machine Learning Based Evaluation, pp. 436\u2013443", "author": ["Semin Kang", "Soo-Yol Ok", "Young-Min Kang"], "venue": null, "citeRegEx": "Kang et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Kang et al\\.", "year": 2012}, {"title": "Visualizing and understanding recurrent networks", "author": ["Andrej Karpathy", "Justin Johnson", "Li Fei-Fei"], "venue": "In ICLR 2016 Workshop,", "citeRegEx": "Karpathy et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Karpathy et al\\.", "year": 2016}, {"title": "Skip-thought vectors", "author": ["Ryan Kiros", "Yukun Zhu", "Ruslan R Salakhutdinov", "Richard Zemel", "Raquel Urtasun", "Antonio Torralba", "Sanja Fidler"], "venue": "In NIPS,", "citeRegEx": "Kiros et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kiros et al\\.", "year": 2015}, {"title": "Neural network music composition by prediction: Exploring the benefits of psychoacoustic constraints and multi-scale processing", "author": ["Michael C. Mozer"], "venue": "Connection Science,", "citeRegEx": "Mozer.,? \\Q1996\\E", "shortCiteRegEx": "Mozer.", "year": 1996}, {"title": "Stacked hourglass networks for human pose estimation", "author": ["Alejandro Newell", "Kaiyu Yang", "Jia Deng"], "venue": "In ECCV,", "citeRegEx": "Newell et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Newell et al\\.", "year": 2016}, {"title": "Style and idea", "author": ["Arnold Schoenberg", "Dika Newlin"], "venue": "Technical report, Williams and Norgate London,", "citeRegEx": "Schoenberg and Newlin.,? \\Q1951\\E", "shortCiteRegEx": "Schoenberg and Newlin.", "year": 1951}, {"title": "Neuroaesthetics in fashion: Modeling the perception of beauty", "author": ["Edgar Simo-Serra", "Sanja Fidler", "Francesc Moreno-Noguer", "Raquel Urtasun"], "venue": "In CVPR,", "citeRegEx": "Simo.Serra et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Simo.Serra et al\\.", "year": 2015}, {"title": "Sparseness meets deepness: 3d human pose estimation from monocular video", "author": ["Xiaowei Zhou", "Menglong Zhu", "Spyridon Leonardos", "Kosta Derpanis", "Kostas Daniilidis"], "venue": null, "citeRegEx": "Zhou et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Zhou et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 6, "context": "In Gatys et al. (2015), photos were turned into paintings by exploiting particular drawing styles such as Van Gogh\u2019s, Kiros et al.", "startOffset": 3, "endOffset": 23}, {"referenceID": 6, "context": "In Gatys et al. (2015), photos were turned into paintings by exploiting particular drawing styles such as Van Gogh\u2019s, Kiros et al. (2015) produced stories about images biased by writing style (e.", "startOffset": 3, "endOffset": 138}, {"referenceID": 6, "context": "In Gatys et al. (2015), photos were turned into paintings by exploiting particular drawing styles such as Van Gogh\u2019s, Kiros et al. (2015) produced stories about images biased by writing style (e.g., romance books), Karpathy et al. (2016) wrote Shakespeare inspired novels, and Simo-Serra et al.", "startOffset": 3, "endOffset": 238}, {"referenceID": 6, "context": "In Gatys et al. (2015), photos were turned into paintings by exploiting particular drawing styles such as Van Gogh\u2019s, Kiros et al. (2015) produced stories about images biased by writing style (e.g., romance books), Karpathy et al. (2016) wrote Shakespeare inspired novels, and Simo-Serra et al. (2015) gave fashion advice.", "startOffset": 3, "endOffset": 302}, {"referenceID": 6, "context": "In Gatys et al. (2015), photos were turned into paintings by exploiting particular drawing styles such as Van Gogh\u2019s, Kiros et al. (2015) produced stories about images biased by writing style (e.g., romance books), Karpathy et al. (2016) wrote Shakespeare inspired novels, and Simo-Serra et al. (2015) gave fashion advice. Music composition is another artistic domain where neural based approaches have been proposed. Early approaches exploiting Recurrent Neural Networks (Bharucha & Todd (1989); Mozer (1996); Chen & Miikkulainen (2001); Eck & Schmidhuber (2002)) date back to the 80\u2019s.", "startOffset": 3, "endOffset": 496}, {"referenceID": 6, "context": "In Gatys et al. (2015), photos were turned into paintings by exploiting particular drawing styles such as Van Gogh\u2019s, Kiros et al. (2015) produced stories about images biased by writing style (e.g., romance books), Karpathy et al. (2016) wrote Shakespeare inspired novels, and Simo-Serra et al. (2015) gave fashion advice. Music composition is another artistic domain where neural based approaches have been proposed. Early approaches exploiting Recurrent Neural Networks (Bharucha & Todd (1989); Mozer (1996); Chen & Miikkulainen (2001); Eck & Schmidhuber (2002)) date back to the 80\u2019s.", "startOffset": 3, "endOffset": 510}, {"referenceID": 6, "context": "In Gatys et al. (2015), photos were turned into paintings by exploiting particular drawing styles such as Van Gogh\u2019s, Kiros et al. (2015) produced stories about images biased by writing style (e.g., romance books), Karpathy et al. (2016) wrote Shakespeare inspired novels, and Simo-Serra et al. (2015) gave fashion advice. Music composition is another artistic domain where neural based approaches have been proposed. Early approaches exploiting Recurrent Neural Networks (Bharucha & Todd (1989); Mozer (1996); Chen & Miikkulainen (2001); Eck & Schmidhuber (2002)) date back to the 80\u2019s.", "startOffset": 3, "endOffset": 538}, {"referenceID": 6, "context": "In Gatys et al. (2015), photos were turned into paintings by exploiting particular drawing styles such as Van Gogh\u2019s, Kiros et al. (2015) produced stories about images biased by writing style (e.g., romance books), Karpathy et al. (2016) wrote Shakespeare inspired novels, and Simo-Serra et al. (2015) gave fashion advice. Music composition is another artistic domain where neural based approaches have been proposed. Early approaches exploiting Recurrent Neural Networks (Bharucha & Todd (1989); Mozer (1996); Chen & Miikkulainen (2001); Eck & Schmidhuber (2002)) date back to the 80\u2019s.", "startOffset": 3, "endOffset": 564}, {"referenceID": 1, "context": "The exception is Boulanger-lewandowski et al. (2012) which generated polyphonic music, i.", "startOffset": 17, "endOffset": 53}, {"referenceID": 11, "context": "As part of the first application we generate a stickman dancing to our music and lyrics that can be sung with, while in the second application we condition on the output of Kiros et al. (2015) which writes a story about an image and convert it into a pop song.", "startOffset": 173, "endOffset": 193}, {"referenceID": 1, "context": "It brings together machines learning researchers that aim to capture the complex structure of music (Eck & Schmidhuber (2002); Boulanger-lewandowski et al. (2012)), as well as music professionals (Chan et al.", "startOffset": 127, "endOffset": 163}, {"referenceID": 1, "context": "It brings together machines learning researchers that aim to capture the complex structure of music (Eck & Schmidhuber (2002); Boulanger-lewandowski et al. (2012)), as well as music professionals (Chan et al. (2006)) and enthusiasts (Johnson; Sun) that want to see how far a computer can get to be a real composer.", "startOffset": 127, "endOffset": 216}, {"referenceID": 1, "context": "It brings together machines learning researchers that aim to capture the complex structure of music (Eck & Schmidhuber (2002); Boulanger-lewandowski et al. (2012)), as well as music professionals (Chan et al. (2006)) and enthusiasts (Johnson; Sun) that want to see how far a computer can get to be a real composer. Real-time music generation is also explored for gaming (Engels et al. (2015)).", "startOffset": 127, "endOffset": 392}, {"referenceID": 1, "context": "It brings together machines learning researchers that aim to capture the complex structure of music (Eck & Schmidhuber (2002); Boulanger-lewandowski et al. (2012)), as well as music professionals (Chan et al. (2006)) and enthusiasts (Johnson; Sun) that want to see how far a computer can get to be a real composer. Real-time music generation is also explored for gaming (Engels et al. (2015)). Early approaches mostly instilled knowledge from music theory into generation, by using rules of how music segments can be stitched together in a plausible way, e.g., Chan et al. (2006). On the other hand, neural networks have been used for music generation since the 80\u2019s (Bharucha & Todd (1989); Mozer (1996); Chen & Miikkulainen (2001); Eck & Schmidhuber (2002)).", "startOffset": 127, "endOffset": 580}, {"referenceID": 1, "context": "It brings together machines learning researchers that aim to capture the complex structure of music (Eck & Schmidhuber (2002); Boulanger-lewandowski et al. (2012)), as well as music professionals (Chan et al. (2006)) and enthusiasts (Johnson; Sun) that want to see how far a computer can get to be a real composer. Real-time music generation is also explored for gaming (Engels et al. (2015)). Early approaches mostly instilled knowledge from music theory into generation, by using rules of how music segments can be stitched together in a plausible way, e.g., Chan et al. (2006). On the other hand, neural networks have been used for music generation since the 80\u2019s (Bharucha & Todd (1989); Mozer (1996); Chen & Miikkulainen (2001); Eck & Schmidhuber (2002)).", "startOffset": 127, "endOffset": 691}, {"referenceID": 1, "context": "It brings together machines learning researchers that aim to capture the complex structure of music (Eck & Schmidhuber (2002); Boulanger-lewandowski et al. (2012)), as well as music professionals (Chan et al. (2006)) and enthusiasts (Johnson; Sun) that want to see how far a computer can get to be a real composer. Real-time music generation is also explored for gaming (Engels et al. (2015)). Early approaches mostly instilled knowledge from music theory into generation, by using rules of how music segments can be stitched together in a plausible way, e.g., Chan et al. (2006). On the other hand, neural networks have been used for music generation since the 80\u2019s (Bharucha & Todd (1989); Mozer (1996); Chen & Miikkulainen (2001); Eck & Schmidhuber (2002)).", "startOffset": 127, "endOffset": 705}, {"referenceID": 1, "context": "It brings together machines learning researchers that aim to capture the complex structure of music (Eck & Schmidhuber (2002); Boulanger-lewandowski et al. (2012)), as well as music professionals (Chan et al. (2006)) and enthusiasts (Johnson; Sun) that want to see how far a computer can get to be a real composer. Real-time music generation is also explored for gaming (Engels et al. (2015)). Early approaches mostly instilled knowledge from music theory into generation, by using rules of how music segments can be stitched together in a plausible way, e.g., Chan et al. (2006). On the other hand, neural networks have been used for music generation since the 80\u2019s (Bharucha & Todd (1989); Mozer (1996); Chen & Miikkulainen (2001); Eck & Schmidhuber (2002)).", "startOffset": 127, "endOffset": 733}, {"referenceID": 1, "context": "It brings together machines learning researchers that aim to capture the complex structure of music (Eck & Schmidhuber (2002); Boulanger-lewandowski et al. (2012)), as well as music professionals (Chan et al. (2006)) and enthusiasts (Johnson; Sun) that want to see how far a computer can get to be a real composer. Real-time music generation is also explored for gaming (Engels et al. (2015)). Early approaches mostly instilled knowledge from music theory into generation, by using rules of how music segments can be stitched together in a plausible way, e.g., Chan et al. (2006). On the other hand, neural networks have been used for music generation since the 80\u2019s (Bharucha & Todd (1989); Mozer (1996); Chen & Miikkulainen (2001); Eck & Schmidhuber (2002)).", "startOffset": 127, "endOffset": 759}, {"referenceID": 1, "context": "It brings together machines learning researchers that aim to capture the complex structure of music (Eck & Schmidhuber (2002); Boulanger-lewandowski et al. (2012)), as well as music professionals (Chan et al. (2006)) and enthusiasts (Johnson; Sun) that want to see how far a computer can get to be a real composer. Real-time music generation is also explored for gaming (Engels et al. (2015)). Early approaches mostly instilled knowledge from music theory into generation, by using rules of how music segments can be stitched together in a plausible way, e.g., Chan et al. (2006). On the other hand, neural networks have been used for music generation since the 80\u2019s (Bharucha & Todd (1989); Mozer (1996); Chen & Miikkulainen (2001); Eck & Schmidhuber (2002)). Mozer (1996) used a Recurrent Neural Network that produced pitch, duration and chord at each time step.", "startOffset": 127, "endOffset": 774}, {"referenceID": 1, "context": "It brings together machines learning researchers that aim to capture the complex structure of music (Eck & Schmidhuber (2002); Boulanger-lewandowski et al. (2012)), as well as music professionals (Chan et al. (2006)) and enthusiasts (Johnson; Sun) that want to see how far a computer can get to be a real composer. Real-time music generation is also explored for gaming (Engels et al. (2015)). Early approaches mostly instilled knowledge from music theory into generation, by using rules of how music segments can be stitched together in a plausible way, e.g., Chan et al. (2006). On the other hand, neural networks have been used for music generation since the 80\u2019s (Bharucha & Todd (1989); Mozer (1996); Chen & Miikkulainen (2001); Eck & Schmidhuber (2002)). Mozer (1996) used a Recurrent Neural Network that produced pitch, duration and chord at each time step. Unlike most other neural network approaches, this work encodes music knowledge into the representation. Eck & Schmidhuber (2002) was first to use LSTMs to generate both melody and chord.", "startOffset": 127, "endOffset": 994}, {"referenceID": 1, "context": "It brings together machines learning researchers that aim to capture the complex structure of music (Eck & Schmidhuber (2002); Boulanger-lewandowski et al. (2012)), as well as music professionals (Chan et al. (2006)) and enthusiasts (Johnson; Sun) that want to see how far a computer can get to be a real composer. Real-time music generation is also explored for gaming (Engels et al. (2015)). Early approaches mostly instilled knowledge from music theory into generation, by using rules of how music segments can be stitched together in a plausible way, e.g., Chan et al. (2006). On the other hand, neural networks have been used for music generation since the 80\u2019s (Bharucha & Todd (1989); Mozer (1996); Chen & Miikkulainen (2001); Eck & Schmidhuber (2002)). Mozer (1996) used a Recurrent Neural Network that produced pitch, duration and chord at each time step. Unlike most other neural network approaches, this work encodes music knowledge into the representation. Eck & Schmidhuber (2002) was first to use LSTMs to generate both melody and chord. Compared to Mozer (1996), the LSTM captured more global music structure across the song.", "startOffset": 127, "endOffset": 1077}, {"referenceID": 1, "context": "It brings together machines learning researchers that aim to capture the complex structure of music (Eck & Schmidhuber (2002); Boulanger-lewandowski et al. (2012)), as well as music professionals (Chan et al. (2006)) and enthusiasts (Johnson; Sun) that want to see how far a computer can get to be a real composer. Real-time music generation is also explored for gaming (Engels et al. (2015)). Early approaches mostly instilled knowledge from music theory into generation, by using rules of how music segments can be stitched together in a plausible way, e.g., Chan et al. (2006). On the other hand, neural networks have been used for music generation since the 80\u2019s (Bharucha & Todd (1989); Mozer (1996); Chen & Miikkulainen (2001); Eck & Schmidhuber (2002)). Mozer (1996) used a Recurrent Neural Network that produced pitch, duration and chord at each time step. Unlike most other neural network approaches, this work encodes music knowledge into the representation. Eck & Schmidhuber (2002) was first to use LSTMs to generate both melody and chord. Compared to Mozer (1996), the LSTM captured more global music structure across the song. Like us, Kang et al. (2012) built upon the randomness of melody by trying to accompany it with drums.", "startOffset": 127, "endOffset": 1169}, {"referenceID": 1, "context": "It brings together machines learning researchers that aim to capture the complex structure of music (Eck & Schmidhuber (2002); Boulanger-lewandowski et al. (2012)), as well as music professionals (Chan et al. (2006)) and enthusiasts (Johnson; Sun) that want to see how far a computer can get to be a real composer. Real-time music generation is also explored for gaming (Engels et al. (2015)). Early approaches mostly instilled knowledge from music theory into generation, by using rules of how music segments can be stitched together in a plausible way, e.g., Chan et al. (2006). On the other hand, neural networks have been used for music generation since the 80\u2019s (Bharucha & Todd (1989); Mozer (1996); Chen & Miikkulainen (2001); Eck & Schmidhuber (2002)). Mozer (1996) used a Recurrent Neural Network that produced pitch, duration and chord at each time step. Unlike most other neural network approaches, this work encodes music knowledge into the representation. Eck & Schmidhuber (2002) was first to use LSTMs to generate both melody and chord. Compared to Mozer (1996), the LSTM captured more global music structure across the song. Like us, Kang et al. (2012) built upon the randomness of melody by trying to accompany it with drums. However, in their model the scale type is enforced. No details about the model are presented, and thus it is virtually impossible to compare to. Boulanger-lewandowski et al. (2012) propose to learn complex polyphonic musical structure which has multiple notes playing in parallel through the song.", "startOffset": 127, "endOffset": 1424}, {"referenceID": 1, "context": "It brings together machines learning researchers that aim to capture the complex structure of music (Eck & Schmidhuber (2002); Boulanger-lewandowski et al. (2012)), as well as music professionals (Chan et al. (2006)) and enthusiasts (Johnson; Sun) that want to see how far a computer can get to be a real composer. Real-time music generation is also explored for gaming (Engels et al. (2015)). Early approaches mostly instilled knowledge from music theory into generation, by using rules of how music segments can be stitched together in a plausible way, e.g., Chan et al. (2006). On the other hand, neural networks have been used for music generation since the 80\u2019s (Bharucha & Todd (1989); Mozer (1996); Chen & Miikkulainen (2001); Eck & Schmidhuber (2002)). Mozer (1996) used a Recurrent Neural Network that produced pitch, duration and chord at each time step. Unlike most other neural network approaches, this work encodes music knowledge into the representation. Eck & Schmidhuber (2002) was first to use LSTMs to generate both melody and chord. Compared to Mozer (1996), the LSTM captured more global music structure across the song. Like us, Kang et al. (2012) built upon the randomness of melody by trying to accompany it with drums. However, in their model the scale type is enforced. No details about the model are presented, and thus it is virtually impossible to compare to. Boulanger-lewandowski et al. (2012) propose to learn complex polyphonic musical structure which has multiple notes playing in parallel through the song. The model is single-track in that it only produces melody, whereas in our work we aim to produce multi-track songs. Just recently, Huang & Wu (2016) proposed a 2-layer LSTM that, like Boulanger-lewandowski et al.", "startOffset": 127, "endOffset": 1690}, {"referenceID": 1, "context": "It brings together machines learning researchers that aim to capture the complex structure of music (Eck & Schmidhuber (2002); Boulanger-lewandowski et al. (2012)), as well as music professionals (Chan et al. (2006)) and enthusiasts (Johnson; Sun) that want to see how far a computer can get to be a real composer. Real-time music generation is also explored for gaming (Engels et al. (2015)). Early approaches mostly instilled knowledge from music theory into generation, by using rules of how music segments can be stitched together in a plausible way, e.g., Chan et al. (2006). On the other hand, neural networks have been used for music generation since the 80\u2019s (Bharucha & Todd (1989); Mozer (1996); Chen & Miikkulainen (2001); Eck & Schmidhuber (2002)). Mozer (1996) used a Recurrent Neural Network that produced pitch, duration and chord at each time step. Unlike most other neural network approaches, this work encodes music knowledge into the representation. Eck & Schmidhuber (2002) was first to use LSTMs to generate both melody and chord. Compared to Mozer (1996), the LSTM captured more global music structure across the song. Like us, Kang et al. (2012) built upon the randomness of melody by trying to accompany it with drums. However, in their model the scale type is enforced. No details about the model are presented, and thus it is virtually impossible to compare to. Boulanger-lewandowski et al. (2012) propose to learn complex polyphonic musical structure which has multiple notes playing in parallel through the song. The model is single-track in that it only produces melody, whereas in our work we aim to produce multi-track songs. Just recently, Huang & Wu (2016) proposed a 2-layer LSTM that, like Boulanger-lewandowski et al. (2012), produces music that is more complex than a single note sequence, and is able to produce chords.", "startOffset": 127, "endOffset": 1761}, {"referenceID": 13, "context": "We use the method in Newell et al. (2016) to track single-frame 2D human pose in the videos.", "startOffset": 21, "endOffset": 42}, {"referenceID": 13, "context": "We use the method in Newell et al. (2016) to track single-frame 2D human pose in the videos. We process the single-frame tracking result to ensure left-right body consistency through time, and then use the method of Zhou et al. (2016) to convert the 2D pose sequence into 3D.", "startOffset": 21, "endOffset": 235}, {"referenceID": 11, "context": "We first generate a story about the photo with the neural storyteller Kiros et al. (2015) and try to accompany the generated text with music.", "startOffset": 70, "endOffset": 90}], "year": 2016, "abstractText": "We present a novel framework for generating pop music. Our model is a hierarchical Recurrent Neural Network, where the layers and the structure of the hierarchy encode our prior knowledge about how pop music is composed. In particular, the bottom layers generate the melody, while the higher levels produce the drums and chords. We conduct several human studies that show strong preference of our generated music over that produced by the recent method by Google. We additionally show two applications of our framework: neural dancing and karaoke, as well as neural story singing.", "creator": "LaTeX with hyperref package"}}}