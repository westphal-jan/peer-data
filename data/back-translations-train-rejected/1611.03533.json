{"id": "1611.03533", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Nov-2016", "title": "Landmark-based consonant voicing detection on multilingual corpora", "abstract": "This paper tests the hypothesis that distinctive feature classifiers anchored at phonetic landmarks can be transferred cross-lingually without loss of accuracy. Three consonant voicing classifiers were developed: (1) manually selected acoustic features anchored at a phonetic landmark, (2) MFCCs (either averaged across the segment or anchored at the landmark), and(3) acoustic features computed using a convolutional neural network (CNN). All detectors are trained on English data (TIMIT),and tested on English, Turkish, and Spanish (performance measured using F1 and accuracy). Experiments demonstrate that manual features outperform all MFCC classifiers, while CNNfeatures outperform both. MFCC-based classifiers suffer an F1reduction of 16% absolute when generalized from English to other languages. Manual features suffer only a 5% F1 reduction,and CNN features actually perform better in Turkish and Span-ish than in the training language, demonstrating that features capable of representing long-term spectral dynamics (CNN and landmark-based features) are able to generalize cross-lingually with little or no loss of accuracy", "histories": [["v1", "Thu, 10 Nov 2016 22:11:16 GMT  (92kb)", "http://arxiv.org/abs/1611.03533v1", "ready to submit to JASA-EL"]], "COMMENTS": "ready to submit to JASA-EL", "reviews": [], "SUBJECTS": "cs.CL cs.SD", "authors": ["xiang kong", "xuesong yang", "mark hasegawa-johnson", "jeung-yoon choi", "stefanie shattuck-hufnagel"], "accepted": false, "id": "1611.03533"}, "pdf": {"name": "1611.03533.pdf", "metadata": {"source": "CRF", "title": "LANDMARK-BASED CONSONANT VOICING DETECTION ON MULTILINGUAL CORPORA", "authors": ["Xiang Kong", "Xuesong Yang", "Jeung-Yoon Choi", "Mark Hasegawa-Johnson", "Stefanie Shattuck-Hufnagel"], "emails": ["jhasegaw}@illinois.edu,", "sshuf}@mit.edu"], "sections": [{"heading": null, "text": "ar Xiv: 161 1,03 533v 1 [cs.C L] 10 Nov 201 6"}, {"heading": "1. Introduction", "text": "This year, the time has come for a reorientation, in which the question is whether this is a country, a country, a country, a country, a country, a country, a city, a city, a city, a city, a city, a city, a city, a city, a city, a city, a city, a city, a city, a city, a city, a city, a city, a city, a city, a city, a city, a city, a city, a city, a city, a city, a city, a city, a city, a city, a city, a city, a city, a city, a city, a city, a city, a city, a city, a city, a city, a city, a city, a city, a city, a city, a city, a city, a city, a city, a city."}, {"heading": "2. ACOUSTIC LANDMARKS AND DISTINCTIVE FEATURES", "text": "Landmarks [1] are defined as points in a statement to allow information about the underlying distinguishing features to be extracted. Four types of landmarks have been proposed in [1]: vowel (V), consonant release (Cr), consonant closure (Cc) and glide (G). Cr landmarks and Cc landmarks are further specialized according to type class: S = Stop, F = Frikativ, N = Nasal. In this work we assume that we have given the correct landmark positions in a speech signal. To convert TIMIT phonetic transcriptions into landmark transcriptions, the following rules were used: Each stop release segment has an Sr landmark at its start time; each stop-closure segment has an Sc landmark at its start time; and each infected, friction, or nasal has a Cc landmark at its start time."}, {"heading": "3. ACOUSTIC FEATURE REPRESENTATIONS", "text": "Within each boundary region, the acoustic characteristics are extracted to distinguish voiced from voiceless consonants, including manually designed acoustic cues (summary see Table 2) and the characteristics learned from deep neural networks."}, {"heading": "3.1. Manually Designed Acoustic Cues", "text": "The duration also refers to the length between the release and the insertion of intonation, both of which are shorter when voiced than the intonation time of the voice (VOT) [9], which contains intonation information about the English registers. It also refers to intonation information for friction and affricates, both of which are shorter when voiced than the intonation time of cross correlation (PNCC): Increasing cross correlation value will exist when intoned consonants are produced (registers, frictions and affricates) [1]. PNCC is originally referred to in [16], and in addition to the use of standardized cross-correlations, we retain its peak, the cross-correlations, which records the cross-correlations of transitions."}, {"heading": "3.2. Convolutional neural networks (CNN)", "text": "A common raw representation of speech signals as inputs is the size of the log-scale mel filter banks over time. However, this paper suggests extracting features in a boundary region that is too short (20ms) for multiple frames, so 1D discrete Fourier transformation and 1D filter banks are considered inputs. Figure 1 illustrates the architecture of Convolutionary Neural Networks, which consist of three types of layer folding, maxpooling, and fully connected layers. In a Convolutionary Layer, each neuron takes local patterns in the previous layer as inputs. All neurons in the same characteristic map share the same weight matrix. A max-pooling layer is stacked after each Convolutionary Layer, which similarly takes local patterns as inputs, and down samples to produce a single output for that local layer. Multiple completely connected layers are merged into multiple building-based modules based on a single contiguous layer of a single max."}, {"heading": "4. EXPERIMENTS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1. Multilingual Corpora", "text": "Only the body TIMIT TRAIN is selected for model training, while others are selected as test sets. Table 3 illustrates the number of samples in this corpus. English Corpus: TIMIT [19] corpus contains broadband recordings of 630 speakers from eight major dialects of American English, each of which reads ten phonetically rich sentences and contains time-oriented orthographic, phonetic and word transcriptions, as well as a 16-bit voice waveform file at 16 kHz for each utterance. Spanish Corpus: The phonetic Albayzin corpus of Spanish is divided into two subcorpus, one for training and another for testing purposes, as it was originally designed to train speech recognition machines. The training subbody consists of 200 phrases; 4 speakers produce 200 phrases every 200 phrases and 160 phrases are produced from these 25 phrases."}, {"heading": "4.2. Feature Extraction", "text": "The calculation of manual acoustic characteristics anchored in a phonetic boundary region and MFCCs (either averaged over the phonetic segment or anchored in the boundary region) and the representation of raw data of speech signals in boundary ranges are presented as follows, or as follows. MFCCs: First, a hammer window is applied whose duration corresponds to either the boundary region or the duration of the entire phone, and the window signal is then converted into MFCC (13) or MFCC (39). Duration, formant, transition, PNCC and H1: a robust RAPT [16] pitch tracking algorithm based on normalized transverse correlation and dynamic programming is applied using wavesurfer1. The basic frequency, probability of intonation (1.0 means spoken and 0.0 means spoken and 0.0 means unspoken boundary frequency) is determined in boundary ranges."}, {"heading": "5. RESULTS", "text": "These models are all trained in English and tested in English, Spanish or Turkish. Support vector machine with radial core is used as a binary classifier based on acoustic characteristics, while CNNs are used as end-to-end classifiers. F1 score of consonant intonation (positive sample) and general accuracy are used as metrics. Due to the unbalanced nature of the training, CNN is weighted with each sample CNFCs and weighted inversely proportional to CNFCs class frequency. The relative error rate increase in performance over English was calculated when applying models to other languages. MFCCs: When calculating average MFCCs across the entire phonetics segment, MFCCs are calculated with dynamic coefficients (MC39)."}, {"heading": "6. CONCLUSION", "text": "In this paper, three distinct traits are applied to build consonant speech detectors to test the theory that distinctive trait classes are resilient to multilingual corpora. As traits, MFCCs (in the landmark region and averaged over the entire duration of telephone utterance), acoustic traits from the landmark region, and traits learned by a Convolutionary Neural Network (CNN) were tested. Classifiers based on these traits are all trained in English and tested in English, Spanish, and Turkish. Results show that MFCCs were unable to detect the voice either in the training language or in the test languages. Manual acoustic traits generalize better to novel languages than MFCCs. Acoustic traits learned by a CNN achieve the best performance on both training languages and non-training languages. We conclude that traits that represent long-term spectral dynamics compared to a phonetic trait (without major loss of directional characteristics) are capable of being broadly applied to CNN."}, {"heading": "7. References", "text": "[1] K. N. Stevens, \"Toward a model for lexical access based on acous-tic landmarks and distinctive features 11,\" The Journal of the Acoustical Society of America, vol. 114, no. 4, pp. 872-1891, 2002. [2] -, \"RLE Technical Report No. 506, MIT, Cambridge, MA, 1985. [4] F. Bell-Berti,\" Control of pharyngeal cavity size for English voiced and voiceless stops, \"he Journal of the Acoustical Society of America, vol. 57, pp. 456-461, 1975. [5] C. Crowther1 and V. Mann1,\" native language factors affected use of vocalic custica to final it."}], "references": [{"title": "Toward a model for lexical access based on acoustic landmarks and distinctive features", "author": ["K.N. Stevens"], "venue": "The Journal of the Acoustical Society of America, vol. 114, no. 4, pp. 872\u20131891, 2002.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1891}, {"title": "Modelling affricate consonants", "author": ["\u2014\u2014"], "venue": "Speech Communication, vol. 13, no. 1, pp. 33\u201343, 1993.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1993}, {"title": "The acoustics of fricative consonants", "author": ["C.H. Shadle"], "venue": "RLE Technical Report No. 506, MIT, Cambridge,MA, 1985.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1985}, {"title": "Control of pharyngeal cavity size for english voiced and voiceless stops", "author": ["F. Bell-Berti"], "venue": "he Journal of the Acoustical Society of America, vol. 57, no. 2, pp. 456\u2013461, 1975.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1975}, {"title": "Native language factors affecting use of vocalic cues to final consonant voicing in english", "author": ["C.S. Crowther1", "V. Mann1"], "venue": "The Journal of the Acoustical Society of America, vol. 92, no. 2, pp. 711\u2013722, 1992.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1992}, {"title": "Detection of consonant voicing: A module for a hierarchical speech recognition system", "author": ["J.-Y. Choi"], "venue": "The Journal of the Acoustical Society of America, vol. 106, no. 4, pp. 2274\u20132274, 1999.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1999}, {"title": "Voice onset time in aphasia, apraxia of speech and dysarthria: a review", "author": ["P. Auzou", "C. Ozsancak", "R.J. Morris", "M. Jan", "F. Eustache", "D. Hannequin"], "venue": "Clinical Linguistics & Phonetics, vol. 14, no. 2, pp. 131\u2013150, 2000.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2000}, {"title": "The interspeech 2008 consonant challenge", "author": ["M. Cooke", "O. Scharenborg"], "venue": "the 9th Annual Conference of the International Speech Communication Association. ISCA Archive, 2008, pp. 1765\u20131768.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2008}, {"title": "Speech parameterization based on phonetic features: application to speech recognition", "author": ["N.N. Bitar", "C.Y.E. Wilson"], "venue": "Fourth European Conference on Speech Communication and Technology, 1995.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1995}, {"title": "An acousticphonetic feature-based system for the automatic recognition of fricative consonants", "author": ["A.M.A. Ali", "J.V. der Spiegel", "P. Mueller"], "venue": "Acoustics, Speech and Signal Processing, vol. 2, pp. 961\u2013964, 1998.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1998}, {"title": "Acoustic-phonetic features for the automatic classification of stop consonants", "author": ["\u2014\u2014"], "venue": "IEEE Transactions on Speech and Audio Processing, vol. 9, no. 8, pp. 833\u2013841, 2001.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2001}, {"title": "Deep learning", "author": ["Y. LeCun", "Y. Bengio", "G. Hinton"], "venue": "Nature, vol. 521, no. 7553, pp. 436\u2013444, 2015.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups", "author": ["G. Hinton", "L. Deng", "D. Yu", "G.E. Dahl", "A.-r. Mohamed", "N. Jaitly", "A. Senior", "V. Vanhoucke", "P. Nguyen", "T.N. Sainath"], "venue": "Signal Processing Magazine, IEEE, vol. 29, no. 6, pp. 82\u201397, 2012.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2012}, {"title": "Landmark-based pronunciation error identification on chinese learning", "author": ["X. Yang", "X. Kong", "M. Hasegawa-Johnson", "Y. Xie"], "venue": "submitted in Speech Prosody, 2016.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2016}, {"title": "A robust algorithm for pitch tracking (rapt)", "author": ["D. Talkin"], "venue": "Speech coding and synthesis, vol. 495, p. 518, 1995.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1995}, {"title": "Role of formant transitions in the voicedvoiceless distinction for stops", "author": ["K.N. Stevens", "D.H. Klatt"], "venue": "The Journal of the Acoustical Society of America, vol. 55, no. 3, pp. 653\u2013659, 1974.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1974}, {"title": "Adam: A method for stochastic optimization", "author": ["D. Kingma", "J. Ba"], "venue": "arXiv preprint arXiv:1412.6980, 2014.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2014}, {"title": "The darpa timit acoustic-phonetic continuous speech corpus cdrom", "author": ["J.S. Garofalo", "L.F. Lamel", "W.M. Fisher", "J.G. Fiscus", "D.S. Pallett", "N.L. Dahlgren"], "venue": "Linguistic Data Consortium, 1993.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1993}], "referenceMentions": [{"referenceID": 0, "context": "In contrast to the conventional data-driven speech recognition model, acoustic correlates of distinctive features are found in an acoustics phonetic recognizer [1] so as to extract interpretable acoustic information.", "startOffset": 160, "endOffset": 163}, {"referenceID": 1, "context": "Obstruent consonants are further categorized by consonant voicing which can be described by the articulator-bound feature [stiff vocal folds] [2].", "startOffset": 142, "endOffset": 145}, {"referenceID": 2, "context": "Shadle [3] studied fricative consonants using mechanical models, theoretical models, and acoustic analysis, and found that the most important parameters for fricatives are the length of the front cavity, the presence of an obstacle and the flow rate.", "startOffset": 7, "endOffset": 10}, {"referenceID": 3, "context": "Speech production mechanism differences between voice and voiceless stops are mainly due to muscle activity, which relaxes the tongue root during voiced stops, altering aerodynamics near the vocal folds in order to maintain voicing during closure [4].", "startOffset": 247, "endOffset": 250}, {"referenceID": 4, "context": "vocalic duration and F1 offset frequency) are also correlates of consonant voicing [5].", "startOffset": 83, "endOffset": 86}, {"referenceID": 5, "context": "A module for detecting consonant voicing based on these acoustic correlates [6] first determines acoustic properties according to consonant production, then extracts acoustic cues, and classifies them to detect consonant voicing.", "startOffset": 76, "endOffset": 79}, {"referenceID": 6, "context": "One of the traditional methods to detect consonant voicing uses mel-frequency cepstral coefficients (MFCCs) [7, 8], e.", "startOffset": 108, "endOffset": 114}, {"referenceID": 7, "context": "One of the traditional methods to detect consonant voicing uses mel-frequency cepstral coefficients (MFCCs) [7, 8], e.", "startOffset": 108, "endOffset": 114}, {"referenceID": 8, "context": "7% to 80% overall accuracy [9, 10].", "startOffset": 27, "endOffset": 34}, {"referenceID": 0, "context": "Landmarks [1] identify times when the acoustic patterns of the linguistically motivated distinctive features are most salient; acoustic cues extracted in the vicinity of landmarks may therefore be more informative for the classification of distinctive features than cues extracted from other times in the signal.", "startOffset": 10, "endOffset": 13}, {"referenceID": 9, "context": "To the best of our knowledge, the highest accuracy for voicing classification of obstruents uses acoustic features extracted with reference to phonetic landmarks, with accuracies of 95% and 96% [11, 12] for stops and fricatives respectively.", "startOffset": 194, "endOffset": 202}, {"referenceID": 10, "context": "To the best of our knowledge, the highest accuracy for voicing classification of obstruents uses acoustic features extracted with reference to phonetic landmarks, with accuracies of 95% and 96% [11, 12] for stops and fricatives respectively.", "startOffset": 194, "endOffset": 202}, {"referenceID": 11, "context": "Deep learning techniques transform raw data into multiple levels of abstraction by stacking multiple layers with non-linearities, thus learning complex features automatically [13].", "startOffset": 175, "endOffset": 179}, {"referenceID": 12, "context": "Though the accuracy of speech recognizers built from deep networks is high [14], results on the cross-language portability of deep networks include both positive and negative outcomes.", "startOffset": 75, "endOffset": 79}, {"referenceID": 0, "context": "Landmarks [1] are defined as points in an utterance around which information about the underlying distinctive features may be extracted.", "startOffset": 10, "endOffset": 13}, {"referenceID": 0, "context": "Four types of landmarks were proposed in [1]: Vowel (V), Consonant release (Cr), Consonant closure (Cc), and Glide (G).", "startOffset": 41, "endOffset": 44}, {"referenceID": 0, "context": "Stevens [1] proposed that distinctive features obtained from closure and release landmark regions should be universal across languages.", "startOffset": 8, "endOffset": 11}, {"referenceID": 13, "context": "After finding landmark positions, we denote the landmark regions as follows [15]:", "startOffset": 76, "endOffset": 80}, {"referenceID": 8, "context": "For stop release segments, duration is the voice onset time (VOT) [9] which carries voicing information about English stops.", "startOffset": 66, "endOffset": 69}, {"referenceID": 0, "context": "Peak normalized value of the cross-correlation (PNCC): Increasing cross correlation value will exist when producing voiced consonants (stops, fricatives and affricates) [[1].", "startOffset": 170, "endOffset": 173}, {"referenceID": 14, "context": "PNCC is originally denoted in [16], and besides using normalized cross-correlation, we retain its peak, which captures cross correlation value transitions.", "startOffset": 30, "endOffset": 34}, {"referenceID": 15, "context": "Formant transitions: Formant transitions [17] are different for voiced and unvoiced consonants.", "startOffset": 41, "endOffset": 45}, {"referenceID": 16, "context": "During backpropagation, a first-order gradient-based optimization method based on adaptive estimates of lower-order moments (Adam) [18] is used.", "startOffset": 131, "endOffset": 135}, {"referenceID": 17, "context": "English Corpus: TIMIT [19] corpus contains broadband recordings of 630 speakers of eight major dialects of American English, each reading ten phonetically rich sentences and includes time-aligned orthographic, phonetic and word transcriptions, as well as a 16-bit, 16kHz speech waveform file for each utterance.", "startOffset": 22, "endOffset": 26}, {"referenceID": 14, "context": "Duration, formant, transition, PNCC and H1: a robust RAPT [16] algorithm for pitch tracking that is based on normalized cross-correlation and dynamic programming is applied using Wavesurfer.", "startOffset": 58, "endOffset": 62}], "year": 2016, "abstractText": "This paper tests the hypothesis that distinctive feature classifiers anchored at phonetic landmarks can be transferred crosslingually without loss of accuracy. Three consonant voicing classifiers were developed: (1) manually selected acoustic features anchored at a phonetic landmark, (2) MFCCs (either averaged across the segment or anchored at the landmark), and (3) acoustic features computed using a convolutional neural network (CNN). All detectors are trained on English data (TIMIT), and tested on English, Turkish, and Spanish (performance measured using F1 and accuracy). Experiments demonstrate that manual features outperform all MFCC classifiers, while CNN features outperform both. MFCC-based classifiers suffer an overall error rate increase of up to 96.1% when generalized from English to other languages. Manual features suffer only an up to 35.2% relative error rate increase, and CNN features actually perform the best on Turkish and Spanish, demonstrating that features capable of representing long-term spectral dynamics (CNN and landmark-based features) are able to generalize cross-lingually with little or no loss of accuracy.", "creator": "LaTeX with hyperref package"}}}