{"id": "1611.03558", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Nov-2016", "title": "Neural Networks Models for Entity Discovery and Linking", "abstract": "This paper describes the USTC_NELSLIP systems submitted to the Trilingual Entity Detection and Linking (EDL) track in 2016 TAC Knowledge Base Population (KBP) contests. We have built two systems for entity discovery and mention detection (MD): one uses the conditional RNNLM and the other one uses the attention-based encoder-decoder framework. The entity linking (EL) system consists of two modules: a rule based candidate generation and a neural networks probability ranking model. Moreover, some simple string matching rules are used for NIL clustering. At the end, our best system has achieved an F1 score of 0.624 in the end-to-end typed mention ceaf plus metric.", "histories": [["v1", "Fri, 11 Nov 2016 01:21:20 GMT  (306kb,D)", "http://arxiv.org/abs/1611.03558v1", "9 pages, 5 figures"]], "COMMENTS": "9 pages, 5 figures", "reviews": [], "SUBJECTS": "cs.CL cs.AI cs.IR", "authors": ["dan liu", "wei lin", "shiliang zhang", "si wei", "hui jiang"], "accepted": false, "id": "1611.03558"}, "pdf": {"name": "1611.03558.pdf", "metadata": {"source": "CRF", "title": "The USTC NELSLIP Systems for Trilingual Entity Detection and Linking Tasks at TAC KBP 2016", "authors": ["Dan Liu", "Wei Lin", "Shiliang Zhang", "Si Wei", "Hui Jiang"], "emails": ["danliu@iflytek.com,", "weilin2@iflytek.com,", "siwei@iflytek.com,", "zsl2008@mail.ustc.edu.cn,", "hj@cse.yorku.ca"], "sections": [{"heading": "1 Introduction", "text": "In this paper, we describe the USTC NELSLIP systems submitted for the TAC KBP Trilingual Entity Discovery and Linking (EDL) task of NIST in 2016. The EDL task requires the recognition of named entities and their nominal mentions in the raw text of trilingual languages (English, Chinese and Spanish) and the further linking of each identified mention to the corresponding node in an existing knowledge database, namely Freebase. For NIL mentions that do not exist in the knowledge database, the EDL system must bundle all NIL mentions and assign a unique ID to each NIL mention cluster. The entire framework of our EDL systems is illustrated in Figure 1. This year, the EDL task has the recognition of nominal mentions to all entity types for all trilinguistic clusters, while each of these types is expanded by a total of 90.000 mentions, while there are different LOGs for each large document."}, {"heading": "2 Mention Detection", "text": "In conventional approaches, we usually treat the recognition of mentions as a sequence marker problem, typically solved by conditional random fields (CRFs). In order to detect nominal mentions, we typically use a noun sentence chunker to detect all possible candidates for nominal mentions. Subsequently, some heuristic post-processing methods are used to identify the true nominal mentions. Unlike conventional methods, in this work, we consider nominal mentions as special named entities and collectively recognize both named and nominal mentions using a single model. In this section, we will describe two different systems designed to detect mentions: the first system uses RNN-based conditional language model to perform the sequence marking for mentions; the other adopts the popular, decoder-encoded structure, or extends it to the BP."}, {"heading": "2.1 RNN based Conditional Language Model", "text": "Named entity recognition (NER) without nested entities can be easily formulated as a typical sequence labeling problem = q q conditional problems, where each output tag can carry Xiv: 161 1,03 558v 1 [cs.C L] 11 Nov 2aligned one by one to an input word. In addition, there are strong dependencies between adjacent output labels. Conditional Random Fields (CRFs) (Lafferty et al., 2001) is a widely used method for sequence labeling. However, linear chain CRFs lack the ability to model long-term dependencies. We think that long-term dependencies may be important in solving some NER cases. Some sequence names can become really long, and the probability for an ORG to occur after a PER (within a certain range) may be quite high. In the past, some high order CRFs have been suggested to address these problems, but high order CRFS are too complex to train them."}, {"heading": "2.2 Attention-based Encoder-Decoder", "text": "It is well known that it is not easy to handle the individual entities with the traditional sequence marker. (Finkel and Manning, 2009) It has been shown that the nested entities can be processed into a linear sequence representation by using a normal character parser. Moreover, the whole phrase is a named entity of FAC and Kentucky Chicken, which is a named entity of PER and Kentucky. (Vinyals et al., 2015) The tree-structured representation for this nested entity can be represented as a linear sequence: [FAC] PER Kentucky] PER, where some paired special symbols are used, such as [FAC]."}, {"heading": "2.3 Model Configurations", "text": "Both conditional RNN-LM and attention-based encoder decoders use a stack of five 1-dimensional convolutional layers as tne encoders to generate the representations for the input word sequences. In all revolutionary layers, we set the filter size and feature cards to 3 and 512. We do not use pool layers, but zero padding is used in each layer. In this way, the length in each revolutionary layer is not changed and remains the input sequence. From it, we can easily retrieve the CNN output at any time, but instead of the 5-layer CNNs, we have also investigated using bidirectional GRUs or LSTMs as encoders, but no gain is observed in our experiments."}, {"heading": "3 Entity Linking", "text": "In the task of linking entities, each mention found must be linked to a known entity in an existing knowledge base, namely Freebase in this task. For any mention that does not match an existing node in Freebase, we must cluster these NIL mentions. In this work, we use a rank-based method for linking entities. For a given mention, we first use a fairly complicated rules-based system to generate all possible freebase nodes as linking targets, each of which is referred to as linking candidate generation. This step is called candidate generation. Next, we train a neural network-based ranking model to classify all of these candidates in such a way that the final linking target is identified. In this step, we have proposed to use many craft features for the NN-based ranking model."}, {"heading": "3.1 Candidate Generation", "text": "It is obvious that the final linking of performance relies heavily on the generated candidate list of Great Britain and Northern Ireland. In this work, we have designed a complicated rules-based system that we can select as candidates for our candidate search. In this module, candidates are generated based on a number of knowledge bases, including Freebase, Wikipedia. We have decided to use Lucene and MySQL to search in our implementation. Entering this module is a recognized reference to the candidate list, which consists of a list of freebase nodes that may match this menu sequence. In the first step, each mention is expanded into a number of different queries, representing different ways to rename the same entities. For example, given a recognized mention of England, we need to expand it to generate a list of different queries that cover England, United Kingdom of Great Britain and Northern Ireland."}, {"heading": "3.2 Neural Networks Ranking Model", "text": "This year it has come to the point where it will be able to retaliate in order to retaliate."}, {"heading": "4 NIL Clustering", "text": "For all mentions identified as NIL by the above NN ranking models, we perform a very simple rule-based algorithm for clustering: i) Different NIL mentions are only grouped in a cluster if their mentioning strings are the same (case-insensitive); ii) The nominal NIL mentions are always grouped to their nearest mention with the same mentioning type. We have examined other more complex string matching methods for NIL clustering, but have observed no improvement."}, {"heading": "5 Experimental Results", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 Entity Discovery Results", "text": "For System 2 and System 3, we use conditioned RNN-LM or attention-based encoder decoder to identify entities. We have observed that these models have achieved quite high precision, but relatively low recall rates. As a result, we have submitted a different system by merging the results of Systems 2 and 3, making this our most powerful system. The official entity discovery performance from the first EDL1 evaluation in 2016 is summarized for these three systems in Table 3."}, {"heading": "5.2 Entity Linking Experimental Results", "text": "For the evaluation of the KBP EDL in 2016, we have just developed a system for linking and linking NIL clusters, as in sections 3 and 4. Here, we report only on the official linking of the results of the best entity identification system (System 1), the performance (in terms of strong match of all elements) of our system is presented in Table 4, and the performance (in terms of typed mention ceaf plus) in Table 5. Results have shown that the English system significantly outperforms the other two systems, due to the fact that the performance of mentioning de-tection for English is usually better than the other two languages, as there are more English data resources available in Freebase and Wikipedia than in Spanish and Chinese."}, {"heading": "6 Conclusions", "text": "In this paper, we have described our submitted systems for the trilingual EDL track evaluation of the 2016 TAC KBP. We have examined several neural network models for both entity discovery and entity linkage; for entity discovery tasks, we have used two neural networks in the popular encoder decoder framework to model long-term dependencies and nested entities in the KBP tasks; for entity linkage, we have proposed some handmade functions and a simple neural network ranking model; and for NIL clustering, we have introduced a very simple rule-based string matching method. Overall, our systems have performed fairly well in both the 2015 KBP data and the official KBP 2016 evaluation."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Kyunghyun Cho", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1409.0473", "citeRegEx": "Bahdanau et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "On the properties of neural machine translation: Encoderdecoder approaches", "author": ["Cho et al.2014] Kyunghyun Cho", "Bart Van Merri\u00ebnboer", "Dzmitry Bahdanau", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1409.1259", "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Nested named entity recognition", "author": ["Finkel", "Manning2009] Jenny Rose Finkel", "Christopher D Manning"], "venue": "In Proceedings of Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Finkel et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Finkel et al\\.", "year": 2009}, {"title": "Conditional random fields: Probabilistic models for segmenting and labeling sequence data", "author": ["Andrew Mccallum", "Fernando C.N. Pereira"], "venue": "In Proceedings of the 18th International Conference on Machine Learning (ICML),", "citeRegEx": "Lafferty et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Lafferty et al\\.", "year": 2001}, {"title": "Recurrent neural network based language model", "author": ["Martin Karafit", "Lukas Burget", "Jan Cernock", "Sanjeev Khudanpur"], "venue": "In Proceedings of Interspeech,", "citeRegEx": "Mikolov et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2010}, {"title": "Grammar as a foreign language", "author": ["\u0141ukasz Kaiser", "Terry Koo", "Slav Petrov", "Ilya Sutskever", "Geoffrey Hinton"], "venue": "In Proceedings of Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Vinyals et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Vinyals et al\\.", "year": 2015}, {"title": "Adadelta: an adaptive learning rate method", "author": ["Matthew D Zeiler"], "venue": "arXiv preprint arXiv:1212.5701", "citeRegEx": "Zeiler.,? \\Q2012\\E", "shortCiteRegEx": "Zeiler.", "year": 2012}], "referenceMentions": [{"referenceID": 3, "context": "Conditional random fields (CRFs) (Lafferty et al., 2001) is a widely-used method for sequence labeling.", "startOffset": 33, "endOffset": 56}, {"referenceID": 4, "context": "(1), this modeling approach is quite similar to language models based on recurrent neural networks (RNN) in (Mikolov et al., 2010) except that each factorized probability depends on the entire input sequence X .", "startOffset": 108, "endOffset": 130}, {"referenceID": 1, "context": "For simplicity, we use one layer of gated recurrent units (GRU) (Cho et al., 2014), which essentially computes all factorized probabilities in eq.", "startOffset": 64, "endOffset": 82}, {"referenceID": 5, "context": "Furthermore, inspired by the idea in (Vinyals et al., 2015), we may easily linearize a tree structure into a linear sequence representation.", "startOffset": 37, "endOffset": 59}, {"referenceID": 5, "context": "Following the same idea in (Vinyals et al., 2015), the tree-structured representation for this nested entity may be represented as a linear sequence:", "startOffset": 27, "endOffset": 49}, {"referenceID": 0, "context": "of input sequence X , and the attention mechanism is similar to (Bahdanau et al., 2014), and the third module is an RNN-based decoder to compute the following conditional probability:", "startOffset": 64, "endOffset": 87}, {"referenceID": 6, "context": "Parameter optimization of all models are performed using AdaDelta (Zeiler, 2012) and early stopping is also used by monitoring a small held-out development set.", "startOffset": 66, "endOffset": 80}], "year": 2016, "abstractText": "This paper describes the USTC NELSLIP systems submitted to the Trilingual Entity Detection and Linking (EDL) track in 2016 TAC Knowledge Base Population (KBP) contests. We have built two systems for entity discovery and mention detection (MD): one uses the conditional RNNLM and the other one uses the attention-based encoder-decoder framework. The entity linking (EL) system consists of two modules: a rule based candidate generation and a neural networks probability ranking model. Moreover, some simple string matching rules are used for NIL clustering. At the end, our best system has achieved an F1 score of 0.624 in the end-to-end typed mention ceaf plus metric.", "creator": "LaTeX with hyperref package"}}}