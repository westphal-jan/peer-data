{"id": "1611.03599", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Nov-2016", "title": "UTCNN: a Deep Learning Model of Stance Classificationon on Social Media Text", "abstract": "Most neural network models for document classification on social media focus on text infor-mation to the neglect of other information on these platforms. In this paper, we classify post stance on social media channels and develop UTCNN, a neural network model that incorporates user tastes, topic tastes, and user comments on posts. UTCNN not only works on social media texts, but also analyzes texts in forums and message boards. Experiments performed on Chinese Facebook data and English online debate forum data show that UTCNN achieves a 0.755 macro-average f-score for supportive, neutral, and unsupportive stance classes on Facebook data, which is significantly better than models in which either user, topic, or comment information is withheld. This model design greatly mitigates the lack of data for the minor class without the use of oversampling. In addition, UTCNN yields a 0.842 accuracy on English online debate forum data, which also significantly outperforms results from previous work as well as other deep learning models, showing that UTCNN performs well regardless of language or platform.", "histories": [["v1", "Fri, 11 Nov 2016 07:05:49 GMT  (69kb,D)", "http://arxiv.org/abs/1611.03599v1", "11 pages, to appear in COLING 2016"]], "COMMENTS": "11 pages, to appear in COLING 2016", "reviews": [], "SUBJECTS": "cs.CL cs.AI cs.LG", "authors": ["wei-fan chen", "lun-wei ku"], "accepted": false, "id": "1611.03599"}, "pdf": {"name": "1611.03599.pdf", "metadata": {"source": "CRF", "title": "UTCNN: a Deep Learning Model of Stance Classification on Social Media Text", "authors": ["Wei-Fan Chen", "Lun-Wei Ku"], "emails": ["viericwf@iis.sinica.edu.tw", "lwku@iis.sinica.edu.tw"], "sections": [{"heading": "1 Introduction", "text": "Most focus on content information and usage models such as Convolutionary Neural Networks (CNN) (Kim, 2014) or Recursive Neural Networks (Socher et al., 2013), but for user-generated posts on social media such as Facebook or Twitter, there is more information that should not be ignored. On social media platforms, a user can act either as the author of a post or as a reader who expresses his or her comments on the posts. In this paper, we classify posts based on authorship, topics and comments. Users and their \"likes\" in particular hold a strong potential for text mining. For example, posts associated with a particular topic are marked with likes and dislikes."}, {"heading": "2 Related Work", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Extra-Linguistic Features for Stance Classification", "text": "In the field of position classification, previous work has shown that text properties are limited, suggesting that the addition of extra-linguistic constraints could improve performance (Hasan et al., 2008; Hasan and Ng, 2013a; Walker et al., 2006). Adding this constraint leads to 1-7% improvements in accuracy for some models and datasets. Hasan and Ng later added interaction constraints and ideological constraints to the same author's comments (Hasan and and and Ng, 2013a): the former models of the relationship between contributions in a sequence of responses, and the latter models of cross-thematic relationships, such as users who oppose abortion, are likely to resist illegal relationships. In focusing postings on online linked assessments (MSar classifications), we have been using since 2010."}, {"heading": "2.2 Deep Learning on Extra-Linguistic Features", "text": "In recent years, neural network models have been applied to the classification of sentiment (Socher et al., 2012; Socher et al., 2013; Kalchbrenner et al., 2014; Johnson and Zhang, 2015; Huang et al., 2016). Text functions can be used in deep networks to capture semantics or sentiment of text. For example, Dong et al. use an adaptive layer in a recursive neural network for target-dependent Twitter sentiment analysis, where targets are subject to topics such as Window 7 or taylor swift (Dong et al., 2014a; Dong et al., 2014b); recursive neural tensor networks (RNTNNNs) use sentence parameter trees to capture sentiment-level sentiment sentiment sentiment for film ratings (Socher et al., 2013); Le and Mikolov predict sentiment by using paragraph vectors to model the respective vectors, vectors and vectors (2014)."}, {"heading": "3 Method", "text": "In this section, we first describe the CNN-based document compilation, which captures the semantic representation of documents at the user and topic level using word representations, and then show how to add comment information to construct the Neural Network User Topic Comment (UTCNN)."}, {"heading": "3.1 User- and Topic-dependent Document Composition", "text": "As shown in Figure 1, we use a generic CNN (Kim, 2014) and two semantic transformations for the dependent theme of the composition of document 1. We get a document with a dedicated user k, a topic j and its compound n words, each word w being associated with a word in which d is the vector dimension. For each word embedded in xw, we apply two dot operations, as in Equation 1: x \u00b2 w = [Uk \u00b7 xw; Tj \u00b7 xw] (1), in which Uk is \"Rdu\" d models of user preference for certain semantics, and Tj \"Rdt\" d models for semantics; du \"and dt\" are the dimensions of transformed user and theme embeddings respectively. We use Uk to semantically model what each user prefers to read and / or write, and use Tj \"x\" to transform the operation \"semantics dependent on each representation of Uxw and the global representation of xforw."}, {"heading": "3.2 UTCNN Model Description", "text": "Figure 2 illustrates the UTCNN model. Since more than one user can interact with a particular topic, we first add a maximum pooling layer after the embedding layer of the user matrix and the embedding layer of the user to form a moderation matrix that embeds Uk and a moderator vector, using Uk for semantic transformation in the document composition process, as mentioned in the previous paragraph. The term moderator is here to designate the pseudo-user who provides the entire semantic / sentiment of all engaged users for a document. Embedding Uk models of the moderator position preference, that is, the pattern of the revealed user position is ready to show its preference whether a user likes to show impartiality with neutral statements and reasonable arguments for a document, or simply shows strong support for an attitude."}, {"heading": "4 Experiment", "text": "We start with the experimental data set and then describe the training process and the baseline implementation. We also implement several variations to show the effects of features: authors, likes, comments and commentators. In the results section, we compare our model with related work."}, {"heading": "4.1 Dataset", "text": "The idea is that the idea is a project, it's a project, it's a project, it's about putting people's interests at the centre, the idea behind it is to put people at the centre, the idea behind it is to put people at the centre, the idea behind it is to put people at the centre, the idea behind it is to put people at the centre, the idea behind it is, the idea behind it is to put people at the centre, the idea behind it is to put people at the centre, the idea behind it is to put people at the centre, the idea behind it is, the idea behind it is, the idea behind it is to put people at the centre, the idea behind it is to put people at the centre, the idea behind it is to put people at the centre, the idea behind it is to put people at the centre, the idea behind it is to put people at the centre, the idea behind it is, the idea behind it is the idea behind it, the idea behind it is the idea behind it."}, {"heading": "4.2 Settings", "text": "In the UTCNN training process, cross-entropy was used as a loss function and AdaGrad as an optimizer; for the FBFans dataset, we learned the 50-dimensional word embeddings across the entire dataset using GloVe6 (Pennington et al., 2014) to capture the word semantics; for the CreateDebate dataset, we used the publicly available 50-dimensional word embeddings, which were also pre-trained using GloVe. These word embeddings were specified in the training process, the learning rate was set to 0.03; all user and topic embeddings were randomly initialized in the range [-0.1 0.1]; matrix embeddings for users and topics were specified at 250 (5 \u00d7 50); vector embeddings for users and topics were set to the length; we applied the LDA theme model (Lead et al., 2003) to the FBBFans datasets that are built around topics with latency."}, {"heading": "4.3 Baselines", "text": "We compare our model with the following basic lines: 1) SVM with uniform, bigram and trigram functions, which represent a standard but quite strong classifier for text functions; 2) SVM with average word embedding, in which a document is represented as a continuous representation, averaging the embedding of the compound words; 3) SVM with average transformed word embedding (the x \u2032 w in Equation 1), in which a document is represented as a continuous representation, averaging the transformed embedding of the compound words; 4) two mature deep learning models for text classification, CNN (Kim, 2014) and Recurrent Convolutional Neural Networks (RCNN) (Lai et al., 2015), in which the hyperparameters are based on their work; 5) the above SVM and deep learning models with commentary information; 6) UTCNN without user information, we present the user information, the model for CNN and ATER only, the model for CNN."}, {"heading": "4.4 Results on FBFans Dataset", "text": "In Table 3 we find the results of UTCNN and the baselines on the FBFans pages. Here, the majority performs well on the basis of FBFans, who are very much fixated on the neutrality of the class. SVM can show a performance comparable to that of Sup and Neu, but it is insufficient to foresee standard information, especially regarding the embedding of people in society. SVM can achieve a performance comparable to that of Sup and Neutral. However, the meaning of the transformed words is insufficient in itself to make a more efficient choice when it comes to modelling the large amounts of data."}, {"heading": "4.5 Results on CreateDebate Dataset", "text": "The results of UTCNN, baselines such as those we implemented on the FBFans platform, show that improvements to the CreateDebate dataset are very useful. We do not assume an excess of sampling on these models because the CreateDebate datasets are almost balanced. In previous work, CNN has proposed Linear Programming (ILP) or Linear Chain Random Fields (CRFs) to integrate text properties, authors, ideologies and user interaction constraints, where text properties are unigramatic, and POS dependencies; the circle of authors tends to require contributions from the same author on the same topic. Ideology constraints aim to capture inferences between topics for the same author. Authors \"interaction models are subject to interaction between users\" contributions and users \"interactions between contributions such as answers (Hasan and Ng, 2013a)."}, {"heading": "5 Conclusion", "text": "We have proposed UTCNN, a neural network model that provides user, topic, content, and comment information for positioning in social media texts. UTCNN learns user embeddings for all users with a minimal active degree, i.e. a post or similar. Topic information obtained from the topic model or predefined labels further improves the UTCNN model. In addition, comment information provides additional guidance for positioning classification. We have shown that UTCNN produces promising and balanced results. In the future, we plan to investigate the effectiveness of UTCNN embeddings of users for classifying author positions."}, {"heading": "Acknowledgements", "text": "The research of this work was partially supported by the Taiwanese Ministry of Science and Technology under the MOST 104-2221-E-001-024-MY2 contract."}], "references": [{"title": "Hinge-loss markov random fields and probabilistic soft logic", "author": ["Stephen H Bach", "Matthias Broecheler", "Bert Huang", "Lise Getoor."], "venue": "arXiv preprint arXiv:1505.04406.", "citeRegEx": "Bach et al\\.,? 2015", "shortCiteRegEx": "Bach et al\\.", "year": 2015}, {"title": "The power of negative thinking: Exploiting label disagreement in the min-cut classification framework", "author": ["Mohit Bansal", "Claire Cardie", "Lillian Lee."], "venue": "COLING (Posters), pages 15\u201318.", "citeRegEx": "Bansal et al\\.,? 2008", "shortCiteRegEx": "Bansal et al\\.", "year": 2008}, {"title": "Latent dirichlet allocation", "author": ["David M Blei", "Andrew Y Ng", "Michael I Jordan."], "venue": "Journal of machine Learning research, 3:993\u20131022.", "citeRegEx": "Blei et al\\.,? 2003", "shortCiteRegEx": "Blei et al\\.", "year": 2003}, {"title": "Collective classification of congressional floor-debate transcripts", "author": ["Clinton Burfoot", "Steven Bird", "Timothy Baldwin."], "venue": "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume 1, pages 1506\u20131515. Association for Computational Linguistics.", "citeRegEx": "Burfoot et al\\.,? 2011", "shortCiteRegEx": "Burfoot et al\\.", "year": 2011}, {"title": "Adaptive recursive neural network for target-dependent twitter sentiment classification", "author": ["Li Dong", "Furu Wei", "Chuanqi Tan", "Duyu Tang", "Ming Zhou", "Ke Xu."], "venue": "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 49\u201354. Association for Computational Linguistics.", "citeRegEx": "Dong et al\\.,? 2014a", "shortCiteRegEx": "Dong et al\\.", "year": 2014}, {"title": "Adaptive multi-compositionality for recursive neural models with applications to sentiment analysis", "author": ["Li Dong", "Furu Wei", "Ming Zhou", "Ke Xu."], "venue": "Proceedings of the Conference of the Association for the Advancement of Artificial Intelligence. AAAI.", "citeRegEx": "Dong et al\\.,? 2014b", "shortCiteRegEx": "Dong et al\\.", "year": 2014}, {"title": "Extra-linguistic constraints on stance recognition in ideological debates", "author": ["Kazi Saidul Hasan", "Vincent Ng."], "venue": "Proceedings of the Sixth International Joint Conference on Natural Language Processing, pages 816\u2013821. Association for Computational Linguistics.", "citeRegEx": "Hasan and Ng.,? 2013a", "shortCiteRegEx": "Hasan and Ng.", "year": 2013}, {"title": "Stance classification of ideological debates: Data, models, features, and constraints", "author": ["Kazi Saidul Hasan", "Vincent Ng."], "venue": "Proceedings of the Sixth International Joint Conference on Natural Language Processing, pages 1348\u20131356. Association for Computational Linguistics.", "citeRegEx": "Hasan and Ng.,? 2013b", "shortCiteRegEx": "Hasan and Ng.", "year": 2013}, {"title": "Why are you taking this stance? identifying and classifying reasons in ideological debates", "author": ["Kazi Saidul Hasan", "Vincent Ng."], "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing, pages 751\u2013762.", "citeRegEx": "Hasan and Ng.,? 2014", "shortCiteRegEx": "Hasan and Ng.", "year": 2014}, {"title": "Modeling rich contexts for sentiment classification with lstm", "author": ["Minlie Huang", "Yujie Cao", "Chao Dong."], "venue": "arXiv preprint arXiv:1605.01478.", "citeRegEx": "Huang et al\\.,? 2016", "shortCiteRegEx": "Huang et al\\.", "year": 2016}, {"title": "Effective use of word order for text categorization with convolutional neural networks", "author": ["Rie Johnson", "Tong Zhang."], "venue": "Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics.", "citeRegEx": "Johnson and Zhang.,? 2015", "shortCiteRegEx": "Johnson and Zhang.", "year": 2015}, {"title": "A convolutional neural network for modelling sentences", "author": ["Nal Kalchbrenner", "Edward Grefenstette", "Phil Blunsom."], "venue": "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics.", "citeRegEx": "Kalchbrenner et al\\.,? 2014", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2014}, {"title": "Convolutional neural networks for sentence classification", "author": ["Yoon Kim."], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 1746\u20131751. Association for Computational Linguistics.", "citeRegEx": "Kim.,? 2014", "shortCiteRegEx": "Kim.", "year": 2014}, {"title": "Recurrent convolutional neural networks for text classification", "author": ["Siwei Lai", "Liheng Xu", "Kang Liu", "Jun Zhao."], "venue": "Proceedings of the Conference of the Association for the Advancement of Artificial Intelligence, pages 2267\u20132273. AAAI.", "citeRegEx": "Lai et al\\.,? 2015", "shortCiteRegEx": "Lai et al\\.", "year": 2015}, {"title": "Distributed representations of sentences and documents", "author": ["Quoc V Le", "Tomas Mikolov."], "venue": "Proceedings of the 31st International Conference on Machine Learning.", "citeRegEx": "Le and Mikolov.,? 2014", "shortCiteRegEx": "Le and Mikolov.", "year": 2014}, {"title": "Glove: Global vectors for word representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher D. Manning."], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 1532\u20131543. Association for Computational Linguistics.", "citeRegEx": "Pennington et al\\.,? 2014", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Context-sensitive twitter sentiment classification using neural network", "author": ["Yafeng Ren", "Yue Zhang", "Meishan Zhang", "Donghong Ji."], "venue": "Proceedings of the Conference of the Association for the Advancement of Artificial Intelligence. AAAI.", "citeRegEx": "Ren et al\\.,? 2016", "shortCiteRegEx": "Ren et al\\.", "year": 2016}, {"title": "Semantic compositionality through recursive matrix-vector spaces", "author": ["Richard Socher", "Brody Huval", "Christopher D Manning", "Andrew Y Ng."], "venue": "Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 1201\u20131211. Association for Computational Linguistics.", "citeRegEx": "Socher et al\\.,? 2012", "shortCiteRegEx": "Socher et al\\.", "year": 2012}, {"title": "Recursive deep models for semantic compositionality over a sentiment treebank", "author": ["Richard Socher", "Alex Perelygin", "Jean Y Wu", "Jason Chuang", "Christopher D Manning", "Andrew Y Ng", "Christopher Potts."], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing, volume 1631, page 1642. Association for Computational Linguistics.", "citeRegEx": "Socher et al\\.,? 2013", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Joint models of disagreement and stance in online debate", "author": ["Dhanya Sridhar", "James Foulds", "Bert Huang", "Lise Getoor", "Marilyn Walker."], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics.", "citeRegEx": "Sridhar et al\\.,? 2015", "shortCiteRegEx": "Sridhar et al\\.", "year": 2015}, {"title": "Learning semantic representations of users and products for document level sentiment classification", "author": ["Duyu Tang", "Bing Qin", "Ting Liu."], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 1014\u20131023. Association for Computational Linguistics.", "citeRegEx": "Tang et al\\.,? 2015a", "shortCiteRegEx": "Tang et al\\.", "year": 2015}, {"title": "User modeling with neural network for review rating prediction", "author": ["Duyu Tang", "Bing Qin", "Ting Liu", "Yuekui Yang."], "venue": "Proceedings of the Twenty-Fourth International Joint Conference on Artificial Intelligence, pages 1340\u20131346.", "citeRegEx": "Tang et al\\.,? 2015b", "shortCiteRegEx": "Tang et al\\.", "year": 2015}, {"title": "Get out the vote: Determining support or opposition from congressional floor-debate transcripts", "author": ["Matt Thomas", "Bo Pang", "Lillian Lee."], "venue": "Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing, pages 327\u2013335. Association for Computational Linguistics.", "citeRegEx": "Thomas et al\\.,? 2006", "shortCiteRegEx": "Thomas et al\\.", "year": 2006}, {"title": "Stance classification using dialogic properties of persuasion", "author": ["Marilyn A Walker", "Pranav Anand", "Robert Abbott", "Ricky Grant."], "venue": "Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 592\u2013596. Association for Computational Linguistics.", "citeRegEx": "Walker et al\\.,? 2012", "shortCiteRegEx": "Walker et al\\.", "year": 2012}], "referenceMentions": [{"referenceID": 13, "context": "Deep neural networks have been widely used in text classification and have achieved promising results (Lai et al., 2015; Ren et al., 2016; Huang et al., 2016).", "startOffset": 102, "endOffset": 158}, {"referenceID": 16, "context": "Deep neural networks have been widely used in text classification and have achieved promising results (Lai et al., 2015; Ren et al., 2016; Huang et al., 2016).", "startOffset": 102, "endOffset": 158}, {"referenceID": 9, "context": "Deep neural networks have been widely used in text classification and have achieved promising results (Lai et al., 2015; Ren et al., 2016; Huang et al., 2016).", "startOffset": 102, "endOffset": 158}, {"referenceID": 12, "context": "Most focus on content information and use models such as convolutional neural networks (CNN) (Kim, 2014) or recursive neural networks (Socher et al.", "startOffset": 93, "endOffset": 104}, {"referenceID": 18, "context": "Most focus on content information and use models such as convolutional neural networks (CNN) (Kim, 2014) or recursive neural networks (Socher et al., 2013).", "startOffset": 134, "endOffset": 155}, {"referenceID": 8, "context": "For example we discuss women\u2019s rights and unwanted babies on the topic of abortion, but we criticize medicine usage or crime when on the topic of marijuana (Hasan and Ng, 2014).", "startOffset": 156, "endOffset": 176}, {"referenceID": 1, "context": "In the stance classification domain, previous work has showed that text features are limited, suggesting that adding extra-linguistic constraints could improve performance (Bansal et al., 2008; Hasan and Ng, 2013a; Walker et al., 2012).", "startOffset": 172, "endOffset": 235}, {"referenceID": 6, "context": "In the stance classification domain, previous work has showed that text features are limited, suggesting that adding extra-linguistic constraints could improve performance (Bansal et al., 2008; Hasan and Ng, 2013a; Walker et al., 2012).", "startOffset": 172, "endOffset": 235}, {"referenceID": 23, "context": "In the stance classification domain, previous work has showed that text features are limited, suggesting that adding extra-linguistic constraints could improve performance (Bansal et al., 2008; Hasan and Ng, 2013a; Walker et al., 2012).", "startOffset": 172, "endOffset": 235}, {"referenceID": 7, "context": "require that posts written by the same author have the same stance (Hasan and Ng, 2013b; Thomas et al., 2006).", "startOffset": 67, "endOffset": 109}, {"referenceID": 22, "context": "require that posts written by the same author have the same stance (Hasan and Ng, 2013b; Thomas et al., 2006).", "startOffset": 67, "endOffset": 109}, {"referenceID": 6, "context": "Hasan and Ng later added user-interaction constraints and ideology constraints (Hasan and Ng, 2013a): the former models the relationship among posts in a sequence of replies and the latter models inter-topic relationships, e.", "startOffset": 79, "endOffset": 100}, {"referenceID": 7, "context": "For example, Hasan and Ng use hidden Markov models (HMMs) to model dependent relationships to the preceding post (Hasan and Ng, 2013b); Burfoot et al.", "startOffset": 113, "endOffset": 134}, {"referenceID": 3, "context": "use iterative classification to repeatedly generate new estimates based on the current state of knowledge (Burfoot et al., 2011); Sridhar et al.", "startOffset": 106, "endOffset": 128}, {"referenceID": 19, "context": "use probabilistic soft logic (PSL) to model reply links via collaborative filtering (Sridhar et al., 2015).", "startOffset": 84, "endOffset": 106}, {"referenceID": 17, "context": "In recent years neural network models have been applied to document sentiment classification (Socher et al., 2012; Socher et al., 2013; Kalchbrenner et al., 2014; Johnson and Zhang, 2015; Huang et al., 2016).", "startOffset": 93, "endOffset": 207}, {"referenceID": 18, "context": "In recent years neural network models have been applied to document sentiment classification (Socher et al., 2012; Socher et al., 2013; Kalchbrenner et al., 2014; Johnson and Zhang, 2015; Huang et al., 2016).", "startOffset": 93, "endOffset": 207}, {"referenceID": 11, "context": "In recent years neural network models have been applied to document sentiment classification (Socher et al., 2012; Socher et al., 2013; Kalchbrenner et al., 2014; Johnson and Zhang, 2015; Huang et al., 2016).", "startOffset": 93, "endOffset": 207}, {"referenceID": 10, "context": "In recent years neural network models have been applied to document sentiment classification (Socher et al., 2012; Socher et al., 2013; Kalchbrenner et al., 2014; Johnson and Zhang, 2015; Huang et al., 2016).", "startOffset": 93, "endOffset": 207}, {"referenceID": 9, "context": "In recent years neural network models have been applied to document sentiment classification (Socher et al., 2012; Socher et al., 2013; Kalchbrenner et al., 2014; Johnson and Zhang, 2015; Huang et al., 2016).", "startOffset": 93, "endOffset": 207}, {"referenceID": 4, "context": "use an adaptive layer in a recursive neural network for target-dependent Twitter sentiment analysis, where targets are topics such as windows 7 or taylor swift (Dong et al., 2014a; Dong et al., 2014b); recursive neural tensor networks (RNTNs) utilize sentence parse trees to capture sentence-level sentiment for movie reviews (Socher et al.", "startOffset": 160, "endOffset": 200}, {"referenceID": 5, "context": "use an adaptive layer in a recursive neural network for target-dependent Twitter sentiment analysis, where targets are topics such as windows 7 or taylor swift (Dong et al., 2014a; Dong et al., 2014b); recursive neural tensor networks (RNTNs) utilize sentence parse trees to capture sentence-level sentiment for movie reviews (Socher et al.", "startOffset": 160, "endOffset": 200}, {"referenceID": 18, "context": ", 2014b); recursive neural tensor networks (RNTNs) utilize sentence parse trees to capture sentence-level sentiment for movie reviews (Socher et al., 2013); Le and Mikolov predict sentiment by using paragraph vectors to model each paragraph as a continuous representation (Le and Mikolov, 2014).", "startOffset": 134, "endOffset": 155}, {"referenceID": 14, "context": ", 2013); Le and Mikolov predict sentiment by using paragraph vectors to model each paragraph as a continuous representation (Le and Mikolov, 2014).", "startOffset": 124, "endOffset": 146}, {"referenceID": 21, "context": "The userword composition vector model (UWCVM) (Tang et al., 2015b) is inspired by the possibility that the strength of sentiment words is user-specific; to capture this they add user embeddings in their model.", "startOffset": 46, "endOffset": 66}, {"referenceID": 20, "context": "In UPNN, a later extension, they further add a product-word composition as product embeddings, arguing that products can also show different tendencies of being rated or reviewed (Tang et al., 2015a).", "startOffset": 179, "endOffset": 199}, {"referenceID": 12, "context": "As shown in Figure 1, we use a general CNN (Kim, 2014) and two semantic transformations for document composition 1 .", "startOffset": 43, "endOffset": 54}, {"referenceID": 7, "context": "To compare with Hasan and Ng\u2019s work, we conducted five-fold cross-validation and present the annotation results as the average number of all folds (Hasan and Ng, 2013b; Hasan and Ng, 2014).", "startOffset": 147, "endOffset": 188}, {"referenceID": 8, "context": "To compare with Hasan and Ng\u2019s work, we conducted five-fold cross-validation and present the annotation results as the average number of all folds (Hasan and Ng, 2013b; Hasan and Ng, 2014).", "startOffset": 147, "endOffset": 188}, {"referenceID": 15, "context": "For FBFans dataset, we learned the 50-dimensional word embeddings on the whole dataset using GloVe6 (Pennington et al., 2014) to capture the word semantics; for CreateDebate dataset we used the publicly available English 50-dimensional word embeddings, pre-trained also using GloVe.", "startOffset": 100, "endOffset": 125}, {"referenceID": 2, "context": "We applied the LDA topic model (Blei et al., 2003) on the FBFans dataset to determine the latent topics with which to build topic embeddings, as there is only one general known topic: nuclear power plants.", "startOffset": 31, "endOffset": 50}, {"referenceID": 12, "context": "CNN (Kim, 2014) \u221a .", "startOffset": 4, "endOffset": 15}, {"referenceID": 12, "context": "637 CNN (Kim, 2014) \u221a \u221a .", "startOffset": 8, "endOffset": 19}, {"referenceID": 13, "context": "648 RCNN (Lai et al., 2015) \u221a .", "startOffset": 9, "endOffset": 27}, {"referenceID": 13, "context": "606 RCNN (Lai et al., 2015) \u221a \u221a .", "startOffset": 9, "endOffset": 27}, {"referenceID": 6, "context": "For the CreateDebate dataset, accuracy was adopted as the evaluation metric to compare the results with related work (Hasan and Ng, 2013a; Hasan and Ng, 2013b; Sridhar et al., 2015).", "startOffset": 117, "endOffset": 181}, {"referenceID": 7, "context": "For the CreateDebate dataset, accuracy was adopted as the evaluation metric to compare the results with related work (Hasan and Ng, 2013a; Hasan and Ng, 2013b; Sridhar et al., 2015).", "startOffset": 117, "endOffset": 181}, {"referenceID": 19, "context": "For the CreateDebate dataset, accuracy was adopted as the evaluation metric to compare the results with related work (Hasan and Ng, 2013a; Hasan and Ng, 2013b; Sridhar et al., 2015).", "startOffset": 117, "endOffset": 181}, {"referenceID": 12, "context": "We pit our model against the following baselines: 1) SVM with unigram, bigram, and trigram features, which is a standard yet rather strong classifier for text features; 2) SVM with average word embedding, where a document is represented as a continuous representation by averaging the embeddings of the composite words; 3) SVM with average transformed word embeddings (the x\u2032w in equation 1), where a document is represented as a continuous representation by averaging the transformed embeddings of the composite words; 4) two mature deep learning models on text classification, CNN (Kim, 2014) and Recurrent Convolutional Neural Networks (RCNN) (Lai et al.", "startOffset": 583, "endOffset": 594}, {"referenceID": 13, "context": "We pit our model against the following baselines: 1) SVM with unigram, bigram, and trigram features, which is a standard yet rather strong classifier for text features; 2) SVM with average word embedding, where a document is represented as a continuous representation by averaging the embeddings of the composite words; 3) SVM with average transformed word embeddings (the x\u2032w in equation 1), where a document is represented as a continuous representation by averaging the transformed embeddings of the composite words; 4) two mature deep learning models on text classification, CNN (Kim, 2014) and Recurrent Convolutional Neural Networks (RCNN) (Lai et al., 2015), where the hyperparameters are based on their work; 5) the above SVM and deep learning models with comment information; 6) UTCNN without user information, representing a pure-text CNN model where we use the same user matrix and user embeddings Uk and uk for each user; 7) UTCNN without the LDA model, representing how UTCNN works with a single-topic dataset; 8) UTCNN without comments, in which the model predicts the stance label given only user and topic information.", "startOffset": 646, "endOffset": 664}, {"referenceID": 12, "context": "CNN (Kim, 2014) \u221a .", "startOffset": 4, "endOffset": 15}, {"referenceID": 13, "context": "614 RCNN (Lai et al., 2015) \u221a .", "startOffset": 9, "endOffset": 27}, {"referenceID": 6, "context": "ILP (Hasan and Ng, 2013a) \u221a .", "startOffset": 4, "endOffset": 25}, {"referenceID": 6, "context": "623 ILP (Hasan and Ng, 2013a) \u221a \u221a .", "startOffset": 8, "endOffset": 29}, {"referenceID": 7, "context": "735 CRF (Hasan and Ng, 2013b) \u221a \u221a .", "startOffset": 8, "endOffset": 29}, {"referenceID": 19, "context": "728 PSL (Sridhar et al., 2015) \u221a \u221a .", "startOffset": 8, "endOffset": 30}, {"referenceID": 6, "context": "constraint tends to require that posts from the same author for the same topic hold the same stance; the ideology constraint aims to capture inferences between topics for the same author; the user-interaction constraint models relationships among posts via user interactions such as replies (Hasan and Ng, 2013a; Hasan and Ng, 2013b).", "startOffset": 291, "endOffset": 333}, {"referenceID": 7, "context": "constraint tends to require that posts from the same author for the same topic hold the same stance; the ideology constraint aims to capture inferences between topics for the same author; the user-interaction constraint models relationships among posts via user interactions such as replies (Hasan and Ng, 2013a; Hasan and Ng, 2013b).", "startOffset": 291, "endOffset": 333}, {"referenceID": 6, "context": "Compared to the ILP (Hasan and Ng, 2013a) and CRF (Hasan and Ng, 2013b) methods, the UTCNN user embeddings encode author and user-interaction constraints, where the ideology constraint is modeled by the topic embeddings and text features are modeled by the CNN.", "startOffset": 20, "endOffset": 41}, {"referenceID": 7, "context": "Compared to the ILP (Hasan and Ng, 2013a) and CRF (Hasan and Ng, 2013b) methods, the UTCNN user embeddings encode author and user-interaction constraints, where the ideology constraint is modeled by the topic embeddings and text features are modeled by the CNN.", "startOffset": 50, "endOffset": 71}, {"referenceID": 19, "context": "The PSL model (Sridhar et al., 2015) jointly labels both author and post stance using probabilistic soft logic (PSL) (Bach et al.", "startOffset": 14, "endOffset": 36}, {"referenceID": 0, "context": ", 2015) jointly labels both author and post stance using probabilistic soft logic (PSL) (Bach et al., 2015) by considering text features and reply links between authors and posts as in Hasan and Ng\u2019s work.", "startOffset": 88, "endOffset": 107}, {"referenceID": 6, "context": "2% improvement in accuracy (Hasan and Ng, 2013a).", "startOffset": 27, "endOffset": 48}], "year": 2016, "abstractText": "Most neural network models for document classification on social media focus on text information to the neglect of other information on these platforms. In this paper, we classify post stance on social media channels and develop UTCNN, a neural network model that incorporates user tastes, topic tastes, and user comments on posts. UTCNN not only works on social media texts, but also analyzes texts in forums and message boards. Experiments performed on Chinese Facebook data and English online debate forum data show that UTCNN achieves a 0.755 macroaverage f-score for supportive, neutral, and unsupportive stance classes on Facebook data, which is significantly better than models in which either user, topic, or comment information is withheld. This model design greatly mitigates the lack of data for the minor class without the use of oversampling. In addition, UTCNN yields a 0.842 accuracy on English online debate forum data, which also significantly outperforms results from previous work as well as other deep learning models, showing that UTCNN performs well regardless of language or platform.", "creator": "TeX"}}}