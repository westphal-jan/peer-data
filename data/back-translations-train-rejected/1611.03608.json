{"id": "1611.03608", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Nov-2016", "title": "Greedy Step Averaging: A parameter-free stochastic optimization method", "abstract": "In this paper we present the greedy step averaging(GSA) method, a parameter-free stochastic optimization algorithm for a variety of machine learning problems. As a gradient-based optimization method, GSA makes use of the information from the minimizer of a single sample's loss function, and takes average strategy to calculate reasonable learning rate sequence. While most existing gradient-based algorithms introduce an increasing number of hyper parameters or try to make a trade-off between computational cost and convergence rate, GSA avoids the manual tuning of learning rate and brings in no more hyper parameters or extra cost. We perform exhaustive numerical experiments for logistic and softmax regression to compare our method with the other state of the art ones on 16 datasets. Results show that GSA is robust on various scenarios.", "histories": [["v1", "Fri, 11 Nov 2016 08:23:30 GMT  (414kb)", "http://arxiv.org/abs/1611.03608v1", "23 pages, 24 figures"]], "COMMENTS": "23 pages, 24 figures", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["xiatian zhang", "fan yao", "yongjun tian"], "accepted": false, "id": "1611.03608"}, "pdf": {"name": "1611.03608.pdf", "metadata": {"source": "CRF", "title": "Greedy Step Averaging: A parameter-free stochastic optimization method", "authors": ["Xiatian Zhang", "Fan Yao", "Yongjun Tian"], "emails": ["yongjun.tian}@tendcloud.com"], "sections": [{"heading": null, "text": "In fact, most of them are able to learn in a different way, to learn in a different way, to learn in a different way, to learn in a different way, to learn in a different way \"(S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S."}], "references": [{"title": "A stochastic approximation method", "author": ["H. Robbins", "S. Monro"], "venue": "Annals of Mathematical Statistics, 22(3):400\u2013407", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1951}, {"title": "Large scale online learning", "author": ["L. Bottou", "Y. LeCun"], "venue": "NIPS", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2003}, {"title": "Incremental gradient algorithms with stepsizes bounded away from zero.\" Computational Optimization and Applications", "author": ["Solodov", "Mikhail V"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1998}, {"title": "Convergence rate of incremental subgradient algorithms.\" Stochastic optimization: algorithms and applications", "author": ["Nedi\u0107", "Angelia", "Dimitri Bertsekas"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2001}, {"title": "Introductory lectures on convex optimization: A basic course", "author": ["Y. Nesterov"], "venue": "Springer", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2004}, {"title": "Stochastic Gradient Descent for Non-smooth Optimization: Convergence Results and Optimal Averaging Schemes.", "author": ["Shamir", "Ohad", "Tong Zhang"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2013}, {"title": "On the momentum term in gradient descent learning algorithms[J", "author": ["N. Qian"], "venue": "Neural networks,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1999}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization.", "author": ["Duchi", "John", "Elad Hazan", "Yoram Singer"], "venue": "Journal of Machine Learning Research", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2011}, {"title": "ADADELTA: an adaptive learning rate method.", "author": ["Zeiler", "Matthew D"], "venue": "arXiv preprint arXiv:1212.5701", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2012}, {"title": "Primal-dual subgradient methods for convex problems", "author": ["Y. Nesterov"], "venue": "Mathematical programming, 120(1):221\u2013259", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2009}, {"title": "Acceleration of stochastic approximation by averaging.", "author": ["Polyak", "Boris T", "Anatoli B. Juditsky"], "venue": "SIAM Journal on Control and Optimization", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1992}, {"title": "A stochastic gradient method with an exponential convergence rate for finite training sets.", "author": ["Roux", "Nicolas L", "Mark Schmidt", "Francis R. Bach"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2012}, {"title": "Accelerating stochastic gradient descent using predictive variance reduction.", "author": ["Johnson", "Rie", "Tong Zhang"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2013}, {"title": "Less than a Single Pass: Stochastically Controlled Stochastic Gradient Method.", "author": ["Lei", "Lihua", "Michael I. Jordan"], "venue": "arXiv preprint arXiv:1609.03261", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2016}, {"title": "Saga: A fast incremental gradient method with support for non-strongly convex composite objectives.", "author": ["Defazio", "Aaron", "Francis Bach", "Simon Lacoste-Julien"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2014}, {"title": "Follow-the-Regularized-Leader and Mirror Descent: Equivalence Theorems and L1 Regularization.\" AISTATS", "author": ["McMahan", "H. Brendan"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2011}, {"title": "Efficient online and batch learning using forward backward splitting.", "author": ["Duchi", "John", "Yoram Singer"], "venue": "Journal of Machine Learning Research", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2009}, {"title": "Dual averaging methods for regularized stochastic learning and online optimization.", "author": ["Xiao", "Lin"], "venue": "Journal of Machine Learning Research", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2010}, {"title": "Efficient backprop.\" Neural networks: Tricks of the trade", "author": ["LeCun", "Yann A"], "venue": null, "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2012}, {"title": "Logarithmic regret algorithms for online convex optimization.", "author": ["Hazan", "Elad", "Amit Agarwal", "Satyen Kale"], "venue": "Machine Learning", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2007}, {"title": "Making gradient descent optimal for strongly convex stochastic optimization.", "author": ["Rakhlin", "Alexander", "Ohad Shamir", "Karthik Sridharan"], "venue": "arXiv preprint arXiv:1109.5647", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2011}, {"title": "An optimal algorithm for stochastic strongly-convex optimization.\" arXiv preprint", "author": ["Hazan", "Elad", "Satyen Kale"], "venue": null, "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2010}, {"title": "Parallelized stochastic gradient descent.\" Advances in neural information processing systems", "author": ["Zinkevich", "Martin"], "venue": null, "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2010}, {"title": "Communication-efficient algorithms for statistical optimization.", "author": ["Zhang", "Yuchen", "Martin J. Wainwright", "John C. Duchi"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2012}, {"title": "On the optimality of averaging in distributed statistical learning.\" Information and Inference", "author": ["Rosenblatt", "Jonathan D", "Boaz Nadler"], "venue": null, "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2016}, {"title": "A reliable effective terascale linear learning system.", "author": ["Agarwal", "Alekh"], "venue": "Journal of Machine Learning Research", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2014}, {"title": "LIBSVM : a library for support vector machines", "author": ["Chih-Chung Chang", "Chih-Jen Lin"], "venue": "ACM Transactions on Intelligent Systems and Technology,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2011}], "referenceMentions": [{"referenceID": 0, "context": "The most prevalent algorithm for these problems is stochastic gradient descent method(SGD) [1, 2].", "startOffset": 91, "endOffset": 97}, {"referenceID": 1, "context": "The most prevalent algorithm for these problems is stochastic gradient descent method(SGD) [1, 2].", "startOffset": 91, "endOffset": 97}, {"referenceID": 5, "context": "are intrinsically contradictive [6] and we have to strike a balance between the two.", "startOffset": 32, "endOffset": 35}, {"referenceID": 4, "context": "where \u03c1 is a constant depending on the condition number of g[5].", "startOffset": 60, "endOffset": 63}, {"referenceID": 2, "context": "For SGD under this strategy, we can reach a sublinear convergence rate[3, 4]: E[g(\u03c9)]\u2212 g(\u03c9) = O(1/k).", "startOffset": 70, "endOffset": 76}, {"referenceID": 3, "context": "For SGD under this strategy, we can reach a sublinear convergence rate[3, 4]: E[g(\u03c9)]\u2212 g(\u03c9) = O(1/k).", "startOffset": 70, "endOffset": 76}, {"referenceID": 9, "context": "Gradient Averaging The Gradient Averaging method [10] is equivalent to the Momentum mentioned above, if we choose the simple arithmetic average to substitute the weighted average in Momentum.", "startOffset": 49, "endOffset": 53}, {"referenceID": 10, "context": "With a suitable choice of step-sizes, this gives the same asymptotic efficiency as Newton-like second-order SG methods and also leads to increased robustness of the convergence rate to the exact sequence of step sizes [11].", "startOffset": 218, "endOffset": 222}, {"referenceID": 10, "context": "It has been proved that under certain assumptions of appropriate step-size, this method enjoys a second-order convergence rate[11].", "startOffset": 126, "endOffset": 130}, {"referenceID": 11, "context": "In [12], the authors show that the SAG iterations have a linear convergence rate.", "startOffset": 3, "endOffset": 7}, {"referenceID": 12, "context": "SV RG Stochastic variance reduced gradient(SVRG) introduces an explicit variance reduction method for SGD[13].", "startOffset": 105, "endOffset": 109}, {"referenceID": 14, "context": "Another variation of SVRG is SAGA [15] which is claimed to support non-strongly convex problems directly and has a better convergence rate.", "startOffset": 34, "endOffset": 38}, {"referenceID": 13, "context": "As a great improvement of SVRG, the computation cost and the communication cost of SCSG do not necessarily scale linearly with sample size n[14].", "startOffset": 140, "endOffset": 144}, {"referenceID": 7, "context": "Adagrad Adagrad [8] is an algorithm for gradient-based optimization, it adapts the learning rate to the parameters, performing larger updates for infrequent and smaller updates for frequent parameters.", "startOffset": 16, "endOffset": 19}, {"referenceID": 8, "context": "Adadelta Adadelta [9] is an extension of Adagrad that seeks to reduce its aggressive, monotonically decreasing learning rate.", "startOffset": 18, "endOffset": 21}, {"referenceID": 18, "context": "Instead, we set a confidence threshold (in fact this idea is also recommended by Yann Lecun for training neural network [19]) at, for example, p\u03021 = 0.", "startOffset": 120, "endOffset": 124}, {"referenceID": 2, "context": "Similarly, in [3] the author also established an inequality \u2016\u2207f(\u03c9)\u2016 \u2264 C\u03b70 under some reasonable assumptions.", "startOffset": 14, "endOffset": 17}, {"referenceID": 8, "context": "2 we see that the performance of Adadelta significantly depends on \u03b5, which is not like what the author claimed in [9].", "startOffset": 115, "endOffset": 118}, {"referenceID": 19, "context": "Since the step sizes of GSA do not necessarily converge to zero, there are several averaging scheme over iteration points to reduce the variance [20, 21, 22].", "startOffset": 145, "endOffset": 157}, {"referenceID": 20, "context": "Since the step sizes of GSA do not necessarily converge to zero, there are several averaging scheme over iteration points to reduce the variance [20, 21, 22].", "startOffset": 145, "endOffset": 157}, {"referenceID": 21, "context": "Since the step sizes of GSA do not necessarily converge to zero, there are several averaging scheme over iteration points to reduce the variance [20, 21, 22].", "startOffset": 145, "endOffset": 157}, {"referenceID": 19, "context": "Another procedure, for which the standard online analysis of SGD applies [20], is to return the average point \u03c9\u0304T = 1 T (\u03c91 + \u00b7 \u00b7 \u00b7+ \u03c9T ).", "startOffset": 73, "endOffset": 77}, {"referenceID": 21, "context": "Recently, in [22] they indicate that O(log(T )/T ) is not the best that one can achieve for strongly convex stochastic problems and have proposed a different algorithm, which is somewhat similar to SGD to achieve the O(1/T ) bound.", "startOffset": 13, "endOffset": 17}, {"referenceID": 20, "context": "Besides, in [21] they present a new average tactics called \u03b1-suffix averaging to recover the O(1/T ) rate.", "startOffset": 12, "endOffset": 16}], "year": 2016, "abstractText": "In this paper we present the greedy step averaging(GSA) method, a parameter-free stochastic optimization algorithm for a variety of machine learning problems. As a gradient-based optimization method, GSA makes use of the information from the minimizer of a single sample\u2019s loss function, and takes average strategy to calculate reasonable learning rate sequence. While most existing gradient-based algorithms introduce an increasing number of hyper parameters or try to make a trade-off between computational cost and convergence rate, GSA avoids the manual tuning of learning rate and brings in no more hyper parameters or extra cost. We perform exhaustive numerical experiments for logistic and softmax regression to compare our method with the other state of the art ones on 16 datasets. Results show that GSA is robust on various scenarios.", "creator": "LaTeX with hyperref package"}}}