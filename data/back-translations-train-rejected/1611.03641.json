{"id": "1611.03641", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Nov-2016", "title": "Improving Reliability of Word Similarity Evaluation by Redesigning Annotation Task and Performance Measure", "abstract": "We suggest a new method for creating and using gold-standard datasets for word similarity evaluation. Our goal is to improve the reliability of the evaluation, and we do this by redesigning the annotation task to achieve higher inter-rater agreement, and by defining a performance measure which takes the reliability of each annotation decision in the dataset into account.", "histories": [["v1", "Fri, 11 Nov 2016 10:06:29 GMT  (42kb,D)", "http://arxiv.org/abs/1611.03641v1", null], ["v2", "Mon, 27 Feb 2017 18:38:56 GMT  (34kb,D)", "http://arxiv.org/abs/1611.03641v2", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["oded avraham", "yoav goldberg"], "accepted": false, "id": "1611.03641"}, "pdf": {"name": "1611.03641.pdf", "metadata": {"source": "CRF", "title": "Improving Reliability of Word Similarity Evaluation by Redesigning Annotation Task and Performance Measure", "authors": ["Oded Avraham"], "emails": ["oavraham1@gmail.com", "yoav.goldberg@gmail.com"], "sections": [{"heading": "1 Introduction", "text": "Calculating the similarity between words is a fundamental challenge in processing natural language. Given a pair of words, a similarity model should assign a value to sim (w1, w2) that reflects the degree of similarity between words, e.g.: sim (singer, musician) = 0.83. While there are many methods of calculating sim (e.g. determining the cosine between vector embeddings derived from word2vec (Mikolov et al., 2013)), there are currently no reliable metrics for the quality of such models. In recent years, word similarity models have shown consistent performance improvement in evaluating with conventional evaluation methods and datasets. But are these evaluation metrics really reliable indicators of model quality? Recently Hill et al (2015) claimed that the answer is no. They identified several problems with the existing datasets and created a new dataset - Sim99-Lex - that does not suffer from them."}, {"heading": "2 Existing Methods and Datasets for Word Similarity Evaluation", "text": "Over the years, several datasets have been used to evaluate word similarity models. Popular datasets include RG (Rubenstein and Goodenough, 1965), WordSim-353 (Finkelstein et al., 2001), WS-Sim (Agirre et al., 2009), and MEN (Bruni et al., 2012). Each of these datasets is a collection of word pairs along with their similarity values as assigned by human annotators. A model is evaluated by assigning a similarity value to each pair by sorting the pairs according to their similarity and calculating the correlation (Spearmans) with the human ranking. Hill et al (2015) had conducted a comprehensive review of these datasets and pointed out some common shortcomings that they have. The main flaw that Hill et al has discussed is the handling of associated but dissimilar words, e.g. (singer, microphone) associates the datasets that contain such pairs."}, {"heading": "3 Problems with the Existing Datasets", "text": "This year, it has come to the point where it only takes one year to get to the next round."}, {"heading": "4 Proposed Improvements", "text": "We propose the following four improvements in dealing with these problems. (1) The annotation task will be an explicit ranking task. Similar to Bruni et al (2012), each pair will be directly compared with a subset of the other pairs. In contrast to Bruni et al, each pair will be compared with only a few carefully selected pairs following the principles in (2) and (3). (2) A data set will focus on a single preferred type (we can create other data sets for tasks where the preferential relationship is different), and only preferential relationship pairs will be presented to the annotators. We propose to spare the annotators the trouble of considering the type of similarity between words in order to let them focus on the strength of similarity. Word pairs that do not follow preferential relationships will not be included in the annotation task, but will still be a part of the data set - we will always add them at the bottom of the rankings to focus on the strength of similarity."}, {"heading": "4.1 A Concrete Dataset", "text": "In this section, we describe the structure of a data set that applies the above improvements. First, > we need to define the preferred relationship (to apply improvements (2)). In the following, we will use the Hyponym-Hypernym relationship. The data set is based on target words. For each target word, we create a group of candidate words that we designate as a target audience. Each candidate word belongs to one of three categories: Positive (in terms of the target, and the type of relationship is the preferred one), Distractors (in terms of the target, but the type of relationship is not the preferred one), and Random (in terms of the target at all). For the target word singer, the target group may include musicians, performers, person and artists as positives, dancers and songs as distractions, and laptops as random.For each target word, human annotators are asked to rate the positive candidates by their similarity to the target word (1)."}, {"heading": "4.2 Scoring Function", "text": "Given a similarity function between the words sim (x, y) and a triplet (wt, w1, w2), the score s (wt, w1, w2) = sim (wt, w2) = sim (wt, w2) \u2212 1. The score s (wt, w1, w2) of the triplet is then: s (wt, w1, w2) = \u03b4 (2R > (w1, w2; wt) \u2212 1. This score ranges between \u2212 1 and 1, is positive if the rating of the model matches more than 50% of the annotations, and is 1 if it matches all of them. The score of the whole dataset C is then: \u2211 wt, w1, w2 \u0445 C max (s (wt, w1, w2), 0)."}, {"heading": "5 Experiments", "text": "The datasets contain Hebrew nouns, but such datasets can be created for different languages and parts of the word - provided the language has basic lexical resources. For our datasets, we have used a dictionary, an encyclopaedia, and a thesaurus to create the hyponym-hypernym pairs, and databases of word association norms (Rubinsten et al., 2005) and category norms (Henik and Kaplan, 1988) to create the distractor pairs and the cohonyme pairs, respectively. The dataset gap is based on 75 targetgroups, each containing 3-6 positive pairs, 2 random pairs, and a random pair containing 476 pairs."}, {"heading": "6 Conclusions", "text": "We introduced a new method of creating and using word similarity records implemented by code.google.com / p / word2vec, with a window size of 2 and a dimensionality of 200.Evaluation reliability by redesigning the annotation task and performance measurement. We created two data sets for Hebrew and showed a high interrater match. Finally, we showed that the data set can be used for a finer-grained analysis of model quality. Future work can apply this method to other languages and relationship types."}, {"heading": "Acknowledgements", "text": "The work was supported by the Israeli Science Foundation (grant number 1555 / 15) and we thank Omer Levy for useful discussions."}], "references": [{"title": "A study on similarity and relatedness using distributional and wordnet-based approaches", "author": ["Eneko Agirre", "Enrique Alfonseca", "Keith Hall", "Jana Kravalova", "Marius Pa\u015fca", "Aitor Soroa."], "venue": "Proceedings of Human Language Technologies: The 2009", "citeRegEx": "Agirre et al\\.,? 2009", "shortCiteRegEx": "Agirre et al\\.", "year": 2009}, {"title": "Distributional semantics in technicolor", "author": ["Elia Bruni", "Gemma Boleda", "Marco Baroni", "NamKhanh Tran."], "venue": "Proceedings of the 50th Annual Meet-", "citeRegEx": "Bruni et al\\.,? 2012", "shortCiteRegEx": "Bruni et al\\.", "year": 2012}, {"title": "Placing search in context: The concept revisited", "author": ["Lev Finkelstein", "Evgeniy Gabrilovich", "Yossi Matias", "Ehud Rivlin", "Zach Solan", "Gadi Wolfman", "Eytan Ruppin."], "venue": "Proceedings of the 10th international conference on World Wide Web, pages 406\u2013", "citeRegEx": "Finkelstein et al\\.,? 2001", "shortCiteRegEx": "Finkelstein et al\\.", "year": 2001}, {"title": "Rating the rating scales", "author": ["Hershey H Friedman", "Taiwo Amoo."], "venue": "Friedman, Hershey H. and Amoo, Taiwo (1999).\u201d Rating the Rating Scales.\u201d Journal of Marketing Management, Winter, pages 114\u2013123.", "citeRegEx": "Friedman and Amoo.,? 1999", "shortCiteRegEx": "Friedman and Amoo.", "year": 1999}, {"title": "Category content: Findings for categories in hebrew and a comparison to findings in the us", "author": ["Avishai Henik", "Limor Kaplan."], "venue": "Psychologia: Israel Journal of Psychology.", "citeRegEx": "Henik and Kaplan.,? 1988", "shortCiteRegEx": "Henik and Kaplan.", "year": 1988}, {"title": "Simlex-999: Evaluating semantic models with (genuine) similarity estimation", "author": ["Felix Hill", "Roi Reichart", "Anna Korhonen."], "venue": "Computational Linguistics.", "citeRegEx": "Hill et al\\.,? 2015", "shortCiteRegEx": "Hill et al\\.", "year": 2015}, {"title": "Better word representations with recursive neural networks for morphology", "author": ["Thang Luong", "Richard Socher", "Christopher D Manning."], "venue": "CoNLL, pages 104\u2013113. Citeseer.", "citeRegEx": "Luong et al\\.,? 2013", "shortCiteRegEx": "Luong et al\\.", "year": 2013}, {"title": "Efficient estimation of word representations in vector space", "author": ["Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean."], "venue": "arXiv preprint arXiv:1301.3781.", "citeRegEx": "Mikolov et al\\.,? 2013", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Contextual correlates of synonymy", "author": ["Herbert Rubenstein", "John B Goodenough."], "venue": "Communications of the ACM, 8(10):627\u2013633.", "citeRegEx": "Rubenstein and Goodenough.,? 1965", "shortCiteRegEx": "Rubenstein and Goodenough.", "year": 1965}, {"title": "Free association norms in the hebrew language", "author": ["O Rubinsten", "D Anaki", "A Henik", "S Drori", "Y Faran."], "venue": "Word norms in Hebrew, pages 17\u201334.", "citeRegEx": "Rubinsten et al\\.,? 2005", "shortCiteRegEx": "Rubinsten et al\\.", "year": 2005}], "referenceMentions": [{"referenceID": 7, "context": ", taking the cosine between vector embeddings derived by word2vec (Mikolov et al., 2013)), there are currently no reliable measures of quality for such models.", "startOffset": 66, "endOffset": 88}, {"referenceID": 8, "context": "Popular ones include RG (Rubenstein and Goodenough, 1965), WordSim-353 (Finkelstein et al.", "startOffset": 24, "endOffset": 57}, {"referenceID": 2, "context": "Popular ones include RG (Rubenstein and Goodenough, 1965), WordSim-353 (Finkelstein et al., 2001), WS-Sim (Agirre et al.", "startOffset": 71, "endOffset": 97}, {"referenceID": 0, "context": ", 2001), WS-Sim (Agirre et al., 2009) and MEN (Bruni et", "startOffset": 16, "endOffset": 37}, {"referenceID": 3, "context": "makes the annotations vulnerable to a variety of biases (Friedman and Amoo, 1999).", "startOffset": 56, "endOffset": 81}, {"referenceID": 3, "context": "makes the annotations vulnerable to a variety of biases (Friedman and Amoo, 1999). Bruni et al (2012) addressed this problem by asking the annotators to rank each pair in comparison to 50 randomly selected pairs.", "startOffset": 57, "endOffset": 102}, {"referenceID": 9, "context": "For our dataset, we used a dictionary, an encyclopedia and a thesaurus to create the hyponym-hypernym pairs, and databases of word association norms (Rubinsten et al., 2005) and categories norms (Henik and Kaplan, 1988) to create the distractors pairs and the cohyponyms pairs, respectively.", "startOffset": 149, "endOffset": 173}, {"referenceID": 4, "context": ", 2005) and categories norms (Henik and Kaplan, 1988) to create the distractors pairs and the cohyponyms pairs, respectively.", "startOffset": 29, "endOffset": 53}, {"referenceID": 5, "context": "We measured the average pairwise inter-rater agreement, and as done in (Hill et al., 2015) \u2013 we excluded any annotator which its agreement with the other was more than one standard deviation below that average (17.", "startOffset": 71, "endOffset": 90}, {"referenceID": 7, "context": "To measure the gap between a human and a model performance on the dataset, we trained a word2vec (Mikolov et al., 2013) model 1 on the Hebrew Wikipedia.", "startOffset": 97, "endOffset": 119}], "year": 2016, "abstractText": "We suggest a new method for creating and using gold-standard datasets for word similarity evaluation. Our goal is to improve the reliability of the evaluation, and we do this by redesigning the annotation task to achieve higher inter-rater agreement, and by defining a performance measure which takes the reliability of each annotation decision in the dataset into account.", "creator": "TeX"}}}