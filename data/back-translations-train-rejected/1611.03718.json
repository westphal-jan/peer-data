{"id": "1611.03718", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Nov-2016", "title": "Hierarchical Object Detection with Deep Reinforcement Learning", "abstract": "We present a method for performing hierarchical object detection in images guided by a deep reinforcement learning agent. The key idea is to focus on those parts of the image that contain richer information and zoom on them. We train an intelligent agent that, given an image window, is capable of deciding where to focus the attention among five different predefined region candidates (smaller windows). This procedure is iterated providing a hierarchical image analysis.We compare two different candidate proposal strategies to guide the object search: with and without overlap. Moreover, our work compares two different strategies to extract features from a convolutional neural network for each region proposal: a first one that computes new feature maps for each region proposal, and a second one that computes the feature maps for the whole image to later generate crops for each region proposal. Experiments indicate better results for the overlapping candidate proposal strategy and a loss of performance for the cropped image features due to the loss of spatial resolution. We argue that, while this loss seems unavoidable when working with large amounts of object candidates, the much more reduced amount of region proposals generated by our reinforcement learning agent allows considering to extract features for each location without sharing convolutional computation among regions.", "histories": [["v1", "Fri, 11 Nov 2016 14:25:54 GMT  (849kb,D)", "http://arxiv.org/abs/1611.03718v1", "Deep Reinforcement Learning Workshop (NIPS 2016). Project page atthis https URL"], ["v2", "Fri, 25 Nov 2016 14:31:07 GMT  (1277kb,D)", "http://arxiv.org/abs/1611.03718v2", "Deep Reinforcement Learning Workshop (NIPS 2016). Project page atthis https URL"]], "COMMENTS": "Deep Reinforcement Learning Workshop (NIPS 2016). Project page atthis https URL", "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["miriam bellver", "xavier giro-i-nieto", "ferran marques", "jordi torres"], "accepted": false, "id": "1611.03718"}, "pdf": {"name": "1611.03718.pdf", "metadata": {"source": "CRF", "title": "Hierarchical Object Detection with Deep Reinforcement Learning", "authors": ["M\u00edriam Bellver Bueno"], "emails": ["miriam.bellver@bsc.es", "xavier.giro@upc.edu", "ferran.marques@upc.edu", "jordi.torres@bsc.es"], "sections": [{"heading": "1 Introduction", "text": "In fact, we are able to go in search of a solution that meets the needs of the individual."}, {"heading": "2 Related Work", "text": "In fact, most of them are able to survive on their own if they do not play by the rules."}, {"heading": "3 Hierarchical Object Detection Model", "text": "In this work, we define the object recognition problem as the sequential decision-making process of a goal-oriented agent that interacts with a visual environment that is our image. At each step, the agent should decide in which region of the image he focuses his attention so that he can find objects in a few steps. We consider the problem as a Markov decision-making process that provides a framework for modeling decision-making when the results are partially uncertain."}, {"heading": "3.1 MDP formulation", "text": "To understand the models for the object recognition task that we have developed, we first define how the Markov decision process b is parameterized. Condition The state consists of the descriptor of the current region and a memory vector. The type of the descriptor defines the two models that we compare in our work: the image zoom model and the Pool45 crop model. These two variations are explained in detail in Section 3.3. The memory vector of the state captures the last 4 actions that the actor has already performed in the search for an object. As the actor learns a refinement of a bounding field, a memory vector that encodes the state of this refinement process is useful to stabilize the search paths. We encode the last 4 actions in a single vector. As there are 6 different actions that are presented in the following section, the memory vector has five dimensions."}, {"heading": "3.2 Q-learning", "text": "The reward of the agent depending on the selected action a in state s is determined by a function Q (s, a), which can be estimated with Q-Learning. Based on Q (s, a), the agent selects the action associated with the highest reward. Q-Learning iteratively updates the action selection policy using the Bellman Equation 3, where s and a are the current state and action corresponding, r is the immediate reward and maxaQ (s \u2032, a \u2032) is the future reward. Finally, \u03b3 represents the discount factor. In our work, we approach the Q function through a Deep Q network trained with Reinforcement Learning [11].Q (s, a) = r + \u03b3maxaQ (s \u2032, a \u2032) (3)."}, {"heading": "3.3 Model", "text": "In our thesis, we examine two different approaches to extracting visual characteristics that are used to train a Deep Q network. Figure 3 shows the two variants with the common learning network for reinforcement. We compare two models to extract the visual characteristics that define the state of our agent: the ImageZooms model and the Pool45 crop model. In the Image Zooms model, each region is magnified to 224x224 and its visual descriptors correspond to the characteristic maps from the Pool5 layer of VGG-16 [17]. In the Pool45 crop model, the image is transmitted in full resolution in VGG-16 [17] through the Pool5 layer. As a Girshick [6], we use the characteristic maps extracted from the entire image for all regions of interest (ROI) by merging them (ROI pooling)."}, {"heading": "3.4 Training", "text": "In this section, we will explain the peculiarities we have chosen to train the Q network. Exploration-Exploitation To learn the deep Q network with reinforcement, we use a -greedy policy that starts with = 1 and decreases in increments from 0.1 to = 0.1. Then we start with random actions, and in each epoch, the agent makes decisions based more on the policies already learned. In fact, to help the agent learn the terminal action, which might be difficult to learn by chance, we force it every time the current region has an IoU > 0.5. With this approach, we can speed up the training. Note that we always carry out exploration so that we do not get stuck in a local minimum.Learning Paths One fact that we realized during the training was that we should not impose which object of the image should be viewed first. At each time step, the agent will focus on the object in the current region with the highest overlap with its current region."}, {"heading": "4 Experiments", "text": "For our object detection experiments, we used images and annotations from the PASCAL VOC dataset [5]. We trained our system on the trainval sets of 2007 and 2012 and tested it on the 2007 test set. We conducted all experiments for only one class, the aircraft category, and included only images with the target class category. This experiment allows us to study the behavior of our agent and estimate the number of regions that need to be analyzed to detect an object."}, {"heading": "4.1 Qualitative results", "text": "We present some qualitative results in Figure 4 to show how our agent behaves on test images. These results are obtained with the image zoom model with overlapping regions, as it provides the best results as explained in the following sections. We observed that the model successfully zooms on to the object in most images and completes the task in a few steps. As seen in the second, third and fourth rows, the agent selects the delimiter field around the object in just two or three steps."}, {"heading": "4.2 Precision-Recall curves", "text": "We will seek the precision and return to curves for various practiced models in which an object is explored, while all regions are analyzed by an agent. We will wait for the reward accessed by a network of six."}, {"heading": "4.3 Number of regions analyzed per object", "text": "A histogram of the number of regions analyzed by our agent is shown in 8. We observe that the largest part of the objects is already found in a single step, which means that the object occupies the largest part of the image. In less than 3 steps we can approach almost all the objects we can detect."}, {"heading": "5 Conclusions", "text": "Our experiments suggest that objects can be detected with very few suggestions from a suitable hierarchy, but that working with a predefined set of regions significantly limits the recall. A possible solution to this problem would be to refine the approximate detection by the agent with a regressor, as in [15]. Finally, our results show the limitations of the characteristics of cut regions from the coil layers, especially when taking small objects into account. We suggest that given the much smaller number of regional suggestions considered by our reinforcement learning agent, the supply of each region through the network should also be taken into account. The presented work is publicly available for reproducibility and expansion at https: / / imatge-upc. github.io / detection-2016-nipsws /."}, {"heading": "Acknowledgments", "text": "This work was developed as part of the BigGraph TEC2013-43935-R project, which is funded by the Spanish Ministerio de Economia y Competitividad and the European Regional Development Fund (ERDF). This work was supported by the Spanish Government's Severo Ochoa Scholarship SEV2015-0493, the TIN2015-65316 project by the contracts of the Spanish Ministry of Science and Innovation 2014-SGR-1051 by the Generalitat de Catalunya. The machine vision group at the UPC is an SGR14 Consolidated Research Group recognized and sponsored by the Catalan Government (Generalitat de Catalunya) through its AGAUR office. We thank NVIDIA Corporation for supporting the GeForce GTX Titan Z used in this work and the support of the BSC / UPC NVIDIA GPU Center of Excellence."}], "references": [{"title": "Multiple object recognition with visual attention", "author": ["Jimmy Ba", "Volodymyr Mnih", "Koray Kavukcuoglu"], "venue": "arXiv preprint arXiv:1412.7755,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2014}, {"title": "Active object localization with deep reinforcement learning", "author": ["Juan C Caicedo", "Svetlana Lazebnik"], "venue": "In Proceedings of the IEEE International Conference on Computer Vision,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2015}, {"title": "Cpmc: Automatic object segmentation using constrained parametric min-cuts", "author": ["Joao Carreira", "Cristian Sminchisescu"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2012}, {"title": "Scalable object detection using deep neural networks", "author": ["Dumitru Erhan", "Christian Szegedy", "Alexander Toshev", "Dragomir Anguelov"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2014}, {"title": "The pascal visual object classes (voc) challenge", "author": ["Mark Everingham", "Luc Van Gool", "Christopher KI Williams", "John Winn", "Andrew Zisserman"], "venue": "International journal of computer vision,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2010}, {"title": "Fast r-cnn", "author": ["Ross Girshick"], "venue": "In Proceedings of the IEEE International Conference on Computer Vision,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2015}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2014}, {"title": "Ssd: Single shot multibox detector", "author": ["Wei Liu", "Dragomir Anguelov", "Dumitru Erhan", "Christian Szegedy", "Scott Reed"], "venue": "arXiv preprint arXiv:1512.02325,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2015}, {"title": "Adaptive object detection using adjacency and zoom prediction", "author": ["Yongxi Lu", "Tara Javidi", "Svetlana Lazebnik"], "venue": "arXiv preprint arXiv:1512.07711,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2015}, {"title": "Recurrent models of visual attention", "author": ["Volodymyr Mnih", "Nicolas Heess", "Alex Graves"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2014}, {"title": "Human-level control through deep reinforcement learning", "author": ["Volodymyr Mnih", "Koray Kavukcuoglu", "David Silver", "Andrei A Rusu", "Joel Veness", "Marc G Bellemare", "Alex Graves", "Martin Riedmiller", "Andreas K Fidjeland", "Georg Ostrovski"], "venue": "Nature, 518(7540):529\u2013533,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2015}, {"title": "Rectified linear units improve restricted boltzmann machines", "author": ["Vinod Nair", "Geoffrey E Hinton"], "venue": "In Proceedings of the 27th International Conference on Machine Learning", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2010}, {"title": "Multiscale combinatorial grouping for image segmentation and object proposal generation", "author": ["Jordi Pont-Tuset", "Pablo Arbel\u00e1ez", "Jonathan T. Barron", "Ferran Marques", "Jitendra Malik"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI),", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2016}, {"title": "You only look once: Unified, real-time object detection", "author": ["Joseph Redmon", "Santosh Divvala", "Ross Girshick", "Ali Farhadi"], "venue": "arXiv preprint arXiv:1506.02640,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2015}, {"title": "Faster r-cnn: Towards real-time object detection with region proposal networks. In Advances in neural information processing", "author": ["Shaoqing Ren", "Kaiming He", "Ross Girshick", "Jian Sun"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2015}, {"title": "Mastering the game of go with deep neural networks and tree", "author": ["David Silver", "Aja Huang", "Chris J Maddison", "Arthur Guez", "Laurent Sifre", "George Van Den Driessche", "Julian Schrittwieser", "Ioannis Antonoglou", "Veda Panneershelvam", "Marc Lanctot"], "venue": "search. Nature,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2016}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "CoRR, abs/1409.1556,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2014}, {"title": "Dropout: a simple way to prevent neural networks from overfitting", "author": ["Nitish Srivastava", "Geoffrey E Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1929}, {"title": "Segmentation as selective search for object recognition", "author": ["Koen EA Van de Sande", "Jasper RR Uijlings", "Theo Gevers", "Arnold WM Smeulders"], "venue": "In 2011 International Conference on Computer Vision,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2011}, {"title": "Show, attend and tell: Neural image caption generation with visual attention", "author": ["Kelvin Xu", "Jimmy Ba", "Ryan Kiros", "Kyunghyun Cho", "Aaron Courville", "Ruslan Salakhutdinov", "Richard S Zemel", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1502.03044,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2015}, {"title": "End-to-end learning of action detection from frame glimpses in videos", "author": ["Serena Yeung", "Olga Russakovsky", "Greg Mori", "Li Fei-Fei"], "venue": "arXiv preprint arXiv:1511.06984,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2015}, {"title": "Attentionnet: Aggregating weak directions for accurate object detection", "author": ["Donggeun Yoo", "Sunggyun Park", "Joon-Young Lee", "Anthony S Paek", "In So Kweon"], "venue": "In Proceedings of the IEEE International Conference on Computer Vision,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2015}, {"title": "Edge boxes: Locating object proposals from edges", "author": ["C Lawrence Zitnick", "Piotr Doll\u00e1r"], "venue": "In European Conference on Computer Vision,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2014}], "referenceMentions": [{"referenceID": 1, "context": "Our algorithm is based on an intelligent agent trained by reinforcement learning that is capable of making decisions to detect an object in a still image, similarly to [2].", "startOffset": 168, "endOffset": 171}, {"referenceID": 8, "context": "Inspired by [9], our agent can top-down explore a set of five different predefined region candidates: four regions representing the four quadrants plus a central region.", "startOffset": 12, "endOffset": 15}, {"referenceID": 10, "context": "The most impressive results are those from DeepMind [11], who have been able to train an agent that plays Atari 2600 video games by observing only their screen pixels, achieving even superhuman performance.", "startOffset": 52, "endOffset": 56}, {"referenceID": 15, "context": "Also they trained a computer that won the Go competition to a professional player for the first time [16].", "startOffset": 101, "endOffset": 105}, {"referenceID": 9, "context": "More specifically to traditional computer vision tasks, reinforcement learning has been applied to learn spatial glimpse policies for image classification [10, 1], for captioning [20] or for activity recognition [21].", "startOffset": 155, "endOffset": 162}, {"referenceID": 0, "context": "More specifically to traditional computer vision tasks, reinforcement learning has been applied to learn spatial glimpse policies for image classification [10, 1], for captioning [20] or for activity recognition [21].", "startOffset": 155, "endOffset": 162}, {"referenceID": 19, "context": "More specifically to traditional computer vision tasks, reinforcement learning has been applied to learn spatial glimpse policies for image classification [10, 1], for captioning [20] or for activity recognition [21].", "startOffset": 179, "endOffset": 183}, {"referenceID": 20, "context": "More specifically to traditional computer vision tasks, reinforcement learning has been applied to learn spatial glimpse policies for image classification [10, 1], for captioning [20] or for activity recognition [21].", "startOffset": 212, "endOffset": 216}, {"referenceID": 1, "context": "It has also been applied for object detection in images [2], casting a Markov Decision Process, as our approach does.", "startOffset": 56, "endOffset": 59}, {"referenceID": 18, "context": "The traditional solutions for object detection are based on region proposals, such as Selective Search [19], CPMC [3] or MCG [13] and other methods based on sliding windows such as EdgeBoxes [23].", "startOffset": 103, "endOffset": 107}, {"referenceID": 2, "context": "The traditional solutions for object detection are based on region proposals, such as Selective Search [19], CPMC [3] or MCG [13] and other methods based on sliding windows such as EdgeBoxes [23].", "startOffset": 114, "endOffset": 117}, {"referenceID": 12, "context": "The traditional solutions for object detection are based on region proposals, such as Selective Search [19], CPMC [3] or MCG [13] and other methods based on sliding windows such as EdgeBoxes [23].", "startOffset": 125, "endOffset": 129}, {"referenceID": 22, "context": "The traditional solutions for object detection are based on region proposals, such as Selective Search [19], CPMC [3] or MCG [13] and other methods based on sliding windows such as EdgeBoxes [23].", "startOffset": 191, "endOffset": 195}, {"referenceID": 5, "context": "Then the first trends based on Convolutional Neural Networks appeared, such as Fast R-CNN [6], that already studied how to share convolutional computation among locations, as they identified that the extraction of features for the hypothesized objects was the bottleneck for object detection.", "startOffset": 90, "endOffset": 93}, {"referenceID": 14, "context": "More recent proposals such as Faster R-CNN [15] have achieved efficient and fast object detection by obtaining cost-free region proposals sharing full-image convolutional features with the detection network.", "startOffset": 43, "endOffset": 47}, {"referenceID": 13, "context": "[14] or MultiBox [4] based on anchors, there are other works that are based on the refinement of predictions.", "startOffset": 0, "endOffset": 4}, {"referenceID": 3, "context": "[14] or MultiBox [4] based on anchors, there are other works that are based on the refinement of predictions.", "startOffset": 17, "endOffset": 20}, {"referenceID": 21, "context": "[22] propose the AttentionNet.", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "The state-of-the-art in object detection is the Single Shot MultiBox Detector (SSD) [8], which works with a number of default boxes of different aspect ratios and scales per each feature map location, and also adjusts them to a better match to the object shape.", "startOffset": 84, "endOffset": 87}, {"referenceID": 1, "context": "Another approach that supports this idea is the Active Object Localization method proposed by Caicedo and Lazebnik [2].", "startOffset": 115, "endOffset": 118}, {"referenceID": 8, "context": "[9] a model is trained to determine if it is required to further divide the current observed region because there are still small objects on it, and in this case, each subregion is analyzed independently.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "This type of memory vector was also used in [2].", "startOffset": 44, "endOffset": 47}, {"referenceID": 1, "context": "Reward The reward functions used are the ones proposed by Caicedo and Lazebnik [2].", "startOffset": 79, "endOffset": 82}, {"referenceID": 1, "context": "5 , because it is the threshold for which a detection is considered positive, and \u03b7 is 3, as in [2].", "startOffset": 96, "endOffset": 99}, {"referenceID": 10, "context": "In our work, we approximate the Q-function by a Deep Q-network trained with Reinforcement Learning [11].", "startOffset": 99, "endOffset": 103}, {"referenceID": 16, "context": "For the Image-Zooms model, each region is resized to 224x224 and its visual descriptors correspond to the feature maps from Pool5 layer of VGG-16 [17].", "startOffset": 146, "endOffset": 150}, {"referenceID": 16, "context": "For the Pool45-Crops model, the image at full-resolution is forwarded into VGG-16 [17] through Pool5 layer.", "startOffset": 82, "endOffset": 86}, {"referenceID": 5, "context": "As Girshick [6], we reuse the feature maps extracted from the whole image for all the regions of interest (ROI) by pooling them (ROI pooling).", "startOffset": 12, "endOffset": 15}, {"referenceID": 7, "context": "As in SSD [8], we choose which feature map to use depending on the scale of the region of interest.", "startOffset": 10, "endOffset": 13}, {"referenceID": 11, "context": "Each fully connected layer is followed by a ReLU [12] activation function and is trained with dropout [18].", "startOffset": 49, "endOffset": 53}, {"referenceID": 17, "context": "Each fully connected layer is followed by a ReLU [12] activation function and is trained with dropout [18].", "startOffset": 102, "endOffset": 106}, {"referenceID": 6, "context": "For learning, we used Adam optimizer [7] with a learning rate of 1e-6 to avoid that the gradients explode.", "startOffset": 37, "endOffset": 40}, {"referenceID": 4, "context": "Our experiments on object detection have used images and annotations from the PASCAL VOC dataset [5].", "startOffset": 97, "endOffset": 100}, {"referenceID": 4, "context": "5, as defined by the Pascal VOC challenge [5].", "startOffset": 42, "endOffset": 45}, {"referenceID": 14, "context": "solution to this problem would be refining the approximate detections provided by the agent with a regressor, as in [15].", "startOffset": 116, "endOffset": 120}], "year": 2016, "abstractText": "We present a method for performing hierarchical object detection in images guided by a deep reinforcement learning agent. The key idea is to focus on those parts of the image that contain richer information and zoom on them. We train an intelligent agent that, given an image window, is capable of deciding where to focus the attention among five different predefined region candidates (smaller windows). This procedure is iterated providing a hierarchical image analysis.We compare two different candidate proposal strategies to guide the object search: with and without overlap. Moreover, our work compares two different strategies to extract features from a convolutional neural network for each region proposal: a first one that computes new feature maps for each region proposal, and a second one that computes the feature maps for the whole image to later generate crops for each region proposal. Experiments indicate better results for the overlapping candidate proposal strategy and a loss of performance for the cropped image features due to the loss of spatial resolution. We argue that, while this loss seems unavoidable when working with large amounts of object candidates, the much more reduced amount of region proposals generated by our reinforcement learning agent allows considering to extract features for each location without sharing convolutional computation among regions. Source code and models are available at https://imatge-upc.github.io/detection-2016-nipsws/.", "creator": "LaTeX with hyperref package"}}}