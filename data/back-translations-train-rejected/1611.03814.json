{"id": "1611.03814", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Nov-2016", "title": "Towards the Science of Security and Privacy in Machine Learning", "abstract": "Advances in machine learning (ML) in recent years have enabled a dizzying array of applications such as data analytics, autonomous systems, and security diagnostics. ML is now pervasive---new systems and models are being deployed in every domain imaginable, leading to rapid and widespread deployment of software based inference and decision making. There is growing recognition that ML exposes new vulnerabilities in software systems, yet the technical community's understanding of the nature and extent of these vulnerabilities remains limited. We systematize recent findings on ML security and privacy, focusing on attacks identified on these systems and defenses crafted to date. We articulate a comprehensive threat model for ML, and categorize attacks and defenses within an adversarial framework. Key insights resulting from works both in the ML and security communities are identified and the effectiveness of approaches are related to structural elements of ML algorithms and the data used to train them. We conclude by formally exploring the opposing relationship between model accuracy and resilience to adversarial manipulation. Through these explorations, we show that there are (possibly unavoidable) tensions between model complexity, accuracy, and resilience that must be calibrated for the environments in which they will be used.", "histories": [["v1", "Fri, 11 Nov 2016 18:57:15 GMT  (573kb,D)", "http://arxiv.org/abs/1611.03814v1", null]], "reviews": [], "SUBJECTS": "cs.CR cs.LG", "authors": ["nicolas papernot", "patrick mcdaniel", "arunesh sinha", "michael wellman"], "accepted": false, "id": "1611.03814"}, "pdf": {"name": "1611.03814.pdf", "metadata": {"source": "CRF", "title": "SoK: Towards the Science of Security and Privacy in Machine Learning", "authors": ["Nicolas Papernot", "Patrick McDaniel", "Arunesh Sinha", "Michael Wellman"], "emails": ["ngp5056@cse.psu.edu,", "mcdaniel@cse.psu.edu,", "arunesh@umich.edu", "wellman@umich.edu"], "sections": [{"heading": null, "text": "This year, it is only a matter of time before there is an agreement, until there is an agreement."}, {"heading": "II. ABOUT MACHINE LEARNING", "text": "We start with a brief overview of how systems apply ML algorithms. In particular, we compare different types of learning tasks and some peculiarities of their practical implementation."}, {"heading": "A. An Overview of Machine Learning Tasks", "text": "Machine Learning provides automated analysis methods for large amounts of data [3]. Tasks solved with machine learning techniques are usually divided into three types, which are characterized by the structure of the data analyzed by the corresponding learning algorithm.Supervised Learning: Methods that trigger an association between inputs and outputs, based on training examples in the form of inputs labeled with corresponding outputs, are supervised learning techniques.If the output data is categorical, the task is called classification, and real-value output domains define regression problems. Classical examples of supervised learning tasks include: object recognition in images [4], machine translation [5] and spam filtering [6].Unsupervised learning: If the method receives unlabeled inputs, its task is not supervised. Unsupervised learning considers problems such as clustering points by similarity data [7], a partial reduction in project dimensions [8]."}, {"heading": "B. Data Collection: Three Use Cases", "text": "Before learning a model that solves an interesting task, training data must be collected, which consists in collecting a generally large number of examples of solutions to the problem that you would like to solve with machine learning. For each of the tasks presented above, we describe an example of a task and how the corresponding training dataset would be collected.The first example task consists of classifying executable software into two categories: malicious and benign. This is a supervised classification problem, where the model must learn a mapping between inputs (executable software) and categorical outcomes (this binary task has only two possible classes).The training data includes a number of designated instances, each of which is clearly marked as malicious or benign [17].Second, we consider the task of extracting a pattern that is representative of normal activities in a computer network.The training data may consist of TCP dumps [18]."}, {"heading": "C. Machine Learning Empirical Process", "text": "We describe the general approach to creating a model of machine learning to solve one of the tasks described above. Training: Once the data is collected and pre-processed, an ML model is selected and trained. Most1 ML models can be regarded as parametric functions h\u03b8 (x), taking an input x and a parameter vector \u03b8. Input x is often presented as a vector of values referred to as characteristics. The space of functions {VC, x 7 \u2192 h\u03b8 (x)} is the set of candidate hypotheses to model the distribution from which the data set was sampled. A learning algorithm analyzes the training data to find the value (s). When learning is monitored, the parameters are adjusted to reduce the gap between model forecasts h\u03b8 (x) and the expected output of the data set."}, {"heading": "D. A Theoretical Model of Learning", "text": "Next, we formalize the semantics of the monitored ML algorithms, or Y = R for the functions captured. We give an overview of the probable deployment (PAC) models, which contain a theoretical underpinning of these algorithms, using the model in sections IV, V, and VI to interpret attacks and defense mechanisms. Such an interpretation helps to find that the data points and defense mechanisms, the general principles of adversarial learning applied across all supervised ML.PAC models of learning, are obtained from sampling samples that are not parametric: for the next neighbor [21].a fixed but unknown probability distribution D over space Z = X \u00d7 Y. Here, X is the space of characteristics, and Y is the space of labels (e.g. Y = 1} for classification, or Y = R for the recorded probability distributions."}, {"heading": "III. THREAT MODEL", "text": "In this section, we taxonomize the definition and scope of threat models in machine learning systems and map the space of security models. We begin by identifying the threat area of systems based on machine learning to inform where and how an adversary will attempt to undermine the attacked system. To illustrate the following sections, we expand earlier approaches to formulate a threat model for ML [25], [26]."}, {"heading": "A. The ML Attack Surface", "text": "The attack interface of a system built with data and machine learning reflects its purpose. However, systems using ML can be viewed within a generalized data processing pipeline (see Figure 1, above). At the conclusion, (a) input functions are collected from sensors or data repositories, (b) they are processed in the digital domain, (c) they are used by the model to produce an output, and (d) the output is transmitted to and responded to an external system or user. To illustrate this, we consider a generic pipeline, an autonomous vehicle, and network intrusion detection systems in Figure 1 (center and bottom). These systems collect sensor inputs (video image, network events) from which model functions (pixels, rivers) are extracted and used within the models. The meaning of the model output (stop sign, network attack) is then interpreted and measures are taken (stopping the traffic of a future car)."}, {"heading": "B. Adversarial Capabilities", "text": "A threat model is also defined by the actions and information available to the enemy; the definition of security is made in relation to stronger or weaker adversaries who have more or less access to the system and its data. (The definition of security is made in relation to selected privacy vulnerabilities that have more or less access to the system and its data.) The term capabilities refers to the what and how of the available attacks and refers to the attack vectors that can be applied to a threat interface. For example, in the network, attacks can be detected against the detection scenarios of attacks on the detection scenarios of an internal adversary who has access to the model used to distinguish attacks from normal behavior, whereas a weaker eaves adversary dropping would only have access to tcaps of network traffic. Here, the attack interface remains for both the attacks and the former attacker is assumed to have much more information and is thus a strictly \"stronger\" adversary. \""}, {"heading": "C. Adversarial Goals", "text": "The last question that arises is whether it is a political system in which the interests of the people are put centre stage. (...) The second question that arises is whether it is a system in which the interests of the people are put centre stage. (...) The third question that arises is whether it is a system in which the interests of the people are put centre stage. (...) The third question is whether it is a system in which the interests of the people are put centre stage. (...) The third question is whether it is put centre stage. (...) The third question is whether it is put centre stage. (...) The third question is whether it is put centre stage. (...) The third question is whether it is put centre stage. (...) The third question is whether it is put centre stage."}, {"heading": "IV. TRAINING IN ADVERSARIAL SETTINGS", "text": "Since the parameters of Hypothesis h are refined during learning, the training data set analyzed is potentially susceptible to manipulation by adversaries. This scenario corresponds to a poisoning attack [25] and is an example of learning in the presence of data that is not necessarily hostile, but nevertheless loud [41]. Poisoning attacks alter the training data set by inserting, editing or removing points with the intention of modifying the decision limits of the target model [39], thereby targeting the integrity of the learning system according to our threat model from Section III. It is reasonably obvious that an unlimited adversary can cause the learner to learn any functions h that lead to the total unavailability of the system. Therefore, all attacks below the target model limit the adversary in their attacks [42]. Also, in the PAC model, modifications of the training data can be considered a change in the distribution D that the training data has generated, resulting in a discrepancy between the training used and the redistributions used."}, {"heading": "A. Targeting Integrity", "text": "In fact, we see ourselves able to change and change the world until we are able to change the world again."}, {"heading": "B. Targeting Privacy", "text": "During training, the confidentiality and privacy of the data and the ML model is not affected by the fact that ML is used, but rather by the extent of the adversary's access to the system hosting the data and the model. This is a traditional access control problem that does not fall within the scope of our discussion. INFERRING IN ADVERSARIAL SETTINGS Opponents can also attack ML systems at inference time. In such settings, they cannot poison the training data or manipulate the model parameters. Therefore, the key feature that distinguishes attackers is their ability to access (but not change) the internals of the model used. White box opponents have knowledge of the ML technique used or the parameters learned. Instead, black box access is a weaker assumption that corresponds to the ability to issue queries to the model or collect an ambient training dataset. Black box opponents can surprisingly compromise the integrity of the model's access to most of the control field."}, {"heading": "A. White-box adversaries", "text": "While it is often difficult to gain access to the data, access to the white box systems is not always unrealistic: for example, ML models trained on data centers are compressed and deployed on smartphones. [62] In the theoretical PAC study, this can be interpreted as modifying the distribution that data generates during inference. [1) We describe strategies that require direct manipulation of model inputs, and (2) we consider indirect perturbations that are resistant to data processing."}, {"heading": "B. Black-box adversaries", "text": "It is about the question to what extent it is about a way, in which it is about the question, to what extent it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way, in which it is about a way and in which it is about a way, in which it is about a way, in which it is about a way and in which it is about a way, in which it is about a way, in which it is about a way and in which it is about a way, in which it is about a way and in which it is about a way, in which it is about which it is about a way, in which it is about which it is about a way, in which it is about which it is about which it is about a way, in which it is about which it is about which it is about which it is about which it is about a way, in which it is about which it is about which it is about which it is about which it is about which it is about which it is about which it is about which it is about which it is about which it is about which it is about which it is about which it is about which it is about which it is about which it is about which it is about a way, in which it is about which it is about which it is about which it is about which it is about which it is about which it is about which is about which it is about which it is about which it is about which it is about which it is"}, {"heading": "VI. TOWARDS ROBUST, PRIVATE, AND ACCOUNTABLE MACHINE LEARNING MODELS", "text": "After presenting attacks carried out during the training in Section IV and conclusions in Section V, we cover efforts at the interface of security, privacy, and ML that are relevant to containing these previously discussed attacks. We draw parallels between the seemingly unrelated objectives: (a) robustness in distribution shifts, (b) learning privacy-preserving models, and (c) fairness and accountability. Many of these issues remain largely open, so we draw lessons that will be useful for future work."}, {"heading": "A. Robustness of models to distribution drifts", "text": "This year it is so far that it will only be a matter of time before it is so far, until it is so far."}, {"heading": "B. Learning and Inferring with Privacy", "text": "One way of defining privacy-preserving models is that they do not disclose additional information about the subjects involved in their training data, which is captured by differentiated privacy [95], a rigorous framework for analyzing the privacy provided by algorithms. Although the privacy of an algorithm is not significantly different from two versions of the data that differ from a single data set, in our case the data set is a training point and the algorithm is the ML model that defines privacy as privacy, but not as privacy, but as privacy when there are two adjacent training datasets."}, {"heading": "C. Fairness and Accountability in Machine Learning", "text": "The opaque nature of ML raises concerns about a lack of fairness and accountability in decisions made on the basis of model forecasts, which is particularly important in applications such as credit decisions or healthcare [106].Fairness: in the pipeline of Figure 1, fairness is relevant to the physical actions taken on the basis of model forecasts. It should not promote discrimination against particular individuals [107]. Training data is perhaps the strongest source of bias in ML. For example, a dishonest data collector could contradictorily attempt to manipulate learning to create a model that discriminates against certain groups. Historical data also naturally reflect social distortions [108]. To learn fair models, Zemel et al. first learn an intermediate representation that encodes a sanitized variant of the data, as first discussed in [109]. Edwards et al. showed that fairness could be achieved by learning to compete with an opponent who tries to predict the sensitive prediction of the model."}, {"heading": "VII. NO FREE LUNCH IN ADVERSARIAL LEARNING", "text": "We begin by pointing out that if a classifying attack is not obvious, this is the result of this first section, the actual attack on any possible input, then it cannot be manipulated. Thus, the presence of contradictory examples is a manifestation of the classifier, which is inaccurate on many inputs. Therefore, the dichotomy between robustness to opposing examples and better prediction is incorrect. As a result, we investigate the interaction between predictive loss, adversarial manipulation in case of inference and complexity of the hypothesis. Recall the theoretical characterization for data poisoning by Kearns and Li [39] (see Section IV). While the poisoning attacks can be measured by the percentage of modified data, the mathematical description of an attack is not obvious."}, {"heading": "VIII. CONCLUSIONS", "text": "The security and privacy of machine learning is an active but still nascent area. We have studied the attack interface of systems based on machine learning; this analysis provides a natural structure for thinking about their threat models, and we have put a lot of work into this framework, which is organized around attacks and defenses; we have formally shown that there is often a fundamental tension between security or privacy and the accuracy of ML predictions in machine learning systems with limited capacity. Overall, the extensive work from the various scientific communities collectively paints a picture that many of the vulnerabilities of machine learning and the countermeasures used to defend it are still unknown - but a science for understanding these vulnerabilities is slowly evolving."}, {"heading": "ACKNOWLEDGMENTS", "text": "We thank Mart\u0131n Abadi, Z. Berkay Celik, Ian Goodfellow, Damien Octeau and Kunal Talwar for their feedback on early versions of this document. Nicolas Papernot is supported by a Google PhD Fellowship in Security. We also thank Megan McDaniel for taking good care of our diet before the deadline. Partially, the research was supported by the Army Research Laboratory under number W911NF-132-0045 (ARL Cyber Security CRA) and the Army Research Office under number W911NF-13-1-0421. The views and conclusions contained in this document are those of the authors and should not be interpreted as representing the official guidelines of the Army Research Laboratory or the U.S. Government."}, {"heading": "A. EXAMPLE OF DEFENSE USING ADDITIONAL DATA", "text": "In Figure 6, we show the original dataset and the true classifier in Figure A. The used hypotheses class contains either a single linear delimiter or two linear delimiters, so this hypotheses class can provide a classifier that is very close to the true classifier. However, for the dataset in Figure A, the learned classifier is shown in Figure B, which is clearly far from optimal. This is not a problem for the hypotheses class; a different data distribution in Figure C can provide for learning a much better classifier, as shown. Another option is to add conflicting examples such as in Figure D (conflicting examples in red), which again makes the learned classifier much better."}, {"heading": "B. PROOF OF THEOREM 1", "text": "Theorem 1. Solve each hypothesis (function) class H and distribution D, and assume that an attacker exists with probability q. If the attacker uses an \u03b1-effective attack against H and D with \u03b1 \u2265 \u03b10 > 0, the loss of the learner is at least Ex, y \u0445 D [lh, y] + q\u03b10Proof. Choose any hypothesis h \u0445 R (H). The loss of the learner is E x, y \u0445. D [lh (x, y)] = \u0109lh (x, y). p (x, y) dxdy. For a randomized classifier h, which is about h1,., hn with the probabilities q1,.., qn or the loss for one (x, y) loss of each (x, y) is the expected loss i qilhi (x, y). Ex, y). Ex, y)."}, {"heading": "C. PROOF OF THEOREM 2", "text": "Theorem 2: Fix each hypothesis (function) class H and distribution D and a \u03b2-rich hypothesis class H. Assume therefore that the attacker is present with probability q and lD < < Ex, y, D [lh] -\u03b2 and l.D < Ex, y \u00b2 -.D (l, h) -\u03b2. In view of the attacker using a \u03b2-effective against H and D with \u03b1 = \u03b10 and the learner using the \u03b2-rich hypothesis class H \u00b2, there is an h-H \u00b2 so that the loss for h is lower than Ex, y-h \u00b2 -D (x, y) qualities. Let the distribution choice of the opponent be in his attack. D. Select the hypothesis h \u00b2 -H \u00b2 so that h \u00b2 -H \u00b2 ability is x."}], "references": [{"title": "Concrete problems in AI safety", "author": ["D. Amodei", "C. Olah", "J. Steinhardt", "P. Christiano", "J. Schulman", "D. Man\u00e9"], "venue": "arXiv preprint arXiv:1606.06565, 2016.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2016}, {"title": "Oblivious multi-party machine learning on trusted processors", "author": ["O. Ohrimenko", "F. Schuster", "C. Fournet", "A. Mehta", "S. Nowozin", "K. Vaswani", "M. Costa"], "venue": "25th USENIX Security Symposium (USENIX Security 16), 2016.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2016}, {"title": "Machine Learning: A Probabilistic Perspective", "author": ["K.P. Murphy"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2012}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "Advances in Neural Information Processing Systems, 2012, pp. 1097\u20131105.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2012}, {"title": "Sequence to sequence learning with neural networks", "author": ["I. Sutskever", "O. Vinyals", "Q.V. Le"], "venue": "Advances in Neural Information Processing Systems, 2014, pp. 3104\u20133112.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2014}, {"title": "Support vector machines for spam categorization", "author": ["H. Drucker", "D. Wu", "V.N. Vapnik"], "venue": "IEEE Transactions on Neural Networks, vol. 10, no. 5, pp. 1048\u20131054, 1999.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1999}, {"title": "Data clustering: A review", "author": ["A.K. Jain", "M.N. Murty", "P.J. Flynn"], "venue": "ACM Computing Surveys, vol. 31, no. 3, pp. 264\u2013323, 1999.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1999}, {"title": "Learning multiple layers of features from tiny images", "author": ["A. Krizhevsky", "G. Hinton"], "venue": "2009.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2009}, {"title": "Stacked convolutional auto-encoders for hierarchical feature extraction", "author": ["J. Masci", "U. Meier", "D. Cire\u015fan", "J. Schmidhuber"], "venue": "International Conference on Artificial Neural Networks and Machine Learning, 2011, pp. 52\u201359.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2011}, {"title": "Why does unsupervised pre-training help deep learning?", "author": ["D. Erhan", "Y. Bengio", "A. Courville", "P.-A. Manzagol", "P. Vincent", "S. Bengio"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2010}, {"title": "Anomaly detection: A survey", "author": ["V. Chandola", "A. Banerjee", "V. Kumar"], "venue": "ACM Computing Surveys, vol. 41, no. 3, pp. 15:1\u201315:58, 2009.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2009}, {"title": "Nash Q-learning for general-sum stochastic games", "author": ["J. Hu", "M.P. Wellman"], "venue": "Journal of Machine Learning Research, vol. 4, pp. 1039\u20131069, 2003.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2003}, {"title": "Reinforcement Learning: An Introduction", "author": ["R.S. Sutton", "A.G. Barto"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1998}, {"title": "Mastering the game of Go with deep neural networks and tree search", "author": ["D. Silver", "A. Huang", "C.J. Maddison", "A. Guez", "L. Sifre", "G. Van Den Driessche", "J. Schrittwieser", "I. Antonoglou", "V. Panneershelvam", "M. Lanctot"], "venue": "Nature, vol. 529, no. 7587, pp. 484\u2013489, 2016.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2016}, {"title": "Pattern recognition", "author": ["C.M. Bishop"], "venue": "Machine Learning, 2006.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2006}, {"title": "Deep learning", "author": ["I. Goodfellow", "Y. Bengio", "A. Courville"], "venue": "2016, Book in preparation for MIT Press (www.deeplearningbook.org).", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2016}, {"title": "Static analysis of executables to detect malicious patterns", "author": ["M. Christodorescu", "S. Jha"], "venue": "12th USENIX Security Symposium (USENIX Security 06), 2006.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2006}, {"title": "Anomaly based network intrusion detection with unsupervised outlier detection", "author": ["J. Zhang", "M. Zulkernine"], "venue": "IEEE International Conference on Communications, vol. 5, 2006, pp. 2388\u20132393.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2006}, {"title": "Outside the closed world: On using machine learning for network intrusion detection", "author": ["R. Sommer", "V. Paxson"], "venue": "2010 IEEE symposium on security and privacy. IEEE, 2010, pp. 305\u2013316.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2010}, {"title": "Next generation intrusion detection: Autonomous reinforcement learning of network attacks", "author": ["J. Cannady"], "venue": "Proceedings of the 23rd national information systems security conference, 2000, pp. 1\u201312.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2000}, {"title": "An introduction to kernel and nearest-neighbor nonparametric regression", "author": ["N.S. Altman"], "venue": "The American Statistician, vol. 46, no. 3, pp. 175\u2013185, 1992.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1992}, {"title": "Neural network learning: Theoretical foundations", "author": ["M. Anthony", "P.L. Bartlett"], "venue": "cambridge university press,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2009}, {"title": "Are loss functions all the same?", "author": ["L. Rosasco", "E. De Vito", "A. Caponnetto", "M. Piana", "A. Verri"], "venue": "Neural Computation,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2004}, {"title": "Learning adversary behavior in security games: A pac model perspective", "author": ["A. Sinha", "D. Kar", "M. Tambe"], "venue": "Proceedings of the 2016 International Conference on Autonomous Agents & Multiagent Systems. International Foundation for Autonomous Agents and Multiagent Systems, 2016, pp. 214\u2013222.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2016}, {"title": "Can machine learning be secure?", "author": ["M. Barreno", "B. Nelson", "R. Sears", "A.D. Joseph", "J.D. Tygar"], "venue": "Proceedings of the 2006 ACM Symposium on Information, computer and communications security. ACM,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2006}, {"title": "The limitations of deep learning in adversarial settings", "author": ["N. Papernot", "P. McDaniel", "S. Jha", "M. Fredrikson", "Z.B. Celik", "A. Swami"], "venue": "Proceedings of the 1st IEEE European Symposium on Security and Privacy. IEEE, 2016.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2016}, {"title": "A new learning paradigm: Learning using privileged information", "author": ["V. Vapnik", "A. Vashist"], "venue": "Neural Networks, vol. 22, no. 5, pp. 544\u2013557, 2009.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2009}, {"title": "Online anomaly detection under adversarial impact", "author": ["M. Kloft", "P. Laskov"], "venue": "International Conference on Artificial Intelligence and Statistics, 2010, pp. 405\u2013412.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2010}, {"title": "Good word attacks on statistical spam filters.", "author": ["D. Lowd", "C. Meek"], "venue": "CEAS,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2005}, {"title": "Intriguing properties of neural networks", "author": ["C. Szegedy", "W. Zaremba", "I. Sutskever", "J. Bruna", "D. Erhan", "I. Goodfellow", "R. Fergus"], "venue": "Proceedings of the 2014 International Conference on Learning Representations. Computational and Biological Learning Society, 2014.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2014}, {"title": "Practical black-box attacks against deep learning systems using adversarial examples", "author": ["N. Papernot", "P. McDaniel", "I. Goodfellow", "S. Jha", "Z.B. Celik", "A. Swami"], "venue": "arXiv preprint arXiv:1602.02697, 2016.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2016}, {"title": "Practical evasion of a learning-based classifier: A case study", "author": ["P. Laskov"], "venue": "2014 IEEE Symposium on Security and Privacy. IEEE, 2014, pp. 197\u2013211.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2014}, {"title": "Statistical fraud detection: A review", "author": ["R.J. Bolton", "D.J. Hand"], "venue": "Statistical science, pp. 235\u2013249, 2002.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2002}, {"title": "Privacy, information technology, and health care", "author": ["T.C. Rindfleisch"], "venue": "Communications of the ACM, vol. 40, no. 8, pp. 92\u2013100, 1997.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 1997}, {"title": "Model inversion attacks that exploit confidence information and basic countermeasures", "author": ["M. Fredrikson", "S. Jha", "T. Ristenpart"], "venue": "Proceedings of the 22nd ACM SIGSAC Conference on Computer and Communications Security. ACM, 2015, pp. 1322\u20131333.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2015}, {"title": "Membership inference attacks against machine learning models", "author": ["R. Shokri", "M. Stronati", "V. Shmatikov"], "venue": "arXiv preprint arXiv:1610.05820, 2016.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2016}, {"title": "Accessorize to a crime: Real and stealthy attacks on state-of-the-art face recognition", "author": ["M. Sharif", "S. Bhagavatula", "L. Bauer", "M.K. Reiter"], "venue": "Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security. ACM, 2016, pp. 1528\u20131540.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2016}, {"title": "Evaluation: from precision, recall and f-measure to roc, informedness, markedness and correlation", "author": ["D.M. Powers"], "venue": "2011.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2011}, {"title": "Learning in the presence of malicious errors", "author": ["M. Kearns", "M. Li"], "venue": "SIAM Journal on Computing, vol. 22, no. 4, pp. 807\u2013837, 1993.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 1993}, {"title": "Nightmare at test time: robust learning by feature deletion", "author": ["A. Globerson", "S. Roweis"], "venue": "Proceedings of the 23rd international conference on Machine learning. ACM, 2006, pp. 353\u2013360.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2006}, {"title": "Noise tolerance under risk minimization", "author": ["N. Manwani", "P.S. Sastry"], "venue": "IEEE Transactions on Cybernetics, vol. 43, no. 3, pp. 1146\u20131151, 2013.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2013}, {"title": "Bounding an attack\u2019s complexity for a simple learning model", "author": ["B. Nelson", "A.D. Joseph"], "venue": "Proc. of the First Workshop on Tackling Computer Systems Problems with Machine Learning Techniques (SysML), Saint-Malo, France, 2006.", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2006}, {"title": "Mining time-changing data streams", "author": ["G. Hulten", "L. Spencer", "P. Domingos"], "venue": "Proceedings of the seventh ACM SIGKDD international conference on Knowledge discovery and data mining. ACM, 2001, pp. 97\u2013106.", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2001}, {"title": "Support vector machines under adversarial label noise.", "author": ["B. Biggio", "B. Nelson", "P. Laskov"], "venue": "ACML,", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2011}, {"title": "Systematic poisoning attacks on and defenses for machine learning in healthcare", "author": ["M. Mozaffari-Kermani", "S. Sur-Kolay", "A. Raghunathan", "N.K. Jha"], "venue": "IEEE journal of biomedical and health informatics, vol. 19, no. 6, pp. 1893\u20131905, 2015.", "citeRegEx": "45", "shortCiteRegEx": null, "year": 1893}, {"title": "Poisoning attacks against support vector machines", "author": ["B. Biggio", "B. Nelson", "L. Pavel"], "venue": "Proceedings of the 29th International Conference on Machine Learning, 2012. 16", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2012}, {"title": "Using machine teaching to identify optimal training-set attacks on machine learners.", "author": ["S. Mei", "X. Zhu"], "venue": "in AAAI,", "citeRegEx": "47", "shortCiteRegEx": "47", "year": 2015}, {"title": "Polygraph: Automatically generating signatures for polymorphic worms", "author": ["J. Newsome", "B. Karp", "D. Song"], "venue": "Security and Privacy, 2005 IEEE Symposium on. IEEE, 2005, pp. 226\u2013241.", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2005}, {"title": "Misleading worm signature generators using deliberate noise injection", "author": ["R. Perdisci", "D. Dagon", "W. Lee", "P. Fogla", "M. Sharif"], "venue": "Security and Privacy, 2006 IEEE Symposium on. IEEE, 2006, pp. 15\u2013pp.", "citeRegEx": "49", "shortCiteRegEx": null, "year": 2006}, {"title": "Is feature selection secure against training data poisoning?", "author": ["H. Xiao", "B. Biggio", "G. Brown", "G. Fumera", "C. Eckert", "F. Roli"], "venue": "Proceedings of the 32nd International Conference on Machine Learning", "citeRegEx": "50", "shortCiteRegEx": "50", "year": 2015}, {"title": "Evasion attacks against machine learning at test time", "author": ["B. Biggio", "I. Corona", "D. Maiorca", "B. Nelson", "N. \u0160rndi\u0107", "P. Laskov", "G. Giacinto", "F. Roli"], "venue": "Machine Learning and Knowledge Discovery in Databases. Springer, 2013, pp. 387\u2013402.", "citeRegEx": "51", "shortCiteRegEx": null, "year": 2013}, {"title": "Explaining and harnessing adversarial examples", "author": ["I.J. Goodfellow", "J. Shlens", "C. Szegedy"], "venue": "International Conference on Learning Representations. Computational and Biological Learning Society, 2015.", "citeRegEx": "52", "shortCiteRegEx": null, "year": 2015}, {"title": "Deepfool: a simple and accurate method to fool deep neural networks", "author": ["S.-M. Moosavi-Dezfooli", "A. Fawzi", "P. Frossard"], "venue": "arXiv preprint arXiv:1511.04599, 2015.", "citeRegEx": "53", "shortCiteRegEx": null, "year": 2015}, {"title": "Data poisoning attacks against autoregressive models", "author": ["S. Alfeld", "X. Zhu", "P. Barford"], "venue": "Thirtieth AAAI Conference on Artificial Intelligence, 2016.", "citeRegEx": "54", "shortCiteRegEx": null, "year": 2016}, {"title": "Hacking smart machines with smarter ones: How to extract meaningful data from machine learning classifiers", "author": ["G. Ateniese", "L.V. Mancini", "A. Spognardi", "A. Villani", "D. Vitali", "G. Felici"], "venue": "International Journal of Security and Networks, vol. 10, no. 3, pp. 137\u2013150, 2015.", "citeRegEx": "55", "shortCiteRegEx": null, "year": 2015}, {"title": "Adversarial perturbations against deep neural networks for malware classification", "author": ["K. Grosse", "N. Papernot", "P. Manoharan", "M. Backes", "P. McDaniel"], "venue": "arXiv preprint arXiv:1606.04435, 2016.", "citeRegEx": "56", "shortCiteRegEx": null, "year": 2016}, {"title": "Adversarial examples in the physical world", "author": ["A. Kurakin", "I. Goodfellow", "S. Bengio"], "venue": "arXiv preprint arXiv:1607.02533, 2016.", "citeRegEx": "57", "shortCiteRegEx": null, "year": 2016}, {"title": "Automatically evading classifiers", "author": ["W. Xu", "Y. Qi", "D. Evans"], "venue": "Proceedings of the 2016 Network and Distributed Systems Symposium, 2016.", "citeRegEx": "58", "shortCiteRegEx": null, "year": 2016}, {"title": "Privacy in pharmacogenetics: An end-to-end case study of personalized warfarin dosing", "author": ["M. Fredrikson", "E. Lantz", "S. Jha", "S. Lin", "D. Page", "T. Ristenpart"], "venue": "23rd USENIX Security Symposium (USENIX Security 14), 2014, pp. 17\u201332.", "citeRegEx": "59", "shortCiteRegEx": null, "year": 2014}, {"title": "Stealing machine learning models via prediction apis", "author": ["F. Tram\u00e8r", "F. Zhang", "A. Juels", "M.K. Reiter", "T. Ristenpart"], "venue": "arXiv preprint arXiv:1609.02943, 2016.", "citeRegEx": "60", "shortCiteRegEx": null, "year": 2016}, {"title": "Transferability in machine learning: from phenomena to black-box attacks using adversarial samples", "author": ["N. Papernot", "P. McDaniel", "I. Goodfellow"], "venue": "arXiv preprint arXiv:1605.07277, 2016.", "citeRegEx": "61", "shortCiteRegEx": null, "year": 2016}, {"title": "Distilling the knowledge in a neural network", "author": ["G. Hinton", "O. Vinyals", "J. Dean"], "venue": "NIPS-14 Workshop on Deep Learning and Representation Learning. arXiv:1503.02531, 2014.", "citeRegEx": "62", "shortCiteRegEx": null, "year": 2014}, {"title": "On the limited memory bfgs method for large scale optimization", "author": ["D.C. Liu", "J. Nocedal"], "venue": "Mathematical programming, vol. 45, no. 1-3, pp. 503\u2013528, 1989.", "citeRegEx": "63", "shortCiteRegEx": null, "year": 1989}, {"title": "The mnist database of handwritten digits", "author": ["Y. LeCun", "C. Cortes"], "venue": "1998.", "citeRegEx": "64", "shortCiteRegEx": null, "year": 1998}, {"title": "Adversarial perturbations of deep neural networks", "author": ["D. Warde-Farley", "I. Goodfellow"], "venue": "Advanced Structured Prediction, T. Hazan, G. Papandreou, and D. Tarlow, Eds, 2016.", "citeRegEx": "65", "shortCiteRegEx": null, "year": 2016}, {"title": "Learning with a strong adversary", "author": ["R. Huang", "B. Xu", "D. Schuurmans", "C. Szepesvari"], "venue": "arXiv preprint arXiv:1511.03034, 2015.", "citeRegEx": "66", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep neural networks are easily fooled: High confidence predictions for unrecognizable images", "author": ["A. Nguyen", "J. Yosinski", "J. Clune"], "venue": "In Computer Vision and Pattern Recognition (CVPR 2015). IEEE, 2015.", "citeRegEx": "67", "shortCiteRegEx": null, "year": 2015}, {"title": "Hidden voice commands", "author": ["N. Carlini", "P. Mishra", "T. Vaidya", "Y. Zhang", "M. Sherr", "C. Shields", "D. Wagner", "W. Zhou"], "venue": "25th USENIX Security Symposium (USENIX Security 16), Austin, TX, 2016.", "citeRegEx": "68", "shortCiteRegEx": null, "year": 2016}, {"title": "Supervision via competition: Robot adversaries for learning tasks", "author": ["L. Pinto", "J. Davidson", "A. Gupta"], "venue": "arXiv preprint arXiv:1610.01685, 2016.", "citeRegEx": "69", "shortCiteRegEx": null, "year": 2016}, {"title": "Generative adversarial nets", "author": ["I. Goodfellow", "J. Pouget-Abadie", "M. Mirza", "B. Xu", "D. Warde-Farley", "S. Ozair", "A. Courville", "Y. Bengio"], "venue": "Advances in Neural Information Processing Systems, 2014, pp. 2672\u2013 2680.", "citeRegEx": "70", "shortCiteRegEx": null, "year": 2014}, {"title": "On attacking statistical spam filters.", "author": ["G.L. Wittel", "S.F. Wu"], "venue": "CEAS,", "citeRegEx": "71", "shortCiteRegEx": "71", "year": 2004}, {"title": "Optimal randomized classification in adversarial settings", "author": ["Y. Vorobeychik", "B. Li"], "venue": "Proceedings of the 2014 international conference on Autonomous agents and multi-agent systems. International Foundation for Autonomous Agents and Multiagent Systems, 2014, pp. 485\u2013492.", "citeRegEx": "72", "shortCiteRegEx": null, "year": 2014}, {"title": "Adversarial learning", "author": ["D. Lowd", "C. Meek"], "venue": "Proceedings of the eleventh ACM SIGKDD international conference on Knowledge discovery in data mining. ACM, 2005, pp. 641\u2013647.", "citeRegEx": "73", "shortCiteRegEx": null, "year": 2005}, {"title": "Query strategies for evading convex-inducing classifiers", "author": ["B. Nelson", "B.I. Rubinstein", "L. Huang", "A.D. Joseph", "S.J. Lee", "S. Rao", "J. Tygar"], "venue": "Journal of Machine Learning Research, vol. 13, no. May, pp. 1293\u2013 1332, 2012.", "citeRegEx": "74", "shortCiteRegEx": null, "year": 2012}, {"title": "Statistical inference considered harmful", "author": ["F. McSherry"], "venue": "2016. [Online]. Available: https://github.com/frankmcsherry/blog/blob/ master/posts/2016-06-14.md", "citeRegEx": "75", "shortCiteRegEx": null, "year": 2016}, {"title": "Visualizing and understanding convolutional networks", "author": ["M.D. Zeiler", "R. Fergus"], "venue": "Computer vision\u2013ECCV 2014. Springer, 2014, pp. 818\u2013833.", "citeRegEx": "76", "shortCiteRegEx": null, "year": 2014}, {"title": "Antidote: Understanding and defending against poisoning of anomaly detectors", "author": ["B.I.P. Rubinstein", "B. Nelson", "L. Huang", "A.D. Joseph", "S.-h. Lau", "S. Rao", "N. Taft", "J.D. Tygar"], "venue": "9th ACM SIGCOMM Conference on Internet measurement. ACM, 2009, pp. 1\u201314.", "citeRegEx": "77", "shortCiteRegEx": null, "year": 2009}, {"title": "Learning SVMs from sloppily labeled data", "author": ["G. Stempfel", "L. Ralaivola"], "venue": "International Conference on Artificial Neural Networks. Springer, 2009, pp. 884\u2013893.", "citeRegEx": "78", "shortCiteRegEx": null, "year": 2009}, {"title": "Robust support vector machine training via convex outlier ablation", "author": ["L. Xu", "K. Crammer", "D. Schuurmans"], "venue": "Twenty-First AAAI National Conference on Artificial Intelligence, vol. 6, 2006, pp. 536\u2013 542.", "citeRegEx": "79", "shortCiteRegEx": null, "year": 2006}, {"title": "A virtual honeypot framework.", "author": ["N. Provos"], "venue": "USENIX Security Symposium,", "citeRegEx": "80", "shortCiteRegEx": "80", "year": 2004}, {"title": "Towards deep neural network architectures robust to adversarial examples", "author": ["S. Gu", "L. Rigazio"], "venue": "Proceedings of the 2015 International Conference on Learning Representations. Computational and Biological Learning Society, 2015.", "citeRegEx": "81", "shortCiteRegEx": null, "year": 2015}, {"title": "A unified gradient regularization family for adversarial examples", "author": ["C. Lyu", "K. Huang", "H.-N. Liang"], "venue": "Data Mining (ICDM), 2015 IEEE International Conference on. IEEE, 2015, pp. 301\u2013309.", "citeRegEx": "82", "shortCiteRegEx": null, "year": 2015}, {"title": "Unifying adversarial training algorithms with flexible deep data gradient regularization", "author": ["I. Ororbia", "G. Alexander", "C.L. Giles", "D. Kifer"], "venue": "arXiv preprint arXiv:1601.07213, 2016.", "citeRegEx": "83", "shortCiteRegEx": null, "year": 2016}, {"title": "Distillation as a defense to adversarial perturbations against deep neural networks", "author": ["N. Papernot", "P. McDaniel", "X. Wu", "S. Jha", "A. Swami"], "venue": "Proceedings of the 37th IEEE Symposium on Security and Privacy. IEEE, 2016.", "citeRegEx": "84", "shortCiteRegEx": null, "year": 2016}, {"title": "On the effectiveness of defensive distillation", "author": ["N. Papernot", "P. McDaniel"], "venue": "arXiv preprint arXiv:1607.05113, 2016.", "citeRegEx": "85", "shortCiteRegEx": null, "year": 2016}, {"title": "Defensive distillation is not robust to adversarial examples", "author": ["N. Carlini", "D. Wagner"], "venue": "arXiv preprint arXiv:1607.04311, 2016.", "citeRegEx": "86", "shortCiteRegEx": null, "year": 2016}, {"title": "Rethinking the inception architecture for computer vision", "author": ["C. Szegedy", "V. Vanhoucke", "S. Ioffe", "J. Shlens", "Z. Wojna"], "venue": "arXiv preprint arXiv:1512.00567, 2015.", "citeRegEx": "87", "shortCiteRegEx": null, "year": 2015}, {"title": "Adversarial perturbations of deep neural networks", "author": ["D. Warde-Farley", "I. Goodfellow"], "venue": "Advanced Structured Prediction, T. Hazan, G. Papandreou, and D. Tarlow, Eds., 2016.", "citeRegEx": "88", "shortCiteRegEx": null, "year": 2016}, {"title": "Large-scale machine learning with stochastic gradient descent", "author": ["L. Bottou"], "venue": "Proceedings of COMPSTAT\u20192010. Springer, 2010, pp. 177\u2013186.", "citeRegEx": "89", "shortCiteRegEx": null, "year": 2010}, {"title": "From physical security to cybersecurity", "author": ["A. Sinha", "T.H. Nguyen", "D. Kar", "M. Brown", "M. Tambe", "A.X. Jiang"], "venue": "Journal of Cybersecurity, p. tyv007, 2015.", "citeRegEx": "90", "shortCiteRegEx": null, "year": 2015}, {"title": "Security and game theory: algorithms, deployed systems, lessons learned", "author": ["M. Tambe"], "venue": null, "citeRegEx": "91", "shortCiteRegEx": "91", "year": 2011}, {"title": "Stackelberg games for adversarial prediction problems", "author": ["M. Br\u00fcckner", "T. Scheffer"], "venue": "Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and data mining. ACM, 2011, pp. 547\u2013555.", "citeRegEx": "92", "shortCiteRegEx": null, "year": 2011}, {"title": "Strategic classification", "author": ["M. Hardt", "N. Megiddo", "C. Papadimitriou", "M. Wootters"], "venue": "Proceedings of the 2016 ACM Conference on Innovations in Theoretical Computer Science, ser. ITCS \u201916, 2016, pp. 111\u2013122. 17", "citeRegEx": "93", "shortCiteRegEx": null, "year": 2016}, {"title": "Feature cross-substitution in adversarial classification", "author": ["B. Li", "Y. Vorobeychik"], "venue": "Advances in Neural Information Processing Systems, 2014, pp. 2087\u20132095.", "citeRegEx": "94", "shortCiteRegEx": null, "year": 2014}, {"title": "The algorithmic foundations of differential privacy", "author": ["C. Dwork", "A. Roth"], "venue": "Foundations and Trends in Theoretical Computer Science, vol. 9, no. 3-4, pp. 211\u2013407, 2014.", "citeRegEx": "95", "shortCiteRegEx": null, "year": 2014}, {"title": "What can we learn privately?", "author": ["S.P. Kasiviswanathan", "H.K. Lee", "K. Nissim", "S. Raskhodnikova", "A. Smith"], "venue": "SIAM Journal on Computing,", "citeRegEx": "96", "shortCiteRegEx": "96", "year": 2011}, {"title": "Rappor: Randomized aggregatable privacy-preserving ordinal response", "author": ["\u00da. Erlingsson", "V. Pihur", "A. Korolova"], "venue": "Proceedings of the 2014 ACM SIGSAC conference on computer and communications security. ACM, 2014, pp. 1054\u20131067.", "citeRegEx": "97", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning privately from multiparty data", "author": ["J. Hamm", "P. Cao", "M. Belkin"], "venue": "arXiv preprint arXiv:1602.03552, 2016.", "citeRegEx": "98", "shortCiteRegEx": null, "year": 2016}, {"title": "Semi-supervised knowledge transfer for deep learning from private training data", "author": ["N. Papernot", "M. Abadi", "\u00da. Erlingsson", "I. Goodfellow", "K. Talwar"], "venue": "arXiv preprint arXiv:1610.05755, 2016.", "citeRegEx": "99", "shortCiteRegEx": null, "year": 2016}, {"title": "Differentially private empirical risk minimization", "author": ["K. Chaudhuri", "C. Monteleoni", "A.D. Sarwate"], "venue": "Journal of Machine Learning Research, vol. 12, no. Mar, pp. 1069\u20131109, 2011.", "citeRegEx": "100", "shortCiteRegEx": null, "year": 2011}, {"title": "Differentially private empirical risk minimization: Efficient algorithms and tight error bounds", "author": ["R. Bassily", "A. Smith", "A. Thakurta"], "venue": "arXiv preprint arXiv:1405.7085, 2014.", "citeRegEx": "101", "shortCiteRegEx": null, "year": 2014}, {"title": "Privacy-preserving deep learning", "author": ["R. Shokri", "V. Shmatikov"], "venue": "22nd ACM SIGSAC Conference on Computer and Communications Security, 2015, pp. 1310\u20131321.", "citeRegEx": "102", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep learning with differential privacy", "author": ["M. Abadi", "A. Chu", "I. Goodfellow", "H.B. McMahan", "I. Mironov", "K. Talwar", "L. Zhang"], "venue": "ACM SIGSAC Conference on Computer and Communications Security. ACM, 2016, pp. 308\u2013318.", "citeRegEx": "103", "shortCiteRegEx": null, "year": 2016}, {"title": "On data banks and privacy homomorphisms", "author": ["R.L. Rivest", "L. Adleman", "M.L. Dertouzos"], "venue": "Foundations of secure computation, vol. 4, no. 11, pp. 169\u2013180, 1978.", "citeRegEx": "104", "shortCiteRegEx": null, "year": 1978}, {"title": "Cryptonets: Applying neural networks to encrypted data with high throughput and accuracy", "author": ["R. Gilad-Bachrach", "N. Dowlin", "K. Laine", "K. Lauter", "M. Naehrig", "J. Wernsing"], "venue": "Proceedings of The 33rd International Conference on Machine Learning, 2016, pp. 201\u2013210.", "citeRegEx": "105", "shortCiteRegEx": null, "year": 2016}, {"title": "Estimation of the warfarin dose with clinical and pharmacogenetic data", "author": ["I.W.P. Consortium"], "venue": "N Engl J Med, vol. 2009, no. 360, pp. 753\u2013 764, 2009.", "citeRegEx": "106", "shortCiteRegEx": null, "year": 2009}, {"title": "Inherent trade-offs in the fair determination of risk scores", "author": ["J. Kleinberg", "S. Mullainathan", "M. Raghavan"], "venue": "arXiv preprint arXiv:1609.05807, 2016.", "citeRegEx": "107", "shortCiteRegEx": null, "year": 2016}, {"title": "Big data\u2019s disparate impact", "author": ["S. Barocas", "A.D. Selbst"], "venue": "California Law Review, vol. 104, 2016.", "citeRegEx": "108", "shortCiteRegEx": null, "year": 2016}, {"title": "Fairness through awareness", "author": ["C. Dwork", "M. Hardt", "T. Pitassi", "O. Reingold", "R. Zemel"], "venue": "Proceedings of the 3rd Innovations in Theoretical Computer Science Conference. ACM, 2012, pp. 214\u2013226.", "citeRegEx": "109", "shortCiteRegEx": null, "year": 2012}, {"title": "Censoring representations with an adversary", "author": ["H. Edwards", "A. Storkey"], "venue": "arXiv preprint arXiv:1511.05897, 2015.", "citeRegEx": "110", "shortCiteRegEx": null, "year": 2015}, {"title": "Interpretable classifiers using rules and bayesian analysis: Building a better stroke prediction model", "author": ["B. Letham", "C. Rudin", "T.H. McCormick", "D. Madigan"], "venue": "The Annals of Applied Statistics, vol. 9, no. 3, pp. 1350\u20131371, 2015.", "citeRegEx": "111", "shortCiteRegEx": null, "year": 2015}, {"title": "Algorithmic transparency via quantitative input influence", "author": ["A. Datta", "S. Sen", "Y. Zick"], "venue": "Proceedings of 37th IEEE Symposium on Security and Privacy, 2016.", "citeRegEx": "112", "shortCiteRegEx": null, "year": 2016}, {"title": "Visualizing higherlayer features of a deep network", "author": ["D. Erhan", "Y. Bengio", "A. Courville", "P. Vincent"], "venue": "University of Montreal, vol. 1341, 2009.", "citeRegEx": "113", "shortCiteRegEx": null, "year": 2009}, {"title": "Visualizing deep convolutional neural networks using natural pre-images", "author": ["A. Mahendran", "A. Vedaldi"], "venue": "International Journal of Computer Vision, pp. 1\u201323, 2016.", "citeRegEx": "114", "shortCiteRegEx": null, "year": 2016}, {"title": "Differential privacy: A survey of results", "author": ["C. Dwork"], "venue": "International Conference on Theory and Applications of Models of Computation. Springer Berlin Heidelberg, 2008, pp. 1\u201319.", "citeRegEx": "115", "shortCiteRegEx": null, "year": 2008}], "referenceMentions": [{"referenceID": 0, "context": "[1].", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "For instance, we do not cover trusted computing platforms for ML [2].", "startOffset": 65, "endOffset": 68}, {"referenceID": 2, "context": "Machine learning provides automated methods of analysis for large sets of data [3].", "startOffset": 79, "endOffset": 82}, {"referenceID": 3, "context": "Classic examples of supervised learning tasks include: object recognition in images [4], machine translation [5], and spam filtering [6].", "startOffset": 84, "endOffset": 87}, {"referenceID": 4, "context": "Classic examples of supervised learning tasks include: object recognition in images [4], machine translation [5], and spam filtering [6].", "startOffset": 109, "endOffset": 112}, {"referenceID": 5, "context": "Classic examples of supervised learning tasks include: object recognition in images [4], machine translation [5], and spam filtering [6].", "startOffset": 133, "endOffset": 136}, {"referenceID": 6, "context": "Unsupervised learning considers problems such as clustering points according to a similarity metric [7], dimensionality reduction to project data in lower dimensional subspaces [8], and model pre-training [10].", "startOffset": 100, "endOffset": 103}, {"referenceID": 7, "context": "Unsupervised learning considers problems such as clustering points according to a similarity metric [7], dimensionality reduction to project data in lower dimensional subspaces [8], and model pre-training [10].", "startOffset": 177, "endOffset": 180}, {"referenceID": 9, "context": "Unsupervised learning considers problems such as clustering points according to a similarity metric [7], dimensionality reduction to project data in lower dimensional subspaces [8], and model pre-training [10].", "startOffset": 205, "endOffset": 209}, {"referenceID": 10, "context": "For instance, clustering may be applied to anomaly detection [11].", "startOffset": 61, "endOffset": 65}, {"referenceID": 11, "context": "Reinforcement learning: Methods that learn a policy for action over time given sequences of actions, observations, and rewards fall in the scope of reinforcement learning [12], [13].", "startOffset": 171, "endOffset": 175}, {"referenceID": 12, "context": "Reinforcement learning: Methods that learn a policy for action over time given sequences of actions, observations, and rewards fall in the scope of reinforcement learning [12], [13].", "startOffset": 177, "endOffset": 181}, {"referenceID": 13, "context": "It was reinforcement learning in combination with supervised and unsupervised methods that recently enabled a computer to defeat a human champion at the game of Go [14].", "startOffset": 164, "endOffset": 168}, {"referenceID": 2, "context": "Readers interested in a broad survey of ML are well served by many books covering this rich topic [3], [15], [16].", "startOffset": 98, "endOffset": 101}, {"referenceID": 14, "context": "Readers interested in a broad survey of ML are well served by many books covering this rich topic [3], [15], [16].", "startOffset": 103, "endOffset": 107}, {"referenceID": 15, "context": "Readers interested in a broad survey of ML are well served by many books covering this rich topic [3], [15], [16].", "startOffset": 109, "endOffset": 113}, {"referenceID": 16, "context": "The training data comprises a set of labeled instances, each an executable clearly marked as malicious or benign [17].", "startOffset": 113, "endOffset": 117}, {"referenceID": 17, "context": "The training data could consist of TCP dumps [18].", "startOffset": 45, "endOffset": 49}, {"referenceID": 18, "context": "commonly encountered in anomaly-based network intrusion detection [19].", "startOffset": 66, "endOffset": 70}, {"referenceID": 19, "context": ") [20].", "startOffset": 2, "endOffset": 6}, {"referenceID": 21, "context": "PAC model of learning: PAC learning model has a very rich and extensive body of work [22].", "startOffset": 85, "endOffset": 89}, {"referenceID": 20, "context": "1A few models are non-parametric: for instance the nearest neighbor [21].", "startOffset": 68, "endOffset": 72}, {"referenceID": 22, "context": "hinge loss [23] used in SVMs or the cross-entropy loss [3].", "startOffset": 11, "endOffset": 15}, {"referenceID": 2, "context": "hinge loss [23] used in SVMs or the cross-entropy loss [3].", "startOffset": 55, "endOffset": 58}, {"referenceID": 23, "context": "In the extreme, a learning accuracy of 100% could be achieved by predicting correctly in the positive probability region with lot of misclassification in the zero probability regions (or more precisely sets with measure zero; a detailed discussion of this fact is present in a recent paper [24]).", "startOffset": 290, "endOffset": 294}, {"referenceID": 24, "context": "For the purpose of exposition of the following Sections, we expand upon previous approaches at articulating a threat model for ML [25], [26].", "startOffset": 130, "endOffset": 134}, {"referenceID": 25, "context": "For the purpose of exposition of the following Sections, we expand upon previous approaches at articulating a threat model for ML [25], [26].", "startOffset": 136, "endOffset": 140}, {"referenceID": 26, "context": "The training data may also include additional features not available at runtime (referred to as privileged information in some settings [27]).", "startOffset": 136, "endOffset": 140}, {"referenceID": 27, "context": ", using false training [28]).", "startOffset": 23, "endOffset": 27}, {"referenceID": 27, "context": "Online attacks such as these have been commonly observed in domains such as SPAM detection and network intrusion detection [28].", "startOffset": 123, "endOffset": 127}, {"referenceID": 24, "context": "Inference Phase: Attacks at inference time\u2014exploratory attacks [25]\u2014do not tamper with the targeted model but instead either cause it to produce adversary selected outputs (incorrect outputs, see Integrity attacks below) or simply use the attack to collect evidence about the model characteristics (reconnaissance, see privacy below).", "startOffset": 63, "endOffset": 67}, {"referenceID": 29, "context": ", adversarial example crafting [30].", "startOffset": 31, "endOffset": 35}, {"referenceID": 30, "context": "model by providing a series of carefully crafted inputs and observing outputs [31].", "startOffset": 78, "endOffset": 82}, {"referenceID": 0, "context": "3D tensor in [0,1] Class probabilities Traffic sign JPEG Car brakes Camera Pre-processing Apply model Output analysis", "startOffset": 13, "endOffset": 18}, {"referenceID": 31, "context": "For example, the adversary can use a substitute model to test potential inputs before submitting them to the victim system [32].", "startOffset": 123, "endOffset": 127}, {"referenceID": 32, "context": ", financial market systems [33].", "startOffset": 27, "endOffset": 31}, {"referenceID": 33, "context": ", medical applications [34].", "startOffset": 23, "endOffset": 27}, {"referenceID": 34, "context": "Machine learning models have enough capacity to capture and memorize elements of their training data [35].", "startOffset": 101, "endOffset": 105}, {"referenceID": 35, "context": "Potential risks are adversaries performing membership test (to know whether an individual is in a dataset or not) [36], recovering of partially known inputs (use the model to complete an input vector with the most likely missing bits), and extraction of the training data using the model\u2019s predictions [35].", "startOffset": 114, "endOffset": 118}, {"referenceID": 34, "context": "Potential risks are adversaries performing membership test (to know whether an individual is in a dataset or not) [36], recovering of partially known inputs (use the model to complete an input vector with the most likely missing bits), and extraction of the training data using the model\u2019s predictions [35].", "startOffset": 302, "endOffset": 306}, {"referenceID": 36, "context": "tion system affect the authentication process\u2019s integrity [37].", "startOffset": 58, "endOffset": 62}, {"referenceID": 37, "context": ", accuracy [38].", "startOffset": 11, "endOffset": 15}, {"referenceID": 29, "context": "However, researchers have shown that the integrity of ML systems may be compromised by adversaries capable of manipulating model inputs [30] or its training data [39].", "startOffset": 136, "endOffset": 140}, {"referenceID": 38, "context": "However, researchers have shown that the integrity of ML systems may be compromised by adversaries capable of manipulating model inputs [30] or its training data [39].", "startOffset": 162, "endOffset": 166}, {"referenceID": 39, "context": "More broadly, machine learning models may also not perform correctly when some of their input features are corrupted or missing [40].", "startOffset": 128, "endOffset": 132}, {"referenceID": 24, "context": "a poisoning attack [25], and is an instance of learning in the presence of non-necessarily adversarial but nevertheless noisy data [41].", "startOffset": 19, "endOffset": 23}, {"referenceID": 40, "context": "a poisoning attack [25], and is an instance of learning in the presence of non-necessarily adversarial but nevertheless noisy data [41].", "startOffset": 131, "endOffset": 135}, {"referenceID": 38, "context": "Poisoning attacks alter the training dataset by inserting, editing, or removing points with the intent of modifying the decision boundaries of the targeted model [39], thus targeting the learning system\u2019s integrity per our threat model from Section III.", "startOffset": 162, "endOffset": 166}, {"referenceID": 41, "context": "Thus, all the attacks below bound the adversary in their attacks [42].", "startOffset": 65, "endOffset": 69}, {"referenceID": 42, "context": "In Section VI-A, we present a line of work that builds on that observation to propose learning strategies robust to distribution drifts [43].", "startOffset": 136, "endOffset": 140}, {"referenceID": 38, "context": "formally analyzed PAC-learnability when the adversary is allowed to modify training samples with probability \u03b2 [39].", "startOffset": 111, "endOffset": 115}, {"referenceID": 43, "context": "showed that this was sufficient to degrade inference performance of classifiers learned with SVMs [44], as long", "startOffset": 98, "endOffset": 102}, {"referenceID": 44, "context": "A similar attack approach has been applied in the context of healthcare [45].", "startOffset": 72, "endOffset": 76}, {"referenceID": 43, "context": "As is the case for the approach in [44], this attack requires that a new ML model be learned for each new candidate poisoning point in order to measure the proposed point\u2019s impact on the updated model\u2019s performance", "startOffset": 35, "endOffset": 39}, {"referenceID": 27, "context": "a model that classifies a test input as malicious when it is too far from the empirical mean of the training data [28].", "startOffset": 114, "endOffset": 118}, {"referenceID": 45, "context": "introduce an attack that also inserts inputs in the training set [46].", "startOffset": 65, "endOffset": 69}, {"referenceID": 46, "context": "domain is continuous [47].", "startOffset": 21, "endOffset": 25}, {"referenceID": 47, "context": "prevented Polygraph, a worm signature generation tool [48], from learning meaningful signatures by inserting perturbations in worm traffic flows [49].", "startOffset": 54, "endOffset": 58}, {"referenceID": 48, "context": "prevented Polygraph, a worm signature generation tool [48], from learning meaningful signatures by inserting perturbations in worm traffic flows [49].", "startOffset": 145, "endOffset": 149}, {"referenceID": 45, "context": "adapted the gradient ascent strategy introduced in [46] to feature selection algorithms like LASSO [50].", "startOffset": 51, "endOffset": 55}, {"referenceID": 49, "context": "adapted the gradient ascent strategy introduced in [46] to feature selection algorithms like LASSO [50].", "startOffset": 99, "endOffset": 103}, {"referenceID": 61, "context": "For instance, ML models trained on data centers are compressed and deployed to smartphones [62], in which case reverse engineering may enable adversaries to", "startOffset": 91, "endOffset": 95}, {"referenceID": 50, "context": "White-Box Full No [51], [52], [53] [30], [26], [54] [55] Through pipeline only No [56], [57], [37] [37]", "startOffset": 18, "endOffset": 22}, {"referenceID": 51, "context": "White-Box Full No [51], [52], [53] [30], [26], [54] [55] Through pipeline only No [56], [57], [37] [37]", "startOffset": 24, "endOffset": 28}, {"referenceID": 52, "context": "White-Box Full No [51], [52], [53] [30], [26], [54] [55] Through pipeline only No [56], [57], [37] [37]", "startOffset": 30, "endOffset": 34}, {"referenceID": 29, "context": "White-Box Full No [51], [52], [53] [30], [26], [54] [55] Through pipeline only No [56], [57], [37] [37]", "startOffset": 35, "endOffset": 39}, {"referenceID": 25, "context": "White-Box Full No [51], [52], [53] [30], [26], [54] [55] Through pipeline only No [56], [57], [37] [37]", "startOffset": 41, "endOffset": 45}, {"referenceID": 53, "context": "White-Box Full No [51], [52], [53] [30], [26], [54] [55] Through pipeline only No [56], [57], [37] [37]", "startOffset": 47, "endOffset": 51}, {"referenceID": 54, "context": "White-Box Full No [51], [52], [53] [30], [26], [54] [55] Through pipeline only No [56], [57], [37] [37]", "startOffset": 52, "endOffset": 56}, {"referenceID": 55, "context": "White-Box Full No [51], [52], [53] [30], [26], [54] [55] Through pipeline only No [56], [57], [37] [37]", "startOffset": 82, "endOffset": 86}, {"referenceID": 56, "context": "White-Box Full No [51], [52], [53] [30], [26], [54] [55] Through pipeline only No [56], [57], [37] [37]", "startOffset": 88, "endOffset": 92}, {"referenceID": 36, "context": "White-Box Full No [51], [52], [53] [30], [26], [54] [55] Through pipeline only No [56], [57], [37] [37]", "startOffset": 94, "endOffset": 98}, {"referenceID": 36, "context": "White-Box Full No [51], [52], [53] [30], [26], [54] [55] Through pipeline only No [56], [57], [37] [37]", "startOffset": 99, "endOffset": 103}, {"referenceID": 57, "context": "Black-Box Yes No [58] [36] [59] [60]", "startOffset": 17, "endOffset": 21}, {"referenceID": 35, "context": "Black-Box Yes No [58] [36] [59] [60]", "startOffset": 22, "endOffset": 26}, {"referenceID": 58, "context": "Black-Box Yes No [58] [36] [59] [60]", "startOffset": 27, "endOffset": 31}, {"referenceID": 59, "context": "Black-Box Yes No [58] [36] [59] [60]", "startOffset": 32, "endOffset": 36}, {"referenceID": 31, "context": "Input x only Yes [32], [30], [52] [60] No [31], [61] Through pipeline only No [57]", "startOffset": 17, "endOffset": 21}, {"referenceID": 29, "context": "Input x only Yes [32], [30], [52] [60] No [31], [61] Through pipeline only No [57]", "startOffset": 23, "endOffset": 27}, {"referenceID": 51, "context": "Input x only Yes [32], [30], [52] [60] No [31], [61] Through pipeline only No [57]", "startOffset": 29, "endOffset": 33}, {"referenceID": 59, "context": "Input x only Yes [32], [30], [52] [60] No [31], [61] Through pipeline only No [57]", "startOffset": 34, "endOffset": 38}, {"referenceID": 30, "context": "Input x only Yes [32], [30], [52] [60] No [31], [61] Through pipeline only No [57]", "startOffset": 42, "endOffset": 46}, {"referenceID": 60, "context": "Input x only Yes [32], [30], [52] [60] No [31], [61] Through pipeline only No [57]", "startOffset": 48, "endOffset": 52}, {"referenceID": 56, "context": "Input x only Yes [32], [30], [52] [60] No [31], [61] Through pipeline only No [57]", "startOffset": 78, "endOffset": 82}, {"referenceID": 24, "context": "When the model is a classifier, the adversary seeks to have it assign a wrong class to perturbed inputs [25].", "startOffset": 104, "endOffset": 108}, {"referenceID": 29, "context": "coined the term adversarial example to refer to such inputs [30].", "startOffset": 60, "endOffset": 64}, {"referenceID": 50, "context": "Similar to concurrent work [51], they formalize the search for adversarial examples as the following minimization problem:", "startOffset": 27, "endOffset": 31}, {"referenceID": 25, "context": "This is a source-target misclassification as the target class l 6= h(x) is chosen [26].", "startOffset": 82, "endOffset": 86}, {"referenceID": 62, "context": "For non-convex models h like DNNs, the authors apply the L-BFGS algorithm [63] to solve Equation 2.", "startOffset": 74, "endOffset": 78}, {"referenceID": 51, "context": "introduced the fast gradient sign method [52].", "startOffset": 41, "endOffset": 45}, {"referenceID": 63, "context": "3The MNIST dataset [64] is a widely used corpus of 70,000 handwritten digits used for validating image processing machine learning systems.", "startOffset": 19, "endOffset": 23}, {"referenceID": 51, "context": "ones, make outside of their training data [52], [65].", "startOffset": 42, "endOffset": 46}, {"referenceID": 64, "context": "ones, make outside of their training data [52], [65].", "startOffset": 48, "endOffset": 52}, {"referenceID": 52, "context": "Follow-up work reduced the size of a perturbation r according to different metrics [53], [66].", "startOffset": 83, "endOffset": 87}, {"referenceID": 65, "context": "Follow-up work reduced the size of a perturbation r according to different metrics [53], [66].", "startOffset": 89, "endOffset": 93}, {"referenceID": 25, "context": "the L0 norm of r [26].", "startOffset": 17, "endOffset": 21}, {"referenceID": 55, "context": "This is the case of malware detectors: in this application, adversarial examples are malware applications classified as benign [56].", "startOffset": 127, "endOffset": 131}, {"referenceID": 51, "context": "It is therefore not surprising that randomly sampled inputs can be constrained to be classified in a class with confidence [52], [67].", "startOffset": 123, "endOffset": 127}, {"referenceID": 66, "context": "It is therefore not surprising that randomly sampled inputs can be constrained to be classified in a class with confidence [52], [67].", "startOffset": 129, "endOffset": 133}, {"referenceID": 51, "context": "Unfortunately, training models with a class specific to rubbish (out of distribution) samples does not mitigate adversarial examples [52].", "startOffset": 133, "endOffset": 137}, {"referenceID": 56, "context": "showed how printouts of adversarial examples produced by the fast gradient sign algorithm were still misclassified by an object recognition model [57].", "startOffset": 146, "endOffset": 150}, {"referenceID": 29, "context": "introduced in [30] to find adversarial examples that are printed on glasses frames, which once worn by an individual result in its face being misclassified by a face recognition model [37].", "startOffset": 14, "endOffset": 18}, {"referenceID": 36, "context": "introduced in [30] to find adversarial examples that are printed on glasses frames, which once worn by an individual result in its face being misclassified by a face recognition model [37].", "startOffset": 184, "endOffset": 188}, {"referenceID": 66, "context": "As a natural extension to [67] (see above), it was shown that rubbish audio can be classified with confidence by a speech", "startOffset": 26, "endOffset": 30}, {"referenceID": 67, "context": "recognition system [68].", "startOffset": 19, "endOffset": 23}, {"referenceID": 36, "context": "Consequences are not as important in terms of security then [37]: the audio does not correspond to any legitimate input expected by the speech system or humans.", "startOffset": 60, "endOffset": 64}, {"referenceID": 53, "context": "[54] look at autoregressive models.", "startOffset": 0, "endOffset": 4}, {"referenceID": 68, "context": "improve a model for grasping objects by introducing a competing model that attempts to snatch objects before the original model successfully grasps them [69].", "startOffset": 153, "endOffset": 157}, {"referenceID": 69, "context": "The two models are trained, \u00e0 la Generative Adversarial Networks [70], with competitive cost functions.", "startOffset": 65, "endOffset": 69}, {"referenceID": 54, "context": "infer statistical information about the training dataset from a trained model h\u03b8 [55]: i.", "startOffset": 81, "endOffset": 85}, {"referenceID": 70, "context": "We focus on strategies designed irrespectively of the domain ML is being applied to, albeit heuristics specific to certain applications exist [71], [29], e.", "startOffset": 142, "endOffset": 146}, {"referenceID": 28, "context": "We focus on strategies designed irrespectively of the domain ML is being applied to, albeit heuristics specific to certain applications exist [71], [29], e.", "startOffset": 148, "endOffset": 152}, {"referenceID": 71, "context": "A PAC based work shows that with no access to the training data or ML algorithm, querying the target model and knowledge of the class of target models allows the adversary to reconstruct the model with similar amount of query data as used in training [72] Thus, a key metric when comparing different attacks is the wealth of information returned by the oracle, and the number of oracle queries.", "startOffset": 251, "endOffset": 255}, {"referenceID": 72, "context": "estimate the cost of misclassification in terms of the number of queries to the black-box model [73].", "startOffset": 96, "endOffset": 100}, {"referenceID": 73, "context": "[74] identify the space of convex inducing classifiers\u2014 those where one of the classes is a convex set\u2014that are ACRE", "startOffset": 0, "endOffset": 4}, {"referenceID": 57, "context": "The fitness of genetic variants obtained by mutation is defined in terms of the oracle\u2019s class probability predictions [58].", "startOffset": 119, "endOffset": 123}, {"referenceID": 31, "context": "explored the strategy of training a substitute model for the targeted one [32].", "startOffset": 74, "endOffset": 78}, {"referenceID": 29, "context": "used the cross-model transferability of adversarial samples [30], [52] to design a black-box attack [31].", "startOffset": 60, "endOffset": 64}, {"referenceID": 51, "context": "used the cross-model transferability of adversarial samples [30], [52] to design a black-box attack [31].", "startOffset": 66, "endOffset": 70}, {"referenceID": 30, "context": "used the cross-model transferability of adversarial samples [30], [52] to design a black-box attack [31].", "startOffset": 100, "endOffset": 104}, {"referenceID": 60, "context": "In a follow-up work [61], they show that the attack generalizes to many ML models by having a logistic regression oracle trained by Amazon misclassify 96% of the adversarial examples crafted.", "startOffset": 20, "endOffset": 24}, {"referenceID": 56, "context": "[57] demonstrated that physical adversarial example (i.", "startOffset": 0, "endOffset": 4}, {"referenceID": 35, "context": "show how to conduct this type of attack, named membership inference, against black-box models [36].", "startOffset": 94, "endOffset": 98}, {"referenceID": 58, "context": "present the model inversion attack [59].", "startOffset": 35, "endOffset": 39}, {"referenceID": 74, "context": "Although the approach illustrates privacy concerns that may arise from giving access to ML models trained on sensitive data, it is unclear whether the genomic information is recovered because of the ML model or the strong correlation between the auxiliary information that the adversary also has access to (the patient\u2019s dosage) [75].", "startOffset": 329, "endOffset": 333}, {"referenceID": 34, "context": "Model inversion enables adversaries to extract training data from observed model predictions [35].", "startOffset": 93, "endOffset": 97}, {"referenceID": 75, "context": "However, the input extracted is not actually a specific point of the training dataset, but rather an average representation of the inputs that are classified in a class\u2014similar to what is done by saliency maps [76].", "startOffset": 210, "endOffset": 214}, {"referenceID": 34, "context": "The demonstration is convincing in [35] because each class corresponds to a single individual.", "startOffset": 35, "endOffset": 39}, {"referenceID": 59, "context": "show how to extract parameters of a model from the observation of its predictions [60].", "startOffset": 82, "endOffset": 86}, {"referenceID": 28, "context": "During inference, an adversary might introduce positively connotated words in spam emails to evade detection, thus creating a test distribution different from the one analyzed during training [29].", "startOffset": 192, "endOffset": 196}, {"referenceID": 76, "context": "[77] pull from robust statistics to build a PCA-based detection model robust to poisoning.", "startOffset": 0, "endOffset": 4}, {"referenceID": 43, "context": "limit the vulnerability of SVMs to training label manipulations by adding a regularization term to the loss function, which in turn reduces the model sensitivity to out-of-diagonal kernel matrix elements [44].", "startOffset": 204, "endOffset": 208}, {"referenceID": 77, "context": "Their approach does not impact the convexity of the optimization problem unlike previous attempts [78], [79], which reduces the impact of the defense mechanism on performance.", "startOffset": 98, "endOffset": 102}, {"referenceID": 78, "context": "Their approach does not impact the convexity of the optimization problem unlike previous attempts [78], [79], which reduces the impact of the defense mechanism on performance.", "startOffset": 104, "endOffset": 108}, {"referenceID": 24, "context": "make proposals to secure learning [25].", "startOffset": 34, "endOffset": 38}, {"referenceID": 79, "context": "Lastly, they suggest the use of disinformation with for instance honeypots [80] and randomization of the ML model behavior.", "startOffset": 75, "endOffset": 79}, {"referenceID": 80, "context": "introduce a new ML model, which they name deep contractive networks, trained using a smoothness penalty [81].", "startOffset": 104, "endOffset": 108}, {"referenceID": 81, "context": "to other gradient-based penalties in [82], [83].", "startOffset": 37, "endOffset": 41}, {"referenceID": 82, "context": "to other gradient-based penalties in [82], [83].", "startOffset": 43, "endOffset": 47}, {"referenceID": 83, "context": "The approach introduced in [84] does not involve the expensive computation of gradient-based penalties.", "startOffset": 27, "endOffset": 31}, {"referenceID": 61, "context": "The technique is an adaptation of distillation [62], a mechanism designed to compress large models into smaller ones while preserving prediction accuracy.", "startOffset": 47, "endOffset": 51}, {"referenceID": 84, "context": "In experiments with the fast gradient sign method [85] and the Jacobian attack [84], larger perturbations are required to achieve misclassification of adversarial examples by the distilled model.", "startOffset": 50, "endOffset": 54}, {"referenceID": 83, "context": "In experiments with the fast gradient sign method [85] and the Jacobian attack [84], larger perturbations are required to achieve misclassification of adversarial examples by the distilled model.", "startOffset": 79, "endOffset": 83}, {"referenceID": 85, "context": "However, [86] identified a variant of the attack of [26] which distillation fails to mitigate on one dataset.", "startOffset": 9, "endOffset": 13}, {"referenceID": 25, "context": "However, [86] identified a variant of the attack of [26] which distillation fails to mitigate on one dataset.", "startOffset": 52, "endOffset": 56}, {"referenceID": 86, "context": "A simpler variant of distillation, called label smoothing [87], improves robustness to adversarial samples crafted using the fast gradient sign method [88].", "startOffset": 58, "endOffset": 62}, {"referenceID": 87, "context": "A simpler variant of distillation, called label smoothing [87], improves robustness to adversarial samples crafted using the fast gradient sign method [88].", "startOffset": 151, "endOffset": 155}, {"referenceID": 25, "context": "However this variant was found to not defend against more computation expensive but precise attacks like the Jacobian-based iterative attack [26].", "startOffset": 141, "endOffset": 145}, {"referenceID": 30, "context": "report that defensive distillation can be evaded using a black-box attack [31].", "startOffset": 74, "endOffset": 78}, {"referenceID": 30, "context": "In [31], this is referred to as gradient masking.", "startOffset": 3, "endOffset": 7}, {"referenceID": 29, "context": "Due to the adversarial example transferability property [30] described", "startOffset": 56, "endOffset": 60}, {"referenceID": 29, "context": "[30] first suggested injecting adversarial samples, correctly labeled, in the training set as a means to make the model robust.", "startOffset": 0, "endOffset": 4}, {"referenceID": 51, "context": "9% on adversarial examples [52].", "startOffset": 27, "endOffset": 31}, {"referenceID": 65, "context": "[66] developed the intuition behind adversarial training, i.", "startOffset": 0, "endOffset": 4}, {"referenceID": 88, "context": "They solve the problem using stochastic gradient descent [89].", "startOffset": 57, "endOffset": 61}, {"referenceID": 51, "context": "Their experimentation shows improvements over [52], but they are often statistically non-significative.", "startOffset": 46, "endOffset": 50}, {"referenceID": 52, "context": "[53], where they apply the defense with an attack and evaluate robustness with another one.", "startOffset": 0, "endOffset": 4}, {"referenceID": 29, "context": "The additions of adversarial samples in the training data [30], [53] or modifying", "startOffset": 58, "endOffset": 62}, {"referenceID": 52, "context": "The additions of adversarial samples in the training data [30], [53] or modifying", "startOffset": 64, "endOffset": 68}, {"referenceID": 65, "context": "the training loss function [66] can be viewed as modifying the training distribution D towards such an end.", "startOffset": 27, "endOffset": 31}, {"referenceID": 89, "context": "Stackelberg games also allow for scalability compared to the corresponding simultaneous move game [90], [91].", "startOffset": 98, "endOffset": 102}, {"referenceID": 90, "context": "Stackelberg games also allow for scalability compared to the corresponding simultaneous move game [90], [91].", "startOffset": 104, "endOffset": 108}, {"referenceID": 91, "context": "[92] first recognized that test data manipulation can be seen in the PAC framework as modifying the", "startOffset": 0, "endOffset": 4}, {"referenceID": 92, "context": "[93]):", "startOffset": 0, "endOffset": 4}, {"referenceID": 91, "context": "[92] add a regularizer for the learner and the cost function c(x, x\u2032) as the l2 distance between x and x\u2032.", "startOffset": 0, "endOffset": 4}, {"referenceID": 72, "context": "[73], Li et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 93, "context": "[94] use a cost function that is relevant for many security settings.", "startOffset": 0, "endOffset": 4}, {"referenceID": 94, "context": "This is captured by differential privacy [95], a rigorous framework to analyze the privacy guarantees provided by algorithms.", "startOffset": 41, "endOffset": 45}, {"referenceID": 95, "context": "An instance of training data randomization is formalized by local privacy [96].", "startOffset": 74, "endOffset": 78}, {"referenceID": 96, "context": "showed that this allowed the developers of a browser to collect meaningful and privacy-preserving usage statistics from users [97].", "startOffset": 126, "endOffset": 130}, {"referenceID": 97, "context": "This strategy was explored in [98], [99].", "startOffset": 30, "endOffset": 34}, {"referenceID": 98, "context": "This strategy was explored in [98], [99].", "startOffset": 36, "endOffset": 40}, {"referenceID": 99, "context": "troducing random noise\u2014drawn from an exponential distribution and scaled using the model sensitivity5\u2014in the cost function minimized during learning, can provide \u03b5-differential privacy [100].", "startOffset": 185, "endOffset": 190}, {"referenceID": 100, "context": "privacy analysis, along with references to many of the works intervening in private empirical risk minimization [101]", "startOffset": 112, "endOffset": 117}, {"referenceID": 101, "context": "showed that large-capacity models like deep neural networks can be trained with multi-party computations from perturbed parameters and provide differential privacy guarantees [102].", "startOffset": 175, "endOffset": 180}, {"referenceID": 102, "context": "introduced an alternative approach to improve the privacy guarantees provided: the strategy followed is to randomly perturb parameters during the stochastic gradient descent performed by the learning algorithm [103].", "startOffset": 210, "endOffset": 215}, {"referenceID": 103, "context": "use homomorphic encryption [104] to encrypt the data in a form that allows a neural network to process it without decrypting it [105].", "startOffset": 27, "endOffset": 32}, {"referenceID": 104, "context": "use homomorphic encryption [104] to encrypt the data in a form that allows a neural network to process it without decrypting it [105].", "startOffset": 128, "endOffset": 133}, {"referenceID": 105, "context": "This is especially important in applications like credit decisions or healthcare [106].", "startOffset": 81, "endOffset": 86}, {"referenceID": 106, "context": "It should not nurture discrimination against specific individuals [107].", "startOffset": 66, "endOffset": 71}, {"referenceID": 107, "context": "data also inherently reflects social biases [108].", "startOffset": 44, "endOffset": 49}, {"referenceID": 108, "context": "first learn an intermediate representation that encodes a sanitized variant of the data, as first discussed in [109].", "startOffset": 111, "endOffset": 116}, {"referenceID": 109, "context": "showed that fairness could be achieved by learning in competition with an adversary trying to predict the sensitive variable from the fair model\u2019s prediction [110].", "startOffset": 158, "endOffset": 163}, {"referenceID": 110, "context": "ing [111].", "startOffset": 4, "endOffset": 9}, {"referenceID": 111, "context": "measures to estimate the influence of specific inputs on the model output [112].", "startOffset": 74, "endOffset": 79}, {"referenceID": 112, "context": "An approach named activation maximization synthesizes an input that highly activates a specific neuron in a neural network [113].", "startOffset": 123, "endOffset": 128}, {"referenceID": 113, "context": "producing synthetic inputs easily interpreted by humans [114] but faithfully representing the model\u2019s behavior.", "startOffset": 56, "endOffset": 61}, {"referenceID": 38, "context": "Recall the theoretical characterization for data poisoning by Kearns and Li [39] (see Section IV).", "startOffset": 76, "endOffset": 80}, {"referenceID": 114, "context": "Note that this result (presented below) is analogous to the no free lunch result in data privacy that captures the tension between utility and privacy [115], [116].", "startOffset": 151, "endOffset": 156}, {"referenceID": 21, "context": "Moreover, insufficient data presents fundamental information theoretic limitations [22] on learning accuracy problems in the benign (without adversaries) setting itself.", "startOffset": 83, "endOffset": 87}, {"referenceID": 93, "context": "Thus, while the above theoretical result and recent empirical work [94] suggests more complex models for defeating adversaries, in practice, availability of data may prohibit the use of this general result.", "startOffset": 67, "endOffset": 71}], "year": 2016, "abstractText": "Advances in machine learning (ML) in recent years have enabled a dizzying array of applications such as data analytics, autonomous systems, and security diagnostics. ML is now pervasive\u2014new systems and models are being deployed in every domain imaginable, leading to rapid and widespread deployment of software based inference and decision making. There is growing recognition that ML exposes new vulnerabilities in software systems, yet the technical community\u2019s understanding of the nature and extent of these vulnerabilities remains limited. We systematize recent findings on ML security and privacy, focusing on attacks identified on these systems and defenses crafted to date. We articulate a comprehensive threat model for ML, and categorize attacks and defenses within an adversarial framework. Key insights resulting from works both in the ML and security communities are identified and the effectiveness of approaches are related to structural elements of ML algorithms and the data used to train them. We conclude by formally exploring the opposing relationship between model accuracy and resilience to adversarial manipulation. Through these explorations, we show that there are (possibly unavoidable) tensions between model complexity, accuracy, and resilience that must be calibrated for the environments in which they will be used.", "creator": "LaTeX with hyperref package"}}}