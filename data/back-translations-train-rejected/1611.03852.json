{"id": "1611.03852", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Nov-2016", "title": "A Connection between Generative Adversarial Networks, Inverse Reinforcement Learning, and Energy-Based Models", "abstract": "Generative adversarial networks (GANs) are a recently proposed class of generative models in which a generator is trained to optimize a cost function that is being simultaneously learned by a discriminator. While the idea of learning cost functions is relatively new to the field of generative modeling, learning costs has long been studied in control and reinforcement learning (RL) domains, typically for imitation learning from demonstrations. In these fields, learning cost function underlying observed behavior is known as inverse reinforcement learning (IRL) or inverse optimal control. While at first the connection between cost learning in RL and cost learning in generative modeling may appear to be a superficial one, we show in this paper that certain IRL methods are in fact mathematically equivalent to GANs. In particular, we demonstrate an equivalence between a sample-based algorithm for maximum entropy IRL and a GAN in which the generator's density can be evaluated and is provided as an additional input to the discriminator. Interestingly, maximum entropy IRL is a special case of an energy-based model. We discuss the interpretation of GANs as an algorithm for training energy-based models, and relate this interpretation to other recent work that seeks to connect GANs and EBMs. By formally highlighting the connection between GANs, IRL, and EBMs, we hope that researchers in all three communities can better identify and apply transferable ideas from one domain to another, particularly for developing more stable and scalable algorithms: a major challenge in all three domains.", "histories": [["v1", "Fri, 11 Nov 2016 20:53:45 GMT  (20kb)", "http://arxiv.org/abs/1611.03852v1", "Submitted to the NIPS 2016 Workshop on Adversarial Training. First two authors contributed equally"], ["v2", "Wed, 16 Nov 2016 18:11:26 GMT  (20kb)", "http://arxiv.org/abs/1611.03852v2", "NIPS 2016 Workshop on Adversarial Training. First two authors contributed equally"], ["v3", "Fri, 25 Nov 2016 08:09:55 GMT  (20kb)", "http://arxiv.org/abs/1611.03852v3", "NIPS 2016 Workshop on Adversarial Training. First two authors contributed equally"]], "COMMENTS": "Submitted to the NIPS 2016 Workshop on Adversarial Training. First two authors contributed equally", "reviews": [], "SUBJECTS": "cs.LG cs.AI", "authors": ["chelsea finn", "paul christiano", "pieter abbeel", "sergey levine"], "accepted": false, "id": "1611.03852"}, "pdf": {"name": "1611.03852.pdf", "metadata": {"source": "CRF", "title": "A Connection Between Generative Adversarial Networks, Inverse Reinforcement Learning, and Energy-Based Models", "authors": ["Chelsea Finn", "Paul Christiano", "Pieter Abbeel", "Sergey Levine"], "emails": ["cbfinn@eecs.berkeley.edu", "paulfchristiano@eecs.berkeley.edu", "pabbeel@eecs.berkeley.edu", "svlevine@eecs.berkeley.edu"], "sections": [{"heading": null, "text": "ar Xiv: 161 1,03 852v 1 [cs.L G] 11 N"}, {"heading": "1 Introduction", "text": "In fact, most of them are able to survive on their own, without being able to survive on their own."}, {"heading": "2 Background", "text": "In this section, we formally define generative adversary networks (GANs), energy-based models (EBMs), and inverse amplification learning (IRL), and introduce notation."}, {"heading": "2.1 Generative Adversarial Networks", "text": "Generative adversarial networks are an approach to generative modeling in which two models are trained simultaneously: a generator G and a discriminator D. The discriminator has the task of classifying its inputs either as output from the generator or as actual samples from the underlying data distribution p (x).The aim of the generator is to generate outputs that are classified by the discriminator as originating from the underlying data distribution [8].Formally, the generator considers noise as input and outputs a sample x \u00b2 G, while the discriminator takes a sample x and outputs the probability D (x) that the sample comes from the data distribution. Loss of the discriminator is the average protocol probability that it assigns to the correct classification, evaluated on the basis of an equal mixture of real samples and outputs from the generator: Ldiscriminator (D) = Ex \u00b2 p [\u2212 logD (x)] +. The loss of the discriminator can be defined by several similar types."}, {"heading": "2.2 Energy-Based Models", "text": "Energy-based models [14] associate an energy value E\u03b8 (x) with a sample x, thereby modelling the data as a Boltzmann distribution: p\u03b8 (x) = 1 Z exp (\u2212 E\u03b8 (x)) (1) The energy function parameters are often chosen to maximise the probability of the data; the biggest challenge in this optimisation is the evaluation of the partition function Z, which is an insoluble sum or an integral element in most high-dimensional problems. A common approach to estimating Z requires scanning the Boltzmann distribution plus (x) within the inner learning loop. Scanning from p\u03b8 (x) can be approximated using the Markov Chain Monte Carlo (MCMC) methods; however, these methods face problems if there are several different modes of distribution and, as a result, arbitrarily large amounts of time are required to produce a series of different samples."}, {"heading": "2.3 Inverse Reinforcement Learning", "text": "The aim of inverse reinforcement learning is to derive the cost function of the demonstrated behavior [15]. It is typically assumed that the demonstrations come from an expert who behaves almost optimally at unknown costs. In this section we discuss MaxEnt IRL and Guided Cost Learning, an algorithm for MaxEnt IRL."}, {"heading": "2.3.1 Maximum entropy inverse reinforcement learning", "text": "Maximum entropy inverse amplification learning models demonstrate using a Boltzmann distribution in which the energy is given by the cost function c\u03b8: p\u03b8 (\u03c4) = 1 Z exp (\u2212 c\u03b8 (\u03c4)), Here \u03c4 = {x1, u1,..., xT, uT} is a trajectory; c\u03b8 (\u03c4) = \u2211 t c\u03b8 (xt, ut) is an learned cost function that is in accordance with the environmental dynamics; xt and ut are the state and action at the time of the step t; and the division function Z is the integral of exp (\u2212 c\u03b8) over all trajectories that are in accordance with the environmental dynamics. 2Under this model, the optimal trajectories have the highest probability, and the expert can generate suboptimal trajectories with a probability that decreases exponentially as the trajectories become more expensive. As in other energy-based models, the parameters are optimized to maximize the small dynamics, and the probability for this is the first application dynamics."}, {"heading": "2.3.2 Guided cost learning", "text": "Guided Cost Learning introduces an iterative, sample-based method of estimating Z in the MaxEnt q formula and can be scaled to high-dimensional state and scope and non-linear cost functions. [7] The algorithm estimates Z by creating a new sample distribution q (\u03c4) and using meaning samples q (empirical samples) (empirical samples) (empirical samples) (empirical samples) q (empirical samples) q (empirical samples) p (empirical samples) [empirical samples) p (empirical samples) p (empirical samples) = empirical samples p (empirical samples) = empirical samples p (empirical sample) [empirical sample) p (empirical sample) = empirical samples [empirical sample) p (empirical sample) p (empirical sample) irical sample (empirical sample) p (empirical sample) p (empirical sample) p (empirical sample) p (empirical sample) p (empirical sample) p (empirical sample)."}, {"heading": "2.4 Direct Maximum Likelihood and Behavioral Cloning", "text": "In this context, it should be noted that the measures in question are measures primarily aimed at achieving objectives and objectives."}, {"heading": "3 GANs and IRL", "text": "We will now show how generative, hostile modeling has been implicitly applied to the setting of inverse reinforcement learning, where the data to be modeled is a series of expert demonstrations. Derivation requires a certain form of discriminator, which we will first discuss in Section 3.1. After making this modification to the discriminator, we will get an algorithm for IRL, as we show in Section 3.2, where the discriminator includes the learned costs and the generator represents policy."}, {"heading": "3.1 A special form of discriminator", "text": "For a fixed generator with a [typically unknown] density q (\u03c4), the optimal discriminator is the following [8]: D \u0445 (\u03c4) = p (\u03c4) p (\u03c4) + q (\u03c4), (3) where p (\u03c4) is the actual distribution of the data. In the traditional GAN algorithm, the discriminator is trained to output this value directly. If the generator density q (\u03c4) can be evaluated, the traditional GAN discriminator can be modified to include this density information. Instead of the discriminator being able to directly estimate the value of Equation 3, it can be used to estimate the stability of the generator by filling the value of q (\u03c4) with its known value. In this case, the new form of the discriminator DTB can be used with the parameters isDTB = p, which directly estimate the value of Equation 3."}, {"heading": "3.2 Equivalence between generative adversarial networks and guided cost learning", "text": "In this section we show that GANs, when applied to IRL problems, optimize the same goal as MaxEnt q. And in fact, the variant of GANs described in the previous section is exactly the same as guided cost learning. Remember that the loss of the discriminator corresponds exactly to the discriminator q.: Ldiscriminator (D\u03b8) = E\u03c4 p [\u2212 logD\u03b8 (\u03c4)] + E\u03c4 q [\u2212 log (1 \u2212 D\u03b8 (\u03c4))] = E\u03c4 p [\u2212 log1 Z exp (\u2212 c\u03b8)) 1 Z exp (\u03c4) 1 Z exp (\u03c4)) 1 Z exp (\u03c4) 1 Z exp (\u03c4) 1 Z exp (\u03c4) + q (sub))."}, {"heading": "3.2.1 Z estimates the partition function", "text": "We can calculate the loss of the discriminator as follows: Ldiscriminator (DTB) = EBA (EBA) = EBA (EBA) [\u2212 log 1 Z exp (\u2212 c\u03b8 (\u03c4))] + EBA [\u2212 logq (\u03c4)]] (6) = logZ + EDA (p [c\u03b8 (\u03c4)] + EDA (log \u00b5 (\u03c4))] \u2212 EBA q [logq (\u03c4)] + E\u03c4 \u0445 q [log \u00b5 (\u03c4)] (7) = logZ + E\u03c4 p [c\u03b8 (\u0442)] \u2212 EBA q [log)] + 2EAT \u2012 \u00b5 [log \u00b5]]. (8) Only the first and last term depend on Z. At the minimization value of Z, the derivation of this term with respect to Z will be the same: The meaning of the discriminator (DDC) = 01 Z = EZ."}, {"heading": "3.2.2 c\u03b8 optimizes the IRL objective", "text": "We return to the discriminator loss, as calculated in Equation 8, and look at the derivative with respect to the parameters. We will show that this is exactly the same as the derivative of the IRL objective. Only the second and fourth terms in the sum depend on the sum. If we differentiate these terms, we get on the other hand: If we differentiate the MaxEnt IRL target, we get: [1 Z-Exp (\u2212 cTB)] + Equality-Exp-Exp-Exp-Exp-Implicit-Implicit-Implicit-Implicit-Implicit-Implicit-Implicit-Implicit-Implicit-Implicit-Implicit-Implicit-Implicit-Implicit-Implicit-Implicit-Implicit-Implicit-Implicit-Implicit-Implicit-Implicit-Implicit-Implicit-Implicit-Implicit-Implicit-Implicit-Implicit-Implicit-Implicit-Implicit-Implicit-Implicit-Implicit-Implicit-Implicit-Implicit-Implicit-Implicit-Implicit-Implicit-Implicit-Implicit-Implicit-Implicit-Implicit-Implicit-Implicit-Implicit-Implicit-Implicit-Implicit-Implicit-Implicit-Implicit-Implicit-Implicit-Implicit-Implicit-Implicit-Implicit-Implicit-Implicit-Implicit-Implicit-Implicit-Implicit-Implicit-Implicit-Implicit-Implicit-Implicit-Implicit-Implicit-Implicit-Implicit-Implicit-Implicit-Implicit-Implicit-Implicit-Implicit-Implicit-Implicit-Implicit-Implicit-Implicit-Implicit-Implicit-Implicit-Implicit-Implicit-Implicit-Implicit-implicit-implicit-"}, {"heading": "3.3 The generator optimizes the MaxEnt IRL objective", "text": "In conclusion, we calculate the loss of the generator: LGenerator (LGenerator) = EGenerator q [1 \u2212 D (\u03c4) \u2212 log (D (\u03c4)] = EGenerator q [logq) \u00b5 (\u03c4) \u2212 log 1 Z exp (\u2212 c\u03b8) \u00b5 EGenerator (D) EGenerator (D) = EGenerator (D) = EGenerator (D) \u2212 log 1 Z exp (\u2212 c\u03b8) \u00b5 EGenerator (D)"}, {"heading": "3.4 Discussion", "text": "There are many obvious differences between the MaxEnt IRL and the GAN optimization problem. But we have shown that after a single key change - the use of a generator q (\u03c4) for which densities can be efficiently evaluated, and the natural incorporation of this information into the discriminator - generative, hostile networks can be considered a sample-based algorithm for the MaxEnt IRL problem. By connecting GANs with the empirical literature on inverse amplification learning [7], this shows that GAN training can improve the quality of samples, even if the density of the generator can be accurately evaluated. By creating this link, we can derive a new, contrary training strategy for energy-based models, which we will discuss in the next section."}, {"heading": "4 GANs for training EBMs", "text": "As discussed in Section 2.2, the primary challenge in the training of EBMs is to estimate the partition function, which occurs by approximate discrimination from the differentiation span induced by energy consumption. Two recent papers have suggested using the partition function to derive quick estimates of the partition function [12, 24]. Specifically, these methods alternate between training a generator to generate samples with minimal energy, and optimizing the parameters of the energy function using the samples to estimate the partition function. However, if the density of the generator is available, we can derive an unbiased estimate of the partition function to produce samples with minimal energy."}, {"heading": "5 Related Work", "text": "The proposed algorithm, Generative Adversarial Imitation Learning (GAIL), has a hostile structure, and the analysis in this paper provides additional insights into what GAIL does. As discussed above, GANs optimize the same goal as MaxEnt IRL. In this case, the GAIL policy is trained to optimize the costs learned through MaxEnt IRL. However, unlike guided cost learning [7], Ho & Ermon uses the typical, unrestricted form of the discriminator [9] and does not use the density of the generator. In this case, the cost function remains implicit within the discriminator and cannot be restored. Unlike guided cost learning [7], however, the discriminator is discarded and the policy is the end result. Bachman & Precup [1] suggests that data generation remains implicitly within the discriminator and cannot be restored."}, {"heading": "6 Discussion", "text": "In this paper, we demonstrated an equivalence between generative adversarial modeling and an algorithm for performing maximum inverse entropy amplification learning. Our derivation used a special form of discriminator that uses probability values from the generator, leading to an unbiased estimate of the underlying energy function. A natural direction for future work is to combine with deep generators that can provide densities, such as auto-regressive models [13, 21] or models that use invertable transformations [6], with generative adversarial modeling. Such an approach could provide more stable education, better generators, and broader applicability to discrete problems such as language. This work also suggests a new algorithm for training energy-based models using generative adversarial networks that trains a neural network model to try out the distribution induced by current energy."}, {"heading": "Acknowledgments", "text": "The authors thank Ian Goodfellow and Joan Bruna for their insightful discussions."}], "references": [{"title": "Data generation as sequential decision making", "author": ["P. Bachman", "D. Precup"], "venue": "In Neural Information Processing Systems (NIPS),", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2015}, {"title": "An actor-critic algorithm for sequence prediction", "author": ["D. Bahdanau", "P. Brakel", "K. Xu", "A. Goyal", "R. Lowe", "J. Pineau", "A. Courville", "Y. Bengio"], "venue": "arXiv preprint arXiv:1607.07086,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2016}, {"title": "Scheduled sampling for sequence prediction with recurrent neural networks", "author": ["S. Bengio", "O. Vinyals", "N. Jaitly", "N. Shazeer"], "venue": "In Neural Information Processing Systems (NIPS),", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2015}, {"title": "Relative entropy inverse reinforcement learning", "author": ["A. Boularias", "J. Kober", "J. Peters"], "venue": "In International Conference on Artificial Intelligence and Statistics (AISTATS),", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2011}, {"title": "Density estimation using real nvp", "author": ["L. Dinh", "J. Sohl-Dickstein", "S. Bengio"], "venue": "arXiv preprint arXiv:1605.08803,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2016}, {"title": "Guided cost learning: Deep inverse optimal control via policy optimization", "author": ["C. Finn", "S. Levine", "P. Abbeel"], "venue": "International Conference on Machine Learning (ICML),", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2016}, {"title": "Generative adversarial nets", "author": ["I. Goodfellow", "J. Pouget-Abadie", "M. Mirza", "B. Xu", "D. Warde-Farley", "S. Ozair", "A. Courville", "Y. Bengio"], "venue": "In Neural Information Processing Systems (NIPS),", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2014}, {"title": "Generative adversarial imitation learning", "author": ["J. Ho", "S. Ermon"], "venue": "Neural Information Processing Systems (NIPS),", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2016}, {"title": "Model-free imitation learning with policy optimization", "author": ["J. Ho", "J.K. Gupta", "S. Ermon"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2016}, {"title": "Learning objective functions for manipulation", "author": ["M. Kalakrishnan", "P. Pastor", "L. Righetti", "S. Schaal"], "venue": "In International Conference on Robotics and Automation (ICRA),", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2013}, {"title": "Deep directed generative models with energy-based probability estimation", "author": ["T. Kim", "Y. Bengio"], "venue": "ICLR Workshop Track,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2016}, {"title": "The neural autoregressive distribution estimator", "author": ["H. Larochelle", "I. Murray"], "venue": "In International Conference on Artificial Intelligence and Statistics (AISTATS),", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2011}, {"title": "A tutorial on energy-based learning", "author": ["Y. LeCun", "S. Chopra", "R. Hadsell", "M. Ranzato", "F. Huang"], "venue": "Predicting structured data,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2006}, {"title": "Algorithms for inverse reinforcement learning", "author": ["A. Ng", "S. Russell"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2000}, {"title": "Reward augmented maximum likelihood for neural structured prediction", "author": ["M. Norouzi", "S. Bengio", "Z. Chen", "N. Jaitly", "M. Schuster", "Y. Wu", "D. Schuurmans"], "venue": "Neural Information Processing Systems (NIPS),", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2016}, {"title": "f-gan: Training generative neural samplers using variational divergence minimization", "author": ["S. Nowozin", "B. Cseke", "R. Tomioka"], "venue": "Neural Information Processing Systems (NIPS),", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2016}, {"title": "Wavenet: A generative model for raw audio", "author": ["A. v. d. Oord", "S. Dieleman", "H. Zen", "K. Simonyan", "O. Vinyals", "A. Graves", "N. Kalchbrenner", "A. Senior", "K. Kavukcuoglu"], "venue": "arXiv preprint arXiv:1609.03499,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2016}, {"title": "Sequence level training with recurrent neural networks", "author": ["M. Ranzato", "S. Chopra", "M. Auli", "W. Zaremba"], "venue": "International Conference on Learning Representations (ICLR),", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2016}, {"title": "A reduction of imitation learning and structured prediction to no-regret online learning", "author": ["S. Ross", "G. Gordon", "A. Bagnell"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2011}, {"title": "Pixel recurrent neural networks", "author": ["A. van den Oord", "N. Kalchbrenner", "K. Kavukcuoglu"], "venue": "International Conference on Machine Learning (ICML),", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2016}, {"title": "Google\u2019s neural machine translation system: Bridging the gap between human and machine translation", "author": ["Y. Wu", "M. Schuster", "Z. Chen", "Q.V. Le", "M. Norouzi", "W. Macherey", "M. Krikun", "Y. Cao", "Q. Gao", "K. Macherey"], "venue": "arXiv preprint arXiv:1609.08144,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2016}, {"title": "Seqgan: Sequence generative adversarial nets with policy gradient", "author": ["L. Yu", "W. Zhang", "J. Wang", "Y. Yu"], "venue": "arXiv preprint arXiv:1609.05473,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2016}, {"title": "Energy-based generative adversarial network", "author": ["J. Zhao", "M. Mathieu", "Y. LeCun"], "venue": "arXiv preprint arXiv:1609.03126,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2016}, {"title": "Modeling purposeful adaptive behavior with the principle of maximum causal entropy", "author": ["B. Ziebart"], "venue": "PhD thesis,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2010}, {"title": "Maximum entropy inverse reinforcement learning", "author": ["B. Ziebart", "A. Maas", "J.A. Bagnell", "A.K. Dey"], "venue": "In AAAI Conference on Artificial Intelligence,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2008}], "referenceMentions": [{"referenceID": 6, "context": "Generative adversarial networks (GANs) are a recently proposed class of generative models in which a generator is trained to optimize a cost function that is being simultaneously learned by a discriminator [8].", "startOffset": 206, "endOffset": 209}, {"referenceID": 13, "context": "While the idea of learning objectives is relatively new to the field of generative modeling, learning cost or reward functions has long been studied in control [5] and was popularized in 2000 for reinforcement learning problems [15].", "startOffset": 228, "endOffset": 232}, {"referenceID": 24, "context": "[26].", "startOffset": 0, "endOffset": 4}, {"referenceID": 3, "context": "with unknown dynamics, using nonlinear function classes, such as neural networks [4, 11, 7].", "startOffset": 81, "endOffset": 91}, {"referenceID": 9, "context": "with unknown dynamics, using nonlinear function classes, such as neural networks [4, 11, 7].", "startOffset": 81, "endOffset": 91}, {"referenceID": 5, "context": "with unknown dynamics, using nonlinear function classes, such as neural networks [4, 11, 7].", "startOffset": 81, "endOffset": 91}, {"referenceID": 18, "context": "This is precisely analogous to the observed ability of inverse reinforcement learning to imitate behaviors that cannot be successfully learned through behavioral cloning [20], direct maximum likelihood regression to the demonstrated behavior.", "startOffset": 170, "endOffset": 174}, {"referenceID": 23, "context": "Interestingly, the maximum entropy formulation of IRL is a special case of an energy-based model (EBM) [25].", "startOffset": 103, "endOffset": 107}, {"referenceID": 10, "context": "Recent works have recognized a connection between EBMs and GANs [12, 24].", "startOffset": 64, "endOffset": 72}, {"referenceID": 22, "context": "Recent works have recognized a connection between EBMs and GANs [12, 24].", "startOffset": 64, "endOffset": 72}, {"referenceID": 10, "context": "In this work, we particularly focus on EBMs trained with maximum likelihood, and expand upon the connection recognized by Kim & Bengio [12] for the case where the generator\u2019s density can be computed.", "startOffset": 135, "endOffset": 139}, {"referenceID": 6, "context": "The goal of the generator is to produce outputs that are classified by the discriminator as coming from the underlying data distribution [8].", "startOffset": 137, "endOffset": 140}, {"referenceID": 6, "context": "The simplest definition, originally proposed in [8], is simply the opposite of the discriminator\u2019s loss.", "startOffset": 48, "endOffset": 51}, {"referenceID": 6, "context": "It is common to instead use the log of the discriminator\u2019s confusion [8].", "startOffset": 69, "endOffset": 72}, {"referenceID": 12, "context": "Energy-based models [14] associate an energy value E\u03b8 (x) with a sample x, modeling the data as a Boltzmann distribution: p\u03b8 (x) = 1 Z exp(\u2212E\u03b8 (x)) (1) The energy function parameters \u03b8 are often chosen to maximize the likelihood of the data; the main challenge in this optimization is evaluating the partition function Z, which is an intractable sum or integral for most high-dimensional problems.", "startOffset": 20, "endOffset": 24}, {"referenceID": 12, "context": "Approximate inference methods can also be used during training, though the energy function may incorrectly assign low energy to some modes if the approximate inference method cannot find them [14].", "startOffset": 192, "endOffset": 196}, {"referenceID": 13, "context": "The goal of inverse reinforcement learning is to infer the cost function underlying demonstrated behavior [15].", "startOffset": 106, "endOffset": 110}, {"referenceID": 24, "context": "The first applications of this model computed Z exactly with dynamic programming [26].", "startOffset": 81, "endOffset": 85}, {"referenceID": 5, "context": "Guided cost learning introduces an iterative sample-based method for estimating Z in the MaxEnt IRL formulation, and can scale to high-dimensional state and action spaces and nonlinear cost functions [7].", "startOffset": 200, "endOffset": 203}, {"referenceID": 23, "context": "A more general form of this equation can be derived for stochastic dynamics [25].", "startOffset": 76, "endOffset": 80}, {"referenceID": 20, "context": "In the field of generative modeling, this approach has most commonly been applied to speech and language generation tasks [22, 18], but has also been applied to image generation [21].", "startOffset": 122, "endOffset": 130}, {"referenceID": 16, "context": "In the field of generative modeling, this approach has most commonly been applied to speech and language generation tasks [22, 18], but has also been applied to image generation [21].", "startOffset": 122, "endOffset": 130}, {"referenceID": 19, "context": "In the field of generative modeling, this approach has most commonly been applied to speech and language generation tasks [22, 18], but has also been applied to image generation [21].", "startOffset": 178, "endOffset": 182}, {"referenceID": 18, "context": "training distribution and makes a catastrophic error [20].", "startOffset": 53, "endOffset": 57}, {"referenceID": 18, "context": "A popular approach for handling this involves incrementally sampling more from the model and drawing less from the data distribution during training [20].", "startOffset": 149, "endOffset": 153}, {"referenceID": 2, "context": "proposed an approximate solution, termed scheduled sampling, that does not require querying the data distribution [3].", "startOffset": 114, "endOffset": 117}, {"referenceID": 6, "context": "For a fixed generator with a [typically unknown] density q(\u03c4), the optimal discriminator is the following [8]: D(\u03c4) = p(\u03c4) p(\u03c4)+ q(\u03c4) , (3) where p(\u03c4) is the actual distribution of the data.", "startOffset": 106, "endOffset": 109}, {"referenceID": 5, "context": "By connecting GANs to the empirical literature on inverse reinforcement learning [7], this demonstrates that GAN training can improve the quality of samples even when the generator\u2019s density can be evaluated exactly.", "startOffset": 81, "endOffset": 84}, {"referenceID": 10, "context": "Two recent papers have proposed to use adversarial training to derive fast estimates of the partition function [12, 24].", "startOffset": 111, "endOffset": 119}, {"referenceID": 22, "context": "Two recent papers have proposed to use adversarial training to derive fast estimates of the partition function [12, 24].", "startOffset": 111, "endOffset": 119}, {"referenceID": 10, "context": "Kim & Bengio proposed a similar energy-based model for generative image modeling, but did not assume they could compute the generator\u2019s density [12].", "startOffset": 144, "endOffset": 148}, {"referenceID": 22, "context": "also proposed an energy-based GAN model with an autoencoder discriminator where the energy is given by the mean-squared error between the data example (generated or real) and the discriminator\u2019s reconstruction [24].", "startOffset": 210, "endOffset": 214}, {"referenceID": 12, "context": "An interesting direction for future exploration is to consider combining the GAN training algorithm discussed here with an objective other than log-likelihood, such as one used with EBMs [14] or different f -divergences used with GANs [17].", "startOffset": 187, "endOffset": 191}, {"referenceID": 15, "context": "An interesting direction for future exploration is to consider combining the GAN training algorithm discussed here with an objective other than log-likelihood, such as one used with EBMs [14] or different f -divergences used with GANs [17].", "startOffset": 235, "endOffset": 239}, {"referenceID": 8, "context": "[10, 9] previously presented a GAN-like algorithm for imitation learning, where the goal is to recover a policy that matches the expert demonstrations.", "startOffset": 0, "endOffset": 7}, {"referenceID": 7, "context": "[10, 9] previously presented a GAN-like algorithm for imitation learning, where the goal is to recover a policy that matches the expert demonstrations.", "startOffset": 0, "endOffset": 7}, {"referenceID": 5, "context": "Unlike guided cost learning [7], however, Ho & Ermon use the typical unconstrained form of the discriminator [9] and do not use the generator\u2019s density.", "startOffset": 28, "endOffset": 31}, {"referenceID": 7, "context": "Unlike guided cost learning [7], however, Ho & Ermon use the typical unconstrained form of the discriminator [9] and do not use the generator\u2019s density.", "startOffset": 109, "endOffset": 112}, {"referenceID": 0, "context": "Bachman & Precup [1] suggested that data generation can be converted into a sequential decisionmaking problem and solved with a reinforcement learning method.", "startOffset": 17, "endOffset": 20}, {"referenceID": 17, "context": "Several recent works have proposed methods for merging maximum likelihood objectives and known reward functions for training sequential language generation models and rely on surrogate reward function such as BLEU score or edit distance [19, 16, 2].", "startOffset": 237, "endOffset": 248}, {"referenceID": 14, "context": "Several recent works have proposed methods for merging maximum likelihood objectives and known reward functions for training sequential language generation models and rely on surrogate reward function such as BLEU score or edit distance [19, 16, 2].", "startOffset": 237, "endOffset": 248}, {"referenceID": 1, "context": "Several recent works have proposed methods for merging maximum likelihood objectives and known reward functions for training sequential language generation models and rely on surrogate reward function such as BLEU score or edit distance [19, 16, 2].", "startOffset": 237, "endOffset": 248}, {"referenceID": 21, "context": "proposed to learn a cost function for sequential data generation using GANs, where the cost is defined as the probability of the discriminator classifying the generated sequence as coming from the data distribution [23].", "startOffset": 215, "endOffset": 219}, {"referenceID": 11, "context": "A natural direction for future work is to experiment with combining deep generators that can provide densities, such as autoregressive models [13, 21] or models that use invertible transformations [6], with generative adversarial modeling.", "startOffset": 142, "endOffset": 150}, {"referenceID": 19, "context": "A natural direction for future work is to experiment with combining deep generators that can provide densities, such as autoregressive models [13, 21] or models that use invertible transformations [6], with generative adversarial modeling.", "startOffset": 142, "endOffset": 150}, {"referenceID": 4, "context": "A natural direction for future work is to experiment with combining deep generators that can provide densities, such as autoregressive models [13, 21] or models that use invertible transformations [6], with generative adversarial modeling.", "startOffset": 197, "endOffset": 200}], "year": 2016, "abstractText": "Generative adversarial networks (GANs) are a recently proposed class of generative models in which a generator is trained to optimize a cost function that is being simultaneously learned by a discriminator. While the idea of learning cost functions is relatively new to the field of generative modeling, learning costs has long been studied in control and reinforcement learning (RL) domains, typically for imitation learning from demonstrations. In these fields, learning the cost function underlying observed behavior is known as inverse reinforcement learning (IRL) or inverse optimal control. While at first the connection between cost learning in RL and cost learning in generative modeling may appear to be a superficial one, we show in this paper that certain IRL methods are in fact mathematically equivalent to GANs. In particular, we demonstrate an equivalence between a sample-based algorithm for maximum entropy IRL and a GAN in which the generator\u2019s density can be evaluated and is provided as an additional input to the discriminator. Interestingly, maximum entropy IRL is a special case of an energy-based model. We discuss the interpretation of GANs as an algorithm for training energy-based models, and relate this interpretation to other recent work that seeks to connect GANs and EBMs. By formally highlighting the connection between GANs, IRL, and EBMs, we hope that researchers in all three communities can better identify and apply transferable ideas from one domain to another, particularly for developing more stable and scalable algorithms: a major challenge in all three domains.", "creator": "LaTeX with hyperref package"}}}