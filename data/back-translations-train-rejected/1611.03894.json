{"id": "1611.03894", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Nov-2016", "title": "Unsupervised Learning For Effective User Engagement on Social Media", "abstract": "In this paper, we investigate the effectiveness of unsupervised feature learning techniques in predicting user engagement on social media. Specifically, we compare two methods to predict the number of feedbacks (i.e., comments) that a blog post is likely to receive. We compare Principal Component Analysis (PCA) and sparse Autoencoder to a baseline method where the data are only centered and scaled, on each of two models: Linear Regression and Regression Tree. We find that unsupervised learning techniques significantly improve the prediction accuracy on both models. For the Linear Regression model, sparse Autoencoder achieves the best result, with an improvement in the root mean squared error (RMSE) on the test set of 42% over the baseline method. For the Regression Tree model, PCA achieves the best result, with an improvement in RMSE of 15% over the baseline.", "histories": [["v1", "Fri, 11 Nov 2016 22:01:41 GMT  (273kb,D)", "http://arxiv.org/abs/1611.03894v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["thai pham", "camelia simoiu"], "accepted": false, "id": "1611.03894"}, "pdf": {"name": "1611.03894.pdf", "metadata": {"source": "CRF", "title": "Unsupervised Learning For Effective User Engagement on Social Media", "authors": ["Thai T. Pham", "Camelia Simoiu"], "emails": ["thaipham@stanford.edu", "csimoiu@stanford.edu"], "sections": [{"heading": "1 Introduction", "text": "Companies are interested in connecting with potential buyers on platforms such as Facebook or their corporate news sites; bloggers are interested in expanding their following and readership by writing compelling articles that elicit a high level of user feedback (in the form of likes, comments, etc.) Such groups will inevitably need to understand the types of content that are likely to generate the most user interest, as well as any underlying patterns in the data such as time trends. Specifically, we focus on predicting the number of feedback (i.e., comments) that a blog post will receive. Given a number of blog documents that have appeared in the past, the task is to predict how much feedback recently published blog posts will receive in the next few hours. One of the first challenges in answering this question is how to effectively pre-process the data."}, {"heading": "2 Literature Review", "text": "Cao et. al. [2] compare PCA, Kernel PCA (KPCA) and Independent Component Analysis (ICA) as applied to the Support Vector Machine (SVM) to extract features to three sets (Sunspot data, Satan Fe data set A, and five real futures contracts).SVM by feature extraction with PCA, KPCA, or ICA can achieve better generalization performance than those without feature extraction, and that KPCA and ICA perform better than PCA in all the data sets studied, with the best performance in KPCA. This is due to the fact that KPCA and ICA can form higher job information of the original inputs than PCA. Based on these results, we expect the (sparse) autoencoder to perform better than PCA, as it is also a non-linear combination of features in KPCA."}, {"heading": "3 Data Set", "text": "This dataset is provided by the UCI Machine Learning Repository and includes a total of 37, 279 blog pages searched. The prediction task is to predict the number of comments for a blog post in the next H = 24 hours. The processed data has a total of 280 features (excluding the target variable, i.e. the number of feedbacks).1 The blog documents (instances) are converted into vectors entered into the machine learning algorithm in the previous 24 hours. This collection corresponds to approximately 6 GB of pure HTML document (i.e., without images).The following features are extracted from each document: \u2022 Basic features: Number of links and feedbacks in the previous 24 hours; Number of links and feedbacks in the previous 48 hours; how the number of links and feedbacks has increased / decreased since the blog's publication date; Number of links and feedbacks in the first 24 hours after publication of the document \u2022 Textual Weekly features: The most discriminating features of the document are: The most characteristics of the word."}, {"heading": "4 Unsupervised Feature Learning Methods", "text": "Before implementing any uncontrolled learning method, we first eliminate four variables that contain only zeros, resulting in 276 predictor variables (i.e. characteristics) and a result vari-1We use the data processed in [1].able. Of these variables, 58 are continuous, and the rest are binary. We center and scale all non-binary variables to have a mean and standard deviation of zero. This step prevents a variable from dominating the variance of the data set solely by recording values in a larger range. We also center and scale the result variable for later tests. Histograms of the number of feedback received for both train and test sets are included in Figures 1 (a) and 1 (b)."}, {"heading": "4.1 Principal Component Analysis", "text": "PCA is a powerful technique for extracting structure from high-dimensional datasets. We implement PCA to capture the intrinsic variability in the data and reduce the dimensionality of the dataset. An important decision in PCA analysis is to determine the optimal number of major components to be used, since the data is projected onto this new foundation of major components before any subsequent analysis. To determine the optimal number of components, we perform two gap-style tests as in [3]. In both tests, the gap formula has the same shape Gap (k) = E [log (Wk)] \u2212 log (Wk), where log (Wk) is the logarithm of any objective function corresponding to the first k major components and E [log (Wk)] is calculated in a similar way, with the expectation of taking uniform samples from the smallest subspace with the original data."}, {"heading": "4.1.1 Gap Test Using Reconstruction Error", "text": "In the first approach, we define Wk as the reconstruction error when we use the first k major components to approximate the data. Specifically, Wk is defined by Wk = [xkij], j = 1,..., 276 as the projection of the ith data point on the k major component approximation. Then, Wk is defined by Wk = \u2211 i, j (xij \u2212 x-kij) 2The steps for this gap-like test are as follow. ii. After calculating the 276 major components for each k major component, we determine Wk as above and take their log values to obtain 276 values, log (Wk) \"s.ii. We uniformly generate B samples from the smallest subspace containing the original data, and find the main components for this newly generated matrix. As above, for each k component {1,..., 276}, we calculate Wk and take its log values."}, {"heading": "4.1.2 Gap Test Using Explained Variation", "text": "In this approach, we define Wk as the variation explained by the first k major components. Specifically, Wk is the ratio of the sum of the variances of the first k major components and the sum of the total variance of all major components. Mathematically, Wk = \u2211 k = 1 Vi \u2211 276 j = 1 Vj, where Vi is the variance of the ith main component. The steps in this gap test are the same as in the reconstruction error approach with the exception of Wk, which we have described above. We then choose the optimum k as the smallest value for k such as this Gap [k] \u2264 Gap [k + 1] \u2212 se [k + 1].We note the difference in the direction of the two conditions when defining the optimal k between the two gap tests. This is expected because for the gap test we want to minimize Wk using the reconstruction error, while for the gap test using the declared variation, we want to maximize the corresponding main gap component."}, {"heading": "4.2 Sparse Autoencoder", "text": "In this thesis, we implement a 3-layer autoencoder (Figure 4) to learn a compressed representation (coding) of the characteristics. Each \"neuron\" (circle) represents a computational unit called input x1, x2,... xn (and a \"+ 1\" intercept term designated as a bias unit) and outputshW (x) T f (1) T f (1) Tx + b (2), where f: R \u2192 R is called the transfer function (\u00b7) to be the hyperbolic tangent function (tanh function). The tanh function has been chosen instead of the sigmoid function since its output, [-1,1], coming closer to the range of our predictor than the sigrid function."}, {"heading": "5 Results and Discussion", "text": "We use two models to predict the number of feedbacks: linear regression (a linear model) and regression tree (a non-linear model). We compare the projected data with the results from PCA and sparse autoencoders for each model. We use the centered and scaled data as input for the models. For PCA, we use the projected data on the K components, and for sparse autoencoders, we use the optimal number of 5 units in the hidden layer. Results are summarized in the table. For linear regression, we achieve an 11% improvement over baseline components, while the sparse autoencoders achieve a 42% improvement."}, {"heading": "6 Conclusions and Future Work", "text": "We show empirically that the use of uncontrolled feature learning to pre-process the data can greatly improve the accuracy of feedback predictions, and these results should be of interest to companies and publishers in pre-screening or editing their social media posts prior to publication to gauge the level of engagement they expect. For example, an automatic editor can highlight posts with low predicted user engagement and alert the author to the potential need for revisions. Second, we can expand the work by trying other uncontrolled feature learning methods such as ICA and Kernel PCA to better understand how these methods (linear versus non-linear) interact with the model type and to what extent our observations can be generalized. Second, we would be interested in investigating whether the results can be further improved by using various transfer functions and additional hidden layers in the sparse auto code (i.e., stacked autocode aspects to better capture the sparse autocode aspects)."}], "references": [{"title": "Feedback prediction for blogs.\u201d Data Analysis, Machine Learning and Knowledge Discovery", "author": ["Buza", "Krisztian"], "venue": "Springer International Publishing,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2014}, {"title": "A comparison of PCA, KPCA and ICA for dimensionality reduction in support vector machine.", "author": ["Cao", "K. Chua", "W. Chong", "H. Lee", "Q. Gu"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2003}, {"title": "Estimating the number of clusters in a data set via the Gap statistic.", "author": ["Robert Tibshirani", "Guenther Walther", "Trevor Hastie"], "venue": "Journal of the Royal Statistical Society, B,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2001}, {"title": "Emergence of simple-cell receptive field properties by learning a sparse code for natural images.", "author": ["Olshausen", "Bruno A"], "venue": "Nature", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1996}], "referenceMentions": [{"referenceID": 1, "context": "[2] compare PCA, Kernel PCA (KPCA), and Independent Component Analysis (ICA) as applied to Support Vector Machine (SVM) for feature extraction to three data sets (sunspot data, Satan Fe data set A, and five real futures contracts).", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "Buza [1] uses the same data set as we do and compares a variety of models to predict the number of future feedbacks for a blog.", "startOffset": 5, "endOffset": 8}, {"referenceID": 0, "context": "Buza [1] examines various models: a multilayer perceptron model, RBF-networks, regression trees (REP-tree, M5Ptree), nearest neighbor models, multivariate linear regression and bagging, however do not preprocess the data.", "startOffset": 5, "endOffset": 8}, {"referenceID": 0, "context": "We use the same data set as in Buza [1], but approach the problem differently.", "startOffset": 36, "endOffset": 39}, {"referenceID": 0, "context": ", features) and one outcome variWe use the processed data provided in [1].", "startOffset": 70, "endOffset": 73}, {"referenceID": 2, "context": "To determine the optimal number of components, we implement two Gap-style tests as in [3].", "startOffset": 86, "endOffset": 89}, {"referenceID": 3, "context": "[4].", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "The tanh function was chosen instead of the sigmoid function since its output range, [-1,1], more closely approximates the range of our predictor variable than the sigmoid function (range is [0,1]).", "startOffset": 85, "endOffset": 91}, {"referenceID": 0, "context": "The tanh function was chosen instead of the sigmoid function since its output range, [-1,1], more closely approximates the range of our predictor variable than the sigmoid function (range is [0,1]).", "startOffset": 191, "endOffset": 196}, {"referenceID": 1, "context": "In order to determine the optimal values of the hyper-parameters and the number of hidden units, we split the data into training, validation and test sets and perform a grid search over the parameter space of the number of units ([2, 5, 10, 15]) and the weight decay \u03bb ([0.", "startOffset": 230, "endOffset": 244}], "year": 2016, "abstractText": "In this paper, we investigate the effectiveness of unsupervised feature learning techniques in predicting user engagement on social media. Specifically, we compare two methods to predict the number of feedbacks (i.e., comments) that a blog post is likely to receive. We compare Principal Component Analysis (PCA) and sparse Autoencoder to a baseline method where the data are only centered and scaled, on each of two models: Linear Regression and Regression Tree. We find that unsupervised learning techniques significantly improve the prediction accuracy on both models. For the Linear Regression model, sparse Autoencoder achieves the best result, with an improvement in the root mean squared error (RMSE) on the test set of 42% over the baseline method. For the Regression Tree model, PCA achieves the best result, with an improvement in RMSE of 15% over the baseline.", "creator": "LaTeX with hyperref package"}}}