{"id": "1611.03954", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Nov-2016", "title": "Multilingual Knowledge Graph Embeddings for Cross-lingual Knowledge Alignment", "abstract": "Many recent works have demonstrated the benefits of knowledge graph embeddings in completing monolingual knowledge graphs. Inasmuch as related knowledge bases are built in several different languages, achieving cross-lingual knowledge alignment will help people in constructing a coherent knowledge base, and assist machines in dealing with different expressions of entity relationships across diverse human languages. Unfortunately, achieving this highly desirable crosslingual alignment by human labor is very costly and errorprone. Thus, we propose MTransE, a translation-based model for multilingual knowledge graph embeddings, to provide a simple and automated solution. By encoding entities and relations of each language in a separated embedding space, MTransE provides transitions for each embedding vector to its cross-lingual counterparts in other spaces, while preserving the functionalities of monolingual embeddings. We deploy three different techniques to represent cross-lingual transitions, namely axis calibration, translation vectors, and linear transformations, and derive five variants for MTransE using different loss functions. Our models can be trained on partially aligned graphs, where just a small portion of triples are aligned with their cross-lingual counterparts. The experiments on cross-lingual entity matching and triple-wise alignment verification show promising results, with some variants consistently outperforming others on different tasks. We also explore how MTransE preserves the key properties of its monolingual counterpart TransE.", "histories": [["v1", "Sat, 12 Nov 2016 04:28:04 GMT  (719kb,D)", "http://arxiv.org/abs/1611.03954v1", null], ["v2", "Tue, 21 Feb 2017 01:20:30 GMT  (506kb,D)", "http://arxiv.org/abs/1611.03954v2", null], ["v3", "Wed, 17 May 2017 19:33:53 GMT  (506kb,D)", "http://arxiv.org/abs/1611.03954v3", "Extended version of the IJCAI-17 paper"]], "reviews": [], "SUBJECTS": "cs.AI cs.CL", "authors": ["muhao chen", "yingtao tian", "mohan yang", "carlo zaniolo"], "accepted": false, "id": "1611.03954"}, "pdf": {"name": "1611.03954.pdf", "metadata": {"source": "CRF", "title": "Multi-lingual Knowledge Graph Embeddings for Cross-lingual Knowledge Alignment", "authors": ["Muhao Chen", "Yingtao Tian", "Mohan Yang", "Carlo Zaniolo"], "emails": ["zaniolo}@cs.ucla.edu;", "yittian@cs.stonybrook.edu"], "sections": [{"heading": "Introduction", "text": "In fact, it is as if most of them are able to survive themselves, and that they are able to survive themselves by going in search of their own identity. (...) It is not as if they are able to identify themselves. (...) It is not as if they are able to survive themselves. (...) It is as if they are able to survive themselves. (...) It is as if they are able to survive themselves. (...) It is as if they are able to survive themselves. (...) It is as if they are able to survive themselves. (...) It is as if they are able to survive themselves. (...) It is as if they are able. \"(...) It is as if they are able to survive themselves. (...) It is as if they are able to survive themselves. (...) It is as if they are able to survive themselves."}, {"heading": "Related Work", "text": "It is. It is. It is. It is. It is. It is. It is. It is. It is. It is. It is. It is. It is. It is. It is. It is. It is. It is. Most people who stand up for the rights of women and men are. It is. It is. It is. It is. It is. It is. It is. It is. It is. It is. It is. It is. It is. It is. It is. It is. It is. It is. It is. It is. It is. It is. It is. It is. It is. It is. It is. It is. It is. (...) It is. (...) It is. (...) It is. It is. (...) It is. (...) It is. It is. (...) It is. It is. (...) It is. It is. It is. It is. (...) It is. It is. It is. (... It is. It is. It is. It is. (...) It is. It is. It is. It is. It is. It is. (... it is. It is. It is. It is. It is. It is. It is. It is. It is. (... it. It is. It is. It is."}, {"heading": "Multilingual Knowledge Graph Embeddings", "text": "With this we begin our modelling with the formalization of multilingual knowledge graphics."}, {"heading": "Multilingual Knowledge Graphs", "text": "In Knowledge Base KB, we use L to denote the set of languages, and L2 to denote the 2-combination pair of L (i.e. the set of disordered language pairs). In a language L, L denotes the language-specific knowledge graph of L, and EL and RL denote the corresponding vocabulary of entity expression and kinship expression. T = (h, r, t) denotes a triple principle in GL, so that h, t, EL and r, RL. Boldfaced h, r, t each denotes the embedding of vectors of head h, relationship r, and tail. In a language pair (L1, L2) - L2, \u03b4 (L1, L2) denotes the alignment group containing the pairs of triples already aligned between L1 and L2."}, {"heading": "Knowledge Model", "text": "For each language, a special k-dimensional embedding space RkL is assigned for vectors of EL and RL, where R is the field of real numbers. We adopt the basic translation-based method of TransE for each language involved. Therefore, its loss function is specified as follows: SK = \u2211 L \u2211 {Li, Lj} \u2211 (h, r, t) \u0445 GL \u0394h + r \u2212 t \u00b2 It measures the plausibility of all given triples. By minimizing the loss function, the knowledge model preserves monolingual relationships between units, while at the same time acting as a regulator for the alignment model. Due to the discrepancy of embedding spaces for each language, the knowledge model divides the knowledge base into fragmented subgroups, which can be trained in parallel."}, {"heading": "Alignment Model", "text": "In fact, it is such that it is a matter of a way in which people find themselves in the most diverse spheres of life. (...) In fact, it is such that people in the most diverse spheres of life in the world, who find themselves in the most diverse spheres of life, find themselves in the most diverse spheres of life. (...) In fact, it is such that people live in the most diverse spheres of life in the world in which they live, who find themselves in the most diverse spheres of life. (...) It is such that people live in the most diverse spheres of life in which they live, in the most diverse spheres of life in which they live, in whom they live, in whom they live in the most diverse spheres of life in which they live, in whom they live, in whom they live in whom they live, in whom they live in the most diverse spheres of life in which they live, in whom they live in whom they live, in whom they live in whom they live, in whom they live in whom they live in the most diverse spheres of life in which they live, in which they live in which they live in whom they live in whom they live, in whom they live in whom they live in the most diverse spheres of life in which they live, in which they live in which they live in which they live in which they live, in which they live in which they live in which they live in which they live in which they live, in which they live in which they live in which they live, in which they live in which they live in which they live in which they live, in which they live in which they live in which they live in which they live, in which they live in which they live in which they live, in which they live in which they live in which they live, in which they live in which they live in which they live."}, {"heading": "Variants of MTransE", "text": "By combining the above two component models, MTransE minimizes the following loss functionJ = SK + \u03b1SA, where \u03b1 is a hyperparameter that weights SK and SA. Since we have presented five variants of the alignment model, each of them defines its specific way of calculating lingual transitions of embedding vectors accordingly. We call Vark the variant of MTransE that uses the k-th alignment model that Sak uses. In practice, the search for a lingual counterpart to a source is always done by querying the closest neighbor from the result point of the lingual transition. We refer to the \u03c4ij function, which maps a lingual transition of a vector from Li to Lj, or simply the search in a bilingual context. As explained, in a multilingual scenario, the solution consists of a series of models of the same variant defined on each language pair in L2. Table 1, the model summarizes the complexity, the transitions, and the complexity of each language."}, {"heading": "Training", "text": "Instead of directly updating J, we optimize SK and \u03b1SA alternately. In detail, we optimize the embedding of vectors in a spherical surface at each epoch. This limitation is applied in the literature (Bordes et al. 2013; Bordes et al. 2013; Weston et al. 2014; Jenatton et al. 2012) and has two important effects: (i) it helps regulate the embedding of vectors in a spherical surface. (Bordes et al. 2013; Bordes et al. 2014; Jenatton et al. 2012) and avoids the case where the training process trivially minimizes the loss function by shrinking the standard of embedding of vectors. (Jenatton et al. 2012)"}, {"heading": "Experiments", "text": "In this section, we evaluate the proposed methods based on two lingual tasks: the lingual entity matching and the triple alignment check. We also conduct experiments on two monolingual tasks. In addition, a case study with examples of knowledge alignment is included in the appendix."}, {"heading": "Data Sets", "text": "The trilingual data used in our experiments comes from DBpedia, a knowledge base that contains trilingual knowledge diagrams extracted from multilingual versions of Wikipedia Infoboxes. We started with the English (En), French (Fr) and German (De) versions of DBPedia and created the monolingual knowledge diagram for each language using a subset of trilingual units under dbo: person domains. By reviewing ILLs on entities and multilingual terms from the DBpedia ontology of some participating relationships, we obtained an English-French and English-German alignment for some triples. Finally, we adjusted the number of participating entities in each language to obtain two sets of data presented in Table 2. WKEFG15k corresponds to the number of nodes (about 15,000) for each of the three languages with FB15k - the largest monolingual chart of 2015, which is used in many recent Zong works."}, {"heading": "Cross-lingual Entity Matching", "text": "The goal of this task is to match the same entities from different languages in KB. Due to the large candidate space, this task is more about ranking the candidates than acquiring the best answer. We perform this task on both sets of data to compare five variants of MTransE. Evaluation protocol. Each model is trained on a complete set of data, while the additional ILLs are used as down-to-earth truth for testing. We record these unidirectional links between English and English-German, i.e., four directions in total. For each ILL (e, e \u2032), we perform a kNN search from the cross-border transition point of e (i.e., we record these unidirectional links between English and English) and record the rank of e. Following the convention, we aggregate two metrics across all test cases, i.e., the ratio of the ranking no greater than 10 hits @ 10 (in percent) and the middle ranking Mean."}, {"heading": "Triple-wise Alignment Verification", "text": "This task is to verify whether a particular pair of aligned triples is really used across borders. It produces a classifier that helps verify triple matching candidates (Nguyen et al. 2011; Rinser et al. 2013). We create positive cases by isolating 20% of the alignment set. Similar to (Socher et al. 2013), we have random positive cases to generate negative cases. Specifically, a pair of correctly aligned triples (T \u2032) is corrupted by (i) random substitution of one of the six elements in the two triples with another element from the same language, or (ii) random substitution of either T or T \"with another triple from the same language. Cases (i) and (ii) each contribute negative cases that are as many as 100% and 50% of the positive cases."}, {"heading": "Monolingual Tasks", "text": "The above experiments have shown the strong capability of MTransE in dealing with country-specific tasks. Now, we report on the results of comparing MTransE with its monolingual counterpart TransE on two monolingual tasks introduced in literature (Bordes et al. 2013; Bordes et al. 2014), namely tail prediction (prediction of t given h and r) and relation prediction (prediction of r given h and t), using the English and French versions of our datasets. Similarly, like previous work (Bordes et al. 2013; Wang et al. 2014; Jia et al. 2016), for each language version 10% triples are selected as a test set, and the remaining one becomes a training set. Each MTransE variant is trained on both language versions of the training set for the knowledge model, while the intersection between the alignment group and the training group is well used for alignment modeling. Transformation models are used as even alignment models."}, {"heading": "Conclusion and Future Work", "text": "Our model MTransE characterizes monolingual relationships and compares three different techniques for learning the translingual alignment of units and relationships. Extensive experiments on the tasks of translingual entity matching and triple alignment verification show that the technique based on linear transformations is the best of the three. Furthermore, we show that MTransE can replace the key characteristics of the monolingual embedding of knowledge graphs in monolingual tasks. The results are very encouraging, but we also show possibilities for further work and improvement. In particular, we should investigate how the simple loss function of the knowledge model used in MTransE can be replaced by advanced ones that include relationship-specific entity transformations. More complex tasks of translingual triple completion can also be performed."}, {"heading": "Appendix", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Examples of Knowledge Alignment", "text": "We have already demonstrated the effectiveness of MTransE in aligning cross-border knowledge, in particular the linear transformation-based variants Var4 and Var5. Now we will discuss several examples to gain insights into how our methods can be used in cross-border knowledge. We will begin by searching for cross-border counterparts of units and relationships. We will select a unit (or relationship) in English and then show the next candidates in French and German. These candidates will be listed by decreasing values of Euclidean distance between their vectors in the target language area and the result point of the cross-border transition. Several examples will be shown in Table 8 and Table 9. In all tables of this subsection, we will highlight the exact answers as bold, and those conceptually close as cursive. In Table 8, we will look at the exact answers to the exact correct answers to the questions of Barack Obama and Paris, in addition to courageous."}], "references": [{"title": "Linking and extending an open multilingual Wordnet", "author": ["Bond", "F. Foster 2013] Bond", "R. Foster"], "venue": null, "citeRegEx": "Bond et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bond et al\\.", "year": 2013}, {"title": "Learning structured embeddings of knowledge bases", "author": ["Bordes"], "venue": null, "citeRegEx": "Bordes,? \\Q2011\\E", "shortCiteRegEx": "Bordes", "year": 2011}, {"title": "Joint learning of words and meaning representations for open-text semantic parsing", "author": ["Bordes"], "venue": null, "citeRegEx": "Bordes,? \\Q2012\\E", "shortCiteRegEx": "Bordes", "year": 2012}, {"title": "Translating embeddings for modeling multi-relational data", "author": ["Bordes"], "venue": "In NIPS,", "citeRegEx": "Bordes,? \\Q2013\\E", "shortCiteRegEx": "Bordes", "year": 2013}, {"title": "A semantic matching energy function for learning with multi-relational data. Machine Learning 94(2):233\u2013259", "author": ["Bordes"], "venue": null, "citeRegEx": "Bordes,? \\Q2014\\E", "shortCiteRegEx": "Bordes", "year": 2014}, {"title": "Open question answering with weakly supervised embedding models", "author": ["Weston Bordes", "A. Usunier 2014] Bordes", "J. Weston", "N. Usunier"], "venue": "ECML-PKDD,", "citeRegEx": "Bordes et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bordes et al\\.", "year": 2014}, {"title": "A unified architecture for natural language processing: Deep neural networks with multitask learning", "author": ["Collobert", "R. Weston 2008] Collobert", "J. Weston"], "venue": null, "citeRegEx": "Collobert et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2008}, {"title": "Dependency tree kernels for relation extraction", "author": ["Culotta", "A. Sorensen 2004] Culotta", "J. Sorensen"], "venue": null, "citeRegEx": "Culotta et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Culotta et al\\.", "year": 2004}, {"title": "Menta: Inducing multilingual taxonomies from Wikipedia", "author": ["de Melo", "G. Weikum 2010] de Melo", "G. Weikum"], "venue": null, "citeRegEx": "Melo et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Melo et al\\.", "year": 2010}, {"title": "Bilbowa: Fast bilingual distributed representations without word alignments", "author": ["Bengio Gouws", "S. Corrado 2015] Gouws", "Y. Bengio", "G. Corrado"], "venue": "In ICML,", "citeRegEx": "Gouws et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gouws et al\\.", "year": 2015}, {"title": "A latent factor model for highly multi-relational data", "author": ["Jenatton"], "venue": null, "citeRegEx": "Jenatton,? \\Q2012\\E", "shortCiteRegEx": "Jenatton", "year": 2012}, {"title": "Knowledge graph embedding via dynamic mapping matrix", "author": ["Ji"], "venue": null, "citeRegEx": "Ji,? \\Q2015\\E", "shortCiteRegEx": "Ji", "year": 2015}, {"title": "Locally adaptive translation for knowledge graph embedding", "author": ["Jia"], "venue": null, "citeRegEx": "Jia,? \\Q2016\\E", "shortCiteRegEx": "Jia", "year": 2016}, {"title": "A systematic exploration of the feature space for relation extraction", "author": ["Jiang", "J. Zhai 2007] Jiang", "C. Zhai"], "venue": "In NAACL HLT,", "citeRegEx": "Jiang et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Jiang et al\\.", "year": 2007}, {"title": "An autoencoder approach to learning bilingual word representations", "author": ["Lauly"], "venue": "In NIPS,", "citeRegEx": "Lauly,? \\Q2014\\E", "shortCiteRegEx": "Lauly", "year": 2014}, {"title": "Learning entity and relation embeddings for knowledge graph completion", "author": ["Lin"], "venue": null, "citeRegEx": "Lin,? \\Q2015\\E", "shortCiteRegEx": "Lin", "year": 2015}, {"title": "Yago3: A knowledge base from multilingual Wikipedias", "author": ["Biega Mahdisoltani", "F. Suchanek 2015] Mahdisoltani", "J. Biega", "F. Suchanek"], "venue": "In CIDR", "citeRegEx": "Mahdisoltani et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mahdisoltani et al\\.", "year": 2015}, {"title": "Text-mining, structured queries, and knowledge management on web document corpora", "author": ["Mousavi"], "venue": null, "citeRegEx": "Mousavi,? \\Q2014\\E", "shortCiteRegEx": "Mousavi", "year": 2014}, {"title": "Multilingual schema matching for Wikipedia infoboxes", "author": ["Nguyen"], "venue": null, "citeRegEx": "Nguyen,? \\Q2011\\E", "shortCiteRegEx": "Nguyen", "year": 2011}, {"title": "Stranse: a novel embedding model of entities and relationships in knowledge bases", "author": ["Nguyen"], "venue": "In NAACL HLT,", "citeRegEx": "Nguyen,? \\Q2016\\E", "shortCiteRegEx": "Nguyen", "year": 2016}, {"title": "Cross-lingual entity matching and infobox alignment in Wikipedia. Information Systems 38(6):887\u2013907", "author": ["Rinser"], "venue": null, "citeRegEx": "Rinser,? \\Q2013\\E", "shortCiteRegEx": "Rinser", "year": 2013}, {"title": "Exact solutions to the nonlinear dynamics of learning in deep linear neural networks. ICLR", "author": ["McClelland Saxe", "A.M. Ganguli 2014] Saxe", "J.L. McClelland", "S. Ganguli"], "venue": null, "citeRegEx": "Saxe et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Saxe et al\\.", "year": 2014}, {"title": "Reasoning with neural tensor networks for knowledge base completion", "author": ["Socher"], "venue": null, "citeRegEx": "Socher,? \\Q2013\\E", "shortCiteRegEx": "Socher", "year": 2013}, {"title": "Probabilistic alignment of relations, instances, and schema", "author": ["Suchanek"], "venue": null, "citeRegEx": "Suchanek,? \\Q2011\\E", "shortCiteRegEx": "Suchanek", "year": 2011}, {"title": "Semi-supervised relation extraction with large-scale word clustering", "author": ["Grishman Sun", "A. Sekine 2011] Sun", "R. Grishman", "S. Sekine"], "venue": null, "citeRegEx": "Sun et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Sun et al\\.", "year": 2011}, {"title": "Wikidata: a free collaborative knowledge base", "author": ["Vrande\u010di\u0107", "D. Kr\u00f6tzsch 2014] Vrande\u010di\u0107", "M. Kr\u00f6tzsch"], "venue": null, "citeRegEx": "Vrande\u010di\u0107 et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Vrande\u010di\u0107 et al\\.", "year": 2014}, {"title": "Knowledge graph embedding by translating on hyperplanes", "author": ["Wang"], "venue": null, "citeRegEx": "Wang,? \\Q2014\\E", "shortCiteRegEx": "Wang", "year": 2014}, {"title": "Connecting language and knowledge bases with embedding models for relation extraction", "author": ["Weston"], "venue": null, "citeRegEx": "Weston,? \\Q2013\\E", "shortCiteRegEx": "Weston", "year": 2013}, {"title": "The general inefficiency of batch training for gradient descent learning", "author": ["Wilson", "D.R. Martinez 2003] Wilson", "T.R. Martinez"], "venue": "Neural Networks", "citeRegEx": "Wilson et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Wilson et al\\.", "year": 2003}, {"title": "Normalized word embedding and orthogonal trans", "author": ["Xing"], "venue": null, "citeRegEx": "Xing,? \\Q2015\\E", "shortCiteRegEx": "Xing", "year": 2015}, {"title": "Unsupervised relation extraction by mining wikipedia texts using information from the web", "author": ["Yan"], "venue": null, "citeRegEx": "Yan,? \\Q2009\\E", "shortCiteRegEx": "Yan", "year": 2009}, {"title": "Network representation learning with rich text information", "author": ["Yang"], "venue": null, "citeRegEx": "Yang,? \\Q2015\\E", "shortCiteRegEx": "Yang", "year": 2015}, {"title": "Aligning knowledge and text embeddings by entity descriptions", "author": ["Zhong"], "venue": null, "citeRegEx": "Zhong,? \\Q2015\\E", "shortCiteRegEx": "Zhong", "year": 2015}, {"title": "Exploring various knowledge in relation extraction", "author": ["Zhou"], "venue": null, "citeRegEx": "Zhou,? \\Q2005\\E", "shortCiteRegEx": "Zhou", "year": 2005}, {"title": "Bilingual word embeddings for phrase-based machine translation", "author": ["Zou"], "venue": null, "citeRegEx": "Zou,? \\Q2013\\E", "shortCiteRegEx": "Zou", "year": 2013}], "referenceMentions": [{"referenceID": 5, "context": "Now we report the results on comparing MTransE with its monolingual counterpart TransE on two monolingual tasks introduced in the literature (Bordes et al. 2013; Bordes et al. 2014), namely tail prediction (predicting t given h and r) and relation prediction (predicting r given h and t), using the English and French versions of our data sets.", "startOffset": 141, "endOffset": 181}], "year": 2017, "abstractText": "Many recent works have demonstrated the benefits of knowledge graph embeddings in completing monolingual knowledge graphs. Inasmuch as related knowledge bases are built in several different languages, achieving cross-lingual knowledge alignment will help people in constructing a coherent knowledge base, and assist machines in dealing with different expressions of entity relationships across diverse human languages. Unfortunately, achieving this highly desirable cross-lingual alignment by human labor is very costly and error-prone. Thus, we propose MTransE, a translationbased model for multilingual knowledge graph embeddings, to provide a simple and automated solution. By encoding entities and relations of each language in a separated embedding space, MTransE provides transitions for each embedding vector to its cross-lingual counterparts in other spaces, while preserving the functionalities of monolingual embeddings. We deploy three different techniques to represent cross-lingual transitions, namely axis calibration, translation vectors, and linear transformations, and derive five variants for MTransE using different loss functions. Our models can be trained on partially aligned graphs, where just a small portion of triples are aligned with their cross-lingual counterparts. The experiments on cross-lingual entity matching and triple-wise alignment verification show promising results, with some variants consistently outperforming others on different tasks. We also explore how MTransE preserves the key properties of its monolingual counterpart TransE.", "creator": "LaTeX with hyperref package"}}}