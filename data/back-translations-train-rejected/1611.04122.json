{"id": "1611.04122", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Nov-2016", "title": "Cross-lingual Dataless Classification for Languages with Small Wikipedia Presence", "abstract": "This paper presents an approach to classify documents in any language into an English topical label space, without any text categorization training data. The approach, Cross-Lingual Dataless Document Classification (CLDDC) relies on mapping the English labels or short category description into a Wikipedia-based semantic representation, and on the use of the target language Wikipedia. Consequently, performance could suffer when Wikipedia in the target language is small. In this paper, we focus on languages with small Wikipedias, (Small-Wikipedia languages, SWLs). We use a word-level dictionary to convert documents in a SWL to a large-Wikipedia language (LWLs), and then perform CLDDC based on the LWL's Wikipedia. This approach can be applied to thousands of languages, which can be contrasted with machine translation, which is a supervision heavy approach and can be done for about 100 languages. We also develop a ranking algorithm that makes use of language similarity metrics to automatically select a good LWL, and show that this significantly improves classification of SWLs' documents, performing comparably to the best bridge possible.", "histories": [["v1", "Sun, 13 Nov 2016 12:20:33 GMT  (66kb)", "http://arxiv.org/abs/1611.04122v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["yangqiu song", "stephen mayhew", "dan roth"], "accepted": false, "id": "1611.04122"}, "pdf": {"name": "1611.04122.pdf", "metadata": {"source": "CRF", "title": "Cross-lingual Dataless Classification for Languages with Small Wikipedia Presence", "authors": ["Yangqiu Song", "Stephen Mayhew"], "emails": ["yqsong@cse.ust.hk", "mayhew2@illinois.edu", "danr@illinois.edu"], "sections": [{"heading": null, "text": "ar Xiv: 161 1.04 122v 1 [cs.C L] 1"}, {"heading": "1. Introduction", "text": "In fact, most of them are able to embark on the search for new paths to travel the world."}, {"heading": "2. Multilingual 20-newsgroups Data", "text": "Since the existing benchmark datasets for multilingual document classification (Lewis, Yang, Rose, & Li, 2004; Hermann & Blunsom, 2014) focus on a small group of languages to test the CLDDC in many languages, we use the data developed by (Song et al., 2016).6 They selected 100 documents from 20 newsgroups (Lang, 1995) that can be 100% correctly classified using the English dataless classification (Song & Roth, 2014), and used Google Translate API7 to translate these documents into 87 languages.8 We use the English label descriptions for the 20 newsgroups as in (Song & Roth, 2014).So we fix the English label room with 20 label descriptions. Notice that the target documents in a foreign language are L from the 87 languages.We use the intersection of Wikipedia title pages that are linked between English and L to perform CLESA for CLDDC."}, {"heading": "3. Bridged Cross-lingual Dataless Classification", "text": "In this section we present the general idea of bridging SWL with LWLs and show some comparative results with other uncontrolled learning or cross-language classification approaches that use two typical languages."}, {"heading": "3.1 Bridging SWLs with LWLs", "text": "We will first select two typical SWLs, i.e. Hausa and Uzbek, as examples to show how to use bridge languages to improve the Dataless classification. Hausa is a language that can be seen under the Afro-Asian family and further on under Chad. Uzbek is a language under the Middle Turkish family. Both of the writing systems refer to Arabic and Latin. The low number of Wikipedia pages and thus the low number of shared semantic spaces used for these two languages (62 for Hausa and 3.082 for Uzbek after overlapping with Wikipedia) means that CLDDC will not be accurate. Results are summarized in Table 2. In fact, the classification results are not satisfactory (0.08 for Hausa and 0.40 for Uzbek). The basic idea of the bridged CLDDC is that we can convert some word levels of SWLs into another language, we can use the other language to build ECLECSA and further data fixes."}, {"heading": "4. Rank Bridging Languages", "text": "Considering that for both Hausa and Uzbeks Arabic is better than English for translated CLDDC 71, and the fact that there are more local languages out of 7,000 in the U.S. https: / / github.com / pprett / nutworld, which cannot be translated into English, we want to evaluate which language can be the best for the SWLs. In Table 3, we show the top ten bridging languages for the target languages Hausa and Uzbek. All translations of words are provided by Google Translate.10 The results show that Arabic is the best bridge for both languages. For all 39 SWLs, we have also reviewed the bridged results based on all 49 LWLs, and we have selected the best bridging languages, and we report the classification results of the original CLDDC results."}, {"heading": "4.1 Similarity Ranking", "text": "The version we have downloaded has 2,679 languages with 198 characteristics, including phonological, grammatical and lexical characteristics. We removed latitude and longitude characteristics, resulting in 196 characteristics. In the light of the above analysis, we found four characteristics that are very useful compared to the others, namely genera, family, macro range and country code. Of these, we manually developed a similarity value for a language pair as follows: Sl (L1, L2) = 50 \u00b7 Igenus (L1, L2) + 50 \u00b7 Igenus (L1, L2) + 50 \u00b7 Imacro range (L1, L2) + 50 \u00b7 Country code (L1, L2) + Icountry code (L2) + i (other} Ii (L1, L2), where Ii (L1, L2) = 1 means that we compare the two languages with each other (L1, L2)."}, {"heading": "4.2 RankSVM", "text": "11. http: / / wals.info / Suppose we have a language pair Li and Lj. We can construct a character vector xij based on the WALS data, where the rth attribute is: x (r) ij = Ir (Li, Lj), (5) where the indicator function identifies both languages with the same WALS attribute value. If we consider Li as a SWL and there are two candidates LWLs Lj and Lk, we can compare Lj and Lk based on their character vectors by projecting them to a real value: wTxij and wTxik. So, if we have many such pairs, we can build a support vector engine to match the projection vector w: 1 2% w \u00b2 i {SWL}, j {WTxik}, which is better than the function Wk-Lk (Lk)."}, {"heading": "4.3 Ranking Results", "text": "In this year it is so far that it concerns a kind of infinite vastness in which most people are able to move, to move, to leave the world, to change the world, to change the world, to change the world, to change the world, to change the world, to change the world, to change the world, to change the world, to change the world, to change the world, to change the world, to change the world, to change the world, to change the world, to change the world, to change the world, to change the world, to change the world, to change the world, to change the world, to change the world, to change the world, to change the world, to change the world, to the world, to change the world, to the world, to the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world, around the world."}, {"heading": "5. Related Work", "text": "In this section we give a brief overview of some related work."}, {"heading": "5.1 Cross-lingual Classification", "text": "In the cross-border classification of documents, we train a classifier on labeled documents in the source language and classify documents in the target language. Existing approaches either require a parallel corpus to train word embedding for different languages (Hermann & Blunsom, 2014), require labeled documents in source and target languages (Xiao & Guo, 2013), use machine translation techniques to translate words (Prettenhofer & Stein, 2010) or documents (Amini & Goutte, 2010), or combine different approaches (Shi, Mihalcea, & Tian, 2010). Among existing approaches, word translation is the cheapest method, while document translation and annotation on the target domain are the most expensive."}, {"heading": "5.2 Cross-lingual Representation Learning", "text": "Our current approach is based on ESA (Gabrilovich & Markovitch, 2009), which builds on the reverse Wikipedia index. Essentially, ESA is a distributive representation of the documentary context of words, viewing each Wikipedia page as a concept that corresponds to an entity, category or theme, and then representing each word through its related concepts. ESA uses the linguistic links in Wikipeida to refer to pages in other languages in English, and then assigning words or texts in two different languages (English and another) to the same semantic space represented by Wikipedia concepts."}, {"heading": "5.3 Pivot based Machine Translation", "text": "Pivot language is used to support machine translation when there are not enough resources to train a translation model from source to target language (Mann & Yarowsky, 2001; Cohn & Lapata, 2007; Utiyama & Isahara, 2007; Wu & Wang, 2009; Leusch, Max, Crego, & Ney, 2010; Paul et al., 2013). Paul et al., (Paul et al., 2013), for example, used 22 Indo-European and Asian languages to evaluate how to select a good pivot language for machine translation."}, {"heading": "5.4 Zero/One-shot Learning", "text": "Zero-Shot Learning (Palatucci, Pomerleau, Hinton, & Mitchell, 2009; Socher, Ganjoo, Manning, & Ng, 2013; Elhoseiny, Saleh,,, & A. Elgammal, 2013; Romera-Paredes & Torr, 2015) and One-Shot Learning (Li et al., 2006; Lake et al., 2015) were first introduced to the computer vision community and are now recognized by the natural language processing community (Yazdani & Henderson, 2015; Lazaridou, Dinu, & Baroni, 2015)."}, {"heading": "6. Conclusion", "text": "CLDDC uses English labels to classify documents in other languages, and is scalable to many languages and adaptable to any label space. However, if Wikipedia is not large enough for one language, performance is not acceptable. In this essay, we simply map words in SWL documents to fiber optic words and perform a dateless classification based on fiber optic. We systematically evaluate 39 fiber optic SWLs and 49 fiber optic LLs. Experiments show that bridging the fiber optic SWLs with fiber optic LLs can greatly improve classification results, and we can generalize from existing ranking results to other languages."}, {"heading": "Acknowledgments", "text": "This work has been endorsed by DARPA under contract numbers HR0011-15-2-0025 and FA8750-13-2-0008. The U.S. government is authorized to reproduce and distribute copies for government purposes, regardless of the copyright notices contained therein. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official guidelines or recommendations of any of the organizations supporting the work, whether express or implied."}], "references": [{"title": "Polyglot: Distributed word representations for multilingual nlp", "author": ["R. Al-Rfou", "B. Perozzi", "S. Skiena"], "venue": "In CoNLL,", "citeRegEx": "Al.Rfou. et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Al.Rfou. et al\\.", "year": 2013}, {"title": "A co-classification approach to learning from multilingual corpora", "author": ["M. Amini", "C. Goutte"], "venue": "Machine Learning,", "citeRegEx": "Amini and Goutte,? \\Q2010\\E", "shortCiteRegEx": "Amini and Goutte", "year": 2010}, {"title": "Importance of semantic representation: Dataless classification", "author": ["Chang", "M.-W", "L. Ratinov", "D. Roth", "V. Srikumar"], "venue": "In AAAI,", "citeRegEx": "Chang et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Chang et al\\.", "year": 2008}, {"title": "Efficient algorithms for ranking with SVMs", "author": ["O. Chapelle", "S.S. Keerthi"], "venue": "Inf. Retr.,", "citeRegEx": "Chapelle and Keerthi,? \\Q2010\\E", "shortCiteRegEx": "Chapelle and Keerthi", "year": 2010}, {"title": "Machine translation by triangulation: Making effective use of multi-parallel corpora", "author": ["T. Cohn", "M. Lapata"], "venue": null, "citeRegEx": "Cohn and Lapata,? \\Q2007\\E", "shortCiteRegEx": "Cohn and Lapata", "year": 2007}, {"title": "Write a classifier: Zero shot learning using purely textual descriptions", "author": ["M. Elhoseiny", "B. Saleh"], "venue": "A.Elgammal", "citeRegEx": "Elhoseiny and Saleh,? \\Q2013\\E", "shortCiteRegEx": "Elhoseiny and Saleh", "year": 2013}, {"title": "Improving vector space word representations using multilingual correlation", "author": ["M. Faruqui", "C. Dyer"], "venue": "In EACL,", "citeRegEx": "Faruqui and Dyer,? \\Q2014\\E", "shortCiteRegEx": "Faruqui and Dyer", "year": 2014}, {"title": "Wikipedia-based semantic interpretation for natural language processing", "author": ["E. Gabrilovich", "S. Markovitch"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Gabrilovich and Markovitch,? \\Q2009\\E", "shortCiteRegEx": "Gabrilovich and Markovitch", "year": 2009}, {"title": "Large margin rank boundaries for ordinal regression", "author": ["R. Herbrich", "T. Graepel", "K. Obermayer"], "venue": null, "citeRegEx": "Herbrich et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Herbrich et al\\.", "year": 2000}, {"title": "Multilingual models for compositional distributed semantics", "author": ["K.M. Hermann", "P. Blunsom"], "venue": "In ACL,", "citeRegEx": "Hermann and Blunsom,? \\Q2014\\E", "shortCiteRegEx": "Hermann and Blunsom", "year": 2014}, {"title": "Statistical methods for psychology", "author": ["D.C. Howell"], "venue": "Cengage Learning", "citeRegEx": "Howell,? \\Q2011\\E", "shortCiteRegEx": "Howell", "year": 2011}, {"title": "Inducing crosslingual distributed representations of words", "author": ["A. Klementiev", "I. Titov", "B. Bhattarai"], "venue": "In COLING,", "citeRegEx": "Klementiev et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Klementiev et al\\.", "year": 2012}, {"title": "Human-level concept learning through probabilistic program induction", "author": ["B.M. Lake", "R. Salakhutdinov", "J.B. Tenenbaum"], "venue": "Science,", "citeRegEx": "Lake et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lake et al\\.", "year": 2015}, {"title": "Newsweeder: Learning to filter netnews", "author": ["K. Lang"], "venue": "In ICML,", "citeRegEx": "Lang,? \\Q1995\\E", "shortCiteRegEx": "Lang", "year": 1995}, {"title": "Hubness and pollution: Delving into crossspace mapping for zero-shot learning", "author": ["A. Lazaridou", "G. Dinu", "M. Baroni"], "venue": "In ACL,", "citeRegEx": "Lazaridou et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lazaridou et al\\.", "year": 2015}, {"title": "Multi-pivot translation by system combination", "author": ["G. Leusch", "A. Max", "J.M. Crego", "H. Ney"], "venue": "In 2010 International Workshop on Spoken Language Translation,", "citeRegEx": "Leusch et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Leusch et al\\.", "year": 2010}, {"title": "Rcv1: A new benchmark collection for text categorization research", "author": ["D.D. Lewis", "Y. Yang", "T.G. Rose", "F. Li"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "Lewis et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Lewis et al\\.", "year": 2004}, {"title": "One-shot learning of object categories", "author": ["F. Li", "R. Fergus", "P. Perona"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell.,", "citeRegEx": "Li et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Li et al\\.", "year": 2006}, {"title": "Deep multilingual correlation for improved word embeddings", "author": ["A. Lu", "W. Wang", "M. Bansal", "K. Gimpel", "K. Livescu"], "venue": "In NAACL-HLT,", "citeRegEx": "Lu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lu et al\\.", "year": 2015}, {"title": "Multipath translation lexicon induction via bridge languages", "author": ["G.S. Mann", "D. Yarowsky"], "venue": null, "citeRegEx": "Mann and Yarowsky,? \\Q2001\\E", "shortCiteRegEx": "Mann and Yarowsky", "year": 2001}, {"title": "Panlingual lexical translation via probabilistic inference", "author": ["Mausam", "S. Soderland", "O. Etzioni", "D.S. Weld", "K. Reiter", "M. Skinner", "M. Sammer", "J. Bilmes"], "venue": "Artif. Intell.,", "citeRegEx": "Mausam et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Mausam et al\\.", "year": 2010}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["T. Mikolov", "I. Sutskever", "K. Chen", "G.S. Corrado", "J. Dean"], "venue": "In NIPS,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Linguistic regularities in continuous space word representations", "author": ["T. Mikolov", "Yih", "W.-t", "G. Zweig"], "venue": "In HLT-NAACL,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Polylingual topic models", "author": ["D. Mimno", "H.M. Wallach", "J. Naradowsky", "D.A. Smith", "A. McCallum"], "venue": "In EMNLP,", "citeRegEx": "Mimno et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Mimno et al\\.", "year": 2009}, {"title": "Zero-shot learning with semantic output codes", "author": ["M. Palatucci", "D. Pomerleau", "G.E. Hinton", "T.M. Mitchell"], "venue": "In NIPS,", "citeRegEx": "Palatucci et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Palatucci et al\\.", "year": 2009}, {"title": "A survey on transfer learning", "author": ["S.J. Pan", "Q. Yang"], "venue": "IEEE Trans. on Knowledge and Data Engineering,", "citeRegEx": "Pan and Yang,? \\Q2010\\E", "shortCiteRegEx": "Pan and Yang", "year": 2010}, {"title": "How to choose the best pivot language for automatic translation of low-resource languages", "author": ["M. Paul", "A.M. Finch", "E. Sumita"], "venue": "ACM Trans. Asian Lang. Inf. Process.,", "citeRegEx": "Paul et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Paul et al\\.", "year": 2013}, {"title": "Translingual document representations from discriminative projections", "author": ["J.C. Platt", "K. Toutanova", "W. tau Yih"], "venue": "In EMNLP,", "citeRegEx": "Platt et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Platt et al\\.", "year": 2010}, {"title": "A wikipedia-based multilingual retrieval model", "author": ["M. Potthast", "B. Stein", "M. Anderka"], "venue": "In ECIR,", "citeRegEx": "Potthast et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Potthast et al\\.", "year": 2008}, {"title": "Cross-language text classification using structural correspondence learning", "author": ["P. Prettenhofer", "B. Stein"], "venue": "In ACL,", "citeRegEx": "Prettenhofer and Stein,? \\Q2010\\E", "shortCiteRegEx": "Prettenhofer and Stein", "year": 2010}, {"title": "An embarrassingly simple approach to zeroshot learning", "author": ["B. Romera-Paredes", "P.H.S. Torr"], "venue": "In ICML,", "citeRegEx": "Romera.Paredes and Torr,? \\Q2015\\E", "shortCiteRegEx": "Romera.Paredes and Torr", "year": 2015}, {"title": "Cross language text classification by model translation and semi-supervised learning", "author": ["L. Shi", "R. Mihalcea", "M. Tian"], "venue": "In EMNLP,", "citeRegEx": "Shi et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Shi et al\\.", "year": 2010}, {"title": "Zero-shot learning through cross-modal transfer", "author": ["R. Socher", "M. Ganjoo", "C.D. Manning", "A.Y. Ng"], "venue": "In NIPS,", "citeRegEx": "Socher et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "On dataless hierarchical text classification", "author": ["Y. Song", "D. Roth"], "venue": "In AAAI,", "citeRegEx": "Song and Roth,? \\Q2014\\E", "shortCiteRegEx": "Song and Roth", "year": 2014}, {"title": "Cross-lingual dataless classification for many languages", "author": ["Y. Song", "S. Upadhyay", "H. Peng", "D. Roth"], "venue": "In IJCAI,", "citeRegEx": "Song et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Song et al\\.", "year": 2016}, {"title": "Exploiting wikipedia for cross-lingual and multilingual information retrieval", "author": ["P. Sorg", "P. Cimiano"], "venue": "Data and Knowledge Engineering,", "citeRegEx": "Sorg and Cimiano,? \\Q2012\\E", "shortCiteRegEx": "Sorg and Cimiano", "year": 2012}, {"title": "Cross-lingual word clusters for direct transfer of linguistic structure", "author": ["O. T\u00e4ckstr\u00f6m", "R. McDonald", "J. Uszkoreit"], "venue": "In NAACL-HLT,", "citeRegEx": "T\u00e4ckstr\u00f6m et al\\.,? \\Q2012\\E", "shortCiteRegEx": "T\u00e4ckstr\u00f6m et al\\.", "year": 2012}, {"title": "Cross-lingual models of word embeddings: An empirical comparison", "author": ["S. Upadhyay", "M. Faruqui", "C. Dyer", "D. Roth"], "venue": null, "citeRegEx": "Upadhyay et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Upadhyay et al\\.", "year": 2016}, {"title": "A comparison of pivot methods for phrase-based statistical machine translation", "author": ["M. Utiyama", "H. Isahara"], "venue": "In NAACL-HLT,", "citeRegEx": "Utiyama and Isahara,? \\Q2007\\E", "shortCiteRegEx": "Utiyama and Isahara", "year": 2007}, {"title": "Revisiting pivot language approach for machine translation", "author": ["H. Wu", "H. Wang"], "venue": "In ACL/IJCNLP,", "citeRegEx": "Wu and Wang,? \\Q2009\\E", "shortCiteRegEx": "Wu and Wang", "year": 2009}, {"title": "Semi-supervised representation learning for cross-lingual text classification", "author": ["M. Xiao", "Y. Guo"], "venue": "In EMNLP,", "citeRegEx": "Xiao and Guo,? \\Q2013\\E", "shortCiteRegEx": "Xiao and Guo", "year": 2013}, {"title": "A model of zero-shot learning of spoken language understanding", "author": ["M. Yazdani", "J. Henderson"], "venue": "In EMNLP,", "citeRegEx": "Yazdani and Henderson,? \\Q2015\\E", "shortCiteRegEx": "Yazdani and Henderson", "year": 2015}, {"title": "Cross-lingual latent topic extraction", "author": ["D. Zhang", "Q. Mei", "C. Zhai"], "venue": "In ACL,", "citeRegEx": "Zhang et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2010}], "referenceMentions": [{"referenceID": 34, "context": "Specifically, it has been applied to classify documents in 180 languages that have at least some Wikipedia presence (Song et al., 2016).", "startOffset": 116, "endOffset": 135}, {"referenceID": 34, "context": "Since the existing benchmark data sets for multilingual document classification (Lewis, Yang, Rose, & Li, 2004; Hermann & Blunsom, 2014) focus on a small set of languages, to test the CLDDC in many languages, we use the data developed by (Song et al., 2016).", "startOffset": 238, "endOffset": 257}, {"referenceID": 13, "context": "6 They selected 100 documents from 20-newsgroups (Lang, 1995) which can be 100% correctly classified using the English dataless classification (Song & Roth, 2014), and used Google Translate API7 to translate these documents into 87 languages.", "startOffset": 49, "endOffset": 61}, {"referenceID": 34, "context": "The results in (Song et al., 2016) also showed that CLDDC for LWLs is in general comparable to supervised classification trained based on 100 labeled document per class.", "startOffset": 15, "endOffset": 34}, {"referenceID": 34, "context": "When seeing more documents, CLDDC can also be further improved by bootstrapping (Song et al., 2016).", "startOffset": 80, "endOffset": 99}, {"referenceID": 10, "context": "\u201d However, the dependent correlation test (Howell, 2011)12 shows this improvement is not significant.", "startOffset": 42, "endOffset": 56}, {"referenceID": 34, "context": "For LWLs, CLDDC is comparable to supervised learning method with about 100 to 200 labeled document per label (Song et al., 2016).", "startOffset": 109, "endOffset": 128}, {"referenceID": 34, "context": "For LWLs, CLDDC is comparable to supervised learning method with about 100 to 200 labeled document per label (Song et al., 2016).", "startOffset": 109, "endOffset": 128}, {"referenceID": 11, "context": "Similar to cross-lingual classification, these representation learning approaches need either parallel corpora (Klementiev et al., 2012; Hermann & Blunsom, 2014), some labeled data in the target domain (Xiao & Guo, 2013), or words being (partially) aligned in a dictionary (Zhang et al.", "startOffset": 111, "endOffset": 161}, {"referenceID": 42, "context": ", 2012; Hermann & Blunsom, 2014), some labeled data in the target domain (Xiao & Guo, 2013), or words being (partially) aligned in a dictionary (Zhang et al., 2010).", "startOffset": 144, "endOffset": 164}, {"referenceID": 34, "context": "It has been shown that CLESA outperforms one of popular the cross-lingual embedding approach (Hermann & Blunsom, 2014) on two benchmark datasets for CLDDC (Song et al., 2016).", "startOffset": 155, "endOffset": 174}, {"referenceID": 26, "context": "Pivot language is used to help machine translation when there is no enough resources to train a translation model from source language to target language (Mann & Yarowsky, 2001; Cohn & Lapata, 2007; Utiyama & Isahara, 2007; Wu & Wang, 2009; Leusch, Max, Crego, & Ney, 2010; Paul et al., 2013).", "startOffset": 154, "endOffset": 292}, {"referenceID": 26, "context": ", (Paul et al., 2013) used 22 Indo-European and Asian languages to evaluate how to select a good pivot language for machine translation.", "startOffset": 2, "endOffset": 21}, {"referenceID": 17, "context": "Elgammal, 2013; Romera-Paredes & Torr, 2015) and one-shot learning (Li et al., 2006; Lake et al., 2015) were first introduced in the computer vision community and are now recognized by the natural language processing community (Yazdani & Henderson, 2015; Lazaridou, Dinu, & Baroni, 2015).", "startOffset": 67, "endOffset": 103}, {"referenceID": 12, "context": "Elgammal, 2013; Romera-Paredes & Torr, 2015) and one-shot learning (Li et al., 2006; Lake et al., 2015) were first introduced in the computer vision community and are now recognized by the natural language processing community (Yazdani & Henderson, 2015; Lazaridou, Dinu, & Baroni, 2015).", "startOffset": 67, "endOffset": 103}], "year": 2016, "abstractText": "This paper presents an approach to classify documents in any language into an English topical label space, without any text categorization training data. The approach, CrossLingual Dataless Document Classification (CLDDC) relies on mapping the English labels or short category description into a Wikipedia-based semantic representation, and on the use of the target language Wikipedia. Consequently, performance could suffer when Wikipedia in the target language is small. In this paper, we focus on languages with small Wikipedias, (Small-Wikipedia languages, SWLs). We use a word-level dictionary to convert documents in a SWL to a large-Wikipedia language (LWLs), and then perform CLDDC based on the LWL\u2019s Wikipedia. This approach can be applied to thousands of languages, which can be contrasted with machine translation, which is a supervision heavy approach and can be done for about 100 languages. We also develop a ranking algorithm that makes use of language similarity metrics to automatically select a good LWL, and show that this significantly improves classification of SWLs\u2019 documents, performing comparably to the best bridge possible.", "creator": "dvips(k) 5.991 Copyright 2011 Radical Eye Software"}}}