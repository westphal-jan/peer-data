{"id": "1611.04125", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Nov-2016", "title": "Joint Representation Learning of Text and Knowledge for Knowledge Graph Completion", "abstract": "Joint representation learning of text and knowledge within a unified semantic space enables us to perform knowledge graph completion more accurately. In this work, we propose a novel framework to embed words, entities and relations into the same continuous vector space. In this model, both entity and relation embeddings are learned by taking knowledge graph and plain text into consideration. In experiments, we evaluate the joint learning model on three tasks including entity prediction, relation prediction and relation classification from text. The experiment results show that our model can significantly and consistently improve the performance on the three tasks as compared with other baselines.", "histories": [["v1", "Sun, 13 Nov 2016 12:32:20 GMT  (223kb,D)", "http://arxiv.org/abs/1611.04125v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["xu han", "zhiyuan liu", "maosong sun"], "accepted": false, "id": "1611.04125"}, "pdf": {"name": "1611.04125.pdf", "metadata": {"source": "CRF", "title": "Joint Representation Learning of Text and Knowledge for Knowledge Graph Completion", "authors": ["Xu Han", "Zhiyuan Liu", "Maosong Sun"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "Humans construct various large-scale knowledge quantities (KGs) to organize structural knowledge about the world, such as Freebase (Bollacker et al., 2008), YAGO (Suchanek et al., 2007), DBPedia (Auer et al., 2007), and WordNet (Miller, 1995). A typical knowledge diagram with nodes corresponding to the entities, and edges corresponding to the relationships between these entities, play an important role in answering questions and searching the web. The facts in the knowledge quantities are usually recorded as a series of relative triples (h, r, t)."}, {"heading": "2 Related Work", "text": "The work in this paper refers to the representation of learning of KGs, words and textual relationships. Related works are reviewed as: Representation learning of KGs. A variety of approaches have been proposed to encode entities and relationships into a continuous low-dimensional space. Inspired by (Mikolov et al., 2013b), TransE (Bordes et al., 2013) considers the relationship r in each (h, r, t) as a translation from h to t within low-dimensional space, i.e., h + r = t, where h and t are entity embedded and relationships. Despite its simplicity, TransE achieves state-of-the-art performance for KGs, especially for these large-format and trace KGs."}, {"heading": "3 The Framework", "text": "In this section, we present the framework of shared learning of representation, starting with notations and definitions."}, {"heading": "3.1 Notations and Definitions", "text": "We refer to a knowledge graph as G = {E, R, T}, where E denotes a set of entities, R denotes a set of relationship types, and T denotes a set of fact triples. Each threefold (h, r, t) indicates that there is a relationship between h, E, and t. We denote a text corpus as D and its vocabulary as V, which contains all words, phrases, and entity names. In corpus D, each sentence is denoted as s = {x1,..., xn}, xi-V, and the length is.For entities, relationships, and words, we use the bold face to indicate their corresponding low-dimensional vectors. For example, the embeddings of h, t-E, r-R, and x-V are each h, t, r, x-Rk of the k dimension."}, {"heading": "3.2 Joint Learning Method", "text": "In this context, it should be noted that the drafts mentioned are the first two drafts, which aim to decipher the drafts for the drafts of the drafts."}, {"heading": "3.3 Representation Learning of KGs", "text": "We select TransE (Bordes et al., 2013) to learn representations of entities and relations of KGs. For each pair of entities (h, t) in a KG G, we define their latent relationship, in which rht is embedded as a translation from h to t, which can be formalized as follows: rht = t \u2212 h. (3) Meanwhile, each pair of three (h, r, t) has an explicit relationship r between h and t. Therefore, we can define the scoring function for each threesome as follows: fr (h, t) = Hrht \u2212 r \u00b2 2 = Hr \u00b2 2. (4) This indicates that for each threesome (h, r, t, t) in T we expect an h + r \u00b2 relationship. Based on the value function above, we can formalize the loss function for all threesome combinations in T as follows: L (G) = Hr, t \u00b2 t \u00b2, T \u00b2, T \u00b2, T \u00b2 h \u00b2, T \u00b2 (4), T (4), T (4), T (2), T (n)."}, {"heading": "3.4 Representation Learning of Textual Relations", "text": "As shown in (Zeng et al., 2014), textual relationships can be learned with deep neural networks and encoded in low-dimensional semantic space. We follow (Zeng et al., 2014) and use Convolutionary Neural Networks (CNN) to model textual relationships from text. CNN is an efficient neural model that is widely used in image processing and has recently proven effective for many NLP tasks such as part-of-speech tagging, called entity recognition and semantic role marking (Collobert et al., 2011)."}, {"heading": "3.4.1 Overall Architecture", "text": "Figure 2 shows the overall architecture of CNN for modeling textual relationships. For a sentence s containing (h, t) with a relationship r, the architecture outputs word embedding s = {x1,.., xn} of the sentence s as input, and after it passes through two layers within CNN, the embedding of the textual relationship rs is output. Our method will further learn to minimize the loss function between r and rs, which can be formalized as: fr (s) = closest layers \u2212 r \u00b2 2. (7) Based on the scoring function, we can formalize the loss function across all sentences in D as follows: L (D) = closest layer and a pooling layer, which will be introduced as follows."}, {"heading": "3.4.2 Input Layer", "text": "In view of a sentence s consisting of n words s = {x1,., xn}, the input layer transforms the words of s into corresponding word embeddings s = {x1,.., xn}. For a word xi in the given sentence, its input embedment xi is composed of two real vectors: its textual word embeddings wi and its position for embedding pi.Textual word embeddings encode the semantics of the corresponding words, which are normally pre-trained by the pure text via word presentation learning, as originally proposed in Section 3.5.Word embeddings (WPE) in (Zeng et al., 2014). WPE is a position trait indicating the relative distances of the given word to the marked entities in the sentence. As shown in Figure 2, the relative distances of the word born to the entities Mark Twain and Florida are \u2212 2 and 2 respectively. We form each word distance to a vector = the dimensions of the vector in the sentence."}, {"heading": "3.4.3 Convolution Layer", "text": "We push a window of size m over the input word sequence. For each move we get an embedding x \u2032 i as: x \u2032 i = [xi \u2212 m \u2212 12;....; xi + m \u2212 1 2], (10), which is achieved by concatenating m vectors in s with xi as the center. In Figure 2, for example, a window glides through the input vectors s and concatenates all three word embeddings. Then we transform x \u2032 i into the hidden layer vector yiyi = tanh (Wx \u2032 i + b), (11) where W-Rkc \u00d7 mkw is the conventional core, b-Rkc is an inclined vector, kc is the dimension of hidden layer vectors yi, KW the dimension of the input vectors xi and m is the window size."}, {"heading": "3.4.4 Pooling Layer", "text": "In the pooling layer, a max pooling operation is applied via the hidden layer vectors y1,..., yn to obtain the final continuous vector as the textual relationship in which rs is embedded, formalized as follows: rs, j = max {y1, j,..., yn, j}, (12), where rs, j is the j-th value of the textual relationship in which rs is embedded, and yi, j is the j-th value of the hidden layer vector yi. After the pooling operation, we can obtain the given textual relationship to the loss function Eq. (7)."}, {"heading": "3.5 Initialization and Implementation Details", "text": "We follow (Mikolov et al., 2013c) and use Skip-Gram to learn word representations from the given text corpus. For relationships and other units, we randomly initialize their embedding. Both the TransE knowledge model and the CNN textual relationship model are simultaneously optimized using stochastic gradient descendence (SGD). The parameters of all models are trained using a batch training algorithm. Note that the gradients of the CNN parameters are propagated back to the entered word embedding, so that the embedding of both units and words can also be learned from plaintext via CNN."}, {"heading": "4 Experiments", "text": "We conduct experiments to predict entities and relations and evaluate the performance of our methods on the basis of various baselines."}, {"heading": "4.1 Experiment Settings", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1.1 Datasets", "text": "Knowledge Diagram. We choose Freebase (Bollacker et al., 2008) as the knowledge diagram for shared learning. Freebase is a widely used, large-scale knowledge diagram. In this paper, we adopt a data set extracted in our experiments from Freebase, FB15K. The data set has been used in many studies on knowledge representation (Bordes et al., 2013; Bordes et al., 2014; Lin et al., 2015). To ensure alignment accuracy, we consider only the sentences with anchor text associated with the units in FB15K, including the number of units, relationships and triplets. We extract 876, 227 sentences from the New York Times articles containing both head and tail units in FB15K for shared learning, and annotate these sentences with the corresponding relationships in triples. The 629 sentences are labeled with the units FB15K, including the 24BT-NYT-units."}, {"heading": "4.1.2 Evaluation Tasks", "text": "In experiments, we evaluate the common learning model and other foundations with three tasks: (1) Entity prediction. The task aims to predict missing entities in a threefold according to the embedding of another entity and relationship. (2) Relationship prediction. The task aims to predict missing relationships in a threefold according to the embedding of head and tail entities. (3) Relationship classification from text. We are also interested in extracting relational facts between new entities that are not included in knowledge graphs. Therefore, we perform relationship classifications from text without taking advantage of embedding entities that we have learned with the structure of knowledge graphs."}, {"heading": "4.1.3 Parameter Settings", "text": "In our common model, we select the learning rate \u03b1k on the knowledge side under {0,1, 0,01, 0,001} and the learning rate \u03b1t on the text side under {0,01, 0,025, 0,05}; the harmonic factor \u03bb = 1 and the margin \u03b3 = 1. We select the harmonic factor \u03c4 under {0,001, 0,0001, 0,0001} to balance the learning ratio between knowledge and text; the dimension of embedding k is chosen under {50, 100, 150}; the optimal configurations are \u03b1k = 0,001, \u03b1t = 0,025, \u03c4 = 0,0001, k = 150. During the learning process, we traverse the text corpus for 10 laps and triple in the knowledge diagram for 3000 laps."}, {"heading": "4.2 Results of Entity Prediction", "text": "In fact, it is such that we are in a time when we are in a time when we are in a time when we are in a time when we are in a time when we are in a time when we are in a time when we are in a time when we are in a time when we are in a time when we are in a time when we are in a time when we are in a time when we are in a time when we are in a time when we are in a time when we are in a time when we are in a time when we are in a time when we are in a time when we are in a time when we are in a time when we are in a time when we are in a time when we are in a time when we are in a time when we are in a time when we are in a time when we are in a time when we are in a time when we are in a time when we are in a time when we are in a time when we are in a time when we are in a time when we are in a time when we are in a time when we are in a time when we are in a time when we are in a time when we are in a time when we are in a time when we are in a time when we are in a time when we are in a time when we are in a time when we are in a time when we are in a time when we are in a time when we are in a time when we are in a time when we are in a time when we are in a time, in a time when we are in a time when we are in a time when we are in a time when we are in a time when we are in a time when we are in a time when we are in a time when we are in a time, in a time when we are in a time when we are in a time, in a time we are in a time we are in a time we are"}, {"heading": "4.3 Results of Relation Prediction", "text": "The task aims to predict the missing relationship between two entities based on their embedding. Specifically, we need to predict the relationship when we get a triple relationship (h,?, t). In this task, the system is asked to find the best result for each missing relationship, according to the similarity values calculated by back and forth. As the number of relationships is much smaller compared to the number of entities, we use the accuracy of the top-1 ranked relationships as a benchmark. As some entities may have more than one relationship between them, we also filter out those triple relationships that appear in knowledge graphs with corrupt relationships. We report on the overvaluation results as well as those in different relationship classes. The evaluation results are shown in Table 3. From Table 3, we find that our common model consistently outperforms TransE in different classes of relationships and in all. The common model also achieves more significant improvements in relation to \"1-1-to N\" and \"1-1.\""}, {"heading": "4.4 Results of Relation Classification from Text", "text": "Most models (Mintz et al., 2009; Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012) use knowledge diagrams as remote supervision to automatically annotate sentences in text corpora, and then extract textual characteristics to build relationship classifiers. Compared to relationship forecasting with embedding, the task uses only plain text to identify relationship facts, and is therefore able to automatically annotate new units that do not necessarily appear in knowledge diagrams. As there is a lot of noise in plain text and remote supervision, it does not make the task easy. With this task, we want to investigate the effectiveness of our common model for learning CNN models. We follow (Weston et al., 2013) to perform evaluations. The evaluation construction includes candidate triples that are combined by entity pairs in testing and different relationships."}, {"heading": "5 Conclusion and Future Work", "text": "In this paper, we propose a model for the shared learning of text and knowledge representations. Our common model embeds entities, relationships, and words in the same continuous latent space. Specifically, we use deep neural networks CNN to encode textual relationships for the shared learning of relationship embeddings. In experiments, we evaluate our common model based on three tasks, including entity prediction, relationship prediction with embeddings, and relationship prediction from text. Experimental results show that our common model can effectively perform representational learning from both knowledge diagrams and plaintext, and will obtain more differentiated entiation entity and relationship embeddings for prediction. In the future, we will explore the following research directions: (1) Remote monitoring can introduce many loud sentences with false relationship annotations. We will explore techniques such as multiple learning to reduce these sounds and improve the effectiveness of shared learning. (2) We will also examine the effectiveness of both short-term and long-term common learning networks, as well as the recurrent networks in common knowledge."}], "references": [{"title": "Dbpedia: A nucleus for a web of open data", "author": ["Auer et al.2007] S\u00f6ren Auer", "Christian Bizer", "Georgi Kobilarov", "Jens Lehmann", "Richard Cyganiak", "Zachary Ives"], "venue": null, "citeRegEx": "Auer et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Auer et al\\.", "year": 2007}, {"title": "Freebase: a collaboratively created graph database for structuring human knowledge", "author": ["Colin Evans", "Praveen Paritosh", "Tim Sturge", "Jamie Taylor"], "venue": "In Proceedings of KDD,", "citeRegEx": "Bollacker et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Bollacker et al\\.", "year": 2008}, {"title": "Translating embeddings for modeling multi-relational data", "author": ["Nicolas Usunier", "Alberto Garcia-Duran", "Jason Weston", "Oksana Yakhnenko"], "venue": "In Proceedings of NIPS,", "citeRegEx": "Bordes et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bordes et al\\.", "year": 2013}, {"title": "A semantic matching energy function for learning with multirelational data", "author": ["Xavier Glorot", "Jason Weston", "Yoshua Bengio"], "venue": "Machine Learning,", "citeRegEx": "Bordes et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bordes et al\\.", "year": 2014}, {"title": "Natural language processing (almost) from scratch", "author": ["Jason Weston", "L\u00e9on Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa"], "venue": null, "citeRegEx": "Collobert et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Classifying relations by ranking with convolutional neural networks", "author": ["Bing Xiang", "Bowen Zhou"], "venue": "In Proceedings of ACL-IJCNLP,", "citeRegEx": "Santos et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Santos et al\\.", "year": 2015}, {"title": "Why does unsupervised pretraining help deep learning", "author": ["Erhan et al.2010] Dumitru Erhan", "Yoshua Bengio", "Aaron Courville", "Pierre-Antoine Manzagol", "Pascal Vincent", "Samy Bengio"], "venue": null, "citeRegEx": "Erhan et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Erhan et al\\.", "year": 2010}, {"title": "Knowledge-based weak supervision for information extraction of overlapping relations", "author": ["Congle Zhang", "Xiao Ling", "Luke Zettlemoyer", "Daniel S Weld"], "venue": "In Proceedings of ACL-HLT,", "citeRegEx": "Hoffmann et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Hoffmann et al\\.", "year": 2011}, {"title": "Relational retrieval using a combination of path-constrained random walks", "author": ["Lao", "Cohen2010] Ni Lao", "William W Cohen"], "venue": "Machine learning,", "citeRegEx": "Lao et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Lao et al\\.", "year": 2010}, {"title": "Random walk inference and learning in a large scale knowledge base", "author": ["Lao et al.2011] Ni Lao", "Tom Mitchell", "William W Cohen"], "venue": "In Proceedings of EMNLP,", "citeRegEx": "Lao et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Lao et al\\.", "year": 2011}, {"title": "Learning entity and relation embeddings for knowledge graph completion", "author": ["Lin et al.2015] Yankai Lin", "Zhiyuan Liu", "Maosong Sun", "Yang Liu", "Xuan Zhu"], "venue": "Proceedings of AAAI", "citeRegEx": "Lin et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2015}, {"title": "Efficient estimation of word representations in vector space", "author": ["Kai Chen", "Greg Corrado", "Jeffrey Dean"], "venue": "Proceedings of ICLR", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean"], "venue": "In Proceedings of NIPS,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Linguistic regularities in continuous space word representations", "author": ["Wen-tau Yih", "Geoffrey Zweig"], "venue": "In Proceedings of HLT-NAACL,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Wordnet: a lexical database for english", "author": ["George A Miller"], "venue": "Communications of the ACM,", "citeRegEx": "Miller.,? \\Q1995\\E", "shortCiteRegEx": "Miller.", "year": 1995}, {"title": "Distant supervision for relation extraction without labeled data", "author": ["Mintz et al.2009] Mike Mintz", "Steven Bills", "Rion Snow", "Dan Jurafsky"], "venue": "In Proceedings of ACL-IJCNLP,", "citeRegEx": "Mintz et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Mintz et al\\.", "year": 2009}, {"title": "Modeling relations and their mentions without labeled text", "author": ["Limin Yao", "Andrew McCallum"], "venue": "In Machine Learning", "citeRegEx": "Riedel et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Riedel et al\\.", "year": 2010}, {"title": "Yago: a core of semantic knowledge", "author": ["Gjergji Kasneci", "Gerhard Weikum"], "venue": "In Proceedings of WWW,", "citeRegEx": "Suchanek et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Suchanek et al\\.", "year": 2007}, {"title": "Multi-instance multi-label learning for relation extraction", "author": ["Julie Tibshirani", "Ramesh Nallapati", "Christopher D Manning"], "venue": "In Proceedings of EMNLP,", "citeRegEx": "Surdeanu et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Surdeanu et al\\.", "year": 2012}, {"title": "Representing text for joint embedding of text and knowledge bases", "author": ["Danqi Chen", "Patrick Pantel", "Pallavi Choudhury", "Michael Gamon"], "venue": "Proceedings of EMNLP", "citeRegEx": "Toutanova et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Toutanova et al\\.", "year": 2015}, {"title": "Knowledge graph and text jointly embedding", "author": ["Wang et al.2014a] Zhen Wang", "Jianwen Zhang", "Jianlin Feng", "Zheng Chen"], "venue": "In Proceedings of EMNLP,", "citeRegEx": "Wang et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2014}, {"title": "Knowledge graph embedding by translating on hyperplanes", "author": ["Wang et al.2014b] Zhen Wang", "Jianwen Zhang", "Jianlin Feng", "Zheng Chen"], "venue": "In Proceedings of AAAI,", "citeRegEx": "Wang et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2014}, {"title": "Connecting language and knowledge bases with embedding models for relation extraction", "author": ["Weston et al.2013] Jason Weston", "Antoine Bordes", "Oksana Yakhnenko", "Nicolas Usunier"], "venue": null, "citeRegEx": "Weston et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Weston et al\\.", "year": 2013}, {"title": "Classifying relations via long short term memory networks along shortest dependency paths", "author": ["Xu et al.2015] Yan Xu", "Lili Mou", "Ge Li", "Yunchuan Chen", "Hao Peng", "Zhi Jin"], "venue": "Proceedings of EMNLP", "citeRegEx": "Xu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2015}, {"title": "Relation classification via convolutional deep neural network", "author": ["Zeng et al.2014] Daojian Zeng", "Kang Liu", "Siwei Lai", "Guangyou Zhou", "Jun Zhao"], "venue": "In Proceedings of COLING,", "citeRegEx": "Zeng et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zeng et al\\.", "year": 2014}, {"title": "Relation classification via recurrent neural network. arXiv preprint arXiv:1508.01006", "author": ["Zhang", "Wang2015] Dongxu Zhang", "Dong Wang"], "venue": null, "citeRegEx": "Zhang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 1, "context": "People construct various large-scale knowledge graphs (KGs) to organize structural knowledge about the world, such as Freebase (Bollacker et al., 2008), YAGO (Suchanek et al.", "startOffset": 127, "endOffset": 151}, {"referenceID": 17, "context": ", 2008), YAGO (Suchanek et al., 2007), DBPedia (Auer et al.", "startOffset": 14, "endOffset": 37}, {"referenceID": 0, "context": ", 2007), DBPedia (Auer et al., 2007) and WordNet (Miller, 1995).", "startOffset": 17, "endOffset": 36}, {"referenceID": 14, "context": ", 2007) and WordNet (Miller, 1995).", "startOffset": 20, "endOffset": 34}, {"referenceID": 9, "context": "Based on the network structure of KGs, many graphbased methods have been proposed to find novel facts between entities (Lao et al., 2011; Lao and Cohen, 2010).", "startOffset": 119, "endOffset": 158}, {"referenceID": 24, "context": "Many efforts are also devoted to extract relational facts from plain text (Zeng et al., 2014; dos Santos et al., 2015).", "startOffset": 74, "endOffset": 118}, {"referenceID": 2, "context": "In recent years, neural-based knowledge representation has been proposed to encode both entities and relations into a low-dimensional space, which are capable to find novel facts (Bordes et al., 2013; Wang et al., 2014b; Lin et al., 2015).", "startOffset": 179, "endOffset": 238}, {"referenceID": 10, "context": "In recent years, neural-based knowledge representation has been proposed to encode both entities and relations into a low-dimensional space, which are capable to find novel facts (Bordes et al., 2013; Wang et al., 2014b; Lin et al., 2015).", "startOffset": 179, "endOffset": 238}, {"referenceID": 19, "context": ", 2014a) performs joint learning simply considering alignment between words and entities, and (Toutanova et al., 2015) extracts textual relations from plain texts using dependency parsing to enhance relation embeddings.", "startOffset": 94, "endOffset": 118}, {"referenceID": 19, "context": ", 2014a) and textual relations in (Toutanova et al., 2015)), or rely on complicated linguistic analysis (dependency parsing in (Toutanova et al.", "startOffset": 34, "endOffset": 58}, {"referenceID": 19, "context": ", 2015)), or rely on complicated linguistic analysis (dependency parsing in (Toutanova et al., 2015)) which may bring inevitable parsing errors.", "startOffset": 76, "endOffset": 100}, {"referenceID": 2, "context": ", 2013b), TransE (Bordes et al., 2013) regards the relation r in each (h, r, t) as a translation from h to t within the lowdimensional space, i.", "startOffset": 17, "endOffset": 38}, {"referenceID": 10, "context": ", 2014b) and TransR (Lin et al., 2015), which is not the focus of this paper and will be left as our future work.", "startOffset": 20, "endOffset": 38}, {"referenceID": 15, "context": "Many works aim to extract relational facts from large-scale text corpora (Mintz et al., 2009; Riedel et al., 2010).", "startOffset": 73, "endOffset": 114}, {"referenceID": 16, "context": "Many works aim to extract relational facts from large-scale text corpora (Mintz et al., 2009; Riedel et al., 2010).", "startOffset": 73, "endOffset": 114}, {"referenceID": 24, "context": "In recent years, deep neural models such as convolutional neural networks (CNN) have been proposed to encode semantics of sentences to identify relations between entities (Zeng et al., 2014; dos Santos et al., 2015).", "startOffset": 171, "endOffset": 215}, {"referenceID": 23, "context": "Many neural models such as recurrent neural networks (RNN) (Zhang and Wang, 2015) and longshort term memory networks (LSTM) (Xu et al., 2015) have also been explored for relation extraction.", "startOffset": 124, "endOffset": 141}, {"referenceID": 6, "context": "As reported in many previous works, deep neural network will benefit significantly if being initialized with pre-trained word embeddings (Erhan et al., 2010).", "startOffset": 137, "endOffset": 157}, {"referenceID": 2, "context": "We select TransE (Bordes et al., 2013) to learn representations of entities and relations from KGs.", "startOffset": 17, "endOffset": 38}, {"referenceID": 24, "context": "As shown in (Zeng et al., 2014), the textual relations can be learned with deep neural networks and encoded in the low-dimensional semantic space.", "startOffset": 12, "endOffset": 31}, {"referenceID": 24, "context": "We follow (Zeng et al., 2014) and apply convolutional neural networks (CNN) to model textual relations from text.", "startOffset": 10, "endOffset": 29}, {"referenceID": 4, "context": "CNN is an efficient neural model widely used in image processing, which has recently been verified to be also effective for many NLP tasks such as part-of-speech tagging, named entity recognition and semantic role labeling (Collobert et al., 2011).", "startOffset": 223, "endOffset": 247}, {"referenceID": 24, "context": "Word position embeddings (WPE) is originally proposed in (Zeng et al., 2014).", "startOffset": 57, "endOffset": 76}, {"referenceID": 1, "context": "We select Freebase (Bollacker et al., 2008) as the knowledge graph for joint learning.", "startOffset": 19, "endOffset": 43}, {"referenceID": 2, "context": "The dataset has been used in many studies on knowledge representation learning (Bordes et al., 2013; Bordes et al., 2014; Lin et al., 2015).", "startOffset": 79, "endOffset": 139}, {"referenceID": 3, "context": "The dataset has been used in many studies on knowledge representation learning (Bordes et al., 2013; Bordes et al., 2014; Lin et al., 2015).", "startOffset": 79, "endOffset": 139}, {"referenceID": 10, "context": "The dataset has been used in many studies on knowledge representation learning (Bordes et al., 2013; Bordes et al., 2014; Lin et al., 2015).", "startOffset": 79, "endOffset": 139}, {"referenceID": 2, "context": "Entity prediction has also been used for evaluation in (Bordes et al., 2013; Wang et al., 2014b; Lin et al., 2015).", "startOffset": 55, "endOffset": 114}, {"referenceID": 10, "context": "Entity prediction has also been used for evaluation in (Bordes et al., 2013; Wang et al., 2014b; Lin et al., 2015).", "startOffset": 55, "endOffset": 114}, {"referenceID": 2, "context": "We follow (Bordes et al., 2013; Wang et al., 2014b; Lin et al., 2015) and use the proportion of correct entities in Top-10 ranked entities (Hits@10) as the evaluation metric.", "startOffset": 10, "endOffset": 69}, {"referenceID": 10, "context": "We follow (Bordes et al., 2013; Wang et al., 2014b; Lin et al., 2015) and use the proportion of correct entities in Top-10 ranked entities (Hits@10) as the evaluation metric.", "startOffset": 10, "endOffset": 69}, {"referenceID": 2, "context": "As mentioned in (Bordes et al., 2013), a corrupted triple may also exist in knowledge graphs, which should not be considered as incorrect.", "startOffset": 16, "endOffset": 37}, {"referenceID": 2, "context": "Since the evaluation setting is identical, we simply report the results of TransE, TransH and TransR from (Bordes et al., 2013; Wang et al., 2014b; Lin et al., 2015), where \u201cunif\u201d and \u201cbern\u201d are two settings to sample negative instances for learning.", "startOffset": 106, "endOffset": 165}, {"referenceID": 10, "context": "Since the evaluation setting is identical, we simply report the results of TransE, TransH and TransR from (Bordes et al., 2013; Wang et al., 2014b; Lin et al., 2015), where \u201cunif\u201d and \u201cbern\u201d are two settings to sample negative instances for learning.", "startOffset": 106, "endOffset": 165}, {"referenceID": 15, "context": "Most models (Mintz et al., 2009; Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012) take knowledge graphs as distant supervision to automatically annotate sentences in text corpora as training instances, and then extract textual features to build relation classifiers.", "startOffset": 12, "endOffset": 99}, {"referenceID": 16, "context": "Most models (Mintz et al., 2009; Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012) take knowledge graphs as distant supervision to automatically annotate sentences in text corpora as training instances, and then extract textual features to build relation classifiers.", "startOffset": 12, "endOffset": 99}, {"referenceID": 7, "context": "Most models (Mintz et al., 2009; Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012) take knowledge graphs as distant supervision to automatically annotate sentences in text corpora as training instances, and then extract textual features to build relation classifiers.", "startOffset": 12, "endOffset": 99}, {"referenceID": 18, "context": "Most models (Mintz et al., 2009; Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012) take knowledge graphs as distant supervision to automatically annotate sentences in text corpora as training instances, and then extract textual features to build relation classifiers.", "startOffset": 12, "endOffset": 99}, {"referenceID": 22, "context": "We follow (Weston et al., 2013) to conduct evaluation.", "startOffset": 10, "endOffset": 31}], "year": 2016, "abstractText": "Joint representation learning of text and knowledge within a unified semantic space enables us to perform knowledge graph completion more accurately. In this work, we propose a novel framework to embed words, entities and relations into the same continuous vector space. In this model, both entity and relation embeddings are learned by taking knowledge graph and plain text into consideration. In experiments, we evaluate the joint learning model on three tasks including entity prediction, relation prediction and relation classification from text. The experiment results show that our model can significantly and consistently improve the performance on the three tasks as compared with other baselines.", "creator": "LaTeX with hyperref package"}}}