{"id": "1611.04146", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Nov-2016", "title": "Commonsense Knowledge Enhanced Embeddings for Solving Pronoun Disambiguation Problems in Winograd Schema Challenge", "abstract": "This paper proposes a general framework to combine context and commonsense knowledge for solving the Winograd Schema (WS) and Pronoun Disambiguation Problems (PDP). In the proposed framework, commonsense knowledge bases (e.g. cause-effect word pairs) are quantized as knowledge constraints. The constraints guide us to learn knowledge enhanced embeddings (KEE) from large text corpus. Based on the pre-trained KEE models, this paper proposes two methods to solve the WS and PDP problems. The first method is an unsupervised method, which represents all the pronouns and candidate mentions in continuous vector spaces based on their contexts and calculates the semantic similarities between all the possible word pairs. The pronoun disambiguation procedure could then be implemented by comparing the semantic similarities between the pronoun (to be resolved) and all the candidate mentions. The second method is a supervised method, which extracts features for all the pronouns and candidate mentions and solves the WS problems by training a typical mention pair classification model. Similar to the first method, the features used in the second method are also extracted based on the KEE models. Experiments conducted on the available PDP and WS test sets show that, these two methods both achieve consistent improvements over the baseline systems. The best performance reaches 62\\% in accuracy on the PDP test set of the first Winograd Schema Challenge.", "histories": [["v1", "Sun, 13 Nov 2016 15:38:32 GMT  (1793kb,D)", "http://arxiv.org/abs/1611.04146v1", "7 pages"], ["v2", "Thu, 22 Dec 2016 02:27:16 GMT  (943kb,D)", "http://arxiv.org/abs/1611.04146v2", "Winograd Schema Challenge, Pronoun Disambiguation Problems, Neural Embedding Methods, Commonsense Knowledge"]], "COMMENTS": "7 pages", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["quan liu", "hui jiang", "zhen-hua ling", "xiaodan zhu", "si wei", "yu hu"], "accepted": false, "id": "1611.04146"}, "pdf": {"name": "1611.04146.pdf", "metadata": {"source": "CRF", "title": "Combing Context and Commonsense Knowledge Through Neural Networks for Solving Winograd Schema Problems", "authors": ["Quan Liu", "Hui Jiang", "Zhen-Hua Ling", "Xiaodan Zhu", "Si Wei", "Yu Hu"], "emails": ["quanliu@mail.ustc.edu.cn,", "hj@cse.yorku.ca,", "zhling@ustc.edu.cn,", "zhu2048@gmail.com", "siwei@iflytek.com,", "yuhu@iflytek.com"], "sections": [{"heading": "Introduction", "text": "This year, more than ever before in the history of the country in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is not a country, but in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country and in which it is a country."}, {"heading": "Motivation", "text": "In this section, we present the main motivation of this work. First, we present the main problems we want to solve, namely the Winograd Scheme (WS) and the Pronoun Disambiguation Problem (PDP), followed by detailed descriptions of our motivation."}, {"heading": "Winograd Schema (WS)", "text": "The Winograd Scheme (WS) evaluates the ability of common sense to argue a system based on a traditional, very specific task for processing natural language: co-reference resolution (Saba 2015). As described in (Levesque, Davis and Morgenstern 2011), a WS is a small reading comprehension test involving only a single binary question. \u2022 Joan made sure to thank Susan for all the help she had given. - Answer A: Joan - Answer B: Susan - Correct Answer: B The correct answers to the above question are obvious to humans. In each of the questions, the corresponding WS has the following four characteristics: 1. Two parties are mentioned in a sentence by noun phrases. You can name two males, two females, two inanimate objects, or 2. or 3. Groups of persons or sentences that replace the reference sentence in the second person."}, {"heading": "Pronoun Disambiguation Problems (PDP)", "text": "The pronoun disambiguation problems (PDP) is a complex problem of nuclear resolution that needs to be addressed directly or modified using examples from literature, biographies, autobiographies, essays, news analysis, and news stories (Morgenstern, Davis, and Ortiz Jr. 2016). Here is a typical PDP example: \u2022 The prairie of Dakota lay so warm and bright under the blazing sun that it seemed impossible that it would ever have been swept away by the wind and snow of this harsh winter. - Snipple: it had ever been swept. - Answer A: the prairie - Answer B: the sun - Correct answer: In the PDP problem, the pronoun to be solved is highlighted in bold. It repeats itself, with a snippet from the context and with multiple candidate answers, in the line following the passage. In the example shown here, we know that the prairie (and not the sun) is more likely to be the problem of the 50% if the problems are not the DP and the four problems are not the DP."}, {"heading": "Motivation: Knowledge Enhanced Embeddings", "text": "The main motivation is described as follows: First, because context is the key to learning the meaning of the word (Harris 1954; Miller and Charles 1991), we represent the pronoun and all the candidates it mentions by their context. For example, in the above-mentioned PDP example, we represent the word prairie of The Dakota and lie so warm and bright. Meanwhile, the word sun could be represented through and bright under the glow and it did not shine. On this basis, the pronoun disambiguity problem is solved by calculating the semantic similarities between the representations of the pronoun and all the corresponding candidates. Second, relying only on context is not good enough to tackle the WS and PDP problems. It is crucial to find an effective strategy to combine context and common sense."}, {"heading": "Model Level", "text": "Motivation for PDP System: Features + Models"}, {"heading": "Texts KBs", "text": "Using the model of large text corpora and common sense CBs, we obtain useful distributed word representations. Based on the pre-trained word representations, we propose two effective methods at the model level to finally solve the MS and PDP problems. The proposed two methods are the method of semantic similarity and the method of neural networks. In the method of semantic similarity, we present all pronouns and candidate names by composing their contexts from words. By further calculating the semantic similarities between the representations of each pronoun and the corresponding candidate mentions, the procedure for answering PDP or MS questions could be implemented by finding the most similar candidate for the pronoun. In the methods of neural networks, the pre-trained KEE models are also used to extract characteristics for all pronouns and candidates."}, {"heading": "The Proposed Methods", "text": "Based on motivation, we will present the KEE model and two methods for solving MS and PDP problems. Before introducing these methods, we will describe the common sense knowledge used in this paper."}, {"heading": "The Commonsense Knowledge", "text": "In the artificial intelligence community, there are open common-sense knowledge bases, such as Cyc (Lenat 1995), ThoughtTreasure (Mueller 1998), and ConcepNet (Liu and Singh 2004). Cyc is an artificial intelligence project that seeks to build a comprehensive ontology and common-sense knowledge base, with the goal of enabling AI applications to execute human thinking. Typical pieces of knowledge represented in the Cycian database are \"every tree is a plant\" and \"plants die at some point.\" ConceptNet is a semantic network that contains many things computers should know about the world, especially when understanding text written by humans. It consists of nodes representing words or short phrases of natural language, and designates relationships between them."}, {"heading": "Knowledge Enhanced Embedding", "text": "In order to combine the context and common sense to solve the MS and PDP problems, this paper suggests treating common knowledge as semantic constraints and expanding knowledge (KEE) based on the constraints generated. The idea of learning the word based on constraints is similar to the work of (Liu et al. 2015). The main difference is the way we generate the constraints on knowledge. In this paper, we propose to create constraints as follows: Knowledge Constraints Since all cause-effect pairs used in this paper contain the corresponding trust weights, i.e. PMI values, we propose to generate semantic inequalities by randomly interpreting two cause-effect pairs. Specifically, for each cause-effect pair, we will create random examples of 5 different pairs from the entire KB set and construct the imbalances by comparing their PMI values."}, {"heading": "Unsupervised Semantic Similarity Method", "text": "The first method proposed in this paper to answer the ES- and PDP-problems, which is shown in Figure 5, is an unattended method. We introduce a simple, unattended semantic similarity method (USSM), which aims to represent the pronoun and all candidate names by composing from the pre-trained KEE embeddings. For the composition function, we design the recently proposed approach called Fixed-size ordinally-forgetting encoding (FOFE). For a sentence, the FOFE function works as follows: In the face of a sequence of words, S = {w1, w2,..., wT} each word wt is first represented by a 1-of-K encoding (FOFE)."}, {"heading": "Neural Knowledge Activated Method", "text": "The difference to the first semantic similarity method is that it does not simply calculate the semantic similarities between the extracted embedding vectors of the (resolved) pronoun and all candidate mentions, but instead uses the composite embedding vectors as input characteristics and trains a deep neural network (DNN).The DNN model works as a mentioning classifier to assess whether two mentions are coreferent or not, which is a widely used technology in the correlation resolution community (Ng 2010).Since the features we have extracted for the formation of the DNN are composed of the knowledge-enhanced embedding, we refer to the method as neural activated method (NKAM).Experiments In this section we present all the experiments that have been performed to evaluate the effectiveness of the proposed methods. This section would be started by introducing the experimental results and data analysis."}, {"heading": "Datasets", "text": "To evaluate the effectiveness of the proposed methods and to keep our methods comparable, all the experimental datasets examined in this paper, including PDP test set1 and WS test set2, are taken from the Winograd Schema Challenge (Morgenstern, Davis and Ortiz Jr 2016).Table 2 lists the size and randomly guessed accuracy of all datasets.The PDP test set contains 60 problems, while the WS test set has 273 problems."}, {"heading": "Experimental setup", "text": "To clarify all the settings for the proposed methods of this work, we describe the experimental setup as follows. Advanced Embedding Setup This paper uses the English Wikipedia corpora to carry the knowledge of advanced embedding. In particular, we use two Wikipedia corporas of different sizes. The first corpus of a smaller size is a data set containing the first billion characters of Wikipedia3, called Wiki-Small in our experiments. The second corpus of a relatively large size is a snapshot of the Wikipedia articles from (Shaoul 2010), named Wiki-Large in our experiments. Both Wikipedia corporations are pre-edited by removing all HTML metadata and hyperlinks."}, {"heading": "Results", "text": "Table 3 shows the overall results. We divide the results based on the text corpus we used for the KEE training. In addition to experimenting with the proposed two methods, i.e., USSM and NKAM, we also construct a system by combing through the USSM and NKAM methods. For each pronoun and its candidate, the system combination method is implemented by interpolating the results calculated by the USSM and NKAM methods (the interpolation coefficients are 70% for NKAM and 30% for USSM). Of the results, we find the USSM method (as an unmonitored method), which achieves a 41.7% and 48.3% accuracy on the PDP test set if the KEE models are trained only on texts (without general knowledge) that are identical to the Skip-gram models) from WikiSmall and Wiki-Large models. If we use the knowledge we practice, we improve the knowledge we practice."}], "references": [{"title": "Elementary school science and math tests as a driver for ai: take the Aristo challenge", "author": ["P. Clark"], "venue": null, "citeRegEx": "Clark,? \\Q2015\\E", "shortCiteRegEx": "Clark", "year": 2015}, {"title": "and Dagan", "author": ["M. Geffet"], "venue": "I.", "citeRegEx": "Geffet and Dagan 2005", "shortCiteRegEx": null, "year": 2005}, {"title": "Z", "author": ["Harris"], "venue": "S.", "citeRegEx": "Harris 1954", "shortCiteRegEx": null, "year": 1954}, {"title": "D", "author": ["Lenat"], "venue": "B.", "citeRegEx": "Lenat 1995", "shortCiteRegEx": null, "year": 1995}, {"title": "H", "author": ["Levesque"], "venue": "J.; Davis, E.; and Morgenstern, L.", "citeRegEx": "Levesque. Davis. and Morgenstern 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "and Singh", "author": ["H. Liu"], "venue": "P.", "citeRegEx": "Liu and Singh 2004", "shortCiteRegEx": null, "year": 2004}, {"title": "Learning semantic word embeddings based on ordinal knowledge constraints", "author": ["Liu"], "venue": "In Proceedings of ACL,", "citeRegEx": "Liu,? \\Q2015\\E", "shortCiteRegEx": "Liu", "year": 2015}, {"title": "Probabilistic reasoning via deep learning: Neural association models. arXiv preprint arXiv:1603.07704", "author": ["Liu"], "venue": null, "citeRegEx": "Liu,? \\Q2016\\E", "shortCiteRegEx": "Liu", "year": 2016}, {"title": "Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781", "author": ["Mikolov"], "venue": null, "citeRegEx": "Mikolov,? \\Q2013\\E", "shortCiteRegEx": "Mikolov", "year": 2013}, {"title": "W", "author": ["G.A. Miller", "Charles"], "venue": "G.", "citeRegEx": "Miller and Charles 1991", "shortCiteRegEx": null, "year": 1991}, {"title": "C", "author": ["L. Morgenstern", "E. Davis", "Ortiz Jr"], "venue": "L.", "citeRegEx": "Morgenstern. Davis. and Ortiz Jr 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "E", "author": ["Mueller"], "venue": "T.", "citeRegEx": "Mueller 1998", "shortCiteRegEx": null, "year": 1998}, {"title": "G", "author": ["V. Nair", "Hinton"], "venue": "E.", "citeRegEx": "Nair and Hinton 2010", "shortCiteRegEx": null, "year": 2010}, {"title": "Supervised noun phrase coreference research: The first fifteen years. In Proceedings of the 48th annual meeting of the association for computational linguistics, 1396\u20131411", "author": ["V. Ng"], "venue": null, "citeRegEx": "Ng,? \\Q2010\\E", "shortCiteRegEx": "Ng", "year": 2010}, {"title": "A", "author": ["Turing"], "venue": "M.", "citeRegEx": "Turing 1950", "shortCiteRegEx": null, "year": 1950}, {"title": "S", "author": ["P.D. Turney", "Mohammad"], "venue": "M.", "citeRegEx": "Turney and Mohammad 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "and Shah", "author": ["K. Warwick"], "venue": "H.", "citeRegEx": "Warwick and Shah 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Linguistic Data Consortium, Philadelphia, PA", "author": ["Weischedel"], "venue": "Ontonotes release", "citeRegEx": "Weischedel,? \\Q2013\\E", "shortCiteRegEx": "Weischedel", "year": 2013}, {"title": "A", "author": ["J. Weston", "A. Bordes", "S. Chopra", "Rush"], "venue": "M.; van Merri\u00ebnboer, B.; Joulin, A.; and Mikolov, T.", "citeRegEx": "Weston et al. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "The fixed-size ordinally-forgetting encoding method for neural network language models", "author": ["Zhang"], "venue": "Proceedings of ACL", "citeRegEx": "Zhang,? \\Q2015\\E", "shortCiteRegEx": "Zhang", "year": 2015}], "referenceMentions": [], "year": 2017, "abstractText": "Solving Winograd Schema Problems Quan Liu\u2020, Hui Jiang\u2021, Zhen-Hua Ling\u2020, Xiaodan Zhu, Si Wei\u00a7, Yu Hu\u2020\u00a7 \u2020 National Engineering Laboratory for Speech and Language Information Processing University of Science and Technology of China, Hefei, Anhui, China \u2021 Department of Electrical Engineering and Computer Science, York University, Canada ` National Research Council Canada, Ottawa, Canada \u00a7 iFLYTEK Research, Hefei, China emails: quanliu@mail.ustc.edu.cn, hj@cse.yorku.ca, zhling@ustc.edu.cn, zhu2048@gmail.com siwei@iflytek.com, yuhu@iflytek.com Abstract", "creator": "LaTeX with hyperref package"}}}