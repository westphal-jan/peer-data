{"id": "1611.04201", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Nov-2016", "title": "CAD2RL: Real Single-Image Flight without a Single Real Image", "abstract": "We propose (CAD)$^2$RL, a flight controller for Collision Avoidance via Deep Reinforcement Learning that can be used to perform collision-free flight in the real world although it is trained entirely in a 3D CAD model simulator. Our method uses only single RGB images from a monocular camera mounted on the robot as the input and is specialized for indoor hallway following and obstacle avoidance. In contrast to most indoor navigation techniques that aim to directly reconstruct the 3D geometry of the environment, our approach directly predicts the probability of collision given the current monocular image and a candidate action. To obtain accurate predictions, we develop a deep reinforcement learning algorithm for learning indoor navigation, which uses the actual performance of the current policy to construct accurate supervision. The collision prediction model is represented by a deep convolutional neural network that directly processes raw image inputs. Our collision avoidance system is entirely trained in simulation and thus addresses the high sample complexity of deep reinforcement learning and avoids the dangers of trial-and-error learning in the real world. By highly randomizing the rendering settings for our simulated training set, we show that we can train a collision predictor that generalizes to new environments with substantially different appearance from the training scenarios. Finally, we evaluate our method in the real world by controlling a real quadrotor flying through real hallways. We demonstrate that our method can perform real-world collision avoidance and hallway following after being trained exclusively on synthetic images, without ever having seen a single real image at the training time. For supplementary video see:", "histories": [["v1", "Sun, 13 Nov 2016 23:08:42 GMT  (7408kb,D)", "http://arxiv.org/abs/1611.04201v1", "11 pages"], ["v2", "Wed, 7 Dec 2016 20:48:48 GMT  (7408kb,D)", "http://arxiv.org/abs/1611.04201v2", "11 pages; Supplementary video:this https URL"], ["v3", "Tue, 30 May 2017 11:47:41 GMT  (8150kb,D)", "http://arxiv.org/abs/1611.04201v3", "To appear at Robotics: Science and Systems Conference (R:SS), 2017. Supplementary video:this https URL"], ["v4", "Thu, 8 Jun 2017 07:21:39 GMT  (8150kb,D)", "http://arxiv.org/abs/1611.04201v4", "To appear at Robotics: Science and Systems Conference (R:SS), 2017. Supplementary video:this https URL"]], "COMMENTS": "11 pages", "reviews": [], "SUBJECTS": "cs.LG cs.CV cs.RO", "authors": ["fereshteh sadeghi", "sergey levine"], "accepted": false, "id": "1611.04201"}, "pdf": {"name": "1611.04201.pdf", "metadata": {"source": "CRF", "title": "(CAD)RL: Real Single-Image Flight without a Single Real Image", "authors": ["Fereshteh Sadeghi", "Sergey Levine"], "emails": ["fsadeghi@cs.washington.edu", "svlevine@eecs.berkeley.edu"], "sections": [{"heading": null, "text": "This year, more than ever before in the history of a country in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in a country, in a country, in a country, in a country, in a country, in a country, in a city, in a city, in a city, in a city, in a city, in a city, in a city, in a city, in a city, in a city, in a city, in a country, in a country, in a country, in a country, in a city, in a city, in a city, in a city, in a city, in a city, in a city, in a city, in a city, in a city, in a city, in a city, in a city, in a city, in a city, in a city, in a city, in a city, in a city, in a city, in a city, in a city, in a city, in a city, in a city, in a city, in a city, in a city, in a city, in a city, in a city, in a city, in a city, in a city, in a city, in a city, in a city, in a city, in a city, in a city, in a city, in a city, in a city, in a city, in a city, in a city, in a city, in a city, in a city, in a city, in a city, in a city, in a city, in a city, in a city, in a city, in a city, in a city, in a city, in a city, in a city, in a city, in a city, in a city, in a city, in a city, in a city, in a city, in a city, in a city, in a city, in a city, in a city, in a city, in a city, in a city, in a city, in a city, in a city, in a city, in a city, in a city, in a city, in a city, in a city, in a city, in a city, in a city, in a city, in a city, in a city,"}, {"heading": "II. RELATED WORK", "text": "This year, it has reached the stage where it will be able to take the lead."}, {"heading": "III. COLLISION AVOIDANCE VIA DEEP REINFORCEMENT LEARNING", "text": "Our goal is to choose measures for indoor navigation that avoid collisions with obstacles such as walls and furniture. Although we do not explicitly consider the general navigation destination (e.g. the direction in which the vehicle should fly to reach a destination), we present a general and flexible collision avoidance method that predicts which actions are more or less likely to cause collisions, which can be easily combined with higher-level navigation objectives. Input to our model consists only of monocular RGB images without depth, IMU inputs, or other sensors, making it suitable for low-cost, low-power platforms. Formally, it designates camera observation at a point in time t, and the action we will define in Section III-A as the goal of the model is to predict P (C | It, at), with C being the discounted expectation of a collision event: P (C | It, at) = t + (S, at)."}, {"heading": "A. Perception-Based Control", "text": "Our perception-based policy uses an action representation that corresponds to positions in the frame space. The image is discredited into an M \u00d7 M grid of containers, and each container has a corresponding action, so that when simply selecting the container, the container is transformed into a velocitycommand vt that corresponds to a vector from the camera position through the image plane at the center of the container, normalizes to a constant target velocity. Intuitively, selecting a container leads the vehicle to fly toward that container in the frame space. A greedy policy can use the P (C | It, at) model to select the action with the lowest probability of collision. We will use \u03c0 (I) = a to designate this policy, so that P\u03c0 (C | I) = P (C | I, \u03c0 (I))).This representation gives the vehicle sufficient freedom to choose any desired navigation direction, ascent and descent to avoid obstacles."}, {"heading": "C. Reinforcing Collision Avoidance", "text": "The initial model can estimate the free space in front of the vehicle, but this does not necessarily directly correspond to the likelihood of a collision (Q = 0 K): The vehicle may be able to maneuver out of the way before hitting an obstacle within 1 meter, or collisions may occur later in the future, even if there is sufficient free space in the current time step, for example due to a narrow cul-de-sac. We therefore use profound reinforcement measures to accurately represent our pre-trained model P (C | It, at), instead of P (l | It, at).For this purpose, we simulate multiple rollouts by flying through a series of training environments using our latest policy. Our score map of M \u00b7 M bins, explained in IIA, determines the space of actions, we consider a total of one M2 actions A = {a1, aM2}, which can be performed after each observation I. We start each episode by placing the agent in a random and random orientation and generating a K-out action."}, {"heading": "D. Network Architecture", "text": "To represent the Q function and the initial free space predictor, use a deep, fully curved neural network based on the VGG16 [30] architecture following [7]. The output score map corresponds to a grid of 41 x 41 containers, which represents the action space for deep reinforcement learning. The network is trained with stochastic gradient descent (SGD), with a cross entropy loss function."}, {"heading": "IV. LEARNING FROM SIMULATION", "text": "In fact, most of them will be able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move,"}, {"heading": "V. EXPERIMENTAL RESULTS", "text": "In order to evaluate the performance of our proposed method, we conducted several experiments and compared them with a number of baselines. First, we explain the details of the setup, the evaluation criteria and the different baselines with which we compare. Then, we present experimental results for two different types of experimental environments. Finally, we discuss experimental results for real flight based on the guidelines obtained with our method."}, {"heading": "A. Experimental Setup", "text": "To do this, we perform continuous episodes that end with the experience of a collision, and count how many steps are taken before a collision takes place. We set the maximum number of steps in each experiment to a fixed number. An episode can begin at any location in the chosen environment, but the choice of starting position can have a major impact on the controller's performance, as some parts of the flight, such as dead ends, sharp curves or doors, can be much more difficult. Therefore, to make an unbiased assessment and test the robustness of the learned strategies, we start each flight from a place randomly selected within the free space of each environment. We use random initialization points and record them throughout all experiments to allow a fair comparison between different methods, including previous work. In the experiments, the quadrotor has a constant speed during each flight, and we change the number of steps in the order in which we represent the percentage of the distance traveled."}, {"heading": "B. Methods", "text": "We compare the performance of our method with previous methods of learning-based avoidance of visual obstacles. We describe the previous methods and basic lines below. 1) Left, right, and straight (LRS) controllers: This method, based on [11], directly predicts the direction of flight from images. Commands are divided into three containers: \"left,\" \"right,\" or \"straight,\" and the predictions are made by a deep revolutionary neural network of raw camera images. Prior to the work, such a model was trained using real-world images collected from three cameras carried manually through forest trails. One camera was directed to the left, one to the right, and one to the left, and the camera images were monitored as right curves, right images as left turns and straight camera images as linear motion. We simulated the same training setup in our training environments, with the cameras 40 degrees away from the forward direction, and used paths to produce the same training data as in our model."}, {"heading": "C. Realistic Environment Evaluation", "text": "This year, it has reached the stage where it will be able to put itself at the forefront in order to pave the way for the future."}, {"heading": "D. Synthetic Environment Test", "text": "This experiment aims to compare (CAD) 2RL in the presence of different corridor geometries, distractions and obstacles in synthetic corridors that differ from those used but are similar in visual style (see Figure 3). Note that the test corridors were deliberately designed to be larger and more challenging. We rendered all images at test time using a random evaluation of 100 different test textures that were not visible during the training. We tested our method and the previous and basic methods under two conditions: in the first state the corridors contained randomly placed furniture and in the second no furniture. Both scenarios have fixtures such as open or closed doors, windows and paintings, but the corridors with furniture represent an additional challenge as the more complex geometry is considerably more elaborate than the scanned corridor used in the previous section, and in the second no furniture. We randomly sampled 100 random locations, which are less challenging places than the intersectional points, as these are consistent in the intersection."}, {"heading": "E. Free Space Prediction Evaluation", "text": "In this experiment, we are interested in how well our predictive model of free space can detect free spaces and obstacles compared to its performance on the synthetic images. To this end, we used the simulator to calculate the mask of free space (FS) / obstacle (O) of 4k rendered images that were uniformly scanned along the corridors. For performance measures, we use precision and Jaccard similarity. Precision shows the ratio of the correctly labeled pixels to \"FS\" or \"O.\" The Jaccard similarity is the intersection of result and soil truth labels for free space and obstacle labels. Table VE summarizes the results obtained. The first line of the table shows the results obtained for the images rendered in our test lanes with test textures. The second line shows the results obtained on the photorealistic images of [17]. Although there is a more realistic gap of 10% accuracy between the performance on the real space and the real space compared to the synthetical ones in the 17."}, {"heading": "F. Real World Flight Experiments", "text": "\"There is no alternative.\" \"There is no alternative.\" \"There is no alternative.\" \"There is no alternative.\" \"There is no alternative.\" \"There is no alternative.\" \"There is no alternative.\" \"There is no alternative.\" \"There is no alternative.\" \"There is no alternative.\" \"There is no alternative.\" \"There is no alternative.\" \"There is no alternative.\" \"There is no alternative.\" \"There is no alternative.\" \"There is no alternative.\" \"There is no alternative.\" \"There is no alternative.\" \"There is no alternative.\" \""}, {"heading": "VI. DISCUSSION AND FUTURE WORK", "text": "In this paper, we presented a method for training deep neural network strategies to avoid obstacles and field sequences, using only simulated monocular RGB images. We described a deep gain learning algorithm that is particularly well suited to this area, which uses multiple branched rollouts to avoid the challenges associated with Q-Function Approximator bootstrapping, resulting in a simple and stable algorithm that is suitable for learning in simulation. We also show that training on randomized simulated scenes produces a model that can successfully fly and avoid obstacles in the real world. Our simulated evaluation also shows that our method outperforms multiple baselines, as well as a previous end-to-end learning method. Our goal in this work is specifically to evaluate the potential of strategies that are fully trained in simulation in order to transfer them to the real world, so that the benefits and limitations of simulated work can be understood."}, {"heading": "ACKNOWLEDGMENT", "text": "This work was made possible by the ONR Young Investi-gator Program Award and the support of Google."}], "references": [{"title": "Autonomous helicopter aerobatics through apprenticeship learning", "author": ["P. Abbeel", "A. Coates", "A.Y. Ng"], "venue": "IJRR", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2010}, {"title": "An application of reinforcement learning to aerobatic helicopter flight", "author": ["P. Abbeel", "A. Coates", "M. Quigley", "A. Ng"], "venue": "NIPS", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2006}, {"title": "Autonomous flight in unstructured and unknown indoor environments", "author": ["A. Bachrach", "R. He", "N. Roy"], "venue": "EMAV", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2009}, {"title": "Pushbroom stereo for high-speed navigation in cluttered environments", "author": ["A.J. Barry", "R. Tedrake"], "venue": "ICRA. IEEE", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2015}, {"title": "Autonomous mav flight in indoor environments using single image perspective cues", "author": ["C. Bills", "J. Chen", "A. Saxena"], "venue": "ICRA", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2011}, {"title": "Monocular vision SLAM for indoor aerial vehicles", "author": ["K. Celik", "S. Chung", "M. Clausman", "A. Somani"], "venue": "IROS", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2009}, {"title": "Semantic image segmentation with deep convolutional nets and fully connected crfs", "author": ["L.-C. Chen", "G. Papandreou", "I. Kokkinos", "K. Murphy", "A.L. Yuille"], "venue": "ICLR", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2015}, {"title": "Efficient reinforcement learning for robots using informative simulated priors", "author": ["M. Cutler", "J.P. How"], "venue": "ICRA", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "Reinforcement learning with multi-fidelity simulators", "author": ["M. Cutler", "T.J. Walsh", "J.P. How"], "venue": "ICRA. IEEE", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2014}, {"title": "Lsd-slam: Large-scale direct monocular slam", "author": ["J. Engel", "T. Sch\u00f6ps", "D. Cremers"], "venue": "ECCV", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2014}, {"title": "J", "author": ["A. Giusti", "J. Guzzi", "D.C. Cire\u015fan", "F.-L. He"], "venue": "P. Rodr\u0131\u0301guez, F. Fontana, M. Faessler, C. Forster, J. Schmidhuber, G. Di Caro, et al. A machine learning approach to visual perception of forest trails for mobile robots. IEEE Robotics and Automation Letters", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2016}, {"title": "RGB-D mapping: Using kinect-style depth cameras for dense 3d modeling of indoor environments", "author": ["P. Henry", "M. Krainin", "E. Herbst", "X. Ren", "D. Fox"], "venue": "International Journal of Robotics Research,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2012}, {"title": "Lsda: Large scale detection through adaptation", "author": ["J. Hoffman", "S. Guadarrama", "E.S. Tzeng", "R. Hu", "J. Donahue", "R. Girshick", "T. Darrell", "K. Saenko"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2014}, {"title": "IM2CAD", "author": ["H. Izadinia", "Q. Shan", "S.M. Seitz"], "venue": "arXiv preprint arXiv:1608.05137", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2016}, {"title": "Deep neural network for real-time autonomous indoor navigation", "author": ["D.K. Kim", "T. Chen"], "venue": "arXiv preprint arXiv:1511.04668", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2015}, {"title": "Parallel tracking and mapping for small ar workspaces", "author": ["G. Klein", "D. Murray"], "venue": "Mixed and Augmented Reality ACM International Symposium on. IEEE", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2007}, {"title": "Automatic loop closure detection using multiple cameras for 3d indoor localization", "author": ["J. Kua", "N. Corso", "A. Zakhor"], "venue": "IS&T/SPIE Electronic Imaging", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2012}, {"title": "Backpropagation applied to handwritten zip code recognition", "author": ["Y. LeCun", "B. Boser", "J.S. Denker", "D. Henderson", "R.E. Howard", "W. Hubbard", "L.D. Jackel"], "venue": "Neural computation", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1989}, {"title": "et al", "author": ["V. Mnih", "K. Kavukcuoglu", "D. Silver", "A.A. Rusu", "J. Veness", "M.G. Bellemare", "A. Graves", "M. Riedmiller", "A.K. Fidjeland", "G. Ostrovski"], "venue": "Human-level control through deep reinforcement learning. Nature", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2015}, {"title": "Vision based control of a quadrotor for perching on planes and lines", "author": ["K. Mohta", "V. Kumar", "K. Daniilidis"], "venue": "ICRA", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep learning helicopter dynamics models", "author": ["A. Punjani", "P. Abbeel"], "venue": "ICRA", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2015}, {"title": "Modified policy iteration algorithms for discounted markov decision problems", "author": ["M.L. Puterman", "M.C. Shin"], "venue": "Management Science", "citeRegEx": "22", "shortCiteRegEx": null, "year": 1978}, {"title": "Playing for data: Ground truth from computer games", "author": ["S.R. Richter", "V. Vineet", "S. Roth", "V. Koltun"], "venue": "arXiv preprint arXiv:1608.02192", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2016}, {"title": "Neural fitted q iteration \u2013 first experiences with a data efficient neural reinforcement learning method", "author": ["M. Riedmiller"], "venue": "European Conference on Machine Learning (ECML)", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2005}, {"title": "Learning monocular reactive uav control in cluttered natural environments", "author": ["S. Ross", "N. Melik-Barkhudarov", "K.S. Shankar", "A. Wendel", "D. Dey", "J.A. Bagnell", "M. Hebert"], "venue": "ICRA. IEEE", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2013}, {"title": "Sim-to-real robot learning from pixels with progressive nets", "author": ["A.A. Rusu", "M. Vecerik", "T. Roth\u00f6rl", "N. Heess", "R. Pascanu", "R. Hadsell"], "venue": "arXiv preprint arXiv:1610.04286", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2016}, {"title": "Stereo vision based indoor/outdoor navigation for flying robots", "author": ["K. Schmid", "T. Tomic", "F. Ruess", "H. Hirschmller", "M. Suppa"], "venue": "IROS", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2013}, {"title": "Vision-based state estimation for autonomous rotorcraft mavs in complex environments", "author": ["S. Shen", "Y. Mulgaonkar", "N. Michael", "V. Kumar"], "venue": "ICRA", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2013}, {"title": "Springer handbook of robotics", "author": ["B. Siciliano", "O. Khatib"], "venue": "Springer Science & Business Media", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2008}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "arXiv preprint arXiv:1409.1556", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2014}, {"title": "Towards adapting deep visuomotor representations from simulated to real environments", "author": ["E. Tzeng", "C. Devin", "J. Hoffman", "C. Finn", "X. Peng", "S. Levine", "K. Saenko", "T. Darrell"], "venue": "arXiv preprint arXiv:1511.07111", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2015}, {"title": "Simultaneous deep transfer across domains and tasks", "author": ["E. Tzeng", "J. Hoffman", "T. Darrell", "K. Saenko"], "venue": "ICCV, pages 4068\u20134076", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2015}, {"title": "Microsoft kinect sensor and its effect", "author": ["Z. Zhang"], "venue": "IEEE multimedia", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2012}], "referenceMentions": [{"referenceID": 2, "context": "Many of the most successful approaches to indoor navigation have used mapping and localization techniques based on 3D perception, including SLAM [3], depth sensors [33], stereo cameras [27],", "startOffset": 145, "endOffset": 148}, {"referenceID": 32, "context": "Many of the most successful approaches to indoor navigation have used mapping and localization techniques based on 3D perception, including SLAM [3], depth sensors [33], stereo cameras [27],", "startOffset": 164, "endOffset": 168}, {"referenceID": 26, "context": "Many of the most successful approaches to indoor navigation have used mapping and localization techniques based on 3D perception, including SLAM [3], depth sensors [33], stereo cameras [27],", "startOffset": 185, "endOffset": 189}, {"referenceID": 5, "context": "and monocular cameras using structure from motion [6].", "startOffset": 50, "endOffset": 53}, {"referenceID": 9, "context": "quire 3D estimation from motion, which remains a challenging open problem despite considerable recent progress [10], [16].", "startOffset": 111, "endOffset": 115}, {"referenceID": 15, "context": "quire 3D estimation from motion, which remains a challenging open problem despite considerable recent progress [10], [16].", "startOffset": 117, "endOffset": 121}, {"referenceID": 4, "context": "In contrast to previous learningbased navigation work [5], our method uses reinforcement learning to obtain supervision that accurately reflects the actual probabilities of collision, instead of separating out obstacle detection and control.", "startOffset": 54, "endOffset": 57}, {"referenceID": 25, "context": "In contrast to prior work on domain adaptation [26], [31], our method does not require even a single real world", "startOffset": 47, "endOffset": 51}, {"referenceID": 30, "context": "In contrast to prior work on domain adaptation [26], [31], our method does not require even a single real world", "startOffset": 53, "endOffset": 57}, {"referenceID": 10, "context": "outperforms several baselines, as well as a prior learningbased method that predicts turning directions [11].", "startOffset": 104, "endOffset": 108}, {"referenceID": 28, "context": "methods for collision-free indoor navigation take a two step approach to the problem: first map out the local environment and determine its geometry, and then compute a collisionfree path for reaching the destination [29].", "startOffset": 217, "endOffset": 221}, {"referenceID": 27, "context": "This approach benefits from independent developments in mapping and localization as well as motion planning [28], [20], [4].", "startOffset": 108, "endOffset": 112}, {"referenceID": 19, "context": "This approach benefits from independent developments in mapping and localization as well as motion planning [28], [20], [4].", "startOffset": 114, "endOffset": 118}, {"referenceID": 3, "context": "This approach benefits from independent developments in mapping and localization as well as motion planning [28], [20], [4].", "startOffset": 120, "endOffset": 123}, {"referenceID": 2, "context": "The 3D geometry of the local environment can be deduced using SLAM with range sensors [3], consumer depth sensors [33], [12], stereo camera pairs [27], as well as monocular cameras", "startOffset": 86, "endOffset": 89}, {"referenceID": 32, "context": "The 3D geometry of the local environment can be deduced using SLAM with range sensors [3], consumer depth sensors [33], [12], stereo camera pairs [27], as well as monocular cameras", "startOffset": 114, "endOffset": 118}, {"referenceID": 11, "context": "The 3D geometry of the local environment can be deduced using SLAM with range sensors [3], consumer depth sensors [33], [12], stereo camera pairs [27], as well as monocular cameras", "startOffset": 120, "endOffset": 124}, {"referenceID": 26, "context": "The 3D geometry of the local environment can be deduced using SLAM with range sensors [3], consumer depth sensors [33], [12], stereo camera pairs [27], as well as monocular cameras", "startOffset": 146, "endOffset": 150}, {"referenceID": 5, "context": "[6].", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "Reconstruction from monocular images is particularly challenging, and despite considerable progress in recent years [10], [16], remains a difficult open problem.", "startOffset": 116, "endOffset": 120}, {"referenceID": 15, "context": "Reconstruction from monocular images is particularly challenging, and despite considerable progress in recent years [10], [16], remains a difficult open problem.", "startOffset": 122, "endOffset": 126}, {"referenceID": 13, "context": "In another recent approach, called IM2CAD, CAD model of a room is generated from a single RGB image [14].", "startOffset": 100, "endOffset": 104}, {"referenceID": 13, "context": "It is worthwhile to mention that, the synthetic data generated by [14] can effectively be used for various robotics simulations.", "startOffset": 66, "endOffset": 70}, {"referenceID": 4, "context": "Learning has previously been used to detect obstacles for indoor flight [5], [15], as well as to directly learn a turn classifier for outdoor forest trail following [11].", "startOffset": 72, "endOffset": 75}, {"referenceID": 14, "context": "Learning has previously been used to detect obstacles for indoor flight [5], [15], as well as to directly learn a turn classifier for outdoor forest trail following [11].", "startOffset": 77, "endOffset": 81}, {"referenceID": 10, "context": "Learning has previously been used to detect obstacles for indoor flight [5], [15], as well as to directly learn a turn classifier for outdoor forest trail following [11].", "startOffset": 165, "endOffset": 169}, {"referenceID": 4, "context": "In contrast to the work of [5], our method directly learns to predict the probability of collision, given an image and a candidate action, without attempting to explicitly detect obstacles.", "startOffset": 27, "endOffset": 30}, {"referenceID": 10, "context": "This is in contrast to the prior work on trail following, which simply predicts the action that will cause the vehicle to follow a trail [11].", "startOffset": 137, "endOffset": 141}, {"referenceID": 10, "context": "Furthermore, unlike [11], our method does not require any human demonstrations or teleoperation at training time.", "startOffset": 20, "endOffset": 24}, {"referenceID": 31, "context": "vision, a number of domain adaptation methods have been proposed that aim to generalize perception systems trained in a source domain into a target domain [32], [13].", "startOffset": 155, "endOffset": 159}, {"referenceID": 12, "context": "vision, a number of domain adaptation methods have been proposed that aim to generalize perception systems trained in a source domain into a target domain [32], [13].", "startOffset": 161, "endOffset": 165}, {"referenceID": 8, "context": "In robotics, simulation to real-world generalization has been addressed using hierarchies of multi-fidelity simulators [9],", "startOffset": 119, "endOffset": 122}, {"referenceID": 7, "context": "priors imposed on Bayesian dynamics models [8].", "startOffset": 43, "endOffset": 46}, {"referenceID": 30, "context": "At the intersection of robotics and computer vision, several works have recently applied domain adaptation techniques to perform transfer for robotic perception systems [31], [26], [25].", "startOffset": 169, "endOffset": 173}, {"referenceID": 25, "context": "At the intersection of robotics and computer vision, several works have recently applied domain adaptation techniques to perform transfer for robotic perception systems [31], [26], [25].", "startOffset": 175, "endOffset": 179}, {"referenceID": 24, "context": "At the intersection of robotics and computer vision, several works have recently applied domain adaptation techniques to perform transfer for robotic perception systems [31], [26], [25].", "startOffset": 181, "endOffset": 185}, {"referenceID": 17, "context": "Our method combines deep neural networks for processing raw camera images [18] with reinforcement learning.", "startOffset": 74, "endOffset": 78}, {"referenceID": 23, "context": "explored in context of Q-iteration [24], and more recently for online Q-learning using temporal-difference algorithms [19].", "startOffset": 35, "endOffset": 39}, {"referenceID": 18, "context": "explored in context of Q-iteration [24], and more recently for online Q-learning using temporal-difference algorithms [19].", "startOffset": 118, "endOffset": 122}, {"referenceID": 6, "context": "analogously to recent work in image segmentation [7].", "startOffset": 49, "endOffset": 52}, {"referenceID": 0, "context": "Since we evaluate every action for each image It, the dataset consists of densely labeled images with labels in the range [0, 1], similarly to the datasets during pretraining, but the labels now correspond to", "startOffset": 122, "endOffset": 128}, {"referenceID": 23, "context": "Our method can be interpreted as a modification of fitted Q-iteration [24], in the sense that we iteratively refit a", "startOffset": 70, "endOffset": 74}, {"referenceID": 21, "context": "Q-function estimator to samples, as well as a variant of modified policy iteration (MPI) [22], in the sense that we estimate Q-values using multi-step rollouts of the current policy.", "startOffset": 89, "endOffset": 93}, {"referenceID": 18, "context": "Unlike conventional RL methods that perform rollouts directly in the test environment [19], we perform rollouts in simulated training hallways.", "startOffset": 86, "endOffset": 90}, {"referenceID": 29, "context": "In order to represent the Q-function and the initial open space predictor, use a deep fully convolutional neural network, built on the VGG16 [30] architecture following [7].", "startOffset": 141, "endOffset": 145}, {"referenceID": 6, "context": "In order to represent the Q-function and the initial open space predictor, use a deep fully convolutional neural network, built on the VGG16 [30] architecture following [7].", "startOffset": 169, "endOffset": 172}, {"referenceID": 0, "context": "Conventionally, learning-based approaches to autonomous flight have relied on learning from demonstration [1], [2], [21], [25].", "startOffset": 106, "endOffset": 109}, {"referenceID": 1, "context": "Conventionally, learning-based approaches to autonomous flight have relied on learning from demonstration [1], [2], [21], [25].", "startOffset": 111, "endOffset": 114}, {"referenceID": 20, "context": "Conventionally, learning-based approaches to autonomous flight have relied on learning from demonstration [1], [2], [21], [25].", "startOffset": 116, "endOffset": 120}, {"referenceID": 24, "context": "Conventionally, learning-based approaches to autonomous flight have relied on learning from demonstration [1], [2], [21], [25].", "startOffset": 122, "endOffset": 126}, {"referenceID": 22, "context": "Our findings in this regard are aligned with the results obtained in other recent works [23], which also used only synthetic renderings to train visual models, but did not explicitly consider wideranging randomization of the training scenes.", "startOffset": 88, "endOffset": 92}, {"referenceID": 10, "context": "1) Left, Right, and Straight (LRS) Controller: This method, based on [11], directly predicts the flight direction", "startOffset": 69, "endOffset": 73}, {"referenceID": 29, "context": "The network was a finetuned VGG16 [30] model, pretrained with ImageNet classification.", "startOffset": 34, "endOffset": 38}, {"referenceID": 16, "context": "to evaluate how well such a model might transfer to a realistic environment, we used a realistic 3D mesh provided by [17].", "startOffset": 117, "endOffset": 121}, {"referenceID": 16, "context": "In [17], the 3D mesh model of a building interior is captured by a backpack system that uses scan-matchingbased localization algorithms combined with an extra stage of image-based alignment for texturing the meshes with", "startOffset": 3, "endOffset": 7}, {"referenceID": 16, "context": "of [17].", "startOffset": 3, "endOffset": 7}, {"referenceID": 16, "context": "Note that our synthetic hallways are much narrower than the real hallways of [17].", "startOffset": 77, "endOffset": 81}, {"referenceID": 16, "context": "This results in smaller free-space areas and larger obstacle areas in the synthetic images compared with [17] where images have more balanced distribution of free-space vs obstacles.", "startOffset": 105, "endOffset": 109}], "year": 2017, "abstractText": "We propose (CAD)RL, a flight controller for Collision Avoidance via Deep Reinforcement Learning that can be used to perform collision-free flight in the real world although it is trained entirely in a 3D CAD model simulator. Our method uses only single RGB images from a monocular camera mounted on the robot as the input and is specialized for indoor hallway following and obstacle avoidance. In contrast to most indoor navigation techniques that aim to directly reconstruct the 3D geometry of the environment, our approach directly predicts the probability of collision given the current monocular image and a candidate action. To obtain accurate predictions, we develop a deep reinforcement learning algorithm for learning indoor navigation, which uses the actual performance of the current policy to construct accurate supervision. The collision prediction model is represented by a deep convolutional neural network that directly processes raw image inputs. Our collision avoidance system is entirely trained in simulation and thus addresses the high sample complexity of deep reinforcement learning and avoids the dangers of trialand-error learning in the real world. By highly randomizing the rendering settings for our simulated training set, we show that we can train a collision predictor that generalizes to new environments with substantially different appearance from the training scenarios. Finally, we evaluate our method in the real world by controlling a real quadrotor flying through real hallways. We demonstrate that our method can perform realworld collision avoidance and hallway following after being trained exclusively on synthetic images, without ever having seen a single real image at the training time. For supplementary video see: https://fsadeghi.github.io/CAD2RL 1Department of Computer Science and Engineering, University of Washington, Seattle, WA 98195 fsadeghi@cs.washington.edu 2Department of Electrical Engineering and Computer Science, University of California, Berkeley, Berkeley, CA 94709 svlevine@eecs.berkeley.edu .", "creator": "LaTeX with hyperref package"}}}