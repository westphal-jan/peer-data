{"id": "1611.04228", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Nov-2016", "title": "Learning Sparse, Distributed Representations using the Hebbian Principle", "abstract": "The \"fire together, wire together\" Hebbian model is a central principle for learning in neuroscience, but surprisingly, it has found limited applicability in modern machine learning. In this paper, we take a first step towards bridging this gap, by developing flavors of competitive Hebbian learning which produce sparse, distributed neural codes using online adaptation with minimal tuning. We propose an unsupervised algorithm, termed Adaptive Hebbian Learning (AHL). We illustrate the distributed nature of the learned representations via output entropy computations for synthetic data, and demonstrate superior performance, compared to standard alternatives such as autoencoders, in training a deep convolutional net on standard image datasets.", "histories": [["v1", "Mon, 14 Nov 2016 02:28:13 GMT  (1453kb,D)", "http://arxiv.org/abs/1611.04228v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["aseem wadhwa", "upamanyu madhow"], "accepted": false, "id": "1611.04228"}, "pdf": {"name": "1611.04228.pdf", "metadata": {"source": "CRF", "title": "Learning Sparse, Distributed Representations using the Hebbian Principle", "authors": ["Aseem Wadhwa", "Upamanyu Madhow"], "emails": ["aseem@ece.ucsb.edu", "madhow@ece.ucsb.edu"], "sections": [{"heading": "1 Introduction", "text": "This year, it is at an all-time high in the history of the European Union."}, {"heading": "2 Related Work", "text": "Much of the work in neuroscience to date on Hebbian Computation has focused on frontend learning (e.g. features such as those in V1 simple cells), which aim to bridge the gap between theoretical and experimental studies of the visual cortex [40, 5], or very simple recognition tasks [16]. However, most of these models are complex in that they include detailed spike timing or lateral connections, and are therefore not well suited to machine learning applications, unlike the abstractions in this paper.AHL is similar to online clustering algorithms that have been extensively studied [39, 3, 10], such as unclear clusters and soft winners-all. However, the focus of this previous literature is on solving the conventional clustering problem, while our goal is to use AHL as a building block for functional extraction in a deep architecture."}, {"heading": "3 AHL: Unsupervised Hebbian Learning", "text": "This year, it is more than ever before in the history of the city in which we find ourselves."}, {"heading": "4 Synthetic data: Activation Code Structure", "text": "Before applying the vMF pdf to a (d \u2212 1) dimensional hypersphere, we can first gain basic insights into the type of code it generates. (1) We use the vMF as a measure of representational power and distribution, and show that AHL is significantly better than clustering. By drawing the synthetic data from a mixture of Mises-Fisher (vMF) distributions, where vMF is the analog distribution of the hypersphere (e.g. the spherical K-mean algorithms can be derived by assuming a mixture of vMF [3]. The vMF pdf is a (d \u2212 1) dimensional hypersphere algorithm by assuming a generative density that is a mixture of vMF [4]."}, {"heading": "5 Unsupervised Image Feature Extraction", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 Architecture", "text": "We use a standard CNN architecture as shown in Fig. 2. There are 3 curved layers interspersed with max pooling layers. Hebbic learning is used to train the weights and distortions of these layers. Layers are learned successively from 1 to 3 in an unattended manner. Training data on each layer consists of N (typically 100 - 200K) normalized activations randomly sampled from the bottom layer. To construct the final features, the 3 intermediate feature maps (Fig. 2) are used (through average pooling and concatenation). To investigate the quality of the representations, these features are used to train a linear SVM for classification. In the first layer, raw image fields are processed as described in [12]."}, {"heading": "5.2 Parameters", "text": "The only parameter that is varied is AT, which acts as a substitute for K, the number of hidden units: If \u03c1T is fixed, the increase of AT results in an increase of K (although K is saturated after one point due to the limitation of the cut back of \u03c1U). In our experiments, we choose AT to obtain a gradually increasing number of characteristics for higher layers (in order of a few hundred), making Ki comparable for the two simulated cases: WTA (Kw = 1) and Soft-WTA (Kw = 3). The filter and pooling quantities depend on the data set and are set next."}, {"heading": "5.3 Datasets", "text": "MNIST [24] is a 10-digit database of 28x28 binary images. We use filter sizes {f1, f2, f3} = {7, 4, 2} and 2 \u00b7 2 max-pooling. However, the size of a neuron's receptive field on the raw image is calculated by projecting its receptive field back onto the underlying layer, and so on, until we reach layer 0 (the input image). In this case, the sizes 7 \u00d7 7, 14 \u00d7 14 and 20 \u00d7 20 pixels for neurons in hidden layers 1, 2 and 3, respectively. An easy way to \"visualize\" a neuron is to display the patches that activate it most. In Fig. 3 (a), we show such top 5 activating patches for randomly selected neurons in layers 1, 2 and 3. It is intuitively pleasant to see neurons in layers that are considered for simple edges in layers 1, epithets, and a combination of higher layers."}, {"heading": "5.4 Results", "text": "The distribution of the active units in the countries, as shown in Fig. 3 (b), is what is expected of the few distributed codes."}, {"heading": "6 Conclusions", "text": "The AHL algorithm presented here represents a radical departure from previous approaches: rather than attempting to minimize a cost function, AHL is able to directly target desirable properties such as scarcity and decoration using neuroplausible mechanisms; the training complexity of AHL is lower than that of autoencoders, while the properties obtained function better in the experiments under consideration here. To the best of our knowledge, this is the first paper in which learning abstractions, which are firmly based on neuroscience models, have been shown to be competitive with modern machine learning techniques. We hope that the promising results reported here will stimulate further research into bridging the gap between neuroscience and machine learning, with the goal of improving our understanding in each realm. As with any uncontrolled approach as we move up the layers, there is a tendency to \"waste\" neurons on modeling functions that are ultimately not informative, with the goal of expanding our understanding in each realm."}], "references": [{"title": "Analyzing the performance of multilayer neural networks for object recognition", "author": ["P. Agrawal", "R. Girshick", "J. Malik"], "venue": "In Computer Vision\u2013ECCV", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2014}, {"title": "A framework for machine vision based on neuro-mimetic front end processing and clustering", "author": ["E. Akbas", "A. Wadhwa", "M. Eckstein", "U. Madhow"], "venue": "In Communication, Control, and Computing (Allerton),", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2014}, {"title": "Frequency-sensitive competitive learning for scalable balanced clustering on high-dimensional hyperspheres", "author": ["A. Banerjee", "J. Ghosh"], "venue": "Neural Networks, IEEE Transactions on,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2004}, {"title": "Cerebral cortex as model builder", "author": ["H. Barlow"], "venue": "In Matters of Intelligence,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1987}, {"title": "Hebbian learning of the statistical and geometrical structure of visual input", "author": ["J.A. Bednar"], "venue": "In Neuromathematics of Vision,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2014}, {"title": "Greedy layer-wise training of deep networks", "author": ["Y. Bengio", "P. Lamblin", "D. Popovici", "H. Larochelle"], "venue": "Advances in neural information processing systems,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2007}, {"title": "Art 2: Self-organization of stable category recognition codes for analog input patterns", "author": ["G.A. Carpenter", "S. Grossberg"], "venue": "Applied optics,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1987}, {"title": "The art of adaptive pattern recognition by a self-organizing neural network", "author": ["G.A. Carpenter", "S. Grossberg"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1988}, {"title": "Marginalized denoising auto-encoders for nonlinear representations", "author": ["M. Chen", "K.Q. Weinberger", "F. Sha", "Y. Bengio"], "venue": "In Proceedings of the 31st International Conference on Machine Learning,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2014}, {"title": "Fuzzy competitive learning", "author": ["F.L. Chung", "T. Lee"], "venue": "Neural Networks,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1994}, {"title": "Selecting receptive fields in deep networks", "author": ["A. Coates", "A.Y. Ng"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2011}, {"title": "An analysis of single-layer networks in unsupervised feature learning", "author": ["A. Coates", "A.Y. Ng", "H. Lee"], "venue": "In International conference on artificial intelligence and statistics,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2011}, {"title": "Nonlinear hebbian learning as a universal principle in unsupervised feature learning", "author": ["C. De Brito", "W. Gerstner"], "venue": "In Deep Learning Workshop ICML\u201915,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2015}, {"title": "What is the goal of sensory coding", "author": ["D.J. Field"], "venue": "Neural computation,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1994}, {"title": "Forming sparse representations by local anti-hebbian learning", "author": ["P. F\u00f6ldiak"], "venue": "Biological cybernetics,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1990}, {"title": "Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position", "author": ["K. Fukushima"], "venue": "Biological cybernetics,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1980}, {"title": "Measuring invariances in deep networks. In Advances in neural information processing", "author": ["I. Goodfellow", "H. Lee", "Q.V. Le", "A. Saxe", "A.Y. Ng"], "venue": null, "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2009}, {"title": "Adaptive pattern classification and universal recoding: I. parallel development and coding of neural feature detectors", "author": ["S. Grossberg"], "venue": "Biological cybernetics,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1976}, {"title": "The organisation of behaviour: a neuropsychological theory", "author": ["D.O. Hebb"], "venue": null, "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1952}, {"title": "movmf: An r package for fitting mixtures of von mises-fisher distributions", "author": ["K. Hornik", "B. Gr\u00fcn"], "venue": "http://CRAN.R-project.org/package=movMF,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2014}, {"title": "Semi-supervised learning with deep generative models", "author": ["D.P. Kingma", "S. Mohamed", "D.J. Rezende", "M. Welling"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2014}, {"title": "Self-organized formation of topologically correct feature maps", "author": ["T. Kohonen"], "venue": "Biological cybernetics,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 1982}, {"title": "Learning multiple layers of features from tiny", "author": ["A. Krizhevsky", "G. Hinton"], "venue": null, "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2009}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 1998}, {"title": "Learning methods for generic object recognition with invariance to pose and lighting", "author": ["Y. LeCun", "F.J. Huang", "L. Bottou"], "venue": "In Computer Vision and Pattern Recognition,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2004}, {"title": "Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations", "author": ["H. Lee", "R. Grosse", "R. Ranganath", "A.Y. Ng"], "venue": "In Proceedings of the 26th Annual International Conference on Machine Learning,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2009}, {"title": "Auxiliary deep generative models", "author": ["L. Maal\u00f8e", "C.K. S\u00f8nderby", "S.K. S\u00f8nderby", "O. Winther"], "venue": "arXiv preprint arXiv:1602.05473,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2016}, {"title": "A winner-take-all method for training sparse convolutional autoencoders", "author": ["A. Makhzani", "B. Frey"], "venue": "arXiv preprint arXiv:1409.2752,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2014}, {"title": "Sparse autoencoder", "author": ["A. Ng"], "venue": "CS294A Lecture notes,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2011}, {"title": "Simplified neuron model as a principal component analyzer", "author": ["E. Oja"], "venue": "Journal of mathematical biology,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 1982}, {"title": "Principles of image representation in visual cortex", "author": ["B.A. Olshausen"], "venue": "The visual neurosciences,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2003}, {"title": "Emergence of simple-cell receptive field properties by learning a sparse code for natural images", "author": ["B.A. Olshausen", "D.J. Field"], "venue": null, "citeRegEx": "32", "shortCiteRegEx": "32", "year": 1996}, {"title": "Unsupervised learning of invariant feature hierarchies with applications to object recognition", "author": ["M.A. Ranzato", "F.J. Huang", "Y.-L. Boureau", "Y. LeCun"], "venue": "In Computer Vision and Pattern Recognition,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2007}, {"title": "Development of the vertebrate neuromuscular junction", "author": ["J.R. Sanes", "J.W. Lichtman"], "venue": "Annual review of neuroscience,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 1999}, {"title": "Optimal unsupervised learning in a single-layer linear feedforward neural network", "author": ["T.D. Sanger"], "venue": "Neural networks,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 1989}, {"title": "Competitive hebbian learning through spike-timingdependent synaptic plasticity", "author": ["S. Song", "K.D. Miller", "L.F. Abbott"], "venue": "Nature neuroscience,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2000}, {"title": "Extracting and composing robust features with denoising autoencoders", "author": ["P. Vincent", "H. Larochelle", "Y. Bengio", "P.-A. Manzagol"], "venue": "In Proceedings of the 25th international conference on Machine learning,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2008}, {"title": "The concave-convex procedure", "author": ["A.L. Yuille", "A. Rangarajan"], "venue": "Neural computation,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2003}, {"title": "Efficient online spherical k-means clustering", "author": ["S. Zhong"], "venue": "In Neural Networks,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2005}, {"title": "A sparse coding model with synaptically local plasticity and spiking neurons can account for the diverse shapes of v1 simple cell receptive fields", "author": ["J. Zylberberg", "J.T. Murphy", "M.R. DeWeese"], "venue": "PLoS Comput Biol,", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2011}], "referenceMentions": [{"referenceID": 18, "context": "The \u201cfire together, wire together\u201d Hebbian principle [19], in which synaptic weights connecting a pair of highly activated pre- and post-synaptic neurons are strengthened, has been the centerpiece in several prominent neuroscientific models of learning developed over the decades, such as self-organizing maps [22] and adaptive resonance theory (ART models) [8].", "startOffset": 53, "endOffset": 57}, {"referenceID": 21, "context": "The \u201cfire together, wire together\u201d Hebbian principle [19], in which synaptic weights connecting a pair of highly activated pre- and post-synaptic neurons are strengthened, has been the centerpiece in several prominent neuroscientific models of learning developed over the decades, such as self-organizing maps [22] and adaptive resonance theory (ART models) [8].", "startOffset": 310, "endOffset": 314}, {"referenceID": 7, "context": "The \u201cfire together, wire together\u201d Hebbian principle [19], in which synaptic weights connecting a pair of highly activated pre- and post-synaptic neurons are strengthened, has been the centerpiece in several prominent neuroscientific models of learning developed over the decades, such as self-organizing maps [22] and adaptive resonance theory (ART models) [8].", "startOffset": 358, "endOffset": 361}, {"referenceID": 39, "context": "More recently, it continues to be used in modeling the development of cortical maps [40, 5].", "startOffset": 84, "endOffset": 91}, {"referenceID": 4, "context": "More recently, it continues to be used in modeling the development of cortical maps [40, 5].", "startOffset": 84, "endOffset": 91}, {"referenceID": 12, "context": "To date, however, these ideas have not been translated into practical machine learning algorithms, although there is renewed interest in this goal; for example, recent work in [13] argues for Hebbian learning as a universal principle for feature learning.", "startOffset": 176, "endOffset": 180}, {"referenceID": 17, "context": "Prior computational models for Hebbian learning show that, by combining it with components such as competition and inhibition, a variety of well-known representations can be obtained, such as clustering (K-means), Principal Component Analysis (PCA), and sparse coding [18, 35, 15].", "startOffset": 268, "endOffset": 280}, {"referenceID": 34, "context": "Prior computational models for Hebbian learning show that, by combining it with components such as competition and inhibition, a variety of well-known representations can be obtained, such as clustering (K-means), Principal Component Analysis (PCA), and sparse coding [18, 35, 15].", "startOffset": 268, "endOffset": 280}, {"referenceID": 14, "context": "Prior computational models for Hebbian learning show that, by combining it with components such as competition and inhibition, a variety of well-known representations can be obtained, such as clustering (K-means), Principal Component Analysis (PCA), and sparse coding [18, 35, 15].", "startOffset": 268, "endOffset": 280}, {"referenceID": 13, "context": "Several works, spanning both neuroscience [14, 15, 31] and machine learning [1] areas, have identified these core properties as indicators of efficient representations learned in the lower and middle layers of deep architectures.", "startOffset": 42, "endOffset": 54}, {"referenceID": 14, "context": "Several works, spanning both neuroscience [14, 15, 31] and machine learning [1] areas, have identified these core properties as indicators of efficient representations learned in the lower and middle layers of deep architectures.", "startOffset": 42, "endOffset": 54}, {"referenceID": 30, "context": "Several works, spanning both neuroscience [14, 15, 31] and machine learning [1] areas, have identified these core properties as indicators of efficient representations learned in the lower and middle layers of deep architectures.", "startOffset": 42, "endOffset": 54}, {"referenceID": 0, "context": "Several works, spanning both neuroscience [14, 15, 31] and machine learning [1] areas, have identified these core properties as indicators of efficient representations learned in the lower and middle layers of deep architectures.", "startOffset": 76, "endOffset": 79}, {"referenceID": 39, "context": ", of features like those observed in V1 simple cells) aimed at bridging the gap between theoretical and experimental studies of the visual cortex [40, 5], or on very simple recognition tasks [16].", "startOffset": 146, "endOffset": 153}, {"referenceID": 4, "context": ", of features like those observed in V1 simple cells) aimed at bridging the gap between theoretical and experimental studies of the visual cortex [40, 5], or on very simple recognition tasks [16].", "startOffset": 146, "endOffset": 153}, {"referenceID": 15, "context": ", of features like those observed in V1 simple cells) aimed at bridging the gap between theoretical and experimental studies of the visual cortex [40, 5], or on very simple recognition tasks [16].", "startOffset": 191, "endOffset": 195}, {"referenceID": 38, "context": "AHL is similar to online clustering algorithms which have been studied extensively [39, 3, 10], such as fuzzy clustering and soft winner-take-all.", "startOffset": 83, "endOffset": 94}, {"referenceID": 2, "context": "AHL is similar to online clustering algorithms which have been studied extensively [39, 3, 10], such as fuzzy clustering and soft winner-take-all.", "startOffset": 83, "endOffset": 94}, {"referenceID": 9, "context": "AHL is similar to online clustering algorithms which have been studied extensively [39, 3, 10], such as fuzzy clustering and soft winner-take-all.", "startOffset": 83, "endOffset": 94}, {"referenceID": 11, "context": "A single layer of K-means with a large number of features (\u223c 4000) was shown to be effective for feature extraction in [12].", "startOffset": 119, "endOffset": 123}, {"referenceID": 10, "context": "Multi-layer K-means followed by SVM has also been used in [11, 2].", "startOffset": 58, "endOffset": 65}, {"referenceID": 1, "context": "Multi-layer K-means followed by SVM has also been used in [11, 2].", "startOffset": 58, "endOffset": 65}, {"referenceID": 0, "context": "In contrast to the heavily correlated K-means centers, due to our introduction of synaptic competition we are able to produce sparse, distributed codes, similar to the empirically observed characteristics of high-performing supervised deep nets [1].", "startOffset": 245, "endOffset": 248}, {"referenceID": 31, "context": "Existing unsupervised learning approaches such as sparse coding [32] and autoencoders [37, 17] seek to optimize a cost function which combines reconstruction error with some form of regularization.", "startOffset": 64, "endOffset": 68}, {"referenceID": 36, "context": "Existing unsupervised learning approaches such as sparse coding [32] and autoencoders [37, 17] seek to optimize a cost function which combines reconstruction error with some form of regularization.", "startOffset": 86, "endOffset": 94}, {"referenceID": 16, "context": "Existing unsupervised learning approaches such as sparse coding [32] and autoencoders [37, 17] seek to optimize a cost function which combines reconstruction error with some form of regularization.", "startOffset": 86, "endOffset": 94}, {"referenceID": 5, "context": "Unsupervised feature learning was originally employed for initializing deep nets prior to finetuning [6], but was soon observed not to offer substantial advantages over carefully scaled random initializations.", "startOffset": 101, "endOffset": 104}, {"referenceID": 20, "context": "However, while the latter represent the current state of the art in classification, deep generative modeling continues to be an active area of research [21, 27], with the goal of reducing dependence on labels, and our results motivate further investigation into Hebbian algorithms as building blocks for semi-supervised learning.", "startOffset": 152, "endOffset": 160}, {"referenceID": 26, "context": "However, while the latter represent the current state of the art in classification, deep generative modeling continues to be an active area of research [21, 27], with the goal of reducing dependence on labels, and our results motivate further investigation into Hebbian algorithms as building blocks for semi-supervised learning.", "startOffset": 152, "endOffset": 160}, {"referenceID": 29, "context": ", N} [30], while a simple modification produces convergence to the top K eigenvectors [35].", "startOffset": 5, "endOffset": 9}, {"referenceID": 34, "context": ", N} [30], while a simple modification produces convergence to the top K eigenvectors [35].", "startOffset": 86, "endOffset": 90}, {"referenceID": 17, "context": "When h(z) = sign(z) and the weights afferent to only the highest activated neuron are updated, a strategy known as the WTA (winner take all), the resulting weights converge to cluster centers, and we get an online version of spherical K-means [18, 39].", "startOffset": 243, "endOffset": 251}, {"referenceID": 38, "context": "When h(z) = sign(z) and the weights afferent to only the highest activated neuron are updated, a strategy known as the WTA (winner take all), the resulting weights converge to cluster centers, and we get an online version of spherical K-means [18, 39].", "startOffset": 243, "endOffset": 251}, {"referenceID": 12, "context": "Other modifications of Hebbian learning can lead to online solutions for other cost functions such as ICA, sparse coding etc [13], but that typically requires introduction of lateral connections, which slows down inference.", "startOffset": 125, "endOffset": 129}, {"referenceID": 13, "context": "\u2022 Sparse, Distributed Code: such a code arises when each post-synaptic neuron has a low probability to fire and the stimulus is forced to be encoded in the activity of a few neurons [14].", "startOffset": 182, "endOffset": 186}, {"referenceID": 14, "context": "This kind of code is a compromise between local (compact code) and totally distributed representations (when there is a single grandmother active neuron) [15].", "startOffset": 154, "endOffset": 158}, {"referenceID": 30, "context": "It has been argued that sparse distributed codes disentangle the causes leading to meaningful representations and also present a pattern that is easier for higher stages of the system to model [31].", "startOffset": 193, "endOffset": 197}, {"referenceID": 0, "context": "The work in [1] presents several empirical studies discovering the sparse and distributed nature of codes in the middle layers of a deep backpropagation trained network.", "startOffset": 12, "endOffset": 15}, {"referenceID": 14, "context": "Decorrelation forces neurons to learn different features and is an important component in most neuroscientific models [15, 40, 5].", "startOffset": 118, "endOffset": 129}, {"referenceID": 39, "context": "Decorrelation forces neurons to learn different features and is an important component in most neuroscientific models [15, 40, 5].", "startOffset": 118, "endOffset": 129}, {"referenceID": 4, "context": "Decorrelation forces neurons to learn different features and is an important component in most neuroscientific models [15, 40, 5].", "startOffset": 118, "endOffset": 129}, {"referenceID": 3, "context": "However, decorrelation is different from orthogonality: neurons still need to capture the \u201csuspicious coincidences\u201d that define objects [4].", "startOffset": 136, "endOffset": 139}, {"referenceID": 39, "context": "This approach has been adopted in several papers building biologically plausible models [40, 5] which conform to the observation that neurons tend to have low mean firing rates that span a small range of values.", "startOffset": 88, "endOffset": 95}, {"referenceID": 4, "context": "This approach has been adopted in several papers building biologically plausible models [40, 5] which conform to the observation that neurons tend to have low mean firing rates that span a small range of values.", "startOffset": 88, "endOffset": 95}, {"referenceID": 16, "context": "Sparse autoencoders [17] also include a penalty term for the mean firing rate of the neurons.", "startOffset": 20, "endOffset": 24}, {"referenceID": 6, "context": "It is worth noting that the idea of recruiting neurons adaptively was proposed in Grossberg\u2019s ART2 model [7] decades ago, but our system architecture and the specifics of neuron recruitment, as well as the inclusion of pruning, is different.", "startOffset": 105, "endOffset": 108}, {"referenceID": 33, "context": "observed in biological studies [34][36].", "startOffset": 31, "endOffset": 35}, {"referenceID": 35, "context": "observed in biological studies [34][36].", "startOffset": 35, "endOffset": 39}, {"referenceID": 8, "context": "In addition to producing a more distributed code, it also leads to increased sparsity in the weights, often seen in supervised deep nets, and also recently reported for marginalized denoising autoencoders in [9].", "startOffset": 208, "endOffset": 211}, {"referenceID": 29, "context": "The decay term can be derived in a manner similar to the one discussed in [30], it is useful for improving the computation speed.", "startOffset": 74, "endOffset": 78}, {"referenceID": 2, "context": ", the spherical K-means algorithm can be derived by assuming a generating density which is a mixture of vMF [3]).", "startOffset": 108, "endOffset": 111}, {"referenceID": 19, "context": ", 5000} is generated using a mixture of five vMF distributions, each generating 1000 IID samples in R using the sampling procedure described in [20].", "startOffset": 144, "endOffset": 148}, {"referenceID": 38, "context": "For comparison, we also run standard batch spherical K-means algorithm (SPKM) [39], initialized with centers randomly drawn from the dataset.", "startOffset": 78, "endOffset": 82}, {"referenceID": 37, "context": "While this is non-convex due to the L2 constraint, we are able to obtain good solutions using \u201cconvex-concave\u201d sequence convex programming (SCP) [38].", "startOffset": 145, "endOffset": 149}, {"referenceID": 11, "context": "At the first layer, raw image patches are processed as described in [12].", "startOffset": 68, "endOffset": 72}, {"referenceID": 23, "context": "MNIST [24] is a 10-digit database with 28x28 binary images.", "startOffset": 6, "endOffset": 10}, {"referenceID": 22, "context": "CIFAR [23] is a dataset of tiny 32\u00d7 32 color images belonging to 10 classes (e.", "startOffset": 6, "endOffset": 10}, {"referenceID": 24, "context": "NORB [25], uniform-normalized, is a synthetic dataset of 5 classes of toys photographed under varying lighting and azimuth conditions.", "startOffset": 5, "endOffset": 9}, {"referenceID": 36, "context": "We replace AHL by an autoencoder in each layer in order to compare against both denoising autoencoders (DAE) [37] (hyperparameters: batch size, noise level, learning rate) and sparse autoencoders (SAE) [29, 17] (hyperparameters: weight decay, sparsity penalty coefficient, target activation).", "startOffset": 109, "endOffset": 113}, {"referenceID": 28, "context": "We replace AHL by an autoencoder in each layer in order to compare against both denoising autoencoders (DAE) [37] (hyperparameters: batch size, noise level, learning rate) and sparse autoencoders (SAE) [29, 17] (hyperparameters: weight decay, sparsity penalty coefficient, target activation).", "startOffset": 202, "endOffset": 210}, {"referenceID": 16, "context": "We replace AHL by an autoencoder in each layer in order to compare against both denoising autoencoders (DAE) [37] (hyperparameters: batch size, noise level, learning rate) and sparse autoencoders (SAE) [29, 17] (hyperparameters: weight decay, sparsity penalty coefficient, target activation).", "startOffset": 202, "endOffset": 210}, {"referenceID": 25, "context": "82% using deep belief networks [26], 0.", "startOffset": 31, "endOffset": 35}, {"referenceID": 32, "context": "64% [33] using unsupervised features fed to a supervised two layer NN, NORB: 2.", "startOffset": 4, "endOffset": 8}, {"referenceID": 1, "context": "52% [2] using finer pooling (over 5x5 regions instead of quadrants), CIFAR: 80% [12], 82% [11], 80.", "startOffset": 4, "endOffset": 7}, {"referenceID": 11, "context": "52% [2] using finer pooling (over 5x5 regions instead of quadrants), CIFAR: 80% [12], 82% [11], 80.", "startOffset": 80, "endOffset": 84}, {"referenceID": 10, "context": "52% [2] using finer pooling (over 5x5 regions instead of quadrants), CIFAR: 80% [12], 82% [11], 80.", "startOffset": 90, "endOffset": 94}, {"referenceID": 27, "context": "1% [28] accuracy, using large number of feature maps (> 4k).", "startOffset": 3, "endOffset": 7}, {"referenceID": 1, "context": "We note that our performance improves further by incorporating some of the techniques reported in earlier work, giving error rates of 2\u2212 3% for NORB by finer pooling as in [2], and accuracy of up to 80% for CIFAR by increasing K1 as in [12].", "startOffset": 172, "endOffset": 175}, {"referenceID": 11, "context": "We note that our performance improves further by incorporating some of the techniques reported in earlier work, giving error rates of 2\u2212 3% for NORB by finer pooling as in [2], and accuracy of up to 80% for CIFAR by increasing K1 as in [12].", "startOffset": 236, "endOffset": 240}], "year": 2016, "abstractText": "The \u201cfire together, wire together\u201d Hebbian model is a central principle for learning in neuroscience, but surprisingly, it has found limited applicability in modern machine learning. In this paper, we take a first step towards bridging this gap, by developing flavors of competitive Hebbian learning which produce sparse, distributed neural codes using online adaptation with minimal tuning. We propose an unsupervised algorithm, termed Adaptive Hebbian Learning (AHL). We illustrate the distributed nature of the learned representations via output entropy computations for synthetic data, and demonstrate superior performance, compared to standard alternatives such as autoencoders, in training a deep convolutional net on standard image datasets.", "creator": "LaTeX with hyperref package"}}}