{"id": "1611.04233", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Nov-2016", "title": "A New Recurrent Neural CRF for Learning Non-linear Edge Features", "abstract": "Conditional Random Field (CRF) and recurrent neural models have achieved success in structured prediction. More recently, there is a marriage of CRF and recurrent neural models, so that we can gain from both non-linear dense features and globally normalized CRF objective. These recurrent neural CRF models mainly focus on encode node features in CRF undirected graphs. However, edge features prove important to CRF in structured prediction. In this work, we introduce a new recurrent neural CRF model, which learns non-linear edge features, and thus makes non-linear features encoded completely. We compare our model with different neural models in well-known structured prediction tasks. Experiments show that our model outperforms state-of-the-art methods in NP chunking, shallow parsing, Chinese word segmentation and POS tagging.", "histories": [["v1", "Mon, 14 Nov 2016 02:48:46 GMT  (94kb)", "http://arxiv.org/abs/1611.04233v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["shuming ma", "xu sun"], "accepted": false, "id": "1611.04233"}, "pdf": {"name": "1611.04233.pdf", "metadata": {"source": "CRF", "title": "A New Recurrent Neural CRF for Learning Non-linear Edge Features", "authors": ["Shuming Ma", "Xu Sun"], "emails": ["xusun}@pku.edu.cn"], "sections": [{"heading": null, "text": "ar Xiv: 161 1,04 233v 1 [cs.C L] 14 Nov 2"}, {"heading": "Introduction", "text": "It is a neumetric, hacu-eaJrh-eaJngr-eaJngr-eaJngr-eaJngr-eaJngr-eaJngr-eaJngr-eaJngr-eaJngr-eaJngr-eaJngr-eaeaJngr-eaeaJngr-eaJngr-eaJngr-eaeaeaJngr-eaJngr-eaeaeaJngr-eaeaeJngr-eaeaeaJngr-eaeaJngr-eaJngr-eaJngr-eaaJngr-eaeaJngr-eaeaeaeaJngr-eaeaeaeJngr-eaeaeaeaeJngr-eaeaeaeJngr-eaeaeaeJngr-eaeaeaeaeaJngr-Jngr-Jngr-eaeaeaeaeaeaeaeaJngr-Jngr-Jngr-eaeaeaeaeaeaeaeaJngr-Jngr-Jngr-Jngr-eaeaeaeaeaeaeaeaeaeaeaJngr-Jngr-Jngr-Jngr-Jngr-Jngr-eaeaeaeaeaeaeaeaeaeaeaJngr-Jngr-Jngr-Jngr-Jngr-Jngr-eaeaeaeaeaeaeaeaeaeaeaeaeaJngr-Jng"}, {"heading": "Background", "text": "In the structured prediction, our goal is to predict the structure y taking into account the observations x. The ith label in the structure y is referred to as yi, and the ith observation is xi.CRF (Lafferty, McCallum and Pereira 2001) is a popular and effective algorithm for structured prediction. It has a log-linear conditional probability in relation to energy functions via local cliques and transition liquids: log (p (y | x)). The results of iElocal (yi, x, i) + \u2211 iEtrans (yi \u2212 1, yi, x, i) (1), where Elocal (yi, x, i) -cliquator is energy function via local cliques at position i, and Etrans (yi \u2212 yi, i) is energy function via transition liquids. The energy functions are used to learn properties of CRF."}, {"heading": "Proposal", "text": "Do and Artieres (2010) show that nonlinear energy functions are better at extracting characteristics for structured predictions. For a nonlinear energy function, we propose a new recurring neural CRF that uses LSTM as an energy function over transition liquids. Therefore, our model is able to learn nonlinear marginal functions."}, {"heading": "Edge Embedding", "text": "In natural language processing, the input structure usually consists of a sequence of words, so the edges of the input structure are connections of adjacent words. We have three methods to create edge embedding from the input structure. Bigram: Bigram embedding is an intuitive method to contain adjacent word characteristics. We can create a Bigram dictionary and assign a vector to each key. It is efficient in several models (Pei, Ge and Chang 2014; Chen et al. 2015), but can suffer from scarcity and low training speed. Concatenation: Concatenation is a useful method to combine information from two words. It is simple and is widely used in previous work (Collobert et al. 2011; Huang, Xu and Yu 2015)."}, {"heading": "Layers", "text": "Figure 1 shows our proposed Recurrent Neural CRF model. Our model contains three layers: input layer, LSTM layer, and CRF layer. Input layer: input layer is used to type words and provide edge embedding for the LSTM layer. Edge embedding occurs by concatenating adjacent word vectors, and it provides raw primary edge properties. LSTM layer: LSTM layer repeatedly enters edge embedding from the input layer and calculates the output as an energy function via transition liquids for the CRF layer. Our LSTM layer normalizes energy output (using the Softmax function) only in the CRF layer. Therefore, our model is gobally normalized, which can solve the label bias problem (Andor et al. 2016). CRF layer: CRF layer should find the output structure (using the Softmax function) only in the CRF layer. Therefore, our model gobally normalizes, which can solve the label bias problem (Andor et al. 2016). CRF layer: CRF layer is designed to find the output structure (using the Softmax function) only in the CRF layer."}, {"heading": "Objective function", "text": "In our model, the objective function is similar to the CRF lens and enables the calculation of gradients using dynamic programming. To learn non-linear properties, we replace the linear energy function with the LSTM output function: p (y | x).exp (\u2211 iti [yi \u2212 1, yi] + \u2211 iElocal (yi, x, i) (7) ti = W (s) hi (8), where ti [yi \u2212 1, yi] is the LSTM energy output, the hidden edge information.The lens has a non-linear transition energy function, which has neither conventional CRF nor LSTM-CRF. The local energy function can be either linear or non-linear. We refer to our model with linear local energy function Edge-based1 and the model with non-linear energy function Edgebased-2. Edge-based edge-based edge-CRF model (1: linear edge-based Edge-1 is the single energy function)."}, {"heading": "Training", "text": "Probabilistic criteria: Probabilistic criteria were first proposed in (Lafferty, McCallum and Pereira 2001).The regularized objective function of recurring neuronal CRF can be described as follows: L (\u03b8) = m \u00b2 j = 1Rj (\u03b8) + \u03bb2 (13), where m is the number of samples in the corpus.We refer to the unnormalized score of a sample for recurring neuronal CRF as: F (xj, yj, \u03b8) = sequential iti [yi \u2212 1, yi] + [yi] (14) And this score for the edge-based model is: F (xj, yj, \u03b8) =."}, {"heading": "Optimization", "text": "To minimize the objective function, we use AdaGrad (Duchi, Hazan, and Singer 2011), a recently widely used algorithm. The parameter \u03b8i for the test update can be calculated as follows: \u03b8t, i = \u03b8t \u2212 1, i \u2212 \u03b1 \u221a t\u03c4 = 1 g 2 \u03c4, igt, i (19), where \u03b1 is the initial learning rate and gt, i is the gradient of the parameter \u03b8i for the tallest update."}, {"heading": "Related Work", "text": "Recently, neural network models have been widely used in natural language processing (Bengio et al. 2003; Mikolov et al. 2010; Socher et al. 2013; Chen et al. 2015; Sun 2016). Among various neural models, recurrent neural networks (Elman 1990) perform well in sequence marking tasks. LSTM (Hochreiter and Schmidhuber 1997; Graves and Schmidhuber 2005) improves the performance of RNN by solving the problem of dwindling and exploding gradients. Later, it is proposed to combine the bi-directional reciprocal model (Graves, Mohamed, and Hinton 2013) with retrograde information. CRF model (Lafferty, McCallum, and Pereira 2001) has achieved much success in natural language processing. Many models attempt to combine CRF with neural networks for more structure dependence (Graves, Mohamed, and Hinton 2013). Peng et al. (2009) introduces a neural conditional model."}, {"heading": "Experiments", "text": "We conduct some experiments to analyze our proposed models. To ensure that our test results are comparable, we use familiar sequence marking tasks such as NP chunking, shallow parsing, POS tagging, and Chinese word segmentation as benchmarks. We compare our model with other common neural models and analyze the effect of nonlinear edge features."}, {"heading": "Tasks", "text": "We present our benchmark tasks as follows: NP Chunking: NP Chunking is short for Noun Phrase Chunking, i.e. the non-recursive core of noun phrases referred to as based NPs. Our data sets are taken from CoNLL-2000, a flat analyzing common task consisting of 8936 sentences in the training set and 2012 sentences in the test set. Furthermore, we divide the training set and extract 90% sentences as a development set. After previous work, we must label the sentences with the BIO2 format, including 3 tags (B-NP, I-NP, O). Our assessment metric is F-Score. We use F-Score: Flat analysis is a task similar to NP Chunking, but it must identify all chunk types (VP, PP, DT...). The data set is also taken from CoNLL-2000, and it contains 23 tags. We use F-Score as a valuation metric POS, but it must identify all chunk types (VP, DT...)."}, {"heading": "Embeddings", "text": "Embeddings are distributed vectors that represent the semantics of words (Bengio et al. 2003; Mikolov et al. 2013). It proves that embeddings can affect the performance of neural models. In our models, we use random initialized word embeddings as well as Senna embeddings (Collobert et al. 2011). Our experiments show that Senna embeddings can slightly improve the performance of our models. We also incorporate the embeddings of features suggested by previous work (Collobert et al. 2011). Characteristics include a window with the last 2 words and the next 2 words, and the word with up to 2 characters. We also use part-of-speech tags in NP chunking and shallow parsing. To alleviate heavy feature engineering, we do not use other features such as Bigram or Trigram, although they can increase accuracy as shown in (Pei, Ge and Chang 2014 and al)."}, {"heading": "Settings", "text": "Our model does not react sensitively to the dimension of hidden states when it is large enough. To balance accuracy and time, we set this number to 300 for NP chunking and shallow parsing, and the number is 200 for POS tagging and Chinese word segmentation. The dimension of input embedding is said to be 100. The initial learning rate of the AdaGrad algorithm is 0.1, and the regulation parameter is 10 \u2212 6. The drop-out method proves to be suitable for avoiding conformity in neural models (Srivastava et al. 2014), but we find that it has limited influence in our models. We also select probability criteria to train our model for its constant convergence and robust performance."}, {"heading": "Baselines", "text": "We choose common neural models, including RNN, LSTM, BiLSTM and BiLSTM-CRF. RNN and LSTM are basic recursive neural models. To learn more bidirectional context information, we also implement Bi-LSTM for our tasks. We compare our model with this model to show the benefits of combining neural model and CRF target. Finally, BiLSTM-CRF is our strong baseline. We compare our model with BiLSTM-CRF to show that learning nonlinear edge features is more important than individual nonlinear node features."}, {"heading": "Results", "text": "This year it has come to the point where we will be able to put ourselves at the top, \"he said.\" We have to put ourselves at the top, \"he said.\" We are able to put ourselves at the top, \"he said.\" We have to put ourselves at the top, \"he said.\" We have to put ourselves at the top. \""}, {"heading": "Significance Tests", "text": "The significance tests indicate that our edge-based 1 model shows a very significant improvement over baseline, with p \u2264 0.004 for NP chunking, p \u2264 0.007 for flat parsing, and p \u2264 0.0001 for POS tagging and Chinese word segmentation. The Edgebased 2 model also has a high statistical significance, with p \u2264 0.0001 for all tasks. The significance tests support the theoretical analysis that our models can exceed baseline accuracy."}, {"heading": "Conclusions", "text": "We propose a new recurrent neural CRF model for learning nonlinear edge features. Our model is capable of fully encoding nonlinear features for CRF. Experiments show that our model outperforms state-of-the-art methods in several structured prediction tasks, including NP chunking, shallow parsing, Chinese word segmentation, and POS tagging."}, {"heading": "Acknowledgements", "text": "This work was partially supported by the National Natural Science Foundation of China (No. 61300063). Xu Sun is the corresponding author of this paper."}], "references": [{"title": "A high-performance semisupervised learning method for text chunking", "author": ["R.K. Ando", "T. Zhang"], "venue": "ACL 2005.", "citeRegEx": "Ando and Zhang,? 2005", "shortCiteRegEx": "Ando and Zhang", "year": 2005}, {"title": "Globally normalized transition-based neural networks", "author": ["D. Andor", "C. Alberti", "D. Weiss", "A. Severyn", "A. Presta", "K. Ganchev", "S. Petrov", "M. Collins"], "venue": "arXiv preprint arXiv:1603.06042.", "citeRegEx": "Andor et al\\.,? 2016", "shortCiteRegEx": "Andor et al\\.", "year": 2016}, {"title": "A neural probabilistic language model", "author": ["Y. Bengio", "R. Ducharme", "P. Vincent", "C. Janvin"], "venue": "Journal of Machine Learning Research 3:1137\u20131155.", "citeRegEx": "Bengio et al\\.,? 2003", "shortCiteRegEx": "Bengio et al\\.", "year": 2003}, {"title": "A fast and accurate dependency parser using neural networks", "author": ["D. Chen", "C.D. Manning"], "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), 740\u2013750.", "citeRegEx": "Chen and Manning,? 2014", "shortCiteRegEx": "Chen and Manning", "year": 2014}, {"title": "Long short-term memory neural networks for chinese word segmentation", "author": ["X. Chen", "X. Qiu", "C. Zhu", "P. Liu", "X. Huang"], "venue": "EMNLP 2015, 1197\u20131206.", "citeRegEx": "Chen et al\\.,? 2015", "shortCiteRegEx": "Chen et al\\.", "year": 2015}, {"title": "Natural language processing (almost) from scratch", "author": ["R. Collobert", "J. Weston", "L. Bottou", "M. Karlen", "K. Kavukcuoglu", "P.P. Kuksa"], "venue": "Journal of Machine Learning Research 12:2493\u20132537.", "citeRegEx": "Collobert et al\\.,? 2011", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Neural conditional random fields", "author": ["T.M.T. Do", "T. Arti\u00e8res"], "venue": "Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics, AISTATS 2010, 177\u2013184.", "citeRegEx": "Do and Arti\u00e8res,? 2010", "shortCiteRegEx": "Do and Arti\u00e8res", "year": 2010}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["J.C. Duchi", "E. Hazan", "Y. Singer"], "venue": "Journal of Machine Learning Research 12:2121\u2013 2159.", "citeRegEx": "Duchi et al\\.,? 2011", "shortCiteRegEx": "Duchi et al\\.", "year": 2011}, {"title": "Neural CRF parsing", "author": ["G. Durrett", "D. Klein"], "venue": "ACL 2015, 302\u2013312.", "citeRegEx": "Durrett and Klein,? 2015", "shortCiteRegEx": "Durrett and Klein", "year": 2015}, {"title": "Finding structure in time", "author": ["J.L. Elman"], "venue": "Cognitive Science 14(2):179\u2013211.", "citeRegEx": "Elman,? 1990", "shortCiteRegEx": "Elman", "year": 1990}, {"title": "Framewise phoneme classification with bidirectional LSTM and other neural network architectures", "author": ["A. Graves", "J. Schmidhuber"], "venue": "Neural Networks 18(5-6):602\u2013610.", "citeRegEx": "Graves and Schmidhuber,? 2005", "shortCiteRegEx": "Graves and Schmidhuber", "year": 2005}, {"title": "Speech recognition with deep recurrent neural networks", "author": ["A. Graves", "A. Mohamed", "G.E. Hinton"], "venue": "ICASSP 2013, 6645\u20136649.", "citeRegEx": "Graves et al\\.,? 2013", "shortCiteRegEx": "Graves et al\\.", "year": 2013}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural Computation 9(8):1735\u20131780.", "citeRegEx": "Hochreiter and Schmidhuber,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber", "year": 1997}, {"title": "Bidirectional LSTM-CRF models for sequence tagging", "author": ["Z. Huang", "W. Xu", "K. Yu"], "venue": "arXiv preprint arXiv:1508.01991.", "citeRegEx": "Huang et al\\.,? 2015", "shortCiteRegEx": "Huang et al\\.", "year": 2015}, {"title": "Conditional random fields: Probabilistic models for segmenting and labeling sequence data", "author": ["J.D. Lafferty", "A. McCallum", "F.C.N. Pereira"], "venue": "Proceedings of the Eighteenth International Conference on Machine Learning (ICML 2001), 282\u2013289.", "citeRegEx": "Lafferty et al\\.,? 2001", "shortCiteRegEx": "Lafferty et al\\.", "year": 2001}, {"title": "Flexible text segmentation with structured multilabel classification", "author": ["R.T. McDonald", "K. Crammer", "F. Pereira"], "venue": "HLT/EMNLP 2005.", "citeRegEx": "McDonald et al\\.,? 2005", "shortCiteRegEx": "McDonald et al\\.", "year": 2005}, {"title": "Recurrent neural network based language model", "author": ["T. Mikolov", "M. Karafi\u00e1t", "L. Burget", "J. Cernock\u00fd", "S. Khudanpur"], "venue": "INTERSPEECH 2010, 1045\u20131048.", "citeRegEx": "Mikolov et al\\.,? 2010", "shortCiteRegEx": "Mikolov et al\\.", "year": 2010}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["T. Mikolov", "I. Sutskever", "K. Chen", "G.S. Corrado", "J. Dean"], "venue": "Advances in Neural Information Processing Systems 26: 27th Annual Conference on Neural Information Processing Systems 2013,", "citeRegEx": "Mikolov et al\\.,? 2013", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Max-margin tensor neural network for chinese word segmentation", "author": ["W. Pei", "T. Ge", "B. Chang"], "venue": "ACL 2014, 293\u2013303.", "citeRegEx": "Pei et al\\.,? 2014", "shortCiteRegEx": "Pei et al\\.", "year": 2014}, {"title": "Conditional neural fields", "author": ["J. Peng", "L. Bo", "J. Xu"], "venue": "Advances in Neural Information Processing Systems 22: 23rd Annual Conference on Neural Information Processing Systems 2009, 1419\u20131427.", "citeRegEx": "Peng et al\\.,? 2009", "shortCiteRegEx": "Peng et al\\.", "year": 2009}, {"title": "Shallow parsing with conditional random fields", "author": ["F. Sha", "F.C.N. Pereira"], "venue": "HLT-NAACL 2003.", "citeRegEx": "Sha and Pereira,? 2003", "shortCiteRegEx": "Sha and Pereira", "year": 2003}, {"title": "Voting between multiple data representations for text chunking", "author": ["H. Shen", "A. Sarkar"], "venue": "Advances in Artificial Intelligence, 18th Conference of the Canadian Society for Computational Studies of Intelligence, 389\u2013400.", "citeRegEx": "Shen and Sarkar,? 2005", "shortCiteRegEx": "Shen and Sarkar", "year": 2005}, {"title": "Guided learning for bidirectional sequence classification", "author": ["L. Shen", "G. Satta", "A.K. Joshi"], "venue": "ACL 2007.", "citeRegEx": "Shen et al\\.,? 2007", "shortCiteRegEx": "Shen et al\\.", "year": 2007}, {"title": "Parsing with compositional vector grammars", "author": ["R. Socher", "J. Bauer", "C.D. Manning", "A.Y. Ng"], "venue": "ACL 2013, 455\u2013465.", "citeRegEx": "Socher et al\\.,? 2013", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Dropout: a simple way to prevent neural networks from overfitting", "author": ["N. Srivastava", "G.E. Hinton", "A. Krizhevsky", "I. Sutskever", "R. Salakhutdinov"], "venue": "Journal of Machine Learning Research 15(1):1929\u20131958.", "citeRegEx": "Srivastava et al\\.,? 2014", "shortCiteRegEx": "Srivastava et al\\.", "year": 2014}, {"title": "Modeling latent-dynamic in shallow parsing: A latent conditional model with improved inference", "author": ["X. Sun", "L. Morency", "D. Okanohara", "Y. Tsuruoka", "J. Tsujii"], "venue": "COLING 2008, 841\u2013848.", "citeRegEx": "Sun et al\\.,? 2008", "shortCiteRegEx": "Sun et al\\.", "year": 2008}, {"title": "Fast online training with frequency-adaptive learning rates for chinese word segmentation and new word detection", "author": ["X. Sun", "H. Wang", "W. Li"], "venue": "ACL 2012, 253\u2013262.", "citeRegEx": "Sun et al\\.,? 2012", "shortCiteRegEx": "Sun et al\\.", "year": 2012}, {"title": "Structure regularization for structured prediction", "author": ["X. Sun"], "venue": "Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems 2014, 2402\u20132410.", "citeRegEx": "Sun,? 2014", "shortCiteRegEx": "Sun", "year": 2014}, {"title": "Asynchronous parallel learning for neural networks and structured models with dense features", "author": ["X. Sun"], "venue": "COLING 2016.", "citeRegEx": "Sun,? 2016", "shortCiteRegEx": "Sun", "year": 2016}, {"title": "Learning structured prediction models: a large margin approach", "author": ["B. Taskar", "V. Chatalbashev", "D. Koller", "C. Guestrin"], "venue": "(ICML 2005), 896\u2013903.", "citeRegEx": "Taskar et al\\.,? 2005", "shortCiteRegEx": "Taskar et al\\.", "year": 2005}, {"title": "Dependency-based gated recursive neural network for chinese word segmentation", "author": ["J. Xu", "X. Sun"], "venue": "ACL 2016.", "citeRegEx": "Xu and Sun,? 2016", "shortCiteRegEx": "Xu and Sun", "year": 2016}, {"title": "Text chunking based on a generalization of winnow", "author": ["T. Zhang", "F. Damerau", "D. Johnson"], "venue": "Journal of Machine Learning Research 2:615\u2013637.", "citeRegEx": "Zhang et al\\.,? 2002", "shortCiteRegEx": "Zhang et al\\.", "year": 2002}, {"title": "Conditional random fields as recurrent neural networks", "author": ["S. Zheng", "S. Jayasumana", "B. Romera-Paredes", "V. Vineet", "Z. Su", "D. Du", "C. Huang", "P.H.S. Torr"], "venue": "2015 IEEE International Conference on Computer Vision, ICCV 2015, 1529\u20131537.", "citeRegEx": "Zheng et al\\.,? 2015", "shortCiteRegEx": "Zheng et al\\.", "year": 2015}, {"title": "A neural probabilistic structured-prediction model for transitionbased dependency parsing", "author": ["H. Zhou", "Y. Zhang", "S. Huang", "J. Chen"], "venue": "ACL 2015, 1213\u20131222.", "citeRegEx": "Zhou et al\\.,? 2015", "shortCiteRegEx": "Zhou et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 3, "context": "For minimizing the effort in feature engineering, neural network models are used to automatically extract features (Chen and Manning 2014; Collobert et al. 2011).", "startOffset": 115, "endOffset": 161}, {"referenceID": 5, "context": "For minimizing the effort in feature engineering, neural network models are used to automatically extract features (Chen and Manning 2014; Collobert et al. 2011).", "startOffset": 115, "endOffset": 161}, {"referenceID": 2, "context": "For minimizing the effort in feature engineering, neural network models are used to automatically extract features (Chen and Manning 2014; Collobert et al. 2011). These models learn dense features, which have better representation of both syntax and semantic information. Because of the success of CRF and neural networks, many models take advantage of both of them. Collobert et al. (2011) used CRF objective to compute sentence-level probability of convolutional neural networks.", "startOffset": 116, "endOffset": 391}, {"referenceID": 2, "context": "For minimizing the effort in feature engineering, neural network models are used to automatically extract features (Chen and Manning 2014; Collobert et al. 2011). These models learn dense features, which have better representation of both syntax and semantic information. Because of the success of CRF and neural networks, many models take advantage of both of them. Collobert et al. (2011) used CRF objective to compute sentence-level probability of convolutional neural networks. Durrett and Klein (2015) introduced a neural CRF model to join sparse features and dense features for parsing.", "startOffset": 116, "endOffset": 507}, {"referenceID": 1, "context": "Andor et al. (2016) proposed a transitionbased neural model with a globally normalized CRF objective, and they use feedforward neural networks to learn neural features.", "startOffset": 0, "endOffset": 20}, {"referenceID": 12, "context": "Huang et al. (2015) provided a solution to combine recurrent structure with CRF structure, and gained good performance in sequence labelling.", "startOffset": 0, "endOffset": 20}, {"referenceID": 1, "context": "Andor et al. (2016) proved that globally normalized CRF objective solved label bias problem for neural models.", "startOffset": 0, "endOffset": 20}, {"referenceID": 4, "context": "It proves to be efficient in several model (Pei, Ge, and Chang 2014; Chen et al. 2015), but it may suffer from sparsity and low training speed.", "startOffset": 43, "endOffset": 86}, {"referenceID": 5, "context": "It is simple and widely used in previous work (Collobert et al. 2011; Huang, Xu, and Yu 2015).", "startOffset": 46, "endOffset": 93}, {"referenceID": 5, "context": "Feedforward networks Convolution model (Collobert et al. 2011) Neural CRF networks (Do and Arti\u00e8res 2010) and Transition-based neural networks (Andor et al.", "startOffset": 39, "endOffset": 62}, {"referenceID": 6, "context": "2011) Neural CRF networks (Do and Arti\u00e8res 2010) and Transition-based neural networks (Andor et al.", "startOffset": 26, "endOffset": 48}, {"referenceID": 1, "context": "2011) Neural CRF networks (Do and Arti\u00e8res 2010) and Transition-based neural networks (Andor et al. 2016) Recurrent networks LSTM-CRF model (Huang, Xu, and Yu 2015) This work", "startOffset": 86, "endOffset": 105}, {"referenceID": 1, "context": "Thus, our model is gobally normalized, which can solve label bias problem (Andor et al. 2016).", "startOffset": 74, "endOffset": 93}, {"referenceID": 6, "context": "Training We have two kinds of criteria to train our models: probabilistic criteria and large margin criteria (Do and Arti\u00e8res 2010).", "startOffset": 109, "endOffset": 131}, {"referenceID": 29, "context": "Large Margin Criteria: Large margin criteria is first introduced by Taskar et al. (2005). In large margin criteria, the margin between the scores of correct tag sequence and incorrect sequence will be larger than a given large margin:", "startOffset": 68, "endOffset": 89}, {"referenceID": 2, "context": "Recently, neural networks models have been widely used in natural language processing (Bengio et al. 2003; Mikolov et al. 2010; Socher et al. 2013; Chen et al. 2015; Sun 2016).", "startOffset": 86, "endOffset": 175}, {"referenceID": 16, "context": "Recently, neural networks models have been widely used in natural language processing (Bengio et al. 2003; Mikolov et al. 2010; Socher et al. 2013; Chen et al. 2015; Sun 2016).", "startOffset": 86, "endOffset": 175}, {"referenceID": 23, "context": "Recently, neural networks models have been widely used in natural language processing (Bengio et al. 2003; Mikolov et al. 2010; Socher et al. 2013; Chen et al. 2015; Sun 2016).", "startOffset": 86, "endOffset": 175}, {"referenceID": 4, "context": "Recently, neural networks models have been widely used in natural language processing (Bengio et al. 2003; Mikolov et al. 2010; Socher et al. 2013; Chen et al. 2015; Sun 2016).", "startOffset": 86, "endOffset": 175}, {"referenceID": 28, "context": "Recently, neural networks models have been widely used in natural language processing (Bengio et al. 2003; Mikolov et al. 2010; Socher et al. 2013; Chen et al. 2015; Sun 2016).", "startOffset": 86, "endOffset": 175}, {"referenceID": 9, "context": "Among various neural models, recurrent neural networks (Elman 1990) proves to perform well in sequence labelling tasks.", "startOffset": 55, "endOffset": 67}, {"referenceID": 12, "context": "LSTM (Hochreiter and Schmidhuber 1997; Graves and Schmidhuber 2005) improves the performance of RNN by solving the vanishing and exploding gradient problem.", "startOffset": 5, "endOffset": 67}, {"referenceID": 10, "context": "LSTM (Hochreiter and Schmidhuber 1997; Graves and Schmidhuber 2005) improves the performance of RNN by solving the vanishing and exploding gradient problem.", "startOffset": 5, "endOffset": 67}, {"referenceID": 1, "context": "Recently, neural networks models have been widely used in natural language processing (Bengio et al. 2003; Mikolov et al. 2010; Socher et al. 2013; Chen et al. 2015; Sun 2016). Among various neural models, recurrent neural networks (Elman 1990) proves to perform well in sequence labelling tasks. LSTM (Hochreiter and Schmidhuber 1997; Graves and Schmidhuber 2005) improves the performance of RNN by solving the vanishing and exploding gradient problem. Later, bidirectional recurrent model (Graves, Mohamed, and Hinton 2013) is proposed to capture the backward information. CRF model (Lafferty, McCallum, and Pereira 2001) has achieved much success in natural language processing. Many models try to combine CRF with neural networks for more structure dependence. Peng et al. (2009) introduces a conditional neural fields model.", "startOffset": 87, "endOffset": 784}, {"referenceID": 1, "context": "Recently, neural networks models have been widely used in natural language processing (Bengio et al. 2003; Mikolov et al. 2010; Socher et al. 2013; Chen et al. 2015; Sun 2016). Among various neural models, recurrent neural networks (Elman 1990) proves to perform well in sequence labelling tasks. LSTM (Hochreiter and Schmidhuber 1997; Graves and Schmidhuber 2005) improves the performance of RNN by solving the vanishing and exploding gradient problem. Later, bidirectional recurrent model (Graves, Mohamed, and Hinton 2013) is proposed to capture the backward information. CRF model (Lafferty, McCallum, and Pereira 2001) has achieved much success in natural language processing. Many models try to combine CRF with neural networks for more structure dependence. Peng et al. (2009) introduces a conditional neural fields model. Collobert et al. (2011) first implements convolutional neural networks with the CRF objective.", "startOffset": 87, "endOffset": 854}, {"referenceID": 1, "context": "Recently, neural networks models have been widely used in natural language processing (Bengio et al. 2003; Mikolov et al. 2010; Socher et al. 2013; Chen et al. 2015; Sun 2016). Among various neural models, recurrent neural networks (Elman 1990) proves to perform well in sequence labelling tasks. LSTM (Hochreiter and Schmidhuber 1997; Graves and Schmidhuber 2005) improves the performance of RNN by solving the vanishing and exploding gradient problem. Later, bidirectional recurrent model (Graves, Mohamed, and Hinton 2013) is proposed to capture the backward information. CRF model (Lafferty, McCallum, and Pereira 2001) has achieved much success in natural language processing. Many models try to combine CRF with neural networks for more structure dependence. Peng et al. (2009) introduces a conditional neural fields model. Collobert et al. (2011) first implements convolutional neural networks with the CRF objective.Zheng et al. (2015) integrates CRF with RNN.", "startOffset": 87, "endOffset": 944}, {"referenceID": 1, "context": "Recently, neural networks models have been widely used in natural language processing (Bengio et al. 2003; Mikolov et al. 2010; Socher et al. 2013; Chen et al. 2015; Sun 2016). Among various neural models, recurrent neural networks (Elman 1990) proves to perform well in sequence labelling tasks. LSTM (Hochreiter and Schmidhuber 1997; Graves and Schmidhuber 2005) improves the performance of RNN by solving the vanishing and exploding gradient problem. Later, bidirectional recurrent model (Graves, Mohamed, and Hinton 2013) is proposed to capture the backward information. CRF model (Lafferty, McCallum, and Pereira 2001) has achieved much success in natural language processing. Many models try to combine CRF with neural networks for more structure dependence. Peng et al. (2009) introduces a conditional neural fields model. Collobert et al. (2011) first implements convolutional neural networks with the CRF objective.Zheng et al. (2015) integrates CRF with RNN. Durrett and Klein (2015) uses feed forward neural networks with CRF for parsing.", "startOffset": 87, "endOffset": 994}, {"referenceID": 1, "context": "Recently, neural networks models have been widely used in natural language processing (Bengio et al. 2003; Mikolov et al. 2010; Socher et al. 2013; Chen et al. 2015; Sun 2016). Among various neural models, recurrent neural networks (Elman 1990) proves to perform well in sequence labelling tasks. LSTM (Hochreiter and Schmidhuber 1997; Graves and Schmidhuber 2005) improves the performance of RNN by solving the vanishing and exploding gradient problem. Later, bidirectional recurrent model (Graves, Mohamed, and Hinton 2013) is proposed to capture the backward information. CRF model (Lafferty, McCallum, and Pereira 2001) has achieved much success in natural language processing. Many models try to combine CRF with neural networks for more structure dependence. Peng et al. (2009) introduces a conditional neural fields model. Collobert et al. (2011) first implements convolutional neural networks with the CRF objective.Zheng et al. (2015) integrates CRF with RNN. Durrett and Klein (2015) uses feed forward neural networks with CRF for parsing. Huang et al. (2015) use recurrent neural networks to learn non-linear node features.", "startOffset": 87, "endOffset": 1070}, {"referenceID": 1, "context": "Recently, neural networks models have been widely used in natural language processing (Bengio et al. 2003; Mikolov et al. 2010; Socher et al. 2013; Chen et al. 2015; Sun 2016). Among various neural models, recurrent neural networks (Elman 1990) proves to perform well in sequence labelling tasks. LSTM (Hochreiter and Schmidhuber 1997; Graves and Schmidhuber 2005) improves the performance of RNN by solving the vanishing and exploding gradient problem. Later, bidirectional recurrent model (Graves, Mohamed, and Hinton 2013) is proposed to capture the backward information. CRF model (Lafferty, McCallum, and Pereira 2001) has achieved much success in natural language processing. Many models try to combine CRF with neural networks for more structure dependence. Peng et al. (2009) introduces a conditional neural fields model. Collobert et al. (2011) first implements convolutional neural networks with the CRF objective.Zheng et al. (2015) integrates CRF with RNN. Durrett and Klein (2015) uses feed forward neural networks with CRF for parsing. Huang et al. (2015) use recurrent neural networks to learn non-linear node features. They show that BiLSTM-CRF is more robust than neural models without CRF. Do and Artieres (2010) suggest feedforward neural networks to learn neural features.", "startOffset": 87, "endOffset": 1231}, {"referenceID": 1, "context": "Recently, neural networks models have been widely used in natural language processing (Bengio et al. 2003; Mikolov et al. 2010; Socher et al. 2013; Chen et al. 2015; Sun 2016). Among various neural models, recurrent neural networks (Elman 1990) proves to perform well in sequence labelling tasks. LSTM (Hochreiter and Schmidhuber 1997; Graves and Schmidhuber 2005) improves the performance of RNN by solving the vanishing and exploding gradient problem. Later, bidirectional recurrent model (Graves, Mohamed, and Hinton 2013) is proposed to capture the backward information. CRF model (Lafferty, McCallum, and Pereira 2001) has achieved much success in natural language processing. Many models try to combine CRF with neural networks for more structure dependence. Peng et al. (2009) introduces a conditional neural fields model. Collobert et al. (2011) first implements convolutional neural networks with the CRF objective.Zheng et al. (2015) integrates CRF with RNN. Durrett and Klein (2015) uses feed forward neural networks with CRF for parsing. Huang et al. (2015) use recurrent neural networks to learn non-linear node features. They show that BiLSTM-CRF is more robust than neural models without CRF. Do and Artieres (2010) suggest feedforward neural networks to learn neural features. Zhou et al. (2015) proposes a transition based neural model with CRF for parsing.", "startOffset": 87, "endOffset": 1312}, {"referenceID": 1, "context": "Finally, Andor et al. (2016) proves that a globally normalized CRF objective helps deal with label bias problem in neural models.", "startOffset": 9, "endOffset": 29}, {"referenceID": 30, "context": "Chinese word segmentation for social media text: Word segmentation is a fundamental task for Chinese language processing (Sun, Wang, and Li 2012; Xu and Sun 2016).", "startOffset": 121, "endOffset": 162}, {"referenceID": 2, "context": "Embeddings are distributed vectors to represent the semantic of words (Bengio et al. 2003; Mikolov et al. 2013).", "startOffset": 70, "endOffset": 111}, {"referenceID": 17, "context": "Embeddings are distributed vectors to represent the semantic of words (Bengio et al. 2003; Mikolov et al. 2013).", "startOffset": 70, "endOffset": 111}, {"referenceID": 5, "context": "In our models, we use random initialized word embeddings as well as Senna embeddings (Collobert et al. 2011).", "startOffset": 85, "endOffset": 108}, {"referenceID": 5, "context": "We also incorporate the feature embeddings as suggested by previous work (Collobert et al. 2011).", "startOffset": 73, "endOffset": 96}, {"referenceID": 4, "context": "To alleviate heavy feature engineering, we do not use other features like bigram or trigram, though they may increase the accuracy as shown in (Pei, Ge, and Chang 2014) and (Chen et al. 2015).", "startOffset": 173, "endOffset": 191}, {"referenceID": 24, "context": "The dropout method proves to avoid overfitting in neural models (Srivastava et al. 2014), but we find it has limited impact in our models.", "startOffset": 64, "endOffset": 88}, {"referenceID": 20, "context": "NP Chunking: In NP Chunking, a popular algorithm is second-order CRF (Sha and Pereira 2003), which can achieve a score of 94.", "startOffset": 69, "endOffset": 91}, {"referenceID": 15, "context": "McDonald et al. (2005) implemented a multilabel learning algorithm, with a score of", "startOffset": 0, "endOffset": 23}, {"referenceID": 15, "context": "NP chunking F1 Shallow parsing F1 POS tagging Acc Sha and Pereira (2003) 94.", "startOffset": 50, "endOffset": 73}, {"referenceID": 15, "context": "NP chunking F1 Shallow parsing F1 POS tagging Acc Sha and Pereira (2003) 94.30 Zhang et al. (2002) 94.", "startOffset": 50, "endOffset": 99}, {"referenceID": 15, "context": "NP chunking F1 Shallow parsing F1 POS tagging Acc Sha and Pereira (2003) 94.30 Zhang et al. (2002) 94.17 Collobert (2011) 97.", "startOffset": 50, "endOffset": 122}, {"referenceID": 0, "context": "29 Ando and Zhang (2005) 94.", "startOffset": 3, "endOffset": 25}, {"referenceID": 0, "context": "29 Ando and Zhang (2005) 94.70 Ando and Zhang (2005) 94.", "startOffset": 3, "endOffset": 53}, {"referenceID": 0, "context": "29 Ando and Zhang (2005) 94.70 Ando and Zhang (2005) 94.39 Sun (2014) 97.", "startOffset": 3, "endOffset": 70}, {"referenceID": 0, "context": "29 Ando and Zhang (2005) 94.70 Ando and Zhang (2005) 94.39 Sun (2014) 97.36 Shen and Sarkar (2005) 95.", "startOffset": 3, "endOffset": 99}, {"referenceID": 0, "context": "29 Ando and Zhang (2005) 94.70 Ando and Zhang (2005) 94.39 Sun (2014) 97.36 Shen and Sarkar (2005) 95.23 Shen and Sarkar (2005) 94.", "startOffset": 3, "endOffset": 128}, {"referenceID": 0, "context": "29 Ando and Zhang (2005) 94.70 Ando and Zhang (2005) 94.39 Sun (2014) 97.36 Shen and Sarkar (2005) 95.23 Shen and Sarkar (2005) 94.01 Huang et al. (2015) 97.", "startOffset": 3, "endOffset": 154}, {"referenceID": 0, "context": "29 Ando and Zhang (2005) 94.70 Ando and Zhang (2005) 94.39 Sun (2014) 97.36 Shen and Sarkar (2005) 95.23 Shen and Sarkar (2005) 94.01 Huang et al. (2015) 97.55 McDonald et al. (2005) 94.", "startOffset": 3, "endOffset": 183}, {"referenceID": 0, "context": "29 Ando and Zhang (2005) 94.70 Ando and Zhang (2005) 94.39 Sun (2014) 97.36 Shen and Sarkar (2005) 95.23 Shen and Sarkar (2005) 94.01 Huang et al. (2015) 97.55 McDonald et al. (2005) 94.29 Collobert et al. (2011) 94.", "startOffset": 3, "endOffset": 213}, {"referenceID": 0, "context": "29 Ando and Zhang (2005) 94.70 Ando and Zhang (2005) 94.39 Sun (2014) 97.36 Shen and Sarkar (2005) 95.23 Shen and Sarkar (2005) 94.01 Huang et al. (2015) 97.55 McDonald et al. (2005) 94.29 Collobert et al. (2011) 94.32 Andor et al. (2016) 97.", "startOffset": 3, "endOffset": 239}, {"referenceID": 0, "context": "29 Ando and Zhang (2005) 94.70 Ando and Zhang (2005) 94.39 Sun (2014) 97.36 Shen and Sarkar (2005) 95.23 Shen and Sarkar (2005) 94.01 Huang et al. (2015) 97.55 McDonald et al. (2005) 94.29 Collobert et al. (2011) 94.32 Andor et al. (2016) 97.44 Sun et al. (2008) 94.", "startOffset": 3, "endOffset": 263}, {"referenceID": 0, "context": "29 Ando and Zhang (2005) 94.70 Ando and Zhang (2005) 94.39 Sun (2014) 97.36 Shen and Sarkar (2005) 95.23 Shen and Sarkar (2005) 94.01 Huang et al. (2015) 97.55 McDonald et al. (2005) 94.29 Collobert et al. (2011) 94.32 Andor et al. (2016) 97.44 Sun et al. (2008) 94.34 Huang et al. (2015) 94.", "startOffset": 3, "endOffset": 289}, {"referenceID": 0, "context": "29 Ando and Zhang (2005) 94.70 Ando and Zhang (2005) 94.39 Sun (2014) 97.36 Shen and Sarkar (2005) 95.23 Shen and Sarkar (2005) 94.01 Huang et al. (2015) 97.55 McDonald et al. (2005) 94.29 Collobert et al. (2011) 94.32 Andor et al. (2016) 97.44 Sun et al. (2008) 94.34 Huang et al. (2015) 94.46 Shen et al. (2007) 97.", "startOffset": 3, "endOffset": 314}, {"referenceID": 0, "context": "Some other models (Ando and Zhang 2005; Shen and Sarkar 2005) make use of extra resources, and greatly improve the performance of Support Vector Machines(SVM).", "startOffset": 18, "endOffset": 61}, {"referenceID": 21, "context": "Some other models (Ando and Zhang 2005; Shen and Sarkar 2005) make use of extra resources, and greatly improve the performance of Support Vector Machines(SVM).", "startOffset": 18, "endOffset": 61}, {"referenceID": 20, "context": "Sun et al. (2008) proposed a latent variable CRF model, improving the score up to 94.", "startOffset": 0, "endOffset": 18}, {"referenceID": 0, "context": "Some other models (Ando and Zhang 2005; Shen and Sarkar 2005) make use of extra resources, and greatly improve the performance of Support Vector Machines(SVM). To the best of our knowledge, few neural models have been introduced for NP Chunking. Our models can outperform all of the above models. We also implement some neural models to compare with our model. LSTM has a score of 94.12, and BiLSTM is better with 94.36% F-score. As a strong baseline, BiLSTM-CRF outperforms them with 94.97% F-score. Our model also performs better than all these neural models, with 95.25% F-score. Shallow Parsing: In shallow parsing, Zhang et al. (2002) proposed a generalized Winnow algorithm which achieve a score of 94.", "startOffset": 19, "endOffset": 640}, {"referenceID": 0, "context": "Some other models (Ando and Zhang 2005; Shen and Sarkar 2005) make use of extra resources, and greatly improve the performance of Support Vector Machines(SVM). To the best of our knowledge, few neural models have been introduced for NP Chunking. Our models can outperform all of the above models. We also implement some neural models to compare with our model. LSTM has a score of 94.12, and BiLSTM is better with 94.36% F-score. As a strong baseline, BiLSTM-CRF outperforms them with 94.97% F-score. Our model also performs better than all these neural models, with 95.25% F-score. Shallow Parsing: In shallow parsing, Zhang et al. (2002) proposed a generalized Winnow algorithm which achieve a score of 94.17%. Ando and Zhang (2005) introduced a SVD based alternating structure optimization algorithm, improving the score up to 94.", "startOffset": 19, "endOffset": 735}, {"referenceID": 0, "context": "Some other models (Ando and Zhang 2005; Shen and Sarkar 2005) make use of extra resources, and greatly improve the performance of Support Vector Machines(SVM). To the best of our knowledge, few neural models have been introduced for NP Chunking. Our models can outperform all of the above models. We also implement some neural models to compare with our model. LSTM has a score of 94.12, and BiLSTM is better with 94.36% F-score. As a strong baseline, BiLSTM-CRF outperforms them with 94.97% F-score. Our model also performs better than all these neural models, with 95.25% F-score. Shallow Parsing: In shallow parsing, Zhang et al. (2002) proposed a generalized Winnow algorithm which achieve a score of 94.17%. Ando and Zhang (2005) introduced a SVD based alternating structure optimization algorithm, improving the score up to 94.39%. Collobert et al. (2011) first introduced the neural network model to shallow parsing.", "startOffset": 19, "endOffset": 862}, {"referenceID": 0, "context": "Some other models (Ando and Zhang 2005; Shen and Sarkar 2005) make use of extra resources, and greatly improve the performance of Support Vector Machines(SVM). To the best of our knowledge, few neural models have been introduced for NP Chunking. Our models can outperform all of the above models. We also implement some neural models to compare with our model. LSTM has a score of 94.12, and BiLSTM is better with 94.36% F-score. As a strong baseline, BiLSTM-CRF outperforms them with 94.97% F-score. Our model also performs better than all these neural models, with 95.25% F-score. Shallow Parsing: In shallow parsing, Zhang et al. (2002) proposed a generalized Winnow algorithm which achieve a score of 94.17%. Ando and Zhang (2005) introduced a SVD based alternating structure optimization algorithm, improving the score up to 94.39%. Collobert et al. (2011) first introduced the neural network model to shallow parsing. They combined the convolutional neural networks with CRF, and reached 94.32% F-score. Huang et al. (2015) combined BiLSTM with a CRF layer, raising the score up to 94.", "startOffset": 19, "endOffset": 1030}, {"referenceID": 0, "context": "Some other models (Ando and Zhang 2005; Shen and Sarkar 2005) make use of extra resources, and greatly improve the performance of Support Vector Machines(SVM). To the best of our knowledge, few neural models have been introduced for NP Chunking. Our models can outperform all of the above models. We also implement some neural models to compare with our model. LSTM has a score of 94.12, and BiLSTM is better with 94.36% F-score. As a strong baseline, BiLSTM-CRF outperforms them with 94.97% F-score. Our model also performs better than all these neural models, with 95.25% F-score. Shallow Parsing: In shallow parsing, Zhang et al. (2002) proposed a generalized Winnow algorithm which achieve a score of 94.17%. Ando and Zhang (2005) introduced a SVD based alternating structure optimization algorithm, improving the score up to 94.39%. Collobert et al. (2011) first introduced the neural network model to shallow parsing. They combined the convolutional neural networks with CRF, and reached 94.32% F-score. Huang et al. (2015) combined BiLSTM with a CRF layer, raising the score up to 94.46%. Our Edge-based model can beat all of these models in performance, and obtain state-of-art result with a score of 94.80%. POS tagging: As an important task in natural language processing, there are lots of work on POS tagging. We make a comparison of our models with some recent work. Sun (2014) introduced a structure regularization method for CRF, which reached 97.", "startOffset": 19, "endOffset": 1391}, {"referenceID": 0, "context": "Some other models (Ando and Zhang 2005; Shen and Sarkar 2005) make use of extra resources, and greatly improve the performance of Support Vector Machines(SVM). To the best of our knowledge, few neural models have been introduced for NP Chunking. Our models can outperform all of the above models. We also implement some neural models to compare with our model. LSTM has a score of 94.12, and BiLSTM is better with 94.36% F-score. As a strong baseline, BiLSTM-CRF outperforms them with 94.97% F-score. Our model also performs better than all these neural models, with 95.25% F-score. Shallow Parsing: In shallow parsing, Zhang et al. (2002) proposed a generalized Winnow algorithm which achieve a score of 94.17%. Ando and Zhang (2005) introduced a SVD based alternating structure optimization algorithm, improving the score up to 94.39%. Collobert et al. (2011) first introduced the neural network model to shallow parsing. They combined the convolutional neural networks with CRF, and reached 94.32% F-score. Huang et al. (2015) combined BiLSTM with a CRF layer, raising the score up to 94.46%. Our Edge-based model can beat all of these models in performance, and obtain state-of-art result with a score of 94.80%. POS tagging: As an important task in natural language processing, there are lots of work on POS tagging. We make a comparison of our models with some recent work. Sun (2014) introduced a structure regularization method for CRF, which reached 97.36% accuracy. Collobert et al. (2011) used a Convolution-CRF model, and obtained 97.", "startOffset": 19, "endOffset": 1500}, {"referenceID": 0, "context": "Some other models (Ando and Zhang 2005; Shen and Sarkar 2005) make use of extra resources, and greatly improve the performance of Support Vector Machines(SVM). To the best of our knowledge, few neural models have been introduced for NP Chunking. Our models can outperform all of the above models. We also implement some neural models to compare with our model. LSTM has a score of 94.12, and BiLSTM is better with 94.36% F-score. As a strong baseline, BiLSTM-CRF outperforms them with 94.97% F-score. Our model also performs better than all these neural models, with 95.25% F-score. Shallow Parsing: In shallow parsing, Zhang et al. (2002) proposed a generalized Winnow algorithm which achieve a score of 94.17%. Ando and Zhang (2005) introduced a SVD based alternating structure optimization algorithm, improving the score up to 94.39%. Collobert et al. (2011) first introduced the neural network model to shallow parsing. They combined the convolutional neural networks with CRF, and reached 94.32% F-score. Huang et al. (2015) combined BiLSTM with a CRF layer, raising the score up to 94.46%. Our Edge-based model can beat all of these models in performance, and obtain state-of-art result with a score of 94.80%. POS tagging: As an important task in natural language processing, there are lots of work on POS tagging. We make a comparison of our models with some recent work. Sun (2014) introduced a structure regularization method for CRF, which reached 97.36% accuracy. Collobert et al. (2011) used a Convolution-CRF model, and obtained 97.29%. Andor et al. (2016) proposed a globally normalized transition based neural model, which made use of feedforward neural networks and achieved 97.", "startOffset": 19, "endOffset": 1571}], "year": 2016, "abstractText": "Conditional Random Field (CRF) and recurrent neural models have achieved success in structured prediction. More recently, there is a marriage of CRF and recurrent neural models, so that we can gain from both non-linear dense features and globally normalized CRF objective. These recurrent neural CRF models mainly focus on encode node features in CRF undirected graphs. However, edge features prove important to CRF in structured prediction. In this work, we introduce a new recurrent neural CRF model, which learns non-linear edge features, and thus makes non-linear features encoded completely. We compare our model with different neural models in well-known structured prediction tasks. Experiments show that our model outperforms state-of-the-art methods in NP chunking, shallow parsing, Chinese word segmentation and POS tagging.", "creator": "dvips(k) 5.991 Copyright 2011 Radical Eye Software"}}}