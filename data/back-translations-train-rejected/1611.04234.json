{"id": "1611.04234", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Nov-2016", "title": "F-Score Driven Max Margin Neural Network for Named Entity Recognition in Chinese Social Media", "abstract": "We focus on named entity recognition (NER) for Chinese social media. With massive unlabeled text and quite limited labelled corpus, we propose a semi-supervised learning model based on B-LSTM neural network. To take advantage of traditional methods in NER such as CRF, we combine transition probability with deep learning in our model. To bridge the gap between label accuracy and F-score of NER, we construct a model which can be directly trained on F-score. When considering the instability of F-score driven method and meaningful information provided by label accuracy, we propose an integrated method to train on both F-score and label accuracy. Our integrated model yields 7.44\\% improvement over previous state-of-the-art result.", "histories": [["v1", "Mon, 14 Nov 2016 02:50:33 GMT  (23kb)", "https://arxiv.org/abs/1611.04234v1", null], ["v2", "Tue, 11 Apr 2017 10:57:34 GMT  (24kb)", "http://arxiv.org/abs/1611.04234v2", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["hangfeng he", "xu sun"], "accepted": false, "id": "1611.04234"}, "pdf": {"name": "1611.04234.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["xusun}@pku.edu.cn"], "sections": [{"heading": null, "text": "ar Xiv: 161 1.04 234v 2 [cs.C L] 11 Apr 201 7 (NER) for Chinese social media. With massive, unlabeled text and relatively limited labeled corpus, we propose a semi-supervised learning model based on the BLSTM neural network. To take advantage of traditional methods in NER such as CRF, we combine transition probability with deep learning in our model. To bridge the gap between labeling accuracy and NER's F score, we construct a model that can be trained directly on the F score. If we consider the instability of the Fscore-driven method and meaningful information provided by labeling accuracy, we propose an integrated method that trains on both F-score and labeling accuracy."}, {"heading": "1 Introduction", "text": "With the evolution of the Internet, social media plays an important role in information exchange, and the tasks of natural language processing in social media are more demanding, attracting the attention of many researchers (Li and Liu, 2015; Habib and van Keulen, 2015; Radford et al., 2015; Cherry and Guo, 2015). As the basis for many downstream applications (Weissenborn et al., 2015; Delgado et al., 2014; Hajishirzi et al., 2013) such as the extraction of information, called Entity Recognition (NER) deserves more research in the dominant and challenging social media text."}, {"heading": "2 Model", "text": "We construct a semi-supervised model based on the neural B-LSTM network and combine the transition probability to a structured output. In our model, we propose a method to train directly on F-score. In addition, we propose an integrated method to train on both F-score and label accuracy."}, {"heading": "2.1 Transition Probability", "text": "B-LSTM neural network can learn from past input traits and the LSTM layer makes it more efficient (Hammerton, 2003; Hochreiter and Schmidhuber, 1997; Chen et al., 2015; Graves et al., 2006). To combine the transition prediction into the neural network B-LSTM, we construct a Max Margin Neural Network (MMNN) (Pei et al., 2014), which is based on the B-LSTM sequence to obtain the sequence information. To combine the transition prediction into the neural network B-LSTM, we construct a Max Margin Neural Network (MNN) (Pei et al., 2014), based on the sequence sequence sequence t: yt = soft probability in the neural network B-LSTM (1), we construct a Max Margin Neural Network (MNN), based on the sequence sequence Nmax et (NN) where the neural (NN) is inserted into the neural network (Nmax-Mi) Pec (NN)."}, {"heading": "2.2 F-Score Driven Training Method", "text": "Max Margin Training Method. (l, l) to describe the difference between the corrected label sequence l and the predicted label sequence l. In fact, the structured margin loss \u2206 (l, l) reflects the loss of label accuracy. Considering the gap between the label accuracy and the F score in NER, we are introducing a new training method to introduce directly to F-score-driven training method, we need to take a look at the gradation of the equation (4).: \u2202 s (x, lmax, \u03b8). (F-Score Driving Training Method. (x, l,).Score In the sequence, we can know that structured margin loss (l, l) is nothing to the subgradient of the regularized target function J (v, l). Margin Loss Less. (l, l)."}, {"heading": "2.3 Word Segmentation Representation", "text": "Text segmentation plays an important role in Chinese word processing. Both Peng and Dredze (2015) and Peng and Dredze (2016) demonstrate the value of text segmentation to the Chinese NER on social media. We present two methods for using text segmentation information in the neural network model. Character and position embeddings To integrate text segmentation information, we add each character with its positioning tag. This method consists of distinguishing the same character at different positions in the word. We need to segment the text and learn position embeddings from the segmented text. Character embeddings and word segmentation functions We can treat word segmentation as discrete features in the neural network model. Discrete features are easy to integrate into the neural network model (Collobert et al., 2011). We use word embeddings from an LSTM prepared for MRA 2006 to prepare the word segmentation function."}, {"heading": "3 Experiments and Analysis", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Datasets", "text": "We use a modified flagged corpus1 as Peng and Dredze (2016) for NER in Chinese social media. Details of the data are listed in Table 1. We also use the same unflagged text as Peng and Dredze (2016) from the Sina Weibo service in China and the text is segmented by a Chinese word segmentation system Jieba2 as Peng and Dredze (2016) so that our results are more comparable to theirs."}, {"heading": "3.2 Parameter Estimation", "text": "Like Mao et al. (2008), we use Bigram features as follows: CnCn + 1 (n = \u2212 2, \u2212 1, 0, 1) and C \u2212 1C11We fix some markup errors in the data. 2https: / / github.com / fxsjy / jieba. We use the Window approach (Collobert et al., 2011) to extract superordinate features from Word feature vectors. We treat Bigram features as discrete features (Collobert et al., 2011) for our neural network. Our models are trained using stochastic gradient pedigree with an L2 regulator. As with parameters in our models, the window size for Word Feature Embedding is 5, Word Embedding Dimension and Hidden Vector Dimension."}, {"heading": "3.3 Results and Analysis", "text": "We evaluate two methods for incorporating word segmentation information. The results of two methods are presented as \u03b2-Table 2. We can see that positional character embedding is better integrated into neural networks. This is probably because positional character embedding can learn information about word segmentation from unlabeled text, while word segmentation can only use training bodies. We incorporate positional character embedding into our next four models. Our first model is a BLSTM-based neural network (baseline). To take advantage of the traditional model (Chieu and Ng, 2002; Mccallum et al., 2001) such as CRF, we combine transition probabilities in our B-LSTM-based vocal network. We design an F-Score-driven training method in our third model F-Score Driven Model I. We propose an integrated training method in our fourth model F-Score Driven Model II."}, {"heading": "4 Conclusions and Future Work", "text": "The results of our experiments also indicate a direction for future work. We can observe that all models in Table 3 achieve a much lower recall than precision (Pink et al., 2014). Therefore, we need to develop some methods to solve the problem."}, {"heading": "Acknowledgements", "text": "Thanks to Shuming Ma for helping to improve the writing. This work was partially supported by the National Natural Science Foundation of China (No. 61673028) and the National High Technology Research and Development Program of China (No. 863 Program, No. 2015AA015404). Corresponding author of this work is Xu Sun. The first author focuses on the design of the method and the experimental results. The corresponding author focuses on the design of the method."}], "references": [{"title": "Long short-term memory neural networks for chinese word segmentation", "author": ["Chen et al.2015] Xinchi Chen", "Xipeng Qiu", "Chenxi Zhu", "Pengfei Liu", "Xuanjing Huang"], "venue": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Lan-", "citeRegEx": "Chen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2015}, {"title": "The unreasonable effectiveness of word representations for twitter named entity recognition", "author": ["Cherry", "Guo2015] Colin Cherry", "Hongyu Guo"], "venue": "In Proceedings of the 2015 Conference of the North American Chapter of the Association for Computa-", "citeRegEx": "Cherry et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Cherry et al\\.", "year": 2015}, {"title": "Named entity recognition: a maximum entropy approach using global information", "author": ["Chieu", "Ng2002] Hai Leong Chieu", "Hwee Tou Ng"], "venue": "In Proceedings of the 19th international conference on Computational linguistics-Volume", "citeRegEx": "Chieu et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Chieu et al\\.", "year": 2002}, {"title": "Natural language processing (almost) from scratch", "author": ["Jason Weston", "L\u00e9on Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Collobert et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "A data driven approach for person name disambiguation in web search results", "author": ["Raquel Mart\u0131\u0301nez", "V\u0131\u0301ctor Fresno", "Soto Montalvo"], "venue": "In Proceedings of COLING", "citeRegEx": "Delgado et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Delgado et al\\.", "year": 2014}, {"title": "Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks", "author": ["Graves et al.2006] Alex Graves", "Santiago Fernndez", "Faustino Gomez", "Jrgen Schmidhuber"], "venue": "In InternationalConference,", "citeRegEx": "Graves et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2006}, {"title": "Need4tweet: A twitterbot for tweets named entity extraction and disambiguation", "author": ["Habib", "van Keulen2015] Mena Habib", "Maurice van Keulen"], "venue": "In Proceedings of ACL-IJCNLP 2015 System Demonstrations,", "citeRegEx": "Habib et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Habib et al\\.", "year": 2015}, {"title": "Joint coreference resolution and named-entity linking with multi-pass sieves", "author": ["Leila Zilles", "Daniel S. Weld", "Luke Zettlemoyer"], "venue": "In Proceedings of the 2013 Conference on Empirical Methods", "citeRegEx": "Hajishirzi et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Hajishirzi et al\\.", "year": 2013}, {"title": "Named entity recognition with long short-term memory", "author": ["James Hammerton"], "venue": "In Proceedings of the seventh conference on Natural language learning at HLT-NAACL 2003-Volume", "citeRegEx": "Hammerton.,? \\Q2003\\E", "shortCiteRegEx": "Hammerton.", "year": 2003}, {"title": "A unifiedmodel for cross-domain and semi-supervised named entity recognition in chinese social media", "author": ["He", "Sun2017] Hangfeng He", "Xu Sun"], "venue": null, "citeRegEx": "He et al\\.,? \\Q2017\\E", "shortCiteRegEx": "He et al\\.", "year": 2017}, {"title": "Long short-term memory", "author": ["Hochreiter", "Schmidhuber1997] Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Bidirectional lstm-crf models for sequence tagging", "author": ["Huang et al.2015] Zhiheng Huang", "Wei Xu", "Kai Yu"], "venue": "arXiv preprint arXiv:1508.01991", "citeRegEx": "Huang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2015}, {"title": "Improving named entity recognition in tweets via detecting non-standard words", "author": ["Li", "Liu2015] Chen Li", "Yang Liu"], "venue": "In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint", "citeRegEx": "Li et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Li et al\\.", "year": 2015}, {"title": "Enhancing sumerian lemmatization by unsupervised named-entity recognition", "author": ["Liu et al.2015] Yudong Liu", "Clinton Burkhart", "James Hearne", "Liang Luo"], "venue": "In Proceedings of the 2015 Conference of the North American Chapter of the Association", "citeRegEx": "Liu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2015}, {"title": "Chinese word segmentation and named entity recognition based on conditional random fields", "author": ["Mao et al.2008] Xinnian Mao", "Yuan Dong", "Saike He", "Sencheng Bao", "Haila Wang"], "venue": "In IJCNLP,", "citeRegEx": "Mao et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Mao et al\\.", "year": 2008}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Learning dictionaries for named entity recognition using minimal supervision", "author": ["Neelakantan", "Collins2015] Arvind Neelakantan", "Michael Collins"], "venue": "Computer Science", "citeRegEx": "Neelakantan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Neelakantan et al\\.", "year": 2015}, {"title": "Max-margin tensor neural network for chinese word segmentation", "author": ["Pei et al.2014] Wenzhe Pei", "Tao Ge", "Baobao Chang"], "venue": "In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume", "citeRegEx": "Pei et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pei et al\\.", "year": 2014}, {"title": "Named entity recognition for chinese social media with jointly trained embeddings", "author": ["Peng", "Dredze2015] Nanyun Peng", "Mark Dredze"], "venue": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Peng et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Peng et al\\.", "year": 2015}, {"title": "Improving named entity recognition for chinese social media with word segmentation representation learning", "author": ["Peng", "Dredze2016] Nanyun Peng", "Mark Dredze"], "venue": "In Proceedings of the 54th Annual Meeting of the Association for Computa-", "citeRegEx": "Peng et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Peng et al\\.", "year": 2016}, {"title": "Analysing recall loss in named entity slot filling", "author": ["Pink et al.2014] Glen Pink", "Joel Nothman", "James R. Curran"], "venue": "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Pink et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pink et al\\.", "year": 2014}, {"title": "Named entity recognition with document-specific kb tag gazetteers", "author": ["Radford et al.2015] Will Radford", "Xavier Carreras", "James Henderson"], "venue": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Radford et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Radford et al\\.", "year": 2015}, {"title": "Enhancing medical named entity recognition with features derived from unsupervised methods", "author": ["Maria Skeppstedt"], "venue": "In Proceedings of the Student Research Workshop at the 14th Conference of the European Chapter of the Association", "citeRegEx": "Skeppstedt.,? \\Q2014\\E", "shortCiteRegEx": "Skeppstedt.", "year": 2014}, {"title": "Latent variable perceptron algorithm for structured classification", "author": ["Sun et al.2009] Xu Sun", "Takuya Matsuzaki", "Daisuke Okanohara", "Jun\u2019ichi Tsujii"], "venue": "In Proceedings of the 21st International Joint", "citeRegEx": "Sun et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Sun et al\\.", "year": 2009}, {"title": "Feature-frequency-adaptive online training for fast and accurate natural language processing", "author": ["Sun et al.2014] Xu Sun", "Wenjie Li", "Houfeng Wang", "Qin Lu"], "venue": "Computational Linguistics,", "citeRegEx": "Sun et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sun et al\\.", "year": 2014}, {"title": "Structure regularization for structured prediction", "author": ["Xu Sun"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Sun.,? \\Q2014\\E", "shortCiteRegEx": "Sun.", "year": 2014}, {"title": "Multi-objective optimization for the joint disambiguation of nouns and named entities", "author": ["Leonhard Hennig", "Feiyu Xu", "Hans Uszkoreit"], "venue": "In Proceedings of the 53rd Annual Meeting of the Association", "citeRegEx": "Weissenborn et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Weissenborn et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 21, "context": "The natural language processing tasks on social media are more challenging which draw attention of many researchers (Li and Liu, 2015; Habib and van Keulen, 2015; Radford et al., 2015; Cherry and Guo, 2015).", "startOffset": 116, "endOffset": 206}, {"referenceID": 26, "context": "As the foundation of many downstream applications (Weissenborn et al., 2015; Delgado et al., 2014; Hajishirzi et al., 2013) such as information extraction, named entity recognition (NER) deserves more research in prevailing and challenging social media text.", "startOffset": 50, "endOffset": 123}, {"referenceID": 4, "context": "As the foundation of many downstream applications (Weissenborn et al., 2015; Delgado et al., 2014; Hajishirzi et al., 2013) such as information extraction, named entity recognition (NER) deserves more research in prevailing and challenging social media text.", "startOffset": 50, "endOffset": 123}, {"referenceID": 7, "context": "As the foundation of many downstream applications (Weissenborn et al., 2015; Delgado et al., 2014; Hajishirzi et al., 2013) such as information extraction, named entity recognition (NER) deserves more research in prevailing and challenging social media text.", "startOffset": 50, "endOffset": 123}, {"referenceID": 23, "context": "NER is a task to identify names in texts and to assign names with particular types (Sun et al., 2009; Sun, 2014; Sun et al., 2014; He and Sun, 2017).", "startOffset": 83, "endOffset": 148}, {"referenceID": 25, "context": "NER is a task to identify names in texts and to assign names with particular types (Sun et al., 2009; Sun, 2014; Sun et al., 2014; He and Sun, 2017).", "startOffset": 83, "endOffset": 148}, {"referenceID": 24, "context": "NER is a task to identify names in texts and to assign names with particular types (Sun et al., 2009; Sun, 2014; Sun et al., 2014; He and Sun, 2017).", "startOffset": 83, "endOffset": 148}, {"referenceID": 22, "context": "The scant labelled Chinese social media corpus makes the task more challenging (Neelakantan and Collins, 2015; Skeppstedt, 2014; Liu et al., 2015).", "startOffset": 79, "endOffset": 146}, {"referenceID": 13, "context": "The scant labelled Chinese social media corpus makes the task more challenging (Neelakantan and Collins, 2015; Skeppstedt, 2014; Liu et al., 2015).", "startOffset": 79, "endOffset": 146}, {"referenceID": 4, "context": ", 2015; Delgado et al., 2014; Hajishirzi et al., 2013) such as information extraction, named entity recognition (NER) deserves more research in prevailing and challenging social media text. NER is a task to identify names in texts and to assign names with particular types (Sun et al., 2009; Sun, 2014; Sun et al., 2014; He and Sun, 2017). It is the informality of social media that discourages accuracy of NER systems. While efforts in English have narrowed the gap between social media and formal domains (Cherry and Guo, 2015), the task in Chinese remains challenging. It is caused by Chinese logographic characters which lack many clues to indicate whether a word is a name, such as capitalization. The scant labelled Chinese social media corpus makes the task more challenging (Neelakantan and Collins, 2015; Skeppstedt, 2014; Liu et al., 2015). To address the problem, one approach is to use the lexical embeddings learnt from massive unlabeled text. To take better advantage of unlabeled text, Peng and Dredze (2015) evaluates three types of embeddings for Chinese text, and shows the effectiveness of positional character embeddings with experiments.", "startOffset": 8, "endOffset": 1024}, {"referenceID": 8, "context": "B-LSTM neural network can learn from past input features and LSTM layer makes it more efficient (Hammerton, 2003; Hochreiter and Schmidhuber, 1997; Chen et al., 2015; Graves et al., 2006).", "startOffset": 96, "endOffset": 187}, {"referenceID": 0, "context": "B-LSTM neural network can learn from past input features and LSTM layer makes it more efficient (Hammerton, 2003; Hochreiter and Schmidhuber, 1997; Chen et al., 2015; Graves et al., 2006).", "startOffset": 96, "endOffset": 187}, {"referenceID": 5, "context": "B-LSTM neural network can learn from past input features and LSTM layer makes it more efficient (Hammerton, 2003; Hochreiter and Schmidhuber, 1997; Chen et al., 2015; Graves et al., 2006).", "startOffset": 96, "endOffset": 187}, {"referenceID": 17, "context": "To combine transition probability into B-LSTM neural network, we construct a Max Margin Neural Network (MMNN) (Pei et al., 2014) based on B-LSTM.", "startOffset": 110, "endOffset": 128}, {"referenceID": 0, "context": "B-LSTM neural network can learn from past input features and LSTM layer makes it more efficient (Hammerton, 2003; Hochreiter and Schmidhuber, 1997; Chen et al., 2015; Graves et al., 2006). However, B-LSTM cannot learn sentence level label information. Huang et al. (2015) combine CRF to use sentence level label information.", "startOffset": 148, "endOffset": 272}, {"referenceID": 17, "context": "We define a structured margin loss\u2206(l, l) as Pei et al. (2014):", "startOffset": 45, "endOffset": 63}, {"referenceID": 3, "context": "The discrete features can be easily incorporated into neural network model (Collobert et al., 2011).", "startOffset": 75, "endOffset": 99}, {"referenceID": 15, "context": "We pre-trained embeddings using word2vec (Mikolov et al., 2013) with the skip-gram training model, without negative sampling and other default parameter settings.", "startOffset": 41, "endOffset": 63}, {"referenceID": 14, "context": "Like Mao et al. (2008), we use bigram features as follow:", "startOffset": 5, "endOffset": 23}, {"referenceID": 3, "context": "We use window approach (Collobert et al., 2011) to extract higher level Features from word feature vectors.", "startOffset": 23, "endOffset": 47}, {"referenceID": 3, "context": "We treat bigram features as discrete features (Collobert et al., 2011) for our neural network.", "startOffset": 46, "endOffset": 70}, {"referenceID": 20, "context": "We can observe all models in Table 3 achieve a much lower recall than precision (Pink et al., 2014).", "startOffset": 80, "endOffset": 99}], "year": 2017, "abstractText": "We focus on named entity recognition (NER) for Chinese social media. With massive unlabeled text and quite limited labelled corpus, we propose a semisupervised learning model based on BLSTM neural network. To take advantage of traditional methods in NER such as CRF, we combine transition probability with deep learning in our model. To bridge the gap between label accuracy and F-score of NER, we construct a model which can be directly trained on F-score. When considering the instability of Fscore driven method and meaningful information provided by label accuracy, we propose an integrated method to train on both F-score and label accuracy. Our integrated model yields substantial improvement over previous state-of-the-art result.", "creator": "LaTeX with hyperref package"}}}