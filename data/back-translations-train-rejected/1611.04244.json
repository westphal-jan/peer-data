{"id": "1611.04244", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Nov-2016", "title": "Classify or Select: Neural Architectures for Extractive Document Summarization", "abstract": "We present two novel and contrasting Recurrent Neural Network (RNN) based architectures for extractive summarization of documents. The Classifier based architecture sequentially accepts or rejects each sentence in the original document order for its membership in the final summary. The Selector architecture, on the other hand, is free to pick one sentence at a time in any arbitrary order to piece together the summary. Our models under both architectures jointly capture the notions of salience and redundancy of sentences. In addition, these models have the advantage of being very interpretable, since they allow visualization of their predictions broken up by abstract features such as information content, salience and redundancy. We show that our models reach or outperform state-of-the-art supervised models on two different corpora. We also recommend the conditions under which one architecture is superior to the other based on experimental evidence.", "histories": [["v1", "Mon, 14 Nov 2016 03:54:10 GMT  (217kb,D)", "http://arxiv.org/abs/1611.04244v1", "arXiv admin note: text overlap witharXiv:1611.04230"]], "COMMENTS": "arXiv admin note: text overlap witharXiv:1611.04230", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["ramesh nallapati", "bowen zhou", "mingbo ma"], "accepted": false, "id": "1611.04244"}, "pdf": {"name": "1611.04244.pdf", "metadata": {"source": "CRF", "title": "EXTRACTIVE DOCUMENT SUMMARIZATION", "authors": ["Ramesh Nallapati", "Bowen Zhou", "Mingbo Ma"], "emails": ["nallapati@us.ibm.com", "zhou@us.ibm.com", "mam@oregonstate.edu"], "sections": [{"heading": "1 INTRODUCTION", "text": "Summary techniques are mainly divided into two categories: extractive and abstract; extractive methods aim to select distinctive snippets, sentences or passages from documents; abstract abstract summary techniques aim to concisely paraphrase the information content in documentaries; traditional methods for extractive summary techniques can generally be categorised into greedy approaches (e.g. Carbonell & Goldstein (1998)); graphically based approaches to document summary (e.g. Radev & Erkan (2004)); and limited optimization approaches (e.g. McDonald (2007); neural web-based approaches to extractive summaries have recently become popular."}, {"heading": "2 TWO ARCHITECTURES", "text": "Our architectures are motivated by two intuitive strategies that people tend to adopt when tasked with extracting meaningful sentences in a document: the first strategy, which we call Classify, involves reading the entire document once to understand its contents, and then traversing through the sentences in the original document order and deciding whether or not each sentence belongs to the summary; the second strategy, which we call Select, involves memorizing the entire document once as before, and then selecting sentences that should each belong to the summary. Qualitatively, the latter strategy appears to be a better one, as it allows us to make globally optimal decisions at each step. While it can be more difficult for people to follow this strategy as we can expect the Select strategy to provide an advantage to the machines, as \"forgetfulness\" is not a real \"concern for them. In this work, we will empirically explore both strategies and make a recommendation on which strategy is optimal."}, {"heading": "2.1 CLASSIFIER ARCHITECTURE", "text": "The probability of the sentence belonging to the summary, P (yj = 1) is given as follows: P (yj = 1 | hj, sj, d, pj) = \u03c3 (score (hj, sj, d, pj) (2) The objective function that is minimized during training is the negative log probability of the training data names: \"(W, w, b) = \u2212 N \u2211 d = 1 (ydj logP (y d j = 1 | hdj, dd) Log (1 \u2212 ydj) Log (hdj = 1 | hdj, sdj, dd) D = 1 (ydj) D = 1 (ydj) D is the size of the training corpus and Nd is the number of sentences in the document."}, {"heading": "2.2 SELECTOR ARCHITECTURE", "text": "In this architecture, the models are not able to make decisions in the sequence of the sentence, but only in the sequence of the sentence, the sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence sequence"}, {"heading": "3 RELATED WORK", "text": "Previous researchers such as Shen et al. (2007) have suggested modelling extractive summaries of documents as a sequence classification problem using conditional random fields. Our approach differs from theirs in the sense that we use RNs in our model that do not require manual characteristics to represent sentences and documents. Generally, the selector architecture includes a ranking of sentences according to a specific criterion, so it corresponds to traditional methods for extractive summaries such as TextRank (Mihalcea & Tarau (2004), which also include a ranking of sentences according to salience and novelty. However, to the best of our knowledge, our selector framework is a novel, deep learning framework for extractive summaries."}, {"heading": "4 EXPERIMENTS AND RESULTS", "text": "This year, it is closer than ever before to being able to take the lead."}, {"heading": "5 DISCUSSION", "text": "We suspect that decision-making in the same order as the original sentence order may be advantageous for the summary of the document, as there is a smooth, sequential discourse structure in the news, starting with the most important climaxes of the story at the beginning and ending with closing remarks. If this is true, then, in scenarios where the sentence sequence is less structured, the selection architecture should be superior, as it has the freedom to select distinctive sentences in any order. Indeed, such scenarios occur in practice, such as the summary of a cluster of tweets on a topic where there is no specific discourse structure between individual tweets, or the summary of multiple documents in which a pair of sentences exists across document boundaries."}, {"heading": "6 CONCLUSION AND FUTURE WORK", "text": "In this paper, we propose two neural architectures for extractive summary. Our proposed models under these architectures are not only very easy to interpret, but also achieve state-of-the-art performance on two different sets of data. We also compare empirically our two frameworks and suggest conditions under which each of them can perform optimally. In our future work, we plan to further investigate the applicability of the novel selector architecture to relatively less structured summary problems such as the summary of multiple documents or recent tweets. In addition, we plan further experiments with the Daily Mail dataset, such as the inclusion of beam search in both model derivatives, and the generation of pseudo-ground truth that could lead to further performance improvements."}, {"heading": "7 APPENDIX", "text": "In this section, we will present some additional qualitative and quantitative analyses of our models, which we hope will shed some light on their behavior."}, {"heading": "7.1 VISUALIZATION OF MODEL OUTPUT", "text": "Our models are not only state-of-the-art, but also have the added advantage of being very easy to interpret. Clearly separated terms in the scoring function (see Equation 1) allow us to filter out various factors that are responsible for classifying / selecting each set. This is illustrated in Figure 3, where we present a representative document from our validation set along with normalized values from each abstract feature of the depth classification model. Such visualization is particularly useful in explaining to the end user the decisions made by the system."}, {"heading": "7.2 LEARNED IMPORTANCE WEIGHTS", "text": "As a confirmation of our intuition, the model learns that highlighting and redundancy are the most important predictive features for the summary of a sentence, followed by position characteristics and content characteristics. When the same model is trained on documents with randomly mixed sentences, it learns very little weight for the position characteristics, which is exactly what one expects."}, {"heading": "7.3 ABLATION EXPERIMENTS", "text": "The performance indicators shown in Table 5 show that removing one of the characteristics results in a small loss of performance. Note that the priority of the characteristics in the ablation experiments does not necessarily correspond to their priority in relation to the learned weights in Table 4, as feature correlations can affect the two metrics differently. Content and redundancy seem to be most important for the depth classifier, while dropping position characteristics is most damaging for the depth selector. Based on this analysis, we plan to investigate the reasons for the poor ablation performance of highlighting or redundancy in the classification and selector models in more detail."}, {"heading": "7.4 REPRESENTATIVE DOCUMENTS AND EXTRACTIVE SUMMARIES", "text": "We show a few representative documents, one each from the Daily Mail and the DUC Corpora, highlighting the sentences selected by Deep Classifier and comparing them with the gold summaries in Table 6. Examples qualitatively show that the model does a reasonably good job of identifying the key messages of a document."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1409.0473,", "citeRegEx": "Bahdanau et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Tgsum: Build tweet guided multi-document summarization", "author": ["Ziqiang Cao", "Chengyao Chen", "Wenjie Li", "Sujian Li", "Furu Wei", "Ming Zhou"], "venue": "dataset. CoRR,", "citeRegEx": "Cao et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Cao et al\\.", "year": 2015}, {"title": "Attsum: Joint learning of focusing and summarization with neural attention", "author": ["Ziqiang Cao", "Wenjie Li", "Sujian Li", "Furu Wei"], "venue": "arXiv preprint arXiv:1604.00125,", "citeRegEx": "Cao et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Cao et al\\.", "year": 2016}, {"title": "The use of mmr, diversity-based reranking for reordering documents and producing summaries", "author": ["Jaime Carbonell", "Jade Goldstein"], "venue": "In Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval,", "citeRegEx": "Carbonell and Goldstein.,? \\Q1998\\E", "shortCiteRegEx": "Carbonell and Goldstein.", "year": 1998}, {"title": "Neural summarization by extracting sentences and words. 54th Annual Meeting of the Association for Computational Linguistics, 2016", "author": ["Jianpeng Cheng", "Mirella Lapata"], "venue": "URL http://arxiv. org/abs/1603.07252", "citeRegEx": "Cheng and Lapata.,? \\Q2016\\E", "shortCiteRegEx": "Cheng and Lapata.", "year": 2016}, {"title": "Empirical evaluation of gated recurrent neural networks on sequence modeling", "author": ["Junyoung Chung", "\u00c7aglar G\u00fcl\u00e7ehre", "KyungHyun Cho", "Yoshua Bengio"], "venue": "CoRR, abs/1412.3555,", "citeRegEx": "Chung et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chung et al\\.", "year": 2014}, {"title": "Opinosis: a graph-based approach to abstractive summarization of highly redundant opinions", "author": ["Kavita Ganesan", "ChengXiang Zhai", "Jiawei Han"], "venue": "In Proceedings of the 23rd international conference on computational linguistics,", "citeRegEx": "Ganesan et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Ganesan et al\\.", "year": 2010}, {"title": "Teaching machines to read and comprehend", "author": ["Karl Moritz Hermann", "Tom\u00e1s Kocisk\u00fd", "Edward Grefenstette", "Lasse Espeholt", "Will Kay", "Mustafa Suleyman", "Phil Blunsom"], "venue": "CoRR, abs/1506.03340,", "citeRegEx": "Hermann et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hermann et al\\.", "year": 2015}, {"title": "Extractive summarization using continuous vector space models", "author": ["Mikael Kageback", "Olof Mogren", "Nina Tahmasebi", "Devdatt Dubhashi"], "venue": null, "citeRegEx": "Kageback et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kageback et al\\.", "year": 2014}, {"title": "A study of global inference algorithms in multi-document summarization", "author": ["Ryan McDonald"], "venue": null, "citeRegEx": "McDonald.,? \\Q2007\\E", "shortCiteRegEx": "McDonald.", "year": 2007}, {"title": "Textrank: Bringing order into texts", "author": ["Rada Mihalcea", "Paul Tarau"], "venue": "Empirical Methods in Natural Language Processing,", "citeRegEx": "Mihalcea and Tarau.,? \\Q2004\\E", "shortCiteRegEx": "Mihalcea and Tarau.", "year": 2004}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Abstractive text summarization using sequence-to-sequence rnns and beyond", "author": ["Ramesh Nallapati", "Bowen Zhou", "Cicero dos Santos", "Caglar Gulcehre", "Bing Xiang"], "venue": "The SIGNLL Conference on Computational Natural Language Learning,", "citeRegEx": "Nallapati et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Nallapati et al\\.", "year": 2016}, {"title": "Sequence-to-sequence rnns for text summarization", "author": ["Ramesh Nallapati", "Bowen Zhou", "Bing Xiang"], "venue": "International Conference on Learning Representations, Workshop track,", "citeRegEx": "Nallapati et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Nallapati et al\\.", "year": 2016}, {"title": "Order matters: Sequence to sequence for sets", "author": ["Manjunath Kudlur Oriol Vinyals", "Samy Bengio"], "venue": "International Conference on Learning Representations,", "citeRegEx": "Vinyals and Bengio.,? \\Q2015\\E", "shortCiteRegEx": "Vinyals and Bengio.", "year": 2015}, {"title": "Topical coherence for graph-based extractive summarization", "author": ["Daraksha Parveen", "Hans-Martin Ramsl", "Michael Strube"], "venue": "In Proceedings of the Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Parveen et al\\.,? \\Q1949\\E", "shortCiteRegEx": "Parveen et al\\.", "year": 1949}, {"title": "Lexrank: Graph-based lexical centrality as salience in text summarization", "author": ["Dragomir Radev", "G\u00fcnes Erkan"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Radev and Erkan.,? \\Q2004\\E", "shortCiteRegEx": "Radev and Erkan.", "year": 2004}, {"title": "A neural attention model for abstractive sentence summarization", "author": ["Alexander M Rush", "Sumit Chopra", "Jason Weston"], "venue": "arXiv preprint arXiv:1509.00685,", "citeRegEx": "Rush et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Rush et al\\.", "year": 2015}, {"title": "Document summarization using conditional random fields", "author": ["Dou Shen", "Jian-Tao Sun", "Hua Li", "Qiang Yang", "Zheng Chen"], "venue": "In Proceedings of IJCAI,", "citeRegEx": "Shen et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Shen et al\\.", "year": 2007}, {"title": "Dynamic pooling and unfolding recursive autoencoders for paraphrase detection", "author": ["Richard Socher", "Eric H. Huang", "Jeffrey Pennin", "Christopher D. Manning", "Andrew Y. Ng"], "venue": null, "citeRegEx": "Socher et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2011}, {"title": "Enhancing single-document summarization by combining ranknet and third-party sources", "author": ["Krysta M. Svore", "Lucy Vanderwende", "Christopher J.C. Burges"], "venue": "In Proceedings of the Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,", "citeRegEx": "Svore et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Svore et al\\.", "year": 2007}, {"title": "Towards a unified approach to simultaneous single-document and multidocument summarizations", "author": ["Xiaojun Wan"], "venue": "Proceedings of the 23rd COLING,", "citeRegEx": "Wan.,? \\Q2010\\E", "shortCiteRegEx": "Wan.", "year": 2010}, {"title": "Automatic generation of story highlights", "author": ["Kristian Woodsend", "Mirella Lapata"], "venue": "Proceedings of the 48th ACL, pp", "citeRegEx": "Woodsend and Lapata.,? \\Q2010\\E", "shortCiteRegEx": "Woodsend and Lapata.", "year": 2010}, {"title": "Optimizing sentence modeling and selection for document summarization", "author": ["Wenpeng Yin", "Yulong Pei"], "venue": "In Proceedings of the 24th International Conference on Artificial Intelligence,", "citeRegEx": "Yin and Pei.,? \\Q2015\\E", "shortCiteRegEx": "Yin and Pei.", "year": 2015}, {"title": "ADADELTA: an adaptive learning rate method", "author": ["Matthew D. Zeiler"], "venue": "CoRR, abs/1212.5701,", "citeRegEx": "Zeiler.,? \\Q2012\\E", "shortCiteRegEx": "Zeiler.", "year": 2012}], "referenceMentions": [{"referenceID": 4, "context": ", McDonald (2007)).", "startOffset": 2, "endOffset": 18}, {"referenceID": 4, "context": "For example, Kageback et al. (2014) employed the recursive autoencoder (Socher et al.", "startOffset": 13, "endOffset": 36}, {"referenceID": 4, "context": "For example, Kageback et al. (2014) employed the recursive autoencoder (Socher et al. (2011)) to summarize documents, producing best performance on the Opinosis dataset (Ganesan et al.", "startOffset": 13, "endOffset": 93}, {"referenceID": 3, "context": "(2011)) to summarize documents, producing best performance on the Opinosis dataset (Ganesan et al. (2010)).", "startOffset": 84, "endOffset": 106}, {"referenceID": 3, "context": "(2011)) to summarize documents, producing best performance on the Opinosis dataset (Ganesan et al. (2010)). Yin & Pei (2015) applied Convolutional Neural Networks (CNN) to project sentences to continuous vector space and then select sentences by minimizing the cost based on their \u2018prestige\u2019 and \u2018diverseness\u2019, on the task of multi-document extractive summarization.", "startOffset": 84, "endOffset": 125}, {"referenceID": 0, "context": "Another related work is that of Cao et al. (2016), who address the problem of query-focused multi-document summarization using query-attention-weighted CNNs.", "startOffset": 32, "endOffset": 50}, {"referenceID": 0, "context": "Recently, with the emergence of strong generative neural models for text Bahdanau et al. (2014), abstractive techniques are also becoming increasingly popular (Rush et al.", "startOffset": 73, "endOffset": 96}, {"referenceID": 0, "context": "Recently, with the emergence of strong generative neural models for text Bahdanau et al. (2014), abstractive techniques are also becoming increasingly popular (Rush et al. (2015), Nallapati et al.", "startOffset": 73, "endOffset": 179}, {"referenceID": 0, "context": "Recently, with the emergence of strong generative neural models for text Bahdanau et al. (2014), abstractive techniques are also becoming increasingly popular (Rush et al. (2015), Nallapati et al. (2016b) and Nallapati et al.", "startOffset": 73, "endOffset": 205}, {"referenceID": 0, "context": "Recently, with the emergence of strong generative neural models for text Bahdanau et al. (2014), abstractive techniques are also becoming increasingly popular (Rush et al. (2015), Nallapati et al. (2016b) and Nallapati et al. (2016a)).", "startOffset": 73, "endOffset": 234}, {"referenceID": 0, "context": "Recently, with the emergence of strong generative neural models for text Bahdanau et al. (2014), abstractive techniques are also becoming increasingly popular (Rush et al. (2015), Nallapati et al. (2016b) and Nallapati et al. (2016a)). Despite the emergence of abstractive techniques, extractive techniques are still attractive as they are less complex, less expensive, and generate grammatically and semantically correct summaries most of the time. In a very recent work, Cheng & Lapata (2016) proposed an attentional encoder-decoder for extractive single-document summarization and trained it on Daily Mail corpus, a large news data set, achieving state-of-the-art performance.", "startOffset": 73, "endOffset": 495}, {"referenceID": 0, "context": "Recently, with the emergence of strong generative neural models for text Bahdanau et al. (2014), abstractive techniques are also becoming increasingly popular (Rush et al. (2015), Nallapati et al. (2016b) and Nallapati et al. (2016a)). Despite the emergence of abstractive techniques, extractive techniques are still attractive as they are less complex, less expensive, and generate grammatically and semantically correct summaries most of the time. In a very recent work, Cheng & Lapata (2016) proposed an attentional encoder-decoder for extractive single-document summarization and trained it on Daily Mail corpus, a large news data set, achieving state-of-the-art performance. Like Cheng & Lapata (2016), our work also focuses only on sentential extractive summarization of single documents using neural networks.", "startOffset": 73, "endOffset": 707}, {"referenceID": 5, "context": "Shared Building Blocks: Both architectures begin with word-level bidirectional Gated Recurrent Unit (GRU) based RNNs (Chung et al. (2014)) run independently over each sentence in the document, where each time-step of the RNN corresponds to a word index in the sentence.", "startOffset": 118, "endOffset": 138}, {"referenceID": 18, "context": "Previous researcher such as Shen et al. (2007) have proposed modeling extractive document summarization as a sequence classification problem using Conditional Random Fields.", "startOffset": 28, "endOffset": 47}, {"referenceID": 18, "context": "Previous researcher such as Shen et al. (2007) have proposed modeling extractive document summarization as a sequence classification problem using Conditional Random Fields. Our approach is different from theirs in the sense that we use RNNs in our model that do not require any handcrafted features for representing sentences and documents. The Selector architecture broadly involves ranking of sentences by some criterion, therefore does correspond to traditional methods for extractive summarization such as TextRank (Mihalcea & Tarau (2004)) that also involve ranking of sentences by salience and novelty.", "startOffset": 28, "endOffset": 545}, {"referenceID": 18, "context": "Previous researcher such as Shen et al. (2007) have proposed modeling extractive document summarization as a sequence classification problem using Conditional Random Fields. Our approach is different from theirs in the sense that we use RNNs in our model that do not require any handcrafted features for representing sentences and documents. The Selector architecture broadly involves ranking of sentences by some criterion, therefore does correspond to traditional methods for extractive summarization such as TextRank (Mihalcea & Tarau (2004)) that also involve ranking of sentences by salience and novelty. However, to the best of our knowledge, our Selector framework is a novel deep learning framework for extractive summarization. Broader efforts are being made in the deep learning community to build more sophisticated sequence to sequence models towards the objective of automatically learning complex tasks such as sorting sequences (Oriol Vinyals (2015); Graves et al.", "startOffset": 28, "endOffset": 965}, {"referenceID": 18, "context": "Previous researcher such as Shen et al. (2007) have proposed modeling extractive document summarization as a sequence classification problem using Conditional Random Fields. Our approach is different from theirs in the sense that we use RNNs in our model that do not require any handcrafted features for representing sentences and documents. The Selector architecture broadly involves ranking of sentences by some criterion, therefore does correspond to traditional methods for extractive summarization such as TextRank (Mihalcea & Tarau (2004)) that also involve ranking of sentences by salience and novelty. However, to the best of our knowledge, our Selector framework is a novel deep learning framework for extractive summarization. Broader efforts are being made in the deep learning community to build more sophisticated sequence to sequence models towards the objective of automatically learning complex tasks such as sorting sequences (Oriol Vinyals (2015); Graves et al. (2014)), but their utility for extractive summarization remains to be explored.", "startOffset": 28, "endOffset": 987}, {"referenceID": 18, "context": "Previous researcher such as Shen et al. (2007) have proposed modeling extractive document summarization as a sequence classification problem using Conditional Random Fields. Our approach is different from theirs in the sense that we use RNNs in our model that do not require any handcrafted features for representing sentences and documents. The Selector architecture broadly involves ranking of sentences by some criterion, therefore does correspond to traditional methods for extractive summarization such as TextRank (Mihalcea & Tarau (2004)) that also involve ranking of sentences by salience and novelty. However, to the best of our knowledge, our Selector framework is a novel deep learning framework for extractive summarization. Broader efforts are being made in the deep learning community to build more sophisticated sequence to sequence models towards the objective of automatically learning complex tasks such as sorting sequences (Oriol Vinyals (2015); Graves et al. (2014)), but their utility for extractive summarization remains to be explored. In the deep learning framework, the extractive summarization work of Cheng & Lapata (2016) is the closest to our work.", "startOffset": 28, "endOffset": 1151}, {"referenceID": 18, "context": "Previous researcher such as Shen et al. (2007) have proposed modeling extractive document summarization as a sequence classification problem using Conditional Random Fields. Our approach is different from theirs in the sense that we use RNNs in our model that do not require any handcrafted features for representing sentences and documents. The Selector architecture broadly involves ranking of sentences by some criterion, therefore does correspond to traditional methods for extractive summarization such as TextRank (Mihalcea & Tarau (2004)) that also involve ranking of sentences by salience and novelty. However, to the best of our knowledge, our Selector framework is a novel deep learning framework for extractive summarization. Broader efforts are being made in the deep learning community to build more sophisticated sequence to sequence models towards the objective of automatically learning complex tasks such as sorting sequences (Oriol Vinyals (2015); Graves et al. (2014)), but their utility for extractive summarization remains to be explored. In the deep learning framework, the extractive summarization work of Cheng & Lapata (2016) is the closest to our work. Their model is based on an encoder-decoder approach where the encoder learns the representation of sentences and documents while the decoder classifies each sentence using an attention mechanism. Broadly, their model is also in the Classifier framework, but architecturally, our approaches are different. While their approach can be termed as a multi-pass approach where both the encoder and decoder consume the same sentence representations, our approach is a deep one where the representations learned by the bidirectional GRU encoder are in turn consumed by the Classifier or Selector models. Another key difference between our work and theirs is that unlike our unsupervised greedy approach to convert abstractive summaries to extractive labels, Cheng & Lapata (2016) chose to train a separate supervised classifier using manually created labels on a subset of the data.", "startOffset": 28, "endOffset": 1951}, {"referenceID": 13, "context": "We note that similar approaches have been employed by other researchers such as Svore et al. (2007) to handle the problem of converting abstractive summaries to extractive ground truth.", "startOffset": 80, "endOffset": 100}, {"referenceID": 1, "context": "We would also like to point readers to a recent work by Cao et al. (2015) that proposes an ILP based approach to solve this problem optimally.", "startOffset": 56, "endOffset": 74}, {"referenceID": 1, "context": "We would also like to point readers to a recent work by Cao et al. (2015) that proposes an ILP based approach to solve this problem optimally. Since this is not the focus of this work, we chose a simple greedy algorithm. Corpora: For our experiments, we used the Daily Mail corpus originally constructed by Hermann et al. (2015) for the task of passage-based question answering, and re-purposed for the task of document summarization as proposed in Cheng & Lapata (2016) for extractive summarization and Nallapati et al.", "startOffset": 56, "endOffset": 329}, {"referenceID": 1, "context": "We would also like to point readers to a recent work by Cao et al. (2015) that proposes an ILP based approach to solve this problem optimally. Since this is not the focus of this work, we chose a simple greedy algorithm. Corpora: For our experiments, we used the Daily Mail corpus originally constructed by Hermann et al. (2015) for the task of passage-based question answering, and re-purposed for the task of document summarization as proposed in Cheng & Lapata (2016) for extractive summarization and Nallapati et al.", "startOffset": 56, "endOffset": 471}, {"referenceID": 1, "context": "We would also like to point readers to a recent work by Cao et al. (2015) that proposes an ILP based approach to solve this problem optimally. Since this is not the focus of this work, we chose a simple greedy algorithm. Corpora: For our experiments, we used the Daily Mail corpus originally constructed by Hermann et al. (2015) for the task of passage-based question answering, and re-purposed for the task of document summarization as proposed in Cheng & Lapata (2016) for extractive summarization and Nallapati et al. (2016a) for abstractive summarization.", "startOffset": 56, "endOffset": 529}, {"referenceID": 1, "context": "We would also like to point readers to a recent work by Cao et al. (2015) that proposes an ILP based approach to solve this problem optimally. Since this is not the focus of this work, we chose a simple greedy algorithm. Corpora: For our experiments, we used the Daily Mail corpus originally constructed by Hermann et al. (2015) for the task of passage-based question answering, and re-purposed for the task of document summarization as proposed in Cheng & Lapata (2016) for extractive summarization and Nallapati et al. (2016a) for abstractive summarization. Overall, we have 196,557 training documents, 12,147 validation documents and 10,396 test documents from the Daily Mail corpus. On average, there are about 28 sentences per document in the training set, and an average of 3-4 sentences in the reference summaries. The average word count per document in the training set is 802. We also used the DUC 2002 single-document summarization dataset1 consisting of 567 documents as an additional out-of-domain test set to evaluate our models. Evaluation: In our experiments below, we evaluate the performance of our models using different variants of the Rouge metric2 computed with respect to the gold abstractive summaries. Following Cheng & Lapata (2016), we use limited length Rouge recall at 75 bytes of summary as well as 275 bytes on the Daily Mail corpus.", "startOffset": 56, "endOffset": 1258}, {"referenceID": 1, "context": "We would also like to point readers to a recent work by Cao et al. (2015) that proposes an ILP based approach to solve this problem optimally. Since this is not the focus of this work, we chose a simple greedy algorithm. Corpora: For our experiments, we used the Daily Mail corpus originally constructed by Hermann et al. (2015) for the task of passage-based question answering, and re-purposed for the task of document summarization as proposed in Cheng & Lapata (2016) for extractive summarization and Nallapati et al. (2016a) for abstractive summarization. Overall, we have 196,557 training documents, 12,147 validation documents and 10,396 test documents from the Daily Mail corpus. On average, there are about 28 sentences per document in the training set, and an average of 3-4 sentences in the reference summaries. The average word count per document in the training set is 802. We also used the DUC 2002 single-document summarization dataset1 consisting of 567 documents as an additional out-of-domain test set to evaluate our models. Evaluation: In our experiments below, we evaluate the performance of our models using different variants of the Rouge metric2 computed with respect to the gold abstractive summaries. Following Cheng & Lapata (2016), we use limited length Rouge recall at 75 bytes of summary as well as 275 bytes on the Daily Mail corpus. On DUC 2002 corpus, following the official guidelines, we use limited length Rouge recall at 75 words. We report the scores from Rouge-1, Rouge-2 and RougeL, which are computed using matches of unigrams, bigrams and longest common subsequences respectively, with the ground truth summaries. Baselines: On all datasets, we use Lead-3 model, which simply produces the leading three sentences of the document as the summary, as a baseline. On the Daily Mail and DUC 2002 corpora, we also report performance of LReg, a feature-rich logistic classifier used as a baseline by Cheng & Lapata (2016). On DUC 2002 corpus, we report several baselines such as Integer Linear Programming based approach (Woodsend & Lapata (2010)), and graph based approaches such as TGRAPH (Parveen et al.", "startOffset": 56, "endOffset": 1956}, {"referenceID": 1, "context": "We would also like to point readers to a recent work by Cao et al. (2015) that proposes an ILP based approach to solve this problem optimally. Since this is not the focus of this work, we chose a simple greedy algorithm. Corpora: For our experiments, we used the Daily Mail corpus originally constructed by Hermann et al. (2015) for the task of passage-based question answering, and re-purposed for the task of document summarization as proposed in Cheng & Lapata (2016) for extractive summarization and Nallapati et al. (2016a) for abstractive summarization. Overall, we have 196,557 training documents, 12,147 validation documents and 10,396 test documents from the Daily Mail corpus. On average, there are about 28 sentences per document in the training set, and an average of 3-4 sentences in the reference summaries. The average word count per document in the training set is 802. We also used the DUC 2002 single-document summarization dataset1 consisting of 567 documents as an additional out-of-domain test set to evaluate our models. Evaluation: In our experiments below, we evaluate the performance of our models using different variants of the Rouge metric2 computed with respect to the gold abstractive summaries. Following Cheng & Lapata (2016), we use limited length Rouge recall at 75 bytes of summary as well as 275 bytes on the Daily Mail corpus. On DUC 2002 corpus, following the official guidelines, we use limited length Rouge recall at 75 words. We report the scores from Rouge-1, Rouge-2 and RougeL, which are computed using matches of unigrams, bigrams and longest common subsequences respectively, with the ground truth summaries. Baselines: On all datasets, we use Lead-3 model, which simply produces the leading three sentences of the document as the summary, as a baseline. On the Daily Mail and DUC 2002 corpora, we also report performance of LReg, a feature-rich logistic classifier used as a baseline by Cheng & Lapata (2016). On DUC 2002 corpus, we report several baselines such as Integer Linear Programming based approach (Woodsend & Lapata (2010)), and graph based approaches such as TGRAPH (Parveen et al.", "startOffset": 56, "endOffset": 2081}, {"referenceID": 1, "context": "We would also like to point readers to a recent work by Cao et al. (2015) that proposes an ILP based approach to solve this problem optimally. Since this is not the focus of this work, we chose a simple greedy algorithm. Corpora: For our experiments, we used the Daily Mail corpus originally constructed by Hermann et al. (2015) for the task of passage-based question answering, and re-purposed for the task of document summarization as proposed in Cheng & Lapata (2016) for extractive summarization and Nallapati et al. (2016a) for abstractive summarization. Overall, we have 196,557 training documents, 12,147 validation documents and 10,396 test documents from the Daily Mail corpus. On average, there are about 28 sentences per document in the training set, and an average of 3-4 sentences in the reference summaries. The average word count per document in the training set is 802. We also used the DUC 2002 single-document summarization dataset1 consisting of 567 documents as an additional out-of-domain test set to evaluate our models. Evaluation: In our experiments below, we evaluate the performance of our models using different variants of the Rouge metric2 computed with respect to the gold abstractive summaries. Following Cheng & Lapata (2016), we use limited length Rouge recall at 75 bytes of summary as well as 275 bytes on the Daily Mail corpus. On DUC 2002 corpus, following the official guidelines, we use limited length Rouge recall at 75 words. We report the scores from Rouge-1, Rouge-2 and RougeL, which are computed using matches of unigrams, bigrams and longest common subsequences respectively, with the ground truth summaries. Baselines: On all datasets, we use Lead-3 model, which simply produces the leading three sentences of the document as the summary, as a baseline. On the Daily Mail and DUC 2002 corpora, we also report performance of LReg, a feature-rich logistic classifier used as a baseline by Cheng & Lapata (2016). On DUC 2002 corpus, we report several baselines such as Integer Linear Programming based approach (Woodsend & Lapata (2010)), and graph based approaches such as TGRAPH (Parveen et al. (2015)) and URANK (Wan (2010)) which achieve very high performance on this corpus.", "startOffset": 56, "endOffset": 2148}, {"referenceID": 1, "context": "We would also like to point readers to a recent work by Cao et al. (2015) that proposes an ILP based approach to solve this problem optimally. Since this is not the focus of this work, we chose a simple greedy algorithm. Corpora: For our experiments, we used the Daily Mail corpus originally constructed by Hermann et al. (2015) for the task of passage-based question answering, and re-purposed for the task of document summarization as proposed in Cheng & Lapata (2016) for extractive summarization and Nallapati et al. (2016a) for abstractive summarization. Overall, we have 196,557 training documents, 12,147 validation documents and 10,396 test documents from the Daily Mail corpus. On average, there are about 28 sentences per document in the training set, and an average of 3-4 sentences in the reference summaries. The average word count per document in the training set is 802. We also used the DUC 2002 single-document summarization dataset1 consisting of 567 documents as an additional out-of-domain test set to evaluate our models. Evaluation: In our experiments below, we evaluate the performance of our models using different variants of the Rouge metric2 computed with respect to the gold abstractive summaries. Following Cheng & Lapata (2016), we use limited length Rouge recall at 75 bytes of summary as well as 275 bytes on the Daily Mail corpus. On DUC 2002 corpus, following the official guidelines, we use limited length Rouge recall at 75 words. We report the scores from Rouge-1, Rouge-2 and RougeL, which are computed using matches of unigrams, bigrams and longest common subsequences respectively, with the ground truth summaries. Baselines: On all datasets, we use Lead-3 model, which simply produces the leading three sentences of the document as the summary, as a baseline. On the Daily Mail and DUC 2002 corpora, we also report performance of LReg, a feature-rich logistic classifier used as a baseline by Cheng & Lapata (2016). On DUC 2002 corpus, we report several baselines such as Integer Linear Programming based approach (Woodsend & Lapata (2010)), and graph based approaches such as TGRAPH (Parveen et al. (2015)) and URANK (Wan (2010)) which achieve very high performance on this corpus.", "startOffset": 56, "endOffset": 2171}, {"referenceID": 1, "context": "We would also like to point readers to a recent work by Cao et al. (2015) that proposes an ILP based approach to solve this problem optimally. Since this is not the focus of this work, we chose a simple greedy algorithm. Corpora: For our experiments, we used the Daily Mail corpus originally constructed by Hermann et al. (2015) for the task of passage-based question answering, and re-purposed for the task of document summarization as proposed in Cheng & Lapata (2016) for extractive summarization and Nallapati et al. (2016a) for abstractive summarization. Overall, we have 196,557 training documents, 12,147 validation documents and 10,396 test documents from the Daily Mail corpus. On average, there are about 28 sentences per document in the training set, and an average of 3-4 sentences in the reference summaries. The average word count per document in the training set is 802. We also used the DUC 2002 single-document summarization dataset1 consisting of 567 documents as an additional out-of-domain test set to evaluate our models. Evaluation: In our experiments below, we evaluate the performance of our models using different variants of the Rouge metric2 computed with respect to the gold abstractive summaries. Following Cheng & Lapata (2016), we use limited length Rouge recall at 75 bytes of summary as well as 275 bytes on the Daily Mail corpus. On DUC 2002 corpus, following the official guidelines, we use limited length Rouge recall at 75 words. We report the scores from Rouge-1, Rouge-2 and RougeL, which are computed using matches of unigrams, bigrams and longest common subsequences respectively, with the ground truth summaries. Baselines: On all datasets, we use Lead-3 model, which simply produces the leading three sentences of the document as the summary, as a baseline. On the Daily Mail and DUC 2002 corpora, we also report performance of LReg, a feature-rich logistic classifier used as a baseline by Cheng & Lapata (2016). On DUC 2002 corpus, we report several baselines such as Integer Linear Programming based approach (Woodsend & Lapata (2010)), and graph based approaches such as TGRAPH (Parveen et al. (2015)) and URANK (Wan (2010)) which achieve very high performance on this corpus. In addition, we also compare with the state-of-the art deep learning supervised extractive model from Cheng & Lapata (2016). Experimental Settings: We used 100-dimensional word2vec (Mikolov et al.", "startOffset": 56, "endOffset": 2348}, {"referenceID": 1, "context": "We would also like to point readers to a recent work by Cao et al. (2015) that proposes an ILP based approach to solve this problem optimally. Since this is not the focus of this work, we chose a simple greedy algorithm. Corpora: For our experiments, we used the Daily Mail corpus originally constructed by Hermann et al. (2015) for the task of passage-based question answering, and re-purposed for the task of document summarization as proposed in Cheng & Lapata (2016) for extractive summarization and Nallapati et al. (2016a) for abstractive summarization. Overall, we have 196,557 training documents, 12,147 validation documents and 10,396 test documents from the Daily Mail corpus. On average, there are about 28 sentences per document in the training set, and an average of 3-4 sentences in the reference summaries. The average word count per document in the training set is 802. We also used the DUC 2002 single-document summarization dataset1 consisting of 567 documents as an additional out-of-domain test set to evaluate our models. Evaluation: In our experiments below, we evaluate the performance of our models using different variants of the Rouge metric2 computed with respect to the gold abstractive summaries. Following Cheng & Lapata (2016), we use limited length Rouge recall at 75 bytes of summary as well as 275 bytes on the Daily Mail corpus. On DUC 2002 corpus, following the official guidelines, we use limited length Rouge recall at 75 words. We report the scores from Rouge-1, Rouge-2 and RougeL, which are computed using matches of unigrams, bigrams and longest common subsequences respectively, with the ground truth summaries. Baselines: On all datasets, we use Lead-3 model, which simply produces the leading three sentences of the document as the summary, as a baseline. On the Daily Mail and DUC 2002 corpora, we also report performance of LReg, a feature-rich logistic classifier used as a baseline by Cheng & Lapata (2016). On DUC 2002 corpus, we report several baselines such as Integer Linear Programming based approach (Woodsend & Lapata (2010)), and graph based approaches such as TGRAPH (Parveen et al. (2015)) and URANK (Wan (2010)) which achieve very high performance on this corpus. In addition, we also compare with the state-of-the art deep learning supervised extractive model from Cheng & Lapata (2016). Experimental Settings: We used 100-dimensional word2vec (Mikolov et al. (2013)) embeddings trained on the Daily Mail corpus as our embedding initialization.", "startOffset": 56, "endOffset": 2428}, {"referenceID": 1, "context": "We would also like to point readers to a recent work by Cao et al. (2015) that proposes an ILP based approach to solve this problem optimally. Since this is not the focus of this work, we chose a simple greedy algorithm. Corpora: For our experiments, we used the Daily Mail corpus originally constructed by Hermann et al. (2015) for the task of passage-based question answering, and re-purposed for the task of document summarization as proposed in Cheng & Lapata (2016) for extractive summarization and Nallapati et al. (2016a) for abstractive summarization. Overall, we have 196,557 training documents, 12,147 validation documents and 10,396 test documents from the Daily Mail corpus. On average, there are about 28 sentences per document in the training set, and an average of 3-4 sentences in the reference summaries. The average word count per document in the training set is 802. We also used the DUC 2002 single-document summarization dataset1 consisting of 567 documents as an additional out-of-domain test set to evaluate our models. Evaluation: In our experiments below, we evaluate the performance of our models using different variants of the Rouge metric2 computed with respect to the gold abstractive summaries. Following Cheng & Lapata (2016), we use limited length Rouge recall at 75 bytes of summary as well as 275 bytes on the Daily Mail corpus. On DUC 2002 corpus, following the official guidelines, we use limited length Rouge recall at 75 words. We report the scores from Rouge-1, Rouge-2 and RougeL, which are computed using matches of unigrams, bigrams and longest common subsequences respectively, with the ground truth summaries. Baselines: On all datasets, we use Lead-3 model, which simply produces the leading three sentences of the document as the summary, as a baseline. On the Daily Mail and DUC 2002 corpora, we also report performance of LReg, a feature-rich logistic classifier used as a baseline by Cheng & Lapata (2016). On DUC 2002 corpus, we report several baselines such as Integer Linear Programming based approach (Woodsend & Lapata (2010)), and graph based approaches such as TGRAPH (Parveen et al. (2015)) and URANK (Wan (2010)) which achieve very high performance on this corpus. In addition, we also compare with the state-of-the art deep learning supervised extractive model from Cheng & Lapata (2016). Experimental Settings: We used 100-dimensional word2vec (Mikolov et al. (2013)) embeddings trained on the Daily Mail corpus as our embedding initialization. We limited the vocabulary size to 150K and the maximum sentence length to 50 words, to speed up computation. We fixed the model hidden state size at 200. We used a batch size of 32 at training time, and employed adadelta (Zeiler (2012)) to train our model.", "startOffset": 56, "endOffset": 2742}, {"referenceID": 15, "context": "However, both models perform worse than graph-based TGRAPH (Parveen et al. (2015)) and URANK (Wan (2010)) algorithms, which are the state-of-the-art models on this corpus.", "startOffset": 60, "endOffset": 82}, {"referenceID": 15, "context": "However, both models perform worse than graph-based TGRAPH (Parveen et al. (2015)) and URANK (Wan (2010)) algorithms, which are the state-of-the-art models on this corpus.", "startOffset": 60, "endOffset": 105}, {"referenceID": 15, "context": "However, both models perform worse than graph-based TGRAPH (Parveen et al. (2015)) and URANK (Wan (2010)) algorithms, which are the state-of-the-art models on this corpus. Deep learning based supervised models such as ours and that of Cheng & Lapata (2016) perform very well on the domain they are trained on, but may suffer from domain adaptation issues when tested on a different corpus such as DUC 2002.", "startOffset": 60, "endOffset": 257}], "year": 2017, "abstractText": "We present two novel and contrasting Recurrent Neural Network (RNN) based architectures for extractive summarization of documents. The Classifier based architecture sequentially accepts or rejects each sentence in the original document order for its membership in the final summary. The Selector architecture, on the other hand, is free to pick one sentence at a time in any arbitrary order to piece together the summary. Our models under both architectures jointly capture the notions of salience and redundancy of sentences. In addition, these models have the advantage of being very interpretable, since they allow visualization of their predictions broken up by abstract features such as information content, salience and redundancy. We show that our models reach or outperform state-of-the-art supervised models on two different corpora. We also recommend the conditions under which one architecture is superior to the other based on experimental evidence.", "creator": "LaTeX with hyperref package"}}}