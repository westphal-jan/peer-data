{"id": "1611.04358", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Nov-2016", "title": "Character-level Convolutional Network for Text Classification Applied to Chinese Corpus", "abstract": "This article provides an interesting exploration of character-level convolutional neural network solving Chinese corpus text classification problem. We constructed a large-scale Chinese language dataset, and the result shows that character-level convolutional neural network works better on Chinese corpus than its corresponding pinyin format dataset. This is the first time that character-level convolutional neural network applied to text classification problem.", "histories": [["v1", "Mon, 14 Nov 2016 12:24:27 GMT  (1953kb,D)", "https://arxiv.org/abs/1611.04358v1", null], ["v2", "Tue, 15 Nov 2016 14:41:23 GMT  (1953kb,D)", "http://arxiv.org/abs/1611.04358v2", "MSc Thesis, 44 pages"]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["weijie huang", "jun wang"], "accepted": false, "id": "1611.04358"}, "pdf": {"name": "1611.04358.pdf", "metadata": {"source": "CRF", "title": "Character-level Convolutional Network for Text Classification Applied to Chinese Corpus", "authors": ["Weijie Huang", "Jun Wang", "Yixin Wu", "Rui Luo"], "emails": [], "sections": [{"heading": null, "text": "At the level of character classification, the Convolutional Network for Text ClassificationApplied to Chinese CorpusWeijie HuangA dissertation submitted in partial compliance with the requirements for the Master of ScienceinWeb Science & Big Data AnalyticsUniversity College London. Supervisor: Dr. Jun Wang Department of Computer ScienceUniversity College LondonNovember 16, 2016ar Xiv: 161 1.04 358v 2 [cs.C L] 15 Nov 201 62 I, Weijie Huang, confirm that the work presented in this dissertation belongs to me. Where information has been derived from other sources, I confirm that it was stated in the thesis. AbstractCompared with word and sentence level Convolutionary Neural Networks (ConvNets), the ConvNets character level exhibits better applicability to misspellings and typos."}, {"heading": "Acknowledgements", "text": "I would like to thank Supervisor Dr. Jun Wang, PhD student Yixin Wu, Rui Luo for their helpful feedback and advice. I learned a lot from them each week during the meeting, and their enthusiasm for research encourages me to complete this work, and I would also like to thank my roommate at the Institute of Education, Qing Xiang, who corrects many grammar mistakes that I did not notice in her spare time. Finally, I would like to thank Ars\u00e8ne Wenger for bringing several new players from the transfer market in these two months, so that I can focus on my thesis mainly with an optimistic attitude."}, {"heading": "1 Introduction 9", "text": "1.1 Background............................................................................................................................."}, {"heading": "2 Related work 15", "text": "2.1 Traditional models......................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................."}, {"heading": "3 Proposed solution 22", "text": "3.1 Pre-processed data......................................................................................................................."}, {"heading": "4 Results and Discussion 27", "text": "4.1. Factors that have affected ConvNets when applied to pinyinformat dataset..................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................."}, {"heading": "5 Conclusion 37", "text": "Annex 38"}, {"heading": "A User Manual 38", "text": "The specifications. The specifications. The specifications. The specifications. The specifications. The specifications. The specifications. The specifications. The specifications. The specifications. The specifications. The specifications. The specifications. The specifications. The specifications. The specifications. The specifications. The specifications. The specifications. The specifications. The specifications. The specifications. The specifications. The specifications. The specifications. The specifications. The specifications. The specifications. The specifications. The specifications. The specifications. The specifications. The specifications. The specifications. The specifications. The specifications. The specifications. The specifications. The specifications. The specifications. The specifications. The specifications. The specifications. The specifications. The specifications. The specifications. The specifications. The specifications. The specifications. The specifications. The specifications."}, {"heading": "1.1 Background", "text": "In general, NLP comprises the following three tasks: Part-Of-Speech Tagging (POS), e.g. text or image classification, to classify the data into different categories; Chunking (CHUNK), to label the segment of a given sentence using syntactic and semantic relationships; and Named Entity Recognition (NER), to identify named entities in the text [5]. These tasks are varied and range from character, word and sentence levels. Nevertheless, they have the same purpose of identifying hierarchical representations of the context [6]. One of the classic tasks for NLP is text classification, also known as documentation classification [4]. This task aims to assign a predefined label to the document. Usually, there are two phases involved in the process, in which certain features can be used for the first and second stages of classification."}, {"heading": "1.2 Previous solutions", "text": "In fact, it is the case that most of them are able to survive themselves if they do not abide by the rules that they have imposed on themselves, and that they are able to survive themselves, \"he told the Deutsche Presse-Agentur.\" I don't think they are able to survive themselves, \"he told the Deutsche Presse-Agentur.\" I don't think they are able to survive themselves, \"he told the Deutsche Presse-Agentur.\" But I don't think they are able to survive themselves, \"he said."}, {"heading": "1.3 Research problems", "text": "1.3. research problems 12Firstly, after a thorough search of relevant Chinese literature, it appears that no researchers have yet applied the revolutionary network at the character level to the Chinese character. Only some of the NLP tasks are based on pinyin encryption. However, pinyin input is one of the most popular forms of text input in the Chinese language [3]. This method represents Chinese characters alphabetically according to their pronunciation (see Figure 1.1). Previous researchers, such as Mathew, used pinyin format data sets as input to detect spam in the mobile text message. [24] and Liu, similarly, with pinyin format data sets for selection [23], demonstrated that pinyin format data sets are used as an efficient method of solving the NLP problems."}, {"heading": "1.4 Contribution", "text": "First, this thesis applies the Convolutionary Character-Level Neural Network to Chinese character sets, which are rarely explored in this NLP area. Results show that Chinese character sets have achieved a better result than the corresponding Pinyin encoding. Second, we have reached the state of the art in this specific task. Structure 14 is among all the other narrow Convolutionary Neural Character-Level Networks. We are also the first to construct a large-scale Chinese character set. Furthermore, we have expanded the Pinyin format, which has reached millions compared to the previous one."}, {"heading": "1.5 Structure", "text": "In this chapter, we have provided some background information on natural language processing, deep learning models and the Chinese language. We have also outlined the research questions and basic information about our model. Afterwards, we will discuss and compare related work on text classification in Chapter 2. Then, in Chapter 3, we will describe the architecture of our model. Next, in Chapter 4, we will show the details of experimental results and hyperparameter settings during the process. In addition, we will present our latest constructed datasets. Afterwards, some observations will be discussed. Finally, in Chapter 5, we will conclude the work done so far and forecast the future directions. Chapter 2 Related workDifferent researchers use different algorithms with regard to text classification. These approaches follow the same scheme, followed by feature extraction followed by classification. Traditional models include SVM [14], Naive Bayes [25], Bag of Words [8], N-gram [2] and their TF-IDF [33] version."}, {"heading": "2.1 Traditional models", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1.1 Bag of Words", "text": "The early references to the concept of \"Bag of Words\" can be found in Harris \"\" Distributional structure. \"This model uses the number of common words that appear as a feature in the training set [8]. This model can classify the topics using these keywords. For example, the three keywords\" Dow \"\" Jones \"\" Indexes \"may appear much more frequently in stock topic articles than Sport2.1. Traditional models 16 topic. However, since some of the words appear in each of these topics, this can affect the result. Therefore, the TF-IDF version\" Bag of Words \"adds an additional feature, the frequency of the reverse document, to reduce these influences. This model is also based on the word level, which means that many words have the same root but not the same number. In a way, by using the stemming technique, we can avoid this problem, but not all words that contain the same root, which can lead to a different problem."}, {"heading": "2.1.2 N-gram", "text": "The N-gram model in the text classification task can be considered an extension of the Bag-of-Word model [2]. In the language model, a N-gram model is typically used. Unlike the Bag-of-Word model described above, the N-gram model uses the most common N-continuous word combinations selected from the dataset as characteristics. For example, the model would calculate the appearance of the Dow Jones Indexes word combination in all areas before applying the predefined class that occupies the highest rank. Moreover, the TF-IDF version adds the frequency of the inverted document to avoid the problem of common words. This model is widely used in the NLP range due to its property, simplicity, and scalability."}, {"heading": "2.1.3 Bag of tricks", "text": "Trick Box is a simple and efficient approach to text classification. This algorithm can train the model on a billion words in no more than ten minutes and split a large number of data sets into millions of classes within a minute. [15] It is one of the greatest models to date as a traditional model in this field. On the one hand, this model can be trained at high speed. On the other hand, the result is quite close to the state of the art that uses ConvNets.2.2. Neural network models 17 at the drawing level."}, {"heading": "2.1.4 Graph representation classification", "text": "Yoshikawa [38] proposed a quick training method for text classification based on the classification of graphs, which treats the input text like a graph, and the graph structure can better represent the text structure; the result shows that graph representation can use rich structural information from texts, and this is the key to improving its accuracy."}, {"heading": "2.2 Neural Network models", "text": "There is also a lot of research that applies methods of deep learning to solve the problem of text classification."}, {"heading": "2.2.1 Recrusive Neural Network", "text": "A recursive neural network often comes with a parser. In Socher's [32] work, a parse tree is used in the feature extraction phase. However, most of the data set is not delivered with a parser, which means that this type of model is not generic enough. As we have observed, no related models are published in these two years."}, {"heading": "2.2.2 Recurrent Neural Network", "text": "A recursive neural network is like a particular recursive neural network. This model introduces the data sequentially, usually from left to right, sometimes bi-directionally. Liu [22] solved the task of sentiment analytics using this model. An embedding layer followed by a recursive layer is used to extract features and then feed them into the classification layer."}, {"heading": "2.2.3 Convolutional Neural Network", "text": "Research has shown that ConvNets is effective for NLP tasks [5]. [4], and the Convolutionary Filter can be used in the Extraction days function. Kim was one of the earliest researchers to use Convolutionary Neural Networking (ConvNets) for system classification of sentences [16]. However, in this paper, Kim [16] proposed a shallow neural network with an onomic layer that uses multiple widths and filters, followed by a maximum pooling layer over time. The fully connected layer with failure layers can then be combined. Neural network models 18features and send to the output layer. The word vectors in this model were initialized using the publicly available word2vec, which was trained on 100 billion words from Google News [26]. Comparison between several variants and traditional models applied to six datasets is reported in this paper, they are film ratings with one sentence per review."}, {"heading": "2.2.4 Convolutional Neural Network and Recurrent Neural Net-", "text": "There is some research that combines both ConvNets and Recurrent Neural Network. [37] Xiao [37] combined both Convolutional Network and Recurrent Network to extract the features and apply them to sentence classifications.2.2. Neural Network Models 21The following model includes embedding layers, Convolutionary layers, Recurrent layers, and the classification layers which are one, three, one, and one layers. Convolutional Network, with up to five layers, is used to extract hierarchical representations of features that serve as input for an LSTM. Word vectors have not been pre-trained, while there is an embedding layer to transform the \"one-hot vectors\" into a sequence of dense vectors. The eight datasets in this paper are the same as Zhang, and Xiao, show that it is possible to use a much smaller model to achieve the same performance."}, {"heading": "3.1 Data preprocessed", "text": "Our model starts with a data pre-processing stage. This stage is used to convert the original characters into encoded characters. However, there are two types of encoding method (see Figure 3.1), which are a uniform encoding and one of m encoding (Sparse encoding). For the uniform encoding, each word in our model is presented as a hot vector. The i-th symbol in this vector is set to one if it is the i-th element, while the rest of the symbol remains zero [41]. However, we use the one m encoding in our model. For the input corpus, we first construct a letter size equal to S, then we use this dictionary to quantify each character, and the characters that are not in the alphabet are replaced by a space."}, {"heading": "3.2 Embedding Layer", "text": "As we can see from Figure 3.2, the embedding layer accepts a two-dimensional tensor of size S * L, which is the encoded string. Normally, the embedding layer is used to reduce the dimension of the input tensor. Also, the zero feed helps to transform the input tensor into a fixed size. However, since we have completed these two operations in the previous steps, we can of course treat the embedding layer like a review table. After processing in the embedding layer, the output form can be treated like an image of size S * L, while the S is like the \"RGB\" dimension in computer vision. By converting the input character into the one-dimensional vector, the ConvNets can then extract the feature through the wavy core. 3.3."}, {"heading": "3.3 Convolutional Layer", "text": "In fact, it is a purely mental game, in which the aim is to find a solution that will put you at the forefront."}, {"heading": "3.4 Fully-Connected Layer", "text": "The fully bonded layer also knows the dense layer. At this point, all the features selected from the max pooling layer are combined. As already mentioned, the max pooling layer selects the k-most feature from each convolutionary core. The fully bonded layer can combine most of the useful assembly and then construct a hierarchical representation for the last stage, the output layer. The output layer used \"softmax\" as a nonlinear activation function, and due to the number of target classes, there are five neurons. Unlike the current state of the art, Conneau did not use any failure layers between the fully bonded layers. For a very deep architectural model, the stack normalization layer might be a better choice. However, since our model is not that deep, we still apply the fail layer and set the failure rate to \"0.1.\" Chapter 4Results and Discussions In this chapter, we will present our results and assign the results from the following two tasks, which we evaluate best based on the task."}, {"heading": "4.1 The factors that influenced the ConvNets when", "text": "Application to Pinyin dataset"}, {"heading": "4.1.1 Task description", "text": "In this task, we validated our models on one of the eight datasets in Zhang [41] s research. This dataset, which is designed to solve the problem of message categorization, is widely used in various researchs.The dataset was collected by Sogou [35], and the encoding is in Pinyin format.By comparing our models with previous studies of the same dataset, we prove that our model can achieve a state-of-the-art result with fewer parameters and a faster training speed among all the narrow ConvNets."}, {"heading": "4.1.2 Dataset Description", "text": "In the field of computer visualization, there are many large data sets used for image classification or object recognition, such as ImageNet [29], CIFAR [19] and their size is millions of layers with more than 1,000 classes."}, {"heading": "4.1. The factors that influenced the ConvNets when applied to pinyin format dataset28", "text": "Dataset is one of the eight large datasets constructed by Zhang. All datasets have character level, and the Pinyin dataset contains five classes, all of which are of equal size."}, {"heading": "4.1.3 Model setting", "text": "Here are the settings used in our experiments, and by comparing different hyperparameters, these settings were found to be best for this specific task. The ConvNets dictionary has to adapt to the context, even though the Pinyin format encoding and the English corpus are both based on the Roman alphabet. In previous research, researchers had to distinguish between upper and lower case letters [41] [37] [6], which means that the dimension of the dataset is at least fifty-two due to the size of the Roman alphabet. The worse result was found with such a distinction. Zhang explained that the differences between letter cases could affect semantics, and that this can lead to a regulation problem [41]. In the Pinyin format, which encodes datasets, there is no distinction between uppercase and lowercase letters. In addition, we added only four basic punctuations to the dictionary, one part in the two-level format, the Pinyin the two-dimensions are not the two-dimensions observed in the statistics."}, {"heading": "4.1. The factors that influenced the ConvNets when applied to pinyin format dataset29", "text": "We did not use a prefabricated method because our model is based entirely on character level, while the word level is usually the prefabricated method to avoid the local optimal problem. We set the drop-out rate to \"0.1\" between the fully connected layers. The drop-out layers prove useful to avoid the overfitting problem, so that we can achieve a similar result in both the training set and the test set. Batch size is 128, i.e. each time the model updates the parameters according to 128 data trained through the back propagation process. Training with the optimizer \"Adam\" [18] and the loss functions are \"categorical cross-entropy.\" Compared to other optimizers like SGD, the \"Adam\" can converge faster and lead to a better result."}, {"heading": "4.1.4 Result and Discussion", "text": "Our ConvNets reached the state of the art in a narrow Convolutionary Network. According to our comparison between different models, the proposed model reached the state of the art when the Convolutionary Layers are limited to seven layers. Meanwhile, the parameters of our model are up to 190 times lower than those of the other models. Detailed information on the result and discussion is listed below. The choice of the dictionary is important. The result in Figure 4.2 shows that the choice of the dictionary is one of the most important factors in character-based ConvNets. With the help of a corresponding dictionary, we reached the state of the art in a tight Convolutionary Network. Two reasons can lead to this result. Firstly, in previous research, the dictionary is not suitable for pinyin formatted coding of data sets. In the English Corpus, 26 Roman letters are used in all English articles."}, {"heading": "4.1. The factors that influenced the ConvNets when applied to pinyin format dataset30", "text": "During the pre-edited phase, we need to use the \"regular expressions\" to replace the word with an empty character when they are not in the dictionary. A customized dictionary replaces the useless characters with an empty character. This step is recognizable because we have removed the noise from an original song that can help extract the characteristics from the datasets. Finally, the corresponding dictionary can significantly reduce the parameters and improve the speed. Fewer parameters mean that we can train the model not only on a specific graphics card such as TITAN X with 12 GB of memory, but also on another graphics card with 4 or 8 GB of memory. ConvNets need stacked layers with proper hyperparameters. The result in Table 4.1 shows that we can achieve the art with less large layers than with the proposed individual cards with fine-tuning parameters."}, {"heading": "4.2. The comparison between Chinese character and its corresponding pinyin format dataset31", "text": "By combining the hierarchical representations in the fully connected layer, we can let the model automatically select suitable characteristics. For example, three sinuous layers with a number of characteristics of 128 are about twice as large as a folding layer with a number of characteristics of 350. Finally, the stacked layer provides a better result and requires fewer parameters. The result in Table 4.1 also shows that the depth of our character layer Con-vNets affects performance. The output of wavelayers is the most important part of this model because it can preserve the hierarchical representations of the context, and this is the key for the next level to classify the classes."}, {"heading": "4.2 The comparison between Chinese character and", "text": "the corresponding Pinyin dataset"}, {"heading": "4.2.1 Task description", "text": "In this task, we validated our models on two large-format datasets. The first dataset is the Pinyin encoding dataset, and another is the Chinese character dataset. These datasets are collected by Sogou [35] and then re-assigned and transformed for this essay. By comparing their performance, we can prove that ConvNets works better on character-level datasets with Chinese characters, and we will discuss the theory underlying this model."}, {"heading": "4.2. The comparison between Chinese character and its corresponding pinyin format dataset32", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.2.2 Dataset Description", "text": "In the field of text classification, there are no large-scale datasets with Chinese characters. Therefore, we have decided to create a new dataset with Chinese characters and the corresponding Pinyin version, the size of which is up to 1,150,000 by using data multiplication. We are combining the SogouCA and SogouCS news corpus from Sogou Lab [35], which comprises more than 3 million news articles and at least twenty categories. We designated the dataset by its domain names, which are part of its URL link. Furthermore, our dataset contains only five categories, namely sports, finance, alternating automotive and technology, as not all categories have enough data. These datasets range from 50,000 to 20,000, which are the five main classes in the original news article. If the length of each message content is less than 20 words, the corresponding data will be removed."}, {"heading": "4.2. The comparison between Chinese character and its corresponding pinyin format dataset33", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.2.3 Model setting", "text": "The dictionary of Chinese characters is different from the Pinyin format. There are more than six thousand common Chinese characters, which means that the dictionary size is quite large. In this task, we used all the Chinese characters that occur in both the learning set and the test set to construct the dictionary, and the dictionary size is 6,653. Normally, we need more than one character in Pinyin format to represent a Chinese character (see Figure 4.4), so we changed the input length to 250 according to the statistics of the learning set."}, {"heading": "4.2.4 Result and Discussion", "text": "We compared the results of the Chinese character encoding and the Pinyin format. To our knowledge, this is the first time that a wavy character-level network has been applied to Chinese characters. We discussed our potential find below. ConvNets character-level word segmentation problem in ChinaThe Table 4.3 shows that the ConvNets character level works well on both Pinyinformat encoding and Chinese character corpus.The error rate of the Pinyin encoding is between 8.41% and 8.87%, while the Chinese character corpus.The results are impressive, showing that the ConvNets character level can efficiently extract the features from the Chinese character corpus.We believe this is the first work to show that ConvNets can be applied to Chinese characters at the character level."}, {"heading": "4.2. The comparison between Chinese character and its corresponding pinyin format dataset34", "text": "Figure 4.3 shows that for the same sentence, different word segmentations can lead to a different meaning. At the ConvNets character level, the unit is character level, which means that revolutionary layers extract the character combination as a feature and ignore the segmentation between words. Previous research has shown that ConvNets at the character level can better handle the spelling and typing problem in the Roman alphabet. Researchers believe that the revolutionary core can ignore the differences between two words that share the same language roots such as suffix and prefix. [41] However, the data set in pinyin format is based on pronunciation. The data set cannot benefit from the language root, but must also suffer from the compressed information."}, {"heading": "4.2. The comparison between Chinese character and its corresponding pinyin format dataset35", "text": "The Chinese dataset can store more information than the Pinyin encoding dataset. Furthermore, this is the main reason that stacked layers have increased and the error rate has increased from 5.47% to 6.88% due to the overpass problem. We believe that there is potential to reduce the error rate by fine-tuning parameters, and the character ConvNets can also do better when the Chinese dataset grows. Pinyin dataset increases the information in some way. As we mentioned in the introductory chapter, with more than six thousand Chinese characters used, there are only four hundred six-syllable combinations, which means that some information is compressed during the process. However, the results show that pinyin datasets in the dataset format are competitive results, and this leads to the question, how can it happen? Here are the reasons we found in the analysis of each word."}, {"heading": "4.2. The comparison between Chinese character and its corresponding pinyin format dataset36", "text": "To some extent, the data augmentation helps to improve the performance in our dataset. In our dataset, we use two different ways to construct the dataset pinyin encoding format, and the size of the combined dataset reached about 1.15 million. The error rate seems to have increased with this combined dataset. However, to some extent, this feature cannot help improve the performance. As Table 4.2 shows, the dataset is combined with two different encodings. If the Constitutional Neural Network extracts a specific feature that comes from pinyin encoding type A, this feature cannot be used in the test set with pinyin encoding type B, even they represent the same meaning in this context. Furthermore, we assume that this will cause a significant decrease in accuracy. Nevertheless, the result remains almost the same, meaning that the data augmentation improves the results in some way, that we improve the results at the level of encryption in type B."}, {"heading": "Appendix A", "text": "User's Guide The source code and dataset are all available from the Github link: https: / / github.com / koalaGreener / Character-level-Convolutional-Network-for-Text-Classification-Applied-to-Chinese-CorpusA.1 Requirements Python2 / Python3KerasTensorflow (Run this model under CUDA, the training speed will be significantly increased, Nvidia video card required.) A.2 Components This repository contains the following components: Models: Different model settings for the different datasets. There are four different folders, which are 2008 models, Chinese characters, pinyim formatA, pinyim formatB. 2008 models represent the task 1 m'odel applied to Zhang's trailer dataset. The other models are all destined for task 2, and the datasets are constructed for this paper."}], "references": [{"title": "Applying convolutional neural networks concepts to hybrid nn-hmm model for speech recognition", "author": ["O. ABDEL-HAMID", "MOHAMED", "A.-R", "H. JIANG", "G. PENN"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2012}, {"title": "N-gram-based text categorization", "author": ["W.B. CAVNAR", "TRENKLE", "J. M"], "venue": "Ann Arbor MI 48113,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1994}, {"title": "A new statistical approach to chinese pinyin input", "author": ["Z. CHEN", "LEE", "K.-F"], "venue": "In Proceedings of the 38th Annual Meeting on Association for Computational Linguistics (Stroudsburg, PA, USA,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2000}, {"title": "A unified architecture for natural language processing: Deep neural networks with multitask learning", "author": ["R. COLLOBERT", "J. WESTON"], "venue": "In Proceedings of the 25th international conference on Machine learning (2008),", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2008}, {"title": "Natural language processing (almost) from scratch", "author": ["R. COLLOBERT", "J. WESTON", "L. BOTTOU", "M. KARLEN", "K. KAVUKCUOGLU", "P. KUKSA"], "venue": "Journal of Machine Learning Research 12,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2011}, {"title": "Very deep convolutional networks for natural language processing", "author": ["A. CONNEAU", "H. SCHWENK", "L. BARRAULT", "Y. LECUN"], "venue": "arXiv preprint arXiv:1606.01781", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2016}, {"title": "Tweet2vec: Character-based distributed representations for social media", "author": ["B. DHINGRA", "Z. ZHOU", "D. FITZPATRICK", "M. MUEHL", "W.W. COHEN"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2016}, {"title": "Deep residual learning for image recognition", "author": ["HE K", "ZHANG X", "REN S", "SUN"], "venue": "arXiv preprint arXiv:1512.03385", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2015}, {"title": "Delving deep into rectifiers: Surpassing human-level performance on imagenet classification", "author": ["HE K", "ZHANG X", "REN S", "SUN"], "venue": "In Proceedings of the IEEE International Conference on Computer Vision", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2015}, {"title": "Mining and summarizing customer reviews", "author": ["HU M", "LIU"], "venue": "In Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining (2004),", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2004}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["S. IOFFE", "C. SZEGEDY"], "venue": "arXiv preprint arXiv:1502.03167", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2015}, {"title": "Text categorization with support vector machines: Learning with many relevant features", "author": ["T. JOACHIMS"], "venue": "In European conference on machine learning", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1998}, {"title": "Bag of tricks for efficient text classification", "author": ["A. JOULIN", "E. GRAVE", "P. BOJANOWSKI", "T. MIKOLOV"], "venue": "arXiv preprint arXiv:1607.01759", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2016}, {"title": "Convolutional neural networks for sentence classification", "author": ["KIM Y"], "venue": "arXiv preprint arXiv:1408.5882", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2014}, {"title": "Character-aware neural language models", "author": ["Y. KIM", "Y. JERNITE", "D. SONTAG", "A.M. RUSH"], "venue": "arXiv preprint arXiv:1508.06615", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2015}, {"title": "Adam: A method for stochastic optimization", "author": ["KINGMA D", "BA"], "venue": "arXiv preprint arXiv:1412.6980", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2014}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. LECUN", "L. BOTTOU", "Y. BENGIO", "P. HAFFNER"], "venue": "Proceedings of the IEEE 86,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1998}, {"title": "Learning question classifiers", "author": ["X. LI", "D. ROTH"], "venue": "In Proceedings of the 19th international conference on Computational linguistics-Volume", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2002}, {"title": "Sentiment analysis and opinion mining", "author": ["LIU B"], "venue": "Synthesis lectures on human language technologies 5,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2012}, {"title": "Short text feature selection for micro-blog mining", "author": ["LIU Z", "YU W", "CHEN W", "WANG S", "WU"], "venue": "In Computational Intelligence and Software Engineering (CiSE),", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2010}, {"title": "Intelligent spam classification for mobile text message", "author": ["K. MATHEW", "B. ISSAC"], "venue": "In Computer Science and Network Technology (ICCSNT),", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2011}, {"title": "A comparison of event models for naive bayes text classification. In AAAI-98 workshop on learning for text categorization", "author": ["A. MCCALLUM", "K NIGAM"], "venue": null, "citeRegEx": "25", "shortCiteRegEx": "25", "year": 1998}, {"title": "Efficient estimation of word representations in vector space", "author": ["T. MIKOLOV", "K. CHEN", "G. CORRADO", "J. DEAN"], "venue": "arXiv preprint arXiv:1301.3781", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2013}, {"title": "Rectified linear units improve restricted boltzmann machines", "author": ["V. NAIR", "G.E. HINTON"], "venue": "In Proceedings of the 27th International Conference on Machine Learning", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2010}, {"title": "Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales. In Proceedings of the 43rd annual meeting on association for computational linguistics", "author": ["PANG B", "LEE"], "venue": null, "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2005}, {"title": "Unitn: Training deep convolutional neural network for twitter sentiment classification", "author": ["A. SEVERYN", "A. MOSCHITTI"], "venue": "In Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2015}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. SIMONYAN", "A. ZISSERMAN"], "venue": "arXiv preprint arXiv:1409.1556", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2014}, {"title": "Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the conference on empirical methods in natural language processing", "author": ["R. SOCHER", "A. PERELYGIN", "J.Y. WU", "J. CHUANG", "C.D. MANNING", "A.Y. NG", "C. POTTS"], "venue": null, "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2013}, {"title": "A statistical interpretation of term specificity and its application in retrieval", "author": ["K. SPARCK JONES"], "venue": "Journal of documentation 28,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 1972}, {"title": "Document modeling with gated recurrent neural network for sentiment classification", "author": ["TANG D", "QIN B", "LIU"], "venue": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2015}, {"title": "Automatic online news issue construction in web environment", "author": ["WANG C", "ZHANG M", "MA S", "RU"], "venue": "In Proceedings of the 17th international conference on World Wide Web (2008),", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2008}, {"title": "Annotating expressions of opinions and emotions in language. Language resources and evaluation", "author": ["J. WIEBE", "T. WILSON", "C. CARDIE"], "venue": null, "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2005}, {"title": "Efficient character-level document classification by combining convolution and recurrent layers", "author": ["XIAO Y", "CHO"], "venue": "arXiv preprint arXiv:1602.00367", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2016}, {"title": "Fast training of a graph boosting for large-scale text classification", "author": ["H. YOSHIKAWA", "T. IWAKURA"], "venue": "In Pacific Rim International Conference on Artificial Intelligence", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2016}, {"title": "Adadelta: an adaptive learning rate method", "author": ["M.D. ZEILER"], "venue": "arXiv preprint arXiv:1212.5701", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2012}, {"title": "Shallow convolutional neural network for implicit discourse relation recognition", "author": ["ZHANG B", "SU J", "XIONG D", "LU Y", "DUAN H", "YAO"], "venue": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2015}, {"title": "Character-level convolutional networks for text classification", "author": ["X. ZHANG", "J. ZHAO", "Y. LECUN"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2015}], "referenceMentions": [{"referenceID": 36, "context": "The result of Bag of Words and N-gram came from Zhang[41], which are the references of this task.", "startOffset": 53, "endOffset": 57}, {"referenceID": 5, "context": "Natural language processing (NLP) is the field in which through analysing data, the machine can extract information from contexts and represent the input information in a different way[6].", "startOffset": 184, "endOffset": 187}, {"referenceID": 4, "context": "Part-Of-Speech tagging (POS), such as text or image classification, to classify the data with different categories; Chunking (CHUNK), to label the segment of a given sentence by using syntactic and semantic relations; and Named Entity Recognition (NER), to tag named entities in text[5].", "startOffset": 283, "endOffset": 286}, {"referenceID": 5, "context": "Nevertheless, they have the same purpose of finding out the hierarchical representations of the context[6].", "startOffset": 103, "endOffset": 106}, {"referenceID": 3, "context": "One of the classic tasks for NLP is text classification, also known as document classification[4].", "startOffset": 94, "endOffset": 97}, {"referenceID": 4, "context": "[5] Take BBC sports website for example, in its content, there are many specific Premier League team names, which can serve as corresponding features for the following classification.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "A common approach to text classification is to use Bag of Words[8] , N-gram[2], and their term frequency-inverse document frequency (TF-IDF)[33] as features, and traditional models such as SVM[14], Naive Bayes[25] as classifiers.", "startOffset": 75, "endOffset": 78}, {"referenceID": 28, "context": "A common approach to text classification is to use Bag of Words[8] , N-gram[2], and their term frequency-inverse document frequency (TF-IDF)[33] as features, and traditional models such as SVM[14], Naive Bayes[25] as classifiers.", "startOffset": 140, "endOffset": 144}, {"referenceID": 11, "context": "A common approach to text classification is to use Bag of Words[8] , N-gram[2], and their term frequency-inverse document frequency (TF-IDF)[33] as features, and traditional models such as SVM[14], Naive Bayes[25] as classifiers.", "startOffset": 192, "endOffset": 196}, {"referenceID": 21, "context": "A common approach to text classification is to use Bag of Words[8] , N-gram[2], and their term frequency-inverse document frequency (TF-IDF)[33] as features, and traditional models such as SVM[14], Naive Bayes[25] as classifiers.", "startOffset": 209, "endOffset": 213}, {"referenceID": 36, "context": "However, recently, many researchers[41][5][16][6], using deep learning model, particularly the convolutional neural networks (ConvNets), have made significant progress in computer vision[9] and speech recognition[1].", "startOffset": 35, "endOffset": 39}, {"referenceID": 4, "context": "However, recently, many researchers[41][5][16][6], using deep learning model, particularly the convolutional neural networks (ConvNets), have made significant progress in computer vision[9] and speech recognition[1].", "startOffset": 39, "endOffset": 42}, {"referenceID": 13, "context": "However, recently, many researchers[41][5][16][6], using deep learning model, particularly the convolutional neural networks (ConvNets), have made significant progress in computer vision[9] and speech recognition[1].", "startOffset": 42, "endOffset": 46}, {"referenceID": 5, "context": "However, recently, many researchers[41][5][16][6], using deep learning model, particularly the convolutional neural networks (ConvNets), have made significant progress in computer vision[9] and speech recognition[1].", "startOffset": 46, "endOffset": 49}, {"referenceID": 7, "context": "However, recently, many researchers[41][5][16][6], using deep learning model, particularly the convolutional neural networks (ConvNets), have made significant progress in computer vision[9] and speech recognition[1].", "startOffset": 186, "endOffset": 189}, {"referenceID": 0, "context": "However, recently, many researchers[41][5][16][6], using deep learning model, particularly the convolutional neural networks (ConvNets), have made significant progress in computer vision[9] and speech recognition[1].", "startOffset": 212, "endOffset": 215}, {"referenceID": 16, "context": "ConvNets, originally invented by LeCun[20] for computer vision, refers to the model that uses convolution kernels to extract local features.", "startOffset": 38, "endOffset": 42}, {"referenceID": 35, "context": "Analyses have shown that ConvNets is effective for NLP tasks[40][30], and the convolution filter can be utilised in the feature extraction stages.", "startOffset": 60, "endOffset": 64}, {"referenceID": 25, "context": "Analyses have shown that ConvNets is effective for NLP tasks[40][30], and the convolution filter can be utilised in the feature extraction stages.", "startOffset": 64, "endOffset": 68}, {"referenceID": 36, "context": "Compared with the model listed above, ConvNets, when applied to text classification has shown rather competitive results[41][5][16][6].", "startOffset": 120, "endOffset": 124}, {"referenceID": 4, "context": "Compared with the model listed above, ConvNets, when applied to text classification has shown rather competitive results[41][5][16][6].", "startOffset": 124, "endOffset": 127}, {"referenceID": 13, "context": "Compared with the model listed above, ConvNets, when applied to text classification has shown rather competitive results[41][5][16][6].", "startOffset": 127, "endOffset": 131}, {"referenceID": 5, "context": "Compared with the model listed above, ConvNets, when applied to text classification has shown rather competitive results[41][5][16][6].", "startOffset": 131, "endOffset": 134}, {"referenceID": 36, "context": "Also, since the ConvNets method can handle the misspelling problem, it works well for user-generated data[41].", "startOffset": 105, "endOffset": 109}, {"referenceID": 4, "context": "Recent approach of using ConvNets on text classification mainly works at the word-level [5] [16].", "startOffset": 88, "endOffset": 91}, {"referenceID": 13, "context": "Recent approach of using ConvNets on text classification mainly works at the word-level [5] [16].", "startOffset": 92, "endOffset": 96}, {"referenceID": 32, "context": "This may diminish the classification accuracy of the task[37].", "startOffset": 57, "endOffset": 61}, {"referenceID": 36, "context": "In the last few years, many researchers found that it is also likely to train a ConvNets at the character-level[41][37][6].", "startOffset": 111, "endOffset": 115}, {"referenceID": 32, "context": "In the last few years, many researchers found that it is also likely to train a ConvNets at the character-level[41][37][6].", "startOffset": 115, "endOffset": 119}, {"referenceID": 5, "context": "In the last few years, many researchers found that it is also likely to train a ConvNets at the character-level[41][37][6].", "startOffset": 119, "endOffset": 122}, {"referenceID": 14, "context": "Kim used a character sequence as an input in his language model[17], and Dhingra applied this idea to predict the hashtags[7].", "startOffset": 63, "endOffset": 67}, {"referenceID": 6, "context": "Kim used a character sequence as an input in his language model[17], and Dhingra applied this idea to predict the hashtags[7].", "startOffset": 122, "endOffset": 125}, {"referenceID": 2, "context": "Pinyin input is one of the most popular forms of text input in the Chinese language[3].", "startOffset": 83, "endOffset": 86}, {"referenceID": 20, "context": "Previous researchers such as Mathew used pinyin format dataset as input to detect spam in mobile text message[24] and Liu, similarly, using pinyin format dataset for feature selection[23], proved that pinyin format dataset could be utilised as an efficient method to solve the NLP problems.", "startOffset": 109, "endOffset": 113}, {"referenceID": 19, "context": "Previous researchers such as Mathew used pinyin format dataset as input to detect spam in mobile text message[24] and Liu, similarly, using pinyin format dataset for feature selection[23], proved that pinyin format dataset could be utilised as an efficient method to solve the NLP problems.", "startOffset": 183, "endOffset": 187}, {"referenceID": 2, "context": "For example, in the Chinese language, there are only 406 syllable combinations can be found in pinyin format representation among more than 6,000 commonly used Chinese characters, which means that some information is compressed during the transforming[3].", "startOffset": 251, "endOffset": 254}, {"referenceID": 11, "context": "Traditional models include SVM [14], Naive Bayes [25], Bag of Words [8], N-gram[2], and their TF-IDF [33] version.", "startOffset": 31, "endOffset": 35}, {"referenceID": 21, "context": "Traditional models include SVM [14], Naive Bayes [25], Bag of Words [8], N-gram[2], and their TF-IDF [33] version.", "startOffset": 49, "endOffset": 53}, {"referenceID": 1, "context": "Traditional models include SVM [14], Naive Bayes [25], Bag of Words [8], N-gram[2], and their TF-IDF [33] version.", "startOffset": 79, "endOffset": 82}, {"referenceID": 28, "context": "Traditional models include SVM [14], Naive Bayes [25], Bag of Words [8], N-gram[2], and their TF-IDF [33] version.", "startOffset": 101, "endOffset": 105}, {"referenceID": 36, "context": "The result in Zhang\u015b paper[41] shows that Bag of Words model and its TF-IDF version can achieve great performances in most of the tasks.", "startOffset": 26, "endOffset": 30}, {"referenceID": 1, "context": "The N-gram model in text classification task can be seen as an extension of the Bag of Word model[2].", "startOffset": 97, "endOffset": 100}, {"referenceID": 12, "context": "This algorithm can train the model in no more than ten minutes on one billion words, and classify a large number of datasets among millions of classes within a minute[15].", "startOffset": 166, "endOffset": 170}, {"referenceID": 33, "context": "Yoshikawa[38] proposed a fast training method for text classification which is based on graph classification.", "startOffset": 9, "endOffset": 13}, {"referenceID": 27, "context": "In Socher\u2019s[32] work, a parse tree is being used in the feature extraction stage.", "startOffset": 11, "endOffset": 15}, {"referenceID": 18, "context": "Liu [22] solved the sentiment analytics task using this model.", "startOffset": 4, "endOffset": 8}, {"referenceID": 4, "context": "Research has also shown that ConvNets is effective for NLP tasks [5][4], and the convolutional filter can be utilised in the feature extraction stages.", "startOffset": 65, "endOffset": 68}, {"referenceID": 3, "context": "Research has also shown that ConvNets is effective for NLP tasks [5][4], and the convolutional filter can be utilised in the feature extraction stages.", "startOffset": 68, "endOffset": 71}, {"referenceID": 13, "context": "Kim was one of the earliest researchers who used convolutional neural networks (ConvNets) for sentence classification[16].", "startOffset": 117, "endOffset": 121}, {"referenceID": 13, "context": "In this paper, Kim[16] proposed a word-level shallow neural network with one convolutional layer using multiple widths and filters followed by a max-pooling layer over time.", "startOffset": 18, "endOffset": 22}, {"referenceID": 22, "context": "The word vectors in this model were initialised using the publicly available word2vec, which was trained on 100 billion words from Google News[26].", "startOffset": 142, "endOffset": 146}, {"referenceID": 24, "context": "The comparison between several variants and traditional models applied on six datasets are reported in this paper, they are movie reviews with one sentence per review [28], TREC question dataset [21], a dataset for classifying the sentence whether subjective or objective[28], customer reviews of various products [12], opinion polarity detection subtask of the MPQA dataset [36], in particular, Stanford Sentiment Treebank [32].", "startOffset": 167, "endOffset": 171}, {"referenceID": 17, "context": "The comparison between several variants and traditional models applied on six datasets are reported in this paper, they are movie reviews with one sentence per review [28], TREC question dataset [21], a dataset for classifying the sentence whether subjective or objective[28], customer reviews of various products [12], opinion polarity detection subtask of the MPQA dataset [36], in particular, Stanford Sentiment Treebank [32].", "startOffset": 195, "endOffset": 199}, {"referenceID": 24, "context": "The comparison between several variants and traditional models applied on six datasets are reported in this paper, they are movie reviews with one sentence per review [28], TREC question dataset [21], a dataset for classifying the sentence whether subjective or objective[28], customer reviews of various products [12], opinion polarity detection subtask of the MPQA dataset [36], in particular, Stanford Sentiment Treebank [32].", "startOffset": 271, "endOffset": 275}, {"referenceID": 9, "context": "The comparison between several variants and traditional models applied on six datasets are reported in this paper, they are movie reviews with one sentence per review [28], TREC question dataset [21], a dataset for classifying the sentence whether subjective or objective[28], customer reviews of various products [12], opinion polarity detection subtask of the MPQA dataset [36], in particular, Stanford Sentiment Treebank [32].", "startOffset": 314, "endOffset": 318}, {"referenceID": 31, "context": "The comparison between several variants and traditional models applied on six datasets are reported in this paper, they are movie reviews with one sentence per review [28], TREC question dataset [21], a dataset for classifying the sentence whether subjective or objective[28], customer reviews of various products [12], opinion polarity detection subtask of the MPQA dataset [36], in particular, Stanford Sentiment Treebank [32].", "startOffset": 375, "endOffset": 379}, {"referenceID": 27, "context": "The comparison between several variants and traditional models applied on six datasets are reported in this paper, they are movie reviews with one sentence per review [28], TREC question dataset [21], a dataset for classifying the sentence whether subjective or objective[28], customer reviews of various products [12], opinion polarity detection subtask of the MPQA dataset [36], in particular, Stanford Sentiment Treebank [32].", "startOffset": 424, "endOffset": 428}, {"referenceID": 34, "context": "Kim selected the stochastic gradient descent (SGD) and Ada-delta update rule[39] for his model.", "startOffset": 76, "endOffset": 80}, {"referenceID": 26, "context": "Only one convolutional layer was constructed in this ConvNets, while the trend in computer vision where significant improvements have been reported using much deeper networks, 19 layers[31], or even up to 152 layers[9].", "startOffset": 185, "endOffset": 189}, {"referenceID": 7, "context": "Only one convolutional layer was constructed in this ConvNets, while the trend in computer vision where significant improvements have been reported using much deeper networks, 19 layers[31], or even up to 152 layers[9].", "startOffset": 215, "endOffset": 218}, {"referenceID": 36, "context": "Zhang was the first one who proposed an entirely character-level convolutional networks for text classification[41].", "startOffset": 111, "endOffset": 115}, {"referenceID": 36, "context": "The input of this model is a sequence of encoding vectors, which is done by \u2018applied an alphabet of size n for the input documents, and then quantise each character using one-hot encoding[41]\u2019.", "startOffset": 187, "endOffset": 191}, {"referenceID": 22, "context": "such as word2vec[26] for the input word vectors in the model.", "startOffset": 16, "endOffset": 20}, {"referenceID": 11, "context": "A comparison was made among traditional models such as SVM[14], Naive Bayes [25], Bag of Words, N-grams[2], and their TF-IDF version[33], also among deep learning models such as word-level ConvNets and long-short-term memory model(LSTM)[11].", "startOffset": 58, "endOffset": 62}, {"referenceID": 21, "context": "A comparison was made among traditional models such as SVM[14], Naive Bayes [25], Bag of Words, N-grams[2], and their TF-IDF version[33], also among deep learning models such as word-level ConvNets and long-short-term memory model(LSTM)[11].", "startOffset": 76, "endOffset": 80}, {"referenceID": 1, "context": "A comparison was made among traditional models such as SVM[14], Naive Bayes [25], Bag of Words, N-grams[2], and their TF-IDF version[33], also among deep learning models such as word-level ConvNets and long-short-term memory model(LSTM)[11].", "startOffset": 103, "endOffset": 106}, {"referenceID": 28, "context": "A comparison was made among traditional models such as SVM[14], Naive Bayes [25], Bag of Words, N-grams[2], and their TF-IDF version[33], also among deep learning models such as word-level ConvNets and long-short-term memory model(LSTM)[11].", "startOffset": 132, "endOffset": 136}, {"referenceID": 5, "context": "Conneau[6] was the first one who implemented a very deep convolutional architecture which is up to 29 convolutional layers (9, 17, 29, 49 respectively) and applied to sentence classification.", "startOffset": 7, "endOffset": 10}, {"referenceID": 5, "context": "The k-max pooling layer is followed to obtained the most important features of the stack of convolutional blocks[6].", "startOffset": 112, "endOffset": 115}, {"referenceID": 32, "context": "The datasets in this paper are the same corpora of Zhang and Xiao [37], and the best results of them are the baseline in this article.", "startOffset": 66, "endOffset": 70}, {"referenceID": 5, "context": "However, in this work, Conneau [6] created an architecture which used many layers of small convolutional kernel, which is size three.", "startOffset": 31, "endOffset": 34}, {"referenceID": 5, "context": "Compared with Zhangs ConvNets architecture, Conneau[6] found that better not to use dropout layer with the fully-connected layers, but only temporal batch normalization[13] after convolutional layers.", "startOffset": 51, "endOffset": 54}, {"referenceID": 10, "context": "Compared with Zhangs ConvNets architecture, Conneau[6] found that better not to use dropout layer with the fully-connected layers, but only temporal batch normalization[13] after convolutional layers.", "startOffset": 168, "endOffset": 172}, {"referenceID": 7, "context": "As described in He[9], the gain in accuracy due to the increase of the depth is limited when using standard ConvNets.", "startOffset": 18, "endOffset": 21}, {"referenceID": 32, "context": "There are also some researches that combine both ConvNets and Recurrent Neural Network[37][34].", "startOffset": 86, "endOffset": 90}, {"referenceID": 29, "context": "There are also some researches that combine both ConvNets and Recurrent Neural Network[37][34].", "startOffset": 90, "endOffset": 94}, {"referenceID": 32, "context": "Xiao[37] combined both convolutional network and recurrent network to extract the features and applied to sentence classification.", "startOffset": 4, "endOffset": 8}, {"referenceID": 32, "context": "Finally, there is an optimal level of local features to be fed into the recurrent layer, because Xiao noticed that the model accuracy does not always increase with the depth of convolutional layers[37].", "startOffset": 197, "endOffset": 201}, {"referenceID": 36, "context": "The i-th symbol in this vector is set to one if it is the i-th element, while the rest of the symbol are remain zero[41].", "startOffset": 116, "endOffset": 120}, {"referenceID": 36, "context": "better result[41].", "startOffset": 13, "endOffset": 17}, {"referenceID": 36, "context": "In previous researches[41][6], the alphabet size are 71 and 72 characters respectively.", "startOffset": 22, "endOffset": 26}, {"referenceID": 5, "context": "In previous researches[41][6], the alphabet size are 71 and 72 characters respectively.", "startOffset": 26, "endOffset": 29}, {"referenceID": 36, "context": "( f \u2217g)(i) = m \u2211 j=1 g( j)\u2217 f (i\u2212 j+m/2)[41]", "startOffset": 40, "endOffset": 44}, {"referenceID": 8, "context": "Also, each layer is initialised by henormal[10], and the border mode is same, which means the length remains 1,000 during this stages.", "startOffset": 43, "endOffset": 47}, {"referenceID": 5, "context": "Encouraged by Conneaus work[6], we only used the kernel size equal to three, so that during this period, the layers can automatically best combine these different \u201ctrigram\u201d features in various layers.", "startOffset": 27, "endOffset": 30}, {"referenceID": 23, "context": "We used the ReLU[27] as our nonlinear activation function, which is widely employed in recent researches.", "startOffset": 16, "endOffset": 20}, {"referenceID": 5, "context": "There are varies types of pooling layers such as max-pooling layer and average pooling layer[6].", "startOffset": 92, "endOffset": 95}, {"referenceID": 36, "context": "In this task, we validated our models on one of the eight datasets in Zhang[41] s research.", "startOffset": 75, "endOffset": 79}, {"referenceID": 30, "context": "The dataset was collected from Sogou[35], and the encoding is pinyin format.", "startOffset": 36, "endOffset": 40}, {"referenceID": 36, "context": "In previous researches, researchers need to distinguish between upper-case and lower-case letters[41][37][6], which means the dimension of the dataset is at least fifty-two due to the Roman alphabet size.", "startOffset": 97, "endOffset": 101}, {"referenceID": 32, "context": "In previous researches, researchers need to distinguish between upper-case and lower-case letters[41][37][6], which means the dimension of the dataset is at least fifty-two due to the Roman alphabet size.", "startOffset": 101, "endOffset": 105}, {"referenceID": 5, "context": "In previous researches, researchers need to distinguish between upper-case and lower-case letters[41][37][6], which means the dimension of the dataset is at least fifty-two due to the Roman alphabet size.", "startOffset": 105, "endOffset": 108}, {"referenceID": 36, "context": "Zhang explained that the differences between letter cases might affect the semantics, and that may lead to a regularisation problem[41].", "startOffset": 131, "endOffset": 135}, {"referenceID": 8, "context": "using Gaussian initialisation scaled [10].", "startOffset": 37, "endOffset": 41}, {"referenceID": 15, "context": "Training performed with optimiser \u2018Adam\u2019[18], and the loss functions is \u2018categorical cross-entropy\u2019.", "startOffset": 40, "endOffset": 44}, {"referenceID": 30, "context": "These datasets are collected by Sogou[35] and then reallocate and transformed for this paper.", "startOffset": 37, "endOffset": 41}, {"referenceID": 30, "context": "We combine the news corpus SogouCA and SogouCS from Sogou Lab[35], which is more than 3 million news articles and at least twenty categories.", "startOffset": 61, "endOffset": 65}, {"referenceID": 7, "context": "This technique is widely used in computer vision [9] and speech recognition [1], to increase the size of the dataset by transforming the signals or rotating the image.", "startOffset": 49, "endOffset": 52}, {"referenceID": 0, "context": "This technique is widely used in computer vision [9] and speech recognition [1], to increase the size of the dataset by transforming the signals or rotating the image.", "startOffset": 76, "endOffset": 79}, {"referenceID": 36, "context": "The result of Bag of Words and N-gram came from Zhang[41], which are the references of this task.", "startOffset": 53, "endOffset": 57}, {"referenceID": 36, "context": "The researchers assume that the convolutional kernel can ignore the differences between two words shared same language roots such as suffix and prefix[41].", "startOffset": 150, "endOffset": 154}, {"referenceID": 36, "context": "Also, the validation on Zhang\u015b dataset[41] show that proper dictionary and hyper-parameters play a major role in pinyin format text classification task.", "startOffset": 38, "endOffset": 42}], "year": 2016, "abstractText": "Compared with word-level and sentence-level convolutional neural networks (ConvNets), the character-level ConvNets has a better applicability for misspellings and typos input. Due to this, recent researches for text classification mainly focus on character-level ConvNets. However, while the majority of these researches employ English corpus for the character-level text classification, few researches have been done using Chinese corpus. This research hopes to bridge this gap, exploring character-level ConvNets for Chinese corpus test classification. We have constructed a large-scale Chinese dataset, and the result shows that character-level ConvNets works better on Chinese character dataset than its corresponding pinyin format dataset, which is the general solution in previous researches. This is the first time that character-level ConvNets has been applied to Chinese character dataset for text classification problem.", "creator": "LaTeX with hyperref package"}}}