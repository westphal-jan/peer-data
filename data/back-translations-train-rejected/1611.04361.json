{"id": "1611.04361", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Nov-2016", "title": "Attending to Characters in Neural Sequence Labeling Models", "abstract": "Sequence labeling architectures use word embeddings for capturing similarity, but suffer when handling previously unseen or rare words. We investigate character-level extensions to such models and propose a novel architecture for combining alternative word representations. By using an attention mechanism, the model is able to dynamically decide how much information to use from a word- or character-level component. We evaluated different architectures on a range of sequence labeling datasets, and character-level extensions were found to improve performance on every benchmark. In addition, the proposed attention-based architecture delivered the best results even with a smaller number of trainable parameters.", "histories": [["v1", "Mon, 14 Nov 2016 12:36:07 GMT  (149kb,D)", "http://arxiv.org/abs/1611.04361v1", "Proceedings of COLING 2016"]], "COMMENTS": "Proceedings of COLING 2016", "reviews": [], "SUBJECTS": "cs.CL cs.LG cs.NE", "authors": ["marek rei", "gamal k o crichton", "sampo pyysalo"], "accepted": false, "id": "1611.04361"}, "pdf": {"name": "1611.04361.pdf", "metadata": {"source": "CRF", "title": "Attending to Characters in Neural Sequence Labeling Models", "authors": ["Marek Rei", "Gamal K.O. Crichton", "Sampo Pyysalo"], "emails": ["marek.rei@cl.cam.ac.uk", "gkoc2@cam.ac.uk", "smp66@cam.ac.uk"], "sections": [{"heading": null, "text": "Sequence marker architectures use word embedding to detect similarities, but suffer from handling previously invisible or rare words. We examine extensions of the character level of such models and propose a new architecture to combine alternative word representations. By using an attention mechanism, the model can dynamically decide how much information should be used by a word or character component. We evaluated different architectures using a set of sequence marker data sets, and marker extensions improved performance at each benchmark. In addition, the proposed attention-based architecture yielded the best results, even with a smaller number of traceable parameters."}, {"heading": "1 Introduction", "text": "Many NLP tasks, including entity recognition (NER), part of language (POS) tagging, and shallow parsing, can be described as types of sequence tagging, and the development of accurate and efficient sequence tagging models is useful for a wide range of downstream applications. Work in this area traditionally involves task-specific feature engineering - for example, the integration of gazettes for named entity recognition, or the use of features from a morphological analyzer in POS tagging. Recent developments in neural architectures and representation learning have opened the door to models that can automatically detect useful properties from the data. Such sequence tagging systems are applicable to many tasks, using only the interface text as input, but able to achieve competitive results."}, {"heading": "2 Bidirectional LSTM for sequence labeling", "text": "We first describe a basic word-level neural network for sequence marking, according to the models described by Lample et al (2016) and Rei and Yannakoudakis (2016), and then propose two alternative methods for including character-level information. Figure 1 shows the general architecture of sequence marking. The model receives a sequence of characters (w1,..., wT) as input, and predicts a label corresponding to each of the input tokens. First, the tokens are assigned to a distributed vector space, resulting in a sequence of word embeddings (x1,..., xT). Next, the embeddings are given as input to two LSTM and Schmidhuber, 1997) components that move in opposite directions through the text, creating context-specific representations. The respective forward and backward-conditioned representations are summarized for each word position."}, {"heading": "3 Character-level sequence labeling", "text": "Distributed embedding maps words into a space where semantically similar words have similar vector representations, allowing models to generalize better. However, they still treat words as atomic units, ignoring any surface or morphological similarity between different words. By constructing models that operate via single characters in each word, we can take advantage of these regularities. This can be especially useful for dealing with invisible words - for example, if we have never seen the word limits before, a character layer model could still derive a representation for that word if it has previously seen the word word wordword and other words with the suffix -s. In contrast, a word layer model can only represent that word with a generic vocabulary representation that is divided between all other invisible words."}, {"heading": "4 Attention over character features", "text": "In fact, it is not that it is a pure swear word, but a pure swear word that changes in the way in which it is used, in the way in which it is used, in the way in which it is used, in the way in which it is used, in the way in which it is used, in the way in which it is used to generate vector.In the way in which it is used, in the way in which it is used to generate vector.In the way in which it is used, in the way in which it is used, in the way in which it is used to generate vector.In the way in which it is used, in the way in which it is used, in the way in which it is used."}, {"heading": "5 Datasets", "text": "We evaluate the sequence of models and character architectures on 8 different datasets. Table 1 contains information on the number of labels and dataset sizes for each of them. \u2022 CoNL00: The CoNLL-2000 Dataset (Tjong Kim Sang and Buchholz, 2000) is a commonly used benchmark for the task of chunking. \u2022 CoNLL-Journal Sections 15-18 from Penn Treebank are used for training, and Section 20 as the test data. As there is no official development, we have separated some of the training sessions for this purpose. \u2022 CoNLL-2003 corpus (Tjong Kim Sang and De Meulder) was created for the joint task."}, {"heading": "6 Experiment settings", "text": "All words that appeared only once in the training data were replaced by the generic OOV token for Word embedding, but were still used in the character-level components; the word embedding was initialized with publicly available pre-formed vectors created with word2vec (Mikolov et al., 2013), and then refined during model training; for the general domain data sets, we used 300-dimensional vectors trained on Google News2; for the biomedical data sets, we used 200-dimensional vectors trained on PubMed and PMC3; the character embedding was set to length 50 and randomly initialized; the LSTM layer size was set to 200 in each direction for both word and character components; the hidden layer d is size 50, and the combined representation m has the same length as the word embedding; and the LSTM layer size was set to 200 in each direction for both word and character components; the hidden layer d is size 50, and the combined representation m is the same length as the word embedding; and the CRF was found to be the baseline layer we used with all the results we found with the greatest benefit."}, {"heading": "7 Results", "text": "While optimizing the hyperparameters for each dataset separately would improve individual performance, we are conducting more controlled experiments on a task-independent model. Therefore, we are using the same hyperparameters from Section 6 on all datasets, and the development set is used only for the stop state. We want to use these experiments to 1) determine on which sequence labeling tasks have an advantage, and 2) which character-based architecture works better. Results for the different model architectures on all 8 datasets are shown in Table 2. As you can see, a character-based component in the sequence labeling architecture improves performance on all benchmarks."}, {"heading": "8 Related work", "text": "There is a wide range of previous work on the construction and optimization of neural architectures that are applicable to bed marking. Collobert et al. (2011) described one of the first task-independent neural tagging models using conventional neural networks. They were able to achieve good results on POS tagging, chunking, NER and semantic role marking without relying on craft characteristics. Irsoy and Cardie (2014) experimented with multi-layered bidirectional recursive networks and found that the deep models trumped random fields on the task of exchanging opinions. Huang et al. (2015) described a bidirectional LSTM model with a CRF layer specializing in the task of named word recognition. Rei and Yannakoudakis et evaluated a number of neural architectures, including conventional and recursive networks."}, {"heading": "9 Conclusion", "text": "Developments in neural network research enable model architectures that work well on a wide range of sequence marker datasets without requiring handmade data. While word-level rendering learning is a powerful tool for automatic discovery of useful features, these models still have certain weaknesses - rare words exhibit low-quality renderings, hitherto invisible words cannot be modeled at all, and morpheme-level information is not shared with the entire vocabulary. In this essay, we examined character model components for a sequence marker architecture that allows the system to learn useful patterns from subordinate units. In addition to a bidirectional LSTM that operates through words, a separate bi-directional LSTM is used to construct word representations from individual characters. We proposed a novel architecture to combine character-based rendering with word insertion, using a bidirectional LSTM that enables attention dynamics to be embedded in each piece of information."}], "references": [{"title": "Improved Transition-Based Parsing by Modeling Characters instead of Words with LSTMs", "author": ["Miguel Ballesteros", "Chris Dyer", "Noah A. Smith."], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP).", "citeRegEx": "Ballesteros et al\\.,? 2015", "shortCiteRegEx": "Ballesteros et al\\.", "year": 2015}, {"title": "Theano: a CPU and GPU math compiler in Python", "author": ["James Bergstra", "Olivier Breuleux", "Frederic Fr\u00e9d\u00e9ric Bastien", "Pascal Lamblin", "Razvan Pascanu", "Guillaume Desjardins", "Joseph Turian", "David Warde-Farley", "Yoshua Bengio."], "venue": "Proceedings of the Python for Scientific Computing Conference (SciPy).", "citeRegEx": "Bergstra et al\\.,? 2010", "shortCiteRegEx": "Bergstra et al\\.", "year": 2010}, {"title": "A document processing pipeline for annotating chemical entities in scientific documents", "author": ["David Campos", "Sergio Matos", "Jose L. Oliveira."], "venue": "Journal of Cheminformatics, 7.", "citeRegEx": "Campos et al\\.,? 2015", "shortCiteRegEx": "Campos et al\\.", "year": 2015}, {"title": "A Joint Model for Word Embedding and Word Morphology", "author": ["Kris Cao", "Marek Rei."], "venue": "Proceedings of the 1st Workshop on Representation Learning for NLP (RepL4NLP-2016).", "citeRegEx": "Cao and Rei.,? 2016", "shortCiteRegEx": "Cao and Rei.", "year": 2016}, {"title": "Natural Language Processing (Almost) from Scratch", "author": ["Ronan Collobert", "Jason Weston", "L\u00e9on Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa."], "venue": "Journal of Machine Learning Research, 12.", "citeRegEx": "Collobert et al\\.,? 2011", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Long Short-term Memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "Neural Computation, 9.", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Bidirectional LSTM-CRF Models for Sequence Tagging", "author": ["Zhiheng Huang", "Wei Xu", "Kai Yu."], "venue": "arXiv:1508.01991.", "citeRegEx": "Huang et al\\.,? 2015", "shortCiteRegEx": "Huang et al\\.", "year": 2015}, {"title": "Opinion Mining with Deep Recurrent Neural Networks", "author": ["Ozan Irsoy", "Claire Cardie."], "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP).", "citeRegEx": "Irsoy and Cardie.,? 2014", "shortCiteRegEx": "Irsoy and Cardie.", "year": 2014}, {"title": "Introduction to the Bio-entity Recognition Task at JNLPBA", "author": ["Jin-Dong Kim", "Tomoko Ohta", "Yoshimasa Tsuruoka", "Yuka Tateisi", "Nigel Collier."], "venue": "Proceedings of the International Joint Workshop on Natural Language Processing in Biomedicine and Its Applications.", "citeRegEx": "Kim et al\\.,? 2004", "shortCiteRegEx": "Kim et al\\.", "year": 2004}, {"title": "Character-Aware Neural Language Models", "author": ["Yoon Kim", "Yacine Jernite", "David Sontag", "Alexander M. Rush."], "venue": "Proceedings of the 30th AAAI Conference on Artificial Intelligence (AAAI16).", "citeRegEx": "Kim et al\\.,? 2016", "shortCiteRegEx": "Kim et al\\.", "year": 2016}, {"title": "CHEMDNER: The drugs and chemical names extraction challenge", "author": ["Martin Krallinger", "Florian Leitner", "Obdulia Rabal", "Miguel Vazquez", "Julen Oyarzabal", "Alfonso Valencia."], "venue": "Journal of Cheminformatics, 7(Suppl 1).", "citeRegEx": "Krallinger et al\\.,? 2015", "shortCiteRegEx": "Krallinger et al\\.", "year": 2015}, {"title": "Conditional random fields: Probabilistic models for segmenting and labeling sequence data", "author": ["John Lafferty", "Andrew McCallum", "Fernando Pereira."], "venue": "Proceedings of the 18th International Conference on Machine Learning.", "citeRegEx": "Lafferty et al\\.,? 2001", "shortCiteRegEx": "Lafferty et al\\.", "year": 2001}, {"title": "Neural Architectures for Named Entity Recognition", "author": ["Guillaume Lample", "Miguel Ballesteros", "Sandeep Subramanian", "Kazuya Kawakami", "Chris Dyer."], "venue": "Proceedings of NAACL-HLT 2016.", "citeRegEx": "Lample et al\\.,? 2016", "shortCiteRegEx": "Lample et al\\.", "year": 2016}, {"title": "Finding Function in Form: Compositional Character Models for Open Vocabulary Word Representation", "author": ["Wang Ling", "Tiago Lu\u0131\u0301s", "Lu\u0131\u0301s Marujo", "Ram\u00f3n Fernandez Astudillo", "Silvio Amir", "Chris Dyer", "Alan W. Black", "Isabel Trancoso"], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing", "citeRegEx": "Ling et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ling et al\\.", "year": 2015}, {"title": "Character-based Neural Machine Translation", "author": ["Wang Ling", "Isabel Trancoso", "Chris Dyer", "Alan W Black."], "venue": "arXiv preprint arXiv:1511.04586.", "citeRegEx": "Ling et al\\.,? 2015b", "shortCiteRegEx": "Ling et al\\.", "year": 2015}, {"title": "Building a large annotated corpus of English: The Penn Treebank", "author": ["Mitchell P. Marcus", "Beatrice Santorini", "Mary Ann Marcinkiewicz."], "venue": "Computational Linguistics, 19.", "citeRegEx": "Marcus et al\\.,? 1993", "shortCiteRegEx": "Marcus et al\\.", "year": 1993}, {"title": "Efficient Estimation of Word Representations in Vector Space", "author": ["Tom\u00e1\u0161 Mikolov", "Greg Corrado", "Kai Chen", "Jeffrey Dean."], "venue": "Proceedings of the International Conference on Learning Representations (ICLR 2013).", "citeRegEx": "Mikolov et al\\.,? 2013", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Gated Word-Character Recurrent Language Model", "author": ["Yasumasa Miyamoto", "Kyunghyun Cho."], "venue": "arXiv preprint arXiv:1606.01700.", "citeRegEx": "Miyamoto and Cho.,? 2016", "shortCiteRegEx": "Miyamoto and Cho.", "year": 2016}, {"title": "The GENIA corpus: An annotated research abstract corpus in molecular biology domain", "author": ["Tomoko Ohta", "Yuka Tateisi", "Jin-Dong Kim."], "venue": "Proceedings of the second international conference on Human Language Technology Research.", "citeRegEx": "Ohta et al\\.,? 2002", "shortCiteRegEx": "Ohta et al\\.", "year": 2002}, {"title": "Compositional Sequence Labeling Models for Error Detection in Learner Writing", "author": ["Marek Rei", "Helen Yannakoudakis."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics.", "citeRegEx": "Rei and Yannakoudakis.,? 2016", "shortCiteRegEx": "Rei and Yannakoudakis.", "year": 2016}, {"title": "Overview of BioCreative II gene mention recognition", "author": ["Jacinto Mata", "W John Wilbur."], "venue": "Genome biology, 9 Suppl 2.", "citeRegEx": "Mata and Wilbur.,? 2008", "shortCiteRegEx": "Mata and Wilbur.", "year": 2008}, {"title": "Introduction to the CoNLL-2000 shared task: Chunking", "author": ["Erik F. Tjong Kim Sang", "Sabine Buchholz."], "venue": "Proceedings of the 2nd Workshop on Learning Language in Logic and the 4th Conference on Computational Natural Language Learning, 7.", "citeRegEx": "Sang and Buchholz.,? 2000", "shortCiteRegEx": "Sang and Buchholz.", "year": 2000}, {"title": "Introduction to the CoNLL-2003 Shared Task: LanguageIndependent Named Entity Recognition", "author": ["Erik F. Tjong Kim Sang", "Fien De Meulder."], "venue": "Proceedings of the seventh conference on Natural language learning at HLT-NAACL 2003.", "citeRegEx": "Sang and Meulder.,? 2003", "shortCiteRegEx": "Sang and Meulder.", "year": 2003}, {"title": "Developing a robust part-of-speech tagger for biomedical text", "author": ["Yoshimasa Tsuruoka", "Yuka Tateishi", "Jin Dong Kim", "Tomoko Ohta", "John McNaught", "Sophia Ananiadou", "Jun\u2019ichi Tsujii"], "venue": "In Proceedings of Panhellenic Conference on Informatics", "citeRegEx": "Tsuruoka et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Tsuruoka et al\\.", "year": 2005}, {"title": "A New Dataset and Method for Automatically Grading ESOL Texts", "author": ["Helen Yannakoudakis", "Ted Briscoe", "Ben Medlock."], "venue": "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies.", "citeRegEx": "Yannakoudakis et al\\.,? 2011", "shortCiteRegEx": "Yannakoudakis et al\\.", "year": 2011}, {"title": "ADADELTA: An Adaptive Learning Rate Method", "author": ["Matthew D. Zeiler."], "venue": "arXiv preprint arXiv:1212.5701.", "citeRegEx": "Zeiler.,? 2012", "shortCiteRegEx": "Zeiler.", "year": 2012}, {"title": "Character-level Convolutional Networks for Text Classification", "author": ["Xiang Zhang", "Junbo Zhao", "Yann LeCun."], "venue": "Advances in Neural Information Processing Systems.", "citeRegEx": "Zhang et al\\.,? 2015", "shortCiteRegEx": "Zhang et al\\.", "year": 2015}, {"title": "Exploring Deep Knowledge Resources in Biomedical Name Recognition", "author": ["GuoDong Zhou", "Jian Su."], "venue": "Workshop on Natural Language Processing in Biomedicine and Its Applications at COLING.", "citeRegEx": "Zhou and Su.,? 2004", "shortCiteRegEx": "Zhou and Su.", "year": 2004}], "referenceMentions": [{"referenceID": 4, "context": "Such sequence labeling systems are applicable to many tasks, using only the surface text as input, yet are able to achieve competitive results (Collobert et al., 2011; Irsoy and Cardie, 2014).", "startOffset": 143, "endOffset": 191}, {"referenceID": 7, "context": "Such sequence labeling systems are applicable to many tasks, using only the surface text as input, yet are able to achieve competitive results (Collobert et al., 2011; Irsoy and Cardie, 2014).", "startOffset": 143, "endOffset": 191}, {"referenceID": 5, "context": "Next, the embeddings are given as input to two LSTM (Hochreiter and Schmidhuber, 1997) components moving in opposite directions through the text, creating context-specific representations.", "startOffset": 52, "endOffset": 86}, {"referenceID": 11, "context": "We first describe a basic word-level neural network for sequence labeling, following the models described by Lample et al. (2016) and Rei and Yannakoudakis (2016), and then propose two alternative methods for incorporating character-level information.", "startOffset": 109, "endOffset": 130}, {"referenceID": 11, "context": "We first describe a basic word-level neural network for sequence labeling, following the models described by Lample et al. (2016) and Rei and Yannakoudakis (2016), and then propose two alternative methods for incorporating character-level information.", "startOffset": 109, "endOffset": 163}, {"referenceID": 11, "context": "Finally, to produce label predictions, we use either a softmax layer or a conditional random field (CRF, Lafferty et al. (2001)).", "startOffset": 105, "endOffset": 128}, {"referenceID": 6, "context": "Following Huang et al. (2015), we can also use a CRF as the output layer, which conditions each prediction on the previously predicted label.", "startOffset": 10, "endOffset": 30}, {"referenceID": 12, "context": "Following Lample et al. (2016), one possible approach is to concatenate the two vectors and use this as the new word-level representation for the sequence labeling model:", "startOffset": 10, "endOffset": 31}, {"referenceID": 1, "context": "This can be achieved by making sure that the optimisation is performed only in one direction; in Theano (Bergstra et al., 2010), the disconnected grad function gives the desired effect.", "startOffset": 104, "endOffset": 127}, {"referenceID": 15, "context": "\u2022 PTB-POS: The Penn Treebank POS-tag corpus (Marcus et al., 1993) contains texts from the Wall Street Journal, annotated for part-of-speech tags.", "startOffset": 44, "endOffset": 65}, {"referenceID": 24, "context": "\u2022 FCEPUBLIC: The publicly released subset of the First Certificate in English (FCE) dataset contains short essays written by language learners and manual corrections by examiners (Yannakoudakis et al., 2011).", "startOffset": 179, "endOffset": 207}, {"referenceID": 10, "context": "\u2022 CHEMDNER: The BioCreative IV Chemical and Drug (Krallinger et al., 2015) NER corpus consists of 10,000 abstracts annotated for mentions of chemical and drug names using a single class.", "startOffset": 49, "endOffset": 74}, {"referenceID": 8, "context": "\u2022 JNLPBA: The JNLPBA corpus (Kim et al., 2004) consists of 2,404 biomedical abstracts and is annotated for mentions of five entity types: CELL LINE, CELL TYPE, DNA, RNA, and PROTEIN.", "startOffset": 28, "endOffset": 46}, {"referenceID": 18, "context": "\u2022 GENIA-POS: The GENIA corpus (Ohta et al., 2002) is one of the most widely used resources for biomedical NLP and has a rich set of annotations including parts of speech, phrase structure syntax, entity mentions, and events.", "startOffset": 30, "endOffset": 49}, {"referenceID": 18, "context": "\u2022 GENIA-POS: The GENIA corpus (Ohta et al., 2002) is one of the most widely used resources for biomedical NLP and has a rich set of annotations including parts of speech, phrase structure syntax, entity mentions, and events. Here, we make use of the GENIA POS annotations, which cover 2,000 PubMed abstracts (approx. 20,000 sentences). We use the same 210-document test set as Tsuruoka et al. (2005), and additionally split off a sample of 210 from the remaining documents as a development set.", "startOffset": 31, "endOffset": 400}, {"referenceID": 16, "context": "The word embeddings were initialised with publicly available pretrained vectors, created using word2vec (Mikolov et al., 2013), and then fine-tuned during model training.", "startOffset": 104, "endOffset": 126}, {"referenceID": 25, "context": "Parameters were optimised using AdaDelta (Zeiler, 2012) with default learning rate 1.", "startOffset": 41, "endOffset": 55}, {"referenceID": 5, "context": "55% by Huang et al. (2015), and 72.", "startOffset": 7, "endOffset": 27}, {"referenceID": 5, "context": "55% by Huang et al. (2015), and 72.70% on JNLPBA compared to 72.55% by Zhou and Su (2004). In some cases, we are also able to beat the previous best results \u2013 87.", "startOffset": 7, "endOffset": 90}, {"referenceID": 2, "context": "48% by Campos et al. (2015), and 41.", "startOffset": 7, "endOffset": 28}, {"referenceID": 2, "context": "48% by Campos et al. (2015), and 41.88% on FCEPUBLIC compared to 41.1% by Rei and Yannakoudakis (2016). Lample et al.", "startOffset": 7, "endOffset": 103}, {"referenceID": 2, "context": "48% by Campos et al. (2015), and 41.88% on FCEPUBLIC compared to 41.1% by Rei and Yannakoudakis (2016). Lample et al. (2016) report a considerably higher result of 90.", "startOffset": 7, "endOffset": 125}, {"referenceID": 0, "context": "There is also research on performing parsing (Ballesteros et al., 2015) and text classification (Zhang et al.", "startOffset": 45, "endOffset": 71}, {"referenceID": 26, "context": ", 2015) and text classification (Zhang et al., 2015) with character-level neural models.", "startOffset": 32, "endOffset": 52}, {"referenceID": 2, "context": "Collobert et al. (2011) described one of the first task-independent neural tagging models using convolutional neural networks.", "startOffset": 0, "endOffset": 24}, {"referenceID": 2, "context": "Collobert et al. (2011) described one of the first task-independent neural tagging models using convolutional neural networks. They were able to achieve good results on POS tagging, chunking, NER and semantic role labeling, without relying on hand-engineered features. Irsoy and Cardie (2014) experimented with multi-layer bidirectional Elman-style recurrent networks, and found that the deep models outperformed conditional random fields on the task of opinion mining.", "startOffset": 0, "endOffset": 293}, {"referenceID": 2, "context": "Collobert et al. (2011) described one of the first task-independent neural tagging models using convolutional neural networks. They were able to achieve good results on POS tagging, chunking, NER and semantic role labeling, without relying on hand-engineered features. Irsoy and Cardie (2014) experimented with multi-layer bidirectional Elman-style recurrent networks, and found that the deep models outperformed conditional random fields on the task of opinion mining. Huang et al. (2015) described a bidirectional LSTM model with a CRF layer, which included hand-crafted features specialised for the task of named entity recognition.", "startOffset": 0, "endOffset": 490}, {"referenceID": 2, "context": "Collobert et al. (2011) described one of the first task-independent neural tagging models using convolutional neural networks. They were able to achieve good results on POS tagging, chunking, NER and semantic role labeling, without relying on hand-engineered features. Irsoy and Cardie (2014) experimented with multi-layer bidirectional Elman-style recurrent networks, and found that the deep models outperformed conditional random fields on the task of opinion mining. Huang et al. (2015) described a bidirectional LSTM model with a CRF layer, which included hand-crafted features specialised for the task of named entity recognition. Rei and Yannakoudakis (2016) evaluated a range of neural architectures, including convolutional and recurrent networks, on the task of error detection in learner writing.", "startOffset": 0, "endOffset": 665}, {"referenceID": 2, "context": "Collobert et al. (2011) described one of the first task-independent neural tagging models using convolutional neural networks. They were able to achieve good results on POS tagging, chunking, NER and semantic role labeling, without relying on hand-engineered features. Irsoy and Cardie (2014) experimented with multi-layer bidirectional Elman-style recurrent networks, and found that the deep models outperformed conditional random fields on the task of opinion mining. Huang et al. (2015) described a bidirectional LSTM model with a CRF layer, which included hand-crafted features specialised for the task of named entity recognition. Rei and Yannakoudakis (2016) evaluated a range of neural architectures, including convolutional and recurrent networks, on the task of error detection in learner writing. The word-level sequence labeling model described in this paper follows the previous work, combining useful design choices from each of them. In addition, we extended the model with two alternative character-level architectures, and evaluated its performance on 8 different datasets. Character-level models have the potential of capturing morpheme patterns, thereby improving generalisation on both frequent and unseen words. In recent years, there has been an increase in research into these models, resulting in several interesting applications. Ling et al. (2015b) described a character-level neural model for machine translation, performing both encoding and decoding on individual characters.", "startOffset": 0, "endOffset": 1374}, {"referenceID": 2, "context": "Collobert et al. (2011) described one of the first task-independent neural tagging models using convolutional neural networks. They were able to achieve good results on POS tagging, chunking, NER and semantic role labeling, without relying on hand-engineered features. Irsoy and Cardie (2014) experimented with multi-layer bidirectional Elman-style recurrent networks, and found that the deep models outperformed conditional random fields on the task of opinion mining. Huang et al. (2015) described a bidirectional LSTM model with a CRF layer, which included hand-crafted features specialised for the task of named entity recognition. Rei and Yannakoudakis (2016) evaluated a range of neural architectures, including convolutional and recurrent networks, on the task of error detection in learner writing. The word-level sequence labeling model described in this paper follows the previous work, combining useful design choices from each of them. In addition, we extended the model with two alternative character-level architectures, and evaluated its performance on 8 different datasets. Character-level models have the potential of capturing morpheme patterns, thereby improving generalisation on both frequent and unseen words. In recent years, there has been an increase in research into these models, resulting in several interesting applications. Ling et al. (2015b) described a character-level neural model for machine translation, performing both encoding and decoding on individual characters. Kim et al. (2016) implemented a language model where encoding is performed by a convolutional network and LSTM over characters, whereas predictions are given on the word-level.", "startOffset": 0, "endOffset": 1522}, {"referenceID": 2, "context": "Cao and Rei (2016) proposed a method for learning both word embeddings and morphological segmentation with a bidirectional recurrent network over characters.", "startOffset": 0, "endOffset": 19}, {"referenceID": 0, "context": "There is also research on performing parsing (Ballesteros et al., 2015) and text classification (Zhang et al., 2015) with character-level neural models. Ling et al. (2015a) proposed a neural architecture that replaces word embeddings with dynamically-constructed characterbased representations.", "startOffset": 46, "endOffset": 173}, {"referenceID": 0, "context": "There is also research on performing parsing (Ballesteros et al., 2015) and text classification (Zhang et al., 2015) with character-level neural models. Ling et al. (2015a) proposed a neural architecture that replaces word embeddings with dynamically-constructed characterbased representations. We applied a similar method for operating over characters, but combined them with word embeddings instead of replacing them, as this allows the model to benefit from both approaches. Lample et al. (2016) described a model where the character-level representation is combined with word embeddings through concatenation.", "startOffset": 46, "endOffset": 499}, {"referenceID": 0, "context": "There is also research on performing parsing (Ballesteros et al., 2015) and text classification (Zhang et al., 2015) with character-level neural models. Ling et al. (2015a) proposed a neural architecture that replaces word embeddings with dynamically-constructed characterbased representations. We applied a similar method for operating over characters, but combined them with word embeddings instead of replacing them, as this allows the model to benefit from both approaches. Lample et al. (2016) described a model where the character-level representation is combined with word embeddings through concatenation. In this work, we proposed an alternative architecture, where the representations are combined using an attention mechanism, and evaluated both approaches on a range of tasks and datasets. Recently, Miyamoto and Cho (2016) have also described a related method for the task of language modelling, combining characters and word embeddings using gating.", "startOffset": 46, "endOffset": 836}], "year": 2016, "abstractText": "Sequence labeling architectures use word embeddings for capturing similarity, but suffer when handling previously unseen or rare words. We investigate character-level extensions to such models and propose a novel architecture for combining alternative word representations. By using an attention mechanism, the model is able to dynamically decide how much information to use from a wordor character-level component. We evaluated different architectures on a range of sequence labeling datasets, and character-level extensions were found to improve performance on every benchmark. In addition, the proposed attention-based architecture delivered the best results even with a smaller number of trainable parameters.", "creator": "TeX"}}}