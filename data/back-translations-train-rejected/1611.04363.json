{"id": "1611.04363", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Nov-2016", "title": "Learning for Expertise Matching with Declination Prediction", "abstract": "We study the problem of finding appropriate experts who are able to complete timely reviews and would not say \"no\" to the invitation. The problem is a central issue in many question-and-answer systems, but has received little research attention. Different from most existing studies that focus on expertise matching, we want to further predict the expert's response: given a question, how can we find the expert who is able to provide a quality review and will agree to do it. We formalize the problem as a ranking problem. We first present an embedding-based question-to-expert distance metric for expertise matching and propose a ranking factor graph (RankFG) model to predict expert response. For online evaluation, we developed a Chrome Extension for reviewer recommendation and deployed it in the Google Chrome Web Store, and then collected the reviewers' feedback. We also used the review bidding of a CS conference for evaluation. In the experiments, the proposed method demonstrates its superiority (+6.6-21.2% by MAP) over several state-of-the-art algorithms.", "histories": [["v1", "Mon, 14 Nov 2016 12:46:24 GMT  (657kb,D)", "http://arxiv.org/abs/1611.04363v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["yujie qian", "jie tang"], "accepted": false, "id": "1611.04363"}, "pdf": {"name": "1611.04363.pdf", "metadata": {"source": "CRF", "title": "Learning for Expertise Matching with Declination Prediction", "authors": ["Yujie Qian", "Jie Tang"], "emails": ["qyj13@mails.tsinghua.edu.cn,", "jietang@tsinghua.edu.cn"], "sections": [{"heading": "1 Introduction", "text": "Another example is peer review, an important part of scientific publishing [21,12]. Many researchers see peer reviews as part of their professional responsibility. Publication groups also use the quality of peer reviews as an indicator of the success of a journal. Statistics also show that about half of the questions on Quora have only one or two answers. One challenge is how to find suitable experts who can answer the relevant questions. Peer review has long been criticized for being ineffective, slow and of low quality. Statistics also show that about half of the questions on Quora have only one or no answer at all. One challenge is how to find suitable experts who are qualified to answer the relevant questions, and the other is how to find experts who will give their answers. The latter is even more serious."}, {"heading": "2 Problem Definition", "text": "Faced with a question, our goal is to find experts with sufficient expertise who are willing to review this paper.We consider a social network G = (V, E) in which V is a set of | V | = N experts and E V \u00b7 V is a set of relationships between experts. There may be different types of relationships in different social networks. For example, relationships in an academic social network include collaborations, the same affiliation and the same nationality; while in a quora-like network, relationships include friendship, response and co-response. Suppose ai designates the attributes of experts vi that might be your own interests, the questions that she / he has answered, or simply the number of his / her posts. We use A = {a1, \u00b7 \u00b7, aN} to name the attributes of all experts. Faced with these definitions, we define our problem that expertise coincides with definition as follows. Problem 1. Expertise Matching with Definition Prediction Let's say A = E."}, {"heading": "3 The Approach", "text": "The simple way to deal with the problem of expert comparison is to design a metric to quantify the similarity between question and expert, and then to evaluate experts on the basis of similarity values. However, the situation is different when considering declination. In addition, developing a high-quality similarity metric between question and expert is also a non-trivial task. In this paper, we propose a ranking factor diagram (RankFG) model. Specifically, for a question q, we first extract candidates through an information retrieval model. To quantify the similarity values between questions and experts, we present an embedding-based matching algorithm. The algorithm uses a limitation-based optimization to model the matching score between the two different types of units - question and expert. Based on the similarity value, the RankFG model can learn and predict who is at high risk of rejecting the assessment."}, {"heading": "3.1 Candidate Generation", "text": "Faced with a question q, we first use all the words in the question to select a list of candidate experts. 3 In particular, we use a language model to retrieve relevant experts from V. 3 In our implementation, we have also tried to use a keyword extraction tool [29] to extract a number of keywords from the question and then use the keywords to select candidate experts. The language model is one of the most advanced approaches to obtaining information. It interprets the relevance between a document and a query word as a generative probability: P (w | d) = Nd Nd + \u03bb \u00b7 N w d Nd + (1 \u2212 Nd Nd + \u03bb) \u00b7 N w D ND (2), where Nd is the number of word marks in document d, Nwd is the word frequency (i.e., occurrence of number) of the word w in d, NwD is the number of word marks in the entire collection, and NwD is the word qd in document."}, {"heading": "3.2 Expertise Matching", "text": "The language model represents only the agreement between question and expert at the course level. We present a more sophisticated q = q = q = Q = O \u00b2 distance metric. The metric is inspired by [16], which was designed for estimating the document distance. Given all the questions and interests of all experts, we first learn a numerical vector representation (also called word embedding) for each word. We use Word2vec with Skipgram [18] to learn the word embedding. Word2vec is a flat neural network architecture consisting of an input layer, a projection layer and an output layer. The training goal is to use an input word to predict surrounding words in context. Formally, we can have the following protocol probability objective function, 1T \u00b2 t = 1 word \u2212 c \u2264 j \u2264 j j = 0 protocol p (wt = 0 protocol p) (wt j | wt), with the size of the T and the training pus being the window size."}, {"heading": "3.3 Ranking Factor Graph", "text": "We propose a ranking factor chart (RankFG) to predict how likely the expert is to accept or decline the invitation. RankFG's graphical model consists of two levels of variables: observations and latent variables. In our problem, each observation represents a question-expert pair (q, vi) and is associated with a latent variable yi to indicate whether the expert agrees with or agrees with the question (or checks the work). Local factor functions are defined to capture the relationships between an observation and its corresponding latent variables. Local factor function: captures the characteristics of each question-expert pair, including a relevance point between the paper and the expert, and any attributes associated with the expert. Defined as exponential functionality (q, vi, yi) = 1Za exp (q, vi, vi, yi)."}, {"heading": "4 Experiments", "text": "In order to empirically evaluate the proposed methods, we conduct the experiments in the peer review task that reviewers find for scientific work. Algorithm 1 Learning algorithm for RankFG. Input: Query questions Q = {q}, G = (V, E, A) and the learning rate \u03b7; Output: Learned parameters \u03b8; \u03b8 \u2190 0; Repeat questions for q-Q do L \u2190 Initialization list; Factor diagram FG \u2190 BuildFactorGraph (L); Repeat questions for vi-L update the messages of vi according to the Summary Product Update Rule [15]; End for until all messages \u00b5 do not change; convert for \u03b8i imbalances."}, {"heading": "4.1 Experimental Setup", "text": "We evaluate our method in the following three different datasets: Relevance: This dataset includes 86 papers and 2,048 evaluation criteria. We asked the doctoral students from the lab to make relevance assessments. We consider the relevance to be binary: relevance and irrelevance. In this dataset, we focus on evaluating the relevant and qualified experts to review the paper.Answer: This dataset was collected by our Chrome extension for reviewers. It includes 183 papers submitted in nine journals and 827 reviews. Among the responses, there are 391 reviews, while the rest are considered \"regressive\" (including \"unavailable\" and \"no response\"). In response, we focus on declination."}, {"heading": "4.2 Experiment Results", "text": "We compare the performance of all methods on three data sets. Table 1 lists the performance comparison on three data sets. We find in relevance, WMD surpasses Jaccard and has the best performance of P @ N within four methods. RankFG uses the relevance between paper and reviewers for the most part, while other features and correlations are not taken into account. On the other hand, the proposed RankFG achieves the best result. RankFG uses the correlation with the reviewer and improves performance. It also confirms the discovery in the survey that a reviewer is a reviewer that a reviewer is the Matrix Matrix Matrix Matrix Matrix Matrix Matrix Matrix Matrix Matrix Matrix Matrix Matrix."}, {"heading": "5 Related Work", "text": "Early work such as latent semantic indexing [3] and keyword matching [5] attempted to solve the problem in an information retrieval system. Later, Mimno et al. compared several language models and a proposed author-persona-topic model [19]. Karimzadehgan et al. proposed modeling aspects for reviewers and modeling aspects for paper aspects and attempted to model several aspects of expertise [11]. The optimization model focuses on solving the optimization problem of panel composition between a list of reviewers and a list of essays. These models generally take into account the limitations of modeling conference papers and aspects for review assignments [1], such as conflicts of interest, paper demand and workload for reviewers. Many methods have been proposed, such as invitation and greedy algorithm [13], creating an Integer-13 [network of reviews], [26] a [network of reviews], [26] and a network of recommendations."}, {"heading": "6 Conclusions", "text": "Our goal is to find experts who are not only relevant to a question / essay, but who will not refuse the invitation. We present an embedding-based question-to-expert distance measurement and propose a ranking factor graph model (RankFG) to find the right experts. RankFG uses both expert matching and other attributes and correlations between experts. To evaluate the proposed methods fairly, we conduct experiments on three sets of data: relevance, response and conference. Compared with several state-of-the-art methods, our method can significantly improve the performance of expert matching."}], "references": [{"title": "Conference paper assignment", "author": ["S. Benferhat", "J. Lang"], "venue": "Int. J. Intell. Syst. 16(10), 1183\u20131192", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2001}, {"title": "Peer reviewing in political science: New survey results", "author": ["P.A. Djupe"], "venue": "PS: Political Science & Politics 48(02), 346\u2013352", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2015}, {"title": "Automating the assignment of submitted manuscripts to reviewers", "author": ["S.T. Dumais", "J. Nielsen"], "venue": "SIGIR\u201992. pp. 233\u2013244", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1992}, {"title": "The ai conference paper assignment problem", "author": ["J. Goldsmith", "R.H. Sloan"], "venue": "AAAI\u201907. pp. 53\u201357", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2007}, {"title": "Recommending papers by mining the web", "author": ["C.B. Haym", "H. Hirsh", "W.W. Cohen", "C. Nevill-manning"], "venue": "IJCAI\u201999. pp. 1\u201311", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1999}, {"title": "Mining for proposal reviewers: lessons learned at the national science foundation", "author": ["S. Hettich", "M.J. Pazzani"], "venue": "KDD\u201906. pp. 862\u2013871", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2006}, {"title": "Annual report of the editors of the american political science review, 2012\u2013 2013", "author": ["J. Ishiyama"], "venue": "PS: Political Science & Politics 47(02), 542\u2013545", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2014}, {"title": "Report of the editors of the american political science review, 2013\u20132014", "author": ["J. Ishiyama"], "venue": "PS: Political Science & Politics 48(02), 396\u2013399", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "Training linear svms in linear time", "author": ["T. Joachims"], "venue": "KDD\u201906. pp. 217\u2013226. ACM", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2006}, {"title": "Constrained multi-aspect expertise matching for committee review assignment", "author": ["M. Karimzadehgan", "C. Zhai"], "venue": "CIKM\u201909. pp. 1697\u20131700", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2009}, {"title": "Multi-aspect expertise matching for review assignment", "author": ["M. Karimzadehgan", "C. Zhai", "G. Belford"], "venue": "CIKM\u201908. pp. 1113\u20131122", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2008}, {"title": "Peer review: crude and understudied, but indispensable", "author": ["J.P. Kassirer", "E.W. Campion"], "venue": "JAMA 272(2), 96\u201397", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1994}, {"title": "Weighted coverage based reviewer assignment", "author": ["N.M. Kou", "U.L. Hou", "N. Mamoulis", "Z. Gong"], "venue": "SIGMOD\u201915. pp. 2031\u20132046. ACM", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "A topic-based reviewer assignment system", "author": ["N.M. Kou", "N. Mamoulis", "Y. Li", "Y. Li", "Z Gong"], "venue": "VLDB\u201915. pp. 1852\u20131855", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2015}, {"title": "Factor graphs and the sum-product algorithm", "author": ["F.R. Kschischang", "B.J. Frey", "H.A. Loeliger"], "venue": "IEEE Transactions on Information Theory 47(2), 498\u2013519", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2001}, {"title": "From word embeddings to document distances", "author": ["M.J. Kusner", "Y. Sun", "N.I. Kolkin", "K.Q. Weinberger"], "venue": "ICML\u201915", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}, {"title": "On good and fair paper-reviewer assignment", "author": ["C. Long", "R.C.W. Wong", "Y. Peng", "L. Ye"], "venue": "ICDM\u201913. pp. 1145\u20131150. IEEE", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2013}, {"title": "Efficient estimation of word representations in vector space", "author": ["T. Mikolov", "K. Chen", "G. Corrado", "J. Dean"], "venue": "arXiv preprint arXiv:1301.3781", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2013}, {"title": "Expertise modeling for matching papers with reviewers", "author": ["D. Mimno", "A. McCallum"], "venue": "KDD\u201907. pp. 500\u2013509", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2007}, {"title": "Fast and robust earth mover\u2019s distances", "author": ["O. Pele", "M. Werman"], "venue": "ICCV\u201909. pp. 460\u2013467. IEEE", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2009}, {"title": "Editorial peer review: its development and rationale", "author": ["D. Rennie"], "venue": "Peer review in health sciences pp. 3\u201313", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1999}, {"title": "On optimization of expertise matching with various constraints", "author": ["W. Tang", "J. Tang", "T. Lei", "C. Tan", "B. Gao", "T. Li"], "venue": "Neurocomputing 76(1), 71\u201383", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2012}, {"title": "Why do peer reviewers decline to review? a survey", "author": ["L. Tite", "S. Schroter"], "venue": "Journal of epidemiology and community health 61(1), 9\u201312", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2007}, {"title": "Why do peer reviewers decline to review manuscripts? a study of reviewer invitation responses", "author": ["M. Willis"], "venue": "Learned Publishing 29(1), 5\u20137", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2016}, {"title": "Patent partner recommendation in enterprise social networks", "author": ["S. Wu", "J. Sun", "J. Tang"], "venue": "WSDM\u201913. pp. 43\u201352", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2013}, {"title": "A reviewer recommendation system based on collaborative intelligence", "author": ["K.H. Yang", "T.L. Kuo", "H.M. Lee", "J.M. Ho"], "venue": "WI-IAT\u201909. vol. 1, pp. 564\u2013567. IET", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2009}, {"title": "Generalized belief propagation", "author": ["J.S. Yedidia", "W.T. Freeman", "Y Weiss"], "venue": "NIPS\u201900. vol. 13, pp. 689\u2013695", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2000}, {"title": "A study of smoothing methods for language models applied to ad hoc information retrieval", "author": ["C. Zhai", "J. Lafferty"], "venue": "SIGIR\u201901. pp. 334\u2013342", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2001}, {"title": "Keyword extraction using support vector machine", "author": ["K. Zhang", "H. Xu", "J. Tang", "J. Li"], "venue": "WAIM\u201906. pp. 85\u201396", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2006}], "referenceMentions": [{"referenceID": 20, "context": "Another example is peer review, an important part of scientific publishing [21,12].", "startOffset": 75, "endOffset": 82}, {"referenceID": 11, "context": "Another example is peer review, an important part of scientific publishing [21,12].", "startOffset": 75, "endOffset": 82}, {"referenceID": 6, "context": "6%) [7] and a decline rate of 49.", "startOffset": 4, "endOffset": 7}, {"referenceID": 7, "context": "6% in 2014 [8].", "startOffset": 11, "endOffset": 14}, {"referenceID": 22, "context": "For peer review, other reasons for experts (researchers) to decline a review invitation include having too many reviews at hand and a tight deadline for completing the review [23,24].", "startOffset": 175, "endOffset": 182}, {"referenceID": 23, "context": "For peer review, other reasons for experts (researchers) to decline a review invitation include having too many reviews at hand and a tight deadline for completing the review [23,24].", "startOffset": 175, "endOffset": 182}, {"referenceID": 1, "context": ", top experts are more likely to decline than junior experts [2].", "startOffset": 61, "endOffset": 64}, {"referenceID": 28, "context": "3 In our implementation, we also tried to use a keyword extraction tool [29] to extract a number of keywords from the question and then use the keywords to select candidate experts.", "startOffset": 72, "endOffset": 76}, {"referenceID": 27, "context": ", occurrence number) of word w in d, N D is the number of word tokens in the entire collection, and N D is the word frequency of word w in the collection D; \u03bb is the Dirichlet smoothing factor and is commonly set according to the average document length in the collection [28].", "startOffset": 272, "endOffset": 276}, {"referenceID": 15, "context": "The metric is inspired by [16], which was designed for estimating document distance.", "startOffset": 26, "endOffset": 30}, {"referenceID": 17, "context": "We use Word2vec with Skipgram [18] to learn the word embeddings4.", "startOffset": 30, "endOffset": 34}, {"referenceID": 19, "context": "The best time complexity for solving the above optimization problem is O(p log p), where p is the number of unique words [20].", "startOffset": 121, "endOffset": 125}, {"referenceID": 15, "context": "An approximate solution can result in a complexity of O(p) [16], by removing each of the two constraints, and then combines the results.", "startOffset": 59, "endOffset": 63}, {"referenceID": 26, "context": "In our work, we choose Loop Belief Propagation (LBP) [27].", "startOffset": 53, "endOffset": 57}, {"referenceID": 14, "context": "Then we apply the sum-product algorithm [15] to factor graph to compute the approximate marginal distributions.", "startOffset": 40, "endOffset": 44}, {"referenceID": 14, "context": "Input: Query questions Q = {q}, G = (V,E,A), and the learning rate \u03b7; Output: learned parameters \u03b8; \u03b8 \u2190 0; repeat for q \u2208 Q do L\u2190 initialization list; Factor graph FG\u2190 BuildFactorGraph(L); repeat for vi \u2208 L do Update the messages of vi according to sum-product update rule [15]; end for until all messages \u03bc do not change; for \u03b8i \u2208 \u03b8 do Calculate gradient\u2207i according to Eq.", "startOffset": 273, "endOffset": 277}, {"referenceID": 15, "context": "Word Mover\u2019s Distance (WMD): We apply Word Mover\u2019s Distance [16] as the measure of expertise matching, and then rank the candidates by WMD.", "startOffset": 60, "endOffset": 64}, {"referenceID": 8, "context": "Specifically, we use the implementation of SVM-Rank [9].", "startOffset": 52, "endOffset": 55}, {"referenceID": 22, "context": "It also confirms the discovery in the survey [23] that whether a reviewer agrees to review depends not only on expertise matching, but also on other factors.", "startOffset": 45, "endOffset": 49}, {"referenceID": 1, "context": "It confirms the finding in a recent survey [2],", "startOffset": 43, "endOffset": 46}, {"referenceID": 2, "context": "Early works such as latent semantic indexing [3] and keyword matching [5] tried to solve the problem in an information retrieval system.", "startOffset": 45, "endOffset": 48}, {"referenceID": 4, "context": "Early works such as latent semantic indexing [3] and keyword matching [5] tried to solve the problem in an information retrieval system.", "startOffset": 70, "endOffset": 73}, {"referenceID": 18, "context": "compared several language models and proposed Author-Persona-Topic model [19].", "startOffset": 73, "endOffset": 77}, {"referenceID": 10, "context": "proposed reviewer aspect modeling and paper aspect modeling, and tried to model multiple aspects of expertise [11].", "startOffset": 110, "endOffset": 114}, {"referenceID": 0, "context": "These models usually consider the constraints in conference paper-reviewer assignment tasks [1], such as conflict of interests, paper demand and reviewer workload.", "startOffset": 92, "endOffset": 95}, {"referenceID": 16, "context": "Many methods have been proposed, such as search and greedy algorithm [17,13], integer linear programming [10], network flow [4] and minimum cost flow [22].", "startOffset": 69, "endOffset": 76}, {"referenceID": 12, "context": "Many methods have been proposed, such as search and greedy algorithm [17,13], integer linear programming [10], network flow [4] and minimum cost flow [22].", "startOffset": 69, "endOffset": 76}, {"referenceID": 9, "context": "Many methods have been proposed, such as search and greedy algorithm [17,13], integer linear programming [10], network flow [4] and minimum cost flow [22].", "startOffset": 105, "endOffset": 109}, {"referenceID": 3, "context": "Many methods have been proposed, such as search and greedy algorithm [17,13], integer linear programming [10], network flow [4] and minimum cost flow [22].", "startOffset": 124, "endOffset": 127}, {"referenceID": 21, "context": "Many methods have been proposed, such as search and greedy algorithm [17,13], integer linear programming [10], network flow [4] and minimum cost flow [22].", "startOffset": 150, "endOffset": 154}, {"referenceID": 25, "context": "Recently, a few systems have also been developed to make reviewer recommendations [26], help with conference reviewer assignment [14] or identifying reviewers for proposals [6].", "startOffset": 82, "endOffset": 86}, {"referenceID": 13, "context": "Recently, a few systems have also been developed to make reviewer recommendations [26], help with conference reviewer assignment [14] or identifying reviewers for proposals [6].", "startOffset": 129, "endOffset": 133}, {"referenceID": 5, "context": "Recently, a few systems have also been developed to make reviewer recommendations [26], help with conference reviewer assignment [14] or identifying reviewers for proposals [6].", "startOffset": 173, "endOffset": 176}, {"referenceID": 24, "context": "[25] presents a patent partner recommendation framework in enterprise social networks, which also uses a ranking factor graph model.", "startOffset": 0, "endOffset": 4}], "year": 2016, "abstractText": "We study the problem of finding appropriate experts who are able to complete timely reviews and would not say \u201cno\u201d to the invitation. The problem is a central issue in many question-and-answer systems, but has received little research attention. Different from most existing studies that focus on expertise matching, we want to further predict the expert\u2019s response: given a question, how can we find the expert who is able to provide a quality review and will agree to do it. We formalize the problem as a ranking problem. We first present an embedding-based question-to-expert distance metric for expertise matching and propose a ranking factor graph (RankFG) model to predict expert response. For online evaluation, we developed a Chrome Extension for reviewer recommendation and deployed it in the Google Chrome Web Store, and then collected the reviewers\u2019 feedback. We also used the review bidding of a CS conference for evaluation. In the experiments, the proposed method demonstrates its superiority (+6.6-21.2% by MAP) over several state-of-the-art algorithms.", "creator": "LaTeX with hyperref package"}}}