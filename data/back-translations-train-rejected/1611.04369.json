{"id": "1611.04369", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Nov-2016", "title": "Feature Engineering and Ensemble Modeling for Paper Acceptance Rank Prediction", "abstract": "Measuring research impact and ranking academic achievement are important and challenging problems. Having an objective picture of research institution is particularly valuable for students, parents and funding agencies, and also attracts attention from government and industry. KDD Cup 2016 proposes the paper acceptance rank prediction task, in which the participants are asked to rank the importance of institutions based on predicting how many of their papers will be accepted at the 8 top conferences in computer science. In our work, we adopt a three-step feature engineering method, including basic features definition, finding similar conferences to enhance the feature set, and dimension reduction using PCA. We propose three ranking models and the ensemble methods for combining such models. Our experiment verifies the effectiveness of our approach. In KDD Cup 2016, we achieved the overall rank of the 2nd place.", "histories": [["v1", "Mon, 14 Nov 2016 12:59:07 GMT  (120kb,D)", "http://arxiv.org/abs/1611.04369v1", "2nd place winner report of KDD Cup 2016. More details can be found atthis https URL"]], "COMMENTS": "2nd place winner report of KDD Cup 2016. More details can be found atthis https URL", "reviews": [], "SUBJECTS": "cs.AI cs.IR", "authors": ["yujie qian", "yinpeng dong", "ye ma", "hailong jin", "juanzi li"], "accepted": false, "id": "1611.04369"}, "pdf": {"name": "1611.04369.pdf", "metadata": {"source": "CRF", "title": "Feature Engineering and Ensemble Modeling for Paper Acceptance Rank Prediction", "authors": ["Yujie Qian", "Yinpeng Dong", "Ye Ma", "Hailong Jin", "Juanzi Li"], "emails": ["y-ma13}@mails.tsinghua.edu.cn,", "tsinghua_phd@163.com,", "lijuanzi@tsinghua.edu.cn", "permissions@acm.org."], "sections": [{"heading": "1. INTRODUCTION", "text": "This year, it is only a matter of time before we reach an agreement."}, {"heading": "2. FRAMEWORK", "text": "The framework of our solution is illustrated in Fig. 1. It mainly consists of three components: feature extracting and pre-processing, model selection and training, model blending and ensemble. Details of these three parts are elaborated in the following sections. Below are some general discussions. The first part of our framework is feature engineering, which is considered the most basic phase of data mining tasks. We first analyze the given MAG data set, build a database for future use, and define certain basic and intuitive features. We then look for methods to expand our collection of features, especially by finding similar conferences at each target conference. Finally, we perform dimension reduction through PCA to achieve better performance. In the second part, we primarily select three ranking models in our experiments, including a simple base model based on ranking scores in history, popular regression models such as linear regression models and learning models based on SVM, and some Rank models."}, {"heading": "3. FEATURE ENGINEERING", "text": "Our feature engineering process essentially consists of three steps. First, we define several basic characteristics for each InstitutionConference-Year tuple. When ranking an institution in a conference, we use the characteristics of the institution in the conference of the last three years. Then, we propose an approach to find similar conferences for each interested conference, and insert these similar conferences into the feature set to include more useful information. Finally, we reduce the size of the feature set to improve the performance of the ranking models."}, {"heading": "3.1 Basic Features", "text": "We define six basic characteristics for each institution conference year tuples. Specifically, these characteristics for a tupel institution A, conference B, year (e) C show the performance of institution A at conference B in year (e) C. The characteristics are in Table 1. All characteristics we use are statistical characteristics that represent the performance of the institution at the conference. We count the number of first and second authors because the first author is the lead author of the work and the second author is usually the mentor or another important author. In addition, we calculate the ranking of the institution according to the metric defined by the KDD Cup organizers. When we create a ranking model for a conference, we create characteristics for each institution in the last three years, each year individually and four years together. Finally, we have a basic characteristic vector of 24 for each institution."}, {"heading": "3.2 Similar Conference Features", "text": "In this context, it should be noted that the measures in question are measures taken by the ECB."}, {"heading": "3.3 Dimension Reduction", "text": "Data with high dimensions cause the problem of the curse of dimensionality and impair the efficiency of algorithms. Dimensional reduction can mitigate the curse of dimensionality and other unwanted problems, as illustrated in [5]. Many dimensionality reduction algorithms have been proposed [13]. Among them, the linear algorithm Principal Component Analysis (PCA) [9] is most popular due to its effectiveness. PCA dimension reduction has the following steps: 1. Decrease of the empirical mean; 2. Calculation of the covariance matrix S = 1 N \u2211 n xnx T n; 3. eigenvalue decomposition. Let V denote the eigenvectors of the uppermost d eigenvalues of S; 4. Reduction of the data dimension Y = V TX. In the paper acceptance rank prediction task, the number of attributes is relatively small (741) and many of them are duplicates (many attributes have the same TX factor as Y = V)."}, {"heading": "4. MODEL", "text": "In this section we present three ranking models and the ensemble methods used in the competition. Each model is suitable for this task and we combine these three models for final prediction."}, {"heading": "4.1 Baseline Model", "text": "The simplest idea to predict the acceptance of an institution's work at a conference this year is to measure its performance at that conference in recent years. It is, of course, to assume that an institution that published a large number of papers at a conference last year or the year before last will still have many accepted papers at the same conference. In accordance with this idea, we propose a baseline model. KDD Cup organizers defined for a conference the Institute Ranking Score (as described in Section 1), which was written as a scoreti, representing the score of the institution i in year. Our base model uses the following metric to predict this year's score: Predti = 1\u03c4 = 1 Scoret \u2212 ji (2), i.e., by using the average institution ranking score in recent years as a prediction value for this year. In the contest, we instead choose the value = 5. Small group emphasizes recent years, but can be unstable if an institution has had an occasional success or failure in recent years."}, {"heading": "4.2 Regression Model", "text": "In supervised learning, the most popular method is regression. The goal of regression is to predict one or more continuous target variables Y givenD dimensional feature vector (x1, x2,..., xD) as input variables. The simplest regression model is linear regression, which includes a linear combination of input variables y (x, w) = w0 + w1x1 +... + wDxD (3), where x = (x1, x2,.., xD).The training goal is to learn the parameters w = (w0, w1,..., wD).We can perform maximum probability estimation for linear regression, which corresponds to minimizing the sum of square errors [1], defined as ED (w) = 12 N \u00b2 n = 1 (yn \u2212 wTvelocity (xn))."}, {"heading": "4.3 Ranking SVM Model", "text": "Rank learning refers to the machine learning approaches of training models in a ranking task. If we consider predicting paper acceptance as a ranking problem, we can apply some learning approaches to rank models in this task. - Existing learning of rank models can be divided into three groups: pointedly, in pairs, and list-wise approaches [8, 3]. In this competition, we use a pair approach: ranking SVM [6]. The pair approach transforms the problem to be ranked into a classification problem - by specifying an item pair, learning a binary classifier to determine which one should be ranked higher. Then, the goal is to minimize the average number of reversals in the ranking order. In the ranking order SVM, we train SVM as a classification problem. Suppose that the training data is {(x (1) i, x (2) i, yi) a.idate.i."}, {"heading": "4.4 Ensemble", "text": "In case of classification problems, the error rate of classifiers can often be reduced by bagging [2], which is a common method of the model ensemble. The final model is the combination of many classifiers by uniform tuning. In case of ranking problems, we can also use the idea of bagging. We train a series of ranking models M = (M1, M2,..., MK) and model Mi gives a prediction ranking score si = (s1i, s 2 i,..., s n i), where sji is the score of the instance j, and n is the total number of ranking instances. Note that the results of each model should be normalized in [0, 1].The ensemble method we use is the average of output scores of all models, while the final prediction bys = 1K K \u2211 i = 1 si (7) ensemble modelling can give more stable ranking results - each model has its own disadvantages - and each model has its own disadvantages - and has its own disadvantages."}, {"heading": "5. EXPERIMENTS", "text": "Various experiments have been conducted to evaluate the performance of the proposed methods and well-designed features. Further experimental analyses on the effectiveness of some components in our framework will also be given. We determine the parameters for our ranking models in the experiments and then predict the paper acceptance ranking in 2016."}, {"heading": "5.1 Experimental Setup", "text": "To demonstrate the effectiveness of our proposed method, we divide the data set into training sets and validation sets. As the MAG data set only contains the full paper list of the targeted conferences from 2011 to 2015, we train our model by predicting paper acceptance in 2014 (using the 2011-2013 data to generate input functions and use the 2014 score as the reason for the truth).We train the ranking model for each targeted conference separately, i.e., SIGMOD, KDD and ICML will have different ranking models. As mentioned in Section 3, we define 6 basic features for each institution conference year tuples. For each institution, we look at their performance in 6 conference settings: targeted conference (full paper), targeted conference (all papers), 3 similar conferences (all papers), and all of these 4 conferences, and further in 4-year settings: separate, 3 years together, and together."}, {"heading": "5.2 Experiment Results", "text": "The results are presented in Table 3, which shows that the regression model achieves the highest NDCG @ 20 in SIGCOMM, KDD, FSE and MM, while the model ensemble achieves the highest NDCG @ 20 in SIGIR, SIGMOD and MobiCom. On average, the model ensemble performs best, the model ensemble does not perform significantly better than the individual model, because these ranking models themselves can perform well, and the number of models is also very limited. However, the interaction of different models can reduce uncertainty and obtain more reliable predictions compared to individual models.We note that the basic method performs well enough compared to regression and ranking models, but the average of the results of the last five years is a very simple approach, and it cannot capture the trend of each institution's research capacity."}, {"heading": "5.3 Discussions", "text": "In the subsection, we examine the effectiveness of the three feature engineering steps in our framework. We report on the results in various settings, starting with the use of basic features, and gradually improve the feature set. All experiments are conducted on the basis of the same training and validation steps as previously mentioned. Baseline model performance is not considered in this subsection, since the baseline model does not use training data, so its prediction is independent of feature engineering methods. We compare the results in three settings: 1. Use only basic features to train the model; Figure 2 shows the performance in each setting at 8 targeted conferences, and we list the average results in Table 4. We hope that the addition of similar conference features and the reduction in performance dimensions can help in predicting. However, the improvement does not always take place at each conference, because the acceptance of the paper is uncertain. On average, we see that the simple addition of SVG to the enhancement does not even have conformity characteristics in the model."}, {"heading": "6. CONCLUSION", "text": "In this article, we present our solution in the KDD Cup 2016 competition. We describe our efforts in the field of feature engineering, including basic feature definition, finding similar conferences and dimension reduction methods. We propose three ranking models and the ensemble method for prediction. Our empirical experiments illustrate the effectiveness of our approaches. In the competition, we came in second place."}, {"heading": "7. ACKNOWLEDGMENTS", "text": "The work is supported by 973 Program (No. 2014CB340504), NSFC-ANR (No. 61261130588), NSFC Key Project (No. 61533018), Tsinghua University Initiative Scientific Research Program (No. 20131089256)."}, {"heading": "8. REFERENCES", "text": "[1] C. Bishop. Pattern Recognition and Machine Learning.Information Science and Statistics. Springer, 2006. [2] L. Breiman. Bagging Predictors. Machine learning, 24 (2): 123-140, 1996. [3] L. Hang. A short introduction to learn to rank. IEICETRANSACTIONS on Information and Systems, 94 (10): 1854-1862, 2011. [4] K. J\u00e4rvelin and J. Kek\u00e4l\u00e4inen. Cumulated gain-based evaluation of ir techniques. ACM Transactions on Information Systems (TOIS), 20 (4): 422-446, 2002. [5] L. O. Jimenez and D. A. Landgrebe. Supervised classification in high-dimensional space: geometrical, statistical, and asymptotical properties of multivariate data. IEEE Transactions on Systems, Man, and Cybernetics, Part C."}], "references": [{"title": "Pattern Recognition and Machine Learning", "author": ["C. Bishop"], "venue": "Information Science and Statistics. Springer,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2006}, {"title": "Bagging predictors", "author": ["L. Breiman"], "venue": "Machine learning, 24(2):123\u2013140,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1996}, {"title": "A short introduction to learning to rank", "author": ["L. Hang"], "venue": "IEICE TRANSACTIONS on Information and Systems, 94(10):1854\u20131862,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2011}, {"title": "Cumulated gain-based evaluation of ir techniques", "author": ["K. J\u00e4rvelin", "J. Kek\u00e4l\u00e4inen"], "venue": "ACM Transactions on Information Systems (TOIS), 20(4):422\u2013446,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2002}, {"title": "Supervised classification in high-dimensional space: geometrical, statistical, and asymptotical properties of multivariate data", "author": ["L.O. Jimenez", "D.A. Landgrebe"], "venue": "IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews), 28(1):39\u201354,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1998}, {"title": "Optimizing search engines using clickthrough data", "author": ["T. Joachims"], "venue": "Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining, pages 133\u2013142. ACM,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2002}, {"title": "Training linear svms in linear time", "author": ["T. Joachims"], "venue": "Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 217\u2013226. ACM,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2006}, {"title": "Learning to rank for information retrieval", "author": ["T.-Y. Liu"], "venue": "Foundations and Trends in Information Retrieval, 3(3):225\u2013331,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2009}, {"title": "Liii", "author": ["K. Pearson"], "venue": "on lines and planes of closest fit to systems of points in space. The London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science, 2(11):559\u2013572,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1901}, {"title": "Scikit-learn: Machine learning in Python", "author": ["F. Pedregosa", "G. Varoquaux", "A. Gramfort", "V. Michel", "B. Thirion", "O. Grisel", "M. Blondel", "P. Prettenhofer", "R. Weiss", "V. Dubourg", "J. Vanderplas", "A. Passos", "D. Cournapeau", "M. Brucher", "M. Perrot", "E. Duchesnay"], "venue": "Journal of Machine Learning Research, 12:2825\u20132830,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2011}, {"title": "An overview of microsoft academic service (mas) and applications", "author": ["A. Sinha", "Z. Shen", "Y. Song", "H. Ma", "D. Eide", "B.-j. P. Hsu", "K. Wang"], "venue": "In Proceedings of the 24th International Conference on World Wide Web,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2015}, {"title": "Arnetminer: extraction and mining of academic social networks", "author": ["J. Tang", "J. Zhang", "L. Yao", "J. Li", "L. Zhang", "Z. Su"], "venue": "Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 990\u2013998. ACM,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2008}, {"title": "Dimensionality reduction: a comparative", "author": ["L. Van Der Maaten", "E. Postma", "J. Van den Herik"], "venue": "J Mach Learn Res,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2009}, {"title": "Academic rankings with repec", "author": ["C. Zimmermann"], "venue": "Econometrics, 1(3):249\u2013280,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2013}], "referenceMentions": [{"referenceID": 11, "context": "Many issues in academic network have been investigated and several systems have been developed, such as DBLP, Google Scholar, Microsoft Academic Search, and Aminer [12].", "startOffset": 164, "endOffset": 168}, {"referenceID": 10, "context": "The competition\u2019s dataset includes the Microsoft Academic Graph (MAG) [11], and any other publicly available data on the Web.", "startOffset": 70, "endOffset": 74}, {"referenceID": 3, "context": "The competition uses NDCG@20 as the evaluation metric [4], which means only the top 20 institutions will be considered in the evaluation.", "startOffset": 54, "endOffset": 57}, {"referenceID": 13, "context": "Christian Zimmermann summarized academic ranking problems and existing methods in [14].", "startOffset": 82, "endOffset": 86}, {"referenceID": 7, "context": "Besides, many learning-based approaches have been proposed to solve the general ranking problems, which are known as learning to rank [8].", "startOffset": 134, "endOffset": 137}, {"referenceID": 4, "context": "Dimension reduction can mitigate the curse-of-dimensionality and other undesired problems, as illustrated in [5].", "startOffset": 109, "endOffset": 112}, {"referenceID": 12, "context": "Many algorithms for dimensionality reduction have been proposed [13].", "startOffset": 64, "endOffset": 68}, {"referenceID": 8, "context": "Among them, the linear algorithm Principal Component Analysis (PCA) [9] is the most popular because of its effectiveness.", "startOffset": 68, "endOffset": 71}, {"referenceID": 0, "context": "We can perform maximumlikelihood estimation for linear regression, which is equivalent to minimize the sum-of-squares error [1], defined as", "startOffset": 124, "endOffset": 127}, {"referenceID": 7, "context": "Existing learning to rank models can be categorized into three groups: pointwise, pairwise, and listwise approaches [8, 3].", "startOffset": 116, "endOffset": 122}, {"referenceID": 2, "context": "Existing learning to rank models can be categorized into three groups: pointwise, pairwise, and listwise approaches [8, 3].", "startOffset": 116, "endOffset": 122}, {"referenceID": 5, "context": "In this competition, we use a pairwise approach: Ranking SVM [6].", "startOffset": 61, "endOffset": 64}, {"referenceID": 1, "context": "In classification problems, the error rate of classifiers can often be reduced by bagging [2] which is a common method of model ensemble.", "startOffset": 90, "endOffset": 93}, {"referenceID": 0, "context": "Note that each model\u2019s output should be normalized into [0, 1].", "startOffset": 56, "endOffset": 62}, {"referenceID": 9, "context": ", Baseline, Regression (using the implementation of scikit-learn toolbox [10]), Ranking SVM (using the implementation of SVMRank [7]), and Ensemble Modeling.", "startOffset": 73, "endOffset": 77}, {"referenceID": 6, "context": ", Baseline, Regression (using the implementation of scikit-learn toolbox [10]), Ranking SVM (using the implementation of SVMRank [7]), and Ensemble Modeling.", "startOffset": 129, "endOffset": 132}], "year": 2016, "abstractText": "Measuring research impact and ranking academic achievement are important and challenging problems. Having an objective picture of research institution is particularly valuable for students, parents and funding agencies, and also attracts attention from government and industry. KDD Cup 2016 proposes the paper acceptance rank prediction task, in which the participants are asked to rank the importance of institutions based on predicting how many of their papers will be accepted at the 8 top conferences in computer science. In our work, we adopt a three-step feature engineering method, including basic features definition, finding similar conferences to enhance the feature set, and dimension reduction using PCA. We propose three ranking models and the ensemble methods for combining such models. Our experiment verifies the effectiveness of our approach. In KDD Cup 2016, we achieved the overall rank of the 2nd place.", "creator": "LaTeX with hyperref package"}}}