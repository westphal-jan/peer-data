{"id": "1611.04491", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Nov-2016", "title": "Ranking medical jargon in electronic health record notes by adapted distant supervision", "abstract": "Objective: Allowing patients to access their own electronic health record (EHR) notes through online patient portals has the potential to improve patient-centered care. However, medical jargon, which abounds in EHR notes, has been shown to be a barrier for patient EHR comprehension. Existing knowledge bases that link medical jargon to lay terms or definitions play an important role in alleviating this problem but have low coverage of medical jargon in EHRs. We developed a data-driven approach that mines EHRs to identify and rank medical jargon based on its importance to patients, to support the building of EHR-centric lay language resources.", "histories": [["v1", "Mon, 14 Nov 2016 17:36:05 GMT  (1162kb)", "http://arxiv.org/abs/1611.04491v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["jinying chen", "abhyuday n jagannatha", "samah j jarad", "hong yu"], "accepted": false, "id": "1611.04491"}, "pdf": {"name": "1611.04491.pdf", "metadata": {"source": "CRF", "title": "Ranking medical jargon in electronic health record notes by adapted distant supervision", "authors": ["Jinying Chen", "Abhyuday N. Jagannatha", "Samah J. Jarad", "Hong Yu"], "emails": ["hong.yu}@umassmed.edu,", "abhyuday@cs.umass.edu,", "samah.fodeh@yale.edu"], "sections": [{"heading": null, "text": "Goal: Allowing patients access to their own electronic medical records (EHR) through online patient portals has the potential to improve patient-centric care. However, medical jargon rich in EHR notes has been shown to be an obstacle to understanding patient-centric EHR. Existing knowledge bases that link medical jargon with technical terms or definitions play an important role in alleviating this problem, but have limited coverage of medical jargon in EHR. We developed a data-driven approach that identifies and evaluates medical jargon by its importance to patients to support the development of EHR-centric lay language resources. Methods: We developed an innovative adapted remote monitoring model (ADS) based on support vector machines to rank medical jargon from EHRs. For remote monitoring, we used open access, collaborative health vocabularies that support the public health jargon that is widely available."}, {"heading": "INTRODUCTION", "text": "This year is the highest in the history of the country."}, {"heading": "MATERIALS AND METHODS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "EHR Corpora and Candidate Terms", "text": "In this study, we used two EHR corpora: EHR-Pittsburg and EHR-UMass.EHR-Pittsburg 1 included 7,839 summarized terms for discharge with 5.4M words. We applied the linguistic filter of the automatic term recognition tool Jate [24] to this corpus and extracted 106,108 terms of candidates (see Step 1 in Figure 2). These terms were further used to identify and classify medical terminology. EHR-UMass included 90 de-identified terms from the UMass Memorial Hospital outpatient clinics. To maximize representativeness, we selected notes from patients with six common primary clinical diagnoses: cancer, COPD, diabetes, heart failure, hypertension and liver failure. We decoded the notes and then asked the physicians to identify for each note important terms for patients to whom we need to identify the most relevant terms for evaluation [25], the EHR-64 physicians were asked to evaluate the most relevant aspects for evaluation, as well as the medical doctors to understand the evaluation of the EHR-64."}, {"heading": "Medical Jargon in CHV", "text": "In this study, we examined the scope of CHV for medical jargon in the EHR. Additionally, we used medical jargon in the CHV to generate training data for the ADS model. We followed [9] (i.e. CHV familiarity score \u2264 0.6) to identify medical terms in the CHV."}, {"heading": "Baseline Systems", "text": "In this context, it should be noted that the measures in question are measures taken by the European Commission, in order to take measures taken by the European Commission."}, {"heading": "The ADS Model", "text": "We are a case of learning positive and unlabeled medical terms from the universe. In particular, Elkan and Noto [32] have shown that a binary classification mechanism that expresses the likelihood of errors in the examples can lead to positive examples and unlabeled examples being used in the new examples. We have used CHV to select and label patients in order to train them. Our approach is based on the assumption that medical terms must be important to patients in order to be used by patients. Specifically, we assume that medical terms that occur in both honors are important to treat patients because they are medical synonyms generated by patients in online health forums. Based on this assumption, we have used medical terms in both EHRs and CHV terms."}, {"heading": "Training and Evaluation Datasets", "text": "We used two sets of data in our study. The EHR-UMASS data set was used to train, evaluate and generate the global ranking of applicant terms, while the EHR-UMASS data set was used for evaluation. Statistics of these two sets of data are in Table 1. Training Using the EHR-Pittsburg DatasetThe number of terms used as positive or blank data was 6,959 and 99,149 respectively (see step 2 in Figure 2). For the training, we divided the data into 10 folds. We used 9 folds to train the ADS model and applied it to the remaining fold to obtain the ranking of applicant terms. We produced a total of 10 ranking outputs to generate a global ranking that we rated using a negative rating that measures system performance for learning from positive and blank data."}, {"heading": "Post-processing", "text": "Since we found that the performance of TF * IDF and C-value were negatively impacted by common terms (e.g. \"patient\" and \"pain\") in EHR-UMass, we added a post-processing process that used a stopword list to filter common terms from the results of the models. This list includes 100 high-frequency, non-medical terms in the EHR-UMass corpus. In addition, we use rotating expressions to classify low-composition terms that contain \"not\" no, \"\" and \"or.\" In our evaluation using the EHR-UMass data set, we report on both conditions: with and without post-processing."}, {"heading": "Evaluation Metrics", "text": "Receiver Operating Characteristic (ROC) and Area Under ROC Curve (ROC-AUC): The ROC curve is a widely used indicator for evaluating ranking outputs. It records the true positive rate (y-coordinate) versus the false positive rate (x-coordinate) at various thresholds. We report on both ROC and ROC-AUC using the R library pROC [41].Indicators for learning from positive and blank data (PU metrics): The evaluation of systems learning from positive and blank data is difficult because the data contains blank examples and we cannot calculate memory and precision. To evaluate, Lee and Liu [42] introduced PU indicators, r2 / Pr [system positive], where r is the probability of positive examples predicted by the system. Recall can be determined by the total number of positive predictions divided by the total number of designated positive examples, the Pk function is estimated as Pk [42]."}, {"heading": "RESULTS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "CHV Coverage of Medical Jargon in EHRs", "text": "From the EHR Pittsburg corpus, we extracted 106,108 candidate terms, where we used MetaMap [43] to select medical terms. A total of 19,503 (18%) of the candidate terms were successfully assigned to concepts of the Unified Medical Language System by MetaMap. However, 4,680 (24%) of these medical terms do not appear in the CHV. We examined these terms manually and found medical terms such as \"Bruton agammaglobulinemia,\" \"molecular diagnostics,\" \"motor symptom\" and \"reactive lymphocytosis\" in most of them. The remaining 86,605 (82%) candidate terms also included medical terms such as \"anticardiolipin,\" \"BGM,\" \"demargination,\" \"heptoglobin,\" \"hypoalimentation\" and \"hypobilirubinemia.\""}, {"heading": "ADS Ranking Performance on EHR-Pittsburg Dataset", "text": "Figure 3 shows the PU ratios depending on the rank k of the ADS model and two base systems on the EHR Pittsburg dataset. The PU ratios curve of ADS quickly peaks at k = 9,229 and then declines sharply. In contrast, the PU ratios of the two base systems are relatively stable. Overall, ADS was consistently better for all k than the two baseline systems."}, {"heading": "ADS Ranking Performance on EHR-UMass Dataset", "text": "Figure 4 shows the ROC curves of ADS, TF * IDF and C-value in the order of EHR-UMass, with and without post-processing. ADS achieved the best performance. Table 2 shows the ROCAUC, in which ADS significantly exceeded TF * IDF and C-value (> 15 points, absolute increases). Although the performance of TF * IDF and C-value was significantly improved after post-processing, ADS still performed better. Figure 4. ROC shows different methods in the order of EHR-UMass terms: (4a) without post-processing and (4b) with post-processing."}, {"heading": "DISCUSSION", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Principle Results", "text": "For this reason, we have developed the ADS model to include terms from the EHR-Pittsburg corpus in the ranking of 100 K candidates and prioritize our annotation of layman terms / definitions for the best-placed terms. ADS evaluates EHR terms based on the assumption that medical terms that occur in both EHR and CHV are important to patients. ADS reached 0.810 ROC-AUC on the EHR-UMass dataset (Table 2). This level of performance is appropriate, especially considering that ADS does not use human-commented training data. This result indirectly confirms the validity of our assumption. It also suggests that we can use ADS to prioritize EHR terms for annotation."}, {"heading": "Interpretation of the PU Metrics Curves", "text": "In Figure 3, the performance of ADHD on the EHR Pittsburg data set peaks at k = 9,229, with a steep decline thereafter. At its peak, ADHD is able to identify 5,248 (75%) of the total (4a) (4b) 6,959 positively labeled terms (i.e. EHR-CHV technical terms), most of which are important medical terms, including \"premature respiratory contraction,\" \"polycythemia,\" \"T-cell lymphoma\" and \"thallium stress test.\" The non-CHV terms classified by ADHD in the top 10K also include many important medical terms, such as \"chronic respiratory insufficiency,\" \"nasogastric decompression,\" \"preoperative chemotherapy.\" In the lower ranks, ADHD is less effective in identifying EHR-CHV technical terms, \"which may include\" high-activated nasogastric decompression, \"\" high-activated nasographic. \""}, {"heading": "ADS and the Baselines TF*IDF and C-Value", "text": "Figure 4 and Table 2 show that ADSs outperform the two baselines of the EHR-UMass dataset. Our results analysis shows that the three models in the top rankings perform similarly (top-n lists, where n < 30) and ADSs perform much better in the bottom rankings. As shown in Table 4, the top 10 errors that ADSs have identified from the EHR-UMass dataset are all EHR-CHV terms (with post-processing). As doctors identify only a few (15 per grade, on average) important medical terms from each EHR-CHV jargon as bold, they do not mark many CHV terms as important. However, this type of error is not critical to our annotation task."}, {"heading": "CONCLUSION", "text": "Our experiments have shown that ADS is more effective than TF * IDF and CValue, two methods widely used to extract and prioritize terms from large corpora texts to build domain-specific lexical resources. EHR terms prioritized in our model are used to enrich a comprehensive knowledge resource to simplify jargon and define EHR."}, {"heading": "ACKNOWLEDGMENTS", "text": "This work was supported by the Investigator Initiated Research (1I01HX001457-01) of the Health Services Research and Development Program of the United States Department of Veterans Affairs. Content is the sole responsibility of the authors and does not represent the views of the United States Department of Veterans Affairs or the U.S. Government. We thank Weisong Liu for the technical assistance in collecting the EHR notes and the UMassMed annotation team, which includes Elaine Freund, Victoria Wang, Andrew Hsu, Barinder Hansra and Sonali Harchandani, for creating the UMass EHR corpus."}], "references": [{"title": "Medical communication: do our patients", "author": ["EB Lerner", "DV Jehle", "DM Janicke"], "venue": null, "citeRegEx": "Lerner et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Lerner et al\\.", "year": 2012}, {"title": "Lay understanding of terms used in cancer consultations", "author": ["K Chapman", "C Abraham", "V Jenkins"], "venue": "Psychooncology 2003;12:557\u201366", "citeRegEx": "Chapman et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Chapman et al\\.", "year": 2003}, {"title": "Patients\u2019 experiences when accessing their on-line electronic patient records in primary care", "author": ["C Pyper", "J Amery", "M Watson"], "venue": "Br J Gen Pr 2004;54:38\u201343", "citeRegEx": "Pyper et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Pyper et al\\.", "year": 2004}, {"title": "Towards consumer-friendly PHRs: patients\u2019 experience with reviewing their health records", "author": ["A Keselman", "L Slaughter", "CA Smith"], "venue": "Proceedings of AMIA Annual Symposium", "citeRegEx": "Keselman et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Keselman et al\\.", "year": 2007}, {"title": "Lay understanding of common medical terminology in oncology", "author": ["AH Pieterse", "NA Jager", "EMA Smets"], "venue": "Psychooncology 2013;22:1186\u201391", "citeRegEx": "Pieterse et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Pieterse et al\\.", "year": 2013}, {"title": "Promoting health literacy", "author": ["McCray AT"], "venue": "J Am Med Inform Assoc 2005;12:152\u2013163", "citeRegEx": "AT.,? \\Q2005\\E", "shortCiteRegEx": "AT.", "year": 2005}, {"title": "Making texts in electronic health records comprehensible to consumers: a prototype translator", "author": ["Q Zeng-Treitler", "S Goryachev", "H Kim"], "venue": "Proceedings of AMIA Annual Symposium", "citeRegEx": "Zeng.Treitler et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Zeng.Treitler et al\\.", "year": 2007}, {"title": "A semantic and syntactic text simplification tool for health content", "author": ["S Kandula", "D Curtis", "Q. Zeng-Treitler"], "venue": "Proceedings of AMIA Annual Symposium", "citeRegEx": "Kandula et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Kandula et al\\.", "year": 2010}, {"title": "Medical text simplification using synonym replacement: Adapting assessment of word difficulty to a compounding language. In: Proceedings of the 3rd Workshop on Predicting and Improving Text Readability for Target Reader Populations at EACL", "author": ["E Abrahamsson", "T Forni", "M Skeppstedt"], "venue": "Association for Computational Linguistics", "citeRegEx": "Abrahamsson et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Abrahamsson et al\\.", "year": 2014}, {"title": "Improving Patients\u2019 Electronic Health Record Comprehension with NoteAid", "author": ["B Polepalli Ramesh", "T Houston", "C Brandt"], "venue": "Stud Health Technol Inform 2013;192:714\u20138", "citeRegEx": "Ramesh et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Ramesh et al\\.", "year": 2013}, {"title": "Exploring and developing consumer health vocabularies", "author": ["QT Zeng", "T. Tse"], "venue": "J Am Med Inform Assoc 2006;13:24", "citeRegEx": "Zeng and Tse,? \\Q2006\\E", "shortCiteRegEx": "Zeng and Tse", "year": 2006}, {"title": "Terminology issues in user access to Web-based medical information", "author": ["AT McCray", "RF Loane", "AC Browne"], "venue": "Proc AMIA Symp 1999;:107\u201311", "citeRegEx": "McCray et al\\.,? \\Q1999\\E", "shortCiteRegEx": "McCray et al\\.", "year": 1999}, {"title": "Patient and clinician vocabulary: how different are they", "author": ["Q Zeng", "S Kogan", "N Ash"], "venue": "Stud Health Technol Inform 2001;84:399\u2013403", "citeRegEx": "Zeng et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Zeng et al\\.", "year": 2001}, {"title": "Evaluation of Controlled Vocabulary Resources for Development of a Consumer Entry Vocabulary for Diabetes", "author": ["TB Patrick", "HK Monga", "MC Sievert"], "venue": "J Med Internet Res 2001;3:e24", "citeRegEx": "Patrick et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Patrick et al\\.", "year": 2001}, {"title": "Characteristics of consumer terminology for health information retrieval", "author": ["Q Zeng", "S Kogan", "N Ash"], "venue": "Methods Inf Med", "citeRegEx": "Zeng et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Zeng et al\\.", "year": 2002}, {"title": "Term identification methods for consumer health", "author": ["Q Zeng", "T Tse", "G Divita"], "venue": null, "citeRegEx": "Zeng et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Zeng et al\\.", "year": 2003}, {"title": "Facilitating the development", "author": ["I Spasi\u0107", "D Schober", "S-A Sansone"], "venue": "J Med Internet Res 2007;9:e4", "citeRegEx": "Spasi\u0107 et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Spasi\u0107 et al\\.", "year": 2007}, {"title": "A Comparative Evaluation of Term", "author": ["Z Zhang", "J Iria", "C Brewster"], "venue": null, "citeRegEx": "Zhang et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2011}, {"title": "Language Resources and Evaluation (LREC)", "author": ["J Chen", "J Zheng", "H. Yu"], "venue": null, "citeRegEx": "Chen et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2008}, {"title": "A statistical interpretation of term specificity and its application", "author": ["K. Sparck Jones"], "venue": "Int J Digit Libr 2000;3:115\u2013130", "citeRegEx": "Jones,? \\Q2000\\E", "shortCiteRegEx": "Jones", "year": 2000}, {"title": "Text classification from positive and unlabeled", "author": ["F Denis", "R Gilleron", "M. Tommasi"], "venue": "J Doc 1972;28:11\u201321", "citeRegEx": "Denis et al\\.,? \\Q1972\\E", "shortCiteRegEx": "Denis et al\\.", "year": 1972}, {"title": "Building text classifiers using positive and unlabeled", "author": ["B Liu", "Y Dai", "X Li"], "venue": null, "citeRegEx": "2002", "shortCiteRegEx": "2002", "year": 1934}, {"title": "A simple probabilistic approach to learning from positive and unlabeled examples", "author": ["Zhang D", "Lee WS"], "venue": "Proceedings of the 5th Annual UK Workshop on Computational Intelligence (UKCI). Citeseer", "citeRegEx": "D and WS.,? \\Q2005\\E", "shortCiteRegEx": "D and WS.", "year": 2005}, {"title": "Text classification without negative examples revisit", "author": ["GPC Fung", "JX Yu", "H Lu"], "venue": "Knowl Data Eng IEEE Trans On 2006;18:6\u201320", "citeRegEx": "Fung et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Fung et al\\.", "year": 2006}, {"title": "Learning classifiers from only positive and unlabeled data", "author": ["C Elkan", "K. Noto"], "venue": "Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining", "citeRegEx": "Elkan and Noto,? \\Q2008\\E", "shortCiteRegEx": "Elkan and Noto", "year": 2008}, {"title": "A bagging SVM to learn from positive and unlabeled examples", "author": ["Mordelet F", "Vert J-P"], "venue": "Pattern Recognit Lett 2014;37:201\u2013209", "citeRegEx": "F and J.P.,? \\Q2014\\E", "shortCiteRegEx": "F and J.P.", "year": 2014}, {"title": "LIBSVM: a library for support vector machines", "author": ["Chang C-C", "Lin C-J"], "venue": "ACM Trans Intell Syst Technol TIST 2011;2:27:1-27:27", "citeRegEx": "C.C and C.J.,? \\Q2011\\E", "shortCiteRegEx": "C.C and C.J.", "year": 2011}, {"title": "Probabilistic outputs for support vector machines and comparisons to regularized likelihood methods", "author": ["J. Platt"], "venue": "Adv Large Margin Classif", "citeRegEx": "Platt,? \\Q1999\\E", "shortCiteRegEx": "Platt", "year": 1999}, {"title": "A note on Platt\u2019s probabilistic outputs for support vector machines. Mach Learn 2007;68:267\u2013276", "author": ["Lin H-T", "Lin C-J", "Weng RC"], "venue": null, "citeRegEx": "H.T et al\\.,? \\Q2007\\E", "shortCiteRegEx": "H.T et al\\.", "year": 2007}, {"title": "Efficient Estimation of Word Representations in Vector Space", "author": ["T Mikolov", "K Chen", "G Corrado"], "venue": "Cs Published Online First:", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["T Mikolov", "I Sutskever", "K Chen"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Distributional semantics resources for biomedical text processing", "author": ["S Pyysalo", "F Ginter", "H Moen"], "venue": "The 5th International Symposium on Languages in Biology and Medicine", "citeRegEx": "Pyysalo et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Pyysalo et al\\.", "year": 2013}, {"title": "pROC: an open-source package for R and S+ to analyze and compare ROC curves", "author": ["X Robin", "N Turck", "A Hainard"], "venue": "BMC Bioinformatics", "citeRegEx": "Robin et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Robin et al\\.", "year": 2011}, {"title": "Learning with positive and unlabeled examples using weighted logistic regression", "author": ["WS Lee", "B. Liu"], "venue": "ICML", "citeRegEx": "Lee and Liu,? \\Q2003\\E", "shortCiteRegEx": "Lee and Liu", "year": 2003}, {"title": "An overview of MetaMap: historical perspective and recent advances", "author": ["Aronson AR", "Lang F-M"], "venue": "J Am Med Inform Assoc 2010;17:229\u201336", "citeRegEx": "AR and F.M.,? \\Q2010\\E", "shortCiteRegEx": "AR and F.M.", "year": 2010}], "referenceMentions": [], "year": 2016, "abstractText": "Objective: Allowing patients to access their own electronic health record (EHR) notes through online patient portals has the potential to improve patient-centered care. However, medical jargon, which abounds in EHR notes, has been shown to be a barrier for patient EHR comprehension. Existing knowledge bases that link medical jargon to lay terms or definitions play an important role in alleviating this problem but have low coverage of medical jargon in EHRs. We developed a data-driven approach that mines EHRs to identify and rank medical jargon based on its importance to patients, to support the building of EHR-centric lay language resources. Methods: We developed an innovative adapted distant supervision (ADS) model based on support vector machines to rank medical jargon from EHRs. For distant supervision, we utilized the open-access, collaborative consumer health vocabulary, a large, publicly available resource that links lay terms to medical jargon. We explored both knowledge-based features from the Unified Medical Language System and distributed word representations (word embeddings) learned from unlabeled large corpora. We evaluated the ADS model using physician-identified important medical terms. Results: Our ADS model significantly surpassed two state-of-the-art automatic term recognition methods, TF*IDF and C-Value, yielding 0.810 ROC-AUC versus 0.710 and 0.667, respectively. Our model identified over 10K important medical jargon terms after ranking over 100K candidate terms mined from over 7,500 EHR narratives. Conclusion: Our work is an important step towards enriching lexical resources that link medical jargon to lay terms/definitions to support patient EHR comprehension. The identified medical jargon terms and their rankings are available upon request.", "creator": "Microsoft\u00ae Word 2016"}}}