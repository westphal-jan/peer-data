{"id": "1611.04503", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Nov-2016", "title": "Zero-resource Machine Translation by Multimodal Encoder-decoder Network with Multimedia Pivot", "abstract": "We propose an approach to build a neural machine translation system with no supervised resources (i.e., no parallel corpora) using multimodal embedded representation over texts and images. Based on the assumption that text documents are often likely to be described with other multimedia information (e.g., images) somewhat related to the content, we try to indirectly estimate the relevance between two languages. Using multimedia as the \"pivot\", we project all modalities into one common hidden space where samples belonging to similar semantic concepts should come close to each other, whatever the observed space of each sample is. This modality-agnostic representation is the key to bridging the gap between different modalities. Putting a decoder on top of it, our network can flexibly draw the outputs from any input modality. Notably, in the testing phase, we need only source language texts as the input for translation. In experiments, we tested our method on two benchmarks to show that it can achieve reasonable translation performance. We compared and investigated several possible implementations and found that an end-to-end model that simultaneously optimized both rank loss in multimodal encoders and cross-entropy loss in decoders performed the best.", "histories": [["v1", "Mon, 14 Nov 2016 18:07:54 GMT  (2709kb)", "http://arxiv.org/abs/1611.04503v1", null], ["v2", "Sun, 21 May 2017 17:36:30 GMT  (2655kb)", "http://arxiv.org/abs/1611.04503v2", null], ["v3", "Sun, 23 Jul 2017 15:52:08 GMT  (2656kb)", "http://arxiv.org/abs/1611.04503v3", "Some error corrections in Sect.2.2 and Table 5, Machine Translation, 2017"]], "reviews": [], "SUBJECTS": "cs.CL cs.AI cs.CV cs.MM", "authors": ["hideki nakayama", "noriki nishida"], "accepted": false, "id": "1611.04503"}, "pdf": {"name": "1611.04503.pdf", "metadata": {"source": "CRF", "title": "Zero-resource Machine Translation by Multimodal Encoder-decoder Network with Multimedia Pivot", "authors": ["Hideki Nakayama", "Noriki Nishida"], "emails": ["nakayama@ci.i.u-tokyo.ac.jp", "nishida@nlab.ci.i.u-tokyo.ac.jp"], "sections": [{"heading": null, "text": "This year, it has reached the point where it will be able to take the lead without being able to seek a solution."}, {"heading": "II. RELATED WORK", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. Resource Problem in Cross-lingual Learning", "text": "Dealing with constraints on the number of high-quality parallel or comparable corpora was therefore one of the most important issues in the field of interlingual learning. A simple approach is to automatically extract parallel corpora, typically from noisy web repositories. Some methods used a bootstrap approach based on translation systems for basics [33], while others used external meta information such as links to the same URL to pair bilingual texts [25], among which images were also used as keys for document conformity in relatively early work. However, these methods are simply based on OCR reading or near-duplicate (copy) recognition of images [22] and therefore cannot identify similarities in semantics, which is a fundamental constraint compared to our work. Another line of work was to translate the MT system of non-parallel data using a different modality for indirect knowledge transfer, which is referred to as void translation."}, {"heading": "B. Computer Vision for Machine Translation", "text": "Grounding a natural language to real representations has always been an important topic in the NLP, for which computer vision would be the first natural choice [28]. After a huge breakthrough in the use of Convolutionary Neural Networks (CNNs) [19], visual recognition has evolved considerably in terms of both accuracy and flexibility, enabling the development of many brand-new technologies, among which the caption, which automatically comments on a description of an input image in natural language, has become one of the hottest topics in recent years [34], [13] Since captions are essentially interpreted as \"translation\" from animation to sentence, it has also attracted more and more attention in the NLP community. Recently, a new field of research called multimodal machine translation [4], [10] which became a subtask in WMT 2016.1, the goal of this task is to use images as inputs in addition to source languages in order to improve translation performance, thus avoiding the overloading of this part of the text."}, {"heading": "C. Multimodal Embedding", "text": "In order to use non-linguistic multimedia as the linchpin for MT, we need a more flexible mechanism for semantically aligning different types of data, the key idea being to derive a common representation shared by all modalities. In other words, regardless of modality, observed data belonging to the same implicit concept should be mapped to roughly the same point in the embedding space. [9] The most classic and standardized method for multimodal learning is probably linear canonical correlation analysis (CCA) [12], which has been successfully applied in visual collaborations such as semantic image acquisition and annotation [7], as well as translational information retrieval [32] [7]. Recent methods based on deep neural networks have shown that pairwise ranking loss significantly improves our multimodal embedding [6] due to the natural ability to learn multimodal coding methods."}, {"heading": "III. OUR APPROACH", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. Overview", "text": "Our goal is to build a translation model from a source language s to a target language t, using the page information (images) as the fulcrum. In the following, we will call a pair of text descriptions d and their counterpart image i a \"document.\" To train the system, we assume that we have N's monolingual documents in the source language, T s = {dsk, i s} Nsk = 1. Likewise, we have N t documents in the target language, T t = {dtk, i t} Nt = 1,1http: / www.statmt.org / wmt16 / multimodal-task.htmlEs and Et are forced to have high correlations with the image coder Ev in multimodal space on which the decoder is trained the target language Dt."}, {"heading": "B. Model Topologies", "text": "For the two-way model, we take the loss of the source image as follows: (Fig. 1: Top): JE 2w (Ts) = \u2211 is also an improved target value (Ev (is), Es (ds)) + s (Ev (is), Es (dsng)}, (1) where \u03b1 is the hyperparameter of the margin and the target function of similarity, s () measures the Dot product. Note that the results of each encoder are standardized as a unit and thus equal to the cosmic similarity. dsng denotes negative (uncoupled) descriptions for the same target function, s () measures the Dot product."}, {"heading": "C. Training Strategy", "text": "We are examining two strategies for training the entire model. The first strategy is the two-step approach, where we first optimize the encoder loss, JE. Then we set the parameters for all encoders and start optimizing the decoder with respect to JD. The second strategy is the end-to-end approach, where we jointly optimize the encoder and decoder losses."}, {"heading": "D. Difference from Closely Related Methods", "text": "Although our work is, as far as we know, the first attempt at machine zero-resource translation using multimedia pivot, there were some theoretically close methods that inspired our model, and the topology of our network is similar to the recently proposed many-to-one sequence-to-sequence model [21]. However, to deal with the zero-shot problem, Firat et al. [5] integrated some synthetic parallel corpora to explicitly include the source-to-target path during training, in other words, they approach the zero-shot problem on the data side as we approach the target side using the zero-shot problem."}, {"heading": "IV. EXPERIMENT", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. Data Set", "text": "For our study, we used two publicly available multilingual image description datasets. The IAPR-TC12 dataset [8] has 20,000 images with their English and German descriptions, the original descriptions were provided in German, and their English translations were added by specialists. The recently published Multi30K dataset [4] is specifically designed for research into multimodal machine translations. It has approximately 30,000 images with English and German descriptions for each image. This is an extension of Flickr30K [38], an English image caption dataset for which German translations of [4] are provided. In this experiment, we used the annotations for the multimodal machine translation task at WMT '16. Since the basic German descriptions for the original test set are reserved for the competition and are not publicly available, we used the original validation datasets for our test sets."}, {"heading": "B. Experimental Setup", "text": "Since the choice of encoders and decoders for each modality was not within the scope of this paper, we used the most common neural models for each domain. We chose most of the neural models we used for each domain. (For most neural models in each domain, we used the methods we developed from the \"fc7\" layer of VGNs-16 and other fully connected layers of 1024 hidden units each, which are only trained during training.) For speech coders and decoders Es, Et, and Dt, we used recurrent neural networks (RNs) with long short-term memories. We used 512-dimensional word embedding and 1024-dimensional hidden units, which should match the dimensions of all encoders so that they can be coupled in multimodal space. We used 5- 12- 12- word embedded and 1024-dimensional units."}, {"heading": "D. Main Results and Discussion", "text": "Table IV shows a detailed comparison of our approach in different configurations. Comparing our results with baselines (Table II), our best results are comparable to sequence-to-sequence models using parallel corpora, which are about 20% as large as our monolingual corpora. A comparison of model topologies shows that the three-way models generally outperform their two-way models. However, when only images were transmitted for decoder training, the performance differences were subtle, and the two-way model sometimes performed better than the three-way model. The most attractive aspect of the three-way approach is that we can use both image and description for decoder training, which always yielded the best results. We may be able to use external monolingual corpora to further improve decoders that we wish to study in our future work."}, {"heading": "V. CONCLUSION", "text": "Unlike many previous studies, in which multimedia was simply used as input in addition to text to enhance machine translation, we did not use parallel corpora for training or image input in the test phase. Our system was made possible by training multimodal encoders that shared a common modality agnostic semantic representation with images as the fulcrum. We compared several possible implementations and demonstrated the feasibility of our approach. In particular, we found that the three-way model is particularly promising in terms of performance and flexibility in dealing with different modality-specific data. Although our goal in this paper was a completely uncontrolled setup, we can of course integrate some parallel data in a semi-monitored manner or external monolingual text corpora in the target language to further improve performance, which is an attractive direction for future research."}], "references": [{"title": "Learning Bilingual Lexicons Using the Visual Similarity of Labeled Web Images", "author": ["S. Bergsma", "B. Van Durme"], "venue": "Proc. IJCAI, 2011, pp. 1764\u20131769.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2011}, {"title": "ImageNet: A Large-scale Hierarchical Image Database", "author": ["J. Deng", "W. Dong", "R. Socher", "L.-J. Li", "K. Li", "L. Fei-Fei"], "venue": "Proc. IEEE CVPR, 2009.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2009}, {"title": "Multi-language Image Description with Neural Sequence Models", "author": ["D. Elliott", "S. Frank", "E. Hasler"], "venue": "arXiv preprint arXiv:1510.04709, 2015, pp. 1\u201314.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2015}, {"title": "Multi30K: Multilingual English-German Image Descriptions", "author": ["D. Elliott", "S. Frank", "K. Sima\u2019an", "L. Specia"], "venue": "arXiv preprint arXiv:1605.00459, 2016.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2016}, {"title": "Zeroresource Translation with Multi-lingual Neural Machine Translation", "author": ["O. Firat", "B. Sankaran", "Y. Al-Onaizan", "F.T.Y. Vural", "K. Cho"], "venue": "Proc. EMNLP, 2016.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2016}, {"title": "Devise: A Deep Visual-semantic Embedding Model", "author": ["A. Frome", "G. Corrado", "J. Shlens"], "venue": "Proc. NIPS, 2013, pp. 1\u201311.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2013}, {"title": "Image-Mediated Learning for Zero-shot Cross-lingual Document Retrieval", "author": ["R. Funaki", "H. Nakayama"], "venue": "Proc. EMNLP, 2015, pp. 585\u2013 590.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2015}, {"title": "The IAPR TC- 12 Benchmark: A New Evaluation Resource for Visual Information Systems", "author": ["M. Gr\u00fcbinger", "P. Clough", "H. M\u00fcller", "T. Deselaers"], "venue": "Proc. LREC, 2006, pp. 13\u201323.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2006}, {"title": "Canonical Correlation Analysis: An Overview with Application to Learning Methods", "author": ["D.R. Hardoon", "S. Szedmak", "J. Shawe-taylor"], "venue": "Neural Computation, vol. 16, no. 12, pp. 2639\u20132664, 2004.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2004}, {"title": "Multimodal Pivots for Image Caption Translation", "author": ["J. Hitschler", "S. Riezler"], "venue": "Proc. ACL, 2016.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2016}, {"title": "Long Short-term Memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural Computation, vol. 9, no. 8, pp. 1\u201332, 1997.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1997}, {"title": "Relations Between Two Sets of Variants", "author": ["H. Hotelling"], "venue": "Biometrika, vol. 28, pp. 321\u2013377, 1936.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1936}, {"title": "DenseCap: Fully Convolutional Localization Networks for Dense Captioning", "author": ["J. Johnson", "A. Karpathy", "L. Fei-Fei"], "venue": "Proc. IEEE CVPR, 2016.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2016}, {"title": "Visual Bilingual Lexicon Induction with Transferred ConvNet Features", "author": ["D. Kiela", "I. Vulic", "S. Clark"], "venue": "Proc. EMNLP, 2015, pp. 148\u2013 158.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2015}, {"title": "Adam: A Method for Stochastic Optimization", "author": ["D. Kingma", "J. Ba"], "venue": "Proc. ICLR, 2014, pp. 1\u201313.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2014}, {"title": "Unifying Visual-Semantic Embeddings with Multimodal Neural Language Models", "author": ["R. Kiros", "R. Salakhutdinov", "R.S. Zemel"], "venue": "Proc. NIPS, 2014, pp. 1\u201313.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2014}, {"title": "Europarl: A Parallel Corpus for Statistical Machine Translation", "author": ["P. Koehn"], "venue": "Proc. of Machine Translation Summit, vol. 11, 2005, pp. 79\u201386.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2005}, {"title": "ImageNet Classification with Deep Convolutional Neural Networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "Proc. NIPS, 2012.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2012}, {"title": "Orange: A Method for Evaluating Automatic Evaluation Metrics for Machine Translation", "author": ["C.-Y. Lin", "F.J. Och"], "venue": "Proc. COLING, 2004, pp. 501\u2013507.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2004}, {"title": "Multitask Sequence to Sequence Learning", "author": ["M.-t. Luong", "Q.V. Le", "I. Sutskever", "O. Vinyals", "L. Kaiser"], "venue": "Proc. ICLR, 2016, pp. 1\u201310.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2016}, {"title": "Issues in Cross-Language Retrieval from Document Image Collections", "author": ["D. Oard"], "venue": "Proc. of Symposium on Document Image Understanding Technology, 1999, pp. 229 \u2013 234.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 1999}, {"title": "BLEU : A Method for Automatic Evaluation of Machine Translation", "author": ["K. Papineni", "S. Roukos", "T. Ward", "W.-j. Zhu"], "venue": "Proc. ACL, 2002, pp. 311\u2013318.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2002}, {"title": "Bridge Correlational Neural Networks for Multilingual Multimodal Representation Learning", "author": ["J. Rajendran", "M.M. Khapra", "S. Chandar", "B. Ravindran"], "venue": "Proc. NAACL-HLT, 2016, pp. 171\u2013181.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2016}, {"title": "Automatic Parallel Fragment Extraction from Noisy Data", "author": ["J. Riesa", "D. Marcu"], "venue": "Proc. NAACL, 2012, pp. 538\u2013542.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2012}, {"title": "A Correlational Encoder Decoder Architecture for Pivot Based Sequence Generation", "author": ["A. Saha", "M.M. Khapra", "S. Chandar", "J. Rajendran", "K. Cho"], "venue": "Proc. COLING, 2016.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2016}, {"title": "Minimum Risk Training for Neural Machine Translation", "author": ["S. Shen", "Y. Cheng", "Z. He", "W. He", "H. Wu", "M. Sun", "Y. Liu"], "venue": "Proc. ACL, 2016, pp. 1\u20139.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2016}, {"title": "Learning Grounded Meaning Representations with Autoencoders", "author": ["C. Silberer", "M. Lapata"], "venue": "Proc. ACL, 2014, pp. 721\u2013732.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2014}, {"title": "Very Deep Convolutional Networks for Large-Scale Image Recoginition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "Proc. ICLR, 2015.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2015}, {"title": "Sequence to Sequence Learning with Neural Networks", "author": ["I. Sutskever", "O. Vinyals", "Q.V. Le"], "venue": "Proc. NIPS, 2014, pp. 3104\u20133112.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2014}, {"title": "The Sentence-Aligned European Patent Corpus", "author": ["W. Taeger"], "venue": "Proc. EAMT, 2011, pp. 177\u2013184.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2011}, {"title": "Improving the Multilingual User Experience of Wikipedia Using Cross-Language Name Search", "author": ["R. Udupa", "M.M. Khapra"], "venue": "Proc. NAACL, 2010, pp. 492\u2013500.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2010}, {"title": "Large Scale Parallel Document Mining for Machine Translation", "author": ["J. Uszkoreit", "J. Ponte", "A.C. Popat", "M. Dubiner"], "venue": "Proc. COLING, 2010, pp. 1101\u20131109.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2010}, {"title": "Show and Tell : A Neural Image Caption Generator", "author": ["O. Vinyals", "A. Toshev", "S. Bengio", "D. Erhan"], "venue": "Proc. IEEE CVPR, 2015.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2015}, {"title": "Multi-Modal Representations for Improved Bilingual Lexicon Learning", "author": ["I. Vuli", "D. Kiela", "S. Clark", "M.-F. Moens"], "venue": "Proc. ACL, 2016, pp. 188\u2013194.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2016}, {"title": "Pivot Language Approach for Phrase-based Statistical Machine Translation", "author": ["H. Wu", "H. Wang"], "venue": "Machine Translation, vol. 21, no. 3, pp. 165\u2013181, 2007.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2007}, {"title": "Revisiting Pivot Language Approach for Machine Translation", "author": ["\u2014\u2014"], "venue": "Proc. IJCNLP-ACL, vol. 1, no. August, 2009, p. 154.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2009}, {"title": "From Image Descriptions to Visual Denotations: New Similarity Metrics for Semantic Inference over Event Descriptions", "author": ["P. Young", "A. Lai", "M. Hodosh", "J. Hockenmaier"], "venue": "Transactions of the Association for Computational Linguistics (TACL), vol. 2, no. April, pp. 67\u201378, 2014.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 28, "context": "Irrespective of traditional statistical machine translation (SMT) [18] or modern neural machine translation (NMT) [30], methods and data have always been mutually indispensable to each other.", "startOffset": 114, "endOffset": 118}, {"referenceID": 25, "context": "Recent state-of-the-art NMT systems have shown that translation can be surprisingly improved with sufficiently large-scale data and high computational power [27].", "startOffset": 157, "endOffset": 161}, {"referenceID": 16, "context": "In some specific domains such as Web news, patents, and Wikipedia, relatively high-quality multilingual translations are made available by content holders or volunteer workers, which have been utilized by researchers for decades [17], [31].", "startOffset": 229, "endOffset": 233}, {"referenceID": 29, "context": "In some specific domains such as Web news, patents, and Wikipedia, relatively high-quality multilingual translations are made available by content holders or volunteer workers, which have been utilized by researchers for decades [17], [31].", "startOffset": 235, "endOffset": 239}, {"referenceID": 17, "context": "Moreover, in recent years, performance of visual recognition has been dramatically improved owing to the huge success of deep learning, where it is now considered to be on a human level for generic image recognition [19].", "startOffset": 216, "endOffset": 220}, {"referenceID": 31, "context": "Some methods exploited a bootstrap approach starting from base translation systems [33], whereas others utilized external meta information such as links to the same URL to coupling bilingual texts [25].", "startOffset": 83, "endOffset": 87}, {"referenceID": 23, "context": "Some methods exploited a bootstrap approach starting from base translation systems [33], whereas others utilized external meta information such as links to the same URL to coupling bilingual texts [25].", "startOffset": 197, "endOffset": 201}, {"referenceID": 20, "context": "However, these methods simply rely on OCR reading or near-duplicate (copy) detection of images [22], and thus they cannot identify similarities in semantics, which is a fundamental limitation as compared to our work.", "startOffset": 95, "endOffset": 99}, {"referenceID": 34, "context": "Most recent works have focused on existing popular language to use as the pivot [36], [37], [5].", "startOffset": 80, "endOffset": 84}, {"referenceID": 35, "context": "Most recent works have focused on existing popular language to use as the pivot [36], [37], [5].", "startOffset": 86, "endOffset": 90}, {"referenceID": 4, "context": "Most recent works have focused on existing popular language to use as the pivot [36], [37], [5].", "startOffset": 92, "endOffset": 95}, {"referenceID": 0, "context": "Therefore, image-based pivots have mainly been used in relatively easier tasks such as bilingual lexicon learning, where image similarity is used as the criteria to estimate relevance between tag words attached to images [1], [14], [35].", "startOffset": 221, "endOffset": 224}, {"referenceID": 13, "context": "Therefore, image-based pivots have mainly been used in relatively easier tasks such as bilingual lexicon learning, where image similarity is used as the criteria to estimate relevance between tag words attached to images [1], [14], [35].", "startOffset": 226, "endOffset": 230}, {"referenceID": 33, "context": "Therefore, image-based pivots have mainly been used in relatively easier tasks such as bilingual lexicon learning, where image similarity is used as the criteria to estimate relevance between tag words attached to images [1], [14], [35].", "startOffset": 232, "endOffset": 236}, {"referenceID": 26, "context": "Grounding a natural language to real-world representations has always been an important topic in NLP, for which computer vision would be the first natural choice [28].", "startOffset": 162, "endOffset": 166}, {"referenceID": 17, "context": "After a huge breakthrough in the use of convolutional neural networks (CNNs) [19], visual recognition has been significantly advanced in terms of both accuracy and flexibility, enabling the development of many brand-new technologies.", "startOffset": 77, "endOffset": 81}, {"referenceID": 32, "context": "Amongst them, image captioning, which automatically annotates a description for an input image with natural language, has become one of the hottest topics in recent years [34], [13].", "startOffset": 171, "endOffset": 175}, {"referenceID": 12, "context": "Amongst them, image captioning, which automatically annotates a description for an input image with natural language, has become one of the hottest topics in recent years [34], [13].", "startOffset": 177, "endOffset": 181}, {"referenceID": 3, "context": "Recently, a new research field called multimodal machine translation was proposed [4], [10], which became a subtask in WMT 2016.", "startOffset": 82, "endOffset": 85}, {"referenceID": 9, "context": "Recently, a new research field called multimodal machine translation was proposed [4], [10], which became a subtask in WMT 2016.", "startOffset": 87, "endOffset": 91}, {"referenceID": 2, "context": "The feasibility of this approach has been demonstrated by some methods, such as the neural machine translation model with image input [3] and visualbased reranking of SMT results [10].", "startOffset": 134, "endOffset": 137}, {"referenceID": 9, "context": "The feasibility of this approach has been demonstrated by some methods, such as the neural machine translation model with image input [3] and visualbased reranking of SMT results [10].", "startOffset": 179, "endOffset": 183}, {"referenceID": 11, "context": "The most classical and standard method for multimodal learning is probably linear canonical correlation analysis (CCA) [12], which has been successfully used in image-language collaborations such as semantic image retrieval and annotation [9], as well as crosslingual information retrieval [32][7].", "startOffset": 119, "endOffset": 123}, {"referenceID": 8, "context": "The most classical and standard method for multimodal learning is probably linear canonical correlation analysis (CCA) [12], which has been successfully used in image-language collaborations such as semantic image retrieval and annotation [9], as well as crosslingual information retrieval [32][7].", "startOffset": 239, "endOffset": 242}, {"referenceID": 30, "context": "The most classical and standard method for multimodal learning is probably linear canonical correlation analysis (CCA) [12], which has been successfully used in image-language collaborations such as semantic image retrieval and annotation [9], as well as crosslingual information retrieval [32][7].", "startOffset": 290, "endOffset": 294}, {"referenceID": 6, "context": "The most classical and standard method for multimodal learning is probably linear canonical correlation analysis (CCA) [12], which has been successfully used in image-language collaborations such as semantic image retrieval and annotation [9], as well as crosslingual information retrieval [32][7].", "startOffset": 294, "endOffset": 297}, {"referenceID": 5, "context": "In more recent methods based on deep neural networks, pairwise ranking loss has been shown to significantly improve multimodal embedding [6] owing to its natural capability of learning discriminative nearest-neighbor metrics and stability in gradient-based learning.", "startOffset": 137, "endOffset": 140}, {"referenceID": 15, "context": "It was successfully used for image captioning within the framework of the deep encoder-decoder model [16].", "startOffset": 101, "endOffset": 105}, {"referenceID": 5, "context": "We use the pair-wise rank loss proposed in [6] for training encoders to map them to one common multimodal space.", "startOffset": 43, "endOffset": 46}, {"referenceID": 15, "context": "Our two-way model is closely related to the image-captioning model proposed by [16] except that we apply different languages to the encoder and decoder parts.", "startOffset": 79, "endOffset": 83}, {"referenceID": 19, "context": "The topology of our network is similar to recently proposed many-to-one sequence-to-sequence model [21].", "startOffset": 99, "endOffset": 103}, {"referenceID": 4, "context": "[5] incorporated some synthetic parallel corpora to explicitly include source-to-target path during training.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[7] and Rajendran et al.", "startOffset": 0, "endOffset": 3}, {"referenceID": 22, "context": "[24] used basically the same idea as our multimodal space, implemented with generalized CCA and neural encoders respectively.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "[26] proposed an endto-end model of multimodal embedding and target decoder, which is almost identical to our two-way model except that their multimodal fusion is based on correlation loss.", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "The IAPR-TC12 dataset [8] has 20,000 images with their English and German descriptions.", "startOffset": 22, "endOffset": 25}, {"referenceID": 3, "context": "The recently published Multi30K dataset [4] is specifically designed for research of multimodal machine translation.", "startOffset": 40, "endOffset": 43}, {"referenceID": 36, "context": "This is an extension of Flickr30K [38], an imagecaption dataset in English, for which German translations are provided by [4].", "startOffset": 34, "endOffset": 38}, {"referenceID": 3, "context": "This is an extension of Flickr30K [38], an imagecaption dataset in English, for which German translations are provided by [4].", "startOffset": 122, "endOffset": 125}, {"referenceID": 27, "context": "For visual encoder E , we employed the public VGG-16 network [29], which is one of the most powerful and widely used CNNs pre-trained on the ImageNet dataset [2].", "startOffset": 61, "endOffset": 65}, {"referenceID": 1, "context": "For visual encoder E , we employed the public VGG-16 network [29], which is one of the most powerful and widely used CNNs pre-trained on the ImageNet dataset [2].", "startOffset": 158, "endOffset": 161}, {"referenceID": 10, "context": "For language encoders and decoders E, E, and D, we used recurrent neural networks (RNNs) with long short-term memory (LSTM) [11].", "startOffset": 124, "endOffset": 128}, {"referenceID": 14, "context": "We used the Adam optimizer [15] with minibatch size 32 for training the network, and we stopped optimization when the validation loss no longer improved.", "startOffset": 27, "endOffset": 31}, {"referenceID": 21, "context": "For evaluation, we used the standard BLEU metrics [23].", "startOffset": 50, "endOffset": 54}, {"referenceID": 18, "context": "Therefore, in this study, we mainly focused on the BLEU+1 metrics [20], which is a modification of BLEU that has smoothing terms for higher-order n-grams, making it possible to evaluate MT performance on short sentences.", "startOffset": 66, "endOffset": 70}, {"referenceID": 28, "context": "To demonstrate the performance we could obtain with a supervised parallel corpus, we show the scores on sequence-tosequence NMT [30] trained on the same RNN architectures changing the number of randomly sampled parallel data.", "startOffset": 128, "endOffset": 132}], "year": 2017, "abstractText": "We propose an approach to build a neural machine translation system with no supervised resources (i.e., no parallel corpora) using multimodal embedded representation over texts and images. Based on the assumption that text documents are often likely to be described with other multimedia information (e.g., images) somewhat related to the content, we try to indirectly estimate the relevance between two languages. Using multimedia as the \u201dpivot\u201d, we project all modalities into one common hidden space where samples belonging to similar semantic concepts should come close to each other, whatever the observed space of each sample is. This modality-agnostic representation is the key to bridging the gap between different modalities. Putting a decoder on top of it, our network can flexibly draw the outputs from any input modality. Notably, in the testing phase, we need only source language texts as the input for translation. In experiments, we tested our method on two benchmarks to show that it can achieve reasonable translation performance. We compared and investigated several possible implementations and found that an end-to-end model that simultaneously optimized both rank loss in multimodal encoders and cross-entropy loss in decoders performed the best.", "creator": "LaTeX with hyperref package"}}}