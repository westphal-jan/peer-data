{"id": "1611.04578", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Nov-2016", "title": "Earliness-Aware Deep Convolutional Networks for Early Time Series Classification", "abstract": "We present Earliness-Aware Deep Convolutional Networks (EA-ConvNets), an end-to-end deep learning framework, for early classification of time series data. Unlike most existing methods for early classification of time series data, that are designed to solve this problem under the assumption of the availability of a good set of pre-defined (often hand-crafted) features, our framework can jointly perform feature learning (by learning a deep hierarchy of \\emph{shapelets} capturing the salient characteristics in each time series), along with a dynamic truncation model to help our deep feature learning architecture focus on the early parts of each time series. Consequently, our framework is able to make highly reliable early predictions, outperforming various state-of-the-art methods for early time series classification, while also being competitive when compared to the state-of-the-art time series classification algorithms that work with \\emph{fully observed} time series data. To the best of our knowledge, the proposed framework is the first to perform data-driven (deep) feature learning in the context of early classification of time series data. We perform a comprehensive set of experiments, on several benchmark data sets, which demonstrate that our method yields significantly better predictions than various state-of-the-art methods designed for early time series classification. In addition to obtaining high accuracies, our experiments also show that the learned deep shapelets based features are also highly interpretable and can help gain better understanding of the underlying characteristics of time series data.", "histories": [["v1", "Mon, 14 Nov 2016 20:55:33 GMT  (924kb,D)", "http://arxiv.org/abs/1611.04578v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["wenlin wang", "changyou chen", "wenqi wang", "piyush rai", "lawrence carin"], "accepted": false, "id": "1611.04578"}, "pdf": {"name": "1611.04578.pdf", "metadata": {"source": "CRF", "title": "Earliness-Aware Deep Convolutional Networks for Early Time Series Classification", "authors": ["Wenlin Wang", "Changyou Chen", "Wenqi Wang", "Piyush Rai", "Lawrence Carin"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "In fact, most of them are able to move into a different world, in which they are able to move, in which they are able to live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live."}, {"heading": "2 Convolutional Neural Nets for Time-Series", "text": "Our framework is based on an architecture of the Convolutionary Neural Network (CNN) for time series data. We first give a brief overview of a basic CNN for time series data [37] before describing the details of our proposed framework.Let us consider a sequence that represents a univariate time series d = [d1, d2,..., dL], where L is the length of the sequence. A classic CNN works for such time series data by composing alternating layers of folding and pooling operations. The revolutionary layer extracts patterns within local regions throughout the input sequence. This process is performed by using a filter, g = [g1, g2,..., gm] that runs length over the sequence and calculates the inner product of the filter at each location in the sequence. Let us designate the output of this filter as oling for each layer."}, {"heading": "3 Earliness-Aware ConvNets", "text": "In this section, we formally define the problem of early time series classification (ETSC) and describe our framework, Earliness-Aware Deep Convolutional Networks (EA-ConvNets), in detail."}, {"heading": "3.1 Problem Definition", "text": "We will designate a univariate time series (a single sequence) as T = {t1, t2,..., tL}, where ti is a scalar measurement series of this time series. Following this notation, we will designate a collection of time series as D = {(Ti, yi)} Ni = 1, where N is the number of time series in the dataset. Ti = {t1i, t2i,..., tLi} stands for the ith time series and yi is the associated designation. The element tji indicates the jth time series value in the ith time series. To simplify the representation, we assume that all time series in D have identical time series, although our framework does not specify this. The task in the Time Series Classification (TSC) is to construct a classification to foresee the class designation of a new time series."}, {"heading": "3.2 Earliness-Aware ConvNets", "text": "We present a comprehensive learning framework for early time series classification by building a multi-scale Convolutionary Architecture of Neural Networks (CNN) coupled with a time series shortening model that allows us to focus on early parts of each time series. CNN enables the extraction of traits on multiple scales from the time series data. In the context of time series data, the extracted traits correspond to shapelets [38], which are highly discriminatory subsequences. Unlike most other existing methods that extract shapelets from time series data using handmade rules (which are often ad hoc, lengthy and time consuming), one distinguishing aspect of our framework is that the shapelets are learned in a purely data-driven manner. Although some newer methods have explored ways of learning shapelets that are optimized for a given dataset [15], this approach consists less of expressive representation than our fully learned one."}, {"heading": "3.2.1 Early-Awareness", "text": "The early awareness component allows EA-ConvNets to focus on the early stage of each time series. To achieve earliness awareness, we take inspiration from the idea of nested suspensions [27], which was originally proposed to stochastically drop contiguous nested sets of hidden units in a neural network by extracting a decay point from a decay distribution. We use a similar strategy to stochastically select for each time series an intersection point that leads to a new truncated version of the original time series (with the same name). To select the intersection point, a previous distribution point P (\u00b7) is used on the timestamps (length) of the time series Ti. For each time series Ti, we first stamp an index s-P (\u00b7) to generate the stochastically truncated time series as Ti truncation."}, {"heading": "3.2.2 Deep Convolutional Feature (Shapelet) Extractor", "text": "The in-depth extraction of our frame is based on a deep Convolutionary Neural Network Architecture, which takes as input the maximum amount of time obtained from the Earliness-Aware stage, extracts a hierarchy of Shapelet-based features for each period and passes them on to the final classification. To capture this, we apply independent 1D conglomerates over the (shortened) period. Since the data of the period can have latte features on multiple scales, a single filter cannot be able to capture them, we need to apply three channels of the ConvNets simultaneously. Each channel of the ConvNets has a different filter size fixed within each ConvNets. In our experiments, we found the selection of filter sizes performs {3%, 5%, 10%} of the L."}, {"heading": "3.2.3 The Final Classifier", "text": "For the final classifier, we link all shapelet-based features from the outputs of the three ConvNets = forward-facing line (one for each filter) and enter them into the final output layer, followed by a softmax transformation. The output of EA-ConvNets is the predictive distribution of any possible designation for the input time series. The objective function is to minimize the probability of transverse entropy loss, which can be easily calculated by forwarding the input line Ti (line 6 in algorithm 1), where W represents all model parameters (e.g. the weights of the deep network), pyi (Ti) is to minimize the probability of transverse entropy loss, which can be easily calculated by forwarding the input line Ti (line 6 in algorithm 1). Since all operations in the objective function are differentiable, all model parameters can be propagated collectively by reversing the algorithm 1."}, {"heading": "4 Related Work", "text": "In fact, most people who have lived in the US in recent years are unable to understand the world and what it is about, and they are people who live in the US, people who grew up in the US, people who grew up in the US, people who grew up in the US, people who grew up in the US, people who grew up in the US, people who grew up in the US, people who have gone around the world."}, {"heading": "5 Experiments", "text": "We evaluate the effectiveness of EA-ConvNets using 12 publicly available benchmark datasets. Below we give a brief description of the datasets and experimental settings and then conduct the following experiments. \u2022 Evaluate our model for the task of early time series classification by comparing it with several state-of-the-art methods for this problem. \u2022 Comparison of EA-ConvNets with state-of-the-art algorithms for fully observed time series data and show that our model also competes with these algorithms in terms of classification accuracy. \u2022 Visualize the learned characteristics (shapelets) in each of the layers of the deep architecture. \u2022 Evaluate the sensitivity of EA-ConvNets to hyperparameters. \u2022 Comparison of EA-ConvNets with the setting of multiple ConvNets, where each ConvNet is trained only on the data with a certain shortened level."}, {"heading": "5.1 Datasets", "text": "We select 12 benchmark data sets from the UCR time series archive [5], with time series varying in length. Our data sets include: ADIAC, FISH, GUN POINT, ITALYPOWERDEMAND, SYNTHETIC CONTROL, TRACE, CRICKET-X, CRICKET-Y, CRICKET-Z, TWO PATTER, NON-INVASIVE FETAL ECG THORAX1 (NONINVTHORAX1) and NON-INVASIVE FETAL ECG THORAX2 (NONINVTHORAX2). Some statistics of the data sets are listed in Table 1."}, {"heading": "5.2 Experimental Setting", "text": "For consistency, we perform all experiments on all datasets using the same architecture as shown in Figure 1. The number of filters is set to {48, 48, 96} for each layer in each channel of the network and is the same across the three channels of the network. As a result, the final representation of the features has a size of 96 x 3 = 288. ReLU is taken over as a nonlinear activation function after each hidden layer. EA-ConvNets is implemented with Torch7 [6] and trained on NVIDIA GTA TITAN graphics cards with 6 GB RAM. We optimize the model with RMSprop [30] using minibatches of size 50. We use 5-fold cross-validation for hyperparameter tuning. Specifically, the EA-ConvNets hyperparameters include the value of the geometric distribution series set using hyperparameter C."}, {"heading": "5.3 Baselines", "text": "We conduct two types of experiments: (1) Early Time Series Classification (ETSC) and (2) Fully Observed Time Series Classification (TSC). \u2022 The former compare EA-ConvNets with the following state-of-the-art baseline: \u2022 Early Classification of Time Series (ECTS) [35] \u2022 Early Differential Form Classification (EDSC) [36] \u2022 Reliable Early Classification Method (RelClass) [26] \u2022 Reliable Early Classification System for Time Series Based on Class Discrimination and Prediction Reliability (ECDIRE) [25] \u2022 Deep Convolutional Neural Network Classifier using the same network architecture as EA-ConvNets with Full Observation as a Training Set (EA-ConvNets-Full). Note that our baseline also includes methods that can learn shapelets from data (EDSC), just as EA-ConvNets [ConvNets] can learn the deep shaping of time series."}, {"heading": "5.4 Early Time Series Classification", "text": "In this section, we compare the earthquake-prone areas with the earthquake-prone areas where global warming is particularly high, with the earthquake-prone areas where global warming is so high that global warming is unprecedented."}, {"heading": "5.5 Comparison with (Full) Time Series Classification Methods", "text": "We also compare EA-ConvNets with several state-of-the-art baselines for the classification of full-time series (TSC), the results of which are shown in Table 2. None of the previous work has compared ETSC classifiers with TSC classifiers under complete observations, as ETSC charges accuracies for early classification with a full classification. To our knowledge, we are the first to conduct such experiments to investigate the extent of the gap between ETSC and TSC algorithms. As shown in Table 2, EA-ConvNets with full observations exceed most TSC baselines across multiple datasets and are largely competitive with state-of-the-art models. EA-ConvNets does not exceed the state of-the-art in some datasets, but the gaps are marginal."}, {"heading": "5.6 Qualitative Results: Deep Shapelet Discovery", "text": "An appealing aspect of our framework is its ability to automatically learn from data a deep representation of the time series data in the form of a hierarchy of formulas (highly discriminatory sub-sequences). To demonstrate the interpretative capability of the formulas learned by EA-ConvNets, we can understand / visualize the formulas learned at any stage of the underlying time series and can be useful in other decision-making tasks (e.g. in medical diagnostics).We visualize the learned filters in all three channels of the network. To visualize the characteristic maps of a particular time series at each stage, we record only those in the first channel to facilitate the ease of visualization. Characteristic maps in the other two channels show similar patterns to the first channel, and are now shown here."}, {"heading": "5.7 Sensitivity to hyperparameters", "text": "EA-ConvNets contains a number of hyperparameters. Among them, the pooling factor, spatial dropout rate and L2 weight breakdown influence the convergence of the training algorithm. The hyperparameter \u03c1, in the geometric distribution for data multiplication, not only influences the convergence of the training algorithm, but also controls the degree of seriousness of our model. Therefore, in this section we mainly analyze the sensitivity of our model to \u03c1. We use FISH, SYNTHETIC CONTROL, CRICKET-X and NONINVTHORAX1 datasets to show the effect of this hyperparameter. EA-ConvNets results on these datasets for different values of \u03c1 are shown in Fig 4. As we can see, the effect of the IKEA variations is different across different datasets. On FISH, the performance is sensitive to IKEA-ConvNets."}, {"heading": "5.8 Comparison with Ensemble of Multiple Models", "text": "To further evaluate the reliability of EA-ConvNets, we compare them with a combination of several models, each of which has been trained by shortening the original time series to a fixed length. Specifically, we compare EAConvNets with a group of deep learning models that share the same architecture as EA-ConvNets but have a fixed trunk length (Fixed EA-ConvNets) and several fixed trunk length classifiers (Fixed 1-NN). For each trunk length, we train independent models and use them at test time to predict the designation of a test time series with the same trunk length. We conduct experiments on four sets of data: FISH, CRICKET-X, TWO-PATTERNS and NONINVTHORAX1. Figure 5 shows the results of the NEA series in very different ways."}, {"heading": "6 Conclusion", "text": "EA-ConvNets uses the information on different scales and captures the interpretable characteristics (shapelets) at a very early stage, which also makes it a robust model if each time series is by nature short-lived. Our experiments suggest that EA-ConvNets delivers lower or comparable test errors at a pre-defined observed length budget at test time, while maintaining comparable classification accuracy with the state-of-the-art time series classification models that use full-time series. To interpret the effectiveness of EA-ConvNets, we present the learned local characteristics (shapelets) and visualize the nonlinear characteristic representations in each layer of the deep model. Our experiments clearly show that the learned characteristic representation is highly discriminatory and can achieve effective early classification. There are a number of interesting future directions in which our general time frame model can only be used for other time structures if we are aware of these time structures."}], "references": [{"title": "Time-sensitive classification of behavioral data", "author": ["Shin Ando", "Einoshin Suzuki"], "venue": "In SDM,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2013}, {"title": "Minimizing response time in time series classification", "author": ["Shin Ando", "Einoshin Suzuki"], "venue": "Knowledge and Information Systems,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2016}, {"title": "Time-series classification with cote: the collective of transformation-based ensembles", "author": ["Anthony Bagnall", "Jason Lines", "Jon Hills", "Aaron Bostrom"], "venue": "Knowledge and Data Engineering, IEEE Transactions on,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2015}, {"title": "Using dynamic time warping to find patterns in time series", "author": ["Donald J Berndt", "James Clifford"], "venue": "In KDD workshop,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1994}, {"title": "The ucr time series classification", "author": ["Yanping Chen", "Eamonn Keogh", "Bing Hu", "Nurjahan Begum", "Anthony Bagnall", "Abdullah Mueen", "Gustavo Batista"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2015}, {"title": "Torch7: A matlab-like environment for machine learning", "author": ["Ronan Collobert", "Koray Kavukcuoglu", "Cl\u00e9ment Farabet"], "venue": "In BigLearn, NIPS Workshop, number EPFL-CONF-192376,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2011}, {"title": "Multi-scale convolutional neural networks for time series classification", "author": ["Zhicheng Cui", "Wenlin Chen", "Yixin Chen"], "venue": "arXiv preprint arXiv:1603.06995,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2016}, {"title": "Early classification of time series as a non myopic sequential decision making problem", "author": ["Asma Dachraoui", "Alexis Bondu", "Antoine Cornu\u00e9jols"], "venue": "In Machine Learning and Knowledge Discovery in Databases,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2015}, {"title": "Early classification of network traffic through multi-classification", "author": ["Alberto Dainotti", "Antonio Pescap\u00e9", "Carlo Sansone"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2011}, {"title": "Boosting interval-based literals: Variable length and early classification. Data Mining in Time Series Databases, page", "author": ["Grupo de Sistemas Inteligentes"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2004}, {"title": "Fast subsequence matching in time-series databases, volume 23", "author": ["Christos Faloutsos", "Mudumbai Ranganathan", "Yannis Manolopoulos"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1994}, {"title": "Extraction of interpretable multivariate patterns for early diagnostics", "author": ["Mohamed F Ghalwash", "Vladan Radosavljevic", "Zoran Obradovic"], "venue": "In Data Mining (ICDM),", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2013}, {"title": "Utilizing temporal patterns for estimating uncertainty in interpretable early decision making", "author": ["Mohamed F Ghalwash", "Vladan Radosavljevic", "Zoran Obradovic"], "venue": "In Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2014}, {"title": "Early classification of multivariate time series using a hybrid hmm/svm model", "author": ["Mohamed F Ghalwash", "Du\u0161an Ramljak", "Zoran Obradovi\u0107"], "venue": "In Bioinformatics and Biomedicine (BIBM),", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2012}, {"title": "Learning time-series shapelets", "author": ["Josif Grabocka", "Nicolas Schilling", "Martin Wistuba", "Lars Schmidt-Thieme"], "venue": "In Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2014}, {"title": "Early classification on multivariate time series", "author": ["Guoliang He", "Yong Duan", "Rong Peng", "Xiaoyuan Jing", "Tieyun Qian", "Lingling Wang"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2015}, {"title": "Imagenet classification with deep convolutional neural networks. In Advances in neural information processing", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E Hinton"], "venue": null, "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2012}, {"title": "Efficient learning of timeseries shapelets", "author": ["Lu Hou James T Kwok", "Jacek M Zurada"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2016}, {"title": "Early classification of ongoing observation", "author": ["Kang Li", "Sheng Li", "Yun Fu"], "venue": "In Data Mining (ICDM),", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2014}, {"title": "Reliable early classification on multivariate time series with numerical and categorical attributes", "author": ["Yu-Feng Lin", "Hsuan-Hsu Chen", "Vincent S Tseng", "Jian Pei"], "venue": "In Advances in Knowledge Discovery and Data Mining,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2015}, {"title": "Time series classification with ensembles of elastic distance measures", "author": ["Jason Lines", "Anthony Bagnall"], "venue": "Data Mining and Knowledge Discovery,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2015}, {"title": "An integrated data mining approach to real-time clinical monitoring and deterioration warning", "author": ["Yi Mao", "Wenlin Chen", "Yixin Chen", "Chenyang Lu", "Marin Kollef", "Thomas Bailey"], "venue": "In Proceedings of the 18th ACM SIGKDD international conference on Knowledge discovery and data mining,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2012}, {"title": "Identifying predictive multi-dimensional time series motifs: an application to severe weather prediction", "author": ["Amy McGovern", "Derek H Rosendahl", "Rodger A Brown", "Kelvin K Droegemeier"], "venue": "Data Mining and Knowledge Discovery,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2011}, {"title": "Early recognition and prediction of gestures", "author": ["Akihiro Mori", "Seiichi Uchida", "Ryo Kurazume", "Rin-ichiro Taniguchi", "Tsutomu Hasegawa", "Hiroaki Sakoe"], "venue": "In Pattern Recognition,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2006}, {"title": "Reliable early classification of time series based on discriminating the classes over time", "author": ["Usue Mori", "Alexander Mendiburu", "Eamonn Keogh", "Jose A Lozano"], "venue": "Data Mining and Knowledge Discovery,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2016}, {"title": "Classifying with confidence from incomplete information", "author": ["Nathan Parrish", "Hyrum S Anderson", "Maya R Gupta", "Dun Yu Hsiao"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2013}, {"title": "Learning ordered representations with nested dropout", "author": ["Oren Rippel", "Michael A Gelbart", "Ryan P Adams"], "venue": "In ICML,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2014}, {"title": "The boss is concerned with time series classification in the presence of noise", "author": ["Patrick Sch\u00e4fer"], "venue": "Data Mining and Knowledge Discovery,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2015}, {"title": "Learning dtwshapelets for time-series classification", "author": ["Mit Shah", "Josif Grabocka", "Nicolas Schilling", "Martin Wistuba", "Lars Schmidt-Thieme"], "venue": "In Proceedings of the 3rd IKDD Conference on Data Science,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2016}, {"title": "Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude", "author": ["T. Tieleman", "G. E Hinton"], "venue": "Technical report, Coursera: Neural Networks for Machine Learning,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2012}, {"title": "Efficient object localization using convolutional networks", "author": ["Jonathan Tompson", "Ross Goroshin", "Arjun Jain", "Yann LeCun", "Christoph Bregler"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2015}, {"title": "Early recognition of sequential patterns by classifier combination", "author": ["Seiichi Uchida", "Kazuma Amamoto"], "venue": "In ICPR,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2008}, {"title": "Lstm-based early recognition of motion patterns", "author": ["Markus Weber", "Marcus Liwicki", "Didier Stricker", "Christopher Scholzel", "Seiichi Uchida"], "venue": "In ICPR. IEEE,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2014}, {"title": "A brief survey on sequence classification", "author": ["Zhengzheng Xing", "Jian Pei", "Eamonn Keogh"], "venue": "ACM SIGKDD Explorations Newsletter,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2010}, {"title": "Early classification on time series", "author": ["Zhengzheng Xing", "Jian Pei", "S Yu Philip"], "venue": "Knowledge and information systems,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2012}, {"title": "Extracting interpretable features for early classification on time series", "author": ["Zhengzheng Xing", "Jian Pei", "S Yu Philip", "Ke Wang"], "venue": "In SDM,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2011}, {"title": "Deep convolutional neural networks on multichannel time series for human activity recognition", "author": ["Jian Bo Yang", "Minh Nhut Nguyen", "Phyo Phyo San", "Xiao Li Li", "Shonali Krishnaswamy"], "venue": "In Proceedings of the 24th International Joint Conference on Artificial Intelligence (IJCAI),", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2015}, {"title": "Time series shapelets: a new primitive for data mining", "author": ["Lexiang Ye", "Eamonn Keogh"], "venue": "In Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2009}, {"title": "Character-level convolutional networks for text classification", "author": ["Xiang Zhang", "Junbo Zhao", "Yann LeCun"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2015}, {"title": "Time series classification using multichannels deep convolutional neural networks", "author": ["Yi Zheng", "Qi Liu", "Enhong Chen", "Yong Ge", "J Leon Zhao"], "venue": "In Web-Age Information Management,", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2014}], "referenceMentions": [{"referenceID": 11, "context": "Examples include medical informatics and clinical prediction, weather reporting, financial markets, network traffic monitoring, etc [12, 34, 9, 22, 14, 23].", "startOffset": 132, "endOffset": 155}, {"referenceID": 33, "context": "Examples include medical informatics and clinical prediction, weather reporting, financial markets, network traffic monitoring, etc [12, 34, 9, 22, 14, 23].", "startOffset": 132, "endOffset": 155}, {"referenceID": 8, "context": "Examples include medical informatics and clinical prediction, weather reporting, financial markets, network traffic monitoring, etc [12, 34, 9, 22, 14, 23].", "startOffset": 132, "endOffset": 155}, {"referenceID": 21, "context": "Examples include medical informatics and clinical prediction, weather reporting, financial markets, network traffic monitoring, etc [12, 34, 9, 22, 14, 23].", "startOffset": 132, "endOffset": 155}, {"referenceID": 13, "context": "Examples include medical informatics and clinical prediction, weather reporting, financial markets, network traffic monitoring, etc [12, 34, 9, 22, 14, 23].", "startOffset": 132, "endOffset": 155}, {"referenceID": 22, "context": "Examples include medical informatics and clinical prediction, weather reporting, financial markets, network traffic monitoring, etc [12, 34, 9, 22, 14, 23].", "startOffset": 132, "endOffset": 155}, {"referenceID": 37, "context": "Unfortunately, most existing methods for this problem are usually based on ad-hoc ways of defining subsequences or hand-crafted patterns [38], mapping them to fixed sized vectors, and subsequently applying a classification method (e.", "startOffset": 137, "endOffset": 141}, {"referenceID": 37, "context": "Shapelets [38] based representations have recently emerged as an effective way to model time series data in problems such as time series classification.", "startOffset": 10, "endOffset": 14}, {"referenceID": 14, "context": "Some recent work has tried learning shapelets from data [15] but the learned shapelets are usually not ideal for early time series classification.", "startOffset": 56, "endOffset": 60}, {"referenceID": 35, "context": "As compared to the stateof-the-art methods for that can learn interpretable features for ETSC problems [36], the deep shapelet based representations learned using our framework, in a purely data-driven manner, can capture patterns in the time series at multiple levels of granularities.", "startOffset": 103, "endOffset": 107}, {"referenceID": 36, "context": "We first provide a brief review of a basic CNN for time series data [37], before describing the details of our proposed framework.", "startOffset": 68, "endOffset": 72}, {"referenceID": 37, "context": "In the context of time series data, the extracted features correspond to shapelets [38] which are subsequences with high discriminative power.", "startOffset": 83, "endOffset": 87}, {"referenceID": 14, "context": "Although some recent methods have looked at ways of learning shapelets that are optimized for a given data set [15], unlike our approach which learns a deep representation of the shapelets, these methods can only learn a shallow representation, which consequently are less expressive and have less discriminative power.", "startOffset": 111, "endOffset": 115}, {"referenceID": 26, "context": "To accomplish Earliness-Aware, we draw inspiration from the idea of nested dropout [27], which was originally proposed to stochastically drop coherent nested sets of hidden units in a neural network by drawing a decay point from some decay distribution.", "startOffset": 83, "endOffset": 87}, {"referenceID": 26, "context": "For choosing the truncation point, as also done in [27], we use a geometric distribution because of its properties of exponential decay and memorylessness.", "startOffset": 51, "endOffset": 55}, {"referenceID": 38, "context": "This is essentially the max-over-time method described in [39], with the ith element of F (T) defined as", "startOffset": 58, "endOffset": 62}, {"referenceID": 30, "context": "To maintain the independence of each filter and avoid overfitting, we employ SpatialDropout\u2020 (originally proposed in [31]) on F (T).", "startOffset": 117, "endOffset": 121}, {"referenceID": 16, "context": "\u2020Applying DropOut on the convolutional layers is also possible, but it typically increases the computation burden without significantly improving the results [17], we thus did not consider this setting.", "startOffset": 158, "endOffset": 162}, {"referenceID": 9, "context": "ETSC was arguably first proposed in [10] and developed along several independent lines, where classifiers make predictions from partial observations of temporal data.", "startOffset": 36, "endOffset": 40}, {"referenceID": 23, "context": "In particular, [24, 32] use boosting to facilitate a chronological restriction of features to achieve early recognition.", "startOffset": 15, "endOffset": 23}, {"referenceID": 31, "context": "In particular, [24, 32] use boosting to facilitate a chronological restriction of features to achieve early recognition.", "startOffset": 15, "endOffset": 23}, {"referenceID": 0, "context": "More recently, [1] proposed an ensemble of classifiers learned on subsequences of different lengths, and the prediction is made by the earliest individual classifier.", "startOffset": 15, "endOffset": 18}, {"referenceID": 1, "context": "Among other recent work, [2] learns the ensemble based early classifier by minimizing an empirical risk function and the response time required to achieve the minimum risk, which is formulized as a quadratic programming problem and solved by an iterative constraint generation algorithm.", "startOffset": 25, "endOffset": 28}, {"referenceID": 34, "context": "In particular, [35] proposed ETCS (Early Classification on Time Series), which introduced the notion of Minimum Prediction Length (MPL), the earliest timestamp for each time series in the training data to find the correct nearest neighbor, and searches all MPLs in a hierarchical fashion.", "startOffset": 15, "endOffset": 19}, {"referenceID": 37, "context": "Inspired by the idea of shaplets [38], which are subsequences of time series that can be highly discriminative for identifying a class label, [36][13][16] extend instance-based methods by discovering a set of discriminative shapelets early in time.", "startOffset": 33, "endOffset": 37}, {"referenceID": 35, "context": "Inspired by the idea of shaplets [38], which are subsequences of time series that can be highly discriminative for identifying a class label, [36][13][16] extend instance-based methods by discovering a set of discriminative shapelets early in time.", "startOffset": 142, "endOffset": 146}, {"referenceID": 12, "context": "Inspired by the idea of shaplets [38], which are subsequences of time series that can be highly discriminative for identifying a class label, [36][13][16] extend instance-based methods by discovering a set of discriminative shapelets early in time.", "startOffset": 146, "endOffset": 150}, {"referenceID": 15, "context": "Inspired by the idea of shaplets [38], which are subsequences of time series that can be highly discriminative for identifying a class label, [36][13][16] extend instance-based methods by discovering a set of discriminative shapelets early in time.", "startOffset": 150, "endOffset": 154}, {"referenceID": 19, "context": "Further, [20] generalizes these methods to the multivariate setting with both numerical and categorical features.", "startOffset": 9, "endOffset": 13}, {"referenceID": 18, "context": "Another recent interesting approach is MD-MDPP+TD [19], which models multivariate time series as a Multivariate Marked Point-Process (Multi-MPP) and leverages the sequential cues as the temporal patterns to make early classification.", "startOffset": 50, "endOffset": 54}, {"referenceID": 13, "context": "For example, [14] combines hidden markov models (HMM) and support vector machines (SVM) for early classification and set up a threshold on the prediction probability to ensure reliability.", "startOffset": 13, "endOffset": 17}, {"referenceID": 25, "context": "[26] proposes a local quadratic discriminant analysis (QDA) based approach.", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "[8] introduces a formal criterion for the detector to express the trade-off between earliness and accuracy.", "startOffset": 0, "endOffset": 3}, {"referenceID": 32, "context": "Among other recent work, [33] utilizes the structure characteristic of the long short term memory (LSTM) to do early recognition during testing time.", "startOffset": 25, "endOffset": 29}, {"referenceID": 24, "context": "More recently, [25] proposes a probability based classifier, which learns timestamps in which the prediction accuracy for each class surpass a user defined threshold.", "startOffset": 15, "endOffset": 19}, {"referenceID": 14, "context": "For TSC, shaplets based methods have received a significant attention recently [15][29][18].", "startOffset": 79, "endOffset": 83}, {"referenceID": 28, "context": "For TSC, shaplets based methods have received a significant attention recently [15][29][18].", "startOffset": 83, "endOffset": 87}, {"referenceID": 17, "context": "For TSC, shaplets based methods have received a significant attention recently [15][29][18].", "startOffset": 87, "endOffset": 91}, {"referenceID": 6, "context": "Since, shaplets can be viewed as a special case of features learned by ConvNets [7], ConvNets to TSC is a promosing direction.", "startOffset": 80, "endOffset": 83}, {"referenceID": 39, "context": "Recently, [40] explored multi-channel CNN to deal with multivariate time series.", "startOffset": 10, "endOffset": 14}, {"referenceID": 6, "context": "In another recent work, [7] proposed a generalized CNN capable of extracting multi-scale characteristics in time series data and achieves state-of-the-art performance on multiple benchmark datasets.", "startOffset": 24, "endOffset": 27}, {"referenceID": 4, "context": "We select 12 benchmark data sets from the UCR time series archive [5], with time series having a variety of lengths.", "startOffset": 66, "endOffset": 69}, {"referenceID": 5, "context": "EA-ConvNets is implemented using Torch7 [6] and trained on NVIDIA GTA TITAN graphics cards with 6GB RAM.", "startOffset": 40, "endOffset": 43}, {"referenceID": 29, "context": "We optimize the model with RMSprop [30] using minibatches of size 50.", "startOffset": 35, "endOffset": 39}, {"referenceID": 34, "context": "\u2022 Early classification on time series (ECTS)[35] \u2022 Early Distinctive Shaplet Classification (EDSC) [36] \u2022 Reliable early classification method (RelClass) [26] \u2022 Reliable early classification framework for time series based on class discriminativeness and reliability of predictions (ECDIRE) [25] \u2022 Deep convolutional neural network classifier with the same network architecture as EA-ConvNets with full observation as the training set (EA-ConvNets-Full).", "startOffset": 44, "endOffset": 48}, {"referenceID": 35, "context": "\u2022 Early classification on time series (ECTS)[35] \u2022 Early Distinctive Shaplet Classification (EDSC) [36] \u2022 Reliable early classification method (RelClass) [26] \u2022 Reliable early classification framework for time series based on class discriminativeness and reliability of predictions (ECDIRE) [25] \u2022 Deep convolutional neural network classifier with the same network architecture as EA-ConvNets with full observation as the training set (EA-ConvNets-Full).", "startOffset": 99, "endOffset": 103}, {"referenceID": 25, "context": "\u2022 Early classification on time series (ECTS)[35] \u2022 Early Distinctive Shaplet Classification (EDSC) [36] \u2022 Reliable early classification method (RelClass) [26] \u2022 Reliable early classification framework for time series based on class discriminativeness and reliability of predictions (ECDIRE) [25] \u2022 Deep convolutional neural network classifier with the same network architecture as EA-ConvNets with full observation as the training set (EA-ConvNets-Full).", "startOffset": 154, "endOffset": 158}, {"referenceID": 24, "context": "\u2022 Early classification on time series (ECTS)[35] \u2022 Early Distinctive Shaplet Classification (EDSC) [36] \u2022 Reliable early classification method (RelClass) [26] \u2022 Reliable early classification framework for time series based on class discriminativeness and reliability of predictions (ECDIRE) [25] \u2022 Deep convolutional neural network classifier with the same network architecture as EA-ConvNets with full observation as the training set (EA-ConvNets-Full).", "startOffset": 291, "endOffset": 295}, {"referenceID": 10, "context": "\u2022 1-Nearest Neighbor (1NN) classifier under full observation with Euclidean distance[11] \u2022 1NN with Dynamic Time Warping (DTW) [4] \u2022 Bag-of-SFA-Symbols (BOSS) [28] \u2022 Elastic Ensemble (PROP) [21] \u2022 Learning Shapelets Models(LTS) [15] \u2022 COTE [3], a weighted vote of over 35 classifiers \u2022 Multi-scale convolutional nerual networks (MCNN)[7]", "startOffset": 84, "endOffset": 88}, {"referenceID": 3, "context": "\u2022 1-Nearest Neighbor (1NN) classifier under full observation with Euclidean distance[11] \u2022 1NN with Dynamic Time Warping (DTW) [4] \u2022 Bag-of-SFA-Symbols (BOSS) [28] \u2022 Elastic Ensemble (PROP) [21] \u2022 Learning Shapelets Models(LTS) [15] \u2022 COTE [3], a weighted vote of over 35 classifiers \u2022 Multi-scale convolutional nerual networks (MCNN)[7]", "startOffset": 127, "endOffset": 130}, {"referenceID": 27, "context": "\u2022 1-Nearest Neighbor (1NN) classifier under full observation with Euclidean distance[11] \u2022 1NN with Dynamic Time Warping (DTW) [4] \u2022 Bag-of-SFA-Symbols (BOSS) [28] \u2022 Elastic Ensemble (PROP) [21] \u2022 Learning Shapelets Models(LTS) [15] \u2022 COTE [3], a weighted vote of over 35 classifiers \u2022 Multi-scale convolutional nerual networks (MCNN)[7]", "startOffset": 159, "endOffset": 163}, {"referenceID": 20, "context": "\u2022 1-Nearest Neighbor (1NN) classifier under full observation with Euclidean distance[11] \u2022 1NN with Dynamic Time Warping (DTW) [4] \u2022 Bag-of-SFA-Symbols (BOSS) [28] \u2022 Elastic Ensemble (PROP) [21] \u2022 Learning Shapelets Models(LTS) [15] \u2022 COTE [3], a weighted vote of over 35 classifiers \u2022 Multi-scale convolutional nerual networks (MCNN)[7]", "startOffset": 190, "endOffset": 194}, {"referenceID": 14, "context": "\u2022 1-Nearest Neighbor (1NN) classifier under full observation with Euclidean distance[11] \u2022 1NN with Dynamic Time Warping (DTW) [4] \u2022 Bag-of-SFA-Symbols (BOSS) [28] \u2022 Elastic Ensemble (PROP) [21] \u2022 Learning Shapelets Models(LTS) [15] \u2022 COTE [3], a weighted vote of over 35 classifiers \u2022 Multi-scale convolutional nerual networks (MCNN)[7]", "startOffset": 228, "endOffset": 232}, {"referenceID": 2, "context": "\u2022 1-Nearest Neighbor (1NN) classifier under full observation with Euclidean distance[11] \u2022 1NN with Dynamic Time Warping (DTW) [4] \u2022 Bag-of-SFA-Symbols (BOSS) [28] \u2022 Elastic Ensemble (PROP) [21] \u2022 Learning Shapelets Models(LTS) [15] \u2022 COTE [3], a weighted vote of over 35 classifiers \u2022 Multi-scale convolutional nerual networks (MCNN)[7]", "startOffset": 240, "endOffset": 243}, {"referenceID": 6, "context": "\u2022 1-Nearest Neighbor (1NN) classifier under full observation with Euclidean distance[11] \u2022 1NN with Dynamic Time Warping (DTW) [4] \u2022 Bag-of-SFA-Symbols (BOSS) [28] \u2022 Elastic Ensemble (PROP) [21] \u2022 Learning Shapelets Models(LTS) [15] \u2022 COTE [3], a weighted vote of over 35 classifiers \u2022 Multi-scale convolutional nerual networks (MCNN)[7]", "startOffset": 334, "endOffset": 337}, {"referenceID": 34, "context": "For ECTS [35], we set the parameter minimum support from 0 to 1 with the dense interval to be 0.", "startOffset": 9, "endOffset": 13}, {"referenceID": 35, "context": "For EDSC [36], we adopt the version based on Chebyshev Inequality and set the bound for Chebyshev condition to be 2.", "startOffset": 9, "endOffset": 13}, {"referenceID": 25, "context": "For RelClass [26], we choose the values for the reliability threshold from among {10\u221230, 10\u221225, 10\u221220, 10\u221215, 10\u221210, 10\u22125, 10\u22124, 10\u22123, 10\u22122, 10\u22121, 0.", "startOffset": 13, "endOffset": 17}], "year": 2016, "abstractText": "We present Earliness-Aware Deep Convolutional Networks (EA-ConvNets), an end-to-end deep learning framework, for early classification of time series data. Unlike most existing methods for early classification of time series data, that are designed to solve this problem under the assumption of the availability of a good set of pre-defined (often hand-crafted) features, our framework can jointly perform feature learning (by learning a deep hierarchy of shapelets capturing the salient characteristics in each time series), along with a dynamic truncation model to help our deep feature learning architecture focus on the early parts of each time series. Consequently, our framework is able to make highly reliable early predictions, outperforming various state-of-the-art methods for early time series classification, while also being competitive when compared to the state-of-the-art time series classification algorithms that work with fully observed time series data. To the best of our knowledge, the proposed framework is the first to perform data-driven (deep) feature learning in the context of early classification of time series data. We perform a comprehensive set of experiments, on several benchmark data sets, which demonstrate that our method yields significantly better predictions than various state-of-the-art methods designed for early time series classification. In addition to obtaining high accuracies, our experiments also show that the learned deep shapelets based features are also highly interpretable and can help gain better understanding of the underlying characteristics of time series data.", "creator": "LaTeX with hyperref package"}}}