{"id": "1611.04581", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Nov-2016", "title": "How to scale distributed deep learning?", "abstract": "Training time on large datasets for deep neural networks is the principal workflow bottleneck in a number of important applications of deep learning, such as object classification and detection in automatic driver assistance systems (ADAS). To minimize training time, the training of a deep neural network must be scaled beyond a single machine to as many machines as possible by distributing the optimization method used for training. While a number of approaches have been proposed for distributed stochastic gradient descent (SGD), at the current time synchronous approaches to distributed SGD appear to be showing the greatest performance at large scale. Synchronous scaling of SGD suffers from the need to synchronize all processors on each gradient step and is not resilient in the face of failing or lagging processors. In asynchronous approaches using parameter servers, training is slowed by contention to the parameter server. In this paper we compare the convergence of synchronous and asynchronous SGD for training a modern ResNet network architecture on the ImageNet classification problem. We also propose an asynchronous method, gossiping SGD, that aims to retain the positive features of both systems by replacing the all-reduce collective operation of synchronous training with a gossip aggregation algorithm. We find, perhaps counterintuitively, that asynchronous SGD, including both elastic averaging and gossiping, converges faster at fewer nodes (up to about 32 nodes), whereas synchronous SGD scales better to more nodes (up to about 100 nodes).", "histories": [["v1", "Mon, 14 Nov 2016 20:59:54 GMT  (670kb,D)", "http://arxiv.org/abs/1611.04581v1", "Extended version of paper accepted at ML Sys 2016 (at NIPS 2016)"]], "COMMENTS": "Extended version of paper accepted at ML Sys 2016 (at NIPS 2016)", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["peter h jin", "qiaochu yuan", "forrest iandola", "kurt keutzer"], "accepted": false, "id": "1611.04581"}, "pdf": {"name": "1611.04581.pdf", "metadata": {"source": "CRF", "title": "How to scale distributed deep learning?", "authors": ["Peter H. Jin", "Qiaochu Yuan", "Forrest Iandola"], "emails": ["phj@berkeley.edu", "qyuan@berkeley.edu", "forresti@berkeley.edu", "keutzer@berkeley.edu"], "sections": [{"heading": "1 Introduction", "text": "In this context, it should be noted that this is a very complex matter."}, {"heading": "2 Background", "text": "In this section, we will describe the synchronous and asynchronous baseline SGD methods, as well as a recently proposed asynchronous method that is more scalable than its predecessor. We will apply the following naming convention for SGD: \u03b8 are the parameters by which the target is minimized, \u03b8 \u0442 is the middle parameter (if applicable), \u03b1 is the step size, \u00b5 is the momentum, subscript i refers to the i-th node of p total node and subscript t refers to the t-th (minibatch) iteration. Additionally, b will refer to the size per node minibatch, while m is summed up to the aggregated minibatch size across all nodes."}, {"heading": "2.1 Synchronous All-Reduce SGD", "text": "In the traditional, synchronous, all-reducing SGD, there are two alternating phases that occur in the lock step: (1) each node computes its local parameter gradients, and (2) all nodes collectively communicate all-to-all to compute an aggregated gradient as if they all formed a large, distributed minibatch; and (2) all nodes collectively communicate all-to-all to compute an aggregated gradient, as if they all formed a large, distributed minibatch; the second phase of gradient exchange forms a barrier and is the communication-intensive phase that is normally implemented by a namesake, all-reducing operation; the time complexity of an overall reduction can be broken down into latency-bound and bandwidth-bound terms; although the latency term is scaled with O (log (p), there are fast ring algorithms that have a bandwidth term independent of p [11]; with modern networks able to handle bandwidth in the order of 1-10 GB / s / s, we can distribute all of the bandwidth before synchronization of the nodes, before synchronizing all of the bandi."}, {"heading": "2.2 Asynchronous Parameter-Server SGD", "text": "Another approach to SGD is that each node performs its own asynchronous gradient updates and occasionally synchronizes its parameters with a central parameter memory, a form of asynchronous SGD popularized by \"Hogwild\" SGD [12], which considered solving sparse problems on shared storage systems of individual machines. \"Downpour\" SGD [5] then generalized the distributed SGD approach, whereby nodes communicate their degrees with a central parameter server. A major weakness of the asynchronous parameter server approach to SGD is that workers communicate all-in-one with a central server and communication throughput is limited by the reception bandwidth of finite connections on the server. An approach to alleviate the communication bottleneck is to introduce a delay between communication circuits, but increasing the delay significantly reduces convergence rates [9]."}, {"heading": "2.3 Elastic Averaging SGD", "text": "Elastic averaging SGD [9] is a new algorithm belonging to the family of asynchronous parameter server methods that, in addition to loss, introduces a modification of the usual stochastic gradient target to achieve faster convergence. Elastic averaging strives to maximize the consensus between the mean and local parameters \u03b8i. (1) The elastic average algorithm is specified in algorithm 2. The consensus target of elastic average is closely related to the extended lagrange of ADMM, and the gradient update from the consensus code was shown by [9] that convergence is significantly faster than vanilla async SGD. However, elastic averaging is not a member of the asynchronous parameter servers."}, {"heading": "3 Gossiping SGD", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Algorithm", "text": "In a nutshell, the synchronous all-reduction algorithm can also be derived by referring to two repetitive phases: (1) calculation of local gradients at each node, and (2) exact aggregation of local gradients via all-reduce. To derive gossip and gossip from this, we would like to replace the synchronous all-reduction operation with an asynchronous-friendly communication pattern. [10] The basic building block we use is a gossip-aggregation algorithm [15, 16] which, in combination with SGD, leads to the gossip algorithm. Asynchronous gossiping SGD was introduced in [10] for the general case of communication between nodes."}, {"heading": "3.2 Analysis", "text": "We assume that all processors are able to communicate with all other processors at each step, and the main result of the convergence is as follows: Theorem 1. Let f be a strongly convex function with L-Lipschitz gradients. Let's assume that we can scan gradients g = f (\u03b8; Xi) + i with additive noise with zero mean E [sustai] = 0 and limited variance E [Ti] \u2264 0.02. Then, if we run the asynchronous pull-gossip algorithm with constant step size 0 < \u03b1 \u2264 2m + L, the expected sum of the squares convergence of the local parameters is limited to the optimal convergence of the local parameters with the optimal convergence E [\u0442ti]."}, {"heading": "4 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Implementation", "text": "Since we wanted to execute our code in cluster environments with Infiniband or more specialized networking, the easiest solution was to target MPI. For our experiments up to p = 16 nodes, we use a local cluster with 16 computers, each consisting of an Nvidia Kepler K80 Dual GPU, an Intel Haswell E5-1680v2 8-core CPU and a Mellanox ConnectX-3 FDR 4 \u00d7 Infiniband (56 Gb / s) CPU. We use only one GPU per K80. For our larger experiments up to p = 128, we use a PU with a total of 10,000 GNU nodes and a GNU with a total of 10,000 GNU nodes NIC."}, {"heading": "4.2 Methodology", "text": "We chose ResNets [7] for our neural network architecture; specifically, we trained ResNet-18, which is small enough to train quickly for experiments but also has features relevant to modern networks, including depth, residual layers and batch normalization [21]; we addressed ImageNet's image classification problem, which consists of 1.28 million training images and 50,000 validation images divided into 1,000 classes [8]; our data magnification is as follows: we performed multi-scale training by scaling the shortest dimension of images to 256 to 480 pixels [22]; we took random 224 x 224 clips and horizontal flips; and we added pixel-by-pixel color noise [23]; we evaluated validation losses and top-1 errors on central sections of the validation set images with the shortest dimension of communication scaled to 256 pixels."}, {"heading": "4.3 Results", "text": "Our first set of experiments compare all-reduce, elastic averaging and push gossip at p = 8 and p = 16 with an aggregated minibatch size m = pb = 256. The results are in Figure 1.For p = 8, elastic averaging with a communication delay \u03c4 = 10 performs ahead of the other methods, interestingly, all-reduce has virtually no synchronization overhead on the system at p = 8 and is as fast as gossip. All methods approach roughly the same minimal loss value. For p = 16, gossip converts faster than elastic average.And both occur before all-reduce. In addition, elastic averaging with both equals = 1 and progress = 10 has problems that lead to the same validation losses as the other methods, once the step size has been annealed, have been reduced to a small value (\u03b1 = 0.001 in this case).We also conduct larger experiments at 32 nodes, we are not gossiping at 12p = 64 and 12p is not in the environment."}, {"heading": "5 Discussion", "text": "But how fast can asynchronous and synchronous SGD algorithms converge both at the beginning of the training (large step sizes) and at the end of the training (large step sizes)? Up to about 32 nodes, asynchronous SGD convergences can converge faster than all other SGD convergences when the step size is large. How can the convergence behavior of asynchronous and synchronous distributed SGD processes vary with the number of nodes? Both elastic averages and gossip spikes seem to converge faster than elastic averages, but all-reduced SGD convergences are most consistent."}, {"heading": "Acknowledgments", "text": "We would like to thank Kostadin Ilov, Ed F. D'Azevedo, and Chris Fuson for their help in making this work possible, as well as Josh Tobin for the insightful discussions, and we would also like to thank the anonymous reviewers for their constructive feedback, which is funded in part by DARPA Award Number HR001112-2-0016 and by ASPIRE Lab industrial sponsors and affiliates Intel, Google, Hewlett-Packard, Huawei, LGE, NVIDIA, Oracle, and Samsung. Resources from the Oak Ridge Leadership Computing Facility at the Oak Ridge National Laboratory, supported by the U.S. Department of Energy's Office of Science under contract number DE-AC05-00OR22725, were used for this research."}, {"heading": "6 Appendix", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6.1 Analysis", "text": "The analysis shown below loosely follows the arguments and uses a combination of techniques derived from [17,16,10,18,9]. To simplify exposure and notation, we focus our attention on the case of the univariate, strongly convex function f with Lipschitz gradients. (Since the sum of square errors in vector components is additive, we generalize our arguments to the case of multivariate functions.) We assume that the gradients of function f can be scanned independently of all processors, up to additive noise with medium and limited variance. (Gaussian noise fulfills these assumptions, for example.) We also assume a fully interconnected network in which all processors are able to communicate with all other processors. Additional assumptions are introduced below as needed. We designate the vector containing all local parameters, the vector containing all local parameters at the time, as a p. \u2212 In the following function, we will refer to the optimal noise value in the local observer."}, {"heading": "6.1.1 Synchronous pull-gossip algorithm", "text": "We begin by analyzing the synchronous version of the pull gossip algorithm described in algorithm 3. (For each processor i, we consistently randomly select the processor we have chosen from which processor i \"pulls\" parameter values originate. (Xt, i) (12) We prove the following rate of convergence for the synchronous pull gossip algorithm. (Theorem 2) Let f be a strongly convex function with L-Lipschitz gradients. (Xt, i) We assume that we can optimally select gradients g = 1. (Xi) +"}, {"heading": "6.1.2 Asynchronous pull-gossip algorithm", "text": "We provide similar analyses for the asynchronous version of the pull gossip algorithm. \u00b7 As is often the case in literature, we model the time steps as the ticks of local processor clocks regulated by Poisson processes. \u2212 More precisely, we assume that each processor has a clock that corresponds to a rate of 1 Poisson process. Since each master clock corresponds to the ticks of a local clock on processor i, this in turn marks the time step in which processor i \"pulls\" the parameter values from the uniformly selected processor i. \"Modeling the time steps of Poisson processes offer beautiful theoretical properties, i.e. the inter-tick time intervals are i.e. exponential variables of the rate p, and the local clock that causes each master clock."}, {"heading": "6.2 Gossip variants", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6.2.1 Gossip with stale parameters (gradient step and gossip at the same time)", "text": "Consider a one-step update of the distributed consensus gradient: \u03b8i, t + 1 = \u03b8i, t \u2212 \u03b1 \u0441i (\u03b8i, t; Xi) \u2212 \u03b2 \u03b8i, t \u2212 1 p \u2211 j, t. (62) Replace the distributed mean 1p \u2211 p = 1 \u03b8j, t with an unbiased one-sample estimator \u03b8ji, t, t, so that ji, t \u00b2 uniform ({1,..., p}) and E [\u03b8ji, t] = 1p \u00b2 p = 1 \u03b8j, t, then we derive the clapping SGD update: successi, t + 1 = successi, t \u2212 \u03b1 \u03c6i (\u03b8i, t; Xi) \u2212 \u03b2 (\u03b8i, t \u2212 \u03b8ji, t) (63) = (1 \u2212 \u03b2) \u0445i, t \u2212 \u03b2phenji, t \u2212 \u03b1 \u0432i (successi, t; Xi) (64)."}, {"heading": "6.2.2 Gossip with fresh parameters (gradient step before gossip)", "text": "Consider a two-step update of the distributed consensus gradient: \u03b8 \"i, t = \u03b8i, t \u2212 \u03b1\" f (\u03b8i, t; Xi) (65) \u03b8i, t + 1 = \u03b8 \"i, t \u2212 \u03b2 \u03b8\" i, t \u2212 1p \"j = 1 \u03b8\" j, t. \"(66) Let us replace the distributed mean 1p\" p \"j = 1 \u03b8\" j, t \"with an unbiased one-sample estimator: ji, t, t, so that ji, t\" uniform ({1,., p}) and E [\u03b8 \"ji, t, t] = 1 p\" p \"j,\" t, \"then we derive the gossip-actualizing SGD:\" i \"i, t = successi, t \u2212 \u03b1\" f (\u03b8i, t; Xi) (67) \u0445i, t \u2212 \u03b2 \"i\" t \"\u03b2\" \u03b2 \"(69)."}, {"heading": "6.3 Implementation details", "text": "We present some more details about our implementation of deep training in the field of Convolutionary Neural Networks in general."}, {"heading": "6.3.1 ImageNet data augmentation", "text": "We found that multi-level training can present a significant performance bottleneck due to the computational effort involved in resizing images, even when using multiple threads and asynchronous loading of data. To address this, we used fast CUDA implementations of linear and cubic interpolation filters to perform image scaling on the GPU during the training, and we also edited ImageNet images so that their largest dimension was no larger than the maximum scale (in our case 480 pixels)."}, {"heading": "6.3.2 ResNet implementation", "text": "We implemented ResNet-18 using stacked remaining wavelayers with 1 \u00d7 1 projection associations. We used the folding and batch normalization cores of cuDNNv4. The highest accuracy of the ImageNet validation rates (mean seeding, top-1) that our implementation of ResNets achieved with the above-mentioned multi-scale data augmentation was about 68.7%; we note that researchers at Facebook independently reproduced ResNet using a more sophisticated data augmentation scheme and achieved 69.6% accuracy using the same evaluation method on their version of ResNet-18.22 https: / / github.com / facebook / fb.resnet.torch"}], "references": [{"title": "Deep Image: Scaling up Image Recognition", "author": ["Ren Wu", "Shengen Yan", "Yi Shan", "Qingqing Dang", "Gang Sun"], "venue": "arXiv preprint arXiv:1501.02876,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2015}, {"title": "FireCaffe: near-linear acceleration of deep neural network training on compute clusters", "author": ["Forrest N. Iandola", "Khalid Ashraf", "Matthew W. Moskewicz", "Kurt Keutzer"], "venue": "arXiv preprint arXiv:1511.00175,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2015}, {"title": "Distributed Deep Learning Using Synchronous Stochastic Gradient Descent", "author": ["Dipankar Das", "Sasikanth Avancha", "Dheevatsa Mudigere", "Karthikeyan Vaidynathan", "Srinivas Sridharan", "Dhiraj Kalamkar", "Bharat Kaul", "Pradeep Dubey"], "venue": "arXiv preprint arXiv:1602.06709,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2016}, {"title": "Revisiting Distributed Synchronous SGD", "author": ["Jianmin Chen", "Rajat Monga", "Samy Bengio", "Rafal Jozefowicz"], "venue": "arXiv preprint arXiv:1604.00981,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2016}, {"title": "Large Scale Distributed Deep Networks", "author": ["Jeffrey Dean", "Greg Corrado", "Rajat Monga", "Kai Chen", "Matthieu Devin", "Mark Mao", "Andrew Senior", "Paul Tucker", "Ke Yang", "Quoc V. Le", "Andrew Y. Ng"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2012}, {"title": "Project Adam: Building an Efficient and Scalable Deep Learning Training System", "author": ["Trishul Chilimbi", "Yutaka Suzue", "Johnson Apacible", "Karthik Kalyanaraman"], "venue": "In 11th USENIX Symposium on Operating Systems Design and Implementation,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2014}, {"title": "Deep Residual Learning for Image Recognition", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"], "venue": "arXiv preprint arXiv:1512.03385,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2015}, {"title": "Deep learning with Elastic Averaging SGD", "author": ["Sixin Zhang", "Anna E. Choromanska", "Yann LeCun"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2015}, {"title": "Asynchronous Gossip Algorithms for Stochastic Optimization", "author": ["S. Sundhar Ram", "A. Nedi\u0107", "V.V. Veeravalli"], "venue": "In Proceedings of the 48th IEEE Conference on Decision and Control,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2009}, {"title": "Optimization of Collective Communication Operations in MPICH", "author": ["Rajeev Thakur", "Rolf Rabenseifner", "William Gropp"], "venue": "International Journal of High Performance Computing Applications,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2005}, {"title": "Hogwild: A Lock-Free Approach to Parallelizing Stochastic Gradient Descent", "author": ["Benjamin Recht", "Christopher Re", "Stephen Wright", "Feng Niu"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2011}, {"title": "More Effective Distributed ML via a Stale Synchronous Parallel Parameter Server", "author": ["Qirong Ho", "James Cipar", "Henggang Cui", "Seunghak Lee", "Jin Kyu Kim", "Phillip B. Gibbons", "Garth A. Gibson", "Greg Ganger", "Eric P. Xing"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2013}, {"title": "Communication Efficient Distributed Machine Learning with the Parameter Server", "author": ["Mu Li", "David G. Andersen", "Alex J. Smola", "Kai Yu"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2014}, {"title": "Gossip-Based Computation of Aggregate Information", "author": ["David Kempe", "Alin Dobra", "Johannes Gehrke"], "venue": "In Proceedings of the 44th Annual IEEE Conference on Foundations of Computer Science,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2003}, {"title": "Randomized Gossip Algorithms", "author": ["Stephen Boyd", "Arpita Ghosh", "Balaji Prabhakar", "Devavrat Shah"], "venue": "IEEE/ACM Transactions on Networking,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2006}, {"title": "Introductory Lectures on Convex Optimization: A Basic Course, volume", "author": ["Yurii Nesterov"], "venue": null, "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2013}, {"title": "Asynchronous Stochastic Convex Optimization for Random Networks: Error Bounds", "author": ["Behrouz Touri", "A. Nedi\u0107", "S. Sundhar Ram"], "venue": "In Information Theory and Applications Workshop (ITA)", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2010}, {"title": "Shelhammer. cuDNN: Efficient Primitives for Deep Learning", "author": ["Sharan Chetlur", "Cliff Woolley", "Philippe Vandermersch", "Jonathan Cohen", "John Tran", "Bryan Catanzaro", "Evan"], "venue": "arXiv preprint arXiv:1410.0759,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2014}, {"title": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift", "author": ["Sergey Ioffe", "Christian Szegedy"], "venue": "In Proceedings of the 32nd International Conference on Machine Learning,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2015}, {"title": "Very Deep Convolutional Networks for Large-Scale Image Recognition", "author": ["Karen Simonyan", "Andrew Zisserman"], "venue": "arXiv preprint arXiv:1409.1556,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2014}, {"title": "ImageNet Classification with Deep Convolutional Neural Networks", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E. Hinton"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2012}, {"title": "Asynchronous Methods for Deep Reinforcement Learning", "author": ["Volodymyr Mnih", "Adria Puigdomenech Badia", "Mehdi Mirza", "Alex Graves", "Timothy P Lillicrap", "Tim Harley", "David Silver", "Koray Kavukcuoglu"], "venue": "arXiv preprint arXiv:1602.01783,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2016}, {"title": "Lecture 6.5-RMSProp: Divide the gradient by a running average of its recent magnitude", "author": ["Tijmen Tieleman", "Geoffrey Hinton"], "venue": "Coursera: Neural Networks for Machine Learning,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2012}, {"title": "Adam: A Method for Stochastic Optimization", "author": ["Diederik Kingma", "Jimmy Ba"], "venue": "In 3rd International Conference for Learning Representations,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "There are two primary approaches to distributed stochastic gradient descent (SGD) for training deep neural networks: (i) synchronous all-reduce SGD based on a fast all-reduce collective communication operation [1, 2, 3, 4], and (ii) asynchronous SGD using a parameter server [5, 6].", "startOffset": 210, "endOffset": 222}, {"referenceID": 1, "context": "There are two primary approaches to distributed stochastic gradient descent (SGD) for training deep neural networks: (i) synchronous all-reduce SGD based on a fast all-reduce collective communication operation [1, 2, 3, 4], and (ii) asynchronous SGD using a parameter server [5, 6].", "startOffset": 210, "endOffset": 222}, {"referenceID": 2, "context": "There are two primary approaches to distributed stochastic gradient descent (SGD) for training deep neural networks: (i) synchronous all-reduce SGD based on a fast all-reduce collective communication operation [1, 2, 3, 4], and (ii) asynchronous SGD using a parameter server [5, 6].", "startOffset": 210, "endOffset": 222}, {"referenceID": 3, "context": "There are two primary approaches to distributed stochastic gradient descent (SGD) for training deep neural networks: (i) synchronous all-reduce SGD based on a fast all-reduce collective communication operation [1, 2, 3, 4], and (ii) asynchronous SGD using a parameter server [5, 6].", "startOffset": 210, "endOffset": 222}, {"referenceID": 4, "context": "There are two primary approaches to distributed stochastic gradient descent (SGD) for training deep neural networks: (i) synchronous all-reduce SGD based on a fast all-reduce collective communication operation [1, 2, 3, 4], and (ii) asynchronous SGD using a parameter server [5, 6].", "startOffset": 275, "endOffset": 281}, {"referenceID": 5, "context": "There are two primary approaches to distributed stochastic gradient descent (SGD) for training deep neural networks: (i) synchronous all-reduce SGD based on a fast all-reduce collective communication operation [1, 2, 3, 4], and (ii) asynchronous SGD using a parameter server [5, 6].", "startOffset": 275, "endOffset": 281}, {"referenceID": 6, "context": "SGD vary with the number of nodes? To compare the strengths and weaknesses of asynchronous and synchronous SGD algorithms, we train a modern ResNet convolutional network [7] on the ImageNet dataset [8] using various distributed SGD methods.", "startOffset": 170, "endOffset": 173}, {"referenceID": 7, "context": "We primarily compare synchronous all-reduce SGD, the recently proposed asynchronous elastic averaging SGD [9], as well as our own method, asynchronous gossiping SGD, based on an algorithm originally developed in a different problem setting [10].", "startOffset": 106, "endOffset": 109}, {"referenceID": 8, "context": "We primarily compare synchronous all-reduce SGD, the recently proposed asynchronous elastic averaging SGD [9], as well as our own method, asynchronous gossiping SGD, based on an algorithm originally developed in a different problem setting [10].", "startOffset": 240, "endOffset": 244}, {"referenceID": 9, "context": "Although the latency term scales with O(log(p)), there are fast ring algorithms which have bandwidth term independent of p [11].", "startOffset": 123, "endOffset": 127}, {"referenceID": 0, "context": "Examples of large-scale synchronous data parallel SGD for distributed deep learning are given in [1], [2], [3], and [4].", "startOffset": 97, "endOffset": 100}, {"referenceID": 1, "context": "Examples of large-scale synchronous data parallel SGD for distributed deep learning are given in [1], [2], [3], and [4].", "startOffset": 102, "endOffset": 105}, {"referenceID": 2, "context": "Examples of large-scale synchronous data parallel SGD for distributed deep learning are given in [1], [2], [3], and [4].", "startOffset": 107, "endOffset": 110}, {"referenceID": 3, "context": "Examples of large-scale synchronous data parallel SGD for distributed deep learning are given in [1], [2], [3], and [4].", "startOffset": 116, "endOffset": 119}, {"referenceID": 10, "context": "This form of asynchronous SGD was popularized by \u201cHogwild\u201d SGD [12], which considered solving sparse problems on single machine shared memory systems.", "startOffset": 63, "endOffset": 67}, {"referenceID": 4, "context": "\u201cDownpour\u201d SGD [5] then generalized the approach to distributed SGD where nodes communicate their gradients with a central parameter server.", "startOffset": 15, "endOffset": 18}, {"referenceID": 7, "context": "One approach for alleviating the communication bottleneck is introducing a delay between rounds of communication, but increasing the delay greatly decreases the rate of convergence [9].", "startOffset": 181, "endOffset": 184}, {"referenceID": 4, "context": "Large scale asynchronous SGD for deep learning was first implemented in Google DistBelief [5] and has also been implemented in [6]; large scale parameter server systems in the non-deep learning setting have also been demonstrated in [13] and [14].", "startOffset": 90, "endOffset": 93}, {"referenceID": 5, "context": "Large scale asynchronous SGD for deep learning was first implemented in Google DistBelief [5] and has also been implemented in [6]; large scale parameter server systems in the non-deep learning setting have also been demonstrated in [13] and [14].", "startOffset": 127, "endOffset": 130}, {"referenceID": 11, "context": "Large scale asynchronous SGD for deep learning was first implemented in Google DistBelief [5] and has also been implemented in [6]; large scale parameter server systems in the non-deep learning setting have also been demonstrated in [13] and [14].", "startOffset": 233, "endOffset": 237}, {"referenceID": 12, "context": "Large scale asynchronous SGD for deep learning was first implemented in Google DistBelief [5] and has also been implemented in [6]; large scale parameter server systems in the non-deep learning setting have also been demonstrated in [13] and [14].", "startOffset": 242, "endOffset": 246}, {"referenceID": 7, "context": "Elastic averaging SGD [9] is a new algorithm belonging to the family of asynchronous parameterserver methods which introduces a modification to the usual stochastic gradient objective to achieve faster convergence.", "startOffset": 22, "endOffset": 25}, {"referenceID": 7, "context": "The consensus objective of elastic averaging is closely related to the augmented Lagrangian of ADMM, and the gradient update derived from the consensus objective was shown by [9] to converge significantly faster than vanilla async SGD.", "startOffset": 175, "endOffset": 178}, {"referenceID": 7, "context": "Because recent published results indicate that elastic averaging dominates previous asynchronous parameter-server methods [9], we will only consider elastic averaging from this point on.", "startOffset": 122, "endOffset": 125}, {"referenceID": 13, "context": "The fundamental building block we use is a gossip aggregation algorithm [15, 16], which combined with SGD leads to the gossiping SGD algorithm.", "startOffset": 72, "endOffset": 80}, {"referenceID": 14, "context": "The fundamental building block we use is a gossip aggregation algorithm [15, 16], which combined with SGD leads to the gossiping SGD algorithm.", "startOffset": 72, "endOffset": 80}, {"referenceID": 8, "context": "Asynchronous gossiping SGD was introduced in [10] for the general case of a sparse communication", "startOffset": 45, "endOffset": 49}, {"referenceID": 13, "context": "It can be shown that repeated rounds of a form of gossiping reduces the diffusion potential by a fixed rate per round [15].", "startOffset": 118, "endOffset": 122}, {"referenceID": 13, "context": "Push-gossiping SGD can be interpreted as an interleaving of a gradient step and a simplified push-sum gossip step [15].", "startOffset": 114, "endOffset": 118}, {"referenceID": 15, "context": "2 Analysis Our analysis of gossiping SGD is based on the analyses in [17, 16, 10, 18, 9].", "startOffset": 69, "endOffset": 88}, {"referenceID": 14, "context": "2 Analysis Our analysis of gossiping SGD is based on the analyses in [17, 16, 10, 18, 9].", "startOffset": 69, "endOffset": 88}, {"referenceID": 8, "context": "2 Analysis Our analysis of gossiping SGD is based on the analyses in [17, 16, 10, 18, 9].", "startOffset": 69, "endOffset": 88}, {"referenceID": 16, "context": "2 Analysis Our analysis of gossiping SGD is based on the analyses in [17, 16, 10, 18, 9].", "startOffset": 69, "endOffset": 88}, {"referenceID": 7, "context": "2 Analysis Our analysis of gossiping SGD is based on the analyses in [17, 16, 10, 18, 9].", "startOffset": 69, "endOffset": 88}, {"referenceID": 17, "context": "0 driver and using the cuBLAS and cuDNNv4 [20] libraries for the core computational kernels.", "startOffset": 42, "endOffset": 46}, {"referenceID": 6, "context": "We chose ResNets [7] for our neural network architecture; specifically, we trained ResNet-18, which is small enough to train rapidly for experimentation, but also possesses features relevant to modern networks, including depth, residual layers, and batch normalization [21].", "startOffset": 17, "endOffset": 20}, {"referenceID": 18, "context": "We chose ResNets [7] for our neural network architecture; specifically, we trained ResNet-18, which is small enough to train rapidly for experimentation, but also possesses features relevant to modern networks, including depth, residual layers, and batch normalization [21].", "startOffset": 269, "endOffset": 273}, {"referenceID": 19, "context": "Our data augmentation is as follows: we performed multi-scale training by scaling the shortest dimension of images to between 256 and 480 pixels [22], we took random 224\u00d7 224 crops and horizontal flips, and we added pixelwise color noise [23].", "startOffset": 145, "endOffset": 149}, {"referenceID": 20, "context": "Our data augmentation is as follows: we performed multi-scale training by scaling the shortest dimension of images to between 256 and 480 pixels [22], we took random 224\u00d7 224 crops and horizontal flips, and we added pixelwise color noise [23].", "startOffset": 238, "endOffset": 242}, {"referenceID": 7, "context": "For gossiping, we used both \u03c4 = 1 and \u03c4 = 10 (the latter is recommended in [9]).", "startOffset": 75, "endOffset": 78}, {"referenceID": 3, "context": "One observation we made consistent with [4] was the following: letting synchronous all-reduce SGD run for many epochs, it will typically converge to a lower optimal validation loss (or higher validation accuracy) than either elastic averaging or gossiping SGD.", "startOffset": 40, "endOffset": 43}, {"referenceID": 21, "context": "recent results on asynchronous methods in deep reinforcement learning [24].", "startOffset": 70, "endOffset": 74}, {"referenceID": 22, "context": "Adaptive versions of SGD such as RMSProp [25] and Adam [26] are also widely used in deep learning, and their corresponding distributed versions may have additional considerations (e.", "startOffset": 41, "endOffset": 45}, {"referenceID": 23, "context": "Adaptive versions of SGD such as RMSProp [25] and Adam [26] are also widely used in deep learning, and their corresponding distributed versions may have additional considerations (e.", "startOffset": 55, "endOffset": 59}, {"referenceID": 21, "context": "sharing the squared gradients in [24]).", "startOffset": 33, "endOffset": 37}], "year": 2016, "abstractText": "Training time on large datasets for deep neural networks is the principal workflow bottleneck in a number of important applications of deep learning, such as object classification and detection in automatic driver assistance systems (ADAS). To minimize training time, the training of a deep neural network must be scaled beyond a single machine to as many machines as possible by distributing the optimization method used for training. While a number of approaches have been proposed for distributed stochastic gradient descent (SGD), at the current time synchronous approaches to distributed SGD appear to be showing the greatest performance at large scale. Synchronous scaling of SGD suffers from the need to synchronize all processors on each gradient step and is not resilient in the face of failing or lagging processors. In asynchronous approaches using parameter servers, training is slowed by contention to the parameter server. In this paper we compare the convergence of synchronous and asynchronous SGD for training a modern ResNet network architecture on the ImageNet classification problem. We also propose an asynchronous method, gossiping SGD, that aims to retain the positive features of both systems by replacing the all-reduce collective operation of synchronous training with a gossip aggregation algorithm. We find, perhaps counterintuitively, that asynchronous SGD, including both elastic averaging and gossiping, converges faster at fewer nodes (up to about 32 nodes), whereas synchronous SGD scales better to more nodes (up to about 100 nodes).", "creator": "LaTeX with hyperref package"}}}