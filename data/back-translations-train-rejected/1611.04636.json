{"id": "1611.04636", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Nov-2016", "title": "When Saliency Meets Sentiment: Understanding How Image Content Invokes Emotion and Sentiment", "abstract": "Sentiment analysis is crucial for extracting social signals from social media content. Due to the prevalence of images in social media, image sentiment analysis is receiving increasing attention in recent years. However, most existing systems are black-boxes that do not provide insight on how image content invokes sentiment and emotion in the viewers. Psychological studies have confirmed that salient objects in an image often invoke emotions. In this work, we investigate more fine-grained and more comprehensive interaction between visual saliency and visual sentiment. In particular, we partition images in several primary scene-type dimensions, including: open-closed, natural-manmade, indoor-outdoor, and face-noface. Using state of the art saliency detection algorithm and sentiment classification algorithm, we examine how the sentiment of the salient region(s) in an image relates to the overall sentiment of the image. The experiments on a representative image emotion dataset have shown interesting correlation between saliency and sentiment in different scene types and in turn shed light on the mechanism of visual sentiment evocation.", "histories": [["v1", "Mon, 14 Nov 2016 22:02:09 GMT  (3568kb)", "http://arxiv.org/abs/1611.04636v1", "7 pages, 5 figures, submitted to AAAI-17"]], "COMMENTS": "7 pages, 5 figures, submitted to AAAI-17", "reviews": [], "SUBJECTS": "cs.AI cs.CV", "authors": ["honglin zheng", "tianlang chen", "jiebo luo"], "accepted": false, "id": "1611.04636"}, "pdf": {"name": "1611.04636.pdf", "metadata": {"source": "CRF", "title": "When Saliency Meets Sentiment: Understanding How Image Content Invokes Emotion and Sentiment", "authors": ["Honglin Zheng", "Tianlang Chen", "Jiebo Luo"], "emails": ["hzheng10@u.rochester.edu,", "t.chen@rochester.edu,", "jluo@cs.rochester.edu"], "sections": [{"heading": "Introduction", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Visual Sentiment Analysis", "text": "In such an era of information explosion, it is increasingly common for people to express their emotions by posting images on social media such as Twitter and Flickr. It is important for both psychologists and computer vision researchers to use such enormous amounts of information to interpret the human emotions contained in the images. In the early days, handcrafted features, such as pixel-level features such as color, texture and composition (Machajdik and Hanbury 2010), were intended to study the emotional response to visual content. Similar methods were ubiquitous until recently, when large-format image datasets such as ImageNet (Deng et al. 2009) and Places Dataset (Zhou et al. 2014) became available. The power of deep learning, especially Convolutionary Neural Networks, has recently been used to discover the feelings expressed in images (You et al. 2015; Jindal et Singh 2015)."}, {"heading": "Saliency Detection", "text": "In the meantime, the perception of emotional perception is shifting from the top to the bottom, from classification factors to in-depth, learning-oriented models. In the past, the sensitivity of people in individual regions has changed markedly, both in terms of the way they move, the way they move, and the way they identify with themselves (Wang et al.) 2015; Zhang et al. 2015, perception is focused on attention analysis, while visual sensory characteristics are focused on analysis. Various psychologists and neuroscientists have been actively investigating the underlying relationship between attention and emotion. There is a persistent discrepancy between those who perceive emotional automatically, namely in the way and in which it occurs."}, {"heading": "Methodology", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Framework Overview", "text": "Figure 2 illustrates our frame with annotations. Details of the steps are as follows: Step 1: For a particular image, we use a state-of-the-art model for detecting prominent objects (Zhang et al. 2016), if any, for each image. Red boxes indicate the identified prominent objects. Step 2: We then use a state-of-the-art model for detecting prominent objects (Campos, Jou and Giro-i-Nieto 2016) to obtain sensory values for both the overall image and all of its prominent objects. Step 3 & 4: Based on the results obtained, we can determine for each image whether a prominent object shares the same sensation with the entire image. Consequently, we can divide the entire dataset into two parts: those images in which at least one of their prominent objects shares the same sensation with the overall image, and those images in which none of their prominent objects shares the same sensation with the entire image."}, {"heading": "Dataset", "text": "We use the data set published by You et al. (2016) to visually predict sentiment. This data set covers 8 different categories, including \"entertainment,\" \"anger,\" \"awe,\" \"satisfaction,\" \"disgust,\" \"excitement,\" \"fear\" and \"sadness.\" For each category, we randomly sample 30% of the first 8,000 images for experimentation. The statistics of our current data set are in Table 1. Note that this work does not actually require an exact emotion label for each image, so we can compile our data set from both labeled and unlabeled images."}, {"heading": "Salient Object Detection", "text": "We use a state-of-the-art saliency detection model (Zhang et al. 2016) to determine whether an image has a prominent object and whether there is one. This model uses the high expressiveness of VGGNet to create a series of distinctive object suggestions for an image, on the basis of which it generates a compact set of detected regions using a subset optimization formulation. For each image in the given dataset, the model detects whether there is a distinctive object in it, and stores all detected distinctive objects as subimages in a corresponding folder for our subsequent scene attribute-based partitioning."}, {"heading": "Visual Sentiment Analysis", "text": "We use the Convolutional Neural Network (CNN) to obtain a visual sentiment prediction. Specifically, we use the Convolutional Neural Network model proposed by Campos, Jou and Giro-i-Bieto (2016). In their work, they have refined a CNN model by using several performance-enhancing techniques to improve the visual sentiment prediction. For each image, we extract the two output nodes of the last fully connected layer and designate the sentiment probability outputs of image i as \"= (Pr,\" Pr () \"), representing the probability of a negative or positive feeling. Based on this notation, we can define the SAR, Sentiment Agreement Rate. For each image i, = min (Pr,\" Pr 4 |), () () of a foreign image () sentiment"}, {"heading": "Attribute Extraction and Categorization", "text": "In fact, most of them are able to survive on their own if they do not put themselves in a position to survive on their own."}], "references": [{"title": "Affective image classification using features inspired by psychology and art theory", "author": ["J. Machajdik", "A. Hanbury"], "venue": "Proceedings of the 18th ACM international conference on Multimedia (pp. 83-92). ACM.", "citeRegEx": "Machajdik and Hanbury,? 2010", "shortCiteRegEx": "Machajdik and Hanbury", "year": 2010}, {"title": "Imagenet: A large-scale hierarchical image database", "author": ["J. Deng", "W. Dong", "R. Socher", "L.J. Li", "K. Li", "L. Fei-Fei"], "venue": "Computer Vision and Pattern Recognition, 2009. CVPR 2009. IEEE Conference on (pp. 248-255). IEEE.", "citeRegEx": "Deng et al\\.,? 2009", "shortCiteRegEx": "Deng et al\\.", "year": 2009}, {"title": "Learning deep features for scene recognition using places database", "author": ["B. Zhou", "A. Lapedriza", "J. Xiao", "A. Torralba", "A. Oliva"], "venue": "Advances in neural information processing systems (pp. 487-495).", "citeRegEx": "Zhou et al\\.,? 2014", "shortCiteRegEx": "Zhou et al\\.", "year": 2014}, {"title": "Image Sentiment Analysis using Deep Convolutional Neural Networks with Domain Specific Fine Tuning", "author": ["S. Jindal", "S. Singh"], "venue": "IEEE International Conference on Information Processing (ICIP)", "citeRegEx": "Jindal and Singh,? 2015", "shortCiteRegEx": "Jindal and Singh", "year": 2015}, {"title": "Robust Image Sentiment Analysis using Progressively Trained and Domain Transferred Deep Networks", "author": ["Q. You", "J. Luo", "H. Jin", "J. Yang"], "venue": "the Twenty-Ninth AAAI Conference on Artificial Intelligence (AAAI).", "citeRegEx": "You et al\\.,? 2015", "shortCiteRegEx": "You et al\\.", "year": 2015}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "Advances in neural information processing systems (pp. 10971105).", "citeRegEx": "Krizhevsky et al\\.,? 2012", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Deep networks for saliency detection via local estimation and global search", "author": ["L. Wang", "H. Lu", "X. Ruan", "M.H. Yang"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (pp. 3183-3192).", "citeRegEx": "Wang et al\\.,? 2015", "shortCiteRegEx": "Wang et al\\.", "year": 2015}, {"title": "Co-saliency detection via looking deep and wide", "author": ["D. Zhang", "J. Han", "C. Li", "J. Wang"], "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 29943002).", "citeRegEx": "Zhang et al\\.,? 2015", "shortCiteRegEx": "Zhang et al\\.", "year": 2015}, {"title": "Effects of attention and emotion on face processing in the human brain: an event-related fMRI study", "author": ["P. Vuilleumier", "J.L. Armony", "J. Driver", "R.J. Dolan"], "venue": "Neuron, 30(3), 829-841.", "citeRegEx": "Vuilleumier et al\\.,? 2001", "shortCiteRegEx": "Vuilleumier et al\\.", "year": 2001}, {"title": "How does a brain build a cognitive code", "author": ["S. Grossberg"], "venue": "Studies of mind and brain (pp. 1-52). Springer Netherlands.", "citeRegEx": "Grossberg,? 1982", "shortCiteRegEx": "Grossberg", "year": 1982}, {"title": "Do threatening stimuli draw or hold visual attention in subclinical anxiety", "author": ["E. Fox", "R. Russo", "R. Bowles", "K. Dutton"], "venue": "Journal of Experimental Psychology: General, 130(4), 681.", "citeRegEx": "Fox et al\\.,? 2001", "shortCiteRegEx": "Fox et al\\.", "year": 2001}, {"title": "Discovering affective regions in deep convolutional neural networks for visual sentiment prediction", "author": ["M. Sun", "J. Yang", "K. Wang", "Shen. H"], "venue": "In IEEE International Conference on Multimedia and Expo (ICME) (pp. 1-6)", "citeRegEx": "Sun et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Sun et al\\.", "year": 2016}, {"title": "What is an object", "author": ["B. Alexe", "T. Deselaers", "V. Ferrari"], "venue": "Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference on (pp. 73-80). IEEE.", "citeRegEx": "Alexe et al\\.,? 2010", "shortCiteRegEx": "Alexe et al\\.", "year": 2010}, {"title": "Where do emotions come from? Predicting the Emotion Stimuli Map", "author": ["K.C. Peng", "A. Sadovnik", "A. Gallagher", "T. Chen"], "venue": "In IEEE International Conference on Image Processing (ICIP) (pp. 614-618)", "citeRegEx": "Peng et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Peng et al\\.", "year": 2016}, {"title": "Sentribute: image sentiment analysis from a mid-level perspective", "author": ["J. Yuan", "S. Mcdonough", "Q. You", "J. Luo"], "venue": "Proceedings of the Second International Workshop on Issues of Sentiment Discovery and Opinion Mining (p. 10). ACM.", "citeRegEx": "Yuan et al\\.,? 2013", "shortCiteRegEx": "Yuan et al\\.", "year": 2013}, {"title": "Object Detectors Emerge in Deep Scene CNNs", "author": ["B. Zhou", "A. Khosla", "A. Lapedriza", "A. Oliva", "A. Torralba"], "venue": "International Conference on Learning Representations (ICLR)", "citeRegEx": "Zhou et al\\.,? 2015", "shortCiteRegEx": "Zhou et al\\.", "year": 2015}, {"title": "Unconstrained salient object detection via proposal subset optimization", "author": ["J. Zhang", "S. Sclaroff", "Z. Lin", "X. Shen", "B. Price", "R. Mech"], "venue": "Proceeding of IEEE Conf. on Computer Vision and Pattern Recognition (CVPR).", "citeRegEx": "Zhang et al\\.,? 2016", "shortCiteRegEx": "Zhang et al\\.", "year": 2016}, {"title": "From Pixels to Sentiment: Fine-tuning CNNs for Visual Sentiment Prediction", "author": ["V. Campos", "B. Jou", "X. Giro-i-Nieto"], "venue": "arXiv preprint arXiv:1604.03489.", "citeRegEx": "Campos et al\\.,? 2016", "shortCiteRegEx": "Campos et al\\.", "year": 2016}, {"title": "Building a large scale dataset for image emotion recognition: The fine print and the benchmark", "author": ["Q. You", "J. Luo", "H. Jin", "J. Yang"], "venue": "the Thirtieth AAAI Conference on Artificial Intelligence (AAAI).", "citeRegEx": "You et al\\.,? 2016", "shortCiteRegEx": "You et al\\.", "year": 2016}, {"title": "Caffe: Convolutional architecture for fast feature embedding", "author": ["Y. Jia", "E. Shelhamer", "J. Donahue", "S. Karayev", "J. Long", "R. Girshick", "S. Guadarrama", "T. Darrell"], "venue": "Proceedings of the 22nd ACM international conference on Multimedia (pp. 675-678). ACM.", "citeRegEx": "Jia et al\\.,? 2014", "shortCiteRegEx": "Jia et al\\.", "year": 2014}, {"title": "Sun attribute database: Discovering, annotating, and recognizing scene attributes", "author": ["G. Patterson", "J. Hays"], "venue": "Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on (pp. 2751-2758). IEEE.", "citeRegEx": "Patterson and Hays,? 2012", "shortCiteRegEx": "Patterson and Hays", "year": 2012}], "referenceMentions": [{"referenceID": 0, "context": ", pixel level features such as color, texture and composition (Machajdik and Hanbury 2010), were designed to study the emotional reaction to visual content.", "startOffset": 62, "endOffset": 90}, {"referenceID": 1, "context": "Similar methods were pervasive for a while until recently, when large scale image datasets such as ImageNet (Deng et al. 2009) and Places Dataset (Zhou et al.", "startOffset": 108, "endOffset": 126}, {"referenceID": 2, "context": "2009) and Places Dataset (Zhou et al. 2014) became available.", "startOffset": 25, "endOffset": 43}, {"referenceID": 4, "context": "The power of deep learning, especially convolutional neural networks, has been harnessed recently to discover the sentiment carried in images (You et al. 2015; Jindal and Singh 2015).", "startOffset": 142, "endOffset": 182}, {"referenceID": 3, "context": "The power of deep learning, especially convolutional neural networks, has been harnessed recently to discover the sentiment carried in images (You et al. 2015; Jindal and Singh 2015).", "startOffset": 142, "endOffset": 182}, {"referenceID": 2, "context": "Recently, inspired by convolutional neural network\u2019s state-of-the-art performance on various computer vision tasks such as image classification (Krizhevsky, Sutskever, and Hinton 2012) and scene recognition (Zhou et al. 2014), researchers start to utilize CNN to capture high-level visual concepts and produce saliency models with better detection performance (Wang et al.", "startOffset": 207, "endOffset": 225}, {"referenceID": 6, "context": "2014), researchers start to utilize CNN to capture high-level visual concepts and produce saliency models with better detection performance (Wang et al. 2015; Zhang et al. 2015).", "startOffset": 140, "endOffset": 177}, {"referenceID": 7, "context": "2014), researchers start to utilize CNN to capture high-level visual concepts and produce saliency models with better detection performance (Wang et al. 2015; Zhang et al. 2015).", "startOffset": 140, "endOffset": 177}, {"referenceID": 8, "context": "There is an ongoing discrepancy between those who suggest that emotional perception is automatic, namely in the manner that it is independent of top-down factors such as attention (Vuilleumier et al. 2001), and those illustrating the dependence on attention (Grossberg, 1980; Fox et al.", "startOffset": 180, "endOffset": 205}, {"referenceID": 10, "context": "2001), and those illustrating the dependence on attention (Grossberg, 1980; Fox et al. 2001).", "startOffset": 58, "endOffset": 92}, {"referenceID": 2, "context": "(2015)\u2019s work, they learn deep features for images from the Places Dataset (Zhou et al. 2014) and are able to generate receptive field of the image for scene recognition task.", "startOffset": 75, "endOffset": 93}, {"referenceID": 10, "context": "Sun et al. (2016) proposed the notion of AR, Affective Region, which is a specific region in the image that contains one or more salient objects that can attract viewers\u2019 attention and carry significant emotion.", "startOffset": 0, "endOffset": 18}, {"referenceID": 10, "context": "Sun et al. (2016) proposed the notion of AR, Affective Region, which is a specific region in the image that contains one or more salient objects that can attract viewers\u2019 attention and carry significant emotion. In their work, an offthe-shelf object detection algorithm (Alexe, Deselaers and Ferrari 2010) is adopted to generate random proposals, and then the sentiment of both the proposals and the entire image will be evaluated to discover affective regions. However, this method utilizes explicit object recognition, which has to generate thousands of proposal windows in order to yield to a high recall rate of affective regions proposals. It also incurs a high computational overhead and may result in regions containing tiny objects that do not even catch people\u2019s attention. In Peng et al. (2016)\u2019s work, they try to predict an Emotion Stimuli Map (ESM), which describes pixels-wise contribution to emotion.", "startOffset": 0, "endOffset": 805}, {"referenceID": 10, "context": "Sun et al. (2016) proposed the notion of AR, Affective Region, which is a specific region in the image that contains one or more salient objects that can attract viewers\u2019 attention and carry significant emotion. In their work, an offthe-shelf object detection algorithm (Alexe, Deselaers and Ferrari 2010) is adopted to generate random proposals, and then the sentiment of both the proposals and the entire image will be evaluated to discover affective regions. However, this method utilizes explicit object recognition, which has to generate thousands of proposal windows in order to yield to a high recall rate of affective regions proposals. It also incurs a high computational overhead and may result in regions containing tiny objects that do not even catch people\u2019s attention. In Peng et al. (2016)\u2019s work, they try to predict an Emotion Stimuli Map (ESM), which describes pixels-wise contribution to emotion. They use a pre-trained and fine-tuned model to predict ESM and conclude that neither saliency nor objectness can correctly predict the image regions that evoke emotion. However, they fail to justify the choice of a dataset that consists of predominantly landscape scenes, and does not give any insight on why the saliency detection method does not work for detecting affective emotion region of such images. Yuan et al. (2013) extract scene descriptor low-level features from the SUN Dataset (Xiao et al.", "startOffset": 0, "endOffset": 1343}, {"referenceID": 2, "context": "In Zhou et al. (2015)\u2019s work, they learn deep features for images from the Places Dataset (Zhou et al.", "startOffset": 3, "endOffset": 22}, {"referenceID": 16, "context": "Details of the steps are as follows: Step 1: For a given image, we use a state-of-the-art saliency detection model (Zhang et al. 2016) to detect salient object(s), if any, for each image.", "startOffset": 115, "endOffset": 134}, {"referenceID": 4, "context": "Dataset We use the data set released by You et al. (2016) for visual sentiment prediction.", "startOffset": 40, "endOffset": 58}, {"referenceID": 16, "context": "We adopt a state-of-the-art saliency detection model (Zhang et al. 2016) to detect whether an image has any salient object and if there is, locate those salient objects.", "startOffset": 53, "endOffset": 72}, {"referenceID": 2, "context": "For each partition we obtain from the previous step, we fine-tune a state of the art scene recognition model, PlacesCNN (Zhou et al. 2014), to further classify it into indoors and outdoors.", "startOffset": 120, "endOffset": 138}, {"referenceID": 19, "context": "This model has the same architecture as the one used in the Caffe reference network (Jia et al. 2014), and it is trained on the Places Database (Zhou et al.", "startOffset": 84, "endOffset": 101}, {"referenceID": 2, "context": "2014), and it is trained on the Places Database (Zhou et al. 2014), which is a benchmark for scene recognition.", "startOffset": 48, "endOffset": 66}, {"referenceID": 20, "context": "Next, we use the deep features from the last fully connected layers to detect 102 SUN scene attributes (Patterson and Hays 2012).", "startOffset": 103, "endOffset": 128}], "year": 2016, "abstractText": "Sentiment analysis is crucial for extracting social signals from social media content. Due to the prevalence of images in social media, image sentiment analysis is receiving increasing attention in recent years. However, most existing systems are black-boxes that do not provide insight on how image content invokes sentiment and emotion in the viewers. Psychological studies have confirmed that salient objects in an image often invoke emotions. In this work, we investigate more fine-grained and more comprehensive interaction between visual saliency and visual sentiment. In particular, we partition images in several primary scene-type dimensions, including: open-closed, natural-manmade, indooroutdoor, and face-noface. Using state of the art saliency detection algorithm and sentiment classification algorithm, we examine how the sentiment of the salient region(s) in an image relates to the overall sentiment of the image. The experiments on a representative image emotion dataset have shown interesting correlation between saliency and sentiment in different scene types and in turn shed light on the mechanism of visual sentiment evocation.", "creator": "Word"}}}