{"id": "1611.04642", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Nov-2016", "title": "Modeling Large-Scale Structured Relationships with Shared Memory for Knowledge Base Completion", "abstract": "Recent studies on knowledge base completion, the task of recovering missing relationships based on recorded relations, demonstrate the importance of learning embeddings from multi-step relations. However, due to the size of knowledge bases, learning multi-step relations directly on top of observed instances could be costly. In this paper, we propose Implicit ReasoNets (IRNs), which is designed to perform large-scale inference implicitly through a search controller and shared memory. Unlike previous work, IRNs use training data to learn to perform multi-step inference through the shared memory, which is also jointly updated during training. While the inference procedure is not operating on top of observed instances for IRNs, our proposed model outperforms all previous approaches on the popular FB15k benchmark by more than 5.7%.", "histories": [["v1", "Mon, 14 Nov 2016 22:54:45 GMT  (375kb,D)", "http://arxiv.org/abs/1611.04642v1", null], ["v2", "Sat, 22 Apr 2017 19:46:44 GMT  (1802kb,D)", "http://arxiv.org/abs/1611.04642v2", null], ["v3", "Sat, 28 Oct 2017 03:02:10 GMT  (1630kb,D)", "http://arxiv.org/abs/1611.04642v3", null]], "reviews": [], "SUBJECTS": "cs.AI cs.CL cs.LG", "authors": ["yelong shen", "po-sen huang", "ming-wei chang", "jianfeng gao"], "accepted": false, "id": "1611.04642"}, "pdf": {"name": "1611.04642.pdf", "metadata": {"source": "CRF", "title": "IMPLICIT REASONET: MODELING LARGE-SCALE STRUCTURED RELATIONSHIPS WITH SHARED MEM- ORY", "authors": ["Yelong Shen", "Po-Sen Huang", "Ming-Wei Chang", "Jianfeng Gao"], "emails": ["yeshen@microsoft.com", "pshuang@microsoft.com", "minchang@microsoft.com", "jfgao@microsoft.com"], "sections": [{"heading": "1 INTRODUCTION", "text": "In fact, it is so that most of them are able to survive themselves, and that they are able to survive themselves, \"he said in an interview with the\" New York Times, \"in which he said the role of the\" New York Times, \"the\" New York Times, \"the\" New York Times, \"the\" New York Times, \"the\" New York Times, \"the\" New York Times, \"the\" New York Times, \"the\" New York Times, \"the\" New York Times, \"the\" New York Times, \"the\" New York Times, \"the\" New York Times, \"the\" New York Times, \"the\" New York Times, \"the\" New York Times, \"the\" New York Times, \"the\" the \"New York Times,\" the \"the\" New York Times, \"the\" the \"the\" New York Times, \"the\" the \"New York Times,\" the \"the\" New York Times, \"the\" the \"New York Times,\" the \"the\" New York Times, \"the\" the \"the\" New York Times, \"the\" the \"the\" New York Times, \"the\" the \"the\" the \"New York Times,\" the \"the\" the \"the\" New York Times, \"the\" the \"the\" the \"New York Times,\" the \"the\" the \"the\" the \"New York Times,\" the \"the\" the \"the\" the \"New York Times,\" the \"the\" the \"the\" the \"the\" New York Times, \"the\" the \"the\" the \"the\" New York Times, \"the\" the \"the\" the \"New York Times,\" the \"the\" the \"the\" New York Times, \"the\" the \"the\" New York Times, \"the\" the \"the\" the \"New York Times,\" the \"the\" the \"the\" New York Times, the \"the\" the \"the\" the \"New York Times,\" the \"New York Times,\" the \"the\" the \"the\" New York Times, the \"the\" the \"the\" the \"the\" the \"New York Times,\" the \"New York Times,\" the \"the\" the \"the\" the \"New York Times,\" the \"the\" the \"the\" the \"the\" the \"New York Times, the\" the \"the\" New York Times"}, {"heading": "2 REASONET FOR IMPLICIT INFERENCE", "text": "In this section, we describe the general architecture of IRNs in a way that is agnostic to KBC = internal search is shared. IRNs consist of four main components: an input component, an output component, a shared memory, and a search controller as shown in Figure 1. In this section, we briefly describe each component / output module: These two modules are task dependent. The input module takes a query and converts the query into a vector representation q. The output module is a function fo that converts the hidden state obtained from the search controller (s) into an output O. We optimize the output model using the output prediction O with respect to a basic truth goal. Shared Memory: The shared memory is referred to as M. It consists of a list of memory vectors, M = {mi} i = 1... I, where mi is a fixed dimensional vector."}, {"heading": "2.1 STOCHASTIC INFERENCE PROCESS", "text": "The conclusion of an IRN is as follows: First, the model converts a task-dependent input into a vector representation by the input module; second, the model uses the input representation to initialize the search controller. At each time step, the search controller determines whether the process will be terminated by sampling from the distribution to the terminate gate. If the result is the termination, the output module will generate a task-dependent prediction based on the search controller states. If the result is continued, the search controller will move on to the next time step and generate an attention vector based on the current search controller state and shared memory. Intuitively, we design an entire process by finding its target through a structure and output of its prediction when a satisfactory answer is found. The detailed inference process is considered in Algorithm 1.The Inference Process of an IRN as a partially observable Process Decision POision (MDP)."}, {"heading": "4 EXPERIMENTAL RESULTS", "text": "In fact, most of us have no idea what we're going to do. (...) Most of us have no idea what we're going to do. (...) Most of us have no idea what we're going to do. (...) Most of us have no idea what we're going to do. (...) Most of us have no idea what we're going to do. (...) Most of us have no idea what we're going to do. (...) Most of us have no idea what we're going to do. (...) Most of us have no idea what we're going to do. (...) Most of us have no idea what we're going to do. (...) Most of us have no idea what we're going to do. (...)"}, {"heading": "6 RELATED WORK", "text": "In many studies, the scoring function fr (h, t) is2 In the example, to find the right path, the model must be searched across observed instances \"215 448: 215 \u2192 101 \u2192 448\" and \"76 493: 76 \u2192 101 \u2192 493.\" and the distance of \"140 \u2192 493\" is longer than \"101 \u2192 493\" (there are four shortest paths between 101 \u2192 493 and three shortest paths between 140 \u2192 493 in training."}, {"heading": "7 CONCLUSION", "text": "In this paper, we propose implicit ReasoNets (IRNs) that infer a shared memory that implicitly models large-scale structured relationships; the follow-up process is guided by a search controller to access the memory that is shared across incidences; we demonstrate and analyze the multi-level inference capability of IRNs in Knowledge Data Completion and Synthesis tasks in the shortest possible time; our model, without using explicit Knowledge Database Information in the inference process, outperforms all previous approaches to the popular FB15k benchmark by more than 5.7%; for future work, we aim to expand IRNs in two ways: first, inspired by Ribeiro et al. (2016), we would like to develop techniques to use ways of generating intelligible argumentation interpretations from shared memory; second, we plan to apply IRNs to unstructured language relationships."}, {"heading": "ACKNOWLEDGMENTS", "text": "We thank Scott Wen-Tau Yih, Kristina Toutanova, Jian Tang and Zachary Lipton for their thoughtful feedback and discussions."}, {"heading": "A DETAILS OF THE GRAPH CONSTRUCTION FOR THE SHORTEST PATH SYNTHESIS TASK", "text": "We construct the underlying graph as follows: On a three-dimensional unit sphere, we randomly create a set of nodes. For each node, we connect its K-nearest neighbors and use the Euclidean distance between two nodes to construct a graph. We randomly create two nodes and calculate the shortest path when it is connected between these two nodes. Since all subways within a shortest path are shortest paths, we create the data set incrementally and remove the instances that are a partial path of already selected paths or a superset of already selected paths. In this case, all the shortest paths cannot be answered by copying directly from another instance. Furthermore, all weights are hidden in the graph and not displayed in the training data, which increases the difficulty of the tasks. We set k = 50 as the default value."}], "references": [{"title": "Semantic parsing on Freebase from questionanswer pairs", "author": ["Jonathan Berant", "Andrew Chou", "Roy Frostig", "Percy Liang"], "venue": "In Proceedings of EMNLP,", "citeRegEx": "Berant et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Berant et al\\.", "year": 2013}, {"title": "Freebase: A collaboratively created graph database for structuring human knowledge", "author": ["Kurt Bollacker", "Colin Evans", "Praveen Paritosh", "Tim Sturge", "Jamie Taylor"], "venue": "In Proceedings of SIGMOD-08,", "citeRegEx": "Bollacker et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Bollacker et al\\.", "year": 2008}, {"title": "Learning structured embeddings of knowledge bases", "author": ["Antoine Bordes", "Jason Weston", "Ronan Collobert", "Yoshua Bengio"], "venue": "In Proceedings of the Twenty-Fifth AAAI Conference on Artificial Intelligence,", "citeRegEx": "Bordes et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Bordes et al\\.", "year": 2011}, {"title": "Translating embeddings for modeling multi-relational data", "author": ["Antoine Bordes", "Nicolas Usunier", "Alberto Garcia-Duran", "Jason Weston", "Oksana Yakhnenko"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Bordes et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bordes et al\\.", "year": 2013}, {"title": "A semantic matching energy function for learning with multi-relational data", "author": ["Antoine Bordes", "Xavier Glorot", "Jason Weston", "Yoshua Bengio"], "venue": "Machine Learning,", "citeRegEx": "Bordes et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bordes et al\\.", "year": 2014}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "author": ["Kyunghyun Cho", "Bart Van Merri\u00ebnboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1406.1078,", "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "WordNet: An Electronic Lexical Database", "author": ["C. Fellbaum"], "venue": null, "citeRegEx": "Fellbaum.,? \\Q1998\\E", "shortCiteRegEx": "Fellbaum.", "year": 1998}, {"title": "Composing relationships with translations", "author": ["Alberto Garc\u00eda-Dur\u00e1n", "Antoine Bordes", "Nicolas Usunier"], "venue": "In EMNLP, pp", "citeRegEx": "Garc\u00eda.Dur\u00e1n et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Garc\u00eda.Dur\u00e1n et al\\.", "year": 2015}, {"title": "Combining two and three-way embeddings models for link prediction in knowledge", "author": ["Alberto Garc\u00eda-Dur\u00e1n", "Antoine Bordes", "Nicolas Usunier", "Yves Grandvalet"], "venue": "bases. CoRR,", "citeRegEx": "Garc\u00eda.Dur\u00e1n et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Garc\u00eda.Dur\u00e1n et al\\.", "year": 2015}, {"title": "Neural turing machines", "author": ["Alex Graves", "Greg Wayne", "Ivo Danihelka"], "venue": "arXiv preprint arXiv:1410.5401,", "citeRegEx": "Graves et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2014}, {"title": "Hybrid computing using a neural network with dynamic external memory", "author": ["Alex Graves", "Greg Wayne", "Malcolm Reynolds", "Tim Harley", "Ivo Danihelka", "Agnieszka Grabska-Barwi\u0144ska", "Sergio G\u00f3mez Colmenarejo", "Edward Grefenstette", "Tiago Ramalho", "John Agapiou"], "venue": null, "citeRegEx": "Graves et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2016}, {"title": "Traversing knowledge graphs in vector space", "author": ["Kelvin Guu", "John Miller", "Percy Liang"], "venue": "arXiv preprint arXiv:1506.01094,", "citeRegEx": "Guu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Guu et al\\.", "year": 2015}, {"title": "Learning to represent knowledge graphs with gaussian embedding", "author": ["Shizhu He", "Kang Liu", "Guoliang Ji", "Jun Zhao"], "venue": "In Proceedings of the 24th ACM International on Conference on Information and Knowledge Management,", "citeRegEx": "He et al\\.,? \\Q2015\\E", "shortCiteRegEx": "He et al\\.", "year": 2015}, {"title": "Knowledge graph embedding via dynamic mapping matrix", "author": ["Guoliang Ji", "Shizhu He", "Liheng Xu", "Kang Liu", "Jun Zhao"], "venue": "In ACL,", "citeRegEx": "Ji et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ji et al\\.", "year": 2015}, {"title": "Planning and acting in partially observable stochastic domains", "author": ["Leslie Pack Kaelbling", "Michael L. Littman", "Anthony R. Cassandra"], "venue": "Artificial Intelligence,", "citeRegEx": "Kaelbling et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Kaelbling et al\\.", "year": 1998}, {"title": "Modeling relation paths for representation learning of knowledge bases", "author": ["Yankai Lin", "Zhiyuan Liu", "Huanbo Luan", "Maosong Sun", "Siwei Rao", "Song Liu"], "venue": "In Proceedings of the Conference on Empirical Methods for Natural Language Processing (EMNLP),", "citeRegEx": "Lin et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2015}, {"title": "Learning entity and relation embeddings for knowledge graph completion", "author": ["Yankai Lin", "Zhiyuan Liu", "Maosong Sun", "Yang Liu", "Xuan Zhu"], "venue": "In Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence,", "citeRegEx": "Lin et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2015}, {"title": "Distant supervision for relation extraction without labeled data", "author": ["Mike Mintz", "Steven Bills", "Rion Snow", "Daniel Jurafsky"], "venue": "In Proceedings of ACL-IJCNLP-09,", "citeRegEx": "Mintz et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Mintz et al\\.", "year": 2009}, {"title": "STransE: a novel embedding model of entities and relationships in knowledge bases", "author": ["Dat Quoc Nguyen", "Kairit Sirts", "Lizhen Qu", "Mark Johnson"], "venue": "In NAACL,", "citeRegEx": "Nguyen et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Nguyen et al\\.", "year": 2016}, {"title": "A three-way model for collective learning on multi-relational data", "author": ["Maximilian Nickel", "Volker Tresp", "Hans-Peter Kriegel"], "venue": "In Proceedings of the 28th international conference on machine learning", "citeRegEx": "Nickel et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Nickel et al\\.", "year": 2011}, {"title": "why should i trust you?\": Explaining the predictions of any classifier", "author": ["Marco Tulio Ribeiro", "Sameer Singh", "Carlos Guestrin"], "venue": null, "citeRegEx": "Ribeiro et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Ribeiro et al\\.", "year": 2016}, {"title": "Relation extraction with matrix factorization and universal schemas", "author": ["Sebastian Riedel", "Limin Yao", "Andrew McCallum", "Benjamin M. Marlin"], "venue": "In HLT-NAACL,", "citeRegEx": "Riedel et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Riedel et al\\.", "year": 2013}, {"title": "Reasonet: Learning to stop reading in machine comprehension", "author": ["Yelong Shen", "Po-Sen Huang", "Jianfeng Gao", "Weizhu Chen"], "venue": null, "citeRegEx": "Shen et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Shen et al\\.", "year": 2016}, {"title": "Reasoning With Neural Tensor Networks For Knowledge Base Completion", "author": ["Richard Socher", "Danqi Chen", "Christopher D. Manning", "Andrew Y. Ng"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Socher et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "A neural network approach to context-sensitive generation of conversational responses", "author": ["Alessandro Sordoni", "Michel Galley", "Michael Auli", "Chris Brockett", "Yangfeng Ji", "Margaret Mitchell", "Jian-Yun Nie", "Jianfeng Gao", "Bill Dolan"], "venue": "arXiv preprint arXiv:1506.06714,", "citeRegEx": "Sordoni et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sordoni et al\\.", "year": 2015}, {"title": "Yago: A Core of Semantic Knowledge", "author": ["F.M. Suchanek", "G. Kasneci", "G. Weikum"], "venue": "In WWW,", "citeRegEx": "Suchanek et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Suchanek et al\\.", "year": 2007}, {"title": "End-to-end memory networks", "author": ["Sainbayar Sukhbaatar", "Jason Weston", "Rob Fergus"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Sukhbaatar et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sukhbaatar et al\\.", "year": 2015}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V Le"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Observed versus latent features for knowledge base and text inference", "author": ["Kristina Toutanova", "Danqi Chen"], "venue": "In Proceedings of the 3rd Workshop on Continuous Vector Space Models and their Compositionality,", "citeRegEx": "Toutanova and Chen.,? \\Q2015\\E", "shortCiteRegEx": "Toutanova and Chen.", "year": 2015}, {"title": "Representing text for joint embedding of text and knowledge bases", "author": ["Kristina Toutanova", "Danqi Chen", "Patrick Pantel", "Hoifung Poon", "Pallavi Choudhury", "Michael Gamon"], "venue": "In EMNLP,", "citeRegEx": "Toutanova et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Toutanova et al\\.", "year": 2015}, {"title": "Compositional learning of embeddings for relation paths in knowledge bases and text", "author": ["Kristina Toutanova", "Xi Victoria Lin", "Scott Wen tau Yih", "Hoifung Poon", "Chris Quirk"], "venue": null, "citeRegEx": "Toutanova et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Toutanova et al\\.", "year": 2016}, {"title": "Knowledge graph embedding by translating on hyperplanes", "author": ["Zhen Wang", "Jianwen Zhang", "Jianlin Feng", "Zheng Chen"], "venue": "In Proceedings of the Twenty-Eighth AAAI Conference on Artificial Intelligence,", "citeRegEx": "Wang et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2014}, {"title": "Mining inference formulas by goal-directed random walks", "author": ["Zhuoyu Wei", "Jun Zhao", "Kang Liu"], "venue": "In EMNLP,", "citeRegEx": "Wei et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Wei et al\\.", "year": 2016}, {"title": "Knowledge base completion via search-based question answering", "author": ["Robert West", "Evgeniy Gabrilovich", "Kevin Murphy", "Shaohua Sun", "Rahul Gupta", "Dekang Lin"], "venue": "In Proceedings of the 23rd international conference on World Wide Web,", "citeRegEx": "West et al\\.,? \\Q2014\\E", "shortCiteRegEx": "West et al\\.", "year": 2014}, {"title": "Simple statistical gradient-following algorithms for connectionist reinforcement learning", "author": ["Ronald J. Williams"], "venue": "Machine Learning,", "citeRegEx": "Williams.,? \\Q1992\\E", "shortCiteRegEx": "Williams.", "year": 1992}, {"title": "Embedding entities and relations for learning and inference in knowledge", "author": ["Bishan Yang", "Wen-tau Yih", "Xiaodong He", "Jianfeng Gao", "Li Deng"], "venue": "bases. CoRR,", "citeRegEx": "Yang et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2014}, {"title": "Semantic parsing via staged query graph generation: Question answering with knowledge base", "author": ["Wen-tau Yih", "Ming-Wei Chang", "Xiaodong He", "Jianfeng Gao"], "venue": "In Proc. of ACL,", "citeRegEx": "Yih et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Yih et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 6, "context": "Knowledge bases such as WordNet (Fellbaum, 1998), Freebase (Bollacker et al.", "startOffset": 32, "endOffset": 48}, {"referenceID": 1, "context": "Knowledge bases such as WordNet (Fellbaum, 1998), Freebase (Bollacker et al., 2008), or Yago (Suchanek et al.", "startOffset": 59, "endOffset": 83}, {"referenceID": 25, "context": ", 2008), or Yago (Suchanek et al., 2007) contain many real-world facts expressed as triples, e.", "startOffset": 17, "endOffset": 40}, {"referenceID": 0, "context": "These knowledge bases are useful for many downstream applications such as question answering (Berant et al., 2013; Yih et al., 2015) and information extraction (Mintz et al.", "startOffset": 93, "endOffset": 132}, {"referenceID": 36, "context": "These knowledge bases are useful for many downstream applications such as question answering (Berant et al., 2013; Yih et al., 2015) and information extraction (Mintz et al.", "startOffset": 93, "endOffset": 132}, {"referenceID": 17, "context": ", 2015) and information extraction (Mintz et al., 2009).", "startOffset": 35, "endOffset": 55}, {"referenceID": 19, "context": "Thus, the knowledge base completion (KBC) task has emerged an important open research problem (Nickel et al., 2011).", "startOffset": 94, "endOffset": 115}, {"referenceID": 11, "context": "By themselves, there is no straightforward way to capture the structured relationships between multiple triples adequately (Guu et al., 2015; Toutanova et al., 2016; Lin et al., 2015a).", "startOffset": 123, "endOffset": 184}, {"referenceID": 30, "context": "By themselves, there is no straightforward way to capture the structured relationships between multiple triples adequately (Guu et al., 2015; Toutanova et al., 2016; Lin et al., 2015a).", "startOffset": 123, "endOffset": 184}, {"referenceID": 0, "context": "These knowledge bases are useful for many downstream applications such as question answering (Berant et al., 2013; Yih et al., 2015) and information extraction (Mintz et al., 2009). However, despite the formidable size of knowledge bases, many important facts are still missing. For example, West et al. (2014) showed that 21% of the 100K most frequent PERSON entities have no recorded nationality in a recent version of Freebase.", "startOffset": 94, "endOffset": 311}, {"referenceID": 0, "context": "These knowledge bases are useful for many downstream applications such as question answering (Berant et al., 2013; Yih et al., 2015) and information extraction (Mintz et al., 2009). However, despite the formidable size of knowledge bases, many important facts are still missing. For example, West et al. (2014) showed that 21% of the 100K most frequent PERSON entities have no recorded nationality in a recent version of Freebase. We seek to infer unknown relations based on the observed triples. Thus, the knowledge base completion (KBC) task has emerged an important open research problem (Nickel et al., 2011). Neural-network based methods have been very popular for solving the KBC task. Following Bordes et al. (2013), one of the most popular approaches for KBC is to learn vector-space representations of entities and relations during training, and then apply linear or bi-linear operations to infer the missing relations at test time.", "startOffset": 94, "endOffset": 723}, {"referenceID": 0, "context": "These knowledge bases are useful for many downstream applications such as question answering (Berant et al., 2013; Yih et al., 2015) and information extraction (Mintz et al., 2009). However, despite the formidable size of knowledge bases, many important facts are still missing. For example, West et al. (2014) showed that 21% of the 100K most frequent PERSON entities have no recorded nationality in a recent version of Freebase. We seek to infer unknown relations based on the observed triples. Thus, the knowledge base completion (KBC) task has emerged an important open research problem (Nickel et al., 2011). Neural-network based methods have been very popular for solving the KBC task. Following Bordes et al. (2013), one of the most popular approaches for KBC is to learn vector-space representations of entities and relations during training, and then apply linear or bi-linear operations to infer the missing relations at test time. However, several recent papers demonstrate limitations of prior approaches relying upon vector-space models alone. By themselves, there is no straightforward way to capture the structured relationships between multiple triples adequately (Guu et al., 2015; Toutanova et al., 2016; Lin et al., 2015a). For example, assume that we want to fill in the missing relation for the triple (Obama, NATIONALITY, ?), a multi-step search procedure might be needed to discover the evidence in the observed triples such as (Obama, BORNIN, Hawaii) and (Hawaii, PARTOF, U.S.A). To address this issue, Guu et al. (2015); Toutanova et al.", "startOffset": 94, "endOffset": 1545}, {"referenceID": 0, "context": "These knowledge bases are useful for many downstream applications such as question answering (Berant et al., 2013; Yih et al., 2015) and information extraction (Mintz et al., 2009). However, despite the formidable size of knowledge bases, many important facts are still missing. For example, West et al. (2014) showed that 21% of the 100K most frequent PERSON entities have no recorded nationality in a recent version of Freebase. We seek to infer unknown relations based on the observed triples. Thus, the knowledge base completion (KBC) task has emerged an important open research problem (Nickel et al., 2011). Neural-network based methods have been very popular for solving the KBC task. Following Bordes et al. (2013), one of the most popular approaches for KBC is to learn vector-space representations of entities and relations during training, and then apply linear or bi-linear operations to infer the missing relations at test time. However, several recent papers demonstrate limitations of prior approaches relying upon vector-space models alone. By themselves, there is no straightforward way to capture the structured relationships between multiple triples adequately (Guu et al., 2015; Toutanova et al., 2016; Lin et al., 2015a). For example, assume that we want to fill in the missing relation for the triple (Obama, NATIONALITY, ?), a multi-step search procedure might be needed to discover the evidence in the observed triples such as (Obama, BORNIN, Hawaii) and (Hawaii, PARTOF, U.S.A). To address this issue, Guu et al. (2015); Toutanova et al. (2016); Lin et al.", "startOffset": 94, "endOffset": 1570}, {"referenceID": 0, "context": "These knowledge bases are useful for many downstream applications such as question answering (Berant et al., 2013; Yih et al., 2015) and information extraction (Mintz et al., 2009). However, despite the formidable size of knowledge bases, many important facts are still missing. For example, West et al. (2014) showed that 21% of the 100K most frequent PERSON entities have no recorded nationality in a recent version of Freebase. We seek to infer unknown relations based on the observed triples. Thus, the knowledge base completion (KBC) task has emerged an important open research problem (Nickel et al., 2011). Neural-network based methods have been very popular for solving the KBC task. Following Bordes et al. (2013), one of the most popular approaches for KBC is to learn vector-space representations of entities and relations during training, and then apply linear or bi-linear operations to infer the missing relations at test time. However, several recent papers demonstrate limitations of prior approaches relying upon vector-space models alone. By themselves, there is no straightforward way to capture the structured relationships between multiple triples adequately (Guu et al., 2015; Toutanova et al., 2016; Lin et al., 2015a). For example, assume that we want to fill in the missing relation for the triple (Obama, NATIONALITY, ?), a multi-step search procedure might be needed to discover the evidence in the observed triples such as (Obama, BORNIN, Hawaii) and (Hawaii, PARTOF, U.S.A). To address this issue, Guu et al. (2015); Toutanova et al. (2016); Lin et al. (2015a) propose different approaches of injecting structured information by directly operating on the observed triplets.", "startOffset": 94, "endOffset": 1590}, {"referenceID": 26, "context": "Compared IRNs to Memory Networks (MemNN) (Weston et al., 2014; Sukhbaatar et al., 2015) and Neural Turing Machines (NTM) (Graves et al.", "startOffset": 41, "endOffset": 87}, {"referenceID": 9, "context": ", 2015) and Neural Turing Machines (NTM) (Graves et al., 2014; 2016), the biggest difference between our model and the existing frameworks is the search controller and the use of the shared memory.", "startOffset": 41, "endOffset": 68}, {"referenceID": 22, "context": "We build upon our previous work (Shen et al., 2016) for using a search controller module to dynamically perform a multi-step inference depending on the complexity of the instance.", "startOffset": 32, "endOffset": 51}, {"referenceID": 9, "context": ", 2015) and Neural Turing Machines (NTM) (Graves et al., 2014; 2016), the biggest difference between our model and the existing frameworks is the search controller and the use of the shared memory. We build upon our previous work (Shen et al., 2016) for using a search controller module to dynamically perform a multi-step inference depending on the complexity of the instance. MemNN and NTM explicitly store inputs (such as graph definition, supporting facts) in the memory. In contrast, in IRNs, we do not explicitly store all the observed inputs in the shared memory. Instead, we directly operate on the shared memory, which modeling the structured relationships implicitly. We randomly initialize the memory and update the memory with respect to task-specific objectives. The idea of exploiting shared memory is proposed by Munkhdalai & Yu (2016) independently.", "startOffset": 42, "endOffset": 851}, {"referenceID": 9, "context": ", 2015) and Neural Turing Machines (NTM) (Graves et al., 2014; 2016), the biggest difference between our model and the existing frameworks is the search controller and the use of the shared memory. We build upon our previous work (Shen et al., 2016) for using a search controller module to dynamically perform a multi-step inference depending on the complexity of the instance. MemNN and NTM explicitly store inputs (such as graph definition, supporting facts) in the memory. In contrast, in IRNs, we do not explicitly store all the observed inputs in the shared memory. Instead, we directly operate on the shared memory, which modeling the structured relationships implicitly. We randomly initialize the memory and update the memory with respect to task-specific objectives. The idea of exploiting shared memory is proposed by Munkhdalai & Yu (2016) independently. Despite of using the same term, the goal and the operations used by IRNs are different from the one used in Munkhdalai & Yu (2016), as IRNs allow the model to perform multi-step for each instance dynamically.", "startOffset": 42, "endOffset": 997}, {"referenceID": 14, "context": "The inference process of an IRN is considered as a Partially Observable Markov Decision Process (POMDP) (Kaelbling et al., 1998) in the reinforcement learning (RL) literature.", "startOffset": 104, "endOffset": 128}, {"referenceID": 22, "context": "We employ the approach from our previous work (Shen et al., 2016), REINFORCE (Williams, 1992) based Contrastive Reward method, to maximize the expected reward.", "startOffset": 46, "endOffset": 65}, {"referenceID": 34, "context": ", 2016), REINFORCE (Williams, 1992) based Contrastive Reward method, to maximize the expected reward.", "startOffset": 19, "endOffset": 35}, {"referenceID": 3, "context": "The goal of KBC tasks (Bordes et al., 2013) is to predict a head or a tail entity given the relation type and the other entity, i.", "startOffset": 22, "endOffset": 43}, {"referenceID": 3, "context": "In this section, we evaluate the performance of our model on the benchmark FB15k and WN18 datasets for KBC tasks (Bordes et al., 2013).", "startOffset": 113, "endOffset": 134}, {"referenceID": 7, "context": "For example, RTransE (Garc\u00eda-Dur\u00e1n et al., 2015) and PTransE (Lin et al.", "startOffset": 21, "endOffset": 48}, {"referenceID": 3, "context": ", 2015a) models are extensions of the TransE (Bordes et al., 2013) model by explicitly exploring multi-step relations in the knowledge base to regularize the trained embeddings.", "startOffset": 45, "endOffset": 66}, {"referenceID": 29, "context": "The NLFeat model (Toutanova et al., 2015) is a log-linear model that makes use of simple node and link features.", "startOffset": 17, "endOffset": 41}, {"referenceID": 2, "context": "In this section, we evaluate the performance of our model on the benchmark FB15k and WN18 datasets for KBC tasks (Bordes et al., 2013). These datasets contain multi-relations between head and tail entities. Given a head entity and a relation, the model produces a ranked list of the entities according to the score of the entity being the tail entity of this triple. To evaluate the ranking, we report mean rank (MR), the mean of rank of the correct entity across the test examples, and hits@10, the proportion of correct entities ranked in the top-10 predictions. Lower MR or higher hits@10 indicates a better prediction performance. We follow the evaluation protocol in Bordes et al. (2013) to report filtered results, where negative examples N are removed from the dataset.", "startOffset": 114, "endOffset": 693}, {"referenceID": 2, "context": "In this section, we evaluate the performance of our model on the benchmark FB15k and WN18 datasets for KBC tasks (Bordes et al., 2013). These datasets contain multi-relations between head and tail entities. Given a head entity and a relation, the model produces a ranked list of the entities according to the score of the entity being the tail entity of this triple. To evaluate the ranking, we report mean rank (MR), the mean of rank of the correct entity across the test examples, and hits@10, the proportion of correct entities ranked in the top-10 predictions. Lower MR or higher hits@10 indicates a better prediction performance. We follow the evaluation protocol in Bordes et al. (2013) to report filtered results, where negative examples N are removed from the dataset. In this case, we can avoid some negative examples being valid and ranked above the target triplet. We use the same hyper-parameters of our model for both FB15k and WN18 datasets. Entity embeddings (which are not shared between input and output modules) and relation embedding are both 100-dimensions. We use the input module and output module to encode subject and object entities, respectively. There are 64 memory vectors with 200 dimensions each, initialized by random vectors with unit L2-norm. We use single-layer GRU with 200 cells as the search controller. We set the maximum inference step of the IRN to 5. We randomly initialize all model parameters, and use SGD as the training algorithm with mini-batch size of 64. We set the learning rate to a constant number, 0.01. To prevent the model from learning a trivial solution by increasing entity embeddings norms, we follow Bordes et al. (2013) to enforce the L2-norm of the entity embeddings as 1.", "startOffset": 114, "endOffset": 1680}, {"referenceID": 2, "context": "In this section, we evaluate the performance of our model on the benchmark FB15k and WN18 datasets for KBC tasks (Bordes et al., 2013). These datasets contain multi-relations between head and tail entities. Given a head entity and a relation, the model produces a ranked list of the entities according to the score of the entity being the tail entity of this triple. To evaluate the ranking, we report mean rank (MR), the mean of rank of the correct entity across the test examples, and hits@10, the proportion of correct entities ranked in the top-10 predictions. Lower MR or higher hits@10 indicates a better prediction performance. We follow the evaluation protocol in Bordes et al. (2013) to report filtered results, where negative examples N are removed from the dataset. In this case, we can avoid some negative examples being valid and ranked above the target triplet. We use the same hyper-parameters of our model for both FB15k and WN18 datasets. Entity embeddings (which are not shared between input and output modules) and relation embedding are both 100-dimensions. We use the input module and output module to encode subject and object entities, respectively. There are 64 memory vectors with 200 dimensions each, initialized by random vectors with unit L2-norm. We use single-layer GRU with 200 cells as the search controller. We set the maximum inference step of the IRN to 5. We randomly initialize all model parameters, and use SGD as the training algorithm with mini-batch size of 64. We set the learning rate to a constant number, 0.01. To prevent the model from learning a trivial solution by increasing entity embeddings norms, we follow Bordes et al. (2013) to enforce the L2-norm of the entity embeddings as 1. We use hits@10 as the validation metric for the IRN. Following the work (Lin et al., 2015a), we add reverse relations into the training triplet set to increase the training data. Following Nguyen et al. (2016), we divide the results of previous work into two groups.", "startOffset": 114, "endOffset": 1944}, {"referenceID": 2, "context": "In this section, we evaluate the performance of our model on the benchmark FB15k and WN18 datasets for KBC tasks (Bordes et al., 2013). These datasets contain multi-relations between head and tail entities. Given a head entity and a relation, the model produces a ranked list of the entities according to the score of the entity being the tail entity of this triple. To evaluate the ranking, we report mean rank (MR), the mean of rank of the correct entity across the test examples, and hits@10, the proportion of correct entities ranked in the top-10 predictions. Lower MR or higher hits@10 indicates a better prediction performance. We follow the evaluation protocol in Bordes et al. (2013) to report filtered results, where negative examples N are removed from the dataset. In this case, we can avoid some negative examples being valid and ranked above the target triplet. We use the same hyper-parameters of our model for both FB15k and WN18 datasets. Entity embeddings (which are not shared between input and output modules) and relation embedding are both 100-dimensions. We use the input module and output module to encode subject and object entities, respectively. There are 64 memory vectors with 200 dimensions each, initialized by random vectors with unit L2-norm. We use single-layer GRU with 200 cells as the search controller. We set the maximum inference step of the IRN to 5. We randomly initialize all model parameters, and use SGD as the training algorithm with mini-batch size of 64. We set the learning rate to a constant number, 0.01. To prevent the model from learning a trivial solution by increasing entity embeddings norms, we follow Bordes et al. (2013) to enforce the L2-norm of the entity embeddings as 1. We use hits@10 as the validation metric for the IRN. Following the work (Lin et al., 2015a), we add reverse relations into the training triplet set to increase the training data. Following Nguyen et al. (2016), we divide the results of previous work into two groups. The first group contains the models that directly optimize a scoring function for the triples in a knowledge base without using extra information. The second group of models make uses of additional information from multi-step relations. For example, RTransE (Garc\u00eda-Dur\u00e1n et al., 2015) and PTransE (Lin et al., 2015a) models are extensions of the TransE (Bordes et al., 2013) model by explicitly exploring multi-step relations in the knowledge base to regularize the trained embeddings. The NLFeat model (Toutanova et al., 2015) is a log-linear model that makes use of simple node and link features. Table 1 presents the experimental results. According to the table, our model significantly outperforms previous baselines, regardless of whether previous approaches use additional information or not. Specifically, on FB15k, the MR of our model surpasses all previous results by 12, and our hit@10 outperforms others by 5.7%. On WN18, the IRN obtains the highest hit@10 while maintaining similar MR results compared to previous work.1 We analyze hits@10 results on FB15k with respect to the relation categories. Following the evaluation in Bordes et al. (2013), we evaluate the performance in four types of relation: 1-1 if a head entity can appear with at most one tail entity, 1-Many if a head entity can appear with many tail entities, Many-1 if multiple heads can appear with the same tail entity, and Many-Many if multiple head entities can appear with multiple tail entities.", "startOffset": 114, "endOffset": 3161}, {"referenceID": 2, "context": "Model Additional Information WN18 FB15k Hits@10 (%) MR Hits@10 (%) MR SE (Bordes et al., 2011) NO 80.", "startOffset": 73, "endOffset": 94}, {"referenceID": 4, "context": "8 162 Unstructured (Bordes et al., 2014) NO 38.", "startOffset": 19, "endOffset": 40}, {"referenceID": 3, "context": "3 979 TransE (Bordes et al., 2013) NO 89.", "startOffset": 13, "endOffset": 34}, {"referenceID": 31, "context": "1 125 TransH (Wang et al., 2014) NO 86.", "startOffset": 13, "endOffset": 32}, {"referenceID": 12, "context": "2 75 KG2E (He et al., 2015) NO 93.", "startOffset": 10, "endOffset": 27}, {"referenceID": 13, "context": "0 59 TransD (Ji et al., 2015) NO 92.", "startOffset": 12, "endOffset": 29}, {"referenceID": 7, "context": "3 91 TATEC (Garc\u00eda-Dur\u00e1n et al., 2015) NO 76.", "startOffset": 11, "endOffset": 38}, {"referenceID": 23, "context": "7 58 NTN (Socher et al., 2013) NO 66.", "startOffset": 9, "endOffset": 30}, {"referenceID": 35, "context": "4 DISTMULT (Yang et al., 2014) NO 94.", "startOffset": 11, "endOffset": 30}, {"referenceID": 18, "context": "7 STransE (Nguyen et al., 2016) NO 94.", "startOffset": 10, "endOffset": 31}, {"referenceID": 7, "context": "7 69 RTransE (Garc\u00eda-Dur\u00e1n et al., 2015) Path 76.", "startOffset": 13, "endOffset": 40}, {"referenceID": 29, "context": "6 58 NLFeat (Toutanova et al., 2015) Node + Link Features 94.", "startOffset": 12, "endOffset": 36}, {"referenceID": 32, "context": "0 Random Walk (Wei et al., 2016) Path 94.", "startOffset": 14, "endOffset": 32}, {"referenceID": 2, "context": "Model Predicting head h Predicting tail t 1-1 1-M M-1 M-M 1-1 1-M M-1 M-M SE (Bordes et al., 2011) 35.", "startOffset": 77, "endOffset": 98}, {"referenceID": 4, "context": "3 Unstructured (Bordes et al., 2014) 34.", "startOffset": 15, "endOffset": 36}, {"referenceID": 3, "context": "6 TransE (Bordes et al., 2013) 43.", "startOffset": 9, "endOffset": 30}, {"referenceID": 31, "context": "0 TransH (Wang et al., 2014) 66.", "startOffset": 9, "endOffset": 28}, {"referenceID": 12, "context": "8 KG2E (He et al., 2015) 92.", "startOffset": 7, "endOffset": 24}, {"referenceID": 13, "context": "4 TransD (Ji et al., 2015) 86.", "startOffset": 9, "endOffset": 26}, {"referenceID": 7, "context": "2 TATEC (Garc\u00eda-Dur\u00e1n et al., 2015) 79.", "startOffset": 8, "endOffset": 35}, {"referenceID": 18, "context": "7 STransE (Nguyen et al., 2016) 82.", "startOffset": 10, "endOffset": 31}, {"referenceID": 27, "context": "We compare the IRN with two baseline approaches: dynamic programming without edge-weight information and a standard sequence-to-sequence model (Sutskever et al., 2014) using a similar parameter size to our model.", "startOffset": 143, "endOffset": 167}, {"referenceID": 2, "context": "(Bordes et al., 2011; 2014; 2013; Wang et al., 2014; Ji et al., 2015; Nguyen et al., 2016).", "startOffset": 0, "endOffset": 90}, {"referenceID": 31, "context": "(Bordes et al., 2011; 2014; 2013; Wang et al., 2014; Ji et al., 2015; Nguyen et al., 2016).", "startOffset": 0, "endOffset": 90}, {"referenceID": 13, "context": "(Bordes et al., 2011; 2014; 2013; Wang et al., 2014; Ji et al., 2015; Nguyen et al., 2016).", "startOffset": 0, "endOffset": 90}, {"referenceID": 18, "context": "(Bordes et al., 2011; 2014; 2013; Wang et al., 2014; Ji et al., 2015; Nguyen et al., 2016).", "startOffset": 0, "endOffset": 90}, {"referenceID": 3, "context": "For example, in TransE (Bordes et al., 2013), the function is implemented as fr(h, t) = \u2016h+ r\u2212 t\u2016, where h, r and t are the corresponding vector representations.", "startOffset": 23, "endOffset": 44}, {"referenceID": 11, "context": "Recently, different studies (Guu et al., 2015; Lin et al., 2015a; Toutanova et al., 2016) demonstrate the importance for models to also learn from multi-step relations.", "startOffset": 28, "endOffset": 89}, {"referenceID": 30, "context": "Recently, different studies (Guu et al., 2015; Lin et al., 2015a; Toutanova et al., 2016) demonstrate the importance for models to also learn from multi-step relations.", "startOffset": 28, "endOffset": 89}, {"referenceID": 30, "context": ", 2015a) or considering all possible paths using a dynamic programming algorithm with the restriction of using linear or bi-linear models only (Toutanova et al., 2016).", "startOffset": 143, "endOffset": 167}, {"referenceID": 21, "context": "Studies such as (Riedel et al., 2013) show that incorporating textual information can further improve the knowledge base completion tasks.", "startOffset": 16, "endOffset": 37}, {"referenceID": 27, "context": "Neural Frameworks Sequence-to-sequence models (Sutskever et al., 2014; Cho et al., 2014) have shown to be successful in many applications such as machine translation and conversation modeling (Sordoni et al.", "startOffset": 46, "endOffset": 88}, {"referenceID": 5, "context": "Neural Frameworks Sequence-to-sequence models (Sutskever et al., 2014; Cho et al., 2014) have shown to be successful in many applications such as machine translation and conversation modeling (Sordoni et al.", "startOffset": 46, "endOffset": 88}, {"referenceID": 24, "context": ", 2014) have shown to be successful in many applications such as machine translation and conversation modeling (Sordoni et al., 2015).", "startOffset": 111, "endOffset": 133}, {"referenceID": 9, "context": "While sequence-to-sequence models are powerful, recent work has shown that the necessity of incorporating an external memory to perform inference in simple algorithmic tasks (Graves et al., 2014; 2016).", "startOffset": 174, "endOffset": 201}, {"referenceID": 2, "context": "For example, in TransE (Bordes et al., 2013), the function is implemented as fr(h, t) = \u2016h+ r\u2212 t\u2016, where h, r and t are the corresponding vector representations. Recently, different studies (Guu et al., 2015; Lin et al., 2015a; Toutanova et al., 2016) demonstrate the importance for models to also learn from multi-step relations. Learning from multi-step relations injects the structured relationships between triples into the model. However, this also poses a technical challenge of considering exponential numbers of multi-step relationships. Prior approaches address this issue by designing path-mining algorithms (Lin et al., 2015a) or considering all possible paths using a dynamic programming algorithm with the restriction of using linear or bi-linear models only (Toutanova et al., 2016). Toutanova & Chen (2015) shows the effectiveness of using simple node and link features that encode structured information on FB15k and WN18.", "startOffset": 24, "endOffset": 822}, {"referenceID": 20, "context": "First, inspired from Ribeiro et al. (2016), we would like to develop techniques to exploit ways to generate human understandable reasoning interpretation from the shared memory.", "startOffset": 21, "endOffset": 43}], "year": 2016, "abstractText": "Recent studies on knowledge base completion, the task of recovering missing relationships based on recorded relations, demonstrate the importance of learning embeddings from multi-step relations. However, due to the size of knowledge bases, learning multi-step relations directly on top of observed instances could be costly. In this paper, we propose Implicit ReasoNets (IRNs), which is designed to perform large-scale inference implicitly through a search controller and shared memory. Unlike previous work, IRNs use training data to learn to perform multi-step inference through the shared memory, which is also jointly updated during training. While the inference procedure is not operating on top of observed instances for IRNs, our proposed model outperforms all previous approaches on the popular FB15k benchmark by more than 5.7%.", "creator": "LaTeX with hyperref package"}}}