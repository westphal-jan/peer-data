{"id": "1611.04666", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Nov-2016", "title": "A Generic Coordinate Descent Framework for Learning from Implicit Feedback", "abstract": "In recent years, interest in recommender research has shifted from explicit feedback towards implicit feedback data. A diversity of complex models has been proposed for a wide variety of applications. Despite this, learning from implicit feedback is still computationally challenging. So far, most work relies on stochastic gradient descent (SGD) solvers which are easy to derive, but in practice challenging to apply, especially for tasks with many items. For the simple matrix factorization model, an efficient coordinate descent (CD) solver has been previously proposed. However, efficient CD approaches have not been derived for more complex models.", "histories": [["v1", "Tue, 15 Nov 2016 01:32:33 GMT  (160kb,D)", "http://arxiv.org/abs/1611.04666v1", null]], "reviews": [], "SUBJECTS": "cs.IR cs.LG", "authors": ["immanuel bayer", "xiangnan he", "bhargav kanagal", "steffen rendle"], "accepted": false, "id": "1611.04666"}, "pdf": {"name": "1611.04666.pdf", "metadata": {"source": "CRF", "title": "A Generic Coordinate Descent Framework for Learning from Implicit Feedback", "authors": ["Immanuel Bayer", "Xiangnan He", "Bhargav Kanagal", "Steffen Rendle"], "emails": ["immanuel.bayer@uni-", "xiangnan@comp.nus.edu.sg", "bhargav@google.com", "srendle@google.com"], "sections": [{"heading": null, "text": "In this paper, we provide a new framework for deriving efficient CD algorithms for complex recommendation models. We identify and present the property of k-separable models. We demonstrate that k-separability is a sufficient property to enable efficient optimization of implicit recommendation problems with CD. We illustrate this framework using a variety of state-of-the-art models, including factoring machines and tucker decomposition. In summary, our work provides theory and building blocks to derive efficient implicit CD algorithms for complex recommendation models."}, {"heading": "1. INTRODUCTION", "text": "In recent years, the focus of recipient system research has shifted from explicit feedback problems to implicit feedback problems, and most of the signals a user sends about his preferences are implicit. Examples of implicit feedback are: a user watches a video, clicks on a link, and so on. Implicit feedback data is much cheaper than explicit feedback because it comes at no additional cost to the user and is therefore available on a much larger scale."}, {"heading": "2. RELATED WORK", "text": "For several years, matrix factorization (MF) has been considered the most effective basic receiver system. Two optimization strategies dominate research on implicit feedback data: the first is the descent of humans from Earth (BPR), which focuses on both consumed and unconsumed goods (SGD); the second is the descent of humans from Earth (BCD), which alternate."}, {"heading": "3. PROBLEM STATEMENT", "text": "Let me be a set of items and let C be a set of contexts. Let S be a set of observed feedback where a tuple (c, i, y, \u03b1) indicates that in context c a score y item i has been confidently assigned \u03b1. See Figure 1 for an illustration. We use a general notation of the context that can include, for example, user, time, place, attributes, history, etc. Section 5 and Section 6 show more examples of the context."}, {"heading": "3.1 Recommender Model", "text": "A recommendation model y: C \u00d7 I \u2192 R is a function that assigns a score to each context item pair. Model y is parameterized through a series of model parameters. Model y is typically used to decide which items should be presented in a given context. The learning task is to find the values of the model parameters that minimize a loss to the data S. For example, a square lossL (\u03b7 | S) = \u2211 (c, i, y, \u03b1) \u0445 S \u03b1 (y, (c, i) \u2212 y) 2 + \u0445\u043e\u0441\u043e\u0441\u043e\u0441\u043e\u043c\u043e\u043c\u043e\u043c\u043e\u0432\u0438\u043d\u0438\u043d\u0438\u043d\u0438\u043d\u0438\u043d\u0438\u0441\u0442\u0438 2 (1), where \u0441\u0432 is a regularization constant for the parameter \u0432."}, {"heading": "3.2 Coordinate Descent Algorithm", "text": "Target (1) can be minimized by coordinate descend (CD). CD iterates by the model parameters and updates one parameter after the other. CD calculates the first L \"and the second derivative L\" with respect to the selected coordinate values: L \"(\u03b8 | S) = 2\" (c, i, y, \u03b1 \")\" S\u03b1 \"(c, i)\" y \"(c, i) + 2\" x \"(2)\" L \"(2)\" (\u03b8 | S) = 2 \"(c, i, \u03b1)\" (y \"(c, i)\" y \"(c, i) + y\" (c, i) + 2 \"x\" x \"(3) and performs a Newton matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matx matrix matrix matrix matrix matx matrix matrix"}, {"heading": "3.3 Learning from Implicit Feedback", "text": "In the case of an implicit recommendation problem, the unused items are significant and cannot be ignored. For example, in Figure 1 (right), the data is useful to learn user preferences. To formalize, the training data Simpl of an implicit problem consists of a set of S + of the observed feedback and all unused tuples S0Simpl = S +, S0, | Simpl | | C | | I | (5) with the set S + (c, i, y, \u03b1) 0: y = \u03b10. (6) S + contains the observed feedback and is of much smaller scale than Simpl, usually | S + | | C | | I |. The implicit learning problem can be specified as minimizing the target in eq. (1) The implicit data Simpl."}, {"heading": "4. GENERIC COORDINATE DESCENT ALGORITHM FOR IMPLICIT FEEDBACK", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Implicit Regularizer", "text": "As discussed in Section 3.3, the reason implicit data formation is a challenge is the large number of implicit examples S0, which are typically summed up across all context item pairs. Note: S0 includes all implicit context item pairs that are not implicit in S +. We now show that we can sum up the optimization criterion across all context item pairs. This reformulation is a prerequisite for later allowing loss decomposition in Section 4.2. In addition, it allows us to study implicit optimization without S +.Lemma 1. Implicit learning can be a combination of learning a small positive set and minimizing the scoring function in Section 4.2."}, {"heading": "5. APPLICATIONS", "text": "In this section we apply iCD to two classes of complex factorization models, namely function-based factorization models and tensor factorization models. We have chosen these two classes because they are very powerful and widely used. In addition, each of them has some interesting properties in terms of deriving iCD algorithms. The provided algorithms can be applied directly to many common tasks of the receiver system. This section also serves as a guide for deriving iCD algorithms in general."}, {"heading": "5.1 Matrix Factorization (MF)", "text": "For MF, the scoring function is isy (c, i): = < wc, hi > = k \u00b2 f = 1 wc, f hi, f (15) with the model parameters \u0442 = {W, H}, where W \u00b2 RC \u00b7 k and H \u00b2 RI \u00b7 k. Algorithm 2 Implicit CD for MF1: Method iCD-MF (S, C, I) 2: W, H \u00b2 N (0, \u03c3) 3: Repeat 4: for f \u00b2 s (1,., k} do 5: for f \u00b2,., k \u00b2 do 6: Compute JI (f, f) 7: Do end for c \u00b2 C 9: Compute L \u00b2 (wc \u00b2, f \u00b2 f \u00b2), f \u00b2 c: c \u00b2, f \u00b2 f \u00b2."}, {"heading": "5.2 Feature-Based Factorization Models", "text": "One of the most powerful extensions of MF is context and element-based modeling (MF)."}, {"heading": "5.3 Tensor Factorization", "text": "Tensor factorization models are multilinear, we want to highlight that existent or factorization learning, that is that are multilinear, we are well in our factorization."}, {"heading": "6. EXPERIMENTS", "text": "The main objective of the experiments is to demonstrate the universality of the iCD framework. We show how iCD can be applied to a wide range of recommendation problems that cannot be solved with MF alone. Efficient coordinated descendant algorithms (CD) have already been proposed for MF models [5] and its performance compared to gradient descendant algorithms such as BPR [13]. Both approaches are considered state-of-the-art, and while CD exceeds BPR for certain datasets [8, 25, 15, 22, 26], BPR has been shown to work better for others [4, 17, 16, 8]. The purpose of our experiments is not to compare BPR and CD on another dataset, but rather to demonstrate the versatility of the iCD framework and how it can serve as a building block for future research on complex recommendation models."}, {"heading": "6.1 Experimental Setup", "text": "We evaluate using a data set of 200,000 users interacting with YouTube. Our subset includes | I | = 68,000 videos. The data set also includes page information about age, country, gender, and device information. We apply iCD to three popular recommendation problems - cold start, offline recommendation, and instant recommendation (see Section 6.2). We compare the following algorithms: \u2022 Popularity: a static recommender that returns the most popular videos. \u2022 Coview: returns based on the video you've seen before, the most frequently chosen next video. \u2022 iCD-MF: User Item Matrix Factorization using iCD for optimization, similar to [5]. \u2022 iCD-FM: a factorization engine with different context characteristics (Section 5.2). We report results for various feature choices.We measure callbacks and NDCG for the 100 best videos returned."}, {"heading": "6.2 Results", "text": "We experiment with multiple FM models that previously interact with the receiver system. To simulate this scenario, we select a random group of users and maintain all of their events for evaluation purposes. We train on the remaining users. The common approach to dealing with cold starts is to represent a user with information. Here, we use the function-based FM model (iCD-FM) with the age of the user, gender, country and device information. Figure 7 shows that attribute-conscious FM achieves a twofold improvement in the basics."}, {"heading": "6.3 Computational Costs", "text": "As explained in Section 3.3, any conventional CD solver, e.g. [11], could solve the implicit feedback problem. Now, we demonstrate that this is not feasible due to the large number of implicit examples. Figure 8 compares the calculation cost of learning FM with the cost of a conventional CD with the cost of an iCD on our 70k item dataset. We use three different context features of Figure 6. The figure shows the relative cost to iCD-FM P. For all three context decisions, a conventional CD shows four orders of magnitude higher calculation costs than iCD. The empirically measured runtime for iCD was in the order of minutes; consequently, the four-fold increase in the runtime of a CD translates to weeks of training for each iteration. Of course, the use of a conventional CD solver to directly optimize the implicit loss is not feasible."}, {"heading": "7. CONCLUSION", "text": "In this paper, we have presented a general, efficient framework for learning recommendation systems from implicit feedback. First, we have shown that learning from implicit feedback can be reformulated to optimize a cheap explicit loss and an expensive implicit regulator. Then, we have introduced the concept of k-separable models. We have shown that the implicit regulator of a k-separable model can be efficiently calculated without iterating all context item pairs. Finally, we have shown that many popular recommendation models are k-separable, including matrix factorization, factorization engines, and tensor factorization. Furthermore, we have provided efficient learning algorithms for these models based on our framework. Our framework is not limited to the models discussed in the paper, but serves as a general blueprint for deriving learning algorithms for recommended systems."}, {"heading": "8. REFERENCES", "text": "[1] C. Cheng, H. Yang, M. R. Lyu, and I. King. WhereYou Like to Go Next: Successive Point-of-Interest Recommendation. In IJCAI, Volume 13, pp. 2605-2611, 2013. [2] Z. Gantner, L. Drumond, C. Freudenthaler, S. Rendle, and L. Schmidt-Thieme. Learning attribute-to-feature mappings for an \"multi-modal factor analysis. UCLA Working Papers in Phonetics, 16 (1): 84, 1970. [4] R. Er and J. McAuley. VBPR: Visual bayesian personalized ranking from implicit feedback."}], "references": [{"title": "Where You Like to Go Next: Successive Point-of-Interest Recommendation", "author": ["C. Cheng", "H. Yang", "M.R. Lyu", "I. King"], "venue": "In IJCAI,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2013}, {"title": "Learning attribute-to-feature mappings for cold-start recommendations", "author": ["Z. Gantner", "L. Drumond", "C. Freudenthaler", "S. Rendle", "L. Schmidt-Thieme"], "venue": "In 2010 IEEE International Conference on Data Mining,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2010}, {"title": "Foundations of the PARAFAC procedure: Models and conditions for an \u201dexplanatory\u201d multi-modal factor analysis", "author": ["R.A. Harshman"], "venue": "UCLA Working Papers in Phonetics,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1970}, {"title": "VBPR: Visual bayesian personalized ranking from implicit feedback", "author": ["R. He", "J. McAuley"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2016}, {"title": "Collaborative filtering for implicit feedback datasets", "author": ["Y. Hu", "Y. Koren", "C. Volinsky"], "venue": "In Proceedings of the 2008 Eighth IEEE International Conference on Data Mining,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2008}, {"title": "Supercharging recommender systems using taxonomies for learning user purchase behavior", "author": ["B. Kanagal", "A. Ahmed", "S. Pandey", "V. Josifovski", "J. Yuan", "L. Garcia-Pueyo"], "venue": "Proc. VLDB Endow.,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2012}, {"title": "The million song dataset challenge", "author": ["B. McFee", "T. Bertin-Mahieux", "D.P. Ellis", "G.R. Lanckriet"], "venue": "In Proceedings of the 21st International Conference on World Wide Web, WWW \u201912 Companion,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2012}, {"title": "Sparse linear methods for top-n recommender systems", "author": ["X. Ning", "G. Karypis. Slim"], "venue": "In 2011 IEEE 11th International Conference on Data Mining,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2011}, {"title": "GBPR: Group Preference Based Bayesian Personalized Ranking for One-Class Collaborative Filtering", "author": ["W. Pan", "L. Chen"], "venue": "In IJCAI,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2013}, {"title": "Fast als-based matrix factorization for explicit and implicit feedback datasets", "author": ["I. Pil\u00e1szy", "D. Zibriczky", "D. Tikk"], "venue": "In Proceedings of the fourth ACM conference on Recommender systems,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2010}, {"title": "Factorization machines with libfm", "author": ["S. Rendle"], "venue": "ACM Trans. Intell. Syst. Technol.,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2012}, {"title": "Improving pairwise learning for item recommendation from implicit feedback", "author": ["S. Rendle", "C. Freudenthaler"], "venue": "In Proceedings of the 7th ACM International Conference on Web Search and Data Mining,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2014}, {"title": "BPR: Bayesian personalized ranking from implicit feedback", "author": ["S. Rendle", "C. Freudenthaler", "Z. Gantner", "L. Schmidt-Thieme"], "venue": "In Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2009}, {"title": "Factorizing personalized markov chains for next-basket recommendation", "author": ["S. Rendle", "C. Freudenthaler", "L. Schmidt-Thieme"], "venue": "In Proceedings of the 19th International Conference on World Wide Web,  WWW", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2010}, {"title": "On the effectiveness of linear models for one-class collaborative filtering", "author": ["S. Sedhain", "A.K. Menon", "S. Sanner", "D. Braziunas"], "venue": "In Proceedings of the 30th Conference on Artificial Intelligence", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2016}, {"title": "TFMAP: optimizing MAP for top-n context-aware recommendation", "author": ["Y. Shi", "A. Karatzoglou", "L. Baltrunas", "M. Larson", "A. Hanjalic", "N. Oliver"], "venue": "In Proceedings of the 35th international ACM SIGIR conference on Research and development in information retrieval,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2012}, {"title": "CLiMF: learning to maximize reciprocal rank with collaborative less-is-more filtering", "author": ["Y. Shi", "A. Karatzoglou", "L. Baltrunas", "M. Larson", "N. Oliver", "A. Hanjalic"], "venue": "In Proceedings of the sixth ACM conference on Recommender systems,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2012}, {"title": "Care to comment?: recommendations for commenting on news stories", "author": ["E. Shmueli", "A. Kagian", "Y. Koren", "R. Lempel"], "venue": "In Proceedings of the 21st international conference on World Wide Web,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2012}, {"title": "Cubesvd: A novel approach to personalized web search", "author": ["J.-T. Sun", "H.-J. Zeng", "H. Liu", "Y. Lu", "Z. Chen"], "venue": "In Proceedings of the 14th International Conference on World Wide Web, WWW", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2005}, {"title": "A unified framework for providing recommendations in social tagging systems based on ternary semantic analysis", "author": ["P. Symeonidis", "A. Nanopoulos", "Y. Manolopoulos"], "venue": "IEEE Trans. on Knowl. and Data Eng.,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2010}, {"title": "Some mathematical notes on three-mode factor analysis", "author": ["L.R. Tucker"], "venue": "Psychometrika, 31:279\u2013311,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 1966}, {"title": "Effective latent models for binary feedback in recommender systems", "author": ["M. Volkovs", "G.W. Yu"], "venue": "In Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2015}, {"title": "Scalable coordinate descent approaches to parallel matrix factorization for recommender systems", "author": ["H.-F. Yu", "C.-J. Hsieh", "S. Si", "I. Dhillon"], "venue": "In Proceedings of the 12th International Conference on Data Mining,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2012}, {"title": "Personalized entity recommendation: A heterogeneous information network approach", "author": ["X. Yu", "X. Ren", "Y. Sun", "Q. Gu", "B. Sturt", "U. Khandelwal", "B. Norick", "J. Han"], "venue": "In Proceedings of the 7th ACM International Conference on Web Search and Data Mining,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2014}, {"title": "Leveraging social connections to improve personalized ranking for collaborative filtering", "author": ["T. Zhao", "J. McAuley", "I. King"], "venue": "In Proceedings of the 23rd ACM International Conference on Conference on Information and Knowledge Management,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2014}, {"title": "Improving latent factor models via personalized feature projection for one class recommendation", "author": ["T. Zhao", "J. McAuley", "I. King"], "venue": "In Proceedings of the 24th ACM International on Conference on Information and Knowledge Management,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2015}], "referenceMentions": [{"referenceID": 4, "context": "However, learning a recommender system from implicit feedback is computationally expensive because the observed actions of a user need to be contrasted against all the non-observed actions [5, 13].", "startOffset": 189, "endOffset": 196}, {"referenceID": 12, "context": "However, learning a recommender system from implicit feedback is computationally expensive because the observed actions of a user need to be contrasted against all the non-observed actions [5, 13].", "startOffset": 189, "endOffset": 196}, {"referenceID": 12, "context": "While SGD is available as a general framework to optimize a broad class of models [13], CD is only available for a few simple models [5, 10].", "startOffset": 82, "endOffset": 86}, {"referenceID": 4, "context": "While SGD is available as a general framework to optimize a broad class of models [13], CD is only available for a few simple models [5, 10].", "startOffset": 133, "endOffset": 140}, {"referenceID": 9, "context": "While SGD is available as a general framework to optimize a broad class of models [13], CD is only available for a few simple models [5, 10].", "startOffset": 133, "endOffset": 140}, {"referenceID": 12, "context": "The first one is Bayesian Personalized Ranking (BPR) [13], a stochastic gradient descent (SGD) framework, that contrasts pairs of consumed to nonconsumed items.", "startOffset": 53, "endOffset": 57}, {"referenceID": 4, "context": "The second one is coordinate descent (CD) also known as alternating least squares on an elementwise loss over both the consumed and non-consumed items [5].", "startOffset": 151, "endOffset": 154}, {"referenceID": 6, "context": "BPR tackles this issue by sampling negative items, but it has been shown that BPR has convergence problems when the number of items is large [7, 12].", "startOffset": 141, "endOffset": 148}, {"referenceID": 11, "context": "BPR tackles this issue by sampling negative items, but it has been shown that BPR has convergence problems when the number of items is large [7, 12].", "startOffset": 141, "endOffset": 148}, {"referenceID": 11, "context": "It requires more complex, nonuniform, sampling strategies for dealing with this problem [12, 6].", "startOffset": 88, "endOffset": 95}, {"referenceID": 5, "context": "It requires more complex, nonuniform, sampling strategies for dealing with this problem [12, 6].", "startOffset": 88, "endOffset": 95}, {"referenceID": 4, "context": "[5] have derived an efficient algorithm that allows to optimize over the large number of non-consumed items without any cost.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "Many authors have compared both CD-MF and BPR-MF on a variety of datasets and some work reports better quality for BPR-MF [4, 17, 16, 8] whereas for other problems CD-MF works better [8, 25, 15, 22, 26].", "startOffset": 122, "endOffset": 136}, {"referenceID": 16, "context": "Many authors have compared both CD-MF and BPR-MF on a variety of datasets and some work reports better quality for BPR-MF [4, 17, 16, 8] whereas for other problems CD-MF works better [8, 25, 15, 22, 26].", "startOffset": 122, "endOffset": 136}, {"referenceID": 15, "context": "Many authors have compared both CD-MF and BPR-MF on a variety of datasets and some work reports better quality for BPR-MF [4, 17, 16, 8] whereas for other problems CD-MF works better [8, 25, 15, 22, 26].", "startOffset": 122, "endOffset": 136}, {"referenceID": 7, "context": "Many authors have compared both CD-MF and BPR-MF on a variety of datasets and some work reports better quality for BPR-MF [4, 17, 16, 8] whereas for other problems CD-MF works better [8, 25, 15, 22, 26].", "startOffset": 122, "endOffset": 136}, {"referenceID": 7, "context": "Many authors have compared both CD-MF and BPR-MF on a variety of datasets and some work reports better quality for BPR-MF [4, 17, 16, 8] whereas for other problems CD-MF works better [8, 25, 15, 22, 26].", "startOffset": 183, "endOffset": 202}, {"referenceID": 24, "context": "Many authors have compared both CD-MF and BPR-MF on a variety of datasets and some work reports better quality for BPR-MF [4, 17, 16, 8] whereas for other problems CD-MF works better [8, 25, 15, 22, 26].", "startOffset": 183, "endOffset": 202}, {"referenceID": 14, "context": "Many authors have compared both CD-MF and BPR-MF on a variety of datasets and some work reports better quality for BPR-MF [4, 17, 16, 8] whereas for other problems CD-MF works better [8, 25, 15, 22, 26].", "startOffset": 183, "endOffset": 202}, {"referenceID": 21, "context": "Many authors have compared both CD-MF and BPR-MF on a variety of datasets and some work reports better quality for BPR-MF [4, 17, 16, 8] whereas for other problems CD-MF works better [8, 25, 15, 22, 26].", "startOffset": 183, "endOffset": 202}, {"referenceID": 25, "context": "Many authors have compared both CD-MF and BPR-MF on a variety of datasets and some work reports better quality for BPR-MF [4, 17, 16, 8] whereas for other problems CD-MF works better [8, 25, 15, 22, 26].", "startOffset": 183, "endOffset": 202}, {"referenceID": 1, "context": "Shifting from simple matrix factorization to more complex factorization models has shown large success in many implicit recommendation problems [2, 4, 18, 1, 9, 24].", "startOffset": 144, "endOffset": 164}, {"referenceID": 3, "context": "Shifting from simple matrix factorization to more complex factorization models has shown large success in many implicit recommendation problems [2, 4, 18, 1, 9, 24].", "startOffset": 144, "endOffset": 164}, {"referenceID": 17, "context": "Shifting from simple matrix factorization to more complex factorization models has shown large success in many implicit recommendation problems [2, 4, 18, 1, 9, 24].", "startOffset": 144, "endOffset": 164}, {"referenceID": 0, "context": "Shifting from simple matrix factorization to more complex factorization models has shown large success in many implicit recommendation problems [2, 4, 18, 1, 9, 24].", "startOffset": 144, "endOffset": 164}, {"referenceID": 8, "context": "Shifting from simple matrix factorization to more complex factorization models has shown large success in many implicit recommendation problems [2, 4, 18, 1, 9, 24].", "startOffset": 144, "endOffset": 164}, {"referenceID": 23, "context": "Shifting from simple matrix factorization to more complex factorization models has shown large success in many implicit recommendation problems [2, 4, 18, 1, 9, 24].", "startOffset": 144, "endOffset": 164}, {"referenceID": 10, "context": ", \u03b7 = 1, can be chosen without risking divergence [11].", "startOffset": 50, "endOffset": 54}, {"referenceID": 22, "context": "For MF, [23] shows a complexity of O(|S| k) and for FM, [11] derives a complexity of O(NZ(X) k) where NZ(X) is the number of non-zero entries in the design matrix X.", "startOffset": 8, "endOffset": 12}, {"referenceID": 10, "context": "For MF, [23] shows a complexity of O(|S| k) and for FM, [11] derives a complexity of O(NZ(X) k) where NZ(X) is the number of non-zero entries in the design matrix X.", "startOffset": 56, "endOffset": 60}, {"referenceID": 22, "context": "Learning models over an explicit loss is already well studied [23, 11], so we focus now on the implicit regularizer", "startOffset": 62, "endOffset": 70}, {"referenceID": 10, "context": "Learning models over an explicit loss is already well studied [23, 11], so we focus now on the implicit regularizer", "startOffset": 62, "endOffset": 70}, {"referenceID": 1, "context": "[2, 11]).", "startOffset": 0, "endOffset": 7}, {"referenceID": 10, "context": "[2, 11]).", "startOffset": 0, "endOffset": 7}, {"referenceID": 1, "context": "For instance, the cold-start problem is commonly solved by replacing or complementing user and item ids with user and item attributes [2].", "startOffset": 134, "endOffset": 137}, {"referenceID": 5, "context": "Also sequential models can be represented by feature based modeling [6].", "startOffset": 68, "endOffset": 71}, {"referenceID": 1, "context": "1 MF with Side Information (MFSI) We start with a feature based extension of matrix factorization similar to [2]:", "startOffset": 109, "endOffset": 112}, {"referenceID": 10, "context": "2 Factorization Machines The Factorization Machine (FM) model [11] is a more complex factorized model that includes biases and interactions between all variables.", "startOffset": 62, "endOffset": 66}, {"referenceID": 19, "context": "For instance, in personalized recommendation of tags for bookmarks [20], the context consists of two variables, the user C1 and the bookmark C2, and the item I corresponds to the tag.", "startOffset": 67, "endOffset": 71}, {"referenceID": 18, "context": "For personalized web search [19], the context consists of the user C1 and the query C2 and the item I to the web page.", "startOffset": 28, "endOffset": 32}, {"referenceID": 18, "context": "Additionally, we want to highlight, that existing tensor factorization learning algorithms [19, 20, 10] require that the k3", "startOffset": 91, "endOffset": 103}, {"referenceID": 19, "context": "Additionally, we want to highlight, that existing tensor factorization learning algorithms [19, 20, 10] require that the k3", "startOffset": 91, "endOffset": 103}, {"referenceID": 9, "context": "Additionally, we want to highlight, that existing tensor factorization learning algorithms [19, 20, 10] require that the k3", "startOffset": 91, "endOffset": 103}, {"referenceID": 2, "context": "1 Parallel Factor Analysis (PARAFAC) We first discuss the Parallel Factor Analysis (PARAFAC) [3] model which is a 3-mode extension of matrix factorization.", "startOffset": 93, "endOffset": 96}, {"referenceID": 20, "context": "2 Tucker Decomposition Tucker Decomposition (TD) [21] is a generalization of PARAFAC which computes all interactions between the factor matrices.", "startOffset": 49, "endOffset": 53}, {"referenceID": 4, "context": "For MF models, efficient coordinate descent algorithms (CD) have been previously proposed [5] and its performance compared against gradient descent algorithms such as BPR [13].", "startOffset": 90, "endOffset": 93}, {"referenceID": 12, "context": "For MF models, efficient coordinate descent algorithms (CD) have been previously proposed [5] and its performance compared against gradient descent algorithms such as BPR [13].", "startOffset": 171, "endOffset": 175}, {"referenceID": 7, "context": "Both approaches are considered state-of-the-art and while CD outperforms BPR on certain datasets [8, 25, 15, 22, 26], BPR has been shown to work better on others [4, 17, 16, 8].", "startOffset": 97, "endOffset": 116}, {"referenceID": 24, "context": "Both approaches are considered state-of-the-art and while CD outperforms BPR on certain datasets [8, 25, 15, 22, 26], BPR has been shown to work better on others [4, 17, 16, 8].", "startOffset": 97, "endOffset": 116}, {"referenceID": 14, "context": "Both approaches are considered state-of-the-art and while CD outperforms BPR on certain datasets [8, 25, 15, 22, 26], BPR has been shown to work better on others [4, 17, 16, 8].", "startOffset": 97, "endOffset": 116}, {"referenceID": 21, "context": "Both approaches are considered state-of-the-art and while CD outperforms BPR on certain datasets [8, 25, 15, 22, 26], BPR has been shown to work better on others [4, 17, 16, 8].", "startOffset": 97, "endOffset": 116}, {"referenceID": 25, "context": "Both approaches are considered state-of-the-art and while CD outperforms BPR on certain datasets [8, 25, 15, 22, 26], BPR has been shown to work better on others [4, 17, 16, 8].", "startOffset": 97, "endOffset": 116}, {"referenceID": 3, "context": "Both approaches are considered state-of-the-art and while CD outperforms BPR on certain datasets [8, 25, 15, 22, 26], BPR has been shown to work better on others [4, 17, 16, 8].", "startOffset": 162, "endOffset": 176}, {"referenceID": 16, "context": "Both approaches are considered state-of-the-art and while CD outperforms BPR on certain datasets [8, 25, 15, 22, 26], BPR has been shown to work better on others [4, 17, 16, 8].", "startOffset": 162, "endOffset": 176}, {"referenceID": 15, "context": "Both approaches are considered state-of-the-art and while CD outperforms BPR on certain datasets [8, 25, 15, 22, 26], BPR has been shown to work better on others [4, 17, 16, 8].", "startOffset": 162, "endOffset": 176}, {"referenceID": 7, "context": "Both approaches are considered state-of-the-art and while CD outperforms BPR on certain datasets [8, 25, 15, 22, 26], BPR has been shown to work better on others [4, 17, 16, 8].", "startOffset": 162, "endOffset": 176}, {"referenceID": 4, "context": "\u2022 iCD-MF: user-item matrix factorization using iCD for optimization, similar to [5].", "startOffset": 80, "endOffset": 83}, {"referenceID": 1, "context": "1 Cold-Start Recommendation In the Cold-Start recommendation [2] scenario, we assume that a user interacts with the recommender system for the first time.", "startOffset": 61, "endOffset": 64}, {"referenceID": 1, "context": "The common approach for dealing with cold-start is to represent a user by side information [2].", "startOffset": 91, "endOffset": 94}, {"referenceID": 13, "context": "We experiment with multiple FM models: (1) iCD-FM A: an FM with user attributes, (2) iCD-FM P: a sequential FM that only uses the previously watched video (similar to FPMC [14] or Coview) and (3) iCD-FM A+P+U: an FM", "startOffset": 172, "endOffset": 176}, {"referenceID": 13, "context": "that uses all signals: attributes, previously watched video and user id (similar to FPMC [14] with user attributes).", "startOffset": 89, "endOffset": 93}, {"referenceID": 10, "context": "[11], could solve the implicit feedback problem.", "startOffset": 0, "endOffset": 4}], "year": 2016, "abstractText": "In recent years, interest in recommender research has shifted from explicit feedback towards implicit feedback data. A diversity of complex models has been proposed for a wide variety of applications. Despite this, learning from implicit feedback is still computationally challenging. So far, most work relies on stochastic gradient descent (SGD) solvers which are easy to derive, but in practice challenging to apply, especially for tasks with many items. For the simple matrix factorization model, an efficient coordinate descent (CD) solver has been previously proposed. However, efficient CD approaches have not been derived for more complex models. In this paper, we provide a new framework for deriving efficient CD algorithms for complex recommender models. We identify and introduce the property of k-separable models. We show that k-separability is a sufficient property to allow efficient optimization of implicit recommender problems with CD. We illustrate this framework on a variety of state-of-the-art models including factorization machines and Tucker decomposition. To summarize, our work provides the theory and building blocks to derive efficient implicit CD algorithms for complex recommender models.", "creator": "LaTeX with hyperref package"}}}