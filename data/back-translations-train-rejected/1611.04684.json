{"id": "1611.04684", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Nov-2016", "title": "Knowledge Enhanced Hybrid Neural Network for Text Matching", "abstract": "Long text brings a big challenge to semantic matching due to their complicated semantic and syntactic structures. To tackle the challenge, we consider using prior knowledge to help identify useful information and filter out noise to matching in long text. To this end, we propose a knowledge enhanced hybrid neural network (KEHNN). The model fuses prior knowledge into word representations by knowledge gates and establishes three matching channels with words, sequential structures of sentences given by Gated Recurrent Units (GRU), and knowledge enhanced representations. The three channels are processed by a convolutional neural network to generate high level features for matching, and the features are synthesized as a matching score by a multilayer perceptron. The model extends the existing methods by conducting matching on words, local structures of sentences, and global context of sentences. Evaluation results from extensive experiments on public data sets for question answering and conversation show that KEHNN can significantly outperform the-state-of-the-art matching models and particularly improve the performance on pairs with long text.", "histories": [["v1", "Tue, 15 Nov 2016 03:11:59 GMT  (221kb,D)", "http://arxiv.org/abs/1611.04684v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["yu wu", "wei wu", "zhoujun li", "ming zhou"], "accepted": false, "id": "1611.04684"}, "pdf": {"name": "1611.04684.pdf", "metadata": {"source": "CRF", "title": "Knowledge Enhanced Hybrid Neural Network for Text Matching", "authors": ["Yu Wu", "Wei Wu", "Zhoujun Li", "Ming Zhou"], "emails": ["wuyu@buaa.edu.cn", "lizj@buaa.edu.cn", "wuwei@microsoft.com", "mingzhou@microsoft.com"], "sections": [{"heading": "Introduction", "text": "This year it is more than ever before."}, {"heading": "Related Work", "text": "Early work on semantic matching is based on dictionaries (Ramos 2003) and uses statistical techniques such as LDA (Lead, Ng and Jordan 2003) and translation models (Koehn, Och and Marcu 2003) to bridge the semantic gaps. Recently, neural networks have proved more effective in capturing semantics in text pairs. Existing methods can be divided into two groups, the first group following a paradigm that performs matching through the first representation of sentences as vectors. Typical models in this group are DSSM (Huang et al. 2013), NTN (Socher et al. 2013), CDSSM (Shen et al. 2014), Arc1 (Hu et al. 2014), CNTN (Qiu and Huang 2015) and LSTMs (Tan, Xiang et al. 2015). However, these methods lose useful information in the sentence representation and lead to the emergence of methods in the second group."}, {"heading": "Approach", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Problem Formalization", "text": "Suppose we have a data set D = {(li, Sx, i, Sy, i)} Ni = 1, where Sx, i = (w0,..., wj,.., wI) and Sy, i = (w \u2032 0,.., w \u2032 j,.., w \u2032 J) are two pieces of text, and wj and w \u2032 j are the j-th word of Sx, i and Sy, i and N respectively, the number of instants. li-i {1,..., C} is a label indicating the degree of matching between Sx, i and Sy, i. In addition to D, we have prior knowledge of Sx, i and Sy, i denotes kx, i and ky, respectively. Our goal is to learn a matching model g (\u00b7, \u00b7) with D and how we compare the previous matching method and the previous matching sections (Sy knowledge) with Sy knowledge."}, {"heading": "Knowledge Gate", "text": "Inspired by the powerful gate mechanism (Hochreiter and Schmidhuber 1997; Chung et al. 2014), which controls information in the processing of sequential data with recurring neural networks (RNN), we propose to use knowledge stores to include previous knowledge in the synchronization, the underlying motivation being that we want to use the previous knowledge to filter out sounds and highlight the useful information for synchronization in a text. ew-Rd formally becomes the embedding of a word w in text Sx and kx-Rndenotes the representation of the previous knowledge of Sx. Knowledge gate-Rs is defined askw = \u03c3 (Wkew + Ukkx), (1) where it is a sigmoid function, andWk-Rd-Rd-n are parameters. With kW we define an extended representation of knowledge for w-Rd = ew (1) ew \u2212 kkx, where it is an element (2)."}, {"heading": "Matching with Multiple Channels", "text": "With the knowledge of extended representations, which we use a multi-layered perceptron (MGRP) to form a form of interaction in the form \"i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i."}, {"heading": "Prior Knowledge Acquisition", "text": "In practice, we can use tags, keywords, themes or units related to the input as instances of prior knowledge, which could be obtained either from the metadata of the input or from additional algorithms and represent a summary of the general semantics of the input. Algorithms include daily recommendations (Wu et al. 2016), keyword extraction (Wu et al. 2015), topic modeling (Lead, Ng and Jordan 2003) and entity linkage (Han, Sun and Zhao 2011) to extract foreknowledge from multiple resources such as web documents, social media and knowledge base. In our experiments, we use question categories as foreknowledge in the QA task because the categories assigned by the ascetics can reflect the intention of the question. For conversations, we can better represent foreknowledge from the large source texts, social media and knowledge base."}, {"heading": "Experiments", "text": "We tested our model on two matching tasks: answer selection for answering questions and answer selection for conversation."}, {"heading": "Baseline", "text": "We considered the following models as baselines: Multi-layer perceptron (MLP): Each sentence is represented as a vector by averaging its word vectors.The two vectors were fed to a two-layer neural network to calculate a suitable score. MLP shared the embedding tables with our model.DeepMatchtopic: the matching model proposed in (Lu and Li 2013), which uses only topic information to Matching.CNNs: the Arc1 model proposed by Hu et al. (2014).CNTN: the evolution neural tensor network (Qiu and Huang 2015) to answer the community question. MatchPyramid: the model proposed by Pang et al. (Pang et al. 2016), which coincides with an approach to image recognition. The model is a special case of our single-channel model."}, {"heading": "Answer Selection", "text": "We used a public dataset of the selection of answers in SemEval 2015 (AlessandroMoschitti, Glass, and Randeree 2015), which collects question and answer pairs from Qatar Living Forum2 and asks to classify the answers into 3 categories (i.e. C = 3 in our model), including good, potential, and worse. The ratio of the three categories is 51: 10: 39. The statistics of the dataset are summarized in Table 2. We used classification accuracy as a yardstick. 2http: / / www.qatarliving.com / forumSpecific Setting In this task, we consider question categories marked as prior knowledge by the testers (both kx and ky). There are 27 categories in the Qatar Living data. Knowledge vector k was initialized by averaging the embedding of words in the category."}, {"heading": "Response Selection", "text": "The goal of the task is to select an appropriate response to a message from a candidate pool to realize a human-machine conversation. We used a public English conversation dataset, the Ubuntu Corpus (Lowe et al. 2015), to conduct the experiment. The corpus consists of a large number of human-to-human dialogues about the Ubuntu technique. Each dialogue contains at least 3 phrases, and we used only the topic kept3http: / / alt.qcri.org / semeval2015 / task3 / index.php? id = data and tools for comparing text pairs and ignoring context information. We used those from Xu et al. (Xu et al. 2016) 4, in which all urls and numbers were replaced by \"url\" and \"numbers."}, {"heading": "Discussions", "text": "We compared our model with two typical matching models: LSTM and MV-LSTM. We divided the text pairs into 4 groups according to length4https: / / www.dropbox.com / s / 2fdn26rj6h9bpvl / ubuntudata.zip? dl = 0the concatenation of the two parts of text. # Pair represents the number of pairs falling into the bucket. Results show that KEHNN works comparatively well with MVLSTM for relatively short text (i.e. length in [0, 30), while KEHNN significantly improves the accuracy of match for long text. Results confirmed our claim that cross-channel matching and prior knowledge can improve accuracy with long text."}, {"heading": "Conclusion", "text": "In this paper, KEHNN has been proposed, which can make use of previous knowledge in semantic matching. Experimental results show that our model can significantly exceed modern matching models for two matching tasks."}], "references": [], "referenceMentions": [], "year": 2016, "abstractText": "Long text brings a big challenge to semantic matching due to<lb>their complicated semantic and syntactic structures. To tackle<lb>the challenge, we consider using prior knowledge to help<lb>identify useful information and filter out noise to matching in<lb>long text. To this end, we propose a knowledge enhanced hy-<lb>brid neural network (KEHNN). The model fuses prior knowl-<lb>edge into word representations by knowledge gates and estab-<lb>lishes three matching channels with words, sequential struc-<lb>tures of sentences given by Gated Recurrent Units (GRU),<lb>and knowledge enhanced representations. The three channels<lb>are processed by a convolutional neural network to generate<lb>high level features for matching, and the features are synthe-<lb>sized as a matching score by a multilayer perceptron. The<lb>model extends the existing methods by conducting matching<lb>on words, local structures of sentences, and global context<lb>of sentences. Evaluation results from extensive experiments<lb>on public data sets for question answering and conversation<lb>show that KEHNN can significantly outperform the-state-of-<lb>the-art matching models and particularly improve the perfor-<lb>mance on pairs with long text.", "creator": "LaTeX with hyperref package"}}}