{"id": "1611.04686", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Nov-2016", "title": "Robust Matrix Regression", "abstract": "Modern technologies are producing datasets with complex intrinsic structures, and they can be naturally represented as matrices instead of vectors. To preserve the latent data structures during processing, modern regression approaches incorporate the low-rank property to the model and achieve satisfactory performance for certain applications. These approaches all assume that both predictors and labels for each pair of data within the training set are accurate. However, in real-world applications, it is common to see the training data contaminated by noises, which can affect the robustness of these matrix regression methods. In this paper, we address this issue by introducing a novel robust matrix regression method. We also derive efficient proximal algorithms for model training. To evaluate the performance of our methods, we apply it to real world applications with comparative studies. Our method achieves the state-of-the-art performance, which shows the effectiveness and the practical value of our method.", "histories": [["v1", "Tue, 15 Nov 2016 03:15:46 GMT  (284kb,D)", "http://arxiv.org/abs/1611.04686v1", "8 pages, 4 tables"]], "COMMENTS": "8 pages, 4 tables", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["hang zhang", "fengyuan zhu", "shixin li"], "accepted": false, "id": "1611.04686"}, "pdf": {"name": "1611.04686.pdf", "metadata": {"source": "CRF", "title": "Robust Matrix Regression", "authors": ["Hang Zhang", "Fengyuan Zhu", "Shixin Li"], "emails": ["hzhang@cse.cuhk.edu.hk,", "fyzhu@cse.cuhk.edu.hk,", "leept416@gmail.com"], "sections": [{"heading": "Introduction", "text": "This year, it has come to the point where one feels able to retaliate in order to retaliate."}, {"heading": "Robust Matrix Regression", "text": "First, we introduce the RMR model to solve the problem of noisy labels with an ADMM algorithm for model training."}, {"heading": "Model", "text": "The R-GLM approach (Zhou and Li, 2014) addresses this problem by enforcing the regression matrix W as low with the core standard penalty. To address this problem, it is an intuitive idea to ignore sounds within a small range {\u2212,} around each label for robust model fit. Motivated by this idea, we propose our RMR by introducing the hinge loss for model fit, where the residual property Xi is defined as a consequence property (W, b) = (tr (W > Xi) + b \u2212 yi (-yi | \u2212) +."}, {"heading": "Solver", "text": "Since the object function contains both a hinge loss and a core standard, the Nesterov method (R-GLM = > q =) used in R-GLM (Zhou and Li, 2014) can be efficiently applied. (Zhou and Li, 2014) is no longer available because the derivation of our loss function is not based on Lipschitz continuity. Nevertheless, since our model is convex with respect to W and b, we derive an efficient learning algorithm based on the alternative direction method of multipliers (ADMM) (Boyd et al., 2011) with the restart rule (Goldstein et al., 2014) to solve the optimization problem. The optimization problem defined in Eq. (3) can be written as follows: arg min W, b H (W, b) + G (4) s.t. S \u2212 W \u2212 W = 0where an auxiliary G = S is 4."}, {"heading": "Theoretical Analysis", "text": "We continue to theoretically analyze the excess risk of our RMR model. In (Wimalawarne, Tomioka and Sugiyama, 2016), we assume that each unit of a data matrix follows the standard Gaussian distribution. Then, the optimization problem of our RMR can be rewritten, where C1 and C2 are certain constants, and l (W, b, Xi, yi) is the hinge loss. Based on the relationship between W and b in equivalent (9), the loss function l (W, b, Xi, yi) can be simplified, where C1 and C2 are certain constants, and l (W, b, Xi, yi) is the hinge loss. Based on the relationship between W and b in equivalent (9), the loss function l (W, b, Xi, yi) asl (W, X, i, yi) asl (W, X, i, yi) is the hinge loss loss."}, {"heading": "Wo = argmin", "text": "W R (W), W (W), W (W), W (W), W (W), W (W), W (W), W (W), W (W), W (W), W (W), W (W), W (W), W (W), W (W), W (W), W (W), W (W), W (W), W (W), W (W), W (W), W (W), W (W), W (W), W (W), W (W), W (W), W (W), W (W), W (W), W (W), W (W), W (W), W (W), W (W), W (W), W (W), W (W), W (W), W (W), W (W), W (W), W (W), W (W), W (W), W (W), W (W), W (W), W (W), W (W), W (W), W (W), W (W), W (W), W (W), W (W), W (W (W), W (W), W (W), W (W (W), W (W), W (W (W), W (W), W (W (W), W (W (W), W (W), W (W (W), W (W (W), W (W (W), W (W (W), W (W (W), W (W (W), W (W (W), W (W (W (W), W (W (W), W (W (W), W (W (W), W (W (W), W (W (W), W (W (W), W (W (W (W), W (W (W), W (W), W (W (W), W (W (W), W (W), W (W (W), W (W (W), W (W), W (W (W"}, {"heading": "Generalized Robust Matrix Regression", "text": "To solve this problem, we are introducing a generalized RMR (G-RMR), which assumes that each noisy data matrix can be decomposed as a latent clean signal plus outlier. Clean signals can be recovered from any noisy signal when learning the regression model."}, {"heading": "Model", "text": "As discussed in (Cande, et al., 2011), it is customary for natural signals to have an empirical correlation, so if we stack each vectorized latent clean data matrix, the resulting matrix should be represented as low. We assign the sparsity attribute with the L1 standard to the outliers for robust modeling. With these settings, the optimization problem of the G-RMR can be defined as the following input matrix: W, bH (W, b) + \u03c4 | W | | | X | \u043c + \u03b3 | E | | 1 (26) s.t. D = X + E. Here, Di denotes the ith noisy input matrix, and we assume that it can be decomposed as Di = Xi + Ei, with Xi as the latently clean matrix representing the signal, and Ei as the outlier. D is amatrix with the ith series as the vector form of Di, X is the sparse restitution of the matrix with the line matrix being considered as the low sensitivity."}, {"heading": "Solver", "text": "The optimization problem for G-RMR includes three other non-smooth terms. Fortunately, the object function in Eq (26) is not yet solved, which means that the optimization problem in Eq (26) is divided into two sub-problems and each sub-problem in ADMM can be solved individually. Therefore, the first sub-problem is to solve W with X and E, which is equivalent to solving the problem in Eq (3), and the second sub-problem is to solve X and E with W."}, {"heading": "Experiments", "text": "In this section we conduct extensive experiments with comparative studies, the proposed method being implemented by Matlab R2015a in a machine with 3.7 GHz quad-core CPU and 16 GB of memory. We examine the performance of our RMR and G-RMR in comparison of two state-of-the-art methods: 1) Classic Support Vector Regression (SVR) (Smola and Schoolhead, 2004); 2) Regulated Matrix Regression (R-GLM) (Zhou and Li, 2014). To evaluate and compare the performance of these algorithms, we apply them to three empirical tasks. First, we elaborate the illustrative examples by examining different geometric and natural forms on the regression matrix. Second, we apply them to real time series data, with each sample being represented as a matrix."}, {"heading": "Shape Recovery", "text": "We use the regression matrix to generate each sample by the following equation: yi = tr (W > Xi) + b + i, (34) where W is the regression matrix illustrated by a waveform, (Xi, yi) is a randomly generated sample, b is a bias term, and i is the noise term on the label yi that is sampled from the laplactic distribution (the probability distribution function is P (x | \u00b5, \u03c3) = 1 2\u03c3 exp (\u2212 x \u2212 \u00b5 | \u03c3). In the experiment, we randomly generate 1000 samples for 10 rounds. In each round, half of the samples are used for modelling and the rest for testing. Then, we calculate the mean and standard deviation from RAE errors on the classification matrix W."}, {"heading": "Financial Time Series Data Analysis", "text": "We continue to evaluate our RMR on the financial time series data. We use daily price data (details can be found in Table 2) from (Akbilgic, Bozdogan and Balaban, 2014), where prices are converted into returns. In this data set, there are 536 daily returns from January 5, 2009 to February 22, 2011. In particular, the days on which the Turkish stock exchange was closed are excluded. Intuitively, the value of an index on a given date can be associated with other and previous values. Therefore, it is natural to process data in matrix form, rather than using a vector to maintain the latent topological structure of the data. Furthermore, there are many fluctuations in different indices due to the complicated stock market and many entries of the data are contaminated accordingly. Therefore, it is imperative to address the above problems with the proposed methods."}, {"heading": "Head Pose Data Analysis", "text": "The data set arose from a study to estimate the human pose using digital images taken by cameras, with 37 people in grayscale 100 x 100 image sizes, which can be naturally represented as data in matrix form. Each person has 133 facial images covering a sphere of vision of \u00b1 90 degrees in yaw position and \u00b1 30 degrees in inclination in 10-degree increments. Several sample images can be found in Fig. 3. As previously discussed, it is difficult to accurately measure the real angle of view of the human head, so it is expected that the caption of each image may contain several small errors, leading to the loud labeling problem. For each person, we record the degree of yaw angle and use the degree of yaw angle [0, 10,... 170, 180, 180 \u00b0] as a label. We then set the degree of face correction to 90 \u00b0, which shows the frontal image. Each image is processed within the yaw angle, with the data being compared by 32 x and 4 x in each column."}, {"heading": "Conclusions", "text": "In this paper, we addressed the problem of robust matrix regression with two new proposed methods, namely RMR and GRMR. For RMR, we introduced the hinge loss for model fitting to improve the robustness of matrix regression methods against the problem of noisy labels, derived an ADMM algorithm for model training, proposed the G-RMR as an extension of RMR to consider noisy predictors through clean recovery of matrix signals during model training, and conducted extensive empirical studies to evaluate the performance of RMR and G-RMR, and our methods provide state-of-the-art performance."}], "references": [{"title": "A novel hybrid rbf neural networks model as a forecaster", "author": ["O. Akbilgic", "H. Bozdogan", "M.E. Balaban"], "venue": "Statistics and Computing 24(3):365\u2013375.", "citeRegEx": "Akbilgic et al\\.,? 2014", "shortCiteRegEx": "Akbilgic et al\\.", "year": 2014}, {"title": "Distributed optimization and statistical learning via the alternating direction method of multipliers", "author": ["S. Boyd", "N. Parikh", "E. Chu", "B. Peleato", "J. Eckstein"], "venue": "Foundations and Trends R", "citeRegEx": "Boyd et al\\.,? 2011", "shortCiteRegEx": "Boyd et al\\.", "year": 2011}, {"title": "Robust principal component analysis", "author": ["E.J. Cand\u00e8s", "X. Li", "Y. Ma", "J. Wright"], "venue": "Journal of the ACM (JACM)", "citeRegEx": "Cand\u00e8s et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Cand\u00e8s et al\\.", "year": 2011}, {"title": "Fast alternating direction optimization methods", "author": ["T. Goldstein", "B. O\u2019Donoghue", "S. Setzer", "R. Baraniuk"], "venue": "SIAM Journal on Imaging Sciences", "citeRegEx": "Goldstein et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Goldstein et al\\.", "year": 2014}, {"title": "On non-ergodic convergence rate of douglas\u2013rachford alternating direction method of multipliers", "author": ["B. He", "X. Yuan"], "venue": "Numerische Mathematik 130(3):567\u2013577.", "citeRegEx": "He and Yuan,? 2015", "shortCiteRegEx": "He and Yuan", "year": 2015}, {"title": "Ridge regression: Biased estimation for nonorthogonal problems", "author": ["A.E. Hoerl", "R.W. Kennard"], "venue": "Technometrics 12(1):55\u201367.", "citeRegEx": "Hoerl and Kennard,? 1970", "shortCiteRegEx": "Hoerl and Kennard", "year": 1970}, {"title": "Robust regression", "author": ["D. Huang", "R. Cabral", "F. De la Torre"], "venue": "IEEE transactions on pattern analysis and machine intelligence 38(2):363\u2013375.", "citeRegEx": "Huang et al\\.,? 2016", "shortCiteRegEx": "Huang et al\\.", "year": 2016}, {"title": "Convergence of a generalized smo algorithm for svm classifier design", "author": ["S.S. Keerthi", "E.G. Gilbert"], "venue": "Machine Learning 46(1-3):351\u2013360.", "citeRegEx": "Keerthi and Gilbert,? 2002", "shortCiteRegEx": "Keerthi and Gilbert", "year": 2002}, {"title": "Support matrix machines", "author": ["L. Luo", "Y. Xie", "Z. Zhang", "W.-J. Li"], "venue": "International Conference on Machine Learning (ICML).", "citeRegEx": "Luo et al\\.,? 2015", "shortCiteRegEx": "Luo et al\\.", "year": 2015}, {"title": "Financial markets: very noisy information processing", "author": ["M. Magdon-Ismail", "A. Nicholson", "Y.S. Abu-Mostafa"], "venue": "Proceedings of the IEEE 86(11):2184\u20132195.", "citeRegEx": "Magdon.Ismail et al\\.,? 1998", "shortCiteRegEx": "Magdon.Ismail et al\\.", "year": 1998}, {"title": "Excess risk bounds for multitask learning with trace norm regularization", "author": ["A. Maurer", "M. Pontil"], "venue": "Conference on Learning Theory (COLT), volume 30, 55\u201376.", "citeRegEx": "Maurer and Pontil,? 2013", "shortCiteRegEx": "Maurer and Pontil", "year": 2013}, {"title": "Sequential minimal optimization: A fast algorithm for training support vector machines", "author": ["J Platt"], "venue": null, "citeRegEx": "Platt,? \\Q1998\\E", "shortCiteRegEx": "Platt", "year": 1998}, {"title": "Fusion of perceptual cues for robust tracking of head pose and position", "author": ["J. Sherrah", "S. Gong"], "venue": "Pattern Recognition 34(8):1565\u20131572.", "citeRegEx": "Sherrah and Gong,? 2001", "shortCiteRegEx": "Sherrah and Gong", "year": 2001}, {"title": "A tutorial on support vector regression", "author": ["A.J. Smola", "B. Sch\u00f6lkopf"], "venue": "Statistics and computing 14(3):199\u2013222.", "citeRegEx": "Smola and Sch\u00f6lkopf,? 2004", "shortCiteRegEx": "Smola and Sch\u00f6lkopf", "year": 2004}, {"title": "Regression shrinkage and selection via the lasso", "author": ["R. Tibshirani"], "venue": "Journal of the Royal Statistical Society. Series B (Methodological) 267\u2013288.", "citeRegEx": "Tibshirani,? 1996", "shortCiteRegEx": "Tibshirani", "year": 1996}, {"title": "Theoretical and experimental analyses of tensor-based regression and classification", "author": ["K. Wimalawarne", "R. Tomioka", "M. Sugiyama"], "venue": "Neural computation 28(4):686\u2013715.", "citeRegEx": "Wimalawarne et al\\.,? 2016", "shortCiteRegEx": "Wimalawarne et al\\.", "year": 2016}, {"title": "Modeling appearances with low-rank svm", "author": ["L. Wolf", "H. Jhuang", "T. Hazan"], "venue": "2007 IEEE Conference on Computer Vision and Pattern Recognition, 1\u20136. IEEE.", "citeRegEx": "Wolf et al\\.,? 2007", "shortCiteRegEx": "Wolf et al\\.", "year": 2007}, {"title": "Regularized matrix regression", "author": ["H. Zhou", "L. Li"], "venue": "Journal of the Royal Statistical Society: Series B (Statistical Methodology) 76(2):463\u2013483.", "citeRegEx": "Zhou and Li,? 2014", "shortCiteRegEx": "Zhou and Li", "year": 2014}], "referenceMentions": [{"referenceID": 5, "context": "Introduction Classical regression methods, such as ridge regression (Hoerl and Kennard, 1970) and lasso (Tibshirani, 1996) are basically designed for data in vector form.", "startOffset": 68, "endOffset": 93}, {"referenceID": 14, "context": "Introduction Classical regression methods, such as ridge regression (Hoerl and Kennard, 1970) and lasso (Tibshirani, 1996) are basically designed for data in vector form.", "startOffset": 104, "endOffset": 122}, {"referenceID": 17, "context": "When using traditional regression methods to process these data, we have to reshape them into vectors, which may destroy the latent topological structural information, such as the correlation between different channels for EEG data (Zhou and Li, 2014), and the spatial relation within an image (Wolf, Jhuang, and Hazan, 2007).", "startOffset": 232, "endOffset": 251}, {"referenceID": 17, "context": "One such model is the regularized matrix regression (RGLM) (Zhou and Li, 2014).", "startOffset": 59, "endOffset": 78}, {"referenceID": 12, "context": "As an example, consider the problem of head pose estimation (Sherrah and Gong, 2001), where for each data pair, the predictor is a two dimensional digital image for the head of a person, while the output denotes the angle of his head.", "startOffset": 60, "endOffset": 84}, {"referenceID": 13, "context": "the support vector regression model (Smola and Sch\u00f6lkopf, 2004), it is an algorithm based on vector-form data, which can ruin the latent structure for matrix regression problem.", "startOffset": 36, "endOffset": 63}, {"referenceID": 17, "context": "The R-GLM approach (Zhou and Li, 2014) addresses this issue by enforcing the regression matrix W to be low-rank representable with nuclear norm penalty.", "startOffset": 19, "endOffset": 38}, {"referenceID": 13, "context": "Similar residual modeling approach has also been used in the method of support vector regression (Smola and Sch\u00f6lkopf, 2004).", "startOffset": 97, "endOffset": 124}, {"referenceID": 8, "context": "Differently, our method can capture such latent structure by incorporating the spectral elastic net penalty (Luo et al., 2015) into the regression matrix W, which can model the correlation of each data matrix effectively.", "startOffset": 108, "endOffset": 126}, {"referenceID": 8, "context": "with 1 2 tr(W>W) + \u03c4 ||W||\u2217 as the spectral elastic net penalty, we incorporate low-rank property into W and consider the group effect of the eigenvalues, to capture the latent structures among data matrices (Luo et al., 2015).", "startOffset": 208, "endOffset": 226}, {"referenceID": 17, "context": "Solver As the object function contains both hinge loss and nuclear norm, the Nesterov method used in R-GLM (Zhou and Li, 2014) is no longer available because the derivative of our loss function is not Lipschitz-continuous.", "startOffset": 107, "endOffset": 126}, {"referenceID": 1, "context": "Nevertheless, since our model is convex with respect to both W and b, we here derive an efficient learning algorithm based on Alternating Direction Method of Multipliers (ADMM) (Boyd et al., 2011) with the restart rule (Goldstein et al.", "startOffset": 177, "endOffset": 196}, {"referenceID": 3, "context": ", 2011) with the restart rule (Goldstein et al., 2014) to solve the optimization problem.", "startOffset": 30, "endOffset": 54}, {"referenceID": 7, "context": "In this way, we can get W and b in each iteration with sequential minimization optimization algorithm (Keerthi and Gilbert, 2002; Platt and others, 1998).", "startOffset": 102, "endOffset": 153}, {"referenceID": 3, "context": "Because the hinge loss and nuclear norm are weakly convex, the convergence property of Algorithm 1 can be proved immediately based on the result in (Goldstein et al., 2014; He and Yuan, 2015).", "startOffset": 148, "endOffset": 191}, {"referenceID": 4, "context": "Because the hinge loss and nuclear norm are weakly convex, the convergence property of Algorithm 1 can be proved immediately based on the result in (Goldstein et al., 2014; He and Yuan, 2015).", "startOffset": 148, "endOffset": 191}, {"referenceID": 10, "context": "Let R(W) and R\u0302(W) be the empirical risk and expected risk respectively (Maurer and Pontil, 2013).", "startOffset": 72, "endOffset": 97}, {"referenceID": 10, "context": "Thus, following Maurer and Pontil (2013), with the Gordan\u2019s theorem, we have E||M\u0303||\u2217 \u2264 \u221a \u03c9( \u221a p+ \u221a q)", "startOffset": 16, "endOffset": 41}, {"referenceID": 2, "context": "Model As discussed in (Cand\u00e8s et al., 2011), it is common for natural signals to contain correlation empirically.", "startOffset": 22, "endOffset": 43}, {"referenceID": 13, "context": "We investigate the performance of our RMR and G-RMR with comparison of two state-of-the-art methods: 1) The classical Support Vector Regression (SVR) (Smola and Sch\u00f6lkopf, 2004); 2) Regularized Matrix Regression (R-GLM) (Zhou and Li, 2014).", "startOffset": 150, "endOffset": 177}, {"referenceID": 17, "context": "We investigate the performance of our RMR and G-RMR with comparison of two state-of-the-art methods: 1) The classical Support Vector Regression (SVR) (Smola and Sch\u00f6lkopf, 2004); 2) Regularized Matrix Regression (R-GLM) (Zhou and Li, 2014).", "startOffset": 220, "endOffset": 239}, {"referenceID": 12, "context": "At last, we apply RMR and G-RMR on the application of human head pose estimation (Sherrah and Gong, 2001).", "startOffset": 81, "endOffset": 105}, {"referenceID": 12, "context": "We further test the performance of our methods on the application of head pose estimation with dataset used in (Sherrah and Gong, 2001) .", "startOffset": 111, "endOffset": 135}], "year": 2016, "abstractText": "Modern technologies are producing datasets with complex intrinsic structures, and they can be naturally represented as matrices instead of vectors. To preserve the latent data structures during processing, modern regression approaches incorporate the low-rank property to the model, and achieve satisfactory performance for certain applications. These approaches all assume that both predictors and labels for each pair of data within the training set are accurate. However, in real world applications, it is common to see the training data contaminated by noises, which can affect the robustness of these matrix regression methods. In this paper, we address this issue by introducing a novel robust matrix regression method. We also derive efficient proximal algorithms for model training. To evaluate the performance of our methods, we apply it on real world applications with comparative studies. Our method achieves the state-of-the-art performance, which shows the effectiveness and the practical value of our method. Introduction Classical regression methods, such as ridge regression (Hoerl and Kennard, 1970) and lasso (Tibshirani, 1996) are basically designed for data in vector form. However, with the development of modern technology, it is common to meet datasets with sample unit not in vector form but instead in matrix form. Examples include the two-dimensional digital images, with quantized values of different colors at certain rows and columns of pixels; and electroencephalography (EEG) data with voltage fluctuations at multiple channels over a period of time. When using traditional regression methods to process these data, we have to reshape them into vectors, which may destroy the latent topological structural information, such as the correlation between different channels for EEG data (Zhou and Li, 2014), and the spatial relation within an image (Wolf, Jhuang, and Hazan, 2007). To tackle this issue, several methods have been proposed to perform regression on data in matrix form directly. One such model is the regularized matrix regression (RGLM) (Zhou and Li, 2014). Given a dataset {Xi, yi}i=1, where N is the sample size, Xi \u2208 Rp\u00d7q denotes the ith data matrix as predictor, and yi \u2208 R is the corresponding output, the R-GLM model aims to learn a function f : Rp\u00d7q \u2192 R to identify the output given a newly observed data matrix with yi = tr(WXi) + b+ , (1) where tr(\u00b7) represents the trace of a matrix, W is the regression matrix with low-rank property to preserve structural information of each data matrix, and b denotes the offset. is zero-mean Gaussian noise to model the small uncertainty of the output. With this setting, the R-GLM has achieved satisfying results in several applications. However, there still exist certain issues that should be further addressed. Firstly, RGLM uses the Gaussian noise for model fitting, and take all deviations of predicted values from labels into account. This setting can be reasonable in certain cases, but may not make sense for particular applications. As an example, consider the problem of head pose estimation (Sherrah and Gong, 2001), where for each data pair, the predictor is a two dimensional digital image for the head of a person, while the output denotes the angle of his head. Because the real angle of head cannot be measured precisely, there should exist certain deviations of provided labels from the real ones empirically. In this case, the regression model should be able to tolerant such small deviations instead of taking them all into account. Another important issue is that, the predictors are also assumed to be noise free, which can be irrational in certain applications. Practically, it is common to see signals corrupted by noise, such as image signals with occlusion, specular reflections or noise (Huang, Cabral, and De la Torre, 2016), and financial data with noise (Magdon-Ismail, Nicholson, and Abu-Mostafa, 1998). Thus, it is important for a regression model to be tolerant of noise on predictors and labels to enhance its robustness empirically. In this paper, we introduce two novel matrix regression methods to tackle the above mentioned issues. We first propose a \u201cRobust Matrix Regression\u201d (RMR) to tackle the noisy label problem, by introducing hinge loss to model the uncertainty of regression labels. In this way, our method only considers error larger than a pre-specified value, and can tolerate error around each labeled output within a small range. This approach is also favored for other advantages in certain scenarios. As an example, in applications like financial time-series prediction, it is common to require not to lose more than money when dealing with data like exchange rates, and this issue can be well addressed with our setting. Even though the hinge loss error has been used in ar X iv :1 61 1. 04 68 6v 1 [ cs .L G ] 1 5 N ov 2 01 6 the support vector regression model (Smola and Sch\u00f6lkopf, 2004), it is an algorithm based on vector-form data, which can ruin the latent structure for matrix regression problem. We then propose efficient ADMM method to solve the optimization problem iteratively. To further enhance the robustness of RMR with noisy predictors, we propose a generalized RMR (G-RMR) by decomposing each data matrix as latent clean signal plus sparse outliers. For model training, we also derive a proximal algorithm to estimate both the regression matrix and latent clean signals iteratively. To evaluate the performance of our methods, we conduct extensive experiments on both approaches with comparison of state-of-the-art ones. Our methods achieve superior performance consistently, which shows their efficiency in real world problems. Notations: We present the scalar values with lower case letters (e.g., x); vectors by bold lower case letters (e.g., x); and matrix by bold upper case letters (e.g., X). For a matrix X, its (i, j)-entity is represented as Xi,j . tr(\u00b7) denotes the trace of a matrix, and {a}+ = max(0, a). We further set ||X||F and ||X||\u2217 as the Frobenius norm and nuclear norm of a matrix X respectively. Robust Matrix Regression We first introduce the RMR model to address the noisy label problem, with an ADMM algorithm for model training. Model For matrix regression, classical techniques need to reshape each matrix Xi into a vector xi, which will destroy its intrinsic structures, resulting in the loss of information. The R-GLM approach (Zhou and Li, 2014) addresses this issue by enforcing the regression matrix W to be low-rank representable with nuclear norm penalty. However, this method is based on the Gaussian loss, which may affect the robustness with existence of noisy labels. To tackle this issue, an intuitive idea is to ignore noises within a small margin {\u2212 , } around each label for robust model fitting. Motivated by this idea, we propose our RMR, by introducing the hinge loss for model fitting, where the residual corresponding to each data Xi is defined as follows hi(W, b) = (|tr(WXi) + b\u2212 yi| \u2212 )+. (2) With the above formulation of residuals, when learning the regression model, our approach only takes residuals larger than into account, thus, the labels contaminated by noise within a small margin is tolerable accordingly. Similar residual modeling approach has also been used in the method of support vector regression (Smola and Sch\u00f6lkopf, 2004). However, this approach is proposed for vector data regression and cannot capture the latent structure within each data matrix. Differently, our method can capture such latent structure by incorporating the spectral elastic net penalty (Luo et al., 2015) into the regression matrix W, which can model the correlation of each data matrix effectively. And the corresponding optimization problem is defined as follows argmin W,b H(W, b) + \u03c4 ||W||\u2217 (3) where H(W, b) = 1 2 tr(W>W)", "creator": "LaTeX with hyperref package"}}}