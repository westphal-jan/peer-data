{"id": "1611.04741", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Nov-2016", "title": "A Neural Architecture Mimicking Humans End-to-End for Natural Language Inference", "abstract": "In this work we use the recent advances in representation learning to propose a neural architecture for the problem of natural language inference. Our approach is aligned to mimic how a human does the natural language inference process given two statements. The model uses variants of Long Short Term Memory (LSTM), attention mechanism and composable neural networks, to carry out the task. Each part of our model can be mapped to a clear functionality humans do for carrying out the overall task of natural language inference. The model is end-to-end differentiable enabling training by stochastic gradient descent. On Stanford Natural Language Inference(SNLI) dataset, the proposed model achieves better accuracy numbers than all published models in literature.", "histories": [["v1", "Tue, 15 Nov 2016 08:48:22 GMT  (165kb,D)", "http://arxiv.org/abs/1611.04741v1", "8 pages, 2 figures"], ["v2", "Fri, 27 Jan 2017 05:36:05 GMT  (167kb,D)", "http://arxiv.org/abs/1611.04741v2", "8 pages, 2 figures"]], "COMMENTS": "8 pages, 2 figures", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["biswajit paria", "k m annervaz", "ambedkar dukkipati", "ankush chatterjee", "sanjay podder"], "accepted": false, "id": "1611.04741"}, "pdf": {"name": "1611.04741.pdf", "metadata": {"source": "CRF", "title": "A Neural Architecture Mimicking Humans End-to-End for Natural Language Inference", "authors": ["Biswajit Paria", "Ambedkar Dukkipati", "Ankush Chatterjee", "Sanjay Podder"], "emails": ["biswajitsc@iitkgp.ac.in", "annervaz.km@csa.iisc.ernet.in", "ad@csa.iisc.ernet.in", "ankushchatterjee@iitkgp.ac.in", "sanjay.podder@accenture.com"], "sections": [{"heading": null, "text": "This year, it has reached the point where it will be able to leave the country without being able to leave it."}, {"heading": "II. PRELIMINARIES AND BACKGROUND", "text": "In an in-depth learning framework, the sentences of natural language are first transformed into a numerical representation by word embedding, and these numerical representations are then encoded using a bidirectional LSTM or a binary tree LSTM to account for various snippets of information along with the context in which they appear. Attention mechanisms are used to learn the portions of information that need to be aligned and processed according to the context, and the pairs generated by attention mechanisms are then processed separately using a number of different operators selected by soft-gating, and the results of the different process pairs are then aggregated or put together to complete the final prediction task. Below, we briefly describe concepts of word embedding and LSTMs. Attention mechanism and composition and their motivations are presented together with the model."}, {"heading": "A. Word Embeddings", "text": "The first challenge in applying deep learning models for NLP is to find a correct numerical representation for words. \"You shall know a word by the company it keeps\" (Firth, J. R. 1957: 11), is one of the most influential ideas in the processing of natural language. Several models for representing a word as a numerical vector, based on the context it appears, originate from this idea. Many vector representations for words have been proposed, including the well-known latent semantic indexing [Dumais, 2004]. Vector representations for words in the context of neural networks have been mapped by [Bengio et al., 2003] In this paper, a distributed vocabulary vector is mapped to each word in the vocabulary, w-Rm. The probability distribution of word strings, P (wt | wt \u2212 1), [vt \u2212 1), vector models are mapped."}, {"heading": "B. Recurrent Neural Network and Long Short Term Memory(LSTM)", "text": "The basic idea behind Recurrent Neural Networks (RNN) is to capture and encode the information that is present in a particular sequence such as text. Faced with a sequence of words, a numerical representation (GloVe or Word2Vec vectors) for a word is fed into a neural network and the results are calculated. As the output of the next word is executed, the output from the previous word (or time step) is also called recursive because they perform the same calculation for each element of a sequence, using the results from previous calculations. At each step, RNN performs the following calculation, RNN (ti) = f (W xti + U). RNN is the output in the language in which W and U are the traceable parameters of the model, and f is a nonlinear function. Bias terms are left here and must be added appropriately."}, {"heading": "III. THE PROPOSED MODEL", "text": "The model first encodes the sentences using a normal biLSTM or a btree-LSTM. This means looking at the different segments of the sentence together with the context, which is an essential part of human processing, as explained above. In the case of Bi-LSTM, the encodings are expanded along with the corresponding word vectors to produce improved encodings. In the case of Btree-LSTM encodings, this improvement is not performed because there is no one-to-one agreement with the number of words in the sentence after encoding. If the Bi-LSTM encodings are performed, there are n encodings for an n-length set, where btree-LSTM encodings are performed, there are 2n-1 encodes. btree-LSTMs considers more possible phrase structures (along with the context) of the input set compared to a BvvLSTM, as shown below (vM \u00b7 S \u00b7 1) (various information)."}, {"heading": "A. Attention Mechanism", "text": "The attention mechanism was recently introduced in the context of machine translation [Bahdanau et al., 2014; Luong et al., 2015], where in words or phrases from one language words or phrases in another language must be mapped or aligned for the purpose of translation. We use a similar concept to learn this alignment for our purpose of NLI. Given two vector sets, a = {a1,.., an} and b = {b1,.., bn}, the attention value (a numerical quantity) vij for each element of the first sentence ai is mapped to each element of the second sentence ai. Forall ai, a, attend (b1, \u00b7, bn), ai), ai), (vi1, \u00b7 \u00b7 \u00b7, vin), where, vij = (bj) Tai-r (br), TaiOne can see that for all i elements of the second sentence i."}, {"heading": "B. Task Composition", "text": "Such an approach to answering questions was introduced by [Andreas et al., 2016]. We adapt this approach for our purpose here. After learning the arrangement of encodings, we need to perform various functions or comparisons, depending on the type of input and the sentence context, to see if they make a positive or negative contribution to the final prediction. In our example, after aligning the encoding to the man and the woman with the encoding of the model, the model has to process whether they are equivalent. Similarly, after aligning the model to a broad awakening, the model needs to make a different type of prediction to verify that a wide awakening is being followed from walking. Again, if all birds are aligned with the Canaries, the model could be used for some kind of relationship or a subset of relationship."}, {"heading": "C. Relevant Previous Work", "text": "NLI is a well-researched problem with a rich literature using classical machine learning techniques. With the advent of deep learning, many models, including LSTMs, have been used for NLI. Recently, the Stanford Natural Language Inference (SNLI) dataset was created [Bowman et al., 2015] using crowd sourcing. Many deep learning models have been benchmarked for NLI using this dataset. The detailed list is available at http: / / nlp.stanford. edu / projects / snli /. This latest thesis Bowman [2016] covers deep learning-based work in detail. Many of the deep learning-based work relied on the creation of encryptions of sentences using LSTMs or evolutionary neural networks or recursive recursive recursive neural networks, and then using these encryptions for the final prediction task Bowman et al. [2016]; Mou et Chendal. [all these types of work are 2015]."}, {"heading": "IV. EXPERIMENTS AND EVALUATION", "text": "In fact, it is in such a way that most people are able to recognize themselves and understand what they are doing, if they see themselves in a position to change the world, \"he said in an interview with the\" New York Times \":\" It is not the first time that the world is able to change the world. \"He added:\" It is not the first time that the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world"}, {"heading": "V. CONCLUSION & FUTURE WORK", "text": "The model is universally differentiated and allows training through simple stochastic gradient lineage. We hope that further experiments and the adjustment of hyperparameters will further improve these outcomes. There are various improvements for the possible and potential future directions of the model. The btree-LSTM currently uses a complete binary tree structure, which is formed by recursively considering adjacent encodings. A binary tree learning scheme, similar to [Bowman et al., 2016] uses various improvements for the possible and potential directions. btree-LSTM currently uses a complete binary tree structure, which is formed by recursively considering adjacent encodings. A binary tree learning scheme, similar to [Bowman et al., 2016] uses the model of storage. The tree structure is based on the arrangement of attention values."}], "references": [{"title": "Learning to compose neural networks for question answering", "author": ["Jacob Andreas", "Marcus Rohrbach", "Trevor Darrell", "Dan Klein"], "venue": "arXiv preprint arXiv:1601.01705,", "citeRegEx": "Andreas et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Andreas et al\\.", "year": 2016}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1409.0473,", "citeRegEx": "Bahdanau et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Learning long-term dependencies with gradient descent is difficult", "author": ["Yoshua Bengio", "Patrice Simard", "Paolo Frasconi"], "venue": "IEEE transactions on neural networks,", "citeRegEx": "Bengio et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 1994}, {"title": "A neural probabilistic language model", "author": ["Yoshua Bengio", "R\u00e9jean Ducharme", "Pascal Vincent", "Christian Janvin"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Bengio et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2003}, {"title": "A brief history of natural logic", "author": ["Johan van Benthem"], "venue": "College Publications,", "citeRegEx": "Benthem.,? \\Q2008\\E", "shortCiteRegEx": "Benthem.", "year": 2008}, {"title": "Pattern recognition and machine learning", "author": ["Christopher M Bishop"], "venue": "springer,", "citeRegEx": "Bishop.,? \\Q2006\\E", "shortCiteRegEx": "Bishop.", "year": 2006}, {"title": "Stochastic Gradient Tricks, volume 7700, page 430445", "author": ["L\u00e9on Bottou"], "venue": null, "citeRegEx": "Bottou.,? \\Q2012\\E", "shortCiteRegEx": "Bottou.", "year": 2012}, {"title": "A large annotated corpus for learning natural language inference", "author": ["Samuel R. Bowman", "Gabor Angeli", "Christopher Potts", "Christopher D. Manning"], "venue": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Lan-", "citeRegEx": "Bowman et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bowman et al\\.", "year": 2015}, {"title": "A fast unified model for parsing and sentence understanding", "author": ["Samuel R Bowman", "Jon Gauthier", "Abhinav Rastogi", "Raghav Gupta", "Christopher D Manning", "Christopher Potts"], "venue": "arXiv preprint arXiv:1603.06021,", "citeRegEx": "Bowman et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Bowman et al\\.", "year": 2016}, {"title": "Modeling Natural Language Semantics in Learned Representations", "author": ["Samuel Ryan Bowman"], "venue": "PhD thesis, Stanford University,", "citeRegEx": "Bowman.,? \\Q2016\\E", "shortCiteRegEx": "Bowman.", "year": 2016}, {"title": "Long shortterm memory-networks for machine reading", "author": ["Jianpeng Cheng", "Li Dong", "Mirella Lapata"], "venue": "arXiv preprint arXiv:1601.06733,", "citeRegEx": "Cheng et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Cheng et al\\.", "year": 2016}, {"title": "Latent semantic analysis", "author": ["Susan T Dumais"], "venue": "Annual review of information science and technology,", "citeRegEx": "Dumais.,? \\Q2004\\E", "shortCiteRegEx": "Dumais.", "year": 2004}, {"title": "Lstm: A search space odyssey", "author": ["Klaus Greff", "Rupesh Kumar Srivastava", "Jan Koutn\u0131\u0301k", "Bas R Steunebrink", "J\u00fcrgen Schmidhuber"], "venue": "arXiv preprint arXiv:1503.04069,", "citeRegEx": "Greff et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Greff et al\\.", "year": 2015}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "Hochreiter and Schmidhuber.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["Sergey Ioffe", "Christian Szegedy"], "venue": "arXiv preprint arXiv:1502.03167,", "citeRegEx": "Ioffe and Szegedy.,? \\Q2015\\E", "shortCiteRegEx": "Ioffe and Szegedy.", "year": 2015}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "Kingma and Ba.,? \\Q2014\\E", "shortCiteRegEx": "Kingma and Ba.", "year": 2014}, {"title": "Ask me anything: Dynamic memory networks for natural language processing", "author": ["Robert English", "Brian Pierce", "Peter Ondruska", "Ishaan Gulrajani", "Richard Socher"], "venue": "arXiv preprint arXiv:1506.07285,", "citeRegEx": "English et al\\.,? \\Q2015\\E", "shortCiteRegEx": "English et al\\.", "year": 2015}, {"title": "Lccs gistexter at duc 2006: Multi-strategy multi-document summarization", "author": ["Finley Lacatusu", "Andrew Hickl", "Kirk Roberts", "Ying Shi", "Jeremy Bensley", "Bryan Rink", "Patrick Wang", "Lara Taylor"], "venue": "In Proceedings of DUC06,", "citeRegEx": "Lacatusu et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Lacatusu et al\\.", "year": 2006}, {"title": "Effective approaches to attention-based neural machine translation", "author": ["Minh-Thang Luong", "Hieu Pham", "Christopher D Manning"], "venue": "arXiv preprint arXiv:1508.04025,", "citeRegEx": "Luong et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Natural language inference", "author": ["Bill MacCartney"], "venue": "PhD thesis,", "citeRegEx": "MacCartney.,? \\Q2009\\E", "shortCiteRegEx": "MacCartney.", "year": 2009}, {"title": "An extended model of natural logic. In Proceedings of the eighth international conference on computational semantics, pages 140\u2013156", "author": ["Bill MacCartney", "Christopher D Manning"], "venue": "Association for Computational Linguistics,", "citeRegEx": "MacCartney and Manning.,? \\Q2009\\E", "shortCiteRegEx": "MacCartney and Manning.", "year": 2009}, {"title": "Semeval-2014 task 1: Evaluation of compositional distributional semantic models on full sentences through semantic relatedness and textual entailment", "author": ["Marco Marelli", "Luisa Bentivogli", "Marco Baroni", "Raffaella Bernardi", "Stefano Menini", "Roberto Zamparelli"], "venue": null, "citeRegEx": "Marelli et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Marelli et al\\.", "year": 2014}, {"title": "A sick cure for the evaluation of compositional distributional semantic models", "author": ["Marco Marelli", "Stefano Menini", "Marco Baroni", "Luisa Bentivogli", "Raffaella Bernardi", "Roberto Zamparelli"], "venue": "In LREC,", "citeRegEx": "Marelli et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Marelli et al\\.", "year": 2014}, {"title": "Efficient estimation of word representations in vector space", "author": ["Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean"], "venue": "arXiv preprint arXiv:1301.3781,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Linguistic regularities in continuous space word representations", "author": ["Tomas Mikolov", "Wen-tau Yih", "Geoffrey Zweig"], "venue": "In HLT-NAACL,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Natural language inference by tree-based convolution and heuristic matching", "author": ["Lili Mou", "Rui Men", "Ge Li", "Yan Xu", "Lu Zhang", "Rui Yan", "Zhi Jin"], "venue": "In The 54th Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "Mou et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Mou et al\\.", "year": 2016}, {"title": "Neural tree indexers for text understanding", "author": ["Tsendsuren Munkhdalai", "Hong Yu"], "venue": "arXiv preprint arXiv:1607.04492,", "citeRegEx": "Munkhdalai and Yu.,? \\Q2016\\E", "shortCiteRegEx": "Munkhdalai and Yu.", "year": 2016}, {"title": "A decomposable attention model for natural language inference", "author": ["Ankur P Parikh", "Oscar T\u00e4ckstr\u00f6m", "Dipanjan Das", "Jakob Uszkoreit"], "venue": null, "citeRegEx": "Parikh et al\\.,? \\Q1933\\E", "shortCiteRegEx": "Parikh et al\\.", "year": 1933}, {"title": "On the difficulty of training recurrent neural networks", "author": ["Razvan Pascanu", "Tomas Mikolov", "Yoshua Bengio"], "venue": "Proceedings of the 30th international conference on machine learning (ICML-13),", "citeRegEx": "Pascanu et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Pascanu et al\\.", "year": 2013}, {"title": "Glove: Global vectors for word representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher D Manning"], "venue": "In EMNLP,", "citeRegEx": "Pennington et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Reasoning about entailment with neural attention", "author": ["Tim Rockt\u00e4schel", "Edward Grefenstette", "Karl Moritz Hermann", "Tom\u00e1\u0161 Ko\u010disk\u1ef3", "Phil Blunsom"], "venue": "arXiv preprint arXiv:1509.06664,", "citeRegEx": "Rockt\u00e4schel et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Rockt\u00e4schel et al\\.", "year": 2015}, {"title": "Parsing natural scenes and natural language with recursive neural networks", "author": ["Richard Socher", "Cliff C Lin", "Chris Manning", "Andrew Y Ng"], "venue": "In Proceedings of the 28th international conference on machine learning", "citeRegEx": "Socher et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2011}, {"title": "Improved semantic representations from treestructured long short-term memory networks", "author": ["Kai Sheng Tai", "Richard Socher", "Christopher D Manning"], "venue": "arXiv preprint arXiv:1503.00075,", "citeRegEx": "Tai et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Tai et al\\.", "year": 2015}, {"title": "Order-embeddings of images and language", "author": ["Ivan Vendrov", "Ryan Kiros", "Sanja Fidler", "Raquel Urtasun"], "venue": "arXiv preprint arXiv:1511.06361,", "citeRegEx": "Vendrov et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Vendrov et al\\.", "year": 2015}, {"title": "Learning natural language inference with long short-term memory-networks", "author": ["Shuohang Wang", "Jing Jiang"], "venue": "In Proceedings of NAACL.,", "citeRegEx": "Wang and Jiang.,? \\Q2015\\E", "shortCiteRegEx": "Wang and Jiang.", "year": 2015}], "referenceMentions": [{"referenceID": 4, "context": "NLI is an important component for natural language understanding systems [Benthem, 2008; MacCartney and Manning, 2009].", "startOffset": 73, "endOffset": 118}, {"referenceID": 20, "context": "NLI is an important component for natural language understanding systems [Benthem, 2008; MacCartney and Manning, 2009].", "startOffset": 73, "endOffset": 118}, {"referenceID": 17, "context": "NLI has multitude of applications including natural language question answering [Harabagiu and Hickl, 2006], semantic search, text summarization [Lacatusu et al., 2006] etc.", "startOffset": 145, "endOffset": 168}, {"referenceID": 19, "context": "compared to strict logical inferencing and is subtly different from deduction in formal logical setting [MacCartney, 2009].", "startOffset": 104, "endOffset": 122}, {"referenceID": 7, "context": "3) We present detailed experimental results on Stanford Natural Language Inference(SNLI) Dataset [Bowman et al., 2015], and shows that proposed model outperforms all the other models", "startOffset": 97, "endOffset": 118}, {"referenceID": 11, "context": "Many vector representations for words have been proposed, including the well known latent semantic indexing [Dumais, 2004].", "startOffset": 108, "endOffset": 122}, {"referenceID": 3, "context": "Vector representations for words in the context of neural networks was proposed by [Bengio et al., 2003]", "startOffset": 83, "endOffset": 104}, {"referenceID": 29, "context": "To effectively utilize the aggregated global information from the corpus without incurring high computational cost, \u2018GloVe\u2019 word vectors were proposed by [Pennington et al., 2014].", "startOffset": 154, "endOffset": 179}, {"referenceID": 5, "context": "RNN(ti) is the output at i timestep, which can either be utilized as is, or can be fed again to a parameterized construct such as softmax [Bishop, 2006], depending on the task at hand.", "startOffset": 138, "endOffset": 152}, {"referenceID": 2, "context": "The vanilla RNNs explained above have difficulty in learning long term dependencies in the sequence via gradient descent training [Bengio et al., 1994].", "startOffset": 130, "endOffset": 151}, {"referenceID": 28, "context": "Also training vanilla RNNs is shown to be difficult because of vanishing and exploding gradient problems [Pascanu et al., 2013].", "startOffset": 105, "endOffset": 127}, {"referenceID": 13, "context": "Long short term memory (LSTM) [Hochreiter and Schmidhuber, 1997], a variant of RNN is shown to be effective in capturing long-term dependencies and easier to train compared to vanilla RNNs.", "startOffset": 30, "endOffset": 64}, {"referenceID": 12, "context": "One can refer to [Greff et al., 2015] for a comprehensive survey of LSTM variants.", "startOffset": 17, "endOffset": 37}, {"referenceID": 12, "context": "A concatenation of a forward LSTM and a backward LSTM is known as bi-directional LSTM (biLSTM) [Greff et al., 2015].", "startOffset": 95, "endOffset": 115}, {"referenceID": 32, "context": "The tree structure can be formed by considering the syntactic parse of the sentence, leading to different variations of Tree LSTM [Tai et al., 2015].", "startOffset": 130, "endOffset": 148}, {"referenceID": 1, "context": "Attention mechanism was introduced in the context of machine translation recently [Bahdanau et al., 2014; Luong et al., 2015], where in words or phrases from one language has to be mapped or aligned to words or phrases in another", "startOffset": 82, "endOffset": 125}, {"referenceID": 18, "context": "Attention mechanism was introduced in the context of machine translation recently [Bahdanau et al., 2014; Luong et al., 2015], where in words or phrases from one language has to be mapped or aligned to words or phrases in another", "startOffset": 82, "endOffset": 125}, {"referenceID": 0, "context": "Such an approach for Question Answering was introduced by [Andreas et al., 2016].", "startOffset": 58, "endOffset": 80}, {"referenceID": 0, "context": "Ideally for this, we should bring in a reinforcement learning mechanism similar to the one that is used in [Andreas et al., 2016], to learn the order of aggregation and a neural network [Socher et al.", "startOffset": 107, "endOffset": 129}, {"referenceID": 31, "context": ", 2016], to learn the order of aggregation and a neural network [Socher et al., 2011] for the learning the aggregation operator.", "startOffset": 64, "endOffset": 85}, {"referenceID": 6, "context": "We minimize this loss averaged across the training samples, to learn the various model parameters using stochastic gradient descent [Bottou, 2012].", "startOffset": 132, "endOffset": 146}, {"referenceID": 7, "context": "Recently Stanford Natural Language Inference(SNLI) dataset was created [Bowman et al., 2015] using crowd sourcing.", "startOffset": 71, "endOffset": 92}, {"referenceID": 7, "context": "Recently Stanford Natural Language Inference(SNLI) dataset was created [Bowman et al., 2015] using crowd sourcing. Many deep learning models have been benchmarked on this dataset for NLI. Detail list is available at http://nlp.stanford. edu/projects/snli/. This recent thesis Bowman [2016] covers deep learning based works in detail.", "startOffset": 72, "endOffset": 290}, {"referenceID": 7, "context": "neural networks, and then using these encodings for the final prediction task Bowman et al. [2016]; Mou et al.", "startOffset": 78, "endOffset": 99}, {"referenceID": 7, "context": "neural networks, and then using these encodings for the final prediction task Bowman et al. [2016]; Mou et al. [2016]; Vendrov et al.", "startOffset": 78, "endOffset": 118}, {"referenceID": 7, "context": "neural networks, and then using these encodings for the final prediction task Bowman et al. [2016]; Mou et al. [2016]; Vendrov et al. [2015] are all these kinds of work.", "startOffset": 78, "endOffset": 141}, {"referenceID": 7, "context": "neural networks, and then using these encodings for the final prediction task Bowman et al. [2016]; Mou et al. [2016]; Vendrov et al. [2015] are all these kinds of work. Bowman et al. (2016) also introduced an efficient mechanism to learn the binary parse of the tree along with creating encodings for the prediction task.", "startOffset": 78, "endOffset": 191}, {"referenceID": 7, "context": "neural networks, and then using these encodings for the final prediction task Bowman et al. [2016]; Mou et al. [2016]; Vendrov et al. [2015] are all these kinds of work. Bowman et al. (2016) also introduced an efficient mechanism to learn the binary parse of the tree along with creating encodings for the prediction task. Works in Rockt\u00e4schel et al. [2015]; Wang", "startOffset": 78, "endOffset": 358}, {"referenceID": 10, "context": "To address the problem of compressing a lot of information in a single LSTM cell, Cheng et al. [2016] introduced Long Short Term Memory Networks(LSTMN) for Natural Language Inference.", "startOffset": 82, "endOffset": 102}, {"referenceID": 10, "context": "To address the problem of compressing a lot of information in a single LSTM cell, Cheng et al. [2016] introduced Long Short Term Memory Networks(LSTMN) for Natural Language Inference. 2. Munkhdalai and Yu (2016) introduced Neural Tree Indexers (NTI), by bringing in attention over tree structures of the sentences.", "startOffset": 82, "endOffset": 212}, {"referenceID": 10, "context": "To address the problem of compressing a lot of information in a single LSTM cell, Cheng et al. [2016] introduced Long Short Term Memory Networks(LSTMN) for Natural Language Inference. 2. Munkhdalai and Yu (2016) introduced Neural Tree Indexers (NTI), by bringing in attention over tree structures of the sentences. 3. Parikh et al. [2016] is another very recent work, which uses the attention mechanism over words, compare them and then aggregate the results.", "startOffset": 82, "endOffset": 339}, {"referenceID": 15, "context": "The models were trained using the Adam\u2019s Optimizer [Kingma and Ba, 2014] in a stochastic gradient descent [Bottou, 2012] fashion.", "startOffset": 51, "endOffset": 72}, {"referenceID": 6, "context": "The models were trained using the Adam\u2019s Optimizer [Kingma and Ba, 2014] in a stochastic gradient descent [Bottou, 2012] fashion.", "startOffset": 106, "endOffset": 120}, {"referenceID": 14, "context": "We used batch normalization [Ioffe and Szegedy, 2015] while training.", "startOffset": 28, "endOffset": 53}, {"referenceID": 7, "context": "The Stanford Natural Language Inference Corpus(SNLI) [Bowman et al., 2015] dataset contains 570k human-written English sentence pairs manually labeled for balanced classification with the labels entailment, contradiction, and neutral, supporting the task of natural language inference.", "startOffset": 53, "endOffset": 74}, {"referenceID": 26, "context": "lished results, but fall short very close to the results reported in not yet published works [Munkhdalai and Yu, 2016; Parikh et al., 2016].", "startOffset": 93, "endOffset": 139}, {"referenceID": 8, "context": "A binary tree learning scheme, similar to [Bowman et al., 2016] can be considered to be incorporated in the model.", "startOffset": 42, "endOffset": 63}, {"referenceID": 0, "context": "the operator outputs using a simple LSTM, the aggregation tree structure learning using appropriate learning mechanism similar to [Andreas et al., 2016] is another major stream of work.", "startOffset": 130, "endOffset": 152}, {"referenceID": 7, "context": "Classifier(hand crafted features) [Bowman et al., 2015] 99.", "startOffset": 34, "endOffset": 55}, {"referenceID": 33, "context": "2 GRU encoders [Vendrov et al., 2015] 98.", "startOffset": 15, "endOffset": 37}, {"referenceID": 25, "context": "0M Tree-based CNN encoders [Mou et al., 2016] 83.", "startOffset": 27, "endOffset": 45}, {"referenceID": 8, "context": "5M SPINN-NP encoders [Bowman et al., 2016] 89.", "startOffset": 21, "endOffset": 42}, {"referenceID": 30, "context": "7M LSTM with attention [Rockt\u00e4schel et al., 2015] 85.", "startOffset": 23, "endOffset": 49}, {"referenceID": 34, "context": "5 252K mLSTM [Wang and Jiang, 2015] 92.", "startOffset": 13, "endOffset": 35}, {"referenceID": 10, "context": "9M Non Peer Reviewed Works LSTM Networks [Cheng et al., 2016] 88.", "startOffset": 41, "endOffset": 61}, {"referenceID": 26, "context": "8 582K NTI with global attention [Munkhdalai and Yu, 2016] 88.", "startOffset": 33, "endOffset": 58}, {"referenceID": 8, "context": "SPINN-NP encoders [Bowman et al., 2016] 80.", "startOffset": 18, "endOffset": 39}, {"referenceID": 34, "context": "5 mLSTM [Wang and Jiang, 2015] 81.", "startOffset": 8, "endOffset": 30}], "year": 2017, "abstractText": "In this work we use the recent advances in representation learning to propose a neural architecture for the problem of natural language inference. Our approach is aligned to mimic how a human does the natural language inference process given two statements. The model uses variants of Long Short Term Memory (LSTM), attention mechanism and composable neural networks, to carry out the task. Each part of our model can be mapped to a clear functionality humans do for carrying out the overall task of natural language inference. The model is end-to-end differentiable enabling training by stochastic gradient descent. On Stanford Natural Language Inference(SNLI) dataset, the proposed model achieves better accuracy numbers than all published models in literature.", "creator": "LaTeX with hyperref package"}}}