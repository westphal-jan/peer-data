{"id": "1611.04786", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Nov-2016", "title": "AdversariaLib: An Open-source Library for the Security Evaluation of Machine Learning Algorithms Under Attack", "abstract": "We present AdversariaLib, an open-source python library for the security evaluation of machine learning (ML) against carefully-targeted attacks. It supports the implementation of several attacks proposed thus far in the literature of adversarial learning, allows for the evaluation of a wide range of ML algorithms, runs on multiple platforms, and has multi-processing enabled. The library has a modular architecture that makes it easy to use and to extend by implementing novel attacks and countermeasures. It relies on other widely-used open-source ML libraries, including scikit-learn and FANN. Classification algorithms are implemented and optimized in C/C++, allowing for a fast evaluation of the simulated attacks. The package is distributed under the GNU General Public License v3, and it is available for download at", "histories": [["v1", "Tue, 15 Nov 2016 10:54:58 GMT  (363kb,D)", "http://arxiv.org/abs/1611.04786v1", null]], "reviews": [], "SUBJECTS": "cs.CR cs.LG", "authors": ["igino corona", "battista biggio", "davide maiorca"], "accepted": false, "id": "1611.04786"}, "pdf": {"name": "1611.04786.pdf", "metadata": {"source": "CRF", "title": "AdversariaLib: An Open-source Library for the Security Evaluation of Machine Learning Algorithms Under Attack", "authors": ["Igino Corona", "Battista Biggio", "Davide Maiorca"], "emails": ["igino.corona@diee.unica.it", "battista.biggio@diee.unica.it", "davide.maiorca@diee.unica.it"], "sections": [{"heading": null, "text": "Keywords: Adversarial Learning, safety assessment, evasive attacks"}, {"heading": "1. Introduction", "text": "In order to get the best out of our knowledge, the library is primarily due to its ability to generalise, i.e., it is able to tread new paths and to tread new paths. In the second half of the last decade, in the second half of the last decade, in the second half of the last decade, in the second half of the last decade, in the second half of the last decade, in the second half of the last decade, in the second half of the last decade, in the second half of the last decade, in the second half of the last decade, in the second half of the last decade, in the second half of the last decade, in the second half of the last decade, in the second half of the last decade, in the second half of the last decade, in the second half of the last decade, in the second half of the last decade, in the second half of the last decade, in the second half of the last decade, in the second half of the last decade, in the second half of the last decade, in the second half of the last decade, in the second half of the last decade, in the second half of the last decade, in the second half of the last decade, in the second half of the last decade, in the second half of the last decade, in the second half of the last decade, in the second half of the last decade, in the second half of the last decade, in the second half of the last decade, in the second half of the last decade, in the second half of the last generation, in the second and fourth, fourth, fourth in the fourth, fourth in the fourth and fourth, fourth, fourth in the fourth and fourth, fourth, fourth, fourth in the fourth and fourth, fourth in the fourth generation, fourth and fourth generation, fourth, fourth and fourth generation, fourth generation, fourth and fourth generation, fourth and fourth generation, fourth generation, fourth generation, fourth generation, fourth and fourth generation, fourth generation, fourth generation, fourth and fourth generation, fourth generation, fourth generation, fourth generation, fourth generation, fourth and fourth generation, fourth generation, fourth generation, fourth generation, fourth generation, fourth generation, fourth generation, fourth generation, fourth generation and fourth generation, fourth generation, fourth generation, fourth generation, fourth generation, fourth generation, fourth generation, fourth generation, fourth generation, fourth generation, fourth generation, fourth generation, fourth generation, fourth generation"}, {"heading": "3. Attack example on handwritten digit recognition", "text": "We report here a simple example to visually demonstrate the effectiveness of a gradient-based evasion attack, adapted to our recent work in Biggio et al. (2013) This example can be used with the Matlab wrapper of AdversariaLib and in particular the main _ mnist.m. A linear SVM is first trained to distinguish between two classes of handwritten digits from the MNIST dataset (LeCun et al., 1995), i.e. 3 (positive class) and 7 (negative class), and then manipulated to evade detection; the attack iteratively modifies the initial 3 to minimize the discriminatory function of the SVM g (x) by gradient descent; the results are shown in Fig. 2. 2. The leftmost 3 represents the initial, unmanipulated attack sample (the second and third digits represent evasive points, i.e. manipulated 3s, which are found to be a function that is not being misinterpreted or manipulated as soon as 7)."}, {"heading": "4. Conclusions and future work", "text": "AdversariaLib is the first open source library for the safety assessment of machine learning against targeted attacks. It is currently implementing the gradient-based evasion attack described in Biggio et al. (2013). Because the library is modular and easily expandable, novel and more complex attacks such as poisoning (Biggio et al., 2012; Kloft et Laskov, 2010) can be implemented just as easily as countermeasures and secure classifiers, hopefully with the help of an emerging community of users and developers."}], "references": [{"title": "Can machine learning be secure", "author": ["Marco Barreno", "Blaine Nelson", "Russell Sears", "Anthony D. Joseph", "J.D. Tygar"], "venue": "In Proc. ACM Symp. Information, Computer and Comm. Sec.,", "citeRegEx": "Barreno et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Barreno et al\\.", "year": 2006}, {"title": "Poisoning attacks against support vector machines", "author": ["Battista Biggio", "Blaine Nelson", "Pavel Laskov"], "venue": "In John Langford and Joelle Pineau, editors, 29th Int\u2019l Conf. on Machine Learning,", "citeRegEx": "Biggio et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Biggio et al\\.", "year": 2012}, {"title": "Security evaluation of pattern classifiers under attack", "author": ["Battista Biggio", "Giorgio Fumera", "Fabio Roli"], "venue": "IEEE Transactions on Knowledge and Data Engineering,", "citeRegEx": "Biggio et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Biggio et al\\.", "year": 2014}, {"title": "Adversarial machine learning", "author": ["L. Huang", "A.D. Joseph", "B. Nelson", "B. Rubinstein", "J.D. Tygar"], "venue": "In 4th ACM Workshop on Artificial Intelligence and Security (AISec", "citeRegEx": "Huang et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2011}, {"title": "Online anomaly detection under adversarial impact", "author": ["M. Kloft", "P. Laskov"], "venue": "In Proceedings of the 13th International Conference on Artificial Intelligence and Statistics (AISTATS),", "citeRegEx": "Kloft and Laskov.,? \\Q2010\\E", "shortCiteRegEx": "Kloft and Laskov.", "year": 2010}, {"title": "Comparison of learning algorithms for handwritten digit recognition", "author": ["Y. LeCun", "L. Jackel", "L. Bottou", "A. Brunot", "C. Cortes", "J. Denker", "H. Drucker", "I. Guyon", "U. M\u00fcller", "E. S\u00e4ckinger", "P. Simard", "V. Vapnik"], "venue": "In Int\u2019l Conf. on Artificial Neural Networks,", "citeRegEx": "LeCun et al\\.,? \\Q1995\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1995}, {"title": "Scikit-learn: Machine learning in Python", "author": ["F. Pedregosa", "G. Varoquaux", "A. Gramfort", "V. Michel", "B. Thirion", "O. Grisel", "M. Blondel", "P. Prettenhofer", "R. Weiss", "V. Dubourg", "J. Vanderplas", "A. Passos", "D. Cournapeau", "M. Brucher", "M. Perrot", "E. Duchesnay"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Pedregosa et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Pedregosa et al\\.", "year": 2011}], "referenceMentions": [{"referenceID": 0, "context": "This has recently raised the issue of evaluating the security of learning algorithms to carefully-targeted attacks, besides designing suitable countermeasures, as pointed out in the growing research area of adversarial machine learning (Barreno et al., 2006; Huang et al., 2011; Biggio et al., 2014).", "startOffset": 236, "endOffset": 299}, {"referenceID": 3, "context": "This has recently raised the issue of evaluating the security of learning algorithms to carefully-targeted attacks, besides designing suitable countermeasures, as pointed out in the growing research area of adversarial machine learning (Barreno et al., 2006; Huang et al., 2011; Biggio et al., 2014).", "startOffset": 236, "endOffset": 299}, {"referenceID": 2, "context": "This has recently raised the issue of evaluating the security of learning algorithms to carefully-targeted attacks, besides designing suitable countermeasures, as pointed out in the growing research area of adversarial machine learning (Barreno et al., 2006; Huang et al., 2011; Biggio et al., 2014).", "startOffset": 236, "endOffset": 299}, {"referenceID": 0, "context": "This has recently raised the issue of evaluating the security of learning algorithms to carefully-targeted attacks, besides designing suitable countermeasures, as pointed out in the growing research area of adversarial machine learning (Barreno et al., 2006; Huang et al., 2011; Biggio et al., 2014). In this work we present AdversariaLib, an open-source python library that can be exploited to this end. To the best of our knowledge, this is the first open-source library that implements the process of security evaluation recently advocated by Biggio et al. (2014). The idea behind a security evaluation is to proactively anticipate the behavior of the adversary to identify potential vulnerabilities of machine learning algorithms, and to", "startOffset": 237, "endOffset": 567}, {"referenceID": 1, "context": ", on her goal, knowledge of the attacked system, and capabilities of manipulating the data (Biggio et al., 2014, 2013). It is thus clear that the attack strongly depends on the targeted classifier. In practice, however, only rarely the adversary may know the targeted classifier completely: although she may realistically know the kind of algorithm used, she may not know the parameters learned after training (e.g., the weights assigned to each feature by a linear classifier). Another interesting feature of AdversariaLib is indeed the possibility of learning an estimate of the targeted classifier, which we refer to as surrogate classifier. This assumes that the adversary can collect a set of surrogate training data, ideally drawn from the same distribution as the targeted classifier\u2019s training set, retrieve the labels assigned to them by the targeted classifier, and learn a surrogate classifier on such data. In Biggio et al. (2013) we showed that attacking a surrogate classifier learned on a relatively small training set may lead to evade the targeted classifier with high probability.", "startOffset": 92, "endOffset": 943}, {"referenceID": 6, "context": "To this end, prlib implements suitable wrappers that exploit other open-source libraries, such as scikit-learn (Pedregosa et al., 2011) and FANN.", "startOffset": 111, "endOffset": 135}, {"referenceID": 1, "context": "This wrapper relies on the APIs of AdversariaLib, and can thus easily execute the evasion attack described by Biggio et al. (2013). An example is presented in the next section.", "startOffset": 110, "endOffset": 131}, {"referenceID": 5, "context": "A linear SVM is first trained to discriminate between two classes of handwritten digits from the MNIST dataset (LeCun et al., 1995), i.", "startOffset": 111, "endOffset": 131}, {"referenceID": 0, "context": "We report here a simple example to visually demonstrate the effectiveness of a gradientbased evasion attack, adapted from our recent work in Biggio et al. (2013). This example can be run using the Matlab wrapper of AdversariaLib and, in particular, the script main_mnist.", "startOffset": 141, "endOffset": 162}, {"referenceID": 0, "context": "This confirms the vulnerability of standard learning algorithms to well-crafted attacks, and the need for more secure learning algorithms, as suggested by Barreno et al. (2006); Huang et al.", "startOffset": 155, "endOffset": 177}, {"referenceID": 0, "context": "This confirms the vulnerability of standard learning algorithms to well-crafted attacks, and the need for more secure learning algorithms, as suggested by Barreno et al. (2006); Huang et al. (2011); Biggio et al.", "startOffset": 155, "endOffset": 198}, {"referenceID": 0, "context": "This confirms the vulnerability of standard learning algorithms to well-crafted attacks, and the need for more secure learning algorithms, as suggested by Barreno et al. (2006); Huang et al. (2011); Biggio et al. (2014).", "startOffset": 155, "endOffset": 220}, {"referenceID": 1, "context": "As the library is modular and easy to extend, novel and more sophisticated attacks can be easily implemented, like poisoning (Biggio et al., 2012; Kloft and Laskov, 2010), as well as countermeasures and secure classifiers, hopefully, with the help of an emerging community of users and developers.", "startOffset": 125, "endOffset": 170}, {"referenceID": 4, "context": "As the library is modular and easy to extend, novel and more sophisticated attacks can be easily implemented, like poisoning (Biggio et al., 2012; Kloft and Laskov, 2010), as well as countermeasures and secure classifiers, hopefully, with the help of an emerging community of users and developers.", "startOffset": 125, "endOffset": 170}, {"referenceID": 1, "context": "Currently, it implements the gradient-based evasion attack described in Biggio et al. (2013). As the library is modular and easy to extend, novel and more sophisticated attacks can be easily implemented, like poisoning (Biggio et al.", "startOffset": 72, "endOffset": 93}], "year": 2016, "abstractText": "We present AdversariaLib, an open-source python library for the security evaluation of machine learning (ML) against carefully-targeted attacks. It supports the implementation of several attacks proposed thus far in the literature of adversarial learning, allows for the evaluation of a wide range of ML algorithms, runs on multiple platforms, and has multiprocessing enabled. The library has a modular architecture that makes it easy to use and to extend by implementing novel attacks and countermeasures. It relies on other widelyused open-source ML libraries, including scikit-learn and FANN. Classification algorithms are implemented and optimized in C/C++, allowing for a fast evaluation of the simulated attacks. The package is distributed under the GNU General Public License v3, and it is available for download at http://sourceforge.net/projects/adversarialib.", "creator": "LaTeX with hyperref package"}}}