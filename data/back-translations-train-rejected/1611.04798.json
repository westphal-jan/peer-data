{"id": "1611.04798", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Nov-2016", "title": "Toward Multilingual Neural Machine Translation with Universal Encoder and Decoder", "abstract": "In this paper, we present our first attempts in building a multilingual Neural Machine Translation framework under a unified approach. We are then able to employ attention-based NMT for many-to-many multilingual translation tasks. Our approach does not require any special treatment on the network architecture and it allows us to learn minimal number of free parameters in a standard way of training. Our approach has shown its effectiveness in an under-resourced translation scenario with considerable improvements up to 2.6 BLEU points. In addition, the approach has achieved interesting and promising results when applied in the translation task that there is no direct parallel corpus between source and target languages.", "histories": [["v1", "Tue, 15 Nov 2016 11:47:42 GMT  (437kb,D)", "http://arxiv.org/abs/1611.04798v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["thanh-le ha", "jan niehues", "alexander waibel"], "accepted": false, "id": "1611.04798"}, "pdf": {"name": "1611.04798.pdf", "metadata": {"source": "CRF", "title": "Toward Multilingual Neural Machine Translation with Universal Encoder and Decoder", "authors": ["Thanh-Le Ha", "Jan Niehues", "Alexander Waibel"], "emails": ["firstname.lastname@kit.edu"], "sections": [{"heading": "1 Introduction", "text": "This year, it is so far that it will be able to erenie.n mention the aforementioned lcihsrc\u00fcehncS."}, {"heading": "2 Neural Machine Translation: Background", "text": "An NMT system consists of an encoder that automatically combines the features of a source set in fixed-length context vectors and a decoder that recursively combines the produced context vectors with the previous target word to generate the most likely word from a target vocabulary. Specifically, a bi-directional recurrent encoder reads each word of a source set x = {x1,..., xn} and encodes a representation of the sentence into a fixed length vector hi concatinated from those of the forward and backward directives: hi = [\u2212 \u2192 h i] \u2212 \u2192 h i = f (\u2212 h \u2212 1, s)."}, {"heading": "3 Universal Encoder and Decoder for Multilingual Neural Machine Translation", "text": "The possible reason for this effort lies in the unique architecture of NMT. Unlike SMT, NMT consists of separate neural networks for the source page and the target page, or the encoder and decoder, which allows these components to map a sentence in any language onto an embedded space representation, which is believed to share a common semantics between the source languages involved. From this common space, the decoder, with some implicit or explicit relevant limitations, could transform the representation into a concrete sentence in any desired language. In this section, we will discuss some related work on this topic. We will then describe a unified approach to a universal, attention-based NMT scheme. Our approach does not require architectural changes and it can be trained to learn a minimum number of parameters compared to other work."}, {"heading": "3.1 Related Work", "text": "By extending the solution of sequence-to-sequence modeling with the help of encoder decoder architectures to multifunctional learning processes, Luong et al. (2016) managed to achieve better performance on some many \u2212 to \u2212 many tasks such as translation, parsing and captions compared to individual tasks. Specifically, in translation, the work uses multiple encoders to translate from multiple languages, and multiple decoders to translate into multiple languages. In this view of multilingual translation, each language in the source or destination page could be modeled by one encoder, depending on the side of the translation. Due to the natural diversity between two tasks in this multi-task learning scenario, such as translation and parsing, it was unable to exhibit the attention mechanism, although it has proven its effectiveness in NMTs."}, {"heading": "3.2 Universal Encoder and Decoder", "text": "This year is the highest in the history of the country."}, {"heading": "4 Evaluation", "text": "In this section, we describe the evaluation of our proposed approach against the strong baselines using NMT in two scenarios: the translation of an underresource-rich language pair and the translation of a language pair that has no parallel data at all."}, {"heading": "4.1 Experimental Settings", "text": "In fact, it is so that most of them are able to move while they are able to move. Most of them are not able to move. Most of them are able to move. Most of them are able to move. Most of them are able to move. Most of them are able to move. Most of them are able to move. Most of them are not able to move. Most of them are able to move. Most of them are able to move. Most of them are not able to move. Most of them are not able to move. Most of them are not able to move. Most of them are able to move."}, {"heading": "4.2 Under-resourced Translation", "text": "First, we consider translation for an underfunded pair of languages. Here, a small part of the large parallel corpus for English-German is used as a simulation for the scenario in which we do not have much parallel data: translation of texts into English into German. We perform language-specific encoding on both the source and the target pages. By including the German monolingual data as additional input (German \u2192 German), which we call the mix source approach, we could easily, naturally enrich the training data. Given this resource situation, it could help our NMT to obtain a better representation of the source page, allowing us to better learn the translation relationship. Including monolingual data in this way could also improve the translation of some rare word types such as named entities. Furthermore, as the ultimate goal of our work, we would like to explore the benefits of multilingualism in NMT."}, {"heading": "4.3 Using large monolingual data in NMT.", "text": "A standard NMT system uses only parallel data. While good parallel corpora are limited in number, it is trivial to obtain monolingual data of any language. To use the use of the German monolingual corpus in an English \u2192 German NMT system, Sennrich et al. (2016a) have a separate German \u2192 English8To make it clearer, only the word vectors of the words related to \"research\" are projected and visualized; the blue words are the English words and the red are the French words. NMT uses the same parallel corpus, then they used this system to translate the German monolingual corpus back into English and form a synthesis of parallel data. Gu \u00fclc, ehre et al. (2015) trained another RNN-based language model to evaluate the monolingual corpus and integrate it into the NMT system."}, {"heading": "4.4 Zero-resourced Translation", "text": "Among the few who are able to survive such a situation are those who are able to survive such a situation and those who are able to survive it."}, {"heading": "5 Conclusion and Future Work", "text": "By treating words in different languages as distinct words and directing attention and translation in the direction of the desired target language, we are able to deploy attention-grabbing NMT towards a multilingual translation system. Our proposed approach reduces the need for a complicated architectural redesign to take into account attention mechanisms. Moreover, the number of free parameters that can be learned in our network does not exceed the size of a single NMT system. With its universality, our approach has proven its effectiveness in an inadequate translation task with significant improvements. Furthermore, the approach to use in the translation task has produced interesting and promising results that there is no direct parallel corpus between source and target languages. Nevertheless, there are issues on which we can continue to work on in the future."}], "references": [{"title": "Kyunghyun Cho", "author": ["Dzmitry Bahdanau"], "venue": "and Yoshua Bengio.", "citeRegEx": "Bahdanau et al.2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Herve Saint-Amand", "author": ["Ondrej Bojar", "Rajen Chatterjee", "Christian Federmann", "Barry Haddow", "Philipp Koehn", "Johannes Leveling", "Christof Monz", "Pavel Pecina", "Matt Post"], "venue": "et al.", "citeRegEx": "Bojar et al.2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Christian Girardi", "author": ["Mauro Cettolo"], "venue": "and Marcello Federico.", "citeRegEx": "Cettolo et al.2012", "shortCiteRegEx": null, "year": 2012}, {"title": "R Cattoni", "author": ["M Cettolo", "J Niehues", "S St\u00fcker", "L Bentivogli"], "venue": "and M Federico.", "citeRegEx": "Cettolo et al.2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Holger Schwenk", "author": ["Kyunghyun Cho", "Bart van Merrienboer", "\u00c7aglar G\u00fcl\u00e7ehre", "Fethi Bougares"], "venue": "and Yoshua Bengio.", "citeRegEx": "Cho et al.2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Dianhai Yu", "author": ["Daxiang Dong", "Hua Wu", "Wei He"], "venue": "and Haifeng Wang.", "citeRegEx": "Dong et al.2015", "shortCiteRegEx": null, "year": 2015}, {"title": "KyungHyun Cho", "author": ["Orhan Firat"], "venue": "and Yoshua Bengio.", "citeRegEx": "Firat et al.2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Lo\u0131\u0308c Barrault", "author": ["\u00c7aglar G\u00fcl\u00e7ehre", "Orhan Firat", "Kelvin Xu", "Kyunghyun Cho"], "venue": "Huei-Chi Lin, Fethi Bougares, Holger Schwenk, and Yoshua Bengio.", "citeRegEx": "G\u00fcl\u00e7ehre et al.2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Long Short-Term Memory", "author": ["Hochreiter", "Schmidhuber1997] Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural Comput.,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Effective approaches to attention-based neural machine translation", "author": ["Hieu Pham", "Christopher D. Manning"], "venue": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP", "citeRegEx": "Luong et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Addressing the rare word problem in neural machine translation", "author": ["Ilya Sutskever", "Quoc V. Le", "Oriol Vinyals", "Wojciech Zaremba"], "venue": "In Proceedings of ACL-IJNLP", "citeRegEx": "Luong et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Oriol Vinyals", "author": ["Minh-Thang Luong", "Quoc V. Le", "Ilya Sutskever"], "venue": "and Lukasz Kaiser.", "citeRegEx": "Luong et al.2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Visualizing data using t-sne", "author": ["Maaten", "Geoffrey Hinton"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Maaten et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Maaten et al\\.", "year": 2008}, {"title": "Todd Ward", "author": ["Kishore Papineni", "Salim Roukos"], "venue": "and Wei-Jing Zhu.", "citeRegEx": "Papineni et al.2002", "shortCiteRegEx": null, "year": 2002}, {"title": "Improving Neural Machine Translation Models with Monolingual Data. In Association for Computational Linguistics (ACL 2016)", "author": ["Barry Haddow", "Alexandra Birch"], "venue": null, "citeRegEx": "Sennrich et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Sennrich et al\\.", "year": 2016}, {"title": "Neural Machine Translation of Rare Words with Subword Units", "author": ["Barry Haddow", "Alexandra Birch"], "venue": "In Association for Computational Linguistics (ACL 2016),", "citeRegEx": "Sennrich et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Sennrich et al\\.", "year": 2016}, {"title": "ADADELTA: An Adaptive Learning Rate Method. CoRR, abs/1212.5701", "author": ["Matthew D. Zeiler"], "venue": null, "citeRegEx": "Zeiler.,? \\Q2012\\E", "shortCiteRegEx": "Zeiler.", "year": 2012}, {"title": "Multi-Source Neural Translation. In The North American Chapter of the Association for Computational Linguistics (NAACL 2016)", "author": ["Zoph", "Knight2016] Barret Zoph", "Kevin Knight"], "venue": null, "citeRegEx": "Zoph et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Zoph et al\\.", "year": 2016}], "referenceMentions": [], "year": 2016, "abstractText": "In this paper, we present our first attempts in building a multilingual Neural Machine Translation framework under a unified approach. We are then able to employ attention-based NMT for manyto-many multilingual translation tasks. Our approach does not require any special treatment on the network architecture and it allows us to learn minimal number of free parameters in a standard way of training. Our approach has shown its effectiveness in an under-resourced translation scenario with considerable improvements up to 2.6 BLEU points. In addition, the approach has achieved interesting and promising results when applied in the translation task that there is no direct parallel corpus between source and target languages.", "creator": "LaTeX with hyperref package"}}}