{"id": "1611.04822", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Nov-2016", "title": "SimDoc: Topic Sequence Alignment based Document Similarity Framework", "abstract": "Document similarity is the problem of formally representing textual documents and then proposing a similarity measure that can be used to compute the linguistic similarity between two documents. Accurate document similarity computation improves many enterprise relevant tasks such as document clustering, text mining, and question-answering. Most contemporary techniques employ bag-of-words (BoW) based document representation models. In this paper, we show that a document's thematic flow, which is often disregarded by bag-of-word techniques, is pivotal in estimating their semantic similarity. In this direction, we propose a novel semantic document similarity framework, called SimDoc. We model documents as topic-sequences, where topics represent latent generative clusters of relative words. We then use a sequence alignment algorithm, that has been adapted from the Smith-Waterman gene-sequencing algorithm, to estimate their semantic similarity. For similarity computation at a finer granularity, we tune the alignment algorithm by integrating it with a word embedding matrix based topic-to-topic similarity measure. A document level similarity score is then computed by again using the sequence alignment algorithm over all sentence pairs. In our experiments, we see that SimDoc outperforms many contemporary bag-of-words techniques in accurately computing document similarity, and on practical applications such as document clustering.", "histories": [["v1", "Tue, 15 Nov 2016 13:31:28 GMT  (323kb,D)", "http://arxiv.org/abs/1611.04822v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["gaurav maheshwari", "priyansh trivedi", "harshita sahijwani", "kunal jha", "sourish dasgupta", "jens lehmann"], "accepted": false, "id": "1611.04822"}, "pdf": {"name": "1611.04822.pdf", "metadata": {"source": "CRF", "title": "SimDoc: Topic Sequence Alignment based Document Similarity Framework", "authors": ["Gaurav Maheshwari", "Priyansh Trivedi", "Harshita Sahijwani", "Kunal Jha", "Sourish Dasgupta", "Jens Lehmann"], "emails": ["@outlook.com", "pc.priyansh@gmail.com", "hjsahijwani@gmail.com", "kunal94jha@gmail.com", "sourish@rygbee.com", "lehmann@uni-bonn.de"], "sections": [{"heading": null, "text": "Keywords Similarity of documents, similarity of semantic texts, Topic modeling"}, {"heading": "1. INTRODUCTION", "text": "In fact, most people who are able to determine themselves, determine themselves and decide what they want and what they want have the same goals, have the same goals and have the same goals, have the same goals and have the same goals, have the same goals, have the same goals and have the same goals, have the same goals and have the same goals, have the same goals and have the same goals."}, {"heading": "2. RELATED WORK", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Short-text Similarity Approaches", "text": "The SemEval Semantic Textual Similarity (STS) series of tasks served as the gold standard platform for calculating short text similarity with a publicly available corpus consisting of 14,000 sentence pairs developed over four years, along with human annotations of similarity for each pair [1]. In accordance with the results of SemEval 2015, the DLS @ CU team packaged the first position in its supervised and unsupervised run in STS [20]. The team's unsupervised system was based on word alignment, where semantically related terms are first aligned over two sentences and later their semantic similarity is calculated as a monotonously increasing function of the alignment degree. Their supervised version used cosmic similarity between the vector representations of the two sentences, along with the output of the unattended systems."}, {"heading": "2.2 Long-text Similarity Approaches", "text": "Many document similarity measures are based on the secondary vector space model, in which documents are presented as weighted high-dimensional vectors. This model is also popularly referred to as the Bag of Words model, in which words are often used as characteristics. However, this model fails to grasp the word order (which greatly expands the semantics of the document) and also ignores the semantics of words. Jaccard similarity [11], which treats documents as sets of tokens, and Okapi BM25 [17] and Lucene similarity [12], which rely on the term frequency and inverse frequency of words in the documents, are other widely used document similarity measures. These measures also have the same limitations as the Bag of Word models. One of the most popular techniques used for document similarity is Explicit Semantic Analysis (ESA)."}, {"heading": "3. BACKGROUND", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Problem Statement", "text": "In view of a pair of text documents 2, D1 and D2, a document similarity measure should be able to represent the text content of the documents using a formal method and calculate the semantic similarity value between the documents. By semantic similarity, we mean the proximity of its semantic content as opposed to syntactic closeness. Document similarity values are usually defined in the real space R via the function \u03c3, which is defined as follows: Definition 1 (Document Similarity Measure, Document Similarity Measure, Document Mess, Document Similarity Measure, Document Measure, Document Measure, Document Approximation, Document Approximation, Document Approximation, Document Approximation, Document Approximation, Document Approximate, Document Approximation, Document Approximate, Document Approximation, Document Approximation, Document Approximent Approximation, Document, Document Approximent, Document Approximent, Document, Document Approximent, Document, Document, Document, Document Approximent, Document, Document, Document, Document, Document, Document, Document, Document, Document, Document, Approximent, Document, Document, Document, Document, Document, Document, Document, Document, Document, Document, Document, Document, Document, Document, Document, Document, Approximation, Document, Document, Document, Document, Document, Document, Approximation, Document, Document, Document, Document, Document, Document, Document, Document, Document, Approximation, Document, Document, Document, Document, Document,"}, {"heading": "3.2 Probabilistic Topic Modeling", "text": "Documents can be presented as BoW, following the assumption of interchangeability [3]. The assumption is that words, when modeled as Bernoulli variables, are conditionally independent within each random sample sequence, with the word variables bound to a specific set of latent random variables known as themes, which means that the common distribution of each sample sequence permutation (i.e. document variables) remains the same, provided that the theme variables are given. In other words, the order of words representing a document does not matter as long as the topics that \"generate\" the appearance of words are known. Interestingly, however, these topics are hidden (in terms of their distributions), and therefore we need a mechanism to discover (i.e. learn) them. This learning process is called topics modeling. In this paper, we use a widely used topics modeling technique known as latent wealth."}, {"heading": "3.3 Smith Waterman Algorithm", "text": "Smith-Waterman algorithm is widely used to calculate the alignment of gene-protein sequences - a very important problem in the field of bioinformatics. Interestingly, we can use it to quantify the degree to which two sequences of tokens, say S1 and S2, are aligned. It uses dynamic programming to determine the sequence segment of S1 that is optimally aligned with S2 (or vice versa). During alignment, the algorithm can either insert, delete, or replace a token if a token mismatch is found in the comparison, thus transforming one of the sequences into the other sequence. However, editing entails a penalty. Penalties for inserting, deleting, or substituting are collectively referred to as a gap penalty scheme. In this paper, we have proposed a flexible penalty scheme that uses a similarity matrix that takes into account the degree of similarity between tokens."}, {"heading": "4. APPROACH", "text": "The SimDoc framework has five core modules: (i) Topic-Model Learner, (ii) Topic-Sequence Inferencer, (iii) Token-Level Similarity Scorer, (iv) Sentence-Level Similarity Scorer and (v) Document-Level Similarity Scorer (see Figure 1). The Topic-Model Learner module receives a training document corpus and encodes each training document into an n-dimensional vector [p1, p2, p3... pn]. Here n represents the number of topics in the trained LDA model, and each value pi represents the likelihood of the document having the ith theme. After the trained model is generated, it is then used by the Topic-Sequence Inferencer to represent a given document as a sequence of latent topics, with the vector representation of documents and a word-to-algorithm being used."}, {"heading": "4.1 Topic-Model Learner", "text": "This is a training module that learns topic distributions from each document (and thus word distribution for each topic) in the train corpus. We use latent dirichlet allocation (LDA) (Section 3.2) based topic modeling for our purpose. It should be noted that an LDA-based topic model is more accurate when it is trained over a fixed domain with a specific vocabulary pattern (i.e. domain-specific linguistic variations and jargon). For example, a topic model trained on computer science documents cannot be used to precisely generate topic distributions from documents with travel blogs. However, it may perform relatively better in related areas such as electrical engineering or statistics or mathematics. The Topic Model Learner first performs text pre-processing on the train corpus, which includes tokenization, lepatization and stop word removal. This pre-processing ensures that the LDA utility model then performs a preprocessed text / pre-text word."}, {"heading": "4.2 Topic-Sequence Inferencer", "text": "When each document of an invisible document pair is fed into the module, it first performs the same NLP pre-processing as the Topic-Model Learner module. It then performs a language normalization for each sentence in the documents, converting passive sentences into their active form. Without this normalization step, the thematic flow of similar sentences (and thus documents) appears different, even if they have the same semantic content.The cleaned document pair is fed into Gensim's trained LDA-Topic Model to infer that the documents are distributed to themes. Then, the module transforms the documents into their theme-based representations. The word-to-theme mapping is done using the inverted Topic-Word distribution index (described in the previous section), where, as the document is guided through the model, each word in the document has the most likely sequence."}, {"heading": "4.3 Token-level Similarity Scorer", "text": "This module is responsible for calculating the remuneration if a topic does not match a topic, while calculating the compensation between two theme sequences (i.e. sentences); the resulting evaluation should represent the degree of connectedness."}, {"heading": "4.4 Sentence-level Similarity Scorer", "text": "This module calculates the similarity between a pair of topicsequence segments (a theme-sequence segment represents a set of a document, as discussed in Section 4.2) We use an adaptation of the Smith Waterman algorithm, which in turn uses the token-level similarity to calculate the alignment results (or vice versa the degree of imbalance) between topicsequence segments. Before formalizing the algorithm, we first describe some preliminary concepts as follows: \u2022 Sai is the ith theme sequence segment segment segment (corresponding to what represents the ith sentence) of the document Da. \u2022 tokenS a x is token on xth position in string segments Segments Segments Segments Segments Segments Segments Segments Segments Segments Segments Segments Segments Segments Segments Segments Segments Segments Segments Segments Segments Segments Segments Segments Segments Segments Segments Segments Segments Segments Segments Segments Segments Segments Segments Segments Segments Segments Segments Segments Segments Segments Segments Segments Segments Segments Segments Segments Segments Segments Segments Segments Segments Segments Segments Segments Segments Segments Segments Segments Segments Segments Segments Segments Segments Segments Segments Segments Segments Segments Segments Segments Segments Segments Segments Segments Segments Segments Segments Segments Segments Segments Segments Segments Segments Segments Segments Segments Segments Segments Segments Segments Segments Segments Segments Segus Segments Segments Segments Segments Segments Segments Segments Segments Segments Segments Segments Segments Segments Segments Segments Segments Segments Segments Segments Segments Segments Segments Se"}, {"heading": "4.5 Document-level Similarity Scorer", "text": "Thematic discourses often extend over more than one sentence in a document. In document pairs with high similarity = sentence similarity, we expect some agreement in this discourse. To model this, we apply the same proposed sequence algorithm, but now, over a sequence of theme-sequence segments (which represent the order in which sentences appear in the given document). During the alignment process, the Document-level Similarity Scorer uses Sentencelevel Similarity Scorer to calculate the degree of agreement between two sentences when it is found. The algorithm can be expressed similarly as follows: \u2022 D is the thematic sequence selection representative of the text of the document, which is divided into segments (sentences). Da = {Sia}; i, M] where M is the number of sentences in the document Da. \u2022 Sai is the number of sentences in the document Da."}, {"heading": "5. EVALUATION", "text": "We will conduct three experiments to measure and analyze the performance of SimDoc. The first two are text analysis tasks: document similarity and document clustering, which will quantify the accuracy of SimDoc. The third task will compare different aspects of SimDoc to better understand the impact of different modules on the overall performance of the system."}, {"heading": "5.1 Document Similarity", "text": "5.1.1 Assessment Setup and Dataset This task would require a robust text extraction module that can capture our system significantly. We are expected to recognize the most similar pair of documents in the set. A dataset of 20,000 such triples was created and made publicly available. [7] They collected the URLs of research papers archived on arXiv.org 6; and based on their keywords and categories, made these triples of URLs. This was usually done in such a way that document 2 (D2) has some subjects that are comparable to document 1 (D1) but none to document 3 (D3). Consequently, the system should report that D1, D2 are more similar than D3. Here, we refer to the research that exists in the URLs in the dataset. In our experiment, we are not retrieving the entire paper from the URLs, but only their abstracts."}, {"heading": "5.2 Document Clustering", "text": "In fact, the fact is that most of them are able to outdo themselves, both in terms of the quality and the quality of the products they manufacture, and in terms of the quality of the products they manufacture, as well as in terms of the quality of the products they manufacture, and in terms of the quality of the products they manufacture, and most of them are able to outdo themselves, both in terms of the quality and the quality of the products they manufacture, as well as in terms of the quality of the products they manufacture, and in terms of the way they are produced."}, {"heading": "5.3 Extended Analysis", "text": "In this context, it should be noted that this is not a purely formal measure, but a purely formal measure, which is a purely formal approach, which is primarily a purely formal approach, which is primarily a purely formal approach, but a purely formal approach, which is primarily a purely formal approach."}, {"heading": "6. CONCLUSION", "text": "In this article, we propose SimDoc - a sequential measure of document similarity. We compared SimDoc with contemporary measures of document similarity such as Jaccard, Lucene Index, BM25, and [6]. SimDoc has achieved high accuracy, making it a promising paradigm for comparing documents based on their semantic content. We can further improve SimDoc by including negation treatment, dependency analyzing complex voice normalization, entity recognition, meaning disambiguation, and a sentence simplification module for normalization of paraphrases."}, {"heading": "7. REFERENCES", "text": "[1] E. Agirre, B. Coecke, et al. Semeval-2015 task 2: Semantictextual similarity, English, s-panish and pilot on interpretability. In Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), June, 2015. [2] D. Blei and J. Lafferty. Correlated topic models. Advances in neural information processing systems, 18: 147, 2006. [3] D. M. Blei, A. Y. Ng, and M. I. Jordan. Latent dirichlet allocation. the Journal of machine Learning research, 3: 993-1022, 2003. [4] C. Brockett and W. B. Dolan. Support vector machines for paraphrase identification and corpus construction. In Proceedings of the 3rd International Workshop on Paraphrasing, pages 1-8, 2005. [5] S. Clark, B. Coecke, and M. Sadradezh."}], "references": [{"title": "Semeval-2015 task 2: Semantic textual similarity, english, s-panish and pilot on interpretability", "author": ["E. Agirre", "C. Banea"], "venue": "In Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2015}, {"title": "Correlated topic models", "author": ["D. Blei", "J. Lafferty"], "venue": "Advances in neural information processing systems,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2006}, {"title": "Latent dirichlet allocation", "author": ["D.M. Blei", "A.Y. Ng", "M.I. Jordan"], "venue": "Journal of machine Learning research,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2003}, {"title": "Support vector machines for paraphrase identification and corpus construction", "author": ["C. Brockett", "W.B. Dolan"], "venue": "In Proceedings of the 3rd International Workshop on Paraphrasing,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2005}, {"title": "A compositional distributional model of meaning", "author": ["S. Clark", "B. Coecke", "M. Sadrzadeh"], "venue": "In Proceedings of the Second Quantum Interaction Symposium", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2008}, {"title": "Document embedding with paragraph vectors", "author": ["A.M. Dai", "C. Olah", "Q.V. Le"], "venue": "arXiv preprint arXiv:1507.07998,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2015}, {"title": "Document embedding with paragraph vectors", "author": ["A.M. Dai", "C. Olah", "Q.V. Le", "G.S. Corrado"], "venue": "In NIPS Deep Learning Workshop,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2014}, {"title": "Computing semantic relatedness using wikipedia-based explicit semantic analysis", "author": ["E. Gabrilovich", "S. Markovitch"], "venue": "In IJCAI,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2007}, {"title": "Learning a concept-based document similarity measure", "author": ["L. Huang", "D. Milne", "E. Frank", "I.H. Witten"], "venue": "Journal of the American Society for Information Science and Technology,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2012}, {"title": "Etude comparative de la distribution florale dans une portion des Alpes et du Jura", "author": ["P. Jaccard"], "venue": "Impr. Corbaz,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1901}, {"title": "Apache lucene-a high-performance, full-featured text search engine library", "author": ["A. Jakarta"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2004}, {"title": "A natural language processing approach to automatic plagiarism detection", "author": ["C.-H. Leung", "Y.-Y. Chan"], "venue": "In Proceedings of the 8th ACM SIGITE conference on Information technology education,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2007}, {"title": "Pachinko allocation: Dag-structured mixture models of topic correlations", "author": ["W. Li", "A. McCallum"], "venue": "In Proceedings of the 23rd international conference on Machine learning,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2006}, {"title": "Learning dependency-based compositional semantics", "author": ["P. Liang", "M.I. Jordan", "D. Klein"], "venue": "Computational Linguistics,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2013}, {"title": "Glove: Global vectors for word representation", "author": ["J. Pennington", "R. Socher", "C.D. Manning"], "venue": "In EMNLP,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2014}, {"title": "Okapi at trec-3", "author": ["S.E. Robertson", "S. Walker", "S. Jones", "M.M. Hancock-Beaulieu", "M. Gatford"], "venue": "NIST SPECIAL PUBLICATION SP,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1995}, {"title": "A vector space model for automatic indexing", "author": ["G. Salton", "A. Wong", "C.-S. Yang"], "venue": "Communications of the ACM,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1975}, {"title": "Identification of common molecular subsequences", "author": ["T.F. Smith", "M.S. Waterman"], "venue": "Journal of molecular biology,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1981}, {"title": "Dls@ cu: Sentence similarity from word alignment", "author": ["M.A. Sultan", "S. Bethard", "T. Sumner"], "venue": "SemEval", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2014}, {"title": "From frequency to meaning: Vector space models of semantics", "author": ["P.D. Turney", "P. Pantel"], "venue": "Journal of artificial intelligence research,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2010}, {"title": "Crowdsourcing inference-rule evaluation", "author": ["N. Zeichner", "J. Berant", "I. Dagan"], "venue": "Association for Computational Linguistics,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2012}], "referenceMentions": [{"referenceID": 19, "context": "1145/1235 hypothesis of distributional semantics (which is primarily a keyword based statistical approach) [21] or by the principle of compositionality as favored in compositional semantics [15].", "startOffset": 107, "endOffset": 111}, {"referenceID": 13, "context": "1145/1235 hypothesis of distributional semantics (which is primarily a keyword based statistical approach) [21] or by the principle of compositionality as favored in compositional semantics [15].", "startOffset": 190, "endOffset": 194}, {"referenceID": 4, "context": "In recent times hybrid approaches, based on the principle of compositional distributional semantics, have been adopted as well [5].", "startOffset": 127, "endOffset": 130}, {"referenceID": 11, "context": "It can power content-based recommender systems, plagiarism detection [13], and document clustering systems.", "startOffset": 69, "endOffset": 73}, {"referenceID": 3, "context": "Further it can be utilized for various complex NLP tasks such as paraphrase identification [4], and textual entailment [22].", "startOffset": 91, "endOffset": 94}, {"referenceID": 20, "context": "Further it can be utilized for various complex NLP tasks such as paraphrase identification [4], and textual entailment [22].", "startOffset": 119, "endOffset": 123}, {"referenceID": 2, "context": "We use Latent Dirichlet Allocation (LDA) [3] topic model to represent documents as sequences of topics.", "startOffset": 41, "endOffset": 44}, {"referenceID": 17, "context": "With the help of these two information, we calculate alignment scores between these sequences using a sequence alignment algorithm, which is an adaptation of SmithWaterman algorithm - a gene/protein sequence alignment algorithm [19].", "startOffset": 228, "endOffset": 232}, {"referenceID": 6, "context": "We show empirically, using data set provided by [7], that the proposed system is capable of accurately calculating the document similarity.", "startOffset": 48, "endOffset": 51}, {"referenceID": 0, "context": "The SemEval Semantic Textual Similarity (STS) task series has served as a gold-standard platform for computing short text similarity with a publicly available corpus, consisting of 14,000 sentence pairs developed over four years, along with human annotations of similarity for each pair [1].", "startOffset": 287, "endOffset": 290}, {"referenceID": 18, "context": "In accordance to the results of SemEval 2015, team DLS@CU bagged the first position in their supervised and unsupervised run in STS [20].", "startOffset": 132, "endOffset": 136}, {"referenceID": 16, "context": "The system combines vector space model [18], word alignment, and Machine", "startOffset": 39, "endOffset": 43}, {"referenceID": 9, "context": "Jaccard similarity[11] which treats documents as sets of tokens, and Okapi BM25[17] and Lucene Similarity[12], which rely on term frequency and inverse document frequency of words in the documents, are other widely used document similarity measures.", "startOffset": 18, "endOffset": 22}, {"referenceID": 15, "context": "Jaccard similarity[11] which treats documents as sets of tokens, and Okapi BM25[17] and Lucene Similarity[12], which rely on term frequency and inverse document frequency of words in the documents, are other widely used document similarity measures.", "startOffset": 79, "endOffset": 83}, {"referenceID": 10, "context": "Jaccard similarity[11] which treats documents as sets of tokens, and Okapi BM25[17] and Lucene Similarity[12], which rely on term frequency and inverse document frequency of words in the documents, are other widely used document similarity measures.", "startOffset": 105, "endOffset": 109}, {"referenceID": 7, "context": "One of the most popular techniques used for document similarity is Explicit Semantic Analysis (ESA) [8].", "startOffset": 100, "endOffset": 103}, {"referenceID": 8, "context": "A prominent approach, proposed in [10], is based on measuring similarity at at both the lexical and semantic levels.", "startOffset": 34, "endOffset": 38}, {"referenceID": 1, "context": "Alternative document similarity techniques have been proposed that are based on statistical topic models [2, 14].", "startOffset": 105, "endOffset": 112}, {"referenceID": 12, "context": "Alternative document similarity techniques have been proposed that are based on statistical topic models [2, 14].", "startOffset": 105, "endOffset": 112}, {"referenceID": 5, "context": "A very recent word2vec based technique, called paragraph vector, is proposed in [6].", "startOffset": 80, "endOffset": 83}, {"referenceID": 0, "context": "Usually, these bounds are normalized to the interval [0, 1].", "startOffset": 53, "endOffset": 59}, {"referenceID": 2, "context": "Documents can be represented as BoW, following the assumption of exchangeability [3].", "startOffset": 81, "endOffset": 84}, {"referenceID": 17, "context": "Smith-Waterman algorithm is widely adopted to calculate gene/ protein sequence alignment - a very important problem in the field of bio-informatics [19].", "startOffset": 148, "endOffset": 152}, {"referenceID": 14, "context": "a high-dimensional vector represention [16]) of top-k most probable words in that topic.", "startOffset": 39, "endOffset": 43}, {"referenceID": 14, "context": "word embeddings) using GloVe based pre-trained word vectors [16].", "startOffset": 60, "endOffset": 64}, {"referenceID": 0, "context": "sentence_similarity : (S i , S b j ) \u2192 [0, 1]; where 1 is the maximum possible similarity between two segments.", "startOffset": 39, "endOffset": 45}, {"referenceID": 0, "context": "\u2013 f \u2208 [0, 1] is a discount factor for the similarity score.", "startOffset": 6, "endOffset": 12}, {"referenceID": 6, "context": "A dataset of 20,000 such triples were generated and made publicly available by [7].", "startOffset": 79, "endOffset": 82}, {"referenceID": 9, "context": "We compare the performance of our system with respect to other contemporary techniques such as Jaccard Index [11], Lucene Index [12], Okapi BM25 [17], and with Para2Vec - the system proposed in [6] (the creators of this dataset).", "startOffset": 109, "endOffset": 113}, {"referenceID": 10, "context": "We compare the performance of our system with respect to other contemporary techniques such as Jaccard Index [11], Lucene Index [12], Okapi BM25 [17], and with Para2Vec - the system proposed in [6] (the creators of this dataset).", "startOffset": 128, "endOffset": 132}, {"referenceID": 15, "context": "We compare the performance of our system with respect to other contemporary techniques such as Jaccard Index [11], Lucene Index [12], Okapi BM25 [17], and with Para2Vec - the system proposed in [6] (the creators of this dataset).", "startOffset": 145, "endOffset": 149}, {"referenceID": 5, "context": "We compare the performance of our system with respect to other contemporary techniques such as Jaccard Index [11], Lucene Index [12], Okapi BM25 [17], and with Para2Vec - the system proposed in [6] (the creators of this dataset).", "startOffset": 194, "endOffset": 197}, {"referenceID": 5, "context": "systems, except for [6].", "startOffset": 20, "endOffset": 23}, {"referenceID": 14, "context": "Upon replacing the pre-trained GloVe (trained by [16] on Wikipedia + Gigaword corpus, 300 dimensions) with pre-trained word2vec (trained by Google on Google News dataset, 300 dimensions) we observed that the overall performance of our system goes down from 72.", "startOffset": 49, "endOffset": 53}, {"referenceID": 5, "context": "We compared SimDoc with contemporary document similarity measures such as Jaccard, Lucene Index, BM25 and [6].", "startOffset": 106, "endOffset": 109}], "year": 2016, "abstractText": "Document similarity is the problem of formally representing textual documents and then proposing a similarity measure that can be used to compute the linguistic similarity between two documents. Accurate document similarity computation improves many enterprise relevant tasks such as document clustering, text mining, and question-answering. Most contemporary techniques employ bag-of-words (BoW) based document representation models. In this paper, we show that a document\u2019s thematic flow, which is often disregarded by bag-of-word techniques, is pivotal in estimating their semantic similarity. In this direction, we propose a novel semantic document similarity framework, called SimDoc. We model documents as topic-sequences, where topics represent latent generative clusters of relative words. We then use a sequence alignment algorithm, that has been adapted from the Smith-Waterman gene-sequencing algorithm, to estimate their semantic similarity. For similarity computation at a finer granularity, we tune the alignment algorithm by integrating it with a word embedding matrix based topic-to-topic similarity measure. A document level similarity score is then computed by again using the sequence alignment algorithm over all sentence pairs. In our experiments, we see that SimDoc outperforms many contemporary bag-of-words techniques in accurately computing document similarity, and on practical applications such as document clustering.", "creator": "LaTeX with hyperref package"}}}