{"id": "1611.04870", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Nov-2016", "title": "Constrained Low-Rank Learning Using Least Squares-Based Regularization", "abstract": "Low-rank learning has attracted much attention recently due to its efficacy in a rich variety of real-world tasks, e.g., subspace segmentation and image categorization. Most low-rank methods are incapable of capturing low-dimensional subspace for supervised learning tasks, e.g., classification and regression. This paper aims to learn both the discriminant low-rank representation (LRR) and the robust projecting subspace in a supervised manner. To achieve this goal, we cast the problem into a constrained rank minimization framework by adopting the least squares regularization. Naturally, the data label structure tends to resemble that of the corresponding low-dimensional representation, which is derived from the robust subspace projection of clean data by low-rank learning. Moreover, the low-dimensional representation of original data can be paired with some informative structure by imposing an appropriate constraint, e.g., Laplacian regularizer. Therefore, we propose a novel constrained LRR method. The objective function is formulated as a constrained nuclear norm minimization problem, which can be solved by the inexact augmented Lagrange multiplier algorithm. Extensive experiments on image classification, human pose estimation, and robust face recovery have confirmed the superiority of our method.", "histories": [["v1", "Tue, 15 Nov 2016 14:50:31 GMT  (368kb)", "http://arxiv.org/abs/1611.04870v1", "14 pages, 7 figures, accepted to appear in IEEE Transactions on Cybernetics"]], "COMMENTS": "14 pages, 7 figures, accepted to appear in IEEE Transactions on Cybernetics", "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["ping li", "jun yu", "meng wang", "luming zhang", "deng cai", "xuelong li"], "accepted": false, "id": "1611.04870"}, "pdf": {"name": "1611.04870.pdf", "metadata": {"source": "CRF", "title": "Constrained Low-rank Learning Using Least Squares Based Regularization", "authors": ["Ping Li"], "emails": [], "sections": [{"heading": null, "text": "ar Xiv: 161 1.04 870v 1 [cs.C V] 15 Nov 201 6Constrained Low-Rank Learning Using Least Squares Based RegularizationPing Li, Member, IEEE, Jun Yu, Member, IEEE, Meng Wang, Member, IEEE, Luming Zhang, Member, IEEE, Deng Cai, Member, IEEE, and Xuelong Li, Fellow, IEEE, Abstract - Low-Rank Learning has lately attracted a lot of attention due to its effectiveness in a rich variety of real-world tasks, e.g. subspace segmentation, image categorization. Most low-rank methods are unable to capture low-dimensional subspace for supervised learning tasks, e.g. classification and regression. This work aims to learn both discriminatory low-rank representation and robust subspace projection in a considered way."}, {"heading": "1 INTRODUCTION", "text": "In fact, most of them are able to survive on their own if they do not put themselves in a position to survive on their own."}, {"heading": "2 RELATED WORK", "text": "This year, it has come to the point that it will only be once before there is such a process, in which there is such a process."}, {"heading": "3 PROBLEM SETTING", "text": "In this essay, we define the limited minor learning problem as follows: Given a collection of data points {x1, x2,.., xn} and their names {y1, y2,.. distributed in k classes, we assume that they are samples taken roughly from a mixture of several sub-ranges [2]. The main goal is to seek the discriminatory subordinate representation Z as well as the robust projection of subspace P. Specifically, we designate the training data by X-Rd-n with each data point stacked in a column, and the data matrix can be broken down into a clean component X-Ra and an error component E-Rd-n, in which A-Rd-M is treated as a dictionary covering the data space, while Z-Rm-N reveals the underlying subspace structure of the data."}, {"heading": "4 OUR METHOD", "text": "This section focuses on the elaboration of the proposed method, including the formulation and optimization framework, as well as algorithmic procedures."}, {"heading": "4.1 Formulation", "text": "As already mentioned, our goal is to jointly search for the discriminant of the lowest rank Z > Rm \u00b7 n and the robust projected portion P > Rd \u00b7 k in an overarching way. Essentially, we must use the differentiation of the ranking order (Z), which is still difficult to solve, due to its discrete nature as a common practice in the lowest ranking order of methods [2], [5] which we use the nuclear standard as its convex accompaniment. In this work, the dictatorial A is set to X. Therefore, our objective function can be formulated as follows: min Z, E, P, P, S, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E. (1) where the nuclear standard is the standard of the standard. (PTXLXTP) + \u03b2, V \u2212 Y, E, V = PTXZ + E, V = PTXZ, 1T n n. (1) where the nuclear standard of the poorer group of a matrix is the poorer."}, {"heading": "4.2 Optimization", "text": "In this part we show how to optimize the objective function in (1) using the advanced J + J (ALM) algorithm (9). (1) To make the objective function divisible and solvable, we introduce the relaxation variable J + E and replace the constraint V = PTXZ in (1), which leads to an equivalent problem: min Z, E, J, J, J, J, J, E, E, E, E, F, S, S, XZ, XZ, Z, J (2), where the constraint of the term is: F (Z) = PTXLXTP, Tr [PTXLXTP] + PTXTXZ \u2212 Y, 2F."}, {"heading": "5 DISCUSSION", "text": "In this section we focus on discussing the advantages, limitations and computational complexity of the proposed CLRR methodology. Algorithm 1 Problem Solving (4) by Inexact ALMInput: The data matrix X, the label matrix Y, the parameters \u03b1, \u03b2, \u03bb. Initialization: Sets all entries of Z, J, E, and updates P, J, Z, and E, each by solving (6), (7), (8), and (15).3: Updating the Lagrange multipliers: updating a + \u00b5 (X \u2212 XZ \u2212 E), updating the others, and updating the P, J, Z, and E, each by solving (6), (7), (8), and (15).3: Updating the Lagrange multipliers: updating a + \u00b5 (X \u2212 XZ \u2212 E), revaluing the data."}, {"heading": "5.1 Advantages", "text": "As we know, CLRR uses explicitly monitored information, i.e., data markers, to guide low-rank learning and robust subspace projection, a feature that distinguishes it from previous work [1], [2], [35]. In this paper, we apply our method to several important real-world applications, i.e. classification is an estimate, and the utilization of robust data is low. For classification in Algorithm 2, we can use either the recovered data XZ or the reduced data representation P XZ. In practice, however, we found the classification of the use of PTXZ less satisfactory than the use of XZ, especially when the number of classes is small. This could be fixed to the dimensions of the predecessor by the number of classes."}, {"heading": "5.2 Constraints", "text": "In all that it is in the oireePnlrteeee\u00fcgr nvo nlrlteeaeBnn nvo rf\u00fc ide nlrteeeaeaeVnlrlrrrrrrrrlrrrrrrrrrrrrln rf\u00fc ide nlrteeeeaeaeaeVnlrrrrrrrrrrlrrrrrrlrrrrrrrrrlrrrrrrrlrrrrlrrrrrlrrrrrrlrrrlrrrrrlrrrrlrrrrrrrlrrrrrlrrrrrrrrlrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr"}, {"heading": "5.3 Computational Complexity Analysis", "text": "To update P, it takes O (n3) to calculate ZZT, making it the most expensive component in solving P. To update J, it takes O (n2r + r2n), where r ranks (Z + 1\u00b5) b. To update Z, it takes O (n2d + ndk) to calculate Ua, Ub, and O (n2d) to calculate Uc. To update E, it takes O (d2) to calculate E. Suppose the updates stop after t iterations, the total cost of CLRR is isO [t (n3 + n2 (r + d) + r2n + dk."}, {"heading": "6 EXPERIMENTS", "text": "In this section, we conducted a wide range of experiments to examine the performance of the proposed CLRR method in three real-world applications, i.e. image classification, human pose estimation and robust facial reconstruction. All experiments were conducted in MATLAB R2014a on Windows 7 with Intel i7-5820K CPU at 3.30 GHz."}, {"heading": "6.1 Image Classification", "text": "In this part, we examine the classification capabilities of several state-of-the-art methods on three publicly available image databases, i.e., COIL100, VOC2012, Caltech101. Database descriptions: COIL1001 contains 7,200 samples and 100 categories, each of which has 72 samples; VOC20122 comes from the Visual Object Classes Challenge 2012 database [41], which includes 17,125 samples from 20 different categories; Caltech1013 consists of 8,677 images from 101 categories. To display the images, we adopt the toolbox used in [42] to extract their characteristics descriptors, i.e., GIST, LBP, dense HOG. GIST [43] describes the spatial envelope of the image and has 512D; LBP [44] the non-uniform local binary patterns and levels to obtain final 1239D characteristics."}, {"heading": "6.2 Human Pose Estimation", "text": "To investigate the regression performance of the proposed CLRR method, we look at the problem of estimating 3D configurations of complex articulated objects from monocular images for applications requiring 3D analysis of the human body. [50] The database is detailed in. We select the image silhouettes because they are more reliable to extract, and the shape context distributions are used to generate 100-D characteristics for each sample. In total, we have used 1691 training points from seven sequences and 418 test points from one sequence, while the silhouette descriptors have 100 dimensions. Just like [50], we simply take back the original motion capture-based training format in the form of Euler angles. Overall, we have used 1691 training points from seven sequences and 418 test points from one sequence, while the silhouette descriptors have 100 dimensions. We compare CLRR with three popular regression methods, including Least Squares Resector (Relevance VR Machines) and Robust Vource Machines (Robust)."}, {"heading": "6.3 Robust Face Recovery", "text": "To investigate the robustness of CLRR, we conducted experiments with the Cohn-Kanade AU-Coded Facial Expression Database4 [52], [53] In accordance with the user agreement, and randomly selected six subjects available, including 848 images, which were cropped and resized to 64 \u00d7 64. Here, we show the recovery effects of our method and LRR from several examples contaminated by artificial noise. To achieve a favorable performance, we adjusted the parameters to the best, and the resulting images are shown in Figure 7. These images indicate the superiority of our method of dealing with the loud scenario. Specifically, the restored faces by CLRR exhibit much less noise compared to LRR. This is because the low-level subspace Z is learned by simultaneously considering the laplactic regulator and the least square regulator."}, {"heading": "7 CONCLUSIONS", "text": "This work has developed a novel method of restricted LowRank representation that provides a meaningful way to collectively learn both the discriminatory lowest rank. http: / / www.pitt.edu / emotion / ck-spread.htmrepresentation as well as the robust projecting subspace. Unlike most low-level learning methods that neglect monitored information, we explicitly use label information via the adaptive least-squares regulator, so that the learned lowest position and low-dimensional subspace have greater discriminatory power. Therefore, of course, it can improve the performance of multiple applications in the real world. Objective function is formulated as a limited rank minimization problem solved by the inaccurate Augmented Lagrange multiplier algorithm. In addition, we have had some discussions about the benefits, limitations, and computational complexity analysis of CLRR. To demonstrate the effectiveness of our method, extensive experiments have been conducted to re-classify faces, and robustly assess the human face."}, {"heading": "ACKNOWLEDGMENTS", "text": "The authors thank the anonymous reviewers for their helpful and constructive comments, which have contributed significantly to the improvement of this manuscript."}], "references": [], "referenceMentions": [], "year": 2016, "abstractText": "Low-rank learning has attracted much attention recently due to its efficacy in a rich variety of real-world tasks, e.g., subspace<lb>segmentation, image categorization. Most low-rank methods are incapable of capturing low-dimensional subspace for supervised<lb>learning tasks, e.g., classification and regression. This work aims to learn both the discriminant low-rank representation and the robust<lb>projecting subspace in a supervised manner. To achieve this goal, we cast the problem into a constrained rank minimization framework<lb>by adopting the least squares regularization. Naturally, the data label structure tends to resemble that of the corresponding low-<lb>dimensional representation, which is derived from the robust subspace projection of clean data by low-rank learning. Moreover, the<lb>low-dimensional representation of original data can be paired with some informative structure by imposing an appropriate constraint,<lb>e.g., Laplacian regularizer. Therefore, we propose a novel Constrained Low-Rank Representation (CLRR) method. The objective<lb>function is formulated as a constrained nuclear norm minimization problem, which can be solved by the inexact Augmented Lagrange<lb>Multiplier algorithm. Extensive experiments on image classification, human pose estimation and robust face recovery have confirmed<lb>the superiority of our method.", "creator": "LaTeX with hyperref package"}}}