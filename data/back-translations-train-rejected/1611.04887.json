{"id": "1611.04887", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Nov-2016", "title": "Interpreting the Syntactic and Social Elements of the Tweet Representations via Elementary Property Prediction Tasks", "abstract": "Research in social media analysis is experiencing a recent surge with a large number of works applying representation learning models to solve high-level syntactico-semantic tasks such as sentiment analysis, semantic textual similarity computation, hashtag prediction and so on. Although the performance of the representation learning models are better than the traditional baselines for the tasks, little is known about the core properties of a tweet encoded within the representations. Understanding these core properties would empower us in making generalizable conclusions about the quality of representations. Our work presented here constitutes the first step in opening the black-box of vector embedding for social media posts, with emphasis on tweets in particular.", "histories": [["v1", "Tue, 15 Nov 2016 15:34:47 GMT  (11kb)", "http://arxiv.org/abs/1611.04887v1", "Presented at NIPS 2016 Workshop on Interpretable Machine Learning in Complex Systems"]], "COMMENTS": "Presented at NIPS 2016 Workshop on Interpretable Machine Learning in Complex Systems", "reviews": [], "SUBJECTS": "cs.CL cs.SI", "authors": ["j ganesh", "manish gupta", "vasudeva varma"], "accepted": false, "id": "1611.04887"}, "pdf": {"name": "1611.04887.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["ganesh.j@research.iiit.ac.in", "gmanish@microsoft.com", "vv@iiit.ac.in"], "sections": [{"heading": null, "text": "ar Xiv: 161 1.04 887v 1 [cs.C L] 15 Nov 201 6Interpreting the Syntactic and Social Elements of the Tweet Representations via Elementary PropertyPrediction TasksGanesh J IIIT Hyderabad, Indiaganesh.j @ research.iiit.ac.in Manish Gupta Microsoft, Hyderabad, India gmanish @ microsoft.com Vasudeva Varma IIIT Hyderabad, India vv @ iiit.ac.in"}, {"heading": "1 Introduction", "text": "Research in social media analysis has recently seen an increase in the number of research papers using representation learning models to solve basic syntactic-semantic tasks, such as sensation analysis [1], semantic textual similarity calculation [2], tweet prediction [3], and so on. Although the performance of representation learning models is better than traditional models for all tasks, little is known about the core characteristics of a tweet encoded within representations. In a recent paper, Hill et al. [4] perform a comparison of different sentence representation models by evaluating them for various high-level semantic tasks, such as paraphrase identification, sensation classification, sensation classification, document response retrieval, and so on. This type of coarse analysis is opaque because it does not clearly disclose the type of information encoded by representations. Our work presented here represents the first step of tweets in the opening of the vector black box in particular."}, {"heading": "2 Elementary Property Prediction Tasks", "text": "In this section, we list the proposed tasks for predicting elementary properties to test the properties of a tweet embed. Table 1 explains all the tasks considered in this study. Note that we use a neural network to create the classifier for tasks predicting elementary properties, which has the following two layers: the representation layer and the Softmax layer on top, the size of which varies according to the specific task."}, {"heading": "3 Representation Models", "text": "In this section, we list the models included in the study."}, {"heading": "3.1 Unsupervised", "text": "\u2022 Bag Of Words (BOW) [17] - This simple representation captures the TF-IDF value of an n-gram. We select top 50K n-grams, with the value of \"n\" up to 5. \u2022 Deferred Dirichlet Allocation (LDA) [18] - We use the topic distribution that results by executing LDA with the number of topics as a tweet representation. \u2022 Bag Of Means (BOM) - We take the average of word embeddings that are generated by executing the glove model [12] to 2 billion tweets with an embedding size greater than 200. \u2022 Deep Structured Semantic Models (DSSM) [9] - This is a deep encoder that is trained to represent queries and documents in common space, for document ranking. We use the publicly available pre-formed encoder to encode the tweets. \u2022 Convolutional DSSM (CDSSM) [10] - This is the convolutional variant of GRAP13 on the GRV."}, {"heading": "3.2 Supervised", "text": "\u2022 Convolutional Neural Network (CNN) - This is a simple CNN proposed in [7]. \u2022 Long Short Term Memory Network (LSTM) [14] - This is a recursive vanilla LSTM model that is applied from the beginning to the end of a tweet, and the last hidden vector is used as a tweet representation. \u2022 Bidirectional LSTM (BLSTM) [14] - This extends LSTM by using two LSTM networks, with a tweet processed from left to right or right to left. Tweet is displayed by concatenating the last hidden vector of the two LSTMs. \u2022 FastText (FT) [8] - This is a simple architecture where the n-gram vectors on average represent a tweet, followed by the Softmax in the last level. This simple model has proven effective for the task of text classification."}, {"heading": "4 Experiments", "text": "In this section, we perform a comprehensive evaluation of all models to determine the meaning of different representation models. In essence, we examine each model (with optimal settings shown in the corresponding work) in terms of the following three perspectives. (1) Property Prediction Task Accuracy - This test identifies the model with the best F1 score for each elementary prediction task. (a) Best of all in: Property Prediction tasks where this model has outperformed all other models. (b) Best of uncontrolled approaches in: Property Prediction tasks where this model has outperformed all other uncontrolled models. (c) Best of supervised approaches in: Property Prediction tasks where this model has outperformed all other supervised models. (2) Property Prediction Task Accuracy versus Tweet Length - This test helps compare the performance of the model for shorter and longer tweets. (a) Positive correlated tasks: Property Prediction tasks are tasks where the model has outperformed."}, {"heading": "5 Results and Analysis", "text": "Fine-grained analyses of various monitored and unattended models discussed in Section 3, on various dimensions discussed in Section 4, are presented in Table 2. Codes used to conduct our experiments are publicly available at: https: / / github.com / ganeshjawahar / fine-tweet /."}, {"heading": "5.1 Property Prediction Task Accuracy", "text": "We summarize the results of the prediction tasks in Table 3. Forecasting length is proving relatively difficult for most models. Models based on recurring architectures such as LSTM, STV, T2V have sufficient capacity to model the length of the tweet well. BLSTM is also best at modeling slang. BLSTM outperforms the LSTM variant in all tasks except \"content,\" which means the power of using information flowing from both directions of the tweet. T2V, which is expected to perform worst in this task due to its ability to work on a finer plane (i.e., characters). In fact, T2V does not outperform other models in any task, which may be mainly due to the fact that the hashtags used to supervise when learning tweet representations are good."}, {"heading": "5.2 Property Prediction Task Accuracy versus Tweet Length", "text": "This setup captures the behavior of the model with the increase in the context size defined by the number of words. In the \"Word Order\" task, we see that the performance of all models is negatively correlated with the expected tweet length. In social tasks such as \"Is Reply\" and \"Reply Time,\" we see a positive correlation between the tweet length and the performance of all models. This insight is intuitive in social media analysis, where additional context is most helpful in modeling social behavior."}, {"heading": "5.3 Sensitivity of Property Prediction Task to Word Order", "text": "This test essentially captures the meaning of the \"natural order of words.\" We found that for most tasks, LDA was invariant in reordering the words in the tweet. This result is not surprising, since LDA considers each word in the tweet independently. CNN, LSTM, and BLSTM rely significantly on the word order for most prediction tasks to achieve good results."}, {"heading": "6 Conclusion", "text": "The openness of social media not only provides a wealth of ways to understand the basic characteristics of the posts, but also helped us gain new insights into different representation models. We observed that CNN, LSTM, and BLSTM among the monitored models encompass most syntactic and social characteristics with high accuracy, while BOW, DSSM, STV, and T2V do so among the unattended models. Length of tweets affects the accuracy of predicting the task, but we found that all models behave similarly when the length of tweets varies."}], "references": [{"title": "Sentiment Embeddings with Applications to Sentiment Analysis. In: TKDE", "author": ["D. Tang", "F. Wei", "B. Qin", "N. Yang", "T. Liu", "M. Zhou"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2016}, {"title": "Siamese CBOW: Optimizing Word Embeddings for Sentence Representations", "author": ["T. Kenter", "A. Borisov", "M. de Rijke"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2016}, {"title": "Tweet2Vec: Character- Based Distributed Representations for Social Media", "author": ["B. Dhingra", "Z. Zhou", "D. Fitzpatrick", "M. Muehl", "W.W. Cohen"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2016}, {"title": "Learning distributed representations of sentences from unlabelled data", "author": ["F. Hill", "K. Cho", "A. Korhonen"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2016}, {"title": "Fine-grained Analysis of Sentence Embeddings Using Auxiliary Prediction Tasks", "author": ["Y. Adi", "E. Kermany", "Y. Belinkov", "O. Lavi", "Y. Goldberg"], "venue": "arXiv preprint arXiv:1608.04207", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2016}, {"title": "Convolutional neural networks for sentence classification", "author": ["Y. Kim"], "venue": "EMNLP", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2014}, {"title": "Bag of Tricks for Efficient Text Classification", "author": ["A. Joulin", "E. Grave", "P. Bojanowski", "T. Mikolov"], "venue": "arXiv preprint arXiv:1607.01759", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2016}, {"title": "Learning deep structured semantic models for web search using clickthrough data", "author": ["P.S. Huang", "X. He", "J. Gao", "L. Deng", "A. Acero", "L. Heck"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2013}, {"title": "A latent semantic model with convolutionalpooling structure for information retrieval", "author": ["Y. Shen", "X. He", "J. Gao", "L. Deng", "G. Mesnil"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2014}, {"title": "Named entity recognition in tweets: an experimental study", "author": ["A. Ritter", "S. Clark", "Mausam", "O. Etzioni"], "venue": "EMNLP", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2011}, {"title": "Glove: Global Vectors for Word Representation", "author": ["J. Pennington", "R. Socher", "C.D. Manning"], "venue": "EMNLP", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2014}, {"title": "Distributed Representations of Sentences and Documents", "author": ["Q.V. Le", "T. Mikolov"], "venue": "In: ICML", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2014}, {"title": "Speech recognition with deep recurrent neural networks. In: ICASSP", "author": ["A. Graves", "A.R. Mohamed", "G. Hinton"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["T. Mikolov", "I. Sutskever", "K. Chen", "G.S. Corrado", "J. Dean"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2013}, {"title": "On the properties of neural machine translation: Encoder-decoder approaches", "author": ["K. Cho", "B. Van Merri\u00ebnboer", "D. Bahdanau", "Y. Bengio"], "venue": "arXiv preprint arXiv:1409.1259", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2014}, {"title": "Latent dirichlet allocation. In: JMLR", "author": ["D.M. Blei", "A.Y. Ng", "M.I. Jordan"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2003}], "referenceMentions": [{"referenceID": 0, "context": "Research in social media analysis is recently seeing a surge in the number of research works applying representation learning models to solve high-level syntactico-semantic tasks such as sentiment analysis [1], semantic textual similarity computation [2], hashtag prediction [3] and so on.", "startOffset": 206, "endOffset": 209}, {"referenceID": 1, "context": "Research in social media analysis is recently seeing a surge in the number of research works applying representation learning models to solve high-level syntactico-semantic tasks such as sentiment analysis [1], semantic textual similarity computation [2], hashtag prediction [3] and so on.", "startOffset": 251, "endOffset": 254}, {"referenceID": 2, "context": "Research in social media analysis is recently seeing a surge in the number of research works applying representation learning models to solve high-level syntactico-semantic tasks such as sentiment analysis [1], semantic textual similarity computation [2], hashtag prediction [3] and so on.", "startOffset": 275, "endOffset": 278}, {"referenceID": 3, "context": "[4] perform a comparison of different sentence representation models by evaluating them for different high-level semantic tasks such as paraphrase identification, sentiment classification, question answering, document retrieval and so on.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[5], which investigates three sentence properties in comparing unsupervised sentence representation models such as average of words vectors and LSTM auto-encoders.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "5 Hashtag Predict whether the word is a hashtag in the tweet or not Randomly choose the word in the tweet which is not a hashtag 6 NE [11] Predict whether the n-gram is a Named Entity (NE) in the tweet or not Randomly choose the n-gram in the tweet which is not a NE", "startOffset": 134, "endOffset": 138}, {"referenceID": 15, "context": "\u2022 Latent Dirichlet Allocation (LDA) [18] - We use the topic distribution resulting by running LDA with number of topics as 200, as tweet representation.", "startOffset": 36, "endOffset": 40}, {"referenceID": 10, "context": "\u2022 Bag Of Means (BOM) - We take the average of the word embeddings obtained by running the Glove [12] model on 2 billion tweets with embedding size as 200.", "startOffset": 96, "endOffset": 100}, {"referenceID": 7, "context": "\u2022 Deep Structured Semantic Models (DSSM) [9] - This is a deep encoder trained to represent query and document in common space, for document ranking.", "startOffset": 41, "endOffset": 44}, {"referenceID": 8, "context": "\u2022 Convolutional DSSM (CDSSM) [10] - This is the convolutional variant of DSSM.", "startOffset": 29, "endOffset": 33}, {"referenceID": 11, "context": "\u2022 Paragraph2Vec (PV) [13] - This model based on Word2Vec [15] learns embedding for a document which is good in predicting the words within it.", "startOffset": 21, "endOffset": 25}, {"referenceID": 13, "context": "\u2022 Paragraph2Vec (PV) [13] - This model based on Word2Vec [15] learns embedding for a document which is good in predicting the words within it.", "startOffset": 57, "endOffset": 61}, {"referenceID": 14, "context": "\u2022 Skip-Thought Vectors (STV) [6] - This is a GRU [16] encoder trained to predict adjacent sentences in a books corpus.", "startOffset": 49, "endOffset": 53}, {"referenceID": 2, "context": "\u2022 Tweet2Vec (T2V) [3] - This is a character composition model working directly on the character sequences to predict the user-annotated hashtags in a tweet.", "startOffset": 18, "endOffset": 21}, {"referenceID": 1, "context": "\u2022 Siamese CBOW (SCBOW) [2] - This model uses averaging of word vectors to represent a sentence, and the objective and data used here is the same as that for STV.", "startOffset": 23, "endOffset": 26}, {"referenceID": 5, "context": "\u2022 Convolutional Neural Network (CNN) - This is a simple CNN proposed in [7].", "startOffset": 72, "endOffset": 75}, {"referenceID": 12, "context": "\u2022 Long Short Term Memory Network (LSTM) [14] - This is a vanilla LSTM based recurrent model, applied from start to the end of a tweet, and the last hidden vector is used as tweet representation.", "startOffset": 40, "endOffset": 44}, {"referenceID": 12, "context": "\u2022 Bi-directional LSTM (BLSTM) [14] - This extends LSTM by using two LSTM networks, processing a tweet left-to-right and right-to-left respectively.", "startOffset": 30, "endOffset": 34}, {"referenceID": 6, "context": "\u2022 FastText (FT) [8] - This is a simple architecture which averages the n-gram vectors to represent a tweet, followed by the softmax in the final layer.", "startOffset": 16, "endOffset": 19}, {"referenceID": 4, "context": "The superior performance of all the models for the \u2018Content\u2019 task in particular is unlike the relatively lower performance reported for in [5], mainly because of the short length of the tweets.", "startOffset": 139, "endOffset": 142}], "year": 2016, "abstractText": "Research in social media analysis is recently seeing a surge in the number of research works applying representation learning models to solve high-level syntactico-semantic tasks such as sentiment analysis [1], semantic textual similarity computation [2], hashtag prediction [3] and so on. Though the performance of the representation learning models are better than the traditional models for all the tasks, little is known about the core properties of a tweet encoded within the representations. In a recent work, Hill et al. [4] perform a comparison of different sentence representation models by evaluating them for different high-level semantic tasks such as paraphrase identification, sentiment classification, question answering, document retrieval and so on. This type of coarse-grained analysis is opaque as it does not clearly reveal the kind of information encoded by the representations. Our work presented here constitutes the first step in opening the black-box of vector embeddings for social media posts, particularly tweets.", "creator": "LaTeX with hyperref package"}}}