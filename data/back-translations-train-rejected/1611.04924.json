{"id": "1611.04924", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Nov-2016", "title": "Robust Semi-Supervised Graph Classifier Learning with Negative Edge Weights", "abstract": "In a semi-supervised learning scenario, (possibly noisy) partially observed labels are used as input to train a classifier, in order to assign labels to unclassified samples. In this paper, we study this classifier learning problem from a graph signal processing (GSP) perspective. Specifically, by viewing a binary classifier as a piecewise constant graph-signal in a high-dimensional feature space, we cast classifier learning as a signal restoration problem via a classical maximum a posteriori (MAP) formulation. Unlike previous graph-signal restoration works, we consider in addition edges with negative weights that signify anti-correlation between samples. One unfortunate consequence is that the graph Laplacian matrix $\\mathbf{L}$ can be indefinite, and previously proposed graph-signal smoothness prior $\\mathbf{x}^T \\mathbf{L} \\mathbf{x}$ for candidate signal $\\mathbf{x}$ can lead to pathological solutions. In response, we derive an optimal perturbation matrix $\\boldsymbol{\\Delta}$ - based on a fast lower-bound computation of the minimum eigenvalue of $\\mathbf{L}$ via a novel application of the Haynsworth inertia additivity formula---so that $\\mathbf{L} + \\boldsymbol{\\Delta}$ is positive semi-definite, resulting in a stable signal prior. Further, instead of forcing a hard binary decision for each sample, we define the notion of generalized smoothness on graph that promotes ambiguity in the classifier signal. Finally, we propose an algorithm based on iterative reweighted least squares (IRLS) that solves the posed MAP problem efficiently. Extensive simulation results show that our proposed algorithm outperforms both SVM variants and graph-based classifiers using positive-edge graphs noticeably.", "histories": [["v1", "Tue, 15 Nov 2016 16:36:29 GMT  (309kb,D)", "http://arxiv.org/abs/1611.04924v1", "13 pages, submitted to IEEE Journal of Selected Topics in Signal Processing (Special Issue on Graph Signal processing)"], ["v2", "Thu, 20 Jul 2017 13:10:24 GMT  (1036kb,D)", "http://arxiv.org/abs/1611.04924v2", "15 pages, revised for IEEE Transactions on Signal and Information Processing over Network"]], "COMMENTS": "13 pages, submitted to IEEE Journal of Selected Topics in Signal Processing (Special Issue on Graph Signal processing)", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["gene cheung", "weng-tai su", "yu mao", "chia-wen lin"], "accepted": false, "id": "1611.04924"}, "pdf": {"name": "1611.04924.pdf", "metadata": {"source": "CRF", "title": "Robust Semi-Supervised Graph Classifier Learning with Negative Edge Weights", "authors": ["Gene Cheung", "Yu Mao", "Chia-Wen Lin"], "emails": ["mao}@nii.ac.jp).", "wengtai2008@hotmail.com,", "cwlin@ee.nthu.edu.tw)."], "sections": [{"heading": null, "text": "In fact, most people are able to know themselves and to understand how they have behaved. (...) Most people in the world do not know what they are to do. (...) They do not know what they are to do. (...) They do not know what they are to do. (...) They do not know what they are to do. (...) They do not know what they are to do. (...) They do not know what they are to do. (...) \"(...)\" They do not know what they are to do. \"(...)\" (...). \"(...)\" They do not know what they are to do. \"(...).\" (...). \"(...).\" (). \"(...).\" (). \"().\" (). \"().\" (). \"().\" (). \"().\" (). \"().\" (). \"().\" (). \"().\"). \"().\" (). \").\" (). \"().\"). \"().\". \").\". \"().\". \").\". \".\". \").\". \").\". \".\". \".\". \".\"). \"().\"). \".\". \".\".. \".\". \".\". \"....\"...... \".........................\").................................... \").................................\")........................................................................... \""}, {"heading": "II. RELATED WORKS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. Robust Classifier Learning", "text": "Classifier learning with labeling noise has aroused great interest, including a workshop 1 in NIPS '10 [26] and a journal for neurocomputing [27]. There is a wide range of approaches, including theoretical (e.g. label propagation in [28]) and application-specific (e.g. emotion detection using an inference algorithm based on a multiplicative refresh rule [29]).Compared to previous work [2] - [5], we opt for the construction of a graph-based classifier, in which each acquired sample is represented as a node in a high-dimensional feature space and connects to other sample nodes in its neighborhood. Compared to previous work on graph-based classifier learning, our approach is novel in the following aspects. First, we learn a binary classifier using a classifier using a classical MAP formulation, but also consider negative edge weights, which mean an anti-correlation. For the smoothness of the lapaph signal signal, we require an intuitive interpretation of the matrax signal to the stability of the matrix."}, {"heading": "B. Graph-Signal Restoration", "text": "Graph signal priorities have been used for image restoration problems such as denoising [9] - [11], interpolation [12] - [16], bit depth enhancement [17], [18] and JPEG de-quantization [19], [20]. Conventional assumption is that the desired graph signal is smooth or band-delimited with respect to a suitable graph with non-negative edge weights, the Interpixel1https: / / people.cs.umass.edu / wallach / workshops / nips2010css / correlation. In contrast, we also include anti-correlation information in the graphs, taking negative edges into account. We also define a general idea of graph smoothness for signal restoration specifically for classification learning."}, {"heading": "C. Alternative Graph-Signal Smoothness Prior", "text": "An alternative line of previous work based its GSP analysis on algebraic theory in traditional digital signal processing based on the shift operator [4], [30] - [32]. Specifically, instead of the graph Laplacian Matrix L, the adjacence matrix W is used as a variation operator to define signal smoothness and graph frequencies. \u2212 As an example of graph signal recovery, a smoothness before x \u2212 Wx \u00b2 pp is suggested for some positive integers p. \u2212 If edge weights are negative, however, such smoothness may become unreasonable. Consider the three-node graph in Figure 2. Assumption p = 2, the smoothness before w = \u2212 1 is: x \u2212 Wx \u00b2 22 = large (I \u2212 W) x 2 = large (I \u2212 W) x 2 = small is the difference between the two."}, {"heading": "D. Negative Edge Weights in Graphs", "text": "Recent studies in the control community have examined the conditions under which one or more negative edge weights would cause a laplac graph to remain indeterminate [34], [35]. However, the analysis is based on the assumption that there are no cycles in the graph with more than one negative edge, which is too restrictive for binary classification graphs. [36] The analysis is considered a signed social network in which each edge denotes either a cohesive (positive edge weights) or oppositive (negative edge weights) relationship between two vertices. The goal is to identify similar groups within the graph and thus resembles a distributed cluster problem that is not by definition monitored. In contrast, our goal is to restore a classification graph signal from partially observed labels that is a semi-monitored learning problem."}, {"heading": "III. PRELIMINARIES", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. Graph Definition", "text": "A graph G (V, E, W) has a defined set V of N nodes and a set E of M edges. Each edge (i, j) linking the nodes i and j is undirected and has a corresponding scalar weight wi, j. In this work, we assume that wi, j can be positive or negative; a negative wi, j would mean that samples in the nodes i and j are anticorrelated - the samples should have very different values. A graph signal x-RN on G is a discrete signal of dimension N - a designation xi for each node (sample) i in V. However, if we limit x as a binary classifier, xi can only assume one of two values indicating the class sample to which I belong, i.e. x-RN is a discreconstructed signal xi for each node (sample) i in V. If we restrict x as a binary classifier, then xi can only assume one of two values indicating the class sample to which I belong, i.e. x-RN is a discreconstructed signal xi for each node (sample) i in V. If we use the reconstructed graph signal RN as a binary classifier, then xi can only assume one of two values indicating the class sample to which I belong, i.e. x-RN is a discreconstructed signal xi for each node (sample) i in V. If we use the reconstructed graph signal x-RN as a binary classifier to reconstruct the reconstruction of the SVN, we can reconstruct the SVN in order to include the hard values we can reconstruct these values into the SVN, instead of the multiplicity of the SVN."}, {"heading": "B. Graph Spectrum", "text": "Considering the edge weight (adjacency) matrix W, we define a diagonal degree matrix D in which di, i = \u2211 j wi, j. A combinatorial graph Laplakic matrix L is then simply L = D \u2212 W [7]. Since L is symmetrical, one can show via the PerronFrobenious Theorem2 that it can be self-decomposed into: L = V\u0102VT (2), where it is a diagonal matrix with real eigenvalues \u03bbk (not necessarily unambiguous) and V is an own matrix of orthogonal eigenvectors vi as columns. If edge weights wi, j are limited to not being negative, then one can show that L is positively semi-defined (PSD), which means that spec. 0,. k and xTLx."}, {"heading": "C. Graph-Signal Smoothness Prior", "text": "Traditionally, signal x for diagram G with non-negative edge weights is considered smooth if each label xi on node i is similar to the label xj on adjacent nodes j with large wi, j. In the diagram frequency range, it means that x contains predominantly low eigenfrequency components; i.e. the coefficients \u03b1 = VTx are zeros or very small for high frequencies. The smoothest signal is the constant vector 1 - the first eigenvector v1 for L, which corresponds to the smallest eigenvalue 1 = 0. Note that v1 = 1 has no zero intersections and higher frequency components vk increasingly have zero intersections according to the node phenomenon theorem [37]. Mathematically, we can write that a signal x is smooth if its diagram Laplace regulator xTLx is small [10], [11]."}, {"heading": "D. General Graph-Signal Restoration", "text": "Given the smoothness defined before (5), we can now formulate a general problem of graph recovery using an MAP formula. Suppose we obtain a partial observation y-RK of the desired graph signal x-RN, where K < N. The observation y can be corrupted by an additive noise z-RK, where z is zero-mean Gaussian noise with covariance Q. Thus, the noise model for y is: y = Hx + z (6), where H {0, 1} K \u00b7 N is a binary matrix that extracts the K components from x corresponding to y. Given the noise model (6), we can define a probability term Pr (y \u2212 x): Pr (y \u2212 x), exp (\u2212 Hx) TQ \u2212 1 (y \u2212 Hx), \u03c31 (y \u2212 Hx), \u03c32), (7) x: Given the smoothness of AP (5), we can classify (7) and MQ (7) where \u2212 is a problem."}, {"heading": "E. Graph Construction", "text": "While our optimization algorithm to be discussed is applicable to all undirected graphs with positive and negative edge weights, the proper assignment of negative edges follows a major difference in actual classification performance. We discuss our edge assignment procedures in a graph G. We first construct a graph G with node V representing N samples. For each sample i we calculate a characteristic vector hi of a dimension D. Then we can determine a non-negative edge weighting wi, j = exp (\u2212 hj) TVP (hi \u2212 hj) \u03c32h (10) where \u03c3h is a parameter."}, {"heading": "F. Example of Graph with Negative Edges", "text": "To illustrate, let us consider a simple example in Fig. 3 (a): a 12-node graph in which nodes 1 to 6 are similar and are connected by edges of weight 1 and nodes 7 to 12 are similar. Nodes between the two classes are connected by edges of weight 0.1, except pairs (2, 8) and (5, 11) - Centering four clusters - connected by edges of weight \u2212 1. Graph Lapalacian L for this graph has two negative eigenvalues \u2212 0.94 and \u2212 0.76, and the corresponding eigenvectors v1 and v2 are shown in Fig. 3 (b). We observe that v1 exhibits the desired behavior of the soil truth classifier x: similar nodes 1 to 6 (nodes 7 to 12) have the same sign. On the other hand, while v2 have values of opposite characters of contiguous magnets (2, 8) and (5, 11), which are consistent with the opposite sets, 2, 2, 2, 2, 2, and 2-related signals (2)."}, {"heading": "IV. FINDING A PERTURBATION MATRIX", "text": "To give an intuition about the effects of this on the eigenvalues of L + \u2206, let us first consider the inequality of Weyl 4. Let us have the symmetrical matrix L-RN-N as described in (2) spectral decomposition L = VN-VT with eigenvalues \u03bbk along the diagonals of the diagonal of the diagonal matrix. Let us have a hermitic matrix with eigenvalues \u04451 \u2264 as described in (11).... \u2264 The inequality of Weyl states that L + \u0445 has eigenvalues \u04451 \u2264. \u2212 \u2264 N, so that: \u03bbi + \u03b31 \u2264 \u0445i \u2264 \u0445i-N (11) In other words, it says in (11) that the i-eigenvalue of L + \u0445i is the i-eigenvalue of L that shifts by an amount in the range. \u2212 Obviously, it implies that a minimum < 0, a minimum is a minimum, a minimum, a minimum, a minimum, a minimum, a minimum, a minimum, a minimum, a minimum, a minimum, a minimum, a minimum, a minimum, a minimum, a minimum, a minimum."}, {"heading": "A. Matrix Perturbation: Minimum-Norm Criteria", "text": "A reasonable choice is to find the minimum standard, i.e. the smallest standard, so that L + PSD is to be found: min. min. min. min. min. min. min. min. min. min. min. min. min. min. min. min. min. min. min. min. min. min. min. min. min. min. min. min. min. min. min. min. min. min. min. min. min. min. min. min. min. min. min. min. min. min. min. min. min. min. min. min. min. min. min. min. min. min. min. min. min. min. min. min. min. min. min. min. min. min. min. min. min. min. min. min. min. min. min. min. min. min. min. min. min. min. min. min. min. min. min. min. min. min. min. min. min. min. min. min. min. min. min. min. min. min. min. min. min. min. min. min. min. min. min. min. min. min. min. min. min. min. min. min. min. min. min. min. min. min. min. min. min. min. min. min. min. min. min. min. min. min. min. min. min. min. min. min. min. min. min. min. min. min. min. min. min. min. min. min. min. min. min. min. min. min. min. min. min. min. min. min. min. min. min. min. min. min. min. min. min. min. min. min. min. min. min. min. min. min. min. min. min. min. min. min. min. min. min. min. min. min. min. min. min. min. min. min. min. min. min. min. min. min. min. min. min. min. min. min. min. min. min. min. min. min. min. min. min. min. min. min. min. min. min. min. min. min. min. min"}, {"heading": "B. Matrix Perturbation: Minimum-Variance Criteria", "text": "The main problem with the minimum standard criteria is that the differentiation between different frequency components (eigenvectors) is removed (eigenvectors) by setting all negative eigenvalues from L to 0. It is therefore desirable to disrupt L in such a way that the frequency preferences are preserved (i.e., low eigenfrequencies continue to be preferred over high frequencies) while minimizing the overall disturbance. We define this term mathematically as a consequence value. First, if we consider the regulator xTLx as a scalar function of eigenvectors vi, we define the gradient gi differentially - the change of the gradient vi - for each eigenvector vi after adding the perturbation matrix to L: gi =."}, {"heading": "C. A Simple Lower Bound for \u03bbmin", "text": "We can calculate a lower limit for \u03bbmin simply as follows: Identified by L + and L \u2212 the graph Laplacian matrices that correspond to edges with positive or negative weights in graph G; clearly L = L + + L \u2212. The Rayleigh quotient for L can be extended as follows: xTLxTx = xT (L + + L \u2212) xxTx (23) Since L + contains only positive edges, PSD, the first term in the numerator xTL + x, is lower limited by 0. For the second term, we can first define and write L \u2212 = \u2212 L \u2212, which is PSD: xTL \u2212 xTx = \u2212 (xTL \u2212 xTx) \u2265 \u2212 max (24), where \u03bb \u2212 max is the largest eigenvalue of L \u2212. Since \u2212 \u03bb \u2212 max is also the lower limit of the Rayleigh quotient for L, it is also the lower limit for the smallest eigenvalue of xi \u2212 xi \u2212 max \u2212."}, {"heading": "V. FAST COMPUTATION", "text": "The complexity of self-decomposition for calculating the smallest eigenvalue \u03bbmin of the graph Laplacian L is O (N3) [41]. For a large graph with a large number of nodes, this can be expensive. We therefore propose a method to calculate a fast lower limit \u03bb # min for \u03bbmin, so that if the error matrix \u0445 # = \u2212 \u03bb # minI is added to L, the resulting L + \u2206 # PSD is."}, {"heading": "A. Matrix Inertia", "text": "The inertia In (A) of a matrix A is a set of three numbers counting the positive, negative, and zero eigenvalues in A: In (A) = (i + (A), i \u2212 (A), i0 (A))) (26), where i + (A), i \u2212 (A), and i0 (A) each denote the number of positive, negative, and zero eigenvalues in matrix A. Inertia is an intrinsic property of the matrix; according to Sylvester's Inertia6, the intertia of a matrix is invariant for each congruent transformation, i.e. In (A) = In (PTAP) (27), where P is an inverted matrix."}, {"heading": "B. Graph Partition", "text": "To reduce complexity, we can divide the node set N into two subsets N1 and N2, so that an intensive calculation is performed separately in the node subsets. Note that the division of a graph into two node sets to reduce complexity also occurs in the cron reduction [42]. However, [42] only the PSD L (possibly with self-loops) is taken into account, while we consider the indefinite L, which requires a disturbance to produce L + \u2206 PSD. Given the two sets N1 and N2, we can write the graph Laplacian L in blocks: L = [L1,1 L1,2 LT1,2 L2,2] (28), where L1,1 and L2,2 are submatrices of the respective dimension | N1 | N2 | N2 | N2 | N2 | according to the node sets N1 and N2, and L1,2 is a matrix of the respective dimension."}, {"heading": "C. Eigenvalue Lower Bound Algorithm", "text": "We propose the following recursive algorithm to find a lower limit \u03bb # min for L. See fig. 4 for an illustration. We initialize t: = 0 and L0: = L. We define a recursive algorithm EvalBound (Lt, t) that returns a lower limit \u03bbtmin for eigenvalues in Lt. It has two steps as described below. Step 1: We first select a node that is set N t in Lt, and then perform the search for width first (BFS) [43] until r nodes are detected. We decompose Lt1,1 to find its smallest eigenvalue Lt1,1."}, {"heading": "D. Proof of Algorithm Correctness", "text": "We now prove that EVENTBound (L, 0) is a lower limit for the true minimal eigenvalue of Lt.6https: / / en.wikipedia.org / wiki / Sylvester% 27s law of inertia 7https: / / en.wikipedia.org / wiki / Schur complementWe first examine the basic case. On a recursive call in step 1, L\u03c4 / Sylvester% perductmin is inhibited so that L\u03c41,1 = L.wikipedia.org / wikipedia.org / Schur complementWe first know the basic case. On a recursive call in step 1, L\u03c4 / Sylvester% perductmin is inhibited so that L\u03c41,1 = L.wikipediaWe know the basic case. On a recursive call in step 1, L\u041a1,1 and Sylvester% 27s law of inertia in step 1, we then know that L\u03c41,1 = Lwikipedia.org / Schur complementWe first know the basic case. On a recursive call in step 1, L\u041a1,1 and Sylvester% 27s law of inertia in step 1, L\u03c4 is perductmin."}, {"heading": "E. Computation Complexity", "text": "Compared to the complexity O (N3) of the self-decomposition of the larger matrix L, this represents a non-trivial saving."}, {"heading": "VI. CLASSIFIER LEARNING WITH NOISY LABELS", "text": "After discussing a quick method of calculating that L + \u2206 PSD is, we now specifically discuss the scenario of noisy label learning, where observed binary labels y are not damaged by Gaussian noise (6), but by a uniform noise typical only of binary classifiers. We first motivate our chosen label noise model and describe the negative log probability in Section VI-A. Beyond the smoothness of the graph signal previously defined in Section III-C, we can additionally define a general smoothness in Section VI-B that promotes ambiguity of the classification signal. We offer a new interpretation of the general smoothness in the graph by considering a graph signal as voltages on an electrical circuit in Section VI-C. After formulating the problem with both previous terms in Section VI-D, we finally propose an algorithm to solve it in Section VI-E."}, {"heading": "A. Label Noise Model", "text": "To model binary label noise, we use a uniform noise model [26], in which the probability of observing yi = xi, 8 1 \u2264 i \u2264 K is 1 \u2212 p and otherwise p; i.e., Pr (yi | xi) = {1 \u2212 p if yi = xi p o.w. (33) This noise model is motivated by the following observation in social media analysis, when labels are often allocated manually by non-experts via crowd sourcing [26] - i.e. employ many non-experts online to assign labels to a subset of data at very low cost. Since non-experts can be unreliable (e.g. a non-expert is not competent in a label assignment task, but pretends to be, or simply assigns labels randomly to minimize mental effort), observations y can lead to label errors or noise that are uniform and independent. \u2212 The probability of a label noise folder (this probability is 34 = \u2212 p) is generic (p = 33)."}, {"heading": "B. Generalized Smoothness", "text": "After deriving a new term of probability in (34), we next describe a generalized version of the smoothness of the graph signal before (3) for the reconstruction of the classifier signal. (1) Positive edge weights for generalized smoothness: Like TGV for images [22], one can also define a term of higher order of smoothness for graph signals using positive edge weights [8]. Specifically, the positive edge diagram Laplacian L + refers to the second derivative of continuous functions [7], and thus L + x calculates the second order difference for the graph signal x. As an example, the 3-node line diagram in Fig. 2 with all edge weights equal to 1 can compile the second smatif difference at the node L +: L + = 1 0 \u2212 1 0 \u2212 1 1 1 1 (36) Using the second row L + 2: of L + 2,: of L + 1, we can compile the second smatif difference at the node 2: xL + L."}, {"heading": "C. Circuit Interpretation of Generalized Smoothness", "text": "It has been shown that additional insights can be gained by interpreting an undirected, weighted graph G as an electrical circuit (34], [35], [42], [44]. We follow a similar approach when trying to understand a generalized smoothness. Suppose we interpret an edge (i, j) as a wire between the nodes i and j and an edge weight wi, j as conductivity (equivalent, 1 / wi, j as resistance) between their two endpoints. Suppose xi and xj represent the voltage at the two endpoints. According to Ohms law9, the current ci, j between the two nodes is the voltage difference at the endpoints x conductive: ci, j = wi, j (xi \u2212 xj) the voltage at the two endpoints. (41) According to Kirchhoff's current law10 (KCL), the net sum of the currents flowing into a node is zero."}, {"heading": "D. Objective Function", "text": "We can combine the probability (34) and the two previous terms to avoid an optimisation intensity defined as follows: min x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x"}, {"heading": "VII. EXPERIMENTATION", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. Experiment Setup", "text": "The first dataset, called \"Sonar, Mines and Rocks,\" contains various patterns that were selected by bouncing suns and rocks at different angles (covering 90 degrees for mines and 180 degrees for rocks) and under different conditions. Each of these patterns is a set of 60 numbers in the range 0.0 to 1.0, in which each number represents the energy within a certain period of time. These patterns are used to classify whether an object is a metal cylinder or a rock. The second dataset is the dataset of the Banana Dataset, an artificial dataset in which 5300 distances belong to several clusters with a banana shape."}, {"heading": "B. Performance Evaluation", "text": "To test the robustness of different classification schemes against label errors, we randomly selected a portion of the samples from the training kit and reversed their labels. Subsequently, all classifiers were trained on the same characteristics and labels. Each test set was classified by the classifiers and the results compared with the ground truth labels."}, {"heading": "1) Numerical comparisons for different label noise:", "text": "The resulting classification error rates for the three datasets based on the minimum standard scheme are presented in Tables I-III, where the percentage of randomly generated training labels ranges from 0% to 25%. Comparisons show that our proposed scheme achieves a lower classification error when compared to five competing schemes on almost all training labels error rates. Parameters used for the three datasets (3, 3, 4, 4, 6, 8, 0.055]), (0.001, 1.2, 10, [0.0303, 0.1019]) are each: (0.001, 3, 0.1019). Compared with the graph classifier with all positive edge weights, our results show that adding negative edge weights can effectively improve classification accuracy by 1-3%."}, {"heading": "VIII. CONCLUSION", "text": "To address the semi-supervised learning problem, in this paper we consider a classifier as a graph signal in a high-dimensional attribute space and present a maximum a-posteriori (MAP) problem to restore the classifier signal containing partial and noisy designations. In contrast to previous graph-based classifier work, we additionally consider edges with negative weights, which imply an anti-correlation between the sample pairs. To achieve a stable signal smoothness, we derive an optimal disturbance matrix so that the matrix sum is positively semi-defined (PSD). We can calculate a 12-speed approximation to a recursive algorithm based on the Haynsworth Inertia additive formula. Finally, we show that generalized smoothness prior to TSI can promote ambiguity in the classifier signal, so that estimated designations can be rejected with low confidence."}], "references": [{"title": "Pattern Recognition and Machine Learning", "author": ["C. Bishop"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2006}, {"title": "Label selection on graphs", "author": ["A. Guillory", "J. Bilmes"], "venue": "Twenty- Third Annual Conference on Neural Information Processing Systems, Vancouver, Canada, December 2009.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2009}, {"title": "Active learning based on locally linear reconstruction", "author": ["L. Zhang", "C. Cheng", "J. Bu", "D. Cai", "X. He", "T. Huang"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 33, no.10, October 2014, pp. 2026\u20132038.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2014}, {"title": "Signal recovery on graphs: Variation minimization", "author": ["S. Chen", "A. Sandryhaila", "J. Moura", "J. Kovacevic"], "venue": "IEEE Transactions on Signal Processing, vol. 63, no.17, September 2015, pp. 4609\u20134624.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2015}, {"title": "Active semi-supervised learning using sampling theory for graph signals", "author": ["A. Gadde", "A. Anis", "A. Ortega"], "venue": "ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, New York, NY, August 2014.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2014}, {"title": "Spectral Graph Theory", "author": ["F. Chung"], "venue": "American Mathematical Society,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1996}, {"title": "The emerging field of signal processing on graphs: Extending high-dimensional data analysis to networks and other irregular domains", "author": ["D.I. Shuman", "S.K. Narang", "P. Frossard", "A. Ortega", "P. Vandergheynst"], "venue": "IEEE Signal Processing Magazine, vol. 30, no.3, May 2013, pp. 83\u201398.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2013}, {"title": "Image classifier learning from noisy labels via generalized graph smoothness priors", "author": ["Y. Mao", "G. Cheung", "C.-W. Lin", "Y. Ji"], "venue": "IEEE IVMSP Workshop, Bordeaux, France, July 2016.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2016}, {"title": "Depth map denoising using graphbased transform and group sparsity", "author": ["W. Hu", "X. Li", "G. Cheung", "O. Au"], "venue": "IEEE International Workshop on Multimedia Signal Processing, Pula, Italy, October 2013.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2013}, {"title": "Redefining self-similarity in natural images for denoising using graph signal gradient", "author": ["J. Pang", "G. Cheung", "W. Hu", "O.C. Au"], "venue": "APSIPA ASC, Siem Reap, Cambodia, December 2014.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2014}, {"title": "Optimal graph Laplacian regularization for natural image denoising", "author": ["J. Pang", "G. Cheung", "A. Ortega", "O.C. Au"], "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing, Brisbane, Australia, April 2015.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "Localized iterative methods for interpolation in graph structured data", "author": ["S.K. Narang", "A. Gadde", "E. Sanou", "A. Ortega"], "venue": "Symposium on Graph Signal Processing in IEEE Global Conference on Signal and Information Processing (GlobalSIP), Austin, TX, December 2013.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2013}, {"title": "Signal processing techniques for interpolation of graph structured data", "author": ["S.K. Narang", "A. Gadde", "A. Ortega"], "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing, Vancouver, Canada, May 2013.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2013}, {"title": "Expansion hole filling in depth-image-based rendering using graph-based interpolation", "author": ["Y. Mao", "G. Cheung", "A. Ortega", "Y. Ji"], "venue": "IEEE International Conference on Acousitics, Speech and Signal Processing, Vancouver, Canada, May 2013.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2013}, {"title": "Image interpolation during DIBR view synthesis using graph Fourier transform", "author": ["Y. Mao", "G. Cheung", "Y. Ji"], "venue": "3DTV-Conference, Budapest, Hungary, July 2014.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2014}, {"title": "On constructing z-dimensional DIBR-synthesized images", "author": ["\u2014\u2014"], "venue": "IEEE Transactions on Multimedia, vol. 18, no.8, August 2016, pp. 1453\u2013 1468.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2016}, {"title": "Image bitdepth enhancement via maximum-a-posteriori estimation of graph AC component", "author": ["P. Wan", "G. Cheung", "D. Florencio", "C. Zhang", "O. Au"], "venue": "IEEE International Conference on Image Processing, Paris, France, October 2014.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2014}, {"title": "Image bit-depth enhancement via maximum-a-posteriori estimation of AC signal", "author": ["\u2014\u2014"], "venue": "IEEE Transactions on Image Processing, vol. 25, no.6, June 2016, pp. 2896\u20132909.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2016}, {"title": "Inter-block soft decoding of JPEG images with sparsity and graph-signal smoothness priors", "author": ["X. Liu", "G. Cheung", "X. Wu", "D. Zhao"], "venue": "IEEE International Conference on Image Processing, Quebec City, Canada, September 2015.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2015}, {"title": "Graph-based dequantization of block-compressed piecewise smooth images", "author": ["W. Hu", "G. Cheung", "M. Kazui"], "venue": "IEEE Signal Processing Letters, vol. 23, no.2, February 2016, pp. 242\u2013246.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2016}, {"title": "On the inertia of some classes of partitioned matrices", "author": ["E.V. Haynsworth", "A.M. Ostrowski"], "venue": "Linear Algebra and its Applications, vol. 1, no.2, 1968, pp. 299\u2013316.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1968}, {"title": "A TGV-based framework for variational image decompression, zooming and reconstruction. Part I: Analytics", "author": ["K. Bredies", "M. Holler"], "venue": "SIAM Jour, vol. 8, no.4, 2015, pp. 2814\u20132850.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2015}, {"title": "Iteratively re-weighted least squares minimization for sparse recovery", "author": ["I. Daubechies", "R. Devore", "M. Fornasier", "S. Gunturk"], "venue": "Communications on Pure and Applied Mathematics, vol. 63, no.1, January 2010, pp. 1\u201338.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2010}, {"title": "A more robust boosting algorithm", "author": ["Y. Freund"], "venue": "May 2009, https://arxiv.org/abs/0905.2138.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2009}, {"title": "The interaction between supervised learning and crowdsourcing", "author": ["A. Brew", "D. Greene", "P. Cunningham"], "venue": "Computational Social Science and the Wisdom of Crowds Workshop at NIPS, Whistler, Canada, December 2010.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2010}, {"title": "Editorial: Special issue on advances in learning with label noise", "author": ["B. Frenay", "A. Kaban"], "venue": "Elsevier: Neurocomputing, vol. 160, July 2015, pp. 1\u20132.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2015}, {"title": "Twitter polarity classification with label propagation over lexical links and the follower graph", "author": ["M. Speriosu", "N. Sudan", "S. Upadhyay", "J. Baldridge"], "venue": "Conference on Empirical Methods in Natural Language Processing, Edinburgh, Scotland, July 2011.  13", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2011}, {"title": "Detecting emotions in social media: A constrained optimization approach", "author": ["Y. Wang", "A. Pal"], "venue": "Twenty-Fourh International Joint Conference on Artificial Intelligence, Buenos Aires, Argentina, July 2015.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2015}, {"title": "Discrete signal processing on graphs", "author": ["A. Sandryhaila", "J. Moura"], "venue": "IEEE Transactions on Signal Processing, vol. 61, no.7, August 2013, pp. 1644\u20131656.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2013}, {"title": "Big data analysis with signal processing on graphs: Representation and processing of massive data sets with irregular structure", "author": ["\u2014\u2014"], "venue": "IEEE Signal Processing Magazine, vol. 31, no.5, August 2014, pp. 80\u2013 90.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2014}, {"title": "Signal denoising on graphs via graph filtering", "author": ["S. Chen", "A. Sandryhaila", "J.M.F. Moura", "J. Kovacevic"], "venue": "IEEE Global Conference on Signal and Information Processing, Austin, TX, December 2014.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2014}, {"title": "Nonlinear total variation based noise removal algorithms", "author": ["L. Rudin", "S. Osher", "E. Fatemi"], "venue": "Physica D, vol. 60, no.1-4, November 1992, pp. 259\u2013268.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 1992}, {"title": "On the definiteness of the weighted laplacian and its connection to effective resistance", "author": ["D. Zelazo", "M. Burger"], "venue": "53rd IEEE Conference on Decision and Control, Los Angeles, CA, December 2014.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2014}, {"title": "On the definiteness of graph laplacians with negative weights: Geometrical and passivity-based approaches", "author": ["Y. Cheng", "S.Z. Khong", "T.T. Georgiou"], "venue": "2016 American Control Conference, Boston, MA, July 2016.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2016}, {"title": "Finding gangs in war from signed networks", "author": ["L. Chu"], "venue": "22nd ACM SIGKDD Conference on Knowledge Discovery and Data Mining, San Francisco, CA, August 2016.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2016}, {"title": "Nodal domain theorems and bipartite subgraphs", "author": ["T. Biyikoglu", "J. Leydold", "P.F. Stadler"], "venue": "Electronic Journal of Linear Algebra, vol. 13, November 2005, pp. 344\u2013351.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2005}, {"title": "Normalized cuts and image segmentation", "author": ["J. Shi", "J. Malik"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 22, no.8, August 2000, pp. 888\u2013905.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2000}, {"title": "Automated variable weighting in k-means type clustering", "author": ["J.Z. Huang", "M.K. Ng", "H. Rong", "Z. Li"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 27, no.5, May 2005, pp. 657\u2013 668.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2005}, {"title": "Modifying the inertia of matrices arising in optimization", "author": ["N. Higham", "S.H. Cheng"], "venue": "ELSEVIER Linear Algebra and its Applications, vol. 275-279, May 1998, pp. 261\u2013279.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 1998}, {"title": "Numerical Linear Algebra", "author": ["L. Trefethen", "D. Bau"], "venue": "SIAM: Society for Industrial and Applied Mathematics,", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 1997}, {"title": "Kron reduction of graphs with applications to electrical networks", "author": ["F. D\u00f6rfler", "F. Bullo"], "venue": "IEEE Transactions on Circuits and Systems I: Regular Papers, vol. 60, no.1, January 2013, pp. 150\u2013163.", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2013}, {"title": "Introduction to Algorithms", "author": ["Cormen", "Leiserson", "Rivest"], "venue": null, "citeRegEx": "43", "shortCiteRegEx": "43", "year": 1986}, {"title": "Resistance distance", "author": ["D. Klein", "M. Randic"], "venue": "Journal of Mathematical Chemistry, vol. 12, no.1, December 1993, pp. 81\u201395.", "citeRegEx": "44", "shortCiteRegEx": null, "year": 1993}, {"title": "Graph Laplacian regularization for inverse imaging: Analysis in the continuous domain", "author": ["J. Pang", "G. Cheung"], "venue": "April 2016, https://arxiv.org/abs/1604.07948.", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2016}, {"title": "Keel: A software tool to assess evolutionary algorithms to data mining problems", "author": ["J.A.-F"], "venue": "Soft Computing, vol. 13, no.3, February 2009, pp. 307\u2013318.", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2009}, {"title": "On optimum recognition error and reject tradeoff", "author": ["C. Chow"], "venue": "IEEE Transactions on Information Theory, vol. 16, no.1, September 1970, pp. 41\u201346.", "citeRegEx": "47", "shortCiteRegEx": null, "year": 1970}], "referenceMentions": [{"referenceID": 0, "context": "A fundamental problem in machine learning is semisupervised learning [1]: given partially observed labels (possibly corrupted by noise) as input, train a classifier so that unclassified samples can also be appropriately assigned labels.", "startOffset": 69, "endOffset": 72}, {"referenceID": 1, "context": "Among many approaches to the problem is a class of graphbased methods [2]\u2013[5] that model each sample as a node in a graph, connected to other nodes via undirected edges, with weights that reflect pairwise distances in a high-dimensional feature space.", "startOffset": 70, "endOffset": 73}, {"referenceID": 4, "context": "Among many approaches to the problem is a class of graphbased methods [2]\u2013[5] that model each sample as a node in a graph, connected to other nodes via undirected edges, with weights that reflect pairwise distances in a high-dimensional feature space.", "startOffset": 74, "endOffset": 77}, {"referenceID": 5, "context": ", low graph frequencies that are eigenvectors of the graph Laplacian matrix) can be exploited for label assignment via spectral graph theory [6].", "startOffset": 141, "endOffset": 144}, {"referenceID": 6, "context": "Conventional formulations in graph signal processing (GSP) [7] use non-negative edge weights that reflect inter-node correlation; an edge weight wi,j = 0 means samples xi and xj are conditionally independent.", "startOffset": 59, "endOffset": 62}, {"referenceID": 7, "context": "To study the implications of negative edge weights, we view a binary classifier as a piecewise constant (PWC) graph-signal and cast classifier learning as a signal restoration problem via a classical maximum a posteriori (MAP) formulation [8].", "startOffset": 239, "endOffset": 242}, {"referenceID": 8, "context": "We show first that a graph Laplacian matrix L with negative edge weights can be indefinite, and a common graph-signal prior called graph Laplacian regularizer [9]\u2013[20] xLx for candidate signal x\u2014measuring signal smoothness with respect to the underlying graph\u2014can lead to pathological solutions.", "startOffset": 159, "endOffset": 162}, {"referenceID": 19, "context": "We show first that a graph Laplacian matrix L with negative edge weights can be indefinite, and a common graph-signal prior called graph Laplacian regularizer [9]\u2013[20] xLx for candidate signal x\u2014measuring signal smoothness with respect to the underlying graph\u2014can lead to pathological solutions.", "startOffset": 163, "endOffset": 167}, {"referenceID": 20, "context": "To efficiently compute an approximate \u2206, we propose a fast recursive algorithm that identifies a lower bound for the smallest eigenvalue of L via a novel application of the Haynsworth Inertia Additivity formula [21].", "startOffset": 211, "endOffset": 215}, {"referenceID": 21, "context": "Second, instead of forcing a hard binary decision for each sample, we define the notion of generalized smoothness on graph\u2014an extension of total generalized variation (TGV) [22] to the graph-signal domain\u2014that promotes the right amount of ambiguity in the classifier signal.", "startOffset": 173, "endOffset": 177}, {"referenceID": 22, "context": "Finally, we propose an algorithm based on iterative reweighted least squares (IRLS) [24] that efficiently solves the posed MAP problem for the noisy label scenario.", "startOffset": 84, "endOffset": 88}, {"referenceID": 23, "context": "Extensive simulation results show that our proposed algorithm outperforms SVM variants, a well-known robust classifier in the machine learning literature called RobustBoost [25], and graph-based classifiers using positive-edge graphs noticeably for both noiseless and noisy label scenarios.", "startOffset": 173, "endOffset": 177}, {"referenceID": 24, "context": "Classifier learning with label noise has garnered much interest, including a workshop1 in NIPS\u201910 [26] and a journal special issue in Neurocomputing [27].", "startOffset": 98, "endOffset": 102}, {"referenceID": 25, "context": "Classifier learning with label noise has garnered much interest, including a workshop1 in NIPS\u201910 [26] and a journal special issue in Neurocomputing [27].", "startOffset": 149, "endOffset": 153}, {"referenceID": 26, "context": ", label propagation in [28]) and application-specific (e.", "startOffset": 23, "endOffset": 27}, {"referenceID": 27, "context": ", emotion detection using inference algorithm based on multiplicative update rule [29]).", "startOffset": 82, "endOffset": 86}, {"referenceID": 1, "context": "In this paper, similar to previous works [2]\u2013[5] we choose to build a graph-based classifier, where each acquired sample is represented as a node in a highdimensional feature space and connects to other sample nodes in its neighborhood.", "startOffset": 41, "endOffset": 44}, {"referenceID": 4, "context": "In this paper, similar to previous works [2]\u2013[5] we choose to build a graph-based classifier, where each acquired sample is represented as a node in a highdimensional feature space and connects to other sample nodes in its neighborhood.", "startOffset": 45, "endOffset": 48}, {"referenceID": 21, "context": "Second, we show how generalized graph smoothness notion\u2014extending TGV [22] to the graph-signal domain\u2014can be interpreted intuitively as Kirchoff\u2019s current law and used to promote ambiguity in the classifier solution.", "startOffset": 70, "endOffset": 74}, {"referenceID": 8, "context": "Graph-signal priors have been used for image restoration problems such as denoising [9]\u2013[11], interpolation [12]\u2013[16], bit-depth enhancement [17], [18] and JPEG de-quantization [19], [20].", "startOffset": 84, "endOffset": 87}, {"referenceID": 10, "context": "Graph-signal priors have been used for image restoration problems such as denoising [9]\u2013[11], interpolation [12]\u2013[16], bit-depth enhancement [17], [18] and JPEG de-quantization [19], [20].", "startOffset": 88, "endOffset": 92}, {"referenceID": 11, "context": "Graph-signal priors have been used for image restoration problems such as denoising [9]\u2013[11], interpolation [12]\u2013[16], bit-depth enhancement [17], [18] and JPEG de-quantization [19], [20].", "startOffset": 108, "endOffset": 112}, {"referenceID": 15, "context": "Graph-signal priors have been used for image restoration problems such as denoising [9]\u2013[11], interpolation [12]\u2013[16], bit-depth enhancement [17], [18] and JPEG de-quantization [19], [20].", "startOffset": 113, "endOffset": 117}, {"referenceID": 16, "context": "Graph-signal priors have been used for image restoration problems such as denoising [9]\u2013[11], interpolation [12]\u2013[16], bit-depth enhancement [17], [18] and JPEG de-quantization [19], [20].", "startOffset": 141, "endOffset": 145}, {"referenceID": 17, "context": "Graph-signal priors have been used for image restoration problems such as denoising [9]\u2013[11], interpolation [12]\u2013[16], bit-depth enhancement [17], [18] and JPEG de-quantization [19], [20].", "startOffset": 147, "endOffset": 151}, {"referenceID": 18, "context": "Graph-signal priors have been used for image restoration problems such as denoising [9]\u2013[11], interpolation [12]\u2013[16], bit-depth enhancement [17], [18] and JPEG de-quantization [19], [20].", "startOffset": 177, "endOffset": 181}, {"referenceID": 19, "context": "Graph-signal priors have been used for image restoration problems such as denoising [9]\u2013[11], interpolation [12]\u2013[16], bit-depth enhancement [17], [18] and JPEG de-quantization [19], [20].", "startOffset": 183, "endOffset": 187}, {"referenceID": 3, "context": "One alternative line of previous works founded their GSP analysis on algebraic theory in traditional digital signal processing that relies on the shift operator [4], [30]\u2013[32].", "startOffset": 161, "endOffset": 164}, {"referenceID": 28, "context": "One alternative line of previous works founded their GSP analysis on algebraic theory in traditional digital signal processing that relies on the shift operator [4], [30]\u2013[32].", "startOffset": 166, "endOffset": 170}, {"referenceID": 30, "context": "One alternative line of previous works founded their GSP analysis on algebraic theory in traditional digital signal processing that relies on the shift operator [4], [30]\u2013[32].", "startOffset": 171, "endOffset": 175}, {"referenceID": 3, "context": "As an example, for graph-signal restoration, a smoothness prior \u2016x\u2212Wx\u2016pp for some positive integer p was proposed in [4].", "startOffset": 117, "endOffset": 120}, {"referenceID": 31, "context": "Suppose a total variation (TV) approach [33] is taken instead, so that a smoothness prior using L but based on l1norm is used instead: i.", "startOffset": 40, "endOffset": 44}, {"referenceID": 32, "context": "Recent studies in the control community have examined the conditions where one or more negative edge weights would induce a graph Laplacian to be indefinite [34], [35].", "startOffset": 157, "endOffset": 161}, {"referenceID": 33, "context": "Recent studies in the control community have examined the conditions where one or more negative edge weights would induce a graph Laplacian to be indefinite [34], [35].", "startOffset": 163, "endOffset": 167}, {"referenceID": 34, "context": "[36] considered a signed social network where each edge denotes either a cohesive (positive edge weight) or oppositive (negative edge weight) relationship between two vertices.", "startOffset": 0, "endOffset": 4}, {"referenceID": 6, "context": "A combinatorial graph Laplacian matrix L is then simply L = D\u2212W [7].", "startOffset": 64, "endOffset": 67}, {"referenceID": 35, "context": "Note that v1 = 1 has no zero-crossings, and higher frequency components vk have increasingly more zero-crossings according to the nodal domain theorem [37].", "startOffset": 151, "endOffset": 155}, {"referenceID": 9, "context": "Mathematically, we can write that a signal x is smooth if its graph Laplacian regularizer xLx is small [10], [11].", "startOffset": 103, "endOffset": 107}, {"referenceID": 10, "context": "Mathematically, we can write that a signal x is smooth if its graph Laplacian regularizer xLx is small [10], [11].", "startOffset": 109, "endOffset": 113}, {"referenceID": 9, "context": "Because of its simplicity in formulation and available closedform solution, this was the approach taken in previous graphsignal restoration work [10], [11], [19], [20].", "startOffset": 145, "endOffset": 149}, {"referenceID": 10, "context": "Because of its simplicity in formulation and available closedform solution, this was the approach taken in previous graphsignal restoration work [10], [11], [19], [20].", "startOffset": 151, "endOffset": 155}, {"referenceID": 18, "context": "Because of its simplicity in formulation and available closedform solution, this was the approach taken in previous graphsignal restoration work [10], [11], [19], [20].", "startOffset": 157, "endOffset": 161}, {"referenceID": 19, "context": "Because of its simplicity in formulation and available closedform solution, this was the approach taken in previous graphsignal restoration work [10], [11], [19], [20].", "startOffset": 163, "endOffset": 167}, {"referenceID": 36, "context": "This weight assignment is similar to those in previous works on spectral clustering [38] and graph-based classifier learning [5], [8], where a closer distance in the D-dimensional feature space leads to a larger edge weight.", "startOffset": 84, "endOffset": 88}, {"referenceID": 4, "context": "This weight assignment is similar to those in previous works on spectral clustering [38] and graph-based classifier learning [5], [8], where a closer distance in the D-dimensional feature space leads to a larger edge weight.", "startOffset": 125, "endOffset": 128}, {"referenceID": 7, "context": "This weight assignment is similar to those in previous works on spectral clustering [38] and graph-based classifier learning [5], [8], where a closer distance in the D-dimensional feature space leads to a larger edge weight.", "startOffset": 130, "endOffset": 133}, {"referenceID": 37, "context": "For more detailed optimization of feature parameters \u039ei,i\u2014which is not the focus of this paper\u2014see [39].", "startOffset": 99, "endOffset": 103}, {"referenceID": 38, "context": "1 in [40], which we rephrase as follows.", "startOffset": 5, "endOffset": 9}, {"referenceID": 38, "context": "1 in [40] states that the optimal perturbation matrix \u2206 with minimum norm \u2016\u2206\u2016, such that L + \u2206 is PSD, is:", "startOffset": 5, "endOffset": 9}, {"referenceID": 38, "context": "See [40] for a complete proof.", "startOffset": 4, "endOffset": 8}, {"referenceID": 35, "context": "To construct a solution for this case, we define a generalized graph Laplacian matrix Lg [37] as the sum of L and an identity matrix5 I scaled by \u2212\u03bbmin:", "startOffset": 89, "endOffset": 93}, {"referenceID": 39, "context": "However, the computation of \u03bbmax is O(N ) in the worst case [41], and this lower bound is often loose in practice.", "startOffset": 60, "endOffset": 64}, {"referenceID": 39, "context": "The complexity of eigen-decomposition to compute the smallest eigenvalue \u03bbmin of graph Laplacian L is O(N) [41].", "startOffset": 107, "endOffset": 111}, {"referenceID": 40, "context": "Note that partitioning a graph into two node sets to reduce complexity is also done in Kron reduction [42].", "startOffset": 102, "endOffset": 106}, {"referenceID": 40, "context": "However, [42] considers only PSD L (possibly with self-loops), while we consider indefinite L that requires perturbation \u2206 to make L + \u2206 PSD.", "startOffset": 9, "endOffset": 13}, {"referenceID": 20, "context": "We can now relate the inertia of L with its sub-matrices using the Haynsworth Inertia Additivity formula [21]:", "startOffset": 105, "endOffset": 109}, {"referenceID": 41, "context": "N t 1 can be chosen by first randomly selecting a node in N , then perform breadth-first search (BFS) [43] until r nodes are discovered.", "startOffset": 102, "endOffset": 106}, {"referenceID": 24, "context": "To model binary label noise, we adopt a uniform noise model [26], where the probability of observing yi = xi,", "startOffset": 60, "endOffset": 64}, {"referenceID": 24, "context": "This noise model is motivated by the following observation in social media analysis, when labels are often assigned manually by non-experts via crowd-sourcing [26]\u2014i.", "startOffset": 159, "endOffset": 163}, {"referenceID": 21, "context": "1) Positive Edge Weights for Generalized Smoothness: Like TGV for images [22], one can also define a higher-order notion of smoothness for graph-signals using positive edge weights [8].", "startOffset": 73, "endOffset": 77}, {"referenceID": 7, "context": "1) Positive Edge Weights for Generalized Smoothness: Like TGV for images [22], one can also define a higher-order notion of smoothness for graph-signals using positive edge weights [8].", "startOffset": 181, "endOffset": 184}, {"referenceID": 6, "context": "Specifically, positive edge graph Laplacian L is related to the second derivative of continuous functions [7], and so Lx computes the second-order difference on graph-signal x.", "startOffset": 106, "endOffset": 109}, {"referenceID": 32, "context": "It has been shown that by interpreting an undirected weighted graph G as an electrical circuit, one can gain additional insights [34], [35], [42], [44].", "startOffset": 129, "endOffset": 133}, {"referenceID": 33, "context": "It has been shown that by interpreting an undirected weighted graph G as an electrical circuit, one can gain additional insights [34], [35], [42], [44].", "startOffset": 135, "endOffset": 139}, {"referenceID": 40, "context": "It has been shown that by interpreting an undirected weighted graph G as an electrical circuit, one can gain additional insights [34], [35], [42], [44].", "startOffset": 141, "endOffset": 145}, {"referenceID": 42, "context": "It has been shown that by interpreting an undirected weighted graph G as an electrical circuit, one can gain additional insights [34], [35], [42], [44].", "startOffset": 147, "endOffset": 151}, {"referenceID": 40, "context": "As done in [42], a generalized graph Laplacian Lg with diagonal element Li,i \u2265 \u2211 j|j 6=i Li,j can be considered a conductance matrix, where an edge (i, j) has branch conductance \u2212Li,j and node i has shunt conductance Li,i\u2212 \u2211 j|j 6=i Li,j \u2265 0.", "startOffset": 11, "endOffset": 15}, {"referenceID": 31, "context": "The graph-signal smoothness term in (5), analogous to the total variation (TV) prior [33] in image restoration [45], promotes a PWC signal x\u0302 during reconstruction, as empirically demonstrated in previous graph-signal restoration works [10], [11], [19], [20].", "startOffset": 85, "endOffset": 89}, {"referenceID": 43, "context": "The graph-signal smoothness term in (5), analogous to the total variation (TV) prior [33] in image restoration [45], promotes a PWC signal x\u0302 during reconstruction, as empirically demonstrated in previous graph-signal restoration works [10], [11], [19], [20].", "startOffset": 111, "endOffset": 115}, {"referenceID": 9, "context": "The graph-signal smoothness term in (5), analogous to the total variation (TV) prior [33] in image restoration [45], promotes a PWC signal x\u0302 during reconstruction, as empirically demonstrated in previous graph-signal restoration works [10], [11], [19], [20].", "startOffset": 236, "endOffset": 240}, {"referenceID": 10, "context": "The graph-signal smoothness term in (5), analogous to the total variation (TV) prior [33] in image restoration [45], promotes a PWC signal x\u0302 during reconstruction, as empirically demonstrated in previous graph-signal restoration works [10], [11], [19], [20].", "startOffset": 242, "endOffset": 246}, {"referenceID": 18, "context": "The graph-signal smoothness term in (5), analogous to the total variation (TV) prior [33] in image restoration [45], promotes a PWC signal x\u0302 during reconstruction, as empirically demonstrated in previous graph-signal restoration works [10], [11], [19], [20].", "startOffset": 248, "endOffset": 252}, {"referenceID": 19, "context": "The graph-signal smoothness term in (5), analogous to the total variation (TV) prior [33] in image restoration [45], promotes a PWC signal x\u0302 during reconstruction, as empirically demonstrated in previous graph-signal restoration works [10], [11], [19], [20].", "startOffset": 254, "endOffset": 258}, {"referenceID": 21, "context": "Recall that the purpose of TGV [22] is to avoid oversmoothing a ramp (linear increase / decrease in pixel intensity) in an image, which would happen if only a TV prior is used.", "startOffset": 31, "endOffset": 35}, {"referenceID": 22, "context": "To accomplish this, we employ the iterative reweighted least squares (IRLS) strategy [24], which has been proven to have superlinear local convergence, and solve (45) iteratively, where the weights b i of iteration t+ 1 is computed using solution x i of the previous iteration t, i.", "startOffset": 85, "endOffset": 89}, {"referenceID": 44, "context": "1) Datasets for Training and Testing: To evaluate the performances of different classification methods, we selected three two-class datasets from the KEEL (Knowledge Extraction based on Evolutionary Learning) database [46], which contains a rich collection of labelled and unlabeled datasets for data mining and analysis.", "startOffset": 218, "endOffset": 222}, {"referenceID": 0, "context": "For each sample (node), we found its three nearest neighbors according to the Euclidean distances between the node and its neighbors, and connected these nodes using edges with non-negative weights that are normalized to [0,1] using the Gaussian kernel in (10).", "startOffset": 221, "endOffset": 226}, {"referenceID": 23, "context": "3) Comparison Schemes: We tested our proposed algorithm against five schemes: i) linear SVM, ii) SVM with a RBF kernel (named SVM-RBF), iii) a more robust version of the famed AdaBoost called RobustBoost [25] that claims robustness against label noise, iv) a graph classifier with the graph-signal smoothness prior (3) where the edge weights of the graph are all positive (named Graph-Pos), and v) a graph classifier with a graph containing negative edge weights where the graph Laplacian L is perturbed by the minimum-norm perturbation criteria in (12) to eliminate negative eigenvalues for numerical stability (named Graph-MinNorm).", "startOffset": 204, "endOffset": 208}, {"referenceID": 45, "context": "We note that a user may define the desired classifier performance as a weighted sum of classification error and rejection rate for different applications, as done in [47].", "startOffset": 166, "endOffset": 170}], "year": 2017, "abstractText": "In a semi-supervised learning scenario, (possibly noisy) partially observed labels are used as input to train a classifier, in order to assign labels to unclassified samples. In this paper, we study this classifier learning problem from a graph signal processing (GSP) perspective. Specifically, by viewing a binary classifier as a piecewise constant graph-signal in a highdimensional feature space, we cast classifier learning as a signal restoration problem via a classical maximum a posteriori (MAP) formulation. Unlike previous graph-signal restoration works, we consider in addition edges with negative weights that signify anti-correlation between samples. One unfortunate consequence is that the graph Laplacian matrix L can be indefinite, and previously proposed graph-signal smoothness prior xLx for candidate signal x can lead to pathological solutions. In response, we derive an optimal perturbation matrix \u2206\u2014based on a fast lower-bound computation of the minimum eigenvalue of L via a novel application of the Haynsworth inertia additivity formula\u2014 so that L + \u2206 is positive semi-definite, resulting in a stable signal prior. Further, instead of forcing a hard binary decision for each sample, we define the notion of generalized smoothness on graph that promotes ambiguity in the classifier signal. Finally, we propose an algorithm based on iterative reweighted least squares (IRLS) that solves the posed MAP problem efficiently. Extensive simulation results show that our proposed algorithm outperforms both SVM variants and graph-based classifiers using positiveedge graphs noticeably.", "creator": "LaTeX with hyperref package"}}}