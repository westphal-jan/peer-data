{"id": "1611.04953", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Nov-2016", "title": "End-to-End Neural Sentence Ordering Using Pointer Network", "abstract": "Sentence ordering is one of important tasks in NLP. Previous works mainly focused on improving its performance by using pair-wise strategy. However, it is nontrivial for pair-wise models to incorporate the contextual sentence information. In addition, error prorogation could be introduced by using the pipeline strategy in pair-wise models. In this paper, we propose an end-to-end neural approach to address the sentence ordering problem, which uses the pointer network (Ptr-Net) to alleviate the error propagation problem and utilize the whole contextual information. Experimental results show the effectiveness of the proposed model. Source codes and dataset of this paper are available.", "histories": [["v1", "Tue, 15 Nov 2016 17:38:10 GMT  (114kb,D)", "http://arxiv.org/abs/1611.04953v1", null], ["v2", "Fri, 25 Nov 2016 16:38:30 GMT  (116kb,D)", "http://arxiv.org/abs/1611.04953v2", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["jingjing gong", "xinchi chen", "xipeng qiu", "xuanjing huang"], "accepted": false, "id": "1611.04953"}, "pdf": {"name": "1611.04953.pdf", "metadata": {"source": "CRF", "title": "End-to-End Neural Sentence Ordering Using Pointer Network", "authors": ["Jingjing Gong", "Xinchi Chen", "Xipeng Qiu", "Xuanjing Huang"], "emails": ["xjhuang}@fudan.edu.cn"], "sections": [{"heading": "1 Introduction", "text": "In fact, most of us are able to play by the rules we've set ourselves, \"he said.\" We've got to play by the rules, \"he said.\" We've got to play by the rules. \"\" We've got to play by the rules, \"he said.\" We've got to play by the rules. \"\" We've got to play by the rules. \"\" We've got to play by the rules, \"he said.\" We've got to play by the rules. \"\" We've got to play by the rules. \"\" We've got to play by the rules. \"\" We've got to play by the rules, \"he said.\" We've got to play by the rules. \"We've got to play by the rules.\" \"We've got to play by the rules.\""}, {"heading": "2 Pointer Network for Neural Sentence Ordering", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Task Description", "text": "The task of the order of propositions is to classify a series of propositions in a clear and consistent way. Specifically, the aim of n propositions s = s1, s2,..., sn is to find the golden order o * for these propositions: so * 1 so * 2 \u00b7 \u00b7 so * n, (1) which has the maximum probability of the given propositions P (o * | s): P (o * | s) > P (o | s), \u0424o *, (2) where o indicates any order of these propositions and vice versa indicates the set of all possible sequences."}, {"heading": "2.2 Model Architecture", "text": "Previous work has mainly focused on pairs of models (Chen et al., 2016; Agrawal et al., 2016; Li and Jurafsky, 2016), which lack contextual information and introduce error propagation for the piping strategy. Instead of splitting the score into independent pairs, in this paper we propose a holistic neural approach based on the pointer network (PtrNet) to evaluate the entire sequence of sentences. Model architecture is shown in Figure 1. (3) The probability of a particular order of o-given sentences P (o | s) could be formalized as: P (o | s) = n-coi = 1P (oi | oi \u2212 1,., o1, s)."}, {"heading": "2.3 Sentence Encoding", "text": "Since Ptr-Net receives fixed length vectors as input, we must first encode sentences with variable length. Inspired by Chen et al. (2016), we have tried three types of encoders: Continuous Bag of Words (CBoW), Convolutional Neural Networks (CNNs), and Long Short Term (LSTM) Neural Networks."}, {"heading": "2.3.1 Continues Bag of Words", "text": "The Continues bag of words (CBoW) model (Mikolov et al., 2013) merely represents the average of the embedding of words in a sentence. Formally, the embedding of nw words in a sentence results in s, w1,.., wnw, the sentence in which Enc (s) is embedded: Enc (s) = 1nw nw, k = 1 wk, (8) where Enc (s), wk, Rde. de is a hyperparameter indicating the size of the word embedding."}, {"heading": "2.3.2 Convolutional Neural Networks", "text": "Convolutionary neural networks (CNNs) (Simard et al., 2003) are biologically inspired variants of multiple layer perceptions (MLPs). Formally, sentences with nw words could be encoded as follows: covk = \u03c6 (W cov \u2212 1 u = 0 wk + u) + bcov), (9) Enc (s) = max k covk, (10) where Wcov \u0394R (d \u00b7 lf) \u00d7 df and bcov \u0412Rdf are trainable parameters and \u03c6 (\u00b7) is tanh function. Here, k = 1,..., nw \u2212 lf + 1, and lf and df hyperparameters, each indicating the filter length and number of character cards. Remarkably, the maximum operation in Eq (10) is an elementary operation."}, {"heading": "2.3.3 Long Short-term Neural Networks", "text": "Long-term neural networks (LSTM) (Hochreiter and Schmidhuber, 1997) are advanced recurrent neural networks (RNNs) that alleviate the problems of disappearance and explosion of gradients. Formally, LSTM has memory cells c-Rdr that are controlled by three types of gates: entrance gate i-Rdr, entrance gate f-Rdr and exit gate o-Rdr: it-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T, (11) ct-T-T-T-T-T-T-T-T, (12) ht-T-T-T-T-T (ct), (13) where Wg-R-T-T-T-T-T-T-T and bg-R-R4dR traable parameters. dr is a hyperparameter that indicates the size of the cell unit as well as the size of the gate unit."}, {"heading": "2.4 Order Prediction", "text": "Given P (o | s), the predicted order o \u00b2 is the one with the highest probability: o \u00b2 = arg max oP (o | s). (15) Since the decryption process to find o \u00b2 is a difficult NP problem, we instead use two strategies to decipher a suboptimal result: greedy decryption and beam search. Greedy decryption In the decryption phase of Prt-Net, greedy strategy o \u00b2 = o \u00b2 1,..., o \u00b2 n step by step determines as: o \u00b2 i = arg max oiP (oi \u00b2 i \u2212 1,..., o \u00b2 1, s). (16) Beam search strategy always retains the top b terms as candidates at each step. Formally, at stage t, each candidate o \u00b2 t1 = o \u00b2 1,..., o \u00b2 t has a probability: P (o \u00b2 t1 | s) = t \u00b2 i = 1P (o \u00b2 i \u2212 b), stage 1 \u00b7 b, stage b, with the probability o \u00b2 (n)."}, {"heading": "3 Training", "text": "Assuming that we have m training examples (xi, yi) m i = 1, where xi indicates a sequence of sentences with a specific permutation of yi and yi is in the gold order o *. In order to obtain more training data, we randomly generate new permutations for xi in each epoch. The aim is to minimize the loss function J (\u03b8): J (\u03b8) = \u2212 1 m \u00b2 i = 1 logP (yi | xi; \u03b8) + \u03bb 2% \u03b8 22, (18) where P (yi | xi; \u03b8) = P (o \u0445 | s = xi; \u03b8) and \u03bb is a hyperparameter of the regulation duration. \u043d displays all trainable parameters. Furthermore, we use AdaGrad (Duchi et al., 2011) with mixed mini-batch to train our model. We also use pre-trained embedding (Turian et al., 2010) as initialization."}, {"heading": "4 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Datasets", "text": "To evaluate the proposed model, we use two sets of data: summaries of arXiv (Chen et al., 2016) and ARE (Sequential Image Narrative Dataset) (Ferraro et al., 2016). Examples in the ARE dataset contain all 5 sets, and we only use captions in this essay. Details of two sets of data are shown in Table 1."}, {"heading": "4.2 Hyper-parameters", "text": "Table 2 gives the details of the hyperparameter configurations. The CNN sentence encoder uses three different filter lengths such as Kim et al. (2015)."}, {"heading": "4.3 Metrics", "text": "To evaluate our model, we use three different metrics: (1) Pair of metrics = 1 sentence = 1 sentence = 1 sentence = 1 sentence = 1 point = 1 point = 1 point = 1 point = 1 point = 1 point. (3) Pair of metrics Pair of metrics (PM) are the fraction of pairs of sentences whose predicted relative order is identical to the order of truth (higher is better). Formally, pair of metrics can be called three points: Precision P, callback R and F -values.P = 1 point i = 1 point (o point i) S (o point i) | | S (o point i), (o point i), (19) R = 1 point \u00b2 m \u00b2 i = 1 point (o point i)."}, {"heading": "4.4 Results", "text": "Since there is no loud sentence here and the number of output sets matches the inputs, three values of PM and LSR are equal in this table. Therefore, we only use PM and LSR to denote the same P, R, F values. As we can see, our model outperforms previous work and achieves the most modern performance. Our model with the LSTM sentence encoder reaches 40.00% in PMR, which means that 2 / 5 texts in the test set of the arXiv dataset are exactly correct, 6.57% higher than the works of Chen et al. (2016). Furthermore, we examine the performance of our model when the record number varies. Since Chen et al. (2016) did not evaluate its results on LSR metrics, we can only compare them with PM and PMR. As shown in Figure 2, our model with CNN and LSTM sentence encoders performs the work of STen."}, {"heading": "4.5 Visualization", "text": "To further understand how our model works, we do some visualizations of Ptr-Net with CNN and LSTM Sentence Encoders. As shown in Table 5, the left (first three columns) are input sentences. They are first encoded by sentence encoders (CNN or LSTM Sentence Encoders) and then sent to the decoder of Ptr-Net. This visualization shows how important each word is in generating the second sentence. The more important the words are, the darker the color that our model predicts. As we can see, the model focuses more on individual words, such as \"first,\" \"second,\" which give strong signals for creating the second sentence. The more important the words are, the darker the color of the sentence."}, {"heading": "4.6 Noisy Sentences", "text": "Interestingly, we find that our model could handle the case well if additional loud sentences exist. To evaluate the performance of our model on loud sentences, we compare three different strategies for supplementing sounds: (1) 0 loud sentence (0 sound); (2) 1 loud sentence (1 sound); (3) 1 loud sentence with a 50% probability (0 / 1 sound). All loud sentences come from our own data sets. Since all texts in the SIND data set contain 5 sentences, it is easy for the model to tell if there is a loud sentence. Therefore, we evaluate our model only on the basis of the arXiv data set as shown in Table 6. As mentioned above, we only use the model with the LSTM sentence encoder to evaluate the ability of our model to rate unambiguous loud sentences, as it always works better than the model with CBoW or CNN sentence encoder. Although the 0 noise version is the same as the case in Table 4, they actually differ from each other on this noise section 1, the noise level is not much different from the noise level in this section 1."}, {"heading": "4.7 Potential Oracle", "text": "In progress..."}, {"heading": "5 Related Work", "text": "Previous work on sentence order has mainly focused on external and downstream applications, such as summarizing multiple sentences and coherence of discourse (Van Dijk, 1985; Grosz et al., 1995; Van Berkum et al., 1999; Elsner et al., 2007; Barzilay and Lapata, 2008). Barzilay and Elhadad (2002) proposed two naive sentence order techniques, such as majority orders and chronological orders, in the context of a summary of several documents. Lapata (2003) proposed a probabilistic model that determines the probability of a given sentence by its adjacent sentence and learns constraints on sentence order from a corpus of domain-specific texts. Okazaki et al. (2004) improved chronological order by dissolving preceding sentences of arranged sentences and combining topical segmentations."}, {"heading": "6 Conclusions", "text": "Previous work has mainly been based on a pair-based learning framework that does not take into account contextual information and always leads to an error multiplication of their learning strategy in the pipeline. In this paper, we propose an end-to-end neural model based on Ptr-Net to solve the problem of ordering sentences. Experimental results show that our model also reaches the state of the art with the help of greedy deciphering strategies. In the future, we would like to further improve performance by re-enrolling candidates in the beam search."}], "references": [{"title": "2016. Sort story: Sorting jumbled images and captions into stories", "author": ["Arjun Chandrasekaran", "Dhruv Batra", "Devi Parikh", "Mohit Bansal"], "venue": "arXiv preprint arXiv:1606.07493", "citeRegEx": "Agrawal et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Agrawal et al\\.", "year": 2016}, {"title": "Inferring strategies for sentence ordering in multidocument news summarization", "author": ["Barzilay", "Elhadad2002] Regina Barzilay", "Noemie Elhadad"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Barzilay et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Barzilay et al\\.", "year": 2002}, {"title": "Modeling local coherence: An entitybased approach", "author": ["Barzilay", "Lapata2008] Regina Barzilay", "Mirella Lapata"], "venue": "Computational Linguistics,", "citeRegEx": "Barzilay et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Barzilay et al\\.", "year": 2008}, {"title": "A bottom-up approach to sentence ordering for multi-document summarization", "author": ["Naoaki Okazaki", "Mitsuru Ishizuka"], "venue": "Information processing & management,", "citeRegEx": "Bollegala et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Bollegala et al\\.", "year": 2010}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["Duchi et al.2011] John Duchi", "Elad Hazan", "Yoram Singer"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Duchi et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Duchi et al\\.", "year": 2011}, {"title": "A unified local and global model for discourse coherence", "author": ["Elsner et al.2007] Micha Elsner", "Joseph L Austerweil", "Eugene Charniak"], "venue": "In HLTNAACL,", "citeRegEx": "Elsner et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Elsner et al\\.", "year": 2007}, {"title": "Visualizing higher-layer features of a deep network", "author": ["Erhan et al.2009] Dumitru Erhan", "Yoshua Bengio", "Aaron Courville", "Pascal Vincent"], "venue": null, "citeRegEx": "Erhan et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Erhan et al\\.", "year": 2009}, {"title": "Centering: A framework for modeling the local coherence of discourse", "author": ["Scott Weinstein", "Aravind K Joshi"], "venue": null, "citeRegEx": "Grosz et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Grosz et al\\.", "year": 1995}, {"title": "Long short-term memory", "author": ["Hochreiter", "Schmidhuber1997] Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Characteraware neural language models", "author": ["Kim et al.2015] Yoon Kim", "Yacine Jernite", "David Sontag", "Alexander M Rush"], "venue": "arXiv preprint arXiv:1508.06615", "citeRegEx": "Kim et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kim et al\\.", "year": 2015}, {"title": "Probabilistic text structuring: Experiments with sentence ordering", "author": ["Mirella Lapata"], "venue": "In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics-Volume", "citeRegEx": "Lapata.,? \\Q2003\\E", "shortCiteRegEx": "Lapata.", "year": 2003}, {"title": "Neural net models for open-domain discourse coherence", "author": ["Li", "Jurafsky2016] Jiwei Li", "Dan Jurafsky"], "venue": "arXiv preprint arXiv:1606.01545", "citeRegEx": "Li et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Li et al\\.", "year": 2016}, {"title": "Visualizing and understanding neural models in nlp", "author": ["Li et al.2015] Jiwei Li", "Xinlei Chen", "Eduard Hovy", "Dan Jurafsky"], "venue": "arXiv preprint arXiv:1506.01066", "citeRegEx": "Li et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Li et al\\.", "year": 2015}, {"title": "Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781", "author": ["Kai Chen", "Greg Corrado", "Jeffrey Dean"], "venue": null, "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Improving chronological sentence ordering by precedence relation", "author": ["Yutaka Matsuo", "Mitsuru Ishizuka"], "venue": "In Proceedings of the 20th international conference on Computational Linguistics,", "citeRegEx": "Okazaki et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Okazaki et al\\.", "year": 2004}, {"title": "Best practices for convolutional neural networks applied to visual document analysis", "author": ["Dave Steinkraus", "John C Platt"], "venue": "In null,", "citeRegEx": "Simard et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Simard et al\\.", "year": 2003}, {"title": "Deep inside convolutional networks: Visualising image classification models and saliency maps. arXiv preprint arXiv:1312.6034", "author": ["Andrea Vedaldi", "Andrew Zisserman"], "venue": null, "citeRegEx": "Simonyan et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Simonyan et al\\.", "year": 2013}, {"title": "Word representations: a simple and general method for semi-supervised learning", "author": ["Turian et al.2010] Joseph Turian", "Lev Ratinov", "Yoshua Bengio"], "venue": "In Proceedings of ACL", "citeRegEx": "Turian et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Turian et al\\.", "year": 2010}, {"title": "Semantic integration in sentences and discourse: Evidence from the n400", "author": ["Peter Hagoort", "Colin Brown"], "venue": "Cognitive Neuroscience, Journal", "citeRegEx": "Berkum et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Berkum et al\\.", "year": 1999}, {"title": "Semantic discourse analysis", "author": ["Van Dijk1985] Teun A Van Dijk"], "venue": "Handbook of discourse analysis,", "citeRegEx": "Dijk.,? \\Q1985\\E", "shortCiteRegEx": "Dijk.", "year": 1985}], "referenceMentions": [{"referenceID": 0, "context": "Recently, sentence ordering task attracts more focus in NLP community (Chen et al., 2016; Agrawal et al., 2016; Li and Jurafsky, 2016) as its importance on many succeed applications such as multi-document summarization, etc.", "startOffset": 70, "endOffset": 134}, {"referenceID": 7, "context": "The goal of sentence ordering is to arrange a set of sentences into a coherent text in a clear and consistent manner (Grosz et al., 1995; Van Berkum et al., 1999; Barzilay and Lapata, 2008).", "startOffset": 117, "endOffset": 189}, {"referenceID": 0, "context": ", 2016; Agrawal et al., 2016; Li and Jurafsky, 2016) as its importance on many succeed applications such as multi-document summarization, etc. The goal of sentence ordering is to arrange a set of sentences into a coherent text in a clear and consistent manner (Grosz et al., 1995; Van Berkum et al., 1999; Barzilay and Lapata, 2008). Most of previous researches of sentence ordering are pair-wise models. Chen et al. (2016) and Agrawal et al.", "startOffset": 8, "endOffset": 424}, {"referenceID": 0, "context": ", 2016; Agrawal et al., 2016; Li and Jurafsky, 2016) as its importance on many succeed applications such as multi-document summarization, etc. The goal of sentence ordering is to arrange a set of sentences into a coherent text in a clear and consistent manner (Grosz et al., 1995; Van Berkum et al., 1999; Barzilay and Lapata, 2008). Most of previous researches of sentence ordering are pair-wise models. Chen et al. (2016) and Agrawal et al. (2016) proposed pair-wise models", "startOffset": 8, "endOffset": 450}, {"referenceID": 10, "context": "Li and Jurafsky (2016) additionally employed the graph based method (Lapata, 2003) to rank sentences.", "startOffset": 68, "endOffset": 82}, {"referenceID": 0, "context": "Previous works mainly focused on pair-wise models (Chen et al., 2016; Agrawal et al., 2016; Li and Jurafsky, 2016), which lack contextual information and introduce error propagation for the pipe line strategy.", "startOffset": 50, "endOffset": 114}, {"referenceID": 13, "context": "Continues bag of words (CBoW) model (Mikolov et al., 2013) simply averages the embeddings of words of a sentence.", "startOffset": 36, "endOffset": 58}, {"referenceID": 15, "context": "Convolutional neural networks (CNNs) (Simard et al., 2003) are biologically-inspired variants of multiple layer perceptions (MLPs).", "startOffset": 37, "endOffset": 58}, {"referenceID": 4, "context": "In addition, we use AdaGrad (Duchi et al., 2011) with shuffled mini-batch to train our model.", "startOffset": 28, "endOffset": 48}, {"referenceID": 17, "context": "We also use pre-trained embeddings (Turian et al., 2010) as initialization.", "startOffset": 35, "endOffset": 56}, {"referenceID": 0, "context": "43 - - (Agrawal et al., 2016) - - - 73.", "startOffset": 7, "endOffset": 29}, {"referenceID": 9, "context": "The CNN sentence encoder uses three different filter lengths as Kim et al. (2015).", "startOffset": 64, "endOffset": 82}, {"referenceID": 6, "context": "How could we determine the importance of each word (the color)? Inspired by the back-propagation strategy (Erhan et al., 2009; Simonyan et al., 2013; Li et al., 2015; Chen et al., 2016), which measures how much each input unit contributes to the final decision, we can approximate the importance of words by their first derivatives.", "startOffset": 106, "endOffset": 185}, {"referenceID": 16, "context": "How could we determine the importance of each word (the color)? Inspired by the back-propagation strategy (Erhan et al., 2009; Simonyan et al., 2013; Li et al., 2015; Chen et al., 2016), which measures how much each input unit contributes to the final decision, we can approximate the importance of words by their first derivatives.", "startOffset": 106, "endOffset": 185}, {"referenceID": 12, "context": "How could we determine the importance of each word (the color)? Inspired by the back-propagation strategy (Erhan et al., 2009; Simonyan et al., 2013; Li et al., 2015; Chen et al., 2016), which measures how much each input unit contributes to the final decision, we can approximate the importance of words by their first derivatives.", "startOffset": 106, "endOffset": 185}, {"referenceID": 7, "context": "Previous works on sentence ordering mainly focused on the external and downstream applications, such as multi-document summarization and discourse coherence (Van Dijk, 1985; Grosz et al., 1995; Van Berkum et al., 1999; Elsner et al., 2007; Barzilay and Lapata, 2008).", "startOffset": 157, "endOffset": 266}, {"referenceID": 5, "context": "Previous works on sentence ordering mainly focused on the external and downstream applications, such as multi-document summarization and discourse coherence (Van Dijk, 1985; Grosz et al., 1995; Van Berkum et al., 1999; Elsner et al., 2007; Barzilay and Lapata, 2008).", "startOffset": 157, "endOffset": 266}, {"referenceID": 0, "context": "Recently, increasing number of researches studied sentence ordering using neural models (Agrawal et al., 2016; Li and Jurafsky, 2016; Chen et al., 2016).", "startOffset": 88, "endOffset": 152}, {"referenceID": 3, "context": ", 1999; Elsner et al., 2007; Barzilay and Lapata, 2008). Barzilay and Elhadad (2002) proposed two naive sentence ordering techniques, such as majority ordering and chronological ordering, in the context of multi-document summarization.", "startOffset": 8, "endOffset": 85}, {"referenceID": 3, "context": ", 1999; Elsner et al., 2007; Barzilay and Lapata, 2008). Barzilay and Elhadad (2002) proposed two naive sentence ordering techniques, such as majority ordering and chronological ordering, in the context of multi-document summarization. Lapata (2003) proposed a probabilistic model that assumes the probability of any given sentence is determined by its adjacent sentence and learns constraints on sentence order from a corpus of domain specific texts.", "startOffset": 8, "endOffset": 250}, {"referenceID": 3, "context": ", 1999; Elsner et al., 2007; Barzilay and Lapata, 2008). Barzilay and Elhadad (2002) proposed two naive sentence ordering techniques, such as majority ordering and chronological ordering, in the context of multi-document summarization. Lapata (2003) proposed a probabilistic model that assumes the probability of any given sentence is determined by its adjacent sentence and learns constraints on sentence order from a corpus of domain specific texts. Okazaki et al. (2004) improved chronological ordering by resolving antecedent sentences of arranged sentences and combining topical segmentation.", "startOffset": 8, "endOffset": 474}, {"referenceID": 2, "context": "Bollegala et al. (2010) presented a bottom-up approach to arrange sentences extracted for multi-document summarization.", "startOffset": 0, "endOffset": 24}, {"referenceID": 0, "context": "Recently, increasing number of researches studied sentence ordering using neural models (Agrawal et al., 2016; Li and Jurafsky, 2016; Chen et al., 2016). Chen et al. (2016) framed sentence ordering as an isolated task and firstly applied neural methods on sentence ordering.", "startOffset": 89, "endOffset": 173}, {"referenceID": 0, "context": "Recently, increasing number of researches studied sentence ordering using neural models (Agrawal et al., 2016; Li and Jurafsky, 2016; Chen et al., 2016). Chen et al. (2016) framed sentence ordering as an isolated task and firstly applied neural methods on sentence ordering. In addition, they designed an interesting task of ordering the coherent sentences from academic abstracts. Agrawal et al. (2016) focused on a very similar ordering task which ranks image-caption pairs, additionally considering the image information.", "startOffset": 89, "endOffset": 404}, {"referenceID": 0, "context": "Recently, increasing number of researches studied sentence ordering using neural models (Agrawal et al., 2016; Li and Jurafsky, 2016; Chen et al., 2016). Chen et al. (2016) framed sentence ordering as an isolated task and firstly applied neural methods on sentence ordering. In addition, they designed an interesting task of ordering the coherent sentences from academic abstracts. Agrawal et al. (2016) focused on a very similar ordering task which ranks image-caption pairs, additionally considering the image information. Li and Jurafsky (2016) mainly applied neural models to judge if a given text is coherent.", "startOffset": 89, "endOffset": 548}], "year": 2016, "abstractText": "Sentence ordering is one of important tasks in NLP. Previous works mainly focused on improving its performance by using pair-wise strategy. However, it is nontrivial for pairwise models to incorporate the contextual sentence information. In addition, error prorogation could be introduced by using the pipeline strategy in pair-wise models. In this paper, we propose an end-to-end neural approach to address the sentence ordering problem, which uses the pointer network (Ptr-Net) to alleviate the error propagation problem and utilize the whole contextual information. Experimental results show the effectiveness of the proposed model. Source codes1 and dataset2 of this paper are available.", "creator": "LaTeX with hyperref package"}}}