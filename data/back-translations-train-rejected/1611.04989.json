{"id": "1611.04989", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Nov-2016", "title": "Recurrent Neural Network based Part-of-Speech Tagger for Code-Mixed Social Media Text", "abstract": "This paper describes Centre for Development of Advanced Computing's (CDACM) submission to the shared task-'Tool Contest on POS tagging for Code-Mixed Indian Social Media (Facebook, Twitter, and Whatsapp) Text', collocated with ICON-2016. The shared task was to predict Part of Speech (POS) tag at word level for a given text. The code-mixed text is generated mostly on social media by multilingual users. The presence of the multilingual words, transliterations, and spelling variations make such content linguistically complex. In this paper, we propose an approach to POS tag code-mixed social media text using Recurrent Neural Network Language Model (RNN-LM) architecture. We submitted the results for Hindi-English (hi-en), Bengali-English (bn-en), and Telugu-English (te-en) code-mixed data.", "histories": [["v1", "Tue, 15 Nov 2016 19:02:35 GMT  (26kb,D)", "http://arxiv.org/abs/1611.04989v1", "7 pages, Published at the Tool Contest on POS tagging for Indian Social Media Text, ICON 2016"], ["v2", "Wed, 16 Nov 2016 04:28:06 GMT  (26kb,D)", "http://arxiv.org/abs/1611.04989v2", "7 pages, Published at the Tool Contest on POS tagging for Indian Social Media Text, ICON 2016"]], "COMMENTS": "7 pages, Published at the Tool Contest on POS tagging for Indian Social Media Text, ICON 2016", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["raj nath patel", "prakash b pimpale", "m sasikumar"], "accepted": false, "id": "1611.04989"}, "pdf": {"name": "1611.04989.pdf", "metadata": {"source": "CRF", "title": "Recurrent Neural Network based Part-of-Speech Tagger for Code-Mixed Social Media Text", "authors": ["Raj Nath Patel", "Prakash B. Pimpale"], "emails": ["rajnathp@cdac.in", "prakash@cdac.in", "sasi@cdac.in"], "sections": [{"heading": "1 Introduction", "text": "Code mixing and code switching are observed in the text or in the language produced by a multilingual user. Code mixing occurs when a user changes the language within a sentence, i.e. a clause, phrase, or word of a language is used within an utterance of another language. While the simultaneous occurrence of speech extracts from two different grammatical systems is known as code switching, language analysis of code-mixed text is a non-trivial task. Traditional approaches to POS tagging are not effective for this text as they do not adhere to any grammatical structure in general. Many studies have shown that RNN-based POS taggers produced comparable results and is also the state-of-the-art for some languages. As-ever, to the best of knowledge, no study has been conducted for RNOS-based models that use RNOS labeling of codemixed data.In this paper we have proposed a POS-Tagger-LM architecture with RLN."}, {"heading": "2 Related Work", "text": "POS labeling has been studied for decades in the literature of Natural Language Processing (NLP). Various methods such as a Support Vector Machine (Ma rquez and Gime \u0301 nez, 2004), Decision Tree (Schmid and Laws, 2008), Hidden Markov Model (HMM) (Kupiec, 1992) and, Conditional Random Field Auto Encoders (Ammar et al., 2014) have been tried for this task. Among these, Neural Network (NN) -based models mainly relate to this work. In the NN family, RNN is a widely used network for various NLP applications (Mikolov et al., 2010; Mikolov et al., 2013b; Neural Network (NN) -based models mainly relate to this work."}, {"heading": "3 Experimental Setup", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 RNN Models", "text": "There are many variants of RNN networks for different applications. For this task we used Elaan (Elman, 1990), Long Short-Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997), Deep LSTM, Gated Recurrent Unit (GRU) (Cho et al., 2014), which are widely used RNN models in the NLP literature. In the following subsections we gave a brief description of each model with mathematical equations (1,2 and 3). In the equations xt and yt are the input and output vectors. ht and ht \u2212 1 represent the current and / or previous hidden states. W \u0445 are the weight matrices and b \u0445 the bias vectors."}, {"heading": "3.1.1 ELMAN", "text": "Elman and Jordon (Jordan, 1986) Networks are the simplest network in the RNN family and are called Simple RNN. The Elman network is defined by the following equations: ht = sigm (Wxhxt + Whhht \u2212 1 + bh) (1) yt = softmax (Whyht + by)"}, {"heading": "3.1.2 LSTM", "text": "LSTM and other complex RNN models solve this problem by introducing a gating mechanism. Many variants of LSTM (Graves, 2013; Yao et al., 2014; Jozefowicz et al., 2015) have been tested in the literature for the various tasks. We implemented the following version: it = sigm (Wxixt + Whiht \u2212 1 + bi) (2) ot = sigm (Wxoxt + Whoht \u2212 1 + bo) ft = sigm (Wxfxt + Whfht \u2212 1 + bf) jt = tanh (Wwherext + Whjht \u2212 1 + bj) ct = ct \u2212 1 ft + it jt ht ht ht = tanh (ct) ot yt = softmax (Whyht \u2212 1 + bf) jt = tanh (Whyht + by), where i, input + Whjht \u2212 1 + bj, memory and content are updated."}, {"heading": "3.1.3 Deep LSTM", "text": "In this essay we used Deep LSTM with two layers. Deep LSTM is created by stacking several LSTM on top of each other. Output of the lower LSTM is input into the upper LSTM. For example, if ht is the output of the lower LSTM, we apply a matrix transformation to form the input texts for the upper LSTM. Matrix transformation allows us to have two successive LSTM layers of different sizes."}, {"heading": "3.1.4 GRU", "text": "The GRU network also uses a different gating mechanism with reset (r) and update (z) gates. The following equations define a GRU model: rt = sigm (Wxrxt + Whrht \u2212 1 + br) (3) zt = sigm (Wxzxt + Whzht \u2212 1 + bz) h-t = tanh (Wxhxt + Whh (rt ht \u2212 1) + bh) ht = zt ht \u2212 1 + (1 \u2212 zt) h-t = softmax (Whyht + by)."}, {"heading": "3.2 Implementation", "text": "All models were implemented with the 3THEANO framework (Bergstra et al., 2010; Bastien et al., 2012).For all models, the word embedding was dimensionality 100, no hidden units 100 and the context word window size 5 (wi \u2212 2wi \u2212 1wiwi + 1wi + 2).We initialized all square weight matrices as random orthogonal matrices. All bias vectors were initialized to zero. Other weight matrices were evaluated from a Gaussian distribution with the mean 0 and the variance 0.0001. we trained all models with truncated back propagation through time (TBPTT) (Werbos, 1990) with the stochastic gradient descent. Default values of the hyperparameters were used for RNN model training as suggested in the literature (Yao et al., 2014; Patel and Sakumar 2016, we set the depth of each of the TT = 50 for each of the BT models)."}, {"heading": "3.3 Data", "text": "The code-mixed data of bn-en, hi-en and te-en were shared separately for the Facebook (fb), Twitter (twt) and Whatsapp (wa) posts and conversations with CoarseGrained (CG) and Fine-Grained (FG) POS annotations. We combined the data from fb, twt and wa for CG and FG annotations for each language pair. The data was divided into training, test and development kits. Test and development3http: / / deeplearning.net / software / theano / # downloadsets were randomly sampled from the complete data. Table 1 shows the size of the different sentences at sentence and token level. Tag-set counts for CG and FG are also provided. We process the text # # for mentions, hashtags, smilies, URLs, numbers and punctuations. By default, all words such as ib @ were assigned to the menus."}, {"heading": "3.4 Methodology", "text": "In the first setting (Simple RNN, LSTM, Deep LSTM, GRU) we learn word representation from scratch with the other model parameters. In the second configuration (GRU Pre) we train word representation (Pre-Training) with the tool word2vec (Mikolov et al., 2013b) and refine it with the formation of other parameters of the network. Pre-Training not only leads to learning minima with better generalization in non-convex optimization (Bengio, 2009; Erhan et al., 2010), but also improves the accuracy of the system (Kreutzer et al., 2015; Patel and Sasikumar, 2016). In the third setting (GRU Pre Lang) we added the language of words as an additional feature with the context in which we learn the words from similar vectors."}, {"heading": "4 Results", "text": "We used the F1 score to evaluate the experiments, the results are shown in Table 2. We trained models as described in Section 3.4. To compare our results, we also trained the Stanford and HunPos markers on the same data, accuracy is shown in Table 2.The table shows that pre-training and language are helpful as an additional feature. Also, the accuracy of our best system (GRU Pre Lang) is similar to that of Stanford and HunPos. GRU models outperform other models (Simple RNN, LSTM, Deep LSTM) in this task, as Chung et al. (2014) reported for a number of NLP tasks."}, {"heading": "5 Submission to the Shared Task", "text": "In the competition, there were two types of submissions, which were initially restricted: limited to the use of data shared by the organisers with the introduced systems of the entrants; secondly, unrestricted: entrants were allowed to use the publicly available resources (training data, introduced systems, etc.) We submitted for all language pairs (hi-en, bn-en and, te-en) and domains (fb, twt and, wa); for submission under duress, the output of GRU Pre Lang was used; we trained Stanford POS taggers with the same data for full submission; Jamatia and Das (2016) evaluated all submitted systems against another gold test set and reported the results."}, {"heading": "6 Analysis", "text": "We conducted a preliminary analysis of our systems and reported on a few points in this section. \u2022 The POS categories that contribute more to error analysis are G X, G V, G N and G J for coarse-grained and V VM, JJ, N NN and N NNP for fine-grained systems. We also performed the confusion matrix analysis and found that these POS tags are usually confused with each other. For example, the G J POS tag was incorrectly marked 28 times with the other POS tags, when it was G N 17 times. \u2022 RNN models require an enormous amount of corpus to train the model parameters. From the results, we can conclude that the results of the best RNN model (GRU Pre Lang) for hires and te-en are comparable to Stanford and HunPos with only about 2K training sets."}, {"heading": "7 Conclusion and Future Work", "text": "We have developed language-independent and generic POS taggers for social media texts using RNN networks. We have tried Simple RNN, LSTM, Deep LSTM and, GRU models. We have shown that GRU outperforms other models and also benefits from pre-training and language as an additional feature. Also, the accuracy of our approach is comparable to that of Stanford and HunPos.In the future, we could try RNN models with more features such as POS tags of context words, prefixes and suffixes, length, position, etc."}], "references": [{"title": "Conditional random field autoencoders for unsupervised structured prediction", "author": ["Ammar et al.2014] Waleed Ammar", "Chris Dyer", "Noah A Smith"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Ammar et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Ammar et al\\.", "year": 2014}, {"title": "Theano: new features and speed improvements", "author": ["Pascal Lamblin", "Razvan Pascanu", "James Bergstra", "Ian Goodfellow", "Arnaud Bergeron", "Nicolas Bouchard", "David WardeFarley", "Yoshua Bengio"], "venue": null, "citeRegEx": "Bastien et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Bastien et al\\.", "year": 2012}, {"title": "Learning long-term dependencies with gradient descent is difficult", "author": ["Bengio et al.1994] Yoshua Bengio", "Patrice Simard", "Paolo Frasconi"], "venue": "In IEEE Transactions on Neural Networks,", "citeRegEx": "Bengio et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 1994}, {"title": "A neural probabilistic language model", "author": ["Bengio et al.2003] Yoshua Bengio", "Rejean Ducharme", "Pascal Vincent", "Christian Jauvin"], "venue": "In Journal of Machine Learning Reseach,", "citeRegEx": "Bengio et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2003}, {"title": "Learning Deep Architectures for AI. Foundations and trends R", "author": ["Yoshua Bengio"], "venue": null, "citeRegEx": "Bengio.,? \\Q2009\\E", "shortCiteRegEx": "Bengio.", "year": 2009}, {"title": "Theano: a CPU and GPU math expression compiler", "author": ["Guillaume Desjardins", "Joseph Turian", "David WardeFarley", "Yoshua Bengio."], "venue": "Proceedings of the Python for scientific computing conference (SciPy), volume 4.", "citeRegEx": "Desjardins et al\\.,? 2010", "shortCiteRegEx": "Desjardins et al\\.", "year": 2010}, {"title": "Learning phrase representations using RNN encoder-decoder for statistical machine", "author": ["Cho et al.2014] Kyunghyun Cho", "Bart Van Merrinboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio"], "venue": null, "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Empirical evaluation of gated recurrent neural networks on sequence modeling", "author": ["Chung et al.2014] Junyoung Chung", "Caglar Gulcehre", "KyungHyun Cho", "Yoshua Bengio"], "venue": null, "citeRegEx": "Chung et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chung et al\\.", "year": 2014}, {"title": "Unsupervised part-of-speech tagging with bilingual graph-based projections", "author": ["Das", "Petrov2011] Dipanjan Das", "Slav Petrov"], "venue": "In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language", "citeRegEx": "Das et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Das et al\\.", "year": 2011}, {"title": "Simpler unsupervised pos tagging with bilingual projections", "author": ["Duong et al.2013] Long Duong", "Paul Cook", "Steven Bird", "Pavel Pecina"], "venue": "ACL", "citeRegEx": "Duong et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Duong et al\\.", "year": 2013}, {"title": "Finding Structure in Time", "author": ["Jeffrey L Elman"], "venue": "Cognitive science,", "citeRegEx": "Elman.,? \\Q1990\\E", "shortCiteRegEx": "Elman.", "year": 1990}, {"title": "Why Does Unsupervised Pre-training Help Deep Learning", "author": ["Erhan et al.2010] Dumitru Erhan", "Yoshua Bengio", "Aaron Courville", "Pierre-Antoine Manzagol", "Pascal Vincent", "Samy Bengio"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Erhan et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Erhan et al\\.", "year": 2010}, {"title": "Simple task-specific bilingual word embeddings", "author": ["Gouws", "S\u00f8gaard2015] Stephan Gouws", "Anders S\u00f8gaard"], "venue": "In Proceedings of NAACL-HLT,", "citeRegEx": "Gouws et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gouws et al\\.", "year": 2015}, {"title": "Generating sequences with recurrent neural networks. arXiv:1308.0850", "author": ["Alex Graves"], "venue": null, "citeRegEx": "Graves.,? \\Q2013\\E", "shortCiteRegEx": "Graves.", "year": 2013}, {"title": "Hunpos: An open source trigram tagger", "author": ["Andr\u00e1s Kornai", "Csaba Oravecz"], "venue": "In Proceedings of the 45th annual meeting of the ACL on interactive poster and demonstration sessions,", "citeRegEx": "Hal\u00e1csy et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Hal\u00e1csy et al\\.", "year": 2007}, {"title": "Long short-term memory", "author": ["Hochreiter", "Schmidhuber1997] Sepp Hochreiter", "Jurgen Schmidhuber"], "venue": "In Neural computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Part-of-speech tagging for code-mixed English-Hindi Twitter and Facebook chat messages", "author": ["Bj\u00f6rn Gamb\u00e4ck", "Amitava Das"], "venue": "RECENT ADVANCES IN,", "citeRegEx": "Jamatia et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Jamatia et al\\.", "year": 2015}, {"title": "Attractor Dynamics and Parallellism in a Connectionist Sequential Machine", "author": ["Michael I Jordan"], "venue": "In Proceedings of 1986 Cognitive Science Conference,", "citeRegEx": "Jordan.,? \\Q1986\\E", "shortCiteRegEx": "Jordan.", "year": 1986}, {"title": "An empirical exploration of recurrent network architectures", "author": ["Wojciech Zaremba", "Ilya Sutskever"], "venue": "In Proceedings of the 32nd International Conference on Machine Learning,", "citeRegEx": "Jozefowicz et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Jozefowicz et al\\.", "year": 2015}, {"title": "QUality Estimation from ScraTCH (QUETCH): Deep Learning for Word-level Translation Quality Estimation", "author": ["Shigehiko Schamoni", "Stefan Riezler"], "venue": "In Proceedings of the Tenth Workshop on Statistical Ma-", "citeRegEx": "Kreutzer et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kreutzer et al\\.", "year": 2015}, {"title": "Robust part-ofspeech tagging using a hidden markov model", "author": ["Julian Kupiec"], "venue": "Computer Speech & Language,", "citeRegEx": "Kupiec.,? \\Q1992\\E", "shortCiteRegEx": "Kupiec.", "year": 1992}, {"title": "A general pos tagger generator based on support vector machines", "author": ["M\u00e0rquez", "Gim\u00e9nez2004] L M\u00e0rquez", "J Gim\u00e9nez"], "venue": "Journal of Machine Learning Research", "citeRegEx": "M\u00e0rquez et al\\.,? \\Q2004\\E", "shortCiteRegEx": "M\u00e0rquez et al\\.", "year": 2004}, {"title": "Recurrent neural network based language model", "author": ["Martin Karafiat", "Lukas Burget", "Jan Cernocky", "Sanjeev Khudanpur"], "venue": "In Proceedings of Interspeech,", "citeRegEx": "Mikolov et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2010}, {"title": "Exploiting Similarities among Languages for Machine Translation", "author": ["Quoc V Le", "Ilya Sutskever"], "venue": "In CoRR,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "A scalable hierarchical distributed language model", "author": ["Mnih", "Hinton2009] Andriy Mnih", "Geoffrey E. Hinton"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Mnih et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2009}, {"title": "Hierarchical Probabilistic Neural Network Language Model", "author": ["Morin", "Bengio2005] Frederic Morin", "Yoshua Bengio"], "venue": "In Aistats,", "citeRegEx": "Morin et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Morin et al\\.", "year": 2005}, {"title": "Translation Quality Estimation using Recurrent Neural Network", "author": ["Patel", "Sasikumar2016] Raj Nath Patel", "M Sasikumar"], "venue": null, "citeRegEx": "Patel et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Patel et al\\.", "year": 2016}, {"title": "Experiments with POS Tagging Code-mixed Indian Social Media Text. ICON", "author": ["Pimpale", "Patel2015] Prakash B. Pimpale", "Raj Nath Patel"], "venue": null, "citeRegEx": "Pimpale et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Pimpale et al\\.", "year": 2015}, {"title": "POS tagging of Chinese Buddhist texts using Recurrent Neural Networks", "author": ["Longlu Qin"], "venue": "Technical report,", "citeRegEx": "Qin.,? \\Q2015\\E", "shortCiteRegEx": "Qin.", "year": 2015}, {"title": "Part-of-Speech Tagging for Code-mixed Indian Social Media Text at ICON", "author": ["Kamal Sarkar"], "venue": null, "citeRegEx": "Sarkar.,? \\Q2015\\E", "shortCiteRegEx": "Sarkar.", "year": 2015}, {"title": "Estimation of conditional probabilities with decision trees and an application to fine-grained pos tagging", "author": ["Schmid", "Laws2008] Helmut Schmid", "Florian Laws"], "venue": "In Proceedings of the 22nd International Conference on Computational Linguistics-", "citeRegEx": "Schmid et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Schmid et al\\.", "year": 2008}, {"title": "Continuous space language models", "author": ["Holger Schwenk"], "venue": "In Computer Speech and Language,", "citeRegEx": "Schwenk.,? \\Q2007\\E", "shortCiteRegEx": "Schwenk.", "year": 2007}, {"title": "Parsing With Compositional Vector Grammars", "author": ["John Bauer", "Christopher D. Manning", "Andrew Y. Ng"], "venue": "In Proceedings of the ACL", "citeRegEx": "Socher et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Recursive deep models for semantic compositionality over a sentiment treebank", "author": ["Alex Perelygin", "Jy Wu"], "venue": "In Proceedings of EMNLP,", "citeRegEx": "Socher et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Feature-Rich Part-of-Speech Tagging with a Cyclic Dependency Network", "author": ["Dan Klein", "Christopher D Manning", "Yoram Singer"], "venue": "In Proceedings of the 2003 Conference of the North American Chapter", "citeRegEx": "Toutanova et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Toutanova et al\\.", "year": 2003}, {"title": "Pos tagging of english-hindi code-mixed social media content", "author": ["Vyas et al.2014] Yogarshi Vyas", "Spandana Gella", "Jatin Sharma", "Kalika Bali", "Monojit Choudhury"], "venue": "In EMNLP,", "citeRegEx": "Vyas et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Vyas et al\\.", "year": 2014}, {"title": "Part-ofspeech tagging with bidirectional long short-term memory recurrent neural network. arXiv preprint arXiv:1510.06168", "author": ["Wang et al.2015] Peilu Wang", "Yao Qian", "Frank K Soong", "Lei He", "Hai Zhao"], "venue": null, "citeRegEx": "Wang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2015}, {"title": "Backpropagation through time: what it does and how to do it", "author": ["Paul J. Werbos"], "venue": "In IEEE,", "citeRegEx": "Werbos.,? \\Q1990\\E", "shortCiteRegEx": "Werbos.", "year": 1990}, {"title": "Recurrent neural networks for language understanding", "author": ["Yao et al.2013] Kaisheng Yao", "Geoffrey Zweig", "MeiYuh Hwang", "Yangyang Shi", "Dong Yu"], "venue": "In INTERSPEECH,", "citeRegEx": "Yao et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Yao et al\\.", "year": 2013}, {"title": "Spoken language understanding using long shortterm memory neural networks", "author": ["Yao et al.2014] Kaisheng Yao", "Baolin Peng", "Yu Zhang", "Dong Yu", "Geoffrey Zweig", "Yangyang Shi"], "venue": "In Spoken Language Technology Workshop (SLT),", "citeRegEx": "Yao et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Yao et al\\.", "year": 2014}, {"title": "ADADELTA: an adaptive learning rate method", "author": ["Matthew D. Zeiler"], "venue": null, "citeRegEx": "Zeiler.,? \\Q2012\\E", "shortCiteRegEx": "Zeiler.", "year": 2012}, {"title": "Unsupervised and Lightly Supervised Part-of-Speech Tagging Using Recurrent Neural Networks", "author": ["Nasredine Semmar", "Laurent Besacier"], "venue": "In Proceedings of the 29th Pacific Asia Conference on Language,", "citeRegEx": "Zennaki et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zennaki et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 39, "context": "Earlier, researchers have adopted RNN-LM architecture for Natural language Understanding (NLU) (Yao et al., 2013; Yao et al., 2014) and Translation Quality Estimation (Patel and Sasikumar, 2016).", "startOffset": 95, "endOffset": 131}, {"referenceID": 40, "context": "Earlier, researchers have adopted RNN-LM architecture for Natural language Understanding (NLU) (Yao et al., 2013; Yao et al., 2014) and Translation Quality Estimation (Patel and Sasikumar, 2016).", "startOffset": 95, "endOffset": 131}, {"referenceID": 3, "context": "RNNLM models are similar to other vectorspace language models (Bengio et al., 2003; Morin and Bengio, 2005; Schwenk, 2007; Mnih and Hinton, 2009) where we represent each word with a high dimensional real-valued vector.", "startOffset": 62, "endOffset": 145}, {"referenceID": 32, "context": "RNNLM models are similar to other vectorspace language models (Bengio et al., 2003; Morin and Bengio, 2005; Schwenk, 2007; Mnih and Hinton, 2009) where we represent each word with a high dimensional real-valued vector.", "startOffset": 62, "endOffset": 145}, {"referenceID": 35, "context": "In this paper, we show that our approach achieves results close to the state-of-the-art systems such as 1Stanford (Toutanova et al., 2003), and 2HunPos (Hal\u00e1csy et al.", "startOffset": 114, "endOffset": 138}, {"referenceID": 14, "context": ", 2003), and 2HunPos (Hal\u00e1csy et al., 2007) .", "startOffset": 21, "endOffset": 43}, {"referenceID": 20, "context": "Different methods like a Support Vector Machine (M\u00e0rquez and Gim\u00e9nez, 2004), Decision Tree (Schmid and Laws, 2008), Hidden Markov Model (HMM) (Kupiec, 1992) and, Conditional Random Field Auto Encoders (Ammar et al.", "startOffset": 142, "endOffset": 156}, {"referenceID": 0, "context": "Different methods like a Support Vector Machine (M\u00e0rquez and Gim\u00e9nez, 2004), Decision Tree (Schmid and Laws, 2008), Hidden Markov Model (HMM) (Kupiec, 1992) and, Conditional Random Field Auto Encoders (Ammar et al., 2014) have been tried for this task.", "startOffset": 201, "endOffset": 221}, {"referenceID": 22, "context": "In NN family, RNN is widely used network for various NLP applications (Mikolov et al., 2010; Mikolov et al., 2013a; Mikolov et al., 2013b; Socher et al., 2013a; Socher et al., 2013b).", "startOffset": 70, "endOffset": 182}, {"referenceID": 9, "context": "(2015) have used RNN for resource-poor languages and reported comparable results with state-of-the-art systems (Das and Petrov, 2011; Duong et al., 2013; Gouws and S\u00f8gaard, 2015).", "startOffset": 111, "endOffset": 178}, {"referenceID": 35, "context": "Wang et al. (2015) have tried Bidirectional Long Short-Term Memory (LSTM) on Penn Treebank WSJ test set, and reported stateof-the-art performance.", "startOffset": 0, "endOffset": 19}, {"referenceID": 28, "context": "Qin (2015) has shown that RNN models outperform Majority Voting (MV) and HMM techniques for POS tagging of Chinese Buddhist text.", "startOffset": 0, "endOffset": 11}, {"referenceID": 28, "context": "Qin (2015) has shown that RNN models outperform Majority Voting (MV) and HMM techniques for POS tagging of Chinese Buddhist text. Zennaki et al. (2015) have used RNN for resource-poor languages and reported comparable results with state-of-the-art systems (Das and Petrov, 2011; Duong et al.", "startOffset": 0, "endOffset": 152}, {"referenceID": 34, "context": "Vyas et al. (2014) and Jamatia et al.", "startOffset": 0, "endOffset": 19}, {"referenceID": 16, "context": "(2014) and Jamatia et al. (2015) have worked on data labeling and automatic POS tagging of such data using various machine learning techniques.", "startOffset": 11, "endOffset": 33}, {"referenceID": 16, "context": "(2014) and Jamatia et al. (2015) have worked on data labeling and automatic POS tagging of such data using various machine learning techniques. Building further on that labeled data, Pimpale and Patel (2015) and, Sarkar (2015) have tried word embedding as an additional feature to the machine learning based classifiers for POS tagging.", "startOffset": 11, "endOffset": 208}, {"referenceID": 16, "context": "(2014) and Jamatia et al. (2015) have worked on data labeling and automatic POS tagging of such data using various machine learning techniques. Building further on that labeled data, Pimpale and Patel (2015) and, Sarkar (2015) have tried word embedding as an additional feature to the machine learning based classifiers for POS tagging.", "startOffset": 11, "endOffset": 227}, {"referenceID": 10, "context": "For this task, we used elaman (Elman, 1990), Long Short-Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997), Deep LSTM, Gated Recurrent Unit (GRU) (Cho et al.", "startOffset": 30, "endOffset": 43}, {"referenceID": 6, "context": "For this task, we used elaman (Elman, 1990), Long Short-Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997), Deep LSTM, Gated Recurrent Unit (GRU) (Cho et al., 2014), which are widely used RNN models in the NLP literature.", "startOffset": 149, "endOffset": 167}, {"referenceID": 17, "context": "Elman and Jordon (Jordan, 1986) networks are the simplest network in RNN family and are known as Simple RNN.", "startOffset": 17, "endOffset": 31}, {"referenceID": 2, "context": "Simple RNN also suffers from the problem of vanishing and exploding gradient (Bengio et al., 1994).", "startOffset": 77, "endOffset": 98}, {"referenceID": 13, "context": "Many variants of LSTM (Graves, 2013; Yao et al., 2014; Jozefowicz et al., 2015) have been tried in literature for the various tasks.", "startOffset": 22, "endOffset": 79}, {"referenceID": 40, "context": "Many variants of LSTM (Graves, 2013; Yao et al., 2014; Jozefowicz et al., 2015) have been tried in literature for the various tasks.", "startOffset": 22, "endOffset": 79}, {"referenceID": 18, "context": "Many variants of LSTM (Graves, 2013; Yao et al., 2014; Jozefowicz et al., 2015) have been tried in literature for the various tasks.", "startOffset": 22, "endOffset": 79}, {"referenceID": 1, "context": "All the models were implemented using 3THEANO framework (Bergstra et al., 2010; Bastien et al., 2012).", "startOffset": 56, "endOffset": 101}, {"referenceID": 38, "context": "We trained all the models using Truncated Back-Propagation-Through-Time (TBPTT) (Werbos, 1990) with the stochastic gradient descent.", "startOffset": 80, "endOffset": 94}, {"referenceID": 40, "context": "Standard values of hyperparameters were used for RNN model training, as suggested in the literature (Yao et al., 2014; Patel and Sasikumar, 2016).", "startOffset": 100, "endOffset": 145}, {"referenceID": 41, "context": "We trained each model for 50 epochs and used Ada-delta (Zeiler, 2012) to adapt the learning rate of each parameter automatically ( = 10\u22126 and \u03c1 = 0.", "startOffset": 55, "endOffset": 69}, {"referenceID": 4, "context": "Pre-training not only guides the learning towards minima with better generalization in non-convex optimization (Bengio, 2009; Erhan et al., 2010) but also improves the accuracy of the system (Kreutzer et al.", "startOffset": 111, "endOffset": 145}, {"referenceID": 11, "context": "Pre-training not only guides the learning towards minima with better generalization in non-convex optimization (Bengio, 2009; Erhan et al., 2010) but also improves the accuracy of the system (Kreutzer et al.", "startOffset": 111, "endOffset": 145}, {"referenceID": 19, "context": ", 2010) but also improves the accuracy of the system (Kreutzer et al., 2015; Patel and Sasikumar, 2016).", "startOffset": 53, "endOffset": 103}, {"referenceID": 7, "context": "GRU models are out-performing other models (Simple RNN, LSTM, Deep LSTM) for this task also as reported by Chung et al. (2014) for a suit of NLP tasks.", "startOffset": 107, "endOffset": 127}], "year": 2017, "abstractText": "This paper describes Centre for Development of Advanced Computing\u2019s (CDACM) submission to the shared task\u2019Tool Contest on POS tagging for CodeMixed Indian Social Media (Facebook, Twitter, and Whatsapp) Text\u2019, collocated with ICON-2016. The shared task was to predict Part of Speech (POS) tag at word level for a given text. The codemixed text is generated mostly on social media by multilingual users. The presence of the multilingual words, transliterations, and spelling variations make such content linguistically complex. In this paper, we propose an approach to POS tag code-mixed social media text using Recurrent Neural Network Language Model (RNN-LM) architecture. We submitted the results for Hindi-English (hi-en), BengaliEnglish (bn-en), and Telugu-English (teen) code-mixed data.", "creator": "TeX"}}}