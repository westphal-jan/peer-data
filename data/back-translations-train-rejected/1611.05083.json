{"id": "1611.05083", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Nov-2016", "title": "Probabilistic Failure Analysis in Model Validation & Verification", "abstract": "Automated fault localization is an important issue in model validation and verification. It helps the end users in analyzing the origin of failure. In this work, we show the early experiments with probabilistic analysis approaches in fault localization. Inspired by the Kullback-Leibler Divergence from Bayesian probabilistic theory, we propose a suspiciousness factor to compute the fault contribution for the transitions in the reachability graph of model checking, using which to rank the potential faulty transitions. To automatically locate design faults in the simulation model of detailed design, we propose to use the statistical model Hidden Markov Model (HMM), which provides statistically identical information to component's real behavior. The core of this method is a fault localization algorithm that gives out the set of suspicious ranked faulty components and a backward algorithm that computes the matching degree between the HMM and the simulation model to evaluate the confidence degree of the localization conclusion.", "histories": [["v1", "Tue, 15 Nov 2016 22:33:48 GMT  (547kb,D)", "https://arxiv.org/abs/1611.05083v1", null], ["v2", "Fri, 18 Nov 2016 11:37:29 GMT  (612kb,D)", "http://arxiv.org/abs/1611.05083v2", "In International Conference on Embedded Real Time Software and Systems (ERTS 2014)"]], "reviews": [], "SUBJECTS": "cs.SE cs.LG", "authors": ["ning ge", "marc pantel", "xavier cr\\'egut"], "accepted": false, "id": "1611.05083"}, "pdf": {"name": "1611.05083.pdf", "metadata": {"source": "CRF", "title": "Probabilistic Failure Analysis in Model Validation & Verification", "authors": ["Ning Ge", "Marc Pantel", "Xavier Cr\u00e9gut"], "emails": ["Xavier.Cregut}@enseeiht.fr"], "sections": [{"heading": null, "text": "Probabilistic Failure Analysis in Model Validation & VerificationNing Ge, Marc Pantel, and Xavier Cre \u00b2 gutUniversity of Toulouse, IRIT / INPT {Ning.Ge | Marc.Pantel | Xavier.Cregut} @ enseeiht.frKeywords: error localization, model verification, verification, validation, probabilistic analysis, hidden markov model"}, {"heading": "1 Introduction", "text": "This year it is more than ever before in the history of the city."}, {"heading": "2 Probabilistic Failure Analysis in Model Checking", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Problem Statements", "text": "A counter-example of a formula being violated is a key service provided by model checkers. Counter-examples produced by model checkers often stand for error trackers, which represent sequences of system states and transitions and are therefore usually tedious and difficult to understand. The origin of the error may lie somewhere along these tracks, requiring lengthy analysis by designers. Our ultimate goal is to provide end users with the suspicious rank of defective elements."}, {"heading": "2.2 Preliminaries", "text": "Accessibility Diagram & Infringement States Accessibility Diagrams are used to solve accessibility problems in the model check. They contain all the states during the execution of a system and all transitions between those states. If a security feature is not met, there are violation states in the accessibility diagram. Finding all the violation states in the accessibility diagram is the first step of error localization. Error contribution of the Transition Definition 1 (Error Tracking). Error contribution (KF) is a suspicion factor for assessing the suspected level of a transition. It is used to evaluate the suspicion of transitions. Error tracesDefinition 2 (Error Tracking). For all states that are on the way from a starting state s0 to an infringement state sv in the accessibility diagram, all outgoing transitions from si to transitions are error tracesDefinition.We do not just look at the transitions on the Sv path to the Sv."}, {"heading": "2.3 Kullback-Lerbler Divergence", "text": "The KL divergence is a measure in statistics that quantifies in bits how close a probability distribution P = {pi} is to a model (or candidate) of the distribution Q = {qi}. The KL divergence of Q via a discrete random variable is defined as asDKL (P-Q) = [P] ln P (i) ln P (i) Q (i) (1) Note: In the above definition 0 ln 00 = 0 ln 0 q = 0, and p ln p 0 =."}, {"heading": "2.4 Ranking Suspicious Faulty Transitions", "text": "Inspired by the TF-IDF algorithm, we propose a probabilistic approach to error localization based on the Kullback-Leibler divergence. A relevance weight CF (t) is calculated to evaluate the contribution of a transition t to error traces leading to injury states, and thus its contribution to the error. In the TF-IDF algorithm, each term in the documents contributes to the semantics of keywords. Some terms are considered significant if they are more relevant to the semantics of keywords. This is similar to the error contribution caused by a given transition in an error track in the model verification. Fig. 3 compares the similarity between the semantic contribution of terms in documents and the error contribution of transitions in error traces.Some terms in documents exhibit a closer semantic relationship to the keywords, the occurrence of these terms depends more on TF's contribution to the transitus trace of documents, the TF's contribution to the transitus trace is measured by the topsitus trace of documents."}, {"heading": "2.5 Experimental Results", "text": "We evaluate our approach based on two significant criteria: effectiveness and efficiency. According to the survey [25], effectiveness can be assessed by a score EXAM in terms of the percentage of statements that need to be verified until the first statement containing the error is reached [6,26]. The error localization techniques in model verification, like other techniques, should be terminated promptly, limited by some resource limitations. Efficiency can be assessed by scalability and performance.Automated test bed The test bed randomly generates systems that may be at a standstill, then applies the proposed analysis algorithm and checks whether it detects the downtime that has been introduced. We use Time Petri Net to model the behavior of the system.For a particular TPN system S (P, R, M), P are the processes that run indefinitely and require a resource before the next task (a task is represented by a transition)."}, {"heading": "1 400 4949 / 15440 2.9092", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2 517 2428 / 7130 1.1244", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3 500 9884 / 31237 3.3533", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4 402 8811 / 26663 2.5998", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5 303 6756 / 18247 1.2196", "text": "The effectiveness rating is given in Table 2. We give the test score, the deviation of the test result, the rank and the deviation for the best and worst cases and then show the average test score and the average rank. The test value varies between 2% and 13% for the best cases and between 4% and 18% for the worst cases. On average, the test value varies between 3% and 16%, giving the marginal results from 1 to 8. Stability is represented by the deviation result. These experimental results show that our approach is effective."}, {"heading": "3 Probabilistic Failure Analysis in Simulation", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Background", "text": "The error localization algorithms usually follow two paradigms: cause-effect analysis and effect-cause analysis. Cause-effect analysis [24,27,18] assumes possible causes (error models). A simulator is used to predict the behavior of the system in the presence of various errors. Subsequently, predictions are compared with the observed behavior. Cause-effect analysis [1,23] establishes a faulty localization based on the observed behavior and expected good functions. Faulty causes are traced back from the identified suspicious components [8,7,9]. The component can be hardware, software modules or function blocks in the system. This method combines a forward localization analysis and an evaluation of the degree of backward confidence. HMM as an abstraction of a component provides statistically identical information about the components that determine the actual degree of error localization between the core components of the behavior."}, {"heading": "3.2 HMM Modeling and Analysis", "text": "A HMM is defined as a statistical model used to represent stochastic processes in which the states are not directly observed. A basic HMM can be described as follows: - N: Number of states- M: Number of observations - MI: initial probability distribution; N: i = 1 MI (i) = 1 - MT: probability distribution of the transitions from states to states; N: j = 1 MT (i, j) = 1, i = 1... N- ME: emission distribution for the observations associated with states; M: j = 1 ME (i, j) = 1, i = 1... example 3 (HMM example). A two-state HMM example, which abstracts the functional limitations of a system, is given by Figure 4, in which the system has two states Healthy (H) and faulty (F) observations (M) representing the system MM, which violate the functional limitations (R) or (V)."}, {"heading": "3.3 Automated Fault Localization Based on Hidden Markov Model", "text": "Each component is mapped to an HMM level. If all the input / output pairs of a component cannot be fully listed, we can get an exact distribution of how that component respects the functional constraints. This approach can be explained by behaving the same way statistically. To measure whether they behave the same way, we introduce an evaluation approach by using test results. The returned evaluation metrics are used to check the parameters in HMM until they approximate the real behavior of the component. System states A component C is mapped to an HMM level."}, {"heading": "3.4 Estimating System\u2019s Behavior & Locating Fault", "text": "If the HMM is confirmed with a high matching level and a high confidence level, we can estimate the status of the component. On the basis of the states calculated from the function states Get Behaviour States (h, Tm, n), for example, a state sequence is derived for which an example is given (FIp, FIp, PIf, FIp,... FIf, FIp,...). In the state behavior sequence, we focus on the status of the component, which is represented as follows: (F, F, P, F..., F, F,...). The status with higher occurrence probability is confirmed as an incorrect status of this component. Trust of this result is guaranteed by the confidence level (\u03c1), and the similarity of the HMM with the component is guaranteed by the matching level (\u00b5)."}, {"heading": "3.5 Experimental Results", "text": "We design a specific test bed to assess the accuracy and efficiency of the method by generating a large number of use cases. Each use case includes: the system architecture that defines the components and ports and their associations; the likelihood of each component failing; the function specification that corresponds to the input / output value. However, the method assumes that every component in the system has a chance of failing if it has design problems; this probability will be zero if no design error is assumed for that component. All function limitations are based on the input / output value itself, and to simplify, they are all bounds that limit the min / max value of the input / output output. If a faulty component exists, the test bed randomly provides an out-of-range value for that component, and to simplify, they are all bounds that limit the system fails to what the model is."}, {"heading": "4 Conclusion", "text": "Inspired by the Kullback-Leibler divergence from Bayian probability theory, we propose a suspicion factor to calculate the error contribution for the transitions in the accessibility curve of the model check. Potential faulty transitions are then classified according to this suspicion factor. To automatically locate design errors in the detailed design simulation model, we propose to use the Hidden Markov Model (HMM) statistical model. HMM provides statistically identical information about the real behavior of the component as an abstraction of a component. At the core of this method is a fault localization process algorithm that outputs the set of suspicious faulty components and a reverse algorithm that calculates the appropriate degree between the HMM and the simulation model to evaluate the degree of reliability of the localization."}, {"heading": "Acknowledgment", "text": "This work was financed by the FUI Projet P and EuroStars HiMoCo projects."}], "references": [{"title": "Multiple fault diagnosis in combinational circuits based on an effect-cause analysis", "author": ["M. Abramovici", "M.A. Breuer"], "venue": "Computers, IEEE Transactions on 100(6), 451\u2013 460", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1980}, {"title": "Distributional clustering of words for text classification", "author": ["L.D. Baker", "A.K. McCallum"], "venue": "Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval. pp. 96\u2013103. ACM", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1998}, {"title": "From symptom to cause: localizing errors in counterexample traces", "author": ["T. Ball", "M. Naik", "S.K. Rajamani"], "venue": "ACM SIGPLAN Notices 38(1), 97\u2013105", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2003}, {"title": "Explaining abstract counterexamples", "author": ["S. Chaki", "A. Groce", "O. Strichman"], "venue": "ACM SIGSOFT Software Engineering Notes. pp. 73\u201382. ACM", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2004}, {"title": "System deadlocks", "author": ["E.G. Coffman", "M. Elphick", "A. Shoshani"], "venue": "ACM Computing Surveys (CSUR) 3(2), 67\u201378", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1971}, {"title": "A family of code coverage-based heuristics for effective fault localization", "author": ["W. Eric Wong", "V. Debroy", "B. Choi"], "venue": "Journal of Systems and Software 83(2), 188\u2013208", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2010}, {"title": "Efficient online analysis of accidental fault localization for dynamic systems using hidden markov model", "author": ["N. Ge", "S. Nakajima", "M. Pantel"], "venue": "Proceedings of the Symposium on Theory of Modeling & Simulation-DEVS Integrative M&S Symposium. p. 16. Society for Computer Simulation International", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2013}, {"title": "Hidden markov model based automated fault localization for integration testing", "author": ["N. Ge", "S. Nakajima", "M. Pantel"], "venue": "Software Engineering and Service Science (ICSESS), 2013 4th IEEE International Conference on. pp. 184\u2013187. IEEE", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2013}, {"title": "Online diagnosis of accidental faults for realtime embedded systems using a hidden markov model", "author": ["N. Ge", "S. Nakajima", "M. Pantel"], "venue": "Simulation 91(10), 851\u2013868", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2015}, {"title": "Time properties verification framework for uml-marte safety critical real-time systems", "author": ["N. Ge", "M. Pantel"], "venue": "European Conference on Modelling Foundations and Applications. pp. 352\u2013367. Springer", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2012}, {"title": "Real-time property specific reduction for time petri net", "author": ["N. Ge", "M. Pantel"], "venue": "International Workshop on Petri Nets and Software Engineering (PNSE@PetriNets). pp. 165\u2013179", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2014}, {"title": "Formal specification and verification of task time constraints for real-time systems", "author": ["N. Ge", "M. Pantel", "X. Cr\u00e9gut"], "venue": "International Symposium On Leveraging Applications of Formal Methods, Verification and Validation. pp. 143\u2013157. Springer", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2012}, {"title": "Time properties dedicated transformation from uml-marte activity to time transition system", "author": ["N. Ge", "M. Pantel", "X. Cr\u00e9gut"], "venue": "ACM SIGSOFT Software Engineering Notes 37(4), 1\u20138", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2012}, {"title": "Automated failure analysis in model checking based on data mining", "author": ["N. Ge", "M. Pantel", "X. Cr\u00e9gut"], "venue": "International Conference on Model and Data Engineering. pp. 13\u201328. Springer", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2014}, {"title": "A uml-marte temporal property verification tool based on model checking", "author": ["N. Ge", "M. Pantel", "X. Cr\u00e9gut"], "venue": "International Conference on Embedded Real Time Software and Systems (ERTS)", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2014}, {"title": "Error explanation with distance metrics", "author": ["A. Groce"], "venue": "Tools and Algorithms for the Construction and Analysis of Systems pp. 108\u2013122", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2004}, {"title": "What went wrong: Explaining counterexamples", "author": ["A. Groce", "W. Visser"], "venue": "Model Checking Software, pp. 121\u2013136. Springer", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2003}, {"title": "Finding and fixing faults", "author": ["B. Jobstmann", "S. Staber", "A. Griesmayer", "R. Bloem"], "venue": "Journal of Computer and System Sciences 78(2), 441\u2013460", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2012}, {"title": "A statistical interpretation of term specificity and its application in retrieval", "author": ["K.S. Jones"], "venue": "Journal of documentation 28(1), 11\u201321", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1972}, {"title": "Cause clue clauses: error localization using maximum satisfiability", "author": ["M. Jose", "R. Majumdar"], "venue": "ACM SIGPLAN Notices. vol. 46, pp. 437\u2013446. ACM", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2011}, {"title": "On information and sufficiency", "author": ["S. Kullback", "R.A. Leibler"], "venue": "The Annals of Mathematical Statistics 22(1), 79\u201386", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1951}, {"title": "A tutorial on hidden markov models and selected applications in speech recognition", "author": ["L.R. Rabiner"], "venue": "Proceedings of the IEEE 77(2), 257\u2013286", "citeRegEx": "22", "shortCiteRegEx": null, "year": 1989}, {"title": "Fault diagnosis and logic debugging using boolean satisfiability", "author": ["A. Smith", "A. Veneris", "M.F. Ali", "A. Viglas"], "venue": "Computer-Aided Design of Integrated Circuits and Systems, IEEE Transactions on 24(10), 1606\u20131621", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2005}, {"title": "On diagnosing multiple stuck-at faults using multiple and single fault simulation in combinational circuits", "author": ["H. Takahashi", "K.O. Boateng", "K.K. Saluja", "Y. Takamatsu"], "venue": "Computer-Aided Design of Integrated Circuits and Systems, IEEE Transactions on 21(3), 362\u2013368", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2002}, {"title": "A survey of software fault localization", "author": ["W.E. Wong", "V. Debroy"], "venue": "University of Texas at Dallas, Tech. Rep. UTDCS-45-09", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2009}, {"title": "Bp neural network-based effective fault localization", "author": ["W.E. Wong", "Y. Qi"], "venue": "International Journal of Software Engineering and Knowledge Engineering 19(04), 573\u2013597", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2009}, {"title": "Isolating cause-effect chains from computer programs", "author": ["A. Zeller"], "venue": "Proceedings of the 10th ACM SIGSOFT symposium on Foundations of software engineering. pp. 1\u201310. ACM", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2002}], "referenceMentions": [{"referenceID": 4, "context": "The execution time is [5,10] for A, and [3,7] for B.", "startOffset": 22, "endOffset": 28}, {"referenceID": 9, "context": "The execution time is [5,10] for A, and [3,7] for B.", "startOffset": 22, "endOffset": 28}, {"referenceID": 2, "context": "The execution time is [5,10] for A, and [3,7] for B.", "startOffset": 40, "endOffset": 45}, {"referenceID": 6, "context": "The execution time is [5,10] for A, and [3,7] for B.", "startOffset": 40, "endOffset": 45}, {"referenceID": 7, "context": "To remove this violation, we can either replace the time constraint of A by [8,10], or replace the time constraint of B by [3,4].", "startOffset": 76, "endOffset": 82}, {"referenceID": 9, "context": "To remove this violation, we can either replace the time constraint of A by [8,10], or replace the time constraint of B by [3,4].", "startOffset": 76, "endOffset": 82}, {"referenceID": 2, "context": "To remove this violation, we can either replace the time constraint of A by [8,10], or replace the time constraint of B by [3,4].", "startOffset": 123, "endOffset": 128}, {"referenceID": 3, "context": "To remove this violation, we can either replace the time constraint of A by [8,10], or replace the time constraint of B by [3,4].", "startOffset": 123, "endOffset": 128}, {"referenceID": 2, "context": "the best case execution time (BCET) of B is 5, then the time constraint of B cannot anymore be replaced by [3,4], thus the suspicion of B is largely decreased.", "startOffset": 107, "endOffset": 112}, {"referenceID": 3, "context": "the best case execution time (BCET) of B is 5, then the time constraint of B cannot anymore be replaced by [3,4], thus the suspicion of B is largely decreased.", "startOffset": 107, "endOffset": 112}, {"referenceID": 2, "context": "Proposed Approach Existing automated fault localization techniques in model checking usually produce a set of suspicious statements without any particular ranking [3,17,16,4,20].", "startOffset": 163, "endOffset": 177}, {"referenceID": 16, "context": "Proposed Approach Existing automated fault localization techniques in model checking usually produce a set of suspicious statements without any particular ranking [3,17,16,4,20].", "startOffset": 163, "endOffset": 177}, {"referenceID": 15, "context": "Proposed Approach Existing automated fault localization techniques in model checking usually produce a set of suspicious statements without any particular ranking [3,17,16,4,20].", "startOffset": 163, "endOffset": 177}, {"referenceID": 3, "context": "Proposed Approach Existing automated fault localization techniques in model checking usually produce a set of suspicious statements without any particular ranking [3,17,16,4,20].", "startOffset": 163, "endOffset": 177}, {"referenceID": 19, "context": "Proposed Approach Existing automated fault localization techniques in model checking usually produce a set of suspicious statements without any particular ranking [3,17,16,4,20].", "startOffset": 163, "endOffset": 177}, {"referenceID": 13, "context": "Our approach will improve the effectiveness of fault localization by providing a suspiciousness factor which is used to rank the suspicious transitions in the verification model [14].", "startOffset": 178, "endOffset": 182}, {"referenceID": 9, "context": "This approach has been applied in our formal verification framework of UML-MARTE designs dedicated to the real-time properties [10,12,13,11,15].", "startOffset": 127, "endOffset": 143}, {"referenceID": 11, "context": "This approach has been applied in our formal verification framework of UML-MARTE designs dedicated to the real-time properties [10,12,13,11,15].", "startOffset": 127, "endOffset": 143}, {"referenceID": 12, "context": "This approach has been applied in our formal verification framework of UML-MARTE designs dedicated to the real-time properties [10,12,13,11,15].", "startOffset": 127, "endOffset": 143}, {"referenceID": 10, "context": "This approach has been applied in our formal verification framework of UML-MARTE designs dedicated to the real-time properties [10,12,13,11,15].", "startOffset": 127, "endOffset": 143}, {"referenceID": 14, "context": "This approach has been applied in our formal verification framework of UML-MARTE designs dedicated to the real-time properties [10,12,13,11,15].", "startOffset": 127, "endOffset": 143}, {"referenceID": 20, "context": "Kullback-Leibler Divergence (also called information divergence, information gain, relative entropy) [21] is a fundamental equation of information theory that qualifies the proximity of two probability distributions.", "startOffset": 101, "endOffset": 105}, {"referenceID": 1, "context": "We give an example of its application to text classification [2].", "startOffset": 61, "endOffset": 64}, {"referenceID": 18, "context": "Another major application is the TF-IDF (Term Frequency - Inverse Document Frequency) algorithm [19].", "startOffset": 96, "endOffset": 100}, {"referenceID": 24, "context": "According to the survey [25], the effectiveness can be assessed by a score EXAM in terms of the percentage of statements that have to be examined until the first statement containing the fault is reached [6,26].", "startOffset": 24, "endOffset": 28}, {"referenceID": 5, "context": "According to the survey [25], the effectiveness can be assessed by a score EXAM in terms of the percentage of statements that have to be examined until the first statement containing the fault is reached [6,26].", "startOffset": 204, "endOffset": 210}, {"referenceID": 25, "context": "According to the survey [25], the effectiveness can be assessed by a score EXAM in terms of the percentage of statements that have to be examined until the first statement containing the fault is reached [6,26].", "startOffset": 204, "endOffset": 210}, {"referenceID": 4, "context": "Coffman identified four conditions that must hold simultaneously in order to have a deadlock [5].", "startOffset": 93, "endOffset": 96}, {"referenceID": 23, "context": "Cause-effect analysis [24,27,18] starts from possible causes", "startOffset": 22, "endOffset": 32}, {"referenceID": 26, "context": "Cause-effect analysis [24,27,18] starts from possible causes", "startOffset": 22, "endOffset": 32}, {"referenceID": 17, "context": "Cause-effect analysis [24,27,18] starts from possible causes", "startOffset": 22, "endOffset": 32}, {"referenceID": 0, "context": "Effect-cause analysis [1,23] reasons faulty localization based on observed behavior and expected good functions.", "startOffset": 22, "endOffset": 28}, {"referenceID": 22, "context": "Effect-cause analysis [1,23] reasons faulty localization based on observed behavior and expected good functions.", "startOffset": 22, "endOffset": 28}, {"referenceID": 21, "context": "In this work, we make a trade-off of cause-effect and effect-cause analyses and propose an Hidden Markov Model (HMM) [22] based approach for the automated localization of faulty components in the simulation [8,7,9].", "startOffset": 117, "endOffset": 121}, {"referenceID": 7, "context": "In this work, we make a trade-off of cause-effect and effect-cause analyses and propose an Hidden Markov Model (HMM) [22] based approach for the automated localization of faulty components in the simulation [8,7,9].", "startOffset": 207, "endOffset": 214}, {"referenceID": 6, "context": "In this work, we make a trade-off of cause-effect and effect-cause analyses and propose an Hidden Markov Model (HMM) [22] based approach for the automated localization of faulty components in the simulation [8,7,9].", "startOffset": 207, "endOffset": 214}, {"referenceID": 8, "context": "In this work, we make a trade-off of cause-effect and effect-cause analyses and propose an Hidden Markov Model (HMM) [22] based approach for the automated localization of faulty components in the simulation [8,7,9].", "startOffset": 207, "endOffset": 214}], "year": 2016, "abstractText": "ion Issue Fault localization in model checking is challenging as the models have usually a concurrent and indeterministic behavior with many possible execution traces. This behavior is due to the use of abstraction in their design. Without precise information, fault localization may not be precise enough. Given a sequential, or synchronized concurrent, program which exhibits less execution traces, various debugging methods are available to detect and locate the faulty statements. In model-based diagnosis, the use of abstraction is mandatory to reduce the state space explosion problem. At the time of writing, the conflict between model precision and verification cost is a key issue in model checking and model-driven engineering (MDE), therefore a compromise is made to remove the unnecessary information for some verification purpose while keeping all the property-related information. This usually leads to model with concurrent and indeterministic behaviors that exhibits a much larger number of execution traces and are consequently much more complicated to debug. Fault Localization Issue Sometimes it is difficult, even for seasoned experts, to analyze the fault origin. We take a simple example (see Ex. 1) to illustrate this issue. Example 1 (Fault Localization Example). Assume a system consists of two concurrent processes A and B. Both execute only once. The execution time is [5,10] for A, and [3,7] for B. The expected temporal property P is Always A After B. It is obvious that P is unsatisfied. The design fault occurs either on A or on B. To remove this violation, we can either replace the time constraint of A by [8,10], or replace the time constraint of B by [3,4]. However, without extra information, A and B exhibit the same suspicion. If an extra information is available, e.g. the best case execution time (BCET) of B is 5, then the time constraint of B cannot anymore be replaced by [3,4], thus the suspicion of B is largely decreased. This example is simple enough to be analyzed manually, while it is impossible for more complex system with thousands of transitions. Any modification on a transition may impact the verification result through time constraint propagation. Proposed Approach Existing automated fault localization techniques in model checking usually produce a set of suspicious statements without any particular ranking [3,17,16,4,20]. Our approach will improve the effectiveness of fault localization by providing a suspiciousness factor which is used to rank the suspicious transitions in the verification model [14]. The suspiciousness factor is computed using the fault contribution of each transition on the error traces derived from the reachability graph. This approach has been applied in our formal verification framework of UML-MARTE designs dedicated to the real-time properties [10,12,13,11,15].", "creator": "LaTeX with hyperref package"}}}