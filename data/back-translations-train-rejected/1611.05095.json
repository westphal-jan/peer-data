{"id": "1611.05095", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Nov-2016", "title": "Learning Dexterous Manipulation Policies from Experience and Imitation", "abstract": "We explore learning-based approaches for feedback control of a dexterous five-finger hand performing non-prehensile manipulation. First, we learn local controllers that are able to perform the task starting at a predefined initial state. These controllers are constructed using trajectory optimization with respect to locally-linear time-varying models learned directly from sensor data. In some cases, we initialize the optimizer with human demonstrations collected via teleoperation in a virtual environment. We demonstrate that such controllers can perform the task robustly, both in simulation and on the physical platform, for a limited range of initial conditions around the trained starting state. We then consider two interpolation methods for generalizing to a wider range of initial conditions: deep learning, and nearest neighbors. We find that nearest neighbors achieve higher performance. Nevertheless, the neural network has its advantages: it uses only tactile and proprioceptive feedback but no visual feedback about the object (i.e. it performs the task blind) and learns a time-invariant policy. In contrast, the nearest neighbors method switches between time-varying local controllers based on the proximity of initial object states sensed via motion capture. While both generalization methods leave room for improvement, our work shows that (i) local trajectory-based controllers for complex non-prehensile manipulation tasks can be constructed from surprisingly small amounts of training data, and (ii) collections of such controllers can be interpolated to form more global controllers. Results are summarized in the supplementary video:", "histories": [["v1", "Tue, 15 Nov 2016 23:31:40 GMT  (1467kb,D)", "http://arxiv.org/abs/1611.05095v1", "Initial draft for a journal submission"]], "COMMENTS": "Initial draft for a journal submission", "reviews": [], "SUBJECTS": "cs.LG cs.RO cs.SY", "authors": ["vikash kumar", "abhishek gupta", "emanuel todorov", "sergey levine"], "accepted": false, "id": "1611.05095"}, "pdf": {"name": "1611.05095.pdf", "metadata": {"source": "META", "title": "Learning Dexterous Manipulation Policies from Experience and Imitation", "authors": ["Vikash Kumar", "Abhishek Gupta", "Emanuel Todorov", "Sergey Levine"], "emails": [], "sections": [{"heading": null, "text": "Tags Reinforcement Learning, Skillful Manipulation, Trajectory Optimization"}, {"heading": "1 Introduction", "text": "This year, it has come to the point where one feels able to go to a place where one is able to go to another world, where one is able to create a new world."}, {"heading": "2 Related Work", "text": "Although robot learning has made significant progress in recent years, with successful results in areas ranging from escape (Abbeel, Coates, Quigley, and Ng 2006) to locomotion (Tedrake, Zhang, and Seung 2004) to manipulation (Peters, Muzzling, and Altun 2010 0a; Theodorou, Buchli, and Schaal 2010; Peters and Schaal 2008), comparatively few methods have been used to control dexterous hands. (van Hoof, Hermans, Neumann, and Peters 2015) report on simple manipulation with a three-finger hand, and our work reports on learning simple hand manipulation skills, such as turning a cylinder with time-varying gausser controllers (Kumar, Todorov, and Levine 2016)."}, {"heading": "3 Overview", "text": "The ADROIT platform, which serves as an experimental platform for all of our external manipulation experiments, is described in detail in Section 4. This system is used in three groups of experiments: the first set of experiments examines external manipulation capabilities from the ground up, with the second group of experiments focusing on complex capabilities to gain a single generalizable amplification and learn from the demonstration, and the third set of experiments examines how different approaches, including the closest neural networks, can be used to learn from multiple learned behaviors that have a single generalizable ability that is successful under different conditions."}, {"heading": "4 System", "text": "Modularity and the simple circuitry of robotic platforms were the overarching philosophy of our system design. The learning algorithm (algorithm 1) is independent of the selected robotic platform, with the exception of Step 3, where the guidelines are sent to the robotic platform for evaluation and the resulting execution paths are recorded. In this way, the training can take place either locally (on the machine controlling the robot) or remotely (if more computing power is required). Manipulation strategies have been studied for two different platforms, which are described in detail below."}, {"heading": "4.1 Hardware Platform", "text": "The ADROIT platform is an anthropomorphic arm-hand system operated by means of a tailor-made high-performance pneumatic actuator. It consists of a 24 dof hand and a 4 dof arm. As our motivation here is to understand finger and wrist-based manipulation strategies, we mounted the 24 dof hand on a fixed base to promote finger-centric behavior. A fixed base severely limits the working space of the overall system, but makes the system accessible only to finger and wrist-based manipulation strategies. For this work, we use the term \"ADROIT\" to refer to a fixed base with 24 dof hand. 20 of the 24-hand joints are operated independently, with the DIP joints equipped with 40 antagonistic tendons."}, {"heading": "4.2 Simulation Platform", "text": "We model the ADROIT hand, including antagonistic tendon transmission, joint coupling and pressure dynamics, with the MuJoCo simulator we developed (Todorov, Erez and Tassa 2012).The pressure dynamics are implemented by extending the standard actuation model with user callbacks. Simulating a 5 s track in 2 ms time span takes about 0.7 s CPU time, or about 0.3 ms CPU time per simulation step, including evaluating the feedback control law (which needs to be interpolated because the orbital accelerator uses 50 ms time steps) and advancing physics simulation. A fast simulator allows us to prototype candidate learning algorithms and cost function designs and quickly evaluate them before testing them on the hardware. Apart from being able to run much faster than the real-time simulation, the initial state of the system must be automatically calibrated (however, the actual state of the system has to be adjusted)."}, {"heading": "5 Reinforcement Learning with Local Linear Models", "text": "In this section we describe the reinforcement learning algorithm (summarized in algorithm 1) that we use to control our pneumatically driven five-finger hand. Derivative in this section follows previous work (Levine and Abbeel 2014), but we describe the algorithm in this algorithm 1 RL with linear-Gaussian controllers 1: initialize p (ut | xt) 2: for iteration k = 1 to K do 3: execute p (ut | xt) to collect trajectory samples 4: adjust the dynamics p (xt + 1 | xt, ut) to {\u03c4j} using linear regression with GMM before 5: adjust p = argminpEp (\u03c4) ['s) s.t. DKL (p (halt), p (halt), p (p), final forsection for completeness. The aim of the method is to learn a time-varying linear-Gaussian control of the shape (p)."}, {"heading": "5.1 Optimizing Linear-Gaussian Controllers", "text": "The simple structure of contemporary linear ghost staging is a very efficient method, which is what it's about: \"It's about the question to what extent people are able to understand the world,\" he says. \"It's about the question to what extent people are able to understand the world.\" \"It's about the question to what extent people are able to understand the world,\" he says. \"It's about the question to what extent they're able to understand the world.\" \"\" It's about the question to what extent they're able to change the world. \"\" \"It's about to what extent they're able to change the world.\" \"\" It's about to what the world is. \"\" It's about changing the world, \"it's about changing the world.\" \"It's about.\" It's about. \"It's about.\" It's about changing the world. \"It's about changing the world.\" It's about changing the world. \"It's about changing the world.\" It's about changing the world. \"It's about changing the world.\" It's about changing the world. \"It's about changing the world.\" It's about changing the world. \"It's about changing the world.\""}, {"heading": "5.2 KL-Constrained Optimization", "text": "For this learning method to produce good results (Bagnell and Schneider 2003; Peters and Peters, 2008 and 2008), it is important to bind the change in the control p (ut | xt) to each iteration (ut | xt). The standard iterative LQR method can drastically adjust the control to each iteration, which can lead to unreliable progress. To solve these problems, we solve the following optimization problem at each iteration: min p (ut | xt) Ep (ell) [t). (p). (p). (p). (p). (p). (p). (p). (p)."}, {"heading": "6 Learning Policies from Experience", "text": "In this section, we will describe our initial experiments in which we learn from the ground up, using the path-centered amplification learning algorithm in the previous section, both on the physical platform and on the simulated ADROIT. Experiments in this section aim to determine whether we can fully learn complex manipulation behaviors from scratch, using only high-level task definitions provided in relation to a cost function, with the controller being learned at the level of valve opening and closing commands.The individual tasks are described in Tables 1 and 2 and are shown in the accompanying video."}, {"heading": "6.1 Hand Behaviors", "text": "In the first group of tasks, we will examine how well trajector-centric reinforcement learning can control the hand to reach target positions. The state space is given by x = (q, q, q, a). Here, q denotes the vector of the wrist angles, q is the vector of the joint angular velocities, a vector of cylinder pressure, and the actions ut correspond to the command signals of the valve, which correspond to real values and the degree to which each valve is opened at each time step. The tasks in this section require that the hand be moved from a given starting position into a certain pose. We arranged the poses so that the finger movements in one task were supported by gravity, and in another task they had to overcome gravity, as shown in Figure 4. Note that for a system of this complexity, even achieving a desired pose can be challenging, especially as the visual actors must maintain the aggregator-antagonist position and balance of the respective forces."}, {"heading": "6.2 Object Manipulation Behaviors", "text": "quot, quot, quot, quot, quot, quot, quot, quot, quot, quot, quot, quot, quot, quot, quot, quot, quot, quot, quot, quot, quot, quot, quot, quot, quot, quot, quot, quot, quot, quot, quot, quot, quot, quot, quot, quot, quot, quot, quot, quot, quot, quot, quot, quot, quot, quot, quot, quot, quot, quot, quot, quot, quot, quot, quot, quot, quot, quot, quot, quot, quot, quot, quot, quot, quot, quot, quot, quot, quot, quot, quot, quot, quot, quot, quot, quot, quot, quot, quot, quot, quot, quot, quot, quot, quot, quot, quot, quot, quot, quot, quot, quot, quot, quot, quot, quot, quot, quot, quot, quot, quot, quot, quot, quot, quot, quot."}, {"heading": "6.3 Results", "text": "The training consisted of about 15 iterations. In each iteration, we performed 5 experiments with different instances of exploratory noise in the controls. The progress of the training as well as the final performance is shown in the video accompanying the template and in the figure at the beginning of the paper. Here, we quantify the performance and robustness to noise. Figure 5 shows how the total cost of motion (measured by the cost functions defined above) has decreased over the course of the iterations of the algorithm. The solid curves are data from the physical system. Note that we observe a very fast convergence across all tasks and task variations. Surprisingly, the manipulation task, which is much more difficult from the point of view of the control, takes approximately the same number of iterations to learn. In the positioning task, we also performed a systematic comparison between learning in the physical system and learning in the simulation."}, {"heading": "6.4 Delayed Robustification", "text": "Finally, we used the simulation platform to quantify the robustness of disturbances in the manipulation task, to quantify how resilient our controllers are to changes in the initial state (remember that the controllers are local), and to see if training with loud initial states, in addition to the exploration noise injected into the controllers, will lead to more robust controllers. We refer to this strategy as delayed robustness in each iteration of the algorithm (algorithm 1). The results of these simulations are shown in Figure 7. We represent the orientation of the object around the vertical axis as a function of time (the black curve is the undisturbed trajectory)."}, {"heading": "7 Learning Policies from Experience and Imitation", "text": "In the previous section, the strengths of our amplification learning algorithm were highlighted, which is outlined in Section 5 by synthesizing the details of skillful manipulation strategies while maintaining the efficiency of the samples. However, as our algorithm makes progress by optimizing a square approximation of costs via a local approximation of dynamics, it can get stuck in local minimums if the approximation of the learned dynamics is not sufficiently accurate or if the costs are not convex. In principle, arbitrary precision can be achieved by increasing the number of gaussian cores used for dynamics and by increasing the number of trajectory samples N used to adjust the dynamics. In practice, collecting an arbitrary number of samples is not feasible due to time and inevitable limitations, especially when physical robots are involved. Furthermore, many useful targets are not convex, and the method cannot conform to optimize, while non-conforming functions such as exlocal experiments may need to optimize."}, {"heading": "7.1 Task Details", "text": "The task in this second set of experiments is to remove an elongated tube from the table. This task is difficult because the cost depends on the final configuration of the tube, which depends discontinuously on the positions of the fingers. In addition, gripping the tube from different starting positions requires the use of several different strategies. Note that the hand is rigid, so that the picking must be done entirely by moving the wrist and fingers. Additionally, the shape of the tube complicates the task. Strategies such as aligning the hand to the main axis of the object and the force closure will not work well, as only the wrist is available for the positioning of the hand. Instead, the hand must use complex finger grips to maneuver the object into position. The considerable weight of the tube (compared to the lifting capacity of the hand) constantly tilts the object out of the grip and orientation."}, {"heading": "7.2 Mujoco Haptix", "text": "While the use of an expert is desirable, the use and use of an expert is exceptionally difficult for dexterousPrepared using sagej.clsmanipulation. This is due to two main factors: First, skillful manipulation strategies are extremely sensitive to minor deviations in contact forces, contact locations and object positions. Therefore, minor deviations from expert demonstrations render the demonstration useless. Second, the technology to capture the details of hand manipulation is often unreliable. Unlike whole-body movements, hand manipulation behavior unfolds in a compact region of space co-inhabited by the objects to be manipulated. This makes motion detection difficult, due to occlusions and, in the case of passive systems, marker confusion. Manipulation also involves a large number of contacts, including dynamic phenomena such as rolling, sliding, stick-slip, deformation and soft contacts. The human hand uses this data to interpret the dynamics, but to interpret the data well."}, {"heading": "7.3 Expert Demonstrations", "text": "We use Mujoco Hapix to record expert demonstrations. The expert first goes through a regression-based calibration process, which maps the Cyberglove sensors on the ADROIT joint space. To enable hand manipulation, we map the joint angle reported by Cyberglove qc on actuator commands using the equation (2): ut = kJ tendon (qt \u2212 qct), (2) where qt is the current joint configuration of the hand, J-Tendom is the tendon jakobian, which maps the joint space to the tendon space, and k is the profit vector. ut is applied as a control on the pneumatic actuators and we let the simulation develop by advancing the physics of the world with time. Hand-object interactions develop with the simulation steps forward quotor. The expert is in a narrow feedback loop with the simulation of the hand movements, quotor movements, quotor movements, quotor movements, Quotor movements, Quotor movements, Quotor movements, Quotor movements, Quotor movements, Quotor movements, Quotor movements, Quotor movements, Quotor movements, Quotor movements, Quotor movements, Quotor movements, Quotor movements, Quotor movements, Quotor movements, Quotor movements, Quotor movements, Quotor movements, Quotor movements, Quotor movements, Quotor movements, Quotor movements, Quotor movements, Quotor movements, Quotor movements, Quotor movements, Quotor movements, Quotor movements, Quotor movements, Quotor movements, Quotor system, Quotor movements, Quotor movements, by the strategy of the interacting."}, {"heading": "7.4 Learning Imitation Policies", "text": "The first attempts to reinvent the pick-up task from scratch are connected with the following cost function: \"(xt, ut).\" \"It is only a matter of time.\" \"It is only a matter of time.\" \"It is only a matter of time.\" \"It is a matter of time.\" \"It is a matter of time.\" \"Why?\", \"\" Why?, \"\" \"Why?.\" \"\". \"\" \"\" Why?. \"\" \"\".. \"\" \"\".. \"\" \"..\" \"\".. \"\" \"..\" \"\".. \"\". \"\" \"..\" \".\" \"\".. \"\". \"\" \"\".. \"\". \"\" \"\" \"\".. \"\" \"\". \"\" \"\" \"\" \"..\" \"\" \"\". \"\" \"\" \"\".. \"\" \".\" \"\" \".\" \"\" \"\".. \"\" \".\" \".\" \".\" \".\" \".\" \".\" \".\" \".\" \".\" \"\" \".\" \".\" \".\" \"\". \"\". \"\" \".\" \"\". \"\" \".\" \".\" \".\" \".\" \"\". \".\" \"\". \".\" \"\". \".\" \".\" \".\" \"\". \".\". \"\". \"\" \".\". \".\" \".\" \".\" \"\". \".\". \"\". \".\". \"\" \".\". \"\" \".\" \".\". \".\" \".\" \".\" \".\" \".\". \".\" \".\" \".\". \"\". \"\". \".\" \".\". \".\" \".\" \".\" \".\". \"\" \"\". \".\". \".\" \".\" \".\" \".\" \".\" \"\". \"\". \".\". \"\". \"\" \"\". \"\". \".\" \".\". \".\" \".\". \"\". \"\". \"\". \".\" \".\" \".\". \"\" \".\" \".\". \"\" \".\". \".\" \".\" \"\". \".\". \""}, {"heading": "8 Policy Generalization", "text": "Section 6 and Section 7 explore the possibilities of trajectory-centered reinforcement measures to learn robust \"local\" strategies for skillfully manipulating freely moving objects. However, the resulting local strategies are successful from specific starting states and are not designed to handle variations in initial conditions, such as the different initial placement of the manipulated object. In this section, we will discuss how to use local strategies to learn a single global policy that is effectively generalized across different starting conditions.Generalizable global strategies can typically be learned through reinforcement learning with more expressive approximations of functions. Deep neural networks represent a particularly general and expressive class of functional approximation systems and have recently been used to learn skills that can range from playing in Atari games (Mnih, Kavukcuoglu, Silver, Rusu, Veness, Bellemare, Gradjski, Riedmiller) to the very tangible and tangible capabilities available in this section (we will have in this section, Riedmiller, OstmilonsFirell)."}, {"heading": "8.1 Task Details", "text": "To analyze the generalization, we vary the alignment of the bar in the initial state. The goal is to learn a strategy for this task that is successful for each initial alignment of the bar. This is particularly challenging because the robot cannot translate or re-align the wrist (since the hand is stationary) and therefore must apply substantially different holding strategies for different alignment of the bar, including the use of additional finger movements to bring the bar into the desired position. In addition to this challenge, the task also inherits all of the difficulties detailed in Section 7, including the high dimensionality of the system and the challenge of delayed rewards. To mitigate the challenge of local optimism, we use expert demonstrations again. We collected a series of 10 demonstrations covering 180 degrees of bar variation. Figure 11 shows the 10 initial configurations, of which an expert was assigned to provide demonstrations."}, {"heading": "8.2 Local Policies", "text": "Before assessing the generalization, we first analyze the performance of the individual local policies that have been trained with imitation and learning from experience. For each task, we evaluate the success or failure of the study according to the following criteria: a successful plucking attempt must result in the object being stationary, missing the bar by a certain height, and the entire bar remains on the ground to ensure a successful grip in the desired target position. Note that partially successful executions where the center of the bar remains sagej.clapped, but one of the endpoints, are marked as failures. This allows us not only to determine whether the pipe has been placed in the desired position - an important condition for all subsequent manipulations that can be applied."}, {"heading": "8.3 Nearest Neighbor", "text": "Based on the results in Figure 12b, we can conclude that any local policy is successful after training in a neighborhood that extends to the boundary of the next local policy, suggesting that a relatively simple neighborhood technique could in principle enable a non-parametric strategy to expand the success region to the full range of the bar's orientation lines. In Figure 15, we evaluate the performance of this strategy of the closest neighbors, which simply uses the local policy trained to orient the bar that comes closest to the initial orientation observed at Euclidean distance. Successful attempts are marked green and failures marked red, and the overall success rate is 90.8%. Although the strategy of the closest neighbors is successful in this case, it requires maintaining the entire local policy and manually specifying the variables on which the closest neighbor queries should be performed (in this case, the bar)."}, {"heading": "8.4 Generalization with Deep Neural Networks", "text": "This year is the highest in the history of the country."}, {"heading": "8.5 Results on ADROIT Hardware Platform", "text": "In this section we present our generalization results for the pick-up task on the hardware platform ADROIT. The details of the task are the same as mentioned above. However, we consider the generalization both in the position and in the orientation of the object. Orientation generalization takes place in a smaller neighborhood than the simulated experiments above, and we train a fully networked neural network with 6 layers and 120 rectilinear (ReLU) units on each layer. Training samples are collected from random samples of local controllers learned around the 4 demonstrations of the expert user. Expert demonstrations were collected at the beginning. \u00b2 It is worth noting that the hand is reset to the initial state of the track controller at the narrowest angle for each rod. Although these states are very similar, they can still provide additional clues for the network. Our ongoing experiments, which we will include in the final version, will relate to random starting positions to control these potentially confusing factors."}, {"heading": "9 Discussion and Future Work", "text": "This year, the time has come for us to find a solution that is capable, that we are able to find a solution, that we are able to find a solution, that we are able to find a solution, that we are able to find a solution, that we are able to find a solution, that we are able to find a solution, that we are able to find a solution, that we are able to find a solution, that we are able to find a solution, that we are able to find a solution, that we are able to find a solution."}, {"heading": "Acknowledgements", "text": "This work has been supported by NIH, NSF and DARPA, and the authors state that there is no conflict of interest."}], "references": [{"title": "An application of reinforcement learning to aerobatic helicopter flight", "author": ["P Abbeel", "A Coates", "M Quigley", "A Ng"], "venue": "Advances in Neural Information Processing Systems (NIPS)", "citeRegEx": "Abbeel et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Abbeel et al\\.", "year": 2006}, {"title": "A positive pressure universal gripper based on the jamming of granular material", "author": ["JR Amend Jr.", "E Brown", "N Rodenberg", "HM Jaeger", "H Lipson"], "venue": "Robotics, IEEE Transactions on", "citeRegEx": "Jr et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Jr et al\\.", "year": 2012}, {"title": "A mathematical theory of adaptive control processes", "author": ["R Bellman", "R Kalaba"], "venue": "Proceedings of the National Academy of Sciences", "citeRegEx": "Bellman and Kalaba,? \\Q1959\\E", "shortCiteRegEx": "Bellman and Kalaba", "year": 1959}, {"title": "A survey on policy search for robotics", "author": ["M Deisenroth", "G Neumann", "J Peters"], "venue": "Foundations and Trends in Robotics", "citeRegEx": "Deisenroth et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Deisenroth et al\\.", "year": 2013}, {"title": "Learning to control a low-cost manipulator using data-efficient reinforcement learning. In: Robotics: Science and Systems (RSS)", "author": ["M Deisenroth", "C Rasmussen", "D Fox"], "venue": null, "citeRegEx": "Deisenroth et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Deisenroth et al\\.", "year": 2011}, {"title": "Learning dexterous manipulation for a soft robotic hand from human demonstration", "author": ["A Gupta", "C Eppner", "S Levine", "P Abbeel"], "venue": "IROS", "citeRegEx": "Gupta et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Gupta et al\\.", "year": 2016}, {"title": "Hierarchical mixtures of experts and the em algorithm", "author": ["M Jordan", "R Jacobs"], "venue": "Neural Computation", "citeRegEx": "Jordan and Jacobs,? \\Q1994\\E", "shortCiteRegEx": "Jordan and Jacobs", "year": 1994}, {"title": "Reinforcement learning in robotics: A survey", "author": ["J Kober", "JA Bagnell", "J Peters"], "venue": "International Journal of Robotic Research", "citeRegEx": "Kober et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Kober et al\\.", "year": 2013}, {"title": "Reinforcement learning to adjust robot movements to new situations. In: Robotics: Science and Systems (RSS)", "author": ["J Kober", "E Oztop", "J Peters"], "venue": null, "citeRegEx": "Kober et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Kober et al\\.", "year": 2010}, {"title": "Mujoco haptix: A virtual reality system for hand manipulation. In: Humanoids", "author": ["V Kumar", "E Todorov"], "venue": null, "citeRegEx": "Kumar and Todorov,? \\Q2015\\E", "shortCiteRegEx": "Kumar and Todorov", "year": 2015}, {"title": "Optimal control with learned local models: Application to dexterous manipulation", "author": ["V Kumar", "E Todorov", "S Levine"], "venue": "In: International Conference on Robotics and Automation (ICRA)", "citeRegEx": "Kumar et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Kumar et al\\.", "year": 2016}, {"title": "Fast, strong and compliant pneumatic actuation for dexterous tendon-driven hands", "author": ["V Kumar", "Z Xu", "E Todorov"], "venue": "In: International Conference on Robotics and Automation (ICRA)", "citeRegEx": "Kumar et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Kumar et al\\.", "year": 2013}, {"title": "Learning neural network policies with guided policy search under unknown dynamics", "author": ["S Levine", "P Abbeel"], "venue": "Advances in Neural Information Processing Systems (NIPS)", "citeRegEx": "Levine and Abbeel,? \\Q2014\\E", "shortCiteRegEx": "Levine and Abbeel", "year": 2014}, {"title": "2015a) End-toend training of deep visuomotor policies", "author": ["S Levine", "C Finn", "T Darrell", "P Abbeel"], "venue": null, "citeRegEx": "Levine et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Levine et al\\.", "year": 2015}, {"title": "Guided policy search", "author": ["S Levine", "V Koltun"], "venue": "In: International Conference on Machine Learning (ICML)", "citeRegEx": "Levine and Koltun,? \\Q2013\\E", "shortCiteRegEx": "Levine and Koltun", "year": 2013}, {"title": "Learning contact-rich manipulation skills with guided policy search", "author": ["S Levine", "N Wagener", "P Abbeel"], "venue": "In: International Conference on Robotics and Automation (ICRA)", "citeRegEx": "Levine et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Levine et al\\.", "year": 2015}, {"title": "Iterative linear quadratic regulator design for nonlinear biological movement systems", "author": ["W Li", "E Todorov"], "venue": "ICINCO", "citeRegEx": "Li and Todorov,? \\Q2004\\E", "shortCiteRegEx": "Li and Todorov", "year": 2004}, {"title": "Samplebased information-theoretic stochastic optimal control", "author": ["R Lioutikov", "A Paraschos", "G Neumann", "J Peters"], "venue": "In: International Conference on Robotics and Automation (ICRA)", "citeRegEx": "Lioutikov et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Lioutikov et al\\.", "year": 2014}, {"title": "Dynamic nonprehensile manipulation: Controllability, planning and experiments", "author": ["K Lynch", "MT Mason"], "venue": "International Journal of Robotics Research", "citeRegEx": "Lynch and Mason,? \\Q1999\\E", "shortCiteRegEx": "Lynch and Mason", "year": 1999}, {"title": "Adaptive optimal feedback control with learned internal dynamics models. In: From Motor Learning to Interaction", "author": ["D Mitrovic", "S Klanke", "S Vijayakumar"], "venue": "Learning in Robots,", "citeRegEx": "Mitrovic et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Mitrovic et al\\.", "year": 2010}, {"title": "Human-level control through deep reinforcement learning", "author": ["V Mnih", "K Kavukcuoglu", "D Silver", "AA Rusu", "J Veness", "MG Bellemare", "A Graves", "M Riedmiller", "AK Fidjeland", "G Ostrovski"], "venue": "Nature", "citeRegEx": "Mnih et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2015}, {"title": "Learning and generalization of motor skills by learning from demonstration", "author": ["P Pastor", "H Hoffmann", "T Asfour", "S Schaal"], "venue": "In: International Conference on Robotics and Automation (ICRA)", "citeRegEx": "Pastor et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Pastor et al\\.", "year": 2009}, {"title": "Relative entropy policy search", "author": ["J Peters", "K M\u00fclling", "Y Altun"], "venue": "In: AAAI. Atlanta", "citeRegEx": "Peters et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Peters et al\\.", "year": 2010}, {"title": "Relative entropy policy search", "author": ["J Peters", "K M\u00fclling", "Y Alt\u00fcn"], "venue": "AAAI Conference on Artificial Intelligence", "citeRegEx": "Peters et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Peters et al\\.", "year": 2010}, {"title": "Reinforcement learning of motor skills with policy gradients", "author": ["J Peters", "S Schaal"], "venue": "Neural Networks", "citeRegEx": "Peters and Schaal,? \\Q2008\\E", "shortCiteRegEx": "Peters and Schaal", "year": 2008}, {"title": "Dart: dense articulated real-time tracking with consumer depth cameras", "author": ["T Schmidt", "R Newcombe", "D Fox"], "venue": null, "citeRegEx": "Schmidt et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Schmidt et al\\.", "year": 2015}, {"title": "Reinforcement Learning: An Introduction", "author": ["R Sutton", "A Barto"], "venue": null, "citeRegEx": "Sutton and Barto,? \\Q1998\\E", "shortCiteRegEx": "Sutton and Barto", "year": 1998}, {"title": "Stochastic policy gradient reinforcement learning on a simple 3d biped", "author": ["R Tedrake", "T Zhang", "H Seung"], "venue": "In: International Conference on Intelligent Robots and Systems (IROS)", "citeRegEx": "Tedrake et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Tedrake et al\\.", "year": 2004}, {"title": "Td-gammon, a self-teaching backgammon program, achieves master-level play", "author": ["G Tesauro"], "venue": "Neural computation", "citeRegEx": "Tesauro,? \\Q1994\\E", "shortCiteRegEx": "Tesauro", "year": 1994}, {"title": "Reinforcement learning of motor skills in high dimensions", "author": ["E Theodorou", "J Buchli", "S Schaal"], "venue": "In: International Conference on Robotics and Automation (ICRA)", "citeRegEx": "Theodorou et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Theodorou et al\\.", "year": 2010}, {"title": "Mujoco: A physics engine for model-based control", "author": ["E Todorov", "T Erez", "Y Tassa"], "venue": "Intelligent Robots and Systems (IROS),", "citeRegEx": "Todorov et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Todorov et al\\.", "year": 2012}, {"title": "Learning robot in-hand manipulation with tactile features. In: Humanoid Robots (Humanoids)", "author": ["H van Hoof", "T Hermans", "G Neumann", "J Peters"], "venue": null, "citeRegEx": "Hoof et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hoof et al\\.", "year": 2015}, {"title": "Simbicon: Simple biped locomotion control", "author": ["K Yin", "K Loken", "M van de Panne"], "venue": "ACM Trans. Graph", "citeRegEx": "Yin et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Yin et al\\.", "year": 2007}, {"title": "Value function approximation and model-predictive control", "author": ["M Zhong", "M Johnson", "Y Tassa", "T Erez", "E Todorov"], "venue": "IEEE Symposium on Adaptive Dynamic Programming and Reinforcement Learning", "citeRegEx": "Zhong et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Zhong et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 18, "context": "Such non-prehensile manipulation is challenging, since the system must reason about both the kinematics and the dynamics of the interaction Lynch and Mason (1999). We present results for learning both local models and control policies that can succeed from a single initial state, as well as more generalizable global policies that can use limited onboard sensing to perform a complex grasping behavior.", "startOffset": 140, "endOffset": 163}, {"referenceID": 2, "context": "Depending on one\u2019s preference of terminology, our method can be classified as model-based Reinforcement Learning (RL), or as adaptive optimal control (Bellman and Kalaba 1959).", "startOffset": 150, "endOffset": 175}, {"referenceID": 26, "context": "domains (Sutton and Barto 1998).", "startOffset": 8, "endOffset": 31}, {"referenceID": 28, "context": "The idea of learning policies without having models still dominates RL, and forms the basis of the most remarkable success stories, both old (Tesauro 1994) and new (Mnih, Kavukcuoglu, Silver, Rusu, Veness, Bellemare, Graves, Riedmiller, Fidjeland and Ostrovski 2015).", "startOffset": 141, "endOffset": 155}, {"referenceID": 12, "context": "Related ideas have been pursued previously (Mitrovic, Klanke and Vijayakumar 2010; Levine and Abbeel 2014; Levine, Wagener and Abbeel 2015b).", "startOffset": 43, "endOffset": 140}, {"referenceID": 24, "context": "Although robotic reinforcement learning has experienced considerable progress in recent years, with successful results in domains ranging from flight (Abbeel, Coates, Quigley and Ng 2006) to locomotion (Tedrake, Zhang and Seung 2004) to manipulation (Peters, M\u00fclling and Altun 2010a; Theodorou, Buchli and Schaal 2010; Peters and Schaal 2008), comparatively few methods have been applied to control dexterous hands.", "startOffset": 250, "endOffset": 342}, {"referenceID": 12, "context": "This algorithm, which follows previous work (Levine and Abbeel 2014), is described in Section 5.", "startOffset": 44, "endOffset": 68}, {"referenceID": 12, "context": "The derivation in this section follows previous work (Levine and Abbeel 2014), but we describe the algorithm in this (a) End pose, learned (b) End pose, human", "startOffset": 53, "endOffset": 77}, {"referenceID": 12, "context": "the number of samples is much lower than the dimensionality of the system (Levine and Abbeel 2014).", "startOffset": 74, "endOffset": 98}, {"referenceID": 16, "context": "This type of iterative approach can be thought of as a variant of iterative LQR (Li and Todorov 2004), where the dynamics are fitted to data.", "startOffset": 80, "endOffset": 101}, {"referenceID": 14, "context": "As shown in previous work (Levine and Koltun 2013), this objective is in fact optimized by setting p(ut|xt) = N (Ktxt + kt,Ct), where Ct = Q\u22121 u,ut.", "startOffset": 26, "endOffset": 50}, {"referenceID": 24, "context": "Using KL-divergence constraints for controller optimization has been proposed in a number of prior works (Bagnell and Schneider 2003; Peters and Schaal 2008; Peters, M\u00fclling and Alt\u00fcn 2010b).", "startOffset": 105, "endOffset": 190}, {"referenceID": 9, "context": "The Mujoco Haptix system (Kumar and Todorov 2015) was developed to facilitate physically-consistent recording of rich hand-object interactions.", "startOffset": 25, "endOffset": 49}, {"referenceID": 6, "context": "Instead we could consider a mixtureof-experts architecture (Jordan and Jacobs 1994), where the gating network corresponds to the switching mechanism in our current nearest neighbor approach, while the expert networks correspond to our local trajectory controllers.", "startOffset": 59, "endOffset": 83}], "year": 2016, "abstractText": "We explore learning-based approaches for feedback control of a dexterous five-finger hand performing non-prehensile manipulation. First, we learn local controllers that are able to perform the task starting at a predefined initial state. These controllers are constructed using trajectory optimization with respect to locally-linear time-varying models learned directly from sensor data. In some cases, we initialize the optimizer with human demonstrations collected via teleoperation in a virtual environment. We demonstrate that such controllers can perform the task robustly, both in simulation and on the physical platform, for a limited range of initial conditions around the trained starting state. We then consider two interpolation methods for generalizing to a wider range of initial conditions: deep learning, and nearest neighbors. We find that nearest neighbors achieve higher performance. Nevertheless, the neural network has its advantages: it uses only tactile and proprioceptive feedback but no visual feedback about the object (i.e. it performs the task blind) and learns a time-invariant policy. In contrast, the nearest neighbors method switches between time-varying local controllers based on the proximity of initial object states sensed via motion capture. While both generalization methods leave room for improvement, our work shows that (i) local trajectory-based controllers for complex nonprehensile manipulation tasks can be constructed from surprisingly small amounts of training data, and (ii) collections of such controllers can be interpolated to form more global controllers. Results are summarized in the supplementary video: https://youtu.be/E0wmO6deqjo", "creator": "LaTeX with hyperref package"}}}