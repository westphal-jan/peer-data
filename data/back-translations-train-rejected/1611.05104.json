{"id": "1611.05104", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Nov-2016", "title": "A Way out of the Odyssey: Analyzing and Combining Recent Insights for LSTMs", "abstract": "LSTMs have become a basic building block for many deep NLP models. In recent years, many improvements and variations have been proposed for deep sequence models in general, and LSTMs in particular. We propose and analyze a series of architectural modifications for LSTM networks resulting in improved performance for text classification datasets. We observe compounding improvements on traditional LSTMs using Monte Carlo test-time model averaging, deep vector averaging (DVA), and residual connections, along with four other suggested modifications. Our analysis provides a simple, reliable, and high quality baseline model.", "histories": [["v1", "Wed, 16 Nov 2016 00:53:01 GMT  (591kb,D)", "http://arxiv.org/abs/1611.05104v1", null], ["v2", "Sat, 17 Dec 2016 06:47:05 GMT  (591kb,D)", "http://arxiv.org/abs/1611.05104v2", null]], "reviews": [], "SUBJECTS": "cs.CL cs.AI", "authors": ["shayne longpre", "sabeek pradhan", "caiming xiong", "richard socher"], "accepted": false, "id": "1611.05104"}, "pdf": {"name": "1611.05104.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Shayne Longpre", "Sabeek Pradhan"], "emails": ["slongpre@cs.stanford.edu", "sabeekp@cs.stanford.edu", "cxiong@salesforce.com", "rsocher@salesforce.com"], "sections": [{"heading": "1 INTRODUCTION", "text": "For example, Caruana et al. (2008) have shown that random forests are a strong starting point for many high-dimensionally monitored learning tasks. In the field of computer vision, off-the-shelf Convolutionary Neural Networks (CNNs) have acquired a reputation as a strong starting line (Sharif Razavian et al., 2014) and a basic building block for more complex models such as answering visual questions (Xiong et al., 2016). In the field of natural language processing (NLP) and other sequential modeling tasks, recurring neural networks (RNNNs), and in particular Long Short-Term Memory (LSTM) networks with a linear projection layer have finally begun to attain a similar status."}, {"heading": "2 LSTM NETWORK", "text": "The basic LSTM layer consists of six equations: it = tanh (Wixt + Riht \u2212 1 + bi) (1) jt = \u03c3 (Wjxt + Rjht \u2212 1 + bj) (2) ft = \u03c3 (Wfxt + Rfht \u2212 1 + bf) (3) ot = tanh (Woxt + Roht \u2212 1 + bo) (4) ct = it jt + ft ct \u2212 1 (5) ht = ot tanh (ct) (6) ar Xiv: 161 1,05 104v 1 [cs.C L] 16 Nov 2Where \u03c3 is the sigmoid function, is element wise multiplication, and vt is the value of variable v at timestep. Each layer receives xt from the layer that came before it, ht \u2212 1 and ct \u2212 1 from the previous timestep, and it outputs the part that comes after it and the next timestba value that is very slow."}, {"heading": "3 MONTE CARLO MODEL AVERAGING", "text": "It is common practice in the application of dropouts in neural networks to scale the weights in the pull time (inverted dropout), which ensures that the expected magnitude of input into each given layer is equivalent between pull and test, which requires efficient calculation of test time forecasts. However, a model trained with a dropout only provides test time forecasts generated without dropout, an approximation of the ensemble of smaller models that dropout means. A higher fidelity method requires test time failures to be performed in a manner consistent with the way the model was trained. To achieve this, we try neural networks with dropout capabilities that are applied to each test example and the average predictions are applied."}, {"heading": "4 DEEP VECTOR AVERAGING", "text": "This is especially true for very long sequences such as the IMDB sentimental dataset (Maas et al., 2011), where deep sequential models do not capture uni- and bigram events over long sequences, which is probably why n-gram-based models such as a NBSVM bigchart (Wang and Manning, 2012) outperform RNN models on such datasets. Iyyer et al. (2015) and others have shown that for general NLP classification tasks, the use of a deep, disordered composition (or sack-of-words) of a sequence can yield strong results. Their solution, the deep average network (DAN), combines the observed effectiveness of depth with the unreasonable effectiveness of disordered representations of long sequences."}, {"heading": "5 RESIDUAL CONNECTIONS", "text": "In fact, it is the case that most of them are in a position to go to another level, rather than to another, namely to another, and indeed to another, to another, to another, to another, to another, to another, to another, to another, to another, to another, to another, to another, to another, to another, to another, to another, to another, to another, to another, to another, to another, to another, to another, to another, to another, to another, to another, to another, to another, to another, to another, to another, to another, to another, to another, to another, to another, to another, to another, to another, to another, to another, to another, to another, to another, to another, to another, to another, to another, to another, to another, to another, to another, to another, to another, to another, to another, to another, to another, to another, to another, to another, to another, to another, to another, to another, to another, to another, to another, to another, to another, to another, to another, to another, to another, to another, to another, to another, to another, to another, to another, to another, to another, to another, to another, to another, to another, to another, to another, to another, to another, to another, to another, to another, to another, to another, to another, to another, to another, to another, to another, to another, to another, to another, to another, to another, to another, to another, to another, to another, to another, to another, to another, to another, to another, to another, to another, to another, to another, to another, to another, to another, to another, to another, to another, to another, to another, to another, to another, to another, to another, to another, to another, to another, to another, to another, to another, to another, to another, to another, to another, to another, to another, to another, to another, another, to another, to another, to another, to another, to another, to another, to another, to another, to another, to another, to another, to another, to another, to"}, {"heading": "6 EXPERIMENTAL RESULTS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6.1 DATASETS", "text": "We chose two commonly used benchmark datasets for our experiments: the Stanford Sentiment Treebank (SST) (Socher et al., 2013) and the IMDB Sentiment dataset (Maas et al., 2011), which allowed us to compare the performance of our models with existing work and the flexibility of LSTMLSTMSTMSoftmaxh (1) th (2) th (3) th (3) txt... xt + 1xt 1h (1) th (2) t 1h (3) t 1h (2) th (3) t (a) Res-V1: An illustration of vertically residual connectionsLSTMSTMSoftmax... xtxt 1 xt 1 xt + 1h (1) t 1 h (1) th (2) t 1h (2) t (2) th (2) t (2) t (3) t (b) Res-V2: An illustrated 1) th (1 t + 1h (1) t (1) th (1) t (1) t (1) t (1) t (1) t (1) t (1) t (1) t t (1) t (1) t t (1) t (1) t (1) t (1) t (1 t (1) t t (1) t (1) t (t (1) t (1) t (1) t (t (1) t (1) t (t (1) t (t (1) t (t (1) t."}, {"heading": "6.2 METHODOLOGY", "text": "To ensure scientific reliability, the addition of each feature is the only change from the previous model (see Figures 4 and 5).The baseline model is a 2-layer stacked LSTM with hidden size 170 for SST and 120 for IMDB, as in Tai et al. (2015).All models in this paper used publicly available 300-dimensional word vectors, pre-schooled with glove to 840 million tokens from Common Crawl Data (Pennington et al., 2014), and both the word vectors and subsequent weight matrices were added with Adam at a learning rate of 10 \u2212 4.The first set of basic feature additives were added to include a forgotten bias and the use of drop-out. Adding a bias of 1.0 to the forget gate (i.e. adding 1.0 to the inside of the sigmoid function in Equation 3) improves results on NLP tasks, especially for learning we are very long-term (dependents)."}, {"heading": "6.3 RESULTS", "text": "Since each of the proposed modifications is operated independently of each other, they are well suited to use in combination with others. Figures 4 and 5 take these features to extremes."}, {"heading": "7 CONCLUSION", "text": "We are examining several easy-to-implement extensions of the basic LSTM network that have a positive impact on performance, including both fairly well-established extensions (Forge Gate bias, dropout, model size increase, bidirectionality) and several novel ones (Monte Carlo model mean, deep vector averaging, residual compounds). We find that these enhancements improve the performance of the LSTM in classification tasks, both in conjunction with and in isolation, with an accuracy close to the state of the art, although it is lighter and uses less information than the current state of the art. Our results suggest that these enhancements should be incorporated into the LSTM fundamentals. 1For IMDB, we only measure results obtained from training based solely on the marked training set. Therefore, we exclude results from unattended models that take advantage of the additional 50,000 unlabeled examples such as Miyato et al. (2016)."}], "references": [{"title": "An empirical evaluation of supervised learning in high dimensions", "author": ["Rich Caruana", "Nikos Karampatziakis", "Ainur Yessenalina"], "venue": "In Proceedings of the 25th international conference on Machine learning,", "citeRegEx": "Caruana et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Caruana et al\\.", "year": 2008}, {"title": "A theoretically grounded application of dropout in recurrent neural networks", "author": ["Yarin Gal"], "venue": "arXiv preprint arXiv:1512.05287,", "citeRegEx": "Gal.,? \\Q2015\\E", "shortCiteRegEx": "Gal.", "year": 2015}, {"title": "Lstm: A search space odyssey", "author": ["Klaus Greff", "Rupesh Kumar Srivastava", "Jan Koutn\u0131\u0301k", "Bas R Steunebrink", "J\u00fcrgen Schmidhuber"], "venue": "arXiv preprint arXiv:1503.04069,", "citeRegEx": "Greff et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Greff et al\\.", "year": 2015}, {"title": "Deep residual learning for image recognition", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"], "venue": "CoRR, abs/1512.03385,", "citeRegEx": "He et al\\.,? \\Q2015\\E", "shortCiteRegEx": "He et al\\.", "year": 2015}, {"title": "Deep networks with stochastic depth", "author": ["Gao Huang", "Yu Sun", "Zhuang Liu", "Daniel Sedra", "Kilian Weinberger"], "venue": "arXiv preprint arXiv:1603.09382,", "citeRegEx": "Huang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2016}, {"title": "Modeling compositionality with multiplicative recurrent neural networks", "author": ["Ozan Irsoy", "Claire Cardie"], "venue": "arXiv preprint arXiv:1412.6577,", "citeRegEx": "Irsoy and Cardie.,? \\Q2014\\E", "shortCiteRegEx": "Irsoy and Cardie.", "year": 2014}, {"title": "Deep unordered composition rivals syntactic methods for text classification", "author": ["Mohit Iyyer", "Varun Manjunatha", "Jordan Boyd-Graber", "Hal Daum\u00e9 III"], "venue": "In Association for Computational Linguistics,", "citeRegEx": "Iyyer et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Iyyer et al\\.", "year": 2015}, {"title": "Visualizing and understanding recurrent networks", "author": ["Andrej Karpathy", "Justin Johnson", "Fei-Fei Li"], "venue": "CoRR, abs/1506.02078,", "citeRegEx": "Karpathy et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Karpathy et al\\.", "year": 2015}, {"title": "Convolutional neural networks for sentence classification", "author": ["Yoon Kim"], "venue": "arXiv preprint arXiv:1408.5882,", "citeRegEx": "Kim.,? \\Q2014\\E", "shortCiteRegEx": "Kim.", "year": 2014}, {"title": "Ask me anything: Dynamic memory networks for natural language processing", "author": ["Ankit Kumar", "Ozan Irsoy", "Peter Ondruska", "Mohit Iyyer", "James Bradbury", "Ishaan Gulrajani", "Victor Zhong", "Romain Paulus", "Richard Socher"], "venue": null, "citeRegEx": "Kumar et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Kumar et al\\.", "year": 2016}, {"title": "Distributed representations of sentences and documents", "author": ["Quoc V Le", "Tomas Mikolov"], "venue": "In ICML,", "citeRegEx": "Le and Mikolov.,? \\Q2014\\E", "shortCiteRegEx": "Le and Mikolov.", "year": 2014}, {"title": "Bridging the gaps between residual learning, recurrent neural networks and visual cortex", "author": ["Qianli Liao", "Tomaso A. Poggio"], "venue": "CoRR, abs/1604.03640,", "citeRegEx": "Liao and Poggio.,? \\Q2016\\E", "shortCiteRegEx": "Liao and Poggio.", "year": 2016}, {"title": "Learning word vectors for sentiment analysis", "author": ["Andrew L. Maas", "Raymond E. Daly", "Peter T. Pham", "Dan Huang", "Andrew Y. Ng", "Christopher Potts"], "venue": "In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,", "citeRegEx": "Maas et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Maas et al\\.", "year": 2011}, {"title": "Ensemble of generative and discriminative techniques for sentiment analysis of movie reviews", "author": ["Gr\u00e9goire Mesnil", "Tomas Mikolov", "Marc\u2019Aurelio Ranzato", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1412.5335,", "citeRegEx": "Mesnil et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Mesnil et al\\.", "year": 2014}, {"title": "Virtual adversarial training for semi-supervised text classification", "author": ["Takeru Miyato", "Andrew M Dai", "Ian Goodfellow"], "venue": "arXiv preprint arXiv:1605.07725,", "citeRegEx": "Miyato et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Miyato et al\\.", "year": 2016}, {"title": "Neural tree indexers for text understanding", "author": ["Tsendsuren Munkhdalai", "Hong Yu"], "venue": "CoRR, abs/1607.04492,", "citeRegEx": "Munkhdalai and Yu.,? \\Q2016\\E", "shortCiteRegEx": "Munkhdalai and Yu.", "year": 2016}, {"title": "Glove: Global vectors for word representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher D Manning"], "venue": "In EMNLP,", "citeRegEx": "Pennington et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Cnn features off-the-shelf: An astounding baseline for recognition", "author": ["Ali Sharif Razavian", "Hossein Azizpour", "Josephine Sullivan", "Stefan Carlsson"], "venue": "In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops,", "citeRegEx": "Razavian et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Razavian et al\\.", "year": 2014}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["Karen Simonyan", "Andrew Zisserman"], "venue": "CoRR, abs/1409.1556,", "citeRegEx": "Simonyan and Zisserman.,? \\Q2014\\E", "shortCiteRegEx": "Simonyan and Zisserman.", "year": 2014}, {"title": "Recursive deep models for semantic compositionality over a sentiment treebank", "author": ["Richard Socher", "Alex Perelygin", "Jean Y Wu", "Jason Chuang", "Christopher D Manning", "Andrew Y Ng", "Christopher Potts"], "venue": "In Proceedings of the conference on empirical methods in natural language processing (EMNLP),", "citeRegEx": "Socher et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Dropout: a simple way to prevent neural networks from overfitting", "author": ["Nitish Srivastava", "Geoffrey E Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Srivastava et al\\.,? \\Q1929\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 1929}, {"title": "Going deeper with convolutions", "author": ["Christian Szegedy", "Wei Liu", "Yangqing Jia", "Pierre Sermanet", "Scott Reed", "Dragomir Anguelov", "Dumitru Erhan", "Vincent Vanhoucke", "Andrew Rabinovich"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "Szegedy et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Szegedy et al\\.", "year": 2015}, {"title": "Inception-v4, inception-resnet and the impact of residual connections on learning", "author": ["Christian Szegedy", "Sergey Ioffe", "Vincent Vanhoucke"], "venue": "CoRR, abs/1602.07261,", "citeRegEx": "Szegedy et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Szegedy et al\\.", "year": 2016}, {"title": "Improved semantic representations from tree-structured long short-term memory networks", "author": ["Kai Sheng Tai", "Richard Socher", "Christopher D Manning"], "venue": "arXiv preprint arXiv:1503.00075,", "citeRegEx": "Tai et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Tai et al\\.", "year": 2015}, {"title": "Residual networks are exponential ensembles of relatively shallow networks", "author": ["Andreas Veit", "Michael J. Wilber", "Serge J. Belongie"], "venue": "CoRR, abs/1605.06431,", "citeRegEx": "Veit et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Veit et al\\.", "year": 2016}, {"title": "Baselines and bigrams: Simple, good sentiment and topic classification", "author": ["Sida Wang", "Christopher D Manning"], "venue": "In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Short Papers-Volume", "citeRegEx": "Wang and Manning.,? \\Q2012\\E", "shortCiteRegEx": "Wang and Manning.", "year": 2012}, {"title": "Dynamic memory networks for visual and textual question answering", "author": ["Caiming Xiong", "Stephen Merity", "Richard Socher"], "venue": "In ICML,", "citeRegEx": "Xiong et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Xiong et al\\.", "year": 2016}, {"title": "An empirical exploration of recurrent network architectures", "author": ["Wojciech Zaremba"], "venue": null, "citeRegEx": "Zaremba.,? \\Q2015\\E", "shortCiteRegEx": "Zaremba.", "year": 2015}], "referenceMentions": [{"referenceID": 26, "context": ", 2014) and basic building block for more complex models like visual question answering (Xiong et al., 2016).", "startOffset": 88, "endOffset": 108}, {"referenceID": 0, "context": "For instance, Caruana et al. (2008) showed random forests to be a strong baseline for many high-dimensional supervised learning tasks.", "startOffset": 14, "endOffset": 36}, {"referenceID": 0, "context": "For instance, Caruana et al. (2008) showed random forests to be a strong baseline for many high-dimensional supervised learning tasks. For computer vision, off-the-shelf convolutional neural networks (CNNs) have earned their reputation as a strong baseline (Sharif Razavian et al., 2014) and basic building block for more complex models like visual question answering (Xiong et al., 2016). For natural language processing (NLP) and other sequential modeling tasks, recurrent neural networks (RNNs), and in particular Long Short-Term Memory (LSTM) networks, with a linear projection layer at the end have begun to attain a similar status. However, the standard LSTM is in many ways lacking as a baseline. Zaremba (2015), Gal (2015), and others show that large improvements are possible using a forget bias, inverted dropout regularization or bidirectionality.", "startOffset": 14, "endOffset": 719}, {"referenceID": 0, "context": "For instance, Caruana et al. (2008) showed random forests to be a strong baseline for many high-dimensional supervised learning tasks. For computer vision, off-the-shelf convolutional neural networks (CNNs) have earned their reputation as a strong baseline (Sharif Razavian et al., 2014) and basic building block for more complex models like visual question answering (Xiong et al., 2016). For natural language processing (NLP) and other sequential modeling tasks, recurrent neural networks (RNNs), and in particular Long Short-Term Memory (LSTM) networks, with a linear projection layer at the end have begun to attain a similar status. However, the standard LSTM is in many ways lacking as a baseline. Zaremba (2015), Gal (2015), and others show that large improvements are possible using a forget bias, inverted dropout regularization or bidirectionality.", "startOffset": 14, "endOffset": 731}, {"referenceID": 27, "context": "Since the h value completely updates at each timestep while the c value maintains part of its own value through multiplication by the forget gate f , h and c complement each other very well, with h forming a \u201cfast\u201d state that can quickly adapt to new information and c forming a \u201cslow\u201d state that allows information to be retained over longer periods of time (Zaremba, 2015).", "startOffset": 359, "endOffset": 374}, {"referenceID": 2, "context": "While various papers have tried to systematically experiment with the 6 core equations constituting an LSTM (Greff et al., 2015; Zaremba, 2015), in general the basic LSTM equations have proven extremely resilient and, if not optimal, at least a local maximum.", "startOffset": 108, "endOffset": 143}, {"referenceID": 27, "context": "While various papers have tried to systematically experiment with the 6 core equations constituting an LSTM (Greff et al., 2015; Zaremba, 2015), in general the basic LSTM equations have proven extremely resilient and, if not optimal, at least a local maximum.", "startOffset": 108, "endOffset": 143}, {"referenceID": 1, "context": "We investigated averaging over the output of the final recurrent layer (just before the projection layer), over the output of the projection layer (the presoftmax unnormalized logits), and the post-softmax normalized probabilities, which is the approach taken by Gal (2015) for language modeling.", "startOffset": 263, "endOffset": 274}, {"referenceID": 7, "context": "Reliably retaining long-range information is a well documented weakness of LSTM networks (Karpathy et al., 2015).", "startOffset": 89, "endOffset": 112}, {"referenceID": 12, "context": "This is especially the case for very long sequences like the IMDB sentiment dataset (Maas et al., 2011),captio where deep sequential models fail to capture uni- and bi-gram occurrences over long sequences.", "startOffset": 84, "endOffset": 103}, {"referenceID": 25, "context": "This is likely why n-gram based models, such as a bi-gram NBSVM (Wang and Manning, 2012), outperform RNN models on such datasetes.", "startOffset": 64, "endOffset": 88}, {"referenceID": 6, "context": "It was shown by Iyyer et al. (2015) and others that for general NLP classification tasks, the use of a deep, unordered composition (or bag-of-words) of a sequence can yield strong results.", "startOffset": 16, "endOffset": 36}, {"referenceID": 3, "context": "For feed-forward convolutional neural networks used in computer vision tasks, residual networks, or ResNets, have obtained state of the art results (He et al., 2015).", "startOffset": 148, "endOffset": 165}, {"referenceID": 18, "context": "(2015) trained convolutional neural networks as deep as 151 layers, compared to 16 layers used in VGGNets (Simonyan and Zisserman, 2014) or 22 layers used in GoogLeNet (Szegedy et al.", "startOffset": 106, "endOffset": 136}, {"referenceID": 21, "context": "(2015) trained convolutional neural networks as deep as 151 layers, compared to 16 layers used in VGGNets (Simonyan and Zisserman, 2014) or 22 layers used in GoogLeNet (Szegedy et al., 2015), and won the 2015 ImageNet Challenge.", "startOffset": 168, "endOffset": 190}, {"referenceID": 4, "context": "Since then, various papers have tried to build upon the ResNet paradigm (Huang et al., 2016; Szegedy et al., 2016), and various others have tried to create convincing theoretical reasons for ResNet\u2019s success (Liao and Poggio, 2016; Veit et al.", "startOffset": 72, "endOffset": 114}, {"referenceID": 22, "context": "Since then, various papers have tried to build upon the ResNet paradigm (Huang et al., 2016; Szegedy et al., 2016), and various others have tried to create convincing theoretical reasons for ResNet\u2019s success (Liao and Poggio, 2016; Veit et al.", "startOffset": 72, "endOffset": 114}, {"referenceID": 11, "context": ", 2016), and various others have tried to create convincing theoretical reasons for ResNet\u2019s success (Liao and Poggio, 2016; Veit et al., 2016).", "startOffset": 101, "endOffset": 143}, {"referenceID": 24, "context": ", 2016), and various others have tried to create convincing theoretical reasons for ResNet\u2019s success (Liao and Poggio, 2016; Veit et al., 2016).", "startOffset": 101, "endOffset": 143}, {"referenceID": 3, "context": "For feed-forward convolutional neural networks used in computer vision tasks, residual networks, or ResNets, have obtained state of the art results (He et al., 2015). Rather than having each layer learn a wholly new representation of the data, as is customary for neural networks, ResNets have each layer (or group of layers) learn a residual which is added to the layer\u2019s input and then passed on to the next layer. More formally, if the input to a layer (or group of layers) is x and the output of that layer (or group of layers) is F (x), then the input to the next layer (or group of layers) is x+ F (x), whereas it would be F (x) in a conventional neural network. This architecture allows the training of far deeper models. He et al. (2015) trained convolutional neural networks as deep as 151 layers, compared to 16 layers used in VGGNets (Simonyan and Zisserman, 2014) or 22 layers used in GoogLeNet (Szegedy et al.", "startOffset": 149, "endOffset": 746}, {"referenceID": 19, "context": "We chose two commonly used benchmark datasets for our experiments: the Stanford Sentiment Treebank (SST) (Socher et al., 2013) and the IMDB sentiment dataset (Maas et al.", "startOffset": 105, "endOffset": 126}, {"referenceID": 12, "context": ", 2013) and the IMDB sentiment dataset (Maas et al., 2011).", "startOffset": 39, "endOffset": 58}, {"referenceID": 12, "context": "For IMDB, we randomly split the training set of 25, 000 examples into training and validation sets containing 22, 500 and 2, 500 examples respectively, as done in Maas et al. (2011).", "startOffset": 163, "endOffset": 182}, {"referenceID": 16, "context": "All models in this paper used publicly available 300 dimensional word vectors, pre-trained using Glove on 840 million tokens of Common Crawl Data (Pennington et al., 2014), and both the word vectors and the subsequent weight matrices were trained using Adam with a learning rate of 10\u22124.", "startOffset": 146, "endOffset": 171}, {"referenceID": 27, "context": "0 to the inside of the sigmoid function in equation 3) improves results across NLP tasks, especially for learning long-range dependencies (Zaremba, 2015).", "startOffset": 138, "endOffset": 153}, {"referenceID": 21, "context": "The baseline model is a 2-layer stacked LSTM with hidden size 170 for SST and 120 for IMDB, as used in Tai et al. (2015). All models in this paper used publicly available 300 dimensional word vectors, pre-trained using Glove on 840 million tokens of Common Crawl Data (Pennington et al.", "startOffset": 103, "endOffset": 121}, {"referenceID": 19, "context": "Model # Params (M) Train Time / Epoch (sec) Test Acc (%) RNTN (Socher et al., 2013) \u2212 \u2212 45.", "startOffset": 62, "endOffset": 83}, {"referenceID": 8, "context": "7 CNN-MC (Kim, 2014) \u2212 \u2212 47.", "startOffset": 9, "endOffset": 20}, {"referenceID": 5, "context": "4 DRNN (Irsoy and Cardie, 2014) \u2212 \u2212 49.", "startOffset": 7, "endOffset": 31}, {"referenceID": 23, "context": "8 CT-LSTM (Tai et al., 2015) 0.", "startOffset": 10, "endOffset": 28}, {"referenceID": 9, "context": "0 DMN (Kumar et al., 2016) \u2212 \u2212 52.", "startOffset": 6, "endOffset": 26}, {"referenceID": 15, "context": "1 NTI-SLSTM-LSTM (Munkhdalai and Yu, 2016) \u2212 \u2212 53.", "startOffset": 17, "endOffset": 42}, {"referenceID": 25, "context": "Model # Params (M) Train Time / Epoch (sec) Test Acc (%) SVM-bi (Wang and Manning, 2012) \u2212 \u2212 89.", "startOffset": 64, "endOffset": 88}, {"referenceID": 6, "context": "2 DAN-RAND (Iyyer et al., 2015) \u2212 \u2212 88.", "startOffset": 11, "endOffset": 31}, {"referenceID": 6, "context": "8 DAN (Iyyer et al., 2015) \u2212 \u2212 89.", "startOffset": 6, "endOffset": 26}, {"referenceID": 25, "context": "4 NBSVM-bi (Wang and Manning, 2012) \u2212 \u2212 91.", "startOffset": 11, "endOffset": 35}, {"referenceID": 13, "context": "2 NBSVM-tri, RNN, Sentence-Vec Ensemble (Mesnil et al., 2014) \u2212 \u2212 92.", "startOffset": 40, "endOffset": 61}, {"referenceID": 23, "context": "Selecting the best results for each model, we see results competitive with state-of-the-art performance for both IMDB1 and SST, even though many state-of-the-art models use either parse-tree information (Tai et al., 2015), multiple passes through the data (Kumar et al.", "startOffset": 203, "endOffset": 221}, {"referenceID": 9, "context": ", 2015), multiple passes through the data (Kumar et al., 2016) or tremendous train and test-time computational and memory expenses (Le and Mikolov, 2014).", "startOffset": 42, "endOffset": 62}, {"referenceID": 10, "context": ", 2016) or tremendous train and test-time computational and memory expenses (Le and Mikolov, 2014).", "startOffset": 76, "endOffset": 98}, {"referenceID": 14, "context": "Thus, we omit results from unsupervised models that leveraged the additional 50, 000 unlabeled examples, such as Miyato et al. (2016).", "startOffset": 113, "endOffset": 134}], "year": 2017, "abstractText": "LSTMs have become a basic building block for many deep NLP models. In recent years, many improvements and variations have been proposed for deep sequence models in general, and LSTMs in particular. We propose and analyze a series of architectural modifications for LSTM networks resulting in improved performance for text classification datasets. We observe compounding improvements on traditional LSTMs using Monte Carlo test-time model averaging, deep vector averaging (DVA), and residual connections, along with four other suggested modifications. Our analysis provides a simple, reliable, and high quality baseline model.", "creator": "LaTeX with hyperref package"}}}