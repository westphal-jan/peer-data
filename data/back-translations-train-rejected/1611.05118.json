{"id": "1611.05118", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Nov-2016", "title": "The Amazing Mysteries of the Gutter: Drawing Inferences Between Panels in Comic Book Narratives", "abstract": "Visual narrative is often a combination of explicit information and judicious omissions, relying on the viewer to supply missing details. In comics, most movements in time and space are hidden in the \"gutters\" between panels. To follow the story, readers logically connect panels together by inferring unseen actions through a process called \"closure\". While computers can now describe the content of natural images, in this paper we examine whether they can understand the closure-driven narratives conveyed by stylized artwork and dialogue in comic book panels. We collect a dataset, COMICS, that consists of over 1.2 million panels (120 GB) paired with automatic textbox transcriptions. An in-depth analysis of COMICS demonstrates that neither text nor image alone can tell a comic book story, so a computer must understand both modalities to keep up with the plot. We introduce three cloze-style tasks that ask models to predict narrative and character-centric aspects of a panel given n preceding panels as context. Various deep neural architectures underperform human baselines on these tasks, suggesting that COMICS contains fundamental challenges for both vision and language.", "histories": [["v1", "Wed, 16 Nov 2016 02:16:09 GMT  (9072kb,D)", "http://arxiv.org/abs/1611.05118v1", null], ["v2", "Sun, 7 May 2017 20:26:24 GMT  (9230kb,D)", "http://arxiv.org/abs/1611.05118v2", null]], "reviews": [], "SUBJECTS": "cs.CV cs.CL", "authors": ["mohit iyyer", "varun manjunatha", "anupam guha", "yogarshi vyas", "jordan boyd-graber", "hal daum\\'e iii", "larry davis"], "accepted": false, "id": "1611.05118"}, "pdf": {"name": "1611.05118.pdf", "metadata": {"source": "CRF", "title": "The Amazing Mysteries of the Gutter: Drawing Inferences Between Panels in Comic Book Narratives", "authors": ["Mohit Iyyer", "Varun Manjunatha", "Anupam Guha", "Yogarshi Vyas", "Jordan Boyd-Graber", "Hal Daum\u00e9 III", "Larry Davis"], "emails": ["miyyer@umiacs.umd.edu", "varunm@umiacs.umd.edu", "aguha@umiacs.umd.edu", "yogarshi@umiacs.umd.edu", "hal@umiacs.umd.edu", "lsd@umiacs.umd.edu", "jordan.boyd.graber@colorado.edu"], "sections": [{"heading": "1. Introduction", "text": "In fact, the author hides from his pages the unspoken conversations and unspoken actions that lurk in the rooms (or in the corridors) in which he is staying. So it is, for example, that he has his snakes in front of the door to hunt them."}, {"heading": "2. Creating a dataset of comic books", "text": "Comics, which cartoonist Will Eisner has defined as sequential art [13], tell their stories in sequences of panels or frames that can contain both images and text. Existing comic data sets [19, 39] are too small to train data-hungry machine learning models for narrative understanding; they also lack diversity in visual style and genre. Therefore, we build our own data set, COMICS, by (1) downloading comics in public space, (2) segmenting each page into panels, (3) extracting textbox positions from panels, and (4) executing OCR on textboxes and editing the output. Table 1 summarizes the content of COMICS. The rest of this section describes every step of our data creation pipeline."}, {"heading": "2.1. Where do our comics come from?", "text": "The \"Golden Age of Comics\" began during the Great Depression in America and lasted until World War II, ending in the mid-1950s with the adoption of strict censorship regulations. Unlike the long, world-building story arcs that were popular in later eras, the comics of the Golden Age tend to be small and self-contained; a single book usually contains several different stories that share a common theme (e.g. crime or mystery). While the best-selling comics of the Golden Age tell of American superheroes triumphing over German and Japanese villains, a variety of other genres (such as romance, humor, and horror) also became popular [18]."}, {"heading": "2.2. Breaking comics into their basic elements", "text": "The DCM comics are distributed as compressed archives of JPEG page scans. To analyze the closure that occurs from panel to panel, we first extract panels from the side images. Next, we extract text boxes from the panels, as both the location and the content of text boxes are important for the understanding of character and storytelling. Panel Segmentation: Previous work on panel segmentation used heuristics [34] or algorithms such as density gradients and recursive cuts [52, 43, 48] based on pages with uniform white backgrounds and clean gutters. Unfortunately, scanned images of eighty-year-old comics are do2http: / / digitalcomicmuseum.com / 3Some of the panels in COMICS contain offensive cartoons and opinions reflecting on this period in American history."}, {"heading": "2.3. OCR", "text": "The final step in our data generation pipeline is to apply OCR to the extracted textbox images. We have experimented unsuccessfully with two trainable open source OCR systems, Tesseract [50] and Ocular [6], as well as Abby's consumer-oriented FineReader.6 The ineffectiveness of these systems is probably due to the considerable variation in comic writing and domain discrepancies with pre-trained language models (comic text is always capitalized, and dialog phenomena such as dialects cannot be adequately represented in training data). Google's Cloud Vision OCR7 performs much better on comics than any other system we have tried. Although it sometimes has difficulty recognizing short words or punctuation, the quality of the transcriptions is good con-4Alternatively, modules for textspotting and recognition [27] can be built into architectures for our downstream tasks, but codialogs would probably be slow, as these modules can be very slow to work."}, {"heading": "3. Data Analysis", "text": "In this section, we explore what makes the understanding of narratives in COMICS difficult and focus specifically on the behavior of intrapanels (how images and text interact within a panel) and interpanel transitions (how narration penetrates from one panel to the next). We characterize panels and transitions using a modified version of the annotation scheme in Scott McCloud's Understanding Comics. [40] Over 90% of the panels rely on text and image to convey information, but the use of a single modality is also important: to understand most transitions between panels, readers must draw complex conclusions that often require common sense (e.g. linking leaps in space and / or time, recognizing when new characters are introduced into an existing scene). We conclude that any model trained to understand the narrative flow in COMICS must link multimodal inputs through closure."}, {"heading": "4. Tasks that test closure", "text": "This year, it is so far that it will be able to retaliate, \"he says.\" But it is not yet that we will be able to do it, \"he says.\" But it is still too early to do it, \"he says,\" but it is still too early to do it. \""}, {"heading": "4.1. Task Difficulty", "text": "In Textcloze and Visual Cloze, we have two levels of difficulty, which differ in the way Cloze candidates are selected: in the simple setting, random text boxes (or panel images) from the entire COMICS dataset. Most false candidates in the simple setting are unrelated to the context provided, as they come from completely different books and genres. Therefore, this setting is easier for models to \"cheat\" by relying on stylistic indicators rather than context-related information. Thus, the task is not trivial; for example, many short dialogs can be applicable in a variety of scenarios. In the hard case, candidates come from nearby pages, so models must rely on context to perform well. In Textcloze, all candidates will likely mention the same character names and entities, while color schemes and textures for visual Cloze become much less distinguishable."}, {"heading": "5. Models & Experiments", "text": "To measure the difficulty of these tasks for deep learning models, we adapt strong baselines for multimodal speech and visual comprehension tasks to the comic domain. We evaluate four different neural models, whose variants were also used to name the Visual Question Answering dataset [2] and to encode the context for visual storytelling [25]: plain text, picture-only text, and two image-text models. Our most powerful model encodes panels with a hierarchical LSTM architecture (see Figure 5). Performance increases when models receive images (in the form of pre-trained VGG-16 features) in addition to text, which supports the insights we draw from our data analysis in Section 3. In addition, models perform much worse on the hard drive in Textcloze and visual Cloze tasks than the simple setting, which confirms our intuition that these tasks are not trivial when we control stylistic differences between candidates."}, {"heading": "5.1. Model definitions", "text": "This year it has come to the point that it has never come as far as this year."}, {"heading": "6. Error Analysis", "text": "Most of them are not able to gain access to the works of art in their entirety. Adam [29] optimizes the models for ten epochs, after which we choose the best model for the dev set. We cannot apply this model because we are not able to grasp the works of art in their entirety."}, {"heading": "7. Related Work", "text": "Our work focuses on three main areas: (1) multimodal tasks requiring language and visual comprehension, (2) computational methods that focus on non-natural images, and (3) models that characterize language-based narratives. Deep learning has renewed the interest in joint vision and language. Datasets such as MS COCO [35] and Visual Genome [31] have enabled captions [54, 28, 56] and answered visual questions [37, 36]. Similar to our character coherence task, researchers have built models that match characters with their visual attributes [15] and speech patterns [21]."}, {"heading": "8. Conclusion & Future Work", "text": "We present the COMICS dataset, which contains over 1.2 million panels from \"Golden Age\" comics. We design three cloze-style tasks on COMICS to explore closures, or how readers combine disparate panels into coherent stories. Experiments with different neural architectures and manual data analysis confirm the importance of multimodal models that combine text and image for the understanding of comics. We also show that context is critical for predicting narrative or character-centric aspects of panels. However, for computers to reach human performance, they need to be able to make better use of context. Readers rely on sound knowledge to understand dramatic scene and camera changes; how can we inject this knowledge into our models?"}], "references": [{"title": "Sort story: Sorting jumbled images and captions into stories", "author": ["H. Agrawal", "A. Chandrasekaran", "D. Batra", "D. Parikh", "M. Bansal"], "venue": "Proceedings of Empirical Methods in Natural Language Processing,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2016}, {"title": "Vqa: Visual question answering", "author": ["S. Antol", "A. Agrawal", "J. Lu", "M. Mitchell", "D. Batra", "C. Lawrence Zitnick", "D. Parikh"], "venue": "International Conference on Computer Vision,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2015}, {"title": "Interactive segmentation for manga", "author": ["Y. Aramaki", "Y. Matsui", "T. Yamasaki", "K. Aizawa"], "venue": "Special Interest Group on Computer Graphics and Interactive Techniques Conference,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2014}, {"title": "Cross-modal scene networks", "author": ["Y. Aytar", "L. Castrejon", "C. Vondrick", "H. Pirsiavash", "A. Torralba"], "venue": "arXiv,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2016}, {"title": "A bayesian mixed effects model of literary character", "author": ["D. Bamman", "T. Underwood", "N.A. Smith"], "venue": "Proceedings of the Association for Computational Linguistics,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2014}, {"title": "Unsupervised transcription of historical documents", "author": ["T. Berg-Kirkpatrick", "G. Durrett", "D. Klein"], "venue": "Proceedings of the Association for Computational Linguistics,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2013}, {"title": "Visual analysis of cartoons: A view from the far side", "author": ["P.J. Carroll", "J.R. Young", "M.S. Guertin"], "venue": "Eye movements and visual cognition. Springer,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1992}, {"title": "Unsupervised learning of narrative schemas and their participants", "author": ["N. Chambers", "D. Jurafsky"], "venue": "Proceedings of the Association for Computational Linguistics,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2009}, {"title": "Return of the devil in the details: Delving deep into convolutional nets", "author": ["K. Chatfield", "K. Simonyan", "A. Vedaldi", "A. Zisserman"], "venue": "British Machine Vision Conference,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2014}, {"title": "The limits of time and transitions: challenges to theories of sequential image comprehension", "author": ["N. Cohn"], "venue": "Studies in Comics, 1(1),", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2010}, {"title": "The state of the art: Object retrieval in paintings using discriminative regions", "author": ["E. Crowley", "A. Zisserman"], "venue": "British Machine Vision Conference,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2014}, {"title": "Face painting: querying art with photos", "author": ["E.J. Crowley", "O.M. Parkhi", "A. Zisserman"], "venue": "British Machine Vision Conference,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2015}, {"title": "Comics & Sequential Art", "author": ["W. Eisner"], "venue": "Poorhouse Press,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1990}, {"title": "Extracting social networks from literary fiction", "author": ["D.K. Elson", "N. Dames", "K.R. McKeown"], "venue": "Proceedings of the Association for Computational Linguistics,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2010}, {"title": "Hello! my name is.", "author": ["M. Everingham", "J. Sivic", "A. Zisserman"], "venue": "buffy\u201d \u2013 automatic naming of characters in TV video. In Proceedings of the British Machine Vision Conference,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2006}, {"title": "Reading without words: Eye movements in the comprehension of comic strips", "author": ["T. Foulsham", "D. Wybrow", "N. Cohn"], "venue": "Applied Cognitive Psychology, 30,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2016}, {"title": "Generative adversarial nets", "author": ["I. Goodfellow", "J. Pouget-Abadie", "M. Mirza", "B. Xu", "D. Warde-Farley", "S. Ozair", "A. Courville", "Y. Bengio"], "venue": "Proceedings of Advances in Neural Information Processing Systems,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2014}, {"title": "Comic Book Encyclopedia: The Ultimate Guide to Characters, Graphic Novels, Writers, and Artists in the Comic Book Universe", "author": ["R. Goulart"], "venue": "HarperCollins,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2004}, {"title": "ebdtheque: a representative database of comics", "author": ["C. Gu\u00e9rin", "C. Rigaud", "A. Mercier", "F. Ammar-Boudjelal", "K. Bertet", "A. Bouju", "J.-C. Burie", "G. Louis", "J.-M. Ogier", "A. Revel"], "venue": "International Conference on Document Analysis and Recognition,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2013}, {"title": "A distorted skull lies in the bottom center: Identifying paintings from text descriptions", "author": ["A. Guha", "M. Iyyer", "J. Boyd-Graber"], "venue": "NAACL Human-Computer Question Answering Workshop,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2016}, {"title": "Naming TV characters by watching and analyzing dialogs", "author": ["M. Haurilet", "M. Tapaswi", "Z. Al-Halah", "R. Stiefelhagen"], "venue": "2016 IEEE Winter Conference on Applications of Computer Vision, WACV 2016, Lake Placid, NY, USA, March 7- 10, 2016,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2016}, {"title": "Panel and speech balloon extraction from comic books", "author": ["A.K.N. Ho", "J.-C. Burie", "J.-M. Ogier"], "venue": "IAPR International Workshop on Document Analysis Systems,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2012}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 1997}, {"title": "Visual storytelling", "author": ["T.-H.K. Huang", "F. Ferraro", "N. Mostafazadeh", "I. Misra", "A. Agrawal", "J. Devlin", "R. Girshick", "X. He", "P. Kohli", "D. Batra"], "venue": "In Conference of the North American Chapter of the Association for Computational Linguistics,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2016}, {"title": "Visual storytelling", "author": ["T.K. Huang", "F. Ferraro", "N. Mostafazadeh", "I. Misra", "A. Agrawal", "J. Devlin", "R.B. Girshick", "X. He", "P. Kohli", "D. Batra", "C.L. Zitnick", "D. Parikh", "L. Vanderwende", "M. Galley", "M. Mitchell"], "venue": "NAACL HLT 2016, The 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, San Diego California, USA, June 12-17, 2016, pages 1233\u20131239,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2016}, {"title": "Feuding families and former friends: Unsupervised learning for dynamic fictional relationships", "author": ["M. Iyyer", "A. Guha", "S. Chaturvedi", "J. Boyd-Graber", "H. Daum\u00e9 III"], "venue": "Conference of the North American Chapter of the Association for Computational Linguistics,", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2016}, {"title": "Reading text in the wild with convolutional neural networks", "author": ["M. Jaderberg", "K. Simonyan", "A. Vedaldi", "A. Zisserman"], "venue": "International Journal of Computer Vision, 116(1),", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2016}, {"title": "Deep visual-semantic alignments for generating image descriptions", "author": ["A. Karpathy", "F. Li"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2015, Boston, MA, USA, June 7-12, 2015,", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2015}, {"title": "Adam: A method for stochastic optimization", "author": ["D. Kingma", "J. Ba"], "venue": "Proceedings of the International Conference on Learning Representations,", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2014}, {"title": "A layered method for determining manga text bubble reading order", "author": ["S. Kovanen", "K. Aizawa"], "venue": "International Conference on Image Processing,", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2015}, {"title": "Visual genome: Connecting language and vision using crowdsourced dense image annotations. 2016", "author": ["R. Krishna", "Y. Zhu", "O. Groth", "J. Johnson", "K. Hata", "J. Kravitz", "S. Chen", "Y. Kalantidis", "L.-J. Li", "D.A. Shamma", "M. Bernstein", "L. Fei-Fei"], "venue": null, "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2016}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "Proceedings of Advances in Neural Information Processing Systems,", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2012}, {"title": "Plot units and narrative summarization", "author": ["W.G. Lehnert"], "venue": "Cognitive Science, 5(4),", "citeRegEx": "33", "shortCiteRegEx": null, "year": 1981}, {"title": "Automatic comic page segmentation based on polygon detection", "author": ["L. Li", "Y. Wang", "Z. Tang", "L. Gao"], "venue": "Multimedia Tools and Applications, 69(1),", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2014}, {"title": "Hierarchical question-image co-attention for visual question answering, 2016", "author": ["J. Lu", "J. Yang", "D. Batra", "D. Parikh"], "venue": null, "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2016}, {"title": "Ask your neurons: A neural-based approach to answering questions about images", "author": ["M. Malinowski", "M. Rohrbach", "M. Fritz"], "venue": "Computer Vision and Pattern Recognition,", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2015}, {"title": "Challenge for manga processing: Sketch-based manga retrieval", "author": ["Y. Matsui"], "venue": "Proceedings of the 23rd Annual ACM Conference on Multimedia,", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2015}, {"title": "Sketch-based manga retrieval using manga109 dataset", "author": ["Y. Matsui", "K. Ito", "Y. Aramaki", "T. Yamasaki", "K. Aizawa"], "venue": "arXiv preprint arXiv:1510.04389,", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2015}, {"title": "Understanding Comics", "author": ["S. McCloud"], "venue": "HarperCollins,", "citeRegEx": "40", "shortCiteRegEx": null, "year": 1994}, {"title": "A computer oriented geodetic data base and a new technique in file sequencing", "author": ["G.M. Morton"], "venue": "International Business Machines Co,", "citeRegEx": "41", "shortCiteRegEx": null, "year": 1966}, {"title": "A corpus and cloze evaluation for deeper understanding of commonsense stories", "author": ["N. Mostafazadeh", "N. Chambers", "X. He", "D. Parikh", "D. Batra", "L. Vanderwende", "P. Kohli", "J. Allen"], "venue": "Conference of the North American Chapter of the Association for Computational Linguistics,", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2016}, {"title": "A robust panel extraction method for manga", "author": ["X. Pang", "Y. Cao", "R.W. Lau", "A.B. Chan"], "venue": "Proceedings of the 22nd ACM international conference on Multimedia,", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2014}, {"title": "A robust panel extraction method for manga", "author": ["X. Pang", "Y. Cao", "R.W.H. Lau", "A.B. Chan"], "venue": "Proceedings of the ACM International Conference on Multimedia,", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2014}, {"title": "Faster R-CNN: Towards real-time object detection with region proposal networks", "author": ["S. Ren", "K. He", "R. Girshick", "J. Sun"], "venue": "Proceedings of Advances in Neural Information Processing Systems,", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2015}, {"title": "Segmentation and indexation of complex objects in comic book images", "author": ["C. Rigaud"], "venue": "PhD thesis, University of La Rochelle, France,", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2014}, {"title": "An active contour model for speech balloon detection in comics", "author": ["C. Rigaud", "J.-C. Burie", "J.-M. Ogier", "D. Karatzas", "J. Van de Weijer"], "venue": "In International Conference on Document Analysis and Recognition,", "citeRegEx": "47", "shortCiteRegEx": "47", "year": 2013}, {"title": "Knowledge-driven understanding of images in comic books", "author": ["C. Rigaud", "C. Gu\u00e9rin", "D. Karatzas", "J.-C. Burie", "J.-M. Ogier"], "venue": "International Journal on Document Analysis and Recognition, 18(3),", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2015}, {"title": "Scripts, Plans, Goals and Understanding: an Inquiry into Human Knowledge Structures", "author": ["R. Schank", "R. Abelson"], "venue": "L. Erlbaum,", "citeRegEx": "49", "shortCiteRegEx": null, "year": 1977}, {"title": "An overview of the tesseract ocr engine", "author": ["R. Smith"], "venue": "International Conference on Document Analysis and Recognition,", "citeRegEx": "50", "shortCiteRegEx": null, "year": 2007}, {"title": "Grounded compositional semantics for finding and describing images with sentences", "author": ["R. Socher", "Q.V. Le", "C.D. Manning", "A.Y. Ng"], "venue": "Transactions of the Association for Computational Linguistics,", "citeRegEx": "51", "shortCiteRegEx": null, "year": 2014}, {"title": "Layout analysis of tree-structured scene frames in comic images", "author": ["T. Tanaka", "K. Shoji", "F. Toyama", "J. Miyamichi"], "venue": "International Joint Conference on Artificial Intelligence,", "citeRegEx": "52", "shortCiteRegEx": null, "year": 2007}, {"title": "Cloze procedure: a new tool for measuring readability", "author": ["W.L. Taylor"], "venue": "Journalism and Mass Communication Quarterly, 30(4),", "citeRegEx": "53", "shortCiteRegEx": null, "year": 1953}, {"title": "Show and tell: A neural image caption generator", "author": ["O. Vinyals", "A. Toshev", "S. Bengio", "D. Erhan"], "venue": "Computer Vision and Pattern Recognition,", "citeRegEx": "54", "shortCiteRegEx": null, "year": 2015}, {"title": "Dynamic memory networks for visual and textual question answering", "author": ["C. Xiong", "S. Merity", "R. Socher"], "venue": "Proceedings of the International Conference of Machine Learning,", "citeRegEx": "55", "shortCiteRegEx": null, "year": 2016}, {"title": "Show, attend and tell: Neural image caption generation with visual attention", "author": ["K. Xu", "J. Ba", "R. Kiros", "K. Cho", "A. Courville", "R. Salakhutdinov", "R. Zemel", "Y. Bengio"], "venue": "Proceedings of the International Conference of Machine Learning,", "citeRegEx": "56", "shortCiteRegEx": null, "year": 2015}, {"title": "Simple baseline for visual question answering", "author": ["B. Zhou", "Y. Tian", "S. Sukhbaatar", "A. Szlam", "R. Fergus"], "venue": "arXiv preprint arXiv:1512.02167,", "citeRegEx": "57", "shortCiteRegEx": null, "year": 2015}, {"title": "Adopting abstract images for semantic scene understanding", "author": ["C.L. Zitnick", "R. Vedantam", "D. Parikh"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell., 38(4):627\u2013638,", "citeRegEx": "58", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 38, "context": "Through a process called closure [40], which involves (1) understanding individual panels and (2) making connective inferences across panels, readers form coherent storylines from seemingly disparate panels such as these.", "startOffset": 33, "endOffset": 37}, {"referenceID": 31, "context": ", photographs), which are the focus of most existing datasets and methods [32, 56, 55].", "startOffset": 74, "endOffset": 86}, {"referenceID": 54, "context": ", photographs), which are the focus of most existing datasets and methods [32, 56, 55].", "startOffset": 74, "endOffset": 86}, {"referenceID": 53, "context": ", photographs), which are the focus of most existing datasets and methods [32, 56, 55].", "startOffset": 74, "endOffset": 86}, {"referenceID": 9, "context": "To understand a story, readers must connect dialogue and narration to characters and environments; furthermore, the text must be read in the proper order, as panels often depict long scenes rather than individual moments [10].", "startOffset": 221, "endOffset": 225}, {"referenceID": 24, "context": "Text plays a much larger role in COMICS than it does for existing datasets of visual stories [25].", "startOffset": 93, "endOffset": 97}, {"referenceID": 12, "context": "Comics, defined by cartoonist Will Eisner as sequential art [13], tell their stories in sequences of panels, or single frames that can contain both images and text.", "startOffset": 60, "endOffset": 64}, {"referenceID": 18, "context": "Existing comics datasets [19, 39] are too small to train data-hungry machine learning models for narrative understanding; additionally, they lack diversity in visual style and genres.", "startOffset": 25, "endOffset": 33}, {"referenceID": 37, "context": "Existing comics datasets [19, 39] are too small to train data-hungry machine learning models for narrative understanding; additionally, they lack diversity in visual style and genres.", "startOffset": 25, "endOffset": 33}, {"referenceID": 17, "context": "While the best-selling Golden Age comics tell of American superheroes triumphing over German and Japanese villains, a variety of other genres (such as romance, humor, and horror) also became popular [18].", "startOffset": 199, "endOffset": 203}, {"referenceID": 33, "context": "Panel segmentation: Previous work on panel segmentation uses heuristics [34] or algorithms such as density gradients and recursive cuts [52, 43, 48] that rely on pages with uniformly white backgrounds and clean gutters.", "startOffset": 72, "endOffset": 76}, {"referenceID": 50, "context": "Panel segmentation: Previous work on panel segmentation uses heuristics [34] or algorithms such as density gradients and recursive cuts [52, 43, 48] that rely on pages with uniformly white backgrounds and clean gutters.", "startOffset": 136, "endOffset": 148}, {"referenceID": 41, "context": "Panel segmentation: Previous work on panel segmentation uses heuristics [34] or algorithms such as density gradients and recursive cuts [52, 43, 48] that rely on pages with uniformly white backgrounds and clean gutters.", "startOffset": 136, "endOffset": 148}, {"referenceID": 46, "context": "Panel segmentation: Previous work on panel segmentation uses heuristics [34] or algorithms such as density gradients and recursive cuts [52, 43, 48] that rely on pages with uniformly white backgrounds and clean gutters.", "startOffset": 136, "endOffset": 148}, {"referenceID": 43, "context": "In particular, we use Faster R-CNN [45] initialized with a pretrained VGG CNN M 1024 model [9] and alternatingly optimize the region proposal network and the detection network.", "startOffset": 35, "endOffset": 39}, {"referenceID": 8, "context": "In particular, we use Faster R-CNN [45] initialized with a pretrained VGG CNN M 1024 model [9] and alternatingly optimize the region proposal network and the detection network.", "startOffset": 91, "endOffset": 94}, {"referenceID": 39, "context": "We compute the midpoint of each panel and sort them using Morton order [41], which gives incorrect orderings only for rare and complicated panel layouts.", "startOffset": 71, "endOffset": 75}, {"referenceID": 21, "context": "4 As with panel segmentation, existing comic textbox detection algorithms [22, 47] could not accurately localize textboxes for our data.", "startOffset": 74, "endOffset": 82}, {"referenceID": 45, "context": "4 As with panel segmentation, existing comic textbox detection algorithms [22, 47] could not accurately localize textboxes for our data.", "startOffset": 74, "endOffset": 82}, {"referenceID": 48, "context": "We unsuccessfully experimented with two trainable open-source OCR systems, Tesseract [50] and Ocular [6], as well as Abbyy\u2019s consumergrade FineReader.", "startOffset": 85, "endOffset": 89}, {"referenceID": 5, "context": "We unsuccessfully experimented with two trainable open-source OCR systems, Tesseract [50] and Ocular [6], as well as Abbyy\u2019s consumergrade FineReader.", "startOffset": 101, "endOffset": 104}, {"referenceID": 26, "context": "4Alternatively, modules for text spotting and recognition [27] could be built into architectures for our downstream tasks, but since comic dialogues can be quite lengthy, these modules would likely perform poorly.", "startOffset": 58, "endOffset": 62}, {"referenceID": 38, "context": "We characterize panels and transitions using a modified version of the annotation scheme in Scott McCloud\u2019s \u201cUnderstanding Comics\u201d [40].", "startOffset": 131, "endOffset": 135}, {"referenceID": 23, "context": "While previous work on visual storytelling focuses on generating text given some context [24], the dialogue-heavy text in COMICS makes evaluation difficult (e.", "startOffset": 89, "endOffset": 93}, {"referenceID": 51, "context": "We want our evaluations to focus specifically on closure, not generated text quality, so we instead use a cloze-style framework [53]: given c candidates\u2014with a single correct option\u2014models must use the context panels to rank the correct candidate higher than the others.", "startOffset": 128, "endOffset": 132}, {"referenceID": 6, "context": "This design is motivated by eyetracking studies in single-panel cartoons, which show that readers look at artwork before reading the text [7], although in many cases atypical font style and text length can invert this order [16].", "startOffset": 138, "endOffset": 141}, {"referenceID": 15, "context": "This design is motivated by eyetracking studies in single-panel cartoons, which show that readers look at artwork before reading the text [7], although in many cases atypical font style and text length can invert this order [16].", "startOffset": 224, "endOffset": 228}, {"referenceID": 1, "context": "We evaluate four different neural models, variants of which were also used to benchmark the Visual Question Answering dataset [2] and encode context for visual storytelling [25]: text-only, image-only, and two image-text models.", "startOffset": 126, "endOffset": 129}, {"referenceID": 24, "context": "We evaluate four different neural models, variants of which were also used to benchmark the Visual Question Answering dataset [2] and encode context for visual storytelling [25]: text-only, image-only, and two image-text models.", "startOffset": 173, "endOffset": 177}, {"referenceID": 22, "context": "Our g function encodes this text on multiple levels: we first compute a representation for each tix with a word embedding sum 12 and then combine multiple textboxes within the same panel using an intrapanel LSTM [23].", "startOffset": 212, "endOffset": 216}, {"referenceID": 49, "context": "11Performance falters slightly on a development set with contrastive max-margin loss functions [51] in place of our softmax alternative.", "startOffset": 95, "endOffset": 99}, {"referenceID": 55, "context": "12As in previous work for visual question answering [57], we observe no noticeable improvement with more sophisticated encoding architectures.", "startOffset": 52, "endOffset": 56}, {"referenceID": 3, "context": "We also try fine-tuning the lower-level layers of VGG-16 [4]; however, this substantially lowers task accuracy even with very small learning rates for the fine-tuned layers.", "startOffset": 57, "endOffset": 60}, {"referenceID": 28, "context": "Models are optimized using Adam [29] for ten epochs, after which we select the best-performing model on the dev set.", "startOffset": 32, "endOffset": 36}, {"referenceID": 30, "context": "Datasets such as MS COCO [35] and Visual Genome [31] have enabled image caption-", "startOffset": 48, "endOffset": 52}, {"referenceID": 52, "context": "ing [54, 28, 56] and visual question answering [37, 36].", "startOffset": 4, "endOffset": 16}, {"referenceID": 27, "context": "ing [54, 28, 56] and visual question answering [37, 36].", "startOffset": 4, "endOffset": 16}, {"referenceID": 54, "context": "ing [54, 28, 56] and visual question answering [37, 36].", "startOffset": 4, "endOffset": 16}, {"referenceID": 35, "context": "ing [54, 28, 56] and visual question answering [37, 36].", "startOffset": 47, "endOffset": 55}, {"referenceID": 34, "context": "ing [54, 28, 56] and visual question answering [37, 36].", "startOffset": 47, "endOffset": 55}, {"referenceID": 14, "context": "Similar to our character coherence task, researchers have built models that match TV show characters with their visual attributes [15] and speech patterns [21].", "startOffset": 130, "endOffset": 134}, {"referenceID": 20, "context": "Similar to our character coherence task, researchers have built models that match TV show characters with their visual attributes [15] and speech patterns [21].", "startOffset": 155, "endOffset": 159}, {"referenceID": 23, "context": "Closest to our own comic book setting is the visual storytelling task, in which systems must generate [24] or reorder [1] stories given a dataset (SIND) of photos from Flikr galleries of \u201cstoryable\u201d events such as weddings and birthday parties.", "startOffset": 102, "endOffset": 106}, {"referenceID": 0, "context": "Closest to our own comic book setting is the visual storytelling task, in which systems must generate [24] or reorder [1] stories given a dataset (SIND) of photos from Flikr galleries of \u201cstoryable\u201d events such as weddings and birthday parties.", "startOffset": 118, "endOffset": 121}, {"referenceID": 56, "context": "[58] discover semantic scene properties from a clip art dataset featuring characters and objects in a limited variety of settings.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "Applications of deep learning to paintings include tasks such as detecting objects in oil paintings [11, 12] and answering questions about artwork [20].", "startOffset": 100, "endOffset": 108}, {"referenceID": 11, "context": "Applications of deep learning to paintings include tasks such as detecting objects in oil paintings [11, 12] and answering questions about artwork [20].", "startOffset": 100, "endOffset": 108}, {"referenceID": 19, "context": "Applications of deep learning to paintings include tasks such as detecting objects in oil paintings [11, 12] and answering questions about artwork [20].", "startOffset": 147, "endOffset": 151}, {"referenceID": 44, "context": "Previous computational work on comics focuses primarily on extracting elements such as panels and textboxes [46]; in addition to the references in Section 2, there is a large body of segmentation research on manga [3, 44, 38, 30].", "startOffset": 108, "endOffset": 112}, {"referenceID": 2, "context": "Previous computational work on comics focuses primarily on extracting elements such as panels and textboxes [46]; in addition to the references in Section 2, there is a large body of segmentation research on manga [3, 44, 38, 30].", "startOffset": 214, "endOffset": 229}, {"referenceID": 42, "context": "Previous computational work on comics focuses primarily on extracting elements such as panels and textboxes [46]; in addition to the references in Section 2, there is a large body of segmentation research on manga [3, 44, 38, 30].", "startOffset": 214, "endOffset": 229}, {"referenceID": 36, "context": "Previous computational work on comics focuses primarily on extracting elements such as panels and textboxes [46]; in addition to the references in Section 2, there is a large body of segmentation research on manga [3, 44, 38, 30].", "startOffset": 214, "endOffset": 229}, {"referenceID": 29, "context": "Previous computational work on comics focuses primarily on extracting elements such as panels and textboxes [46]; in addition to the references in Section 2, there is a large body of segmentation research on manga [3, 44, 38, 30].", "startOffset": 214, "endOffset": 229}, {"referenceID": 40, "context": "We follow previous work in language-based narrative understanding; very similar to our text cloze task is the \u201cStory Cloze Test\u201d [42], in which models must predict the ending to a short (four sentences long) story.", "startOffset": 129, "endOffset": 133}, {"referenceID": 13, "context": "Others have studied characters [14, 5, 26] and narrative structure [49, 33, 8] in novels.", "startOffset": 31, "endOffset": 42}, {"referenceID": 4, "context": "Others have studied characters [14, 5, 26] and narrative structure [49, 33, 8] in novels.", "startOffset": 31, "endOffset": 42}, {"referenceID": 25, "context": "Others have studied characters [14, 5, 26] and narrative structure [49, 33, 8] in novels.", "startOffset": 31, "endOffset": 42}, {"referenceID": 47, "context": "Others have studied characters [14, 5, 26] and narrative structure [49, 33, 8] in novels.", "startOffset": 67, "endOffset": 78}, {"referenceID": 32, "context": "Others have studied characters [14, 5, 26] and narrative structure [49, 33, 8] in novels.", "startOffset": 67, "endOffset": 78}, {"referenceID": 7, "context": "Others have studied characters [14, 5, 26] and narrative structure [49, 33, 8] in novels.", "startOffset": 67, "endOffset": 78}, {"referenceID": 16, "context": "Readers rely on commonsense knowledge to make sense of dramatic scene and camera changes; how can we inject such knowledge into our models? Another potentially intriguing direction, especially given recent advances in generative adversarial networks [17], is generating artwork given dialogue (or vice versa).", "startOffset": 250, "endOffset": 254}], "year": 2016, "abstractText": "Visual narrative is often a combination of explicit information and judicious omissions, relying on the viewer to supply missing details. In comics, most movements in time and space are hidden in the \u201cgutters\u201d between panels. To follow the story, readers logically connect panels together by inferring unseen actions through a process called \u201cclosure\u201d. While computers can now describe what is explicitly depicted in natural images, in this paper we examine whether they can understand the closure-driven narratives conveyed by stylized artwork and dialogue in comic book panels. We construct a dataset, COMICS, that consists of over 1.2 million panels (120 GB) paired with automatic textbox transcriptions. An in-depth analysis of COMICS demonstrates that neither text nor image alone can tell a comic book story, so a computer must understand both modalities to keep up with the plot. We introduce three cloze-style tasks that ask models to predict narrative and character-centric aspects of a panel given n preceding panels as context. Various deep neural architectures underperform human baselines on these tasks, suggesting that COMICS contains fundamental challenges for both vision and language.", "creator": "LaTeX with hyperref package"}}}