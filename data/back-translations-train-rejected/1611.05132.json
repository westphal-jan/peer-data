{"id": "1611.05132", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Nov-2016", "title": "Convergence rate of stochastic k-means", "abstract": "We analyze online \\cite{BottouBengio} and mini-batch \\cite{Sculley} $k$-means variants. Both scale up the widely used $k$-means algorithm via stochastic approximation, and have become popular for large-scale clustering and unsupervised feature learning. We show, for the first time, that starting with any initial solution, they converge to a \"local optimum\" at rate $O(\\frac{1}{t})$ (in terms of the $k$-means objective) under general conditions. In addition, we show if the dataset is clusterable, when initialized with a simple and scalable seeding algorithm, mini-batch $k$-means converges to an optimal $k$-means solution at rate $O(\\frac{1}{t})$ with high probability. The $k$-means objective is non-convex and non-differentiable: we exploit ideas from recent work on stochastic gradient descent for non-convex problems \\cite{ge:sgd_tensor, balsubramani13} by providing a novel characterization of the trajectory of $k$-means algorithm on its solution space, and circumvent the non-differentiability problem via geometric insights about $k$-means update.", "histories": [["v1", "Wed, 16 Nov 2016 03:28:08 GMT  (434kb,D)", "http://arxiv.org/abs/1611.05132v1", "arXiv admin note: substantial text overlap witharXiv:1610.04900"]], "COMMENTS": "arXiv admin note: substantial text overlap witharXiv:1610.04900", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["cheng tang", "claire monteleoni"], "accepted": false, "id": "1611.05132"}, "pdf": {"name": "1611.05132.pdf", "metadata": {"source": "CRF", "title": "Convergence rate of stochastic k-means", "authors": ["Cheng Tang", "Claire Monteleoni"], "emails": ["tangch@gwu.edu", "cmontel@gwu.edu"], "sections": [{"heading": "1 Introduction", "text": "In fact, it is about the question of how the future of the world is organized, and about the question of how the future of the world is organized. (...) It is about the question of how the future of the world is organized. (...) It is about the question of how the future of the world is organized. (...) It is about the question of how the future of the world is organized. (...) It is about the question of how the future of the world is organized. (...) It is about the question of how the future of the world is organized. (...) It is about the question of how the future of the world is organized. (...) It is about the question of how the future of the world is organized, about the future of the world, about the world of the world, about the world of the world, about the world of the world, about the world of the world, about the world of the world, about the world of the world of the world, about the world of the world of the world, about the world of the world of the world of the world, about the world of the world of the world of the world, about the world of the world of the world of the world, about the world of the world of the world of the world of the world of the world, about the world of the world of the world of the world of the world, about the world of the world of the world of the world of the world of the world, about the world of the world of the world of the world of the world of the world of the world of the world, about the world of the world of the world of the world of the world of the world of the world of the world of the world, the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world, the world of the world of the world of the world of the world of the world of the world of the world of the world of the world, the world of the world of the world of the world of the world of the world of the world of the world of the world of the world, the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of"}, {"heading": "3 Main result", "text": "With two weak assumptions about the characteristics of stationary points in a DatasetX = > result, stochastic k-averages as a whole show a convergence rate (1t). (A0) We assume that all stationary points are non-borderline, i.e. that all stationary points are non-borderline (A1). (A1) We assume that there is an upper borderline B-max rate (A). (A1) We assume that all stationary points are non-borderline (rmin, 0) -stable. (A1) We assume that there is an upper borderline. (A). (A). (A). (A). (A). (A)............................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................"}, {"heading": "3.1 Related work and proof outline", "text": "Our main sources of inspiration stem from recent advances in non-convex stochastic optimizations for unattended learning problems (8, 3]. [8] Studies the convergence of stochastic gradient lineage (SGD) for the tensor decomposition problem, which amounts to finding a local optimum of a nonconvex objective function composed exclusively of saddle points and local optima. [8] Studies the convergence of stochastic lineage (SGD) in their analytical frames, we divide our analysis of algorithm 1 into two phases, global convergence and local convergence, indicated by the distance from the current solution to stationary points, (Ct, C). We use convergence: = (Ct, C) as shorthand.Significant decrease in k means objectively, if the global convergence is large, the algorithm is not global in the convergence phase."}, {"heading": "4 Experiments", "text": "To verify the O (1t) convergence rate of theory 1, we run stochastic k averages with different learning rates, mini-batch size, and k on RCV1 [12]. The data set is relatively large: it has manually categorized 804414 Newswire stories with 103 topics, each story being a 47236-dimensional sparse vector; it was used in [16] for empirical evaluation of mini-batch averages. We used Python and his Scikit Learn package [14] for our experiments where stochastic k averages are implemented. We deactivated the centering shift and modified its source code to enable a user-defined learning rate (their learning rate is fixed as \u03b7tr: = n-tr convergence rate i-n-r, as in [5, 16] which we call BBS rate."}, {"heading": "5 Appendix A: complete version of Section 2", "text": "To facilitate our analysis of mini-batch k-means, we are building a framework to follow the solution path produced by batch k-means; it switches between two solution spaces: the space of all k-means-cases, which we define by {C}, and the space of all k-clusterings, which we produce by {A}, and the space of all k-clusterings, which we define by {A}, and the space of all k-means-cases, which we define by {A). One problem with batch k-means is that it can produce degenerated solutions: if the solution has Ct k-centroids, it is possible for the data points to be mapped to only k-centroids. < k centroids cases, starting with | C0 | k, we will consider the extended clustering space {A}, the unification of all k-clusterings, which we have through {A}, and which we discuss with {K-classes."}, {"heading": "6 Appendix B: proofs of main theorems", "text": "A subtlety that we must point out before the evidence is given is that in algorithm 1 both the learning rate \u03b7tr and the update rule: ctr \u2190 (1 \u2212 \u03b7tr) ct \u2212 1r + \u03b7tr c-tr are defined only for a cluster r that is \"sampled\" during the t-th iteration. However, even if the cluster r is not \"sampled,\" i.e. ctr = ct \u2212 1r, the same update rule with c-tr = ct \u2212 1r and the same learning rate still applies in this case. Therefore, in our analysis we treat each cluster r equally with the same learning rate \u03b7t = c \u00b2 to + t and distinguish between a sampled and unsampled cluster only by defining c-tr."}, {"heading": "6.1 Proofs leading to Theorem 1", "text": "The proof for Lemma 1 + 2 + 1er + 1er + 1er + 1er, 1er + 1er, 1er + 1er, 1er + 1er, 1er + 1er), 1er, 1er, 1er, 1er, 1er, 1er, 1er, 1er, 1er, 1er, 1er, 1er, 1er, 1er, 1er, 1er, 1er, 1er, 1er, 1er, 1er, 1er, 1er, 1er, 1er, 1er, 1er, 1er, 1er, 1er, 1er, 1er, 1er, 1er, 1er, 1er, 1er, 1er, 1er, 1er, 1er, 1er, 1er, 1er, 1er, 1er, 1er, 1er, 1er, 1er, 1er, 1er, 1er, 1er, 1er, 1er, 1er, 1er, 1er, 1er, 1er, 1er, 1er, 1er, 1er, 1er, 1er, 1er, 1er, 1er, 1er, 1er, 1er, 1er, 1er, 1er, 1er, 1er, 1er, 1er, 1er, 1er, 1er, 1er, 1er, 1er, 1er, 1er, 1er, 1er, 1er, 1er, 1er, 1er, 1er, 1er, 1er, 1er, 1er, 1er, 1er, 1er, 1er, 1er, 1er, 1er, 1er, 1er, 1er, 1er, 1er, 1er, 1er, 1er, 1er, 1er, 1er, 1er, 1er, 1er, 1er, 1er, 1er, 1er, 1er, 1er, 1er, 1er, 1er, 1er, 1er, 1er, 1er, 1er, 1er, 1er, 1er, 1er, 1er, 1er, 1er, 1er, 1er, 1er, 1er, 1er, 1er, 1er, 1er, 1er, 1er, 1er, 1er, 1er, 1er"}, {"heading": "G := {\u2203T \u2265 1,\u2203A\u2217\u2217 \u2208 {A\u2217}[k], s.t. At = A\u2217\u2217, \u2200t \u2265 T}", "text": "Secondly, conditioning on G, let us note that the expected convergence rate of the algorithm on all stationary clusters A * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *"}, {"heading": "6.2 Proofs leading to Theorem 3", "text": "In the analysis of this section, we use Et \u2212 \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 \u2212 \u2212 \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 \u2212 \u2212 p \u2212 \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 \u2212 \u2212 p \u2212 p \u2212 \u2212 \u2212 \u2212 p \u2212 \u2212 \u2212 p \u2212 p \u2212 p \u2212 \u2212 \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 \u2212 \u2212 \u2212"}, {"heading": "7 Appendix C: Proofs leading to Theorem 2", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "7.1 Existence of stable stationary point under geometric assumptions on the dataset", "text": "s (c) s (c) s (c) s (c) s (c) s s (c) s (c) s s (c) s (c) s (c) s (c) s (c) s (c) s (c) s (c) s (c) s (c) s (c) s (c) s (c) s (c) s (c) s (c) s (c) s (c) s (c) s (c) s (c) s (c) s (c) s (c) s) s (c) s (c) s (c) s (c) s (c) s (c) s (c) s (c) s (c) s (c) s (c) s (c) s (c) s (c) s (c) s (c) s (c) s (c) s (c) s (c) s (c) s (c) s (c) s (c) s (c) s (c) s (c) s (c) s (c) s (c) s (c) s (c) s (c) s (c) s (c) s (c) s (c) s (c) s (c) s (c) s (c) s (c) s (c (c) s (c) s (c) s (c) s (c) s (c (c) s (c) s (c) s (c) s (c) s (c (c) s (c) s (c) s (c) s (c) s (c (c) s (c (c) s (c) s (c) s (c (c) s (c) s (c (c) s (c) s (c) s (c (c) s (c) s (c (c) s (c) s (c (c) s (c) s (c (c) s (c) s (c (c) s (c (c (c) s (c) s (c) s (c (c) s (c) s (c) s (c (c) s (c) s (c (c (c) s (c) s (c) s ("}, {"heading": "7.2 Proofs regarding seeding guarantee", "text": "Lemma 4 of [17]. Assumption (B1) and (B2) is for stationary clustering with a probability of at least 1 \u2212 mo exp (\u2212 2). If we obtain seeds from algorithm 2, then we can (C0, C). First remark: Assumption (B1) fulfills the mean assumption in definition 1 of [17]. Therefore, the application of Theorem 4 of [17] is with a probability of less than 2 \u2212 r and a probability of more than 1 \u2212 k exp (\u2212 mopmin).Proof. First remark: Assumption (B1) fulfills the mean assumption in definition 1 of [17]."}, {"heading": "8 Appendix D: technical lemmas", "text": "Lemma 15. Letwt, gt \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 vectors of the dimension Rd \u2212 \u03b2 \u03b2 \u03b2 \u03b2 \u03b2 \u03b2 \u03b2 \u03b2 \u03b2 \u03b2 \u03b2 \u03b2 (\u03b2 \u03b2 \u03b2 = 1). T let us repeatedly apply the following updated version ewt = (1 \u2212 1t) wt \u2212 1 + 1 t = 1 gt. Thus, the claim applies to T = 1. Let us replace the claim for T + 1, then for T + 1, by the updated rule rulewT + 1 = (1 \u2212 1) wT + 1T + 1 gT + 1 t = 1 gt. Thus, let us replace the claim for T + 1, then for T + 1, by the updated rule rulewT + 1 = (1 \u2212 1) wT + 1T + 1 gT + 1 = 1 t + 1 t = (1 \u2212 1 T + 1) 1 T = 1 t = 1 t = (1 T + 1 t = 1 t)."}], "references": [{"title": "k-means++: the advantages of careful seeding", "author": ["David Arthur", "Sergei Vassilvitskii"], "venue": "In Proceedings of the Eighteenth Annual ACM-SIAM Symposium on Discrete Algorithms,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2007}, {"title": "Improved spectral-norm bounds for clustering. In Approximation, Randomization, and Combinatorial Optimization. Algorithms and Techniques - 15th International Workshop", "author": ["Pranjal Awasthi", "Or Sheffet"], "venue": "APPROX 2012, and 16th International Workshop,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2012}, {"title": "The fast convergence of incremental PCA. In Advances in Neural Information Processing Systems", "author": ["Akshay Balsubramani", "Sanjoy Dasgupta", "Yoav Freund"], "venue": "Annual Conference on Neural Information Processing Systems", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2013}, {"title": "Online learning and stochastic approximations", "author": ["L\u00e9on Bottou"], "venue": "On-line learning in neural networks,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1998}, {"title": "Convergence properties of the k-means algorithms", "author": ["L\u00e9on Bottou", "Yoshua Bengio"], "venue": "In Advances in Neural Information Processing Systems 7, [NIPS Conference,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1994}, {"title": "Scatter/gather: A cluster-based approach to browsing large document collections", "author": ["Douglas R. Cutting", "Jan O. Pedersen", "David R. Karger", "John W. Tukey"], "venue": "In Proceedings of the 15th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1992}, {"title": "Using the triangle inequality to accelerate k-means", "author": ["Charles Elkan"], "venue": "In Machine Learning, Proceedings of the Twentieth International Conference (ICML", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2003}, {"title": "Escaping from saddle points - online stochastic gradient for tensor decomposition", "author": ["Rong Ge", "Furong Huang", "Chi Jin", "Yang Yuan"], "venue": "In Proceedings of The 28th Conference on Learning Theory, COLT", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2015}, {"title": "Data clustering: 50 years beyond k-means", "author": ["Anil K. Jain"], "venue": "Pattern Recognition Letters,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2010}, {"title": "A local search approximation algorithm for k-means clustering", "author": ["Tapas Kanungo", "David M. Mount", "Nathan S. Netanyahu", "Christine D. Piatko", "Ruth Silverman", "Angela Y. Wu"], "venue": "Comput. Geom.,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2004}, {"title": "Clustering with spectral norm and the k-means algorithm", "author": ["Amit Kumar", "Ravindran Kannan"], "venue": "In 51th Annual IEEE Symposium on Foundations of Computer Science,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2010}, {"title": "Rcv1: A new benchmark collection for text categorization research", "author": ["David D. Lewis", "Yiming Yang", "Tony G. Rose", "Fan Li"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2004}, {"title": "Least squares quantization in pcm", "author": ["S. Lloyd"], "venue": "Information Theory, IEEE Transactions on,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1982}, {"title": "Scikit-learn: Machine learning in Python", "author": ["F. Pedregosa", "G. Varoquaux", "A. Gramfort", "V. Michel", "B. Thirion", "O. Grisel", "M. Blondel", "P. Prettenhofer", "R. Weiss", "V. Dubourg", "J. Vanderplas", "A. Passos", "D. Cournapeau", "M. Brucher", "M. Perrot", "E. Duchesnay"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2011}, {"title": "Making gradient descent optimal for strongly convex stochastic optimization", "author": ["Alexander Rakhlin", "Ohad Shamir", "Karthik Sridharan"], "venue": "In Proceedings of the 29th International Conference on Machine Learning,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2012}, {"title": "Web-scale k-means clustering", "author": ["D. Sculley"], "venue": "In Proceedings of the 19th International Conference on World Wide Web, WWW", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2010}], "referenceMentions": [{"referenceID": 4, "context": "We analyze online [5] and mini-batch [16] k-means variants.", "startOffset": 18, "endOffset": 21}, {"referenceID": 15, "context": "We analyze online [5] and mini-batch [16] k-means variants.", "startOffset": 37, "endOffset": 41}, {"referenceID": 7, "context": "The k-means objective is non-convex and non-differentiable: we exploit ideas from recent work on stochastic gradient descent for non-convex problems [8, 3] by providing a novel characterization of the trajectory of k-means algorithm on its solution space, and circumvent the non-differentiability problem via geometric insights about k-means update.", "startOffset": 149, "endOffset": 155}, {"referenceID": 2, "context": "The k-means objective is non-convex and non-differentiable: we exploit ideas from recent work on stochastic gradient descent for non-convex problems [8, 3] by providing a novel characterization of the trajectory of k-means algorithm on its solution space, and circumvent the non-differentiability problem via geometric insights about k-means update.", "startOffset": 149, "endOffset": 155}, {"referenceID": 12, "context": "Lloyd\u2019s algorithm (batch k-means) [13] is one of the most popular heuristics for clustering [9].", "startOffset": 34, "endOffset": 38}, {"referenceID": 8, "context": "Lloyd\u2019s algorithm (batch k-means) [13] is one of the most popular heuristics for clustering [9].", "startOffset": 92, "endOffset": 95}, {"referenceID": 6, "context": "Even with fast implementations such as [7], which reduces the computation for finding the closest centroid of each point, the per-iteration running time still depends linearly on n, making it a computational bottleneck for large datasets.", "startOffset": 39, "endOffset": 42}, {"referenceID": 3, "context": "To scale up the centroid-update phase, a plausible recipe is the \u201cstochastic approximation\u201d scheme [4]: the overall idea is, at each iteration, the centroids are updated using one (online [5]) or a few (mini-batch [16]) randomly sampled points instead of the entire dataset.", "startOffset": 99, "endOffset": 102}, {"referenceID": 4, "context": "To scale up the centroid-update phase, a plausible recipe is the \u201cstochastic approximation\u201d scheme [4]: the overall idea is, at each iteration, the centroids are updated using one (online [5]) or a few (mini-batch [16]) randomly sampled points instead of the entire dataset.", "startOffset": 188, "endOffset": 191}, {"referenceID": 15, "context": "To scale up the centroid-update phase, a plausible recipe is the \u201cstochastic approximation\u201d scheme [4]: the overall idea is, at each iteration, the centroids are updated using one (online [5]) or a few (mini-batch [16]) randomly sampled points instead of the entire dataset.", "startOffset": 214, "endOffset": 218}, {"referenceID": 15, "context": "Empirically, stochastic k-means has gained increasing attention for large-scale clustering and is included in widely used machine learning packages, such as Sofia-ML [16] and scikit-learn [14].", "startOffset": 166, "endOffset": 170}, {"referenceID": 13, "context": "Empirically, stochastic k-means has gained increasing attention for large-scale clustering and is included in widely used machine learning packages, such as Sofia-ML [16] and scikit-learn [14].", "startOffset": 188, "endOffset": 192}, {"referenceID": 11, "context": "Figure 1a demonstrates the efficiency of stochastic k-means against batch k-means on the RCV1 dataset [12].", "startOffset": 102, "endOffset": 106}, {"referenceID": 15, "context": "(a) Figure from [16], demonstrating the relative performance of online, mini-batch, and batch k-means.", "startOffset": 16, "endOffset": 20}, {"referenceID": 4, "context": "Algorithm 1 and stochastic k-means [5, 16] are equivalent up to the choice of learning rate and sampling scheme (the proof of equivalence is in Appendix A).", "startOffset": 35, "endOffset": 42}, {"referenceID": 15, "context": "Algorithm 1 and stochastic k-means [5, 16] are equivalent up to the choice of learning rate and sampling scheme (the proof of equivalence is in Appendix A).", "startOffset": 35, "endOffset": 42}, {"referenceID": 4, "context": "In [5, 16], the per-cluster learning rate is chosen as \u03b7 r := n\u0302tr \u2211 i\u2264t n\u0302 i r ; in our analysis, we choose a flat learning rate \u03b7 = c \u2032 to+t for all clusters, where c\u2032, to > 0 are some fixed constants (empirically, no obvious differences are observed; see Section 3.", "startOffset": 3, "endOffset": 10}, {"referenceID": 15, "context": "In [5, 16], the per-cluster learning rate is chosen as \u03b7 r := n\u0302tr \u2211 i\u2264t n\u0302 i r ; in our analysis, we choose a flat learning rate \u03b7 = c \u2032 to+t for all clusters, where c\u2032, to > 0 are some fixed constants (empirically, no obvious differences are observed; see Section 3.", "startOffset": 3, "endOffset": 10}, {"referenceID": 0, "context": "The major advantage of Algorithm 2 over other choices of initialization, such as the k-means++ [1] or random sampling, is that its running time is independent of the data size while providing seeding guarantee.", "startOffset": 95, "endOffset": 98}, {"referenceID": 10, "context": "We assume, similar to [11], that the means of each pair of clusters are well-separated and that the points from the two clusters are separated by a \u201cmargin\u201d, that is, \u2200x \u2208 A r \u222aA s , the distance from x to the bisector of c r and c s is lower bounded.", "startOffset": 22, "endOffset": 26}, {"referenceID": 5, "context": "The proposed clustering scheme is reminiscent to the Buckshot algorithm [6], widely used in the domain of document clustering.", "startOffset": 72, "endOffset": 75}, {"referenceID": 7, "context": "Our major source of inspiration comes from recent advances in non-convex stochastic optimization for unsupervised learning problems [8, 3].", "startOffset": 132, "endOffset": 138}, {"referenceID": 2, "context": "Our major source of inspiration comes from recent advances in non-convex stochastic optimization for unsupervised learning problems [8, 3].", "startOffset": 132, "endOffset": 138}, {"referenceID": 7, "context": "[8] studies the convergence of stochastic gradient descent (SGD) for the tensor decomposition problem, which amounts to finding a local optimum of a nonconvex objective function composed exclusively of saddle points and local optima.", "startOffset": 0, "endOffset": 3}, {"referenceID": 15, "context": "The learning rate in [16, 5], where \u03b7 r := n\u0302tr \u2211 i\u2264t n\u0302 i r , is intuitively making this adaptation: in the case when the clustering assignment does not change between iterations, it can be seen that E\u03b7 r \u2248 1 tpr(m) , so the effective learning rate \u03b7 t rp t r(m) is balanced for different clusters and is roughly 1t .", "startOffset": 21, "endOffset": 28}, {"referenceID": 4, "context": "The learning rate in [16, 5], where \u03b7 r := n\u0302tr \u2211 i\u2264t n\u0302 i r , is intuitively making this adaptation: in the case when the clustering assignment does not change between iterations, it can be seen that E\u03b7 r \u2248 1 tpr(m) , so the effective learning rate \u03b7 t rp t r(m) is balanced for different clusters and is roughly 1t .", "startOffset": 21, "endOffset": 28}, {"referenceID": 14, "context": "Lemma 3 resembles the standard iteration-wise convergence statement in SGD analysis, typically via convexity or smoothness of a function [15].", "startOffset": 137, "endOffset": 141}, {"referenceID": 10, "context": "Instead, our analysis relies on the geometric property of Voronoi diagram and the mean operation used in a k-means iteration, similar to those in recent works on batch k-means [11, 2, 17].", "startOffset": 176, "endOffset": 187}, {"referenceID": 1, "context": "Instead, our analysis relies on the geometric property of Voronoi diagram and the mean operation used in a k-means iteration, similar to those in recent works on batch k-means [11, 2, 17].", "startOffset": 176, "endOffset": 187}, {"referenceID": 2, "context": "To deal with this, we exploit probability tools developed in [3].", "startOffset": 61, "endOffset": 64}, {"referenceID": 2, "context": "[3] studies the convergence of stochastic PCA algorithms, where the objective function is the non-convex Rayleigh quotient, which has a plateau-like component.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "We upper bound the probability of this bad event (Proposition 1) using techniques that derive tight concentration inequality via moment generating functions from [3], which in turn implies a lower bound on the probability of \u03a9t, t \u2265 i.", "startOffset": 162, "endOffset": 165}, {"referenceID": 11, "context": "To verify the O( 1t ) global convergence rate of Theorem 1, we run stochastic k-means with varying learning rate, mini-batch size, and k on RCV1 [12].", "startOffset": 145, "endOffset": 149}, {"referenceID": 15, "context": "The dataset is relatively large in size: it has manually categorized 804414 newswire stories with 103 topics, where each story is a 47236-dimensional sparse vector; it was used in [16] for empirical evaluation of mini-batch k-means.", "startOffset": 180, "endOffset": 184}, {"referenceID": 13, "context": "We used Python and its scikit-learn package [14] for our experiments, which has stochastic k-means implemented.", "startOffset": 44, "endOffset": 48}, {"referenceID": 4, "context": "We disabled centroid relocation and modified their source code to allow a user-defined learning rate (their learning rate is fixed as \u03b7 r := n\u0302tr \u2211 i\u2264t n\u0302 i r , as in [5, 16], which we refer to as BBS-rate).", "startOffset": 167, "endOffset": 174}, {"referenceID": 15, "context": "We disabled centroid relocation and modified their source code to allow a user-defined learning rate (their learning rate is fixed as \u03b7 r := n\u0302tr \u2211 i\u2264t n\u0302 i r , as in [5, 16], which we refer to as BBS-rate).", "startOffset": 167, "endOffset": 174}, {"referenceID": 0, "context": "[1] David Arthur and Sergei Vassilvitskii.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "[2] Pranjal Awasthi and Or Sheffet.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[3] Akshay Balsubramani, Sanjoy Dasgupta, and Yoav Freund.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "[4] L\u00e9on Bottou.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[5] L\u00e9on Bottou and Yoshua Bengio.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[6] Douglas R.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[7] Charles Elkan.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[8] Rong Ge, Furong Huang, Chi Jin, and Yang Yuan.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "[9] Anil K.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "[10] Tapas Kanungo, David M.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[11] Amit Kumar and Ravindran Kannan.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[12] David D.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[13] S.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[14] F.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[15] Alexander Rakhlin, Ohad Shamir, and Karthik Sridharan.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[16] D.", "startOffset": 0, "endOffset": 4}, {"referenceID": 4, "context": "Equivalence of Algorithm 1 to stochastic k-means Here, we formally show that Algorithm 1 with specific instantiation of sample size m and learning rates \u03b7 r is equivalent to online k-means [5] and mini-batch k-means [16].", "startOffset": 189, "endOffset": 192}, {"referenceID": 15, "context": "Equivalence of Algorithm 1 to stochastic k-means Here, we formally show that Algorithm 1 with specific instantiation of sample size m and learning rates \u03b7 r is equivalent to online k-means [5] and mini-batch k-means [16].", "startOffset": 216, "endOffset": 220}, {"referenceID": 4, "context": "3, [5]].", "startOffset": 3, "endOffset": 6}, {"referenceID": 15, "context": "When m > 1, the update of Algorithm 1 is equivalent to that described from line 3 to line 14 in [Algorithm 1, [16]] with mini-batch size m.", "startOffset": 110, "endOffset": 114}, {"referenceID": 4, "context": "3, [5]].", "startOffset": 3, "endOffset": 6}, {"referenceID": 4, "context": "We substitute index k in [5] with r used in Algorithm 1.", "startOffset": 25, "endOffset": 28}, {"referenceID": 4, "context": "According to the update rule in [5], \u2206nk = 1 if the sampled point xi is assigned to cluster with center wk.", "startOffset": 32, "endOffset": 35}, {"referenceID": 4, "context": "Therefore, the update of the k-th centroid according to online k-means in [5] is: wk \u2190 wk + 1 nk (xi \u2212 wk)1{\u2206nk=1}", "startOffset": 74, "endOffset": 77}, {"referenceID": 15, "context": "For the second claim, consider line 4 to line 14 in [Algorithm 1, [16]].", "startOffset": 66, "endOffset": 70}, {"referenceID": 15, "context": "Hence, the updates in Algorithm 1 and line 4 to line 14 in [Algorithm 1, [16]] are equivalent.", "startOffset": 73, "endOffset": 77}, {"referenceID": 2, "context": "Lemma 9 (adapted from [3]).", "startOffset": 22, "endOffset": 25}, {"referenceID": 2, "context": ", [3]) \u2200\u03b2 > 0, k \u2265 1, \u03a0t=1(1\u2212 \u03b2 to + (i\u2212 t+ 1) ) \u2264 ( to + i\u2212 k + 1 to + i )", "startOffset": 2, "endOffset": 5}, {"referenceID": 10, "context": "4 of [11]).", "startOffset": 5, "endOffset": 9}, {"referenceID": 10, "context": "4 of [11]; we include it here for completeness.", "startOffset": 5, "endOffset": 9}, {"referenceID": 9, "context": "1 of [10]).", "startOffset": 5, "endOffset": 9}, {"referenceID": 2, "context": "Lemma 17 (Lemma D1 of [3]).", "startOffset": 22, "endOffset": 25}], "year": 2016, "abstractText": "We analyze online [5] and mini-batch [16] k-means variants. Both scale up the widely used k-means algorithm via stochastic approximation, and have become popular for large-scale clustering and unsupervised feature learning. We show, for the first time, that starting with any initial solution, they converge to a \u201clocal optimum\u201d at rateO( 1t ) (in terms of the k-means objective) under general conditions. In addition, we show if the dataset is clusterable, when initialized with a simple and scalable seeding algorithm, mini-batch k-means converges to an optimal k-means solution at rate O( 1t ) with high probability. The k-means objective is non-convex and non-differentiable: we exploit ideas from recent work on stochastic gradient descent for non-convex problems [8, 3] by providing a novel characterization of the trajectory of k-means algorithm on its solution space, and circumvent the non-differentiability problem via geometric insights about k-means update.", "creator": "LaTeX with hyperref package"}}}