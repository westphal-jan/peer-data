{"id": "1611.05138", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Nov-2016", "title": "S3Pool: Pooling with Stochastic Spatial Sampling", "abstract": "Feature pooling layers (e.g., max pooling) in convolutional neural networks (CNNs) serve the dual purpose of providing increasingly abstract representations as well as yielding computational savings in subsequent convolutional layers. We view the pooling operation in CNNs as a two-step procedure: first, a pooling window (e.g., $2\\times 2$) slides over the feature map with stride one which leaves the spatial resolution intact, and second, downsampling is performed by selecting one pixel from each non-overlapping pooling window in an often uniform and deterministic (e.g., top-left) manner. Our starting point in this work is the observation that this regularly spaced downsampling arising from non-overlapping windows, although intuitive from a signal processing perspective (which has the goal of signal reconstruction), is not necessarily optimal for \\emph{learning} (where the goal is to generalize). We study this aspect and propose a novel pooling strategy with stochastic spatial sampling (S3Pool), where the regular downsampling is replaced by a more general stochastic version. We observe that this general stochasticity acts as a strong regularizer, and can also be seen as doing implicit data augmentation by introducing distortions in the feature maps. We further introduce a mechanism to control the amount of distortion to suit different datasets and architectures. To demonstrate the effectiveness of the proposed approach, we perform extensive experiments on several popular image classification benchmarks, observing excellent improvements over baseline models. Experimental code is available at", "histories": [["v1", "Wed, 16 Nov 2016 04:17:52 GMT  (2668kb,D)", "http://arxiv.org/abs/1611.05138v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.CV", "authors": ["shuangfei zhai", "hui wu", "abhishek kumar", "yu cheng", "yongxi lu", "zhongfei zhang", "rogerio feris"], "accepted": false, "id": "1611.05138"}, "pdf": {"name": "1611.05138.pdf", "metadata": {"source": "CRF", "title": "S3Pool: Pooling with Stochastic Spatial Sampling", "authors": ["Shuangfei Zhai", "Hui Wu", "Abhishek Kumar", "Yu Cheng"], "emails": ["szhai2@binghamton.edu", "wuhu@us.ibm.com", "abhishk@us.ibm.com", "chengyu@us.ibm.com", "yol070@ucsd.edu", "zhongfei@cs.binghamton.edu", "rsferis@us.ibm.com"], "sections": [{"heading": null, "text": "We consider the pooling process in CNNs to be a two-step process: first, a pooling window (e.g. 2 \u00bd 2) slides over the characteristics chart with step one, in which the spatial resolution remains intact, and second, downsampling is done by selecting a pixel from each non-overlapping pooling window in an often uniform and deterministic manner (e.g. top left). Our starting point in this work is the observation that this regular downsampling, which arises from non-overlapping windows, although it is intuitive from a signal processing perspective (which has the goal of signal reconstruction), is not necessarily optimal for learning (where the goal is generalization)."}, {"heading": "1 Introduction", "text": "This year, it is more than ever before in the history of the city."}, {"heading": "2 Related Work", "text": "The idea of spatial pooling goes back to the groundbreaking work of Hubel and Wiesel [11] on complex cells in the visual cerebral cortex and the early CNN architectures by Yann Lecun et al. [15] Prior to the re-emergence of deep neural networks in computer vision, various approaches based on bags and fishing vectors also had spatial pooling as an essential component of the visual recognition pipeline, e.g. through orderless bag systems in computer vision, spatial pyramid aggregation [14], or task-oriented pooling [23]. In modern CNN architectures, spatial pooling plays a fundamental role in achieving invariance (to a certain degree) on image transformations and produces more compact representations for efficient processing in subsequent layers."}, {"heading": "3 Model Description", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 A Two-Step View of Max Pooling", "text": "It is perhaps the most widely used pooling option in deep CNNs, which usually follows one or more revolutionary planes to reduce the spatial dimensions of the features. Let's look at the input feature map before a pooling plane, where c is the number of channels and h and w are the height and width. A maximum pooling layer with a pooling window of size k.k and step speed is defined by the function z \"Pskpxq,\" in which z. \"s\" max. \"s.\""}, {"heading": "3.2 Pooling with Stochastic Spatial Sampling", "text": "This year, it is closer than ever before to an agreement between the two countries."}, {"heading": "4 Experiments", "text": "We evaluate S3Pool with three common image classification benchmarks: CIFAR-10, CIFAR-100 and STL-10. Both CIFAR-10 and CIFAR-100 consist of 32 x 32 color images, each with 50,000 images for training and 10,000 images for testing. STL-10 consists of 96 x 96 color images distributed evenly across 10 classes, with 5,000 images for training and 8,000 images for testing. All three data sets have relatively few examples, which makes proper regulation extremely important. We note that our goal is not to achieve state-of-the-art results on these data sets, but rather to provide a fair analysis of the effectiveness of S3Pool compared to other pooling and regulation methods."}, {"heading": "4.1 CIFAR-10 and CIFAR-100", "text": "This year, it is so far that it is only a matter of time before it is ready, until it is ready."}, {"heading": "4.2 STL-10", "text": "Compared to CIFAR-10 / CIFAR-100, STL-10 has many fewer training examples and larger image sizes. We adopt the 18-layer ResNet-based architecture based on this data set and test various pooling methods by replacing the step-2 turns with stochastic pooling [26] and S3Pool with different grid size settings. We follow similar training protocols as in Section 4.1, except that all models are trained for 200 epochs, with the learning rate reduced by a factor of 10 in the 150th epoch, without data augmentation. Results are summarized in Table 5. All variants of S3Pool significantly improve the performance of the baseline ResNet. In particular, S3Pool with the strongest regulation (S3Pool-96-48-24-12) achieves the most advanced test error on STL-10 and exceeds supervised learning [28], 3 approaches compared to the cost of basic training by even 3 percent."}, {"heading": "4.3 Visualization", "text": "Despite the convenient visualization of stochastic spatial scanning as shown in Figure 1, it is still unclear whether the same intuition is used in higher layers, and / or whether several S3Pool layers are stacked in a deep space. For this purpose, we get a deconventional network [27] at the top. The output of the deconventional network is then determined by the inputs of the S3Pool layer."}, {"heading": "5 Conclusions", "text": "We proposed S3Pool, a novel pooling method for CNNs. S3Pool expands the standard max pooling by splitting the pooling into two stages: max pooling with step 1 and non-deterministic spatial downsampling by randomly sampling rows and columns from a characteristic map. In fact, S3Pool implicitly expands the training data at each pooling stage, enabling a superior generalization capability of the learned model. Extensive experiments on CIFAR-10 and CIFAR-100 have shown that S3Pool, either in conjunction with data augmentation or not, significantly exceeds the standard max pooling, dropout and an existing stochastic pooling approach. In particular, by adapting the level of stochasticity introduced by S3Pool using a simple mechanism, we have a highly modern result on STL-10. In addition, S3Pool is easy to implement pool and makes the comparison of the pool less computer-supported pool a choice to make."}], "references": [{"title": "Selecting receptive fields in deep networks", "author": ["A. Coates", "A.Y. Ng"], "venue": "NIPS", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2011}, {"title": "Visual categorization with bags of keypoints", "author": ["G. Csurka", "C. Dance", "L. Fan", "J. Willamowski", "C. Bray"], "venue": "ECCV Workshop", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2004}, {"title": "Discriminative unsupervised feature learning with convolutional neural networks", "author": ["A. Dosovitskiy", "J.T. Springenberg", "M. Riedmiller", "T. Brox"], "venue": "NIPS", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2014}, {"title": "Maxout networks", "author": ["I.J. Goodfellow", "D. Warde-Farley", "M. Mirza", "A. Courville", "Y. Bengio"], "venue": "ICML", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2013}, {"title": "Fractional max-pooling", "author": ["B. Graham"], "venue": "arXiv preprint arXiv:1412.6071", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2014}, {"title": "Pyramid match kernels: Discriminative classification with sets of image features", "author": ["K. Grauman", "T. Darrell"], "venue": "ICCV", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2005}, {"title": "Learnednorm pooling for deep feedforward and recurrent neural networks", "author": ["C. Gulcehre", "K. Cho", "R. Pascanu", "Y. Bengio"], "venue": "MLKDD", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2014}, {"title": "Spatial pyramid pooling in deep convolutional networks for visual recognition", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "ECCV", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep residual learning for image recognition", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "CVPR", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2016}, {"title": "Sampling theory in Fourier and signal analysis: foundations", "author": ["J.R. Higgins"], "venue": "Oxford University Press on Demand", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1996}, {"title": "Receptive fields", "author": ["D. Hubel", "T. Wiesel"], "venue": "binocular interaction and functional architecture in the cats visual cortex. The Journal of Physiology, 160:106\u2013154", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1962}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["S. Ioffe", "C. Szegedy"], "venue": "ICML", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2015}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "NIPS", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2012}, {"title": "Beyond bags of features: Spatial pyramid matching for recognizing natural scene categories", "author": ["S. Lazebnik", "C. Schmid", "J. Ponce"], "venue": "CVPR", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2006}, {"title": "Gradientbased learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE, 86(11):2278\u20132324", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1998}, {"title": "Generalizing pooling functions in convolutional neural networks: Mixed", "author": ["C. Lee", "P. Gallagher", "Z. Tu"], "venue": "gated, and tree. In AISTATS", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2016}, {"title": "Network in network", "author": ["M. Lin", "Q. Chen", "S. Yan"], "venue": "ICLR", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep multipatch aggregation network for image style", "author": ["X. Lu", "Z. Lin", "X. Shen", "R. Mech", "J. Wang"], "venue": "aesthetics, and quality estimation. In ICCV", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2015}, {"title": "Communication in the presence of noise", "author": ["C.E. Shannon"], "venue": "Proceedings of the IRE", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1949}, {"title": "Best practices for convolutional neural networks applied to visual document analysis", "author": ["P.Y. Simard", "D. Steinkraus", "J.C. Platt"], "venue": "ICDAR", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2003}, {"title": "Improving deep neural networks with probabilistic maxout units", "author": ["J.T. Springenberg", "M. Riedmiller"], "venue": "ICLR Workshop", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2014}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["N. Srivastava", "G. Hinton", "A. Krizhevsky", "I. Sutskever", "R. Salakhutdinov"], "venue": "JMLR", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2014}, {"title": "Task-driven feature pooling for image classification", "author": ["G. Xie", "X. Zhang", "X. Shu", "S. Yan", "C. Liu"], "venue": "ICCV", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep representation learning with target coding", "author": ["S. Yang", "P. Luo", "C.C. Loy", "K.W. Shum", "X. Tang"], "venue": "AAAI", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2015}, {"title": "Adadelta: an adaptive learning rate method", "author": ["M.D. Zeiler"], "venue": "arXiv preprint arXiv:1212.5701", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2012}, {"title": "Stochastic pooling for regularization of deep convolutional neural networks", "author": ["M.D. Zeiler", "R. Fergus"], "venue": "ICLR", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2013}, {"title": "Deconvolutional networks", "author": ["M.D. Zeiler", "D. Krishnan", "G.W. Taylor", "R. Fergus"], "venue": "CVPR", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2010}, {"title": "Stacked what-where auto-encoders", "author": ["J. Zhao", "M. Mathieu", "R. Goroshin", "Y. Lecun"], "venue": "ICLR Workshop", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 18, "context": "Our starting point in this work is the observation that although this uniformly spaced spatial downsampling is reasonable from a signal processing perspective which aims for signal reconstruction [19] and is also computationally friendly, it is not necessarily the optimal design for the purpose of learning which aims for generalization to unseen examples2.", "startOffset": 196, "endOffset": 200}, {"referenceID": 9, "context": "Higgins writes [10]: \u201cWhat is special about equidistantly spaced sample points?\u201d; and then finding that the answer is \u201cWithin certain limitations, nothing at all.", "startOffset": 15, "endOffset": 19}, {"referenceID": 19, "context": "The stochastic nature of S3Pool enables it to produce different feature maps at each pass for the same training examples, which amounts to implicitly performing a sort of data augmentation [20], but at intermediate layers.", "startOffset": 189, "endOffset": 193}, {"referenceID": 16, "context": "In our experiments, we show that S3Pool yields excellent results on three standard image classification benchmarks, with two state-ofthe-art architectures, namely network in network [17], and residual networks [9].", "startOffset": 182, "endOffset": 186}, {"referenceID": 8, "context": "In our experiments, we show that S3Pool yields excellent results on three standard image classification benchmarks, with two state-ofthe-art architectures, namely network in network [17], and residual networks [9].", "startOffset": 210, "endOffset": 213}, {"referenceID": 21, "context": "We also extensively experiment with different data augmentation strategies, and show that under each setting, S3Pool is able to outperform other counterparts such as dropout [22] and stochastic pooling [26].", "startOffset": 174, "endOffset": 178}, {"referenceID": 25, "context": "We also extensively experiment with different data augmentation strategies, and show that under each setting, S3Pool is able to outperform other counterparts such as dropout [22] and stochastic pooling [26].", "startOffset": 202, "endOffset": 206}, {"referenceID": 10, "context": "The idea of spatial feature pooling dates back to the seminal work by Hubel and Wiesel [11] about complex cells in the mammalian visual cortex and the early CNN architectures developed by Yann Lecun et al.", "startOffset": 87, "endOffset": 91}, {"referenceID": 14, "context": "[15].", "startOffset": 0, "endOffset": 4}, {"referenceID": 1, "context": ", through orderless bag-of-features [2, 6], spatial pyramid aggregation [14], or task-driven feature pooling [23].", "startOffset": 36, "endOffset": 42}, {"referenceID": 5, "context": ", through orderless bag-of-features [2, 6], spatial pyramid aggregation [14], or task-driven feature pooling [23].", "startOffset": 36, "endOffset": 42}, {"referenceID": 13, "context": ", through orderless bag-of-features [2, 6], spatial pyramid aggregation [14], or task-driven feature pooling [23].", "startOffset": 72, "endOffset": 76}, {"referenceID": 22, "context": ", through orderless bag-of-features [2, 6], spatial pyramid aggregation [14], or task-driven feature pooling [23].", "startOffset": 109, "endOffset": 113}, {"referenceID": 15, "context": "Hybrid pooling [16, 18] combines different types of pooling into the same network architecture.", "startOffset": 15, "endOffset": 23}, {"referenceID": 17, "context": "Hybrid pooling [16, 18] combines different types of pooling into the same network architecture.", "startOffset": 15, "endOffset": 23}, {"referenceID": 25, "context": "Stochastic pooling [26] randomly picks the activation within each pooling region according to a multinomial distribution.", "startOffset": 19, "endOffset": 23}, {"referenceID": 3, "context": "Max-out networks [4, 21] perform pooling across different feature maps.", "startOffset": 17, "endOffset": 24}, {"referenceID": 20, "context": "Max-out networks [4, 21] perform pooling across different feature maps.", "startOffset": 17, "endOffset": 24}, {"referenceID": 7, "context": "Spatial pyramid pooling [8] aggregates features at multiple scales, and is usually applied to extract fixed-length feature vectors from region proposals for object detection.", "startOffset": 24, "endOffset": 27}, {"referenceID": 4, "context": "Fractional pooling [5] proposes to use pooling strides of less than 2 by applying mixed pooling strides of 1 and 2 at different locations.", "startOffset": 19, "endOffset": 22}, {"referenceID": 25, "context": "(b) Stochastic pooling [26], pooling window k \u201c 2, stride s \u201c 2", "startOffset": 23, "endOffset": 27}, {"referenceID": 25, "context": "Stochastic pooling [26] adapts the first step by choosing the activation with a stochastic procedure (b).", "startOffset": 19, "endOffset": 23}, {"referenceID": 6, "context": "Learning-based methods for spatial feature pooling have also been proposed [7, 1].", "startOffset": 75, "endOffset": 81}, {"referenceID": 0, "context": "Learning-based methods for spatial feature pooling have also been proposed [7, 1].", "startOffset": 75, "endOffset": 81}, {"referenceID": 25, "context": ", average pooling, stochastic pooling [26]).", "startOffset": 38, "endOffset": 42}, {"referenceID": 19, "context": "The benefit of S3Pool is thus obvious in that, each draw from the pooling step will produce different yet plausible downsampled feature maps, which is equivalent to performing data augmentation [20] at the pooling layer level.", "startOffset": 194, "endOffset": 198}, {"referenceID": 12, "context": "However, compared with traditional data augmentation, such as image cropping [13], the distortion introduced by S3Pool is more aggressive.", "startOffset": 77, "endOffset": 81}, {"referenceID": 25, "context": "To further illustrate the idea of S3Pool and its difference from the standard max pooling, and another nondeterministic variant of max pooling [26], we demonstrate the different pooling processes in Figure 2 using a toy feature map of size 1\u02c64\u02c64.", "startOffset": 143, "endOffset": 147}, {"referenceID": 25, "context": "From the two-step view of max pooling, stochastic pooling [26] modifies the first step: instead of outputing a deterministic maximum in each pooling window of k \u02c6 k, it randomly draws a response according to the magnitude of the activation; the second downsampling step, however, remains the same as in max pooling.", "startOffset": 58, "endOffset": 62}, {"referenceID": 25, "context": "Different from stochastic pooling [26] and deterministic max pooling, S3Pool offers the flexibility to control the amount of distortion introduced in each sampling step by varying the grid size g in each layer.", "startOffset": 34, "endOffset": 38}, {"referenceID": 16, "context": "For CIFAR-10 and CIFAR-100, we experiment with two state-of-the-art architectures, network in network (NIN) [17] and residual networks (ResNet) [9], both of which are well established architectures, but with different designs.", "startOffset": 108, "endOffset": 112}, {"referenceID": 8, "context": "For CIFAR-10 and CIFAR-100, we experiment with two state-of-the-art architectures, network in network (NIN) [17] and residual networks (ResNet) [9], both of which are well established architectures, but with different designs.", "startOffset": 144, "endOffset": 147}, {"referenceID": 16, "context": "The architectures we use in this paper differ slightly from those in [17, 9], which we summarize in Table 1.", "startOffset": 69, "endOffset": 76}, {"referenceID": 8, "context": "The architectures we use in this paper differ slightly from those in [17, 9], which we summarize in Table 1.", "startOffset": 69, "endOffset": 76}, {"referenceID": 11, "context": "Batch normalization [12] is applied to each convolutional layer for each of the two models, with ReLU as the nonlinearity.", "startOffset": 20, "endOffset": 24}, {"referenceID": 16, "context": "Table 2: Control experiments with NIN [17] on CIFAR-10 and CIFAR-100 (best seen in color).", "startOffset": 38, "endOffset": 42}, {"referenceID": 25, "context": "[26] N N 0.", "startOffset": 0, "endOffset": 4}, {"referenceID": 25, "context": "[26] N Y 0.", "startOffset": 0, "endOffset": 4}, {"referenceID": 25, "context": "[26] Y N 0.", "startOffset": 0, "endOffset": 4}, {"referenceID": 25, "context": "[26] Y Y 0.", "startOffset": 0, "endOffset": 4}, {"referenceID": 8, "context": "For ResNet, we follow the original design in [9] by replacing the Pool-2-2 layer with stride 2 convolution, without dropout.", "startOffset": 45, "endOffset": 48}, {"referenceID": 25, "context": "[26] with pooling window of size 2\u02c6 2 and stride 2\u02c6 2.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "We train all the models with ADADELTA [25] with an initial learning rate of 1 and a batch size of 128.", "startOffset": 38, "endOffset": 42}, {"referenceID": 25, "context": "We observe that for every combination of dataset type, network architecture and data augmentation technique (denoted by rows with the same color in Table 2 and Table 3), S3Pool achieves the lowest testing error, while yielding higher training errors than NIN with dropout, ResNet and their counterparts with stochastic pooling [26].", "startOffset": 327, "endOffset": 331}, {"referenceID": 16, "context": "Table 3: Control experiments with ResNet [17] on CIFAR-10 and CIFAR-100 (best seen in color).", "startOffset": 41, "endOffset": 45}, {"referenceID": 25, "context": "[26] N N 0.", "startOffset": 0, "endOffset": 4}, {"referenceID": 25, "context": "[26] N Y 0.", "startOffset": 0, "endOffset": 4}, {"referenceID": 25, "context": "[26] Y N 0.", "startOffset": 0, "endOffset": 4}, {"referenceID": 25, "context": "[26] Y Y 0.", "startOffset": 0, "endOffset": 4}, {"referenceID": 25, "context": "The results indicate that, by varying the number of training examples from as low as 1000 to 10000, S3Pool achieves consistently lower testing errors compared with the baseline ResNet as well as stochastic pooling [26].", "startOffset": 214, "endOffset": 218}, {"referenceID": 25, "context": "[26] 0.", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": "[28] 25.", "startOffset": 0, "endOffset": 4}, {"referenceID": 2, "context": "[3] 27.", "startOffset": 0, "endOffset": 3}, {"referenceID": 23, "context": "[24] 26.", "startOffset": 0, "endOffset": 4}, {"referenceID": 25, "context": "85 adopt the 18-layer ResNet based architecture on this dataset, and test different pooling methods by replacing the stride 2 convolutions by stochastic pooling [26] and S3Pool with different grid size settings.", "startOffset": 161, "endOffset": 165}, {"referenceID": 23, "context": "In particular, S3Pool with the strongest regularization (S3Pool-96-48-24-12) achieves the state-ofthe-art testing error on STL-10, outperforming supervised learning [24] as well as semi-supervised learning [28, 3] approaches.", "startOffset": 165, "endOffset": 169}, {"referenceID": 27, "context": "In particular, S3Pool with the strongest regularization (S3Pool-96-48-24-12) achieves the state-ofthe-art testing error on STL-10, outperforming supervised learning [24] as well as semi-supervised learning [28, 3] approaches.", "startOffset": 206, "endOffset": 213}, {"referenceID": 2, "context": "In particular, S3Pool with the strongest regularization (S3Pool-96-48-24-12) achieves the state-ofthe-art testing error on STL-10, outperforming supervised learning [24] as well as semi-supervised learning [28, 3] approaches.", "startOffset": 206, "endOffset": 213}, {"referenceID": 26, "context": ", using the test model of S3Pool) and stack a deconvolutional network [27] on top.", "startOffset": 70, "endOffset": 74}], "year": 2016, "abstractText": "Feature pooling layers (e.g., max pooling) in convolutional neural networks (CNNs) serve the dual purpose of providing increasingly abstract representations as well as yielding computational savings in subsequent convolutional layers. We view the pooling operation in CNNs as a two-step procedure: first, a pooling window (e.g., 2 \u02c6 2) slides over the feature map with stride one which leaves the spatial resolution intact, and second, downsampling is performed by selecting one pixel from each non-overlapping pooling window in an often uniform and deterministic (e.g., top-left) manner. Our starting point in this work is the observation that this regularly spaced downsampling arising from non-overlapping windows, although intuitive from a signal processing perspective (which has the goal of signal reconstruction), is not necessarily optimal for learning (where the goal is to generalize). We study this aspect and propose a novel pooling strategy with stochastic spatial sampling (S3Pool), where the regular downsampling is replaced by a more general stochastic version. We observe that this general stochasticity acts as a strong regularizer, and can also be seen as doing implicit data augmentation by introducing distortions in the feature maps. We further introduce a mechanism to control the amount of distortion to suit different datasets and architectures. To demonstrate the effectiveness of the proposed approach, we perform extensive experiments on several popular image classification benchmarks, observing excellent improvements over baseline models 1.", "creator": "LaTeX with hyperref package"}}}