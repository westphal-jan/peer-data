{"id": "1611.05141", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Nov-2016", "title": "Training Spiking Deep Networks for Neuromorphic Hardware", "abstract": "We describe a method to train spiking deep networks that can be run using leaky integrate-and-fire (LIF) neurons, achieving state-of-the-art results for spiking LIF networks on five datasets, including the large ImageNet ILSVRC-2012 benchmark. Our method for transforming deep artificial neural networks into spiking networks is scalable and works with a wide range of neural nonlinearities. We achieve these results by softening the neural response function, such that its derivative remains bounded, and by training the network with noise to provide robustness against the variability introduced by spikes. Our analysis shows that implementations of these networks on neuromorphic hardware will be many times more power-efficient than the equivalent non-spiking networks on traditional hardware.", "histories": [["v1", "Wed, 16 Nov 2016 04:32:22 GMT  (67kb,D)", "http://arxiv.org/abs/1611.05141v1", "10 pages, 3 figures, 4 tables; the \"methods\" section of this article draws heavily onarXiv:1510.08829"]], "COMMENTS": "10 pages, 3 figures, 4 tables; the \"methods\" section of this article draws heavily onarXiv:1510.08829", "reviews": [], "SUBJECTS": "cs.NE cs.LG", "authors": ["eric hunsberger", "chris eliasmith"], "accepted": false, "id": "1611.05141"}, "pdf": {"name": "1611.05141.pdf", "metadata": {"source": "CRF", "title": "Training Spiking Deep Networks for Neuromorphic Hardware", "authors": ["Eric Hunsberger", "Chris Eliasmith"], "emails": ["ehunsber@uwaterloo.ca", "celiasmith@uwaterloo.ca"], "sections": [{"heading": null, "text": "Our method of transforming deep artificial neural networks into spiking networks is scalable and works with a wide range of neural nonlinearity. We achieve these results by attenuating the neural response function in such a way that its derivatives remain limited, and by forming the network with noise to provide robustness against spikes-induced variability. Our analysis shows that implementing these networks on neuromorphic hardware will be many times more energy efficient than the corresponding non-spiking networks on traditional hardware."}, {"heading": "1 Introduction", "text": "Early successes with the MNIST database [1] were then tested on the more difficult but similarly large CIFAR 10 [2] and Street View house number [3] datasets, culminating in the application of deep revolutionary neural networks to ImageNet [6], a very large and challenging dataset with 1.2 million images in 1,000 categories. Recently, considerable efforts have been made to introduce neural \"spikings\" into deep ANNs [7, 8, 9, 10, 11, 12] so that networked nodes in the network transmit information about instantaneous single bits (spikes) rather than transferring real \"spikings\" into deep ANNs."}, {"heading": "2 Methods", "text": "We first train a network on static images using traditional deep learning techniques; we call this the ANN. Then we take the parameters (weights and distortions) from the ANN and use them to connect spiking neurons to form the spiking neural network (SNN). A key challenge is to train the ANN to transmit it to a spiking network, and to minimize the classification error of the resulting SNN."}, {"heading": "2.1 Convolutional ANN", "text": "We base our network on that of Krizhevsky et al. [6], which won the ImageNet ILSVRC 2012 competition. A smaller variant of the network achieved 11% error on the CIFAR 10 dataset. The network uses a series of generalized revolutionary layers, where such a layer consists of a set of revolutionary weights, followed by a neural nonlinearity, a pooling layer, and finally a local contrast normalization layer. These generalized revolutionary layers are followed by either locally connected layers, fully connected layers, or both with a neural nonlinearity. In the case of the original network, nonlinearity is a reflected linear (ReLU) function, and pooling layers perform a maximum summary. The details of the network can be found in [6] and the code is usable."}, {"heading": "2.2 Leaky integrate-and-fire neurons", "text": "Our network uses a modified leaky integrate-and-fire (LIF) neuron nonlinearity = constant time instead of rectified linear nonlinearity. Previous work has maintained the rectified linear nonlinearity for the ANN and replaced it with the spiking integrate-and-fire (IF) neuron model in the SNN [11, 10], because the static fire curve of the IF neuron model is a rectified line. Our motivation for using the LIF neuron model is that it and it shows that more complex, nonlinear neuron models can be used in such networks. Therefore, these methods can be extended to the idiosyncratic neuron types used by some neuromorphic hardware (e.g. [14]. LIF neuron dynamics is defined by the RC v (t) v (t) and J (t) (x) equation, where the membrane is (1)."}, {"heading": "2.3 Training with noise", "text": "The formation of neural networks with different types of noise at the inputs is not a new idea. > Denoising autoencoders [16] have been successfully applied to data sets such as MNIST and are learning more robust solutions with lower generalization errors than their non-noisy counterparts. In a biological spiking neural network, synapses between neurons perform some measurement of spikes-induced variability because the post-synaptic current induced by the release of neurotransmitters is distributed over time. We use a similar mechanism in our networks to attenuate some of the variabilities introduced by spikes. \u03b1 function \u03b1 (t) = (t / percs) e \u2212 t / \u03c4s is a simple second-order low-pass filter inspired by biology [17]. We chose this synaptic filter for our networks because it provides better noise reduction than a first-order low-pass filter."}, {"heading": "2.4 Conversion to a spiking network", "text": "Finally, we convert the trained ANN into an SNN. The parameters in the spiking network (i.e. weights and distortions) are all identical to the ANN. The folding process also remains the same, since folding can be rewritten as simple connecting weights (synapses) between pre-synaptic neuron i and post-synaptic neuron j. (How the brain could learn connecting weight patterns, i.e. filters, that repeat themselves at different points in space, is a much more difficult problem that we will not address here.) Likewise, the average pool process can be written as a simple connecting weight matrix, and this matrix can be multiplied by the convolutionary weight matrix of the following layer to obtain direct connecting weights between neurons.2The only component of the network that changes when we move from the ANN to the SNN are the neurons themselves. The most significant change in the spike is that we remove the LIF-high equilibrium model (LIF-LIF)."}, {"heading": "3 Results", "text": "We tested our methods on five datasets: MNIST [1], SVHN [18], CIFAR-10 and CIFAR-100 [19], and the large ImageNet ILSVRC-2012 dataset [20]. Our best result for each dataset is shown in Table 1. Using our methods has allowed us to build spinning networks that perform almost as well as their non-spinning counterparts with the same number of neurons. All datasets show minimal loss of accuracy in the transformation from ANN to SNN. 32For computational efficiency, we calculate the convolution and pooling data separately. 3The ILSVRC-2012 datasets actually show a marginal increase in accuracy, although this is probably not statistically significant and could be because the spiking LIF neurons have harder limits than their soft LIF-rate counterparts."}, {"heading": "3.1 Efficiency", "text": "In fact, most of them are able to survive on their own if they do not put themselves in the position they are in."}, {"heading": "4 Discussion", "text": "This year is the highest in the history of the country."}], "references": [{"title": "Gradient-based learning applied to document recognition", "author": ["Y. Lecun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE, vol. 86, no. 11, pp. 2278\u20132324, 1998.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1998}, {"title": "Convolutional deep belief networks on CIFAR-10", "author": ["A. Krizhevsky"], "venue": "Tech. Rep., 2010.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2010}, {"title": "Convolutional neural networks applied to house numbers digit classification", "author": ["P. Sermanet", "S. Chintala", "Y. LeCun"], "venue": "International Conference on Pattern Recognition (ICPR), 2012.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2012}, {"title": "Deeply-supervised nets", "author": ["C.-Y. Lee", "S. Xie", "P.W. Gallagher", "Z. Zhang", "Z. Tu"], "venue": "International Conference on Artificial Intelligence and Statistics (AISTATS), vol. 38, 2015, pp. 562\u2013570.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2015}, {"title": "Discriminative learning of sum-product networks", "author": ["R. Gens", "P. Domingos"], "venue": "Advances in Neural Information Processing Systems (NIPS), 2012, pp. 1\u20139.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2012}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G. Hinton"], "venue": "Advances in Neural Information Processing Systems, 2012.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2012}, {"title": "A Large-Scale Model of the Functioning Brain", "author": ["C. Eliasmith", "T.C. Stewart", "X. Choo", "T. Bekolay", "T. DeWolf", "C. Tang", "D. Rasmussen"], "venue": "Science, vol. 338, no. 6111, pp. 1202\u20131205, Nov. 2012.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2012}, {"title": "Event-driven contrastive divergence for spiking neuromorphic systems", "author": ["E. Neftci", "S. Das", "B. Pedroni", "K. Kreutz-Delgado", "G. Cauwenberghs"], "venue": "Frontiers in Neuroscience, vol. 7, no. 272, 2013.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2013}, {"title": "Real-time classification and sensor fusion with a spiking deep belief network", "author": ["P. O\u2019Connor", "D. Neil", "S.-C. Liu", "T. Delbruck", "M. Pfeiffer"], "venue": "Frontiers in Neuroscience, vol. 7, Jan. 2013.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2013}, {"title": "Fast-Classifying, High-Accuracy Spiking Deep Networks Through Weight and Threshold Balancing", "author": ["P.U. Diehl", "D. Neil", "J. Binas", "M. Cook", "S.-C. Liu", "M. Pfeiffer"], "venue": "IEEE International Joint Conference on Neural Networks (IJCNN), 2015.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "Spiking Deep Convolutional Neural Networks for Energy-Efficient Object Recognition", "author": ["Y. Cao", "Y. Chen", "D. Khosla"], "venue": "International Journal of Computer Vision, vol. 113, no. 1, pp. 54\u201366, Nov. 2014.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2014}, {"title": "Convolutional Networks for Fast, Energy-Efficient Neuromorphic Computing", "author": ["S.K. Esser", "P.A. Merolla", "J.V. Arthur", "A.S. Cassidy", "R. Appuswamy", "A. Andreopoulos", "D.J. Berg", "J.L. Mckinstry", "T. Melano", "D.R. Barch", "C. di Nolfo", "P. Datta", "A. Amir", "B. Taba", "M.D. Flickner", "D.S. Modha"], "venue": "arXiv preprint, vol. 1603, no. 08270, pp. 1\u20137, 2016.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2016}, {"title": "Conversion of Artificial Recurrent Neural Networks to Spiking Neural Networks for Low-power Neuromorphic Hardware", "author": ["P.U. Diehl", "G. Zarrella", "A. Cassidy", "B.U. Pedroni", "E. Neftci"], "venue": "arXiv preprint, vol. 1601, no. 04187, 2016.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2016}, {"title": "Neurogrid: A mixed-analog-digital multichip system for large-scale neural simulations", "author": ["B.V. Benjamin", "P. Gao", "E. McQuinn", "S. Choudhary", "A.R. Chandrasekaran", "J.-M. Bussat", "R. Alvarez- Icaza", "J.V. Arthur", "P.A. Merolla", "K. Boahen"], "venue": "Proceedings of the IEEE, vol. 102, no. 5, pp. 699\u2013716, 2014.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2014}, {"title": "Spiking Deep Networks with LIF Neurons", "author": ["E. Hunsberger", "C. Eliasmith"], "venue": "arXiv:1510.08829 [cs], pp. 1\u20139, 2015.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2015}, {"title": "Extracting and composing robust features with denoising autoencoders", "author": ["P. Vincent", "H. Larochelle", "Y. Bengio", "P.-A. Manzagol"], "venue": "International Conference on Machine Learning (ICML), 2008, pp. 1096\u2013 1103.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2008}, {"title": "Reliability of spike timing in neocortical neurons.", "author": ["Z.F. Mainen", "T.J. Sejnowski"], "venue": "Science (New York, N.Y.),", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1995}, {"title": "Reading Digits in Natural Images with Unsupervised Feature Learning", "author": ["Y. Netzer", "T. Wang", "A. Coates", "A. Bissacco", "B. Wu", "A.Y. Ng"], "venue": "NIPS workshop on deep learning and unsupervised feature learning, 2011, pp. 1\u20139.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2011}, {"title": "Learning Multiple Layers of Features from Tiny Images", "author": ["A. Krizhevsky"], "venue": "Master\u2019s thesis, University of Toronto, 2009.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2009}, {"title": "ImageNet Large Scale Visual Recognition Challenge", "author": ["O. Russakovsky", "J. Deng", "H. Su", "J. Krause", "S. Satheesh", "S. Ma", "Z. Huang", "A. Karpathy", "A. Khosla", "M. Bernstein", "A.C. Berg", "L. Fei-Fei"], "venue": "International Journal of Computer Vision, vol. 115, no. 3, pp. 211\u2013252, 2015. 9", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2015}, {"title": "A million spiking-neuron integrated circuit with a scalable communication network and interface", "author": ["P.A. Merolla", "J.V. Arthur", "R. Alvarez-Icaza", "A.S. Cassidy", "J. Sawada", "F. Akopyan", "B.L. Jackson", "N. Imam", "C. Guo", "Y. Nakamura", "B. Brezzo", "I. Vo", "S.K. Esser", "R. Appuswamy", "B. Taba", "A. Amir", "M.D. Flickner", "W.P. Risk", "R. Manohar", "D.S. Modha"], "venue": "Science, vol. 345, no. 6197, pp. 668\u2013673, 2014.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2014}, {"title": "Bayesian computation emerges in generic cortical microcircuits through spike-timing-dependent plasticity.", "author": ["B. Nessler", "M. Pfeiffer", "L. Buesing", "W. Maass"], "venue": "PLoS computational biology,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2013}, {"title": "Simultaneous unsupervised and supervised learning of cognitive functions in biologically plausible spiking neural networks", "author": ["T. Bekolay", "C. Kolbeck", "C. Eliasmith"], "venue": "Proc. 35th Annual Conference of the Cognitive Science Society, 2013, pp. 169\u2013174. 10", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2013}], "referenceMentions": [{"referenceID": 0, "context": "Early successes with the MNIST database [1] were subsequently tested on the more difficult but similarly sized CIFAR-10 [2] and Street-view house numbers [3] datasets.", "startOffset": 40, "endOffset": 43}, {"referenceID": 1, "context": "Early successes with the MNIST database [1] were subsequently tested on the more difficult but similarly sized CIFAR-10 [2] and Street-view house numbers [3] datasets.", "startOffset": 120, "endOffset": 123}, {"referenceID": 2, "context": "Early successes with the MNIST database [1] were subsequently tested on the more difficult but similarly sized CIFAR-10 [2] and Street-view house numbers [3] datasets.", "startOffset": 154, "endOffset": 157}, {"referenceID": 3, "context": "[4]), as well as on larger datasets (e.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[5]).", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "This work has culminated with the application of deep convolutional neural networks to ImageNet [6], a very large and challenging dataset with 1.", "startOffset": 96, "endOffset": 99}, {"referenceID": 6, "context": "There has recently been considerable effort to introduce neural \u201cspiking\u201d into deep ANNs [7, 8, 9, 10, 11, 12], such that connected nodes in the network transmit information via instantaneous single bits (spikes), rather than transmitting real-valued activities.", "startOffset": 89, "endOffset": 110}, {"referenceID": 7, "context": "There has recently been considerable effort to introduce neural \u201cspiking\u201d into deep ANNs [7, 8, 9, 10, 11, 12], such that connected nodes in the network transmit information via instantaneous single bits (spikes), rather than transmitting real-valued activities.", "startOffset": 89, "endOffset": 110}, {"referenceID": 8, "context": "There has recently been considerable effort to introduce neural \u201cspiking\u201d into deep ANNs [7, 8, 9, 10, 11, 12], such that connected nodes in the network transmit information via instantaneous single bits (spikes), rather than transmitting real-valued activities.", "startOffset": 89, "endOffset": 110}, {"referenceID": 9, "context": "There has recently been considerable effort to introduce neural \u201cspiking\u201d into deep ANNs [7, 8, 9, 10, 11, 12], such that connected nodes in the network transmit information via instantaneous single bits (spikes), rather than transmitting real-valued activities.", "startOffset": 89, "endOffset": 110}, {"referenceID": 10, "context": "There has recently been considerable effort to introduce neural \u201cspiking\u201d into deep ANNs [7, 8, 9, 10, 11, 12], such that connected nodes in the network transmit information via instantaneous single bits (spikes), rather than transmitting real-valued activities.", "startOffset": 89, "endOffset": 110}, {"referenceID": 11, "context": "There has recently been considerable effort to introduce neural \u201cspiking\u201d into deep ANNs [7, 8, 9, 10, 11, 12], such that connected nodes in the network transmit information via instantaneous single bits (spikes), rather than transmitting real-valued activities.", "startOffset": 89, "endOffset": 110}, {"referenceID": 6, "context": "While one goal of this work is to better understand the brain by trying to reverse engineer it [7], another goal is to build energy-efficient neuromorphic systems that use a similar spiking communication method, for image categorization [10, 11, 12] or other applications [13].", "startOffset": 95, "endOffset": 98}, {"referenceID": 9, "context": "While one goal of this work is to better understand the brain by trying to reverse engineer it [7], another goal is to build energy-efficient neuromorphic systems that use a similar spiking communication method, for image categorization [10, 11, 12] or other applications [13].", "startOffset": 237, "endOffset": 249}, {"referenceID": 10, "context": "While one goal of this work is to better understand the brain by trying to reverse engineer it [7], another goal is to build energy-efficient neuromorphic systems that use a similar spiking communication method, for image categorization [10, 11, 12] or other applications [13].", "startOffset": 237, "endOffset": 249}, {"referenceID": 11, "context": "While one goal of this work is to better understand the brain by trying to reverse engineer it [7], another goal is to build energy-efficient neuromorphic systems that use a similar spiking communication method, for image categorization [10, 11, 12] or other applications [13].", "startOffset": 237, "endOffset": 249}, {"referenceID": 12, "context": "While one goal of this work is to better understand the brain by trying to reverse engineer it [7], another goal is to build energy-efficient neuromorphic systems that use a similar spiking communication method, for image categorization [10, 11, 12] or other applications [13].", "startOffset": 272, "endOffset": 276}, {"referenceID": 13, "context": "[14]).", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "We extend our previous results [15] to additional datasets, and most notably demonstrate that it scales to the large ImageNet dataset.", "startOffset": 31, "endOffset": 35}, {"referenceID": 5, "context": "[6], which won the ImageNet ILSVRC-2012 competition.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "The details of the network can be found in [6] and code is available1.", "startOffset": 43, "endOffset": 46}, {"referenceID": 10, "context": "Past work has kept the rectified linear nonlinearity for the ANN and substituted in the spiking integrate-and-fire (IF) neuron model in the SNN [11, 10], since the static firing curve of the IF neuron model is a rectified line.", "startOffset": 144, "endOffset": 152}, {"referenceID": 9, "context": "Past work has kept the rectified linear nonlinearity for the ANN and substituted in the spiking integrate-and-fire (IF) neuron model in the SNN [11, 10], since the static firing curve of the IF neuron model is a rectified line.", "startOffset": 144, "endOffset": 152}, {"referenceID": 13, "context": "[14]).", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "Denoising autoencoders [16] have been successfully applied to datasets like MNIST, learning more robust solutions with lower generalization error than their non-noisy counterparts.", "startOffset": 23, "endOffset": 27}, {"referenceID": 16, "context": "The \u03b1-function \u03b1(t) = (t/\u03c4s)es is a simple second-order lowpass filter, inspired by biology [17].", "startOffset": 92, "endOffset": 96}, {"referenceID": 0, "context": "We tested our methods on five datasets: MNIST [1], SVHN [18], CIFAR-10 and CIFAR-100 [19], and the large ImageNet ILSVRC-2012 dataset [20].", "startOffset": 46, "endOffset": 49}, {"referenceID": 17, "context": "We tested our methods on five datasets: MNIST [1], SVHN [18], CIFAR-10 and CIFAR-100 [19], and the large ImageNet ILSVRC-2012 dataset [20].", "startOffset": 56, "endOffset": 60}, {"referenceID": 18, "context": "We tested our methods on five datasets: MNIST [1], SVHN [18], CIFAR-10 and CIFAR-100 [19], and the large ImageNet ILSVRC-2012 dataset [20].", "startOffset": 85, "endOffset": 89}, {"referenceID": 19, "context": "We tested our methods on five datasets: MNIST [1], SVHN [18], CIFAR-10 and CIFAR-100 [19], and the large ImageNet ILSVRC-2012 dataset [20].", "startOffset": 134, "endOffset": 138}, {"referenceID": 9, "context": "88% (22k) [10] SVHN 6.", "startOffset": 10, "endOffset": 14}, {"referenceID": 10, "context": "57% (28k) [11] CIFAR-100 44.", "startOffset": 10, "endOffset": 14}, {"referenceID": 11, "context": "Table 2: Our error rates compared with recent results on the TrueNorth (TN) neuromorphic chip [12], as well as other best results in the literature.", "startOffset": 94, "endOffset": 98}, {"referenceID": 11, "context": "The most significant recent results are from [12], who implemented networks for a number of datasets on both one and eight TrueNorth chips.", "startOffset": 45, "endOffset": 49}, {"referenceID": 9, "context": "The most significant difference between our results and that of [10] and [11] is that we use LIF neurons and can generalize to other neuron types, whereas their methods (and those of [12]) are specific to IF neurons.", "startOffset": 64, "endOffset": 68}, {"referenceID": 10, "context": "The most significant difference between our results and that of [10] and [11] is that we use LIF neurons and can generalize to other neuron types, whereas their methods (and those of [12]) are specific to IF neurons.", "startOffset": 73, "endOffset": 77}, {"referenceID": 11, "context": "The most significant difference between our results and that of [10] and [11] is that we use LIF neurons and can generalize to other neuron types, whereas their methods (and those of [12]) are specific to IF neurons.", "startOffset": 183, "endOffset": 187}, {"referenceID": 5, "context": "[6], except that they also used multiview testing where the classifier output is the average output of the classifier run on nine random patches from each testing image (increasing the accuracy by about 2%).", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[6] 14.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "We first show the original ANN based on [6], and then the effects of each subsequent modification.", "startOffset": 40, "endOffset": 43}, {"referenceID": 20, "context": "[21]), we assume that each neuron update takes as much energy as 0.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[14].", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "Notably, all other state-of-the-art methods use integrate-and-fire (IF) neurons [11, 10, 12], which are straightforward to fit to the rectified linear units commonly used in deep convolutional networks.", "startOffset": 80, "endOffset": 92}, {"referenceID": 9, "context": "Notably, all other state-of-the-art methods use integrate-and-fire (IF) neurons [11, 10, 12], which are straightforward to fit to the rectified linear units commonly used in deep convolutional networks.", "startOffset": 80, "endOffset": 92}, {"referenceID": 11, "context": "Notably, all other state-of-the-art methods use integrate-and-fire (IF) neurons [11, 10, 12], which are straightforward to fit to the rectified linear units commonly used in deep convolutional networks.", "startOffset": 80, "endOffset": 92}, {"referenceID": 13, "context": "[14]).", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "Networks could also be trained offline as described here and then fine-tuned online using an STDP rule [22, 23] to help further reduce errors associated with converting from rate-based to spike-based networks, while avoiding difficulties with training a network in spiking neurons from scratch.", "startOffset": 103, "endOffset": 111}, {"referenceID": 22, "context": "Networks could also be trained offline as described here and then fine-tuned online using an STDP rule [22, 23] to help further reduce errors associated with converting from rate-based to spike-based networks, while avoiding difficulties with training a network in spiking neurons from scratch.", "startOffset": 103, "endOffset": 111}], "year": 2016, "abstractText": "We describe a method to train spiking deep networks that can be run using leaky integrate-and-fire (LIF) neurons, achieving state-of-the-art results for spiking LIF networks on five datasets, including the large ImageNet ILSVRC-2012 benchmark. Our method for transforming deep artificial neural networks into spiking networks is scalable and works with a wide range of neural nonlinearities. We achieve these results by softening the neural response function, such that its derivative remains bounded, and by training the network with noise to provide robustness against the variability introduced by spikes. Our analysis shows that implementations of these networks on neuromorphic hardware will be many times more power-efficient than the equivalent non-spiking networks on traditional hardware.", "creator": "LaTeX with hyperref package"}}}