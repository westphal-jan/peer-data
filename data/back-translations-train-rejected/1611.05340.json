{"id": "1611.05340", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Nov-2016", "title": "Approximating Wisdom of Crowds using K-RBMs", "abstract": "An important way to make large training sets is to gather noisy labels from crowds of non experts. We propose a method to aggregate noisy labels collected from a crowd of workers or annotators. Eliciting labels is important in tasks such as judging web search quality and rating products. Our method assumes that labels are generated by a probability distribution over items and labels. We formulate the method by drawing parallels between Gaussian Mixture Models (GMMs) and Restricted Boltzmann Machines (RBMs) and show that the problem of vote aggregation can be viewed as one of clustering. We use K-RBMs to perform clustering. We finally show some empirical evaluations over real datasets.", "histories": [["v1", "Wed, 16 Nov 2016 16:01:48 GMT  (28kb,D)", "http://arxiv.org/abs/1611.05340v1", "8 pages, 1 figure"], ["v2", "Thu, 17 Nov 2016 02:48:04 GMT  (28kb,D)", "http://arxiv.org/abs/1611.05340v2", "8 pages, 1 figure"]], "COMMENTS": "8 pages, 1 figure", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["abhay gupta"], "accepted": false, "id": "1611.05340"}, "pdf": {"name": "1611.05340.pdf", "metadata": {"source": "CRF", "title": "Vote Aggregation as a Clustering Problem", "authors": ["Abhay Gupta"], "emails": ["abhgup@microsoft.com"], "sections": [{"heading": "1 Introduction", "text": "It has become one of the most important sectors in Europe in recent years, one of the most important sectors in the world, one where people are able to flourish and where people are able to flourish, \"he said."}, {"heading": "2 Related Work", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Restricted Boltzmann Machines", "text": "The Restricted Boltzmann Machine is a two-part, undirected graphical model with visible (observed) units and hidden (latent) units. RBM can be understood as MRF with latent factors that explain the input visible data using binary latent variables. RBM consists of visible data v of dimension L, which can take real values or binary values, and stochastic binary variables h of dimension K. The parameters of the model are the weight matrix W-RLxK, which defines a potential between visible input variables and stochastic binary variables, the distortions c-RL for visible units and the distortions b-RK for hidden units. If the visible units have real values, the model is called a Gaussian RBM, and its common probability distribution can be defined as follows: P (v, h) = 1Z-Exp (\u2212 E (v, 1) = Raussian probability distribution."}, {"heading": "2.2 Gaussian-softmax RBMs", "text": "We define the Gauss-softmax RBM as Gauss-softmax RBM with the restriction that at most one hidden unit can be activated at a time when the input is given, i.e., \u2211 j hj \u2264 1. The energy function of the Gauss-softmax RBM can be described in vectorized form as follows: E (v, h) = 1 2\u03c32 | | v \u2212 c | | 2 \u2212 1 \u03c3 vTWh \u2212 bTh (5) subject to \u2211 j hj \u2264 1. In this model, the conditional probabilities of the visible or hidden units can be calculated on the basis of the other layer as follows: P (v | h) = N (v; \u03c3Wh + c, \u03c32I) (6) P (hj = 1 | v) = exp (1\u03c3w T j v + bj) 1 + \u2211 j \u2032 exp (1 \u03c3w T j j v + bj \u2032) (7), where wj-j is the column of W (bj + w = 1 W)."}, {"heading": "2.3 K-Means", "text": "The k-mean clustering is an unattended algorithm that assigns clusters to data points. It can be written as follows: \u2022 Randomly select k cluster centers, \u00b5 (0) = \u00b5 (0) 1, \u00b5 (0) 2, \u00b7 \u00b7 \u00b7, \u00b5 (0) k. \u2022 Assign an incoming data point xj to the nearest cluster center C (t) (j) = mini | | \u00b5i \u2212 xj | | 2 \u2022 \u00b5i becomes the center of the cluster. \u00b5t + 1i = min\u00b5 \u2211 j: C (j) = i | \u00b5 \u2212 xj | 2The procedure repeats until convergence, i.e. all points are assigned to the best cluster centers and over many attempts r, we take the best possible solution as cluster assignment."}, {"heading": "2.4 Gaussian Mixture Models", "text": "The Gaussian mixing model is a directional graphical model in which the probability of visible units is expressed as a convex combination of Gaussians. The probability of a GMM with K + 1 Gaussians can be described as follows: P (v) = K \u2211 k = 0 \u03c0kN (v; \u00b5k, \u041ak) (8) For the rest of the paper, we refer to GMM with common spherical covariance as GMM (\u00b5k, \u03c32I). K means can be understood as a special case of GMM with spherical covariance by granting GMM with arbitrarily positive, defined covariance matrices. Compared to GMM, the formation of K means is highly efficient; therefore, it is plausible to understand K means as a special case of GMM with spherical covariance by treating the GMM algorithm as a learned equation."}, {"heading": "3 Vote Aggregation as a Clustering Problem", "text": "The following section outlines the evidence for vote aggregation as a special case for cluster problems when attempting to model the problem using RBMs."}, {"heading": "3.1 Vote Aggregation using RBMs", "text": "In a vote aggregation, the data is the observed labels. Therefore, we can see that learning RBMs is similar to aggregating voices according to the Dawid Skene algorithm, which also minimizes the negative protocol probability of the observed labels. But, in the training of RBMs, we often come across the normalization constant Z, which is insoluble, and that makes it difficult to train an RBM, and we need to approach Z to learn the ideal parameters for it. Therefore, it becomes difficult to apply RBMs directly to aggregate voices."}, {"heading": "3.2 Equivalence between Gaussian Mixture Models and RBMs with a softmax constraint", "text": "In this section we show that a Gaussian RBM with hidden Softmax units can be converted into a Gaussian mixing model and vice versa. This connection between mixing models and RBMs with a Softmax constraint completes the chain of connections between K averages, GMMs and Gaussian Softmax RBMs and helps us visualize voice aggregation as a cluster problem. As Eq.6 shows, the conditional probability of visible units follows a Gaussian distribution given the hidden unit activations for Gaussian Softmax RBMs. From this perspective, the Gaussian Softmax RBM can be considered a mixture of Gaussians whose middle components correspond to possible hidden unit configurations. In this section, we show an explicit equivalence between these two models by showing the conversion equations between GMM (\u00b5k, \u03c32I) with K + 1 Gaussian components and the Gaussian Softmax BM with hidden units."}, {"heading": "1. From Gaussian-softmax RBM to GMM(\u00b5k, \u03c32I):", "text": "Let's start with the decomposition of the chain rule: P (v, h) = P (v | h) (h), (9) where P (h) = 1Z-exp (\u2212 E (v, h))) dv (10) Since there is only a finite number of hidden unit configurations, we can explicitly enumerate the previous probabilities: P (hj = 1) = exp (\u2212 E (hj = 1, v)) dv (\u2212 E (hj = 1, v)) dv (11) If we define p (\u2212 E (v, hj = 1)) dv, then we have P (hj = 1) = \u03c0 (hj = 1)."}, {"heading": "2. From GMM(\u00b5k, \u03c32I) to Gaussian-softmax RBM:", "text": "Let us suppose that we have the following Gaussian mixture with K + 1 components and the common spherical covariance \u03c32I: P (v) = K \u2211 j = 0 \u03c0jN (v; \u00b5j, \u03c3 2I) (13) This GMM can be converted into a Gaussian Softmax RBM with the following transformations: c = \u00b50 (14) wj = 1\u03c3 (\u00b5j \u2212 c) (15) b = log \u03c00 \u2212 1 2 | | wj | | 2 \u2212 1 \u03c3 wTj c (16) It is easy to see that the conditional distribution P (v | hj = 1) can be formulated as a Gaussian distribution with an average \u00b5j = \u03c3wj + c, which can be reproduced with Gaussian Softmax RBM.Furthermore, we can restore the rear probabilities of the hidden units specified as visible units as follows: P (hj = 1 | ausyausyj + c, which can be reconstructed with Gausyj Softmax RBMJ = BMJ _ 2)."}, {"heading": "3.3 From GMMs to Clustering Assignments", "text": "GMMs learn a density function over the data as they try to maximize their probability. From the maximum probability estimate, it follows that the equation a GMM is trying to learn is the maximum probability. However, since we do not know the probability, we maximize the marginal probability given by the maximum probability. P (y = i | xj) = maximum probability k = 1 P (yj = i, xj), i.e. P (y = i | xj), where k is the number of clusters. From the perspective of the Gaussian Bayes classifier, P (y = i | xj) = P (xj | y = i) P (y = i) / P (xj), i.e. P (y = i | xj) m / 2 | | | | 1 / 2 exp [\u2212 12 (xj \u2212 \u00b5i), p = 1 (xj \u2212 valyi) P (y \u2212 agyi) (17)."}, {"heading": "4 Clustering using K-RBMs", "text": "Our system uses K component RBMs. Each RBM component learns a nonlinear subspace. The visible units vi, i = 1, \u00b7 \u00b7 \u00b7, I correspond to an I-dimensional visible (input) space and the hidden units hj, j = 1, \u00b7 \u00b7 \u00b7, J correspond to a learned nonlinear J-dimensional subspace."}, {"heading": "4.1 K-RBM Model", "text": "Each component of RBM has a set of symmetrical weights (and asymmetrical distortions) wk-R (I + 1) x (J + 1), which learn a nonlinear subspace. Note that these weights include forward and backward distorted terms. We call this error after kn. The total reconstruction error t in each iteration is given by \u0445N = 1 Mink knob. K RBMs are trained simultaneously. During RBM training, we associate data points with RBMs, based on how well each component of RBM is able to reconstruct the data \u00b7 \u00b7 The K RBM points are trained simultaneously with the initial components of RK-Points."}, {"heading": "4.2 Methodology", "text": "As with traditional K-mean clusters, the algorithm switches between two steps: (1) the calculation of a data point with a cluster and (2) the updating of cluster parameters. In K-RBMs, the n-th data point is associated with the kth RBM (cluster) when its reconstruction error from this RBM is lowest compared to other RBMS, i.e. when kn < k \u2032 n \u0442 k 6 = k \u2032, k \u00b2, k \u00b2 ig {1, \u00b7 \u00b7, K}. Once all points are associated with one of the RBMS, the weights of the RBMS are learned in a batch update. In hard clusters, the data points are exhausted (i.e. each data point must be associated with a cluster) and split discretely (i.e. each data point is associated with only one cluster)."}, {"heading": "5 Experimental Results", "text": "In this section, we report on empirical results of our method on real crowdsourcing data. We look at the L0 error metric. Let's give y the true rating and yb the estimate. The error metrics are defined as: (1) L0 = I (y 6 = yb). All research results (code and data sets) are reproducible and available at: https: / / github.com / gupta-abhay / deep-voteaggregate."}, {"heading": "5.1 Data", "text": "The rating scale is 5-level: perfect, excellent, good, fair or bad. On average, each pair was labeled by 6 different workers, and each worker labeled 90 pairs. More than 10 workers labeled only one pair.Dog Image Labeling We selected the images of 4 dog breeds from the Stanford dog dataset [8]: Norfolk Terrier (172), Norwich Terrier (185), Irish Wolfhound (218) and Scottish Deerhound (232). The number of images for each breed is in brackets. In total, there are 807 images. One worker labeled at most one image, and each image was labeled ten times."}, {"heading": "5.2 Architectures", "text": "There are four architectures for both datasets. We consider two RBMs, binary-binary RBMs and gaussian-binary RBMs. The architectures are the following:"}, {"heading": "5.2.1 Web search relevance rating", "text": "1. Binary binary RBM with 30 visible units and 5 hidden units. 2. Binary binary RBM with 18 visible units and 3 hidden units. 3. Gaussian binary RBM with 6 visible units and 5 hidden units. 4. Gaussian binary RBM with 6 visible units and 3 hidden units."}, {"heading": "5.2.2 Dog Image Labeling", "text": "1. Binary binary RBM with 40 visible units and 4 hidden units. 2. Binary binary RBM with 20 visible units and 2 hidden units. 3. Gaussian binary RBM with 10 visible units and 4 hidden units. 4. Gaussian binary RBM with 10 visible units and 2 hidden units."}, {"heading": "5.3 Results", "text": "We report on the results, both L0 and L1 errors of tables 1 and 2. The L0 error of the Dawid Skene model on the web search data is 0.17 and the error on the dog data is 0.21."}, {"heading": "5.4 Dicussion and Analysis", "text": "This may be because RBMs collect binary data and are therefore able to capture the one-hot encodings in a good way. Furthermore, we see that when we use Gaussian binary RBMs, we get 100% errors in the web data. This may be because Gaussian sampling the data for this dataset is not ideal. If we try CD-k over k = 2, we get huge reconstruction errors for each data point. However, CD-2 exceeds CD-1 and CD-2 CD-1. Also, PCD gives huge reconstruction errors for the web dataset, but provides results comparable to CD-1 for the dog dataset. We give a plot for the average reconstruction error per sample, as the RBM for the web dataset continues in Figure 1."}], "references": [{"title": "Maximum likeihood estimation of observer error-rates using the EM algorithm", "author": ["A.P. Dawid", "A.M. Skene"], "venue": "Journal of the Royal Statistical Society,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1979}, {"title": "Learning from crowds", "author": ["V.C. Raykar", "S. Yu", "L.H. Zhao", "G.H. Valadez", "C. Florin", "L. Bogoni", "L. Moy"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2010}, {"title": "Variational inference for crowdsourcing", "author": ["Q. Liu", "J. Peng", "A. Ihler"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2012}, {"title": "Optimistic knowledge gradient policy for optimal budget allocation in crowdsourcing", "author": ["X. Chen", "Q. Lin", "D. Zhou"], "venue": "In Proceedings of the 30th International Conferences on Machine Learning,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2013}, {"title": "Learning from the wisdom of crowds by minimax entropy", "author": ["D. Zhou", "J.C. Platt", "S. Basu", "Y. Mao"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2012}, {"title": "On general laws and the meaning of measurement in psychology", "author": ["G. Rasch"], "venue": "In Proceedings of the 4th Berkeley Symposium on Mathematical Statistics and Probability,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1961}, {"title": "Statistical theories of mental test scores", "author": ["F.M. Lord", "M.R. Novick"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1968}, {"title": "Reducing the dimensionality of data with neural networks", "author": ["G.E. Hinton", "R. Salakhutdinov"], "venue": "Science, vol. 313, no. 5786, pp. 504\u2013507, 2006.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2006}, {"title": "November. Efficient learning of sparse, distributed, convolutional feature representations for object recognition", "author": ["K. Sohn", "D.Y. Jung", "H. Lee", "A.O. Hero III"], "venue": "In Computer Vision (ICCV),", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2011}, {"title": "Training products of experts by minimizing contrastive divergence", "author": ["G.E. Hinton"], "venue": "Neural Computation, vol. 14, no. 8, pp. 1771\u20131800, 2002.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2002}, {"title": "Learning multiple non-linear sub-spaces using k-rbms", "author": ["S. Chandra", "S. Kumar", "C.V. Jawahar"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (pp. 2778-2785)", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2013}], "referenceMentions": [{"referenceID": 0, "context": "An advanced approach for label aggregation is suggested by Dawid and Skene[1].", "startOffset": 74, "endOffset": 77}, {"referenceID": 1, "context": "One may further assume a prior distribution over worker confusion matrices and perform Bayesian inference [2][3][4].", "startOffset": 106, "endOffset": 109}, {"referenceID": 2, "context": "One may further assume a prior distribution over worker confusion matrices and perform Bayesian inference [2][3][4].", "startOffset": 109, "endOffset": 112}, {"referenceID": 3, "context": "One may further assume a prior distribution over worker confusion matrices and perform Bayesian inference [2][3][4].", "startOffset": 112, "endOffset": 115}, {"referenceID": 4, "context": "(2012)[5] propose a minimax entropy principle for crowdsourcing.", "startOffset": 6, "endOffset": 9}, {"referenceID": 5, "context": "Such kinds of objectivity arguments have been widely discussed in the literature of mental test theory [6][7].", "startOffset": 103, "endOffset": 106}, {"referenceID": 6, "context": "Such kinds of objectivity arguments have been widely discussed in the literature of mental test theory [6][7].", "startOffset": 106, "endOffset": 109}, {"referenceID": 7, "context": "We first draw a parallel between the Restricted Boltzmann Machine (RBM) learning and the Expectation Maximization (EM) algorithm of the David-Skene algorithm and then show that Gaussian-Softmax RBMs[9] can be approximated by a Gaussian Mixture Model (GMM), whose specific conditions lead to a direct mapping to the traditional K-means algorithm[10][11].", "startOffset": 198, "endOffset": 201}, {"referenceID": 8, "context": "We first draw a parallel between the Restricted Boltzmann Machine (RBM) learning and the Expectation Maximization (EM) algorithm of the David-Skene algorithm and then show that Gaussian-Softmax RBMs[9] can be approximated by a Gaussian Mixture Model (GMM), whose specific conditions lead to a direct mapping to the traditional K-means algorithm[10][11].", "startOffset": 344, "endOffset": 348}, {"referenceID": 10, "context": "To then elucidate the clustering paradigm, we perform clustering using theK-RBM model as proposed in [14].", "startOffset": 101, "endOffset": 105}, {"referenceID": 9, "context": ", contrastive divergence approximation [12].", "startOffset": 39, "endOffset": 43}, {"referenceID": 4, "context": "1 Data Web search relevance rating The web search relevance rating dataset contains 2665 query-URL pairs and 177 workers[5].", "startOffset": 120, "endOffset": 123}], "year": 2017, "abstractText": "An important way to make large training sets is to gather noisy labels from crowds of non experts. We propose a method to aggregate noisy labels collected from a crowd of workers or annotators. Eliciting labels is important in tasks such as judging web search quality and rating products. Our method assumes that labels are generated by a probability distribution over items and labels. We formulate the method by drawing parallels between Gaussian Mixture Models (GMMs) and Restricted Boltzmann Machines (RBMs) and show that the problem of vote aggregation can be viewed as one of clustering. We use K-RBMs to perform clustering. We finally show some empirical evaluations over real datasets.", "creator": "LaTeX with hyperref package"}}}