{"id": "1611.05369", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Nov-2016", "title": "Fast On-Line Kernel Density Estimation for Active Object Localization", "abstract": "A major goal of computer vision is to enable computers to interpret visual situations---abstract concepts (e.g., \"a person walking a dog,\" \"a crowd waiting for a bus,\" \"a picnic\") whose image instantiations are linked more by their common spatial and semantic structure than by low-level visual similarity. In this paper, we propose a novel method for prior learning and active object localization for this kind of knowledge-driven search in static images. In our system, prior situation knowledge is captured by a set of flexible, kernel-based density estimations---a situation model---that represent the expected spatial structure of the given situation. These estimations are efficiently updated by information gained as the system searches for relevant objects, allowing the system to use context as it is discovered to narrow the search.", "histories": [["v1", "Wed, 16 Nov 2016 17:04:35 GMT  (2836kb,D)", "http://arxiv.org/abs/1611.05369v1", "arXiv admin note: text overlap witharXiv:1607.00548"]], "COMMENTS": "arXiv admin note: text overlap witharXiv:1607.00548", "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["anthony d rhodes", "max h quinn", "melanie mitchell"], "accepted": false, "id": "1611.05369"}, "pdf": {"name": "1611.05369.pdf", "metadata": {"source": "CRF", "title": "Fast On-Line Kernel Density Estimation for Active Object Localization", "authors": ["Anthony D. Rhodes", "Max H. Quinn", "Melanie Mitchell"], "emails": ["arhodespdx@gmail.com", "quinn.max@gmail.com", "mm@pdx.edu"], "sections": [{"heading": null, "text": "In fact, most people who are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move and to move."}, {"heading": "II. BACKGROUND", "text": "In fact, most of them will be able to play by the rules that they need for their work, and they will be able to play by the rules that they need for their work."}, {"heading": "III. DATASET AND SPECIFIC TASK", "text": "Following [17], in this study we use the Portland State Dog-Walking Images. [2] This data set currently contains 700 photographs taken in various locations. Each image is an example of a \"dog-walking\" situation in a natural environment. (Figure 1 gives some examples from this data set.) In each image, we focus on a simplified subset of the dog-walking situation: photographs in which there is exactly one (human) dog walker, dog, leash, along with unlabeled \"filth\" (such as non-dog-handlers, buildings, etc.) as in Figure 1. There are 500 such images in this subset. The task of Situate is to locate the objects that define the situation - dog walker, dog and leash - in a test image based on an object that suggests an object in the middle as an object, i.e., the object is described as a unit (as a unit)."}, {"heading": "IV. SITUATE\u2019S ACTIVE OBJECT LOCALIZATION ALGORITHM", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. Learned Situation Models", "text": "Situate learns a probabilistic model of situational structure - a situational model - by following two common distributions via ground truth boxes in the training data.Joint Location is the common distribution across the location (Boundingbox Center) of the handler, dog and leash in an image. Joint Dimensions is the common distribution of the width and height of the Boundingbox of these three objects within an image. In short, these two common distributions encode expectations about the spatial and scale relationships between the relevant objects in the situation: when the system locates an object in a test image, the learned Joint Distributions can be conditioned to the characteristics of that object in order to predict where and what size the other objects are likely to be. The system also learns previous distributions via Boundingbox width and height for each object category. Prior distribution across the locations is uniform for each category, as we do not want the system to capture the presentations of the photographer's image center learning relevant proximity to the objects."}, {"heading": "B. Running Situate on a Test Image", "text": "In fact, most of the people who are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to dance, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance"}, {"heading": "C. A Sample Run of Situate; Prior Results", "text": "Figure 2 illustrates situational active search with visualizations of the workspace and probability distributions from a run on an example screen. Prior to this run, the program learned a situational model from training images as described in Section IV-A. The previous and shared distributions were learned as multivariate Gaussians. The caption in Figure 2 describes the dynamics of this run. In [17], we compared Situate's performance with that of several variations as well as with a recently published category-independent object recognition system [18]. Our results support the hypothesis that Situates active, context-related search method was able to locate the three relevant objects with dramatically fewer object suggestions than the comparison systems that did not use active search or context-related information."}, {"heading": "V. FAST KERNEL DENSITY ESTIMATION WITH", "text": "CONTEXT-BASED IMPORTANCE CLUSTERINGAs described above, the Joint Location and Joint BBDimensions distributions used in [17] have been calculated as multivariate Gaussian distributions learned from a series of training images. On the one hand, this model limitation is computationally efficient, making it desirable for real-time probability density estimates. However, all parametric assumptions inevitably limit the meaningfulness - and thus the general utility - of a model. In this section, we present an efficient algorithm for calculating non-parametric probability density estimates. In contrast to parametric methods, non-parametric methods do not represent global a priori assumptions about the shape of a distribution function. These models are therefore highly flexible and capable of mapping useful patterns in different datasets."}, {"heading": "A. Overview of Kernel Density Estimation", "text": "\"We will estimate the density f at a point where we have the most similarity with the density at the point where we contribute the most to the estimate of the density.,\" xN, with the following formula: f) \"We will formalize the density f at a point, x1, xN, with the following formula: f).\" (z) \"We will estimate the density f at a point, x1, xN, with which we estimate the density f at a point.\" (z) \"We will estimate the density f at a point, x1, xN, with the following formula: f.\" (z) \"We will estimate the density f at a point.\" (z) \"We will estimate the density f at a point, x1.\" (z) We will estimate the density f. \"(z). xN, with the following formula: f.\" (z) We will estimate the density f at a point, x1. \"(z)"}, {"heading": "B. Context-Based Importance Clustering", "text": "In fact, it is such that most people who are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to fight, to fight, to fight, to fight, to move, to move, to fight, to fight, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move,"}, {"heading": "C. Kernel Density Estimation with Multipole Expansions", "text": "The second innovation is to apply a fast approximation technology to the estimation of distributions: the method of multipolar expansions (in short, multipolar expansions are a physical method for estimating probability densities with Taylor expansions).Let us apply the multipolar method for estimating equations by applying the multipolar method for estimating equations by forming the center of the Taylor series expansion. (The key terms of this method are that the schematics of the factorized Gaussians can be expressed in [21], the kernel estimation of the centroid x-te-te-te-te-te-te-te-te-expansion in the Taylor series).We leave out the details for a detailed treatment, see [21] the multipolar form of these factorized Gaussians, which we estimate, the multipolar estimation via the centroid x-th-ten-ten-ten-ten-ten-ten-ten-ten-ten-ten-ten-ten-ten-ten-ten-ten-ten-ten-ten-ten-ten-ten-ten-ten-ten-ten-ten-ten-ten-ten-ten-ten."}, {"heading": "D. Stochastic Filtering", "text": "A significant problem arises when we consider performing this density approximation for a large M (i.e., for many different punctual approximations), which may be required in cases where comprehensive, interpretable models are desired. The problem is that the inevitable errors in the approach to Taylor can be accumulated. Although the overall error in our density approximation can be improved by selecting a sufficiently large order for the Taylor expansions (e.g. a multivariate square, cubic, etc.), the error margin can still be potentially exaggerated if it extends over points that are a large distance from the center of each Gaussian nucleus; of course, this problem is further amplified by summarizing the size of the sample points, N, Growos. There are a few suggested means in the literature on this problem of aggregated errors. The authors in [20] simply propose that the points over which the density estimate is performed limit to a small subset of space."}, {"heading": "E. MIC-Situate Algorithm", "text": "The following steps in our algorithm, Multipole Density Estimation with Importance Clustering (MIC Situate): Suppose we have a training set S, and Situate runs on a test screen T. As described in Section IV at each step in a run, Situate randomly selects an object category, tries a location and width and height of the delimiter box from its current distributions for the given object category, to create an object suggestion, and evaluates this object suggestion to determine whether it should be added to the workspace. Suppose that L object suggestions have been added to the workspace, with values {l1,.., lL}. (For example, l1 could be the (width, height) values of a detected dog handler, and l2 could be the (width, height) values of a detected dog delimiting field.) Whenever a new object suggestion is added to the workspace, we make the cluster size with the following object:"}, {"heading": "VI. EXPERIMENTAL RESULTS", "text": "In this section, we present the methods described above for the MIC Situate algorithm, which applies both our novel meaning clustering technique and our fast, non-parametric, multipolar method for learning a flexible knowledge representation of boundary box sizes of objects to active object localization. (In reporting, we use the term closed situation to refer to an image for which a method has successfully localized all three relevant objects within a maximum number of iterations; we use the term failed situation to refer to an image that has not resulted in a closed situation captured within the maximum allocated iterations. (In total, we tested four different methods for object localization in dog situations: (1) multipoles (using the non-parametric multipole method with the meaning described above), as described above (2) multipoles (no IC method): non-parametric annular data without meaning (where clustering is generated)."}, {"heading": "VII. CONCLUSIONS AND FUTURE WORK", "text": "Our work has provided the following contributions: (1) We have proposed a new approach to actively locating objects in visual situations, using a knowledge-driven search with adaptable probability models. (2) We have developed an innovative, universal machine learning process that is capable of efficiently creating flexible models in a challenging online setting. (3) In addition, this estimation method can also be scaled well in conjunction with importance clusters with a large number of observed variables. (4) We have applied these techniques to the problem of conditional density estimation. (5) As proof of concept, we have applied our algorithm to a highly variable and challenging data collection. The work described in this paper is an early step in our broader research goal: the development of a system that makes symbolic knowledge with a subordinate situation applicable in a profound way to visual situations. (1) The work that makes our system adaptable to a long-term interoperability is the ability to the system."}, {"heading": "ACKNOWLEDGMENTS", "text": "This material is based on work supported by the National Science Foundation under grant number IIS-1423651. Any opinions, findings, conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the National Science Foundation."}], "references": [{"title": "The Copycat project: A model of mental fluidity and analogy-making", "author": ["D.R. Hofstadter", "M. Mitchell"], "venue": "Advances in Connectionist and Neural Computation Theory, K. Holyoak and J. Barnden, Eds. Ablex Publishing Corporation, 1994, vol. 2, pp. 31\u2013112.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1994}, {"title": "Surfaces and Essences", "author": ["D. Hofstadter", "E. Sander"], "venue": "Basic Books,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2013}, {"title": "Visual objects in context", "author": ["M. Bar"], "venue": "Nature Reviews Neuroscience, vol. 5, no. 8, pp. 617\u2013629, 2004.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2004}, {"title": "More than meets the eye: The active selection of diagnostic information across spatial locations and scales during scene categorization", "author": ["G.L. Malcolm", "P.G. Schyns"], "venue": "Scene Vision: Making Sense of What We See, K. Kveraga and M. Bar, Eds. MIT Press, 2014, pp. 27\u201344.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2014}, {"title": "Scene context guides eye movements during visual search", "author": ["M. Neider", "G. Zelinsky"], "venue": "Vision Research, vol. 46, no. 5, pp. 614\u2013621, 2006.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2006}, {"title": "Meaning in visual search", "author": ["M.C. Potter"], "venue": "Science, vol. 187, pp. 965\u2013966, 1975.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1975}, {"title": "Expectation (and attention) in visual cognition.", "author": ["C. Summerfield", "T. Egner"], "venue": "Trends in Cognitive Sciences,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2009}, {"title": "The Pascal visual object classes (VOC) challenge", "author": ["M. Everingham", "L. Gool", "C.K.I. Williams", "J. Winn", "A. Zisserman"], "venue": "International Journal of Computer Vision, vol. 88, no. 2, pp. 303\u2013338, 2010.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2010}, {"title": "Deep residual learning for image recognition", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "arXiv:1512.03385, 2015.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "Fast R-CNN", "author": ["R. Girshick"], "venue": "International Conference on Computer Vision (ICCV). IEEE, 2015, pp. 1440\u20131448.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "You only look once: Unified, real-time object detection", "author": ["J. Redmon", "S. Divvala", "R. Girshick", "A. Farhadi"], "venue": "arXiv:1506.02640, 2015.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2015}, {"title": "ImageNet large scale visual recognition challenge", "author": ["O. Russakovsky", "J. Deng", "H. Su", "J. Krause", "S. Satheesh", "S. Ma", "Z. Huang", "A. Karpathy", "A. Khosla", "M. Bernstein", "A.C. Berg", "L. Fei-Fei"], "venue": "International Journal of Computer Vision, vol. 115, no. 3, pp. 211\u2013252, 2015.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "Adaptive gaze control for object detection.", "author": ["G.C.H.E. de Croon", "E.O. Postma", "H.J. van den Herik"], "venue": "Cognitive Computation,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2011}, {"title": "An active search strategy for efficient object class detection", "author": ["A. Gonzalez-Garcia", "A. Vezhnevets", "V. Ferrari"], "venue": "Conference on Computer Vision and Pattern Recognition (CVPR). IEEE, 2015, pp. 3022\u20133031.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2015}, {"title": "Adaptive object detection using adjacency and zoom prediction", "author": ["Y. Lu", "T. Javidi", "S. Lazebnik"], "venue": "arXiv:1512.07711, 2015.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}, {"title": "Active object localization in visual situations", "author": ["M.H. Quinn", "A.D. Rhodes", "M. Mitchell"], "venue": "arXiv:1607.00548, 2016.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2016}, {"title": "Prime object proposals with randomized Prim\u2019s algorithm", "author": ["S. Manen", "M. Guillaumin", "L.V. Gool"], "venue": "International Conference on Computer Vision (ICCV). IEEE, 2013, pp. 2536\u20132543.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2013}, {"title": "Understanding of internal clustering validation measures", "author": ["Y. Liu", "Z. Li", "H. Xiong", "X. Gao", "J. Wu"], "venue": "2010 IEEE International Conference on Data Mining. IEEE, 2010, pp. 911\u2013916.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2010}, {"title": "Efficient on-line nonparametric kernel density estimation", "author": ["C.G. Lambert", "S.E. Harrington", "C.R. Harvey", "A. Glodjo"], "venue": "Algorithmica, vol. 25, no. 1, pp. 37\u201357, 1999.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 1999}, {"title": "Efficient kernel machines using the improved fast Gauss transform", "author": ["C. Yang", "R. Duraiswami", "L.S. Davis"], "venue": "Advances in neural information processing systems, 2004, pp. 1561\u20131568.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2004}, {"title": "Positive definite kernels: past, present and future", "author": ["G. Fasshauer"], "venue": "Dolomite Research Notes on Approximation, 2011.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2011}, {"title": "Classes of Kernels for Machine Learning: A Statistics Perspective", "author": ["M.G. Genton"], "venue": "Journal of Machine Learning Research, vol. 2, no. Dec, pp. 299\u2013312, 2001.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2001}, {"title": "Conditional Density Estimation via Least-Squares Density Ratio Estimation", "author": ["M. Sugiyama", "I. Takeuchi", "T. Suzuki", "T. Kanamori"], "venue": "AIS- TATS, pp. 781\u2013788, 2010.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2010}, {"title": "Clustering to minimize the maximum intercluster distance", "author": ["T.F. Gonzalez"], "venue": "Theoretical Computer Science, vol. 38, pp. 293\u2013306, 1985.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 1985}, {"title": "Optimal algorithms for approximate clustering", "author": ["T. Feder", "D. Greene"], "venue": "Proceedings of the twentieth annual ACM symposium on Theory of computing - STOC \u201988. New York, New York, USA: ACM Press, 1988, pp. 434\u2013444.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 1988}, {"title": "A Bayesian Approach to Parameter Estimation for Kernel Density Estimation via Transformations", "author": ["Q. Liu", "D. Pitt", "X. Zhang", "X. Wu"], "venue": "Annals of Actuarial Science, vol. 5, no. 2, pp. 181\u2013193, 2011.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2011}, {"title": "Analogy-Making as Perception: A Computer Model", "author": ["M. Mitchell"], "venue": null, "citeRegEx": "28", "shortCiteRegEx": "28", "year": 1993}], "referenceMentions": [{"referenceID": 0, "context": "Different instances can be visually dissimilar, but conceptually analogous, and can even require \u201cconceptual slippage\u201d from a prototype [1] (e.", "startOffset": 136, "endOffset": 139}, {"referenceID": 1, "context": "While the term situation can be applied to any abstract concept [3], most people would consider a visual situation category to be\u2014like Dog-Walking\u2014a named concept that invokes a collection of objects, regions, attributes, actions, and goals with particular spatial, temporal, and/or semantic relationships to one another.", "startOffset": 64, "endOffset": 67}, {"referenceID": 2, "context": "This interaction enables a human viewer to very quickly locate relevant aspects of the situation [4]\u2013[8].", "startOffset": 97, "endOffset": 100}, {"referenceID": 6, "context": "This interaction enables a human viewer to very quickly locate relevant aspects of the situation [4]\u2013[8].", "startOffset": 101, "endOffset": 104}, {"referenceID": 7, "context": "5 [9].", "startOffset": 2, "endOffset": 5}, {"referenceID": 8, "context": ", [10]\u2013[12]).", "startOffset": 2, "endOffset": 6}, {"referenceID": 10, "context": ", [10]\u2013[12]).", "startOffset": 7, "endOffset": 11}, {"referenceID": 7, "context": "Popular benchmark datasets for object-localization and detection include Pascal VOC [9] and ILSVRC [13].", "startOffset": 84, "endOffset": 87}, {"referenceID": 11, "context": "Popular benchmark datasets for object-localization and detection include Pascal VOC [9] and ILSVRC [13].", "startOffset": 99, "endOffset": 103}, {"referenceID": 12, "context": ", [14]\u2013[17]).", "startOffset": 2, "endOffset": 6}, {"referenceID": 15, "context": ", [14]\u2013[17]).", "startOffset": 7, "endOffset": 11}, {"referenceID": 15, "context": "directed search requires dramatically fewer object proposals than methods that do not use such information [17].", "startOffset": 107, "endOffset": 111}, {"referenceID": 15, "context": "While this approach\u2014active search with dynamically updated situation models\u2014shows promise for efficient object localization, in the work reported in [17] it was limited by our use of low-dimensional parametric distributions to represent prior knowledge and perceived context.", "startOffset": 149, "endOffset": 153}, {"referenceID": 15, "context": "We report preliminary experiments testing this algorithm on the dataset of [17] and assess its potential for more general applications in knowledge-based computer vision tasks.", "startOffset": 75, "endOffset": 79}, {"referenceID": 15, "context": "Following [17], in this study we use the \u201cPortland State Dog-Walking Images\u201d [2].", "startOffset": 10, "endOffset": 14}, {"referenceID": 15, "context": "In the version of Situate described in [17], the joint distributions (and prior distributions over bounding-box dimensions) were modeled as multivariate Gaussians.", "startOffset": 39, "endOffset": 43}, {"referenceID": 15, "context": "In [17] we compared Situate\u2019s performance with that of several variations, as well as a recently published categoryindependent object detection system [18].", "startOffset": 3, "endOffset": 7}, {"referenceID": 16, "context": "In [17] we compared Situate\u2019s performance with that of several variations, as well as a recently published categoryindependent object detection system [18].", "startOffset": 151, "endOffset": 155}, {"referenceID": 15, "context": "As we described above, the Joint Location and Joint BBDimensions distributions used in [17] were computed as multivariate Gaussian distributions, learned from a set of training images.", "startOffset": 87, "endOffset": 91}, {"referenceID": 15, "context": "In the system described in [17], this was done by conditioning the learned joint multivariate Gaussian width/height distributions on the detected dog-walker in order to form updated Gaussian distributions for \u201cdog\u201d and \u201cleash\u201d.", "startOffset": 27, "endOffset": 31}, {"referenceID": 17, "context": "The number of clusters we use for k-means is rendered optimally from a range of possible values, according to a conventional internal clustering validation measure based on a variance ratio criterion (Calinski-Harabasz index) [19].", "startOffset": 226, "endOffset": 230}, {"referenceID": 18, "context": "In short, multipole expansions are a physicsinspired method [20] for estimating probability densities with Taylor expansions.", "startOffset": 60, "endOffset": 64}, {"referenceID": 19, "context": "The key advantage of this method is that, following the scheme of the factorized Gaussians presented in [21], the kernel estimate about the centroid x\u2217 (i.", "startOffset": 104, "endOffset": 108}, {"referenceID": 19, "context": ", the center of the Taylor series expansion) can be expressed in factored form (we omit the details here for brevity, see [21] for a detailed treatment).", "startOffset": 122, "endOffset": 126}, {"referenceID": 18, "context": "The multipole form of this factorization [20] is the following expression:", "startOffset": 41, "endOffset": 45}, {"referenceID": 20, "context": "[22], [23], with asymptotic convergence properties (subject to choice of bandwidth).", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "[22], [23], with asymptotic convergence properties (subject to choice of bandwidth).", "startOffset": 6, "endOffset": 10}, {"referenceID": 22, "context": "By comparison, other conventional conditional density estimation procedures, such as the least-squares method, require O(MN) computations [24].", "startOffset": 138, "endOffset": 142}, {"referenceID": 18, "context": "The authors in [20] simply suggest limiting the points over which the density estimation is performed to a small subset of the space, but this is a fairly weak and impractical compromise for a general problem setting.", "startOffset": 15, "endOffset": 19}, {"referenceID": 19, "context": "Alternatively, the authors in [21] suggest performing a constrained clustering of the density space and then estimating", "startOffset": 30, "endOffset": 34}, {"referenceID": 23, "context": "Various approximate solutions exist, including an adaptive, greedy algorithm called \u201cfarthest point clustering\u201d [25] and a more computationally-", "startOffset": 112, "endOffset": 116}, {"referenceID": 24, "context": "efficient version given by [26].", "startOffset": 27, "endOffset": 31}, {"referenceID": 25, "context": "density estimations used the following conventional \u201crule of thumb\u201d bandwidth [27]:", "startOffset": 78, "endOffset": 82}, {"referenceID": 0, "context": "In the longer term, our goal is to extend Situate to incorporate important aspects of Hofstadter and Mitchells Copycat architecture [1] in order to give it the ability to quickly and flexibly recognize visual actions, object groupings, relationships, and to be able to make analogies (with appropriate conceptual slippages) between a given image and situation prototypes.", "startOffset": 132, "endOffset": 135}, {"referenceID": 26, "context": "This interleaving was shown to be essential to the ability to create appropriate, and even creative analogies [28].", "startOffset": 110, "endOffset": 114}], "year": 2016, "abstractText": "A major goal of computer vision is to enable computers to interpret visual situations\u2014abstract concepts (e.g., \u201ca person walking a dog,\u201d \u201ca crowd waiting for a bus,\u201d \u201ca picnic\u201d) whose image instantiations are linked more by their common spatial and semantic structure than by low-level visual similarity. In this paper, we propose a novel method for prior learning and active object localization for this kind of knowledge-driven search in static images. In our system, prior situation knowledge is captured by a set of flexible, kernel-based density estimations\u2014 a situation model\u2014that represent the expected spatial structure of the given situation. These estimations are efficiently updated by information gained as the system searches for relevant objects, allowing the system to use context as it is discovered to narrow the search. More specifically, at any given time in a run on a test image, our system uses image features plus contextual information it has discovered to identify a small subset of training images\u2014 an importance cluster\u2014that is deemed most similar to the given test image, given the context. This subset is used to generate an updated situation model in an on-line fashion, using an efficient multipole expansion technique. As a proof of concept, we apply our algorithm to a highly varied and challenging dataset consisting of instances of a \u201cdog-walking\u201d situation. Our results support the hypothesis that dynamically-rendered, context-based probability models can support efficient object localization in visual situations. Moreover, our approach is general enough to be applied to diverse machine learning paradigms requiring interpretable, probabilistic representations generated from partially observed data.", "creator": "LaTeX with hyperref package"}}}