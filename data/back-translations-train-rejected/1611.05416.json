{"id": "1611.05416", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Nov-2016", "title": "Composing Music with Grammar Argumented Neural Networks and Note-Level Encoding", "abstract": "Creating any aesthetically pleasing piece of art, like music, has been a long time dream for artificial intelligence research. Based on recent success of long-short term memory (LSTM) on sequence learning, we put forward a novel system to reflect the thinking pattern of a musician. For data representation, we propose a note-level encoding method, which enables our model to simulate how human composes and polishes music phrases. To avoid failure against music theory, we invent a novel method, grammar argumented (GA) method. It can teach machine basic composing principles. In this method, we propose three rules as argumented grammars and three metrics for evaluation of machine-made music. Results show that comparing to basic LSTM, grammar argumented model's compositions have higher contents of diatonic scale notes, short pitch intervals, and chords.", "histories": [["v1", "Wed, 16 Nov 2016 19:42:40 GMT  (351kb,D)", "http://arxiv.org/abs/1611.05416v1", "5 pages, 5 figures"], ["v2", "Wed, 7 Dec 2016 20:51:36 GMT  (1776kb,D)", "http://arxiv.org/abs/1611.05416v2", "6 pages, 4 figures"]], "COMMENTS": "5 pages, 5 figures", "reviews": [], "SUBJECTS": "cs.LG cs.AI cs.SD", "authors": ["zheng sun", "jiaqi liu", "zewang zhang", "jingwen chen", "zhao huo", "ching hua lee", "xiao zhang"], "accepted": false, "id": "1611.05416"}, "pdf": {"name": "1611.05416.pdf", "metadata": {"source": "CRF", "title": "Grammar Argumented LSTM Neural Networks with Note-Level Encoding for Music Composition", "authors": ["Zheng Sun", "Jiaqi Liu", "Zewang Zhang", "Jingwen Chen", "Zhao Huo", "Ching Hua Lee", "Xiao Zhang"], "emails": ["zhangxiao}@{mail2,", "huozhao@cupl.edu.cn", "calvin-lee@ihpc.a-star.edu.sg"], "sections": [{"heading": null, "text": "Index terms - music composition, note-level encoding, neural networks LSTM, grammar argued methodF"}, {"heading": "1 INTRODUCTION", "text": "In fact, it is the case that one is able to find a solution that is capable of finding a solution, that is able to find a solution, and that is able to find a solution that is capable of finding a solution, that is able to find a solution, that is able to find a solution, and that is able to find a solution that is able to find a solution, that is able to find a solution, that is able to find a solution, that is able to find a solution, that is able to find a solution, that is able to find a solution. \""}, {"heading": "2 METHODS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Note-Level Encoding", "text": "During music composition, composers often focus on several typical musical phrases. [11], [15], [18], [17], or treat pitches and durations in separate neural networks. [20] In this work, we encode music as a quantitative span of time. [17], [19], or process pitches and durations in separate neural networks. [20], [21]. We encode music with notes."}, {"heading": "2.2 Long-Short Term Memory Neural Networks", "text": "Each hidden layer receives both input from the previous layer and input from itself (1 time step in the past). It allows networks to remember information from previous time steps. However, simple RNN does not work well in long-term dependence due to disappearing gradients. The graphical structure of LSTM in Fig.2 shows how the data flows through the LSTM module. In the following, we describe how the hidden state of u (t) of the LSTM is calculated. To get u (t) for each time step, we enter data v (t) and u (t \u2212 1) from the last time step into the LSTM module. bi, bo, bf, bc denote associated biases."}, {"heading": "2.3 Grammar Argumented Method", "text": "In fact, most of them are able to survive on their own."}, {"heading": "3 EXPERIMENTS", "text": "Our model consists of an LSTM layer and a fully connected layer. The LSTM layer consists of 128 cells and its input dimension is 89, which is the length of the binary representation of notes. There are 89 nodes in a fully connected layer and it is also the output layer. The size of our data set is 30,000 to speed up the training process, we split it into mini-batches, and the batch size is 64. We use Adam [23] to perform the descent optimization of gradients, the learning rate is set to 0.001. We build our model on a high-level neural network library Keras [24] and use TensorF low [25] as the tensor manipulation library. We train this model with original data sets and label this weight set with Orig. With the GA method, we use Orig to generate 100k notes for each rule, and get three sets of changed data with each data set."}, {"heading": "4 RESULTS", "text": "The score of this piece is shown in Fig.4. Firstly, it is strong evidence that the machine excludes overtones and prefers tones in C major scale. In this piece there is only one overtone. Secondly, it is noteworthy that the machine has already learned to use repeated rhythmic structures in a piece, as humans always do, e.g. bars 4-5 and bars 10-11. Listen from the beginning that the whole piece sounds soft and lyrical, which is in line with the music in the data set. bars 3, 4, 6 and 12 have faster bars sandwiched between slow melodies."}, {"heading": "5 EVALUATIONS", "text": "In accordance with the music theory described in Section 2.3, we present three types of metrics: percentages of notes in4Fig. 3. The grammatically argued method. With a music phrase or note sequence, a well-trained neural LSTM network is able to predict the next note. The output layer consists of 89 nodes, according to the note-level encryption method, the left 30 nodes represent the duration and the right 59 nodes the pitch. We try out these two parts because duration and pitch are all uniform codes. After the sampling result is converted to note, we check it with the composition rule, only notes that consist of rule can be added to the phrase. If a non-compressive note is predicted, we continue to try on the output distribution until the composition appears. Then, we add this modified note and its associated input to the original training group. Figure 4. A composition module comprises MIX notes."}, {"heading": "5.1 pDia", "text": "Table 1 shows the percentages of seven notes in the C major scale. The GA method works with the C major scale rule. At E6, the ratio increases by one percentage point. In addition to Dia and MIX, the tri mode also generates a high pDia. This phenomenon can be explained by pitch intervals. Since pitch intervals are contained in triads in the major scale, adding data supplemented by three-class rule also improves the pDia."}, {"heading": "5.2 pSPI", "text": "We calculate each pitch interval in the composition and calculate the percentage of pitch intervals in an octave.High pSPI contributes to a harmonious composition. As shown in Table 2, SPI and MIX mode can compose music with a high pSPI. We also note that without the GA method (Orig mode) models produce more inharmonious tones, leading to poorer compositions.5"}, {"heading": "5.3 pTri", "text": "Table 3 shows percentages of triads. In tri and MIX mode, the machine composition includes more triads than other results. We note that music composed in MIX mode works well with all three metrics. It indicates that these three rules are not contradictory, they are coherent rules in music theory."}, {"heading": "6 CONCLUSION", "text": "Although it is difficult for machines to learn rules of music theory using simple neural LSTM networks, we propose a method of grammatical reasoning and allow our model to learn these rules by inserting rule-modified data into the dataset. GA Method significantly reduces the amount of discordant notes. Our original method of note encoding also contributes to this successful system. At the note level, the machine thinks like musicians and is able to learn advanced logic from the dataset. Finally, three metrics provide a solution for the evaluation of machine-made music. Our GA Method has the potential to include rules of music theory at the highest level, such as repeated paragraphs, and it provides an approach to music with a global structure."}, {"heading": "ACKNOWLEDGMENTS", "text": "The authors would like to thank Chuangjie Ren and Qingpei Liu for helpful discussions about neural networks."}], "references": [{"title": "Draw: A recurrent neural network for image generation", "author": ["K. Gregor", "I. Danihelka", "A. Graves", "D.J. Rezende", "D. Wierstra"], "venue": "Computer Science, 2015.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2015}, {"title": "Generating sequences with recurrent neural networks", "author": ["A. Graves"], "venue": "Computer Science, 2014.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2014}, {"title": "A neural algorithm of artistic style", "author": ["L.A. Gatys", "A.S. Ecker", "M. Bethge"], "venue": "Computer Science, 2015.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2015}, {"title": "Pixel recurrent neural networks", "author": ["A. van den Oord", "N. Kalchbrenner", "K. Kavukcuoglu"], "venue": "arXiv preprint arXiv:1601.06759, 2016.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2016}, {"title": "A method for composing simple traditional music by computer", "author": ["G.M. Rader"], "venue": "Communications of the ACM, vol. 17, no. 11, pp. 631\u2013 638, 1974.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1974}, {"title": "Ai methods in algorithmic composition: a comprehensive survey", "author": ["J.D. Fern\u00e1nd Ndez", "F. Vico"], "venue": "Journal of Artificial Intelligence Research, vol. 48, no. 48, pp. 513\u2013582, 2013.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2013}, {"title": "Genotator: an environment for exploring the application of evolutionary techniques in computerassisted composition", "author": ["THYWISSEN", "KURT"], "venue": "Organised Sound, vol. 4, no. 2, pp. 127\u2013133, 1999.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1999}, {"title": "Computer modeling of musical intelligence in emi", "author": ["D. Cope"], "venue": "Computer Music Journal, vol. 16, no. 16, pp. 69\u201387, 1992.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1992}, {"title": "Harmonising chorales in the style of johann sebastian bach", "author": ["M. Allan"], "venue": "Master\u2019s Thesis, School of Informatics, University of Edinburgh, 2002.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2002}, {"title": "A connectionist approach to algorithmic composition", "author": ["P.M. Todd"], "venue": "Computer Music Journal, vol. 13, no. 4, pp. 27\u201343, 1989.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1989}, {"title": "Neural network music composition by prediction: Exploring the benefits of psychoacoustic constraints and multiscale processing", "author": ["M.C. MOZER"], "venue": "Connection Science, vol. 6, no. 2-3, pp. 247\u2013280, 1994.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1994}, {"title": "Modeling temporal dependencies in high-dimensional sequences: Application to polyphonic music generation and transcription", "author": ["N. Boulanger-Lewandowski", "Y. Bengio", "P. Vincent"], "venue": "arXiv preprint arXiv:1206.6392, 2012.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2012}, {"title": "Artificial neural networks and machine learning icann 2014", "author": ["S. Wermter", "C. Weber", "W. Duch", "T. Honkela", "P. Koprinkovahristova"], "venue": "Lecture Notes in Computer Science, vol. 8681, 2014.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning musical structure directly from sequences of music", "author": ["D. Eck", "J. Lapalme"], "venue": "University of Montreal, Department of Computer Science, CP, vol. 6128, 2008.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2008}, {"title": "Recurrent neural networks for music computation", "author": ["J.A. Franklin"], "venue": "Informs Journal on Computing, vol. 18, no. 3, pp. 321\u2013338, 2006.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2006}, {"title": "Polyphonic music generation by modeling temporal dependencies using a rnn-dbn", "author": ["K. Goel", "R. Vohra", "J. Sahoo"], "venue": "Artificial Neural Networks and Machine Learning\u2013ICANN 2014. Springer, 2014, pp. 217\u2013224.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2014}, {"title": "Finding temporal structure in music: Blues improvisation with lstm recurrent networks", "author": ["D. Eck", "J. Schmidhuber"], "venue": "Neural Networks for Signal Processing, 2002. Proceedings of the 2002 12th IEEE Workshop on. IEEE, 2002, pp. 747\u2013756.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2002}, {"title": "Polyphonic music modelling with lstm-rtrbm", "author": ["Q. Lyu", "Z. Wu", "J. Zhu"], "venue": "Proceedings of the 23rd Annual ACM Conference on Multimedia Conference. ACM, 2015, pp. 991\u2013994.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2015}, {"title": "Neural network music composition by prediction: Exploring the benefits of psychoacoustic constraints and multiscale processing", "author": ["M.C. Mozer"], "venue": "Connection Science, vol. 6, no. 2-3, pp. 247\u2013280, 1994.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 1994}, {"title": "Recurrent neural networks for music computation", "author": ["J.A. Franklin"], "venue": "INFORMS Journal on Computing, vol. 18, no. 3, pp. 321\u2013338, 2006.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2006}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural computation, vol. 9, no. 8, pp. 1735\u20131780, 1997.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 1997}, {"title": "Adam: A method for stochastic optimization", "author": ["D. Kingma", "J. Ba"], "venue": "Computer Science, 2014.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2014}, {"title": "Keras", "author": ["F. Chollet"], "venue": "https://github.com/fchollet/keras, 2015.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2015}, {"title": "TensorFlow: Large-scale machine learning on heterogeneous systems", "author": ["M. Abadi", "A. Agarwal", "P. Barham", "E. Brevdo", "Z. Chen", "C. Citro", "G.S. Corrado", "A. Davis", "J. Dean", "M. Devin", "S. Ghemawat", "I. Goodfellow", "A. Harp", "G. Irving", "M. Isard", "Y. Jia", "R. Jozefowicz", "L. Kaiser", "M. Kudlur", "J. Levenberg", "D. Man\u00e9", "R. Monga", "S. Moore", "D. Murray", "C. Olah", "M. Schuster", "J. Shlens", "B. Steiner", "I. Sutskever", "K. Talwar", "P. Tucker", "V. Vanhoucke", "V. Vasudevan", "F. Vi\u00e9gas", "O. Vinyals", "P. Warden", "M. Wattenberg", "M. Wicke", "Y. Yu", "X. Zheng"], "venue": "2015, software available from tensorflow.org. [Online]. Available: http://tensorflow.org/", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "C REATING all forms of art [1], [2], [3], [4], including music, has been a long time pursue for artificial intelligence (AI) research.", "startOffset": 27, "endOffset": 30}, {"referenceID": 1, "context": "C REATING all forms of art [1], [2], [3], [4], including music, has been a long time pursue for artificial intelligence (AI) research.", "startOffset": 32, "endOffset": 35}, {"referenceID": 2, "context": "C REATING all forms of art [1], [2], [3], [4], including music, has been a long time pursue for artificial intelligence (AI) research.", "startOffset": 37, "endOffset": 40}, {"referenceID": 3, "context": "C REATING all forms of art [1], [2], [3], [4], including music, has been a long time pursue for artificial intelligence (AI) research.", "startOffset": 42, "endOffset": 45}, {"referenceID": 4, "context": "In the early years, symbolic AI methods were popular and specific grammars describing a set of rules drive the composition [5], [6].", "startOffset": 123, "endOffset": 126}, {"referenceID": 5, "context": "In the early years, symbolic AI methods were popular and specific grammars describing a set of rules drive the composition [5], [6].", "startOffset": 128, "endOffset": 131}, {"referenceID": 6, "context": "This method was latter improved by evolutionary algorithms in different ways [7], including the famous EMI project [8].", "startOffset": 77, "endOffset": 80}, {"referenceID": 7, "context": "This method was latter improved by evolutionary algorithms in different ways [7], including the famous EMI project [8].", "startOffset": 115, "endOffset": 118}, {"referenceID": 8, "context": "Latter, statistic model such as Markov chains and Hidden Markov model(HMM) became popular in algorithmic composition [9].", "startOffset": 117, "endOffset": 120}, {"referenceID": 9, "context": "In the meanwhile, neural network(NN) has made remarkable progress in recognition and other field [10], including music composition using Recurrent neural networks(RNN) [11], [12], [13], [14] and Long-Short Term Memory(LSTM) [15], [16].", "startOffset": 168, "endOffset": 172}, {"referenceID": 10, "context": "In the meanwhile, neural network(NN) has made remarkable progress in recognition and other field [10], including music composition using Recurrent neural networks(RNN) [11], [12], [13], [14] and Long-Short Term Memory(LSTM) [15], [16].", "startOffset": 174, "endOffset": 178}, {"referenceID": 11, "context": "In the meanwhile, neural network(NN) has made remarkable progress in recognition and other field [10], including music composition using Recurrent neural networks(RNN) [11], [12], [13], [14] and Long-Short Term Memory(LSTM) [15], [16].", "startOffset": 180, "endOffset": 184}, {"referenceID": 12, "context": "In the meanwhile, neural network(NN) has made remarkable progress in recognition and other field [10], including music composition using Recurrent neural networks(RNN) [11], [12], [13], [14] and Long-Short Term Memory(LSTM) [15], [16].", "startOffset": 186, "endOffset": 190}, {"referenceID": 13, "context": "In the meanwhile, neural network(NN) has made remarkable progress in recognition and other field [10], including music composition using Recurrent neural networks(RNN) [11], [12], [13], [14] and Long-Short Term Memory(LSTM) [15], [16].", "startOffset": 224, "endOffset": 228}, {"referenceID": 14, "context": "In the meanwhile, neural network(NN) has made remarkable progress in recognition and other field [10], including music composition using Recurrent neural networks(RNN) [11], [12], [13], [14] and Long-Short Term Memory(LSTM) [15], [16].", "startOffset": 230, "endOffset": 234}, {"referenceID": 9, "context": "However, the related works either represent music as quantized time series [11], [13], [15], [17], [18], [19], or treat pitches and durations in separate neural networks [20], [21].", "startOffset": 75, "endOffset": 79}, {"referenceID": 11, "context": "However, the related works either represent music as quantized time series [11], [13], [15], [17], [18], [19], or treat pitches and durations in separate neural networks [20], [21].", "startOffset": 81, "endOffset": 85}, {"referenceID": 13, "context": "However, the related works either represent music as quantized time series [11], [13], [15], [17], [18], [19], or treat pitches and durations in separate neural networks [20], [21].", "startOffset": 87, "endOffset": 91}, {"referenceID": 15, "context": "However, the related works either represent music as quantized time series [11], [13], [15], [17], [18], [19], or treat pitches and durations in separate neural networks [20], [21].", "startOffset": 93, "endOffset": 97}, {"referenceID": 16, "context": "However, the related works either represent music as quantized time series [11], [13], [15], [17], [18], [19], or treat pitches and durations in separate neural networks [20], [21].", "startOffset": 99, "endOffset": 103}, {"referenceID": 17, "context": "However, the related works either represent music as quantized time series [11], [13], [15], [17], [18], [19], or treat pitches and durations in separate neural networks [20], [21].", "startOffset": 105, "endOffset": 109}, {"referenceID": 18, "context": "However, the related works either represent music as quantized time series [11], [13], [15], [17], [18], [19], or treat pitches and durations in separate neural networks [20], [21].", "startOffset": 170, "endOffset": 174}, {"referenceID": 19, "context": "However, the related works either represent music as quantized time series [11], [13], [15], [17], [18], [19], or treat pitches and durations in separate neural networks [20], [21].", "startOffset": 176, "endOffset": 180}, {"referenceID": 20, "context": "However, simple RNN does not perform well in long-term dependency because of vanishing gradients [22].", "startOffset": 97, "endOffset": 101}, {"referenceID": 21, "context": "We use Adam [23] to perform gradient descent optimization, learning rate is set to 0.", "startOffset": 12, "endOffset": 16}, {"referenceID": 22, "context": "We build our model on a high-level neural networks library Keras [24] and use TensorF low [25] as its tensor manipulation library.", "startOffset": 65, "endOffset": 69}, {"referenceID": 23, "context": "We build our model on a high-level neural networks library Keras [24] and use TensorF low [25] as its tensor manipulation library.", "startOffset": 90, "endOffset": 94}], "year": 2016, "abstractText": "Creating any aesthetically pleasing piece of art, like music, has been a long time dream for artificial intelligence research. Based on recent success of long-short term memory (LSTM) on sequence learning, we put forward a novel system to reflect the thinking pattern of a musician. For data representation, we propose a note-level encoding method, which enables our model to simulate how human composes and polishes music phrases. To avoid failure against music theory, we invent a novel method, grammar argumented (GA) method. It can teach machine basic composing principles. In this method, we propose three rules as argumented grammars and three metrics for evaluation of machine-made music. Results show that comparing to basic LSTM, grammar argumented model\u2019s compositions have higher contents of diatonic scale notes, short pitch intervals, and chords.", "creator": "LaTeX with hyperref package"}}}