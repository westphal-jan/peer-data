{"id": "1611.05497", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Nov-2016", "title": "Explicable Robot Planning as Minimizing Distance from Expected Behavior", "abstract": "In order for robots to be integrated effectively into human work-flows, it is not enough to address the question of autonomy but also how their actions or plans are being perceived by their human counterparts. When robots generate task plans without such considerations, they may often demonstrate what we refer to as inexplicable behavior from the point of view of humans who may be observing it. This problem arises due to the human observer's partial or inaccurate understanding of the robot's deliberative process and/or the model (i.e. capabilities of the robot) that informs it. This may have serious implications on the human-robot work-space, from increased cognitive load and reduced trust in the robot from the human, to more serious concerns of safety in human-robot interactions. In this paper, we propose to address this issue by learning a distance function that can accurately model the notion of explicability, and develop an anytime search algorithm that can use this measure in its search process to come up with progressively explicable plans. As the first step, robot plans are evaluated by human subjects based on how explicable they perceive the plan to be, and a scoring function called explicability distance based on the different plan distance measures is learned. We then use this explicability distance as a heuristic to guide our search in order to generate explicable robot plans, by minimizing the plan distances between the robot's plan and the human's expected plans. We conduct our experiments in a toy autonomous car domain, and provide empirical evaluations that demonstrate the usefulness of the approach in making the planning process of an autonomous agent conform to human expectations.", "histories": [["v1", "Wed, 16 Nov 2016 23:24:38 GMT  (634kb,D)", "http://arxiv.org/abs/1611.05497v1", "8 pages, 8 figures"]], "COMMENTS": "8 pages, 8 figures", "reviews": [], "SUBJECTS": "cs.AI cs.RO", "authors": ["anagha kulkarni", "tathagata chakraborti", "yantian zha", "satya gautam vadlamudi", "yu zhang", "subbarao kambhampati"], "accepted": false, "id": "1611.05497"}, "pdf": {"name": "1611.05497.pdf", "metadata": {"source": "CRF", "title": "Explicable Robot Planning as Minimizing Distance from Expected Behavior", "authors": ["Anagha Kulkarni", "Tathagata Chakraborti", "Yantian Zha", "Satya Gautam Vadlamudi", "Yu Zhang", "Subbarao Kambhampati"], "emails": ["vsatyagautam@gmail.com"], "sections": [{"heading": null, "text": "These advances have, of course, encouraged the robot's ability to work where autonomous robots and humans can work side by side. However, if the plans generated by autonomous robots are difficult for the human observer to understand, the robot's unexpected behavior can raise several concerns: it can increase the cognitive load that impedes the team's productivity, and the result in safety issues and distrust of the robot. This discrepancy between the robot's plans and human expectations can be explained in terms of the difference in the actual robot model and the human understanding of the robot model."}, {"heading": "II. RELATED WORK", "text": "The idea that robots work alongside humans to perform tasks has been a popular field of research, and it is particularly fascinating because the robot has to look at humans in a loop while making its own decisions, an important prerequisite for this is the ability to infer human intention and plan. Various plan recognition algorithms [6], [7] can be used to perform plan recognition based on certain observations as a result of the agent's interaction with the environment. Once the human intention and plan have been identified, researchers have also discussed how the robot can use this information while avoiding conflict [8], [9] or provide proactive help to humans in the loop [10], [11]. There is also work on simultaneous plan recognition and generation [12]. However, most of the previous work focuses only on how robots can create plans based on derived human intuition."}, {"heading": "III. BACKGROUND", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. Planning", "text": "A classical planning problem can be defined as a tuple P = < M, I, G >, where M = < F, A > is the domain model (which consists of a finite quantity F of liquids defining the state of the world and a series of operators or actions A), and I F and G F are the initial or target states of the problem, respectively. Each action a A is a tuple of the form < pre (a), eff (a), c (a) > where c (a) denotes the cost of an action, pre (a) F is the set of prerequisites for action a, and eff (a) F is the set of effects. The solution of the planning problem is a plan or sequence of actions."}, {"heading": "B. Plan Distance Measures", "text": "These plan distances are action, causal linkage, and sequence of states Distances. Although these distance measurements do not meet certain mathematical properties, they provide a good domain independent measure of the difference between two plans. Since the goal is to predict the differences in the explicability of the distances between the robot plans and the human expected plans, the intuition is that they can capture different aspects of the Plans.1) Action Distance using a combination of plan-distance measurements: We call the amount of unique actions in a plan A (causal linkage) = {a \u00b2 the action variables A (causal linkage) and A (causal linkage) of the action variables. The action variables A (causal R) and A (causal linkage) of the two plans are each the action distances that are calculated as action distances."}, {"heading": "IV. PROPOSED METHODOLOGY", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. Explicability Distance", "text": "Since, without the model, we do not know which plan distance is most relevant for the acquisition of explanability, we present a general formulation in this section. A more detailed formulation can be found in the following section. Let us be a three-dimensional vector, so that we have a vector for a robot plan, \u03c0R, derived from MR (R), and for an explainable plan \u03c0H, derived from MH (R). We now define the explainable distance of a robot plan, Exp (\u03c0R, \u03c0H) > T. As a regression-based function of the three plan distances, we define the explainable distance of a robot plan, Exp (\u03c0R), using b as a parameter vector: Exp (\u03c0R / \u03c0H) \u0432f (\u0445, b) (4) To train our regression model, we use plan tracks whose actions have been assigned by human subjects. Afterwards, we can calculate the explanability of a plan based on the averages of the individual action values."}, {"heading": "B. Plan Generation", "text": "We will now present the details of our plan Generation Phase, where we will now use the explainable distance to conduct our search to generate the most explicit robotic plan for a given problem. 1) Non-monotonicity: We will now discuss the nonmonotonous behavior shown by explainable function and how it affects the planning generation.1 The explainable distance function is completely monotonous in nature, meaning as the subplan grows, the explainable distance can both increase and decrease. This is because a new action can either positively or negatively contribute to the overall explainable score of the plan. As mentioned above, the explainable score is prefix.Observation 1: Explicability score of a subplan P can increase, remain the same or even decrease if it is expanded with one or more measures. Consider the following example, in a car domain, the goal of the car is explicitly to move the left lane."}, {"heading": "V. EXPERIMENTAL ANALYSIS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. Autonomous Car Simulation Experiment", "text": "1) Model: Autonomous cars are a topic of interest from the point of view of explainability issue. < 2) In the recent past, the self-driving cars of Google (19) were seen in the news as \"too safe\" on the roads. These autonomous cars, which are governed by strict traffic rules, find it difficult to dazzle their robot plans to algorithm 1 Reconciliation Search Input: Planning problem P = < MR (R), MR (R), MR (R), MR (R), MR (R), MR (R), S), S (S), S (S), S (S), S (S), S (S), S (S), S (S), S (S), S (S), S, S, S, S, S (S), S, S, S, S, S (S), S, S (S), S, S, S, S (S), S, S, S, S, S (S), S, S (S), S, S, S (S), S, S, S, S, S (S), S, S, S (S). < 2), S (R (R), S (R), S (R), S (R), S (R), S (R), S (R), S (R (R), S (R), S (R), S (R), S (R), S (R, S (R), S (R), S (R), S (R), S (R), S (R), S (R), S (R), S (R), S (R), S (R), S (R), S (R), S (R), S (R), S (R), S (R), S (R), S (R), S (R), S, S (R), S), S (R), S (R (R), S), S, S (R), S, S (R), S (R), S (R (R), S), S"}, {"heading": "VI. CONCLUSION", "text": "We have evaluated our hypothesis in the simulated Autonomous Auto PDDL domain. We have generated training samples in the robotic domain and backed them up with human scores. We have also created plans in the human model to find the distances between the plans in two areas. We have looked at the relationships between the scores and the distance measurements of the plans. We have learned the regression model that would best capture the explanability of the training samples. In summary, we have proven our hypothesis that we can use the mental model of the human robot model to assess the explanability of a robot plan as a function of the plan distance measurements between the robot plan and the plan that humans would expect from the robot. We have also shown that the explanability distance measurement can be used to distort the robot planning process to create plans that are more consistent with what humans expect."}], "references": [{"title": "Adaptive aiding of human-robot teaming effects of imperfect automation on performance, trust, and workload", "author": ["E. de Visser", "R. Parasuraman"], "venue": "Journal of Cognitive Engineering and Decision Making, vol. 5, no. 2, pp. 209\u2013231, 2011.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2011}, {"title": "Pddl-the planning domain definition language", "author": ["D. McDermott", "M. Ghallab", "A. Howe", "C. Knoblock", "A. Ram", "M. Veloso", "D. Weld", "D. Wilkins"], "venue": "1998.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1998}, {"title": "Generating diverse plans to handle unknown and partially known user preferences", "author": ["T.A. Nguyen", "M. Do", "A.E. Gerevini", "I. Serina", "B. Srivastava", "S. Kambhampati"], "venue": "Artificial Intelligence, vol. 190, no. 0, pp. 1 \u2013 31, 2012. [Online]. Available: http://www.sciencedirect.com/science/article/pii/S0004370212000707", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2012}, {"title": "The fast downward planning system", "author": ["M. Helmert"], "venue": "CoRR, vol. abs/1109.6051, 2011. [Online]. Available: http://arxiv.org/abs/1109.6051", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2011}, {"title": "Generalized plan recognition.", "author": ["H.A. Kautz", "J.F. Allen"], "venue": "in AAAI,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1986}, {"title": "Probabilistic plan recognition using offthe-shelf classical planners", "author": ["M. Ram\u0131rez", "H. Geffner"], "venue": "Proceedings of the Conference of the Association for the Advancement of Artificial Intelligence (AAAI 2010). Citeseer, 2010, pp. 1121\u20131126.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2010}, {"title": "Planning with resource conflicts in human-robot cohabitation", "author": ["T. Chakraborti", "Y. Zhang", "D.E. Smith", "S. Kambhampati"], "venue": "Proceedings of the 2016 International Conference on Autonomous Agents & Multiagent Systems. International Foundation for Autonomous Agents and Multiagent Systems, 2016, pp. 1069\u20131077.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2016}, {"title": "Human-aware task planning for mobile robots", "author": ["M. Cirillo", "L. Karlsson", "A. Saffiotti"], "venue": "Advanced Robotics, 2009. ICAR 2009. International Conference on, June 2009, pp. 1\u20137.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2009}, {"title": "Planning for serendipity", "author": ["T. Chakraborti", "G. Briggs", "K. Talamadupula", "Y. Zhang", "M. Scheutz", "D. Smith", "S. Kambhampati"], "venue": "IEEE/RSJ International Conference on Intelligent Robots and Systems, 2015.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "Coordination in human-robot teams using mental modeling and plan recognition", "author": ["K. Talamadupula", "G. Briggs", "T. Chakraborti", "M. Scheutz", "S. Kambhampati"], "venue": "Intelligent Robots and Systems (IROS 2014), 2014 IEEE/RSJ International Conference on, Sept 2014, pp. 2957\u20132962.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2014}, {"title": "Concurrent plan recognition and execution for human-robot teams.", "author": ["S.J. Levine", "B.C. Williams"], "venue": "ICAPS,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2014}, {"title": "Plan explainability and predictability for cobots", "author": ["Y. Zhang", "S. Sreedharan", "A. Kulkarni", "T. Chakraborti", "S.K. Hankz Hankui Zhuo"], "venue": "CoRR, vol. abs/1511.08158, 2015. [Online]. Available: http://arxiv.org/abs/1511.08158", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "Generating legible motion", "author": ["A. Dragan", "S. Srinivasa"], "venue": "Proceedings of Robotics: Science and Systems, Berlin, Germany, June 2013.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2013}, {"title": "A survey of socially interactive robots", "author": ["T.W. Fong", "I. Nourbakhsh", "K. Dautenhahn"], "venue": "Robotics and Autonomous Systems, 2003.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2003}, {"title": "Cost-based anticipatory action selection for human\u2013robot fluency", "author": ["G. Hoffman", "C. Breazeal"], "venue": "Robotics, IEEE Transactions on, vol. 23, no. 5, pp. 952\u2013961, 2007.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2007}, {"title": "Capability models and their applications in planning", "author": ["Y. Zhang", "S. Sreedharan", "S. Kambhampati"], "venue": "Proceedings of the 2015 International Conference on Autonomous Agents and Multiagent Systems. International Foundation for Autonomous Agents and Multiagent Systems, 2015, pp. 1151\u20131159.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2015}, {"title": "Measuring plan diversity: Pathologies in existing approaches and a new plan distance metric.", "author": ["R.P. Goldman", "U. Kuter"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2015}, {"title": "Google\u2019s driverless cars run into problem: Cars with drivers", "author": ["M. Richtel", "C. Dougherty"], "venue": "The New York Times, vol. 9, p. 1, 2015.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "from the robot can raise several concerns: it may increase cognitive load, hamper the productivity of the team, and result in safety concerns and distrust towards the robot [1].", "startOffset": 173, "endOffset": 176}, {"referenceID": 1, "context": "In this paper, we represent both models in PDDL [2], but they can differ in terms of their action representations, preconditions, effects, and costs.", "startOffset": 48, "endOffset": 51}, {"referenceID": 2, "context": "To answer the second question, we explore the relationship between three existing plan distance measures: action set, causal link set and state sequence distances [3], [4] and the plan explicability distance.", "startOffset": 168, "endOffset": 171}, {"referenceID": 3, "context": "To address the third question, we integrate the explicablity distance in the search process of the Fast-Downward planner [5].", "startOffset": 121, "endOffset": 124}, {"referenceID": 4, "context": "Various plan recognition algorithms [6], [7] can be applied to perform", "startOffset": 36, "endOffset": 39}, {"referenceID": 5, "context": "Various plan recognition algorithms [6], [7] can be applied to perform", "startOffset": 41, "endOffset": 44}, {"referenceID": 6, "context": "After the intent and the plan of the human is identified, researchers have also discussed how the robot can utilize this information while avoiding conflicts [8], [9] or providing proactive help to the human in the loop [10], [11].", "startOffset": 158, "endOffset": 161}, {"referenceID": 7, "context": "After the intent and the plan of the human is identified, researchers have also discussed how the robot can utilize this information while avoiding conflicts [8], [9] or providing proactive help to the human in the loop [10], [11].", "startOffset": 163, "endOffset": 166}, {"referenceID": 8, "context": "After the intent and the plan of the human is identified, researchers have also discussed how the robot can utilize this information while avoiding conflicts [8], [9] or providing proactive help to the human in the loop [10], [11].", "startOffset": 220, "endOffset": 224}, {"referenceID": 9, "context": "After the intent and the plan of the human is identified, researchers have also discussed how the robot can utilize this information while avoiding conflicts [8], [9] or providing proactive help to the human in the loop [10], [11].", "startOffset": 226, "endOffset": 230}, {"referenceID": 10, "context": "There is also work on performing simultaneous plan recognition and generation [12].", "startOffset": 78, "endOffset": 82}, {"referenceID": 11, "context": "The motivation for generating explicable task plans was first provided in our recent paper [13].", "startOffset": 91, "endOffset": 95}, {"referenceID": 12, "context": "While explicability focuses on task plans, a related notion of \u201clegibility\u201d has been studied in the context of motion planning [14] and has been shown to be useful in generating socially acceptable behaviors for robots [15], [16].", "startOffset": 127, "endOffset": 131}, {"referenceID": 13, "context": "While explicability focuses on task plans, a related notion of \u201clegibility\u201d has been studied in the context of motion planning [14] and has been shown to be useful in generating socially acceptable behaviors for robots [15], [16].", "startOffset": 219, "endOffset": 223}, {"referenceID": 14, "context": "While explicability focuses on task plans, a related notion of \u201clegibility\u201d has been studied in the context of motion planning [14] and has been shown to be useful in generating socially acceptable behaviors for robots [15], [16].", "startOffset": 225, "endOffset": 229}, {"referenceID": 15, "context": "There also exists learnable models that do not assume completeness in the first place [17].", "startOffset": 86, "endOffset": 90}, {"referenceID": 11, "context": "Another note is that in [13], [14] and this work, since the model is one level deeper, which is about the robot model from the humans perspective, learning methods are adopted.", "startOffset": 24, "endOffset": 28}, {"referenceID": 12, "context": "Another note is that in [13], [14] and this work, since the model is one level deeper, which is about the robot model from the humans perspective, learning methods are adopted.", "startOffset": 30, "endOffset": 34}, {"referenceID": 2, "context": "We now look at the three plan distance measures introduced in [3] and later refined in [4].", "startOffset": 87, "endOffset": 90}, {"referenceID": 16, "context": "Although these distance metrics do not satisfy certain mathematical properties [18], they provide a good domain independent measure of the difference between any two plans.", "startOffset": 79, "endOffset": 83}, {"referenceID": 2, "context": "Given the action sets A(\u03c0) and A(\u03c0) of two plans \u03c0 and \u03c0 respectively, the action distance, \u03b4a, is computed as the ratio of the actions that are exclusive to each plan to all the actions in the plans [4].", "startOffset": 200, "endOffset": 203}, {"referenceID": 2, "context": "The length of the sequences may differ and therefore there are multiple ways to define this distance measure [4].", "startOffset": 109, "endOffset": 112}, {"referenceID": 3, "context": "We implement this search in the Fast-Downward planner [5].", "startOffset": 54, "endOffset": 57}, {"referenceID": 17, "context": "In the recent past, Google\u2019s self-driving cars [19] have been in the news for being \u201ctoo safe\u201d on the roads.", "startOffset": 47, "endOffset": 51}], "year": 2016, "abstractText": "In order for robots to be integrated effectively into human work-flows, it is not enough to address the question of autonomy but also how their actions or plans are being perceived by their human counterparts. When robots generate task plans without such considerations, they may often demonstrate what we refer to as inexplicable behavior from the point of view of humans who may be observing it. This problem arises due to the human observer\u2019s partial or inaccurate understanding of the robot\u2019s deliberative process and/or the model (i.e. capabilities of the robot) that informs it. This may have serious implications on the human-robot work-space, from increased cognitive load and reduced trust in the robot from the human, to more serious concerns of safety in human-robot interactions. In this paper, we propose to address this issue by learning a distance function that can accurately model the notion of explicability, and develop an anytime search algorithm that can use this measure in its search process to come up with progressively explicable plans. As the first step, robot plans are evaluated by human subjects based on how explicable they perceive the plan to be, and a scoring function called explicability distance based on the different plan distance measures is learned. We then use this explicability distance as a heuristic to guide our search in order to generate explicable robot plans, by minimizing the plan distances between the robot\u2019s plan and the human\u2019s expected plans. We conduct our experiments in a toy autonomous car domain, and provide empirical evaluations that demonstrate the usefulness of the approach in making the planning process of an autonomous agent conform to human expectations.", "creator": "TeX"}}}