{"id": "1611.05521", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Nov-2016", "title": "Robust Hashing for Multi-View Data: Jointly Learning Low-Rank Kernelized Similarity Consensus and Hash Functions", "abstract": "Learning hash functions/codes for similarity search over multi-view data is attracting increasing attention, where similar hash codes are assigned to the data objects characterizing consistently neighborhood relationship across views. Traditional methods in this category inherently suffer three limitations: 1) they commonly adopt a two-stage scheme where similarity matrix is first constructed, followed by a subsequent hash function learning; 2) these methods are commonly developed on the assumption that data samples with multiple representations are noise-free,which is not practical in real-life applications; 3) they often incur cumbersome training model caused by the neighborhood graph construction using all $N$ points in the database ($O(N)$). In this paper, we motivate the problem of jointly and efficiently training the robust hash functions over data objects with multi-feature representations which may be noise corrupted. To achieve both the robustness and training efficiency, we propose an approach to effectively and efficiently learning low-rank kernelized \\footnote{We use kernelized similarity rather than kernel, as it is not a squared symmetric matrix for data-landmark affinity matrix.} hash functions shared across views. Specifically, we utilize landmark graphs to construct tractable similarity matrices in multi-views to automatically discover neighborhood structure in the data. To learn robust hash functions, a latent low-rank kernel function is used to construct hash functions in order to accommodate linearly inseparable data. In particular, a latent kernelized similarity matrix is recovered by rank minimization on multiple kernel-based similarity matrices. Extensive experiments on real-world multi-view datasets validate the efficacy of our method in the presence of error corruptions.", "histories": [["v1", "Thu, 17 Nov 2016 01:21:26 GMT  (262kb,D)", "http://arxiv.org/abs/1611.05521v1", "Accepted to appear in Image and Vision Computing"]], "COMMENTS": "Accepted to appear in Image and Vision Computing", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["lin wu", "yang wang"], "accepted": false, "id": "1611.05521"}, "pdf": {"name": "1611.05521.pdf", "metadata": {"source": "CRF", "title": "Robust Hashing for Multi-View Data: Jointly Learning Low-Rank Kernelized Similarity Consensus and Hash Functions", "authors": ["Lin Wu", "Yang Wang"], "emails": ["lin.wu@adelaide.edu.au", "wangy@cse.unsw.edu.au"], "sections": [{"heading": null, "text": "Learning hash functions / codes to search for similarities across multiple views is attracting increasing attention, with similar hash codes assigned to data objects that characterize the consistent relationship between views. Traditional methods in this category are inherently subject to three limitations: 1) They typically use a two-step scheme in which a similarity matrix is constructed first, followed by hash function learning; 2) These methods are usually developed on the assumption that data samples with multiple representations are noise-free, which is not practicable in real-world applications; 3) they often have a cumbersome training model caused by the construction of neighborhood graphs, using all the N points in the database (O (N). In this essay, we encourage the problem of collectively and efficiently forming the robust hash functions over data objects with multifunction representations that are corrupted by noise, to achieve both the efficiency of training and the robustness we can suggest."}, {"heading": "1 Introduction", "text": "Hashing is dramatically efficient for finding similarity using low-dimensional binary codes with low storage costs. Intensive hashing methods applicable to individual data sources have been suggested that can be classified into data-independent hashing, such as local sensitive hashing (LSH) [Datar et al., 2004] and data-dependent hashing or learning-based hashing [Wei\u00df et al., 2008; Wang et al., 2010]. In real-world situations, data objects can be broken down into multi-view areas where each view can characterize its individual property, e.g. an image can be described by color histograms and textures, and the two features prove to be complementary [Wang et al., 2014; Wang et al., 2013; Wang et al., 2015c; Wu et al., 2013; Wu et al., 2016; Wang et al., 2016; Wang et al., 2015d; Wang et al."}, {"heading": "1.1 Motivation", "text": "Despite improved performance through existing multiview hashing methods [Zhang et al., 2011; Kim et al., 2012; Masci et al., 2014; Song et al., 2011; Liu et al., 2012b; Shen et al., 2015], some basic limitations can be identified: \u2022 The learning process is carried out through a two-step mechanism in which hash functions are learned based on a preconstructed data similarity matrix. Their methods usually assume that data samples using multiple views are noise-free, while in real applications data objects may be noisy (e.g. missing values in pixels), resulting in corresponding similarity matrices being corrupted by significant noise [Wang et al., 2015d; Xia et al., 2014]. Furthermore, the restoration of consen-ar Xiv et al: 161 1.05 521v 1 [cs.L G] 17 Nov 2sus or required similarity values in terms of views."}, {"heading": "1.2 Our Method", "text": "In this year we have it in the hand in which we are, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand, in the hand,"}, {"heading": "1.3 Contributions", "text": "The main contributions of this paper are threefold. \u2022 We motivate the problem of robust hash over multiview data with nonlinear data distribution and propose to learn the robust hash functions and a low-level kernel similarity matrix shared by views. \u2022 An iterative method is proposed to optimize low-level restoration to learn the robust hash functions. For efficiency reasons, the neighborhood graph is approximated by using boundary curves with a sparse link between data points. \u2022 Extensive experiments performed on real-world multiview data sets confirm the effectiveness of our method in error corruptions for multiview feature representations."}, {"heading": "2 Related Work", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Multi-view Learning based Hashing", "text": "Some recent representative work includes Multiple Feature Hashing (MFH) [Song et al., 2011], Composite Hashing with Multiple Sources (CHMS) [Zhang et al., 2011], Compact Kernel Hashing with Multiple Features (CKH) [Liu et al., 2012b] and Multi-view Sequential Spectral Hashing (SSH) [Kim et al., 2012]. However, these methods often have drawbacks in that they typically use spectral diagram techniques (e.g. k-NN diagram) to model similarities between data points. In general, the complexity of constructing the similarity matrix isO (N2) for N data points that is not pragmatic in large-scale applications. In addition, the similarity matrix produced by constructing diagrams is very sensitive to interference. To avoid constructing similarity matrices, we are separated from each other by multiple spaces and uniformats."}, {"heading": "2.2 Low-rank Modeling", "text": "Due to its ability to restore the underlying structure among data objects [Wright et al., 2009a; Candes and Recht, 2009; Li et al., 2015; Zhang et al., 2016; Zhang et al., 2014; Zhang et al., 2015], it has achieved astonishing success in many applications such as data compression [Wright et al., 2009a], subspace clustering [Liu et al., 2013; Deng et al., 2013; Zhang et al., 2014] and image processing [Zhou et al., 2013; Zhang et al., 2016; Zhang et al., 2015]. Thus, Zhang et al. consider a common formulation of restoration of low-rank and sparse subspace structures for robust imaging."}, {"heading": "3 Robust Multi-view Hashing", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Preliminary and Problem Definition", "text": "Unless otherwise specified, it is an unforeseen, unforeseen, unforeseen, unforeseen, unforeseen, unforeseen, unforeseen, unforeseen, unforeseen, unforeseen, unforeseen, unforeseen, unforeseen, unforeseen, unforeseen, unforeseen, unforeseen, unforeseen, unforeseen, unforeseen, unforeseen, unforeseen, unforeseen, unforeseen, unforeseen, unforeseen, unforeseen, unforeseen."}, {"heading": "3.2 Low-rank Kernelized Similarity Recovery from Multi-views", "text": "Faced with a collection of high-dimensional multiview data samples, which may contain specific errors for each visual representation, we construct several nonlinear feature spaces K (m = 1,.., M), each representing a feature view. To use multiple complementary representations, we propose to derive a consensus matrix K (m = 1,..., M) from corrupt data objects, which is restored by corrupt data objects and shared across views. This low-level nonlinear similarity matrix is considered the most necessary component, while each view also contains individual non-required information, including redundancy and errors. We explicitly model redundancy through thrift, as the Multiview study suggests that every single view is sufficient to identify most similarity structures, and the discrepancy between required component and data sample is sparse [Kumar et al., 2011] may become a reality due to a communication error or sensor error."}, {"heading": "3.3 Objective Function", "text": "Many studies [Weiss et al., 2008; Song et al., 2011] have shown the advantages arising from the use of the local structure of education. (...) On the other hand, we propose to learn together by preserving local similarities in different points of view, while exploiting the local structure in each point of view. (...) To define the local structure in each point of view, we define M affinity matrices S (m). (...) M), one for each point of view, that is, S (...) ij (...) ij (...) j (...) j (...) j (...) j (...) j (...) i) i) i) else.where Nk (...) is the k-next neighbor set, and the Euclidean distance is used in each feature to be."}, {"heading": "4 Optimization", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Compute W and b", "text": "Fixed with other variables, and determining the derivative problem of Eq. (8) w.r.t. (Q = = Q = = Q = = 0, we get (K = 0, K = 0, K = 0, K = 0, K = 0. (9) Determining the derivative problem of Eq. (8) w.r.t. (K = 0, K + 0, K + 0, K = 0, K = 0, K = 0, K = 0, K = 0. (9) in Eq. (10), we have K (K = 1, K + 0, K = 0, K = 0, K = 0, K = 0, K = 0, K = 0, K = 0, K = 0, K = 0, K = 0, K = 0, K = 0, K, K, K, K, K, K, K, K, K, K, K, K"}, {"heading": "K\u0302 = arg min", "text": "M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-M-"}, {"heading": "K\u0302 = arg min", "text": "K-1 2 | | K-C | | 2F = arg minK-1,..., K-N = 12 N-1 | | K-i-Ci | | 22.s.t.K-0, K-i (i = 1,..., N) \u2265 0. (18) Therefore, the problem can be divided into equivalent (18) into N independent sub-problems: minK-i 1 2 | | K-i-Ci | | 2 2 2 2, subject to K-i \u2265 0. Each sub-problem is a proximal operator problem that can be efficiently solved by the projection algorithm in [Duchi et al., 2008]."}, {"heading": "4.3 Learning Hash Codes", "text": "Once the hash function implemented by W and b has been learned by exploiting the kernelized similarity consensus K \u0442, we can generate hash codes for both database samples and query samples called xt via Equation (19).yt = characters (W T [K (xt, Z1),.., K (xt, ZR)] T + b), (19), where W-RR \u00d7 P, K (xt, Zi) represents the similarity between xt and the i-th landmark by using the Gaussian RBF kernel over the concatenated feature space for all views."}, {"heading": "4.4 Out-of-Sample Extension", "text": "An essential component of hashing is the generation of binary code for new samples, known as q = q = source issues. A common solution is the Nystro-m extension [Bengio et al., 2004]. However, this is not practical for large-scale hashing, since the Nystro-m extension is as expensive as the complete search for the nearest neighbors with a complexity of O (N) for N data points. To solve the problem of expansion outside the sample, we apply a non-parametric regression approach inspired by Shen et al. [Shen et al., 2013]. Specifically, hashing embeds means Y = {y1, y1,."}, {"heading": "5 Complexity Analysis", "text": "We analyze the time complexity with respect to the iteration of the optimization strategy. The complexity of the calculation K-LcK-T and (K-LcK-T + \u03b4I) -1 in Equation (11) is O (R2N) and O (R3), respectively. Usually, R (N) limits are generated offline by scalable K mean clusters for less than 50 iterations, while the complexity of the calculation W O (R2N) + O (R3) remains. The complexity of the calculation of hash codes for a new sample is O (dZN). Overall, the time complexity is O (dZN + R3 + R2N) \u2248 O (dZN + R2N) in an iteration that is linear with respect to the training quantity."}, {"heading": "6 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6.1 Experimental Settings", "text": "We compare our method with recently proposed state-of-the-art multiple feature hashing algorithms: \u2022 Multiple feature hashing (MFH) [Song et al., 2011]: This method uses the local structure in each feature and global consistency in the optimization of hashing functions. \u2022 Composite kernel hashing with multiple features (CHMS) [Zhang et al., 2011]: This method treats a linear combination of view-specific similarities as an average similarity that can be inserted into a spectral hashing framework. \u2022 Compact kernel hashing with multiple features (CKH) [Liu et al., 2012b]: It is a multiple feature hashing framework that combines several cores with each other. \u2022 Sequential spectral hashing with multiple representations (SSH et al., 2012]: This method constructs an average similarity for the compilation of view-specific similarities."}, {"heading": "6.2 Results", "text": "We report on the mean precision recall curves of Hamming Ranking, and the mean precision (MAP) W.r.t. different number of hashing bits over 1K query images. The results are shown in Fig.1, which can be calculated from top 100 retrieved samples. It can be seen from the top sub-figure of Fig.1 that our method achieves an increase in precision and retrieval over all counterparts and the second best is MVLH. This can demonstrate the superiority of using non-linear hashing functions in nonlinear space. More importantly, the latent consensus that the similarity of the matrix achieved by low minimization is not only effective in using complementary information from multiple views, but also robust against the presence of errors. The subfigure (below) in Figure 1."}, {"heading": "6.3 Parameter Tuning", "text": "In this experiment we test different parameter settings for our algorithm to study the power sensitivity. We learn three parameters: \u03b1, \u03bb and \u03b2, according to the concept of the required component, the unnecessary decomposition and learning hash functions in Equation (8). For these parameters we adjust them from {10 \u2212 5, 10 \u2212 3, 10 \u2212 1, 100, 101, 102, 103}. We fix one of the parameters in \u03b1, \u03bb and \u03b2 to report the MAP while the other two parameters change. The results are shown in Fig.3 (a) by fixing \u03bb = 10 \u2212 3, we show the power variance on different pairs of \u03b1 and \u03b2. We can observe that our algorithms achieve a relatively higher MAP when \u03b1 = 0.1, and \u03b2 = 100. The similar performance can also be seen from Fig.3 (b) and Fig.3 (c)."}, {"heading": "6.4 Out-of-Sample Case", "text": "The MINIST dataset [LeCun et al., 1998] consists of 70K images, each of 784 dimensions, from handwritten digits from \"0\" to \"9.\" As in Fig.5, our method achieves the best results. From this dataset, we can clearly see that our method far exceeds MVLH, which increases with increasing code length. This also shows the advantage of low-level kernel embedding as a tool for hashing by embedding high-dimensional data in a low-dimensional space. This method of reducing dimensionality not only preserves the local neighborhood, but also reveals a global structure."}, {"heading": "7 Conclusion", "text": "In this paper, we will motivate the problem of robust hashing for similarity search over multiview data objects in a practical scenario in which error corruptions for visual feature representations are presented. In contrast to existing multi-view hashing methods, which provide a two-phase scheme of constructing similarity matrices and separately learning hash functions, we propose a novel technique to jointly learn hash functions and a latent, corruption-free kernel-based similarity among multiple representations with potential noise disturbances. Extensive experiments with real-world multiview data sets demonstrate the superiority of our method in terms of their effectiveness."}], "references": [{"title": "Neural Comput", "author": ["Yoshua Bengio", "Olivier Delalleau", "Nicolas Le Roux", "Jean-Franois Paiement", "Pascal Vincent", "Marie Ouimet. Learning eigenfunctions links spectral embedding", "kernel pca"], "venue": "16(10):2197\u2013 2219,", "citeRegEx": "Bengio et al.. 2004", "shortCiteRegEx": null, "year": 2004}, {"title": "A singular value thresholding algorithm", "author": ["Cai et al", "2010] Jian-Feng Cai", "Emmanuel J. Cands", "Zuowei Shen"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q2010\\E", "shortCiteRegEx": "al. et al\\.", "year": 2010}, {"title": "Foundations of Computational Mathmatics", "author": ["Emmanuel J. Candes", "Benjamin Recht. Exact matrix completion via convex optimization"], "venue": "9(6):717\u2013772,", "citeRegEx": "Candes and Recht. 2009", "shortCiteRegEx": null, "year": 2009}, {"title": "Nus-wide: a real-world web image database from national university of singapore", "author": ["Tat-Seng Chua", "Jinhui Tang", "Richang Hong", "Haojie Li", "Zhiping Luo"], "venue": "ACM CIVR,", "citeRegEx": "Chua et al.. 2009", "shortCiteRegEx": null, "year": 2009}, {"title": "Locality-sensitive hashing scheme based on p-stable distribution", "author": ["Mayur Datar", "Nicole Immorlica", "Piotr Indyk", "Vahab S. Mirrokni"], "venue": "SOCG,", "citeRegEx": "Datar et al.. 2004", "shortCiteRegEx": null, "year": 2004}, {"title": "IEEE Transactions on Neural Networks and Learing Systems", "author": ["Yue Deng", "Qionghai Dai", "Risheng Liu", "Zengke Zhang", "Sanqing Hu. Low-rank structure learning via nonconvex heuristic recovery"], "venue": "24(3):383\u2013396,", "citeRegEx": "Deng et al.. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "Efficient projections onto the `1-ball for learning in high dimensions", "author": ["John Duchi", "Shai Shalev-Shwartz", "Yoram Singer", "Tushar Chandra"], "venue": "ICML,", "citeRegEx": "Duchi et al.. 2008", "shortCiteRegEx": null, "year": 2008}, {"title": "In ECCV", "author": ["Saehoon Kim", "Yoonseop Kang", "Seungjin Choi. Sequential spectral learning to hash with multiple representations"], "venue": "pages 538\u2013551,", "citeRegEx": "Kim et al.. 2012", "shortCiteRegEx": null, "year": 2012}, {"title": "Kernelized locality-sensitive hashing for scalable image search", "author": ["Brian Kulis", "Kristen Grauman"], "venue": "ICCV,", "citeRegEx": "Kulis and Grauman. 2009", "shortCiteRegEx": null, "year": 2009}, {"title": "A co-training approach for multi-view spectral clustering", "author": ["Abhishek Kumar", "Hal Daume III"], "venue": "ICML,", "citeRegEx": "Kumar and III. 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "In IJCAI", "author": ["Shaishav Kumar", "Raghavendra Udupa. Learning hash functions for cross-view similarity search"], "venue": "pages 1360\u20131365,", "citeRegEx": "Kumar and Udupa. 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "Co-regularized multi-view spectral clustering", "author": ["Abhishek Kumar", "Piyush Rai", "Hal Daum"], "venue": "NIPS,", "citeRegEx": "Kumar et al.. 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "Gradient-based learning applied to document recognition", "author": ["Yann LeCun", "Leon Bottou", "Yoshua Bengio", "Patrick Haaffner"], "venue": "Proceedings of IEEE,", "citeRegEx": "LeCun et al.. 1998", "shortCiteRegEx": null, "year": 1998}, {"title": "In SIAM Data Mining", "author": ["Sheng Li", "Ming Shao", "Yun Fu. Multiview low-rank analysis for outlier detection"], "venue": "pages 748\u2013756,", "citeRegEx": "Li et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "The augmented lagrange multiplier method for exact recovery of corrupted low-rank matrices", "author": ["Zhouchen Lin", "Minming Chen", "Yi Ma"], "venue": "arXiv:1009.5055,", "citeRegEx": "Lin et al.. 2010", "shortCiteRegEx": null, "year": 2010}, {"title": "Machine Learning", "author": ["Zhouchen Lin", "Risheng Liu", "Huan Li. Linearized alternating direction method with parallel splitting", "adaptive penalty for separable convex programs in machine learning"], "venue": "(2):287\u2013325,", "citeRegEx": "Lin et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Robust subspace segmentation by low-rank representation", "author": ["Guangcai Liu", "Zhuochen Lin", "Yong Yu"], "venue": "ICML,", "citeRegEx": "Liu et al.. 2010a", "shortCiteRegEx": null, "year": 2010}, {"title": "Large graph construction for scalable semi-supervised learning", "author": ["Wei Liu", "Jun Wang", "Shih-Fu Chang"], "venue": "ICML,", "citeRegEx": "Liu et al.. 2010b", "shortCiteRegEx": null, "year": 2010}, {"title": "Hashing with graphs", "author": ["Wei Liu", "Jun Wang", "Sanjiv Kumar", "Shih-Fu Chang"], "venue": "ICML,", "citeRegEx": "Liu et al.. 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "In CVPR", "author": ["Wei Liu", "Jun Wang", "Rongrong Ji", "Yugang Jiang", "Shih-Fu Chang. Supervised hashing with kernels"], "venue": "pages 2074 \u2013 2081,", "citeRegEx": "Liu et al.. 2012a", "shortCiteRegEx": null, "year": 2012}, {"title": "In ACM Multimedia", "author": ["Xianglong Liu", "Junfeng He", "Di Liu", "Bo Lang. Compact kernel hashing with multiple features"], "venue": "pages 881\u2013884,", "citeRegEx": "Liu et al.. 2012b", "shortCiteRegEx": null, "year": 2012}, {"title": "Mach", "author": ["Guangcan Liu", "Zhuochen Lin", "Shuicheng Yan", "Ju Sun", "Yong Yu", "Yi Ma. Robust recovery of subspace structures by low-rank representation. IEEE Trans. Pattern Anal"], "venue": "Intell., 35(1):171\u2013184,", "citeRegEx": "Liu et al.. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "IJCV", "author": ["David Lowe. Distinctive image features from scale-invariant keypoints"], "venue": "60:91\u2013110,", "citeRegEx": "Lowe. 2004", "shortCiteRegEx": null, "year": 2004}, {"title": "IEEE TPAMI", "author": ["Jonathan Masci", "Michael M. Bronstein", "Alexander M. Bronstein", "Jurgen Schmidhuber. Multimodal similarity-preserving hashing"], "venue": "36(4):824\u2013830,", "citeRegEx": "Masci et al.. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Modeling the shape of the scene: a holistic representation of the spatial envelope", "author": ["Aude Oliva", "Antonio Torralba"], "venue": "IJCV, 42(3):145\u2013175,", "citeRegEx": "Oliva and Torralba. 2001", "shortCiteRegEx": null, "year": 2001}, {"title": "Comparing apples to oranges: a scalable solution with heterogeneous hashing", "author": ["Mingdong Ou", "Peng Cui", "Fei Wang", "Jun Wang", "Wenwu Zhu", "Shiqiang Yang"], "venue": "ACM SIGKDD, pages 230\u2013238,", "citeRegEx": "Ou et al.. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "In CVPR", "author": ["Fumin Shen", "Chunhua Shen", "Qinfeng Shi", "Anton van den Hengel", "Zhenmin Tang. Inductive hashing on manifolds"], "venue": "pages 1562 \u2013 1569,", "citeRegEx": "Shen et al.. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "In ACM Multimedia", "author": ["Xiaobo Shen", "Fumin Shen", "Quan-Sen Sun", "Yun-Hao Yuan. Multi-view latent hashing for efficient multimedia search"], "venue": "pages 831\u2013834,", "citeRegEx": "Shen et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "In ACM Multimedia", "author": ["Jingkuan Song", "Yi Yang", "Zi Huang", "Heng-Tao Shen", "Richang Hong. Multiple feature hashing for real-time large scale near-duplicate video retrieval"], "venue": "pages 423\u2013432,", "citeRegEx": "Song et al.. 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "In CVPR", "author": ["Jun Wang", "Sanjiv Kumar", "Shih-Fu Chang. Semi-supervised hashing for scalable image retrieval"], "venue": "pages 3424 \u2013 3431,", "citeRegEx": "Wang et al.. 2010", "shortCiteRegEx": null, "year": 2010}, {"title": "Towards metric fusion on multi-view data: a crossview based graph random walk approach", "author": ["Yang Wang", "Xuemin Lin", "Qing Zhang"], "venue": "ACM CIKM, pages 805\u2013810,", "citeRegEx": "Wang et al.. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "Exploiting correlation consensus: Towards subspace clustering for multi-modal data", "author": ["Yang Wang", "Xuemin Lin", "Lin Wu", "Wenjie Zhang", "Qing Zhang"], "venue": "ACM Multimedia, pages 981\u2013984,", "citeRegEx": "Wang et al.. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "In IJCAI", "author": ["Qifan Wang", "Luo Si", "Bin Shen. Learning to hash on partial multi-modal data"], "venue": "pages 3904\u20133910,", "citeRegEx": "Wang et al.. 2015a", "shortCiteRegEx": null, "year": 2015}, {"title": "Effective multi-query expansions: Robust landmark retrieval", "author": ["Yang Wang", "Xuemin Lin", "Lin Wu", "Wenjie Zhang"], "venue": "ACM Multimedia, pages 79\u201388,", "citeRegEx": "Wang et al.. 2015b", "shortCiteRegEx": null, "year": 2015}, {"title": "Lbmch: Learning bridging mapping for cross-modal hashing", "author": ["Yang Wang", "Xuemin Lin", "Lin Wu", "Wenjie Zhang", "Qing Zhang"], "venue": "ACM SIGIR,", "citeRegEx": "Wang et al.. 2015c", "shortCiteRegEx": null, "year": 2015}, {"title": "IEEE Transactions on Image Processing", "author": ["Yang Wang", "Xuemin Lin", "Lin Wu", "Wenjie Zhang", "Qing Zhang", "Xiaodi Huang. Robust subspace clustering for multi-view data by exploiting correlation consensus"], "venue": "24(11):3939\u20133949,", "citeRegEx": "Wang et al.. 2015d", "shortCiteRegEx": null, "year": 2015}, {"title": "IEEE Transactions on Neural Networks and Learning System", "author": ["Yang Wang", "Wenjie Zhang", "Lin Wu", "Xuemin Lin", "Xiang Zhao. Unsupervised metric fusion over multiview data by graph random walk-based crossview diffusion"], "venue": "99:1\u201314,", "citeRegEx": "Wang et al.. 2015e", "shortCiteRegEx": null, "year": 2015}, {"title": "Knowledge and Information Systems", "author": ["Yang Wang", "Xuemin Lin", "Lin Wu", "Qing Zhang", "Wenjie Zhang. Shifting multi-hypergraphs via collaborative probabilistic voting"], "venue": "46(3):515\u2013536,", "citeRegEx": "Wang et al.. 2016a", "shortCiteRegEx": null, "year": 2016}, {"title": "Iterative views agreement: An iterative low-rank based structured optimization method to multi-view spectral clustering", "author": ["Yang Wang", "Wenjie Zhang", "Lin Wu", "Xuemin Lin", "Meng Fang", "Shirui Pan"], "venue": "IJCAI,", "citeRegEx": "Wang et al.. 2016b", "shortCiteRegEx": null, "year": 2016}, {"title": "In ACM SIGKDD", "author": ["Ying Wei", "Yangqiu Song", "Yi Zhen", "Bo Liu", "Qiang Yang. Scalable heterogeneous translated hashing"], "venue": "pages 791\u2013800,", "citeRegEx": "Wei et al.. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Spectral hashing", "author": ["Yair Weiss", "Antonio Torralba", "Rob Fergus"], "venue": "NIPS,", "citeRegEx": "Weiss et al.. 2008", "shortCiteRegEx": null, "year": 2008}, {"title": "Robust principal component analysis: Exact recovery of corrupted low-rank matrices by convex optimization", "author": ["John Wright", "Yigang Peng", "Yi Ma", "Arvind Ganesh", "Shankar Rao"], "venue": "NIPS,", "citeRegEx": "Wright et al.. 2009a", "shortCiteRegEx": null, "year": 2009}, {"title": "Robust principal component analysis: exact recovery of corrupted low-rank matrices via convex optimization", "author": ["John Wright", "Yigang Peng", "Yi Ma", "Arvind Ganesh", "Shankar Rao"], "venue": "NIPS,", "citeRegEx": "Wright et al.. 2009b", "shortCiteRegEx": null, "year": 2009}, {"title": "Efficient image and tag co-ranking: a bregman divergence optimization method", "author": ["Lin Wu", "Yang Wang", "John Shepherd"], "venue": "ACM Multimedia,", "citeRegEx": "Wu et al.. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "Exploiting attribute correlations: A novel trace lasso-based weakly supervised dictionary learning method", "author": ["Lin Wu", "Yang Wang", "Shirui Pan"], "venue": "IEEE Transactions on Cybernetics,", "citeRegEx": "Wu et al.. 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "In AAAI", "author": ["Rongkai Xia", "Yan Pan", "Lei Du", "Jian Yin. Robust multi-view spectral clustering via low-rank", "sparse decomposition"], "venue": "pages 2149\u20132155,", "citeRegEx": "Xia et al.. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "A survey on multi-view learning", "author": ["Chang Xu", "Dacheng Tao", "Chao Xu"], "venue": "arXiv:1304.5634,", "citeRegEx": "Xu et al.. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "In CVPR", "author": ["Guangnan Ye", "Dong Liu", "I-Hong Jhuo", "Shih-Fu Chang. Robust late fusion with rank minimization"], "venue": "pages 3021\u20133028,", "citeRegEx": "Ye et al.. 2012", "shortCiteRegEx": null, "year": 2012}, {"title": "Self-tuning spectral clustering", "author": ["Lihi Zelnik-Manor", "Pietro Perona"], "venue": "NIPS,", "citeRegEx": "Zelnik.Manor and Perona. 2004", "shortCiteRegEx": null, "year": 2004}, {"title": "In ACM SIGIR", "author": ["Dan Zhang", "Fei Wang", "Luo Si. Composite hashing with multiple information sources"], "venue": "pages 225\u2013234,", "citeRegEx": "Zhang et al.. 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "Neural Networks", "author": ["Zhao Zhang", "Shuicheng Yan", "Mingbo Zhao. Similarity preserving low-rank representation for enhanced data representation", "effective subspace learning"], "venue": "53:81\u201394,", "citeRegEx": "Zhang et al.. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Knowledge-Based Systems", "author": ["Zhao Zhang", "Shuicheng Yan", "Mingbo Zhao", "Fanzhang Li. Bilinear low-rank coding framework", "extension for robust image recovery", "feature representation"], "venue": "86:143\u2013157,", "citeRegEx": "Zhang et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "IEEE Transactions on Image Processing", "author": ["Zhao Zhang", "Fanzhang Li", "Mingbo Zhao", "Li Zhang", "Shuicheng Yan. Joint low-rank", "sparse principal feature coding for enhanced robust repersentation", "visual classification"], "venue": "25(6):2429\u20132443,", "citeRegEx": "Zhang et al.. 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "In AAAI", "author": ["Shuai Zheng", "Xiao Cai", "Chris Ding", "Feiping Nie", "Heng Huang. A closed form solution to multi-view low-rank regression"], "venue": "pages 1973\u2013 1979,", "citeRegEx": "Zheng et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "author": ["Xiaowei Zhou", "Can Yang", "Weichuan Yu. Moving object detection by detecting contiguous outliers in the low-rank representation"], "venue": "35(3):597\u2013 610,", "citeRegEx": "Zhou et al.. 2013", "shortCiteRegEx": null, "year": 2013}], "referenceMentions": [{"referenceID": 4, "context": "Intensive hashing methods valid on single data source have been proposed which can be classified into data-independent hashing such as locality sensitive hashing (LSH) [Datar et al., 2004] and data-dependent hashing or learning based hashing [Weiss et al.", "startOffset": 168, "endOffset": 188}, {"referenceID": 40, "context": ", 2004] and data-dependent hashing or learning based hashing [Weiss et al., 2008; Wang et al., 2010].", "startOffset": 61, "endOffset": 100}, {"referenceID": 29, "context": ", 2004] and data-dependent hashing or learning based hashing [Weiss et al., 2008; Wang et al., 2010].", "startOffset": 61, "endOffset": 100}, {"referenceID": 31, "context": ", an image can be described by color histograms and textures, and the two features turn out to be complementary to each other [Wang et al., 2014; Wang et al., 2013; Wang et al., 2015c; Wu et al., 2013; Wu et al., 2016; Wang et al., 2016b; Wang et al., 2015d; Wang et al., 2015e; Wang et al., 2016a].", "startOffset": 126, "endOffset": 298}, {"referenceID": 30, "context": ", an image can be described by color histograms and textures, and the two features turn out to be complementary to each other [Wang et al., 2014; Wang et al., 2013; Wang et al., 2015c; Wu et al., 2013; Wu et al., 2016; Wang et al., 2016b; Wang et al., 2015d; Wang et al., 2015e; Wang et al., 2016a].", "startOffset": 126, "endOffset": 298}, {"referenceID": 34, "context": ", an image can be described by color histograms and textures, and the two features turn out to be complementary to each other [Wang et al., 2014; Wang et al., 2013; Wang et al., 2015c; Wu et al., 2013; Wu et al., 2016; Wang et al., 2016b; Wang et al., 2015d; Wang et al., 2015e; Wang et al., 2016a].", "startOffset": 126, "endOffset": 298}, {"referenceID": 43, "context": ", an image can be described by color histograms and textures, and the two features turn out to be complementary to each other [Wang et al., 2014; Wang et al., 2013; Wang et al., 2015c; Wu et al., 2013; Wu et al., 2016; Wang et al., 2016b; Wang et al., 2015d; Wang et al., 2015e; Wang et al., 2016a].", "startOffset": 126, "endOffset": 298}, {"referenceID": 44, "context": ", an image can be described by color histograms and textures, and the two features turn out to be complementary to each other [Wang et al., 2014; Wang et al., 2013; Wang et al., 2015c; Wu et al., 2013; Wu et al., 2016; Wang et al., 2016b; Wang et al., 2015d; Wang et al., 2015e; Wang et al., 2016a].", "startOffset": 126, "endOffset": 298}, {"referenceID": 38, "context": ", an image can be described by color histograms and textures, and the two features turn out to be complementary to each other [Wang et al., 2014; Wang et al., 2013; Wang et al., 2015c; Wu et al., 2013; Wu et al., 2016; Wang et al., 2016b; Wang et al., 2015d; Wang et al., 2015e; Wang et al., 2016a].", "startOffset": 126, "endOffset": 298}, {"referenceID": 35, "context": ", an image can be described by color histograms and textures, and the two features turn out to be complementary to each other [Wang et al., 2014; Wang et al., 2013; Wang et al., 2015c; Wu et al., 2013; Wu et al., 2016; Wang et al., 2016b; Wang et al., 2015d; Wang et al., 2015e; Wang et al., 2016a].", "startOffset": 126, "endOffset": 298}, {"referenceID": 36, "context": ", an image can be described by color histograms and textures, and the two features turn out to be complementary to each other [Wang et al., 2014; Wang et al., 2013; Wang et al., 2015c; Wu et al., 2013; Wu et al., 2016; Wang et al., 2016b; Wang et al., 2015d; Wang et al., 2015e; Wang et al., 2016a].", "startOffset": 126, "endOffset": 298}, {"referenceID": 37, "context": ", an image can be described by color histograms and textures, and the two features turn out to be complementary to each other [Wang et al., 2014; Wang et al., 2013; Wang et al., 2015c; Wu et al., 2013; Wu et al., 2016; Wang et al., 2016b; Wang et al., 2015d; Wang et al., 2015e; Wang et al., 2016a].", "startOffset": 126, "endOffset": 298}, {"referenceID": 49, "context": "Consequently, a wealth of multi-view hashing methods [Zhang et al., 2011; Kim et al., 2012; Masci et al., 2014; Song et al., 2011; Liu et al., 2012b; Shen et al., 2015] are developed in order to effectively leverage complementary priors from multi-views to achieve performance improvement in similarity search.", "startOffset": 53, "endOffset": 168}, {"referenceID": 7, "context": "Consequently, a wealth of multi-view hashing methods [Zhang et al., 2011; Kim et al., 2012; Masci et al., 2014; Song et al., 2011; Liu et al., 2012b; Shen et al., 2015] are developed in order to effectively leverage complementary priors from multi-views to achieve performance improvement in similarity search.", "startOffset": 53, "endOffset": 168}, {"referenceID": 23, "context": "Consequently, a wealth of multi-view hashing methods [Zhang et al., 2011; Kim et al., 2012; Masci et al., 2014; Song et al., 2011; Liu et al., 2012b; Shen et al., 2015] are developed in order to effectively leverage complementary priors from multi-views to achieve performance improvement in similarity search.", "startOffset": 53, "endOffset": 168}, {"referenceID": 28, "context": "Consequently, a wealth of multi-view hashing methods [Zhang et al., 2011; Kim et al., 2012; Masci et al., 2014; Song et al., 2011; Liu et al., 2012b; Shen et al., 2015] are developed in order to effectively leverage complementary priors from multi-views to achieve performance improvement in similarity search.", "startOffset": 53, "endOffset": 168}, {"referenceID": 20, "context": "Consequently, a wealth of multi-view hashing methods [Zhang et al., 2011; Kim et al., 2012; Masci et al., 2014; Song et al., 2011; Liu et al., 2012b; Shen et al., 2015] are developed in order to effectively leverage complementary priors from multi-views to achieve performance improvement in similarity search.", "startOffset": 53, "endOffset": 168}, {"referenceID": 27, "context": "Consequently, a wealth of multi-view hashing methods [Zhang et al., 2011; Kim et al., 2012; Masci et al., 2014; Song et al., 2011; Liu et al., 2012b; Shen et al., 2015] are developed in order to effectively leverage complementary priors from multi-views to achieve performance improvement in similarity search.", "startOffset": 53, "endOffset": 168}, {"referenceID": 49, "context": "Despite improved performance delivered by existing multiview hashing methods [Zhang et al., 2011; Kim et al., 2012; Masci et al., 2014; Song et al., 2011; Liu et al., 2012b; Shen et al., 2015], some fundamental limitations can be identified:", "startOffset": 77, "endOffset": 192}, {"referenceID": 7, "context": "Despite improved performance delivered by existing multiview hashing methods [Zhang et al., 2011; Kim et al., 2012; Masci et al., 2014; Song et al., 2011; Liu et al., 2012b; Shen et al., 2015], some fundamental limitations can be identified:", "startOffset": 77, "endOffset": 192}, {"referenceID": 23, "context": "Despite improved performance delivered by existing multiview hashing methods [Zhang et al., 2011; Kim et al., 2012; Masci et al., 2014; Song et al., 2011; Liu et al., 2012b; Shen et al., 2015], some fundamental limitations can be identified:", "startOffset": 77, "endOffset": 192}, {"referenceID": 28, "context": "Despite improved performance delivered by existing multiview hashing methods [Zhang et al., 2011; Kim et al., 2012; Masci et al., 2014; Song et al., 2011; Liu et al., 2012b; Shen et al., 2015], some fundamental limitations can be identified:", "startOffset": 77, "endOffset": 192}, {"referenceID": 20, "context": "Despite improved performance delivered by existing multiview hashing methods [Zhang et al., 2011; Kim et al., 2012; Masci et al., 2014; Song et al., 2011; Liu et al., 2012b; Shen et al., 2015], some fundamental limitations can be identified:", "startOffset": 77, "endOffset": 192}, {"referenceID": 27, "context": "Despite improved performance delivered by existing multiview hashing methods [Zhang et al., 2011; Kim et al., 2012; Masci et al., 2014; Song et al., 2011; Liu et al., 2012b; Shen et al., 2015], some fundamental limitations can be identified:", "startOffset": 77, "endOffset": 192}, {"referenceID": 35, "context": ", missing values in pixels), resulting in corresponding similarity matrices being corrupted by considerable noises [Wang et al., 2015d; Xia et al., 2014].", "startOffset": 115, "endOffset": 153}, {"referenceID": 45, "context": ", missing values in pixels), resulting in corresponding similarity matrices being corrupted by considerable noises [Wang et al., 2015d; Xia et al., 2014].", "startOffset": 115, "endOffset": 153}, {"referenceID": 13, "context": "sus or requisite similarity values across views in the presence of noise contamination remains an unresolved challenge in multi-view data analysis [Li et al., 2015; Zheng et al., 2015; Ye et al., 2012].", "startOffset": 147, "endOffset": 201}, {"referenceID": 53, "context": "sus or requisite similarity values across views in the presence of noise contamination remains an unresolved challenge in multi-view data analysis [Li et al., 2015; Zheng et al., 2015; Ye et al., 2012].", "startOffset": 147, "endOffset": 201}, {"referenceID": 47, "context": "sus or requisite similarity values across views in the presence of noise contamination remains an unresolved challenge in multi-view data analysis [Li et al., 2015; Zheng et al., 2015; Ye et al., 2012].", "startOffset": 147, "endOffset": 201}, {"referenceID": 19, "context": "This motivates us to deliver a framework to jointly and effectively learn similarity matrices and robust hash functions with kernel functions plugged because the kernel trick is able to tackle linearly inseparable data [Liu et al., 2012a].", "startOffset": 219, "endOffset": 238}, {"referenceID": 21, "context": "To this end, a latent kernelized similarity matrix is recovered shared across views by using lowrank representation (LRR) [Liu et al., 2013] which is robust to corrupted observations.", "startOffset": 122, "endOffset": 140}, {"referenceID": 18, "context": "To this end, we are further motivated to employ an landmark graph to build an approximate neighborhood graph using landmarks [Liu et al., 2011; Liu et al., 2010b], in which the similarity between a pair of data points is measured with respect to a small number of landmarks (typically a few hundred).", "startOffset": 125, "endOffset": 162}, {"referenceID": 17, "context": "To this end, we are further motivated to employ an landmark graph to build an approximate neighborhood graph using landmarks [Liu et al., 2011; Liu et al., 2010b], in which the similarity between a pair of data points is measured with respect to a small number of landmarks (typically a few hundred).", "startOffset": 125, "endOffset": 162}, {"referenceID": 18, "context": "The resulting graph is built in O(N) time and sufficiently sparse with performance approaching to true k-NN graphs as the number of landmarks increases [Liu et al., 2011].", "startOffset": 152, "endOffset": 170}, {"referenceID": 9, "context": "The proposed method is also different from partial view study [Kumar and III, 2011; Wang et al., 2015a], where they consider the case that data examples with some modalities are missing.", "startOffset": 62, "endOffset": 103}, {"referenceID": 32, "context": "The proposed method is also different from partial view study [Kumar and III, 2011; Wang et al., 2015a], where they consider the case that data examples with some modalities are missing.", "startOffset": 62, "endOffset": 103}, {"referenceID": 46, "context": "Our approach follows the setting of multi-view learning which aims to improve existing single view model by learning a model utilizing data collected from multiple channels [Xu et al., 2013; Zheng et al., 2015; Kumar et al., 2011; Wang et al., 2014; Wang et al., 2015b; Xia et al., 2014] where all data samples have full information in all views.", "startOffset": 173, "endOffset": 287}, {"referenceID": 53, "context": "Our approach follows the setting of multi-view learning which aims to improve existing single view model by learning a model utilizing data collected from multiple channels [Xu et al., 2013; Zheng et al., 2015; Kumar et al., 2011; Wang et al., 2014; Wang et al., 2015b; Xia et al., 2014] where all data samples have full information in all views.", "startOffset": 173, "endOffset": 287}, {"referenceID": 11, "context": "Our approach follows the setting of multi-view learning which aims to improve existing single view model by learning a model utilizing data collected from multiple channels [Xu et al., 2013; Zheng et al., 2015; Kumar et al., 2011; Wang et al., 2014; Wang et al., 2015b; Xia et al., 2014] where all data samples have full information in all views.", "startOffset": 173, "endOffset": 287}, {"referenceID": 31, "context": "Our approach follows the setting of multi-view learning which aims to improve existing single view model by learning a model utilizing data collected from multiple channels [Xu et al., 2013; Zheng et al., 2015; Kumar et al., 2011; Wang et al., 2014; Wang et al., 2015b; Xia et al., 2014] where all data samples have full information in all views.", "startOffset": 173, "endOffset": 287}, {"referenceID": 33, "context": "Our approach follows the setting of multi-view learning which aims to improve existing single view model by learning a model utilizing data collected from multiple channels [Xu et al., 2013; Zheng et al., 2015; Kumar et al., 2011; Wang et al., 2014; Wang et al., 2015b; Xia et al., 2014] where all data samples have full information in all views.", "startOffset": 173, "endOffset": 287}, {"referenceID": 45, "context": "Our approach follows the setting of multi-view learning which aims to improve existing single view model by learning a model utilizing data collected from multiple channels [Xu et al., 2013; Zheng et al., 2015; Kumar et al., 2011; Wang et al., 2014; Wang et al., 2015b; Xia et al., 2014] where all data samples have full information in all views.", "startOffset": 173, "endOffset": 287}, {"referenceID": 14, "context": "These principles are formulated into an objective function, which is optimized based on the inexact Augmented Lagrangian Multiplier (ALM) scheme [Lin et al., 2010].", "startOffset": 145, "endOffset": 163}, {"referenceID": 25, "context": "We remark that several cross-view semantic hashing algorithms [Ou et al., 2013; Wei et al., 2014; Kumar and Udupa, 2011] have been developed to embed multiple high dimensional features from heterogeneous data sources into one Hamming space, while preserving their original similarities.", "startOffset": 62, "endOffset": 120}, {"referenceID": 39, "context": "We remark that several cross-view semantic hashing algorithms [Ou et al., 2013; Wei et al., 2014; Kumar and Udupa, 2011] have been developed to embed multiple high dimensional features from heterogeneous data sources into one Hamming space, while preserving their original similarities.", "startOffset": 62, "endOffset": 120}, {"referenceID": 10, "context": "We remark that several cross-view semantic hashing algorithms [Ou et al., 2013; Wei et al., 2014; Kumar and Udupa, 2011] have been developed to embed multiple high dimensional features from heterogeneous data sources into one Hamming space, while preserving their original similarities.", "startOffset": 62, "endOffset": 120}, {"referenceID": 28, "context": "Some recent representative works include Multiple Feature Hashing (MFH) [Song et al., 2011], Composite Hashing with Multiple Sources (CHMS) [Zhang et al.", "startOffset": 72, "endOffset": 91}, {"referenceID": 49, "context": ", 2011], Composite Hashing with Multiple Sources (CHMS) [Zhang et al., 2011], Compact Kernel Hashing with multiple features (CKH) [Liu et al.", "startOffset": 56, "endOffset": 76}, {"referenceID": 20, "context": ", 2011], Compact Kernel Hashing with multiple features (CKH) [Liu et al., 2012b], and Multi-view Sequential Spectral Hashing (SSH) [Kim et al.", "startOffset": 61, "endOffset": 80}, {"referenceID": 7, "context": ", 2012b], and Multi-view Sequential Spectral Hashing (SSH) [Kim et al., 2012].", "startOffset": 59, "endOffset": 77}, {"referenceID": 27, "context": "[Shen et al., 2015] present a Multi-View Latent Hashing (MVLH) to learn hash codes by performing matrix factorization on a unified kernel feature space over multiple views.", "startOffset": 0, "endOffset": 19}, {"referenceID": 21, "context": "In this aspect, we attentively employ the low-rank representation (LRR) [Liu et al., 2013] to recover latent subspace structures from corrupted data.", "startOffset": 72, "endOffset": 90}, {"referenceID": 41, "context": "Low-rank modeling in attracting increasing attention due to its capability of recovering the underlying structure among data objects [Wright et al., 2009a; Candes and Recht, 2009; Li et al., 2015; Zhang et al., 2016; Zhang et al., 2014; Zhang et al., 2015].", "startOffset": 133, "endOffset": 256}, {"referenceID": 2, "context": "Low-rank modeling in attracting increasing attention due to its capability of recovering the underlying structure among data objects [Wright et al., 2009a; Candes and Recht, 2009; Li et al., 2015; Zhang et al., 2016; Zhang et al., 2014; Zhang et al., 2015].", "startOffset": 133, "endOffset": 256}, {"referenceID": 13, "context": "Low-rank modeling in attracting increasing attention due to its capability of recovering the underlying structure among data objects [Wright et al., 2009a; Candes and Recht, 2009; Li et al., 2015; Zhang et al., 2016; Zhang et al., 2014; Zhang et al., 2015].", "startOffset": 133, "endOffset": 256}, {"referenceID": 52, "context": "Low-rank modeling in attracting increasing attention due to its capability of recovering the underlying structure among data objects [Wright et al., 2009a; Candes and Recht, 2009; Li et al., 2015; Zhang et al., 2016; Zhang et al., 2014; Zhang et al., 2015].", "startOffset": 133, "endOffset": 256}, {"referenceID": 50, "context": "Low-rank modeling in attracting increasing attention due to its capability of recovering the underlying structure among data objects [Wright et al., 2009a; Candes and Recht, 2009; Li et al., 2015; Zhang et al., 2016; Zhang et al., 2014; Zhang et al., 2015].", "startOffset": 133, "endOffset": 256}, {"referenceID": 51, "context": "Low-rank modeling in attracting increasing attention due to its capability of recovering the underlying structure among data objects [Wright et al., 2009a; Candes and Recht, 2009; Li et al., 2015; Zhang et al., 2016; Zhang et al., 2014; Zhang et al., 2015].", "startOffset": 133, "endOffset": 256}, {"referenceID": 41, "context": "It has striking success in many applications such as data compression [Wright et al., 2009a], subspace clustering [Liu et al.", "startOffset": 70, "endOffset": 92}, {"referenceID": 21, "context": ", 2009a], subspace clustering [Liu et al., 2013; Deng et al., 2013; Zhang et al., 2014], and image processing [Zhou et al.", "startOffset": 30, "endOffset": 87}, {"referenceID": 5, "context": ", 2009a], subspace clustering [Liu et al., 2013; Deng et al., 2013; Zhang et al., 2014], and image processing [Zhou et al.", "startOffset": 30, "endOffset": 87}, {"referenceID": 50, "context": ", 2009a], subspace clustering [Liu et al., 2013; Deng et al., 2013; Zhang et al., 2014], and image processing [Zhou et al.", "startOffset": 30, "endOffset": 87}, {"referenceID": 54, "context": ", 2014], and image processing [Zhou et al., 2013; Zhang et al., 2016; Zhang et al., 2015].", "startOffset": 30, "endOffset": 89}, {"referenceID": 52, "context": ", 2014], and image processing [Zhou et al., 2013; Zhang et al., 2016; Zhang et al., 2015].", "startOffset": 30, "endOffset": 89}, {"referenceID": 51, "context": ", 2014], and image processing [Zhou et al., 2013; Zhang et al., 2016; Zhang et al., 2015].", "startOffset": 30, "endOffset": 89}, {"referenceID": 52, "context": "For instance, in [Zhang et al., 2016], Zhang et al.", "startOffset": 17, "endOffset": 37}, {"referenceID": 46, "context": "Nowadays, data are usually collected from diverse domains or obtained from various feature extractors, and each group of features can be regarded as a particular view [Xu et al., 2013].", "startOffset": 167, "endOffset": 184}, {"referenceID": 21, "context": "In practice, the underlying structure of data could be multiple subspaces, and thus Low-Rank Representation (LRR) is designed to find subspace structures in noisy data [Liu et al., 2013; Zhang et al., 2014].", "startOffset": 168, "endOffset": 206}, {"referenceID": 50, "context": "In practice, the underlying structure of data could be multiple subspaces, and thus Low-Rank Representation (LRR) is designed to find subspace structures in noisy data [Liu et al., 2013; Zhang et al., 2014].", "startOffset": 168, "endOffset": 206}, {"referenceID": 13, "context": "The multi-view low-rank analysis [Li et al., 2015] is a recently proposed multi-view learning approach, which introduces low-rank constraint to reveal the intrinsic structure of data, and identifies outliers for the representation coefficients in low-rank matrix recovery.", "startOffset": 33, "endOffset": 50}, {"referenceID": 8, "context": "Following the Kernelized Locality Sensitive Hashing [Kulis and Grauman, 2009], we uniformly select R samples from the training set X , denoted by Zr (r = 1, .", "startOffset": 52, "endOffset": 77}, {"referenceID": 18, "context": "Cp = \u2211R r=1Wrp\u03c6(Zr) indicates the linear combination of R landmarks, which can be the cluster centers [Liu et al., 2011] via scalable R-means clustering over the feature space with d dimensions.", "startOffset": 102, "endOffset": 120}, {"referenceID": 19, "context": "The kernel function is plugged into hash function because the kernel trick has been theoretically and empirically proved to be able to tackle the data distribution that is almost linearly inseparable [Liu et al., 2012a].", "startOffset": 200, "endOffset": 219}, {"referenceID": 11, "context": "We explicitly model the redundancy via sparsity since multi-view study suggests that each individual view is sufficient to identify most of the similarity structure, and the deviation between requisite component and data sample is sparse [Kumar et al., 2011].", "startOffset": 238, "endOffset": 258}, {"referenceID": 21, "context": "Thus, an `2,1-norm is adopted to characterize errors since they usually cause column sparsity in an affinity matrix [Liu et al., 2013].", "startOffset": 116, "endOffset": 134}, {"referenceID": 40, "context": "Many studies [Weiss et al., 2008; Song et al., 2011] have shown the benefits to exploit local structure of the training data to infer accurate and compact hash codes.", "startOffset": 13, "endOffset": 52}, {"referenceID": 28, "context": "Many studies [Weiss et al., 2008; Song et al., 2011] have shown the benefits to exploit local structure of the training data to infer accurate and compact hash codes.", "startOffset": 13, "endOffset": 52}, {"referenceID": 18, "context": "To avoid the computational bottleneck, we employ a landmark graph by using a small set of L points called landmarks to approximate the data neighborhood structure [Liu et al., 2011].", "startOffset": 163, "endOffset": 181}, {"referenceID": 18, "context": "Thus, the landmark graph provides a powerful approximation to the adjacency matrix S as \u015c = F\u039b\u22121FT where \u039b = diag(F 1) \u2208 RL\u00d7L [Liu et al., 2011].", "startOffset": 126, "endOffset": 144}, {"referenceID": 40, "context": "Following spectral hashing [Weiss et al., 2008], we relax the constraints yi \u2208 {\u22121, 1} to be yi = W K\u0302i + b, then we have", "startOffset": 27, "endOffset": 47}, {"referenceID": 14, "context": "To compute K\u0302 and E, we employ an efficient optimization technique, the inexact augmented Lagrange multiplier (ALM) algorithm [Lin et al., 2010].", "startOffset": 126, "endOffset": 144}, {"referenceID": 16, "context": "The rank minimization problem has been well studied in literature [Liu et al., 2010a; Wright et al., 2009b].", "startOffset": 66, "endOffset": 107}, {"referenceID": 42, "context": "The rank minimization problem has been well studied in literature [Liu et al., 2010a; Wright et al., 2009b].", "startOffset": 66, "endOffset": 107}, {"referenceID": 15, "context": "where S%(x) = max(x\u2212%, 0)+min(x+%, 0) is the shrinkage operator [Lin et al., 2015].", "startOffset": 64, "endOffset": 82}, {"referenceID": 6, "context": "Each subproblem is a proximal operator problem, which can be efficiently solved by the projection algorithm in [Duchi et al., 2008].", "startOffset": 111, "endOffset": 131}, {"referenceID": 0, "context": "A widely used solution is the Nystr\u00f6m extension [Bengio et al., 2004].", "startOffset": 48, "endOffset": 69}, {"referenceID": 26, "context": "[Shen et al., 2013].", "startOffset": 0, "endOffset": 19}, {"referenceID": 26, "context": "To this end, we employ a prototype algorithm [Shen et al., 2013] to approximate yq using only a small base set:", "startOffset": 45, "endOffset": 64}, {"referenceID": 28, "context": "\u2022 Multiple feature hashing (MFH) [Song et al., 2011]: This method exploits local structure in each feature and global consistency in the optimization of hashing functions.", "startOffset": 33, "endOffset": 52}, {"referenceID": 49, "context": "\u2022 Composite hashing with multiple sources (CHMS) [Zhang et al., 2011]: This method treats a linear combination of view-specific similarities as an average similarity which can be plugged into a spectral hashing framework.", "startOffset": 49, "endOffset": 69}, {"referenceID": 20, "context": "\u2022 Compact kernel hashing with multiple features (CKH) [Liu et al., 2012b]: It is a multiple feature hashing framework where multiple kernels are linearly combined.", "startOffset": 54, "endOffset": 73}, {"referenceID": 7, "context": "\u2022 Sequential spectral hashing with multiple representations (SSH) [Kim et al., 2012]: This method constructs an average similarity matrix to assemble view-specific similarity matrices.", "startOffset": 66, "endOffset": 84}, {"referenceID": 27, "context": "\u2022 Multi-View Latent Hashing (MVLH) [Shen et al., 2015]: This is an unsupervised multi-view hashing approach where binary codes are learned by the latent factors shared by multiple views from an unified kernel feature space.", "startOffset": 35, "endOffset": 54}, {"referenceID": 24, "context": "Every image is assigned to a mutually exclusive class label and for each image, we extract 512-dimensional GIST feature [Oliva and Torralba, 2001] and 300-dimensional bag-of-words quantized from dense SIFT features [Lowe, 2004] to be two views.", "startOffset": 120, "endOffset": 146}, {"referenceID": 22, "context": "Every image is assigned to a mutually exclusive class label and for each image, we extract 512-dimensional GIST feature [Oliva and Torralba, 2001] and 300-dimensional bag-of-words quantized from dense SIFT features [Lowe, 2004] to be two views.", "startOffset": 215, "endOffset": 227}, {"referenceID": 3, "context": "\u2022 NUS-WIDE [Chua et al., 2009] contains 269,648 labeled images crawled from Flickr and is manually annotated with 81 categories.", "startOffset": 11, "endOffset": 30}, {"referenceID": 48, "context": "The parameter \u03c3 is learned via the self-tuning strategy [Zelnik-Manor and Perona, 2004].", "startOffset": 56, "endOffset": 87}, {"referenceID": 19, "context": "Evaluation Metric The mean precision-recall and mean average precision (MAP) are computed over the retrieved set consisting of the samples with the hamming distance [Liu et al., 2012a] using 8 to 32 bits to a specific query.", "startOffset": 165, "endOffset": 184}, {"referenceID": 7, "context": "SSH has a gain in efficiency compared with MFH and CHMS on account of their approximation on the K-nearest graph construction [Kim et al., 2012].", "startOffset": 126, "endOffset": 144}, {"referenceID": 12, "context": "The MINIST dataset [LeCun et al., 1998] consists of 70K images, each of 784 dimensions, of handwritten digits from \u201c0\u201d to \u201c9\u201d.", "startOffset": 19, "endOffset": 39}], "year": 2016, "abstractText": "Learning hash functions/codes for similarity search over multi-view data is attracting increasing attention, where similar hash codes are assigned to the data objects characterizing consistently neighborhood relationship across views. Traditional methods in this category inherently suffer three limitations: 1) they commonly adopt a two-stage scheme where similarity matrix is first constructed, followed by a subsequent hash function learning; 2) these methods are commonly developed on the assumption that data samples with multiple representations are noise-free,which is not practical in reallife applications; 3) they often incur cumbersome training model caused by the neighborhood graph construction using all N points in the database (O(N)). In this paper, we motivate the problem of jointly and efficiently training the robust hash functions over data objects with multi-feature representations which may be noise corrupted. To achieve both the robustness and training efficiency, we propose an approach to effectively and efficiently learning low-rank kernelized 1 hash functions shared across views. Specifically, we utilize landmark graphs to construct tractable similarity matrices in multi-views to automatically discover neighborhood structure in the data. To learn robust hash functions, a latent low-rank kernel function is used to construct hash functions in order to accommodate linearly inseparable data. In particular, a latent kernelized similarity matrix is recovered by rank minimization on multiple kernel-based similarity matrices. Extensive experiments on realworld multi-view datasets validate the efficacy of our method in the presence of error corruptions. We use kernelized similarity rather than kernel, as it is not a squared symmetric matrix for data-landmark affinity matrix.", "creator": "LaTeX with hyperref package"}}}