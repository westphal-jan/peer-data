{"id": "1611.05546", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Nov-2016", "title": "Zero-Shot Visual Question Answering", "abstract": "Part of the appeal of Visual Question Answering (VQA) is its promise to answer new questions about previously unseen images. Most current methods demand training questions that illustrate every possible concept, and will therefore never achieve this capability, since the volume of required training data would be prohibitive. Answering general questions about images requires methods capable of Zero-Shot VQA, that is, methods able to answer questions beyond the scope of the training questions. We propose a new evaluation protocol for VQA methods which measures their ability to perform Zero-Shot VQA, and in doing so highlights significant practical deficiencies of current approaches, some of which are masked by the biases in current datasets. We propose and evaluate several strategies for achieving Zero-Shot VQA, including methods based on pretrained word embeddings, object classifiers with semantic embeddings, and test-time retrieval of example images. Our extensive experiments are intended to serve as baselines for Zero-Shot VQA, and they also achieve state-of-the-art performance in the standard VQA evaluation setting.", "histories": [["v1", "Thu, 17 Nov 2016 03:21:00 GMT  (6168kb,D)", "https://arxiv.org/abs/1611.05546v1", null], ["v2", "Sun, 20 Nov 2016 21:51:24 GMT  (6168kb,D)", "http://arxiv.org/abs/1611.05546v2", null]], "reviews": [], "SUBJECTS": "cs.CV cs.AI cs.CL", "authors": ["damien teney", "anton van den hengel"], "accepted": false, "id": "1611.05546"}, "pdf": {"name": "1611.05546.pdf", "metadata": {"source": "CRF", "title": "Zero-Shot Visual Question Answering", "authors": ["Damien Teney", "Anton van den Hengel"], "emails": ["damien.teney@adelaide.edu.au", "anton.vandenhengel@adelaide.edu.au"], "sections": [{"heading": "1. Introduction", "text": "This year it is more than ever before."}, {"heading": "2. Related work", "text": "The task of visual response has become increasingly interested in the use of additional terms. [3] Most newer methods are variations on the idea of a common embedding of the image and the question associated with a deep neural network. Image and question are each guided by a conventional approach (CNN) and a recurrent neural network (e.g. an LSTM), producing representations of the image and question in a common space that can then be fed together into a vocabulary of possible answers. Consultations [30] for a recent study of the literature are trained to a certain degree."}, {"heading": "3. Dataset for Zero-Shot VQA", "text": "We propose a data set for VQA with a \"zero shot\" aspect in terms of questions and answers, but not the visual concepts in the accompanying images. For example, we consider the question How many zebras are in this image? a zero shot if no training question concerns zebras. However, images that contain zebras can appear in the training set (with questions concerning other elements of these images) or be used to train auxiliary components, such as an image classifier that recognizes zebras. This distinction reflects the fact that CNNs that have been pre-trained on ImageNet [4] are often used in existing VQA methods, and the fact that VQA is the task we are actually interested in."}, {"heading": "3.1. Repurposing the Visual7W dataset", "text": "In multiple choice VQA, each training or test instance is a tuple of an image, a question, and multiple choice of answers (four in the data set considered here); the questions and answers are given as text in natural language; exactly one of the answers is marked as correct and is used for supervised training and evaluation; a data set is divided into training, evaluation, and test columns; the words used in the questions and answers of the VQA data sets follow a long-tail distribution typical of natural language [12, 36]; in other words, most questions and answers consist of words from a small vocabulary, but a large number of other words appear very rarely; the typical strategy in VQA methods is to focus on a limited vocabulary and a limited number of possible answers; this makes training practically easier, and the performance penalty remains reasonable, as the rare data sets appear in only a small fraction of the test instance."}, {"heading": "3.2. Building zero-shot splits", "text": "The words in the subsets are randomly selected from those that occur less than 20 times in the entire data set, ensuring that these invisible words are semantically rich, as opposed to traditional verbs or stop words. These words typically describe fine-grained categories and very specific concepts (see examples in Fig. 2 and in the supplementary material).The validation and test splits are formed from all instances that contain at least one word from their reserved sentence, eliminating any overlap between the sets. The training set is composed of all remaining instances to ensure that the images are not separated between the training and validation / test sets, as in the original splits, so as not to promote match. An analysis of the resulting splits is given in Table 1."}, {"heading": "4. Methods for zero-shot VQA", "text": "We are looking at a neural network for VQA with a simple architecture. Our main objective is to evaluate additional features and pre-trained representations of the inputs (image, question and candidate answers). A simple architecture allows for the evaluation of these in relative isolation. In particular, our method does not include an attention mechanism, unlike many current approaches. Applying attention to the proposed features has no obvious implementation and may warrant further research of its own. Also, note that each of the proposed improvements is evaluated on the basis of a relatively simple implementation. The goal is not to obtain the single most powerful model, but to direct future research into areas with the greatest chance of success and to provide reference services for basic implementations."}, {"heading": "4.1. Baseline method", "text": "Our network architecture is comparable to baselines evaluated in other studies by VQA [3, 11, 35]. The general principle (see Figure 1) is to map the inputs, i.e. a question, an image and answers of the candidate to vector representations in a common space. The mappings to create these representations are learned so that interactions (e.g. distances, products, row comparisons,..) between elements in this space capture semantic compatibility. Our baseline presents the question with a bag of words (BoW). Each word is represented by a fixed-length vector with a reference table that links each possible word to a learned vector (unknown words at test points get an empty vector of zeros). The BoW representation is the average of all non-empty vectors of the question words. We refer to this representation as the learned word xxxx. Additional features described below, where final attributes are required (Figure 3)."}, {"heading": "4.2. Improved language representations", "text": "This common practice has two advantages. First, the upstream embedding reflects word relationships and has shown empirically that complex semantic relationships can be captured in their vector space. Second, the upstream task is unattended and embedding can be learned from very large amounts of data, which cover a much richer vocabulary than a VQA training. Specifically, we evaluate the embedding of different dimensions {50,100,200,300} that are upstream, and the embedding of words in a vocabulary. We propose to embed words with the same stem (e.g. flowers, flowers and blossoms)."}, {"heading": "4.3. Improved image representations", "text": "Explicit object detection In addition to the global CNN features that represent the input image, we consider the use of explicit object detection in the scene. We get candidate detection through the YOLO method, which is pre-trained for Pascal VOC [21], as a set of detection values, each associated with the class of the object detected. We keep all detection values above a certain threshold (varies during several experiments in terms of different callbacks). We turn the detection set into a fixed vector with a similar bag of words as for our questions (see above): We associate the possible classes with a learned embedding (i.e. a viewing table) and add these embedding across all detections. Semantic object class embedding The above detections do not associate semantic before the classes considered by the detector. However, these classes are known by their names, and we experiment with the detection method."}, {"heading": "5. Experiments", "text": "We conduct extensive experiments on both the original and zero-shot split of the Visual7W dataset. As assumed in our premise, we observe different behaviors in the two cases, and the proposed improvements have different effects on the average overall performance of the two settings. Each experiment below considers a variation at a point in time of the base model, including pre-trained word embeddings of dimension 300, unless otherwise noted. Pre-trained word embeddings are common practice and have a major positive impact, and thus are de facto our reference for fair comparisons of additional improvements. Details of implementation are included in the supplementary material."}, {"heading": "5.1. Masking inputs", "text": "First of all, we get an indication of the difficulty of the data sets by training a model with limited input, masking the question and / or the image, which forces the model to rely on the distortions of the data sets. In fact, when masking both the question and the image, the only input is the amount of multiple-choice answers, and the model can only learn to select the common ones that can be seen during the training. [11] As already observed, this strategy is sufficient to achieve high performance with the standard splits. It is much less effective in the Z.S. setting, e.g. when masking the question, 62.7% versus 52.6% in the default or Z.S. setting (see Table 2 and Fig. 4, bottom right). In other words, answers in the Z.S. setting cannot be so easily guessed."}, {"heading": "5.2. Improved representations", "text": "This year, it will be ready to leave the country in which it is located."}, {"heading": "5.3. Comparison with the state-of-the-art", "text": "Finally, we evaluate a model that incorporates all of the proposed improvements (see Table 2), which achieves the best overall performance in both the standard and the ZQS split; the relative gains from combined improvements are not strictly cumulative, indicating some overlap between the capabilities introduced by each individual; some of the individual gains are likely to be due to increased model capacity, the benefits of which will eventually be saturated; in the standard splits, our best model significantly exceeds the existing state of the art on this dataset [11]; we also trained our baseline and best models on reduced training data (random subsets); and we estimate a gentle drop in performance, especially in the ZQS setting using our best method (Figure 4, bottom center), indicating a good generalization that should be one of the main objectives of VQA systems, as argued in the introduction."}, {"heading": "6. Conclusions", "text": "We have rearranged the Visual7W dataset to allow for evaluation that focuses exclusively on such test cases. This setting requires more generalization skills and leads to a more honest assessment of deep image comprehension. This setting also motivates alternative strategies. We have shown that additional auxiliary data used for both pre-training visual representations and during the test period are beneficial not only for ZS-VQA, but also in the traditional environment. Extending these strategies provides promising directions for future research."}, {"heading": "B. Examples of test questions", "text": "This year it is so far that it is only a matter of time before it will be so far, until it is so far."}], "references": [{"title": "Zipfs law and the internet", "author": ["L.A. Adamic", "B.A. Huberman"], "venue": "Glottometrics, 3(1):143\u2013150,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2002}, {"title": "VQA: Visual Question Answering", "author": ["S. Antol", "A. Agrawal", "J. Lu", "M. Mitchell", "D. Batra", "C.L. Zitnick", "D. Parikh"], "venue": "Proc. IEEE Int. Conf. Comp. Vis.,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2015}, {"title": "Imagenet: A large-scale hierarchical image database", "author": ["J. Deng", "W. Dong", "R. Socher", "L.-J. Li", "K. Li", "L. Fei- Fei"], "venue": "Proc. IEEE Conf. Comp. Vis. Patt. Recogn.,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2009}, {"title": "Language to logical form with neural attention", "author": ["L. Dong", "M. Lapata"], "venue": "Proc. Conf. Association for Computational Linguistics,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2016}, {"title": "Multimodal compact bilinear pooling for visual question answering and visual grounding", "author": ["A. Fukui", "D.H. Park", "D. Yang", "A. Rohrbach", "T. Darrell", "M. Rohrbach"], "venue": "arXiv preprint arXiv:1606.01847,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2016}, {"title": "Visual Turing test for computer vision systems", "author": ["D. Geman", "S. Geman", "N. Hallonquist", "L. Younes"], "venue": "Proceedings of the National Academy of Sciences, 112(12):3618\u20133623,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2015}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["X. Glorot", "Y. Bengio"], "venue": "Proc. Int. Conf. Artificial Intell. & Stat., pages 249\u2013256,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2010}, {"title": "Pointing the unknown words", "author": ["\u00c7. G\u00fcl\u00e7ehre", "S. Ahn", "R. Nallapati", "B. Zhou", "Y. Bengio"], "venue": "arXiv preprint arXiv:1603.08148,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2016}, {"title": "Deep residual learning for image recognition", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "Proc. IEEE Conf. Comp. Vis. Patt. Recogn.,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2016}, {"title": "Revisiting visual question answering baselines", "author": ["A. Jabri", "A. Joulin", "L. van der Maaten"], "venue": "arXiv preprint arXiv:1606.08390,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2016}, {"title": "Visual genome: Connecting language and vision using crowdsourced dense image annotations", "author": ["R. Krishna", "Y. Zhu", "O. Groth", "J. Johnson", "K. Hata", "J. Kravitz", "S. Chen", "Y. Kalantidis", "L.-J. Li", "D.A. Shamma", "M. Bernstein", "L. Fei-Fei"], "venue": "arXiv preprint arXiv:1602.07332,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2016}, {"title": "A multi-world approach to question answering about real-world scenes based on uncertain input", "author": ["M. Malinowski", "M. Fritz"], "venue": "Proc. Advances in Neural Inf. Process. Syst., pages 1682\u20131690,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2014}, {"title": "Towards a Visual Turing Challenge", "author": ["M. Malinowski", "M. Fritz"], "venue": "arXiv preprint arXiv:1410.8027,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2014}, {"title": "Ask Your Neurons: A Neural-based Approach to Answering Questions about Images", "author": ["M. Malinowski", "M. Rohrbach", "M. Fritz"], "venue": "Proc. IEEE Int. Conf. Comp. Vis.,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2015}, {"title": "The Stanford CoreNLP natural language processing toolkit", "author": ["C.D. Manning", "M. Surdeanu", "J. Bauer", "J. Finkel", "S.J. Bethard", "D. McClosky"], "venue": "Association for Computational Linguistics (ACL) System Demonstrations, pages 55\u2013 60,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2014}, {"title": "Efficient estimation of word representations in vector space", "author": ["T. Mikolov", "K. Chen", "G. Corrado", "J. Dean"], "venue": "arXiv preprint arXiv:1301.3781,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2013}, {"title": "Glove: Global Vectors for Word Representation", "author": ["J. Pennington", "R. Socher", "C. Manning"], "venue": "Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2014}, {"title": "An algorithm for suffix stripping", "author": ["M. Porter"], "venue": "Program, pages 130\u2013137,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 1980}, {"title": "You only look once: Unified, real-time object detection", "author": ["J. Redmon", "S.K. Divvala", "R.B. Girshick", "A. Farhadi"], "venue": "arXiv preprint arXiv:1506.02640,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2015}, {"title": "Image Question Answering: A Visual Semantic Embedding Model and a New Dataset", "author": ["M. Ren", "R. Kiros", "R. Zemel"], "venue": "Proc. Advances in Neural Inf. Process. Syst.,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2015}, {"title": "Where to look: Focus regions for visual question answering", "author": ["K.J. Shih", "S. Singh", "D. Hoiem"], "venue": "Proc. IEEE Conf. Comp. Vis. Patt. Recogn.,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2016}, {"title": "Graph-structured representations for visual question answering", "author": ["D. Teney", "L. Liu", "A. van den Hengel"], "venue": "arXiv preprint arXiv:1609.05600,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2016}, {"title": "Order- Embeddings of Images and Language", "author": ["I. Vendrov", "R. Kiros", "S. Fidler", "R. Urtasun"], "venue": "Proc. Int. Conf. Learn. Representations,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2016}, {"title": "Matching networks for one shot learning", "author": ["O. Vinyals", "C. Blundell", "T.P. Lillicrap", "K. Kavukcuoglu", "D. Wierstra"], "venue": "arXiv preprint arXiv:1606.04080,", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2016}, {"title": "Explicit knowledge-based reasoning for visual question answering", "author": ["P. Wang", "Q. Wu", "C. Shen", "A. v. d. Hengel", "A. Dick"], "venue": "arXiv preprint arXiv:1511.02570,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2015}, {"title": "What Value Do Explicit High Level Concepts Have in Vision to Language Problems", "author": ["Q. Wu", "C. Shen", "A. v. d. Hengel", "L. Liu", "A. Dick"], "venue": "In Proc. IEEE Conf. Comp. Vis. Patt. Recogn.,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2016}, {"title": "Image captioning and visual question answering based on attributes and their related external knowledge", "author": ["Q. Wu", "C. Shen", "A. v. d. Hengel", "P. Wang", "A. Dick"], "venue": "arXiv preprint arXiv:1603.02814,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2016}, {"title": "Visual Question Answering: A Survey of Methods and Datasets", "author": ["Q. Wu", "D. Teney", "P. Wang", "C. Shen", "A. Dick", "A. van den Hengel"], "venue": "arXiv preprint arXiv:1607.05910,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2016}, {"title": "Ask Me Anything: Free-form Visual Question Answering Based on Knowledge from External Sources", "author": ["Q. Wu", "P. Wang", "C. Shen", "A. Dick", "A. v. d. Hengel"], "venue": "In Proc. IEEE Conf. Comp. Vis. Patt. Recogn.,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2016}, {"title": "Hierarchical memory networks for answer selection on unknown words", "author": ["J. Xua", "J. Shia", "Y. Yaoa", "S. Zhenga", "B. Xua", "B. Xu"], "venue": "arXiv preprint arXiv:1609.08843,", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2016}, {"title": "ADADELTA: an adaptive learning rate method", "author": ["M.D. Zeiler"], "venue": "arXiv preprint arXiv:1212.5701,", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2012}, {"title": "Yin and yang: Balancing and answering binary visual questions", "author": ["P. Zhang", "Y. Goyal", "D. Summers-Stay", "D. Batra", "D. Parikh"], "venue": "Proc. IEEE Conf. Comp. Vis. Patt. Recogn.,", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2016}, {"title": "Simple baseline for visual question answering", "author": ["B. Zhou", "Y. Tian", "S. Sukhbaatar", "A. Szlam", "R. Fergus"], "venue": "arXiv preprint arXiv:1512.02167,", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 11, "context": "The recent interest [13, 3, 30], in part, reflects the enthusiasm for VQA as an indicator of progress towards deep scene understanding, which is the overarching goal of computer vision [7, 14].", "startOffset": 20, "endOffset": 31}, {"referenceID": 1, "context": "The recent interest [13, 3, 30], in part, reflects the enthusiasm for VQA as an indicator of progress towards deep scene understanding, which is the overarching goal of computer vision [7, 14].", "startOffset": 20, "endOffset": 31}, {"referenceID": 27, "context": "The recent interest [13, 3, 30], in part, reflects the enthusiasm for VQA as an indicator of progress towards deep scene understanding, which is the overarching goal of computer vision [7, 14].", "startOffset": 20, "endOffset": 31}, {"referenceID": 5, "context": "The recent interest [13, 3, 30], in part, reflects the enthusiasm for VQA as an indicator of progress towards deep scene understanding, which is the overarching goal of computer vision [7, 14].", "startOffset": 185, "endOffset": 192}, {"referenceID": 12, "context": "The recent interest [13, 3, 30], in part, reflects the enthusiasm for VQA as an indicator of progress towards deep scene understanding, which is the overarching goal of computer vision [7, 14].", "startOffset": 185, "endOffset": 192}, {"referenceID": 27, "context": "A number of VQA datasets have been introduced, and a variety of methods have demonstrated impressive (yet possibly converging) results (see [30] for a survey).", "startOffset": 140, "endOffset": 144}, {"referenceID": 1, "context": "A small number of frequent words constitute a large fraction of the correct answers, and exploiting these statistical regularities achieves deceptively strong performance [3, 11, 34].", "startOffset": 171, "endOffset": 182}, {"referenceID": 9, "context": "A small number of frequent words constitute a large fraction of the correct answers, and exploiting these statistical regularities achieves deceptively strong performance [3, 11, 34].", "startOffset": 171, "endOffset": 182}, {"referenceID": 31, "context": "A small number of frequent words constitute a large fraction of the correct answers, and exploiting these statistical regularities achieves deceptively strong performance [3, 11, 34].", "startOffset": 171, "endOffset": 182}, {"referenceID": 9, "context": "For example, the stateof-the-art method in [11] achieves an impressive accuracy of almost 65% correct answers on the Visual7W dataset.", "startOffset": 43, "endOffset": 47}, {"referenceID": 1, "context": "[3].", "startOffset": 0, "endOffset": 3}, {"referenceID": 27, "context": "Consult [30] for a recent survey of the literature.", "startOffset": 8, "endOffset": 12}, {"referenceID": 1, "context": "Several such datasets are available and they have increased in size and quality [3, 12, 13, 22, 36].", "startOffset": 80, "endOffset": 99}, {"referenceID": 10, "context": "Several such datasets are available and they have increased in size and quality [3, 12, 13, 22, 36].", "startOffset": 80, "endOffset": 99}, {"referenceID": 11, "context": "Several such datasets are available and they have increased in size and quality [3, 12, 13, 22, 36].", "startOffset": 80, "endOffset": 99}, {"referenceID": 19, "context": "Several such datasets are available and they have increased in size and quality [3, 12, 13, 22, 36].", "startOffset": 80, "endOffset": 99}, {"referenceID": 16, "context": "the vectors used to represent the words, can be pretrained on a language modeling task [19, 17].", "startOffset": 87, "endOffset": 95}, {"referenceID": 15, "context": "the vectors used to represent the words, can be pretrained on a language modeling task [19, 17].", "startOffset": 87, "endOffset": 95}, {"referenceID": 20, "context": "Such pretrained embeddings showed benefit for VQA for example in [23, 6, 11, 24].", "startOffset": 65, "endOffset": 80}, {"referenceID": 4, "context": "Such pretrained embeddings showed benefit for VQA for example in [23, 6, 11, 24].", "startOffset": 65, "endOffset": 80}, {"referenceID": 9, "context": "Such pretrained embeddings showed benefit for VQA for example in [23, 6, 11, 24].", "startOffset": 65, "endOffset": 80}, {"referenceID": 21, "context": "Such pretrained embeddings showed benefit for VQA for example in [23, 6, 11, 24].", "startOffset": 65, "endOffset": 80}, {"referenceID": 21, "context": "The syntactic structure of language for VQA has received much less attention, but recent work suggest that explicit parsing can bring useful information [24].", "startOffset": 153, "endOffset": 157}, {"referenceID": 25, "context": "An exception is [28], where the authors use language as an explicit intermediate representation for VQA.", "startOffset": 16, "endOffset": 20}, {"referenceID": 28, "context": "Finally, a few methods consider the test-time retrieval of additional data from knowledge bases [31, 27, 29].", "startOffset": 96, "endOffset": 108}, {"referenceID": 24, "context": "Finally, a few methods consider the test-time retrieval of additional data from knowledge bases [31, 27, 29].", "startOffset": 96, "endOffset": 108}, {"referenceID": 26, "context": "Finally, a few methods consider the test-time retrieval of additional data from knowledge bases [31, 27, 29].", "startOffset": 96, "endOffset": 108}, {"referenceID": 28, "context": "In comparison, the information from knowledge bases in [31, 27, 29] is purely textual in nature.", "startOffset": 55, "endOffset": 67}, {"referenceID": 24, "context": "In comparison, the information from knowledge bases in [31, 27, 29] is purely textual in nature.", "startOffset": 55, "endOffset": 67}, {"referenceID": 26, "context": "In comparison, the information from knowledge bases in [31, 27, 29] is purely textual in nature.", "startOffset": 55, "endOffset": 67}, {"referenceID": 1, "context": "butions of words is a known issue in VQA datasets [3, 12, 36, 34] and more generally in natural language [1, 9].", "startOffset": 50, "endOffset": 65}, {"referenceID": 10, "context": "butions of words is a known issue in VQA datasets [3, 12, 36, 34] and more generally in natural language [1, 9].", "startOffset": 50, "endOffset": 65}, {"referenceID": 31, "context": "butions of words is a known issue in VQA datasets [3, 12, 36, 34] and more generally in natural language [1, 9].", "startOffset": 50, "endOffset": 65}, {"referenceID": 0, "context": "butions of words is a known issue in VQA datasets [3, 12, 36, 34] and more generally in natural language [1, 9].", "startOffset": 105, "endOffset": 111}, {"referenceID": 7, "context": "butions of words is a known issue in VQA datasets [3, 12, 36, 34] and more generally in natural language [1, 9].", "startOffset": 105, "endOffset": 111}, {"referenceID": 7, "context": "Let us finally note a recent surge of interest in better handling of rare and unknown words in various natural language applications [9, 32, 5].", "startOffset": 133, "endOffset": 143}, {"referenceID": 29, "context": "Let us finally note a recent surge of interest in better handling of rare and unknown words in various natural language applications [9, 32, 5].", "startOffset": 133, "endOffset": 143}, {"referenceID": 3, "context": "Let us finally note a recent surge of interest in better handling of rare and unknown words in various natural language applications [9, 32, 5].", "startOffset": 133, "endOffset": 143}, {"referenceID": 2, "context": "This distinction reflects the fact that CNNs pre-trained on ImageNet [4] are commonly used in existing VQA methods, and the fact that VQA is the task that we are actually interested in.", "startOffset": 69, "endOffset": 72}, {"referenceID": 10, "context": "The words used in the questions and answers of VQA datasets follow a long-tail distribution typical in natural language [12, 36].", "startOffset": 120, "endOffset": 128}, {"referenceID": 10, "context": "Visual7W is itself a subset of the Visual Genome dataset [12] the highest quality dataset for VQA currently available, in terms of size, answer distribution, human performance, and the quality of the multiple choice answers.", "startOffset": 57, "endOffset": 61}, {"referenceID": 1, "context": "Our network architecture is similar to baselines evaluated in other studies of VQA [3, 11, 35].", "startOffset": 83, "endOffset": 94}, {"referenceID": 9, "context": "Our network architecture is similar to baselines evaluated in other studies of VQA [3, 11, 35].", "startOffset": 83, "endOffset": 94}, {"referenceID": 32, "context": "Our network architecture is similar to baselines evaluated in other studies of VQA [3, 11, 35].", "startOffset": 83, "endOffset": 94}, {"referenceID": 8, "context": "The image is represented with global (image-wide) features of dimension 2048 extracted from the last pooling layer of a ResNet-152 [10] pretrained for image recognition on ImageNet.", "startOffset": 131, "endOffset": 135}, {"referenceID": 16, "context": "This common practice [19, 17] has two advantages.", "startOffset": 21, "endOffset": 29}, {"referenceID": 15, "context": "This common practice [19, 17] has two advantages.", "startOffset": 21, "endOffset": 29}, {"referenceID": 17, "context": "Concretely, we replace every word in the input question and/or answer by its stem, obtained either with the classical Porter algorithm [20], or with the dictionary-based algorithm of the Stanford CoreNLP library [16].", "startOffset": 135, "endOffset": 139}, {"referenceID": 14, "context": "Concretely, we replace every word in the input question and/or answer by its stem, obtained either with the classical Porter algorithm [20], or with the dictionary-based algorithm of the Stanford CoreNLP library [16].", "startOffset": 212, "endOffset": 216}, {"referenceID": 18, "context": "We obtain candidate detection from the YOLO method pretrained on Pascal VOC [21] as set of detections scores, each with the class of the detected object.", "startOffset": 76, "endOffset": 80}, {"referenceID": 22, "context": "for other vision and language tasks [25].", "startOffset": 36, "endOffset": 40}, {"referenceID": 22, "context": "Whereas our baseline uses a symmetric product to relate x and x, the idea of an order embedding is to place a hierarchy between the two modalities by measuring their compatibility with an antisymmetric operation (consult [25] for details).", "startOffset": 221, "endOffset": 225}, {"referenceID": 9, "context": "As observed before [11], this strategy is sufficient to achieve a high performance with the standard splits.", "startOffset": 19, "endOffset": 23}, {"referenceID": 14, "context": "The classical rule-based Porter algorithm performs worse than our baseline, and the improvements are obtained with a modern algorithm [16].", "startOffset": 134, "endOffset": 138}, {"referenceID": 13, "context": "LSTM Q+I [15] 52.", "startOffset": 9, "endOffset": 13}, {"referenceID": 4, "context": "6 \u2013 \u2013 \u2013 \u2013 MCB [6] 62.", "startOffset": 14, "endOffset": 17}, {"referenceID": 9, "context": "[11] 64.", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "[26]) which could be adapted here.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "Explicit object detections We use detections at different levels of recall from YOLO [21] by varying a threshold on the minimum detection score (a specific model is trained for a specific threshold).", "startOffset": 85, "endOffset": 89}, {"referenceID": 9, "context": "On the standard splits, our best model clearly surpasses the existing state-of-the-art on this dataset [11].", "startOffset": 103, "endOffset": 107}], "year": 2016, "abstractText": "Part of the appeal of Visual Question Answering (VQA) is its promise to answer new questions about previously unseen images. Most current methods demand training questions that illustrate every possible concept, and will therefore never achieve this capability, since the volume of required training data would be prohibitive. Answering general questions about images requires methods capable of Zero-Shot VQA, that is, methods able to answer questions beyond the scope of the training questions. We propose a new evaluation protocol for VQA methods which measures their ability to perform Zero-Shot VQA, and in doing so highlights significant practical deficiencies of current approaches, some of which are masked by the biases in current datasets. We propose and evaluate several strategies for achieving Zero-Shot VQA, including methods based on pretrained word embeddings, object classifiers with semantic embeddings, and test-time retrieval of example images. Our extensive experiments are intended to serve as baselines for Zero-Shot VQA, and they also achieve state-of-theart performance in the standard VQA evaluation setting.", "creator": "LaTeX with hyperref package"}}}