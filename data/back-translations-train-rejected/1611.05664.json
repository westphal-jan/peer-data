{"id": "1611.05664", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Nov-2016", "title": "Learning to detect and localize many objects from few examples", "abstract": "The current trend in object detection and localization is to learn predictions with high capacity deep neural networks trained on a very large amount of annotated data and using a high amount of processing power. In this work, we propose a new neural model which directly predicts bounding box coordinates. The particularity of our contribution lies in the local computations of predictions with a new form of local parameter sharing which keeps the overall amount of trainable parameters low. Key components of the model are spatial 2D-LSTM recurrent layers which convey contextual information between the regions of the image. We show that this model is more powerful than the state of the art in applications where training data is not as abundant as in the classical configuration of natural images and Imagenet/Pascal VOC tasks. We particularly target the detection of text in document images, but our method is not limited to this setting. The proposed model also facilitates the detection of many objects in a single image and can deal with inputs of variable sizes without resizing.", "histories": [["v1", "Thu, 17 Nov 2016 12:51:18 GMT  (754kb,D)", "http://arxiv.org/abs/1611.05664v1", null]], "reviews": [], "SUBJECTS": "cs.CV cs.AI", "authors": ["bastien moysset", "christoper kermorvant", "christian wolf"], "accepted": false, "id": "1611.05664"}, "pdf": {"name": "1611.05664.pdf", "metadata": {"source": "CRF", "title": "Learning to detect and localize many objects from few examples", "authors": ["Bastien Moysset", "Christoper Kermorvant", "Christian Wolf"], "emails": [], "sections": [{"heading": null, "text": "We show that this model is more powerful than the state-of-the-art in applications where training data is not as abundant as in the classical configuration of natural images and Imagenet / Pascal VOC tasks. We specifically target the recognition of text in document images, but our method is not limited to this setting. The proposed model also facilitates the detection of many objects in a single image and can handle input of different sizes without resizing."}, {"heading": "1. Introduction", "text": "This year, it has reached the point where it will be able to retaliate."}, {"heading": "1.1. Related work", "text": "Previous (pre-deep learning) work on object recognition was done by mapping local characteristics [17] or by splitting objects into mixtures of parts and solving combinatorial problems [7]. Early work on deep learning technology extended the approach to deep neuralization. To avoid a large number of positions and aspect gradients, R-CNN [9] carried out the concept suggestions, followed by revolutionary networks to classify each proposal. The concept was improved as Fast R-CNN [8] and Faster R-CNN."}, {"heading": "2. Delving again into convolutions, pooling,", "text": "This year, it will only take a few days to reach an agreement between the two countries."}, {"heading": "3. A local spatially recurrent model", "text": "We propose a new model for detecting a large number of (potentially) small objects from a small number of training examples, i.e. a model with a small number of traceable parameters. We achieve this using two techniques: A) Feature Sharing - we predict different object locations of local characteristics. Specifically, the starting layer of a single object surrounding a box is not fully connected to the previous layer, as illustrated in Figure 2c. Outputs are connected by 1 \u00d7 1 turns, i.e. each element (i, j) of the last characteristic card is connected to its own set of K output modules, each module consisting of relative x and y positions, width and height, and a confidence point number used to confirm the presence of an object at the predicted position. Objectives here are twofold: \u2022 To drastically reduce the number of parameters in the output layer, by significantly reducing parameters by eliminating starving layers in the image, we will significantly increase the displacement requirements."}, {"heading": "4. Training", "text": "The model is trained with stochastic gradient descent (SGD) using mini-stacks of size 8 and dropout for regulation. During the training, object predictors (network outputs) must be matched with targets, i.e. with basic truth object positions. Similar to the strategy in MultiBox [6], this is done globally over the entire image, allowing any Bounding Box predictor to respond to any location in an image. We refer to M as the number of predicted objects, since M = I * J * K, where I and J are the width and height of the last characteristic card, and K as the number of predictors per characteristic card location; we refer to N as the number of reference objects in the image. Matching is a combinatorial problem using matrix X, where Xnm = 1 when hypothesis m is matched with target n, and 0 otherwise. For each forward pass for each image X is estimated to minimize the following function."}, {"heading": "5. Experimental results", "text": "We tested the proposed model and baselines on the publicly available Maurdor dataset [4]. This highly heterogeneous dataset consists of 8773 illustrations of documents (train: 6592; valid: 1110; test: 1071) in mixed French, English and Arabic text, both handwritten and printed; the dataset is commented on at paragraph level. Therefore, we use the technique detailed in [3] to obtain line-level annotations, and we retain only those pages where we are satisfied that automatic line positioning has worked well. We receive a limited dataset with 3995 training pages, 697 validation pages and 616 test pages used for training, validation and testing to evaluate intersections via trade union and detEval metrics. For the Bag of Word metrics, we evaluate in English entirely on the 265 pages, and in the 507 pages on the full French mouse set, to avoid the mouse set classification."}, {"heading": "5.1. Metrics", "text": "We evaluate the performance of our method using three different metrics: Intersection over Unification - IoU is a commonly used metric in object recognition and image segmentation. It is given as the ratio of the intersection and the union of reference and hypotheses objects. Reference objects and hypotheses objects are matched by threshold of their IoU score. In the most common versions, only one hypothesis can be assigned to a reference field, the others are considered as errors / insertions. As an alternative to reporting IoU directly after threshold IoU, an F measurement of precision and recall can be calculated. DetEval - DetEval [31] is the metric chosen for the ICDAR-robust reading series of competitions. Its main advantage is that it allows many-too many matches between reference objects and hypotheses objects, which is important in applications where the fragmentation of objects should be allowed (and should be easily punished by the case)."}, {"heading": "5.2. Baselines", "text": "Traditional Text Segmentation Methods - For comparison, we used two text line segmentation techniques based on image processing (without machine learning). Shi et al. [28] use controllable directional filters to create an adaptive local connectivity map. Line positions are determined by the positions of the connected components extracted from the binarization of this connectivity map. These positions are refined by heuristic post-processing. Nicolaou et al. [19] \"s proposed method follows the whitest and blackest paths in a blurred image to find lines and interlines. Yolo and MultiBox - For YOLO, we used two classes for the object classification part of the model, handwritten text lines and printed text lines. It helped the model learn better than without classification. Both systems were tested in two different configurations: the original architecture, which was adapted to large-area image classification, handwritten text lines and printed text lines."}, {"heading": "5.3. Architectures", "text": "The calculated hyperparameters are detailed in Table 1. The width and height of the characteristic cards is given for illustration, but may of course vary. In particular, the aspect ratio of the image may vary. We would like to emphasize once again that the number of parameters is independent of the actual size of the input image. The inputs of our network are raw gray-scale images with width normalization. The use of color images did not improve the results of our task. Note that the number of weights in our last layer, the position prediction layer, is rather low: 3,700. To be able to predict the same number of objects with the same number of input functions, MultiBox [6] and Yolo [26] would have needed 15,688,200 parameters. For the training we used a learning rate of 10 \u2212 4 and minibatches of size 8. Dropout with 0.5 probability."}, {"heading": "5.4. Results and discussion", "text": "This year, most of them are not yet able to play by the rules."}, {"heading": "5.5. Implementation", "text": "For the implementation of the proposed method no deep learning framework was used, as until recently and in the Theano version of Doetsch et al. [5] there was no 2D LSTM implementation in Tensorflow, Torch, Theano or Caffe. The system was implemented with our internal framework, which was implemented in C + +, including SG optimizer, dropout, etc. Also for this reason the model was trained on CPUs. For YOLO we used two different implementations: We implemented and trained our own implementation in Tensorflow and we also used the official source code published by authors 2."}, {"heading": "6. Conclusion", "text": "To optimize the invariance and limit the number of traceable parameters, we divided parameters of the output layer across spatial image blocks and implemented the output layer as a 1 x 1 fold. To deal with objects larger than the receptive field, and in order2http: / / pjreddie.com / darknet / yoloto, we enabled the model to collect features from the global context, we added 2D-LSTM layers between the revolutionary layers. We compared the proposed model with the state of the art in object recognition, in particular with YOLO [26] and Multibox [6]. We measured the recognition performance and word recognition performance of a subsequent classifier. Our experiments showed that the proposed model significantly exceeds both methods, even if its hyperparameters are optimized for targeted configuration."}], "references": [{"title": "Inside-outside net: Detecting objects in context with skip pooling and recurrent neural networks", "author": ["S. Bell", "L. Zitnick", "K. Bala", "R. Girshick"], "venue": "In IEEE Conf. on Computer Vision and Pattern Recognition,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2016}, {"title": "Joint line segmentation and transcription for endto-end handwritten paragraph recognition", "author": ["T. Bluche"], "venue": "In Advances in Neural Information Processing System,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2016}, {"title": "Automatic line segmentation and ground-truth alignment of handwritten documents", "author": ["T. Bluche", "B. Moysset", "C. Kermorvant"], "venue": "In Int. Conf. on Frontiers in Handwriting Recognition,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2014}, {"title": "The maurdor project - improving automatic processing of digital documents", "author": ["S. Brunessaux", "P. Giroux", "B. Grilheres", "M. Manta", "M. Bodin", "K. Choukri", "O. Galibert", "J. Kahn"], "venue": "In Document Analysis Systems,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2014}, {"title": "Returnn: The rwth extensible training framework for universal recurrent neural networks", "author": ["P. Doetsch", "A. Zeyer", "P. Voigtlaender", "I. Kulikov", "R. Schl\u00fcter", "H. Ney"], "venue": "arXiv preprint arXiv:1608.00895,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2016}, {"title": "Scalable object detection using deep neural networks", "author": ["D. Erhan", "C. Szegedy", "A. Toshev", "D. Anguelov"], "venue": "In IEEE Conf. on Computer Vision and Pattern Recognition,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2014}, {"title": "Object detection with discriminatively trained partbased models", "author": ["P.F. Felzenszwalb", "R.B. Girshick", "D. McAllester", "D. Ramanan"], "venue": "IEEE Trans. on Pattern Analysis and Machine Intelligence,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2010}, {"title": "Fast R-CNN", "author": ["R. Girshick"], "venue": "In Int. Conf. on Computer Vision,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2015}, {"title": "Rich feature hierarchies for accurate object detection and semantic segmentation", "author": ["R. Girshick", "J. Donahue", "T. Darrell", "J. Malik"], "venue": "In IEEE Conf. on Computer Vision and Pattern Recognition,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2014}, {"title": "Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks", "author": ["A. Graves", "S. Fernandez", "F. Gomez", "J. Schmidhuber"], "venue": "In Int. Conf. on Machine Learning,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2006}, {"title": "Offline handwriting recognition with multidimensional recurrent neural networks", "author": ["A. Graves", "J. Schmidhuber"], "venue": "In Advances in Neural Information Processing System,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2008}, {"title": "Offline handwriting recognition with multidimensional recurrent neural networks", "author": ["A. Graves", "J. Schmidhuber"], "venue": "In NIPS,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2009}, {"title": "Synthetic data for text localisation in natural images", "author": ["A. Gupta", "A. Vedaldi", "A. Zisserman"], "venue": "In IEEE Conf. on Computer Vision and Pattern Recognition,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2016}, {"title": "Squeezenet: Alexnet-level accuracy with 50\u00d7 fewer parameters and <0.5MB model size", "author": ["F. Iandola", "S. Hand", "M. Moskewicz", "K. Ashraf"], "venue": "In Openreview submission to ICLR 2017,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2016}, {"title": "Ssd: Single shot multibox detector", "author": ["W. Liu", "D. Anguelov", "D. Erhan", "C. Szegedy", "S. Reed", "C.-Y. Fu", "A. Berg"], "venue": "In European Conference on Computer Vision, 2016", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2016}, {"title": "Fully convolutional networks for semantic segmentation", "author": ["J. Long", "E. Shelhamer", "T. Darrell"], "venue": "In IEEE Conf. on Computer Vision and Pattern Recognition,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2015}, {"title": "Distinctive image features from scale-invariant keypoints", "author": ["D. Lowe"], "venue": "Int. Journal of Computer Visioon,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2004}, {"title": "Algorithms for the assignment and transportation problems", "author": ["J. Munkres"], "venue": "Journal of the Society for Industrial and Applied Mathematics,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1957}, {"title": "Handwritten Text Line Segmentation by Shredding Text into its Lines", "author": ["A. Nicolaou", "B. Gatos"], "venue": "In Int. Conf. on Document Analysis and Recognition,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2009}, {"title": "Dropout improves recurrent neural networks for handwriting recognition", "author": ["V. Pham", "T. Bluche", "C. Kermorvant", "J. Louradour"], "venue": "In Int. Conf. on Frontiers in Handwriting Recognition,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2014}, {"title": "Learning to refine object segments", "author": ["P. Pinheiro", "T. Lin", "R. Collobert", "P. Dollar"], "venue": "In European Conference on Computer Vision,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2016}, {"title": "From image-level to pixel-level labeling with convolutional networks", "author": ["P. Pinheiro", "R. Collobert"], "venue": "In CVPR,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2015}, {"title": "Learning to segment object candidates", "author": ["P. Pinheiro", "R. Collobert", "P. Dollar"], "venue": "In Advances in Neural Information Processing System,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2015}, {"title": "Europeana newspapers ocr workflow evaluation", "author": ["S. Pletschacher", "C. Clausner", "A. Antonacopoulos"], "venue": "In Workshop on Historical Document Imaging and Processing,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2015}, {"title": "Label-free supervision of neural networks with physics and domain knowledge", "author": ["S.E.R. Stewart"], "venue": "Pre-print: arXiv:1609.05566,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2016}, {"title": "You only look once: Unified, real-time object detection", "author": ["J. Redmon", "S. Divvala", "R. Girshick", "A. Farhadi"], "venue": "In IEEE Conf. on Computer Vision and Pattern Recognition, June 2016", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2016}, {"title": "Faster R-CNN: Towards real-time object detection with region proposal networks", "author": ["S. Ren", "K. He", "R. Girshick", "J. Sun"], "venue": "In Advances in Neural Information Processing System,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2015}, {"title": "A Steerable Directional Local Profile Technique for Extraction of Handwritten Arabic Text Lines", "author": ["Z. Shi", "S. Setlur", "V. Govindaraju"], "venue": "In Int. Conf. on Document Analysis and Recognition,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2009}, {"title": "Scalable, high-quality object detection", "author": ["C. Szegedy", "S. Reed", "D. Erhan", "D. Anguelov"], "venue": "Pre-print: arXiv:1412.1441,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2015}, {"title": "Matching networks for one shot learning", "author": ["O. Vinyals", "C. Blundell", "K. Kavukcuoglu", "T. Lillicrap", "D. Wierstra"], "venue": "arXiv preprint arXiv:1606.0408,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2016}, {"title": "Object count/Area Graphs for the Evaluation of Object Detection and Segmentation Algorithms", "author": ["C. Wolf", "J.-M. Jolion"], "venue": "Int. Journ. on Document Analysis and Recognition,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2006}, {"title": "Scene labeling with lstm recurrent neural networks", "author": ["W. Wonmin", "T. Breuel", "F. Raue", "M. Liwicki"], "venue": "In IEEE Conf. on Computer Vision and Pattern Recognition,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2015}, {"title": "Multi-scale context aggregation by dilated convolutions", "author": ["F. Yu", "V. Koltun"], "venue": "In Int. Conf. on Learning Representations,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2016}, {"title": "Multi-oriented text detection with fully convolutional networks. preprint arXiv:1604.04018, 2016", "author": ["Z. Zhang", "C. Zhang", "W. Shen", "C. Yao", "W. Liu", "X. Bai"], "venue": null, "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2016}], "referenceMentions": [{"referenceID": 8, "context": "Object detection and localization in images is currently dominated by approaches which first create proposals (hypothesis bounding boxes) followed by feature extraction and pooling on these boxes and classification, the latter steps being usually performed by deep networks [9, 8, 27, 26, 1].", "startOffset": 274, "endOffset": 291}, {"referenceID": 7, "context": "Object detection and localization in images is currently dominated by approaches which first create proposals (hypothesis bounding boxes) followed by feature extraction and pooling on these boxes and classification, the latter steps being usually performed by deep networks [9, 8, 27, 26, 1].", "startOffset": 274, "endOffset": 291}, {"referenceID": 26, "context": "Object detection and localization in images is currently dominated by approaches which first create proposals (hypothesis bounding boxes) followed by feature extraction and pooling on these boxes and classification, the latter steps being usually performed by deep networks [9, 8, 27, 26, 1].", "startOffset": 274, "endOffset": 291}, {"referenceID": 25, "context": "Object detection and localization in images is currently dominated by approaches which first create proposals (hypothesis bounding boxes) followed by feature extraction and pooling on these boxes and classification, the latter steps being usually performed by deep networks [9, 8, 27, 26, 1].", "startOffset": 274, "endOffset": 291}, {"referenceID": 0, "context": "Object detection and localization in images is currently dominated by approaches which first create proposals (hypothesis bounding boxes) followed by feature extraction and pooling on these boxes and classification, the latter steps being usually performed by deep networks [9, 8, 27, 26, 1].", "startOffset": 274, "endOffset": 291}, {"referenceID": 26, "context": "Very recent methods also use deep networks for the proposal step [27, 26, 15], sometimes sharing features between localization and classification.", "startOffset": 65, "endOffset": 77}, {"referenceID": 25, "context": "Very recent methods also use deep networks for the proposal step [27, 26, 15], sometimes sharing features between localization and classification.", "startOffset": 65, "endOffset": 77}, {"referenceID": 14, "context": "Very recent methods also use deep networks for the proposal step [27, 26, 15], sometimes sharing features between localization and classification.", "startOffset": 65, "endOffset": 77}, {"referenceID": 8, "context": "R-CNN [9]), leaving the burden of validation to a subsequent classifier.", "startOffset": 6, "endOffset": 9}, {"referenceID": 0, "context": "\u2022 we allow for the detection and localization of a relatively high number of potentially small objects in an image, which is especially hard for existing methods [1].", "startOffset": 162, "endOffset": 165}, {"referenceID": 5, "context": "(a) Multibox [6]; (b) Yolo [26] and SSD [15]; these three models pass through a fully connected layer, which connects to a set of bounding box outputs; architecture wise, these three models are identical.", "startOffset": 13, "endOffset": 16}, {"referenceID": 25, "context": "(a) Multibox [6]; (b) Yolo [26] and SSD [15]; these three models pass through a fully connected layer, which connects to a set of bounding box outputs; architecture wise, these three models are identical.", "startOffset": 27, "endOffset": 31}, {"referenceID": 14, "context": "(a) Multibox [6]; (b) Yolo [26] and SSD [15]; these three models pass through a fully connected layer, which connects to a set of bounding box outputs; architecture wise, these three models are identical.", "startOffset": 40, "endOffset": 44}, {"referenceID": 25, "context": "Similar to models like YOLO [26] and Single-Shot Detector [15], our outputs are assigned to local regions of the image.", "startOffset": 28, "endOffset": 32}, {"referenceID": 14, "context": "Similar to models like YOLO [26] and Single-Shot Detector [15], our outputs are assigned to local regions of the image.", "startOffset": 58, "endOffset": 62}, {"referenceID": 16, "context": "Earlier (pre-deep learning) work on object recognition proceeded through matching of local features [17] or by decomposing objects into mixtures of parts and solving combinatorial problems [7].", "startOffset": 100, "endOffset": 104}, {"referenceID": 6, "context": "Earlier (pre-deep learning) work on object recognition proceeded through matching of local features [17] or by decomposing objects into mixtures of parts and solving combinatorial problems [7].", "startOffset": 189, "endOffset": 192}, {"referenceID": 8, "context": "aspect ratios, R-CNN [9] introduced the concept object proposals, created by separate methods, followed by convolutional networks to classify each proposal.", "startOffset": 21, "endOffset": 24}, {"referenceID": 7, "context": "The concept was improved as Fast R-CNN [8] and Faster R-CNN [27].", "startOffset": 39, "endOffset": 42}, {"referenceID": 26, "context": "The concept was improved as Fast R-CNN [8] and Faster R-CNN [27].", "startOffset": 60, "endOffset": 64}, {"referenceID": 5, "context": "proposed Multibox [6, 29], which performs direct regression of bounding box locations instead of relying on object proposals.", "startOffset": 18, "endOffset": 25}, {"referenceID": 28, "context": "proposed Multibox [6, 29], which performs direct regression of bounding box locations instead of relying on object proposals.", "startOffset": 18, "endOffset": 25}, {"referenceID": 25, "context": "YOLO [26] and the Single-Shot Detector [15] can be seen as variants of this concept, they will be discussed in more detail in section 2.", "startOffset": 5, "endOffset": 9}, {"referenceID": 14, "context": "YOLO [26] and the Single-Shot Detector [15] can be seen as variants of this concept, they will be discussed in more detail in section 2.", "startOffset": 39, "endOffset": 43}, {"referenceID": 22, "context": "Some recent work strives to detect and localize objects with pixel-wise precision, which somewhat blurs the boundaries between object detection and semantic segmentation [23, 21].", "startOffset": 170, "endOffset": 178}, {"referenceID": 20, "context": "Some recent work strives to detect and localize objects with pixel-wise precision, which somewhat blurs the boundaries between object detection and semantic segmentation [23, 21].", "startOffset": 170, "endOffset": 178}, {"referenceID": 21, "context": "Methods which learn to segment without pixelwise ground truth have also been proposed [22].", "startOffset": 86, "endOffset": 90}, {"referenceID": 11, "context": "Context through spatial 2D-recurrent networks has been proposed as early as in [12].", "startOffset": 79, "endOffset": 83}, {"referenceID": 0, "context": "Similarly to our method, inside-Outside-Nets [1] contain 2D spatial context layers collecting information from 4 different directions.", "startOffset": 45, "endOffset": 48}, {"referenceID": 31, "context": "Other recent work uses 2D recurrent networks for semantic segmentation [32].", "startOffset": 71, "endOffset": 75}, {"referenceID": 33, "context": "CNNs have been used before for text detection, for instance in [34], a Fully convolutional network (FCN) is used to classify each position of a salient map as text or nontext.", "startOffset": 63, "endOffset": 67}, {"referenceID": 12, "context": "In [13], a YOLO-related method is proposed for the detection of text in natural images but only few objects are present in the images.", "startOffset": 3, "endOffset": 7}, {"referenceID": 29, "context": "The problem of dataset sizes has been addressed before, with strategies reaching from external memories [30] and unsupervised learning, for instance by learning feature extraction from physics [25].", "startOffset": 104, "endOffset": 108}, {"referenceID": 24, "context": "The problem of dataset sizes has been addressed before, with strategies reaching from external memories [30] and unsupervised learning, for instance by learning feature extraction from physics [25].", "startOffset": 193, "endOffset": 197}, {"referenceID": 13, "context": "Choosing when to pool and to reduce can be critical, and optimizations can lead to large decreases in the numbers of trainable parameters [14].", "startOffset": 138, "endOffset": 142}, {"referenceID": 32, "context": "An alternative to in-betweenlayer pooling is changing the size of filters, especially as \u201ca trou\u201d computation in order to keep the number of parameters low [33].", "startOffset": 156, "endOffset": 160}, {"referenceID": 15, "context": "1\u00d71, with a high feature dimension (1\u00d71\u00d74096 in the network for semantic segmentation proposed in [16]).", "startOffset": 98, "endOffset": 102}, {"referenceID": 5, "context": "Several state of the art models, Multibox [6], YOLO [26] and Single-Shot Detector (SSD) [15] tackle this through an architecture sketched in figure 2a and 2b1.", "startOffset": 42, "endOffset": 45}, {"referenceID": 25, "context": "Several state of the art models, Multibox [6], YOLO [26] and Single-Shot Detector (SSD) [15] tackle this through an architecture sketched in figure 2a and 2b1.", "startOffset": 52, "endOffset": 56}, {"referenceID": 14, "context": "Several state of the art models, Multibox [6], YOLO [26] and Single-Shot Detector (SSD) [15] tackle this through an architecture sketched in figure 2a and 2b1.", "startOffset": 88, "endOffset": 92}, {"referenceID": 0, "context": "In contrast to [1], we use real LSTM models with trainable transition matrices.", "startOffset": 15, "endOffset": 18}, {"referenceID": 25, "context": "(7\u00d77 for [26], 9\u00d79 for [15]).", "startOffset": 9, "endOffset": 13}, {"referenceID": 14, "context": "(7\u00d77 for [26], 9\u00d79 for [15]).", "startOffset": 23, "endOffset": 27}, {"referenceID": 5, "context": "Although there is no principled difference in how the last fully connected layer is actually implemented in the three models, we display the output layer differently for Multibox [6] (figure 2a) and for YOLO [26] or SSD [15] (figure 2b).", "startOffset": 179, "endOffset": 182}, {"referenceID": 25, "context": "Although there is no principled difference in how the last fully connected layer is actually implemented in the three models, we display the output layer differently for Multibox [6] (figure 2a) and for YOLO [26] or SSD [15] (figure 2b).", "startOffset": 208, "endOffset": 212}, {"referenceID": 14, "context": "Although there is no principled difference in how the last fully connected layer is actually implemented in the three models, we display the output layer differently for Multibox [6] (figure 2a) and for YOLO [26] or SSD [15] (figure 2b).", "startOffset": 220, "endOffset": 224}, {"referenceID": 25, "context": "For the latter two, and also in accordance with the figures of the respective papers, the outputs are shown as a spatial grid (7\u00d77 for [26], 9\u00d79 for [15]).", "startOffset": 135, "endOffset": 139}, {"referenceID": 14, "context": "For the latter two, and also in accordance with the figures of the respective papers, the outputs are shown as a spatial grid (7\u00d77 for [26], 9\u00d79 for [15]).", "startOffset": 149, "endOffset": 153}, {"referenceID": 25, "context": "In the case of [26], this assignment is purely spatial: outputs of a given cell are trained to provide predictions of a spatial region corresponding to this cell (see section 4).", "startOffset": 15, "endOffset": 19}, {"referenceID": 14, "context": "For SSD[15] , we do not show the way how this model handles multiple scales.", "startOffset": 7, "endOffset": 11}, {"referenceID": 10, "context": "We address both these concerns through context layers consisting of Multi-Dimensional Long-Short term memory models [11], which are inserted between the convolutional layers.", "startOffset": 116, "endOffset": 120}, {"referenceID": 5, "context": "Similar to the strategy in MultiBox [6], this is done globally over the entire image, which allows each bounding box predictor to respond to any location in an image.", "startOffset": 36, "endOffset": 39}, {"referenceID": 17, "context": "This is a well known bi-partite graph matching problem, which can be solved with the Hungarian algorithm [18].", "startOffset": 105, "endOffset": 109}, {"referenceID": 5, "context": "As mentioned earlier, our matching strategy is similar to the one described in MultiBox [6] and has the same global property brought by the confidence term (albeit applied to local outputs, compared to the global outputs in [6]).", "startOffset": 88, "endOffset": 91}, {"referenceID": 5, "context": "As mentioned earlier, our matching strategy is similar to the one described in MultiBox [6] and has the same global property brought by the confidence term (albeit applied to local outputs, compared to the global outputs in [6]).", "startOffset": 224, "endOffset": 227}, {"referenceID": 14, "context": "On the other hand, in SSD[15] and YOLO[26], matching is done locally, i.", "startOffset": 25, "endOffset": 29}, {"referenceID": 25, "context": "On the other hand, in SSD[15] and YOLO[26], matching is done locally, i.", "startOffset": 38, "endOffset": 42}, {"referenceID": 14, "context": "In SSD and MultiBox, the matching process is restricted to a fixed dictionary of anchor locations obtained arbitrarily[15] or with clustering[6], which helps the network to create outputs specialized to regions in the image.", "startOffset": 118, "endOffset": 122}, {"referenceID": 5, "context": "In SSD and MultiBox, the matching process is restricted to a fixed dictionary of anchor locations obtained arbitrarily[15] or with clustering[6], which helps the network to create outputs specialized to regions in the image.", "startOffset": 141, "endOffset": 144}, {"referenceID": 3, "context": "We tested the proposed model and the baselines on the publicly available Maurdor dataset [4].", "startOffset": 89, "endOffset": 92}, {"referenceID": 2, "context": "For this reason, we use the technique detailed in [3] to get annotation", "startOffset": 50, "endOffset": 53}, {"referenceID": 30, "context": "DetEval \u2014 DetEval [31] is the metric chosen for the ICDAR robust reading series of competitions.", "startOffset": 18, "endOffset": 22}, {"referenceID": 23, "context": "Bag-of-words recognition error \u2014 BoW is a goal oriented method described in [24], which measures the performance of a subsequent text recognition module.", "startOffset": 76, "endOffset": 80}, {"referenceID": 19, "context": "We use the recognition model from [20], which is based on deep convolutional networks and spatial/sequence modelling with 2D-LSTM layers.", "startOffset": 34, "endOffset": 38}, {"referenceID": 9, "context": "Assigning character labels to network outputs is performed with the Connectionist Temporal Classification framework [10].", "startOffset": 116, "endOffset": 120}, {"referenceID": 1, "context": "Recent follow-up work solves this problem with attention based mechanisms [2], this will be investigated in future work.", "startOffset": 74, "endOffset": 77}, {"referenceID": 27, "context": "[28] use steerable directional filters to create an adaptive local connectivity map.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "[19] follows the whitest and blackest paths in a blurred image to find lines and interlines.", "startOffset": 0, "endOffset": 4}, {"referenceID": 5, "context": "To be able to predict the same number of objects, with the same number of input features, MultiBox[6] and Yolo[26] would have needed 15,688,200 parameters.", "startOffset": 98, "endOffset": 101}, {"referenceID": 25, "context": "To be able to predict the same number of objects, with the same number of input features, MultiBox[6] and Yolo[26] would have needed 15,688,200 parameters.", "startOffset": 110, "endOffset": 114}, {"referenceID": 27, "context": "[28] 40.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "[19] 36.", "startOffset": 0, "endOffset": 4}, {"referenceID": 5, "context": "Multibox [6] 11.", "startOffset": 9, "endOffset": 12}, {"referenceID": 5, "context": "2% Multibox [6] (optimized) 48.", "startOffset": 12, "endOffset": 15}, {"referenceID": 30, "context": "Detection performance with detEval[31].", "startOffset": 34, "endOffset": 38}, {"referenceID": 27, "context": "[28] 35.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "[19] 46.", "startOffset": 0, "endOffset": 4}, {"referenceID": 5, "context": "Multibox [6] 4.", "startOffset": 9, "endOffset": 12}, {"referenceID": 5, "context": "0% Multibox [6] (optimized) 28.", "startOffset": 12, "endOffset": 15}, {"referenceID": 27, "context": "[28] 48.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "[19] 65.", "startOffset": 0, "endOffset": 4}, {"referenceID": 5, "context": "Multibox [6] 27.", "startOffset": 9, "endOffset": 12}, {"referenceID": 5, "context": "8% Multibox [6] (optimized) 32.", "startOffset": 12, "endOffset": 15}, {"referenceID": 27, "context": "[28] and Nicolaou et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "[19] perform poorly when the threshold is low, i.", "startOffset": 0, "endOffset": 4}, {"referenceID": 5, "context": "On the other hand, methods based on direct regression as MultiBox [6] and our proposed method are more robust and achieve better general recall, an advantage which is bought with a slight drop in precision.", "startOffset": 66, "endOffset": 69}, {"referenceID": 5, "context": "Multibox [6] is significantly outperformed by our method, even if we optimize its hyper-parameters (on the validation set).", "startOffset": 9, "endOffset": 12}, {"referenceID": 25, "context": "YOLO [26] proved to be impossible to apply to this kind of problem, at least in its current shape.", "startOffset": 5, "endOffset": 9}, {"referenceID": 4, "context": "[5], no 2D-LSTMs implementation was, up to our knowledge, yet existing in Tensorflow, Torch, Theano or Caffe.", "startOffset": 0, "endOffset": 3}, {"referenceID": 25, "context": "We compared the proposed model to the state of the art in object detection, in particular to YOLO [26] and Multibox [6].", "startOffset": 98, "endOffset": 102}, {"referenceID": 5, "context": "We compared the proposed model to the state of the art in object detection, in particular to YOLO [26] and Multibox [6].", "startOffset": 116, "endOffset": 119}], "year": 2016, "abstractText": "The current trend in object detection and localization is to learn predictions with high capacity deep neural networks trained on a very large amount of annotated data and using a high amount of processing power. In this work, we propose a new neural model which directly predicts bounding box coordinates. The particularity of our contribution lies in the local computations of predictions with a new form of local parameter sharing which keeps the overall amount of trainable parameters low. Key components of the model are spatial 2D-LSTM recurrent layers which convey contextual information between the regions of the image. We show that this model is more powerful than the state of the art in applications where training data is not as abundant as in the classical configuration of natural images and Imagenet/Pascal VOC tasks. We particularly target the detection of text in document images, but our method is not limited to this setting. The proposed model also facilitates the detection of many objects in a single image and can deal with inputs of variable sizes without resizing.", "creator": "LaTeX with hyperref package"}}}