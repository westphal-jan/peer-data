{"id": "1611.05675", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Nov-2016", "title": "Study on Feature Subspace of Archetypal Emotions for Speech Emotion Recognition", "abstract": "Feature subspace selection is an important part in speech emotion recognition. Most of the studies are devoted to finding a feature subspace for representing all emotions. However, some studies have indicated that the features associated with different emotions are not exactly the same. Hence, traditional methods may fail to distinguish some of the emotions with just one global feature subspace. In this work, we propose a new divide and conquer idea to solve the problem. First, the feature subspaces are constructed for all the combinations of every two different emotions (emotion-pair). Bi-classifiers are then trained on these feature subspaces respectively. The final emotion recognition result is derived by the voting and competition method. Experimental results demonstrate that the proposed method can get better results than the traditional multi-classification method.", "histories": [["v1", "Thu, 17 Nov 2016 13:32:59 GMT  (527kb)", "http://arxiv.org/abs/1611.05675v1", "5 pages, 4 figures, ICASSP-2017"]], "COMMENTS": "5 pages, 4 figures, ICASSP-2017", "reviews": [], "SUBJECTS": "cs.LG cs.AI", "authors": ["xi ma", "zhiyong wu", "jia jia", "mingxing xu", "helen meng", "lianhong cai"], "accepted": false, "id": "1611.05675"}, "pdf": {"name": "1611.05675.pdf", "metadata": {"source": "CRF", "title": "STUDY ON FEATURE SUBSPACE OF ARCHETYPAL EMOTIONS FOR SPEECH EMOTION RECOGNITION", "authors": ["Xi Ma", "Zhiyong Wu", "Jia Jia", "Mingxing Xu", "Helen Meng", "Lianhong Cai"], "emails": ["max15@mails.tsinghua.edu.cn,", "zywu@se.cuhk.edu.hk,", "hmmeng@se.cuhk.edu.hk,", "jjia@tsinghua.edu.cn", "xumx@tsinghua.edu.cn", "clh-dcs@tsinghua.edu.cn"], "sections": [{"heading": null, "text": "ar Xiv: 161 1.05 675v 1 [cs.L G] 17 NIndex Terms - language emotion recognition, feature subspace, emotion pair"}, {"heading": "1. INTRODUCTION", "text": "Emotional recognition plays an important role in many applications, especially in human-computer interaction systems, which are becoming increasingly common nowadays. As one of the most important means of communication between people, voice has attracted a great deal of attention among researchers [1]. Language contains a wealth of emotional information on how to extract such information from the original speech signal is of great importance for the recognition of language feelings. As an important part of emotion recognition, the selection of feature subspaces has aroused many research interests. Existing research on the subspace selection of features can be divided into three categories, including the artificial selection of emotion-related features, the automatic feature selection algorithms for selecting feature subgroups from a large number of feature candidates, and the transformation method for mapping the original feature space in favor of emotion recognition. Most of this research is dedicated to the search for a common and global feature subspace, which can represent all types of emotions most strongly, but the studies have shown that different emotions are distinct from each other exactly."}, {"heading": "2. RELATED WORK", "text": "As a common problem for many classification problems [2], feature selection aims to select a subset of features that are most relevant for the target concept [3] or to reduce the dimension of features to reduce computing time and improve performance [4]. There are many studies on feature selection for the recognition of speech sensations. Prosode-based acoustic features, including pitch-related, energy-related and time-related characteristics, are frequently used for the recognition of speech sensations in [5-7]. Spectral-based acoustic features also play an important role in emotion recognition, such as linear predictor coefficients (LPC) [8], linear predictor dimension coefficients (LPCC) [9] and Mel frequency Cepstral coefficients (MFCC) [10]. In [11], language quality characteristics that are also suggested in relation to the selection algorithms of emotion are also different."}, {"heading": "3. METHOD", "text": "Our study is based on archetypal emotions. The emotional pair consists of two different types of archetypal emotions, such as anger and happiness. For all possible combinations of archetypal emotional pairs, the bi-classification and matching method is used to distinguish each emotion pair and to derive the final emotion recognition result. As shown in Figure 1, the entire method comprises four steps: feature extraction, subspace selection, emotion classification and choice."}, {"heading": "3.1. Feature Extraction", "text": "The acoustic characteristics used include the following low-level descriptors (LLDs): intensity, volume, 12 MFCC, pitch (F0), intonation probability, envelope F0, 8 SPF (Line Spectral Frequencies), zero crossing rate. From these LLDs delta regression coefficients are calculated and the following statistical functions are applied to the LLDs and delta coefficients: max. / min. and the respective relative position within input, range, arithmetic mean, 2 linear regression coefficients and linear and quadratic errors, standard deviation, skew, kurtosis, quartile 1-3 and 3 interquartile ranges. All features are expression-level characteristics. In the selection stage, the relevant characteristics or the relevant feature space are derived from the above large feature set."}, {"heading": "3.2. Feature Subspace Selection", "text": "Unlike traditional methods, which distinguish all emotions with only one global trait subspace, this work selects different trait subspaces for different combinations of emotion pairs. For a particular emotion pair, the corresponding trait subspace should be the best performance in distinguishing the two emotions of the pair. To verify the universality of our idea, the methods of trait subset selection and trait-space transformation were taken into account. Genetic algorithm (GA) is used for trait subset selection, while neural network (NN) is used for trait-space transformation.GA is a kind of stochastic search and optimization algorithm that simulates the natural evolutionary process. We use a fixed number of traits to form a vector (the so-called individual), and a fixed number of individuals to form the first population. Crossover and mutation operations are then used to calculate a new individual's fitness level by comparing the fitness level to the new fitness level that is generated by generating a large population."}, {"heading": "3.3. Emotion Classification", "text": "By using the attribute subspace obtained in the previous step, a particular classifier can be trained for a particular emotion pair and determined to distinguish the emotions in that emotion pair. Since each classifier is related to only one particular emotion pair, we call it Bi-Classifier. Two basic classification algorithms are used to select characteristics, including logistic regression (LR) and support vector machine (SVM). For the transformation of characteristics, the Neural Network (NN) is used as classifier."}, {"heading": "3.4. Voting Decision", "text": "After the emotion differentiation result for each emotion pair has been determined in the previous emotion classification step, a matching and competition method is finally used to integrate the emotion classification results for all emotion pairs in order to derive the final emotion-recognition result. The selection decision process will be performed in algorithm 1. It can be proved that the final emotion recognition result can be correctly derived by the above-mentioned selection decision algorithm, since all bi-classifiers provide the correct differentiation result for each emotion pair. Theorem 1. Theorem 1. Voting decision will be able to derive the correct result, since all bi-classifiers are in the correct situation. Proof. In view of the symbol definitions in algorithm 1, the goal of emotion and ei-E. Thus, R is correct: nei = M nej \u2212 j < j = \u21d2 K \u21d2 = \u21d2 K = \u2212 i: K = 6 \u2212 i: K = 1: K-1 = K-1-1 = K-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-"}, {"heading": "4. EXPERIMENT", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1. Experimental Setting", "text": "In this study, we used the well-known Berlin Emotional Database (EmoDB) [18]. Ten actors (5 male and 5 female) simulated the emotions and produced 10 German sentences (5 short and 5 longer). EmoDB comprises 535 expressions covering 6 archetypal emotions and 1 neutral emotion from everyday communication, namely anger, fear, happiness, sadness, disgust, boredom and neutrality. Our work focuses on independent emotion recognition of the speaker, so the samples of 8 actors (4 male and 4 female) are used as a training kit, and the samples from the other 2 actors (1 male and 1 female) are used as a test kit. The 5x cross-validation method is used to perform the experiments. OpenSmile Toolkit [19] is used to extract acoustic characteristics, and a total of 988 features are associated with the experimental unit."}, {"heading": "4.2. Experimental Result", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.2.1. Feature Selection Method", "text": "First, we perform the function selection experiment by comparing the degree of similarity of the function subram for each emotion pair and the global function subram for all emotions. Figure 2 shows the number of common features (vertical axis) that are shared between the function subram for a particular emotion pair and the global function subram for all emotions, with the horizontal axis representing different emotional pairs (e.g. N-A is the Neutral-Anger pair). It should be noted that all function subspaces (including emotion-specific function subspaces and the global function subram for all emotions) contain 50 selected features as described in Section 4.1. The figure shows that the number of common features between each emotion pair and all emotions is no more than 5 (5 of 50), indicating that the function subram of each emotion pair for each emotion space is different from the global function subram for each emotion space."}, {"heading": "4.2.2. Feature Space Transformation", "text": "Similarly, we are also conducting experiments in the scenario of Feature Space Transformation to validate the efficiency of our proposed Divide and Conquer idea for emotion recognition. The accuracy of emotion recognition (recognition rate) by using different classification criteria in Feature Space Transformation is presented in Table 2, and the recognition of different emotions is illustrated in Figure 4.From the experimental results we can draw the same conclusion as the feature selection method. It is stated that Table 2. Comparison of emotion recognition accuracy by using Feature Space Transformation. (P < 0.05) Neural Network Bi-Classification and Voting 0.652Multi-Classification 0.552Neutral Anger Boredom Fear Happiness Sadnessemotion00.51ac cura cyBi-clf and Voting (NN) Multi-clf (NN) Figure 4. Emotion detection of different emotions for different classification methods of Space is also effective in the use of Feature Transformation and Voting (Bi) Feature. \""}, {"heading": "5. CONCLUSION", "text": "In this paper, we present a \"bi-classification and matching\" method by distinguishing different pairs of emotions in different trait spaces. Experimental results have shown that this method can achieve better results than the traditional multi-class classification method. Furthermore, our method is a kind of divide-and-conquer algorithm that transforms a complex multi-class problem into many simple two-class problems. This idea makes it possible to increase the performance of emotion recognition in several classes by optimizing the emotion classification for each emotion pair. Therefore, in the future, we will dedicate ourselves to optimizing different emotion pairs."}, {"heading": "6. ACKNOWLEDGEMENT", "text": "This work is supported by the National High Technology Research and Development Program of China (2015AA016305), the National Natural Science Foundation of China (NSFC) (61375027, 61433018 and 61370023), the Joint Fund of the NSFCRGC (Research Grant Council of Hong Kong) (61531166002, N CUHK404 / 15) and the Major Program for National Social Science Foundation of China (13 & ZD189).1git @ github.com: mxmaxi007 / Emotion Recognition.git"}, {"heading": "7. REFERENCES", "text": "[1] M.S. Kamel M. El Ayadi and F. Karray, \"Survey on speech emotion recognition: features, classification systems and database,\" Pattern Recognition, vol. 44, no. 3, pp. 572-587, 2011. [2] M. Dash and H. Liu, \"Feature selection for classification,\" Intelligent Data Analysis, vol. 1, no. 3, pp. 131- 156, 1997. [3] I. Guyon and A. Elisseeff, \"Journal of machine learning research, no. 3, pp. 1157-1182, 2003. [4] J.P. Hespanha P.N. Belhumeur and D.J. Kriegman,\" Eigenfaces vs. fisherfaces: Recognition using class specific linear projection, \"IEEE Transactions on Pattern Analysis and Machine Intelligence, vol."}], "references": [{"title": "Survey on speech emotion recognition: features, classification schemes and database", "author": ["M.S. Kamel M. El Ayadi", "F. Karray"], "venue": "Pattern Recognition, vol. 44, no. 3, pp. 572\u2013587, 2011.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2011}, {"title": "Feature selection for classification", "author": ["M. Dash", "H. Liu"], "venue": "Intelligent Data Analysis, vol. 1, no. 3, pp. 131\u2013 156, 1997.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1997}, {"title": "An introduction to variable and feature selection", "author": ["I. Guyon", "A. Elisseeff"], "venue": "Journal of machine learning research, , no. 3, pp. 1157\u20131182, 2003.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2003}, {"title": "Eigenfaces vs. fisherfaces: Recognition using class specific linear projection", "author": ["J.P. Hespanha P.N. Belhumeur", "D.J. Kriegman"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 19, no. 7, pp. 711\u2013720, 1997.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1997}, {"title": "Analysis of emotionally salient aspects of fundamental frequency for emotion detection", "author": ["S. Lee C. Busso", "S. Narayanan"], "venue": "IEEE Transactions on Audio, Speech, and Language Processing, vol. 17, no. 4, pp. 582\u2013596, 2009.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2009}, {"title": "Emotion recognition in human-computer interaction", "author": ["E. Douglas-Cowie R. Cowie", "N. Tsapatsoulis"], "venue": "IEEE Signal Processing Magazine, vol. 18, no. 1, pp. 32\u201380, 2001.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2001}, {"title": "Classifier-based learning of nonlinear feature manifold for visualization of emotional speech prosody", "author": ["J. Kortelainen E. Vayrynen", "T. Seppanen"], "venue": "IEEE Transactions on Affective Computing, vol. 4, no. 1, pp. 47\u201356, 2013.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2013}, {"title": "Effectiveness of linear prediction characteristics of the speech wave for automatic speaker identification and verification", "author": ["B.S. Atal"], "venue": "the Journal of the Acoustical Society of America, vol. 55, no. 6, pp. 1304\u20131312, 1974.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1974}, {"title": "Effectiveness of linear prediction characteristics of the speech wave for automatic speaker identification and verification comparison of parametric representations for monosyllabic word recognition in continuously spoken sentences", "author": ["S. Davis", "P. Mermelstein"], "venue": "IEEE Transactions on Acoustics, Speech, and Signal Processing, vol. 28, no. 4, pp. 357\u2013366, 1980.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1980}, {"title": "The role of voice quality in communicating emotion, mood and attitude", "author": ["C. Gobl", "A.N. Chasaide"], "venue": "Speech Communication, vol. 40, no. 1-2, pp. 189\u2013212, 2003.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2003}, {"title": "Fast and accurate sequential floating forward feature selection with the bayes classifier applied to speech emotion recognition", "author": ["D. Ververidis", "C. Kotropoulos"], "venue": "Signal Processing, vol. 88, no. 12, pp. 2956\u20132970, 2008.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2008}, {"title": "Feature subset search using genetic algorithms", "author": ["V. Kadirkamanathan F.J. Ferri", "J. Kittler"], "venue": "in: IEE/IEEE Workshop on Natural Algorithms in Signal Processing, IEE. 1993, Press.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1993}, {"title": "Feature learning in deep neural networks - studies on speech recognition tasks", "author": ["M.L. Seltzer D. Yu", "J. Li"], "venue": "arXiv:1301.3605, 2013.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2013}, {"title": "Three dimensions of emotion", "author": ["H. Schlosberg"], "venue": "Psychological Review, vol. 61, no. 2, pp. 81\u201388, 1954.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1954}, {"title": "What\u2019s basic about basic emotions", "author": ["A. Ortony", "T.J. Turner"], "venue": "Psychological Review, vol. 97, no. 3, pp. 315\u2013331, 1990.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1990}, {"title": "Psychological Motivated Multi- Stage Emotion Classification Exploiting Voice Quality Features, Speech Recognition, France", "author": ["M. Lugger", "B. Yang"], "venue": "Mihelic and Janez Zibert (Ed.), InTech,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2008}, {"title": "A database of german emotional speech", "author": ["A. Paeschke F. Burkhardt", "M. Rolfes"], "venue": "Proceedings Interspeech 2005, 2005, pp. 1517\u20131520.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2005}, {"title": "Recent developments in opensmile, the munich open-source multimedia feature extractor", "author": ["F. Weninger F. Eyben", "F. Gross"], "venue": "Proceedings of the 21st ACM international conference on Multimedia. 2013, pp. 835\u2013838, ACM New York, ISBN: 978-1-4503-2404-5 DOI:10.1145/2502081.2502224.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2013}], "referenceMentions": [{"referenceID": 0, "context": "As one of the main communication media between human beings, voice has received widespread attention from researchers [1].", "startOffset": 118, "endOffset": 121}, {"referenceID": 1, "context": "As a common issue for many classification problems [2], feature selection aims to pick a subset of features that are most relevant to the target concept [3] or to reduce the dimension of features for reducing computational time as well as improving the performance [4].", "startOffset": 51, "endOffset": 54}, {"referenceID": 2, "context": "As a common issue for many classification problems [2], feature selection aims to pick a subset of features that are most relevant to the target concept [3] or to reduce the dimension of features for reducing computational time as well as improving the performance [4].", "startOffset": 153, "endOffset": 156}, {"referenceID": 3, "context": "As a common issue for many classification problems [2], feature selection aims to pick a subset of features that are most relevant to the target concept [3] or to reduce the dimension of features for reducing computational time as well as improving the performance [4].", "startOffset": 265, "endOffset": 268}, {"referenceID": 4, "context": "In [5\u20137], prosody-based acoustic features, including pitchrelated, energy-related and timing features have been widely used for recognizing speech emotion.", "startOffset": 3, "endOffset": 8}, {"referenceID": 5, "context": "In [5\u20137], prosody-based acoustic features, including pitchrelated, energy-related and timing features have been widely used for recognizing speech emotion.", "startOffset": 3, "endOffset": 8}, {"referenceID": 6, "context": "In [5\u20137], prosody-based acoustic features, including pitchrelated, energy-related and timing features have been widely used for recognizing speech emotion.", "startOffset": 3, "endOffset": 8}, {"referenceID": 7, "context": "Spectral-based acoustic features also play an important role in emotion recognition, such as Linear Predictor Coefficients (LPC) [8], Linear Predictor Cepstral Coefficients (LPCC) [9] and Mel-frequency Cepstral Coefficients (MFCC) [10].", "startOffset": 180, "endOffset": 183}, {"referenceID": 8, "context": "Spectral-based acoustic features also play an important role in emotion recognition, such as Linear Predictor Coefficients (LPC) [8], Linear Predictor Cepstral Coefficients (LPCC) [9] and Mel-frequency Cepstral Coefficients (MFCC) [10].", "startOffset": 231, "endOffset": 235}, {"referenceID": 9, "context": "In [11], voice quality features have also been shown to be related to emotions.", "startOffset": 3, "endOffset": 7}, {"referenceID": 10, "context": "For example, Sequential Floating Forward Selection (SFFS) [12] is an iterative method that can find a subset of features near to the optimal one.", "startOffset": 58, "endOffset": 62}, {"referenceID": 11, "context": "Some evolutionary algorithms such as Genetic Algorithm (GA) [13] are often used in feature selection.", "startOffset": 60, "endOffset": 64}, {"referenceID": 3, "context": "Feature space transformation is another type of method, including Principal Component Analysis (PCA) [4] and Neural Network (NN) [14].", "startOffset": 101, "endOffset": 104}, {"referenceID": 12, "context": "Feature space transformation is another type of method, including Principal Component Analysis (PCA) [4] and Neural Network (NN) [14].", "startOffset": 129, "endOffset": 133}, {"referenceID": 13, "context": "dominance model [15].", "startOffset": 16, "endOffset": 20}, {"referenceID": 14, "context": "Besides, discrete emotional labels, the so-called archetypal emotions [16], are common used in speech emotion recognition.", "startOffset": 70, "endOffset": 74}, {"referenceID": 15, "context": "[17] has proposed a hierarchical approach to classify the speech emotions with the dimensional model.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "In this study, we used the well known Berlin emotional database (EmoDB) [18].", "startOffset": 72, "endOffset": 76}, {"referenceID": 17, "context": "OpenSmile toolkit [19] is used to extract acoustic features, and a total of 988 features are obtained.", "startOffset": 18, "endOffset": 22}], "year": 2016, "abstractText": "Feature subspace selection is an important part in speech emotion recognition. Most of the studies are devoted to finding a feature subspace for representing all emotions. However, some studies have indicated that the features associated with different emotions are not exactly the same. Hence, traditional methods may fail to distinguish some of the emotions with just one global feature subspace. In this work, we propose a new divide and conquer idea to solve the problem. First, the feature subspaces are constructed for all the combinations of every two different emotions (emotion-pair). Bi-classifiers are then trained on these feature subspaces respectively. The final emotion recognition result is derived by the voting and competition method. Experimental results demonstrate that the proposed method can get better results than the traditional multi-classification method.", "creator": "LaTeX with hyperref package"}}}