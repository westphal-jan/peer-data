{"id": "1611.05740", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Nov-2016", "title": "Fast Non-Parametric Tests of Relative Dependency and Similarity", "abstract": "We introduce two novel non-parametric statistical hypothesis tests. The first test, called the relative test of dependency, enables us to determine whether one source variable is significantly more dependent on a first target variable or a second. Dependence is measured via the Hilbert-Schmidt Independence Criterion (HSIC). The second test, called the relative test of similarity, is use to determine which of the two samples from arbitrary distributions is significantly closer to a reference sample of interest and the relative measure of similarity is based on the Maximum Mean Discrepancy (MMD). To construct these tests, we have used as our test statistics the difference of HSIC statistics and of MMD statistics, respectively. The resulting tests are consistent and unbiased, and have favorable convergence properties. The effectiveness of the relative dependency test is demonstrated on several real-world problems: we identify languages groups from a multilingual parallel corpus, and we show that tumor location is more dependent on gene expression than chromosome imbalance. We also demonstrate the performance of the relative test of similarity over a broad selection of model comparisons problems in deep generative models.", "histories": [["v1", "Thu, 17 Nov 2016 15:36:31 GMT  (2676kb,D)", "http://arxiv.org/abs/1611.05740v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["wacha bounliphone", "eugene belilovsky", "arthur tenenhaus", "ioannis antonoglou", "arthur gretton", "matthew b blashcko"], "accepted": false, "id": "1611.05740"}, "pdf": {"name": "1611.05740.pdf", "metadata": {"source": "CRF", "title": "Fast Non-Parametric Tests of Relative Dependency and Similarity", "authors": ["Wacha Bounliphone", "Eugene Belilovsky", "Arthur Tenenhaus", "Ioannis Antonoglou", "Arthur Gretton", "Matthew B. Blaschko"], "emails": ["wacha.bounliphone@centralesupelec.fr", "eugene.belilovsky@inria.fr", "arthur.tenenhaus@centralesupelec.fr", "ioannisa@google.com", "arthur.gretton@gmail.com", "matthew.blaschko@esat.kuleuven.be"], "sections": [{"heading": null, "text": "We present two novel non-parametric statistical hypotheses tests: The first test, called the relative test of dependence, allows us to determine whether a source variable is significantly more dependent on a first target variable or a second. Dependence is measured using the Hilbert Schmidt Independence Criterion (HSIC); the second test, called the relative test of similarity, is used to determine which of the two samples from arbitrary distributions comes much closer to a reference sample of interest, and the relative degree of similarity is based on the maximum Mean Discrepancy (MMD). To construct these tests, we used as test statistics the difference between HSIC statistics and MMD statistics, respectively. The resulting tests are consistent and unbiased and (based on U statistics) have favorable convergence characteristics. The effectiveness of the relative dependence test is based on multiple open-language Xirov comparative problems, and we demonstrate more strongly that multilingual Xirov problems are multilingual."}, {"heading": "1. Introduction", "text": "This article is based on and extends to Bounliphone et al. (2015, 2016). We address two related problems using analog tools that are based on estimating correlated U statistics for dependence and similarity in a non-parametric environment. However, the first problem (called relative dependency testing) is to compare multiple dependencies to determine which of the two variables affects the third most by suggesting a statistical test of the null hypothesis that a source variable is more dependent on a first target variable, that a source variable is more dependent on a second target variable. Many recent studies on dependency measurement have focused on non-parametric measurements of dependence, which are applied even when the dependence is non-linear or the variables are multivariate (for example, images, strings, and graphs)."}, {"heading": "2. Motivation and Background Material", "text": "In this section, we begin with a formal definition of the two problems and introduce essential background knowledge necessary for the development of our later theory. Our goal is to formulate a statistical test that answers the following questions: Let x, y and z be a random variable defined in a topological space X \u00b7 Y \u00b7 Z, with the corresponding Borel probabilities Px, Py and Pz. In view of the observations Xm: = {x1,..., xm}, Ym: = {y1, ym} and Zm: = {z1,..., zm}, so that (xi, yi, zi) are independently and identically distributed (i.i.d.) of Px \u00d7 Py \u00d7 Pz.Problem 1 (Relative dependence testing) Is the dependence between x and y stronger than the dependence between x and z? In view of the observations Xm: = {x1,..., xm}, xm}, xm: Py \u00d7 Pz.Problem 1 (Relative dependence testing) Is the dependence between x and y stronger than the dependence between x and z? Mility Xm: = {x1, yi, zi}, xm}, xm}, Ym: Py \u00d7 Pz.Problem 1 (Relative dependence testing) Is the dependence between x and y greater than the dependence between x and y? Mility Xm: = {x1, yi, militia}, Psisim, mility, Psisim, Psisisity, Psisisim, Psisity, Psisisisity, Psisisisisisim, Pmility, Psim, Psisity, Psisim, Psisim, Psisim, mility, Psimility, Psisim, mility, Psim, Psisisim, Psisim, Psisisisimility, mility, Psimility, Psisisimility, Psim, Psim, Psim, Psisisim, Psisisisisisim, Psisim, Psisisim, mility, mility, mility, mility, mility, mility, Psir,"}, {"heading": "2.1 Kernel Mean Embedding of Distributions", "text": "This section introduces the concept of kernel embedding (Berlinet and Thomas-Agnan, 2011; Smola et al., 2007), where the idea is to generalize the Hilbert space embedding of distributions by the concept of the core property of a Dirac scale distribution. < The kernel mean of embedding was used to define metrics for probability distributions that are relevant to many problems in statistics and machine learning. First, we will briefly review the properties of the reproducing kernel Hilbert space (RKHS) (Aronszajn, 1950). An RKHS H with a reproducing kernel k (x, y) is a Hilbert space of functions f: X \u2192 R with an internal product < its element k (x, \u00b7) fulfills the reproducing property: < f, k (x, \u00b7 H) > with the reproducing property."}, {"heading": "2.2 The Maximum Mean Discrepancy and the Hilbert-Schmidt Independence Criterion", "text": "In this section we give a formal definition of Maximum Mean Discrepancy (MMD) = Universal Functions (MMD) and the Hilbert Schmidt Independence Criterion (HSIC). Definition 1 Let Px and Py be the marginal distributions on domains X and Y and let F be a unit in a characteristic RKHS H (with the continuous attribute mapping (x).F of each x-X, so that the inner product between the properties is given by the positively defined core function k (x, x): = < p (x), p (x), p (x), p (x), p (x), p (x), p (x), and we define the maximum mean discrepancy in an RKHS F asMMD [F, Py] = 1, p)."}, {"heading": "2.3 Statistical Hypothesis Testing", "text": "After we have defined the two tests statistics (the HSIC [F, G, Pxy], as a measure of dependence, and the MMD [F, Px, Py], as a measure of similarity for probability), we will address the problems Pb. 1 and Pb. 2 as statistical hypotheses tests. We have briefly described the framework of the statistical hypotheses tests (Lehmann et al., 1986) as it applies in this context. (Pb. 1) - Relative HSIC We refer to the common sample of the observations defined i.e. with Borel probability test Pxyz, as it applies to the domain X \u2212 Y \u00b7 Z and the nuclei k, l and d assigned HSIC thresholds F, G, and H, respectively, as significance. The statistical relative independence test THSIC: Xm \u00d7 Xm x, Mnull x, MICM, the use of the zero hypothesis HIC0: HSd and ICd is assigned."}, {"heading": "3. A relative test of dependency", "text": "In this section, we calculate two dependent HSIC statistics and derive from them the common asymptotic distribution of these dependent variables, which is used to construct a consistent test for the Relative HSIC problem (p. 1). Next, we construct a simpler consistent test by calculating two independent HSIC statistics on sample subsets. While the simpler strategy is superficially attractive and somewhat less complex to implement, we prove that the dependent strategy is stricter."}, {"heading": "3.1 Joint asymptotic distribution of HSIC and test", "text": "In view of the observations Xm: = {x1, \u2192 xm}, Ym: = {y1,..., ym} and Zm: = {z1,..., zm} i.i.d. of Px, Py and Pz respectively by k, l and d the kernel blanks F, G and H. In addition, K, L and D, Rm and Pz are kernel matrices respectively, the Kij = k (xi, xj), lij = l (yj, yj) and dij = d (zi, zj) I blanks. Leave HSICu [F, G, (Xm, Ym) and HSICu [F, H, Zm) respectively the unbiased estimators of HSIC [F, G, Pxy] and HSIC [F, H, Pxz] written as the sum of U statistics with kernels hijqr and giqr respectively as described."}, {"heading": "3.2 A simple consistent test via uncorrelated HSICs", "text": "From the result in Equation (11), a simple, consistent test of relative dependence can be constructed as follows: Split the samples of Px into two sets of equal size, designated by X and X, and drop the second half of the sample pairs with Y and the first half of the sample pairs with Z. We will designate the remaining samples as Y and Z. We can now determine the resulting common distribution of m1 / 2 [HSICu [F, H, Pxz]), (\u03c32X \u2032 Y \u2032 00, Ym / 2), HSICu [F, H, (Xm / 2, Zm / 2)] T asN ((((HSIC [F, G, Pxy] HSIC [F, H, Pxz])), (\u03c32X \u2032 Z \u2032), Z \u2032), (22), which we will write as N (ME, Zm / 2)."}, {"heading": "3.3 The dependent test is more powerful", "text": "While half of the samples result in a consistent test, we might expect a loss of power over the approach in Section 3.1, where q is due to the increase in variance with a smaller sample size. In this section, we demonstrate that the approach in Section 3.1, which depends on dependence, is stronger than that in Section 3.2, regardless of Pxy and Pxz. We refer to the simple and consistent approach in Section 3.2, the independent approach and the approach of lower variance in Section 3.1, the dependent approach. The following theorem compares these approaches. Theorem 8 The asymptotic relative efficiency (ARE) of the independent approach to the dependent approach is always greater than 1.Remark 9 Asymptotic relative efficiency is defined in e.g. Serfling (2009, Chapter 5, Section 1.15.4). If mA and mB are the sample sizes where tests \"perform equally well\" (i.e. have the same performance), then the ratio mB represents relative AmB."}, {"heading": "3.4 Generalizing to more than two HSIC statistics", "text": "The generalization of the dependence test to more than three random variables follows from the earlier derivation by applying successive rotations to a higher-dimensional common Gaussian distribution over several HSIC statistics. In view of the observations X1: = {x11,..., x1m},..., Xn: = {xn1,..., xnm} i.i.d. from each of Px1,..., Pxn we designate the nuclei uniquely associated with the respective reproduction of Hilbert spaces F1,..., Fn. We define a generalized statistical test, Tg: (Xm \u00d7... \u00d7 Xm) \u2192 {0, 1} to test the null hypothesis H0: (x, y) that a construction {1,..., 2 v (x, y) HSIC [F1,..., Fn, Px1... xn], the first matrix formula for the first (previous) and the first (vn-1), (the vn-1), (the first) (vn)."}, {"heading": "4. A relative test of similarity", "text": "To maximize the statistical efficiency of the test, we use samples from the reference distribution referred to by Px to calculate the MMD estimates with two candidate distributions Py and Pz. Therefore, we first consider two MMD estimates MM2u [F, Xm, Yn] and MMD2u [F, Xm, Zr], and how the data sample Xm is identical between them, these estimates are correlated. Therefore, we first derive the common asymptotic distribution of these two metrics and use these to construct a statistical test."}, {"heading": "5. Relation of the HSIC and the Relative MMD", "text": "We propose two new statistical hypotheses tests that use an equivalent mathematical derivative and use two statistics that are linked together (as in Gretton et al. (2012a, Section 7.3). MMD can be asMMD [F, Px, Py] = \"\u00b5Px \u2212 \u00b5Py\" H. (33) Suppose that V is an RKHS over X \u00b7 Y with kernel v ((x, y), (x \u2032, y \u2032). If x and y are independent, then \u00b5Pxy = \"Px\u00b5Py.\" Therefore, we can use the \"\u00b5Pxy \u2212\" -Px\u00b5Py-V as a measure of dependence. Let's say that v (x, y \u2032), (x \u2032, y \u2032) l (y \u2032) of the \"Kyx\" matrix is \"the\" matrix, \"then the\" Kutrix \"and\" Kutrix \"matrix\" can be."}, {"heading": "6. Experiments", "text": "We analyze the Relative HSIC and the Relative MMD in an extensive series of experiments. The section contains results on synthetic experiments and real data."}, {"heading": "6.1 Experiments for the relative dependency test", "text": "We apply our estimates of statistical dependence to three difficult problems: the first is a synthetic data experiment in which we can directly control the relative degree of functional dependence between variables; the second experiment uses a multilingual corpus to determine the relative relationships between European languages; and the last experiment consists of a 3-block dataset combining gene expression, comparative genomic hybridization, and a qualitative phenotype measured on a sample of glioma patients."}, {"heading": "6.1.1 Synthetic experiments", "text": "We have constructed 3 distributions as defined in Equation (35) and as shown in Figure 1. Let us not \u0445 U [(0, 2\u03c0)], (35) (a) x1 \u0445 t + \u03b31N (0, 1) y1 \u0445 sin (t) + \u03b31N (0, 1) (b) x2 \u0445 t cos (t) + \u03b32N (0, 1) y2 \u0445 t sin (t) + \u03b32N (0, 1) (c) x3 \u0445 t cos (t) + \u03b33N (0, 1) y3 \u0445 t sin (t) + \u03b33N (0, 1) These distributions are specified in such a way that we can control the relative degree of functional dependence between the variations by varying the relative magnitude of the scaling parameters \u03b31, \u03b32 and \u04213. The question is then whether the dependence between (a) and (b) is greater than the dependence between (a) and (c).In these experiments, we have specified \u04411, where the size of the two parameters is unequal to the two."}, {"heading": "6.1.2 Multilingual data", "text": "In this section, we demonstrate dependency tests to predict the relative similarity of different languages. We use a real-world dataset taken from the parallel European parliamentary corpus Koehn (2005). We select 3000 random documents written jointly in the following languages: Finnish (fi), Italian (it), French (fr), Spanish (es), Portuguese (pt), English (en), Dutch (nl), Danish (da) and Swedish (sv). These languages can be roughly categorized into Romance, Germanic or Uralic groups (Gray and Atkinson, 2003). In this dataset, we consider each language as a random variable and each document as an observation.Our first goal is to test whether the statistical dependence between two languages in the same group is greater than the statistical dependence between nearby languages (Gray and Atkinson, 2003)."}, {"heading": "6.1.3 Pediatric Glioma Data", "text": "Brain tumors are the most common solid tumors in children and have the highest mortality rate of all pediatric cancers. Despite advances in multimodality therapy, children with pediatric advanced gliomas (pHGG) inevitably have an overall survival rate of about 20% at age 5. Depending on location (e.g. brain stem, central nuclei, or supratentorial), pHGG exhibits different characteristics in terms of radiological appearance, histology, and prognostication.The hypothesis is that pHGG has different genetic origins and oncogenic pathways depending on location. Thus, the biological processes involved in tumor development show different characteristics in terms of radiological appearance, histology, and prognostication.To evaluate such hypotheses, prior to treatment, frozen tumor samples of 53 children with newly diagnosed pHGG cells and oncogenic pathways were analyzed."}, {"heading": "6.2 Experiments for the relative similarity test", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6.2.1 Synthetic experiments", "text": "We verify the validity of the hypotheses test described above using a synthetic dataset in which we can directly control the relative similarity between the distributions. We constructed three Gaussian distributions, as shown in Fig. 6. These Gaussian distributions are specified by different means so that we can control the degree of relative similarity between them. The question is whether the similarity between X and Z is greater than the similarity between X and Y. In these experiments, we used a Gaussian nucleus with bandwidth selected as the mean pair distance between the data points, and we fixed \u00b5Y = [\u2212 20, \u2212 20], \u00b5Z = [20, 20] and varied \u00b5X so that \u00b5X = (1 \u2212 \u03b3) \u00b5Y + \u03b3\u00b5Z, for 41 regularly staggered values of g. [0.1, 0.9] (to avoid the degenerated cases Px = Py or Px = Pz) and the varied \u00b5Y-X-comparative performance in Fz."}, {"heading": "6.2.2 Model selection for deep unsupervised neural networks", "text": "This year it is more than ever before."}, {"heading": "6.2.3 Discussion", "text": "In these experiments, we have seen that the RelativeMMD test can be used to compare deep generative models that obtain judgments consistent with other metrics. Comparisons to other metrics are important to verify our test, but it can mask the fact that MMD is a valid evaluation technology. In many ways, when assessing models where the likelihood of a calculation is not possible, MMD is an appropriate and tractable method that can be considered in addition to the Parzen windows, it is more appropriate than Parzen windows, as it allows to directly look at the discrepancy between the test data and the model samples while allowing the results. In such a situation, comparing the performance of multiple models using the MMMD is comparable to a single set of test samples."}, {"heading": "7. Conclusion", "text": "We have described two new non-parametric statistical hypotheses tests, in which analog mathematical derivatives are based on the estimation of two correlated U-statistics; the first test of relative similarity determines whether a source random variable is significantly more dependent on one target variable or another; the test is based on the Hilbert-Schmidt criterion; and the second test of relative similarity determines whether one model generates samples much closer to the reference distribution than the other; the criterion is based on the maximum Mean Discrepance; we have shown that both tests are consistent, stricter than a test with uncorrelated statistics, and the calculation requirements of the tests are quadratic in sample size; and we have applied the test of relative dependence to the problem of identifying relative dependencies between languages using a multilingual corpus and discovering the relative relationships between gliomas and genetic information."}, {"heading": "Acknowledgments", "text": "This work is funded by the Internal Funds KU Leuven, ERC Grant 259112, FP7-MC-CIG 334380, the Royal Academy of Engineering through the Newton Alumni Scheme and DIGITEO 2013-0788D-SOPRANO. WB is partially supported by a CentraleSupe \u0301 lec Fellowship."}, {"heading": "Appendix A. Detailed Derivations of the MMD Variance and Covariance", "text": "The variance and covariance for a U is described in Hoeffding (1948, Eq. 5,13) and Serfling (2009, chap. 5)."}], "references": [{"title": "Limit theorems for U-processes", "author": ["M.A. Arcones", "E. Gine"], "venue": "The Annals of Probability,", "citeRegEx": "Arcones and Gine.,? \\Q1993\\E", "shortCiteRegEx": "Arcones and Gine.", "year": 1993}, {"title": "Joint measures and cross-covariance operators", "author": ["C.R. Baker"], "venue": "Transactions of the American Mathematical Society,", "citeRegEx": "Baker.,? \\Q1973\\E", "shortCiteRegEx": "Baker.", "year": 1973}, {"title": "Deep generative stochastic networks trainable by backprop", "author": ["Y. Bengio", "E. Thibodeau-Laufer", "G. Alain", "J. Yosinski"], "venue": "In Proceedings of the 31st International Conference on Machine Learning,", "citeRegEx": "Bengio et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2014}, {"title": "Reproducing kernel Hilbert spaces in probability and statistics", "author": ["A. Berlinet", "C. Thomas-Agnan"], "venue": null, "citeRegEx": "Berlinet and Thomas.Agnan.,? \\Q2011\\E", "shortCiteRegEx": "Berlinet and Thomas.Agnan.", "year": 2011}, {"title": "Mapping the origins and expansion of the Indo-European language family", "author": ["R. Bouckaert", "P. Lemey", "M. Dunn", "S.J. Greenhill", "A.V. Alekseyenko", "A.J. Drummond", "R.D. Gray", "M.A. Suchard", "Q.D. Atkinson"], "venue": null, "citeRegEx": "Bouckaert et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Bouckaert et al\\.", "year": 2012}, {"title": "A low variance consistent test of relative dependency", "author": ["W. Bounliphone", "A. Gretton", "A. Tenenhaus", "M.B. Blaschko"], "venue": "Proceedings of The 32nd International Conference on Machine Learning,", "citeRegEx": "Bounliphone et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bounliphone et al\\.", "year": 2015}, {"title": "A test of relative similarity for model selection in generative models", "author": ["W. Bounliphone", "E. Belilovsky", "M.B. Blaschko", "I. Antonoglou", "A. Gretton"], "venue": "In International Conference on Learning Representations,", "citeRegEx": "Bounliphone et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Bounliphone et al\\.", "year": 2016}, {"title": "A geometric approach to compare variables in a regression model", "author": ["J. Bring"], "venue": "The American Statistician,", "citeRegEx": "Bring.,? \\Q1996\\E", "shortCiteRegEx": "Bring.", "year": 1996}, {"title": "Learning non-linear combinations of kernels", "author": ["C. Cortes", "M. Mohri", "A. Rostamizadeh"], "venue": "In Neural Information Processing Systems,", "citeRegEx": "Cortes et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Cortes et al\\.", "year": 2009}, {"title": "Multiple regression in psychological research and practice", "author": ["R.B. Darlington"], "venue": "Psychological Bulletin,", "citeRegEx": "Darlington.,? \\Q1968\\E", "shortCiteRegEx": "Darlington.", "year": 1968}, {"title": "Nonlinear canonical analysis and independence tests", "author": ["J. Dauxois", "G.M. Nkiet"], "venue": "Annals of Statistics,", "citeRegEx": "Dauxois and Nkiet.,? \\Q1998\\E", "shortCiteRegEx": "Dauxois and Nkiet.", "year": 1998}, {"title": "Multivariate generalizations of the Wald-Wolfowitz and Smirnov two-sample tests", "author": ["J.H. Friedman", "L.C. Rafsky"], "venue": "The Annals of Statistics,", "citeRegEx": "Friedman and Rafsky.,? \\Q1979\\E", "shortCiteRegEx": "Friedman and Rafsky.", "year": 1979}, {"title": "Dimensionality reduction for supervised learning with reproducing kernel Hilbert spaces", "author": ["K. Fukumizu", "F.R. Bach", "M.I. Jordan"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Fukumizu et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Fukumizu et al\\.", "year": 2004}, {"title": "Kernel measures of conditional dependence", "author": ["K. Fukumizu", "A. Gretton", "X. Sun", "B. Sch\u00f6lkopf"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Fukumizu et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Fukumizu et al\\.", "year": 2008}, {"title": "Tumorigenesis in the brain: Location, location, location", "author": ["R.J. Gilbertson", "D.H. Gutmann"], "venue": "Cancer research,", "citeRegEx": "Gilbertson and Gutmann.,? \\Q2007\\E", "shortCiteRegEx": "Gilbertson and Gutmann.", "year": 2007}, {"title": "Generative adversarial nets", "author": ["I. Goodfellow", "J. Pouget-Abadie", "M. Mirza", "B. Xu", "D. Warde-Farley", "S. Ozair", "A. Courville", "Y. Bengio"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Goodfellow et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2014}, {"title": "Language-tree divergence times support the Anatolian theory of Indo-European origin", "author": ["R.D. Gray", "Q.D. Atkinson"], "venue": "Nature, 426(6965):435\u2013439,", "citeRegEx": "Gray and Atkinson.,? \\Q2003\\E", "shortCiteRegEx": "Gray and Atkinson.", "year": 2003}, {"title": "Consistent nonparametric tests of independence", "author": ["A. Gretton", "L. Gyorfi"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Gretton and Gyorfi.,? \\Q2010\\E", "shortCiteRegEx": "Gretton and Gyorfi.", "year": 2010}, {"title": "Measuring statistical dependence with Hilbert-Schmidt norms", "author": ["A. Gretton", "O. Bousquet", "A.J. Smola", "B. Sch\u00f6lkopf"], "venue": "In Algorithmic Learning Theory,", "citeRegEx": "Gretton et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Gretton et al\\.", "year": 2005}, {"title": "Kernel methods for measuring independence", "author": ["A. Gretton", "R. Herbrich", "A. Smola", "O. Bousquet", "B. Schoelkopf"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Gretton et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Gretton et al\\.", "year": 2005}, {"title": "A kernel method for the two-sample-problem", "author": ["A. Gretton", "K.M. Borgwardt", "M. Rasch", "B. Sch\u00f6lkopf", "A.J. Smola"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Gretton et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Gretton et al\\.", "year": 2006}, {"title": "A kernel statistical test of independence", "author": ["A. Gretton", "K. Fukumizu", "C.-H. Teo", "L. Song", "B. Sch\u00f6lkopf", "A.J. Smola"], "venue": "In Neural Information Processing Systems,", "citeRegEx": "Gretton et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Gretton et al\\.", "year": 2008}, {"title": "A kernel twosample test", "author": ["A. Gretton", "K.M. Borgwardt", "M.J. Rasch", "B. Sch\u00f6lkopf", "A. Smola"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Gretton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Gretton et al\\.", "year": 2012}, {"title": "Optimal kernel choice for large-scale two-sample tests", "author": ["A. Gretton", "D. Sejdinovic", "H. Strathmann", "S. Balakrishnan", "M. Pontil", "K. Fukumizu", "B.K. Sriperumbudur"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Gretton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Gretton et al\\.", "year": 2012}, {"title": "Structural modelling with sparse kernels", "author": ["S.R. Gunn", "J.S. Kandola"], "venue": "Machine Learning,", "citeRegEx": "Gunn and Kandola.,? \\Q2002\\E", "shortCiteRegEx": "Gunn and Kandola.", "year": 2002}, {"title": "Permutation tests for equality of distributions in high-dimensional settings", "author": ["P. Hall", "N. Tajvidi"], "venue": null, "citeRegEx": "Hall and Tajvidi.,? \\Q2002\\E", "shortCiteRegEx": "Hall and Tajvidi.", "year": 2002}, {"title": "A consistent multivariate test of association based on ranks of distances", "author": ["R. Heller", "Y. Heller", "M. Gorfine"], "venue": null, "citeRegEx": "Heller et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Heller et al\\.", "year": 2013}, {"title": "A fast learning algorithm for deep belief nets", "author": ["G. Hinton", "S. Osindero", "Y.-W. Teh"], "venue": "Neural computation,", "citeRegEx": "Hinton et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2006}, {"title": "A class of statistics with asymptotically normal distribution", "author": ["W. Hoeffding"], "venue": "The Annals of Mathematical Statistics,", "citeRegEx": "Hoeffding.,? \\Q1948\\E", "shortCiteRegEx": "Hoeffding.", "year": 1948}, {"title": "Probability inequalities for sums of bounded random variables", "author": ["W. Hoeffding"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "Hoeffding.,? \\Q1963\\E", "shortCiteRegEx": "Hoeffding.", "year": 1963}, {"title": "Auto-encoding variational Bayes", "author": ["D.P. Kingma", "M. Welling"], "venue": "In International Conference on Learning Representations,", "citeRegEx": "Kingma and Welling.,? \\Q2014\\E", "shortCiteRegEx": "Kingma and Welling.", "year": 2014}, {"title": "Equitability, mutual information, and the maximal information coefficient", "author": ["J.B. Kinney", "G.S. Atwal"], "venue": "Proceedings of the National Academy of Sciences,", "citeRegEx": "Kinney and Atwal.,? \\Q2014\\E", "shortCiteRegEx": "Kinney and Atwal.", "year": 2014}, {"title": "Europarl: A parallel corpus for statistical machine translation", "author": ["P. Koehn"], "venue": "In MT summit,", "citeRegEx": "Koehn.,? \\Q2005\\E", "shortCiteRegEx": "Koehn.", "year": 2005}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "The neural autoregressive distribution estimator", "author": ["H. Larochelle", "I. Murray"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Larochelle and Murray.,? \\Q2011\\E", "shortCiteRegEx": "Larochelle and Murray.", "year": 2011}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE,", "citeRegEx": "LeCun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "Testing statistical hypotheses, volume 150", "author": ["E.L. Lehmann", "J.P. Romano", "G. Casella"], "venue": null, "citeRegEx": "Lehmann et al\\.,? \\Q1986\\E", "shortCiteRegEx": "Lehmann et al\\.", "year": 1986}, {"title": "Generative moment matching networks", "author": ["Y. Li", "K. Swersky", "R. Zemel"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Li et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Li et al\\.", "year": 2015}, {"title": "Expression profiling of ependymomas unravels localization and tumor grade-specific tumorigenesis", "author": ["T. Palm", "D. Figarella-Branger", "F. Chapon", "C. Lacroix", "F. Gray", "F. Scaravilli", "D.W. Ellison", "I. Salmon", "M. Vikkula", "C. Godfraind"], "venue": "Cancer, 115(17):3955\u20133968,", "citeRegEx": "Palm et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Palm et al\\.", "year": 2009}, {"title": "Mesenchymal transition and PDGFRA amplification/mutation are key distinct oncogenic events in pediatric diffuse intrinsic pontine gliomas", "author": ["S. Puget", "C. Philippe", "D. Bax", "B. Job", "P. Varlet", "M.P. Junier", "F. Andreiuolo", "D. Carvalho", "R. Reis", "L. Guerrini-Rousseau"], "venue": "PloS one,", "citeRegEx": "Puget et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Puget et al\\.", "year": 2012}, {"title": "Detecting novel associations in large", "author": ["D. Reshef", "Y. Reshef", "H. Finucane", "S. Grossman", "G. McVean", "P. Turnbaugh", "E. Lander", "M. Mitzenmacher", "P. Sabeti"], "venue": null, "citeRegEx": "Reshef et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Reshef et al\\.", "year": 2011}, {"title": "An exact distribution-free test comparing two multivariate distributions based on adjacency", "author": ["P.R. Rosenbaum"], "venue": "Journal of the Royal Statistical Society: Series B (Statistical Methodology),", "citeRegEx": "Rosenbaum.,? \\Q2005\\E", "shortCiteRegEx": "Rosenbaum.", "year": 2005}, {"title": "Learning representations by backpropagating errors", "author": ["D.E. Rumelhart", "G.E. Hinton", "R.J. Williams"], "venue": "Neurocomputing: Foundations of Research,", "citeRegEx": "Rumelhart et al\\.,? \\Q1988\\E", "shortCiteRegEx": "Rumelhart et al\\.", "year": 1988}, {"title": "Deep Boltzmann machines", "author": ["R. Salakhutdinov", "G.E. Hinton"], "venue": "In International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "Salakhutdinov and Hinton.,? \\Q2009\\E", "shortCiteRegEx": "Salakhutdinov and Hinton.", "year": 2009}, {"title": "Equivalence of distancebased and RKHS-based statistics in hypothesis testing", "author": ["D. Sejdinovic", "B. Sriperumbudur", "A. Gretton", "K. Fukumizu"], "venue": "Annals of Statistics,", "citeRegEx": "Sejdinovic et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Sejdinovic et al\\.", "year": 2013}, {"title": "Approximation theorems of mathematical statistics", "author": ["R.J. Serfling"], "venue": null, "citeRegEx": "Serfling.,? \\Q2009\\E", "shortCiteRegEx": "Serfling.", "year": 2009}, {"title": "A Hilbert space embedding for distributions", "author": ["A. Smola", "A. Gretton", "L. Song", "B. Sch\u00f6lkopf"], "venue": "In Algorithmic learning theory,", "citeRegEx": "Smola et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Smola et al\\.", "year": 2007}, {"title": "Feature selection via dependence maximization", "author": ["L. Song", "A. Smola", "A. Gretton", "J. Bedo", "K. Borgwardt"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Song et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Song et al\\.", "year": 2012}, {"title": "On the influence of the kernel on the consistency of support vector machines", "author": ["I. Steinwart"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Steinwart.,? \\Q2002\\E", "shortCiteRegEx": "Steinwart.", "year": 2002}, {"title": "Measuring and testing dependence by correlation of distances", "author": ["G. Sz\u00e9kely", "M. Rizzo", "N. Bakirov"], "venue": "Annals of Statistics,", "citeRegEx": "Sz\u00e9kely et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Sz\u00e9kely et al\\.", "year": 2007}, {"title": "On the convergence of moments in the central limit theorem", "author": ["B. von Bahr"], "venue": "The Annals of Mathematical Statistics, 36(3):808\u2013818,", "citeRegEx": "Bahr.,? \\Q1965\\E", "shortCiteRegEx": "Bahr.", "year": 1965}, {"title": "Kernel-based conditional independence test and application in causal discovery", "author": ["K. Zhang", "J. Peters", "D. Janzing", "B. Sch\u00f6lkopf"], "venue": "In 27th Conference on Uncertainty in Artificial Intelligence,", "citeRegEx": "Zhang et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2011}], "referenceMentions": [{"referenceID": 21, "context": "The statistics for such tests are diverse, and include kernel measures of covariance (Gretton et al., 2008; Zhang et al., 2011) and correlation (Dauxois and Nkiet, 1998; Fukumizu et al.", "startOffset": 85, "endOffset": 127}, {"referenceID": 51, "context": "The statistics for such tests are diverse, and include kernel measures of covariance (Gretton et al., 2008; Zhang et al., 2011) and correlation (Dauxois and Nkiet, 1998; Fukumizu et al.", "startOffset": 85, "endOffset": 127}, {"referenceID": 10, "context": ", 2011) and correlation (Dauxois and Nkiet, 1998; Fukumizu et al., 2008), distance covariances (which are instances of kernel tests) (Sz\u00e9kely et al.", "startOffset": 24, "endOffset": 72}, {"referenceID": 13, "context": ", 2011) and correlation (Dauxois and Nkiet, 1998; Fukumizu et al., 2008), distance covariances (which are instances of kernel tests) (Sz\u00e9kely et al.", "startOffset": 24, "endOffset": 72}, {"referenceID": 49, "context": ", 2008), distance covariances (which are instances of kernel tests) (Sz\u00e9kely et al., 2007; Sejdinovic et al., 2013), kernel regression tests (Cortes et al.", "startOffset": 68, "endOffset": 115}, {"referenceID": 44, "context": ", 2008), distance covariances (which are instances of kernel tests) (Sz\u00e9kely et al., 2007; Sejdinovic et al., 2013), kernel regression tests (Cortes et al.", "startOffset": 68, "endOffset": 115}, {"referenceID": 8, "context": ", 2013), kernel regression tests (Cortes et al., 2009; Gunn and Kandola, 2002), rankings (Heller et al.", "startOffset": 33, "endOffset": 78}, {"referenceID": 24, "context": ", 2013), kernel regression tests (Cortes et al., 2009; Gunn and Kandola, 2002), rankings (Heller et al.", "startOffset": 33, "endOffset": 78}, {"referenceID": 26, "context": ", 2009; Gunn and Kandola, 2002), rankings (Heller et al., 2013), and space partitioning approaches (Gretton and Gyorfi, 2010; Reshef et al.", "startOffset": 42, "endOffset": 63}, {"referenceID": 17, "context": ", 2013), and space partitioning approaches (Gretton and Gyorfi, 2010; Reshef et al., 2011; Kinney and Atwal, 2014).", "startOffset": 43, "endOffset": 114}, {"referenceID": 40, "context": ", 2013), and space partitioning approaches (Gretton and Gyorfi, 2010; Reshef et al., 2011; Kinney and Atwal, 2014).", "startOffset": 43, "endOffset": 114}, {"referenceID": 31, "context": ", 2013), and space partitioning approaches (Gretton and Gyorfi, 2010; Reshef et al., 2011; Kinney and Atwal, 2014).", "startOffset": 43, "endOffset": 114}, {"referenceID": 15, "context": "First, likelihoods can be difficult to compute for some families of recently proposed models based on deep learning (Goodfellow et al., 2014; Li et al., 2015).", "startOffset": 116, "endOffset": 158}, {"referenceID": 37, "context": "First, likelihoods can be difficult to compute for some families of recently proposed models based on deep learning (Goodfellow et al., 2014; Li et al., 2015).", "startOffset": 116, "endOffset": 158}, {"referenceID": 5, "context": "This article is based upon and extends Bounliphone et al. (2015, 2016). We address two related problems using analogous tools based on estimating correlated U -statistics for dependency and similarity in a non-parametric setting. The first problem (called the relative dependency test) is to compare multiple dependencies to determine which of two variables most strongly influences the third, by proposing a statistical test of the null hypothesis that a source variable is more dependent to a first target variable against the alternative hypothesis that a source variable is more dependent to a second target variable. Much recent research on dependence measurement has focused on non-parametric measures of dependence, which apply even when the dependence is nonlinear, or the variables are multivariate or non-Euclidean (for instance images, strings, and graphs). The statistics for such tests are diverse, and include kernel measures of covariance (Gretton et al., 2008; Zhang et al., 2011) and correlation (Dauxois and Nkiet, 1998; Fukumizu et al., 2008), distance covariances (which are instances of kernel tests) (Sz\u00e9kely et al., 2007; Sejdinovic et al., 2013), kernel regression tests (Cortes et al., 2009; Gunn and Kandola, 2002), rankings (Heller et al., 2013), and space partitioning approaches (Gretton and Gyorfi, 2010; Reshef et al., 2011; Kinney and Atwal, 2014). Specialization of such methods to univariate linear dependence can yield similar tests to classical approaches such as Darlington (1968); Bring (1996).", "startOffset": 39, "endOffset": 1518}, {"referenceID": 5, "context": "This article is based upon and extends Bounliphone et al. (2015, 2016). We address two related problems using analogous tools based on estimating correlated U -statistics for dependency and similarity in a non-parametric setting. The first problem (called the relative dependency test) is to compare multiple dependencies to determine which of two variables most strongly influences the third, by proposing a statistical test of the null hypothesis that a source variable is more dependent to a first target variable against the alternative hypothesis that a source variable is more dependent to a second target variable. Much recent research on dependence measurement has focused on non-parametric measures of dependence, which apply even when the dependence is nonlinear, or the variables are multivariate or non-Euclidean (for instance images, strings, and graphs). The statistics for such tests are diverse, and include kernel measures of covariance (Gretton et al., 2008; Zhang et al., 2011) and correlation (Dauxois and Nkiet, 1998; Fukumizu et al., 2008), distance covariances (which are instances of kernel tests) (Sz\u00e9kely et al., 2007; Sejdinovic et al., 2013), kernel regression tests (Cortes et al., 2009; Gunn and Kandola, 2002), rankings (Heller et al., 2013), and space partitioning approaches (Gretton and Gyorfi, 2010; Reshef et al., 2011; Kinney and Atwal, 2014). Specialization of such methods to univariate linear dependence can yield similar tests to classical approaches such as Darlington (1968); Bring (1996). For many problems in data analysis, however, the question of whether dependence exists is secondary: there may be multiple dependencies, and the question becomes which dependence is the strongest.", "startOffset": 39, "endOffset": 1532}, {"referenceID": 33, "context": "Permutation testing or other generic strategies are often computationally prohibitive, bearing in mind the relatively high computational requirements of deep networks (Krizhevsky et al., 2012).", "startOffset": 167, "endOffset": 192}, {"referenceID": 15, "context": "We treat the two trained networks being compared as generative models (Goodfellow et al., 2014; Hinton et al., 2006; Salakhutdinov and Hinton, 2009), and test whether the first candidate model generates samples significantly closer to a reference validation set.", "startOffset": 70, "endOffset": 148}, {"referenceID": 27, "context": "We treat the two trained networks being compared as generative models (Goodfellow et al., 2014; Hinton et al., 2006; Salakhutdinov and Hinton, 2009), and test whether the first candidate model generates samples significantly closer to a reference validation set.", "startOffset": 70, "endOffset": 148}, {"referenceID": 43, "context": "We treat the two trained networks being compared as generative models (Goodfellow et al., 2014; Hinton et al., 2006; Salakhutdinov and Hinton, 2009), and test whether the first candidate model generates samples significantly closer to a reference validation set.", "startOffset": 70, "endOffset": 148}, {"referenceID": 29, "context": "The derivation of our test utilizes classical results of U -statistics (Hoeffding, 1963; Serfling, 2009; Arcones and Gine, 1993).", "startOffset": 71, "endOffset": 128}, {"referenceID": 45, "context": "The derivation of our test utilizes classical results of U -statistics (Hoeffding, 1963; Serfling, 2009; Arcones and Gine, 1993).", "startOffset": 71, "endOffset": 128}, {"referenceID": 0, "context": "The derivation of our test utilizes classical results of U -statistics (Hoeffding, 1963; Serfling, 2009; Arcones and Gine, 1993).", "startOffset": 71, "endOffset": 128}, {"referenceID": 0, "context": "The derivation of our test utilizes classical results of U -statistics (Hoeffding, 1963; Serfling, 2009; Arcones and Gine, 1993). In particular, we make use of results by Hoeffding (1963) and Serfling (2009) to determine the asymptotic joint distributions of the statistics (see Theorems 5 & 11).", "startOffset": 105, "endOffset": 188}, {"referenceID": 0, "context": "The derivation of our test utilizes classical results of U -statistics (Hoeffding, 1963; Serfling, 2009; Arcones and Gine, 1993). In particular, we make use of results by Hoeffding (1963) and Serfling (2009) to determine the asymptotic joint distributions of the statistics (see Theorems 5 & 11).", "startOffset": 105, "endOffset": 208}, {"referenceID": 3, "context": "1 Kernel Mean Embedding of Distributions This section presents the notion of kernel mean embeddings (Berlinet and Thomas-Agnan, 2011; Smola et al., 2007), where the idea is to generalize the Hilbert-space embedding of distributions by the kernel feature map of a distribution to Dirac measures.", "startOffset": 100, "endOffset": 153}, {"referenceID": 46, "context": "1 Kernel Mean Embedding of Distributions This section presents the notion of kernel mean embeddings (Berlinet and Thomas-Agnan, 2011; Smola et al., 2007), where the idea is to generalize the Hilbert-space embedding of distributions by the kernel feature map of a distribution to Dirac measures.", "startOffset": 100, "endOffset": 153}, {"referenceID": 13, "context": "The notion of universal kernels and characteristic kernels are essential to the study of kernel mean embeddings (Fukumizu et al., 2008).", "startOffset": 112, "endOffset": 135}, {"referenceID": 48, "context": "The kernel k is said to be universal if the corresponding RKHS H is dense in the space of bounded continuous functions on X (Steinwart, 2002).", "startOffset": 124, "endOffset": 141}, {"referenceID": 3, "context": "1 Kernel Mean Embedding of Distributions This section presents the notion of kernel mean embeddings (Berlinet and Thomas-Agnan, 2011; Smola et al., 2007), where the idea is to generalize the Hilbert-space embedding of distributions by the kernel feature map of a distribution to Dirac measures. The kernel mean embedding has been used to define metrics for probability distributions which is important for many problems in statistics and machine learning. First, we briefly review the properties of the Reproducing Kernel Hilbert-space (RKHS) (Aronszajn, 1950). A RKHS H with a reproducing kernel k(x, y) is a Hilbert space of functions f : X \u2192 R with inner product \u3008\u00b7, \u00b7\u3009H. Its element k(x, \u00b7) satisfies the reproducing property: \u3008f, k(x, \u00b7)\u3009H = f(x) for any f \u2208 H and consequently, \u3008k(x, \u00b7), k(y, \u00b7)\u3009H = k(x, y). We define the feature map \u03c6 : X \u2192 H by \u03c6(x) = k(x, \u00b7) and using the reproducing property, we obtain that k(x, y) = \u3008\u03c6(x), \u03c6(y)\u3009H. We extend the notion of feature map to the mean embedding of probability measure : Suppose that a space P(X ) consists of all Borel probability measures P on some input space X . we define the mean embedding \u03bc of P associated with a reproduction kernel k by a mapping \u03bc : P(X ) \u2192 H, by \u03bcP = EX\u223cP [k(x, \u00b7)] = \u222b X k(\u00b7, x)dP(x). The distribution P is mapped to its expected feature map, i.e., to a point in a potentially infinite-dimensional and implicit feature space. The mean embedding \u03bc has the property that Ex\u223cP [f(X)] = \u3008\u03bcP, f\u3009H for any f \u2208 H. The notion of universal kernels and characteristic kernels are essential to the study of kernel mean embeddings (Fukumizu et al., 2008). The kernel k is said to be universal if the corresponding RKHS H is dense in the space of bounded continuous functions on X (Steinwart, 2002). It was shown that for a universal kernel k, \u2016\u03bcP\u2212\u03bcQ\u2016H iff P = Q, i.e. the map \u03bc is injective. The kernel k is said to be characteristic if the map \u03bc is injective and the RKHS H is said to be characteristic if its reproducing kernel is characteristic. This notion was introduced by Fukumizu et al. (2008) and it was shown that Gaussian and Laplacian kernels are characteristic on Rd.", "startOffset": 101, "endOffset": 2076}, {"referenceID": 1, "context": "We assume that Ex[k(x, x)] < \u221e and Ey[l(y, y\u2032)] < \u221e, the cross-covariance operator (see Baker (1973) and Fukumizu et al.", "startOffset": 88, "endOffset": 101}, {"referenceID": 1, "context": "We assume that Ex[k(x, x)] < \u221e and Ey[l(y, y\u2032)] < \u221e, the cross-covariance operator (see Baker (1973) and Fukumizu et al. (2004)) Cyx : H \u2192 F is defined as Cyx := Eyx [\u03c6(y)\u2297 \u03c6(x)]\u2212\u03bcPx\u2297\u03bcPy = \u03bcPyx\u2212\u03bcPy\u2297\u03bcPx .", "startOffset": 88, "endOffset": 128}, {"referenceID": 20, "context": "HSIC determines independence: HSIC [F ,G,Pxy] = 0 if and only if Pxy = PxPy when kernels k and l are characteristic on their respective marginal domains (Gretton et al., 2006).", "startOffset": 153, "endOffset": 175}, {"referenceID": 36, "context": "We briefly described the framework of statistical hypothesis testing (Lehmann et al., 1986) as it applies in this context.", "startOffset": 69, "endOffset": 91}, {"referenceID": 18, "context": "Proof In Gretton et al. (2005a) a finite sample bound is given for a single HSIC statistic.", "startOffset": 9, "endOffset": 32}, {"referenceID": 16, "context": "These languages can be broadly categorized into either the Romance, Germanic or Uralic groups (Gray and Atkinson, 2003).", "startOffset": 94, "endOffset": 119}, {"referenceID": 30, "context": "corpus Koehn (2005). We choose 3000 random documents in common written in: Finnish (fi), Italian (it), French (fr), Spanish (es), Portuguese (pt), English (en), Dutch (nl), German (de), Danish (da) and Swedish (sv).", "startOffset": 7, "endOffset": 20}, {"referenceID": 15, "context": "These languages can be broadly categorized into either the Romance, Germanic or Uralic groups (Gray and Atkinson, 2003). In this dataset, we considered each language as a random variable and each document as an observation. Our first goal is to test if the statistical dependence between two languages in the same group is greater than the statistical dependence between languages in different groups. For pre-processing, we removed stop-words (http://www.nltk.org) and performed stemming (http://snowball.tartarus.org). We applied the TF-IDF model as a feature representation and used a Gaussian kernel with the bandwidth \u03c3 set per language as the median pairwise distance between documents. In Tab. 1, a selection of tests between language groups (Germanic, Romance, and Uralic) is given: all p-values strongly support that our relative dependence test finds the different language groups with very high significance. Further, if we focus on the Romance family, our test enables one to answer more fine-grained questions about the relative similarity of languages within the same group. As before, we determine the ground truth similarities from the topology of the tree of European languages determined by the linguistics community Gray and Atkinson (2003); Bouckaert et al.", "startOffset": 95, "endOffset": 1260}, {"referenceID": 4, "context": "As before, we determine the ground truth similarities from the topology of the tree of European languages determined by the linguistics community Gray and Atkinson (2003); Bouckaert et al. (2012) as illustrated in Fig.", "startOffset": 172, "endOffset": 196}, {"referenceID": 16, "context": "The tests are ordered such that a low p-value corresponds with a confirmation of the topology of the tree of Romance languages determined by the linguistics community Gray and Atkinson (2003).", "startOffset": 167, "endOffset": 192}, {"referenceID": 39, "context": "In order to evaluate such hypotheses, pre-treatment frozen tumor samples were obtained from 53 children with newly diagnosed pHGG from Necker Enfants Malades (Paris, France) from Puget et al. (2012). The 53 tumors are divided into 3 locations: supratentorial (HEMI), central nuclei (MIDL), and brain stem (DIPG).", "startOffset": 179, "endOffset": 199}, {"referenceID": 16, "context": "The tests are ordered such that a low p-value corresponds with a confirmation of the topology of the tree of Romance languages determined by the linguistics community Gray and Atkinson (2003). Figure 4: Partial tree of Romance languages adapted from Gray and Atkinson (2003).", "startOffset": 167, "endOffset": 192}, {"referenceID": 16, "context": "The tests are ordered such that a low p-value corresponds with a confirmation of the topology of the tree of Romance languages determined by the linguistics community Gray and Atkinson (2003). Figure 4: Partial tree of Romance languages adapted from Gray and Atkinson (2003).", "startOffset": 167, "endOffset": 275}, {"referenceID": 14, "context": "The empirical relative dependency is consistent with findings in the medical literature, and provides additional statistical support for the importance of tumor location in Glioma (Gilbertson and Gutmann, 2007; Palm et al., 2009; Puget et al., 2012).", "startOffset": 180, "endOffset": 249}, {"referenceID": 38, "context": "The empirical relative dependency is consistent with findings in the medical literature, and provides additional statistical support for the importance of tumor location in Glioma (Gilbertson and Gutmann, 2007; Palm et al., 2009; Puget et al., 2012).", "startOffset": 180, "endOffset": 249}, {"referenceID": 39, "context": "The empirical relative dependency is consistent with findings in the medical literature, and provides additional statistical support for the importance of tumor location in Glioma (Gilbertson and Gutmann, 2007; Palm et al., 2009; Puget et al., 2012).", "startOffset": 180, "endOffset": 249}, {"referenceID": 30, "context": "2) can be found in recent work on unsupervised learning with deep neural networks (Kingma and Welling, 2014; Bengio et al., 2014; Larochelle and Murray, 2011; Salakhutdinov and Hinton, 2009; Li et al., 2015; Goodfellow et al., 2014).", "startOffset": 82, "endOffset": 232}, {"referenceID": 2, "context": "2) can be found in recent work on unsupervised learning with deep neural networks (Kingma and Welling, 2014; Bengio et al., 2014; Larochelle and Murray, 2011; Salakhutdinov and Hinton, 2009; Li et al., 2015; Goodfellow et al., 2014).", "startOffset": 82, "endOffset": 232}, {"referenceID": 34, "context": "2) can be found in recent work on unsupervised learning with deep neural networks (Kingma and Welling, 2014; Bengio et al., 2014; Larochelle and Murray, 2011; Salakhutdinov and Hinton, 2009; Li et al., 2015; Goodfellow et al., 2014).", "startOffset": 82, "endOffset": 232}, {"referenceID": 43, "context": "2) can be found in recent work on unsupervised learning with deep neural networks (Kingma and Welling, 2014; Bengio et al., 2014; Larochelle and Murray, 2011; Salakhutdinov and Hinton, 2009; Li et al., 2015; Goodfellow et al., 2014).", "startOffset": 82, "endOffset": 232}, {"referenceID": 37, "context": "2) can be found in recent work on unsupervised learning with deep neural networks (Kingma and Welling, 2014; Bengio et al., 2014; Larochelle and Murray, 2011; Salakhutdinov and Hinton, 2009; Li et al., 2015; Goodfellow et al., 2014).", "startOffset": 82, "endOffset": 232}, {"referenceID": 15, "context": "2) can be found in recent work on unsupervised learning with deep neural networks (Kingma and Welling, 2014; Bengio et al., 2014; Larochelle and Murray, 2011; Salakhutdinov and Hinton, 2009; Li et al., 2015; Goodfellow et al., 2014).", "startOffset": 82, "endOffset": 232}, {"referenceID": 37, "context": "As noted by several authors, the evaluation of generative models is a challenging open problem (Li et al., 2015; Goodfellow et al., 2014), and the distributions of samples from these models are very complex and difficult to evaluate.", "startOffset": 95, "endOffset": 137}, {"referenceID": 15, "context": "As noted by several authors, the evaluation of generative models is a challenging open problem (Li et al., 2015; Goodfellow et al., 2014), and the distributions of samples from these models are very complex and difficult to evaluate.", "startOffset": 95, "endOffset": 137}, {"referenceID": 30, "context": "In the experiments in the sequel we focus on the recently introduced variational autoencoder (VAE) (Kingma and Welling, 2014) and the generative moment matching networks (GMMN) (Li et al.", "startOffset": 99, "endOffset": 125}, {"referenceID": 37, "context": "In the experiments in the sequel we focus on the recently introduced variational autoencoder (VAE) (Kingma and Welling, 2014) and the generative moment matching networks (GMMN) (Li et al., 2015).", "startOffset": 177, "endOffset": 194}, {"referenceID": 30, "context": "minimizing a regularized variational lower bound (Kingma and Welling, 2014).", "startOffset": 49, "endOffset": 75}, {"referenceID": 42, "context": "Both these models can be trained using standard backpropagation (Rumelhart et al., 1988).", "startOffset": 64, "endOffset": 88}, {"referenceID": 27, "context": "Using the latent variable prior we can directly sample the data distribution of these models without using MCMC procedures (Hinton et al., 2006; Salakhutdinov and Hinton, 2009).", "startOffset": 123, "endOffset": 176}, {"referenceID": 43, "context": "Using the latent variable prior we can directly sample the data distribution of these models without using MCMC procedures (Hinton et al., 2006; Salakhutdinov and Hinton, 2009).", "startOffset": 123, "endOffset": 176}, {"referenceID": 35, "context": "We use the MNIST and FreyFace datasets for our analysis (LeCun et al., 1998; Kingma and Welling, 2014; Goodfellow et al., 2014).", "startOffset": 56, "endOffset": 127}, {"referenceID": 30, "context": "We use the MNIST and FreyFace datasets for our analysis (LeCun et al., 1998; Kingma and Welling, 2014; Goodfellow et al., 2014).", "startOffset": 56, "endOffset": 127}, {"referenceID": 15, "context": "We use the MNIST and FreyFace datasets for our analysis (LeCun et al., 1998; Kingma and Welling, 2014; Goodfellow et al., 2014).", "startOffset": 56, "endOffset": 127}, {"referenceID": 30, "context": "Variational Auto-Encoder Sample Size and Architecture Experiments We use the architecture from Kingma and Welling (2014) with a hidden layer at both the encoder and decoder and a latent variable layer as shown in Fig.", "startOffset": 95, "endOffset": 121}, {"referenceID": 30, "context": "In addition we use the supervised task test data to evaluate the variational lower bound of the data under the two models (Kingma and Welling, 2014).", "startOffset": 122, "endOffset": 148}, {"referenceID": 37, "context": "Generative Moment Matching Networks Architecture Experiments We demonstrate our hypothesis test on a different class of deep generative models called Generative Moment Matching Networks (GMMN) (Li et al., 2015).", "startOffset": 193, "endOffset": 210}, {"referenceID": 37, "context": "We use the same training set of 55000, validation set of 5000 and test set of 10000 as in (Li et al., 2015; Goodfellow et al., 2014).", "startOffset": 90, "endOffset": 132}, {"referenceID": 15, "context": "We use the same training set of 55000, validation set of 5000 and test set of 10000 as in (Li et al., 2015; Goodfellow et al., 2014).", "startOffset": 90, "endOffset": 132}, {"referenceID": 36, "context": "Li et al. (2015) proposes to use that model along with an auto-encoder, which is the setup we employ in this work.", "startOffset": 0, "endOffset": 17}, {"referenceID": 36, "context": "Li et al. (2015) proposes to use that model along with an auto-encoder, which is the setup we employ in this work. Here a standard auto-encoder model is trained on the data to obtain a low dimensional representation, then a GMMN network is trained on the latent representations (Fig. 10). We use the relative similarity test to evaluate various architectural choices in this new class of models. We start from the baseline model specified in Li et al. (2015) and associated software.", "startOffset": 0, "endOffset": 459}, {"referenceID": 41, "context": "Other two sample tests are not easily extendable to relative tests (Rosenbaum, 2005; Friedman and Rafsky, 1979; Hall and Tajvidi, 2002).", "startOffset": 67, "endOffset": 135}, {"referenceID": 11, "context": "Other two sample tests are not easily extendable to relative tests (Rosenbaum, 2005; Friedman and Rafsky, 1979; Hall and Tajvidi, 2002).", "startOffset": 67, "endOffset": 135}, {"referenceID": 25, "context": "Other two sample tests are not easily extendable to relative tests (Rosenbaum, 2005; Friedman and Rafsky, 1979; Hall and Tajvidi, 2002).", "startOffset": 67, "endOffset": 135}, {"referenceID": 30, "context": "There are two primary advantages of the MMD over the variational lower bound, where it is known (Kingma and Welling, 2014): first, we have a characterization of the asymptotic behavior, which allows us to determine when the difference in performance is significant; second, comparing two lower bounds produced from two different models is unreliable, as we do not know how conservative either lower bound is.", "startOffset": 96, "endOffset": 122}], "year": 2016, "abstractText": "We introduce two novel non-parametric statistical hypothesis tests. The first test, called the relative test of dependency, enables us to determine whether one source variable is significantly more dependent on a first target variable or a second. Dependence is measured via the Hilbert-Schmidt Independence Criterion (HSIC). The second test, called the relative test of similarity, is use to determine which of the two samples from arbitrary distributions is significantly closer to a reference sample of interest and the relative measure of similarity is based on the Maximum Mean Discrepancy (MMD). To construct these tests, we have used as our test statistics the difference of HSIC statistics and of MMD statistics, respectively. The resulting tests are consistent and unbiased, and (being based on U -statistics) have favorable convergence properties. The effectiveness of the relative dependency test is demonstrated on several real-world problems: we identify languages groups from a multilingual parallel corpus, and we show that tumor location is more dependent on gene expression \u2217. These authors contributed equally. c \u00a90 Bounliphone, et al.. ar X iv :1 61 1. 05 74 0v 1 [ cs .A I] 1 7 N ov 2 01 6", "creator": "LaTeX with hyperref package"}}}