{"id": "1611.05743", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Nov-2016", "title": "Relational Multi-Manifold Co-Clustering", "abstract": "Co-clustering targets on grouping the samples (e.g., documents, users) and the features (e.g., words, ratings) simultaneously. It employs the dual relation and the bilateral information between the samples and features. In many realworld applications, data usually reside on a submanifold of the ambient Euclidean space, but it is nontrivial to estimate the intrinsic manifold of the data space in a principled way. In this study, we focus on improving the co-clustering performance via manifold ensemble learning, which is able to maximally approximate the intrinsic manifolds of both the sample and feature spaces. To achieve this, we develop a novel co-clustering algorithm called Relational Multi-manifold Co-clustering (RMC) based on symmetric nonnegative matrix tri-factorization, which decomposes the relational data matrix into three submatrices. This method considers the intertype relationship revealed by the relational data matrix, and also the intra-type information reflected by the affinity matrices encoded on the sample and feature data distributions. Specifically, we assume the intrinsic manifold of the sample or feature space lies in a convex hull of some pre-defined candidate manifolds. We want to learn a convex combination of them to maximally approach the desired intrinsic manifold. To optimize the objective function, the multiplicative rules are utilized to update the submatrices alternatively. Besides, both the entropic mirror descent algorithm and the coordinate descent algorithm are exploited to learn the manifold coefficient vector. Extensive experiments on documents, images and gene expression data sets have demonstrated the superiority of the proposed algorithm compared to other well-established methods.", "histories": [["v1", "Wed, 16 Nov 2016 05:33:04 GMT  (120kb)", "http://arxiv.org/abs/1611.05743v1", "11 pages, 4 figures, published in IEEE Transactions on Cybernetics (TCYB)"]], "COMMENTS": "11 pages, 4 figures, published in IEEE Transactions on Cybernetics (TCYB)", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["ping li", "jiajun bu", "chun chen", "zhanying he", "deng cai"], "accepted": false, "id": "1611.05743"}, "pdf": {"name": "1611.05743.pdf", "metadata": {"source": "CRF", "title": "Relational Multi-Manifold Co-Clustering", "authors": ["Ping Li", "Jiajun Bu"], "emails": ["ing}@zju.edu.cn).", "cai@cad.zju.edu.cn)"], "sections": [{"heading": null, "text": "This year, it has reached the stage where it will be able to take the lead."}, {"heading": "II. RELATED WORKS", "text": "This year has come to the point where we will be able to move to another world, where we will be able to move to another world, where we will be able to move to another world, where we will be able to move to another world, where we will be able to move to another world, where we will be able to move to another world, where we will be able to move to another world, where we will be able to move to another world, where we will be able to move to another world, where we will be able to move to another world, where we will be in which we are in which we are in which we are in which we are \"in which we are.\""}, {"heading": "III. RELATIONAL MULTI-MANIFOLD CO-CLUSTERING", "text": "In this part we present our RMC approach. We briefly review the symmetrical NMTF-based co-clustering of multi-typical relational data and describe the diverse ensemble learning from which we arrive at our goal. We provide the multiplicative updating rules of the disassembled matrices as well as the manifold optimization algorithms for coefficients. We start with the problem formulation below."}, {"heading": "A. Problem Formulation", "text": "The general problem is to generate multi-typical relational data into clusters. Given a K-type relational dataset X = {X1, X2,..., XK}, in which each Xk represents the data objects of the k-th type, we define an intertypical relational matrix R with the Rij \u011dRni \u00b7 nj, i 6 = j submatrix, which reflects the inter-type relationship between the i-th type and the j-th data objects. To model the intra-type structural information for each data type, we define an intra-type relational matrix W consisting of a set of affinity matrices encoded on the data, which indicates the intra-type relationship of components within the k-th data type. Both the intertypical and intra-type relationship between the different data objects and the intra-type matrix data are frequently used in Ruplix or multiple Ruplighted data objects."}, {"heading": "B. Symmetric NMTF", "text": "Symmetrical nonnegative matrix tri-factorization (SNMTF) uses both the intertypical relationship and the intra-typical information of multiple relational data [38]. Similar to the previous work, i.e. Long's BVD [30], Ding's ONMTF [19] and Gu's DRCC [23], SNMTF has an important property that they all split the data matrix into three low-level matrices, i.e. (K = 2) R12 \u2248 G1S12G T 2, (1) where G1, Rn1 \u00b7 c1 and G2 \u2012 Rn2 \u00b7 c2 the cluster indicator matrices for X1 and X2, i.e. c1, c2 \u2012 n2 \u2012 n2. The middle matrix S12, Rc1 \u00b7 cMDRS \u00b7 c2 can be considered a compact representation of R12 [30] absorbing the different scales of other matrices."}, {"heading": "C. Manifold Ensemble Learning", "text": "As can be seen from the existing graph, the intrinsic manifold plays a crucial role in maintaining the local geometric structure in data space. However, it is not a trivial task to discover a suitable intrinsic manifold in reality. Therefore, it is said that an automatic and data-driven manifold approach is invaluable for a manifold co-clustering based on regularization. In this work, we adopt a novel learning paradigm called manifold ensemble learning in order to get as close as possible to the true intrinsic manifold. This idea is inspired by the work in [21], which combines the automatic intrinsic manifold approach and semi-supervised classification. Our assumption is that a number of initial assumptions of the graph Laplacian are available and that the intrinsic manifold of the sample or feature space mantle is present in the shell of these candidates."}, {"heading": "D. Objective Function", "text": "Based on the above analysis, it is natural for us to use the advantages of manifold ensemble learning to approximate the intrinsic manifolds in the sample or in the attribute space. Specifically, we take this idea into the symmetrical nonnegative matrix-tri-factorization framework and propose a novel co-cluster approach called Relational Multi-Manifold Co-Clustering (RMC). Now, it is easy to arrive at the objective function, i.e. min G, S, \u00b5-R \u2212 GSGT-2F + \u03b1Tr [G T (q-i = 1\u00b5iL-i) G] + \u03b2-\u00b5 2 2, s.t.q \u0445 i = 1\u00b5i = 1, \u00b5 0, G 0, (4), where the compromise parameter \u03b1 > 0, \u03b2 > 0 is used to regulate the contribution of the ensemble manifold regulating factor to the target, the mangle ratio is applied from 2 to the adjustment of the norm-\u00b5 to the adjustment of only."}, {"heading": "E. Optimization", "text": "In this section we will examine how to optimize the target in Eq. (4) It can easily be stated that the objective function is non-convex in G, S, \u00b5, but it is convex in each of them. It is therefore unrealistic to find the global minimum, since no closed-form solution can be achieved. We present an alternative scheme to optimize the target, as most of the previous work does. [6], [30], [38]. However, it is more difficult to optimize our target, as it has a critical coefficient vector to be solved. Details are shown below. (1) Computation of S: When fixing G and \u00b5, the objective is miniming JS \u2212 R \u2212 GSGT, 2F."}, {"heading": "F. Our RMC Approach", "text": "In summary, we present the primary procedures of the proposed Relational Multi-Manifold Co-Clustering (RMC) approach in Algorithm 3.As can be seen, we finally obtain a partition matrix G, which is used to cluster samples and characteristics at the same time. As the intrinsic multiplicity is approximated at most by manifold ensemble learning, the local geometric structure of the sample or characteristic space can be better respected, resulting in more promising cluster results. In particular, we define the RMC approach using EMDA to learn the manifold coefficient briefly as RMC-E, and using CDA as RMC-C. Note that we omit the convergence evidence of the multiplicative update rules here, since our method essentially follows the similar mode of many existing co-cluster algorithms, e.g. CC-cluster formation, factoring DR23, [TM39] which is an important choice for us to investigate."}, {"heading": "IV. EXPERIMENTS", "text": "In this section, we will examine the cluster performance of the proposed method using a wide range of collected data sets. A number of interesting experiments have been conducted to demonstrate the effectiveness of our approach to clustering document, image and gene expression data. We will first give brief descriptions of the data sets and evaluation criteria, then present parameter settings for all compared algorithms and report the corresponding results. Finally, we will examine the parameter selection and distribution of the diverse coefficients."}, {"heading": "A. Data Corpora", "text": "We perform the performance evaluations on several different data sets, i.e., two text corpora, three image databases and three gene expression data. Their important statistics are summarized in Table I and the brief descriptions of them are presented hereinafter. Text corpora. NGroups5 is selected from the popular newsgroup data collection 20Newsgroups1. We use a subset of the 20news-bydate version that removes the duplicates and some headers. This subset contains five different topics related to 4,052 documents. RCV1-5 is a subset selected from the Reuters Corpus Volume I2 (RCV1) collection. We select five topics contained in a smaller RCV1 database. This subset is associated with the \"M141,\" \"GCAT,\" \"G151,\" \"G158,\" \"\" \"G159\" topics."}, {"heading": "B. Performance Comparison", "text": "In order to investigate the cluster performance of the proposed algorithm, we compare it with some state-of-the-art approaches, which are clearly listed below. \u2022 KM: Conventional k-mean method. \u2022 NMF: Nonnegative matrix factorization [26]. \u2022 GNMF: Graph regularized nonnegative matrix factor-ization, which takes local geometric structure into account through sample diagrams regularization. \u2022 DRCC: Dual regularized co-clustering, which uses both sample and function graph regularization [23]. \u2022 ONMTF: Orthogonal nonnegative matrix tri-factorization with the bi-orthogonality constraints forced [19]."}, {"heading": "C. Evaluation Criteria", "text": "We use two popular criteria to measure cluster performance [5], [23], i.e. Accuracy (AC) and Normalized Mutual Information (NMI).AC denotes the percentage of correct labels estimated by the cluster algorithm. At a data point xi is ai and gi is the estimated and true label, then we have AC = \u2211 n = 1 \u03b4 (gi, map (ai)) n, (10) where n is the total number of samples. NMI evaluates how narrowly the cluster algorithm is able to reconstruct the underlying label distribution in the data corpus (\u00b7) when the two entries are equal and otherwise zero. The permutation mapping function card (ai) forms each cluster designation ai on the equivalent label of the data set.NMI evaluates how narrowly the cluster algorithm is able to reconstruct the underlying label distribution in the cluster corpus c, each cluster designation ai on the equivalent label of the data set.NMI evaluates how narrowly the cluster algorithm is able to reconstruct the underlying label distribution in the cluster corpus c, and the cluster corpus c (C) respectively (C) and C (C)."}, {"heading": "D. Parameter Settings", "text": "To ensure fairness, we perform them using different parameter settings and report the best results.For all methods except KM, the number of samples or feature clusters is determined by the actual number of classes in all the data sets collected.Note that there is no parameter selection for KM, NMF, and ONMTF once the number of clusters is given. For the graph-based methods GNMF, DRCC, SNMTF, OSNTF, and RMC, the number of closest neighbors is set to a small number of 5 to ensure the location-preserving property.The regularization parameters are all searched from the grid {0.001, 0.1, 1, 1, 10, 100, 500, 1500}. As for the co-clustering methods, the regularization parameters for the sample graph chart and feature chart are all set to the same. \"Except for the RMC, the other graph-based methods are based on cosratics.\""}, {"heading": "E. Results", "text": "The averaged results of the various algorithms are in Table III and IV in relation to AC and NMI, respectively. The best results on each data set are highlighted in bold. In addition, the asterisk symbol \"*\" in addition to our results indicator RMC is statistically and significantly better than the other well established methods at a significance level of 01,000. From these experiments, several interesting points can be pointed out for coclustering. \u2022 The clustering performance of RMC-E is systematically and consistently better than the comparative algorithms, which confirms that the diverse ensemble learning is beneficial for the graph-based symmetric non-negative matrix factoring methods. \u2022 In addition to gene expression data, RMC-E generally outperforms RMC-C on the remaining data sets. We attribute this to the fact that they each use two different optimization methods to learn the multiple coefficients."}, {"heading": "F. Parameter Selection", "text": "There are three important parameters in our RMC method, i.e. the manifold regularization parameter \u03b1, the matching tolerance parameter \u03b2 and the number of nearest neighbors p. As the number of nearest neighbors has been set to 5 for all graph-based approaches and no distortion will occur during the comparison, it makes sense to neglect the model selection on the parameter meter p. As already mentioned in the parameter settings, the overmatched tolerance parameter \u03b2 was empirically set at \u03b2 = 0.1\u03b1 [21] in order to reduce the degree of parameter freedom in the objective function of the RMC. Therefore, it is only desirable to examine the influence of the parameter \u03b1 on the cluster performance of our method. To do this, we varied the parameter \u03b1 within a wide real value range, i.e., {0.001, 0.01, 0.1, 100, 500, 1500}. Figure 1 and 2 show the accuracy and opposite values respectively, if we can considerably magnify these values with the help of the EMDA by comparing the RMC."}, {"heading": "G. Study on Manifold Coefficients", "text": "Since manifold ensemble learning plays an essential role in the proposed RMC method, it is worth investigating the manifold coefficients learned through two different optimization algorithms, i.e. EMDA and CDA. We use the histogram to draw the distribution of the manifold coefficients derived from RMC among the best parameter settings. Figure 3 and 4 illustrate the histogram of the manifold coefficients for RMC-E and RMC-C. From these bars, it is easy to find that CDA only selects two or three manifolds for most datasets, suggesting that there may be some important information losses, which means that the improvement in cluster performance does not seem inspiring, as shown in Table III and IV. In contrast, EMDA assigns a different coefficient or weight to each manifolder, and the convex combination of all candidate falcons RMC indicates that the most important RMC manifolds are the two RMC manifolds together, as RMC manifold manifolds and RMC manifolds are the most important."}, {"heading": "V. CONCLUSIONS", "text": "This paper introduces a novel co-clustering approach called Relational Multi-mannifold Co-clustering (RMC), which is based on symmetrical non-negative matrix trifactorization. Our approach takes into account the inter-type relationship and intra-type information of both the sample and the trait data at the same time. The basic idea is to use multiple ensemble learning to maximize the performance of the co-cluster. To achieve this, we try to learn a meaningful convex combination of candidate manifolds so that they can approximate the true intrinsic manifolds of both the sample and the trait spaces. To optimize the objective function, we apply the popular alternating optimization method to update the factorized matrices. Unlike the existing matrix factorization-based co-clustering methods, however, there is a multiple cost-efficient approach that can be used in our coclustering method."}, {"heading": "ACKNOWLEDGMENT", "text": "This work was partially supported by the National Natural Science Foundation of China under funding programmes 91120302, 61222207, 60905001 and 61173186, the National Basic Research Programme of China (973 Programme) under funding programmes 2013CB336500, the Basic Research Fund for Central Universities under funding programmes 2012FZA5017 and the Key S & T Innovation Group Project of Zhejiang Province under funding programmes 2009R50009."}], "references": [], "referenceMentions": [], "year": 2016, "abstractText": "Co-clustering targets on grouping the samples<lb>(e.g., documents, users) and the features (e.g., words, ratings)<lb>simultaneously. It employs the dual relation and the bilateral<lb>information between the samples and features. In many real-<lb>world applications, data usually reside on a submanifold of<lb>the ambient Euclidean space, but it is nontrivial to estimate<lb>the intrinsic manifold of the data space in a principled<lb>way. In this study, we focus on improving the co-clustering<lb>performance via manifold ensemble learning, which is able to<lb>maximally approximate the intrinsic manifolds of both the<lb>sample and feature spaces. To achieve this, we develop a<lb>novel co-clustering algorithm called Relational Multi-manifold<lb>Co-clustering (RMC) based on symmetric nonnegative matrix<lb>tri-factorization, which decomposes the relational data matrix<lb>into three submatrices. This method considers the inter-<lb>type relationship revealed by the relational data matrix,<lb>and also the intra-type information reflected by the affinity<lb>matrices encoded on the sample and feature data distributions.<lb>Specifically, we assume the intrinsic manifold of the sample<lb>or feature space lies in a convex hull of some pre-defined<lb>candidate manifolds. We want to learn a convex combination<lb>of them to maximally approach the desired intrinsic manifold.<lb>To optimize the objective function, the multiplicative rules<lb>are utilized to update the submatrices alternatively. Besides,<lb>both the entropic mirror descent algorithm and the coordinate<lb>descent algorithm are exploited to learn the manifold coeffi-<lb>cient vector. Extensive experiments on documents, images and<lb>gene expression data sets have demonstrated the superiority<lb>of the proposed algorithm compared to other well-established<lb>methods.", "creator": "dvips(k) 5.991 Copyright 2011 Radical Eye Software"}}}