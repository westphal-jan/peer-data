{"id": "1611.05774", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Nov-2016", "title": "What Do Recurrent Neural Network Grammars Learn About Syntax?", "abstract": "Recurrent neural network grammars (RNNG) are a recently proposed probabilistic generative modeling family for natural language. They show state-of-the-art language modeling and parsing performance. We investigate what information they learn, from a linguistic perspective, through various ablations to the model and the data, and by augmenting the model with an attention mechanism (GA-RNNG) to enable closer inspection. We find that explicit modeling of composition is crucial for achieving the best performance. Through the attention mechanism, we find that headedness plays a central role in phrasal representation (with the model's latent attention largely agreeing with predictions made by hand-crafted rules, albeit with some important differences). By training grammars without non-terminal labels, we find that phrasal representations depend minimally on non-terminals, providing support for the endocentricity hypothesis.", "histories": [["v1", "Thu, 17 Nov 2016 16:41:41 GMT  (638kb,D)", "http://arxiv.org/abs/1611.05774v1", null], ["v2", "Tue, 10 Jan 2017 19:15:08 GMT  (526kb,D)", "http://arxiv.org/abs/1611.05774v2", "10 pages. To appear in EACL 2017, Valencia, Spain"]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["adhiguna kuncoro", "miguel ballesteros", "lingpeng kong", "chris dyer", "graham neubig", "noah a smith"], "accepted": false, "id": "1611.05774"}, "pdf": {"name": "1611.05774.pdf", "metadata": {"source": "CRF", "title": "What Do Recurrent Neural Network Grammars Learn About Syntax?", "authors": ["Adhiguna Kuncoro", "Miguel Ballesteros", "Lingpeng Kong", "Chris Dyer", "Graham Neubig", "Noah A. Smith"], "emails": ["akuncoro@cs.cmu.edu", "lingpenk@cs.cmu.edu", "gneubig@cs.cmu.edu", "miguel.ballesteros@ibm.com,", "nasmith@cs.washington.edu,", "cdyer@google.com"], "sections": [{"heading": "1 Introduction", "text": "In this article, we focus on a recently proposed class of probability distributions, focusing on recursive neural network grammatics (RNG; Dyer et al., 2016b), which are designed to model syntactical derivatives of natural theories. We focus on RNG as generative probability models of trees and give a brief summary of what they have \"discovered.\" In a sense, such models can be understood as a means to test or confirm a certain aspect of a theory. We talk about the assumptions of a model and sometimes examine its effects to understand what it \"discovered.\" For this reason, such models can be considered mini-scientists. Neural networks, including RNG, are able to represent much larger classes than conventional probability models, giving them more freedom. Unfortunately, they tend to be bad mini-scientists because their parameters are difficult to interpret."}, {"heading": "2 Recurrent Neural Network Grammars", "text": "An RNG defines a common probability distribution through string terminals and phrase structures that are not terminated. [2] Formally, the RNNG is defined by a triple < N, \u03a3, \u0432 >, where N describes the set of non-terminal symbols (NP, VP, etc.). [3] Unlike previous work, which relied on handwritten rules to compose more fine-grained phrases (Collins, 1997; Klein and Manning, 2003), the RNNG implicitly parameterizes the information passed on by the composition of the phrases (in relation to the neural network architecture), it can weaken the strong assumptions of independence in classical probable context-free grammars. The RNNG is based on an abstract state machine such as those used in the transition phase, with its algorithmic state consisting of a stack of partially closed constituents."}, {"heading": "3 Composition is Key", "text": "Faced with the same data and a discriminatory (and non-generative) educational goal, 5 RNNGs were found that analyze with significantly higher accuracy than the model developed by Vinyals et al. (2015), which presents y as a \"linearized\" sequence of symbols and brackets, without explicitly grasping the tree structure or narrowing down the y as a well-shaped tree (see Table 1), the latter model literally predicts the sequence of non-terminal, terminal, and bracket data structures from left to right, suggesting that the heart of the RNNG is the composition function (Fig. 2) that Vinyals et al. do not have. Indeed, the three data structures of the RNNG - stack, buffer, and action history - are redundant. For example, the action history and buffer content of the NNNG are fully determinable."}, {"heading": "3.1 Ablated RNNGs", "text": "In fact, it is a reactionary, reactionary, reactionary and reactionary party, capable of relying on reactionary, reactionary and reactionary forces."}, {"heading": "4 Gated Attention RNNG", "text": "Having established that the composition function is the key to the performance of RNNG (\u00a7 3), we are now trying to understand the nature of the learned composed phrases representations. Like most neural networks, interpreting the behavior of the composition function is a challenge. Fortunately, linguistic theories offer a number of hypotheses about the nature of the representations of phrases that can provide a conceptual framework for understanding them."}, {"heading": "4.1 Linguistic Hypotheses", "text": "We consider two theories about phratical representation: the first is that phrasal representations are strongly determined by a privileged lexical head. Extending grammars to include lexical head information has a long history in parsing, starting with the Collins models (1997), and more recent syntax theories such as the \"mere phrase structure\" hypothesis of the Minimalist program (Chomsky, 1993) postulate that phrases are represented purely by single lexical heads. Suggestions for multi-headed phrases (to deal with tricky cases such as conjunction) also exist (Jackendoff, 1977; Keenan, 1987). Regarding the question of the role of heads in phrasal representation, are the phrases learned from single lexical heads or multiple heads dependent on single lexical heads? Or do the representations combine information from all children without a prominent head? Regarding the role of heads in phrasal representation, the Phrase is neither a new material exrasive representation (NP) for an internal excentric representation (NP) nor a new one-excentric representation (NP)."}, {"heading": "4.2 Gated Attention Composition", "text": "In order to investigate what the RNNG has learned about headaches (and later endocentricity), we have developed a variant of the composition that allows the use of an explicit attention mechanism (Bahdanau et al., 2015), and we have developed a series of approaches that address the question of the extent to which it is worth drawing people's attention (from 0 to 1), and that the entirety of children is 1), and that the parental phrase is represented by the sum of the individual children outlined from their attention weight to their non-terminal type. Our weighted sum is more expressive than traditional head rules, but it allows attention to be divided into several constitutions. Head rules are analogous to draw all attention to a constitutive that contains the lexical copy. We are now formally defining the composition of the GA-RNNNG."}, {"heading": "5 Headedness in Phrases", "text": "We are now using the attention mechanism to find out what the RNNG learns about mind."}, {"heading": "5.1 The Heads GA-RNNG Learns", "text": "In fact, most people will be able to move to another world in which they are in the position in which they find themselves."}, {"heading": "5.2 Comparison to Existing Head Rules", "text": "In order to better measure the overlap between the attention vectors and the existing head rules, we converted the trees in PTB \u00a7 23 into a dependence representation using the attention weights. In this case, attention weight acts as a \"dynamic\" head rule, in which all other components within the same compound phrase are considered to modify the overlap between the resulting dependence tree and the conversion results of the same trees, which are recursively repeated.The head of the composite representation for \"S\" at the top of the tree is tied to a special root symbol and acts as the head of the sentencing judge. We evaluate the overlap between the resulting dependence tree and the conversion results of the same trees using the Collins (Collins, 1997) and Stanford Dependencies (De Marneffe et al., 2006) head rules. The results are overlapped using the standard evaluation scriptAS criteria and the conversion results of the same trees using the Collins (Collins, 1997) and Stanford Dependencies (De Marneffe et al., 2006) head rules."}, {"heading": "6 The Role of Non-terminal Labels", "text": "Encouraged by our finding that GA-RNNGs learn an idea of headiness, we next investigate whether heads are sufficient to produce phrase representations (in accordance with an endocentric theory of phrasal representation) or whether additional non-terminal information is required. If the endocentric hypothesis is correct (i.e., the representation of a phrase is constructed from the inside according to its components, but independent of explicit category designations), then the non-terminal types should be easily derived from the endocentric composition of the representation, and that removing the non-terminal information would not make much difference in performance. Specifically, we suggest that we use a GA-RNNG model on unlabeled trees (only brackets without non-terminal types), which are referred to as U-GA-RNNG. This idea has been explored in research on methods for learning syntax with less complete annotations (Pereira and Shabeira, 1992, Manning's Reduction, and Small Categories)."}, {"heading": "7 Related Work", "text": "The problem of understanding neural network models in NLP has already been investigated for sequential RNNs (Karpathy et al., 2015; Li et al., 2016). Shi et al. (2016) showed that sequence-to-sequence translation models capture a certain amount of syntactic knowledge as a by-product of the translation goal. Our experiment on12We see similar clustering for the undiscontinued GARNG model, which will not be shown soon. The importance of the compositional function was motivated by Vinyals et al. (2015) and Wiseman and Rush (2016), who achieved competitive analytical accuracy with sequence-to-sequence models. Extensive previous work on phrase structure analysis typically uses the probable context-free grammaticformalism with lexicalized (Collins, 1997) and non-terminals (Johnson, 1998; Klein and Manning, 2003) augmentations."}, {"heading": "8 Conclusion", "text": "Using ablation scenarios and a novel variant with attention mechanism (GA-RNNG), we investigate syntax, a probabilistic generative language model based on neural networks. Composition function, an important differentiator between the RNNG and other neural syntax models, is critical for good performance. Using attention weight vectors, we find that the model learns something similar to heads, even though the attention vectors are not completely aligned around a single component. We show some cases where the attention vector is divided and measure the relationship to existing head rules. RNGs without nonterminal information during training support the hypothesis that phase representations are largely endocentric, and a cluster analysis of representations shows that traditional nonterminal categories naturally fall out of the clustering of composed phrases."}, {"heading": "Acknowledgments", "text": "This work was partially funded by the Defense Advanced Research Projects Agency (DARPA) Information Innovation Office (I2O) under the Low Resource Languages for Emergent Incidents (LORELEI) program, issued by DARPA / I2O under contract number HR0011-15-C-0114, and partially supported by contract number W911NF-151-0543 with DARPA and the Army Research Office (ARO)."}], "references": [{"title": "Globally normalized transition-based neural networks", "author": ["Daniel Andor", "Chris Alberti", "David Weiss", "Aliaksei Severyn", "Alessandro Presta", "Kuzman Ganchev", "Slav Petrov", "Michael Collins."], "venue": "Proc. of ACL.", "citeRegEx": "Andor et al\\.,? 2016", "shortCiteRegEx": "Andor et al\\.", "year": 2016}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "Proc. of ICLR.", "citeRegEx": "Bahdanau et al\\.,? 2015", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Training with exploration improves a greedy stack-LSTM parser", "author": ["Miguel Ballesteros", "Yoav Goldberg", "Chris Dyer", "Noah A. Smith."], "venue": "Proc. of EMNLP.", "citeRegEx": "Ballesteros et al\\.,? 2016", "shortCiteRegEx": "Ballesteros et al\\.", "year": 2016}, {"title": "Recovering latent information in treebanks", "author": ["David Chiang", "Daniel M. Bikel."], "venue": "Proc. of COLING.", "citeRegEx": "Chiang and Bikel.,? 2002", "shortCiteRegEx": "Chiang and Bikel.", "year": 2002}, {"title": "Parsing as language modeling", "author": ["Do Kook Choe", "Eugene Charniak."], "venue": "Proc. EMNLP.", "citeRegEx": "Choe and Charniak.,? 2016", "shortCiteRegEx": "Choe and Charniak.", "year": 2016}, {"title": "Remarks on nominalization", "author": ["Noam Chomsky."], "venue": "Readings in English Transformational Grammar.", "citeRegEx": "Chomsky.,? 1970", "shortCiteRegEx": "Chomsky.", "year": 1970}, {"title": "Three generative, lexicalised models for statistical parsing", "author": ["Michael Collins."], "venue": "Proc. of EACL.", "citeRegEx": "Collins.,? 1997", "shortCiteRegEx": "Collins.", "year": 1997}, {"title": "Stanford typed dependencies manual", "author": ["Marie-Catherine De Marneffe", "Christopher D. Manning"], "venue": null, "citeRegEx": "Marneffe and Manning.,? \\Q2008\\E", "shortCiteRegEx": "Marneffe and Manning.", "year": 2008}, {"title": "Generating typed dependency parses from phrase structure parses", "author": ["Marie-Catherine De Marneffe", "Bill MacCartney", "Christopher D. Manning."], "venue": "Proc. of LREC.", "citeRegEx": "Marneffe et al\\.,? 2006", "shortCiteRegEx": "Marneffe et al\\.", "year": 2006}, {"title": "Deep biaffine attention for neural dependency parsing", "author": ["Timothy Dozat", "Christopher D. Manning."], "venue": "ArXiv.", "citeRegEx": "Dozat and Manning.,? 2016", "shortCiteRegEx": "Dozat and Manning.", "year": 2016}, {"title": "Corrigendum to recurrent neural network grammars", "author": ["Chris Dyer", "Adhiguna Kuncoro", "Miguel Ballesteros", "Noah A. Smith."], "venue": "CoRR.", "citeRegEx": "Dyer et al\\.,? 2016a", "shortCiteRegEx": "Dyer et al\\.", "year": 2016}, {"title": "Recurrent neural network grammars", "author": ["Chris Dyer", "Adhiguna Kuncoro", "Miguel Ballesteros", "Noah A. Smith."], "venue": "Proc. of NAACL.", "citeRegEx": "Dyer et al\\.,? 2016b", "shortCiteRegEx": "Dyer et al\\.", "year": 2016}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "Neural Comput.", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Extended constituent-to-dependency conversion for English", "author": ["Richard Johansson", "Pierre Nugues."], "venue": "Proc. of NODALIDA.", "citeRegEx": "Johansson and Nugues.,? 2007", "shortCiteRegEx": "Johansson and Nugues.", "year": 2007}, {"title": "PCFG models of linguistic tree representations", "author": ["Mark Johnson."], "venue": "Comput. Linguist.", "citeRegEx": "Johnson.,? 1998", "shortCiteRegEx": "Johnson.", "year": 1998}, {"title": "Visualizing and understanding recurrent networks", "author": ["Andrej Karpathy", "Justin Johnson", "Fei-Fei Li."], "venue": "CoRR.", "citeRegEx": "Karpathy et al\\.,? 2015", "shortCiteRegEx": "Karpathy et al\\.", "year": 2015}, {"title": "Multiply-headed noun phrases", "author": ["Edward L. Keenan."], "venue": "Linguistic Inquiry, 18(3):481\u2013490.", "citeRegEx": "Keenan.,? 1987", "shortCiteRegEx": "Keenan.", "year": 1987}, {"title": "Simple and accurate dependency parsing using bidirectional LSTM feature representations", "author": ["Eliyahu Kiperwasser", "Yoav Goldberg."], "venue": "TACL.", "citeRegEx": "Kiperwasser and Goldberg.,? 2016", "shortCiteRegEx": "Kiperwasser and Goldberg.", "year": 2016}, {"title": "A generative constituent-context model for improved grammar induction", "author": ["Dan Klein", "Christopher D. Manning."], "venue": "Proc. of ACL.", "citeRegEx": "Klein and Manning.,? 2002", "shortCiteRegEx": "Klein and Manning.", "year": 2002}, {"title": "Accurate unlexicalized parsing", "author": ["Dan Klein", "Christopher D. Manning."], "venue": "Proc. of ACL.", "citeRegEx": "Klein and Manning.,? 2003", "shortCiteRegEx": "Klein and Manning.", "year": 2003}, {"title": "An empirical comparison of parsing methods for stanford dependencies", "author": ["Lingpeng Kong", "Noah A. Smith."], "venue": "CoRR.", "citeRegEx": "Kong and Smith.,? 2014", "shortCiteRegEx": "Kong and Smith.", "year": 2014}, {"title": "Distilling an ensemble of greedy dependency parsers into one MST parser", "author": ["Adhiguna Kuncoro", "Miguel Ballesteros", "Lingpeng Kong", "Chris Dyer", "Noah A. Smith."], "venue": "Proc. of EMNLP.", "citeRegEx": "Kuncoro et al\\.,? 2016", "shortCiteRegEx": "Kuncoro et al\\.", "year": 2016}, {"title": "Visualizing and understanding neural models in nlp", "author": ["Jiwei Li", "Xinlei Chen", "Eduard Hovy", "Dan Jurafsky."], "venue": "Proc. of NAACL.", "citeRegEx": "Li et al\\.,? 2016", "shortCiteRegEx": "Li et al\\.", "year": 2016}, {"title": "Effective self-training for parsing", "author": ["David McClosky", "Eugene Charniak", "Mark Johnson."], "venue": "Proc. NAACL.", "citeRegEx": "McClosky et al\\.,? 2006", "shortCiteRegEx": "McClosky et al\\.", "year": 2006}, {"title": "The Need for Biases in Learning Generalizations", "author": ["Tom M. Mitchell."], "venue": "Department of Computer Science, Laboratory for Computer Science Research, Rutgers University.", "citeRegEx": "Mitchell.,? 1980", "shortCiteRegEx": "Mitchell.", "year": 1980}, {"title": "Insideoutside reestimation from partially bracketed corpora", "author": ["Fernando Pereira", "Yves Schabes."], "venue": "Proc. of ACL.", "citeRegEx": "Pereira and Schabes.,? 1992", "shortCiteRegEx": "Pereira and Schabes.", "year": 1992}, {"title": "Improved inference for unlexicalized parsing", "author": ["Slav Petrov", "Dan Klein."], "venue": "Proc. of NAACL.", "citeRegEx": "Petrov and Klein.,? 2007", "shortCiteRegEx": "Petrov and Klein.", "year": 2007}, {"title": "Learning accurate, compact, and interpretable tree annotation", "author": ["Slav Petrov", "Leon Barrett", "Romain Thibaux", "Dan Klein."], "venue": "Proc. of COLING-ACL.", "citeRegEx": "Petrov et al\\.,? 2006", "shortCiteRegEx": "Petrov et al\\.", "year": 2006}, {"title": "Does string-based neural mt learn source syntax? In Proc", "author": ["Xing Shi", "Inkit Padhi", "Kevin Knight."], "venue": "of EMNLP.", "citeRegEx": "Shi et al\\.,? 2016", "shortCiteRegEx": "Shi et al\\.", "year": 2016}, {"title": "Bayesian symbol-refined tree substitution grammars for syntactic parsing", "author": ["Hiroyuki Shindo", "Yusuke Miyao", "Akinori Fujino", "Masaaki Nagata."], "venue": "Proc. ACL.", "citeRegEx": "Shindo et al\\.,? 2012", "shortCiteRegEx": "Shindo et al\\.", "year": 2012}, {"title": "Visualizing high-dimensional data using tsne", "author": ["Laurens van der Maaten", "Geoffrey E. Hinton."], "venue": "Journal of Machine Learning Research, 9.", "citeRegEx": "Maaten and Hinton.,? 2008", "shortCiteRegEx": "Maaten and Hinton.", "year": 2008}, {"title": "Grammar as a foreign language", "author": ["Oriol Vinyals", "Lukasz Kaiser", "Terry Koo", "Slav Petrov", "Ilya Sutskever", "Geoffrey Hinton."], "venue": "Proc. of NIPS.", "citeRegEx": "Vinyals et al\\.,? 2015", "shortCiteRegEx": "Vinyals et al\\.", "year": 2015}, {"title": "Graph-based dependency parsing with bidirectional LSTM", "author": ["Wenhui Wang", "Baobao Chang."], "venue": "Proc. of ACL.", "citeRegEx": "Wang and Chang.,? 2016", "shortCiteRegEx": "Wang and Chang.", "year": 2016}, {"title": "Sequence-to-sequence learning as beam-search optimization", "author": ["Sam Wiseman", "Alexander M. Rush."], "venue": "Proc. of EMNLP.", "citeRegEx": "Wiseman and Rush.,? 2016", "shortCiteRegEx": "Wiseman and Rush.", "year": 2016}, {"title": "Fast and accurate shiftreduce constituent parsing", "author": ["Muhua Zhu", "Yue Zhang", "Wenliang Chen", "Min Zhang", "Jingbo Zhu."], "venue": "Proc. ACL.", "citeRegEx": "Zhu et al\\.,? 2013", "shortCiteRegEx": "Zhu et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 11, "context": "In this paper, we focus on a recently proposed class of probability distributions, recurrent neural network grammars (RNNGs; Dyer et al., 2016b), designed to model syntactic derivations of natural language sentences.", "startOffset": 117, "endOffset": 144}, {"referenceID": 31, "context": "RNNGs have less inductive bias relative to traditional unlexicalized probabilistic context-free grammars, but more than models that parse by transducing word sequences to linearized parse trees represented as strings (Vinyals et al., 2015).", "startOffset": 217, "endOffset": 239}, {"referenceID": 24, "context": "Inductive bias is necessary for learning (Mitchell, 1980); we believe the important question is not \u201chow little can a model get away with?\u201d but rather the benefit of different forms of inductive bias as data vary.", "startOffset": 41, "endOffset": 57}, {"referenceID": 6, "context": "Unlike previous works that rely on hand-crafted rules to compose more fine-grained phrase representations (Collins, 1997; Klein and Manning, 2003), the RNNG implicitly parameterizes the information passed through compositions of phrases (in \u0398 and the neural network architecture), hence weakening the strong independence assumptions in classical probabilistic context-free grammars.", "startOffset": 106, "endOffset": 146}, {"referenceID": 19, "context": "Unlike previous works that rely on hand-crafted rules to compose more fine-grained phrase representations (Collins, 1997; Klein and Manning, 2003), the RNNG implicitly parameterizes the information passed through compositions of phrases (in \u0398 and the neural network architecture), hence weakening the strong independence assumptions in classical probabilistic context-free grammars.", "startOffset": 106, "endOffset": 146}, {"referenceID": 10, "context": "This figure is due to Dyer et al. (2016b).", "startOffset": 22, "endOffset": 42}, {"referenceID": 12, "context": "At each time step, the model encodes the stack, buffer, and past actions, with a separate LSTM (Hochreiter and Schmidhuber, 1997) network for each component as features to define a distribution over the next action to take (conditioned on the full algorithmic state).", "startOffset": 95, "endOffset": 129}, {"referenceID": 10, "context": "The overall architecture is illustrated in Figure 1; examples of full action sequences can be found in Dyer et al. (2016b). A key element of the RNNG is the composition function, which reduces a completed constituent into a single element on the stack.", "startOffset": 103, "endOffset": 123}, {"referenceID": 11, "context": "Figure 2: RNNG composition function on each REDUCE operation; the network on the right models the structure on the left (Dyer et al., 2016b).", "startOffset": 120, "endOffset": 140}, {"referenceID": 10, "context": "4 We report all RNNG performance based on the Corrigendum to Recurrent Neural Network Grammars (Dyer et al., 2016a).", "startOffset": 95, "endOffset": 115}, {"referenceID": 31, "context": "Given the same data and a discrimintive (rather than generative) training objective,5 RNNGs were found to parse with significantly higher accuracy than the model of Vinyals et al. (2015) that represents y as a \u201clinearized\u201d sequence of symbols and parentheses without explicitly capturing the tree structure, or constraining the y to be a wellformed tree (see Table 1).", "startOffset": 165, "endOffset": 187}, {"referenceID": 31, "context": "Model F1 Vinyals et al. (2015) \u2013 WSJ only 88.", "startOffset": 9, "endOffset": 31}, {"referenceID": 10, "context": "In Dyer et al. (2016b) and this paper, the proposal distribution is the discriminative variant of the RNNG, see Dyer et al.", "startOffset": 3, "endOffset": 23}, {"referenceID": 10, "context": "In Dyer et al. (2016b) and this paper, the proposal distribution is the discriminative variant of the RNNG, see Dyer et al. (2016b). As mentioned before, RNNGs can be trained either generatively or discriminatively (conditional on the sentence); to isolate the impact of architectural differences between the RNNG model and Vinyals et al.", "startOffset": 3, "endOffset": 132}, {"referenceID": 8, "context": "We trained each ablation from scratch, and compared these models on three tasks: English phrase-structure parsing (labeled F1), Table 2; dependency parsing, Table 3 by converting parse output to Stanford dependencies (De Marneffe et al., 2006), using the tool by Kong and Smith (2014), and language modeling, Table 4.", "startOffset": 221, "endOffset": 285}, {"referenceID": 31, "context": "Note that the ablated RNNG without a stack is similar to Vinyals et al. (2015), who encoded a (partial) phrase-structure tree as a sequence of open and close parentheses, terminals, and non-terminal symbols; our action history is quite close to this, with each NT(X) capturing a left parenthesis and X nonterminal, and each REDUCE capturing a right parenthesis.", "startOffset": 57, "endOffset": 79}, {"referenceID": 24, "context": "Model F1 Petrov and Klein (2007) 90.", "startOffset": 9, "endOffset": 33}, {"referenceID": 24, "context": "Model F1 Petrov and Klein (2007) 90.1 Shindo et al. (2012) 91.", "startOffset": 9, "endOffset": 59}, {"referenceID": 24, "context": "Model F1 Petrov and Klein (2007) 90.1 Shindo et al. (2012) 91.1 Zhu et al. (2013)\u2020 91.", "startOffset": 9, "endOffset": 82}, {"referenceID": 22, "context": "3 McClosky et al. (2006)\u2020 92.", "startOffset": 2, "endOffset": 25}, {"referenceID": 22, "context": "3 McClosky et al. (2006)\u2020 92.1 Vinyals et al. (2015)\u2020 92.", "startOffset": 2, "endOffset": 53}, {"referenceID": 4, "context": "1 Choe and Charniak (2016) 92.", "startOffset": 2, "endOffset": 27}, {"referenceID": 4, "context": "1 Choe and Charniak (2016) 92.6 Choe and Charniak (2016)\u2020 93.", "startOffset": 2, "endOffset": 57}, {"referenceID": 1, "context": "9 Ballesteros et al. (2016) 93.", "startOffset": 2, "endOffset": 28}, {"referenceID": 1, "context": "9 Ballesteros et al. (2016) 93.6 91.4 Wang and Chang (2016) 94.", "startOffset": 2, "endOffset": 60}, {"referenceID": 1, "context": "9 Ballesteros et al. (2016) 93.6 91.4 Wang and Chang (2016) 94.1 91.8 Kuncoro et al. (2016) 94.", "startOffset": 2, "endOffset": 92}, {"referenceID": 0, "context": "1 Andor et al. (2016) 94.", "startOffset": 2, "endOffset": 22}, {"referenceID": 0, "context": "1 Andor et al. (2016) 94.6 92.8 Dozat and Manning (2016) 95.", "startOffset": 2, "endOffset": 57}, {"referenceID": 0, "context": "1 Andor et al. (2016) 94.6 92.8 Dozat and Manning (2016) 95.4 93.8 Choe and Charniak (2016)\u2020 95.", "startOffset": 2, "endOffset": 92}, {"referenceID": 5, "context": "Augmenting grammars with lexical head information has a long history in parsing, starting with the models of Collins (1997), and recent theories of syntax such as the \u201cbare phrase structure\u201d hypothesis of the Minimalist Program (Chomsky, 1993) posit that phrases are represented purely by single lexical heads.", "startOffset": 109, "endOffset": 124}, {"referenceID": 16, "context": "likewise exist (Jackendoff, 1977; Keenan, 1987).", "startOffset": 15, "endOffset": 47}, {"referenceID": 5, "context": "To illustrate the contrast between endocentric and exocentric representations, an endocentric representation is representing a noun phrase with a noun category, whereas S\u2192NP VP exocentrically introduces a new syntactic category that is neither NP nor VP (Chomsky, 1970).", "startOffset": 254, "endOffset": 269}, {"referenceID": 1, "context": "To investigate what the RNNG learns about headedness (and later endocentricity), we propose a variant of the composition function that makes use of an explicit attention mechanism (Bahdanau et al., 2015) and a sigmoid gate with multiplicative interactions, henceforth called GA-RNNG.", "startOffset": 180, "endOffset": 203}, {"referenceID": 22, "context": "Similar with Li et al. (2016) where sequential LSTMs discover polarity information in sentiment analysis, although perhaps more surprising as polarity information is less intuitively central to syntactic modeling.", "startOffset": 13, "endOffset": 30}, {"referenceID": 13, "context": "tions are assigned a lower priority when composing PPs, although more consistent with the Johansson head rule (Johansson and Nugues, 2007).", "startOffset": 110, "endOffset": 138}, {"referenceID": 6, "context": "We evaluate the overlap between the resulting dependency tree and conversion results of the same trees using the Collins (Collins, 1997) and Stanford Dependencies (De Marneffe et al.", "startOffset": 121, "endOffset": 136}, {"referenceID": 25, "context": "This idea has been explored in research on methods for learning syntax with less complete annotation (Pereira and Schabes, 1992).", "startOffset": 101, "endOffset": 128}, {"referenceID": 18, "context": "A key finding from Klein and Manning (2002) was that, given bracketing structure, simple dimensionality reduction techniques could reveal conventional non-terminal categories with high accuracy; Petrov et al.", "startOffset": 19, "endOffset": 44}, {"referenceID": 18, "context": "A key finding from Klein and Manning (2002) was that, given bracketing structure, simple dimensionality reduction techniques could reveal conventional non-terminal categories with high accuracy; Petrov et al. (2006) also showed that latent vari-", "startOffset": 19, "endOffset": 216}, {"referenceID": 15, "context": "The problem of understanding neural network models in NLP has been previously studied for sequential RNNs (Karpathy et al., 2015; Li et al., 2016).", "startOffset": 106, "endOffset": 146}, {"referenceID": 22, "context": "The problem of understanding neural network models in NLP has been previously studied for sequential RNNs (Karpathy et al., 2015; Li et al., 2016).", "startOffset": 106, "endOffset": 146}, {"referenceID": 15, "context": "The problem of understanding neural network models in NLP has been previously studied for sequential RNNs (Karpathy et al., 2015; Li et al., 2016). Shi et al. (2016) showed that sequenceto-sequence machine translation models capture a certain degree of syntactic knowledge as a byproduct of the translation objective.", "startOffset": 107, "endOffset": 166}, {"referenceID": 31, "context": "the importance of composition function was motivated by Vinyals et al. (2015) and Wiseman and Rush (2016), who achieved competitive parsing accuracy with sequence-to-sequence models.", "startOffset": 56, "endOffset": 78}, {"referenceID": 31, "context": "the importance of composition function was motivated by Vinyals et al. (2015) and Wiseman and Rush (2016), who achieved competitive parsing accuracy with sequence-to-sequence models.", "startOffset": 56, "endOffset": 106}, {"referenceID": 6, "context": "Extensive previous work on phrase-structure parsing typically employs the probabilistic context-free grammar formalism, with lexicalized (Collins, 1997) and non-terminal (Johnson, 1998; Klein and Manning, 2003) augmentations; the RNNG has less inductive bias than these earlier models and hence a weaker independence assumption.", "startOffset": 137, "endOffset": 152}, {"referenceID": 14, "context": "Extensive previous work on phrase-structure parsing typically employs the probabilistic context-free grammar formalism, with lexicalized (Collins, 1997) and non-terminal (Johnson, 1998; Klein and Manning, 2003) augmentations; the RNNG has less inductive bias than these earlier models and hence a weaker independence assumption.", "startOffset": 170, "endOffset": 210}, {"referenceID": 19, "context": "Extensive previous work on phrase-structure parsing typically employs the probabilistic context-free grammar formalism, with lexicalized (Collins, 1997) and non-terminal (Johnson, 1998; Klein and Manning, 2003) augmentations; the RNNG has less inductive bias than these earlier models and hence a weaker independence assumption.", "startOffset": 170, "endOffset": 210}, {"referenceID": 3, "context": "The conjecture that fine-grained non-terminal rules and labels can be discovered given weaker bracketing structures were based on several studies (Chiang and Bikel, 2002; Klein and Manning, 2002; Petrov et al., 2006).", "startOffset": 146, "endOffset": 216}, {"referenceID": 18, "context": "The conjecture that fine-grained non-terminal rules and labels can be discovered given weaker bracketing structures were based on several studies (Chiang and Bikel, 2002; Klein and Manning, 2002; Petrov et al., 2006).", "startOffset": 146, "endOffset": 216}, {"referenceID": 27, "context": "The conjecture that fine-grained non-terminal rules and labels can be discovered given weaker bracketing structures were based on several studies (Chiang and Bikel, 2002; Klein and Manning, 2002; Petrov et al., 2006).", "startOffset": 146, "endOffset": 216}], "year": 2016, "abstractText": "Recurrent neural network grammars (RNNG) are a recently proposed probablistic generative modeling family for natural language. They show state-ofthe-art language modeling and parsing performance. We investigate what information they learn, from a linguistic perspective, through various ablations to the model and the data, and by augmenting the model with an attention mechanism (GA-RNNG) to enable closer inspection. We find that explicit modeling of composition is crucial for achieving the best performance. Through the attention mechanism, we find that headedness plays a central role in phrasal representation (with the model\u2019s latent attention largely agreeing with predictions made by hand-crafted head rules, albeit with some important differences). By training grammars without non-terminal labels, we find that phrasal representations depend minimally on non-terminals, providing support for the endocentricity hypothesis.", "creator": "TeX"}}}