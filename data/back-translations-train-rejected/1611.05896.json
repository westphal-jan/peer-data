{"id": "1611.05896", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Nov-2016", "title": "Answering Image Riddles using Vision and Reasoning through Probabilistic Soft Logic", "abstract": "In this work, we explore a genre of puzzles (\"image riddles\") which involves a set of images and a question. Answering these puzzles require both capabilities involving visual detection (including object, activity recognition) and, knowledge-based or commonsense reasoning. We compile a dataset of over 3k riddles where each riddle consists of 4 images and a groundtruth answer. The annotations are validated using crowd-sourced evaluation. We also define an automatic evaluation metric to track future progress. Our task bears similarity with the commonly known IQ tasks such as analogy solving, sequence filling that are often used to test intelligence.", "histories": [["v1", "Thu, 17 Nov 2016 21:10:33 GMT  (14527kb,D)", "http://arxiv.org/abs/1611.05896v1", "14 pages, 10 figures"]], "COMMENTS": "14 pages, 10 figures", "reviews": [], "SUBJECTS": "cs.CV cs.AI", "authors": ["somak aditya", "yezhou yang", "chitta baral", "yiannis aloimonos"], "accepted": false, "id": "1611.05896"}, "pdf": {"name": "1611.05896.pdf", "metadata": {"source": "CRF", "title": "Answering Image Riddles using Vision and Reasoning through Probabilistic Soft Logic", "authors": ["Somak Aditya", "Yezhou Yang", "Chitta Baral", "Yiannis Aloimonos"], "emails": ["saditya1@asu.edu", "yz.yang@asu.edu", "chitta@asu.edu", "yiannis@cs.umd.edu"], "sections": [{"heading": null, "text": "We are developing a Probabilistic Reasoning-based approach based on probabilistic common sense to answer these puzzles with reasonable accuracy. We are demonstrating the results of our approach using both automatic and human assessments. Our approach is producing some promising results for these puzzles and provides a solid foundation for future experiments. We are making the entire data set and related materials available to the community on the ImageRiddle website (http: / / bit.ly / 22f9Ala)."}, {"heading": "1. Introduction", "text": "In fact, it is the case that most people who are in a position to put themselves in another world, to put themselves in another world, in which they are able to put themselves in a position, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they, in which they live, in which they, in which they live, in which they, in which they, in which they live, in which they, in which they, in which they live, in which they, in which they live, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, live, in which they, in which they, in which they, live, in which they, in which they, in which they, in which they, in fact, in which they, live, in which they, in which they, in which they, live, in which they, in which they, live, in which they, in which they, in which they, in which they, in fact, live, in which they, in which they, in which they, in which they, in fact, in which they, in fact, are able to put themselves are able to put themselves, in a different world, in a different world, in a different world, in a different world, in which they, in which they are able to put themselves, in"}, {"heading": "2. Related Work", "text": "The problem of image riddles has some similarities to the genre of theme modeling [3] and zero-shot learning [13]. However, this data set presents some unique challenges: i) the possible set of target names is the entire Natural Language vocabulary; ii) each image, when grouped with different image sets, can be assigned to a different label; iii) almost all target names in the data set are unique (3k examples with 3k class labels).These challenges make it difficult to perform 2 examples: word analogy tasks (male:: female:::: king:?); numerical sequence filling tasks: (1, 2, 3, 5,?).3PSL proves to be a powerful framework for high-level computer vision tasks such as activity detection [16]. Direct adoption of theme model-based or zero-shot data processing methods is a response to data processing."}, {"heading": "3. Background", "text": "In this section, we briefly introduce the various techniques and knowledge sources used in our system."}, {"heading": "3.1. Probabilistic Soft Logic (PSL)", "text": "x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x,"}, {"heading": "3.2. ConceptNet", "text": "ConceptNet [22] is a multilingual knowledge diagram that encodes common sense knowledge about the world and is primarily used to support systems that attempt to understand natural language text. ConceptNet knowledge is semi-curated; the nodes in the diagram are words or short phrases written in natural language; the nodes are connected by edges (so-called assessments) that are labeled with meaningful relationships (selected from a well-defined set of closed relation labels); for example: (reptile, IsA, animal), (reptile, hasProperty, cold-blooded) are some edges; each edge has an associated confidence value. Also, compared to other knowledge databases such as WordNet, YAGO, NELL [23, 20] ConceptNet has more comprehensive coverage of English-language words and phrases. These characteristics make this knowledge diagram a perfect apparent source for general knowledge."}, {"heading": "3.3. Word2vec", "text": "Word2vec uses the theory of distributional semantics 4 to capture word meanings and generate word embeddings (vectors).The pre-trained word embeddings have been successfully used in numerous applications for processing natural language, and the induced vector space is known to capture the graduated similarities between words with reasonable accuracy [19]. Throughout the essay, we use the 3 million word vectors trained on the Google News corpus for word2vec-based similarities [19]."}, {"heading": "4. Approach", "text": "Considering a set of images (in our case four: {I1, I2, I3, I4}), the goal is to determine a set of words (T) based on how well the word links these images semantically. In this paper, we present an approach that uses Probabilistic Reasoning based on a probabilistic knowledge base (ConceptNet) and uses additional semantic knowledge about words from Word2vec. Using these knowledge sources, we predict the answers to the puzzles."}, {"heading": "4.1. Outline of our Framework", "text": "Algorithm 1. Solving puzzles 1: Procedure UNRIDDLER (I = {I1, I2, I3, I4}, Kcnet) 2: for s'Sk do 5: Ts, Wm (s, Ts) = retrieveTargets (s, Kcnet);.Wm (s, tj) = sim (s, tj).Ts 6: end for 7: Tk = rankTopTargets (P, Sk), TSk, Wm); 8: I (T) = inferConfidenceStageI (Tk, P (Sk, Ik)."}, {"heading": "4.2. Image Classification", "text": "Neural networks that have been trained on extensive image sources and numerous image classes are proving to be very effective. Studies have shown that Convolutionary Neural Networks (CNN) can produce near-accuracy in image classification on a human scale [12], and related work has been used in various visual recognition tasks such as scene labeling [5] and object recognition [7]. To take advantage of these advances, we use state-of-the-art detection provided by the Clarifai API [21] and the Deep Residual Network Architecture by [8] (using the trained ResNet 200 model). For each image (IC), we use Top 20 detection (Sk). Let's call these detections seeds. An example can be found in Figure 2."}, {"heading": "4.3. Rank and Retrieve Related Words", "text": "Our goal is to logically infer words or phrases that represent (higher or lower) concepts that best explain the coexistence of the seeds in a scene. Let's say that for \"hand\" and \"care,\" implicit words could be \"massage,\" \"sick,\" \"pain,\" etc. For \"transportation\" and \"sitting,\" implicit words / phrases could be \"sitting on the bus,\" \"sitting on the plane,\" etc. The reader might be inclined to derive other concepts. However, \"inferring\" means deriving \"logical\" conclusions. Therefore, we prefer concepts that have strong explainable links to the seeds. A logical choice would be to traverse a knowledge diagram like ConceptNet and find the common accessible nodes from these seeds. As this is quite unworkable from a computational point, we use the association space matrix representation of ConceptNet, in which the words are represented as vectors representing the similarity between the two words."}, {"heading": "4.3.1 Retrieve Related Words For a Seed", "text": "Visual similarity: We observe that in objects, the ConceptNet similarity yields a poor result (see Table 1). So we define a metric called visual similarity. Let's name the similar words as targets. In this metric, we represent the seed and the target as vectors. To define the dimensions for each seed, we use a series of relationships (HasA, HasA, PartOf and MemberOf). We question ConceptNet to obtain the related words (say, W1, W2, W3...) among such relationships for the seed and its superclasses. Each of these relation word pairs (i.e. HasA-W1, HasA-W2, PartOf-W3) becomes a separate dimension. The values for the seed vector are those assigned to the claims."}, {"heading": "4.3.2 Rank Targets", "text": "The columns of Wm provide vector representations for the target words (t-TSk) in space. We calculate cosinal similarities for each target with such an image vector and rearrange the targets. We look at the uppermost \u03b8 # t targets and call it Tk."}, {"heading": "4.4. Probabilistic Reasoning and Inference", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.4.1 PSL Inference Stage I", "text": "Considering a set of goals Tk and a set of goals (Sk, P, Sk, Ik), we build an inference model to derive a set of probable goals (T, K, K) from it. We model the common distribution with PSL, as this formality assumes Markov Random Field, which obeys the characteristics of Gibbs Distribution. (In addition, a PSL model can best be presented as an indirect model that includes seeds and goals, as given in Figure 3. Formulation.) We add two sets of rules to define seed target potentials, we add rules of form wtij."}, {"heading": "4.4.2 PSL Inference Stage II", "text": "To jointly determine the most likely common goals, we look at the goals and seeds from all the images together. Let's assume that the seeds and the goals are nodes in a knowledge chart. Then, the most appropriate target nodes should consider similar properties as an appropriate answer to the puzzle: i) a target node should be connected to the high-weight seeds in an image, i.e. it should refer to the important aspects of the image; ii) a target node should be connected to seeds from all the images. Formulation: Here we use the rules wtij: sik: sik \u2192 tjk for each word sik: Sk and target tjk: T-k for all k goals {1, 2.., 4}. To make the set of goals compete with each other, we add the restriction g 4 k = 1% j: tjk = T-k I (tjk)."}, {"heading": "5. Experiments and Results", "text": "In this section, we present the results of the validation experiments of the newly introduced Image Riddle dataset, followed by an empirical evaluation of the proposed approach versus pure vision baselines."}, {"heading": "5.1. Dataset Validation and Analysis", "text": "We have collected a series of 3333 puzzles from the Internet (puzzle websites), each of which has 4 images (66 x 66 cm, 6 KB in size) and a related basic truth label. To verify the basic truth answers, we define the measurements: i) \"correctness\" - how correct and appropriate the answers are, and ii) \"difficulty\" - how difficult the puzzles are. We perform a rating based on the Amazon Mechanical Turker. We ask them to rate the correctness from 1-65. The \"difficulty\" is from 1-76. According to the Turks, the average correctness rating is 4.4 (with a standard deviation of 1.5). The \"difficulty ratings\" show the following distribution: toddler (0.27%), younger child (8.96%), older child (30.3%), teenager (36.7%), adult (19%), linguist (3.6%), no one (0.64%). In short, the average age to answer this query seems to be closer to (17%)."}, {"heading": "5.2. System Evaluation", "text": "The presented approach suggests the following hypotheses, which require empirical testing: I) the proposed approach (and its variants) achieve reasonable accuracy in solving the puzzles; II) the individual phases of the framework improve the final accuracy of the answers. In addition, we are also experimenting to observe the effects of using commercial classification methods such as Clarifai versus a published state-of-the-art image classification methodology. 51: Completely confusing, incorrect, 2: refers to an image, 3 and 4: connects two or three images, 5: connects all four images, but could be a better answer, 6: connects all images and an appropriate answer. 6These gradations are taken from the instructions of the VQA AMT [1]. 1: A toddler can solve them (ages: 3-4), 2: A younger child can solve them (age: 58), 3: An older child can solve them (age: 9-12), 4: A teenager can solve them (age: 3-4), a child can solve them (age: 7), and only an adult can solve them."}, {"heading": "5.2.1 Systems", "text": "This year, the number of registered refugees in the EU has multiplied in recent years, and the number of registered asylum seekers in the EU has multiplied in recent years."}, {"heading": "5.2.2 Experiment I: Automatic Evaluation", "text": "We evaluate the performance of the proposed approach on the dataset 3333 Image Riddles with automatic and Amazon Mechanical Turker (AMT) -based ratings. As a yardstick, we use word2vec similarity yardsticks. An answer to a puzzle can have several semantically similar answers. Therefore, it makes sense to use such a measurement yardstick. For each puzzle, we calculate the maximum similarity between the basic truth and the top 10 detections from one approach. To calculate the similarity of phrases, we use the n-similarity method of the gensim.models.word2vec package. The average of these maximum similarities is shown in percentage formulars.To select the parameters in the parameter vector climate. To select the parameters, we used a random search in the parameter space over the first 500 puzzles over 500 combinations. The final set of parameters used and their values are presented in All 3.1 of the results (RVB and 33) based on the variations of the GR and GR (UR)."}, {"heading": "5.2.3 Experiment II: Human Evaluation", "text": "We perform an AMT-based comparative evaluation of the results of the proposed approach (GUR + All using Clarifai) and two vision-only baselines. We define two metrics: i) \"correctness\" and ii) \"intelligence.\" The Turks are presented with a scenario: We have three separate robots that have tried to solve this puzzle. They have to evaluate the answer by the correctness and the degree of intelligence (explanability) shown by the answer. Correctness is defined as before. Furthermore, the Turks are asked to rate intelligence on a scale of 1-48. We give the percentage of total puzzles per each value of correctness and intelligence in Figure 4. In these histograms we expect an increase in the smallest buckets for the \"more correct\" and \"smarter\" systems.. Figure 4. AMT Results of The GUR + All (our), Clarifai (Baseline 1) and ResidualNet (Baseline 2)."}, {"heading": "5.2.4 Analysis", "text": "Experiment I shows that the GUR variant (GUR + All in Table 2) achieves the best results in terms of word2vec-based accuracy. A similar trend is reflected in the AMT-based ratings (Figure 4). Our system has increased the percentage of puzzles for the farthest right-hand containers, i.e. more \"correct\" and \"intelligent\" answers are produced for a greater number of puzzles. word2vec-based accuracy brings the performance of the ResNet baseline close to that of the GUR variant. However, as shown in Figure 4, the AMT evaluation of correctness clearly shows that the ResNet baseline lags behind in predicting significant replies. Experiment II also contains what the Turks think about the intelligence of the systems that have tried to solve the puzzles. This also puts the GUR variant at the top. The two above experiments demonstrate empirically: 81, Not Intelligent, 2: Very Intelligent, 3: Very Intelligent."}, {"heading": "6. Conclusion and Future Works", "text": "In this paper, we presented a probabilistic reasoning-based approach to solving a new class of image puzzles called \"Image Riddles.\" We have collected more than 3k such puzzles; the crowd-sourced evaluation of the data set demonstrates the validity of the annotations and the difficulty of the puzzles; we demonstrate empirically that our approach to vision-only baselines is improving and provides a stronger foundation for future experiments.The task of \"Image Riddles\" is consistent with traditional IQ test questions such as solving analogy and sequence questions, which are commonly used to test human intelligence; this task of \"Image Riddles\" is also consistent with the current trend of VQA datasets that require visual recognition and thinking skills; however, it focuses more on the combination of visual and thinking skills; in addition to the task, the proposed approach introduces a novel sequencing model to improve related words (from a general vocabulary) to a higher level that can be used."}, {"heading": "A. BiasedUnRiddler (BUR): A Variation of the", "text": "\"We still have a problem.\" (\"We\"), \"we.\" (\"We\"), \"we.\" (\"We\"), \"we.\" (\"We\"), \"we.\" (\"We\"), \"we.\" (\"We\"), \"we.\" (\"We\"), \"we.\" (\"We\"), \"we.\" (\"We\"), \"we.\" (\"We\"), \"we.\" (\"We\"), \"we.\" (\"We\"), \"we.\" (\"We.\" (\"We.\" (\"We.\" (\"We.\" (\"),\" We. \"(\" We. \"(\" We. \"(\"), \"We.\" (\"We.\"), \"We.\" (\"We.\"), \"We.\" (\"We.\"), \"We.\" (\"We.\"), \"We.\" (\"We.\"), \"We. (\" We. \"(\" We. \"),\" We. (\"We.\"), \"We. (\" We. \"(\" We. \"),\" We. (\"We. (\"), \"We. (\" We. \"),\" We. (\"),\" We. (\"We. (\"), \"We. (. (\" We. (\"),\" We. (\"),\" We. (\"We. (\"), \"(\"), \"We. (\" We. (\"),\" We. (\"),\" (. (\"We. (\"), \"We. (\"), \"We. (. (\" We. (\"),\" We. (\"),\" We. (. (\"),\"), \"We. (. (\"), \"),\" We. (. (\"We. (. (\" We. \"),\" We. (. (\"),\"), \"),\" (. (\"We. (\"), \"We. (\" We. (\"We. (\"), \"),\"), \"),\" (\"We. (\" We. (. (We. (\"),\" We. ("}, {"heading": "C. Detailed Accuracy Histograms For Different Variants", "text": "In this section, we present the accuracy histograms for the entire dataset for all variants (using the Clarifai API) of our approach (listed in Table 2 of the essay) and add the accuracy histograms for variants using the BUR approach. The diagrams are shown in Figure 8. The diagrams show the shift toward greater accuracy as we move along the stages of our pipeline.D. Visual similarity: Additional results Additional results for visual similarity are shown in Tables 7 and 8.10The output of PSL Level I for BUR is completely independent of the other images. Essentially, we predict for each image all the relevant concepts from a large vocabulary, as there is some limited evidence from a small set of class labels."}, {"heading": "E. More Positive and Negative Results", "text": "Variant 3 of the pipeline. We get better results with Clarifai detections rather than Residual Network detections. Based on our observations, one of the key characteristics of the ResidualNetwork confidence distribution is that there are only a few detectors (1-3) that have the strongest confidence results and the other detectors have very negligible confidence results. These top detectors are often quite noisome. For example, for the ResidualNetwork detectors there are: Triceratops, Wallaby, Armadillo, Pig, Fox Pig, Wild Boar, Grey Fox, Indian Elephant, Red Fox, Egyptian Cat, Wombat, Tusker, Arctic Fox, Electric Fox, Toy Fox, Toy Fox Fox-Fox-Fox-Fox-Fox."}], "references": [], "referenceMentions": [], "year": 2016, "abstractText": "In this work, we explore a genre of puzzles (\u201cimage riddles\u201d) which involves a set of images and a question. An-<lb>swering these puzzles require both capabilities involving vi-<lb>sual detection (including object, activity recognition) and,<lb>knowledge-based or commonsense reasoning. We compile<lb>a dataset of over 3k riddles where each riddle consists of 4<lb>images and a groundtruth answer. The annotations are validated using crowd-sourced evaluation. We also define an<lb>automatic evaluation metric to track future progress. Our<lb>task bears similarity with the commonly known IQ tasks<lb>such as analogy solving, sequence filling that are often used<lb>to test intelligence. We develop a Probabilistic Reasoning-based approach<lb>that utilizes probabilistic commonsense knowledge to an-<lb>swer these riddles with a reasonable accuracy. We demon-<lb>strate the results of our approach using both automatic and<lb>human evaluations. Our approach achieves some promising<lb>results for these riddles and provides a strong baseline for future attempts. We make the entire dataset and related ma-<lb>terials publicly available to the community in ImageRiddle<lb>Website (http://bit.ly/22f9Ala).", "creator": "LaTeX with hyperref package"}}}