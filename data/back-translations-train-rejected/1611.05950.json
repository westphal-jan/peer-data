{"id": "1611.05950", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Nov-2016", "title": "Analysis of a Design Pattern for Teaching with Features and Labels", "abstract": "We study the task of teaching a machine to classify objects using features and labels. We introduce the Error-Driven-Featuring design pattern for teaching using features and labels in which a teacher prefers to introduce features only if they are needed. We analyze the potential risks and benefits of this teaching pattern through the use of teaching protocols, illustrative examples, and by providing bounds on the effort required for an optimal machine teacher using a linear learning algorithm, the most commonly used type of learners in interactive machine learning systems. Our analysis provides a deeper understanding of potential trade-offs of using different learning algorithms and between the effort required for featuring (creating new features) and labeling (providing labels for objects).", "histories": [["v1", "Fri, 18 Nov 2016 02:04:57 GMT  (264kb,D)", "http://arxiv.org/abs/1611.05950v1", "Also available atthis https URL"]], "COMMENTS": "Also available atthis https URL", "reviews": [], "SUBJECTS": "cs.AI cs.LG", "authors": ["christopher meek", "patrice simard", "xiaojin zhu"], "accepted": false, "id": "1611.05950"}, "pdf": {"name": "1611.05950.pdf", "metadata": {"source": "CRF", "title": "Analysis of a Design Pattern for Teaching with Features and Labels", "authors": ["Christopher Meek", "Patrice Simard", "Xiaojin Zhu"], "emails": [], "sections": [{"heading": "Introduction", "text": "This year it has come to the point that it will only be a matter of time before it will happen, until it does."}, {"heading": "Related Work", "text": "In this area, we agree that we are at a stage where we will be able to change the world."}, {"heading": "Features, Labels and Learning Algorithms", "text": "These three concepts are the core terms we need to discuss the cost of training a machine to classify objects. Thus, these definitions can form the basis for the rest of the work. In addition to providing these definitions, we also describe two properties of learning algorithms that relate to machine learning, and we describe two specific learning algorithms that are used in the rest of the work. We are interested in building a classifier of objects. We use x and xi to name certain objects of interest. We use y and yi for certain labels and Y to name the space of possible labels. For the binary classification Y = {1}. The classification function is a function from X to Y.2 The set of classification functions is referred to by C = X \u2192 Y."}, {"heading": "Teaching Patterns, Protocols and Costs", "text": "In this section, we present our Error Drive Featuring (EDF) design patterns for teaching and two teaching protocols. While programming design patterns are formalized best practices that a programmer can use to design software solutions to common problems, a teaching pattern (or teaching pattern) is a formalized best practice that a teacher can use to teach a computer. We use a pair of teaching protocols to examine the risks and benefits of our EDF teaching pattern. A teaching protocol is an algorithmic description of a method by which a teacher teaches a student. To study a teaching pattern, we force the teacher to follow the teaching pattern in one protocol, and in the other to give the teacher full control over his or her actions. We compare our teaching protocols by comparing the optimal teaching costs and defining the associated costs with the optimal cost of teaching in a subsequent section."}, {"heading": "Optimal Feature Set Teaching Costs", "text": "Next, we define a set of costs for a feature set. The first measure is a measure of the cost of specifying the feature set. In practice, different features may require a different effort to specify, and the cost of specifying different features depends on the interface through which features are communicated to the learner. The second measure of a feature set is a measure of the cost of specifying a target classification function using the feature set and a given learning algorithm. We measure the optimal concept specification cost based on the size of the minimum concept set for F using the learner L when F is sufficient and infinitely different. The third measure of a feature set is a measure of the cost of proving that the feature set is insufficient for a given learning algorithm."}, {"heading": "Analysis of Teaching Protocols", "text": "This year it is more than ever before."}, {"heading": "Bounding Optimal Teaching Cost and Feature Set Costs", "text": "This section determines the optimal teaching costs for Llin and L1NN based on the teaching protocols defined in Section. In this section we assume that there are a limited number of realizable objects (i.e., | X | < \u221e)."}, {"heading": "Bounding Optimal Feature Set Costs", "text": "We provide a number of proposals, each of which has narrow limits on optimal concept specification costs and optimal invalidation costs for Llin and L1NN. These proposals are presented in Table 2 with their complete statements with evidence set out in the full paper.The fact that the optimal concept specification costs are unlimited as a function of the size of the feature set for L1NN is due to the fact that the 1NN classifier is of high capacity.However, Section 7 limits the potential additional effort required to define the concept when adding a feature for Llin. It is important to note that the optimal concept specification costs for Llin can only be two labeled objects, but not generally. In fact, d > 1 can be constructed with a set of objects and a feature set of size d requiring d + 1 objects to specify a linear hyperplane that generalizes all objects."}, {"heading": "Bounding Teaching Costs", "text": "In this section, we will consider the limitation of the cost of teaching a target classification function c \u043c with the L1NN and Llin learning algorithms. First, we will consider L1NN. Under Proposal 6, we cannot limit the risk of adding a bad feature and therefore cannot limit the cost of teaching our teaching protocols. However, we can set limits on our teaching protocols with Llin. The following proposal provides for a cap on the cost of teaching a feature set. Proposal 3 The labelling costs for an adequate feature set F using an optimal teacher and the open feature protocol with Llin learning algorithm are \u2264 | F | + 1. For the error-driven feature protocol, calculating the cost is more difficult as we need to take into account the cost of invalid feature sets. Proposal 4 shows a useful link between the invalidation sets for nested feature sets when using a linear classification. Proposal 4 If a T is an invalid feature set, is the invalidity classification F."}, {"heading": "Appendix", "text": "This implies that there is a grading class of learning algorithms that is consistent with any (honest) training, including the fact that the L and L learning algorithms are then insufficient to define a set of points for Fi, Target Concept and Learning Algorithms. (This implies that there is a grading class of learning algorithms in the hypothesis that is consistent with any (honest) learning algorithm. (This implies that there is a grading class of learning algorithms that is consistent with any (honest) learning algorithm. (This implies that there is a grading class of learning algorithms that is consistent with any (honest) learning algorithm.) This implies that there is a grading class of learning algorithms that is consistent with any (honest) learning algorithm. (This implies that there is a grading class of learning algorithms that is consistent with the learning algorithms)."}], "references": [{"title": "Queries revisited", "author": ["D. Angluin"], "venue": "Theor. Comput. Sci. 313(2):175\u2013194.", "citeRegEx": "Angluin,? 2004", "shortCiteRegEx": "Angluin", "year": 2004}, {"title": "On exact specification by examples", "author": ["M. Anthony", "G. Brightwell", "D. Cohen", "J. Shawe-Taylor"], "venue": "Proceedings of the Fifth Annual Workshop on Computational Learning Theory, COLT \u201992, 311\u2013318. New York, NY, USA: ACM.", "citeRegEx": "Anthony et al\\.,? 1992", "shortCiteRegEx": "Anthony et al\\.", "year": 1992}, {"title": "Measuring teachability using variants of the teaching dimension", "author": ["F.J. Balbach"], "venue": "Theor. Comput. Sci. 397(1-3):94\u2013113.", "citeRegEx": "Balbach,? 2008", "shortCiteRegEx": "Balbach", "year": 2008}, {"title": "Recursive teaching dimension, VC-dimension and sample compression", "author": ["T. Doliwa", "G. Fan", "H.U. Simon", "S. Zilles"], "venue": "Journal of Machine Learning Research 15:3107\u20133131.", "citeRegEx": "Doliwa et al\\.,? 2014", "shortCiteRegEx": "Doliwa et al\\.", "year": 2014}, {"title": "Design Patterns: Elements of Reusable Object-Oriented Software", "author": ["E. Gamma", "R. Helm", "R. Johnson", "J.M. Vlissides"], "venue": "Addison-Wesley.", "citeRegEx": "Gamma et al\\.,? 1995", "shortCiteRegEx": "Gamma et al\\.", "year": 1995}, {"title": "On the complexity of teaching", "author": ["S. Goldman", "M. Kearns"], "venue": "Journal of Computer and Systems Sciences 50(1):20\u201331.", "citeRegEx": "Goldman and Kearns,? 1995", "shortCiteRegEx": "Goldman and Kearns", "year": 1995}, {"title": "Teaching dimension and the complexity of active learning", "author": ["S. Hanneke"], "venue": "Proceedings of the 20th Annual Conference on Computational Learning Theory (COLT), 6681.", "citeRegEx": "Hanneke,? 2007", "shortCiteRegEx": "Hanneke", "year": 2007}, {"title": "Generalized teaching dimensions and the query complexity of learning", "author": ["T. Heged\u0171s"], "venue": "Proceedings of the Eighth Annual Conference on Computational Learning Theory, COLT \u201995, 108\u2013117. New York, NY, USA: ACM.", "citeRegEx": "Heged\u0171s,? 1995", "shortCiteRegEx": "Heged\u0171s", "year": 1995}, {"title": "How many queries are needed to learn? J", "author": ["L. Hellerstein", "K. Pillaipakkamnatt", "V. Raghavan", "D. Wilkins"], "venue": "ACM 43(5):840\u2013862.", "citeRegEx": "Hellerstein et al\\.,? 1996", "shortCiteRegEx": "Hellerstein et al\\.", "year": 1996}, {"title": "The teaching dimension of linear learners", "author": ["J. Liu", "X. Zhu", "H. Ohannessian"], "venue": "Proceedings of The 33rd International Conference on Machine Learning, ICML \u201916, 117\u2013126.", "citeRegEx": "Liu et al\\.,? 2016", "shortCiteRegEx": "Liu et al\\.", "year": 2016}, {"title": "Analysis of a design pattern for teaching with features and labels", "author": ["C. Meek", "P. Simard", "X. Zhu"], "venue": "NIPS 2016 Future of Interactive Machine Learning Workshop.", "citeRegEx": "Meek et al\\.,? 2016", "shortCiteRegEx": "Meek et al\\.", "year": 2016}, {"title": "A characterization of prediction errors", "author": ["C. Meek"], "venue": "ArXiv.", "citeRegEx": "Meek,? 2016", "shortCiteRegEx": "Meek", "year": 2016}, {"title": "Active Learning", "author": ["B. Settles"], "venue": "Synthesis Lectures on Artificial Intelligence and Machine Learning. Morgan & Claypool.", "citeRegEx": "Settles,? 2012", "shortCiteRegEx": "Settles", "year": 2012}, {"title": "Machine teaching: an inverse problem to machine learning and an approach toward optimal education", "author": ["X. Zhu"], "venue": "AAAI.", "citeRegEx": "Zhu,? 2015", "shortCiteRegEx": "Zhu", "year": 2015}, {"title": "Models of cooperative teaching and learning", "author": ["S. Zilles", "S. Lange", "R. Holte", "M. Zinkevich"], "venue": "Journal of Machine Learning Research 12:349\u2013384.", "citeRegEx": "Zilles et al\\.,? 2011", "shortCiteRegEx": "Zilles et al\\.", "year": 2011}], "referenceMentions": [{"referenceID": 11, "context": "First we note that this work shares a common roots with the work of Meek (2016) but there the focus is on prediction errors rather than teaching effort.", "startOffset": 68, "endOffset": 80}, {"referenceID": 5, "context": "For instance, the idea of a concept teaching set is closely related to that of a teaching sequence (Goldman and Kearns 1995) and our optimal concept specification cost is essentially the specification number of a hypothesis (Anthony et al 1992); we add concept to distinguish it from representation specification cost.", "startOffset": 99, "endOffset": 124}, {"referenceID": 0, "context": "Other existing concepts include the exclusion dimension (Angluin 1994) and the unique specification dimension (Hedigus 1995) and the certificate size (Hellerstein et al 1996) which are similar to our invalidation cost. In addition, Liu et al (2016) define the teaching dimension of a hypothesis which is equivalent to the specification number and our concept specification cost.", "startOffset": 57, "endOffset": 249}, {"referenceID": 0, "context": "Other existing concepts include the exclusion dimension (Angluin 1994) and the unique specification dimension (Hedigus 1995) and the certificate size (Hellerstein et al 1996) which are similar to our invalidation cost. In addition, Liu et al (2016) define the teaching dimension of a hypothesis which is equivalent to the specification number and our concept specification cost. They also provide bounds on the concept specification cost for linear classifiers. Their results are related to our Proposition 7 but, unlike our result, assume that the space of objects is dense. In the terms of Zhu (2015), we provide the hypothesis specific teaching dimension for pool-based teaching.", "startOffset": 57, "endOffset": 603}, {"referenceID": 11, "context": "This paper is an extended version of the paper by Meek et al (2016).", "startOffset": 50, "endOffset": 68}, {"referenceID": 6, "context": "Not surprisingly, the work on active learning is related to work on teaching dimension (Hanneke 2007).", "startOffset": 87, "endOffset": 101}, {"referenceID": 11, "context": "An excellent survey of research in this area is given by Settles (2012). Not surprisingly, the work on active learning is related to work on teaching dimension (Hanneke 2007).", "startOffset": 57, "endOffset": 72}, {"referenceID": 11, "context": "For a related but alternative teaching protocol that allows for mislabeling errors see Meek (2016). In this protocol, if the current feature set is not sufficient, a teacher adds labeled examples to find an invalidation set which then enables them to add a feature to improve the feature representation.", "startOffset": 87, "endOffset": 99}], "year": 2016, "abstractText": "We study the task of teaching a machine to classify objects using features and labels. We introduce the Error-Driven-Featuring design pattern for teaching using features and labels in which a teacher prefers to introduce features only if they are needed. We analyze the potential risks and benefits of this teaching pattern through the use of teaching protocols, illustrative examples, and by providing bounds on the effort required for an optimal machine teacher using a linear learning algorithm, the most commonly used type of learners in interactive machine learning systems. Our analysis provides a deeper understanding of potential trade-offs of using different learning algorithms and between the effort required for featuring and labeling.", "creator": "LaTeX with hyperref package"}}}