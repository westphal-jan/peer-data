{"id": "1611.05990", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Nov-2016", "title": "Monte Carlo Connection Prover", "abstract": "Monte Carlo Tree Search (MCTS) is a technique to guide search in a large decision space by taking random samples and evaluating their outcome. In this work, we study MCTS methods in the context of the connection calculus and implement them on top of the leanCoP prover. This includes proposing useful proof-state evaluation heuristics that are learned from previous proofs, and proposing and automatically improving suitable MCTS strategies in this context. The system is trained and evaluated on a large suite of related problems coming from the Mizar proof assistant, showing that it is capable to find new and different proofs. To our knowledge, this is the first time MCTS has been applied to theorem proving.", "histories": [["v1", "Fri, 18 Nov 2016 06:30:09 GMT  (29kb)", "http://arxiv.org/abs/1611.05990v1", null]], "reviews": [], "SUBJECTS": "cs.LO cs.AI cs.LG", "authors": ["michael f\\\"arber", "cezary kaliszyk", "josef urban"], "accepted": false, "id": "1611.05990"}, "pdf": {"name": "1611.05990.pdf", "metadata": {"source": "META", "title": "Monte Carlo Connection Prover", "authors": ["Michael F\u00e4rber", "Cezary Kaliszyk"], "emails": ["michael.faerber@uibk.ac.at", "cezary.kaliszyk@uibk.ac.at", "josef.urban@gmail.com"], "sections": [{"heading": null, "text": "ar Xiv: 161 1.05 990v 1 [cs.L O] 18 Nov 201 6Monte Carlo Tree Search (MCTS) is a technique to guide the search in a large decision room by taking random samples and evaluating their results. In this thesis, we examine MCTS methods in the context of compound calculation and implement them on the basis of the leanCoP auditor. This includes suggesting useful heuristics to evaluate evidence learned from previous evidence, and suggesting and automatically improving suitable MCTS strategies in this context. The system is trained and evaluated on a large number of related problems originating from the Mizar proof assistant and demonstrates that it is capable of finding new and other evidence. Ours is the first time that MCTS is applied to theory testing."}, {"heading": "1. Introduction", "text": "This year it has come to the point that it has never come as far as this year."}, {"heading": "2. Monte Carlo Tree Search", "text": "Monte Carlo Tree Search (MCTS) is a method for searching large areas by random sampling, targeting promising parts of the search area. It was applied to a wide range of games and domains on 1 November 2016 and often achieves state-of-the-art results (Browne et al. 2012). One of the most famous MCTS applications is AlphaGo, the first program to beat a professional Go player in a non-handicap tournament (Silver et al. 2016). The idea of MCTS is to perform random play-outs of a game, called simulations, and select the move where the play-outs were most successful. To achieve good results, it is crucial to influence the simulations in such a way that they cover a diverse range of government space (exploration), but also to focus on the parts where the most successful play-outs took place (exploitation). One of the most influential methods for striking the balance between exploration and exclusion was MCTS and coception problems (SUCTS)."}, {"heading": "2.1 Characterisation of MCTS Problems", "text": "To characterize a problem such as the theorem, which is traceable by MCTS, with rules and heuristics, it is convenient to describe the problem. Rules describe the initial state and the legal steps that are possible in a given state, while heuristics estimate the quality of a given move and a (final) state. To describe rules, we use: \u2022 A series of states S \u2022 An initial state S \u2022 An initial state S \u2022 S \u2022 2S, which maps a state to its permitted successor state. To describe the heuristics: \u2022 A transition weight function \u03b4w: S \u2192 S \u2192 R \u2265 0, where \u03b4w (s, t) in a random simulation indicates the weight of the transition from state s to t. \u2022 A reward function r: S \u2192 [0, 1]."}, {"heading": "2.1.1 Example: Travelling Salesman Problem", "text": "Consider as an example of an MCTS application the problem of the driving salesman (TSP): In the face of a list of cities {c1,.., cn} with a distance function d, we find a permutation X of [1,.., n] such that r (X) = \u2211 i d (cXi, cXi + 1) is minimal. We can formulate the rules of this problem as follows: \u2022 The state arrangement S returns all possible (potentially unfinished) paths of the driving salesman, i.e. permutations of subsets of {1,.., n}. \u2022 The initial state s0 is the empty sequence []. \u2022 The state adjustment function \u03b4 returns all possible paths on which the driving salesman has visited another city, i.e. vice versa ([X1,.,.,., Xi, j]. \u2212 u. The hayristics for the reward (TSP) is represented as sufficient (TSP) for the function (1)."}, {"heading": "2.2 Monte Carlo Tree Search Algorithm", "text": "MCTS maintains a decision tree in which the nodes correspond to the states S of the underlying problem (say, the current positions of the stones on a go board), and the children of a node correspond to succession states \u03b4 (s) (e.g., the configurations of the board after the other player has placed a stone).In addition to the state, a node contains statistics on how often the node has been passed and on the rewards of simulations started from and under the node. When starting MCTS, the decision tree is initialized with a root node corresponding to the initial state s0 \u0441S. An iteration of the MCTS algorithm includes the following steps: 1. Selection: The MCTS tree is passed from the root to a leaf (with at least one unexplored successor state).2. Simulation: A (random) simulation is performed from the leaf. 3. Expansion: New nodes corresponding to the initial simulation state are followed by actual succession state."}, {"heading": "2.3 UCT", "text": "UCT (Upper Confidence Bounds applied to Trees) is a version of MCTS with a selection strategy that attempts to balance exploitation and exploration: it selects the node j that maximizes Xj + Cp \u221a 2 ln nj, where: \u2022 Xj is the average reward for the node j, \u2022 nj is the number of times the node j was selected, \u2022 n is the total number of times the parent node of j washes. UCT assumes that rewards are always in the range [0, 1], which is important to maintain the balance between exploration and exploitation."}, {"heading": "3. Connection Proving and MCTS", "text": "In this section we present the clause connection calculation, discuss possible ways to model the connection calculation as a game and describe how it can be integrated into MCTS."}, {"heading": "3.1 Clausal Connection Calculus", "text": "The Connection Calculation is a targeted search method (Bible 1993). It works on a matrix M, which represents the set of sentences derived from the Skolem normal form of a series of first-order formulas. A version used in the automated theory tester leanCoP is shown in Figure 1. Evidence searching is initialized with the \"Start\" rule and succeeds when all branches of the evidence tree are closed by applying the other rules. The \"state\" of a branch is characterized by a triple (C, M, path), where C is the current sentence, M is the matrix containing all input clauses, and path is the active path."}, {"heading": "3.2 Connection Proving as a Game", "text": "MCTS has been applied to two-player games such as Go (Silver et al. 2016), as well as to single-player games such as SameGame (Schadd et al. 2012). It is possible to model the evidence search as a one-player game. We compare the two approaches. In the case of a single player, the player selects an action that is applied to an open branch, and possibly adds new open branches. The second player wins when branches have been closed, and loses when there is a branch for which no rule of proof applies. In the case of two players, the first player selects an open branch and, if the branch clause is not empty, a literal branch. The second player then applies an action to the selected branch, which must either result in the branch being closed or at least one new open branch being produced, with the literal branch being removed from the clause. If the second player is unable to do one of the two branches, the first player then has quickly gained the first branch, and the second branch can be terminated for the first branch, and there is literally no evidence for the second branch."}, {"heading": "3.3 Single-Player Connection Proving Game", "text": "In order to model individual connections that turn out to be MCTS problems, we introduce a non-branching version of the connection calculation (see Figure 2), in which substitutions are explicit and each rule has only one premise. Since closed evidence trees in the branching and non-branching connection calculations are isomorphic, the non-branching calculation preserves solidity and completeness. With this calculation, we can specify the rules of the connection that prove to be as in Section 2.1: \u2022 The set of states S is the set of tuples (targets, \u03c3) in which targets are a set of sentence-path pairs and \u03c3 is a replacement. \u2022 The initial state s0 is ((((((), {}), {})). It must be added to all positive clauses in M in order to correctly imitate the starting rule of the branching connection calculation. \u2022 The state transfer function (s) delivers all possible premises of the non-branching connection rules."}, {"heading": "3.4 Main Loop", "text": "In order to find good moves for two-player games, MCTS is usually performed with a fixed time limit or a number of iterations to select and use the best first move from the1. In practice, MCTS is limited to a limited subset of possible calculation prerequisites, just as leanCoP does. 3 2016 / 11 / 21root. It is possible to use MCTS as an oracle in a conventional theory tester: In the face of a evidence state with high uncertainty about which approach is most appropriate (for example, if one has to choose between several renewal clauses), one can execute MCTS and obtain the most promising next evidence knot by continuing the conventional evidence search.This approach has two problems. First, it is unclear in which situations it is advantageous to execute MCTS rather than exhaustively search all the evidence options. Second, extracting only the first successor of the Monte Carlo search tree is the best reason for using the MCTS search tree as a follow tree."}, {"heading": "3.5 Simulation Strategies", "text": "Once the selection strategy has selected a child node with at least one unexplored successor state, the simulation strategy selects an unexplored successor state and launches a random simulation. In each iteration of the simulation, we calculate the number of successor states \u03b4 (s), i.e. all possible evidence steps that can be applied in the current evidence state. Next, one of the successor states is randomly selected, with the simulation prejudiced against the transition weight \u03b4w, and the simulation continues with the selected successor state. No traceability is performed during the simulation because information on the search history would need to be stored in the evidence state. The simulation returns the sequence of states traversed to the current state when either \u03b4 (s) is the empty amount, i.e. no rule of proof is applicable, or a maximum depth of simulation is reached. In the event that s has no other open objectives, i.e. a proof has been found, the evidence is returned."}, {"heading": "3.6 Expansion Strategies", "text": "Once the simulation strategy is completed with a resulting sequence of states, the expansion strategy is responsible for adding a new child node to the node from which the simulation started. As the chance of revisiting a node with its depth greatly decreases, most deeper nodes are not revisited and their storage consumes a significant amount of memory. Therefore, we will add only a single leaf node to the source node. Taking into account deeper sub-trees depending on the expected number of future visits to the node, in order to minimize the costly recalculation of the succession states, future work could consider. Our leaf expansion strategies are: \u2022 Expansion of the first node: This strategy adds the first node of the simulation, i.e. the previously unvisited action of the selected node to its parent node. \u2022 Expansion of the best node: This strategy takes the state of the simulation with the lowest number of sub-targets on top of it."}, {"heading": "4. Proof State Evaluation Heuristics", "text": "One of the most important advantages of MCTS for games is that it does not require an evaluation function to estimate the likelihood of winning for a non-final state. In a chess game, for example, it is not necessary to find a function to estimate how likely a player is to win the game, but it is only necessary to determine at the end of a game which player has won, and perhaps by how much. However, in the theoretical test we are very rarely able to finish a game with a clear result, which means that we still have to estimate the success of branches. Even if we come across an industry in which we are unable to prove any further and are thus in a final state, this cannot be considered a \"loss\" in the sense of the game, because we may have only one fault that has not made us able to complete the proof. It is clear that we need a finer idea of \"success\" to assess the potential of such situations, to prove it by making some other decisions."}, {"heading": "4.1 Provability Estimates", "text": "The linear combination of their estimates forms the reward function r (s) of our Monte Carlo Connection Prover. The first method is based on the fact that, at the end of a proof, the number of open partial goals is zero and the number of open partial goals is always smaller or equal to the number of previously opened partial goals. Thus, the first reward function for a state is simply the ratio between open and previously opened partial goals. The second method is based on data collected from previous evidence attempts. We assume that certain goals are statistically easier to conclude than others, and certain target letters are also easier than others. This is similar to the approach used, for example, in FEMaLeCoP (Kaliszyk and Urban 2015b)."}, {"heading": "4.2 Machine-learnt Provability Estimates", "text": "We say that a literal L1 has been successfully proven when either the reduction rule has been applied to the C clause or the extension rule has been applied and its left branch has been completely closed (see Figure 1). A literal l proved unsuccessful 5 2016 / 11 / 21, when the reduction rule was not applicable, and for all the extension rule that was applicable, its left branches have not been closed. We now define our literal collection of evidence: if we have not seen the literal l in previous evidence, then the success rate is pp + n. To take into account letters that we have rarely seen, we introduce the rule of certainty (x) = 1 \u2212 C 1xD + 1 (where C and D are constants) and calculate the entire rule of proof literally as proof. \u00b7 (pp + n) If the rule of proof is proven that the rule of proof is clear, the rule of proof is necessary."}, {"heading": "5. Implementation", "text": "The implementation is based on the source code of the FEMaLeCoP tester (Kaliszyk and Urban 2015b), an OCaml version of leanCoP (Otten 2008), enhanced with machine learning techniques. For all the testers listed below, we have implemented the usual leanCoP optimizations that are not part of the calculation, such as lemmings and regularity checking. In addition, all testers below use the same sample format as the leanCoP tactic described in Kaliszyk et al. 2015 to automatically find evidence for HOL Light, so adapting HOL Light Proof tactics to our Monte Carlo system should be straightforward."}, {"heading": "5.1 Lazy List Connection Prover", "text": "The FEMaLeCoP implementation uses continuation passing style (CPS) to implement traceability that supports the \"cut\" prologue. Although this is very efficient, it does not offer a simple representation of different ways to close a sub-target, as this information is \"hidden\" in the sequel. However, MCTS requires an explicit set of successor states for each audit state. To achieve this, we first created a version of FEMaLeCoP that uses lazy lists instead of continuations to represent different evidence options: to close a sub-target, there may be different reduction and extension steps. So, for a given literal l, we return a lazy list of evidence trees that prove l. We can then only filter out and trace closed evidence trees by going through the list of evidence trees."}, {"heading": "5.2 Non-branching Calculus", "text": "lazyCoP served as the basis for an implementation of the non-branching calculation: In the non-branching calculation, we only return the next nodes of the Proof sub-trees as a list of actions to close a sub-target and not the entire Proof sub-tree to close the sub-target. This distinction is important because it enables us to perform quick random outputs of the Proof search without branching. The resulting implementation of the non-branching Proof search also works independently of MCTS, for example by iterative deepening together with a depth search in the Proof Search decision tree."}, {"heading": "5.3 Monte Carlo Connection Prover", "text": "Since the non-branching evidence search already closely matches the MCTS problem description in Section 2.1, it is now relatively easy to replace the depth search tactic with MCTS. The biggest adjustment needed was to replace the global array-based substitution with a local substitution for each branch of evidence examined, choosing a list-based substitution. We refer to the resulting system as monteCoP."}, {"heading": "5.4 Monte Carlo Tree Search", "text": "We implemented the UCT variant of MCTS and validated its function with the traveling vendor problem as illustrated in subsection 2.1.1. We added an optimization in MCTS that is specific to automated theorem evidence: If the list of unvisited and visited child nodes of a node is empty, the node itself will be deleted. Without this deletion, nodes that have no hope of ever contributing to the search for evidence would continue to be visited from time to time, as the UCT exploration term of the nodes would grow. In addition, we use the transition weight function not only to distort the selection of successor states during the simulation, but also when we select an unexplored action of a Monte Carlo tree node.6 2016 / 11 / 21"}, {"heading": "5.5 Training Data", "text": "In order to collect literal refutation statistics, as described in Section 4, we have created a lazyCoP version, which contains statistics for the last iteration of the iterative indentation when evidence is found. To account for the same literal occurrence between different clauses, we also register for each literal clause from which it comes. An important aspect is the consistent scolemisation, which ensures that constants introduced during the scolemisation bear the same name for different problems and trial runs. Initially, we saved the symbolic representations of clauses and words in the literal statistics. However, the names of the skolem constants can become so large that loading the training data during the correction search took a considerable amount of time. We solved this problem by storing only hashes of the symbolic representations of words and their clauses, which reduced the size of the training data by several orders of magnitude. In order to combine the preliminaries of each of positive training data, we combine the different examples with each negative word."}, {"heading": "5.6 Related Work", "text": "randoCoP (Raths and Otten 2008) is another random liaison checker. It runs leanCoP several times and mixes the order of clauses and literals at the beginning of each sample search. The difference with our approach is that randoCoP is random only at the beginning of each sample search, according to which the behavior is deterministic. In monteCoP, however, the restart behavior is inherently given by UCT, and randoCoP affects the sample search at any point where multiple options are available to close a sub-target."}, {"heading": "6. Evaluation", "text": "We used the problems of the MZR @ Turing area of CASC-J6 (Sutcliffe 2013), consisting of 1000 training problems known before the competition, and 400 test problems evaluating the quality of the submitted competitors. All 1400 problems originate from the MPTP2078 problem set (Alama et al. 2011), but are sometimes supplemented by facts that are not necessary for proof in order to make evidence attempts more difficult. We mapped the problems of the MZR @ Turing area to its bushy counterparts (with a minimized number of dependencies) of the MPTP2078 dataset and used them for evaluation."}, {"heading": "6.1 Parameter Optimisation", "text": "First, we performed lazyCoP with a timeout of 300s on the 1000 training problems and found 314 solved problems together with literal statistics as described in Section 5.5. Subsequently, we divided the solved problems into a parameter training set of 264 and a parameter test set of 50 problems. On the parameter training set, we performed ParamILS (Hutter et al. 2009) with monteCoP, using a timeout of 2s. We performed 32 parallel ParamILS instances with different start configurations, with each instance going through a total time of 80000s. Next, we obtained the best resulting ParamILS configuration in terms of the number of conclusions per problem solved on the parameter test set."}, {"heading": "6.2 Results", "text": "As a baseline comparison, we chose a monteCoP configuration that resembles the width-first search by having a constant reward function and a simulation depth of 1. This results in a consistent growth of the search tree and only promotes exploration, since the reward of all nodes is the same. This version of monteCoP occupies 41 problems of the benchmark. Next, we chose a monteCoP configuration with a higher depth of simulation, maintaining the constant reward function and selecting potential next reward steps with the same probability. This corresponds to a randomized beam search with increasing beam depth. In this configuration, monteCoP 49 solves problems. Finally, we guide the evidence search by distorting the selection clauses in favor of smaller clauses, applying the simulation strategy in subsection 3.5, and with the reward function introduced in section 4, using the literal evidence data from turret cycles in favor of smaller clauses, using the only CoP 84% of the training problem is the unique one."}, {"heading": "6.3 Inferences", "text": "In monteCoP, we also count the number of MCTS iterations, which is roughly the number of nodes in the Monte Carlo search tree, i.e. the number of random proof searches, except that our version of MCTS can also delete nodes during an iteration if they have no succession orders.The average number of monteCoP-MCTS iterations is 5532. The average number of conclusions is 320295 for lazyCoP and 117163 for monteCoP. On average, monteCoP performed 9.6 times as many conclusions as lazyCoP on the problems that both testers solved. This can be explained by the fact that monteCoP always calculates all possible extension steps, even if the expansion step is not attempted afterwards."}, {"heading": "6.4 Experiments", "text": "Our hypothesis was that the optimal value of Cp is problem-dependent. Therefore, we experimented with an oscillating Cp (i) = Cp, 0 + a sin (2\u03c0 i T), where i is the number of current MCTS iterations, a is the oscillating amplitude and T is the oscillating duration. Unfortunately, this did not lead to a significant improvement. We were surprised to see that the best node expansion strategy (see Section 3.6), which corresponds to the LeanCoP shortening strategy, solved only 14 problems. We tried to avoid random rewards in the range [0, 1] which proved to be as many problems as constant rewards. This is due to the fact that the selection strategy calculates the average reward from all previous simulations that do not add up to convergence for these multiple rewards with CTS times for all MTS times."}, {"heading": "6.5 MPTP2078", "text": "In addition to the problems in the MZR @ Turing dataset, we performed an evaluation of all MPTP2078 problems. Here, the best lazyCoP strategy solves 468 problems, while monteCoP solves 296. Of these 296, 56 are unique, i.e. 12.0% of the problems solved by lazyCoP."}, {"heading": "7. Conclusion", "text": "We have described and implemented a Monte Carlo tree-search-based liaison auditor with machine-learning management euristics, which we evaluated on two sets of data, with our system able to detect 17.9% new problems compared to a conventional liaison auditor. You can use our system as a consultant in a theory auditor to choose between different possible sections of evidence. As a future work, new evidence euristics, such as using neural networks, could help to better estimate promising regions of evidence-gathering."}], "references": [{"title": "Premise selection for mathematics by corpus analysis and kernel methods", "author": ["J. Alama", "D. K\u00fchlwein", "E. Tsivtsivadze", "J. Urban", "T. Heskes"], "venue": "CoRR, abs/1108.3446,", "citeRegEx": "Alama et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Alama et al\\.", "year": 2011}, {"title": "Deduction - automated logic", "author": ["W. Bibel"], "venue": null, "citeRegEx": "Bibel.,? \\Q1993\\E", "shortCiteRegEx": "Bibel.", "year": 1993}, {"title": "Hammering towards QED", "author": ["J. Blanchette", "C. Kaliszyk", "L. Paulson", "J. Urban"], "venue": "Journal of Formalized Reasoning,", "citeRegEx": "Blanchette et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Blanchette et al\\.", "year": 2016}, {"title": "A learning-based fact selector for Isabelle/HOL", "author": ["J.C. Blanchette", "D. Greenaway", "C. Kaliszyk", "D. K\u00fchlwein", "J. Urban"], "venue": "J. Autom. Reasoning,", "citeRegEx": "Blanchette et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Blanchette et al\\.", "year": 2016}, {"title": "Premise selection and external provers for HOL4", "author": ["T. Gauthier", "C. Kaliszyk"], "venue": "In Certified Programs and Proofs (CPP\u201915), LNCS. Springer,", "citeRegEx": "Gauthier and Kaliszyk.,? \\Q2015\\E", "shortCiteRegEx": "Gauthier and Kaliszyk.", "year": 2015}, {"title": "Mizar in a nutshell", "author": ["A. Grabowski", "A. Korni\u0142owicz", "A. Naumowicz"], "venue": "J. Formalized Reasoning,", "citeRegEx": "Grabowski et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Grabowski et al\\.", "year": 2010}, {"title": "HOL Light: A tutorial introduction", "author": ["J. Harrison"], "venue": "FMCAD, volume 1166 of LNCS,", "citeRegEx": "Harrison.,? \\Q1996\\E", "shortCiteRegEx": "Harrison.", "year": 1996}, {"title": "ParamILS: an automatic algorithm configuration framework", "author": ["F. Hutter", "H.H. Hoos", "K. Leyton-Brown", "T. St\u00fctzle"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Hutter et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Hutter et al\\.", "year": 2009}, {"title": "Learning-assisted automated reasoning with Flyspeck", "author": ["C. Kaliszyk", "J. Urban"], "venue": "J. Autom. Reasoning,", "citeRegEx": "Kaliszyk and Urban.,? \\Q2014\\E", "shortCiteRegEx": "Kaliszyk and Urban.", "year": 2014}, {"title": "MizAR 40 for Mizar 40", "author": ["C. Kaliszyk", "J. Urban"], "venue": "J. Autom. Reasoning,", "citeRegEx": "Kaliszyk and Urban.,? \\Q2015\\E", "shortCiteRegEx": "Kaliszyk and Urban.", "year": 2015}, {"title": "Certified connection tableaux proofs for HOL Light and TPTP", "author": ["C. Kaliszyk", "J. Urban", "J. Vyskocil"], "venue": "Proceedings of the 2015 Conference on Certified Programs and Proofs,", "citeRegEx": "Kaliszyk et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kaliszyk et al\\.", "year": 2015}, {"title": "Bandit based MonteCarlo planning", "author": ["L. Kocsis", "C. Szepesv\u00e1ri"], "venue": "Machine Learning: ECML 2006, 17th European Conference on Machine Learning, Berlin,", "citeRegEx": "Kocsis and Szepesv\u00e1ri.,? \\Q2006\\E", "shortCiteRegEx": "Kocsis and Szepesv\u00e1ri.", "year": 2006}, {"title": "First-order theorem proving and Vampire", "author": ["L. Kov\u00e1cs", "A. Voronkov"], "venue": "CAV, volume 8044 of LNCS,", "citeRegEx": "Kov\u00e1cs and Voronkov.,? \\Q2013\\E", "shortCiteRegEx": "Kov\u00e1cs and Voronkov.", "year": 2013}, {"title": "Single-player Monte-Carlo tree search for SameGame", "author": ["M.P.D. Schadd", "M.H.M. Winands", "M.J.W. Tak", "J.W.H.M. Uiterwijk"], "venue": "Knowl.-Based Syst.,", "citeRegEx": "Schadd et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Schadd et al\\.", "year": 2012}, {"title": "A brief overview of HOL4", "author": ["K. Slind", "M. Norrish"], "venue": "TPHOLs 2008,", "citeRegEx": "Slind and Norrish.,? \\Q2008\\E", "shortCiteRegEx": "Slind and Norrish.", "year": 2008}, {"title": "The 6th IJCAR automated theorem proving system competition - CASC-J6", "author": ["G. Sutcliffe"], "venue": "AI Commun.,", "citeRegEx": "Sutcliffe.,? \\Q2013\\E", "shortCiteRegEx": "Sutcliffe.", "year": 2013}, {"title": "MaLeCoP machine learning connection prover", "author": ["J. Urban", "J. Vyskocil", "P. Step\u00e1nek"], "venue": "Automated Reasoning with Analytic Tableaux and Related Methods - 20th International Conference,", "citeRegEx": "Urban et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Urban et al\\.", "year": 2011}, {"title": "The Isabelle framework", "author": ["M. Wenzel", "L.C. Paulson", "T. Nipkow"], "venue": "TPHOLs, volume 5170 of LNCS,", "citeRegEx": "Wenzel et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Wenzel et al\\.", "year": 2008}], "referenceMentions": [{"referenceID": 17, "context": "2011), leanCoP (Otten 2008), and interactive theorem provers (ITPs) such as Isabelle (Wenzel et al. 2008), HOL4 (Slind and Norrish 2008), HOL Light (Harrison 1996), and Mizar (Grabowski et al.", "startOffset": 85, "endOffset": 105}, {"referenceID": 5, "context": "2008), HOL4 (Slind and Norrish 2008), HOL Light (Harrison 1996), and Mizar (Grabowski et al. 2010) have led to a significant improvement of the usability of ITPs (Blanchette et al.", "startOffset": 75, "endOffset": 98}, {"referenceID": 2, "context": "2010) have led to a significant improvement of the usability of ITPs (Blanchette et al. 2016a, Blanchette et al. (2016b), Gauthier and Kaliszyk (2015), Kaliszyk and Urban (2014), Kaliszyk and Urban (2015a)).", "startOffset": 70, "endOffset": 121}, {"referenceID": 2, "context": "2010) have led to a significant improvement of the usability of ITPs (Blanchette et al. 2016a, Blanchette et al. (2016b), Gauthier and Kaliszyk (2015), Kaliszyk and Urban (2014), Kaliszyk and Urban (2015a)).", "startOffset": 70, "endOffset": 151}, {"referenceID": 2, "context": "2010) have led to a significant improvement of the usability of ITPs (Blanchette et al. 2016a, Blanchette et al. (2016b), Gauthier and Kaliszyk (2015), Kaliszyk and Urban (2014), Kaliszyk and Urban (2015a)).", "startOffset": 70, "endOffset": 178}, {"referenceID": 2, "context": "2010) have led to a significant improvement of the usability of ITPs (Blanchette et al. 2016a, Blanchette et al. (2016b), Gauthier and Kaliszyk (2015), Kaliszyk and Urban (2014), Kaliszyk and Urban (2015a)).", "startOffset": 70, "endOffset": 206}, {"referenceID": 8, "context": "We can also build on previous machine-learning extensions of leanCoP (Kaliszyk and Urban 2015b, Urban et al. (2011)).", "startOffset": 70, "endOffset": 116}, {"referenceID": 13, "context": "2016), as well as to single-player games such as SameGame (Schadd et al. 2012).", "startOffset": 58, "endOffset": 78}, {"referenceID": 13, "context": "A similar approach is taken in (Schadd et al. 2012).", "startOffset": 31, "endOffset": 51}, {"referenceID": 10, "context": "Furthermore, all provers below use the same proof format as the leanCoP tactic described in (Kaliszyk et al. 2015) to automatically find proofs for HOL Light, therefore adapting the HOL Light proof tactic to use our Monte Carlo system should be straightforward.", "startOffset": 92, "endOffset": 114}, {"referenceID": 0, "context": "All 1400 problems are taken from the MPTP2078 problem set (Alama et al. 2011), but sometimes augmented with facts that are not necessary for the proof to make proof attempts harder.", "startOffset": 58, "endOffset": 77}, {"referenceID": 7, "context": "On the parameter training set, we ran ParamILS (Hutter et al. 2009) with monteCoP, using a problem timeout of 2s.", "startOffset": 47, "endOffset": 67}, {"referenceID": 13, "context": "(Schadd et al. 2012) suggests running MCTS multiple times with different random seeds to avoid MCTS getting stuck in local maxima.", "startOffset": 0, "endOffset": 20}], "year": 2016, "abstractText": "Monte Carlo Tree Search (MCTS) is a technique to guide search in a large decision space by taking random samples and evaluating their outcome. In this work, we study MCTS methods in the context of the connection calculus and implement them on top of the leanCoP prover. This includes proposing useful proof-state evaluation heuristics that are learned from previous proofs, and proposing and automatically improving suitable MCTS strategies in this context. The system is trained and evaluated on a large suite of related problems coming from the Mizar proof assistant, showing that it is capable to find new and different proofs. To our knowledge, this is the first time MCTS has been applied to theorem proving.", "creator": "LaTeX with hyperref package"}}}