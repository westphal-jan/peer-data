{"id": "1611.06174", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Nov-2016", "title": "Stratified Knowledge Bases as Interpretable Probabilistic Models (Extended Abstract)", "abstract": "In this paper, we advocate the use of stratified logical theories for representing probabilistic models. We argue that such encodings can be more interpretable than those obtained in existing frameworks such as Markov logic networks. Among others, this allows for the use of domain experts to improve learned models by directly removing, adding, or modifying logical formulas.", "histories": [["v1", "Fri, 18 Nov 2016 17:51:56 GMT  (13kb,D)", "http://arxiv.org/abs/1611.06174v1", "Presented at NIPS 2016 Workshop on Interpretable Machine Learning in Complex Systems"]], "COMMENTS": "Presented at NIPS 2016 Workshop on Interpretable Machine Learning in Complex Systems", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["ondrej kuzelka", "jesse davis", "steven schockaert"], "accepted": false, "id": "1611.06174"}, "pdf": {"name": "1611.06174.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Jesse Davis", "Steven Schockaert"], "emails": ["KuzelkaO@cardiff.ac.uk", "jesse.davis@cs.kuleuven.be", "SchockaertS1@cardiff.ac.uk"], "sections": [{"heading": "1 Introduction", "text": "The use of symbolic representations to encode generative models is attractive because they can be more transparent than black box models such as neural networks. A notable example of such a symbolic framework are Markov logic networks (MLNs [5]), which use weighted first-order formulas to encode probabilistic models. These logical formulas can give us some important insights into what regularities have been identified in a given dataset. Unfortunately, the weights used in MLNs are difficult to interpret, as their intuitive meaning depends on the interaction with the weights of other formulas [6, 2]. This means that we cannot readily use MLNs to generate explanations for predictions (since the formulas cannot be isolated), and this makes it difficult for domain experts to directly verify or improve learned MLNs."}, {"heading": "2 Logical encoding of density estimation trees", "text": "This transformation is illustrated in Figure 2. The density estimation tree on the left side of the formula \"formula\" may, among other things, indicate that any world in which Antarctica has a probability of 0.25 is wrong. The representation on the right side of Figure 2 encodes the same knowledge using possible logic [1]. Specifically, each branch of the density tree can be regarded as a combination of letters; e.g. the leftmost association of the letters \"Bird\" Bird \"Bird\" Bird \"Bird.\" Bird \"Bird\" and Flies \"have a probability of 0.25. The representation on the right side of Figure 2 encodes the same knowledge using possible logic.\""}, {"heading": "3 Inference", "text": "However, it turns out that the MAP conclusion corresponds to a standard procedure for inconsistency-tolerant reasoning in possible logic. Specifically, to verify whether a formula \u03b2 is true in all highly probable models of \u03b1, w.r.t. a possible logic theory B, which we can pursue as follows: First, we add (\u03b1, 1) to B. Then we find the minimum weight w [0, 1] so that the formula \u03b2 applies in all highly probable models of \u03b1, w.r.t. a possible logic theory B, which we can proceed as follows (\u03b1, 1) to B. Then we find the minimum weight w [0, 1] such that the series of formulas appear with a weight strictly higher than w, than we can find a consistent series of classical formulas Hay.It can be shown that a formula \u03b2 can be shown in all high-stranded models of Hay.It can be shown that a formula \u03b2 we can derive from Hay.It can be shown that a formula \u03b2."}, {"heading": "4 Conclusions and future work", "text": "Stratified (probabilistic) knowledge databases can logically encode the information gathered from density trees, leading to several important advantages for interpretable machine learning. First, SKBs can be used to generate logical justifications (proofs) to support the predictions made from them. Furthermore, the explicit symbolic nature of SKBs allows the user with a basic understanding of the statement logic to modify the SKBs relatively easily. These properties make SKBs remarkable not only compared to black box models such as neural networks, but also compared to other symbolic methods such as Markov logic networks. Several interesting directions for future work remain. For example, in [2] we have shown that for any probability distribution it is possible to construct a compact SKBs that capture MAPinference precisely with evidence of limited size. The techniques presented in [2] can of course be applied to summarize such evidence, which can only be derived from small density trees."}, {"heading": "Acknowledgment", "text": "Jesse Davis is partially supported by the KU Leuven Research Fund (C22 / 15 / 015) and FWO-Vlaanderen (G.0356.12, SBO-150033)."}], "references": [{"title": "Possibilistic logic", "author": ["D. Dubois", "J. Lang", "H. Prade"], "venue": "D. N. D. Gabbay, C. Hogger J. Robinson, editor, Handbook of Logic in Artificial Intelligence and Logic Programming, volume 3, pages 439\u2013513. Oxford University Press", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1994}, {"title": "Encoding markov logic networks in possibilistic logic", "author": ["O. Ku\u017eelka", "J. Davis", "S. Schockaert"], "venue": "Proc. UAI", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2015}, {"title": "Interpretable encoding of densities using possibilistic logic", "author": ["O. Ku\u017eelka", "J. Davis", "S. Schockaert"], "venue": "Proc. ECAI, pages 1239\u20131247", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2016}, {"title": "Density estimation trees", "author": ["P. Ram", "A.G. Gray"], "venue": "Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 627\u2013635. ACM", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2011}, {"title": "Markov logic networks", "author": ["M. Richardson", "P. Domingos"], "venue": "Mach. learn., 62(1-2):107\u2013136", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2006}, {"title": "Coherence and compatibility of markov logic networks", "author": ["M. Thimm"], "venue": "Proc. ECAI, pages 891\u2013896", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 4, "context": "One notable example of such a symbolic framework is Markov logic networks (MLNs [5]), which use weighted first-order formulas to encode probabilistic models.", "startOffset": 80, "endOffset": 83}, {"referenceID": 5, "context": "Unfortunately, the weights that are used in MLNs can be hard to interpret, as their intuitive meaning depends on the interplay with the weights of other formulas [6, 2].", "startOffset": 162, "endOffset": 168}, {"referenceID": 1, "context": "Unfortunately, the weights that are used in MLNs can be hard to interpret, as their intuitive meaning depends on the interplay with the weights of other formulas [6, 2].", "startOffset": 162, "endOffset": 168}, {"referenceID": 3, "context": "Density estimation trees [4] encode probabilistic models in a symbolic way by explicitly defining sets of possible worlds (i.", "startOffset": 25, "endOffset": 28}, {"referenceID": 2, "context": "The approach we propose starts by converting a density estimation tree into a stratified logical theory, and then uses different forms of pruning to obtain an (often significantly) more compact theory, in which relationships between variables such as \u201cif a then typically \u00acb\u201d are modelled explicitly [3].", "startOffset": 300, "endOffset": 303}, {"referenceID": 1, "context": "The density estimation tree on the left-hand side encodes, among others, that every world in which the \u2217This extended abstract describes our recent work [2, 3] from the perspective of interpretable machine learning.", "startOffset": 153, "endOffset": 159}, {"referenceID": 2, "context": "The density estimation tree on the left-hand side encodes, among others, that every world in which the \u2217This extended abstract describes our recent work [2, 3] from the perspective of interpretable machine learning.", "startOffset": 153, "endOffset": 159}, {"referenceID": 0, "context": "The representation on the right-hand side of Figure 2 encodes the same knowledge using possibilistic logic [1].", "startOffset": 107, "endOffset": 110}, {"referenceID": 2, "context": "While the change between B and B\u2032 is perhaps not drastic, we have shown in [3] that (in extreme cases) this method can lead to theories which are exponentially more compact than the initial density estimation tree.", "startOffset": 75, "endOffset": 78}, {"referenceID": 2, "context": "Experimental results in [3] show that thus reducing the possibilistic logic theories to 10% of their initial size usually does not lead to significantly lower predictive accuracies for MAPinference.", "startOffset": 24, "endOffset": 27}, {"referenceID": 2, "context": "In [3] we show how such weights can be found using geometric programming.", "startOffset": 3, "endOffset": 6}, {"referenceID": 0, "context": "Then we find the minimal weight w \u2208 [0, 1] such that the set of formulas which appear with a weight that is strictly higher than w forms a consistent set B\u03b1 of classical formulas.", "startOffset": 36, "endOffset": 42}, {"referenceID": 2, "context": "In particular, we can show that [3]:", "startOffset": 32, "endOffset": 35}, {"referenceID": 1, "context": "For example, in [2] we have shown that, for any probability distribution, it is possible to construct a compact SKB which exactly captures MAPinference with evidence sets of bounded size.", "startOffset": 16, "endOffset": 19}, {"referenceID": 1, "context": "The techniques presented in [2] can naturally be applied for summarizing what can be derived from density estimation trees when only small evidence sets are considered.", "startOffset": 28, "endOffset": 31}, {"referenceID": 1, "context": "Another interesting direction would be to lift our approach into the first-order setting, similar to how we lifted the method in [2] to encode MLNs.", "startOffset": 129, "endOffset": 132}], "year": 2016, "abstractText": "In this paper, we advocate the use of stratified logical theories for representing probabilistic models. We argue that such encodings can be more interpretable than those obtained in existing frameworks such as Markov logic networks. Among others, this allows for the use of domain experts to improve learned models by directly removing, adding, or modifying logical formulas.", "creator": "LaTeX with hyperref package"}}}