{"id": "1611.06216", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Nov-2016", "title": "Generative Deep Neural Networks for Dialogue: A Short Review", "abstract": "Researchers have recently started investigating deep neural networks for dialogue applications. In particular, generative sequence-to-sequence (Seq2Seq) models have shown promising results for unstructured tasks, such as word-level dialogue response generation. The hope is that such models will be able to leverage massive amounts of data to learn meaningful natural language representations and response generation strategies, while requiring a minimum amount of domain knowledge and hand-crafting. An important challenge is to develop models that can effectively incorporate dialogue context and generate meaningful and diverse responses. In support of this goal, we review recently proposed models based on generative encoder-decoder neural network architectures, and show that these models have better ability to incorporate long-term dialogue history, to model uncertainty and ambiguity in dialogue, and to generate responses with high-level compositional structure.", "histories": [["v1", "Fri, 18 Nov 2016 20:11:51 GMT  (529kb,D)", "http://arxiv.org/abs/1611.06216v1", "6 pages, 1 figure, 3 tables; NIPS 2016 workshop on Learning Methods for Dialogue"]], "COMMENTS": "6 pages, 1 figure, 3 tables; NIPS 2016 workshop on Learning Methods for Dialogue", "reviews": [], "SUBJECTS": "cs.CL cs.AI cs.NE", "authors": ["iulian vlad serban", "ryan lowe", "laurent charlin", "joelle pineau"], "accepted": false, "id": "1611.06216"}, "pdf": {"name": "1611.06216.pdf", "metadata": {"source": "CRF", "title": "Generative Deep Neural Networks for Dialogue: A Short Review", "authors": ["Iulian Vlad Serban"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "Researchers have recently begun to investigate sequence-to-sequence (Seq2Seq) models for dialog applications, which typically use neural networks to present dialog stories and generate or select appropriate responses; such models are able to use large amounts of data to learn meaningful natural speech representations and generational strategies, while requiring a minimum of domain knowledge and manual labor. Although the Seq2Seq framework differs from the well-established goal-oriented setting [Gorin et al., 1997, Young, 2000, Singh et al., 2002], these models have already been applied to several real-world applications, with Microsoft's Xiaoice system [Markoff and Mozur, 2015] and Google's Smart Reply system [Kannan et al., 2016] as two prominent examples. Researchers have mainly studied two types of Seq2Seq models and Yal models, the first being generative models that typically generate cross-replies using 2014 and 2014 models."}, {"heading": "2 Models", "text": "The fact is that we are in a position, in a time, in a time, in a time, in a time, in a time, in a time, in a time, in a time, in a time, in a time, in a time, in a time, in a time, in a time, in a time, in a time, in a time, in a time, in a time, in a time, in a time, in a time, in a time, in a time, in a time, in a time, in a time, in a time, in a time, in a time, in a time, in a time, in a time, in a time, in a time, in a time, in which we are in a time, in which we are in a time, in which we are in a time, in which we are in a time, in which we are in a time, in which we are in a time, in which we are in a time, in which we are in a time, in which we are in a time, in which we are in a time, in which we are in a time, in which we are in a time, in which we are in a time, in which we are in a time, in which we are in a time, in which we are in a time, in which we are in a time, in a time, in which we are in a time, in which we are in a time, in which we are in a time, in which we are in a time, in a time, in which we are in a time, in which we are in a time, in a time, in which we are in a time, in which we are in a time, in a time, in which we are in a time, in a time, in which we are in a time, in a time, in which we are in a time, in which we are in a time, in a time, in which we are in a time, in a time, in which we are in a time, in which we are in a time, in a time, in which we are in a time, in which we are in a time, in a time, in a time, in which we are in a time, in a time, in which we are in a time, in a time, in a time, in which we are in a time, in which we are"}, {"heading": "3 Experiments", "text": "We apply our generative models to the generation of dialogue answers on the Ubuntu Dialogue Corpus [Lowe et al., 2015]. This task has been extensively investigated in recent literature [Ritter et al., 2011, Sordoni et al., 2015, Li et al., 2016a].Corpus: The Ubuntu Dialogue Corpus. Technical problems range from software and hardware-related problems (e.g. installing packages, fixing faulty drivers) to information needs (e.g. identifying software).Rating: We conduct a human study in the laboratory to evaluate the model replies. We compare them with 5 assessment guidelines we apply the answers between the evaluation candidates and 4 evaluation guidelines we each with 4 evaluation guidelines - whereby we apply our generative models to the generation of dialogue answers on the Ubuntu Dialogue Corpus [Lowe et al., 2015] we apply the answers between the evaluation candidates and 4 evaluation guidelines we each with 4 evaluation guidelines - whereby we perform our generative models on the generation of dialogue words on the Ubuntu Dialogue Corpus [Lowe et al., 2015] We apply 5 evaluation guidelines we apply the answers between the evaluation candidates and 4 evaluation guidelines we each with 4 evaluation guidelines with 4 evaluation guidelines - e.g. 4 evaluation guidelines we use 4 evaluation guidelines (e.g. 4 evaluation guidelines)."}, {"heading": "Model F1 Activity F1 Entity Human Fluency Human Relevancy", "text": "Results: The results are given in Table 2. MrRNN with noun representations achieve an F1 entity value of 6.31, while all other models between 0.87 and 2.53 receive less than half of the F1 values, and human evaluators consistently rate their frequency and relevance significantly higher than all base models. MrRNN with activity representations achieves an F1 activity value of 11.43, while all other models between 1.18 and 4.63 achieve less than half of the F1 activity values and perform substantially better than the base models with the F1 entity value. This indicates that MrRNNs have learned to model high-level, goal-oriented sequential structures in the Ubuntu domain. Following these, VHRED performs better than the appeal and LSTM models etc. Both activities and these entities suggest that the responses to the HREM generation structure are better than those to the REHM delicts."}, {"heading": "4 Discussion", "text": "We have presented generative models for generating dialogue responses; we have proposed architectural modifications with inductive distortions in the direction of 1) incorporating longer-term contexts; 2) dealing with uncertainty and ambiguity; and 3) generating diverse and thematic responses with high compositional structure. Our experiments demonstrate the advantage of architectural modifications quantitatively through human experiments and qualitatively through manual inspections; these experiments demonstrate the need for further research into generative model architectures. Although we have focused on three generative models, other model architectures such as memory-based models [Bordes and Weston, 2016, Weston et al., 2015] and attention-based models [Shang et al., 2015] have shown promising results and therefore deserve the attention of future researchers. In another line of work, researchers have begun to propose alternative training and reaction selection criteria [Weston, 2016]."}, {"heading": "Appendix", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Twitter Results", "text": "Corpus: We are experimenting with a Twitter Dialogue Corpus [Ritter et al., 2011] that contains about one million dialogues, the task of which is to generate expressions that are attached to existing Twitter conversations. This task is typically categorized as a non-targeted task, since any fluid and topic-specific answer can be appropriate. Evaluation: We are conducting a human study on Amazon Mechanical Turk (AMT). We are showing human evaluators a dialogue context along with two potential answers: an answer that is generated from any model based on the dialogue context. We are asking evaluators to choose the answer that is most appropriate for the dialogue context. If the evaluators are indifferent, they cannot choose an answer. For each model pair, we are conducting two experiments: one in which the sample contexts contain at least 80 unique tokens (long context), and one in which they contain at least 20 (not necessarily unique) tokens (short context)."}], "references": [{"title": "Learning end-to-end goal-oriented dialog", "author": ["A. Bordes", "J. Weston"], "venue": "arXiv preprint arXiv:1605.07683,", "citeRegEx": "Bordes and Weston.,? \\Q2016\\E", "shortCiteRegEx": "Bordes and Weston.", "year": 2016}, {"title": "Deep neural network approach for the dialog state tracking challenge", "author": ["M. Henderson", "B. Thomson", "S. Young"], "venue": "In Proceedings of the SIGDIAL 2013 Conference,", "citeRegEx": "Henderson et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Henderson et al\\.", "year": 2013}, {"title": "Neural utterance ranking model for conversational dialogue systems", "author": ["M. Inaba", "K. Takahashi"], "venue": "In 17th Annual Meeting of the Special Interest Group on Discourse and Dialogue,", "citeRegEx": "Inaba and Takahashi.,? \\Q2016\\E", "shortCiteRegEx": "Inaba and Takahashi.", "year": 2016}, {"title": "Smart reply: Automated response suggestion for email", "author": ["A. Kannan", "K. Kurach", "S. Ravi", "T. Kaufmann", "A. Tomkins", "B. Miklos", "G. Corrado", "L. Luk\u00e1cs", "M. Ganea", "P. Young"], "venue": "In Proceedings of the ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD),", "citeRegEx": "Kannan et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Kannan et al\\.", "year": 2016}, {"title": "A diversity-promoting objective function for neural conversation models", "author": ["J. Li", "M. Galley", "C. Brockett", "J. Gao", "B. Dolan"], "venue": "In NAACL,", "citeRegEx": "Li et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Li et al\\.", "year": 2016}, {"title": "Deep reinforcement learning for dialogue generation", "author": ["J. Li", "W. Monroe", "A. Ritter", "D. Jurafsky"], "venue": "arXiv preprint arXiv:1606.01541,", "citeRegEx": "Li et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Li et al\\.", "year": 2016}, {"title": "The Ubuntu Dialogue Corpus: A Large Dataset for Research in Unstructured Multi-Turn Dialogue Systems", "author": ["R. Lowe", "N. Pow", "I. Serban", "J. Pineau"], "venue": "In Proc. of SIGDIAL-2015,", "citeRegEx": "Lowe et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lowe et al\\.", "year": 2015}, {"title": "Mozur. For sympathetic ear, more chinese turn to smartphone program", "author": ["P.J. Markoff"], "venue": "NY Times,", "citeRegEx": "Markoff,? \\Q2015\\E", "shortCiteRegEx": "Markoff", "year": 2015}, {"title": "Multidomain dialog state tracking using recurrent neural networks", "author": ["N. Mrk\u0161i\u0107", "D.O. S\u00e9aghdha", "B. Thomson", "M. Ga\u0161i\u0107", "P.-H. Su", "D. Vandyke", "T.-H. Wen", "S. Young"], "venue": "In HLT-NAACL,", "citeRegEx": "Mrk\u0161i\u0107 et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mrk\u0161i\u0107 et al\\.", "year": 2015}, {"title": "Data-driven response generation in social media", "author": ["A. Ritter", "C. Cherry", "W.B. Dolan"], "venue": "In EMNLP,", "citeRegEx": "Ritter et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Ritter et al\\.", "year": 2011}, {"title": "Multiresolution recurrent neural networks: An application to dialogue response generation", "author": ["I.V. Serban", "T. Klinger", "G. Tesauro", "K. Talamadupula", "B. Zhou", "Y. Bengio", "A. Courville"], "venue": "arXiv preprint arXiv:1606.00776,", "citeRegEx": "Serban et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Serban et al\\.", "year": 2016}, {"title": "Building end-to-end dialogue systems using generative hierarchical neural network models", "author": ["I.V. Serban", "A. Sordoni", "Y. Bengio", "A.C. Courville", "J. Pineau"], "venue": "In AAAI,", "citeRegEx": "Serban et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Serban et al\\.", "year": 2016}, {"title": "A hierarchical latent variable encoder-decoder model for generating dialogues", "author": ["I.V. Serban", "A. Sordoni", "R. Lowe", "L. Charlin", "J. Pineau", "A. Courville", "Y. Bengio"], "venue": "arXiv preprint arXiv:1605.06069,", "citeRegEx": "Serban et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Serban et al\\.", "year": 2016}, {"title": "Neural responding machine for short-text conversation", "author": ["L. Shang", "Z. Lu", "H. Li"], "venue": "In ACL-IJCNLP,", "citeRegEx": "Shang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Shang et al\\.", "year": 2015}, {"title": "Optimizing dialogue management with reinforcement learning: Experiments with the njfun system", "author": ["S. Singh", "D. Litman", "M. Kearns", "M. Walker"], "venue": "JAIR, 16:105\u2013133,", "citeRegEx": "Singh et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Singh et al\\.", "year": 2002}, {"title": "A neural network approach to context-sensitive generation of conversational responses", "author": ["A. Sordoni", "M. Galley", "M. Auli", "C. Brockett", "Y. Ji", "M. Mitchell", "J.-Y. Nie", "J. Gao", "B. Dolan"], "venue": "In Conference of the North American Chapter of the Association", "citeRegEx": "Sordoni et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sordoni et al\\.", "year": 2015}, {"title": "Learning from real users: Rating dialogue success with neural networks for reinforcement learning in spoken dialogue systems", "author": ["P.-H. Su", "D. Vandyke", "M. Gasic", "D. Kim", "N. Mrksic", "T.-H. Wen", "S. Young"], "venue": "In SIGDIAL,", "citeRegEx": "Su et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Su et al\\.", "year": 2015}, {"title": "A neural conversational model", "author": ["O. Vinyals", "Q. Le"], "venue": "ICML, Workshop,", "citeRegEx": "Vinyals and Le.,? \\Q2015\\E", "shortCiteRegEx": "Vinyals and Le.", "year": 2015}, {"title": "Semantically conditioned lstm-based natural language generation for spoken dialogue systems", "author": ["T.-H. Wen", "M. Gasic", "N. Mrksic", "P.-H. Su", "D. Vandyke", "S. Young"], "venue": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Wen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wen et al\\.", "year": 2015}, {"title": "A network-based end-to-end trainable task-oriented dialogue system", "author": ["T.-H. Wen", "M. Gasic", "N. Mrksic", "L.M. Rojas-Barahona", "P.-H. Su", "S. Ultes", "D. Vandyke", "S. Young"], "venue": null, "citeRegEx": "Wen et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Wen et al\\.", "year": 2016}, {"title": "Dialog-based language learning", "author": ["J. Weston"], "venue": "arXiv preprint arXiv:1604.06045,", "citeRegEx": "Weston.,? \\Q2016\\E", "shortCiteRegEx": "Weston.", "year": 2016}, {"title": "Probabilistic methods in spoken\u2013dialogue systems", "author": ["S. Young"], "venue": "Philosophical Transactions of the Royal Society of London. Series A: Mathematical, Physical and Engineering Sciences,", "citeRegEx": "Young.,? \\Q2000\\E", "shortCiteRegEx": "Young.", "year": 2000}, {"title": "Strategy and policy learning for non-task-oriented conversational systems", "author": ["Z. Yu", "Z. Xu", "A.W. Black", "A.I. Rudnicky"], "venue": "In 17th Annual Meeting of the Special Interest Group on Discourse and Dialogue,", "citeRegEx": "Yu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Yu et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 3, "context": ", 2002], these models have already been applied to several real-world applications, with Microsoft\u2019s system Xiaoice [Markoff and Mozur, 2015] and Google\u2019s Smart Reply system [Kannan et al., 2016] as two prominent examples.", "startOffset": 174, "endOffset": 195}, {"referenceID": 6, "context": "We apply our generative models to dialogue response generation on the Ubuntu Dialogue Corpus [Lowe et al., 2015].", "startOffset": 93, "endOffset": 112}, {"referenceID": 10, "context": "the activity-entity metrics proposed by Serban et al. [2016a]. These metrics measure whether the model response contains the same activities (e.", "startOffset": 40, "endOffset": 62}, {"referenceID": 13, "context": ", 2015] and attention-based models [Shang et al., 2015] have also demonstrated promising results and therefore deserve the attention of future research.", "startOffset": 35, "endOffset": 55}, {"referenceID": 20, "context": "In another line of work, researchers have started proposing alternative training and response selection criteria [Weston, 2016].", "startOffset": 113, "endOffset": 127}, {"referenceID": 4, "context": "Li et al. [2016a] propose ranking candidate responses according to a mutual information criterion, in order to incorporate dialogue context efficiently and retrieve on-topic responses.", "startOffset": 0, "endOffset": 18}, {"referenceID": 4, "context": "Li et al. [2016a] propose ranking candidate responses according to a mutual information criterion, in order to incorporate dialogue context efficiently and retrieve on-topic responses. Li et al. [2016b] further propose a model trained using reinforcement learning to optimize a hand-crafted reward function.", "startOffset": 0, "endOffset": 203}, {"referenceID": 4, "context": "Li et al. [2016a] propose ranking candidate responses according to a mutual information criterion, in order to incorporate dialogue context efficiently and retrieve on-topic responses. Li et al. [2016b] further propose a model trained using reinforcement learning to optimize a hand-crafted reward function. Both these models are motivated by the lack of diversity observed in the generative model responses. Similarly, Yu et al. [2016] propose a hybrid model\u2014combining retrieval models, neural networks and hand-crafted rules\u2014trained using reinforcement learning to optimize a hand-crafted reward function.", "startOffset": 0, "endOffset": 437}], "year": 2016, "abstractText": "Researchers have recently started investigating deep neural networks for dialogue applications. In particular, generative sequence-to-sequence (Seq2Seq) models have shown promising results for unstructured tasks, such as word-level dialogue response generation. The hope is that such models will be able to leverage massive amounts of data to learn meaningful natural language representations and response generation strategies, while requiring a minimum amount of domain knowledge and hand-crafting. An important challenge is to develop models that can effectively incorporate dialogue context and generate meaningful and diverse responses. In support of this goal, we review recently proposed models based on generative encoder-decoder neural network architectures, and show that these models have better ability to incorporate long-term dialogue history, to model uncertainty and ambiguity in dialogue, and to generate responses with high-level compositional structure.", "creator": "LaTeX with hyperref package"}}}