{"id": "1611.06342", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Nov-2016", "title": "Quantized neural network design under weight capacity constraint", "abstract": "The complexity of deep neural network algorithms for hardware implementation can be lowered either by scaling the number of units or reducing the word-length of weights. Both approaches, however, can accompany the performance degradation although many types of research are conducted to relieve this problem. Thus, it is an important question which one, between the network size scaling and the weight quantization, is more effective for hardware optimization. For this study, the performances of fully-connected deep neural networks (FCDNNs) and convolutional neural networks (CNNs) are evaluated while changing the network complexity and the word-length of weights. Based on these experiments, we present the effective compression ratio (ECR) to guide the trade-off between the network size and the precision of weights when the hardware resource is limited.", "histories": [["v1", "Sat, 19 Nov 2016 11:21:25 GMT  (5481kb,D)", "http://arxiv.org/abs/1611.06342v1", "This paper is accepted at NIPS 2016 workshop on Efficient Methods for Deep Neural Networks (EMDNN). arXiv admin note: text overlap witharXiv:1511.06488"]], "COMMENTS": "This paper is accepted at NIPS 2016 workshop on Efficient Methods for Deep Neural Networks (EMDNN). arXiv admin note: text overlap witharXiv:1511.06488", "reviews": [], "SUBJECTS": "cs.LG cs.NE", "authors": ["sungho shin", "kyuyeon hwang", "wonyong sung"], "accepted": false, "id": "1611.06342"}, "pdf": {"name": "1611.06342.pdf", "metadata": {"source": "CRF", "title": "Quantized neural network design under weight capacity constraint", "authors": ["Sungho Shin", "Kyuyeon Hwang"], "emails": ["sungho.develop@gmail.com,", "kyuyeon.hwang@gmail.com,", "wysung@snu.ac.kr"], "sections": [{"heading": null, "text": "The complexity of deep neural network algorithms for hardware implementation can be reduced either by scaling the number of units or reducing the word length of the weights. However, both approaches can accompany performance degradation, although many types of research are being conducted to solve this problem. Therefore, it is an important question which is more effective for hardware optimization between network size scaling and weight quantization. In this study, the performance of fully networked deep neural networks (FCDNs) and Convolutionary neural Networks (CNNs) is evaluated as network complexity and word length of weights change. Based on these experiments, we present the effective compression rate (ECR) to derive the trade-off between network size and precision of weights with limited hardware resource."}, {"heading": "1 Introduction", "text": "Deep Neural Networks (DNNs) are beginning to find many real-time applications such as speech recognition, autonomous driving, gesture recognition, and robot control (Sak et al., 2015; Chen et al., 2015; Shin & Sung, 2016; Corradini et al., 2015) Recent work shows that the precision required to implement fully networked deep neural networks (FCDNNs), Convolutionary Neural Networks (CNNs), or Recursive Neural Networks (RNNNNs) need not be very high, especially when the quantified networks are retrained to learn the effects of reduced precision. In the fixed point optimization examples shown in Hwang & Sung, neural networks with ternary weights performed reasonably well, close to that of flow point arithmetics."}, {"heading": "2 Related Work", "text": "The fixed-point design of ternary weight DNNs performs reasonably well, very close to floating-point results (Hwang & Sung, 2014; Anwar et al., 2015a; Shin et al., 2016).The ternary weight-based FCDNN is used for VLSI implementations where the algorithms can only operate with very low-power on-chip memory (Kim et al., 2014).The CNN is implemented by XNOR bitcounting operations (Rastegari et al., 2016).The design of binary weight-based deep neural networks is also being investigated (Courbariaux et al., 2015).Shrunk floating-point weights are used for efficient GPU-based implementations where small or less important weights need to be reduced to zero in order to reduce the number of arithmetic operations and the storage space for weight storage (Yu, et al., 2015b, et al.)."}, {"heading": "3 Fixed-Point FCDNN and CNN Design", "text": "This section explains the design of FCDNN and CNN with different network complexity and weight accuracy."}, {"heading": "3.1 FCDNN and CNN Design", "text": "The reference DNN has four hidden layers. Each of the hidden layers has Nh units; the value of Nh is changed to control the complexity of the network. We conduct experiments with Nh values of 32, 64, 128, 256, 512 and 1024. The input layer of the network has 1353 units to accept 11 images of a filter bank on Fourier transformation with 40 coefficients (+ energy) distributed on a mel scale, along with their first and second temporary derivatives. The output layer consists of 61 Softmax units corresponding to 61 target phoneme labels (Mohamed et al., 2012). Phoneme recognition experiments were performed on the TIMIT corpus."}, {"heading": "3.2 Fixed-Point Optimization of DNNs", "text": "Reducing the word length of weights brings several advantages to hardware-based implementation of neural networks: First, it reduces computational accuracy, thus reducing the number of gates needed for multipliers; second, it reduces the memory size for storing weights, which would be a great advantage if stored on a chip instead of external DRAMs or NAND flash memory. Note that FCDNNNs require a very large number of weights; third, lower computational accuracy or minimization of off-chip memory access results in low power consumption.The design of the fixed-point DNN algorithm consists of three steps: floating-point training, direct quantification and retraining of weights. See Hwang & Sung (2014) for details."}, {"heading": "4 Analysis of Quantization Effects", "text": "The performance of direct 2 bits (ternary planes), direct 3 bits (7 planes), retrain-based 2 bits and retrain-based 3 bits is compared to the floating-point results. Direct quantization does not show good results for each network size. In this figure, the performance of the floating-point network is almost saturated when the network size is about 1024. Note that the TIMIT corpus used for training only has 3 hours of data. Thus, the network can be considered with 1024 hidden units in the \"training data region.\" Here, the gap between floating-point and fixed commands has almost disappeared when the network varies in the \"training data region.\""}, {"heading": "5 Efficient DNN Design with Hardware Constraints", "text": "As the number of quantization levels decreases, the space required decreases at the expense of sacrificing accuracy. Therefore, there may be a trade-off between reducing the network size and aggressive quantization. Figure 2a shows the framewise phoneme error rate on the TIMIT corpus, while the layer size of FCDNs with a different number of quantization bits varies from 2 to 8 bits. Note that the network has four hidden layers that contain the same number of units. However, in this section we propose a policy for determining the optimal bit width when the desired accuracy or network size is given. Note that we assume that 2n \u2212 1 quantization levels are represented by n bits (i.e. 2 bits are required to represent an external weight). For simplicity, all layers are quantified with the same number of quantization layers when the network size or network size is specified."}, {"heading": "6 Conclusion", "text": "Hardware-efficient deep neural networks can be designed either by reducing the number of units in each layer or by reducing the number of bits for weight quantification. We evaluate the performance of fixed neural networks and analyze the trade-off between the complexity and precision of weights. This study shows that low-performance hardware-efficient DNNs can be designed with highly quantified weights. In the low-performance region, DNN performance increases very quickly as the network size grows. Therefore, it is possible to offset the quantization effects by slightly increasing the network size. However, for a high-performance DNN design, it is difficult to compensate for quantization effects by increasing the network size, and therefore strict quantization does not result in efficient hardware design. The effective compression ratio is given for a DNN design when network size and precision vary."}, {"heading": "Acknowledgments", "text": "This work was partially supported by the Brain Korea 21 Plus Project and the Korean Government-funded National Research Foundation of Korea (NRF) (No. 2015R1A2A1A10056051)."}], "references": [{"title": "Fixed point optimization of deep convolutional neural networks for object recognition", "author": ["Anwar", "Sajid", "Hwang", "Kyuyeon", "Sung", "Wonyong"], "venue": "In Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "Anwar et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Anwar et al\\.", "year": 2015}, {"title": "Structured pruning of deep convolutional neural networks", "author": ["Anwar", "Sajid", "Hwang", "Kyuyeon", "Sung", "Wonyong"], "venue": "arXiv preprint arXiv:1512.08571,", "citeRegEx": "Anwar et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Anwar et al\\.", "year": 2015}, {"title": "Deepdriving: Learning affordance for direct perception in autonomous driving", "author": ["Chen", "Chenyi", "Seff", "Ari", "Kornhauser", "Alain", "Xiao", "Jianxiong"], "venue": "In Proceedings of the IEEE International Conference on Computer Vision, pp", "citeRegEx": "Chen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2015}, {"title": "Robust control of robot arms via quasi sliding modes and neural networks", "author": ["Corradini", "Maria Letizia", "Giantomassi", "Andrea", "Ippoliti", "Gianluca", "Longhi", "Sauro", "Orlando", "Giuseppe"], "venue": "In Advances and Applications in Sliding Mode Control systems,", "citeRegEx": "Corradini et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Corradini et al\\.", "year": 2015}, {"title": "Binaryconnect: Training deep neural networks with binary weights during propagations", "author": ["Courbariaux", "Matthieu", "Bengio", "Yoshua", "David", "Jean-Pierre"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Courbariaux et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Courbariaux et al\\.", "year": 2015}, {"title": "Deep compression: Compressing deep neural network with pruning, trained quantization and huffman coding", "author": ["Han", "Song", "Mao", "Huizi", "Dally", "William J"], "venue": "CoRR, abs/1510.00149,", "citeRegEx": "Han et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Han et al\\.", "year": 2015}, {"title": "Fixed-point feedforward deep neural network design using weights +1, 0, and -1", "author": ["Hwang", "Kyuyeon", "Sung", "Wonyong"], "venue": "In Signal Processing Systems (SiPS),", "citeRegEx": "Hwang et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Hwang et al\\.", "year": 2014}, {"title": "X1000 real-time phoneme recognition VLSI using feed-forward deep neural networks", "author": ["Kim", "Jonghong", "Hwang", "Kyuyeon", "Sung", "Wonyong"], "venue": "In Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "Kim et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kim et al\\.", "year": 2014}, {"title": "Learning multiple layers of features from tiny images", "author": ["Krizhevsky", "Alex", "Hinton", "Geoffrey"], "venue": null, "citeRegEx": "Krizhevsky et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2009}, {"title": "Acoustic modeling using deep belief networks. Audio, Speech, and Language Processing", "author": ["Mohamed", "Abdel-rahman", "Dahl", "George E", "Hinton", "Geoffrey"], "venue": "IEEE Transactions on,", "citeRegEx": "Mohamed et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Mohamed et al\\.", "year": 2012}, {"title": "XNOR-Net: Imagenet classification using binary convolutional neural networks", "author": ["Rastegari", "Mohammad", "Ordonez", "Vicente", "Redmon", "Joseph", "Farhadi", "Ali"], "venue": "arXiv preprint arXiv:1603.05279,", "citeRegEx": "Rastegari et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Rastegari et al\\.", "year": 2016}, {"title": "Fast and accurate recurrent neural network acoustic models for speech recognition", "author": ["Sak", "Ha\u015fim", "Senior", "Andrew", "Rao", "Kanishka", "Beaufays", "Fran\u00e7oise"], "venue": "arXiv preprint arXiv:1507.06947,", "citeRegEx": "Sak et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sak et al\\.", "year": 2015}, {"title": "Dynamic hand gesture recognition for wearable devices with low complexity recurrent neural networks", "author": ["Shin", "Sungho", "Sung", "Wonyong"], "venue": "In Circuits and Systems (ISCAS),", "citeRegEx": "Shin et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Shin et al\\.", "year": 2016}, {"title": "Fixed-point performance analysis of recurrent neural networks", "author": ["Shin", "Sungho", "Hwang", "Kyuyeon", "Sung", "Wonyong"], "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "Shin et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Shin et al\\.", "year": 2016}, {"title": "Exploiting sparseness in deep neural networks for large vocabulary speech recognition", "author": ["Yu", "Dong", "Seide", "Frank", "Li", "Gang", "Deng"], "venue": "In Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "Yu et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Yu et al\\.", "year": 2012}], "referenceMentions": [{"referenceID": 11, "context": "Deep neural networks (DNNs) begin to find many real-time applications, such as speech recognition, autonomous driving, gesture recognition, and robotic control (Sak et al., 2015; Chen et al., 2015; Shin & Sung, 2016; Corradini et al., 2015).", "startOffset": 160, "endOffset": 240}, {"referenceID": 2, "context": "Deep neural networks (DNNs) begin to find many real-time applications, such as speech recognition, autonomous driving, gesture recognition, and robotic control (Sak et al., 2015; Chen et al., 2015; Shin & Sung, 2016; Corradini et al., 2015).", "startOffset": 160, "endOffset": 240}, {"referenceID": 3, "context": "Deep neural networks (DNNs) begin to find many real-time applications, such as speech recognition, autonomous driving, gesture recognition, and robotic control (Sak et al., 2015; Chen et al., 2015; Shin & Sung, 2016; Corradini et al., 2015).", "startOffset": 160, "endOffset": 240}, {"referenceID": 2, "context": ", 2015; Chen et al., 2015; Shin & Sung, 2016; Corradini et al., 2015). Recent works show that the precision required for implementing fully-connected deep neural networks (FCDNNs), convolutional neural networks (CNNs) or recurrent neural networks (RNNs) needs not be very high, especially when the quantized networks are trained again to learn the effects of lowered precision. In the fixed-point optimization examples shown in Hwang & Sung (2014), neural networks with ternary weights showed quite good performance which was close to that of floating-point arithmetic.", "startOffset": 8, "endOffset": 448}, {"referenceID": 12, "context": "Fixed-point design of DNNs with ternary weights show quite good performances that are very close to the floating-point results (Hwang & Sung, 2014; Anwar et al., 2015a; Shin et al., 2016).", "startOffset": 127, "endOffset": 187}, {"referenceID": 7, "context": "The ternary weight based FCDNN is used for VLSI implementations, by which the algorithms can operate with only on-chip memory consuming very low power (Kim et al., 2014).", "startOffset": 151, "endOffset": 169}, {"referenceID": 10, "context": "The CNN is implemented by XNOR-bitcounting operations (Rastegari et al., 2016).", "startOffset": 54, "endOffset": 78}, {"referenceID": 4, "context": "Binary weight based deep neural network design is also studied (Courbariaux et al., 2015).", "startOffset": 63, "endOffset": 89}, {"referenceID": 14, "context": "Pruned floating-point weights are utilized for efficient GPU-based implementations, where small valued or less important weights are forced to zero to reduce the number of arithmetic operations and the memory space for weight storage (Yu et al., 2012; Han et al., 2015; Anwar et al., 2015b).", "startOffset": 234, "endOffset": 290}, {"referenceID": 5, "context": "Pruned floating-point weights are utilized for efficient GPU-based implementations, where small valued or less important weights are forced to zero to reduce the number of arithmetic operations and the memory space for weight storage (Yu et al., 2012; Han et al., 2015; Anwar et al., 2015b).", "startOffset": 234, "endOffset": 290}, {"referenceID": 9, "context": "The output layer consists of 61 softmax units which correspond to 61 target phoneme labels (Mohamed et al., 2012).", "startOffset": 91, "endOffset": 113}, {"referenceID": 9, "context": "The output layer consists of 61 softmax units which correspond to 61 target phoneme labels (Mohamed et al., 2012). Phoneme recognition experiments were performed on the TIMIT corpus. The CNN used is for CIFAR-10 dataset (Krizhevsky & Hinton, 2009). It contains a training set of 50,000 and a test set of 10,000 images. We divided the training set to 40,000 images for training and 10,000 images for validation. The reference CNN has 3 convolution and max-pooling layers, a fully connected hidden layer with 64 units, and the output with 10 softmax units. We control the number of feature maps in each convolution layer. The reference size has 32-32-64 feature maps with a 5 by 5 kernel size as used in Krizhevskey (2014). To know the effects of network size variation, the number of feature maps is reduced or increased.", "startOffset": 92, "endOffset": 721}], "year": 2016, "abstractText": "The complexity of deep neural network algorithms for hardware implementation can be lowered either by scaling the number of units or reducing the word-length of weights. Both approaches, however, can accompany the performance degradation although many types of research are conducted to relieve this problem. Thus, it is an important question which one, between the network size scaling and the weight quantization, is more effective for hardware optimization. For this study, the performances of fully-connected deep neural networks (FCDNNs) and convolutional neural networks (CNNs) are evaluated while changing the network complexity and the word-length of weights. Based on these experiments, we present the effective compression ratio (ECR) to guide the trade-off between the network size and the precision of weights when the hardware resource is limited.", "creator": "LaTeX with hyperref package"}}}