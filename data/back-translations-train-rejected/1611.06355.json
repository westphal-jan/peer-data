{"id": "1611.06355", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Nov-2016", "title": "Invertible Conditional GANs for image editing", "abstract": "Generative Adversarial Networks (GANs) have recently demonstrated to successfully approximate complex data distributions. A relevant extension of this model is conditional GANs (cGANs), where the introduction of external information allows to determine specific representations of the generated images. In this work, we evaluate encoders to inverse the mapping of a cGAN, i.e., mapping a real image into a latent space and a conditional representation. This allows, for example, to reconstruct and modify real images of faces conditioning on arbitrary attributes. Additionally, we evaluate the design of cGANs. The combination of an encoder with a cGAN, which we call Invertible cGAN (IcGAN), enables to re-generate real images with deterministic complex modifications.", "histories": [["v1", "Sat, 19 Nov 2016 12:35:01 GMT  (2392kb,D)", "http://arxiv.org/abs/1611.06355v1", "Accepted paper at NIPS 2016 Workshop on Adversarial Training"]], "COMMENTS": "Accepted paper at NIPS 2016 Workshop on Adversarial Training", "reviews": [], "SUBJECTS": "cs.CV cs.AI", "authors": ["guim perarnau", "joost van de weijer", "bogdan raducanu", "jose m \\'alvarez"], "accepted": false, "id": "1611.06355"}, "pdf": {"name": "1611.06355.pdf", "metadata": {"source": "CRF", "title": "Invertible Conditional GANs for image editing", "authors": ["Guim Perarnau", "Joost van de Weijer", "Bogdan Raducanu", "Jose M. \u00c1lvarez"], "emails": ["guimperarnau@gmail.com", "joost@cvc.uab.es", "bogdan@cvc.uab.es", "jose.alvarez@nicta.com.au"], "sections": [{"heading": "1 Introduction", "text": "In this case, you need to change the attributes of a facial expression to reactivate them."}, {"heading": "2 Related work", "text": "There are several approaches to generative models, including two promising approaches that have recently advanced the state of the art with highly plausible generated images. The first approach is Variational Autoencoders (UAE) [1, 4, 7, 8], which impose a previous representation space z (e.g. normal distribution) to regulate and restrict the model to try it out. However, the main limitation of VAEs is the pixel-by-pixel reconstruction error that causes the output images to look blurry; the second approach is Generative Adversarial Nets (GANs). Originally developed by Goodfellow et al. [3], GANs with a deeper architecture (DCGAN) have been improved, which results in the output images looking blurry. [2] Recent advances have led to several techniques that improve overall performance for the formation of GANs."}, {"heading": "3 Background: Generative Adversarial Networks", "text": "A GAN consists of two neural networks, a generator G and a discriminator D. Once both networks are iteratively formed, they compete against each other in a minimax game. The generator aims to approximate the underlying unknown data distribution pdata, in order to deceive the discriminator, while the discriminator is focused on being able to detect which samples are real or generated. On convergence, we want pdata = pg, where pg is the generator distribution.More formally expressed, taking into account the function v (\u03b8g, \u03b8d), where hecg and \u03b8d are the parameters of the generator G or discriminator D, we can take GAN training as an optimization min g max d v (\u03b8g, \u03b8d) = Ex-pdata [logD (x)] + Ez data [log (G (z))))))))), (1) where z-labels are a vector noise that can be augmented from a known simple distribution pz (e.g. GANY) with strict GANcale-conditional framework (GANY)."}, {"heading": "4 Invertible Conditional GANs", "text": "We present invertible conditional GANs (IcGANs) that consist of a cGAN and an encoder. Although encoders have recently been introduced to the GAN framework [12, 13, 16], we are the first to include and use the conditional information y in the design of the encoding process. In Section 4.1, we explain how and why an encoder is included in the GAN framework for a conditional setting. In Section 4.2, we present our approach to refining cGANs in two aspects: conditional position and conditional sampling. The model architecture is described in Section 4.3."}, {"heading": "4.1 Encoder", "text": "A generator x \u2032 = G (z, y \u2032) from a GAN frame does not have the ability to map a real image x to its latent representation z. To overcome this problem, we can train an encoder / inference network E to reverse this mapping (z, y) = E (x). This inversion would allow us to have a latent representation z of a real image x, and then we could explore the latent space by interpolating or adding variations to it, which would lead to variations on the generated image x. In combination with a cGAN, the latent representation z of z is achieved, explicitly controlled variations can be added to an input image via conditional information. (e.g. we create a certain digit in MNIST or specific attributes on a face dataset). We call this combination Invertible cGAN, as now the mapping can be inverted."}, {"heading": "4.2 Conditional GAN", "text": "The first is to find the optimal conditional position of the generator and the discriminator, which in our opinion has not been addressed before. Second, we discuss the best approach to conditional information for the generator. The second is that the conditional position of the generator and the discriminator is always linked to the filter level."}, {"heading": "4.3 Model architecture", "text": "Conditional GAN The work of this work is based on the Torch implementation of DCGAN1 [2]. We use the recommended configuration for the DCGAN, which trains with the Adam Optimizer [19] (\u03b21 = 0.5, \u03b22 = 0.999, = 10 \u2212 8), with a learning rate of 0.0002 and a mini batch size of 64 (random samples taken at each update step) over 25 epochs. The output image size used as the initial image size is 64 \u00d7 64. In addition, we train the cGAN with the matching-aware discriminator method by Reed et al. [16]. In Figure 3 we show an overview architecture of both the generator and the discriminator for the cGAN. For a more detailed description of the model see Table 1.Encoder For simplicity, we show the architecture of the IND encoder (Table 2), as they are the configuration that provides the best performance for the cGAN. For a more detailed description of the model, see Table 1.Encoder For simplicity, we show the architecture of the IND encoder (Table 2), since they are the configuration that replaces the same layer after the last batch and non-activation layer, i.e. the same non-activation layer after the last one, non-activation layer is activated."}, {"heading": "5 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 Datasets", "text": "We use two image datasets of varying complexity and variation, MNIST [5] and CelebFaces Attributes (CelebA) [6]. MNIST is a digital dataset of grayscale images, consisting of 60,000 training images and 10,000 test images. Each sample is a 28 x 28 cm image labeled with the number class (0 to 9). CelebA is a dataset consisting of 202,599 color facial images and 40 binary vectors with attributes. We use the aligned and cropped version and scale the images to 64 x 64. We also use the official traction and test partitions, 182K for training and 20K for testing. Of the original 40 attributes, we filter those that do not have a clear visual impact on the generated images, leaving a total of 18 attributes. We evaluate the quality of the samples produced from both datasets. However, a quantitative evaluation is only performed on CelebA, which is more complex than NIST."}, {"heading": "5.2 Evaluating the conditional GAN", "text": "The objectives of this experiment are two. First, we evaluate the overall performance of the cGAN using an attribute predictor network (Anet) on the CelebA dataset. Second, we test the effects of adding y at different levels of the cGAN (Section 4.2, Conditional Position). We use an Anet2 as a way to perform a quantitative assessment in a manner similar to Salimane et al. Conceptual Model [9], since the performance given by this Anet (i.e., what attributes are recognized on a generated sample) is a good indicator of the generator's ability to model it. In other words, if the predicted Anet attributes y \u2032 are used closer to the original attributes y to generate an image x \u2032, we expect the generator to successfully learn the ability to generate new images taking into account the semantic meaning of the attributes."}, {"heading": "5.3 Evaluating the encoder", "text": "In this experiment, we prioritize the visual quality of reconstructed samples as an evaluation criterion. A comparison of these different configurations is shown in Figure 4a and in Figure 4b, we focus on reconstructed IND samples. On another level, the fact that the generator is able to reconstruct invisible images from the test set is generalizing and indicates that it is not overly adapted, i.e. it is not only able to memorize and reproduce training patterns. 2The architecture of Anet is the same as Ey from Table 2.In addition, we compare the various encoder configurations in a quantitative way using the minimum square reconstruction losses Le as a criterion."}, {"heading": "5.4 Evaluating the IcGAN", "text": "To test whether the model is capable of correctly encoding and regenerating a real image by retaining its most important attributes, we take real samples of MNIST and CelebA test sets and reconstruct them using modifications of conditional information y. The result of this process is shown in Figure 5, where we show a subset of 9 of the 18 for CelebA attributes for image clarity. We can see that in MNIST we are able to obtain the handwritten style of real invisible digits and replicate this style on all other digits. On the other hand, in CelebA we can see how reconstructed faces generally match the specified attribute. In addition, we noticed that faces with unusual conditions (e.g. by looking away from the camera, the face is not centered) are most likely to be loud. In addition, attributes such as moustache, especially for women, are limited to indicating that the generator is some unusual."}, {"heading": "6 Conclusions", "text": "We are introducing an encoder in a conditional environment within the GAN framework, a model we call Invertible Conditional GANs (IcGANs). It solves the problem that GANs are unable to infer real samples from a latent representation z, while at the same time allowing to explicitly control complex attributes of generated samples with conditional information y. We are also refining the performance of cGANS by testing the optimal position in which the conditional information y is inserted into the model. We have found that for the generator y should be added at the input level, while the discriminator works best when y is on the first layer. In addition, we are evaluating several ways to train an encoder."}], "references": [{"title": "DRAW: A Recurrent Neural Network For Image Generation", "author": ["K. Gregor", "I. Danihelka", "A. Graves", "D. Jimenez Rezende", "D. Wierstra"], "venue": "International Conference on Machine Learning (ICML), pp. 1462\u20131471, 2015.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2015}, {"title": "Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks", "author": ["A. Radford", "L. Metz", "S. Chintala"], "venue": "International Conference on Learning Representations (ICLR), 2016.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2016}, {"title": "Generative adversarial nets", "author": ["I. Goodfellow", "J. Pouget-Abadie", "M. Mirza", "B. Xu", "D. Warde-Farley", "S. Ozair", "A. Courville", "Y. Bengio"], "venue": "Advances in Neural Information Processing Systems 27, Z. Ghahramani, M. Welling, C. Cortes, N. D. Lawrence, and K. Q. Weinberger, Eds. Curran Associates, Inc., 2014, pp. 2672\u20132680. [Online]. Available: http://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2014}, {"title": "Auto-encoding variational bayes", "author": ["D.P. Kingma", "M. Welling"], "venue": "International Conference on Learning Representations (ICLR), 2014. [Online]. Available: http://arxiv.org/abs/1312.6114", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2014}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. Lecun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE, 1998, pp. 2278\u20132324.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1998}, {"title": "Deep learning face attributes in the wild", "author": ["Z. Liu", "P. Luo", "X. Wang", "X. Tang"], "venue": "Proceedings of International Conference on Computer Vision (ICCV), December 2015.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2015}, {"title": "Stochastic backpropagation and approximate inference in deep generative models", "author": ["D.J. Rezende", "S. Mohamed", "D. Wierstra"], "venue": "Proceedings of the 31st International Conference on Machine Learning (ICML), vol. 32, pp. 1278\u20131286, 2014.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2014}, {"title": "Semi-supervised learning with deep generative models", "author": ["D.P. Kingma", "D.J. Rezende", "S. Mohamed", "M. Welling"], "venue": "Proceedings of Neural Information Processing Systems (NIPS), 2014. [Online]. Available: http://arxiv.org/abs/1406.5298", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2014}, {"title": "Improved techniques for training GANs", "author": ["T. Salimans", "I.J. Goodfellow", "W. Zaremba", "V. Cheung", "A. Radford", "X. Chen"], "venue": "Neural Information Processing Systems (NIPS), 2016. [Online]. Available: http://arxiv.org/abs/1606.03498", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2016}, {"title": "InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets", "author": ["X. Chen", "Y. Duan", "R. Houthooft", "J. Schulman", "I. Sutskever", "P. Abbeel"], "venue": "Neural Information Processing Systems (NIPS), 2016. [Online]. Available: http://arxiv.org/abs/1606.03657", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2016}, {"title": "Learning what and where to draw", "author": ["S. Reed", "Z. Akata", "S. Mohan", "S. Tenka", "B. Schiele", "H. Lee"], "venue": "Advances in Neural Information Processing Systems (NIPS), 2016.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2016}, {"title": "Adversarially Learned Inference", "author": ["V. Dumoulin", "I. Belghazi", "B. Poole", "A. Lamb", "M. Arjovsky", "O. Mastropietro", "A. Courville"], "venue": "Arxiv, 2016. [Online]. Available: http://arxiv.org/abs/1606.00704", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2016}, {"title": "Adversarial feature learning", "author": ["J. Donahue", "P. Kr\u00e4henb\u00fchl", "T. Darrell"], "venue": "CoRR, vol. abs/1605.09782, 2016. [Online]. Available: http://arxiv.org/abs/1605.09782", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2016}, {"title": "Adversarial autoencoders", "author": ["A. Makhzani", "J. Shlens", "N. Jaitly", "I. Goodfellow"], "venue": "International Conference on Learning Representations (ICLR), 2016. [Online]. Available: http://arxiv.org/abs/1511. 05644", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2016}, {"title": "Autoencoding beyond pixels using a learned similarity metric", "author": ["A.B.L. Larsen", "S.K. S\u00f8nderby", "O. Winther"], "venue": "Proceedings of the 33rd International Conference on Machine Learning (ICML), pp. 1558\u20131566, 2015. [Online]. Available: http://arxiv.org/abs/1512.09300", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2015}, {"title": "Generative adversarial text to image synthesis", "author": ["S.E. Reed", "Z. Akata", "X. Yan", "L. Logeswaran", "B. Schiele", "H. Lee"], "venue": "International Conference on Machine Learning (ICML), 2016. [Online]. Available: http://arxiv.org/abs/1605.05396", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2016}, {"title": "Conditional Generative Adversarial Nets", "author": ["M. Mirza", "S. Osindero"], "venue": "CoRR, vol. abs/1411.1784, 2014. [Online]. Available: http://arxiv.org/abs/1411.1784", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2014}, {"title": "Conditional generative adversarial nets for convolutional face generation", "author": ["J. Gauthier"], "venue": "Class project for Stanford CS231N: Convolutional Neural Networks for Visual Recognition, Winter semester 2014, 2014. [Online]. Available: http://cs231n.stanford.edu/reports/jgauthie_final_report.pdf", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2014}, {"title": "Adam: A Method for Stochastic Optimization", "author": ["D. Kingma", "J. Ba"], "venue": "International Conference for Learning Representations (ICLR), 2014. [Online]. Available: http://www.arxiv.org/pdf/1412.6980.pdf 9", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 0, "context": "Natural image generation has been a strong research topic for many years, but it has not been until 2015 that promising results have been achieved with deep learning techniques combined with generative modeling [1, 2].", "startOffset": 211, "endOffset": 217}, {"referenceID": 1, "context": "Natural image generation has been a strong research topic for many years, but it has not been until 2015 that promising results have been achieved with deep learning techniques combined with generative modeling [1, 2].", "startOffset": 211, "endOffset": 217}, {"referenceID": 2, "context": "Generative Adversarial Networks (GANs) [3] is one of the state-of-the-art approaches for image generation.", "startOffset": 39, "endOffset": 42}, {"referenceID": 3, "context": "Variational Autoencoders [4]), which focus on an image reconstruction loss.", "startOffset": 25, "endOffset": 28}, {"referenceID": 4, "context": "We apply this model to MNIST [5] and CelebA [6] datasets, which allows performing meaningful and realistic editing operations on them by arbitrarily changing the conditional information y.", "startOffset": 29, "endOffset": 32}, {"referenceID": 5, "context": "We apply this model to MNIST [5] and CelebA [6] datasets, which allows performing meaningful and realistic editing operations on them by arbitrarily changing the conditional information y.", "startOffset": 44, "endOffset": 47}, {"referenceID": 0, "context": "The first one is Variational Autoencoders (VAE) [1, 4, 7, 8], which impose a prior representation space z (e.", "startOffset": 48, "endOffset": 60}, {"referenceID": 3, "context": "The first one is Variational Autoencoders (VAE) [1, 4, 7, 8], which impose a prior representation space z (e.", "startOffset": 48, "endOffset": 60}, {"referenceID": 6, "context": "The first one is Variational Autoencoders (VAE) [1, 4, 7, 8], which impose a prior representation space z (e.", "startOffset": 48, "endOffset": 60}, {"referenceID": 7, "context": "The first one is Variational Autoencoders (VAE) [1, 4, 7, 8], which impose a prior representation space z (e.", "startOffset": 48, "endOffset": 60}, {"referenceID": 2, "context": "[3], GANs have been improved with a deeper architecture (DCGAN) by Radford et al.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "[2].", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "The latest advances introduced several techniques that improve the overall performance for training GANs [9] and an unsupervised approach to disentangle feature representations [10].", "startOffset": 105, "endOffset": 108}, {"referenceID": 9, "context": "The latest advances introduced several techniques that improve the overall performance for training GANs [9] and an unsupervised approach to disentangle feature representations [10].", "startOffset": 177, "endOffset": 181}, {"referenceID": 10, "context": "Additionally, the most advanced and recent work on cGANs trains a model to generate realistic images from text descriptions and landmarks [11].", "startOffset": 138, "endOffset": 142}, {"referenceID": 1, "context": "(DCGANs) [2], which we will add a conditional extension.", "startOffset": 9, "endOffset": 12}, {"referenceID": 11, "context": "[12] and Donahue et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[13] also proposed an encoder in GANs, but in a non-conditional and jointly trained setting.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[14] and Larsen et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[15] proposed a similar idea to this paper by combining a VAE and a GAN with promising results.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[16] implemented an encoder in a similar fashion to our approach.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "GAN framework can be extended with conditional GANs (cGANs) [17].", "startOffset": 60, "endOffset": 64}, {"referenceID": 11, "context": "Even though encoders have recently been introduced into the GAN framework [12, 13, 16], we are the first ones to include and leverage the conditional information y into the design of the encoding process.", "startOffset": 74, "endOffset": 86}, {"referenceID": 12, "context": "Even though encoders have recently been introduced into the GAN framework [12, 13, 16], we are the first ones to include and leverage the conditional information y into the design of the encoding process.", "startOffset": 74, "endOffset": 86}, {"referenceID": 15, "context": "Even though encoders have recently been introduced into the GAN framework [12, 13, 16], we are the first ones to include and leverage the conditional information y into the design of the encoding process.", "startOffset": 74, "endOffset": 86}, {"referenceID": 15, "context": "Our approach consists of training an encoder E once the cGAN has been trained, as similarly considered by Reed et al [16].", "startOffset": 117, "endOffset": 121}, {"referenceID": 11, "context": "[12] and Donahue et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[13] proposed different approaches on how to train an encoder in the GAN framework.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "Consequently, we implemented our aforementioned approach which performs nearly equally [13] to their strategy.", "startOffset": 87, "endOffset": 91}, {"referenceID": 15, "context": "In the generator, y \u223c pdata and z \u223c pz (where pz = N (0, 1)) are always concatenated in the filter dimension at the input level [16\u201318].", "startOffset": 128, "endOffset": 135}, {"referenceID": 16, "context": "In the generator, y \u223c pdata and z \u223c pz (where pz = N (0, 1)) are always concatenated in the filter dimension at the input level [16\u201318].", "startOffset": 128, "endOffset": 135}, {"referenceID": 17, "context": "In the generator, y \u223c pdata and z \u223c pz (where pz = N (0, 1)) are always concatenated in the filter dimension at the input level [16\u201318].", "startOffset": 128, "endOffset": 135}, {"referenceID": 15, "context": "As for the discriminator, different authors insert y in different parts of the model [16\u201318].", "startOffset": 85, "endOffset": 92}, {"referenceID": 16, "context": "As for the discriminator, different authors insert y in different parts of the model [16\u201318].", "startOffset": 85, "endOffset": 92}, {"referenceID": 17, "context": "As for the discriminator, different authors insert y in different parts of the model [16\u201318].", "startOffset": 85, "endOffset": 92}, {"referenceID": 15, "context": "\u2022 Direct interpolation: interpolate between label vectors y from the training set [16].", "startOffset": 82, "endOffset": 86}, {"referenceID": 17, "context": "As Gauthier [18] pointed out, unlike the previous two approaches, this method could overfit the model by using the conditional information to reproduce the images of the training set.", "startOffset": 12, "endOffset": 16}, {"referenceID": 1, "context": "Conditional GAN The work of this paper is based on the Torch implementation of the DCGAN1 [2].", "startOffset": 90, "endOffset": 93}, {"referenceID": 18, "context": "We use the recommended configuration for the DCGAN, which trains with the Adam optimizer [19] (\u03b21 = 0.", "startOffset": 89, "endOffset": 93}, {"referenceID": 15, "context": "[16].", "startOffset": 0, "endOffset": 4}, {"referenceID": 4, "context": "We use two image datasets of different complexity and variation, MNIST [5] and CelebFaces Attributes (CelebA) [6].", "startOffset": 71, "endOffset": 74}, {"referenceID": 5, "context": "We use two image datasets of different complexity and variation, MNIST [5] and CelebFaces Attributes (CelebA) [6].", "startOffset": 110, "endOffset": 113}, {"referenceID": 8, "context": "Inception model [9], as the output given by this Anet (i.", "startOffset": 16, "endOffset": 19}], "year": 2016, "abstractText": "Generative Adversarial Networks (GANs) have recently demonstrated to successfully approximate complex data distributions. A relevant extension of this model is conditional GANs (cGANs), where the introduction of external information allows to determine specific representations of the generated images. In this work, we evaluate encoders to inverse the mapping of a cGAN, i.e., mapping a real image into a latent space and a conditional representation. This allows, for example, to reconstruct and modify real images of faces conditioning on arbitrary attributes. Additionally, we evaluate the design of cGANs. The combination of an encoder with a cGAN, which we call Invertible cGAN (IcGAN), enables to re-generate real images with deterministic complex modifications.", "creator": "LaTeX with hyperref package"}}}