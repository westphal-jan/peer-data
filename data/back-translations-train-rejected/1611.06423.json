{"id": "1611.06423", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Nov-2016", "title": "Incorporating Pass-Phrase Dependent Background Models for Text-Dependent Speaker Verification", "abstract": "In this paper, we propose a pass-phrase dependent background model (PBM) for text dependent (TD) speaker verification (SV) to integrate pass-phrase identification process (without an additional separate identification system) in the conventional TD-SV system, where a PBM is derived from a text-independent background model through adaptation using the utterances of a particular pass-phrase. During training, pass-phrase specific target speaker models are derived from the particular PBM using the training data for the respective target model. While testing, the best PBM is first selected for the test utterance in the maximum likelihood (ML) sense and following the selected PBM is used for the log likelihood ratio (LLR) calculation with respect to the claimant model. The proposed method incorporates the pass-phrase identification step in the LLR calculation, which is not considered in conventional standalone TD-SV based systems. The performance of the proposed method is compared to conventional text-independent background model based TD-SV systems using a Gaussian mixture model (GMM)-universal background model (UBM), Hidden Markov model (HMM)-UBM and i-vector paradigms. In addition, we consider two approaches to build PBMs: one is speaker independent and the other is speaker dependent. We show that the proposed method significantly reduces the error rate of text dependent speaker verification for the non-target types: target-wrong and imposter-wrong while it maintains comparable TD-SV performance when imposters speak a correct utterance with respect to the conventional system. Experiments are conducted on the RedDots challenge and the RSR2015 databases which consist of short utterances.", "histories": [["v1", "Sat, 19 Nov 2016 20:12:19 GMT  (153kb)", "https://arxiv.org/abs/1611.06423v1", null], ["v2", "Mon, 12 Jun 2017 16:42:56 GMT  (154kb)", "http://arxiv.org/abs/1611.06423v2", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["a k sarkar", "zheng-hua tan"], "accepted": false, "id": "1611.06423"}, "pdf": {"name": "1611.06423.pdf", "metadata": {"source": "CRF", "title": "Incorporating Pass-Phrase Dependent Background Models for Text-Dependent Speaker Verification", "authors": ["A. K. Sarkar", "Zheng-Hua Tan"], "emails": ["akc@es.aau.dk,", "zt@es.aau.dk"], "sections": [{"heading": null, "text": "ar Xiv: 161 1.06 423v 2 [cs.C L] 12 Jun 2017In this thesis we propose passphrase-dependent background models (PBMs) for text-dependent (TD) speaker verification (SV) in order to integrate the passphrase identification process into the conventional TD-SV system, whereby a PBM is derived from a text-dependent background model by adaptation using the expressions of a certain passphrase. During the training, passphrase-specific target speaker models are derived from the respective PBM, whereby the training data for the respective target model is used. During the test, the best PBM for the test statement in the sense of maximum probability (ML) is first selected and the selected PBM is then used for the log likelihood ratio (LLR) calculation with respect to the target model of the applicant. First, the proposed method takes into account the passphrase-level of standification stage of the LSV, which is not considered in the conventional LR-LiD verification systems."}, {"heading": "1. Introduction", "text": "In fact, most people are able to decide for themselves what they want and what they want."}, {"heading": "2. Text-dependent speaker verification methods", "text": "In this section we briefly describe the techniques GMM-UBM, HMMUBM and i-vector for text-dependent loudspeaker verification, which are used to compare the performance of the proposed methods with the existing ones."}, {"heading": "2.1. Gaussian mixture model- universal background model based method", "text": "In this method, a large-scale Gaussian mixing model called GMMUBM [2] is constructed using data with different text content from non-target speakers. It represents a large acoustic space covering all possible attributes available in the training data. Subsequently, password-specific target speaker models are derived from the text-independent GMM-UBM with a maximum a posteriori (MAP) fit, using the training data for the respective target model. In the test phase, a test statement X = {x1, x2,..., xL} is used against the applicant and GMM-UBM \u03bbUBM for the calculation of the log probability ratio, namely in the case that p (xt | qr) sets the probability value for a certain feature vector xt = 1 {log p (xt | qr) \u2212 log p (xt | qr) \u2212 log p (xt | qUBM)} (1 feature), with a preference light (Uxt | qM) for a feature Bxt in relation to the model."}, {"heading": "2.2. Hidden Markov model - universal background model based method", "text": "In this method [16] an HMM called \"HMM-UBM\" is constructed with data from many non-target speakers without any speech transcription. A dummyword (like \"Hi\") is used as a label (without breaking phonetic level) for all training language data during the HMM training. HMM-UBM is trained by merging all training data and by iteratively updating its parameters using the tree-welch estimation algorithm. However, since no transcriptions are taken into account during the HMM training, the state transition probabilities of the HMM-UBM [19] will naturally reflect the speaker-independent temporal information within the data. However, this temporal information is not taken into account in the conventional GMM-UBM-based TD-SV systems."}, {"heading": "2.3. i-vector based method", "text": "In this method, a speech expression is represented by a vector in a low-dimensional subspace (GMM-UBM) (GMM-UBM supervector), which is called a total variability space, in which loudspeaker and channel information are assumed to be denser in each case. It is generally expressed as, M = m + Tw (2), where an i-vector is designated. The following steps are involved in the i-vector estimation for a given speech signal X = {x1, x2, xL} using the GMM-UBM components and the entire variability space. The following steps are involved in the i-vector estimation for a given speech signal."}, {"heading": "3. Proposed methods", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1. Gaussian mixture model- universal background model based PBM TD-SV system", "text": "In this case, specific background models are derived from the text-independent GMM-UBM = text-dependent GMM-UBM with MAP adaptation by pooling the data of the respective passphrase of many speakers. With regard to the real scenarios, we consider two approaches: one is called speaker-independent and the other is speaker-dependent. \u2022 Speaker-independent PBM (SI-PBM): PBM does not use data / information from a specific target speaker, we call it speaker-independent PBM (SD-PBM): In this case, there is a global series of PBMs common to all target speakers. Since this PBM approach does not use data / information from a specific target speaker, we call it speaker-independent. \u2022 Speaker-dependent PBM (SD-PBM): In this case, each target speaker model creates specific PBBM by pooling the passphrase specific training data from both target speakers and non-target speakers."}, {"heading": "3.2. HiddenMarkov model- universal backgroundmodel based PBM TD-SV system", "text": "This system is similar to the GMM-UBM-based PBM TD-SV system. The main difference is that HMM-UBM instead of GMM-UBM.Since we consider both speaker independent and dependent training strategies for building PBMs, we get two subsystems called HMM-UBM Speaker Independent PBM (HMM-UBM-SI-PBM) and HMM-UBM Speaker Independent PBM (HMM-UBM-SD-PBM) TD-SV systems."}, {"heading": "3.3. i-vector based PBM TD-SV system", "text": "We also examine the GMM-UBM PBM based on the TD-SV system in the PBM models. In the enrolment phase, the statistics relating to the respective PBM are estimated based on the target model of the specific passphrase for all training speech files that belong individually to the respective target model. Then, the statistics are projected over the entire variability space to obtain an i-vector for each speech file. Afterwards, the target is represented by an average i-vector, which is calculated via the speech file-wise i-vector. Before the first-order statistics are projected onto the T-space, they are centralized in relation to the text-independent GMM-UBM. Both the proposed method and the conventional system use the same T-space. Algorithm 3 describes the enrolment phase of the target speakers in the PBM-based i-vector."}, {"heading": "4. Experimental setup", "text": "This year, the time has come for such a process to take place in the first half of the year, in which such a process takes place."}, {"heading": "5. Results and Discussion", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1. Systems based on Gaussian mixture model", "text": "Do you see how the situation has developed in recent years? Do you see how the situation in Turkey has developed? Do you see how the situation in Turkey has developed? Do you see how the situation has developed in recent years? Do you see how the situation in Turkey has developed? Do you see how the situation in Turkey has developed? Do you see how the situation in Turkey has developed? Do you see how the situation in Turkey has developed?"}, {"heading": "5.2. Systems based on Hidden Markov model", "text": "Tables 4a and 4b compare the performance of the proposed PBM method with the base system according to the HMM paradigm for different mixtures per state in HMM-UBMs. Similar to the GMM-UBM-based PBM systems, we can see from Tables 4a-4b that the PBM system has lower error rates for non-target types than the base data: target and fraud rates in both databases. If the RedDots database is incorrect, SI-PBM systems show better or comparable performance over different mixtures in the HMM-UBM state compared to the baseline. However, this could be due to the same reason as in Section 5.1 (for the number of Gausian components per state in the HMM-UBM target system, which does not show a target value of BBM-BM in full scope compared to the SI-PBM targets and base systems)."}, {"heading": "5.3. Systems based on i-vector", "text": "Tables 5a and 5b show the performance of the proposed PBM methods, which are integrated into an i-vector paradigm, and that of an i-vector baseline for verification of text-dependent loudspeakers. Tables 5a and 5b show that the proposed PBM methods on the i-vector framework have significantly lower error rates for target-false and scammer-false and comparable error rates for scammers compared to the baseline. This again demonstrates the usefulness of the proposed PBM methods for incorporating the password phrase identification process. If we now compare the PBM-based system on the i-vector paradigm with GMM and HMM, it can be observed from Tables 2, 4 and 5 that the PBM system is based on GMM and HMM, the error rate for target-false and fraud-based data banks could significantly decrease the duration of short phrases in nature compared to the Rases in 2015."}, {"heading": "6. Conclusion and future work", "text": "In this paper, we proposed password-dependent background models (PBMs) for text-dependent speaker verification (TD-SV) to integrate the password-finding process into conventional TD-SV systems by deriving PBMs from the text-independent background model and adjusting the data for a specific password phrase via the speakers. During the training, the appropriate PBM, using its training data from the respective password phrase, selects the best PBM for the respective test word statement in the highest possible probability sense and uses it for a log-likelihood score calculation with respect to the applicant. The effectiveness of the proposed techniques was compared with the conventional text-dependent verification systems for speakers under GMM-UBM and HMM-UBM. Furthermore, we showed that the proposed concept can be incorporated into an i-vector-based TD-SV system to verify the error rates we demonstrated during the recent Reddit and Photonic Rots experiments."}, {"heading": "Acknowledgments", "text": "This work is partially supported by the iSocioBot project, which is funded by the Danish Council for Independent Research - Technology and Production Sciences (# 1335-00162) and the OCTAVE project (# 647850), which is funded by the European Research Agency (REA) of the European Commission under its Horizon 2020 Framework Programme. The views expressed in this article are those of the authors and do not include an official position of the European Commission."}], "references": [{"title": "Speaker Verification using Adapted Gaussian Mixture Models", "author": ["D.A. Reynolds", "T.F. Quatieri", "R.B. Dunn"], "venue": "Digital Signal Processing", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2000}, {"title": "A Novel Scheme for Speaker Recognition using a Phonetically- Aware Deep Neural Network", "author": ["Y. Lei", "N. Scheffer", "L. Ferrer", "M. McLaren"], "venue": "in: Proc. of IEEE Int. Conf. Acoust. Speech Signal Processing (ICASSP),", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2014}, {"title": "Content Matching for Short Duration Speaker Recognition", "author": ["N. Scheffer", "Y. Lei"], "venue": "in: Proc. of Interspeech,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2014}, {"title": "Deep Feature for Textdependent Speaker Verification", "author": ["Y. Liu", "Y. Qian", "N. Chen", "T. Fu", "Y. Zhang", "K. Yu"], "venue": "Speech Communication", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2015}, {"title": "Front-End Factor Analysis for Speaker Verification", "author": ["N. Dehak", "P. Kenny", "R. Dehak", "P. Ouellet", "P. Dumouchel"], "venue": "IEEE Trans. on Audio, Speech and Language Processing", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2011}, {"title": "RSR2015: Database for Textdependent Speaker Verification using Multiple Pass-phrases", "author": ["A. Larcher", "K.A. Lee", "B. Ma", "H. Li"], "venue": "in: Proc. of Interspeech,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2012}, {"title": "Speaker Verification Based on Broad Phonetic Categories", "author": ["S. Kajarekar", "H. Hermansky"], "venue": "in: Proc. of Odyssey Speaker and Language Recognition Workshop,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2001}, {"title": "Improving a GMM Speaker Verification System by Phonetic Weighting", "author": ["R. Auckenthaler", "E. Parris", "M. Carey"], "venue": "in: Proc. of IEEE Int. Conf. Acoust. Speech Signal Processing (ICASSP),", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1999}, {"title": "Domain Adaptation for Text Dependent Speaker Verification", "author": ["H. Aronowitz", "A. Rendel"], "venue": "in: Proc. of Interspeech,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2014}, {"title": "Multiple Background Models for Speaker Verification using the Concept of Vocal Tract Length and MLLR Super-vector", "author": ["A. Sarkar", "S. Umesh"], "venue": "International Journal of Speech Technology", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2012}, {"title": "Utterance Verification for Text-dependent Speaker Recognition: A Comparative Assessment using the RedDots Corpus", "author": ["T. Kinnunen", "M. Sahidullah", "I. Kukanov", "H. Delgado", "M. Todisco", "A. Sarkar", "N.B. Thomsen", "V. Hautamaki", "N. Evans", "Z.-H. Tan"], "venue": "in: in Proc. of Interspeech,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2016}, {"title": "Text Dependent Speaker Verification using Unsuper-vised HMM-UBM and Temporal GMM-UBM", "author": ["A.K. Sarkar", "Z.-H. Tan"], "venue": "in: in Proc. of Interspeech,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2016}, {"title": "Text-dependent Speaker Verification: Classifiers, Databases and RSR2015", "author": ["A. Larcher", "K.A. Lee", "B. Ma", "H. Li"], "venue": "Speech Communication", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2014}, {"title": "A Tutorial on Hidden Markov Models and Selected Applications in Speech Recognition", "author": ["L.R. Rabiner"], "venue": "Proc. of the IEEE", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1989}, {"title": "Maximum a Posteriori Estimation for Multivariate Gaussian Mixture Observations of Markov Chains", "author": ["J.-L. Gauvain", "C.-H. Lee"], "venue": "IEEE Trans. on Speech and Audio Processing", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1994}, {"title": "Variance-Spectra Based Normalization for i-vector Standard and Probabilistic Linear Discriminant Anal ysis", "author": ["P.M. Bousquet"], "venue": "in: Proc. of Odyssey Speaker and Language Recognition Workshop", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2012}, {"title": "Computer Vision: Models Learning and Inference", "author": ["S.J. Prince"], "venue": null, "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2012}, {"title": "Mixture of PLDA Models in I-Vector Space for Gender-Independent Speaker Recognition", "author": ["M. Senoussaoui"], "venue": "in: Proc. of Interspeech,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2011}, {"title": "Low-complexity Variable Frame Rate Analysis for Speech Recognition and Voice Activity Detection", "author": ["Z.-H. Tan", "B. Lindbeg"], "venue": "IEEE Journal of Selected Topics in Signal Processing", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2010}, {"title": "A Free Signal Processing and Machine Learning Toolbox for Researchers, in: 20th ACMConference onMultimedia Systems", "author": ["A. Anjos", "L.E. Shafey", "R. Wallace", "M. G\u00fcnther", "C. McCool", "S. Marcel", "Bob"], "venue": null, "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2012}, {"title": "Comparison of Parametric Representations for Monosyllabic Word Recognition in Continuously Spoken Sentences", "author": ["S.B. Davis", "P. Mermelstein"], "venue": "IEEE Trans. Acoust. Speech Signal Processing", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 1980}, {"title": "The Det Curve in Assessment of Detection Task Performance", "author": ["A. Martin", "G. Doddington", "T. Kamm", "M. Ordowskiand", "M. Przybocki"], "venue": "in: Proc. of Eur. Conf. Speech Commun. and Tech. (Eurospeech),", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 1997}], "referenceMentions": [{"referenceID": 0, "context": "Speaker verification (SV) [1, 2] is the process of authentication of a person\u2019s claimed identity by analyzing his/her speech signal.", "startOffset": 26, "endOffset": 32}, {"referenceID": 1, "context": "Examples are deep neural network (DNN) [3, 4, 5], i-vector [4, 6], hierarchical multi-Layer acoustic model (HiLAM) [2, 7], phonedependent Hidden Markov model (HMM) [8, 9], domain adaptation [10] and phonetic higher level maximum likelihood linear regression (MLLR) super-vector based features [11].", "startOffset": 39, "endOffset": 48}, {"referenceID": 2, "context": "Examples are deep neural network (DNN) [3, 4, 5], i-vector [4, 6], hierarchical multi-Layer acoustic model (HiLAM) [2, 7], phonedependent Hidden Markov model (HMM) [8, 9], domain adaptation [10] and phonetic higher level maximum likelihood linear regression (MLLR) super-vector based features [11].", "startOffset": 39, "endOffset": 48}, {"referenceID": 3, "context": "Examples are deep neural network (DNN) [3, 4, 5], i-vector [4, 6], hierarchical multi-Layer acoustic model (HiLAM) [2, 7], phonedependent Hidden Markov model (HMM) [8, 9], domain adaptation [10] and phonetic higher level maximum likelihood linear regression (MLLR) super-vector based features [11].", "startOffset": 39, "endOffset": 48}, {"referenceID": 2, "context": "Examples are deep neural network (DNN) [3, 4, 5], i-vector [4, 6], hierarchical multi-Layer acoustic model (HiLAM) [2, 7], phonedependent Hidden Markov model (HMM) [8, 9], domain adaptation [10] and phonetic higher level maximum likelihood linear regression (MLLR) super-vector based features [11].", "startOffset": 59, "endOffset": 65}, {"referenceID": 4, "context": "Examples are deep neural network (DNN) [3, 4, 5], i-vector [4, 6], hierarchical multi-Layer acoustic model (HiLAM) [2, 7], phonedependent Hidden Markov model (HMM) [8, 9], domain adaptation [10] and phonetic higher level maximum likelihood linear regression (MLLR) super-vector based features [11].", "startOffset": 59, "endOffset": 65}, {"referenceID": 0, "context": "Examples are deep neural network (DNN) [3, 4, 5], i-vector [4, 6], hierarchical multi-Layer acoustic model (HiLAM) [2, 7], phonedependent Hidden Markov model (HMM) [8, 9], domain adaptation [10] and phonetic higher level maximum likelihood linear regression (MLLR) super-vector based features [11].", "startOffset": 115, "endOffset": 121}, {"referenceID": 5, "context": "Examples are deep neural network (DNN) [3, 4, 5], i-vector [4, 6], hierarchical multi-Layer acoustic model (HiLAM) [2, 7], phonedependent Hidden Markov model (HMM) [8, 9], domain adaptation [10] and phonetic higher level maximum likelihood linear regression (MLLR) super-vector based features [11].", "startOffset": 115, "endOffset": 121}, {"referenceID": 6, "context": "Examples are deep neural network (DNN) [3, 4, 5], i-vector [4, 6], hierarchical multi-Layer acoustic model (HiLAM) [2, 7], phonedependent Hidden Markov model (HMM) [8, 9], domain adaptation [10] and phonetic higher level maximum likelihood linear regression (MLLR) super-vector based features [11].", "startOffset": 164, "endOffset": 170}, {"referenceID": 7, "context": "Examples are deep neural network (DNN) [3, 4, 5], i-vector [4, 6], hierarchical multi-Layer acoustic model (HiLAM) [2, 7], phonedependent Hidden Markov model (HMM) [8, 9], domain adaptation [10] and phonetic higher level maximum likelihood linear regression (MLLR) super-vector based features [11].", "startOffset": 164, "endOffset": 170}, {"referenceID": 8, "context": "Examples are deep neural network (DNN) [3, 4, 5], i-vector [4, 6], hierarchical multi-Layer acoustic model (HiLAM) [2, 7], phonedependent Hidden Markov model (HMM) [8, 9], domain adaptation [10] and phonetic higher level maximum likelihood linear regression (MLLR) super-vector based features [11].", "startOffset": 190, "endOffset": 194}, {"referenceID": 1, "context": "In [3, 4], phonetic information is incorporated into an i-vector system by accumulating statistics from speech with respect to a pre-defined phonetic class through an DNN based automatic speech recognition (ASR) system.", "startOffset": 3, "endOffset": 9}, {"referenceID": 2, "context": "In [3, 4], phonetic information is incorporated into an i-vector system by accumulating statistics from speech with respect to a pre-defined phonetic class through an DNN based automatic speech recognition (ASR) system.", "startOffset": 3, "endOffset": 9}, {"referenceID": 3, "context": "In [5], the intermediate output of the DNN layers are used to vectorize characterization of speech data.", "startOffset": 3, "endOffset": 6}, {"referenceID": 0, "context": "HiLAM builds a HMM model by concatenating the speech segment-wise adapted models from the Gaussian mixture model- universal background model (GMMUBM) [2].", "startOffset": 150, "endOffset": 153}, {"referenceID": 8, "context": "In domain adaptation [10], the mismatch between the text-independent and the text-dependent data is reduced by transforming the text-independent data to better match the textdependent task (using the a-priori transcription knowledge of the text-dependent data).", "startOffset": 21, "endOffset": 25}, {"referenceID": 6, "context": "In conventional HMM based TDSV systems [8, 9], phoneme (context) dependent speaker models are built using the knowledge of speech transcriptions.", "startOffset": 39, "endOffset": 45}, {"referenceID": 7, "context": "In conventional HMM based TDSV systems [8, 9], phoneme (context) dependent speaker models are built using the knowledge of speech transcriptions.", "startOffset": 39, "endOffset": 45}, {"referenceID": 9, "context": "In [12, 13, 14], a multiple background model concept is proposed to improve the performance of the conventional speaker verification system, by training background models (BMs) based on the vocal tract length (VTL) characteristic of target speakers as in [14] and passphrases as in [12, 13] for text-independent and text-dependent speaker verification, respectively.", "startOffset": 3, "endOffset": 15}, {"referenceID": 9, "context": "In [12, 13, 14], a multiple background model concept is proposed to improve the performance of the conventional speaker verification system, by training background models (BMs) based on the vocal tract length (VTL) characteristic of target speakers as in [14] and passphrases as in [12, 13] for text-independent and text-dependent speaker verification, respectively.", "startOffset": 255, "endOffset": 259}, {"referenceID": 9, "context": "During enrollment, target speaker models are derived from the BM based on VTL in [14] and pass-phrase of target data in [12, 13]).", "startOffset": 81, "endOffset": 85}, {"referenceID": 9, "context": "However, this [12, 13, 14] does not incorporate the passphrase identification process to address the following two nontarget types: target-wrong and imposter-wrong.", "startOffset": 14, "endOffset": 26}, {"referenceID": 10, "context": "Recently, in [15], the authors proposed a fusion system which combines the score/decision of an utterance verification system with a conventional SV system to improve the performance of the TD-SV system against target/imposter-wrong non-target trials.", "startOffset": 13, "endOffset": 17}, {"referenceID": 9, "context": "In the test phase, the best PBM is selected for a particular test utterance in the maximum likelihood (ML) sense and the best selected PBM is used as an alternative hypothesis for the loglikelihood ratio calculation with respect to the claimant model, which differs from [12, 13, 14].", "startOffset": 271, "endOffset": 283}, {"referenceID": 9, "context": "Furthermore two strategies are considered for building the PBM: one is called speakerindependent (SI) and the other is speaker-dependent (SD) again in contrast to [12, 13, 14] where only SI-PBM concept is considered.", "startOffset": 163, "endOffset": 175}, {"referenceID": 9, "context": "The main salient feature of the proposed method is that it incorporates the utterance identification in the LLR calculation process in the conventional approach in contrast to [12, 13, 14].", "startOffset": 176, "endOffset": 188}, {"referenceID": 11, "context": "We study the performance of the proposed PBM based TDSV system under the GMM and recently proposed HMM modeling [16] and i-vector paradigms.", "startOffset": 112, "endOffset": 116}, {"referenceID": 12, "context": "Experiments are conducted on the RedDots Challenge [17] and the RSR2015 [18] databases.", "startOffset": 72, "endOffset": 76}, {"referenceID": 0, "context": "In this method, a largeGaussian mixturemodel called GMMUBM [2] is built using data with different textual contents from non-target speakers.", "startOffset": 59, "endOffset": 62}, {"referenceID": 11, "context": "In this method [16], an HMM called \u2018HMM-UBM\u2019 is built using data from many non-target speakers without any speech", "startOffset": 15, "endOffset": 19}, {"referenceID": 13, "context": "Since no transcriptions are considered during HMM training, state transition probabilities of the HMM-UBM [19] will inherently reflect the speaker-independent temporal information available within the data.", "startOffset": 106, "endOffset": 110}, {"referenceID": 14, "context": "Similarly to the GMM-UBM TD-SV system, pass-phrasespecific target speaker models are derived from the textindependent HMM-UBM with MAP adaptation [20] using the training data for the particular target model.", "startOffset": 146, "endOffset": 150}, {"referenceID": 5, "context": "It should be noted that the HMM-UBM approach is different from the HiLAM proposed in [7] in two ways: First, HMMUBM is trained by pooling all training data together using the Baum-Welch re-estimation algorithm, and then target speaker models are derived from the HMM-UBM with MAP using the training data of particular target speaker model.", "startOffset": 85, "endOffset": 88}, {"referenceID": 4, "context": "For more details about the i-vector see [6].", "startOffset": 40, "endOffset": 43}, {"referenceID": 15, "context": "For more details about the PLDA based scoring see [22, 23, 24].", "startOffset": 50, "endOffset": 62}, {"referenceID": 16, "context": "For more details about the PLDA based scoring see [22, 23, 24].", "startOffset": 50, "endOffset": 62}, {"referenceID": 17, "context": "For more details about the PLDA based scoring see [22, 23, 24].", "startOffset": 50, "endOffset": 62}, {"referenceID": 15, "context": "Prior to PLDA, i-vectors are post-processed for session variability compensation using an iterative conditioning algorithm called spherical normalization (Sph) proposed in [22].", "startOffset": 172, "endOffset": 176}, {"referenceID": 15, "context": "It has been shown in [22] that Sph improves the speaker verification performance of PLDA based systems when compared to other conventional approaches.", "startOffset": 21, "endOffset": 25}, {"referenceID": 12, "context": "3sess-pwd eval m task) as per protocols in [17] and [18], respectively.", "startOffset": 52, "endOffset": 56}, {"referenceID": 10, "context": "In case of the RedDots database, a disjoint set of nine speakers\u2019 data (excluded from the evaluation) are used for SI-PBMs (approximately 148 files per pass-phrase) and the remaining speakers are considered for the evaluation [15].", "startOffset": 226, "endOffset": 230}, {"referenceID": 12, "context": "For SV systems on RedDots, GMM-UBM and HMM-UBM are trained using data of various textual contents from the RSR2015 database [18] over 157 male non-target speakers (approximately 42000 utterances).", "startOffset": 124, "endOffset": 128}, {"referenceID": 11, "context": "In case of HMM-UBM, 14 states and different numbers of Gaussians per state are considered as per [16] and inspired from [25].", "startOffset": 97, "endOffset": 101}, {"referenceID": 18, "context": "In case of HMM-UBM, 14 states and different numbers of Gaussians per state are considered as per [16] and inspired from [25].", "startOffset": 120, "endOffset": 124}, {"referenceID": 19, "context": "The BOB toolkit [28] is used for implementing the i-vector system.", "startOffset": 16, "endOffset": 20}, {"referenceID": 20, "context": "For cepstral analysis, 57 dimensional MFCC [29] (19 static+\u2206+\u2206\u2206) feature vectors are extracted from speech signals using 20ms hamming window with 10ms overlap of adjacent frames.", "startOffset": 43, "endOffset": 47}, {"referenceID": 21, "context": "System performances are evaluated in terms of equal error rate (EER) and minimum detection cost function (MinDCF) [31].", "startOffset": 114, "endOffset": 118}], "year": 2017, "abstractText": "In this paper, we propose pass-phrase dependent background models (PBMs) for text-dependent (TD) speaker verification (SV) to integrate the pass-phrase identification process into the conventional TD-SV system, where a PBM is derived from a textindependent background model through adaptation using the utterances of a particular pass-phrase. During training, pass-phrase specific target speaker models are derived from the particular PBM using the training data for the respective target model. While testing, the best PBM is first selected for the test utterance in the maximum likelihood (ML) sense and the selected PBM is then used for the log likelihood ratio (LLR) calculation with respect to the claimant model. The proposed method incorporates the pass-phrase identification step in the LLR calculation, which is not considered in conventional standalone TD-SV systems. The performance of the proposed method is compared to conventional text-independent backgroundmodel based TD-SV systems using either Gaussian mixture model (GMM)-universal background model (UBM) or Hidden Markov model (HMM)-UBM or i-vector paradigms. In addition, we consider two approaches to build PBMs: speaker-independent and speaker-dependent. We show that the proposed method significantly reduces the error rates of text-dependent speaker verification for the non-target types: target-wrong and imposter-wrong while it maintains comparable TD-SV performance when imposters speak a correct utterance with respect to the conventional system. Experiments are conducted on the RedDots challenge and the RSR2015 databases that consist of short utterances.", "creator": "LaTeX with hyperref package"}}}