{"id": "1611.06478", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Nov-2016", "title": "Visualizing Linguistic Shift", "abstract": "Neural network based models are a very powerful tool for creating word embeddings, the objective of these models is to group similar words together. These embeddings have been used as features to improve results in various applications such as document classification, named entity recognition, etc. Neural language models are able to learn word representations which have been used to capture semantic shifts across time and geography. The objective of this paper is to first identify and then visualize how words change meaning in different text corpus. We will train a neural language model on texts from a diverse set of disciplines philosophy, religion, fiction etc. Each text will alter the embeddings of the words to represent the meaning of the word inside that text. We will present a computational technique to detect words that exhibit significant linguistic shift in meaning and usage. We then use enhanced scatterplots and storyline visualization to visualize the linguistic shift.", "histories": [["v1", "Sun, 20 Nov 2016 06:50:16 GMT  (291kb)", "http://arxiv.org/abs/1611.06478v1", null]], "reviews": [], "SUBJECTS": "cs.CL cs.HC", "authors": ["salman mahmood", "rami al-rfou", "klaus mueller"], "accepted": false, "id": "1611.06478"}, "pdf": {"name": "1611.06478.pdf", "metadata": {"source": "CRF", "title": "Visualizing Linguistic Shift", "authors": ["Salman Mahmood", "Rami Al-Rfou", "Klaus Mueller"], "emails": ["@cs.stonybrook.edu"], "sections": [{"heading": null, "text": "Key words: linguistic change, neural language models, word embedding, storyline visualization."}, {"heading": "1 INTRODUCTION", "text": "The Internet is a collection of materials produced by people from different eras, geography, culture and disciplines. Therefore, the language on the Internet shows a rich variety of words and meanings. Words can acquire new meanings that could be relevant only to a niche group of people. As the Internet becomes more and more dialogical, it is necessary to recognize these linguistic changes. Words can have more than one meaning; however, the context of the word can be used to determine the intended meaning of the word. The word \"fair\" has several meanings. But in the sentence \"It is not fair that I have to do all the work,\" the word fair means reasonable or simple, while in the sentence \"you have fair skin that you are likely to be sunburned,\" the meaning of the word is fair light or pale skin. In these sentences it is clear that the context in which the word is used to determine the meaning of the word."}, {"heading": "2 RELATED WORK", "text": "Since our work lies at the interface of different fields, we will divide this section into four parts: (1) language switching, (2) language switching visualization, (3) word embedding, (4) storyline visualization."}, {"heading": "2.1 Linguistic Shift", "text": "Bamman et al. [1] use Twitter data to investigate language changes in different genders, Bamman et al. [2, 11] also investigate how geography and other contextual factors influence the meaning of words by using the vector space representation of words. Einstein et al. [10] propose a model that identifies geographic and demographic factors that propagate new words in online texts. [8,12] present a model for measuring language change over time. In [18] Muhammad et al present detailed studies on distribution measures."}, {"heading": "2.2 Linguistic Shift Visualization", "text": "Recently, the use of visual and dynamic applications in corporate and literary studies has increased. Hilpert [6] uses dynamic motion diagrams to understand the arrangement of relative words, the sequence of graphs provides the researcher with an understanding of complicated linguistic development. Motion diagrams are also used by others in this field [4, 5, 24]. Siirtola et al. [24] developed a tool, the Text Variation Explorer (TVE), which allows interactive investigations of the behavior of linguistic parameters influenced by the size and intersection of text windows, and performs main interactive component analyses based on a set of words specified by the user. Kulkarni et al. [12] use dimension reduction on their word vector and visualize it using a scatter diagram."}, {"heading": "2.3 Word Embeddings", "text": "The mapping of concepts to continuous space can be traced back to Hinton [Hinton], who suggested that concepts can be represented by distributed activity patterns in networks on neuron-like units. Bengio et al. [3] presented a neural language model for learning word embeddings, the model of which exceeded traditional n-gram-based techniques. Various methods were proposed that scale and accelerate large neural models [19, 23]. Mikolov et al. [15] proposed skip-gram models for learning word embeddings and showed that these models are capable of learning linguistic patterns as linear relationships between vectors [16, 17]. These embeddings were used in various tasks for processing natural language such as the so-called entity recognition (NER). Tsuboi et al. [27] integrated word2vec embeddings with GloVe embeddings to provide a comparison of tag systems and showed better results in the individual systems."}, {"heading": "2.4 Storyline Visualization", "text": "The visualization of storylines was inspired by the xkcd comic \"Movie Narrative Charts.\" [20] In a visualization of storylines, we visualize a collection of units using line diagrams in which lines converge when there is interaction between units and otherwise diverge. Storyline visualizations have been used in visualization tools for a variety of datasets. Vehlow et al. [28] use this technique to visualize dynamic community structures by representing communities as convergent units. Kim et al. [10] visualize genealogical data in which individuals are represented by lines that converge and diverge to indicate marriage and divorce. Cui et al. [4] use streams of text to visualize the evolution of themes, themes merge and split as they interact with each other."}, {"heading": "3 PROBLEM STATEMENT", "text": "Our goal is to find words that exhibit significant shifts in meaning between corpora of different disciplines. Faced with a corpora containing texts from different disciplines, such as physics, law, religion, etc., in which corpus from each discipline is defined as #, we build a common vocabulary by overlapping the words in the corpus of each discipline #. For each word in, we want to quantify how much the meaning of that word has changed and be able to imagine how the word has developed across different corpus."}, {"heading": "4 METHOD", "text": "Figure 1 gives an overview of the method, the method can be divided into three phases, we will first perform some data pre-processing, then we will train the embedding in each corpus using a neural language model, and the last step is to identify words with the greatest shift in meaning."}, {"heading": "4.1 Data Preprocessing", "text": "The text from the corpora is converted into sentences and the whole punctuation is removed from the text. The first training is done on a large data set to ensure that there is a large number of words in the model and that the embedding of these words is well defined. We use the Wikipedia data set to train the original word2vec model."}, {"heading": "4.2 Word Embeddings", "text": "Word2vec is a neural language model that can be used to educate words in one of two ways, the first method predicts the word in context, the model uses the preceding and following words, so the input is in the model. The second method predicts the context of the word, the model takes the word as input and returns the context, \"\" \"\" \"\", \"\""}, {"heading": "4.3 Distance Metric", "text": "The word embedding that has been learned is then used to find words that have a maximum linguistic change. To find these words, we use an ensemble technique in which we use the sum of two distance measurements, incrementally the Euclidean distance and the closest adjacent distance. The incremental Euclidean distance is calculated by finding the distance moved by the word in the embedding of each model, and this distance is normalized by using the protocol of the number of words in each model. To be more specific, the incremental Euclidean distance is defined as follows: = TUV WXYPZ, WXYP [\\ Z] ^ _ ('VZ P) # a (3), where the word is the incremental Euclidean distance, # is the embedding of the word in the field (,) which is the distance between the vectors a and b, and # the word is the closest to the neighbor, is the difference between the most similar neighbors that have a similar neighbor."}, {"heading": "K =20.", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5 VISUALIZATION", "text": "To provide a frame of reference, we use adjacent words on each segment. The adjacent words have a similar context and can therefore convey an understanding of what the word means at this point. To visualize the linguistic shifts in the data, we use advanced scatter charts and storyline visualization. In this section, we explain the methodology used to create these visualizations to generate this visualization, and we trained our model on corpus from four different areas: Wikipedia, fiction (Lord of the Rings trilogy), religious (King James version), and politics (A Preface to Politics by Walter Lippmann). We used these texts because they represent a very diverse group, and it will be interesting to see how words change the meaning within these texts. These corpora are used to train the model and find the words that move the heart the most, as a result of the most described method4. \""}, {"heading": "5.1 Enhanced Scatterplot", "text": "The scattering shows how the word heart changes its position in different embeddings. To determine the meaning of the word, we use the closest neighbors of the words. As the embedding of the adjacent words also changes, we use the mean of the embeddings in different sections. Then, we use a dimension reduction technique to reduce the number of dimensions to 2 and visualize the graph as a scatter chart (see Figure 2). The illustration shows the word \"heart\" as it is influenced by the text of another body. We see that the word heart is closer to biological organs in the Wikipedia corpus. In fictional works, its meaning is closer to words that represent pain such as pain and palpitations. In religious and political texts, the meaning is closer to feelings such as merriment and grief. The visualization can show how the word heart changes the meaning in different bodies. The limitation of this visualization is that the words lie closely together, making it difficult to determine how the words will be grouped into the few embeddings."}, {"heading": "5.2 Storyline Visualization", "text": "The visualization of the action is a technique inspired by the illustration of Munroe. [20] The visualization of the action shows the interaction between different characters in the film The Lord of the Rings. Lines that are strung together represent the spatial proximity of the characters. However, the visualization of the action is an effective way to convey how the relationships between different entities change over time. The visualization of the action fits well into our scheme, as it allows us to show how the relationship between different words develops over different segments. We believe that the visualization of the action is an effective visualization technology for this scenario because, while it does not convey the exact position of the words, it conveys very effectively how the words are summarized. It will allow us to determine whether the word has moved into another neighborhood and also shows the overall development of the word."}, {"heading": "6 CONCLUSION AND FUTURE WORK", "text": "In this paper, we have presented a method for finding words that exhibit linguistic shifts, and we have evaluated visualization techniques that visualize the linguistic shifting of words. We show that the visualization of scatter diagrams can be expanded to include visualization of storylines to gain more insight into the data. In the future, we intend to develop a comprehensive tool for analyzing linguistic shifts in data. We will use some of the visualization techniques mentioned in this paper, as well as provide other data, such as language building blocks, word frequency, etc., that are commonly used in analyzing linguistic shifts."}], "references": [{"title": "Distributed representations of geographically situated language.", "author": ["Bamman", "David", "Chris Dyer", "Noah A. Smith"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2014}, {"title": "Gender identity and lexical variation in social media.", "author": ["Bamman", "David", "Jacob Eisenstein", "Tyler Schnoebelen"], "venue": "Journal of Sociolinguistics", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2014}, {"title": "Sen\u00e9cal. \"Adaptive importance sampling to accelerate training of a neural probabilistic language model.", "author": ["Bengio", "Yoshua", "Jean-S\u00e9bastien"], "venue": "Neural Networks, IEEE Transactions on 19.4", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2008}, {"title": "Using Visual Analytics Tool for Improving Data Comprehension.", "author": ["G\u00e9ryk", "Jan"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2015}, {"title": "Dynamic visualizations of language change: Motion charts on the basis of bivariate and multivariate data from diachronic corpora.\"International", "author": ["Hilpert", "Martin"], "venue": "Journal of Corpus Linguistics", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2011}, {"title": "The time course of language change.\" Computers and the Humanities", "author": ["Juola", "Patrick"], "venue": "77-96.cognitive science society. Vol", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1986}, {"title": "Diffusion of lexical change in social media", "author": ["J. Eisenstein", "B. O\u2019Connor", "N.A. Smith", "E.P. Xing"], "venue": "PLoS ONE,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2014}, {"title": "Tracing genealogical data with timenets.", "author": ["Kim", "Nam Wook", "Stuart K. Card", "Jeffrey Heer"], "venue": "Proceedings of the International Conference on Advanced Visual Interfaces. ACM,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2010}, {"title": "On Minimizing Crossings in Storyline Visualizations.\"arXiv preprint arXiv:1509.00442", "author": ["Kostitsyna", "Irina"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2015}, {"title": "Freshman or Fresher? Quantifying the Geographic Variation of Internet Language.", "author": ["Kulkarni", "Vivek", "Bryan Perozzi", "Steven Skiena"], "venue": "arXiv preprint arXiv:1510.06786", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2015}, {"title": "Statistically Significant Detection of Linguistic Change.\"Proceedings of the 24th International", "author": ["Kulkarni", "Vivek"], "venue": "Conference on World Wide Web. International World Wide Web Conferences Steering Committee,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2015}, {"title": "Storyflow: Tracking the evolution of stories.\" Visualization and Computer Graphics", "author": ["Liu", "Shixia"], "venue": "IEEE Transactions on 19.12", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2013}, {"title": "Efficient estimation of word representations in vector space.\" arXiv preprint", "author": ["Mikolov", "Tomas"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality.\" Advances in neural information processing", "author": ["Mikolov", "Tomas"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2013}, {"title": "Linguistic Regularities in Continuous Space Word Representations.\" HLT- NAACL", "author": ["Mikolov", "Tomas", "Wen-tau Yih", "Geoffrey Zweig"], "venue": null, "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2013}, {"title": "Distributional measures of semantic distance: A survey.\" arXiv preprint", "author": ["Mohammad", "Saif M", "Graeme Hirst"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2012}, {"title": "Hierarchical probabilistic neural network language model.", "author": ["Morin", "Frederic", "Yoshua Bengio"], "venue": "Proceedings of the international workshop on artificial intelligence and statistics", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2005}, {"title": "Plotweaver.\" Ogievetsky. http://ogievetsky", "author": ["Ogievetsky", "Vadim"], "venue": "com/PlotWeaver", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2013}, {"title": "Large-scale deep unsupervised learning using graphics processors.", "author": ["Raina", "Rajat", "Anand Madhavan", "Andrew Y. Ng"], "venue": "Proceedings of the 26th annual international conference on machine learning", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2009}, {"title": "Text Variation Explorer: Towards interactive visualization tools for corpus linguistics.", "author": ["Siirtola", "Harri"], "venue": "International Journal of Corpus Linguistics", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2014}, {"title": "Design considerations for optimizing storyline visualizations.\" Visualization and Computer Graphics", "author": ["Tanahashi", "Yuzuru", "Kwan-Liu Ma"], "venue": "IEEE Transactions on 18.12", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2012}, {"title": "An Efficient Framework for Generating Storyline Visualizations from Streaming", "author": ["Tanahashi", "Yuzuru", "C. Hsueh", "K. Ma"], "venue": null, "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2015}, {"title": "Neural networks leverage corpus-wide information for part-of-speech tagging.", "author": ["Tsuboi", "Yuta"], "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2014}, {"title": "Visualizing the evolution of communities in dynamic graphs.\"Computer", "author": ["C Vehlow"], "venue": "Graphics Forum. Vol. 34. No", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "[1] use Twitter data to study language shift in different genders, Bamman et al.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "[2, 11] also explore how geography and other contextual factors affect word meaning by using vector space representation of words.", "startOffset": 0, "endOffset": 7}, {"referenceID": 8, "context": "[2, 11] also explore how geography and other contextual factors affect word meaning by using vector space representation of words.", "startOffset": 0, "endOffset": 7}, {"referenceID": 7, "context": "[10] propose a model that identifies geographic and demographic factors that spread new words in online text.", "startOffset": 0, "endOffset": 4}, {"referenceID": 5, "context": "[8,12] present a model for measuring language change over time.", "startOffset": 0, "endOffset": 6}, {"referenceID": 9, "context": "[8,12] present a model for measuring language change over time.", "startOffset": 0, "endOffset": 6}, {"referenceID": 15, "context": "In [18] Muhammad et al.", "startOffset": 3, "endOffset": 7}, {"referenceID": 4, "context": "Hilpert [6] uses dynamic motion charts to track the arrangements of the relative words, the sequence of graphs give the researcher an understanding of the complicated linguistic development.", "startOffset": 8, "endOffset": 11}, {"referenceID": 3, "context": "Motion charts are also used by others in this area as well [4, 5, 24].", "startOffset": 59, "endOffset": 69}, {"referenceID": 19, "context": "Motion charts are also used by others in this area as well [4, 5, 24].", "startOffset": 59, "endOffset": 69}, {"referenceID": 19, "context": "[24] designed a tool, Text Variation Explorer (TVE), that allows interactive examination of the behavior of linguistic parameters affected by text window size and overlap, and performs interactive principal component analysis based on a user given set of words.", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "[12] use dimension reduction on their word vector and visualize them using a scatterplot.", "startOffset": 0, "endOffset": 4}, {"referenceID": 2, "context": "[3] presented a neural language model for learning word embeddings, their model outperformed the traditional n-gram based techniques.", "startOffset": 0, "endOffset": 3}, {"referenceID": 16, "context": "Various methods have been proposed that scale and speed up large neural models [19, 23].", "startOffset": 79, "endOffset": 87}, {"referenceID": 12, "context": "[15]", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "proposed skip-gram models for learning word embeddings and demonstrated that these models have the capacity to learn linguistic patterns as linear relationships between vectors [16, 17].", "startOffset": 177, "endOffset": 185}, {"referenceID": 14, "context": "proposed skip-gram models for learning word embeddings and demonstrated that these models have the capacity to learn linguistic patterns as linear relationships between vectors [16, 17].", "startOffset": 177, "endOffset": 185}, {"referenceID": 22, "context": "[27] incorporated word2vec embeddings with GloVe embeddings into a Part of Speech (POS) tagging system and show that it produces better results compared to the individual systems.", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "[28] use this technique to visualize dynamic community structure, by portraying communities as converged entities.", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "[10] visualize genealogical data where individuals are represented using lines that converge and diverge to indicate marriage and divorce.", "startOffset": 0, "endOffset": 4}, {"referenceID": 3, "context": "[4] use text stream to visualize the evolution of topics, topics merge and split as they interact with each other.", "startOffset": 0, "endOffset": 3}, {"referenceID": 17, "context": "created an online editing tool, Plot Weaver [21], that allows users to create a storyline visualization by interactive editing.", "startOffset": 44, "endOffset": 48}, {"referenceID": 20, "context": "[25] and Liu et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[14], they have created schemes that produce results on par with professional artists.", "startOffset": 0, "endOffset": 4}, {"referenceID": 8, "context": "[11] and Tanahashi et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "[26] have developed a scheme to create storyline visualizations for streaming data.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "We optimize the process by mapping the problem from a classification of 1-out-of-V to a hierarchical classification problem [19].", "startOffset": 124, "endOffset": 128}, {"referenceID": 21, "context": "The method we use to generate storylines is inspired by [26], however their method is made primarily for streaming data and one of their main concerns is the conservation of the user\u2019s mental mapping during the constant layout updates.", "startOffset": 56, "endOffset": 60}], "year": 2016, "abstractText": "Neural network based models are a very powerful tool for creating word embeddings, the objective of these models is to group similar words together. These embeddings have been used as features to improve results in various applications such as document classification, named entity recognition, etc. Neural language models are able to learn word representations which have been used to capture semantic shifts across time and geography. The objective of this paper is to first identify and then visualize how words change meaning in different text corpus. We will train a neural language model on texts from a diverse set of disciplines \u2013 philosophy, religion, fiction etc. Each text will alter the embeddings of the words to represent the meaning of the word inside that text. We will present a computational technique to detect words that exhibit significant linguistic shift in meaning and usage. We then use enhanced scatterplots and storyline visualization to visualize the linguistic shift", "creator": "Word"}}}