{"id": "1611.06492", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Nov-2016", "title": "Recurrent Memory Addressing for describing videos", "abstract": "Deep Neural Network architectures with external memory components allow the model to perform inference and capture long term dependencies, by storing information explicitly. In this paper, we generalize Key-Value Memory Networks to a multimodal setting, introducing a novel key-addressing mechanism to deal with sequence-to-sequence models. The advantages of the framework are demonstrated on the task of video captioning, i.e generating natural language descriptions for videos. Conditioning on the previous time-step attention distributions for the key-value memory slots, we introduce a temporal structure in the memory addressing schema. The proposed model naturally decomposes the problem of video captioning into vision and language segments, dealing with them as key-value pairs. More specifically, we learn a semantic embedding (v) corresponding to each frame (k) in the video, thereby creating (k, v) memory slots. This allows us to exploit the temporal dependencies at multiple hierarchies (in the recurrent key-addressing; and in the language decoder). Exploiting this flexibility of the framework, we additionally capture spatial dependencies while mapping from the visual to semantic embedding. Extensive experiments on the Youtube2Text dataset demonstrate usefulness of recurrent key-addressing, while achieving competitive scores on BLEU@4, METEOR metrics against state-of-the-art models.", "histories": [["v1", "Sun, 20 Nov 2016 10:07:54 GMT  (1454kb,D)", "http://arxiv.org/abs/1611.06492v1", "Under review at CVPR 2017"], ["v2", "Thu, 23 Mar 2017 14:01:20 GMT  (1455kb,D)", "http://arxiv.org/abs/1611.06492v2", null]], "COMMENTS": "Under review at CVPR 2017", "reviews": [], "SUBJECTS": "cs.CV cs.CL", "authors": ["arnav kumar jain", "abhinav agarwalla", "kumar krishna agrawal", "pabitra mitra"], "accepted": false, "id": "1611.06492"}, "pdf": {"name": "1611.06492.pdf", "metadata": {"source": "CRF", "title": "Recurrent Memory Addressing for describing videos", "authors": ["Kumar Krishna Agrawal", "Arnav Kumar Jain", "Abhinav Agarwalla", "Pabitra Mitra"], "emails": ["pabitra}@iitkgp.ac.in", "BLEU@4,"], "sections": [{"heading": null, "text": "The proposed model naturally breaks down the problem of video captioning into image and language segments and treats them as key-value pairs. Specifically, we learn a semantic embedding (v) that matches each image (k) in the video, creating space (k, v), which allows us to exploit the time dependencies in multiple hierarchies (in the recurring keyaddress and in the speech decoder). Taking advantage of this flexibility of the framework, we also capture spatial dependencies while mapping from visual to semantic embedding. Extensive experiments with the Youtube2Text dataset show the usefulness of recurring key addressing, while achieving competitive values with the EU-4-TER models of OMER metrics."}, {"heading": "1. Introduction", "text": "In fact, most of them are only a matter of time before there is an agreement, in which there is an agreement."}, {"heading": "2. Related Work", "text": "In fact, the fact is that most of them will be able to put themselves in a situation where they are able to survive on their own, and in which they have got into a situation where they are able, in which they are able to flourish."}, {"heading": "3. Recurrent Memory Addressing for videos", "text": "Our model is a general extension of the encoder decoder framework [9, 3, 45, 42, 17] in a storage network [44, 25]. The encoder network learns a mapping from the input sequence to a fixed-length vector representation used by the decoder to generate output sequences. Unlike conventional encoder decoder architectures, our model (see fig. 2) consists of an encoder module, key value stores and a decoder module."}, {"heading": "3.1. Encoder", "text": "The encoder network E maps a given sequence X = {I1,..., IT} to the corresponding sequence of context display vectors {k1,..., kT}. Since we are dealing with videos (image sequence), we define two different encoders to achieve the mapping. CNN Encoders: Faced with an input image It-RNxM, CNN encoders learn a mapping f: RNxM \u2192 RD when we look at the output of fully connected layers in standard ConvNet architectures [33]. In the case of folding outputs, the encoders learn f: RNxM \u2192 RLxD, where L is the number of context vectors of dimensionality D.RNN encoder: Consider an input sequence of length T. The RNN encoder processes the input X sequentially, generating hidden states at each time step, where L = it is directly transmitted to the suggested unit (2) during the suggested length \u2212"}, {"heading": "3.2. Key-Value Memories", "text": "The model is based on a key-value memory network [25] with storage spaces as vector pairs (k1, v1),..., (kn, vn).The keys and values serve the purpose of transforming the visual spatial context into the linguistic space and effectively capturing the relationships between the video features and textual descriptions.The memory definition, addressing and the reading scheme are outlined below: Key (K): In our model, feature vectors for each individual image are extracted using CNN encoders. To include the sequential structure (video is a sequence of images), these key feature vectors are passed through an RNN encoder, and the hidden state in each timestep is extracted as a key kt.kt = g (Es), kt \u2212 1) (3) This allows the model to capture the time variation between frames and take into account the time signal."}, {"heading": "3.3. Decoder", "text": "The common probability of generating the output sequence can be broken down as follows: p (y | x1,..., xT) = max = 1 p (yt | hT, y1,.., yt \u2212 1) (6) where hT is the context vector that X = {x1,..., xT} and y = {y1,..., yN} is the output sequence. Therefore, to train the network, the overall goal is to maximize the log probability of the output word sequence. To this end, recursive neural networks are often used for tasks such as generating natural language such as machine translation, image editing, and video description generation. Vanilla RNNNxt are difficult to train for dependencies over long dities, as they suffer from the problem of disappearing gradient [5]. Therefore, we use Long Short Term Memory (LSTM) [15], because they are known to store the context over a longer period of time."}, {"heading": "4. Key Addressing", "text": "Soft attention mechanism have recently been successful in Image Captioning [45] and Video Description Generation = q] tasks because they help to focus on relevant parts of the features rather than mean pooling. In [41], the feature vectors were simply averaged, leading to loss of the temporal relations between frames of the video. Soft attention mechanism weighings each frame allows to exploit the temporal structure in video. They have also been used in the Key-Value Memory Networks [25] to focus on relevant keys and reads the weighted sum of values. We propose a recurrent key addressing mechanism that looks at the previous attention distribution via keys in order to obtain the new relevance values. This helps to use multiple hierarchies in sequence-to-sequence architectures, as the model selects relevant frames based on the previous key distribution and the previously generated words. There is a Key Addressing RNN (referred to as Memory LSTM in Fig. 4) to keep an overview of the previous attention and the hidden ones."}, {"heading": "5. Experimental Setup", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1. Dataset", "text": "The proposed approach is benchmarked on the Youtube2Text [7] dataset, which consists of 1,970 YouTube videos with multiple descriptions commented on by Amazon Mechanical Turk. The videos are generally short (9 seconds on average) and represent a single activity, with activities ranging from everyday objects to animals, scenarios, actions, landscapes, etc. The dataset consists of 80,839 annotations and vocabulary of about 16,000 words with an average of 41 annotations per clip and 8 words per sentence. Training, validation and test kits have 1,200, 100, and 670 videos that have exactly the same splitters as in previous work with captions [47, 4, 27].Key-Value Memories We select 28 equally sized frames and pass them through a pre-formed VGG-16 [33] and GooNet [38] for their state."}, {"heading": "5.2. Model Specifications", "text": "The results of various variations are presented in Table 1. VGG encoder uses functions encoded from the last folding layer in VGG-16 CNN [33], along with an LSTM decoder. Here, we define q = ht \u2212 1 for key addressing below [25]. GoogLeNet encoder is exactly the same as VGG encoder with the only difference that GoogLeNet is used for function encoding. t-KeyAddressing complements GoogLeNet encoder by addressing keys both by the last hidden state \u2212 1 and by the previous gradual attention distribution on the keys. Finally, m-KeyAddressing summarizes all previous attention distributions by an LSTM, after which its hidden state is used for key addressing along with the last hidden state of the decoder. These variations help us to identify architectural changes that lead to great improvements in comparison of the results of S6."}, {"heading": "5.3. Model Comparisons", "text": "Pan et al. [28] explicitly learn a visual-semantic joint-embedding model to exploit the relationship between visual features and generated language, which is then used in a similar encoder-decoder framework. Yao et al. [47] use, in addition to local attention, a temporal attention mechanism for global attention via 3-D folding networks. Ballas et al. [4] proposed an encoder to learn spatio-temporal features across frames, introducing a variant of GRU with folding operations (GRU-RCN). In the current state of the art, Yu et al. [49] model the decoder as a paragraph generator that describes the videos across multiple sentences using stacked LSTMs."}, {"heading": "5.4. Evaluation Metrics", "text": "We evaluate our approach using standard evaluation metrics for video subtitling tasks, namely BLEU [29], METEOR [22] and CIDER [39], to compare the generated sequences with human annotations. Treating this as a sequence-to-sequence model and comparing it to the given truth, higher values on the metrics indicate better performance. The metric value is calculated based on the orientation and similarity between the generated and the reference descriptions of the candidates. To obtain the results reported in the paper, we use the code attached to the Microsoft COCO evaluation script [8]."}, {"heading": "5.5. Training Details", "text": "The model predicts the next output word taking into account the previous words and the input video. Therefore, the goal is to maximize the probability of the loss function in the log: L = 1N N \u2211 i = 1 | yi | \u2211 j = 1 log p (yij | yi < j, xn, \u03b8) (19), where N is the total number of video description pairs and the length of each description yi is | yi |. Here, xn refers to the input video provided as context for the decoder. We train our network parameters \u03b8 by stochastic gradient-based first order optimization with an adaptive learning rate using the Adadelta [51] optimizer. We set the stack size in our implementation to 64. We optimize hyperparameters, namely the number of hidden units in the decoder LSTM, key address LSTM, learning rate, word embedding dimension for the random search [6]."}, {"heading": "5.6. Results", "text": "We are testing our proposed model in the default setting with visual features extracted from a pre-fabricated VGG-16 [33] and GoogLeNet [38] model. These models are capable of surpassing the baseline of S2VT [40], and the basic encoder decoder model introduced in [47] on all three metrics follows the basic approach as in [25], i.e. without tracking the last time steps. We observe that the use of features from upstream GoogleNet rather than VGG-16 improves the results, and that the basic encoder decoder model introduced in relation to all three metrics."}, {"heading": "6. Conclusion", "text": "By dissecting the visual and linguistic components, we explicitly exploit the temporal structure of a variety of hierarchies. Extensive experiments with the proposed model exceed strong baselines across multiple metrics and achieve competitive values compared to current benchmarks. To our knowledge, this is the first proposed work for captioning videos in a storage network and does not rely on heavily commented videos to create semantic intermediate embedding to support the decoder. Further work concerns exploring the effectiveness of the model on longer videos and creating fine-grained descriptions with more complex decoders."}], "references": [{"title": "Vqa: Visual question answering", "author": ["S. Antol", "A. Agrawal", "J. Lu", "M. Mitchell", "D. Batra", "C. Lawrence Zitnick", "D. Parikh"], "venue": "Proceedings of the IEEE International Conference on Computer Vision, pages 2425\u20132433,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2015}, {"title": "Layer normalization", "author": ["J.L. Ba", "J.R. Kiros", "G.E. Hinton"], "venue": "arXiv preprint arXiv:1607.06450,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2016}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["D. Bahdanau", "K. Cho", "Y. Bengio"], "venue": "arXiv preprint arXiv:1409.0473,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2014}, {"title": "Delving deeper into convolutional networks for learning video representations", "author": ["N. Ballas", "L. Yao", "C. Pal", "A. Courville"], "venue": "arXiv preprint arXiv:1511.06432,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning long-term dependencies with gradient descent is difficult", "author": ["Y. Bengio", "P. Simard", "P. Frasconi"], "venue": "IEEE transactions on neural networks, 5(2):157\u2013166,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1994}, {"title": "Random search for hyperparameter optimization", "author": ["J. Bergstra", "Y. Bengio"], "venue": "Journal of Machine Learning Research, 13(Feb):281\u2013305,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2012}, {"title": "Collecting highly parallel data for paraphrase evaluation", "author": ["D.L. Chen", "W.B. Dolan"], "venue": "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume 1, pages 190\u2013200. Association for Computational Linguistics,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2011}, {"title": "Microsoft coco captions: Data collection and evaluation server", "author": ["X. Chen", "H. Fang", "T.-Y. Lin", "R. Vedantam", "S. Gupta", "P. Doll\u00e1r", "C.L. Zitnick"], "venue": "arXiv preprint arXiv:1504.00325,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning phrase representations using rnn encoder\u2013decoder for statistical machine translation", "author": ["K. Cho", "B. Van Merri\u00ebnboer", "\u00c7. G\u00fcl\u00e7ehre", "D. Bahdanau", "F. Bougares", "H. Schwenk", "Y. Bengio"], "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1724\u20131734, Doha, Qatar, Oct.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2014}, {"title": "Empirical evaluation of gated recurrent neural networks on sequence modeling", "author": ["J. Chung", "C. Gulcehre", "K. Cho", "Y. Bengio"], "venue": "arXiv preprint arXiv:1412.3555,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2014}, {"title": "A thousand frames in just a few words: Lingual description of videos through latent topics and sparse object stitching", "author": ["P. Das", "C. Xu", "R.F. Doell", "J.J. Corso"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2634\u20132641,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2013}, {"title": "ImageNet: A Large-Scale Hierarchical Image Database", "author": ["J. Deng", "W. Dong", "R. Socher", "L.-J. Li", "K. Li", "L. Fei-Fei"], "venue": "CVPR09,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2009}, {"title": "From captions to visual concepts and back", "author": ["H. Fang", "S. Gupta", "F. Iandola", "R.K. Srivastava", "L. Deng", "P. Doll\u00e1r", "J. Gao", "X. He", "M. Mitchell", "J.C. Platt"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2015}, {"title": "Devise: A deep visual-semantic embedding model", "author": ["A. Frome", "G.S. Corrado", "J. Shlens", "S. Bengio", "J. Dean", "T. Mikolov"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2013}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural computation, 9(8):1735\u20131780,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1997}, {"title": "Densecap: Fully convolutional localization networks for dense captioning", "author": ["J. Johnson", "A. Karpathy", "L. Fei-Fei"], "venue": "arXiv preprint arXiv:1511.07571,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep visual-semantic alignments for generating image descriptions", "author": ["A. Karpathy", "L. Fei-Fei"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 3128\u20133137,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2015}, {"title": "Unifying visual-semantic embeddings with multimodal neural language models", "author": ["R. Kiros", "R. Salakhutdinov", "R.S. Zemel"], "venue": "arXiv preprint arXiv:1411.2539,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2014}, {"title": "Babytalk: Understanding and generating simple image descriptions", "author": ["G. Kulkarni", "V. Premraj", "V. Ordonez", "S. Dhar", "S. Li", "Y. Choi", "A.C. Berg", "T.L. Berg"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, 35(12):2891\u2013 2903,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2013}, {"title": "Ask me anything: Dynamic memory networks for natural language processing", "author": ["A. Kumar", "O. Irsoy", "J. Su", "J. Bradbury", "R. English", "B. Pierce", "P. Ondruska", "I. Gulrajani", "R. Socher"], "venue": "arXiv preprint arXiv:1506.07285,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2015}, {"title": "Professor Forcing: A New Algorithm for Training Recurrent Networks", "author": ["A. Lamb", "A. Goyal", "Y. Zhang", "S. Zhang", "A. Courville", "Y. Bengio"], "venue": "ArXiv e-prints, Oct.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2016}, {"title": "Meteor universal: language specific translation evaluation for any target language", "author": ["M.D.A. Lavie"], "venue": "ACL 2014, page 376,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2014}, {"title": "Gradientbased learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE, 86(11):2278\u20132324,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 1998}, {"title": "Nltk: The natural language toolkit", "author": ["E. Loper", "S. Bird"], "venue": "Proceedings of the ACL-02 Workshop on Effective Tools and Methodologies for Teaching Natural Language Processing and Computational Linguistics - Volume 1, ETMTNLP \u201902, pages 63\u201370, Stroudsburg, PA, USA,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2002}, {"title": "Key-value memory networks for directly reading documents", "author": ["A. Miller", "A. Fisch", "J. Dodge", "A.-H. Karimi", "A. Bordes", "J. Weston"], "venue": "arXiv preprint arXiv:1606.03126,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2016}, {"title": "Recurrent models of visual attention", "author": ["V. Mnih", "N. Heess", "A. Graves"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2014}, {"title": "Hierarchical recurrent neural encoder for video representation with application to captioning", "author": ["P. Pan", "Z. Xu", "Y. Yang", "F. Wu", "Y. Zhuang"], "venue": "arXiv preprint arXiv:1511.03476,", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2015}, {"title": "Jointly modeling embedding and translation to bridge video and language", "author": ["Y. Pan", "T. Mei", "T. Yao", "H. Li", "Y. Rui"], "venue": "arXiv preprint arXiv:1505.01861,", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2015}, {"title": "Bleu: a method for automatic evaluation of machine translation", "author": ["K. Papineni", "S. Roukos", "T. Ward", "W.-J. Zhu"], "venue": "Proceedings of the 40th annual meeting on association for computational linguistics, pages 311\u2013318. Association for Computational Linguistics,", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2002}, {"title": "Reasoning about entailment with neural attention", "author": ["T. Rockt\u00e4schel", "E. Grefenstette", "K.M. Hermann", "T. Ko\u010disk\u1ef3", "P. Blunsom"], "venue": "arXiv preprint arXiv:1509.06664,", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2015}, {"title": "Translating video content to natural language descriptions", "author": ["M. Rohrbach", "W. Qiu", "I. Titov", "S. Thater", "M. Pinkal", "B. Schiele"], "venue": "Proceedings of the IEEE International Conference on Computer Vision, pages 433\u2013440,", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2013}, {"title": "Building end-to-end dialogue systems using generative hierarchical neural network models", "author": ["I.V. Serban", "A. Sordoni", "Y. Bengio", "A. Courville", "J. Pineau"], "venue": "Proceedings of the 30th AAAI Conference on Artificial Intelligence (AAAI- 16),", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2016}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "arXiv preprint arXiv:1409.1556,", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2014}, {"title": "Ucf101: A dataset of 101 human actions classes from videos in the wild", "author": ["K. Soomro", "A.R. Zamir", "M. Shah"], "venue": "arXiv preprint arXiv:1212.0402,", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2012}, {"title": "Deep networks with internal selective attention through feedback connections", "author": ["M.F. Stollenga", "J. Masci", "F. Gomez", "J. Schmidhuber"], "venue": "Advances in Neural Information Processing Systems, pages 3545\u20133553,", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2014}, {"title": "End-to-end memory networks", "author": ["S. Sukhbaatar", "J. Weston", "R. Fergus"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2015}, {"title": "Sequence to sequence learning with neural networks", "author": ["I. Sutskever", "O. Vinyals", "Q.V. Le"], "venue": "Advances in neural information processing systems, pages 3104\u20133112,", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2014}, {"title": "Going deeper with convolutions", "author": ["C. Szegedy", "W. Liu", "Y. Jia", "P. Sermanet", "S. Reed", "D. Anguelov", "D. Erhan", "V. Vanhoucke", "A. Rabinovich"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 1\u20139,", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2015}, {"title": "Cider: Consensus-based image description evaluation", "author": ["R. Vedantam", "C. Lawrence Zitnick", "D. Parikh"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 4566\u20134575,", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2015}, {"title": "Sequence to sequence \u2013 video to text", "author": ["S. Venugopalan", "M. Rohrbach", "J. Donahue", "R. Mooney", "T. Darrell", "K. Saenko"], "venue": "Proceedings of the IEEE International Conference on Computer Vision (ICCV),", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2015}, {"title": "Translating videos to natural language using deep recurrent neural networks", "author": ["S. Venugopalan", "H. Xu", "J. Donahue", "M. Rohrbach", "R. Mooney", "K. Saenko"], "venue": "NAACL HLT,", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2015}, {"title": "Show and tell: A neural image caption generator", "author": ["O. Vinyals", "A. Toshev", "S. Bengio", "D. Erhan"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 3156\u20133164,", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2015}, {"title": "Backpropagation through time: what it does and how to do it", "author": ["P.J. Werbos"], "venue": "Proceedings of the IEEE, 78(10):1550\u2013 1560,", "citeRegEx": "43", "shortCiteRegEx": null, "year": 1990}, {"title": "Memory networks", "author": ["J. Weston", "S. Chopra", "A. Bordes"], "venue": "arXiv preprint arXiv:1410.3916,", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2014}, {"title": "Show, attend and tell: Neural image caption generation with visual attention", "author": ["K. Xu", "J. Ba", "R. Kiros", "K. Cho", "A.C. Courville", "R. Salakhutdinov", "R.S. Zemel", "Y. Bengio"], "venue": "CoRR, abs/1502.03044,", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2015}, {"title": "Encode, review, and decode: Reviewer module for caption generation", "author": ["Z. Yang", "Y. Yuan", "Y. Wu", "R. Salakhutdinov", "W.W. Cohen"], "venue": "arXiv preprint arXiv:1605.07912,", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2016}, {"title": "Describing videos by exploiting temporal  structure", "author": ["L. Yao", "A. Torabi", "K. Cho", "N. Ballas", "C. Pal", "H. Larochelle", "A. Courville"], "venue": "Proceedings of the IEEE International Conference on Computer Vision, pages 4507\u20134515,", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2015}, {"title": "Image captioning with semantic attention", "author": ["Q. You", "H. Jin", "Z. Wang", "C. Fang", "J. Luo"], "venue": "arXiv preprint arXiv:1603.03925,", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2016}, {"title": "Video paragraph captioning using hierarchical recurrent neural networks", "author": ["H. Yu", "J. Wang", "Z. Huang", "Y. Yang", "W. Xu"], "venue": "arXiv preprint arXiv:1510.07712,", "citeRegEx": "49", "shortCiteRegEx": null, "year": 2015}, {"title": "Recurrent neural network regularization", "author": ["W. Zaremba", "I. Sutskever", "O. Vinyals"], "venue": "arXiv preprint arXiv:1409.2329,", "citeRegEx": "50", "shortCiteRegEx": null, "year": 2014}, {"title": "Adadelta: an adaptive learning rate method", "author": ["M.D. Zeiler"], "venue": "arXiv preprint arXiv:1212.5701,", "citeRegEx": "51", "shortCiteRegEx": null, "year": 2012}], "referenceMentions": [{"referenceID": 22, "context": "With impressive results in object detection and scene understanding, Convolution Neural Networks (CNNs) [23] have become the staple for extracting feature representations from images.", "startOffset": 104, "endOffset": 108}, {"referenceID": 14, "context": "Recurrent Neural Networks (RNNs) with Long Short Term Memory (LSTM) [15] units or Gated Recurrent Units (GRUs)[10], have similarly emerged as generative models of choice for dealing with sequences in domains ranging from language modeling, machine translation to speech recognition.", "startOffset": 68, "endOffset": 72}, {"referenceID": 9, "context": "Recurrent Neural Networks (RNNs) with Long Short Term Memory (LSTM) [15] units or Gated Recurrent Units (GRUs)[10], have similarly emerged as generative models of choice for dealing with sequences in domains ranging from language modeling, machine translation to speech recognition.", "startOffset": 110, "endOffset": 114}, {"referenceID": 15, "context": "Advancements in these fundamental problems make tackling challenging problems, like captioning [16, 45], dialogue [32] and visual question answering [1] more viable.", "startOffset": 95, "endOffset": 103}, {"referenceID": 44, "context": "Advancements in these fundamental problems make tackling challenging problems, like captioning [16, 45], dialogue [32] and visual question answering [1] more viable.", "startOffset": 95, "endOffset": 103}, {"referenceID": 31, "context": "Advancements in these fundamental problems make tackling challenging problems, like captioning [16, 45], dialogue [32] and visual question answering [1] more viable.", "startOffset": 114, "endOffset": 118}, {"referenceID": 0, "context": "Advancements in these fundamental problems make tackling challenging problems, like captioning [16, 45], dialogue [32] and visual question answering [1] more viable.", "startOffset": 149, "endOffset": 152}, {"referenceID": 2, "context": "A common underlying approach in these proposed models is the notion of \u201dattention mechanisms\u201d, which refers to selectively focusing on segments of sequences [3, 47] or images [35] to generate corresponding outputs.", "startOffset": 157, "endOffset": 164}, {"referenceID": 46, "context": "A common underlying approach in these proposed models is the notion of \u201dattention mechanisms\u201d, which refers to selectively focusing on segments of sequences [3, 47] or images [35] to generate corresponding outputs.", "startOffset": 157, "endOffset": 164}, {"referenceID": 34, "context": "A common underlying approach in these proposed models is the notion of \u201dattention mechanisms\u201d, which refers to selectively focusing on segments of sequences [3, 47] or images [35] to generate corresponding outputs.", "startOffset": 175, "endOffset": 179}, {"referenceID": 44, "context": "Such attention based approaches are specially attractive for captioning problems, since they allow the network to focus on patches of the image conditioned on the previously generated tokens [45, 16], often referred to as spatial attention.", "startOffset": 191, "endOffset": 199}, {"referenceID": 15, "context": "Such attention based approaches are specially attractive for captioning problems, since they allow the network to focus on patches of the image conditioned on the previously generated tokens [45, 16], often referred to as spatial attention.", "startOffset": 191, "endOffset": 199}, {"referenceID": 46, "context": "To incorporate this temporal attention into the model, [47, 49, 27] extend this soft alignment to video captioning.", "startOffset": 55, "endOffset": 67}, {"referenceID": 48, "context": "To incorporate this temporal attention into the model, [47, 49, 27] extend this soft alignment to video captioning.", "startOffset": 55, "endOffset": 67}, {"referenceID": 26, "context": "To incorporate this temporal attention into the model, [47, 49, 27] extend this soft alignment to video captioning.", "startOffset": 55, "endOffset": 67}, {"referenceID": 36, "context": "Most of these approaches, treat the problem of video captioning in the sequence-to-sequence paradigm [37] with attentive encoders and decoders.", "startOffset": 101, "endOffset": 105}, {"referenceID": 45, "context": "For one, applying attention sequentially provides the model with local context at the generative decoder [46].", "startOffset": 105, "endOffset": 109}, {"referenceID": 27, "context": "Secondly, these models jointly learn the multimodal embedding in a visual-semantic space [28, 49] at the RNN decoder.", "startOffset": 89, "endOffset": 97}, {"referenceID": 48, "context": "Secondly, these models jointly learn the multimodal embedding in a visual-semantic space [28, 49] at the RNN decoder.", "startOffset": 89, "endOffset": 97}, {"referenceID": 27, "context": "While [28] tries to address this issue with an auxiliary loss, the model suffers from the first drawback.", "startOffset": 6, "endOffset": 10}, {"referenceID": 24, "context": "To address the aforementioned issues, we introduce a model which generalizes Key-Value Memory Networks [25] to a multimodal setting for video captioning.", "startOffset": 103, "endOffset": 107}, {"referenceID": 6, "context": "\u2022 The proposed model is evaluated on the YouTube dataset [7], where we outperform strong baselines while reporting competitive results against state-of-art models.", "startOffset": 57, "endOffset": 60}, {"referenceID": 18, "context": "To deal with the multimodal nature of the problem, classical approaches relied on manually engineered templates [19, 11].", "startOffset": 112, "endOffset": 120}, {"referenceID": 10, "context": "To deal with the multimodal nature of the problem, classical approaches relied on manually engineered templates [19, 11].", "startOffset": 112, "endOffset": 120}, {"referenceID": 12, "context": "And while some recent approaches in this direction show promise [13], the models lack generalization to deal with complex scenes, videos.", "startOffset": 64, "endOffset": 68}, {"referenceID": 13, "context": "As an alternative approach, [14, 18] suggest learning a joint visual-semantic embedding, effectively a mapping from the visual to language space.", "startOffset": 28, "endOffset": 36}, {"referenceID": 17, "context": "As an alternative approach, [14, 18] suggest learning a joint visual-semantic embedding, effectively a mapping from the visual to language space.", "startOffset": 28, "endOffset": 36}, {"referenceID": 30, "context": "The motivation of our work is strongly aligned with [31], who generate semantic representations for images using CRF models, as context for the language decoder.", "startOffset": 52, "endOffset": 56}, {"referenceID": 2, "context": "Building on this, and Encoder-Decoder [3, 9] models for machine translation , [42, 41] develop models which compute average fixed-length representations (for images, videos respectively) from image features.", "startOffset": 38, "endOffset": 44}, {"referenceID": 8, "context": "Building on this, and Encoder-Decoder [3, 9] models for machine translation , [42, 41] develop models which compute average fixed-length representations (for images, videos respectively) from image features.", "startOffset": 38, "endOffset": 44}, {"referenceID": 41, "context": "Building on this, and Encoder-Decoder [3, 9] models for machine translation , [42, 41] develop models which compute average fixed-length representations (for images, videos respectively) from image features.", "startOffset": 78, "endOffset": 86}, {"referenceID": 40, "context": "Building on this, and Encoder-Decoder [3, 9] models for machine translation , [42, 41] develop models which compute average fixed-length representations (for images, videos respectively) from image features.", "startOffset": 78, "endOffset": 86}, {"referenceID": 32, "context": "The visual representations for the images are usually transferred from pretrained convolution networks [33, 38].", "startOffset": 103, "endOffset": 111}, {"referenceID": 37, "context": "The visual representations for the images are usually transferred from pretrained convolution networks [33, 38].", "startOffset": 103, "endOffset": 111}, {"referenceID": 36, "context": "Addressing this, [37] propose Sequence-to-Sequence models for accounting for the temporal structure, and [40] extend it to a video-captioning setting.", "startOffset": 17, "endOffset": 21}, {"referenceID": 39, "context": "Addressing this, [37] propose Sequence-to-Sequence models for accounting for the temporal structure, and [40] extend it to a video-captioning setting.", "startOffset": 105, "endOffset": 109}, {"referenceID": 42, "context": "However, passing a fixed vector as context at each time step, creates a bottleneck for the flow of gradients using Backpropagate Through Time (BPTT) [43] at the encoder.", "startOffset": 149, "endOffset": 153}, {"referenceID": 25, "context": "Meanwhile, the notion of visual attention, has a rich literature in Psychology and Neuroscience, and has recently found application in Computer Vision [26] and Machine Translation [3].", "startOffset": 151, "endOffset": 155}, {"referenceID": 2, "context": "Meanwhile, the notion of visual attention, has a rich literature in Psychology and Neuroscience, and has recently found application in Computer Vision [26] and Machine Translation [3].", "startOffset": 180, "endOffset": 183}, {"referenceID": 44, "context": "quences, representative works [45, 47, 16, 4, 49, 27, 28] have significantly pushed the state-of-the-art in individual domain.", "startOffset": 30, "endOffset": 57}, {"referenceID": 46, "context": "quences, representative works [45, 47, 16, 4, 49, 27, 28] have significantly pushed the state-of-the-art in individual domain.", "startOffset": 30, "endOffset": 57}, {"referenceID": 15, "context": "quences, representative works [45, 47, 16, 4, 49, 27, 28] have significantly pushed the state-of-the-art in individual domain.", "startOffset": 30, "endOffset": 57}, {"referenceID": 3, "context": "quences, representative works [45, 47, 16, 4, 49, 27, 28] have significantly pushed the state-of-the-art in individual domain.", "startOffset": 30, "endOffset": 57}, {"referenceID": 48, "context": "quences, representative works [45, 47, 16, 4, 49, 27, 28] have significantly pushed the state-of-the-art in individual domain.", "startOffset": 30, "endOffset": 57}, {"referenceID": 26, "context": "quences, representative works [45, 47, 16, 4, 49, 27, 28] have significantly pushed the state-of-the-art in individual domain.", "startOffset": 30, "endOffset": 57}, {"referenceID": 27, "context": "quences, representative works [45, 47, 16, 4, 49, 27, 28] have significantly pushed the state-of-the-art in individual domain.", "startOffset": 30, "endOffset": 57}, {"referenceID": 2, "context": "where,yi is the readout, ci is the context from the encoder and si = f(si\u22121, yi\u22121, ci), is the hidden state of decoder RNN (See [3] for details).", "startOffset": 128, "endOffset": 131}, {"referenceID": 45, "context": "However, as discussed in Section 1, sequential attention provides the decoder with local context [46].", "startOffset": 97, "endOffset": 101}, {"referenceID": 47, "context": "Additionally, providing a semantic input which is closer to language space, as context to the decoder significantly improves on the capability of the model [48].", "startOffset": 156, "endOffset": 160}, {"referenceID": 43, "context": "Our work closely brings these advances together in a Memory Networks framework [44, 36, 20].", "startOffset": 79, "endOffset": 91}, {"referenceID": 35, "context": "Our work closely brings these advances together in a Memory Networks framework [44, 36, 20].", "startOffset": 79, "endOffset": 91}, {"referenceID": 19, "context": "Our work closely brings these advances together in a Memory Networks framework [44, 36, 20].", "startOffset": 79, "endOffset": 91}, {"referenceID": 24, "context": "While we introduce Key-Value Memory Networks [25] in a multimodal setting, there are several other key differences from previous works.", "startOffset": 45, "endOffset": 49}, {"referenceID": 24, "context": "Key-Value MemNNs [25] are originally proposed for QA task in the language domain, providing the last timestep hidden state, as input to the classifier.", "startOffset": 17, "endOffset": 21}, {"referenceID": 47, "context": "While similar in motivation to [48, 30], the model architecture and domain of application, especially on capturing global temporal dynamics in videos as opposed to images or entailment, is significantly different.", "startOffset": 31, "endOffset": 39}, {"referenceID": 29, "context": "While similar in motivation to [48, 30], the model architecture and domain of application, especially on capturing global temporal dynamics in videos as opposed to images or entailment, is significantly different.", "startOffset": 31, "endOffset": 39}, {"referenceID": 8, "context": "Our model is a general extension of the encoder-decoder framework [9, 3, 45, 42, 17], in a Memory Networks [44, 25] setting.", "startOffset": 66, "endOffset": 84}, {"referenceID": 2, "context": "Our model is a general extension of the encoder-decoder framework [9, 3, 45, 42, 17], in a Memory Networks [44, 25] setting.", "startOffset": 66, "endOffset": 84}, {"referenceID": 44, "context": "Our model is a general extension of the encoder-decoder framework [9, 3, 45, 42, 17], in a Memory Networks [44, 25] setting.", "startOffset": 66, "endOffset": 84}, {"referenceID": 41, "context": "Our model is a general extension of the encoder-decoder framework [9, 3, 45, 42, 17], in a Memory Networks [44, 25] setting.", "startOffset": 66, "endOffset": 84}, {"referenceID": 16, "context": "Our model is a general extension of the encoder-decoder framework [9, 3, 45, 42, 17], in a Memory Networks [44, 25] setting.", "startOffset": 66, "endOffset": 84}, {"referenceID": 43, "context": "Our model is a general extension of the encoder-decoder framework [9, 3, 45, 42, 17], in a Memory Networks [44, 25] setting.", "startOffset": 107, "endOffset": 115}, {"referenceID": 24, "context": "Our model is a general extension of the encoder-decoder framework [9, 3, 45, 42, 17], in a Memory Networks [44, 25] setting.", "startOffset": 107, "endOffset": 115}, {"referenceID": 32, "context": "CNN Encoder: Given an input image It \u2208 R , the CNN encoders learn a mapping f : R \u2192 R , if we consider the output of fully-connected layers in standard ConvNet architectures [33].", "startOffset": 174, "endOffset": 178}, {"referenceID": 14, "context": "In this work we use modified version of an LSTM [15] unit, as proposed in [50] to implement g.", "startOffset": 48, "endOffset": 52}, {"referenceID": 49, "context": "In this work we use modified version of an LSTM [15] unit, as proposed in [50] to implement g.", "startOffset": 74, "endOffset": 78}, {"referenceID": 24, "context": "The model is built around a Key-Value Memory Network [25] with memory slots as vector pairs (k1, v1), .", "startOffset": 53, "endOffset": 57}, {"referenceID": 3, "context": "Implicitly this also helps, preserving high level information about motion in the video [4].", "startOffset": 88, "endOffset": 91}, {"referenceID": 47, "context": "Values (V ): Jointly learning visual-semantic embedding with supervisory signal only from annotated descriptions in Encoder-Decoder models is difficult [48].", "startOffset": 152, "endOffset": 156}, {"referenceID": 44, "context": "This could be obtained from any pretrained model which jointly models visual and semantic embedding for images [45, 16, 42].", "startOffset": 111, "endOffset": 123}, {"referenceID": 15, "context": "This could be obtained from any pretrained model which jointly models visual and semantic embedding for images [45, 16, 42].", "startOffset": 111, "endOffset": 123}, {"referenceID": 41, "context": "This could be obtained from any pretrained model which jointly models visual and semantic embedding for images [45, 16, 42].", "startOffset": 111, "endOffset": 123}, {"referenceID": 15, "context": "In this work we use a pretrained DenseCap model [16], to obtain semantic embedding corresponding to every image.", "startOffset": 48, "endOffset": 52}, {"referenceID": 4, "context": "Vanilla RNNs are difficult to train for long range dependencies as they suffer from the vanishing gradient problem [5].", "startOffset": 115, "endOffset": 118}, {"referenceID": 14, "context": "Thus, we use Long Short Term Memory(LSTM) [15] as they are known to memorize context for longer period of time using controllable memory units.", "startOffset": 42, "endOffset": 46}, {"referenceID": 44, "context": "Soft attention mechanism have recently been successful in Image Captioning[45] and Video Description Generation [47] tasks because they help to focus on relevant parts of the features rather than mean pooling.", "startOffset": 74, "endOffset": 78}, {"referenceID": 46, "context": "Soft attention mechanism have recently been successful in Image Captioning[45] and Video Description Generation [47] tasks because they help to focus on relevant parts of the features rather than mean pooling.", "startOffset": 112, "endOffset": 116}, {"referenceID": 40, "context": "In [41], the feature vectors were simply averaged, leading to loss of the temporal relationships between frames of the video.", "startOffset": 3, "endOffset": 7}, {"referenceID": 24, "context": "They were also used in the Key-Value Memory Networks[25] to focus on more relevant keys and reads the weighted sum of values.", "startOffset": 52, "endOffset": 56}, {"referenceID": 6, "context": "Youtube2Text The proposed approach is benchmarked on the Youtube2Text[7] dataset which consists of 1,970 Youtube videos with multiple descriptions annotated through Amazon Mechanical Turk.", "startOffset": 69, "endOffset": 72}, {"referenceID": 46, "context": "The training, validation and test sets have 1,200, 100 and 670 videos respectively which is exactly the same splits as in previous work on video captioning [47, 4, 27].", "startOffset": 156, "endOffset": 167}, {"referenceID": 3, "context": "The training, validation and test sets have 1,200, 100 and 670 videos respectively which is exactly the same splits as in previous work on video captioning [47, 4, 27].", "startOffset": 156, "endOffset": 167}, {"referenceID": 26, "context": "The training, validation and test sets have 1,200, 100 and 670 videos respectively which is exactly the same splits as in previous work on video captioning [47, 4, 27].", "startOffset": 156, "endOffset": 167}, {"referenceID": 32, "context": "Key-Value Memories We select 28 equally spaced frames and pass them through a pretrained VGG-16[33] and GoogleNet[38] because of their state of the art performance in object detection on Imagenet[12] database.", "startOffset": 95, "endOffset": 99}, {"referenceID": 37, "context": "Key-Value Memories We select 28 equally spaced frames and pass them through a pretrained VGG-16[33] and GoogleNet[38] because of their state of the art performance in object detection on Imagenet[12] database.", "startOffset": 113, "endOffset": 117}, {"referenceID": 11, "context": "Key-Value Memories We select 28 equally spaced frames and pass them through a pretrained VGG-16[33] and GoogleNet[38] because of their state of the art performance in object detection on Imagenet[12] database.", "startOffset": 195, "endOffset": 199}, {"referenceID": 46, "context": "[47]) 0.", "startOffset": 0, "endOffset": 4}, {"referenceID": 26, "context": "[27]) 0.", "startOffset": 0, "endOffset": 4}, {"referenceID": 46, "context": "[47]) 0.", "startOffset": 0, "endOffset": 4}, {"referenceID": 48, "context": "[49] 0.", "startOffset": 0, "endOffset": 4}, {"referenceID": 39, "context": "[40]) .", "startOffset": 0, "endOffset": 4}, {"referenceID": 3, "context": "[4]) 0.", "startOffset": 0, "endOffset": 3}, {"referenceID": 44, "context": "The values are generated from a pre-trained image captioning module following [45, 16], which identifies salient regions in an image, and generates a caption for each of these regions.", "startOffset": 78, "endOffset": 86}, {"referenceID": 15, "context": "The values are generated from a pre-trained image captioning module following [45, 16], which identifies salient regions in an image, and generates a caption for each of these regions.", "startOffset": 78, "endOffset": 86}, {"referenceID": 23, "context": "Preprocessing: The video descriptions are tokenized using the wordpunct tokenizer from the NLTK toolbox[24].", "startOffset": 103, "endOffset": 107}, {"referenceID": 32, "context": "VGG-Encoder uses features encoded from the last convolution layer in VGG-16 CNN [33] along with a LSTM decoder.", "startOffset": 80, "endOffset": 84}, {"referenceID": 24, "context": "Here, we define q = ht\u22121 for the key addressing following [25].", "startOffset": 58, "endOffset": 62}, {"referenceID": 27, "context": "[28] explicitly learn a visual-semantic joint embedding model for exploiting the relationship between visual features and generated language, which is then used in a similar encoder-decoder framework.", "startOffset": 0, "endOffset": 4}, {"referenceID": 46, "context": "[47] utilizes a temporal attention mechanism for global attention apart from local attention using 3-D Convolution Networks.", "startOffset": 0, "endOffset": 4}, {"referenceID": 3, "context": "[4] proposed an encoder to learn spatial-temporal features across frames, introducing a variant GRU with convolution operations (GRU-RCN).", "startOffset": 0, "endOffset": 3}, {"referenceID": 48, "context": "[49] model the decoder as a paragraph generator, describing the videos over multiple sentences using stacked LSTMs.", "startOffset": 0, "endOffset": 4}, {"referenceID": 28, "context": "We evaluate our approach using standard evaluation metrics for video captioning tasks, namely BLEU [29], METEOR [22] and CIDEr[39] to compare the generated sequences with the human annotations.", "startOffset": 99, "endOffset": 103}, {"referenceID": 21, "context": "We evaluate our approach using standard evaluation metrics for video captioning tasks, namely BLEU [29], METEOR [22] and CIDEr[39] to compare the generated sequences with the human annotations.", "startOffset": 112, "endOffset": 116}, {"referenceID": 38, "context": "We evaluate our approach using standard evaluation metrics for video captioning tasks, namely BLEU [29], METEOR [22] and CIDEr[39] to compare the generated sequences with the human annotations.", "startOffset": 126, "endOffset": 130}, {"referenceID": 7, "context": "We use the code accompanying the Microsoft COCO Evaluation script [8] to obtain the results reported in the paper.", "startOffset": 66, "endOffset": 69}, {"referenceID": 50, "context": "We train our network parameters \u03b8 through first order stochastic gradient-based optimization with an adaptive learning rate using the Adadelta [51] optimizer.", "startOffset": 143, "endOffset": 147}, {"referenceID": 5, "context": "We optimize hyperparameters, namely number of hidden units in Decoder LSTM, key addressing LSTM, learning rate, word embedding dimension for the log loss using random search [6].", "startOffset": 174, "endOffset": 177}, {"referenceID": 32, "context": "We test our proposed model in the basic setting with visual features extracted from a pretrained VGG-16 [33] and GoogLeNet[38] models, as depicted by the first two lines of Table 1.", "startOffset": 104, "endOffset": 108}, {"referenceID": 37, "context": "We test our proposed model in the basic setting with visual features extracted from a pretrained VGG-16 [33] and GoogLeNet[38] models, as depicted by the first two lines of Table 1.", "startOffset": 122, "endOffset": 126}, {"referenceID": 24, "context": "The key addressing in these models, follows the basic approach as in [25], i.", "startOffset": 69, "endOffset": 73}, {"referenceID": 39, "context": "These models are able to outperform the baselines of S2VT [40], and the Basic Encoder-Decoder model introduced in [47] on all three metrics.", "startOffset": 58, "endOffset": 62}, {"referenceID": 46, "context": "These models are able to outperform the baselines of S2VT [40], and the Basic Encoder-Decoder model introduced in [47] on all three metrics.", "startOffset": 114, "endOffset": 118}, {"referenceID": 41, "context": "We observe that using features from pretrained GoogleNet rather than VGG-16 improves on the results, following [42].", "startOffset": 111, "endOffset": 115}, {"referenceID": 46, "context": "Using GoogleNet to extract features, we achieve significantly better results compared to the temporal attention introduced by [47], where they do not utilize attention distribution from previous time steps further in the model.", "startOffset": 126, "endOffset": 130}, {"referenceID": 46, "context": "Additionally, [47]", "startOffset": 14, "endOffset": 18}, {"referenceID": 26, "context": "This model, referred to as m-KeyAddressing, outperforms strong baselines from [27] by a significant margin on BLEU@4.", "startOffset": 78, "endOffset": 82}, {"referenceID": 26, "context": "While the improvements on METEOR are significant compared to tKeyAddressing, [27] learns a hierarchical representation of the video, thereby performing slightly better.", "startOffset": 77, "endOffset": 81}, {"referenceID": 48, "context": "In the current setting, our model is unable to outperform [49] which uses a stacked LSTM at the decoder and generates more granular descriptions.", "startOffset": 58, "endOffset": 62}, {"referenceID": 1, "context": "A possible approach for improvement in this direction is using more sophisticated regularizers for training the decoder, as proposed in [2, 21].", "startOffset": 136, "endOffset": 143}, {"referenceID": 20, "context": "A possible approach for improvement in this direction is using more sophisticated regularizers for training the decoder, as proposed in [2, 21].", "startOffset": 136, "endOffset": 143}, {"referenceID": 3, "context": "It is to be noted, that we do not finetune the encoder, or the image captioning module as compared to [4] which finetunes the encoder CNN on UCF101 action recognition set[34].", "startOffset": 102, "endOffset": 105}, {"referenceID": 33, "context": "It is to be noted, that we do not finetune the encoder, or the image captioning module as compared to [4] which finetunes the encoder CNN on UCF101 action recognition set[34].", "startOffset": 170, "endOffset": 174}, {"referenceID": 48, "context": "as in [49], [4], [40].", "startOffset": 6, "endOffset": 10}, {"referenceID": 3, "context": "as in [49], [4], [40].", "startOffset": 12, "endOffset": 15}, {"referenceID": 39, "context": "as in [49], [4], [40].", "startOffset": 17, "endOffset": 21}], "year": 2017, "abstractText": "Deep Neural Network architectures with external memory components allow the model to perform inference and capture long term dependencies, by storing information explicitly. In this paper, we generalize Key-Value Memory Networks to a multimodal setting, introducing a novel keyaddressing mechanism to deal with sequence-to-sequence models. The advantages of the framework are demonstrated on the task of video captioning, i.e generating natural language descriptions for videos. Conditioning on the previous time-step attention distributions for the key-value memory slots, we introduce a temporal structure in the memory addressing schema. The proposed model naturally decomposes the problem of video captioning into vision and language segments, dealing with them as key-value pairs. More specifically, we learn a semantic embedding (v) corresponding to each frame (k) in the video, thereby creating (k, v) memory slots. This allows us to exploit the temporal dependencies at multiple hierarchies (in the recurrent keyaddressing; and in the language decoder). Exploiting this flexibility of the framework, we additionally capture spatial dependencies while mapping from the visual to semantic embedding. Extensive experiments on the Youtube2Text dataset demonstrate usefulness of recurrent key-addressing, while achieving competitive scores on BLEU@4, METEOR metrics against state-of-the-art models.", "creator": "LaTeX with hyperref package"}}}