{"id": "1611.06530", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Nov-2016", "title": "Prototypical Recurrent Unit", "abstract": "The difficulty in analyzing LSTM-like recurrent neural networks lies in the complex structure of the recurrent unit, which induces highly complex nonlinear dynamics. In this paper, we design a new simple recurrent unit, which we call Prototypical Recurrent Unit (PRU). We verify experimentally that PRU performs comparably to LSTM and GRU. This potentially enables PRU to be a prototypical example for analytic study of LSTM-like recurrent networks. Along these experiments, the memorization capability of LSTM-like networks is also studied and some insights are obtained.", "histories": [["v1", "Sun, 20 Nov 2016 15:39:43 GMT  (388kb)", "http://arxiv.org/abs/1611.06530v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["dingkun long", "richong zhang", "yongyi mao"], "accepted": false, "id": "1611.06530"}, "pdf": {"name": "1611.06530.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Dingkun Long", "Richong Zhang", "Yongyi Mao"], "emails": ["zhangrc}@act.buaa.edu.cn,", "yymao@eecs.uottawa.ca"], "sections": [{"heading": null, "text": "ar Xiv: 161 1,06 530v 1 [cs.L G] 20 N"}, {"heading": "Introduction", "text": "This year, it is only a matter of time before an agreement is reached."}, {"heading": "State-Space Representations", "text": "In fact, most of them are able to determine for themselves what they want and what they want."}, {"heading": "Prototypical Recurrent Unit (PRU)", "text": "In view of the fact that there is no loss of expressiveness in the type II representation to arrive at a simplified recursive unit, we will remain within this unit. That is, for some predetermined decisions of the vector spaces X, Y and S, we will design two functions: F: X \u00b7 S and G: S \u2192 S for (X, Y, S, F, G) II. It is our hope that the designed recursive unit will capture the essence of a recursive unit in LSTM and GRU networks, but remain as simple as possible. From the previous literature [20], the following properties of LSTM and GRU are decisive for its effectiveness. The recursive unit behaves according to a nonlinear system in which nonlinearity is determined by the use of nonlinear activation functions such as sigmoid, tanh or ReLU functions.2. The evolution from state st to state st + 1 is additive."}, {"heading": "Experimental Study", "text": "Our experimental study serves two purposes. First, we would like to verify that the designed PRU behaves similarly to LSTM and GRU. To this end, experiments must be conducted not only for real-world applications where one has no control over the data sets, but also for certain meaningful tasks where we have full control over the data. Such controllable tasks will allow comparisons of these recurring units across arbitrary ranges of data parameter settings to fully demonstrate the performance of the compared recurring units and reduce the risk of being distorted by the statistics of a particular database. Second, we would like to take the opportunity to investigate a fundamental aspect of the recurring networks, namely their ability to remember. It has been experimentally observed and intuitively justified that the LSTM / GRU-like recurring unit has a \"long-term memory.\" [11] Motivated by such observations, we are interested in investigating the traceability of these recurring units."}, {"heading": "Memorization Problem", "text": "To describe this problem, we first imagine a \"memorizing machine\" Mmem that behaves as follows: \"For each given non-negative integer I and N, an input sequence x1, x2,.., xI + N of the scalar values are fed to the machine, where for t = 1, 2,.., I, xt takes a value in {+ 1, \u2212 1} each with the probability 1 / 2 and for t = 1, I + 2,., I + N, is drawn independently of a Gaussian distribution with zero mean and variance 2. After processing the input sequence, the machine generates an output vector (x1, x2,., xI) T of dimension I. That is, as a function, the machine Mmem behaves according to toMmem (x1, x2, xI + N)., xI, xI).T then in the function, the behavior of the machine Mmem.\""}, {"heading": "Adding Problem", "text": "To describe the problem, let Madd be an \"addition machine,\" which is a function that maps a length N sequence (x1, x2,.. xN) to a real number. Specifically, each xt, t = 1, 2,.., N, is a vector in R2, and we can write xt as (xt [1], xt [2]) T. At each t, xt [1] is a random value drawn with variance \u03b42 regardless of the zero-mean problem distribution; and in the sequence (x1], x2 [2], xN [2]), there are exactly two 1 whose locations are randomly assigned; the remaining values of the sequence are all equal to 0. The behavior of the addition machine is given by Madd (x1, x2,., xN): = N."}, {"heading": "Character Prediction Problem", "text": "Let Mchar be a \"character prediction machine,\" which takes an input sequence (x1, x2,.., xN) of any length N and produces an output sequence of equal length. The input sequence is fed to the machine one symbol per time unit, and at any time the machine is defined by a function M tchar, which is represented by M tchar (x1,.., xt): = xt + 1.That is, for each input sequence, the input sequence is shifted in time. Here, each symbol xt is a character in a K-letter alphabet. Each character in the alphabet is represented by a length K as a hot vector. The goal of the character prediction problem is then to train a model that simulates the behavior of Mchar. Modeling: Both X and Y are taken as RK in the models."}, {"heading": "Concluding Remarks", "text": "This work presents a new recurring entity, PRU. Due to its very simple structure, PRU is proving to be as powerful as LSTM and GRU, potentially allowing the use of 1https: / / github.com / karpaththy / char-rnnPRU as a prototype example for analyzing LSTM-like recurring networks. Its complexity advantage could also make it a practical alternative to LSTM and GRU. This work is just the beginning of a journey to understanding recurring networks. We hope that PRU can provide some comfort to this important endeavor."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio"], "venue": "Computer Science,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2014}, {"title": "Learning long-term dependencies with gradient descent is difficult", "author": ["Yoshua Bengio", "Patrice Simard", "Paolo Frasconi"], "venue": "IEEE transactions on neural networks,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1994}, {"title": "Long short-term memorynetworks for machine reading", "author": ["Jianpeng Cheng", "Li Dong", "Mirella Lapata"], "venue": "arXiv preprint arXiv:1601.06733,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2016}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "author": ["Kyunghyun Cho", "Bart Van Merri\u00ebnboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2014}, {"title": "Empirical evaluation of gated recurrent neural networks on sequence modeling", "author": ["Junyoung Chung", "Caglar Gulcehre", "KyungHyun Cho", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1412.3555,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2014}, {"title": "Distributed representations, simple recurrent networks, and grammatical structure", "author": ["Jeffrey L Elman"], "venue": "Machine learning,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1991}, {"title": "Long short-term memory in recurrent neural networks", "author": ["Felix Gers"], "venue": "PhD thesis, Universita\u0308t Hannover,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2001}, {"title": "Learning to forget: Continual prediction with lstm", "author": ["Felix A Gers", "J\u00fcrgen Schmidhuber", "Fred Cummins"], "venue": "Neural computation,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2000}, {"title": "Learning precise timing with lstm recurrent networks", "author": ["Felix A Gers", "Nicol N Schraudolph", "J\u00fcrgen Schmidhuber"], "venue": "Journal of machine learning research,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2002}, {"title": "Speech recognition with deep recurrent neural networks", "author": ["Alex Graves", "Abdel-rahman Mohamed", "Geoffrey Hinton"], "venue": "IEEE international conference on acoustics, speech and signal processing,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2013}, {"title": "Lstm: A search space odyssey", "author": ["K Greff", "R.K. Srivastava", "J Koutnik", "B.R. Steunebrink", "J Schmidhuber"], "venue": "IEEE Transactions on Neural Networks and Learning Systems,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2015}, {"title": "Reducing the dimensionality of data with neural networks", "author": ["G.E. Hinton", "R.R. Salakhutdinov"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2006}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1997}, {"title": "An empirical exploration of recurrent network architectures", "author": ["Rafal Jozefowicz", "Wojciech Zaremba", "Ilya Sutskever"], "venue": "In Proceedings of The 32nd International Conference on Machine Learning,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2015}, {"title": "Nonlinear Systems. Prentice-Hall, Englewood Cliffs, NJ", "author": ["H.K. Khalil"], "venue": "3nd edition,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2002}, {"title": "Characteraware neural language models", "author": ["Yoon Kim", "Yacine Jernite", "David Sontag", "Alexander M Rush"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2016}, {"title": "Learning longer memory in recurrent neural networks", "author": ["Tomas Mikolov", "Armand Joulin", "Sumit Chopra", "Michael Mathieu", "Marc\u2019Aurelio Ranzato"], "venue": "arXiv preprint arXiv:1412.7753,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2014}, {"title": "Recurrent neural network based language model", "author": ["Tomas Mikolov", "Martin Karafi\u00e1t", "Lukas Burget", "Jan Cernock\u1ef3", "Sanjeev Khudanpur"], "venue": "In Interspeech,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2010}, {"title": "On the difficulty of training recurrent neural networks", "author": ["Razvan Pascanu", "Tomas Mikolov", "Yoshua Bengio"], "venue": "ICML (3),", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2013}, {"title": "Building end-to-end dialogue systems using generative hierarchical neural network models", "author": ["Iulian V Serban", "Alessandro Sordoni", "Yoshua Bengio", "Aaron Courville", "Joelle Pineau"], "venue": "In Proceedings of the 30th AAAI Conference on Artificial Intelligence", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2016}, {"title": "Unsupervised learning of video representations using lstms", "author": ["Nitish Srivastava", "Elman Mansimov", "Ruslan Salakhutdinov"], "venue": "CoRR, abs/1502.04681,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2015}, {"title": "Pixel recurrent neural networks", "author": ["Aaron van den Oord", "Nal Kalchbrenner", "Koray Kavukcuoglu"], "venue": "arXiv preprint arXiv:1601.06759,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2016}, {"title": "Adadelta: an adaptive learning rate method", "author": ["Matthew D Zeiler"], "venue": "arXiv preprint arXiv:1212.5701,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2012}], "referenceMentions": [{"referenceID": 11, "context": ", [12, 17]).", "startOffset": 2, "endOffset": 10}, {"referenceID": 3, "context": "This work is motivated by the thrust of understanding recurrent neural networks, particularly LSTM/GRU-like networks [4, 8, 9, 13, 23].", "startOffset": 117, "endOffset": 134}, {"referenceID": 7, "context": "This work is motivated by the thrust of understanding recurrent neural networks, particularly LSTM/GRU-like networks [4, 8, 9, 13, 23].", "startOffset": 117, "endOffset": 134}, {"referenceID": 8, "context": "This work is motivated by the thrust of understanding recurrent neural networks, particularly LSTM/GRU-like networks [4, 8, 9, 13, 23].", "startOffset": 117, "endOffset": 134}, {"referenceID": 12, "context": "This work is motivated by the thrust of understanding recurrent neural networks, particularly LSTM/GRU-like networks [4, 8, 9, 13, 23].", "startOffset": 117, "endOffset": 134}, {"referenceID": 21, "context": "This work is motivated by the thrust of understanding recurrent neural networks, particularly LSTM/GRU-like networks [4, 8, 9, 13, 23].", "startOffset": 117, "endOffset": 134}, {"referenceID": 0, "context": "These networks have demonstrated to be the state-of-the-art models for time series or sequence data [1, 10, 21].", "startOffset": 100, "endOffset": 111}, {"referenceID": 9, "context": "These networks have demonstrated to be the state-of-the-art models for time series or sequence data [1, 10, 21].", "startOffset": 100, "endOffset": 111}, {"referenceID": 19, "context": "These networks have demonstrated to be the state-of-the-art models for time series or sequence data [1, 10, 21].", "startOffset": 100, "endOffset": 111}, {"referenceID": 2, "context": ", [3] [22]).", "startOffset": 2, "endOffset": 5}, {"referenceID": 20, "context": ", [3] [22]).", "startOffset": 6, "endOffset": 10}, {"referenceID": 6, "context": "A particularly interesting observation regarding these networks is that they appear to possess \u201clong-term memory\u201d, namely, being able to selectively \u201cremember\u201d the information from many time steps ago [7].", "startOffset": 201, "endOffset": 204}, {"referenceID": 4, "context": "Insights from previous research suggest that additive evolution appear essential for LSTM-like networks to avoid the \u201cgradient-vanishing\u201d problem under back-propagation [5,14,18].", "startOffset": 169, "endOffset": 178}, {"referenceID": 13, "context": "Insights from previous research suggest that additive evolution appear essential for LSTM-like networks to avoid the \u201cgradient-vanishing\u201d problem under back-propagation [5,14,18].", "startOffset": 169, "endOffset": 178}, {"referenceID": 16, "context": "Insights from previous research suggest that additive evolution appear essential for LSTM-like networks to avoid the \u201cgradient-vanishing\u201d problem under back-propagation [5,14,18].", "startOffset": 169, "endOffset": 178}, {"referenceID": 17, "context": "Using these three kinds of recurrent unit, we not only experiment on constructing a standard language model for character prediction [19], but also test the recurrent units for two controlled learning tasks, the Adding Problem [13], and the Memorization Problem.", "startOffset": 133, "endOffset": 137}, {"referenceID": 12, "context": "Using these three kinds of recurrent unit, we not only experiment on constructing a standard language model for character prediction [19], but also test the recurrent units for two controlled learning tasks, the Adding Problem [13], and the Memorization Problem.", "startOffset": 227, "endOffset": 231}, {"referenceID": 14, "context": "In system theory [15], a (discrete-time) system can be understood as any physical or conceptual device that responds to an input sequence x1, x2, .", "startOffset": 17, "endOffset": 21}, {"referenceID": 14, "context": "causal systems [15].", "startOffset": 15, "endOffset": 19}, {"referenceID": 5, "context": "It is easy to verify that the recurrent unit in RNN [6], LSTM and GRU networks can all be expressed this way.", "startOffset": 52, "endOffset": 55}, {"referenceID": 18, "context": "From the previous literature [20], the following properties of LSTM and GRU appear crucial for their effectiveness.", "startOffset": 29, "endOffset": 33}, {"referenceID": 10, "context": "It has been experimentally observed and intuitively justified that LSTM/GRU-like recurrent unit has \u201clong-term memory\u201d [11].", "startOffset": 119, "endOffset": 123}, {"referenceID": 17, "context": "The Character Prediction Problem is a well-known problem in the real-world application domain [19] [16].", "startOffset": 94, "endOffset": 98}, {"referenceID": 15, "context": "The Character Prediction Problem is a well-known problem in the real-world application domain [19] [16].", "startOffset": 99, "endOffset": 103}, {"referenceID": 12, "context": "The Adding Problem is a controllable task, first introduced in [13].", "startOffset": 63, "endOffset": 67}, {"referenceID": 1, "context": "The Memorization Problem is also a controllable task that we introduce in this work, inspired by the idea of a similar task presented in [2].", "startOffset": 137, "endOffset": 140}, {"referenceID": 0, "context": ", N , is a vector in R, and we may write xt as (xt[1], xt[2])T .", "startOffset": 50, "endOffset": 53}, {"referenceID": 1, "context": ", N , is a vector in R, and we may write xt as (xt[1], xt[2])T .", "startOffset": 57, "endOffset": 60}, {"referenceID": 0, "context": "At each t, xt[1] is a random value drawn independently from the zero-mean Gaussian distribution with variance \u03b4; and in the sequence (x1[2], x2[2], .", "startOffset": 13, "endOffset": 16}, {"referenceID": 1, "context": "At each t, xt[1] is a random value drawn independently from the zero-mean Gaussian distribution with variance \u03b4; and in the sequence (x1[2], x2[2], .", "startOffset": 136, "endOffset": 139}, {"referenceID": 1, "context": "At each t, xt[1] is a random value drawn independently from the zero-mean Gaussian distribution with variance \u03b4; and in the sequence (x1[2], x2[2], .", "startOffset": 143, "endOffset": 146}, {"referenceID": 1, "context": ", xN [2]), there are exactly two 1\u2019s, the locations of which are randomly assigned; the remaining values of the sequence all are equal to 0.", "startOffset": 5, "endOffset": 8}, {"referenceID": 0, "context": "t=1 xt[1] \u00b7 xt[2].", "startOffset": 6, "endOffset": 9}, {"referenceID": 1, "context": "t=1 xt[1] \u00b7 xt[2].", "startOffset": 14, "endOffset": 17}, {"referenceID": 22, "context": "Mini-batched SGD with Adadelta dynamic learning rate [24] is used for optimization.", "startOffset": 53, "endOffset": 57}], "year": 2016, "abstractText": "The difficulty in analyzing LSTM-like recurrent neural networks lies in the complex structure of the recurrent unit, which induces highly complex nonlinear dynamics. In this paper, we design a new simple recurrent unit, which we call Prototypical Recurrent Unit (PRU). We verify experimentally that PRU performs comparably to LSTM and GRU. This potentially enables PRU to be a prototypical example for analytic study of LSTM-like recurrent networks. Along these experiments, the memorization capability of LSTM-like networks is also studied and some insights are obtained.", "creator": "LaTeX with hyperref package"}}}