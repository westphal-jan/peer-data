{"id": "1611.06539", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Nov-2016", "title": "Efficient Stochastic Inference of Bitwise Deep Neural Networks", "abstract": "Recently published methods enable training of bitwise neural networks which allow reduced representation of down to a single bit per weight. We present a method that exploits ensemble decisions based on multiple stochastically sampled network models to increase performance figures of bitwise neural networks in terms of classification accuracy at inference. Our experiments with the CIFAR-10 and GTSRB datasets show that the performance of such network ensembles surpasses the performance of the high-precision base model. With this technique we achieve 5.81% best classification error on CIFAR-10 test set using bitwise networks. Concerning inference on embedded systems we evaluate these bitwise networks using a hardware efficient stochastic rounding procedure. Our work contributes to efficient embedded bitwise neural networks.", "histories": [["v1", "Sun, 20 Nov 2016 16:05:07 GMT  (307kb,D)", "http://arxiv.org/abs/1611.06539v1", "6 pages, 3 figures, Workshop on Efficient Methods for Deep Neural Networks at Neural Information Processing Systems Conference 2016, NIPS 2016, EMDNN 2016"]], "COMMENTS": "6 pages, 3 figures, Workshop on Efficient Methods for Deep Neural Networks at Neural Information Processing Systems Conference 2016, NIPS 2016, EMDNN 2016", "reviews": [], "SUBJECTS": "cs.NE cs.LG", "authors": ["sebastian vogel", "christoph schorn", "re guntoro", "gerd ascheid"], "accepted": false, "id": "1611.06539"}, "pdf": {"name": "1611.06539.pdf", "metadata": {"source": "CRF", "title": "Efficient Stochastic Inference of Bitwise Deep Neural Networks", "authors": ["Sebastian Vogel", "Robert Bosch", "Christoph Schorn", "Gerd Ascheid"], "emails": ["sebastian.vogel@de.bosch.com", "christoph.schorn@de.bosch.com", "andre.guntoro@de.bosch.com", "ascheid@ice.rwth-aachen.de"], "sections": [{"heading": "1 Introduction", "text": "Research in recent years has shown tremendous progress in solving complex problems through deep learning approaches. Classification tasks based on image data in particular have been a major goal for deep neural networks (DNNs) [8, 14]. One challenge to harnessing the strengths of deep learning methods in embedded systems is their enormous computing costs. Even relatively small DNNs often require millions of parameters and billions of operations to perform a single classification. Model compression approaches can help ease storage requirements and reduce the number of operations required for DNNs. While some approaches take into account specific network topologies [8, 11] another stream of research focuses on reducing the precision of model parameters. Recent publications of bitter neural networks (BNNNs) have shown that network weights and activations can be reduced from a high-precision floating point to a binary representation."}, {"heading": "2 Related Work", "text": "Some recent studies have shown that weights (and activations) of DNNs can be discredited to a very small number of quantization levels while maintaining a high classification performance [1, 4, 5, 10, 12, 15, 16] using a method already outlined by [6]. For each iteration of the back-propagation learning algorithm, the high-precision weights of the network are projected onto discredited values, using the discrete weights to calculate weight updates based on the weight profile, which are then applied to the high-precision weights. This method can be used either as a fine-tuning step for several epochs after regular training [1, 10, 12] or since the start of training [4, 5, 15, 16]. [4] Recently, clipping followed by stochastic rounding has been introduced as a method to project high-precision to binary (-1, + 1) weights."}, {"heading": "3 Stochastic Inference", "text": "Our methods are based on neural networks trained with stochastic weight projections. In this section, we show that by applying these projections at test time, a stochastic ensemble of BNs can be created whose aggregate classification power exceeds that of the underlying high-precision floating-point model, while preserving the benefits of bitwise and multiplication-free calculations."}, {"heading": "3.1 Stochastic Network Ensembles", "text": "Depending on the number of discrete values, we speak of binary or ternary network weights. Clipping limits the numerical range of weights to the interval [\u2212 1, 1] and the projection W 7 \u2192 W d is done by stochastic rounding: sround (w) = dwe, with the probability p = 0 bwc \u2212 wbwc \u2212 dwe, with the probability 1 \u2212 p = 0 dwe \u2212 dwe, with the probability 1. (1) The best test results in [4] were obtained with the high-precision neural network parameters W. Discretized values, however, are much more suitable for dedicated hardware accelerators, which is why we investigate inferences based on W d. One approach is to perform inferences at test times with the same weight discretization as in the training method. The reason for this is that the network has been optimized for these projections by minimizing the variation of the classification W 1 in the group."}, {"heading": "3.2 Experimental Results", "text": "For the first evaluation of our method, which ConvNet has developed on the basis of CIFAR-10 classification datasets, which contains 60 000 images in 32 x 32 pixels RGB resolution and 10 different classes, we use the setup described in the training, but with the activation of extremities and tusks. The network structure is 128C3-128C3-256C3-256C3-512C3-1024FC-1024C4-000C4-000C3-000C3-000C3-000C3-000C3-000C3-000C3-000C3-000C3-000C3-000C3-000C3-000C3-000C3-000C3-000C3-000C3-000C3-000C3-000C3-000C3-000C3-000C3-000C3-000C3-000C3-000C3-000C3-000C3-000C3-000C3-000C3-000C3-000C3-00000C3-000C3-000C3-00000C3-000C3-0000000-000000000-00000-00000-00000-00000-00000-0000000-00000-00000-00000-0000000-00000-0000000-00000-0000000-00000-00000-00000-00000-00000-00000-00000-00000-00000-00000-00000-00000-00000-00000-00000-00000-00000-00000-00000-00000-00000-00000-00000-00000-00000-00000-00000-00000-00000-00000-00000-00000-00000-00000-00000-00000-00000-00000-00000-00000 0000"}, {"heading": "4 Efficient Stochastic Rounding in Hardware", "text": "In order to fully exploit the performance of bitwise neural networks in terms of accuracy, the BNN must be evaluated more than once and therefore an efficient integration of a stochastic rounding machine is necessary. Based on the publications [2] and [3] a simple multiplexer can be used to perform sround (x) (see Eqn. (1)) Suppose the probability of the selected signal of an N-to-1 multiplexer to the route signal ini (0, 1) to the output is evenly distributed, the probability that the output signal can be written from 1 is asP (out = 1) = N, the i = 1 iniP (sel = i) = N, i = 1 ini N. (2) Hence, the probability P (out = 1) is determined by the number of inputs at the input."}, {"heading": "5 Conclusion and Outlook", "text": "We studied bitwise neural networks with stochastically projected weights during inference. Results show that an ensemble-based decision of multiple versions of such a BNN improves performance compared to inference based on high-precision shadow weights. Furthermore, we presented for the first time a hardware-efficient stochastic rounding method used on bitwise DNNs. Our results show that this technique can be used for test time conferences that allow efficient hardware implementation in embedded systems.The methods proposed in [4] and [5] are based on stochastic projections during training. Future research will examine the integration of our generalized form of stochastic rounding into the training process."}], "references": [{"title": "Fixed point optimization of deep convolutional neural networks for object recognition", "author": ["S. Anwar", "K. Hwang", "W. Sung"], "venue": "In 2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2015}, {"title": "Lookup table based neural network using fpga", "author": ["S.L. Bade"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1994}, {"title": "Fpga-based stochastic neural networks-implementation", "author": ["S.L. Bade", "B.L. Hutchings"], "venue": "In Proceedings of the IEEE Workshop on FPGAs for Custom Computing Machines", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1994}, {"title": "BinaryConnect: Training Deep Neural Networks with binary weights during propagations", "author": ["M. Courbariaux", "Y. Bengio", "J.-P. David"], "venue": "ArXiv e-prints,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2015}, {"title": "Binarized Neural Networks: Training Deep Neural Networks with Weights and Activations Constrained to +1 or -1", "author": ["M. Courbariaux", "I. Hubara", "D. Soudry", "R. El-Yaniv", "Y. Bengio"], "venue": "ArXiv e-prints,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2016}, {"title": "Weight discretization paradigm for optical neural networks", "author": ["E. Fiesler", "A. Choudry", "H.J. Caulfield"], "venue": "In Proceedings of SPIE,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1990}, {"title": "Deep Learning with Limited Numerical Precision", "author": ["S. Gupta", "A. Agrawal", "K. Gopalakrishnan", "P. Narayanan"], "venue": "In Proceedings of The 32nd International Conference on Machine Learning,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2015}, {"title": "Deep Residual Learning for Image Recognition", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "ArXiv e-prints, December", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2015}, {"title": "Deep Networks with Stochastic Depth", "author": ["G. Huang", "Y. Sun", "Z. Liu", "D. Sedra", "K. Weinberger"], "venue": "ArXiv e-prints, March 2016", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2016}, {"title": "Fixed-point feed forward deep neural network design using weights +1, 0 and -1", "author": ["K. Hwang", "W. Sung"], "venue": "IEEE Workshop on Signal Processing Systems (SiPS),", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2014}, {"title": "SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and <0.5MB model size", "author": ["F.N. Iandola", "S. Han", "M.W. Moskewicz", "K. Ashraf", "W.J. Dally", "K. Keutzer"], "venue": "ArXiv e-prints,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2016}, {"title": "X1000 real-time phoneme recognition vlsi using feed-forward deep neural networks", "author": ["J. Kim", "K. Hwang", "W. Sung"], "venue": "In 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2014}, {"title": "Learning Multiple Layers of Features from Tiny Images", "author": ["A. Krizhevsky"], "venue": "Technical report,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2009}, {"title": "ImageNet Classification with Deep Convolutional Neural Networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2012}, {"title": "Deep neural networks are robust to weight binarization and other non-linear distortions", "author": ["P. Merolla", "R. Appuswamy", "J. Arthur", "S.K. Esser", "D. Modha"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2016}, {"title": "XNOR-Net: ImageNet Classification Using Binary Convolutional Neural Networks", "author": ["M. Rastegari", "V. Ordonez", "J. Redmon", "A. Farhadi"], "venue": "ArXiv e-prints, March 2016", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2016}, {"title": "The German Traffic Sign Recognition Benchmark: A multi-class classification competition", "author": ["J. Stallkamp", "M. Schlipsing", "J. Salmen", "C. Igel"], "venue": "IEEE International Joint Conference on Neural Networks,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2011}, {"title": "Device for generating binary sequences for stochastic computing", "author": ["M. van Daalen", "P. Jeavons", "J. Shawe-Taylor", "D. Cohen"], "venue": "In Electronics Letters,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1993}], "referenceMentions": [{"referenceID": 7, "context": "Especially classification tasks based on image data have been a major target for deep neural networks (DNNs) [8, 14].", "startOffset": 109, "endOffset": 116}, {"referenceID": 13, "context": "Especially classification tasks based on image data have been a major target for deep neural networks (DNNs) [8, 14].", "startOffset": 109, "endOffset": 116}, {"referenceID": 7, "context": "While some approaches consider special network topologies [8, 11], another stream of research focuses on precision reduction of the model parameters.", "startOffset": 58, "endOffset": 65}, {"referenceID": 10, "context": "While some approaches consider special network topologies [8, 11], another stream of research focuses on precision reduction of the model parameters.", "startOffset": 58, "endOffset": 65}, {"referenceID": 4, "context": "Recent publications of bitwise neural networks (BNNs) have shown that network weights and activations can be reduced from a high-precision floating-point down to a binary representation, while maintaining classification accuracy on benchmark datasets [5].", "startOffset": 251, "endOffset": 254}, {"referenceID": 3, "context": "which employed this training method have so far only analyzed deterministic projections during test-time [4, 5, 15].", "startOffset": 105, "endOffset": 115}, {"referenceID": 4, "context": "which employed this training method have so far only analyzed deterministic projections during test-time [4, 5, 15].", "startOffset": 105, "endOffset": 115}, {"referenceID": 14, "context": "which employed this training method have so far only analyzed deterministic projections during test-time [4, 5, 15].", "startOffset": 105, "endOffset": 115}, {"referenceID": 0, "context": "Some recent studies have shown that weights (and activations) of DNNs can be discretized to a very low number of quantization levels while maintaining high classification performance [1, 4, 5, 10, 12, 15, 16].", "startOffset": 183, "endOffset": 208}, {"referenceID": 3, "context": "Some recent studies have shown that weights (and activations) of DNNs can be discretized to a very low number of quantization levels while maintaining high classification performance [1, 4, 5, 10, 12, 15, 16].", "startOffset": 183, "endOffset": 208}, {"referenceID": 4, "context": "Some recent studies have shown that weights (and activations) of DNNs can be discretized to a very low number of quantization levels while maintaining high classification performance [1, 4, 5, 10, 12, 15, 16].", "startOffset": 183, "endOffset": 208}, {"referenceID": 9, "context": "Some recent studies have shown that weights (and activations) of DNNs can be discretized to a very low number of quantization levels while maintaining high classification performance [1, 4, 5, 10, 12, 15, 16].", "startOffset": 183, "endOffset": 208}, {"referenceID": 11, "context": "Some recent studies have shown that weights (and activations) of DNNs can be discretized to a very low number of quantization levels while maintaining high classification performance [1, 4, 5, 10, 12, 15, 16].", "startOffset": 183, "endOffset": 208}, {"referenceID": 14, "context": "Some recent studies have shown that weights (and activations) of DNNs can be discretized to a very low number of quantization levels while maintaining high classification performance [1, 4, 5, 10, 12, 15, 16].", "startOffset": 183, "endOffset": 208}, {"referenceID": 15, "context": "Some recent studies have shown that weights (and activations) of DNNs can be discretized to a very low number of quantization levels while maintaining high classification performance [1, 4, 5, 10, 12, 15, 16].", "startOffset": 183, "endOffset": 208}, {"referenceID": 5, "context": "They employ a method which has already been sketched out by [6].", "startOffset": 60, "endOffset": 63}, {"referenceID": 0, "context": "This method can be used either as a fine-tuning step for several epochs after regular training [1, 10, 12] or from the beginning of the training [4, 5, 15, 16].", "startOffset": 95, "endOffset": 106}, {"referenceID": 9, "context": "This method can be used either as a fine-tuning step for several epochs after regular training [1, 10, 12] or from the beginning of the training [4, 5, 15, 16].", "startOffset": 95, "endOffset": 106}, {"referenceID": 11, "context": "This method can be used either as a fine-tuning step for several epochs after regular training [1, 10, 12] or from the beginning of the training [4, 5, 15, 16].", "startOffset": 95, "endOffset": 106}, {"referenceID": 3, "context": "This method can be used either as a fine-tuning step for several epochs after regular training [1, 10, 12] or from the beginning of the training [4, 5, 15, 16].", "startOffset": 145, "endOffset": 159}, {"referenceID": 4, "context": "This method can be used either as a fine-tuning step for several epochs after regular training [1, 10, 12] or from the beginning of the training [4, 5, 15, 16].", "startOffset": 145, "endOffset": 159}, {"referenceID": 14, "context": "This method can be used either as a fine-tuning step for several epochs after regular training [1, 10, 12] or from the beginning of the training [4, 5, 15, 16].", "startOffset": 145, "endOffset": 159}, {"referenceID": 15, "context": "This method can be used either as a fine-tuning step for several epochs after regular training [1, 10, 12] or from the beginning of the training [4, 5, 15, 16].", "startOffset": 145, "endOffset": 159}, {"referenceID": 3, "context": "[4] has recently introduced clipping followed by stochastic rounding as a method for projecting high-precision to binary (-1, +1) weights.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "Before, [7] used a similar method but with a relatively large number of discretization levels and presented a neural network hardware accelerator using multiply-accumulate-units for stochastic rounding.", "startOffset": 8, "endOffset": 11}, {"referenceID": 3, "context": "We employ the method introduced in [4] during training and inference.", "startOffset": 35, "endOffset": 38}, {"referenceID": 3, "context": "Best test-time results in [4] were achieved with the high-precision neural network parameters W .", "startOffset": 26, "endOffset": 29}, {"referenceID": 12, "context": "For the first evaluation of our method, we train a ConvNet on the CIFAR-10 classification dataset [13], which contains 60 000 images in 32\u00d732 pixel RGB resolution and 10 different classes.", "startOffset": 98, "endOffset": 102}, {"referenceID": 3, "context": "the setup described in [4] for training, but with sign3 activation function as in [5] and stochastic ternary weights.", "startOffset": 23, "endOffset": 26}, {"referenceID": 4, "context": "the setup described in [4] for training, but with sign3 activation function as in [5] and stochastic ternary weights.", "startOffset": 82, "endOffset": 85}, {"referenceID": 3, "context": "After training the model for 500 epochs with hyperparameters from [4] and without any preprocessing or augmentations on the dataset, we select high-precision model parameters which have the lowest error on the validation set.", "startOffset": 66, "endOffset": 69}, {"referenceID": 8, "context": "We apply a commonly used simple data augmentation method [9], consisting of a random translation of up to 4 pixels in the image plane and a random flip around the vertical axis.", "startOffset": 57, "endOffset": 60}, {"referenceID": 16, "context": "In addition, we test our method on the German Traffic Sign Recognition Benchmark dataset [17].", "startOffset": 89, "endOffset": 93}, {"referenceID": 1, "context": "Based on the publications [2] and [3], a simple multiplexer can be used to perform sround(x) (see Eqn.", "startOffset": 26, "endOffset": 29}, {"referenceID": 2, "context": "Based on the publications [2] and [3], a simple multiplexer can be used to perform sround(x) (see Eqn.", "startOffset": 34, "endOffset": 37}, {"referenceID": 1, "context": "Whereas [2] considers the N equations (3) as an overdetermined problem and proposes a numerical solution, we present an analytic solution to the problem.", "startOffset": 8, "endOffset": 11}, {"referenceID": 17, "context": "Bitstreams for selj with the corresponding frequencies can be generated using a linear feedback shift register (LFSR) in combination with Daalen modulators [18].", "startOffset": 156, "endOffset": 160}, {"referenceID": 17, "context": "Moreover, the mean network performance is preserved when only a single LFSR is used to generate a random base bitstream which is then subject to different modulations [18] to generate PRBS with appropriate frequencies of 1\u2019s (see Eqn.", "startOffset": 167, "endOffset": 171}], "year": 2016, "abstractText": "Recently published methods enable training of bitwise neural networks which allow reduced representation of down to a single bit per weight. We present a method that exploits ensemble decisions based on multiple stochastically sampled network models to increase performance figures of bitwise neural networks in terms of classification accuracy at inference. Our experiments with the CIFAR-10 and GTSRB datasets show that the performance of such network ensembles surpasses the performance of the high-precision base model. With this technique we achieve 5.81% best classification error on CIFAR-10 test set using bitwise networks. Concerning inference on embedded systems we evaluate these bitwise networks using a hardware efficient stochastic rounding procedure. Our work contributes to efficient embedded bitwise neural networks.", "creator": "LaTeX with hyperref package"}}}