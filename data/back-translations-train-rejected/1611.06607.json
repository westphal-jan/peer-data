{"id": "1611.06607", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Nov-2016", "title": "A Hierarchical Approach for Generating Descriptive Image Paragraphs", "abstract": "Recent progress on image captioning has made it possible to generate novel sentences describing images in natural language, but compressing an image into a single sentence can describe visual content in only coarse detail. While one new captioning approach, dense captioning, can potentially describe images in finer levels of detail by captioning many regions within an image, it in turn is unable to produce a coherent story for an image. In this paper we overcome these limitations by generating entire paragraphs for describing images, which can tell detailed, unified stories. We develop a model that decomposes both images and paragraphs into their constituent parts, detecting semantic regions in images and using a hierarchical recurrent neural network to reason about language. Linguistic analysis confirms the complexity of the paragraph generation task, and thorough experiments on a new dataset of image and paragraph pairs demonstrate the effectiveness of our approach.", "histories": [["v1", "Sun, 20 Nov 2016 23:10:51 GMT  (1782kb,D)", "http://arxiv.org/abs/1611.06607v1", null], ["v2", "Mon, 10 Apr 2017 17:59:15 GMT  (1784kb,D)", "http://arxiv.org/abs/1611.06607v2", "CVPR 2017 spotlight"]], "reviews": [], "SUBJECTS": "cs.CV cs.CL", "authors": ["jonathan krause", "justin johnson", "ranjay krishna", "li fei-fei"], "accepted": false, "id": "1611.06607"}, "pdf": {"name": "1611.06607.pdf", "metadata": {"source": "CRF", "title": "A Hierarchical Approach for Generating Descriptive Image Paragraphs", "authors": ["Jonathan Krause", "Justin Johnson", "Ranjay Krishna", "Li Fei-Fei"], "emails": ["jkrause@cs.stanford.edu", "jcjohns@cs.stanford.edu", "ranjaykrishna@cs.stanford.edu", "feifeili@cs.stanford.edu"], "sections": [{"heading": "1. Introduction", "text": "In fact, most of the cases mentioned are \"unforeseen,\" in which there is an \"unforeseen\" situation, in which there is an \"unforeseen\" situation."}, {"heading": "2. Related Work", "text": "In fact, most of them are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move and to move."}, {"heading": "3. Paragraphs are Different", "text": "To what extent does the description of pictures with paragraphs differ from the sentence-level caption? To roughly describe these sentences, we have collected a novel set of paragraph comments, compared with 19,551 MS COCO [19] and Visual Genome [15] illustrations, in which each picture was commented with a paragraph description. Comments were collected on Amazon Mechanical Turk by comparing our collected paragraph comments with the five associated sentence headings from MS COCO. Although it is clear that the paragraph is longer and more descriptive than each sentence, we further point out that a single paragraph can be more detailed than all five sentence headings, even when combined."}, {"heading": "4. Method", "text": "Our model takes an image as input and creates a natural-language paragraph that describes it, and is designed to exploit the compositional structure of both the images and the paragraphs. Fig. 2 provides an overview. We first break down the input image by recognizing objects and other interesting regions, and then aggregate features in these regions to produce a bundled representation that comprehensively expresses image semantics. This feature vector is used as input by a hierarchically recurring neural network consisting of two levels: a sentence RNN and a word RNN. The sentence RNN receives the image attributes, determines how many sentences are to be generated in the resulting paragraph, and generates an input topic vector for each sentence. In light of this topic vector, the word RNN generates the words of a single sentence. We also show how to transfer knowledge from a dense caption task [10] to our model for paragraph generation."}, {"heading": "4.1. Region Detector", "text": "The region detector receives an input image of size 3 \u00b7 H \u00b7 W, detects areas of interest and generates a characteristic vector of dimension D = 4096 for each region. Our region detector follows the design of [24, 10], and here, for the sake of completeness, we give a summary: The image is reduced so that its longest edge is 720 pixels, and is then guided through a revolutionary network initialized from the 16-layer VGG network [25]. The resulting characteristic map is processed by a region suggestion network [24], which regresses from a series of anchors to suggest interesting regions. These interesting regions are projected onto the evolutionary characteristic map, and the corresponding region of the characteristic map is reshaped to a fixed size using two-dimensional interpolation and processed by two completely connected layers to provide a vector of dimension D for each region."}, {"heading": "4.2. Region Pooling", "text": "The region detector generates a series of vectors v1,.., vM, RD, each describing a different region in the input image. We would like to aggregate these vectors into a single pooled vector vp, vM, RP, which describes the content of the image in a compact way. To this end, we learn a projection matrix Wpool, RP, D and bias bpool, RP; the pooled vector vp is calculated by projecting each region vector using a wpool and taking an elementary maximum, so that vp = max M is i = 1 (Wpoolvi + bpool). Alternative approaches to representing collections of regions, such as spatial attention [28], may also be possible and supplement the model proposed in this paper."}, {"heading": "4.3. Hierarchical Recurrent Network", "text": "ieD nree\u00fcgBnlrhe\u00fccn rf\u00fc ide rf\u00fc ide rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rfu the rfu the rfu the rfu the rfu the rfu the rfu the rfu the rfu the rfu the rfu the rfu the rfu the rfu the rfu the rfu the rfu the rfu"}, {"heading": "4.4. Training and Sampling", "text": "Training data consists of pairs (x, y), where x is an image and y is a basic truth paragraph description for this image, where y has S sentences, the ith sentence has Ni words and yij is the jest word of the ith sentence. After calculating the summarized region vp for the image, we roll out the sentence RNN for S time lapse and output a distribution pi for each sentence over the {CONTINUE, STOP} states. We feed the sentence topic vectors to S copies of the word RNN, unroll the ith copy for Ni time lapse, and produce a distribution pi over each word of each sentence. Our training loss' (x, y) for the example (x, y) is a weighted sum of two cross-entropy terms: a sentence loss' sent after the stopping distribution unit pi, and a word loss' after the choice of the word loss' pij 'word': 'S-word' (x = 1) after the word sensed (x, p = 1)."}, {"heading": "4.5. Transfer Learning", "text": "Transfer learning has become ubiquitous in computer vision. For tasks such as object recognition [24] and image labels [5, 11, 27, 28], it has become standard practice not only to process images with Convolutionary Neural Networks, but also to initialize the weights of these networks from weights optimized for image classification, such as the 16-layer VGG network [25]. Initialization from a pre-formed Convolutionary Network enables a form of knowledge transfer from large classification datasets and is particularly effective for data sets of limited size. Could transfer learning also be useful for paragraph generation? We propose to use transfer learning in two ways. First, we initialize our region recognition network from a model designed for dense image labels [10]; although our model is end-to-end differentiable, we retain this sub-network during the training for both efficiency processing and we have been able to avoid the overpass parameters of the N."}, {"heading": "5. Experiments", "text": "In this section we describe our paragraph generation experiments on the collected data described in paragraph 3, which we divide into 14,575 training, 2,487 validation and 2,489 test images."}, {"heading": "5.1. Baselines", "text": "Sentence-Concat: To demonstrate the difference between sentence level and paragraph captions, this is baseline examples, linking five sentence descriptions from a model [11] that best describes the image as a whole (according to the model), and using subsequent sentences to generate a diverse range of sentences, the motivation for this is as follows: The image entertainment model first produces the sentence that describes the image as a whole (according to the model), and use the subsequent sentences to repeat the same sentence from the beam. We have confirmed that this approach works better than using just one beam search or just sampling, as the intention is to make the strongest possible comparison on a standard caption. We also point out that while Sentence-Concat is trained on MS COCO, all images are written in our dataset."}, {"heading": "5.2. Implementation Details", "text": "All neural language models are based on two levels of LSTM [8] units with 512 dimensions. The feature pooling dimension P is 1024, and we set \u03bbsent = 5.0 and \u03bbword = 1.0 based on validation performance. Training is done using stochastic gradient pedigree with Adam [13] learning rate updates implemented in Torch. Crucially, selecting the checkpoint model is based on the best combined METEOR and CIDER values of the validation set - although models tend to minimize validation losses relatively quickly, it takes much longer for the values for METEOR and CIDER to stop improving."}, {"heading": "5.3. Main Results", "text": "We present our most important results in generating paragraphs in Tab. 2, which are evaluated using six language metrics: CIDER [26], METEOR [4], and BLEU- {1,2,3,4} [23]. The Sentence-Concat method performs poorly, reaching the lowest values in all metrics. Its lackluster performance provides further evidence of the stark differences between individual sentence descriptions and the paragraph generation. Surprisingly, the persistent approach performs relatively well, especially in CIDER, METEOR, and BLEU-1, where it competes with some of the neural approaches. This makes sense: the approach is provided with a strong presence of image content, as it is received as input at regional level, and the many explicit \"it is / are\" statements that they make, although they are uninteresting, and leads to decent results."}, {"heading": "5.4. Qualitative Results", "text": "We present qualitative results from our model and the sentence-concat and template baselines in Fig. 3. Some interesting characteristics of our model's predictions are the use of co-reference in the first example (\"a bus,\" \"it,\" \"the bus\") and its ability to capture relationships between objects in the second example. Also, noteworthy is the order in which our model describes the image: the first sentence tends to be quite high, middle sentences give some details about the scene elements previously mentioned in the description, and the last sentence often describes something in the background that other methods cannot capture. Anecdotally, we observed that this follows the same order in which most of our human commentators tended to describe images. The failure in the last line highlights another interesting phenomenon: Although our model was wrong with regard to the semantics of the image in general and the girl referred to as \"a woman,\" she has learned that \"woman\" consistently \"rapes with her pronoun\" frequently, \"the phrase is associated with her."}, {"heading": "5.5. Paragraph Language Analysis", "text": "In order to shed a quantitative light on the linguistic phenomena generated, we present statistics on the language generated by a representative distribution of methods in Tab. 3. Our hierarchical approach generates texts of similar average length and variance to human descriptions, with Sentence-Concat and the template approach being somewhat shorter and less varied in length. Sentence-Concat is also the least varied method, although all automatic methods are far less varied than human sentences, suggesting ample scope for improvement. According to this diversity metric, it can be observed that the template approach is in fact the most varied automatic method, due to the fact that the method is hard-coded to describe each region of the scene one after the other, regardless of its importance or how interesting such an output can be. (see Fig. 3) While both our hierarchical approach and the template method produce text with similar proportions of nouns and verbs as human paragraphs, our COS approach was only capable of generating a COS, although a larger number of pronouns had a 19."}, {"heading": "5.6. Generating Paragraphs from Fewer Regions", "text": "As an exploratory experiment to highlight the interpretative capability of our model, we are investigating the generation of paragraphs from a smaller number of regions than the M = 50 used in the rest of the work. Instead, we are giving our method access to the few identified regions only as input, hoping that the generated paragraph will focus only on those regions and prefer not to describe other parts of the image. Results for a handful of images are shown in Fig. 4, although the input is extremely outside the sample compared to training data, the results are still quite reasonable - the model generates paragraphs in which the detected regions are described without much mention of objects or landscapes outside the detection areas. In the example of the upper right image, despite some linguistic errors, the paragraph generated by our model mentions dough, catchers, dirt, and grass, all of which appear in the regions detected above but do not pay much attention to the pitcher or the referee in the background."}, {"heading": "6. Conclusion", "text": "In this paper, we introduced the task of describing images with long descriptive paragraphs, and presented a hierarchical neural approach to generation that effectively utilizes the compositional structure of images and language. We demonstrated that paragraph generation is different from traditional image captioning, and tailored our model to these differences. We demonstrated experimentally the advantages of our approach over traditional image captioning methods, and demonstrated how knowledge can be effectively transferred to the task of subtitling at the regional level. We also demonstrated the advantages of our model in terms of interpretability, and demonstrated how descriptive paragraphs can be generated with only a subset of image regions. We expect further opportunities for knowledge transfer in tasks at the interface of vision and language, and project that visual and linguistic compositionality will continue to be at the center of effective paragraph generation."}], "references": [{"title": "Listen", "author": ["W. Chan", "N. Jaitly", "Q.V. Le", "O. Vinyals"], "venue": "attend, and spell: A neural network for large vocabulary conversational speech recognition. In ICASSP", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2016}, {"title": "Tokensregex: Defining cascaded regular expressions over tokens", "author": ["A.X. Chang", "C.D. Manning"], "venue": "Technical report, CSTR 2014-02, Department of Computer Science, Stanford University", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2014}, {"title": "Mind\u2019s eye: A recurrent visual representation for image caption generation", "author": ["X. Chen", "C. Lawrence Zitnick"], "venue": "CVPR", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2015}, {"title": "Meteor universal: Language specific translation evaluation for any target language", "author": ["M. Denkowski", "A. Lavie"], "venue": "EACL Workshop on Statistical Machine Translation", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2014}, {"title": "Long-term recurrent convolutional networks for visual recognition and description", "author": ["J. Donahue", "L. Anne Hendricks", "S. Guadarrama", "M. Rohrbach", "S. Venugopalan", "K. Saenko", "T. Darrell"], "venue": "CVPR", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2015}, {"title": "Hierarchical recurrent neural networks for long-term dependencies", "author": ["S. El Hihi", "Y. Bengio"], "venue": "NIPS", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1995}, {"title": "Every picture tells a story: Generating sentences from images", "author": ["A. Farhadi", "M. Hejrati", "M.A. Sadeghi", "P. Young", "C. Rashtchian", "J. Hockenmaier", "D. Forsyth"], "venue": "ECCV", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2010}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural computation", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1997}, {"title": "Framing image description as a ranking task: Data", "author": ["M. Hodosh", "P. Young", "J. Hockenmaier"], "venue": "models and evaluation metrics. Journal of Artificial Intelligence Research, 47:853\u2013 899", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2013}, {"title": "DenseCap: Fully convolutional localization networks for dense captioning", "author": ["J. Johnson", "A. Karpathy", "L. Fei-Fei"], "venue": "CVPR", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2016}, {"title": "Deep visual-semantic alignments for generating image descriptions", "author": ["A. Karpathy", "L. Fei-Fei"], "venue": "CVPR", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep fragment embeddings for bidirectional image sentence mapping", "author": ["A. Karpathy", "A. Joulin", "L. Fei-Fei"], "venue": "NIPS", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2014}, {"title": "Adam: A method for stochastic optimization", "author": ["D. Kingma", "J. Ba"], "venue": "ICLR", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "A clockwork RNN", "author": ["J. Koutnik", "K. Greff", "F. Gomez", "J. Schmidhuber"], "venue": "ICML", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2014}, {"title": "Visual genome: Connecting language and vision using crowdsourced dense image annotations", "author": ["R. Krishna", "Y. Zhu", "O. Groth", "J. Johnson", "K. Hata", "J. Kravitz", "S. Chen", "Y. Kalantidis", "L.-J. Li", "D.A. Shamma", "M. Bernstein", "L. Fei-Fei"], "venue": "arXiv preprint arXiv:1602.07332", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2016}, {"title": "Baby talk: Understanding and generating image descriptions", "author": ["G. Kulkarni", "V. Premraj", "S. Dhar", "S. Li", "Y. Choi", "A.C. Berg", "T.L. Berg"], "venue": "CVPR", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2011}, {"title": "A hierarchical neural autoencoder for paragraphs and documents", "author": ["J. Li", "M.-T. Luong", "D. Jurafsky"], "venue": "ACL", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2015}, {"title": "Hierarchical recurrent neural network for document modeling", "author": ["R. Lin", "S. Liu", "M. Yang", "M. Li", "M. Zhou", "S. Li"], "venue": "EMNLP", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2015}, {"title": "Microsoft coco: Common objects in context", "author": ["T.-Y. Lin", "M. Maire", "S. Belongie", "J. Hays", "P. Perona", "D. Ramanan", "P. Doll\u00e1r", "C.L. Zitnick"], "venue": "ECCV", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2014}, {"title": "The stanford CoreNLP natural language processing toolkit", "author": ["C.D. Manning", "M. Surdeanu", "J. Bauer", "J.R. Finkel", "S. Bethard", "D. McClosky"], "venue": "ACL (System Demonstrations), pages 55\u201360", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep captioning with multimodal recurrent neural networks (m-RNN)", "author": ["J. Mao", "W. Xu", "Y. Yang", "J. Wang", "Z. Huang", "A. Yuille"], "venue": "ICLR", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2015}, {"title": "Building a large annotated corpus of english: The penn treebank", "author": ["M.P. Marcus", "M.A. Marcinkiewicz", "B. Santorini"], "venue": "Computational linguistics, 19(2):313\u2013330", "citeRegEx": "22", "shortCiteRegEx": null, "year": 1993}, {"title": "Bleu: a method for automatic evaluation of machine translation", "author": ["K. Papineni", "S. Roukos", "T. Ward", "W.-J. Zhu"], "venue": "ACL, pages 311\u2013318. Association for Computational Linguistics", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2002}, {"title": "Faster r-cnn: Towards real-time object detection with region proposal networks", "author": ["S. Ren", "K. He", "R. Girshick", "J. Sun"], "venue": "NIPS", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2015}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "ICLR", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2015}, {"title": "CIDEr: Consensus-based image description evaluation", "author": ["R. Vedantam", "C. Lawrence Zitnick", "D. Parikh"], "venue": "CVPR", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2015}, {"title": "Show and tell: A neural image caption generator", "author": ["O. Vinyals", "A. Toshev", "S. Bengio", "D. Erhan"], "venue": "CVPR", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2015}, {"title": "Show", "author": ["K. Xu", "J. Ba", "R. Kiros", "K. Cho", "A. Courville", "R. Salakhudinov", "Y. Bengio"], "venue": "attend, and tell: Neural image caption generation with visual attention. In ICML", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2015}, {"title": "Describing videos by exploiting temporal structure", "author": ["L. Yao", "A. Torabi", "K. Cho", "N. Ballas", "C. Pal", "H. Larochelle", "A. Courville"], "venue": "ICCV", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2015}, {"title": "Image captioning with semantic attention", "author": ["Q. You", "H. Jin", "Z. Wang", "C. Fang", "J. Luo"], "venue": "CVPR", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2016}, {"title": "From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions", "author": ["P. Young", "A. Lai", "M. Hodosh", "J. Hockenmaier"], "venue": "Transactions of the Association for Computational Linguistics, 2:67\u201378", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2014}, {"title": "Video paragraph captioning using hierarchical recurrent neural networks", "author": ["H. Yu", "J. Wang", "Z. Huang", "Y. Yang", "W. Xu"], "venue": "CVPR", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 18, "context": "With the advent of large datasets pairing images with natural language descriptions [19, 31, 9, 15] it has recently become possible to generate novel sentences describing images [3, 5, 11, 21, 27].", "startOffset": 84, "endOffset": 99}, {"referenceID": 30, "context": "With the advent of large datasets pairing images with natural language descriptions [19, 31, 9, 15] it has recently become possible to generate novel sentences describing images [3, 5, 11, 21, 27].", "startOffset": 84, "endOffset": 99}, {"referenceID": 8, "context": "With the advent of large datasets pairing images with natural language descriptions [19, 31, 9, 15] it has recently become possible to generate novel sentences describing images [3, 5, 11, 21, 27].", "startOffset": 84, "endOffset": 99}, {"referenceID": 14, "context": "With the advent of large datasets pairing images with natural language descriptions [19, 31, 9, 15] it has recently become possible to generate novel sentences describing images [3, 5, 11, 21, 27].", "startOffset": 84, "endOffset": 99}, {"referenceID": 2, "context": "With the advent of large datasets pairing images with natural language descriptions [19, 31, 9, 15] it has recently become possible to generate novel sentences describing images [3, 5, 11, 21, 27].", "startOffset": 178, "endOffset": 196}, {"referenceID": 4, "context": "With the advent of large datasets pairing images with natural language descriptions [19, 31, 9, 15] it has recently become possible to generate novel sentences describing images [3, 5, 11, 21, 27].", "startOffset": 178, "endOffset": 196}, {"referenceID": 10, "context": "With the advent of large datasets pairing images with natural language descriptions [19, 31, 9, 15] it has recently become possible to generate novel sentences describing images [3, 5, 11, 21, 27].", "startOffset": 178, "endOffset": 196}, {"referenceID": 20, "context": "With the advent of large datasets pairing images with natural language descriptions [19, 31, 9, 15] it has recently become possible to generate novel sentences describing images [3, 5, 11, 21, 27].", "startOffset": 178, "endOffset": 196}, {"referenceID": 26, "context": "With the advent of large datasets pairing images with natural language descriptions [19, 31, 9, 15] it has recently become possible to generate novel sentences describing images [3, 5, 11, 21, 27].", "startOffset": 178, "endOffset": 196}, {"referenceID": 9, "context": "One recent alternative to sentence-level captioning is the task of dense captioning [10], which overcomes this limitation by detecting many regions of interest in an image and describing each with a short phrase.", "startOffset": 84, "endOffset": 88}, {"referenceID": 18, "context": "Here we show an image with its sentence-level captions from MS COCO [19] (top) and the paragraph used in this work (bottom).", "startOffset": 68, "endOffset": 72}, {"referenceID": 14, "context": "In addition, we also demonstrate for the first time the ability to transfer visual and linguistic knowledge from large-scale region captioning [15], which we show has the ability to improve paragraph generation.", "startOffset": 143, "endOffset": 147}, {"referenceID": 18, "context": "To validate our method, we collected a dataset of image and paragraph pairs, which complements the whole-image and region-level annotations of MS COCO [19] and Visual Genome [15].", "startOffset": 151, "endOffset": 155}, {"referenceID": 14, "context": "To validate our method, we collected a dataset of image and paragraph pairs, which complements the whole-image and region-level annotations of MS COCO [19] and Visual Genome [15].", "startOffset": 174, "endOffset": 178}, {"referenceID": 6, "context": "One line of work treats the problem as a ranking task, using images to retrieve relevant captions from a database and vice-versa [7, 9, 12].", "startOffset": 129, "endOffset": 139}, {"referenceID": 8, "context": "One line of work treats the problem as a ranking task, using images to retrieve relevant captions from a database and vice-versa [7, 9, 12].", "startOffset": 129, "endOffset": 139}, {"referenceID": 11, "context": "One line of work treats the problem as a ranking task, using images to retrieve relevant captions from a database and vice-versa [7, 9, 12].", "startOffset": 129, "endOffset": 139}, {"referenceID": 15, "context": "Early work uses handwritten templates to generate language [16] while more recent methods train recurrent neural network language models conditioned on image features [3, 5, 11, 21, 27, 30] and sample from them to generate text.", "startOffset": 59, "endOffset": 63}, {"referenceID": 2, "context": "Early work uses handwritten templates to generate language [16] while more recent methods train recurrent neural network language models conditioned on image features [3, 5, 11, 21, 27, 30] and sample from them to generate text.", "startOffset": 167, "endOffset": 189}, {"referenceID": 4, "context": "Early work uses handwritten templates to generate language [16] while more recent methods train recurrent neural network language models conditioned on image features [3, 5, 11, 21, 27, 30] and sample from them to generate text.", "startOffset": 167, "endOffset": 189}, {"referenceID": 10, "context": "Early work uses handwritten templates to generate language [16] while more recent methods train recurrent neural network language models conditioned on image features [3, 5, 11, 21, 27, 30] and sample from them to generate text.", "startOffset": 167, "endOffset": 189}, {"referenceID": 20, "context": "Early work uses handwritten templates to generate language [16] while more recent methods train recurrent neural network language models conditioned on image features [3, 5, 11, 21, 27, 30] and sample from them to generate text.", "startOffset": 167, "endOffset": 189}, {"referenceID": 26, "context": "Early work uses handwritten templates to generate language [16] while more recent methods train recurrent neural network language models conditioned on image features [3, 5, 11, 21, 27, 30] and sample from them to generate text.", "startOffset": 167, "endOffset": 189}, {"referenceID": 29, "context": "Early work uses handwritten templates to generate language [16] while more recent methods train recurrent neural network language models conditioned on image features [3, 5, 11, 21, 27, 30] and sample from them to generate text.", "startOffset": 167, "endOffset": 189}, {"referenceID": 4, "context": "Similar methods have also been applied to generate captions for videos [5, 29, 32].", "startOffset": 71, "endOffset": 82}, {"referenceID": 28, "context": "Similar methods have also been applied to generate captions for videos [5, 29, 32].", "startOffset": 71, "endOffset": 82}, {"referenceID": 31, "context": "Similar methods have also been applied to generate captions for videos [5, 29, 32].", "startOffset": 71, "endOffset": 82}, {"referenceID": 27, "context": "[28] generate captions using a recurrent network with attention, so that for each generated word the model produces a distribution over image regions.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "Karpathy and Fei-Fei [11] use a ranking loss to align image regions with sentence fragments but do not do generation with the model.", "startOffset": 21, "endOffset": 25}, {"referenceID": 9, "context": "[10] introdue the task of dense captioning, which detects and describes regions of interest, but these descriptions are independent and do not form a coherent whole.", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "Alternative recurrent architectures such as long-short term memory (LSTM) [8] help alleviate this problem through a gating mechanism that improves gradient flow.", "startOffset": 74, "endOffset": 77}, {"referenceID": 5, "context": "Early pioneering work applied hierarchical recurrent networks to simple algorithmic problems [6].", "startOffset": 93, "endOffset": 96}, {"referenceID": 13, "context": "The Clockwork RNN [14] uses a related technique for audio signal generation, spoken word classification, and handwriting recognition; a similar hierarchical architecture was also used in [1] for speech recognition.", "startOffset": 18, "endOffset": 22}, {"referenceID": 0, "context": "The Clockwork RNN [14] uses a related technique for audio signal generation, spoken word classification, and handwriting recognition; a similar hierarchical architecture was also used in [1] for speech recognition.", "startOffset": 187, "endOffset": 190}, {"referenceID": 16, "context": "[17] introduce a hierarchical autoencoder, and Lin et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "[18] use different recurrent units to model sentences and words.", "startOffset": 0, "endOffset": 4}, {"referenceID": 31, "context": "[32], who generate multisentence descriptions for cooking videos using a different hierarchical model.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "Sentences COCO [19] Paragraphs Ours", "startOffset": 15, "endOffset": 19}, {"referenceID": 21, "context": "between sentences of the same image, and part of speech distributions are aggregated from Penn Treebank [22] part of speech tags.", "startOffset": 104, "endOffset": 108}, {"referenceID": 18, "context": "tion, we collected a novel dataset of paragraph annotations, comparised of 19,551 MS COCO [19] and Visual Genome [15] images, where each image has been annotated with a paragraph description.", "startOffset": 90, "endOffset": 94}, {"referenceID": 14, "context": "tion, we collected a novel dataset of paragraph annotations, comparised of 19,551 MS COCO [19] and Visual Genome [15] images, where each image has been annotated with a paragraph description.", "startOffset": 113, "endOffset": 117}, {"referenceID": 25, "context": "To examine the issue of sentence diversity, we compute the average CIDEr [26] similarity between COCO sentences for each image and between the individual sentences in each collected paragraph, defining the final diversity score as 100 minus the average CIDEr similarity.", "startOffset": 73, "endOffset": 77}, {"referenceID": 19, "context": "Diving deeper, we performed a simple linguistic analysis on COCO sentences and our collected paragraphs, comprised of annotating each word with a part of speech tag from Penn Treebank via Stanford CoreNLP [20] and aggregating parts of speech into higher-level linguistic categories.", "startOffset": 205, "endOffset": 209}, {"referenceID": 9, "context": "We also show how to transfer knowledge from a dense image captioning [10] task to our model for paragraph generation.", "startOffset": 69, "endOffset": 73}, {"referenceID": 23, "context": "Our region detector follows the design of [24, 10], and we provide a summary here for completeness: The image is resized so that its longest edge is 720 pixels, and is then passed through a convolutional network initialized from the 16-layer VGG network [25].", "startOffset": 42, "endOffset": 50}, {"referenceID": 9, "context": "Our region detector follows the design of [24, 10], and we provide a summary here for completeness: The image is resized so that its longest edge is 720 pixels, and is then passed through a convolutional network initialized from the 16-layer VGG network [25].", "startOffset": 42, "endOffset": 50}, {"referenceID": 24, "context": "Our region detector follows the design of [24, 10], and we provide a summary here for completeness: The image is resized so that its longest edge is 720 pixels, and is then passed through a convolutional network initialized from the 16-layer VGG network [25].", "startOffset": 254, "endOffset": 258}, {"referenceID": 23, "context": "The resulting feature map is processed by a region proposal network [24], which regresses from a set of anchors to propose regions of interest.", "startOffset": 68, "endOffset": 72}, {"referenceID": 23, "context": "Given a dataset of images and ground-truth regions of interest, the region detector can be trained in an end-to-end fashion as in [24] for object detection and [10] for dense captioning.", "startOffset": 130, "endOffset": 134}, {"referenceID": 9, "context": "Given a dataset of images and ground-truth regions of interest, the region detector can be trained in an end-to-end fashion as in [24] for object detection and [10] for dense captioning.", "startOffset": 160, "endOffset": 164}, {"referenceID": 14, "context": "Since paragraph descriptions do not have annotated groundings to regions of interest, we use a region detector trained for dense image captioning on the Visual Genome dataset [15], using the publicly available implementation of [10].", "startOffset": 175, "endOffset": 179}, {"referenceID": 9, "context": "Since paragraph descriptions do not have annotated groundings to regions of interest, we use a region detector trained for dense image captioning on the Visual Genome dataset [15], using the publicly available implementation of [10].", "startOffset": 228, "endOffset": 232}, {"referenceID": 27, "context": "Alternative approaches for representing collections of regions, such as spatial attention [28], may also be possible, and are complementary to the model proposed in this paper.", "startOffset": 90, "endOffset": 94}, {"referenceID": 7, "context": "We adopt the standard LSTM architecture [8] for both the word RNN and the sentence RNN.", "startOffset": 40, "endOffset": 43}, {"referenceID": 26, "context": "We follow the input formulation of [27]: the first and second inputs to the RNN are the topic vector and a special START token, and subsequent inputs are learned embedding vectors for the words of the sentence.", "startOffset": 35, "endOffset": 39}, {"referenceID": 23, "context": "For tasks such as object detection [24] and image captioning [5, 11, 27, 28], it has become standard practice not only to process images with convolutional neural networks, but also to initialize the weights of these networks from weights that had been tuned for image classification, such as the 16-layer VGG network [25].", "startOffset": 35, "endOffset": 39}, {"referenceID": 4, "context": "For tasks such as object detection [24] and image captioning [5, 11, 27, 28], it has become standard practice not only to process images with convolutional neural networks, but also to initialize the weights of these networks from weights that had been tuned for image classification, such as the 16-layer VGG network [25].", "startOffset": 61, "endOffset": 76}, {"referenceID": 10, "context": "For tasks such as object detection [24] and image captioning [5, 11, 27, 28], it has become standard practice not only to process images with convolutional neural networks, but also to initialize the weights of these networks from weights that had been tuned for image classification, such as the 16-layer VGG network [25].", "startOffset": 61, "endOffset": 76}, {"referenceID": 26, "context": "For tasks such as object detection [24] and image captioning [5, 11, 27, 28], it has become standard practice not only to process images with convolutional neural networks, but also to initialize the weights of these networks from weights that had been tuned for image classification, such as the 16-layer VGG network [25].", "startOffset": 61, "endOffset": 76}, {"referenceID": 27, "context": "For tasks such as object detection [24] and image captioning [5, 11, 27, 28], it has become standard practice not only to process images with convolutional neural networks, but also to initialize the weights of these networks from weights that had been tuned for image classification, such as the 16-layer VGG network [25].", "startOffset": 61, "endOffset": 76}, {"referenceID": 24, "context": "For tasks such as object detection [24] and image captioning [5, 11, 27, 28], it has become standard practice not only to process images with convolutional neural networks, but also to initialize the weights of these networks from weights that had been tuned for image classification, such as the 16-layer VGG network [25].", "startOffset": 318, "endOffset": 322}, {"referenceID": 9, "context": "First, we initialize our region detection network from a model trained for dense image captioning [10]; although our model is end-to-end differentiable, we keep this sub-network fixed during training both for efficiency and also to prevent overfitting.", "startOffset": 98, "endOffset": 102}, {"referenceID": 9, "context": "Second, we can also initialize the word embedding vectors, recurrent network weights, and output linear projection of the word RNN from a language model that had been trained on region-level captions [10], fine-tuning these parameters during training to be better suited for the task of paragraph generation.", "startOffset": 200, "endOffset": 204}, {"referenceID": 14, "context": "This initialization strategy allows our model to utilize linguistic knowledge learned on large-scale region caption datasets [15] to produce better paragraph descriptions, and we validate the efficacy of this strategy in our experiments.", "startOffset": 125, "endOffset": 129}, {"referenceID": 10, "context": "Sentence-Concat: To demonstrate the difference between sentence-level and paragraph captions, this baseline samples and concatenates five sentence captions from a model [11] trained on MS COCO captions [19].", "startOffset": 169, "endOffset": 173}, {"referenceID": 18, "context": "Sentence-Concat: To demonstrate the difference between sentence-level and paragraph captions, this baseline samples and concatenates five sentence captions from a model [11] trained on MS COCO captions [19].", "startOffset": 202, "endOffset": 206}, {"referenceID": 10, "context": "Image-Flat: This model uses a flat representation for both images and language, and is equivalent to the standard image captioning model NeuralTalk [11].", "startOffset": 148, "endOffset": 152}, {"referenceID": 10, "context": "38 Image-Flat ([11]) 12.", "startOffset": 15, "endOffset": 19}, {"referenceID": 10, "context": "We use the publically available implementation of [11], which uses the 16-layer VGG network [25] to extract CNN features and projects them as input into an LSTM [8], training the whole model jointly end-to-end.", "startOffset": 50, "endOffset": 54}, {"referenceID": 24, "context": "We use the publically available implementation of [11], which uses the 16-layer VGG network [25] to extract CNN features and projects them as input into an LSTM [8], training the whole model jointly end-to-end.", "startOffset": 92, "endOffset": 96}, {"referenceID": 7, "context": "We use the publically available implementation of [11], which uses the 16-layer VGG network [25] to extract CNN features and projects them as input into an LSTM [8], training the whole model jointly end-to-end.", "startOffset": 161, "endOffset": 164}, {"referenceID": 15, "context": "Template: This method represents a very different approach to generating paragraphs, similar in style to an openworld version of more classical methods like BabyTalk [16], which converts a structured representation of an image into text via a handful of manually specified templates.", "startOffset": 166, "endOffset": 170}, {"referenceID": 9, "context": "The first step of our template-based baseline is to detect and describe many regions in a given target image using a pre-trained dense captioning model [10], which produces a set of region descriptions tied with bounding boxes and detection scores.", "startOffset": 152, "endOffset": 156}, {"referenceID": 1, "context": "The region descriptions are parsed into a set of subjects, verbs, objects, and various modifiers according to part of speech tagging and a handful of TokensRegex [2] rules, which we find suffice to parse the vast majority (\u2265 99%) of the fairly simplistic and short region-level descriptions.", "startOffset": 162, "endOffset": 165}, {"referenceID": 7, "context": "All baseline neural language models use two layers of LSTM [8] units with 512 dimensions.", "startOffset": 59, "endOffset": 62}, {"referenceID": 12, "context": "Training is done via stochastic gradient descent with Adam [13] learning rate updates, implemented in Torch.", "startOffset": 59, "endOffset": 63}, {"referenceID": 25, "context": "2, which are evaluated across six language metrics: CIDEr [26], METEOR [4], and BLEU-{1,2,3,4} [23].", "startOffset": 58, "endOffset": 62}, {"referenceID": 3, "context": "2, which are evaluated across six language metrics: CIDEr [26], METEOR [4], and BLEU-{1,2,3,4} [23].", "startOffset": 71, "endOffset": 74}, {"referenceID": 22, "context": "2, which are evaluated across six language metrics: CIDEr [26], METEOR [4], and BLEU-{1,2,3,4} [23].", "startOffset": 95, "endOffset": 99}, {"referenceID": 9, "context": "This makes sense: the template approach is provided with a strong prior about image content since it receives region-level captions [10] as input, and the many expletive \u201cthere is/are\u201d statements it makes, though uninteresting, are safe, resulting in decent scores.", "startOffset": 132, "endOffset": 136}, {"referenceID": 25, "context": "Of particular note is the large gap between humans our the best model on CIDEr and METEOR, which are both designed to correlate well with human judgment [26, 4].", "startOffset": 153, "endOffset": 160}, {"referenceID": 3, "context": "Of particular note is the large gap between humans our the best model on CIDEr and METEOR, which are both designed to correlate well with human judgment [26, 4].", "startOffset": 153, "endOffset": 160}, {"referenceID": 18, "context": "Our hierarchical method also had a much wider vocabulary compared to the Template approach, though Sentence-Concat, trained on hundreds of thousands of MS COCO [19] captions, is a bit larger.", "startOffset": 160, "endOffset": 164}], "year": 2016, "abstractText": "Recent progress on image captioning has made it possible to generate novel sentences describing images in natural language, but compressing an image into a single sentence can describe visual content in only coarse detail. While one new captioning approach, dense captioning, can potentially describe images in finer levels of detail by captioning many regions within an image, it in turn is unable to produce a coherent story for an image. In this paper we overcome these limitations by generating entire paragraphs for describing images, which can tell detailed, unified stories. We develop a model that decomposes both images and paragraphs into their constituent parts, detecting semantic regions in images and using a hierarchical recurrent neural network to reason about language. Linguistic analysis confirms the complexity of the paragraph generation task, and thorough experiments on a new dataset of image and paragraph pairs demonstrate the effectiveness of our approach.", "creator": "LaTeX with hyperref package"}}}