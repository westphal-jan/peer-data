{"id": "1611.06624", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Nov-2016", "title": "Temporal Generative Adversarial Nets with Singular Value Clipping", "abstract": "In this paper we propose a generative model, the Temporal Generative Adversarial Network (TGAN), which can learn a semantic representation of unlabelled videos, and is capable of generating consistent videos. Unlike an existing GAN that generates videos with a generator consisting of 3D deconvolutional layers, our model exploits two types of generators: a temporal generator and an image generator. The temporal generator consists of 1D deconvolutional layers and outputs a set of latent variables, each of which corresponds to a frame in the generated video, and the image generator transforms them into a video with 2D deconvolutional layers. This representation allows efficient training of the network parameters. Moreover, it can handle a wider range of applications including the generation of a long sequence, frame interpolation, and the use of pre-trained models. Experimental results demonstrate the effectiveness of our method.", "histories": [["v1", "Mon, 21 Nov 2016 01:10:50 GMT  (5743kb,D)", "http://arxiv.org/abs/1611.06624v1", null], ["v2", "Mon, 24 Jul 2017 01:33:45 GMT  (6972kb,D)", "http://arxiv.org/abs/1611.06624v2", null], ["v3", "Fri, 18 Aug 2017 02:32:16 GMT  (6573kb,D)", "http://arxiv.org/abs/1611.06624v3", "to appear in ICCV 2017"]], "reviews": [], "SUBJECTS": "cs.LG cs.CV", "authors": ["masaki saito", "eiichi matsumoto", "shunta saito"], "accepted": false, "id": "1611.06624"}, "pdf": {"name": "1611.06624.pdf", "metadata": {"source": "CRF", "title": "Temporal Generative Adversarial Nets", "authors": ["Masaki Saito", "Eiichi Matsumoto"], "emails": ["matsumoto}@preferred.jp"], "sections": [{"heading": "1. Introduction", "text": "In fact, it is only a matter of time before most of them will be able to decide whether or not they want to stay in the US, whether or not they want to stay in the US, or whether or not they want to stay in Europe."}, {"heading": "2. Related work", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1. Natural image generation", "text": "A common approach to image generation is the use of undirected graphic models such as Boltzmann machines [34, 19, 4]. However, due to the difficulty of approximating color gradients, it has been empirically observed that such deep graphic models often do not find a good representation of natural images with sufficient diversity. Gregor et al. [8] and Dosvotiskiy et al. [3] both proposed models that use recurring and devolutionary networks and successfully generate natural images. However, both models use monitored learning and require additional information such as labels. The Generative Adversarial Network (GAN) that we mainly used in this study is a model of unattended learning that finds good representations of samples by simultaneously training two different image networks that use Li and GAN networks for efficient years."}, {"heading": "2.2. Video recognition and unsupervised learning", "text": "In this context, it must be established that this is a phenomenon which is a phenomenon which is a phenomenon which is a phenomenon which is a phenomenon which is a phenomenon which is a phenomenon which is a phenomenon which is a phenomenon which is a phenomenon which is a phenomenon which is a phenomenon which is a phenomenon which is a phenomenon which is a phenomenon which is a phenomenon which is a phenomenon which is a phenomenon which is a phenomenon which is a phenomenon which is a phenomenon which is a phenomenon which is a phenomenon which is a phenomenon which is a phenomenon which is a phenomenon which is a phenomenon which is a phenomenon which is a phenomenon which is a phenomenon which is a phenomenon which is a phenomenon which is a phenomenon which is a phenomenon which is a phenomenon which is a phenomenon which is a phenomenon which is a phenomenon which is a phenomenon which is a phenomenon which is a phenomenon which is a phenomenon which is a phenomenon which is a phenomenon which is a phenomenon which is a phenomenon which is a phenomenon which is a phenomenon which is a phenomenon which is a phenomenon which is a phenomenon which is a phenomenon which is a phenomenon which is a phenomenon which is a phenomenon which is a phenomenon which is a phenomenon which is a phenomenon which is a phenomenon which is a particular, which is a phenomenon which is a phenomenon which is a certain, which is a certain, which is determined, which is a certain,"}, {"heading": "3. Temporal Generative Adversarial Nets", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1. Generative Adversarial Nets", "text": "Before going into the details of TGAN, we briefly explain the existing GANs [5]. A GAN uses two networks called generator and discriminator.The generator G1: RK1 \u2192 RM is a function that generates samples x-RM that resemble a sample in the given dataset. The input is a latent variable z1, where z1 is randomly drawn from a given distribution pG1 (z1), e.g. an even distribution. The discriminator D1: RM \u2192 [0, 1] is a classifier that distinguishes whether a given sample comes from the dataset or from G1.The GAN simultaneously trains the two networks by playing a non-cooperative game; the generator wins if it creates an image that the discriminator misclassifies, while the discriminator wins if it correctly classifies the input samples."}, {"heading": "3.2. Temporal GAN", "text": "Here we present our model based on the discussion above. Let T > 0 be the number of frames to be generated, and G0: RK0 \u2192 RT \u00b7 K1 be the temporal generator that receives another latent variable z0 \u0445 RK0 as argument and generates T latent variables, each of which corresponds to the argument of G1. In our model, z0 is randomly drawn from a distribution pG0 (z0). Note that the latent variable z1 is not randomly drawn from pG1, but is generated from the temporal generator. That is, to generate a video with T frames, our model first generates T latent variables labeled with [z11,., z T 1], and then converts them into video images with the image generator. These outputs are represented as [G1 (z11) to produce a video with T frames."}, {"heading": "3.3. Two-step training", "text": "In unattended learning of the image, the recently released DCGAN can successfully find a good correspondence between the latent space and the image space. By this method, we break down a complicated training process of the two networks mentioned above into the following two steps. First, we train the image generator G1 and the image discriminator D1 using a video dataset to correctly learn the image space formed by the video dataset. As an image dataset, we simply use all the images contained in the video dataset. Afterwards, we obtain presentsG1 and presentsD1 with equivalents (1). The training process of the two networks is the same as that of existing studies (e.g. [5, 30]). Secondly, with the trained two networks, we train the time generator G0 and the time discriminator D0 under the condition that almost all parameters in G1 and D1 are fixed. In the image generator, we can simply transfer all the learned parameters from G1 and the time discriminator D0 in each layer (1)."}, {"heading": "3.4. Network configuration", "text": "This subsection describes the configuration of our four networks: the temporal generator, the image generator, the image discriminator, the image discriminator, and the temporal discriminator. Table 1 shows a typical network environment. Note that the settings vary slightly depending on the dataset. Due to space constraints, we only consider the settings of the UCF-101 [36] and include the rest in the supplementary material. Temporal generator Unlike typical CNNs that perform two-dimensional convolutions in the spatial direction, the deconvolutionary layers in the temporal generator perform a one-dimensional conversion in the temporal direction. Temporal generators We use temporary image generators Unlike typical CNNs that perform two-dimensional convolutions in the spatial direction, the deconvolutionary layers in the temporal generator perform a one-dimensional conversion."}, {"heading": "4. Applications", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1. Frame interpolation", "text": "One of the advantages of our model is that we can create an intermediate frame between two adjacent images. Since the problem of video generation in our model is formulated as an estimate of a trajectory in latent space, our generator can easily produce long sequences in which an intermediate frame is naturally interpolated by merely interpolating the trajectory. Specifically, we interpolate the trajectory in latent space with the two-line filter that is the same in the last layer of the temporal generator (see Section 3.4)."}, {"heading": "4.2. Generating videos of arbitrary length", "text": "A second advantage is that our model can also append new additional frames with other movements, i.e. because the time generator does not contain fully connected layers, the generator can also generate videos of any length by inserting additional latent variables into the time generator and treating them as a characteristic chart. To this end, we redefine a latent variable as z0 = [z10,.. z N 0], which consists of N latent variables. The size of each element is the same as the previous one, i.e., z0-RN-K0. In this representation, we can also consider this latent variable as a characteristic chart with K0 channels. Since the time generator now consists of completely deconvolutionary layers, it can also produce more than F frames. To produce a long, consistent video, one previous motion and another in the generated frames can be semantically similar."}, {"heading": "4.3. Pre-trained models", "text": "A third advantage is that our model can use the existing pre-trained generator and the discriminator learned on another image dataset as an image generator and discriminator in our model. Although the advantage of using the pre-trained model is to speed up learning (i.e. skip the first training step), it also applies to the generation of various \"unknown\" frames that do not contain a specific video dataset. Through the experiments, we observed that there are advantages and disadvantages compared to using pre-trained network models (see Section 5.3.1)."}, {"heading": "5. Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1. Datasets", "text": "We conducted experiments with the following datasets. 3D character We first train our models on a simple video dataset consisting of 5,184 videos with 75 frames. This means that we used 388,800 frames in the training of the image generators. In this dataset, the 3D figure named Hatsune Miku [26] takes 16 different short-term actions such as waving hands and looking back.Since the number of movements is relatively low, we expanded the videos by evenly changing camera angles and eventually obtained 324 videos from one motion. All videos have a resolution of 64 x 64 pixels and were shot at 12 frames per second.Moving MNIST To investigate the characteristics of our models, we also trained the models on the movable MNIST dataset [37], in which each video consists of two digits moving within a 64 x 64 field. In these videos, two digits move linear and the direction and magnitude of the motion vectors are general."}, {"heading": "5.2. Training configuration", "text": "All parameters used in the Optimizer are the same as those of the original GAN. Specifically, we use the Adam Optimizer [17] with a learning rate of 0.0002 and pulse term \u03b21 of 0.5. All weights are initialized by a zero-centric Gaussian distribution with a standard deviation of 0.02. Note that in both training steps (i.e. training image networks and time networks), the above parameters are all the same. We used Chainer [43] for both implementation and training. We observed that the discriminator easily wins against the generator on the complicated datasets and the training cannot be continued. To avoid this, we added Gaussian noise to all levels of discriminators when using the Moving MNIST and UCF-101 datasets. All scale parameters after the batch normalization layer are not used."}, {"heading": "5.3. Qualitative evaluation", "text": "We trained our proposed model on the above datasets and visually confirmed the quality of the results. Fig.2 shows examples of videos generated by the generator trained on the 3D character dataset. Our model successfully creates plausible videos in which the character performs semantically significant actions. However, it should be noted that the generated frames in Fig.2 do not exist in the 3D character datasets. We also created smooth transitions between the two videos by linearly interpolating the two latent variables. The results are shown in Fig.3. Our model successfully generates plausible movements from the latent variables and does not simply store the existing movements from the datasets. The interesting property can be seen in Fig.2 we train and create the model on the moving MNIST dataset."}, {"heading": "5.3.1 Three applications", "text": "We performed the following experiments to demonstrate the effectiveness of the three applications described in Section 4.1. To show that our model can be applied to the interpolation of images, we created intermediate images by interpolating two adjacent latent variables of the image space. These results are shown in Fig.6. However, as it is evident that these images look natural and are plausible, we also applied our method to the character generator based on the 3D character set. In this case, \u03c32 was set to 0.1 and 10 in Equation (3) and the length of z0 respectively, which means that our model generated 160 images per video. We show the sample result in Fig.7; we confirmed that the longer sequences generated are consistent. Due to the constraints of space, we provide further results in the upstream Models 101, which we initially trained with trained dogs. \""}, {"heading": "5.4. Quantitative evaluation", "text": "To investigate what kind of characteristics are generated by each discriminator, we extracted two characteristic vectors from the image discriminator and the temporal discriminator and applied them to the problem of action detection. Specifically, we used the activations in the last layer of the image discriminator and the first layer of the temporal discriminator and constructed two classifiers that use a linear SVM with a triple cross-validation consisting of a group. However, for the evaluation of the image discriminator, we extracted 15 images per clip at regular intervals and decided the result according to the majority rule. As with the image discriminator, we extracted 10 short clips with 16 images per clip at regular intervals in the temporal discriminator. We compare this with the model proposed by Vondrick et al. [45].Table 2 shows quantitative results. You can see that the image discriminator achieves a performance equal to or greater than that of the existing model."}, {"heading": "6. Summary and future work", "text": "We propose a generative model that learns semantic representations of videos and can generate realistic sequences. To solve this challenging problem, i.e., unattended learning of videos, we formulate a generative process of videos as a pair of (i) a function that generates a set of latent variables that represent the frame space, and (ii) a function that converts them into a sequence. With this representation, our model can not only generate plausible videos, but also perform frame interpolation of the generated video and handle videos of any length. Through several experiments, we confirmed the usefulness of our models. Despite the promising results mentioned above, few problems remain. The first is the end-to-end training. We speculate that the quality of the frames generated in TGAN could benefit from an end-to-end training, but we are not yet able to perform such training in this manner successfully."}], "references": [], "referenceMentions": [], "year": 2016, "abstractText": "In this paper we propose a generative model, the Temporal Generative Adversarial Network (TGAN), which can learn a semantic representation of unlabelled videos, and is capable of generating consistent videos. Unlike an existing GAN that generates videos with a generator consisting of 3D deconvolutional layers, our model exploits two types of generators: a temporal generator and an image generator. The temporal generator consists of 1D deconvolutional layers and outputs a set of latent variables, each of which corresponds to a frame in the generated video, and the image generator transforms them into a video with 2D deconvolutional layers. This representation allows efficient training of the network parameters. Moreover, it can handle a wider range of applications including the generation of a long sequence, frame interpolation, and the use of pre-trained models. Experimental results demonstrate the effectiveness of our method.", "creator": "LaTeX with hyperref package"}}}