{"id": "1611.06639", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Nov-2016", "title": "Text Classification Improved by Integrating Bidirectional LSTM with Two-dimensional Max Pooling", "abstract": "Recurrent Neural Network (RNN) is one of the most popular architectures used in Natural Language Processsing (NLP) tasks because its recurrent structure is very suitable to process variable-length text. RNN can utilize distributed representations of words by first converting the tokens comprising each text into vectors, which form a matrix. And this matrix includes two dimensions: the time-step dimension and the feature vector dimension. Then most existing models usually utilize one-dimensional (1D) max pooling operation or attention-based operation only on the time-step dimension to obtain a fixed-length vector. However, the features on the feature vector dimension are not mutually independent, and simply applying 1D pooling operation over the time-step dimension independently may destroy the structure of the feature representation. On the other hand, applying two-dimensional (2D) pooling operation over the two dimensions may sample more meaningful features for sequence modeling tasks. To integrate the features on both dimensions of the matrix, this paper explores applying 2D max pooling operation to obtain a fixed-length representation of the text. This paper also utilizes 2D convolution to sample more meaningful information of the matrix. Experiments are conducted on six text classification tasks, including sentiment analysis, question classification, subjectivity classification and newsgroup classification. Compared with the state-of-the-art models, the proposed models achieve excellent performance on 4 out of 6 tasks. Specifically, one of the proposed models achieves highest accuracy on Stanford Sentiment Treebank binary classification and fine-grained classification tasks.", "histories": [["v1", "Mon, 21 Nov 2016 03:26:29 GMT  (125kb)", "http://arxiv.org/abs/1611.06639v1", "11 pages"]], "COMMENTS": "11 pages", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["peng zhou", "zhenyu qi", "suncong zheng", "jiaming xu", "hongyun bao", "bo xu"], "accepted": false, "id": "1611.06639"}, "pdf": {"name": "1611.06639.pdf", "metadata": {"source": "CRF", "title": "Text Classification Improved by Integrating Bidirectional LSTM with Two-dimensional Max Pooling", "authors": ["Peng Zhou", "Zhenyu Qi", "Suncong Zheng", "Jiaming Xu", "Hongyun Bao", "Bo Xu"], "emails": ["xubo}@ia.ac.cn"], "sections": [{"heading": null, "text": "ar Xiv: 161 1,06 639v 1 [cs.C L] 21 Nov 201 6"}, {"heading": "1 Introduction", "text": "It is an essential component in many NLP applications, such as sentiment analysis (Socher et al., 2013), relationship extraction (Zeng et al., 2014), and spam detection (Wang and Manning, 2012).In this way, it has garnered considerable attention from many researchers and various types of models based on a traditional method based on the success of word (BoW) models that treat texts as disordered sentences (Wang and Manning, 2012).In this way, however, it fails to encrypt word sequences and syntactic properties based on neural networks that have made tremendous advances in text classification, and the more significant advances shown compared to BoW models. The challenge for textual modeling is how to capture characteristics for different text units such as phrases, sentences, and documents. Benefiting from its recurrent structure, RNN, as an alternative type of neural networks, is very suitable to work with length."}, {"heading": "2 Related Work", "text": "These models typically consist of a projection layer that maps words of the text into vectors, and then they combine the vectors with different neural networks to create a fixed-length representation. Depending on the structure, they can be divided into four categories: Recursive neural networks (RecNN1), RNN, CNN and other neural networks are combined in a bottom-up manner. (2013) Recursive neural networks were introduced: RecNN is defined by recursive tree structures; in the manner of recursive models, information from the leaf nodes of a tree and its internal nodes is combined with each other. (2013) Recursive neural tensor networks were introduced to build representations of phrases and sentences by combining neighborhood constituents based on the parsing tree. Irsoy and Cardie (2014) 1To avoid confusion with RNN, we have proposed recursive neural networks as recursive networks."}, {"heading": "3 Model", "text": "As shown in Figure 1, the overall model consists of four parts: BLSTM layer, two-dimensional folding layer, two-dimensional maximum pooling layer and output layer. The details of the various components are described in the following sections."}, {"heading": "3.1 BLSTM Layer", "text": "The main idea is to introduce an adaptive gating mechanism that determines the degree to which the previous state is maintained and the extracted characteristics of the current data input are stored. Considering a sequence S = {x1, x2,.., xl}, in which l is the length of the input text, LSTM processes this word for word. In the time-step-t, the memory and the hidden state ht are updated with the following equations: It is ft ot c = \u03c3 \u03c3tanh W \u00b7 [ht \u2212 1, xt] (1) ct = ft-ct \u2212 1 + it is c-t (2) ht = ot-tanh (ct) (3), in which xt is the input at the current time step, i, f and o is the input gate activation, forget gate activation or c output gate activation."}, {"heading": "3.2 Convolutional Neural Networks", "text": "Since BLSTM can access both the future and the past context, Hello is related to all other words in the text. It is possible to effectively treat the matrix, which consists of attribute vectors, as an \"image,\" so that 2D folding and 2D max pooling processes can be used to gather more meaningful information."}, {"heading": "3.2.1 Two-dimensional Convolution Layer", "text": "A matrix H = {h1, h2,., hl}, H \u0435Rl \u00b7 d w is obtained from the BLSTM plane, where dw is the size of word embedding, and then a narrow folding is used (Kalchburner et al., 2014) to extract local features via H. A folding operation involves a 2D filter m-Rk \u00b7 d, which is applied to a window of k words and d attribute vectors. For example, a attribute oi, j is generated from a vector window Hi: i + k \u2212 1, j: j + d \u2212 1 byoi, j = f (m \u00b7 Hi: i + k \u2212 1, j + d \u2212 1 + b) (5), where i ranges from 1 to (l \u2212 k + 1), j ranges from 1 to (dw \u2212 d + 1), and b \u2212 R is a non-linear function, such as the hyperbolic filter (l \u2212 a filter + 1) or a pw = 1."}, {"heading": "3.2.2 Two-dimensional Max Pooling Layer", "text": "Then the maximum 2D pooling operation is used to obtain a fixed length vector. In a maximum 2D pooling function, the maximum value is applied to any possible window of matrix O: pi, j = down (Oi: i + p1, j: j + p2) (7), where Down (\u00b7) is the maximum 2D pooling function, i = (1, 1 + p1, \u00b7 \u00b7, 1 + (l \u2212 k + 1 / p1 \u2212 1) \u00b7 p1) and j = (1, 1 + p2, \u00b7, 1 + (dw \u2212 d + 1 / p2 \u2212 1) \u00b7 p2). Then, the pool results are combined as follows: h = [p1,1, p1,1 + p2, \u00b7 \u00b7 \u00b7, p1 + (l \u2212 k + 1 \u2212 1) \u00b7 p1,1 + (dw \u2212 d + 1 / p2 \u2212 1) \u00b7 p2] (8%, p2 + 1), where the length is + 1 dp."}, {"heading": "3.3 Output Layer", "text": "For text classification, the output h \u0445 of 2D Max Pooling Layer is the complete representation of the input text S. And then it is passed to a Softmax classification layer to predict the semantic relation etiquette y \u0445 from a discrete set of classes Y. The classifier takes the hidden state h \u0445 (y | s) = Softmax (W (s) h * + b (s)) (9) y \u00b2 = Argmax y p (y | s) (10) A reasonable training target that needs to be minimized is the categorical entropy loss. The loss is calculated as a regularized sum: J (s) = \u2212 1mm \u00b2 i = 1ti log (yi) + 1 \u00b0 C \u00b2 2 F (11), where t \u00b2 Rm is the most uniformly represented basic truth, y \u00b2 Rm is the estimated probability for each class by Softmax, m the number of target classes, and L2 is the regulation hyper parameter, where the Rm is the most uniformly estimated class by Rm, and Rm is the maximum uniformity for each class by Softmax."}, {"heading": "4 Experimental Setup", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Datasets", "text": "The proposed models are tested on the basis of six sets of data. Summary statistics of the data sets can be found in Table 1. \u2022 MR2: Sentence Polarity Dataset from Pang and Lee (2005). The task is to identify positive / negative ratings. \u2022 SST-13: Stanford Sentiment Treebank is an extension of the MR from Socher et al. (2013). The goal is to classify a review as fine-grained labels (very negative, negative, neutral, positive, very positive). \u2022 SST-2: Same as SST-1, but with neutral ratings and binary labels (negative, positive). Phrases and sentences are used for both experiments to train the model, but only sets are used at test dates (Socher et al., 2013; Le and Mikolov, 2014). Thus, the training set is an order of magnitude larger than those in Table 1. \u2022 Subjectivity data sets (Pang and Lee, 2004)."}, {"heading": "4.2 Word Embeddings", "text": "In particular, our experiments use Pennington et al. (2014) on 6 billion tokens from Wikipedia 2014 and Gigaword 5. Words that do not appear in the pre-trained words are initialized by random samples from an even distribution in [\u2212 0.1, 0.1]. Word embeddings are fine-tuned during the training to improve the performance of the classification. 2https: / / www.cs.cornell.edu / people / pabo / movie-review-data / 3http: / / nlp.stanford.edu / sentiments / 4http: / / www.cs.cornell.edu / people / pabo / movie-review-data / 5http: / / cogcomp.cs.illinois.edu / sentiments / 4http: / / www.cs.cornell.edu / people / pabo / movie-review-data / qsqsqsqsqsqsqsqsqsqsqsqsqsqsqsqsqsqsqsqsqsqsqsqsqsqsqsqsqsqsqsqsqsqsqsqsqsqsqsqsqsqsqsqsqsqsqsqsqsqsqsqsqsqsqsqsqsqsqsqsqsqsqsqsqsqsqsqsqsqsqsqsqsqsqsqsqsqsqsqsqsqqqsqsqqsqsqsqqsqqsqqqsqsqqqqsqqqqqsqsqqqqsqsqqqqqsqqqqqsqqqqsqqqqqqqqsqqqqqqqqqqqqqsqqqqqqqqqqqqqqqsqqqqqqqqqqqqqqqsqqqqqqqqqqqqqqqqqqqqqqqq"}, {"heading": "4.3 Hyper-parameter Settings", "text": "For data sets without a standard development set, we randomly select 10% of the training data as a development set. The final hyperparameters are as follows: the dimension of the word embedding is 300, the hidden units of LSTM 300. For regulation, we use 100 turn filters for window sizes of (3.3), 2D pooling sizes of (2.2), we set the minibatch size as 10 and the learning rate of AdaDelta as the default value of 1.0. For regulation, we use dropout operations (Hinton et al., 2012) with a failure rate of 0.5 for word embedding, 0.2 for the BLSTM layer and 0.4 for the penultimate layer, also applying l2 penalty with the coefficient 10 \u2212 5 over the parameters. These values are selected via a grid embedding, allowing us to further optimize the development of ST-1."}, {"heading": "5 Results", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 Overall Performance", "text": "This work implements four models, BLSTM, BLSTM-Att, BLSTM-2DPooling and BLSTM-2DCNN. Table 2 represents the performance of the four models and other state-of-the-art models on six classification tasks. The BLSTM-2DCNN model performs excellently on 4 of 6 tasks, and in particular it achieves 52.4% and 89.5% test accuracy on SST-1 and SST-2 respectively. BLSTM-2DPooling performs worse than the state-of-the-art models. While we expect performance gains from the use of 2D convolution, we are surprised at the magnitude of the gains. BLSTM-CNN beats all baselines on SST-1, SST-2 and TREC datasets that contain BLS-2D datasets."}, {"heading": "5.2 Effect of Sentence Length", "text": "Figure 2 shows the performance of the four models at different sentence lengths. In the figure, the x-axis represents sentence lengths and the y-axis accuracy. The sentences collected in the test sentence are no longer than 45 words. Accuracy here is the average of sentence length sentences in the window [l \u2212 2, l + 2]. Each data point is a mean over 5 runs, and error bars have been omitted for clarification. It was found that both BLSTM-2DPooling and BLSTM-2DCNN exceed the other two models, suggesting that both 2D folding and 2D max pooling operations can encode semantically useful structural information. At the same time, the accuracy decreases with increasing sentence length. In future work, we would like to investigate neural mechanisms to obtain long-term text dependencies."}, {"heading": "5.3 Effect of 2D Convolutional Filter and 2D Max Pooling Size", "text": "We are interested in what the best 2D filter and the best pool size are to achieve better performance. We are conducting experiments with the SST-1 dataset with BLSTM-2DCNN and set the number of feature maps to 100. To make it easy, we set these two dimensions to the same values, so that both the filter and the pooling are square matrices. To the horizontal axis, c means 2D turn filter size, and the five different color bar diagrams on each c represent different maximum 2D pool size from 2 to 6. Figure 3 shows that different filter and pool sizes can get different accuracy. the best accuracy is 52.6 for 2D filter size (5.5) and 2D maximum pool size (5.5), showing that finer tuning can further improve the performance reported here. And, if a larger filter is used, it can detect more, and the performance can also be improved."}, {"heading": "6 Conclusion", "text": "This paper presents two combination models, one is BLSTM-2DPooling, the other is BLSTM2DCNN, which can be considered an extension of BLSTM-2DPooling. Both models can capture not only the time step dimension, but also the characteristic vector dimension information. Experiments are performed on six text classification tasks. Results of the experiments show that BLSTM-2DCNN is not only better than RecNN, RNNN and CNN models, but also works better than BLSTM-2DPooling and DSCNN (Zhang et al., 2016). In particular, BLSTM-2DCNN achieves the highest accuracy for SST-1 and SST-2 datasets. To better understand the effectiveness of the proposed two models, this work also performs a sensitivity analysis on SST-1 datasets."}, {"heading": "Acknowledgements", "text": "We thank anonymous reviewers for their constructive comments. This research was funded by the National High Technology Research and Development Program of China (No.2015AA015402), the National Natural Science Foundation of China (No.61602479) and the Strategic Priority Research Program of the Chinese Academy of Sciences (Promotion No.XDB02070005)."}], "references": [{"title": "Improving methods for single-label text categorization", "author": [], "venue": "Ph.D. thesis, Universidade Te\u0301cnica de Lisboa", "citeRegEx": "Cachopo.,? \\Q2007\\E", "shortCiteRegEx": "Cachopo.", "year": 2007}, {"title": "Long short-term memory-networks for machine reading", "author": ["Cheng et al.2016] Jianpeng Cheng", "Li Dong", "Mirella Lapata"], "venue": "arXiv preprint arXiv:1601.06733", "citeRegEx": "Cheng et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Cheng et al\\.", "year": 2016}, {"title": "Document classification by topic labeling", "author": ["Sandeep Chougule", "Girish K Palshikar", "Sutanu Chakraborti"], "venue": "In Proceedings of the 36th international ACM SIGIR conference on Research and development in information retrieval,", "citeRegEx": "Hingmire et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Hingmire et al\\.", "year": 2013}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors. arXiv preprint arXiv:1207.0580", "author": ["Nitish Srivastava", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan R Salakhutdinov"], "venue": null, "citeRegEx": "Hinton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2012}, {"title": "Long short-term memory", "author": ["Hochreiter", "Schmidhuber1997] Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Deep recursive neural networks for compositionality in language", "author": ["Irsoy", "Cardie2014] Ozan Irsoy", "Claire Cardie"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Irsoy et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Irsoy et al\\.", "year": 2014}, {"title": "Deep unordered composition rivals syntactic methods for text classification", "author": ["Iyyer et al.2015] Mohit Iyyer", "Varun Manjunatha", "Jordan Boyd-Graber", "Hal Daum\u00e9 III"], "venue": "In Proceedings of the Association for Computational Linguistics", "citeRegEx": "Iyyer et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Iyyer et al\\.", "year": 2015}, {"title": "A convolutional neural network for modelling sentences", "author": ["Edward Grefenstette", "Phil Blunsom"], "venue": "arXiv preprint arXiv:1404.2188", "citeRegEx": "Kalchbrenner et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2014}, {"title": "Convolutional neural networks for sentence classification. arXiv preprint arXiv:1408.5882", "author": ["Yoon Kim"], "venue": null, "citeRegEx": "Kim.,? \\Q2014\\E", "shortCiteRegEx": "Kim.", "year": 2014}, {"title": "Imagenet classification with deep convolutional neural networks. In Advances in neural information processing", "author": ["Ilya Sutskever", "Geoffrey E Hinton"], "venue": null, "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Recurrent convolutional neural networks for text classification", "author": ["Lai et al.2015] Siwei Lai", "Liheng Xu", "Kang Liu", "Jun Zhao"], "venue": "In AAAI,", "citeRegEx": "Lai et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lai et al\\.", "year": 2015}, {"title": "Distributed representations of sentences and documents. arXiv preprint arXiv:1405.4053", "author": ["Le", "Mikolov2014] Quoc V Le", "Tomas Mikolov"], "venue": null, "citeRegEx": "Le et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Le et al\\.", "year": 2014}, {"title": "Compositional distributional semantics with long short term memory", "author": ["Le", "Zuidema2015] Phong Le", "Willem Zuidema"], "venue": "arXiv preprint arXiv:1503.02510", "citeRegEx": "Le et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Le et al\\.", "year": 2015}, {"title": "Gradient-based learning applied to document recognition", "author": ["LeCun et al.1998] Yann LeCun", "L\u00e9on Bottou", "Yoshua Bengio", "Patrick Haffner"], "venue": "Proceedings of the IEEE,", "citeRegEx": "LeCun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "Molding cnns for text: non-linear, nonconsecutive convolutions", "author": ["Lei et al.2015] Tao Lei", "Regina Barzilay", "Tommi Jaakkola"], "venue": "arXiv preprint arXiv:1508.04112", "citeRegEx": "Lei et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lei et al\\.", "year": 2015}, {"title": "Learning question classifiers", "author": ["Li", "Roth2002] Xin Li", "Dan Roth"], "venue": "In Proceedings of the 19th international conference on Computational linguistics-Volume", "citeRegEx": "Li et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Li et al\\.", "year": 2002}, {"title": "Recurrent neural network for text classification with multi-task learning", "author": ["Liu et al.2016] Pengfei Liu", "Xipeng Qiu", "Xuanjing Huang"], "venue": "arXiv preprint arXiv:1605.05101", "citeRegEx": "Liu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2016}, {"title": "Discriminative neural sentence modeling by tree-based convolution", "author": ["Mou et al.2015] Lili Mou", "Hao Peng", "Ge Li", "Yan Xu", "Lu Zhang", "Zhi Jin"], "venue": "arXiv preprint arXiv:1504.01106", "citeRegEx": "Mou et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mou et al\\.", "year": 2015}, {"title": "A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts", "author": ["Pang", "Lee2004] Bo Pang", "Lillian Lee"], "venue": "In Proceedings of the 42nd annual meeting on Association for Computational Linguistics,", "citeRegEx": "Pang et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Pang et al\\.", "year": 2004}, {"title": "Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales", "author": ["Pang", "Lee2005] Bo Pang", "Lillian Lee"], "venue": "In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics,", "citeRegEx": "Pang et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Pang et al\\.", "year": 2005}, {"title": "Glove: Global vectors for word representation", "author": ["Richard Socher", "Christopher D Manning"], "venue": "In EMNLP,", "citeRegEx": "Pennington et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Bidirectional recurrent neural networks", "author": ["Schuster", "Paliwal1997] Mike Schuster", "Kuldip K Paliwal"], "venue": "Signal Processing, IEEE Transactions", "citeRegEx": "Schuster et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Schuster et al\\.", "year": 1997}, {"title": "Recursive deep models for semantic compositionality over a sentiment treebank", "author": ["Alex Perelygin", "Jean Y Wu", "Jason Chuang", "Christopher D Manning", "Andrew Y Ng", "Christopher Potts"], "venue": "In Proceedings of the conference on empirical methods in natural language processing (EMNLP),", "citeRegEx": "Socher et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Improved semantic representations from tree-structured long short-term memory networks. arXiv preprint arXiv:1503.00075", "author": ["Tai et al.2015] Kai Sheng Tai", "Richard Socher", "Christopher D Manning"], "venue": null, "citeRegEx": "Tai et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Tai et al\\.", "year": 2015}, {"title": "Target-dependent sentiment classification with long short term memory. arXiv preprint arXiv:1512.01100", "author": ["Tang et al.2015] Duyu Tang", "Bing Qin", "Xiaocheng Feng", "Ting Liu"], "venue": null, "citeRegEx": "Tang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Tang et al\\.", "year": 2015}, {"title": "Word representations: a simple and general method for semi-supervised learning. In Proceedings of the 48th annual meeting of the association for computational linguistics, pages 384\u2013394", "author": ["Turian et al.2010] Joseph Turian", "Lev Ratinov", "Yoshua Bengio"], "venue": null, "citeRegEx": "Turian et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Turian et al\\.", "year": 2010}, {"title": "Baselines and bigrams: Simple, good sentiment and topic classification", "author": ["Wang", "Manning2012] Sida Wang", "Christopher D Manning"], "venue": "In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Short Papers-Volume", "citeRegEx": "Wang et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2012}, {"title": "Don\u2019t follow me: Spam detection in twitter. In Security and Cryptography (SECRYPT)", "author": ["Alex Hai Wang"], "venue": "Proceedings of the 2010 International Conference on,", "citeRegEx": "Wang.,? \\Q2010\\E", "shortCiteRegEx": "Wang.", "year": 2010}, {"title": "Learning text representation using recurrent convolutional neural network with highway layers. arXiv preprint arXiv:1606.06905", "author": ["Wen et al.2016] Ying Wen", "Weinan Zhang", "Rui Luo", "Jun Wang"], "venue": null, "citeRegEx": "Wen et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Wen et al\\.", "year": 2016}, {"title": "Hierarchical attention networks for document classification", "author": ["Yang et al.2016] Zichao Yang", "Diyi Yang", "Chris Dyer", "Xiaodong He", "Alex Smola", "Eduard Hovy"], "venue": "In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies", "citeRegEx": "Yang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2016}, {"title": "Multichannel variable-size convolution for sentence classification", "author": ["Yin", "Sch\u00fctze2016] Wenpeng Yin", "Hinrich Sch\u00fctze"], "venue": "arXiv preprint arXiv:1603.04513", "citeRegEx": "Yin et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Yin et al\\.", "year": 2016}, {"title": "Adadelta: An adaptive learning rate method", "author": ["Matthew D Zeiler"], "venue": "arXiv preprint arXiv:1212.5701", "citeRegEx": "Zeiler.,? \\Q2012\\E", "shortCiteRegEx": "Zeiler.", "year": 2012}, {"title": "Relation classification via convolutional deep neural network", "author": ["Zeng et al.2014] Daojian Zeng", "Kang Liu", "Siwei Lai", "Guangyou Zhou", "Jun Zhao"], "venue": "In COLING,", "citeRegEx": "Zeng et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zeng et al\\.", "year": 2014}, {"title": "A sensitivity analysis of (and practitioners\u2019 guide to) convolutional neural networks for sentence classification", "author": ["Zhang", "Wallace2015] Ye Zhang", "Byron Wallace"], "venue": "arXiv preprint arXiv:1510.03820", "citeRegEx": "Zhang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2015}, {"title": "Dependency sensitive convolutional neural networks for modeling sentences and documents", "author": ["Zhang et al.2016] Rui Zhang", "Honglak Lee", "Dragomir Radev"], "venue": "In Proceedings of NAACL-HLT,", "citeRegEx": "Zhang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2016}, {"title": "Self-adaptive hierarchical sentence model", "author": ["Zhao et al.2015] Han Zhao", "Zhengdong Lu", "Pascal Poupart"], "venue": "arXiv preprint arXiv:1504.05070", "citeRegEx": "Zhao et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zhao et al\\.", "year": 2015}, {"title": "A c-lstm neural network for text classification", "author": ["Zhou et al.2015] Chunting Zhou", "Chonglin Sun", "Zhiyuan Liu", "Francis Lau"], "venue": "arXiv preprint arXiv:1511.08630", "citeRegEx": "Zhou et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zhou et al\\.", "year": 2015}, {"title": "Attention-based bidirectional long short-term memory networks for relation classification", "author": ["Zhou et al.2016] Peng Zhou", "Wei Shi", "Jun Tian", "Zhenyu Qi", "Bingchen Li", "Hongwei Hao", "Bo Xu"], "venue": "In The 54th Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "Zhou et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Zhou et al\\.", "year": 2016}, {"title": "Long short-term memory over recursive structures", "author": ["Zhu et al.2015] Xiaodan Zhu", "Parinaz Sobhani", "Hongyu Guo"], "venue": "In Proceedings of the 32nd International Conference on Machine Learning,", "citeRegEx": "Zhu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zhu et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 22, "context": "Text classification is an essential component in many NLP applications, such as sentiment analysis (Socher et al., 2013), relation extraction (Zeng et al.", "startOffset": 99, "endOffset": 120}, {"referenceID": 32, "context": ", 2013), relation extraction (Zeng et al., 2014) and spam detection (Wang, 2010).", "startOffset": 29, "endOffset": 48}, {"referenceID": 27, "context": ", 2014) and spam detection (Wang, 2010).", "startOffset": 27, "endOffset": 39}, {"referenceID": 10, "context": "Then RNN utilizes 1D max pooling operation (Lai et al., 2015) or attention-based operation (Zhou et al.", "startOffset": 43, "endOffset": 61}, {"referenceID": 37, "context": ", 2015) or attention-based operation (Zhou et al., 2016), which extracts maximum values or generates a weighted representation over", "startOffset": 37, "endOffset": 56}, {"referenceID": 7, "context": "Convolutional Neural Networks (CNN) (Kalchbrenner et al., 2014; Kim, 2014) utilizes 1D convolution to perform the feature mapping, and then applies 1D max pooling operation over the time-step dimension to obtain a fixed-length output.", "startOffset": 36, "endOffset": 74}, {"referenceID": 8, "context": "Convolutional Neural Networks (CNN) (Kalchbrenner et al., 2014; Kim, 2014) utilizes 1D convolution to perform the feature mapping, and then applies 1D max pooling operation over the time-step dimension to obtain a fixed-length output.", "startOffset": 36, "endOffset": 74}, {"referenceID": 13, "context": "Unlike in NLP, CNN in image processing tasks (LeCun et al., 1998; Krizhevsky et al., 2012) applies 2D convolution and 2D pooling operation to get a representation of the input.", "startOffset": 45, "endOffset": 90}, {"referenceID": 9, "context": "Unlike in NLP, CNN in image processing tasks (LeCun et al., 1998; Krizhevsky et al., 2012) applies 2D convolution and 2D pooling operation to get a representation of the input.", "startOffset": 45, "endOffset": 90}, {"referenceID": 22, "context": "Socher et al. (2013) introduced recursive neural tensor network to build representations of phrases and sentences by combining neighbour constituents based on the parsing tree.", "startOffset": 0, "endOffset": 21}, {"referenceID": 22, "context": "Socher et al. (2013) introduced recursive neural tensor network to build representations of phrases and sentences by combining neighbour constituents based on the parsing tree. Irsoy and Cardie (2014) To avoid confusion with RNN, we named Recursive Neural Networks as RecNN.", "startOffset": 0, "endOffset": 201}, {"referenceID": 13, "context": "Convolution Neural Networks: CNN (LeCun et al., 1998) is a feedforward neural network with 2D convolution layers and 2D pooling layers, originally developed for image processing.", "startOffset": 33, "endOffset": 53}, {"referenceID": 7, "context": "Then CNN is applied to NLP tasks, such as sentence classification (Kalchbrenner et al., 2014; Kim, 2014), and relation classification (Zeng et al.", "startOffset": 66, "endOffset": 104}, {"referenceID": 8, "context": "Then CNN is applied to NLP tasks, such as sentence classification (Kalchbrenner et al., 2014; Kim, 2014), and relation classification (Zeng et al.", "startOffset": 66, "endOffset": 104}, {"referenceID": 32, "context": ", 2014; Kim, 2014), and relation classification (Zeng et al., 2014).", "startOffset": 48, "endOffset": 67}, {"referenceID": 34, "context": "The proposed model BLSTM-2DCNN is most relevant to DSCNN (Zhang et al., 2016) and RCNN (Wen et al.", "startOffset": 57, "endOffset": 77}, {"referenceID": 28, "context": ", 2016) and RCNN (Wen et al., 2016).", "startOffset": 17, "endOffset": 35}, {"referenceID": 19, "context": "Tang et al. (2015) developed target dependent Long ShortTerm Memory Networks (LSTM (Hochreiter and Schmidhuber, 1997)), where target information is automatically taken into account.", "startOffset": 0, "endOffset": 19}, {"referenceID": 19, "context": "Tai et al. (2015) generalized LSTM to Tree-LSTM where each LSTM unit gains information from its children units.", "startOffset": 0, "endOffset": 18}, {"referenceID": 19, "context": "Tai et al. (2015) generalized LSTM to Tree-LSTM where each LSTM unit gains information from its children units. Zhou et al. (2016) introduced BLSTM with attention mechanism to automatically select features that have a decisive effect on classification.", "startOffset": 0, "endOffset": 131}, {"referenceID": 19, "context": "Tai et al. (2015) generalized LSTM to Tree-LSTM where each LSTM unit gains information from its children units. Zhou et al. (2016) introduced BLSTM with attention mechanism to automatically select features that have a decisive effect on classification. Yang et al. (2016) introduced a hierarchical network with two levels of attention mechanisms, which are word attention and sentence attention, for document classification.", "startOffset": 0, "endOffset": 272}, {"referenceID": 19, "context": "Tai et al. (2015) generalized LSTM to Tree-LSTM where each LSTM unit gains information from its children units. Zhou et al. (2016) introduced BLSTM with attention mechanism to automatically select features that have a decisive effect on classification. Yang et al. (2016) introduced a hierarchical network with two levels of attention mechanisms, which are word attention and sentence attention, for document classification. This paper also implements an attention-based model BLSTM-Att like the model in Zhou et al. (2016). Convolution Neural Networks: CNN (LeCun et al.", "startOffset": 0, "endOffset": 524}, {"referenceID": 6, "context": "Then CNN is applied to NLP tasks, such as sentence classification (Kalchbrenner et al., 2014; Kim, 2014), and relation classification (Zeng et al., 2014). The difference is that the common CNN in NLP tasks is made up of 1D convolution layers and 1D pooling layers. Kim (2014) defined a CNN architecture with two channels.", "startOffset": 67, "endOffset": 276}, {"referenceID": 6, "context": "Then CNN is applied to NLP tasks, such as sentence classification (Kalchbrenner et al., 2014; Kim, 2014), and relation classification (Zeng et al., 2014). The difference is that the common CNN in NLP tasks is made up of 1D convolution layers and 1D pooling layers. Kim (2014) defined a CNN architecture with two channels. Kalchbrenner et al. (2014) proposed a dynamic k-max pooling mechanism for sentence modeling.", "startOffset": 67, "endOffset": 349}, {"referenceID": 6, "context": "Then CNN is applied to NLP tasks, such as sentence classification (Kalchbrenner et al., 2014; Kim, 2014), and relation classification (Zeng et al., 2014). The difference is that the common CNN in NLP tasks is made up of 1D convolution layers and 1D pooling layers. Kim (2014) defined a CNN architecture with two channels. Kalchbrenner et al. (2014) proposed a dynamic k-max pooling mechanism for sentence modeling. (Zhang and Wallace, 2015) conducted a sensitivity analysis of one-layer CNN to explore the effect of architecture components on model performance. Yin and Sch\u00fctze (2016) introduced multichannel embeddings and unsupervised pretraining to improve classification accuracy.", "startOffset": 67, "endOffset": 585}, {"referenceID": 6, "context": "Iyyer et al. (2015) introduced a deep averaging network, which fed an unweighted average of word embeddings through multiple hidden layers before classification.", "startOffset": 0, "endOffset": 20}, {"referenceID": 6, "context": "Iyyer et al. (2015) introduced a deep averaging network, which fed an unweighted average of word embeddings through multiple hidden layers before classification. Zhou et al. (2015) used CNN to extract a sequence of higher-level phrase representations, then the representations were fed into a LSTM to obtain the sentence representation.", "startOffset": 0, "endOffset": 181}, {"referenceID": 7, "context": "Then narrow convolution is utilized (Kalchbrenner et al., 2014) to extract local features over H .", "startOffset": 36, "endOffset": 63}, {"referenceID": 31, "context": "Training is done through stochastic gradient descent over shuffled mini-batches with the AdaDelta (Zeiler, 2012) update rule.", "startOffset": 98, "endOffset": 112}, {"referenceID": 22, "context": "\u2022 SST-13: Stanford Sentiment Treebank is an extension of MR from Socher et al. (2013). The aim is to classify a review as fine-grained labels (very negative, negative, neutral, positive, very positive).", "startOffset": 65, "endOffset": 86}, {"referenceID": 22, "context": "For both experiments, phrases and sentences are used to train the model, but only sentences are scored at test time (Socher et al., 2013; Le and Mikolov, 2014).", "startOffset": 116, "endOffset": 159}, {"referenceID": 0, "context": "We use the bydate version preprocessed by Cachopo (2007). We select four major categories (comp, politics, rec and religion) followed by Hingmire et al.", "startOffset": 42, "endOffset": 57}, {"referenceID": 0, "context": "We use the bydate version preprocessed by Cachopo (2007). We select four major categories (comp, politics, rec and religion) followed by Hingmire et al. (2013).", "startOffset": 42, "endOffset": 160}, {"referenceID": 25, "context": "2 Word Embeddings The word embeddings are pre-trained on much larger unannotated corpora to achieve better generalization given limited amount of training data (Turian et al., 2010).", "startOffset": 160, "endOffset": 181}, {"referenceID": 20, "context": "In particular, our experiments utilize the GloVe embeddings7 trained by Pennington et al. (2014) on 6 billion tokens of Wikipedia 2014 and Gigaword 5.", "startOffset": 72, "endOffset": 97}, {"referenceID": 3, "context": "For regularization, we employ Dropout operation (Hinton et al., 2012) with dropout rate of 0.", "startOffset": 48, "endOffset": 69}, {"referenceID": 7, "context": "We only tune these hyperparameters, and more finer tuning, such as using different numbers of hidden units of LSTM layer, or using wide convolution (Kalchbrenner et al., 2014), may further improve the performance.", "startOffset": 148, "endOffset": 175}, {"referenceID": 10, "context": "Compared with RCNN (Lai et al., 2015), BLSTM2DCNN achieves a comparable result.", "startOffset": 19, "endOffset": 37}, {"referenceID": 34, "context": "Compared with DSCNN (Zhang et al., 2016), BLSTM-2DCNN outperforms it on five datasets.", "startOffset": 20, "endOffset": 40}, {"referenceID": 22, "context": "NN Model SST-1 SST-2 Subj TREC MR 20Ng ReNN RNTN (Socher et al., 2013) 45.", "startOffset": 49, "endOffset": 70}, {"referenceID": 7, "context": "CNN DCNN (Kalchbrenner et al., 2014) 48.", "startOffset": 9, "endOffset": 36}, {"referenceID": 8, "context": "0 - CNN-non-static (Kim, 2014) 48.", "startOffset": 19, "endOffset": 30}, {"referenceID": 8, "context": "6 - CNN-MC (Kim, 2014) 47.", "startOffset": 11, "endOffset": 22}, {"referenceID": 17, "context": "2 92 - TBCNN(Mou et al., 2015) 51.", "startOffset": 12, "endOffset": 30}, {"referenceID": 14, "context": "0 - Molding-CNN (Lei et al., 2015) 51.", "startOffset": 16, "endOffset": 34}, {"referenceID": 10, "context": "RNN RCNN (Lai et al., 2015) 47.", "startOffset": 9, "endOffset": 27}, {"referenceID": 38, "context": "49 S-LSTM (Zhu et al., 2015) - 81.", "startOffset": 10, "endOffset": 28}, {"referenceID": 23, "context": "9 - - - LSTM (Tai et al., 2015) 46.", "startOffset": 13, "endOffset": 31}, {"referenceID": 23, "context": "9 - - - BLSTM (Tai et al., 2015) 49.", "startOffset": 14, "endOffset": 32}, {"referenceID": 23, "context": "5 - - - Tree-LSTM (Tai et al., 2015) 51.", "startOffset": 18, "endOffset": 36}, {"referenceID": 1, "context": "0 - - - LSTMN (Cheng et al., 2016) 49.", "startOffset": 14, "endOffset": 34}, {"referenceID": 16, "context": "3 - - - Multi-Task (Liu et al., 2016) 49.", "startOffset": 19, "endOffset": 37}, {"referenceID": 6, "context": "8 - - - DAN (Iyyer et al., 2015) 48.", "startOffset": 12, "endOffset": 32}, {"referenceID": 35, "context": "5 AdaSent (Zhao et al., 2015) - - 95.", "startOffset": 10, "endOffset": 29}, {"referenceID": 36, "context": "0 - - - C-LSTM (Zhou et al., 2015) 49.", "startOffset": 15, "endOffset": 34}, {"referenceID": 34, "context": "6 - DSCNN (Zhang et al., 2016) 49.", "startOffset": 10, "endOffset": 30}, {"referenceID": 22, "context": "RNTN: Recursive deep models for semantic compositionality over a sentiment treebank (Socher et al., 2013).", "startOffset": 84, "endOffset": 105}, {"referenceID": 7, "context": "DCNN: A convolutional neural network for modeling sentences (Kalchbrenner et al., 2014).", "startOffset": 60, "endOffset": 87}, {"referenceID": 8, "context": "CNN-nonstatic/MC: Convolutional neural networks for sentence classification (Kim, 2014).", "startOffset": 76, "endOffset": 87}, {"referenceID": 17, "context": "TBCNN: Discriminative neural sentence modeling by tree-based convolution (Mou et al., 2015).", "startOffset": 73, "endOffset": 91}, {"referenceID": 14, "context": "Molding-CNN: Molding CNNs for text: non-linear, non-consecutive convolutions (Lei et al., 2015).", "startOffset": 77, "endOffset": 95}, {"referenceID": 10, "context": "RCNN: Recurrent Convolutional Neural Networks for Text Classification (Lai et al., 2015).", "startOffset": 70, "endOffset": 88}, {"referenceID": 38, "context": "S-LSTM: Long short-term memory over recursive structures (Zhu et al., 2015).", "startOffset": 57, "endOffset": 75}, {"referenceID": 23, "context": "LSTM/BLSTM/Tree-LSTM: Improved semantic representations from tree-structured long shortterm memory networks (Tai et al., 2015).", "startOffset": 108, "endOffset": 126}, {"referenceID": 1, "context": "LSTMN: Long short-term memory-networks for machine reading (Cheng et al., 2016).", "startOffset": 59, "endOffset": 79}, {"referenceID": 16, "context": "Multi-Task: Recurrent Neural Network for Text Classification with Multi-Task Learning (Liu et al., 2016).", "startOffset": 86, "endOffset": 104}, {"referenceID": 6, "context": "DAN: Deep unordered composition rivals syntactic methods for text classification (Iyyer et al., 2015).", "startOffset": 81, "endOffset": 101}, {"referenceID": 35, "context": "AdaSent: Selfadaptive hierarchical sentence model (Zhao et al., 2015).", "startOffset": 50, "endOffset": 69}, {"referenceID": 36, "context": "C-LSTM: A C-LSTM Neural Network for Text Classification (Zhou et al., 2015).", "startOffset": 56, "endOffset": 75}, {"referenceID": 34, "context": "DSCNN: Dependency Sensitive Convolutional Neural Networks for Modeling Sentences and Documents (Zhang et al., 2016).", "startOffset": 95, "endOffset": 115}, {"referenceID": 34, "context": "The experiments results demonstrate that BLSTM-2DCNN not only outperforms RecNN, RNN and CNN models, but also works better than the BLSTM-2DPooling and DSCNN (Zhang et al., 2016).", "startOffset": 158, "endOffset": 178}], "year": 2016, "abstractText": "Recurrent Neural Network (RNN) is one of the most popular architectures used in Natural Language Processsing (NLP) tasks because its recurrent structure is very suitable to process variablelength text. RNN can utilize distributed representations of words by first converting the tokens comprising each text into vectors, which form a matrix. And this matrix includes two dimensions: the time-step dimension and the feature vector dimension. Then most existing models usually utilize one-dimensional (1D) max pooling operation or attention-based operation only on the time-step dimension to obtain a fixed-length vector. However, the features on the feature vector dimension are not mutually independent, and simply applying 1D pooling operation over the time-step dimension independently may destroy the structure of the feature representation. On the other hand, applying two-dimensional (2D) pooling operation over the two dimensions may sample more meaningful features for sequence modeling tasks. To integrate the features on both dimensions of the matrix, this paper explores applying 2D max pooling operation to obtain a fixed-length representation of the text. This paper also utilizes 2D convolution to sample more meaningful information of the matrix. Experiments are conducted on six text classification tasks, including sentiment analysis, question classification, subjectivity classification and newsgroup classification. Compared with the state-of-the-art models, the proposed models achieve excellent performance on 4 out of 6 tasks. Specifically, one of the proposed models achieves highest accuracy on Stanford Sentiment Treebank binary classification and fine-grained classification tasks.", "creator": "LaTeX with hyperref package"}}}