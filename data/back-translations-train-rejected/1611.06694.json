{"id": "1611.06694", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Nov-2016", "title": "Training Sparse Neural Networks", "abstract": "Deep neural networks with lots of parameters are typically used for large-scale computer vision tasks such as image classification. This is a result of using dense matrix multiplications and convolutions. However, sparse computations are known to be much more efficient. In this work, we train and build neural networks which implicitly use sparse computations. We introduce additional gate variables to perform parameter selection and show that this is equivalent to using a spike-and-slab prior. We experimentally validate our method on both small and large networks and achieve state-of-the-art compression results for sparse neural network models.", "histories": [["v1", "Mon, 21 Nov 2016 09:24:24 GMT  (76kb,D)", "http://arxiv.org/abs/1611.06694v1", null]], "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["suraj srinivas", "akshayvarun subramanya", "r venkatesh babu"], "accepted": false, "id": "1611.06694"}, "pdf": {"name": "1611.06694.pdf", "metadata": {"source": "CRF", "title": "Training Sparse Neural Networks", "authors": ["Suraj Srinivas", "Akshayvarun Subramanya", "R. Venkatesh Babu"], "emails": ["surajsrinivas@grads.cds.iisc.ac.in", "akshayvarun07@gmail.com", "venky@cds.iisc.ac.in"], "sections": [{"heading": "Introduction", "text": "Large networks with many millions of parameters are often used for large-scale tasks such as image classification (Krizhevsky, Sutskever, and Hinton 2012), (Simonyan and Zisserman 2015), (Szegedy et al. 2015). However, these networks typically use dense calculations. Would it be advantageous to use sparse calculations instead? Apart from the fact that there are fewer parameters to store (O (mn) toO (k))) 1, sparse calculations also reduce forward evaluation time (O (mnp) to O (kp) 2. In addition, a smaller number of parameters can help to avoid overadjustment.Regularizers are often used to prevent overadjustment, which usually limit the size ('2 /' 1) of weights."}, {"heading": "Problem Formulation", "text": "To understand the motivation behind our method, let us first define our idea of the computational complexity of a neural network, where Gsi is a vector of parameter indexes for the ith layer, i.e., GSI = {0, 1} ni. Here, each layer contains GSI ni elements. Zero indicates the absence of a parameter and one indicates presence. Thus, for a dense neural network, GSI is a vector of all ones, i.e.; GSI = {1} ni. For a sparse parameter vector vector vector vector of all ones, i.e., GSI = {1} ni. For a sparse parameter vector vector vector vector, GSI would be predominantly composed of nulos. Let us call as an index set of a neural network. For these vectors, our notion of complexity is simply the total number of parameters in the net define.The neural layer 1."}, {"heading": "Promoting Sparsity", "text": "Given our formalism of gate variables, the question arises as to how we can ensure that the learned Bernoulli parameters are low - or, in our case, usually below 0.5. A plausible option is to use the \"2 or\" 1 regularizer on the gate variables. However, this does not mean that there will be values greater than 0.5. To take this into account, we need a bi-modal regularizer, i.e. a regularizer that ensures that some values are large but most values are small. To this end, we use a regularizer given by w \u00b7 (1 \u2212 w), which was introduced by (Murray and Ng 2010) to learn binary values for parameters. However, what is important to us is that this regularizer has the aforementioned bi-modal property, as in Fig. 2aOur general regularizer is simply a combination of this regularized rule equal to the same as the \"1\" or \"1\" or \"1\" for each of the traditional functions, as well as the \"1\" for our \"or\" 1 = for our observer."}, {"heading": "An Alternate Interpretation", "text": "Now that we have arrived at the objective function in Equation 2, it is obvious to ask the question - how do we know that it solves the original goal in Equation 1? From this perspective, we now deduce Equation 2. If we assume the formulation of gate variables, we can rewrite the goal in Equation 1 as follows."}, {"heading": "Relation to Spike-and-Slab priors", "text": "We observe that our problem formulation is very similar to spike-and-slab types used in Bayesian statistics for variable selection (Mitchell and Beauchamp 1988). By and large, these priors are mixtures of two distributions - one with very low variance (spike) and another with comparatively high variance (slab). By placing a large mass on top, we can expect to obtain parameter vectors with large differences (4). Let's also consider for a moment the previous ones for weight matrices of neural networks. P (W) = 1Z: \"i exp\" (-) \"i exp\" (wi))) - \"n:\" i \"i\" s. \""}, {"heading": "Comparison with LASSO", "text": "The main difference between the above method and LASSO is that LASSO is primarily a shrink operator, i.e. it shrinks all parameters until many of them are close to zero, not in the case of spike-and-slab priors, which are at the same time very sparse and can produce large values, due to the richer parameterization of these priors."}, {"heading": "Practical issues", "text": "In this section, we will discuss some practical issues related to our method. Ironically, our method uses twice the number of parameters as a typical neural network, as we have two sets of variables - weights and gates. As a result, the model size doubles during training. However, we multiply the two to obtain sparse matrices, which significantly reduces the model size at test time. Essentially, we do not need to store both sets of parameters during testing, only one element-wide product of the two is required. Although the model is twice as large at train time, we find that the speed of training / feedback-forward evaluation is not affected due to the fact that only elementary operations are used. Our method can be applied to both convential tensors and fully connected matrices. However, while we perform compression, we find that convective layers are less susceptible to compression, as fully connected layers have due to the small number of parameters they possess."}, {"heading": "Related Work", "text": "There has been a lot of recent work on the compression of neural networks. Weight loss techniques have been proposed by LeCun et al. (1989) and Hassibi et al. (1993), which introduced Optimal Brain Damage and Optimal Brain Surgery. Recently, Srinivas and Babu (2015a), a neural circumcision technology based on neural similarities, have also been proposed. In contrast, we perform weight circumcision based on learning processes rather than handmade rules.Previous experiments have also been made to conserve neural networks. Han et al. (2015) create sparse networks alternating between weight circumcision and network training. A similar strategy is followed by Collins and Kohli. On the other hand, our method performs both weight circumcision and network training."}, {"heading": "Experiments", "text": "In this section, we will conduct experiments to evaluate the effectiveness of our method. First, we will conduct some experiments to understand the typical behavior of the method. These experiments will be conducted primarily on LeNet-5 (LeCun et al. 1998). Second, we will use our method of network compression on two networks - LeNet-5 and AlexNet. These networks will be trained on the basis of MNIST and ILSVRC-2012 datasets respectively. Our implementation is based on Lasagne, a theano-based library."}, {"heading": "Analysis of Proposed method", "text": "We will describe experiments to analyze the behavior of our method. First, we will analyze the effects of hyperparameters. Second, we will examine the effects of different model sizes on the resulting thriftiness. For all analytical experiments, we will consider the LeNet-5 network. However, LeNet-5 consists of two 5 x 5 convolutionary layers with 20 and 50 filters and two fully connected layers with 500 and 10 (baseline) neurons. For the analysis, we will only examine the effects that sparsify the third fully connected layers. Effect of hyperparameters In Section 2.1, we described that we use maximum probability sampling (i.e. threshold) instead of unbiased sampling from a Bernoulli. In these experiments, we will study the relative effects of hyperparameters on both methods. In the sample, it is described that the probability is sampled (d.1; threshold) instead of unbiased sampling from a Bernoulli, we will examine the hyperparameter effects on both."}, {"heading": "Compression Performance", "text": "In fact, it is such that we are able to see ourselves able to hide and suffer a negligible loss in accuracy. The table shows that we are able to achieve the results of LeNet-5-compression. For the proposed methods - 1, 2, 4, 5, 6, 7, 7, 8, 8, 8, 8, 8, 8, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 9, 9, 9, 9, 9, 9, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,"}, {"heading": "Conclusion", "text": "We have introduced a new method for learning neural networks with sparse connections, which can be interpreted as learning weights and simultaneous pruning. By adopting a learning-based approach to pruning weights, we achieve the optimum degree of sparseness. This enables us to achieve up-to-date results in compressing deep neural networks."}], "references": [{"title": "Estimating or propagating gradients through stochastic neurons for conditional computation", "author": ["L\u00e9onard Bengio", "Y. Courville 2013] Bengio", "N. L\u00e9onard", "A. Courville"], "venue": "arXiv preprint arXiv:1308.3432", "citeRegEx": "Bengio et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2013}, {"title": "An exploration of parameter redundancy in deep networks with circulant projections", "author": ["Cheng"], "venue": "In Proceedings of the IEEE International Conference on Computer Vision,", "citeRegEx": "Cheng,? \\Q2015\\E", "shortCiteRegEx": "Cheng", "year": 2015}, {"title": "Memory bounded deep convolutional networks. CoRR abs/1412.1442", "author": ["Collins", "M.D. Kohli 2014] Collins", "P. Kohli"], "venue": null, "citeRegEx": "Collins et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Collins et al\\.", "year": 2014}, {"title": "Predicting parameters in deep learning", "author": ["Denil"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Denil,? \\Q2013\\E", "shortCiteRegEx": "Denil", "year": 2013}, {"title": "Exploiting linear structure within convolutional networks for efficient evaluation", "author": ["Denton"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Denton,? \\Q2014\\E", "shortCiteRegEx": "Denton", "year": 2014}, {"title": "Compressing deep convolutional networks using vector quantization", "author": ["Gong"], "venue": "arXiv preprint arXiv:1412.6115", "citeRegEx": "Gong,? \\Q2014\\E", "shortCiteRegEx": "Gong", "year": 2014}, {"title": "Learning both weights and connections for efficient neural networks. arXiv preprint arXiv:1506.02626", "author": ["Han"], "venue": null, "citeRegEx": "Han,? \\Q2015\\E", "shortCiteRegEx": "Han", "year": 2015}, {"title": "Second order derivatives for network pruning: Optimal brain surgeon", "author": ["Stork Hassibi", "B. others 1993] Hassibi", "D. G Stork"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Hassibi et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Hassibi et al\\.", "year": 1993}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["Sutskever Krizhevsky", "A. Hinton 2012] Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Optimal brain damage", "author": ["LeCun"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "LeCun,? \\Q1989\\E", "shortCiteRegEx": "LeCun", "year": 1989}, {"title": "Gradient-based learning applied to document recognition", "author": ["LeCun"], "venue": "Proceedings of the IEEE", "citeRegEx": "LeCun,? \\Q1998\\E", "shortCiteRegEx": "LeCun", "year": 1998}, {"title": "Bayesian variable selection in linear regression", "author": ["Mitchell", "T.J. Beauchamp 1988] Mitchell", "J.J. Beauchamp"], "venue": "Journal of the American Statistical Association", "citeRegEx": "Mitchell et al\\.,? \\Q1988\\E", "shortCiteRegEx": "Mitchell et al\\.", "year": 1988}, {"title": "Acdc: A structured efficient linear layer. arXiv preprint arXiv:1511.05946", "author": ["Moczulski"], "venue": null, "citeRegEx": "Moczulski,? \\Q2015\\E", "shortCiteRegEx": "Moczulski", "year": 2015}, {"title": "An algorithm for nonlinear optimization problems with binary variables. Computational Optimization and Applications 47(2):257\u2013288", "author": ["Murray", "W. Ng 2010] Murray", "Ng", "K.-M"], "venue": null, "citeRegEx": "Murray et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Murray et al\\.", "year": 2010}, {"title": "Tensorizing neural networks", "author": ["Novikov"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Novikov,? \\Q2015\\E", "shortCiteRegEx": "Novikov", "year": 2015}, {"title": "Very deep convolutional networks for largescale image recognition", "author": ["Simonyan", "K. Zisserman 2015] Simonyan", "A. Zisserman"], "venue": "In International Conference on Learning Representations", "citeRegEx": "Simonyan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Simonyan et al\\.", "year": 2015}, {"title": "2015a. Data-free parameter pruning for deep neural networks", "author": ["Srinivas", "S. Babu 2015a] Srinivas", "R.V. Babu"], "venue": null, "citeRegEx": "Srinivas et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Srinivas et al\\.", "year": 2015}, {"title": "Learning the architecture of deep neural networks. arXiv preprint arXiv:1511.05497", "author": ["Srinivas", "S. Babu 2015b] Srinivas", "R.V. Babu"], "venue": null, "citeRegEx": "Srinivas et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Srinivas et al\\.", "year": 2015}, {"title": "Going deeper with convolutions", "author": ["Szegedy"], "venue": "In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)", "citeRegEx": "Szegedy,? \\Q2015\\E", "shortCiteRegEx": "Szegedy", "year": 2015}, {"title": "Deep fried convnets", "author": ["Yang"], "venue": "arXiv preprint arXiv:1412.7149", "citeRegEx": "Yang,? \\Q2014\\E", "shortCiteRegEx": "Yang", "year": 2014}], "referenceMentions": [{"referenceID": 0, "context": "How do we estimate gradients for gate variables, given that they are binary stochastic variables, rather than real-valued and smooth? In other words, how do we backpropagate through the bernoulli sampling step? Bengio et al. (2013) investigated this problem and empirically verified the efficacy of different possible solutions.", "startOffset": 211, "endOffset": 232}, {"referenceID": 0, "context": "How do we estimate gradients for gate variables, given that they are binary stochastic variables, rather than real-valued and smooth? In other words, how do we backpropagate through the bernoulli sampling step? Bengio et al. (2013) investigated this problem and empirically verified the efficacy of different possible solutions. They conclude that the simplest way of computing gradients - the straight-through estimator works best overall. Our experiments also agree with this observation. The straight-through estimator simply involves backpropagating through a stochastic neuron as if it were an identity function. If the sampling step is given by g \u223c bernoulli(g), then the gradient dg s dg = 1 is used. Another issue of consideration is that of ensuring that g always lies in [0, 1] so that it is a valid bernoulli parameter. Bengio et al. (2013) use a sigmoid activation function to achieve this.", "startOffset": 211, "endOffset": 852}, {"referenceID": 4, "context": "Weight-pruning techniques were popularized by LeCun et al.(1989) and Hassibi et al.", "startOffset": 46, "endOffset": 65}, {"referenceID": 3, "context": "(1989) and Hassibi et al.(1993), who introduced Optimal Brain Damage and Optimal Brain Surgery respectively.", "startOffset": 11, "endOffset": 32}, {"referenceID": 3, "context": "(1989) and Hassibi et al.(1993), who introduced Optimal Brain Damage and Optimal Brain Surgery respectively. Recently, Srinivas and Babu (2015a) proposed a neuron pruning technique, which relied on neuronal similarity.", "startOffset": 11, "endOffset": 145}, {"referenceID": 3, "context": "Han et al.(2015) create sparse networks by alternating between weight pruning and network training.", "startOffset": 0, "endOffset": 17}, {"referenceID": 3, "context": "Han et al.(2015) create sparse networks by alternating between weight pruning and network training. A similar strategy is followed by Collins and Kohli (2014). On the other hand, our method performs both weight pruning and network training simultaneously.", "startOffset": 0, "endOffset": 159}, {"referenceID": 2, "context": "Denil et al.(2013) demonstrated that most of the parameters of a model can be predicted given only a few parameters.", "startOffset": 0, "endOffset": 19}, {"referenceID": 2, "context": "Denil et al.(2013) demonstrated that most of the parameters of a model can be predicted given only a few parameters. At training time, they learn only a few parameters and predict the rest. Yang et al.(2014) propose an Adaptive Fastfood transform, which is an efficient reparametrization of fully-connected layer weights.", "startOffset": 0, "endOffset": 208}, {"referenceID": 2, "context": "Denil et al.(2013) demonstrated that most of the parameters of a model can be predicted given only a few parameters. At training time, they learn only a few parameters and predict the rest. Yang et al.(2014) propose an Adaptive Fastfood transform, which is an efficient reparametrization of fully-connected layer weights. This results in a reduction of complexity for weight storage and computation. Novikov et al.(2015) use tensor decompositions to obtain a factorization of tensors with small number of parameters.", "startOffset": 0, "endOffset": 421}, {"referenceID": 1, "context": "Cheng et al.(2015) make use of circulant matrices to re-paramaterize fully connected layers.", "startOffset": 0, "endOffset": 19}, {"referenceID": 1, "context": "Cheng et al.(2015) make use of circulant matrices to re-paramaterize fully connected layers. Some recent works have also focussed on using approximations of weight matrices to perform compression. Gong et al.(2014) use a clustering-based product quantization approach to build an indexing scheme that reduces the space occupied by the matrix on disk.", "startOffset": 0, "endOffset": 215}], "year": 2016, "abstractText": "Deep neural networks with lots of parameters are typically used for large scale computer vision tasks such as image classification. This is a result of using dense matrix multiplications and convolutions. However, sparse computations are known to be much more efficient. In this work, we train and build neural networks which implicitly use sparse computations. We introduce additional gate variables to perform parameter selection and show that this is equivalent to using a spike-andslab prior. We experimentally validate our method on both small and large networks and achieve state-of-theart compression results for sparse neural network models.", "creator": "LaTeX with hyperref package"}}}