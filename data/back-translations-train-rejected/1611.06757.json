{"id": "1611.06757", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Nov-2016", "title": "Non-Local Color Image Denoising with Convolutional Neural Networks", "abstract": "We propose a novel deep network architecture for grayscale and color image denoising that is based on a non-local image model. Our motivation for the overall design of the proposed network stems from variational methods that exploit the inherent non-local self-similarity property of natural images. We build on this concept and introduce deep networks that perform non-local processing and at the same time they significantly benefit from discriminative learning. Experiments on the Berkeley segmentation dataset, comparing several state-of-the-art methods, show that the proposed non-local models achieve the best reported denoising performance both for grayscale and color images for all the tested noise levels. It is also worth noting that this increase in performance comes at no extra cost on the capacity of the network compared to existing alternative deep network architectures. In addition, we highlight a direct link of the proposed non-local models to convolutional neural networks. This connection is of significant importance since it allows our models to take full advantage of the latest advances on GPU computing in deep learning and makes them amenable to efficient implementations through their inherent parallelism.", "histories": [["v1", "Mon, 21 Nov 2016 12:36:10 GMT  (7536kb,D)", "http://arxiv.org/abs/1611.06757v1", "15 pages"], ["v2", "Mon, 10 Jul 2017 20:06:26 GMT  (7537kb,D)", "http://arxiv.org/abs/1611.06757v2", "15 pages, accepted to CVPR 2017"]], "COMMENTS": "15 pages", "reviews": [], "SUBJECTS": "cs.CV cs.AI", "authors": ["stamatios lefkimmiatis"], "accepted": false, "id": "1611.06757"}, "pdf": {"name": "1611.06757.pdf", "metadata": {"source": "CRF", "title": "Non-local Color Image Denoising with Convolutional Neural Networks", "authors": ["Stamatios Lefkimmiatis"], "emails": ["s.lefkimmiatis@skoltech.ru"], "sections": [{"heading": "1. Introduction", "text": "In fact, most people who are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, and to move."}, {"heading": "2. Variational Image Restoration Revisited", "text": "The goal of image recovery is to restore a grayscale or color image X from a corrupt observation Y, with the later obtained corresponding to the observation modely = x + n. (1) In this setting, x-RN \u00b7 C are the vectorized versions of the observed or latent images, N is the number of pixels, C is the number of image channels, and n is considered i.i.d Gaussian noise with variance \u03c32.Due to the inappropriateness of the problem investigated [38], equation. (1), which relates the latent image to the observation, the solution cannot be clearly characterized. This implies that in order to obtain a physically or statistically meaningful solution, the image evidence must be combined with appropriate image priorities. One of the most popular and powerful strategies available in the literature to combine observation and prior information is the variational approach."}, {"heading": "2.1. Image Regularization", "text": "Choosing a suitable regulator is very important as it is one of the main factors determining the quality of the restored image. Therefore, much effort has been put into designing novel regulatory functions that can model important image characteristics and thus lead to improved reconstruction outcomes. Most of the existing regulatory methods are based on either a synthesis or analysis-based approach. Synthesis-based regulation takes place in a sparsifying domain, such as the waveband the restored image is achieved through the application of an inverse transformation [13]. On the other hand, analysis-based regulation includes regulators that are applied directly to the image to be restored. The latter regulatory strategy has been reported to achieve better reconstruction results [9, 35] and is therefore largely preferred."}, {"heading": "2.2. Objective Function Minimization", "text": "In addition to the formulation of the objective function and the correct selection of the regularizer, another important aspect of the variation approach is the minimization strategy used to obtain the solution. In the case under consideration, the solution of the pictorial problem can be mathematically formulated as follows: x * = argmin a \u2264 xn \u2264 b1 \u00b2 y \u2212 x \u00b2 22 + \u03bbR \u2211 r = 1\u03c6 (Lrx) = argmin x1 \u00b2 y \u2212 x \u00b2 22 + \u03bbR \u2211 r = 1\u03c6 (Lrx) + \u03b9C (x) (4), where \u03b9C is the indicator function of the convex quantity C = {x-RN | xn-N = 1,. N}. The indicator function \u03b9C takes the value 0 if x \u00b2 C and + \u02da otherwise. The presence of this additional term in Equation (4) stems from the fact that these kinds of limitations on image intensities naturally arise. For example, it is reasonable that a negative form (not a specific range should be restored)."}, {"heading": "2.3. Proximal Gradient Method", "text": "There are a number of powerful optimization strategies for dealing with equation. (4) However, the simplest approach we will follow in this paper is the direct use of a gradient descent algorithm. Since the indicator function \u03b9C is not smooth, we will use the proximal gradient method [28] instead of the classic gradient descent algorithm. According to this method, the objective function is divided into two terms, one of which is differentiable. Here, we assume that the potential function \u03c6 is smooth and therefore we can calculate its partial derivatives. In this case, the division we choose for the objective function has the form E (x) = f (x) + \u03b9C (x), where f (x) is defined as a potential function, asf (x)."}, {"heading": "3. Proposed Non-Local Network", "text": "In this paper, we take a different approach from traditional regulatory methods. Instead of manually selecting the exact forms of potential function and regulatory operator, we design a network that is able to learn these sets directly from training data. The basic idea is to roll out the method of the proximal gradient and use a limited number of iterations derived in Equivalent (9) to construct the graph of the network. Then, we learn the relevant parameters by training the network using pairs of corrupt and ground-truth data. Next, we describe in detail the overall architecture of the proposed network, which is discriminatively trained in imaging. First, we motivate and derive its structure for processing grayscale images, and then explain the necessary modifications for processing color images."}, {"heading": "3.1. Non-Local Regularization Operator", "text": "rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the ru the rf\u00fc the rfu the rfu the ru the rfu the ru the r"}, {"heading": "3.1.1 Convolutional Implementation of the Non-Local Operator", "text": "As we explain below, both the non-local operator defined in (12) and its addition defined in (13) can be calculated by means of folding operations and transposing them. Therefore, they can be efficiently implemented with modern software libraries such as OMP and cuDNN that support multi-thread CPU and parallel GPU implementations. Specifically, image patch extraction and the 2D patch transform, fr = FPrx, can be combined and calculated by passing the image X from a folding layer. To get the desired output, the filter base should consist of as many 2D filters as the number of coefficients in the transformation domain. Furthermore, support for these filters should correspond to the size of the image fields, implying that in our case F filters with support for Px \u00d7 Py should be used."}, {"heading": "3.2. Parameterization of the Potential Function", "text": "In addition to the non-local operator L, we must also model the potential function \u03c6. We do this indirectly by representing its partial derivatives \u0435i as a linear combination of radial basic functions (RBFs), i.e. we can choose between i (x) = M [17], but in this work we use Gaussian RBFs, \u03c1j (r) = exp (\u2212 \u03b5jr2). For our network, we use M = 63 Gaussian nuclei, whose centers are evenly distributed, and they all share the same precision parameters \u03b5. Representing its i using mixtures of RBFs is very powerful and allows us to approach arbitrary nonlinear functions with high accuracy, which is an important advantage over conventional regulation methods, which are only effective if we closely embrace the potential of the individual functional layers we have in our network."}, {"heading": "3.3. Color Image Denoising", "text": "To deal with RGB color images, a simple approach would be to use the same network to process each image channel independently, but this would result in suboptimal restoration performance since the network would not be able to explore the existing correlations between the different channels. To circumvent this limitation, we follow a strategy similar to that of [7], and before feeding the noisy color image into the network, we apply the same adverse color transformation that results in one brightness and two color channels. Due to the nature of the color transformation, the luminance channel contains most valuable information about primitive image structures and has a higher signal-tonoise ratio (SNR) than the two color channels. We use this fact, and since the block matching operation can be sensitive to the presence of noise, we perform the grouping of the patches only from the luminance channel. Then, we use exactly the same image of the group that we weighted for another network that we have weighted."}, {"heading": "4. Discriminative Network Training", "text": "We train our network, which consists of the same filter parameters, for grayscale and color images that we use as an objective function, with the image data from the i.i.d Gaussian noise. (D) The network parameters from the i.i.d Gaussian noise. (D) The network parameters from the i.i.d Gaussian noise. (D) The network parameters from the i.i.d Gaussian noise (D) are learned by means of a loss minimization strategy, the Q pairs of train data {y (D), x (D)} Q = 1, where y (G) is a noisy input and x (G) is the corresponding soil truth image. To achieve increased capacity for the network, we learn different parameters for each stage. (D) The overall architecture of the network does not exactly correspond to the gradient method, but to an adaptive version."}, {"heading": "5. Experiments", "text": "In fact, most of us are able to outdo ourselves, \"he told the German Press Agency in an interview with\" Welt am Sonntag \":\" It's not as if the world is in order. \"He pointed out that the US is able to change the world:\" It's not as if the world is in order. \"He pointed out that the US is able to put the world in order,\" but it's not as if the world is in order to keep the world in order. \""}, {"heading": "6. Conclusions and Future Work", "text": "In this paper, we have proposed a novel network architecture for unmasking grayscale and color images. We believe that the design of the resulting models was inspired by non-local variation methods and utilizes the non-local property of self-similarity of natural images. We believe that non-local modeling combined with discriminatory learning are the key factors for the improved recovery performance that our models achieve compared to several current state-of-the-art methods. Meanwhile, the proposed models have direct connections to revolutionary neural networks and can therefore take full advantage of the latest advances in parallel GPU computing in depth. We are confident that image recovery is just one of the many inverse imaging problems that our non-local networks can successfully deal with. We believe that a very interesting direction of research is to investigate the necessary modifications to the design of our current non-local models that would allow them to be efficiently applied to other major reconstruction problems."}, {"heading": "A. Derivative Calculations", "text": "In this section we consider the necessary derivatives for the course of the network functions to be useful."}, {"heading": "B. Grayscale and Color Image Denoising Comparisons", "text": "In this section, we provide additional grayscale and color image results for different noise levels. For grayscale image recognition, we compare the performance of our non-local models with TNRD [6], MLP [5], EPLL [41], and BM3D [7], while for color image identification, we compare our non-local CNN with the state-of-the-art CBM3D method [7]. In addition to visual comparisons, we provide the PSNR score (in dB) of each method in the image captions for quantitative comparison."}, {"heading": "C. Acknowledgments", "text": "The author thanks NVIDIA for supporting this work by donating a Tesla K-40 GPU."}], "references": [{"title": "Flow fields: Dense correspondence fields for highly accurate large displacement optical flow estimation", "author": ["C. Bailer", "B. Taetz", "D. Stricker"], "venue": "Proc. IEEE Int. Conf. on Computer Vision, pages 4015\u20134023", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2015}, {"title": "Introduction to Inverse Problems in Imaging", "author": ["M. Bertero", "P. Boccacci"], "venue": "IOP Publishing", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1998}, {"title": "Total generalized variation", "author": ["K. Bredies", "K. Kunisch", "T. Pock"], "venue": "SIAM J. Imaging Sci., 3:492\u2013526", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2010}, {"title": "Image denoising methods", "author": ["A. Buades", "B. Coll", "J.-M. Morel"], "venue": "A new nonlocal principle. SIAM review, 52:113\u2013147", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2010}, {"title": "Image denoising: Can plain neural networks compete with bm3d? In Proc", "author": ["H.C. Burger", "C.J. Schuler", "S. Harmeling"], "venue": "IEEE Int. Conf. Computer Vision and Pattern Recognition, pages 2392\u20132399", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2012}, {"title": "Trainable nonlinear reaction diffusion: A flexible framework for fast and effective image restoration", "author": ["Y. Chen", "T. Pock"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2016}, {"title": "Image denoising by sparse 3-d transform-domain collaborative filtering", "author": ["K. Dabov", "A. Foi", "V. Katkovnik", "K. Egiazarian"], "venue": "IEEE Trans. Image Process., 16(8):2080\u20132095", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2007}, {"title": "Bm3d frames and variational image deblurring", "author": ["A. Danielyan", "V. Katkovnik", "K. Egiazarian"], "venue": "IEEE Trans. Image Process., 21(4):1715\u20131728", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2012}, {"title": "Analysis versus synthesis in signal priors", "author": ["M. Elad", "P. Milanfar", "R. Rubinstein"], "venue": "Inverse problems, 23(3):947", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2007}, {"title": "Nonlocal discrete regularization on weighted graphs: a framework for image and manifold processing", "author": ["A. Elmoataz", "O. Lezoray", "S. Bougleux"], "venue": "IEEE Trans. Image Proces., 17:1047\u20131060", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2008}, {"title": "Scalable object detection using deep neural networks", "author": ["D. Erhan", "C. Szegedy", "A. Toshev", "D. Anguelov"], "venue": "Proc. IEEE Int. Conf. Computer Vision and Pattern Recognition, pages  2147\u20132154", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2014}, {"title": "Majorization\u2013minimization algorithms for wavelet-  based image restoration", "author": ["M. Figueiredo", "J. Bioucas-Dias", "R. Nowak"], "venue": "IEEE Trans. Image Process., 16:2980\u20132991", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2007}, {"title": "Nonlocal operators with applications 13  (a)  (b)  (c) (d) Figure 8", "author": ["G. Gilboa", "S. Osher"], "venue": "Color image denoising. (a) Original image, (b) Noisy image corrupted with Gaussian noise (\u03c3 = 25) ; PSNR = 20.34 dB. (c) Denoised image using  CNLNet5\u00d75 ; PSNR = 31.14 dB. (d) Denoised image using CBM3D [7] ; PSNR = 30.75 dB. Images are best viewed magnified on screen. Note the differences of the denoised results in the highlighted region. to image processing. Multiscale Model. Simul., 7:1005\u2013 1028", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2008}, {"title": "Weighted nuclear norm minimization with application to image denoising", "author": ["S. Gu", "L. Zhang", "W. Zuo", "X. Feng"], "venue": "Proc. IEEE Int. Conf. Computer Vision and Pattern Recognition, pages 2862\u20132869", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep residual learning for image recognition", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "Proc. IEEE Int. Conf. Computer Vision and Pattern Recognition", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2016}, {"title": "Handbook of neural network signal processing", "author": ["Y.H. Hu", "J.-N. Hwang"], "venue": "CRC press", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2001}, {"title": "Accurate image superresolution using very deep convolutional networks", "author": ["J. Kim", "K. Lee", "K.M. Lee"], "venue": "Proc. IEEE Int. Conf. Computer Vision and Pattern Recognition, pages 1646\u20131654", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2016}, {"title": "Deblurring and denoising of images by nonlocal functionals", "author": ["S. Kindermann", "S. Osher", "P.W. Jones"], "venue": "Multiscale Model. Simul., 4:1091\u20131115", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2005}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "Advances in neural information processing systems, pages 1097\u20131105", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2012}, {"title": "Hessianbased norm regularization for image restoration with biomedical applications", "author": ["S. Lefkimmiatis", "A. Bourquard", "M. Unser"], "venue": "IEEE Trans. Image Process., 21(3):983\u2013995", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2012}, {"title": "Non-local Structure Tensor functionals for image regularization", "author": ["S. Lefkimmiatis", "S. Osher"], "venue": "IEEE Trans. Comput. Imaging, 1:16\u201329", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2015}, {"title": "Structure tensor total variation", "author": ["S. Lefkimmiatis", "A. Roussos", "P. Maragos", "M. Unser"], "venue": "SIAM J. Imaging Sci., 8:1090\u20131122", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2015}, {"title": "Hessian Schattennorm regularization for linear inverse problems", "author": ["S. Lefkimmiatis", "J. Ward", "M. Unser"], "venue": "IEEE Trans. Image Process., 22(5):1873\u20131888", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2013}, {"title": "Non-local sparse models for image restoration", "author": ["J. Mairal", "F. Bach", "J. Ponce", "G. Sapiro", "A. Zisserman"], "venue": "Proc. IEEE Int. Conf. Computer Vision, pages 2272\u20132279", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2009}, {"title": "A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics", "author": ["D. Martin", "C. Fowlkes", "D. Tal", "J. Malik"], "venue": "Proc. IEEE Int. Conf. Computer Vision, pages 416\u2013423", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2001}, {"title": "Numerical optimization", "author": ["J. Nocedal", "S. Wright"], "venue": "Springer Science & Business Media", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2006}, {"title": "Proximal Algorithms", "author": ["N. Parikh", "S. Boyd"], "venue": "Now Publishers", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2013}, {"title": "Faster r-cnn: Towards real-time object detection with region proposal networks", "author": ["S. Ren", "K. He", "R. Girshick", "J. Sun"], "venue": "Advances in neural information processing systems, pages 91\u201399", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2015}, {"title": "Fields of experts", "author": ["S. Roth", "M.J. Black"], "venue": "International Journal of Computer Vision, 82(2):205\u2013229", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2009}, {"title": "Nonlinear total variation based noise removal algorithms", "author": ["L. Rudin", "S. Osher", "E. Fatemi"], "venue": "Physica D, 60:259\u2013268", "citeRegEx": "31", "shortCiteRegEx": null, "year": 1992}, {"title": "Learning representations by back-propagating errors", "author": ["D.E. Rumelhart", "G.E. Hinton", "R.J. Williams"], "venue": "Nature, 323(6088):533\u2013536", "citeRegEx": "32", "shortCiteRegEx": null, "year": 1986}, {"title": "minFunc: unconstrained differentiable multivariate optimization in Matlab", "author": ["M. Schmidt"], "venue": "http://www.cs.ubc. ca/ \u0303schmidtm/Software/", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2005}, {"title": "Shrinkage fields for effective image restoration", "author": ["U. Schmidt", "S. Roth"], "venue": "Proc. IEEE Int. Conf. Computer Vision and Pattern Recognition, pages 2774\u20132781", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2014}, {"title": "Signal restoration with overcomplete wavelet transforms: Comparison of analysis and synthesis priors", "author": ["I. Selesnick", "M. Figueiredo"], "venue": "SPIE ", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2009}, {"title": "Matconvnet \u2013 convolutional neural networks for matlab", "author": ["A. Vedaldi", "K. Lenc"], "venue": "Proceeding of the ACM Int. Conf. on Multimedia", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep Gaussian conditional random field network: A model-based deep network for discriminative denoising", "author": ["R. Vemulapalli", "O. Tuzel", "M.-Y. Liu"], "venue": "Proc. IEEE Int. Conf. Computer Vision and Pattern Recognition, pages 4801\u2013 4809", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2016}, {"title": "Computational Methods for Inverse Problems", "author": ["C.R. Vogel"], "venue": "SIAM", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2002}, {"title": "Image denoising and inpainting with deep neural networks", "author": ["J. Xie", "L. Xu", "E. Chen"], "venue": "Advances in Neural Information Processing Systems, pages 341\u2013349", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2012}, {"title": "Regularization on discrete spaces", "author": ["D. Zhou", "B. Sch\u00f6lkopf"], "venue": "Pattern Recognition, pages 361\u2013368. Springer", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2005}, {"title": "From learning models of natural image patches to whole image restoration", "author": ["D. Zoran", "Y. Weiss"], "venue": "Proc. IEEE Int. Conf. Computer Vision, pages 479\u2013486. IEEE", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2011}], "referenceMentions": [{"referenceID": 14, "context": "Deep learning methods have been successfully applied in various computer vision tasks, including image classification [16,20] and object detection [11,29], and have dramatically improved the performance of these systems, setting the new state-of-the-art.", "startOffset": 118, "endOffset": 125}, {"referenceID": 18, "context": "Deep learning methods have been successfully applied in various computer vision tasks, including image classification [16,20] and object detection [11,29], and have dramatically improved the performance of these systems, setting the new state-of-the-art.", "startOffset": 118, "endOffset": 125}, {"referenceID": 10, "context": "Deep learning methods have been successfully applied in various computer vision tasks, including image classification [16,20] and object detection [11,29], and have dramatically improved the performance of these systems, setting the new state-of-the-art.", "startOffset": 147, "endOffset": 154}, {"referenceID": 27, "context": "Deep learning methods have been successfully applied in various computer vision tasks, including image classification [16,20] and object detection [11,29], and have dramatically improved the performance of these systems, setting the new state-of-the-art.", "startOffset": 147, "endOffset": 154}, {"referenceID": 4, "context": "Recently, very promising results have also been reported for image processing applications such as image restoration [5, 39], super-resolution [18] and optical flow [1].", "startOffset": 117, "endOffset": 124}, {"referenceID": 37, "context": "Recently, very promising results have also been reported for image processing applications such as image restoration [5, 39], super-resolution [18] and optical flow [1].", "startOffset": 117, "endOffset": 124}, {"referenceID": 16, "context": "Recently, very promising results have also been reported for image processing applications such as image restoration [5, 39], super-resolution [18] and optical flow [1].", "startOffset": 143, "endOffset": 147}, {"referenceID": 0, "context": "Recently, very promising results have also been reported for image processing applications such as image restoration [5, 39], super-resolution [18] and optical flow [1].", "startOffset": 165, "endOffset": 168}, {"referenceID": 32, "context": "Only very recently, Schmidt and Roth [34] and Chen and Pock [6] introduced deep networks whose architecture is specifically tailored to certain image restoration problems.", "startOffset": 37, "endOffset": 41}, {"referenceID": 5, "context": "Only very recently, Schmidt and Roth [34] and Chen and Pock [6] introduced deep networks whose architecture is specifically tailored to certain image restoration problems.", "startOffset": 60, "endOffset": 63}, {"referenceID": 6, "context": "A notable example is the Block Matching and 3D Collaborative Filtering (BM3D) method [7] which is a very efficient and highly engineered approach that held the state-of-the-art record in image denoising for almost a decade.", "startOffset": 85, "endOffset": 88}, {"referenceID": 36, "context": "Due to the ill-posedness of the studied problem [38], Eq.", "startOffset": 48, "endOffset": 52}, {"referenceID": 1, "context": "This variational restoration approach has also direct links to Bayesian estimation methods and can be interpreted either as a penalized maximum likelihood or a maximum a posteriori (MAP) estimation problem [2, 13].", "startOffset": 206, "endOffset": 213}, {"referenceID": 11, "context": "This variational restoration approach has also direct links to Bayesian estimation methods and can be interpreted either as a penalized maximum likelihood or a maximum a posteriori (MAP) estimation problem [2, 13].", "startOffset": 206, "endOffset": 213}, {"referenceID": 11, "context": "Synthesis-based regularization takes place in a sparsifying-domain, such as the wavelet basis, and the restored image is obtained by applying an inverse transform [13].", "startOffset": 163, "endOffset": 167}, {"referenceID": 8, "context": "For general inverse problems, the latter regularization strategy has been reported to lead to better reconstruction results [9, 35] and therefore is mostly preferred.", "startOffset": 124, "endOffset": 131}, {"referenceID": 33, "context": "For general inverse problems, the latter regularization strategy has been reported to lead to better reconstruction results [9, 35] and therefore is mostly preferred.", "startOffset": 124, "endOffset": 131}, {"referenceID": 2, "context": "Common choices for L are differential operators of the first or of higher orders such as the gradient [3, 31], the structure tensor [23], the Laplacian and the Hessian [21,24], or wavelet-like operators such as wavelets, curvelets and ridgelets (see [13] and references therein).", "startOffset": 102, "endOffset": 109}, {"referenceID": 29, "context": "Common choices for L are differential operators of the first or of higher orders such as the gradient [3, 31], the structure tensor [23], the Laplacian and the Hessian [21,24], or wavelet-like operators such as wavelets, curvelets and ridgelets (see [13] and references therein).", "startOffset": 102, "endOffset": 109}, {"referenceID": 21, "context": "Common choices for L are differential operators of the first or of higher orders such as the gradient [3, 31], the structure tensor [23], the Laplacian and the Hessian [21,24], or wavelet-like operators such as wavelets, curvelets and ridgelets (see [13] and references therein).", "startOffset": 132, "endOffset": 136}, {"referenceID": 19, "context": "Common choices for L are differential operators of the first or of higher orders such as the gradient [3, 31], the structure tensor [23], the Laplacian and the Hessian [21,24], or wavelet-like operators such as wavelets, curvelets and ridgelets (see [13] and references therein).", "startOffset": 168, "endOffset": 175}, {"referenceID": 22, "context": "Common choices for L are differential operators of the first or of higher orders such as the gradient [3, 31], the structure tensor [23], the Laplacian and the Hessian [21,24], or wavelet-like operators such as wavelets, curvelets and ridgelets (see [13] and references therein).", "startOffset": 168, "endOffset": 175}, {"referenceID": 11, "context": "Common choices for L are differential operators of the first or of higher orders such as the gradient [3, 31], the structure tensor [23], the Laplacian and the Hessian [21,24], or wavelet-like operators such as wavelets, curvelets and ridgelets (see [13] and references therein).", "startOffset": 250, "endOffset": 254}, {"referenceID": 29, "context": "of the above regularizers is the Total Variation (TV) [31], where the regularization operator corresponds to the gradient and the potential function to the `2 vector norm.", "startOffset": 54, "endOffset": 58}, {"referenceID": 9, "context": "More recently, a different regularization paradigm has been introduced where non-local operators are employed to define new regularization functionals [10, 14, 19, 22, 40].", "startOffset": 151, "endOffset": 171}, {"referenceID": 12, "context": "More recently, a different regularization paradigm has been introduced where non-local operators are employed to define new regularization functionals [10, 14, 19, 22, 40].", "startOffset": 151, "endOffset": 171}, {"referenceID": 17, "context": "More recently, a different regularization paradigm has been introduced where non-local operators are employed to define new regularization functionals [10, 14, 19, 22, 40].", "startOffset": 151, "endOffset": 171}, {"referenceID": 20, "context": "More recently, a different regularization paradigm has been introduced where non-local operators are employed to define new regularization functionals [10, 14, 19, 22, 40].", "startOffset": 151, "endOffset": 171}, {"referenceID": 38, "context": "More recently, a different regularization paradigm has been introduced where non-local operators are employed to define new regularization functionals [10, 14, 19, 22, 40].", "startOffset": 151, "endOffset": 171}, {"referenceID": 3, "context": "A nonexhaustive list of these methods is the non-local means filter (NLM) [4], BM3D [7], the Learned Simultaneous Sparse Coding (LSSC) [25], and the Weighted Nuclear Norm Minimization (WNNM) [15].", "startOffset": 74, "endOffset": 77}, {"referenceID": 6, "context": "A nonexhaustive list of these methods is the non-local means filter (NLM) [4], BM3D [7], the Learned Simultaneous Sparse Coding (LSSC) [25], and the Weighted Nuclear Norm Minimization (WNNM) [15].", "startOffset": 84, "endOffset": 87}, {"referenceID": 23, "context": "A nonexhaustive list of these methods is the non-local means filter (NLM) [4], BM3D [7], the Learned Simultaneous Sparse Coding (LSSC) [25], and the Weighted Nuclear Norm Minimization (WNNM) [15].", "startOffset": 135, "endOffset": 139}, {"referenceID": 13, "context": "A nonexhaustive list of these methods is the non-local means filter (NLM) [4], BM3D [7], the Learned Simultaneous Sparse Coding (LSSC) [25], and the Weighted Nuclear Norm Minimization (WNNM) [15].", "startOffset": 191, "endOffset": 195}, {"referenceID": 26, "context": "Since the indicator function \u03b9C is non-smooth, instead of the classical gradient descent algorithm we employ the proximal gradient method [28].", "startOffset": 138, "endOffset": 142}, {"referenceID": 11, "context": "It is also worth noting that this decoupled formulation of the potential function is met frequently in image regularization, as in wavelet regularization [13], anisotropic TV [12] and Field-of-Experts (FoE) [30].", "startOffset": 154, "endOffset": 158}, {"referenceID": 28, "context": "It is also worth noting that this decoupled formulation of the potential function is met frequently in image regularization, as in wavelet regularization [13], anisotropic TV [12] and Field-of-Experts (FoE) [30].", "startOffset": 207, "endOffset": 211}, {"referenceID": 26, "context": "where \u03b3 is a step size and prox\u03b3t\u03b9C is the proximal operator [28] related to the indicator function \u03b9C .", "startOffset": 61, "endOffset": 65}, {"referenceID": 12, "context": "As mentioned earlier, non-local regularization methods have been shown to produce superior reconstruction results than their local counterparts [14,22] for several inverse problems, including image denoising.", "startOffset": 144, "endOffset": 151}, {"referenceID": 20, "context": "As mentioned earlier, non-local regularization methods have been shown to produce superior reconstruction results than their local counterparts [14,22] for several inverse problems, including image denoising.", "startOffset": 144, "endOffset": 151}, {"referenceID": 7, "context": "The non-local operator L : R 7\u2192 RR\u00b7F described above bears strong resemblance to the BM3D analysis operator studied in [8].", "startOffset": 119, "endOffset": 122}, {"referenceID": 7, "context": "(11), while for the operator of [8] a 1D Haar wavelet transform is applied on the group.", "startOffset": 32, "endOffset": 35}, {"referenceID": 15, "context": "There are a few radial functions to choose from [17], but in this work we use Gaussian RBFs, \u03c1j (r) = exp ( \u2212\u03b5jr ) .", "startOffset": 48, "endOffset": 52}, {"referenceID": 6, "context": "To circumvent this limitation, we follow a similar strategy as in [7] and before we feed the noisy color image to the network, we apply the same opponent color transformation which results to one luminance and two chrominance channels.", "startOffset": 66, "endOffset": 69}, {"referenceID": 5, "context": ") TNRD7\u00d77 [6] MLP [5] CBM3D [7] CNLNet5\u00d75 15 31.", "startOffset": 10, "endOffset": 13}, {"referenceID": 4, "context": ") TNRD7\u00d77 [6] MLP [5] CBM3D [7] CNLNet5\u00d75 15 31.", "startOffset": 18, "endOffset": 21}, {"referenceID": 6, "context": ") TNRD7\u00d77 [6] MLP [5] CBM3D [7] CNLNet5\u00d75 15 31.", "startOffset": 28, "endOffset": 31}, {"referenceID": 28, "context": "Color image denoising comparisons for three different noise levels over the standard set of 68 [30] Berkeley images.", "startOffset": 95, "endOffset": 99}, {"referenceID": 5, "context": "The same approach has been followed in [6, 34].", "startOffset": 39, "endOffset": 46}, {"referenceID": 32, "context": "The same approach has been followed in [6, 34].", "startOffset": 39, "endOffset": 46}, {"referenceID": 25, "context": "t the parameters \u0398 we employ the L-BFGS algorithm [27] (we use the available implementation of [33]).", "startOffset": 50, "endOffset": 54}, {"referenceID": 31, "context": "t the parameters \u0398 we employ the L-BFGS algorithm [27] (we use the available implementation of [33]).", "startOffset": 95, "endOffset": 99}, {"referenceID": 5, "context": "(d) Denoised image using TNRD7\u00d77 [6] ; PSNR = 29.", "startOffset": 33, "endOffset": 36}, {"referenceID": 4, "context": "(e) Denoised image using MLP [5] ; PSNR = 29.", "startOffset": 29, "endOffset": 32}, {"referenceID": 13, "context": "(f) Denoised image using WNNM [15] ; PSNR = 29.", "startOffset": 30, "endOffset": 34}, {"referenceID": 6, "context": "(d) Denoised image using CBM3D [7] ; PSNR = 25.", "startOffset": 31, "endOffset": 34}, {"referenceID": 30, "context": "Here, it suffices to say that the gradient of the loss function can be efficiently computed using the back-propagation algorithm [32], which is a clever implementation of the chain-rule.", "startOffset": 129, "endOffset": 133}, {"referenceID": 24, "context": "To train our grayscale and color non-local models we generated the training data using the Berkeley segmentation dataset (BSDS) [26] which consists of 500 images.", "startOffset": 128, "endOffset": 132}, {"referenceID": 28, "context": "We note that the 68 BSDS images of [30] that are used for the comparisons reported in Tables 1 and 2 are strictly excluded from the training set.", "startOffset": 35, "endOffset": 39}, {"referenceID": 34, "context": "The proposed models were trained on a NVIDIA Tesla K-40 GPU and the software we used for training and testing was built on top of MatConvnet [36].", "startOffset": 141, "endOffset": 145}, {"referenceID": 6, "context": ") BM3D [7] LSSC [25] EPLL [41] WNNM [15] CSF7\u00d77 [34] TNRD7\u00d77 [6] DGCRF8 [37] MLP [5] NLNet5\u00d75 NLNet7\u00d77 15 31.", "startOffset": 7, "endOffset": 10}, {"referenceID": 23, "context": ") BM3D [7] LSSC [25] EPLL [41] WNNM [15] CSF7\u00d77 [34] TNRD7\u00d77 [6] DGCRF8 [37] MLP [5] NLNet5\u00d75 NLNet7\u00d77 15 31.", "startOffset": 16, "endOffset": 20}, {"referenceID": 39, "context": ") BM3D [7] LSSC [25] EPLL [41] WNNM [15] CSF7\u00d77 [34] TNRD7\u00d77 [6] DGCRF8 [37] MLP [5] NLNet5\u00d75 NLNet7\u00d77 15 31.", "startOffset": 26, "endOffset": 30}, {"referenceID": 13, "context": ") BM3D [7] LSSC [25] EPLL [41] WNNM [15] CSF7\u00d77 [34] TNRD7\u00d77 [6] DGCRF8 [37] MLP [5] NLNet5\u00d75 NLNet7\u00d77 15 31.", "startOffset": 36, "endOffset": 40}, {"referenceID": 32, "context": ") BM3D [7] LSSC [25] EPLL [41] WNNM [15] CSF7\u00d77 [34] TNRD7\u00d77 [6] DGCRF8 [37] MLP [5] NLNet5\u00d75 NLNet7\u00d77 15 31.", "startOffset": 48, "endOffset": 52}, {"referenceID": 5, "context": ") BM3D [7] LSSC [25] EPLL [41] WNNM [15] CSF7\u00d77 [34] TNRD7\u00d77 [6] DGCRF8 [37] MLP [5] NLNet5\u00d75 NLNet7\u00d77 15 31.", "startOffset": 61, "endOffset": 64}, {"referenceID": 35, "context": ") BM3D [7] LSSC [25] EPLL [41] WNNM [15] CSF7\u00d77 [34] TNRD7\u00d77 [6] DGCRF8 [37] MLP [5] NLNet5\u00d75 NLNet7\u00d77 15 31.", "startOffset": 72, "endOffset": 76}, {"referenceID": 4, "context": ") BM3D [7] LSSC [25] EPLL [41] WNNM [15] CSF7\u00d77 [34] TNRD7\u00d77 [6] DGCRF8 [37] MLP [5] NLNet5\u00d75 NLNet7\u00d77 15 31.", "startOffset": 81, "endOffset": 84}, {"referenceID": 28, "context": "Grayscale image denoising comparisons for three different noise levels over the standard set of 68 [30] Berkeley images.", "startOffset": 99, "endOffset": 103}, {"referenceID": 5, "context": "[6], while the results of DGCRF8 are taken from [37] .", "startOffset": 0, "endOffset": 3}, {"referenceID": 35, "context": "[6], while the results of DGCRF8 are taken from [37] .", "startOffset": 48, "endOffset": 52}, {"referenceID": 28, "context": "In Table 1 we report comparisons of our proposed NLNet5\u00d75 and NLNet 5 7\u00d77 models with several recent stateof-the-art denoising methods on the standard evaluation dataset of 68 images [30].", "startOffset": 183, "endOffset": 187}, {"referenceID": 4, "context": "From these results we observe that both our non-local models lead to the best overall performance, with the only exception being the case of \u03c3 = 50 where the MLP denoising method [5] achieves a slightly better average PSNR compared to that of NLNet5\u00d75.", "startOffset": 179, "endOffset": 182}, {"referenceID": 6, "context": "An important remark to make here is that most of the denoising methods that were considered previously have been explicitly designed to treat single-channel images, with the most notable exception being the BM3D, for which it indeed exists a color-version (CBM3D) [7].", "startOffset": 264, "endOffset": 267}, {"referenceID": 5, "context": "For grayscale image denoising we compare the performance of our non-local models with TNRD [6], MLP [5], EPLL [41] and BM3D [7], while for color image denoising we compare our non-local CNN with the state-of-the-art CBM3D method [7].", "startOffset": 91, "endOffset": 94}, {"referenceID": 4, "context": "For grayscale image denoising we compare the performance of our non-local models with TNRD [6], MLP [5], EPLL [41] and BM3D [7], while for color image denoising we compare our non-local CNN with the state-of-the-art CBM3D method [7].", "startOffset": 100, "endOffset": 103}, {"referenceID": 39, "context": "For grayscale image denoising we compare the performance of our non-local models with TNRD [6], MLP [5], EPLL [41] and BM3D [7], while for color image denoising we compare our non-local CNN with the state-of-the-art CBM3D method [7].", "startOffset": 110, "endOffset": 114}, {"referenceID": 6, "context": "For grayscale image denoising we compare the performance of our non-local models with TNRD [6], MLP [5], EPLL [41] and BM3D [7], while for color image denoising we compare our non-local CNN with the state-of-the-art CBM3D method [7].", "startOffset": 124, "endOffset": 127}, {"referenceID": 6, "context": "For grayscale image denoising we compare the performance of our non-local models with TNRD [6], MLP [5], EPLL [41] and BM3D [7], while for color image denoising we compare our non-local CNN with the state-of-the-art CBM3D method [7].", "startOffset": 229, "endOffset": 232}], "year": 2017, "abstractText": "We propose a novel deep network architecture for grayscale and color image denoising that is based on a non-local image model. Our motivation for the overall design of the proposed network stems from variational methods that exploit the inherent non-local self-similarity property of natural images. We build on this concept and introduce deep networks that perform non-local processing and at the same time they significantly benefit from discriminative learning. Experiments on the Berkeley segmentation dataset, comparing several state-of-the-art methods, show that the proposed non-local models achieve the best reported denoising performance both for grayscale and color images for all the tested noise levels. It is also worth noting that this increase in performance comes at no extra cost on the capacity of the network compared to existing alternative deep network architectures. In addition, we highlight a direct link of the proposed non-local models to convolutional neural networks. This connection is of significant importance since it allows our models to take full advantage of the latest advances on GPU computing in deep learning and makes them amenable to efficient implementations through their inherent parallelism.", "creator": "LaTeX with hyperref package"}}}