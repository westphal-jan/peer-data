{"id": "1611.06777", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Nov-2016", "title": "Effective Deterministic Initialization for $k$-Means-Like Methods via Local Density Peaks Searching", "abstract": "The $k$-means clustering algorithm is popular but has the following main drawbacks: 1) the number of clusters, $k$, needs to be provided by the user in advance, 2) it can easily reach local minima with randomly selected initial centers, 3) it is sensitive to outliers, and 4) it can only deal with well separated hyperspherical clusters. In this paper, we propose a Local Density Peaks Searching (LDPS) initialization framework to address these issues. The LDPS framework includes two basic components: one of them is the local density that characterizes the density distribution of a data set, and the other is the local distinctiveness index (LDI) which we introduce to characterize how distinctive a data point is compared with its neighbors. Based on these two components, we search for the local density peaks which are characterized with high local densities and high LDIs to deal with 1) and 2). Moreover, we detect outliers characterized with low local densities but high LDIs, and exclude them out before clustering begins. Finally, we apply the LDPS initialization framework to $k$-medoids, which is a variant of $k$-means and chooses data samples as centers, with diverse similarity measures other than the Euclidean distance to fix the last drawback of $k$-means. Combining the LDPS initialization framework with $k$-means and $k$-medoids, we obtain two novel clustering methods called LDPS-means and LDPS-medoids, respectively. Experiments on synthetic data sets verify the effectiveness of the proposed methods, especially when the ground truth of the cluster number $k$ is large. Further, experiments on several real world data sets, Handwritten Pendigits, Coil-20, Coil-100 and Olivetti Face Database, illustrate that our methods give a superior performance than the analogous approaches on both estimating $k$ and unsupervised object categorization.", "histories": [["v1", "Mon, 21 Nov 2016 13:26:37 GMT  (871kb,D)", "http://arxiv.org/abs/1611.06777v1", "16 pages, 9 figures, journal paper"]], "COMMENTS": "16 pages, 9 figures, journal paper", "reviews": [], "SUBJECTS": "cs.LG cs.CV", "authors": ["fengfu li", "hong qiao", "bo zhang"], "accepted": false, "id": "1611.06777"}, "pdf": {"name": "1611.06777.pdf", "metadata": {"source": "CRF", "title": "Effective Deterministic Initialization for k-Means-Like Methods via Local Density Peaks Searching", "authors": ["Fengfu Li", "Hong Qiao", "Bo Zhang"], "emails": [], "sections": [{"heading": null, "text": "Index terms - clustering, k-means, k-medoids, search for local density tips, deterministic initializationF"}, {"heading": "1 INTRODUCTION", "text": "In fact, it is so that most of them are able to survive themselves, and that they are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...)"}, {"heading": "2 RELATED WORKS", "text": "The k-means algorithm mGiven a data set X = {x (i) improved Rp | i = 1, 2,., m}, the k-means algorithm is a)., c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c), c \"c, c\" c, c, c \"c, c\" c, c, c \"c, c, c\" c, c, c \"c, c, c\" c, \"c,\" c, \"c,\" c, \"c,\" c, \"c,\" c, \"c,\" c, \"c,\" c., \"c,\" c, \"c,\" c, \"c,\" c, \"c,\" c, \"c,\" c, \"c,\" c, \"c,\" c, \"c,\" c, \"c,\" c, \"c,\" c, \"c,\" c, \"c,\" c, \"c,\" c, \"c,\" c, \"c,\" c, \"c,\" c, \"c,\" c, \"c,\" c, \"c,\" c, \"c,\" c, \"c,\" c, \"c,\" c, c, \"c,\" c, \"c,\" c, \"c,\" c, c, \"c,\" c, \"c,\" c, \"c,\" c, \"c, c, c,\" c, \"c,\" c, \"c, c, c,\" c, \"c,\" c, \"c,\" c, c, \"c,\" c, \"c, c,\" c, \"c,\" c, \"c,\" c, \"c,\" c, \"c,\" c, \"c,\" c., \"c,\" c, \"c,\" c, \"c,\" c, \",\" c, \"c,\" c, \"c,\", \"c,\" c, \"c,\" c., \"c,\", \"c,\" c, \"c,\" c, \",\" c, \",\" c,"}, {"heading": "2.3 Clustering by fast search and find of density peaks", "text": "Clustering by Quick Searching and Finding Density Tips (CFSFDP) [7] is a novel cluster center with a relatively high density. It characterizes a cluster center with a higher density than its neighbors and a relatively large distance from other high density points. However, the density of x (i) is defined as\u03c1i = m \u2211 j = 1 (dij \u2212 dc) (5), where the distance of x (i) = 1 if z < 0 and vice versa dij is the distance between x (i) and x (j) and dc is a boundary distance. Intuitively, \u03c1i corresponds to the number of points whose distance from x (i) is smaller than dc. Another measure we call the global differentiation index (GDI) is the minimum distance between x (i) and any other high density point."}, {"heading": "3 LOCAL DENSITY AND LDI BASED INITIALIZATION", "text": "FOR k-MEANS-LIKE METHODS In this section, we propose an initialization frame for the k-mean and k-medoid algorithms. Fig. 2 illustrates the overall diagram for the cluster frame, with Part I being the initialization frame. First, we present two basic measurements: one is local density and the other is the Local Distinction Index (LDI). Based on these two measurements, we propose a search algorithm for local density peaks to find the local density peaks that can be used to estimate the number of k clusters, as well as the initial grain for k-mean or k-medoids. In addition, outliers can be detected and removed using the measurements. Below is a detailed description of the proposed method."}, {"heading": "3.1 Local density and local distinctiveness index", "text": "The local density is the density distribution of a dataset. We use the Kernel Density Estimate (KDE) (KDE) [33] to calculate the local density. Suppose the samples X are generated from a random distribution with an unknown density. Then, KDE is the distance between x (i) and x (j), and K (z) (z) is a kernel function that is satisfactory that 1) K (z) \u2265 0, 2) K (z) dz = 1) K (\u2212 z) = K (z). A popular choice for the kernel function is the default Gaussian function: K (z) = 1. It is known that the Gaussian function is compared with a uniform kernel function."}, {"heading": "3.2 Local density peaks searching", "text": "In the CFSFDP algorithm, a density peak is characterized by a higher density than its neighbors and a relatively large distance from the points with higher densities. We use the similar idea to search for the local density peaks by assuming that a local density level should have a high local density and a large local differentiation index. Figure 4 provides an intuitive explanation of how this works. To find the local density peaks, we introduce a quantitative measurement gap to estimate the potential of a point that should be a local density peak. To define the local distinctiveness, the local value is defined as: (1 \u2212 12) 2 \u2212 1 2 (1 \u2212 2) -2 (1 \u2212 2)."}, {"heading": "3.3 Estimating k via local density peaks searching", "text": "k averages and k medoids require the number k of clusters as input. However, it is not always easy to determine the best value of k [20]. Therefore, learning k is a fundamental problem for the cluster algorithms. In this case, we use the LDPS algorithm to estimate k, which equals the number of local density tips. In algorithm 1, the minimum gap of \u03b3c value between the selected local density tips and the other points. In this case, we set the estimated k to \u2212 1.Compared with x averages and dipping agents, which are incremental methods that use the splitting / merging rules to estimate k, the estimated k method will fail if the resulting p value is too small. In this case, we set the estimated k to x averages and dipping agents, which are incremental methods that use the splitting / merging rules to estimate k, our method does not need to use ergonomic subsets of k, which are more manually splitted, and therefore the FSF rules are used to compose the mean values, and thus are more stable."}, {"heading": "3.4 Selecting initial seeds with local density peaks", "text": "The selection of suitable initial seeds for the cluster centers is a very important initialization step and plays an essential role for k means and k medoids to function properly. At this point, we assume that we have already known the true number k of clusters (either specified by the user or estimated by using the LDPS algorithm). Let's name the true number of clusters used for the cluster algorithms. If the true number k elements of clusters is not specified by the user in advance, we use the estimated k as the k value. In addition, let's take the local density tips {x (i) | i-i-ldp} obtained by the LDPS algorithm as the initial seeds. In fact, we select the first k elements with the leading \u03b3c values as the initial seeds, that is, Is = k-p-p-i = 1 {Icsi}, where the initial seeds avoid the initial seeds being the indices of the initial seeds because they are relatively dense by the geometric (11)."}, {"heading": "3.5 Outliers detection and removal", "text": "Here we develop a simple algorithm for detecting and eliminating the outliers, based on local density and LDI. First, we define the \u03b3o value as follows: \u03b3oi = (1 \u2212 12 \u03c12i \u2212 1 2 (1 \u2212 \u03b4li) 2) 2. (12) This definition is similar to \u03b3c, except that \u03b3o is a decreasing function and \u03b3c increases as it increases. Low density but high LDIs receive high \u03b3o values and are therefore considered outliers. 7 Secondly, we use a threshold of \u03b3o, which is denoted by \u03b3ot, to identify the outliers with the principle that \u03b3o the outlier should be greater than \u0421ot. For example, if we set \u0421o = 0.95, the points are treated as outliers with \u0421o = 0.1 < 0.1 and \u0421l > 0.8."}, {"heading": "3.6 Model selection for the LDPS algorithm", "text": "The accuracy of the estimation of k obtained by the LDPS algorithm strongly depends on the bandwidth h for the local density estimate and the neighbourhood quantity r for the calculation of LDI. Use \u03b8 = (h, r) to denote the (normalized) parameters, where h = h / d * and r = r / d *. Fig. 5 (a) shows the results of the LDPS algorithm with different parameters \u03b8 on the R15 dataset. As shown in Fig. 5 (a), the estimated k of the soil truth k * is only the same if the parameters \u03b8 are correctly selected."}, {"heading": "3.6.1 Parameters choosing by grid search", "text": "In many real-world applications, we do not know beforehand what the true number k * of clusters is. Therefore, we need to define certain criteria to evaluate the estimated number k of clusters and to select the model to optimize the criterion. Mathematically, it can be used as a criterion to evaluate how good the estimated k will be. As discussed in Section 3.3, \u03c4 * indicates the maximum gap of the \u03b3c value between the selected local density peaks and the other points. Mathematically, it can be written as a function of the dissimilarity matrix D with parameters \u03b8 (see Algorithm 1). The parameters that maximize the difference lead to the most pronounced local density peaks. Therefore, we select the parameters by solving the optimization problem: The dissimilarity matrix D = arg max."}, {"heading": "4 LDPS-MEANS AND LDPS-MEDOIDS", "text": "In the previous section, we proposed the LDPS algorithm for initializing k-means and k-medoids. However, for k-means any kind of dissimilarity matrix D can be used as input. In view of this difference, they use different procedures for updating the cluster centers. In this section, we propose two novel cluster procedures, LDPS means (algorithm 2) and LDPS medoids (algorithm 3), as a combination of the LDPS initialization algorithm (algorithm 1) with the cluster procedures of k-means and k-medoids respectively. Your cluster framework is implemented as shown in Fig. 2.Algorithm 2: The LDPS-means algorithm means dissimilarity: XDDPS means-means: XDDPS means-means: XDDPS means-means XDPS means: XDPS-means XD: means XDPS-means XDmeans: XD: means XDPS-means: XD: means XDPS means: XDPS means: XDPS means XDmeans: XD means: XDPS means: XDPS means: XDPS means: XDPS means: XDPS means: XDPS means: means XDmeans: XDPS means: XDPS means: XDPS means: XDPS means: XDPS means: means: XDPS means: XDPS means: XDPS means: means: XDPS means: XDPS means: XDPS means: XDPS means: XDPS means: XDPS means XDPS means: DPS means: XDPS means: XDPS means XDPS means: XDPS means XDPS means: means: DPS means: XDPS means: XDPS means XDPS means: means: DPS means: XDPS means: XDPS means: DPS means XDPS means XDPS means: XDPS means: XDPS means: DPS means: DPS means: DPS means XDPS means: DPS means: DPS means: DPS means XDPS means: DPS means: DPS means: DPS means: DPS means: DPS means XDPS"}, {"heading": "4.1 Dissimilarity measures for LDPS-medoids", "text": "A deviation measurement is the inversion of a deviation measurement [35], which is a real word function that quantifies the similarity between two objects. It can be regarded as a kind of distance without fulfilling the distance axioms. It evaluates the deviation between the data samples, and the larger it is, the less similar the samples are. 8 Algorithm 3 The LDPS Medoid algorithm is used as input: D, k (optional), h, r, \u03b3ot output: Ic, Io and \u0432, 1: Execute the LDPS algorithm to obtain the visual deviations, \u03b4l, Ildp and \u03c4, 2: If the k algorithm is not given, then guess k = Ildp | and put 4: End, if 5: Select-Samen with indices is the desired-desired-desired-desired-desired-desired-desired-desired-desired-desired-desired-desired-desired-desired-desired-desired-desired-desired-desired-desired-desired-desired-desired-desired-desired-desired-desired-desired-desired-desired-desired-desired-desired-desired-desired-desired-desired-desired-desired-desired-desired-desired-desired-desired-desired-desired-desired-desired-desired-desired-desired-indices (11); 6: If the k algorithm is not given, then 3: Estimate the k algorithm with k = k = Ildness-desired-desired-desired-desired-desired-desired-desired-desired-desired-desired-desired-desired-desired-desired-desired-desired-desired-desired-desirable-desired-desired-desired-desired-desired-desired-desired-desired-desired-desired-desired-desired-desired-desired-desired-desired-desired-desired-desired-ness-desired-desired-desired-desired"}, {"heading": "5 PERFORMANCE ANALYSIS", "text": "In this section, we analyze the performance of local density and LDI-based cluster methods. To simplify the analysis, we start from the following assumptions: 1) The clusters are spherically distributed, 2) each cluster has a constant number (m0) of data points, 3) the clusters do not overlap and can be separated by k-means with suitable initial data. We use both the final SSE and the number of iterations (updates) as criteria for evaluating the performance of k-means and LDPS-means. Among the above assumptions, we have the following results. Theorem 1. Among the assumptions 1) -3) above, the average number of repetitions that k-means need to achieve the competitive performance of LDPS-means is O (ek)."}, {"heading": "6 EXPERIMENTS", "text": "In this section, we conduct experiments to evaluate the effectiveness of the proposed methods. Experiments consist mainly of two parts: one is to evaluate the performance of the LDPS algorithm in estimating k and the other is the cluster performance of LDPS means and LDPS medoids obtained by the deterministic LDPS initialization algorithm (algorithm 1). We also evaluate the impact of the outlier detection and removal method on the performance of the cluster algorithm. All experiments are performed on a single PC with Intel i7-4770 CPU (4 cores) and 16G RAM."}, {"heading": "6.1 The compared methods", "text": "In the first part of the estimation of k averages, we compare the LDPS method with x averages, dip averages and CFSFDP based on the estimation of cluster number k. the x mean is parameter-free. For dip averages, we set the significance level \u03b1 = 0 for the dip test and the vote share vthd = 1% as in [22]. For CFSFDP, we follow the suggestion in [7] to select dc so that the average number of neighbors is about 1% to 2% of the total number of data points in the dataset. Formula (13) is used to estimate the parameters in the LDPS algorithm. We designate LDPS with the square of the euclidean distance and the manifold-based inequality as LDPS (E) and LDPS (M). In the cluster part, we compare the cluster performance of LDPS averages and LDPS medoids with k averages and k averages, respectively."}, {"heading": "6.2 The data sets", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6.2.1 Overview of the data sets", "text": "For evaluation, we use both synthetic data sets and real data sets. Four different types of synthetic data sets are used: the A-sets [41] have different cluster numbers, k, the S-sets [42] have different data distributions, the dimensions vary with dimensionality p and the shape sets [7] have different shapes. They can be downloaded from the web page of cluster data sets 1. We have made certain changes to the S-sets and the dimensions, as these two sets can be easily separated from each other. For more details, see Section 6.4. Real data sets include handwritten pendigits [43], Coil-20 [44], Coil-100 [45] and Olivetti Face Database [46]."}, {"heading": "6.2.2 Attribute normalization", "text": "In cluster tasks, normalization of attributes is an important pre-processing step to prevent attributes with large ranges from dominating the calculation of distance. In addition, it helps to obtain more accurate numerical calculations. In our experiments, the attributes are generally normalized to the interval [0, 1] by min-max normalization. In particular, (x (i) j) normalizes = x (i) j \u2212 xminjxmaxj \u2212 xminjwo x (i) j normalizes the jth attribute of the data point x (i), xminj = minl x (l) j and x max j = maxl x (l) jist. For gray images with pixel values in [0, 255], we simply normalize the pixels by dividing 255."}, {"heading": "6.3 Performance criteria", "text": "To estimate k, we use the simple criterion that better performance is achieved when the estimated k rate is close to the basic truth k *. # 1. http: / / cs.joensuu.fi / sipu / datasets / For the task of clustering synthetic data sets, three criteria are used to evaluate the performance of the initial seeds: 1) the total CPU time cost to achieve SSE *, 2) the number of repetitions (# repe) required to achieve SSE * and 3) the number of assignment siterations (# iter) when SSE * is achieved in the repetition. We first perform LDPS means to achieve a cap for SSE *, which is called SSE * 0. We perform k means repeatedly and sequentially up to 1000 times and capture the minimum SSE * designated by SSE * k."}, {"heading": "6.4 Experiments on synthetic data sets", "text": "We use four types of synthetic data sets. The A-sets contain three two-dimensional sets A1, A2 and A3 with different numbers of circular clusters (k \u0445 = 20, 35, 50). Each cluster has 150 data points. We create a new set A0 by using five clusters of A1 with the designations 1 \u2212 5. The S-sets S1 to S4 consist of the data points, which are sampled from two-dimensional Gaussian clusters N (\u00b5, \u03a3) with 100 data points each in each cluster. Their centers are the same as those of the min-max normalized A3 set. We set the target value: p = \u03c3 \u00b7 I2, where the target value is 0.002, 0.006 and 0.008, with I2 the identity matrix in R2.We also create four Dim sets Dp with the dimensions p = 3, 6, 9, 12. The Dim sets are Gaussian sets, which distribute in multi-dimensional spaces."}, {"heading": "6.4.1 Performance on the estimation of k", "text": "The results of the estimated k are summarized in TABLE 1. This table shows that the x averages are not split for most datasets, but get the correct result for the D12 group. dip-means achieves better results than x averages in most cases, but it underestimates k for most datasets. In particular, dip-means does not detect valid clusters in D6, D9 and D12 due to its relatively high dimensionality. CFSFDP achieves better results than dip averages for most datasets. Although CFSFDP has underestimated k for the A3, S3, S4 and all Dim sets, it gets a very good estimate of k (very close to the true cluster number k) for shape sets. Note that CFSFDP fails in the flame group, compound group and aggregation group Lim compared to most DAME sets."}, {"heading": "6.4.2 Clustering performance", "text": "We first compare the cluster performance of the LDPS averages with k averages and k averages + + on the A-sets to verify the result of theorem 1. the experimental results are listed in TABLE 2. As shown in TABLE 2, the cluster performance of the LDPS averages gets much better with increasing k-sets. On sets A2 and A3, the LDPS mean significantly exceeds the k averages and k averages + +. This is consistent with Theorem 1. We then conduct experiments on S-sets and Dim-sets to demonstrate the ability of the LDPS averages to separate clusters with varying complexity of data distributions and dimensions, respectively. Experimental results are listed in TABLE 3. The table shows that compared to k averages and k averages + +, the LDPS averages based on most SDD averages take much less time, and finally, the SD averages estimated by much less time."}, {"heading": "6.5 Experiments on Handwritten Pendigits", "text": "We are now conducting experiments with the real data set Handwritten Pendigits to evaluate the performance of LDPS averages and LDPS medoids in clustering for general purposes. This data set can be downloaded from the UCI Machine Learning Repository2. The Handwritten Pendigits data set contains a total of 10992 data points with 16-dimensional characteristics, each representing a digit from 0 \u2212 9 written by a human subject. The data set consists of a training data set PDtr10 and a test data set PD te 10 with 7494 and 3498 samples, respectively. In addition to the complete data set, we are also considering three subsets containing the digits {1,3,5} (PDtr3 and PDte3), {0,2,4,6,7} (PDtr5 and PDte5) and {0,1,2,3,4,5,6,7} (PDte8 and PDte8)."}, {"heading": "6.5.1 Performance on the estimation of k", "text": "In Table 5, the results of the k estimate on the handwritten pendigy. x-means fails on all of these datasets. Dip-means also fails on all of these datasets, although it comes closest to the true cluster number on the PDtr10 set when compared to all other comparison methods. CFSFDP (E) and CFSFDP (M) get an underestimated k in most cases. Compared to the CFSFDP methods, the LDPS methods get the correct k due to the use of multiple distance in most datasets. LDPS (M) achieves better results than the LDPS (E) on the PDtr10 and PD 10 sets."}, {"heading": "6.5.2 Clustering performance", "text": "We now compare the unattended object classification performance of LDPS averages and LDPS medoids with k averages and k medoids. Results are presented in TABLE 6. As shown in the table, LDPS averages perform better than k averages on most data sets, with the exception of the rtrue criterion for PDtr10, PD te 8 and PD te 10. LDPS (E) fails to estimate the correct k. k medoids achieves better results than other comparison methods due to the use of multiple distance as a measure of dissimilarity. However, LDPS medoids achieve the best results on all data sets with all criteria. 2. http: / / archive.ics.uci.edu / ml / 12"}, {"heading": "6.6 Experiments on Coil-20", "text": "We now look at the real-world data set, Coil-20 [44], which is used for the task of unattended clustering of objects. Coil-20 contains 20 objects, and each object contains 72 images taken 5 degrees apart while the object rotates on a turntable. We compress each image into 32 x 32 pixels at 256 grayscale per pixel. To compare the performance of the comparison methods with different categories, we select three subgroups of Coil-20: Coil-5 (objects 1,3,5,7 and 9), Coil-10 (objects with even numbers), Coil-15 (objects other than 3,7,11,15 and 19). Figure 7 shows some examples of the twenty objects. We use the CW-SSIM index and t-nn to construct the 13TABLE 10 clustering performance comparison on the large coil sets, where re is the error rate, rrt stands for the rate of true association, the fraction is the same category that paints the images."}, {"heading": "6.6.1 Performance on the estimation of k", "text": "The estimated cluster number k of coil-20 and its subsets is represented in TABLE 7. x averages, on the other hand, miss a reasonable estimate of k for these sets. Dip averages, CFSFDP (E) and LDPS (E) also do not obtain a meaningful number of clusters, because the Euclidean distance cannot properly measure the differences between these objects. Compared with these four methods, CFSFDP (M) achieves better results, although it has k on coil-15 and coil-20. LDPS (M) obtained reasonable estimates of k for most datasets. It underestimates k on coil-20, as the objects 3, 6 and 19 are very similar (see Fig. 7); they are assigned to the same category of \"cars\" by the LDPS (M) algorithm."}, {"heading": "6.6.2 Clustering performance", "text": "Table 8 shows the cluster results on the coil sets. In contrast to the results on the pendigits sets, LDPS averages perform worse than k averages on these sets. This may be because LDPS (E) failed to estimate the correct k on the coilsets. LDPS medoids, on the other hand, learned the correct k on these sets and selected the initial seeds with high quality. As a result, the cluster performance of LDPS medoids is much better than that of any other comparison methods on the coil sets."}, {"heading": "6.7 Experiments on Coil-100", "text": "Coil-100 is the third set of real-world data we have considered. Unlike Coil-20, which has a small number of categories, the true cluster number k-100 of Coil-100 is very large. Therefore, it is used for the task of forming unattended object clusters with a large number of categories. Coil-100 data set contains 100 object categories consisting of 7200 color images, and each object has 72 images taken 5 degrees apart as the object rotates on a turntable. Color images are converted to gray images and reduced to 32 x 32 pixels. We select three subsets of Coil-100, namely Coil-25 (objects 1, 5, 9, \u00b7 \u00b7 \u00b7, 93 and 97), Coil-50 (objects with even numbers), and Coil-75 (coil-25 + Coil-50). The varied distance is calculated using the graph distance using the CW-SSW index and the neighborhood-n-4."}, {"heading": "6.7.1 Performance on the estimation of k", "text": "Since the number of clusters of coil-100 is large, the density of certain local density peaks can easily be dominated by the highest density. Therefore, it is very difficult to obtain a balanced local density distribution for the local density peaks. To deal with this difficulty, the local density is normalized as follows: \u03c1 = (\u03c1\u03c1) 1 / 4. (14) The main purpose of local density normalization (14) is to increase the relatively small local densities. In this case \u03c1 is also in the range of (0, 1] and is in the same increasing order as the original \u03c1.TABLE 9 shows the results of the estimated cluster number k on the large coil sets. LDPS methods use the new normalized local density \u03c1 (defined in (14)) as the local density value."}, {"heading": "6.7.2 Clustering performance", "text": "The new normalized local density \u03c1, defined in (14), is also used for this task. Based on the new normalized local density \u03c1, the cluster results are presented in TABLE 10. LDPSmedoids again receives the best cluster results compared to 14TABLE 11 results of the estimated k of the comparison methods on the olive sets. Data sets x-means dip-means CFSFDP (E) CFSFDP (M) LDPS (E) Oliv.-10 18 1 4 4 4 4 10 Oliv.-20 33 1 1 1 1 18 5 20 Oliv.-30 45 1 1 21 \u2212 1 30 Oliv.-40 56 1 28 \u2212 1 36TABLE 12 Cluster performance comparison of the comparison methods on the olive sets where re is the error rate, rt stands for the rate of true association, which is the fraction of the pairs of images from the same true category correctly placed in the same learned category."}, {"heading": "6.8 Experiments on Olivetti Face Database", "text": "The last real data set is the Olivetti Face Database, which is used for unattended face grouping. Olivetti Face Database is formerly the ORL Database of Faces, which consists of 400 face images of 40 individuals. Images are taken at different times, with different illumination, facial expression and facial details. The size of each image is 64 x 64 pixels. Here, too, we select three subsets: olive-10 (faces 2, 6, 10, \u00b7 \u00b7, 34 and 38), olive-20 (faces with odd numbers) and olive-30 (olive-10 + olive-20). The entire Olivetti Face Database is called olive-40. Fig. 9 shows some sample areas of the Olivetti Face Database.The multiple removal of the olive sets is approximated by graph distance with the CW-SSIM index and the 3-nn neighborhood."}, {"heading": "6.8.1 Performance on the estimation of k", "text": "Due to the limited samples in each category (m0 = 10) and the high dimensionality (p = 4096), it is a difficult task to estimate k for the olive quantities, but a relatively large number of Fig. 9. The first two categories of olive-40. The images of the first person vary with the angle of view to the camera, and the images of the second person differ greatly from the facial expressions. Table 11 lists the estimated k of the comparison methods on the olive sets. X-mean, dip medium, CFSFDP (E) and LDPS (E) all do not learn a reasonable k. The estimated k of olive-40 of the CFSFDP with the CWSSIM index is about 30 [7], which is very close to k compared to the previous methods. CFSFDP (M) learns a reasonable k for olive-10 and olive-20, but severely underestimated k for olive-30 and olive-40. LDPS (see set consistency number) comes closer to the first truth with the olive-20."}, {"heading": "6.8.2 Clustering performance", "text": "TABLE 12 shows the cluster results on the olive sets. Kmeans and LDPS averages achieved poor results on the last three olive sets due to their use of the Euclidean distance as a measure of dissimilarity. Using the multiple distance, k-medoids and LDPS-medoids achieved much better results than k-medoids and LDPS-medoids. LDPS-medoids outperform k-medoids with the correctly selected starting seeds. In addition, LDPS-medoids receive when setting k to 42 for olive-40 re = 18.5%, rt = 74.0% and rf = 0.9%. This improves re by 15.9%, rt by 8.8% and rf by 25% compared to the results reported in [7], where re = 22.0%, rt = 68% and rf = 1.2%."}, {"heading": "7 CONCLUSION AND FUTURE WORKS", "text": "In this paper, we have proposed a new method, the LDPS algorithm, to learn the appropriate number of clusters and deterministically select the initial seeds of clusters with15high quality for the k-means-like methods. In addition, two new methods, LDPS means and LDPS medoids, have also been proposed as a combination of the LDPS initialization algorithm and the k-means-like clustering algorithm. Performance analysis and experimental results have shown that our methods have the following advantages: 1) The LDPS algorithm can learn an appropriate number of clusters for datasets with balanced samples in each category. Furthermore, it can handle a variety of data distributions with an appropriate dissimilarity measurement. 2) The initial seeds selected by LDPS means / LDPSmedoids are geometrically close to the centers of clusters. As a result, our methods can achieve a very good SSE solution, sometimes achieved by means of the means."}, {"heading": "APPENDIX A PROOF OF THEOREM 1", "text": "In order to prove the theorem, we need two basic results in mathematical analysis [48]. Lemma 1. lim n \u2192 \u221e (n!) 1 nn = 1 e.Lemma 2. lim n \u2192 \u221e (1 + 1n) n = e.Proof of Theorem 1: First, local density peaks would be geometrically close to the center of the clusters as analyzed in sec. 3.2 under conditions 1) -3). With the local density peaks as the starting seed, the LDPS mean could separate the cluster with only O (1) iterations. Second, the k mean achieves competitive performance when the starting seeds are selected with each cluster. The statistical event that the mean could achieve this in a repetition forms a Bernoulli distribution [49], with probability P (success) cond. 2) = mk0 / (mk) mk).Furthermore, the distribution of P (success) between two different repetitions is independent and identical."}], "references": [{"title": "A survey of clustering data mining techniques", "author": ["P. Berkhin"], "venue": "Group. Multidimens. Data. Springer, 2006, pp. 25\u201371.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2006}, {"title": "The importance of encoding versus training with sparse coding and vector quantization", "author": ["A. Coates", "A.Y. Ng"], "venue": "Proc. 28th ICML, 2011, pp. 921\u2013928.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2011}, {"title": "Randomized dimensionality reduction for k-means clustering", "author": ["C. Boutsidis", "A. Zouzias", "M.W. Mahoney", "P. Drineas"], "venue": "IEEE Trans. Inf. Theory, vol. 61, no. 2, pp. 1045\u20131062, 2015.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning manifolds with k-means and k-flats", "author": ["G. Canas", "T. Poggio", "L. Rosasco"], "venue": "Proc. NIPS, 2012, pp. 2465\u20132473.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2012}, {"title": "A density-based algorithm for discovering clusters in large spatial databases with noise", "author": ["M. Ester", "H.-P. Kriegel", "J. Sander", "X. Xu"], "venue": "KDD, vol. 96, no. 34, 1996, pp. 226\u2013231.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1996}, {"title": "Semi-supervised kernel mean shift clustering", "author": ["S. Anand", "S. Mittal", "O. Tuzel", "P. Meer"], "venue": "IEEE Trans. Pattern Anal. Math. Intell., vol. 36, no. 6, pp. 1201\u20131215, 2014.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2014}, {"title": "Clustering by fast search and find of density peaks", "author": ["A. Rodriguez", "A. Laio"], "venue": "Sci., vol. 344, no. 6191, pp. 1492\u20131496, 2014.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2014}, {"title": "Algorithms for hierarchical clustering: an overview", "author": ["F. Murtagh", "P. Contreras"], "venue": "Data Min. and Knowl. Discov., vol. 2, no. 1, pp. 86\u201397, 2012.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2012}, {"title": "Data clustering: 50 years beyond k-means", "author": ["A.K. Jain"], "venue": "Pattern Recogn. Letters, vol. 31, no. 8, pp. 651\u2013666, 2010.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2010}, {"title": "A simple and fast algorithm for kmedoids clustering", "author": ["H.-S. Park", "C.-H. Jun"], "venue": "Expert Syst. with Applicat., vol. 36, no. 2, pp. 3336\u20133341, 2009.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2009}, {"title": "The expectation-maximization algorithm", "author": ["T.K. Moon"], "venue": "IEEE Signal Process. Mag., vol. 13, no. 6, pp. 47\u201360, 1996.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1996}, {"title": "Feature selection based on sensitivity analysis of fuzzy isodata", "author": ["Q. Liu", "Z. Zhao", "Y.-X. Li", "Y. Li"], "venue": "Neurocomput., vol. 85, pp. 29\u201337, 2012.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2012}, {"title": "Data clustering: a review", "author": ["A.K. Jain", "M.N. Murty", "P.J. Flynn"], "venue": "ACM comput. surveys, vol. 31, no. 3, pp. 264\u2013323, 1999.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1999}, {"title": "Survey of clustering algorithms", "author": ["R. Xu", "D. Wunsch"], "venue": "IEEE Trans. Neural Netw., vol. 16, no. 3, pp. 645\u2013678, 2005.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2005}, {"title": "Top 10 algorithms in data mining", "author": ["X. Wu", "V. Kumar", "J.R. Quinlan"], "venue": "Knowl. and Inf. Syst., vol. 14, no. 1, pp. 1\u201337, 2008.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2008}, {"title": "K-means-type algorithms: a generalized convergence theorem and characterization of local optimality", "author": ["S.Z. Selim", "M.A. Ismail"], "venue": "IEEE Trans. Pattern Anal. Math. Intell., no. 1, pp. 81\u201387, 1984.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1984}, {"title": "Convergence properties of the k-means algorithms", "author": ["L. Bottou", "Y. Bengio"], "venue": "Proc. NIPS, pp. 585\u2013592, 1995.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1995}, {"title": "A comparative study of efficient initialization methods for the k-means clustering algorithm", "author": ["M.E. Celebi", "H.A. Kingravi", "P.A. Vela"], "venue": "Expert Systems with Applications, vol. 40, no. 1, pp. 200\u2013 210, 2013.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2013}, {"title": "Linear, deterministic, and order-invariant initialization methods for the k-means clustering algorithm", "author": ["M.E. Celebi", "H.A. Kingravi"], "venue": "Partitional Clustering Algorithms. Springer, 2015, pp. 79\u201398.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2015}, {"title": "X-means: Extending k-means with efficient estimation of the number of clusters.", "author": ["D. Pelleg", "A.W. Moore"], "venue": "in Proc. ICML,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2000}, {"title": "Learning the k in k-means", "author": ["G. Hamerly", "C. Elkan"], "venue": "Proc. NIPS, 2004, pp. 281\u2013288.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2004}, {"title": "Dip-means: an incremental clustering method for estimating the number of clusters", "author": ["A. Kalogeratos", "A. Likas"], "venue": "Proc. NIPS, 2012, pp. 2393\u20132401.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2012}, {"title": "The dip test of unimodality", "author": ["J.A. Hartigan", "P. Hartigan"], "venue": "The Ann. of Stat., pp. 70\u201384, 1985.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 1985}, {"title": "k-means++: The advantages of careful seeding", "author": ["D. Arthur", "S. Vassilvitskii"], "venue": "Proc. 8th ann. ACM-SIAM symposium on Discrete alg. Society for Industrial and Applied Mathematics, 2007, pp. 1027\u20131035.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2007}, {"title": "The minmax k-means clustering algorithm", "author": ["G. Tzortzis", "A. Likas"], "venue": "Pattern Recogn., vol. 47, no. 7, pp. 2505\u20132516, 2014.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2014}, {"title": "In search of deterministic methods for initializing k-means and gaussian mixture clustering", "author": ["T. Su", "J.G. Dy"], "venue": "Intelligent Data Analysis, vol. 11, no. 4, pp. 319\u2013338, 2007.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2007}, {"title": "An initialization method for the k-means algorithm using neighborhood model", "author": ["F. Cao", "J. Liang", "G. Jiang"], "venue": "Computers & Mathematics with Applications, vol. 58, no. 3, pp. 474\u2013483, 2009.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2009}, {"title": "A survey of outlier detection methodologies", "author": ["V.J. Hodge", "J. Austin"], "venue": "Art. Intell. Review, vol. 22, no. 2, pp. 85\u2013126, 2004.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2004}, {"title": "Improving k-means by outlier removal", "author": ["V. Hautam\u00e4ki", "S. Cherednichenko", "I. K\u00e4rkk\u00e4inen", "T. Kinnunen", "P. Fr\u00e4nti"], "venue": "Image Anal. Springer, 2005, pp. 978\u2013987.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2005}, {"title": "Robust clustering by pruning outliers", "author": ["J.-S. Zhang", "Y.-W. Leung"], "venue": "IEEE Trans. Syst., Man, Cybern. B, vol. 33, no. 6, pp. 983\u2013998, 2003.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2003}, {"title": "Finding groups in data: an introduction to cluster analysis", "author": ["L. Kaufman", "P.J. Rousseeuw"], "venue": null, "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2009}, {"title": "A reference bayesian test for nested hypotheses and its relationship to the schwarz criterion", "author": ["R.E. Kass", "L. Wasserman"], "venue": "J. of the Amer. Stat. Associat., vol. 90, no. 431, pp. 928\u2013934, 1995.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 1995}, {"title": "Density estimation", "author": ["S.J. Sheather"], "venue": "Stat. Sci., vol. 19, no. 4, pp. 588\u2013597, 2004.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2004}, {"title": "A practical guide to support vector classification", "author": ["C.-W. Hsu", "C.-C. Chang", "C.-J. Lin"], "venue": "2003.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2003}, {"title": "Similarity measures", "author": ["S. Santini", "R. Jain"], "venue": "IEEE Trans. Pattern Anal. Math. Intell., vol. 21, no. 9, pp. 871\u2013883, 1999.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 1999}, {"title": "Local search heuristics for k-median and facility location problems", "author": ["V. Arya", "N. Garg", "R. Khandekar", "A. Meyerson", "K. Munagala", "V. Pandit"], "venue": "SIAM J. on Comput., vol. 33, no. 3, pp. 544\u2013562, 2004.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2004}, {"title": "The manifold ways of perception", "author": ["H.S. Seung", "D.D. Lee"], "venue": "Sci., vol. 290, no. 5500, pp. 2268\u20132269, 2000.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2000}, {"title": "A novel graph-based k-means for nonlinear manifold clustering and representative selection", "author": ["E. Tu", "L. Cao", "J. Yang", "N. Kasabov"], "venue": "Neurocomput., vol. 143, pp. 109\u2013122, 2014.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2014}, {"title": "Complex wavelet structural similarity: A new image similarity  16 index", "author": ["M.P. Sampat", "Z. Wang", "S. Gupta", "A.C. Bovik", "M.K. Markey"], "venue": "IEEE Trans. Image Process., vol. 18, no. 11, pp. 2385\u20132401, 2009.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2009}, {"title": "A new manifold distance for visual object categorization", "author": ["F. Li", "X. Huang", "H. Qiao", "B. Zhang"], "venue": "Proc. 12th World Congress on Intelligent Control and Automation, 2016, pp. 2232\u20132236.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2016}, {"title": "Dynamic local search algorithm for the clustering problem", "author": ["I. K\u00e4rkk\u00e4inen", "P. Fr\u00e4nti"], "venue": "Research Report A. University of Joensuu, 2002.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2002}, {"title": "Iterative shrinking method for clustering problems", "author": ["P. Fr\u00e4nti", "O. Virmajoki"], "venue": "Pattern Recogn., vol. 39, no. 5, pp. 761\u2013775, 2006.", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2006}, {"title": "Methods of combining multiple classifiers based on different representations for pen-based handwritten digit recognition", "author": ["F. Alimoglu", "E. Alpaydin"], "venue": "Proc. of the 5th TAINN. Citeseer, 1996.", "citeRegEx": "43", "shortCiteRegEx": null, "year": 1996}, {"title": "Columbia object image library (coil-20)", "author": ["S.A. Nene", "S.K. Nayar", "H. Murase"], "venue": "CUCS-005-96, Tech. Rep., Feb. 1996.", "citeRegEx": "44", "shortCiteRegEx": null, "year": 1996}, {"title": "Columbia object image library (coil-100)", "author": ["S. Nayar", "S.A. Nene", "H. Murase"], "venue": "Department of Comp. Science, Columbia University, Tech. Rep. CUCS-006-96, 1996.", "citeRegEx": "45", "shortCiteRegEx": null, "year": 1996}, {"title": "Parameterisation of a stochastic model for human face identification", "author": ["F. Samaria", "A. Harter"], "venue": "Proc. 2rd WACV, 1994, pp. 138\u2013 142.", "citeRegEx": "46", "shortCiteRegEx": null, "year": 1994}, {"title": "Non-metric affinity propagation for unsupervised image categorization", "author": ["D. Dueck", "B.J. Frey"], "venue": "Proc. ICCV, 2007, pp. 1\u2013 8.", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2007}, {"title": "Principles of mathematical analysis", "author": ["W. Rudin"], "venue": "New York: McGraw- Hill Science,", "citeRegEx": "48", "shortCiteRegEx": "48", "year": 1964}, {"title": "Probability, random variables, and stochastic processes", "author": ["A. Papoulis", "S.U. Pillai"], "venue": "Tata McGraw-Hill Education,", "citeRegEx": "49", "shortCiteRegEx": "49", "year": 2002}], "referenceMentions": [{"referenceID": 0, "context": "C LUSTERING methods are important techniques for exploratory data analysis with wide applications ranging from data mining [1], vector quantization [2], dimension reduction [3], to manifold learning [4].", "startOffset": 123, "endOffset": 126}, {"referenceID": 1, "context": "C LUSTERING methods are important techniques for exploratory data analysis with wide applications ranging from data mining [1], vector quantization [2], dimension reduction [3], to manifold learning [4].", "startOffset": 148, "endOffset": 151}, {"referenceID": 2, "context": "C LUSTERING methods are important techniques for exploratory data analysis with wide applications ranging from data mining [1], vector quantization [2], dimension reduction [3], to manifold learning [4].", "startOffset": 173, "endOffset": 176}, {"referenceID": 3, "context": "C LUSTERING methods are important techniques for exploratory data analysis with wide applications ranging from data mining [1], vector quantization [2], dimension reduction [3], to manifold learning [4].", "startOffset": 199, "endOffset": 202}, {"referenceID": 4, "context": "The approaches to achieve this aim include methods based on density estimation such as DBSCAN [5], mean-shift clustering [6] and clustering by fast search and find of density peaks [7], methods that recursively find nested clusters [8], and partitional methods based on minimizing objective functions such as k-means [9], k-medoids [10], the EM algorithm [11] and ISODATA [12].", "startOffset": 94, "endOffset": 97}, {"referenceID": 5, "context": "The approaches to achieve this aim include methods based on density estimation such as DBSCAN [5], mean-shift clustering [6] and clustering by fast search and find of density peaks [7], methods that recursively find nested clusters [8], and partitional methods based on minimizing objective functions such as k-means [9], k-medoids [10], the EM algorithm [11] and ISODATA [12].", "startOffset": 121, "endOffset": 124}, {"referenceID": 6, "context": "The approaches to achieve this aim include methods based on density estimation such as DBSCAN [5], mean-shift clustering [6] and clustering by fast search and find of density peaks [7], methods that recursively find nested clusters [8], and partitional methods based on minimizing objective functions such as k-means [9], k-medoids [10], the EM algorithm [11] and ISODATA [12].", "startOffset": 181, "endOffset": 184}, {"referenceID": 7, "context": "The approaches to achieve this aim include methods based on density estimation such as DBSCAN [5], mean-shift clustering [6] and clustering by fast search and find of density peaks [7], methods that recursively find nested clusters [8], and partitional methods based on minimizing objective functions such as k-means [9], k-medoids [10], the EM algorithm [11] and ISODATA [12].", "startOffset": 232, "endOffset": 235}, {"referenceID": 8, "context": "The approaches to achieve this aim include methods based on density estimation such as DBSCAN [5], mean-shift clustering [6] and clustering by fast search and find of density peaks [7], methods that recursively find nested clusters [8], and partitional methods based on minimizing objective functions such as k-means [9], k-medoids [10], the EM algorithm [11] and ISODATA [12].", "startOffset": 317, "endOffset": 320}, {"referenceID": 9, "context": "The approaches to achieve this aim include methods based on density estimation such as DBSCAN [5], mean-shift clustering [6] and clustering by fast search and find of density peaks [7], methods that recursively find nested clusters [8], and partitional methods based on minimizing objective functions such as k-means [9], k-medoids [10], the EM algorithm [11] and ISODATA [12].", "startOffset": 332, "endOffset": 336}, {"referenceID": 10, "context": "The approaches to achieve this aim include methods based on density estimation such as DBSCAN [5], mean-shift clustering [6] and clustering by fast search and find of density peaks [7], methods that recursively find nested clusters [8], and partitional methods based on minimizing objective functions such as k-means [9], k-medoids [10], the EM algorithm [11] and ISODATA [12].", "startOffset": 355, "endOffset": 359}, {"referenceID": 11, "context": "The approaches to achieve this aim include methods based on density estimation such as DBSCAN [5], mean-shift clustering [6] and clustering by fast search and find of density peaks [7], methods that recursively find nested clusters [8], and partitional methods based on minimizing objective functions such as k-means [9], k-medoids [10], the EM algorithm [11] and ISODATA [12].", "startOffset": 372, "endOffset": 376}, {"referenceID": 0, "context": "For more information about clustering methods, see [1], [9], [13], [14].", "startOffset": 51, "endOffset": 54}, {"referenceID": 8, "context": "For more information about clustering methods, see [1], [9], [13], [14].", "startOffset": 56, "endOffset": 59}, {"referenceID": 12, "context": "For more information about clustering methods, see [1], [9], [13], [14].", "startOffset": 61, "endOffset": 65}, {"referenceID": 13, "context": "For more information about clustering methods, see [1], [9], [13], [14].", "startOffset": 67, "endOffset": 71}, {"referenceID": 14, "context": "studied one [15].", "startOffset": 12, "endOffset": 16}, {"referenceID": 15, "context": "[16], [17], [18], [19]).", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "[16], [17], [18], [19]).", "startOffset": 6, "endOffset": 10}, {"referenceID": 17, "context": "[16], [17], [18], [19]).", "startOffset": 12, "endOffset": 16}, {"referenceID": 18, "context": "[16], [17], [18], [19]).", "startOffset": 18, "endOffset": 22}, {"referenceID": 19, "context": "x-means [20] is one of the first such attempts that use the splitting and merging rules for the number of centers to increase and decrease as the algorithm ar X iv :1 61 1.", "startOffset": 8, "endOffset": 12}, {"referenceID": 20, "context": "g-means [21] works similarly as x-means except it assumes that the clusters are generated from the Gaussian distributions.", "startOffset": 8, "endOffset": 12}, {"referenceID": 21, "context": "dip-means [22], however, only assumes each cluster to admit a unimodal distribution and verifies this by Hartigans\u2019 dip test [23].", "startOffset": 10, "endOffset": 14}, {"referenceID": 22, "context": "dip-means [22], however, only assumes each cluster to admit a unimodal distribution and verifies this by Hartigans\u2019 dip test [23].", "startOffset": 125, "endOffset": 129}, {"referenceID": 23, "context": "The k-means++ algorithm [24] aims to avoid poor quality data partitioning during the restarts and achieves O(log k)-competitive results with the optimal clustering.", "startOffset": 24, "endOffset": 28}, {"referenceID": 24, "context": "The Min-Max k-means algorithm [25] deals with the initialization problem of k-means by alternating the objective function to be weighted by the variance of each cluster.", "startOffset": 30, "endOffset": 34}, {"referenceID": 25, "context": "Methods such as PCA-Part and Var-Part [26] use a deterministic approach based on PCA and a variance of data to hierarchically split the data set into k parts where initial seeds are selected.", "startOffset": 38, "endOffset": 42}, {"referenceID": 17, "context": "For many other deterministic initialization methods, see [18], [19], [27] and the references quoted there.", "startOffset": 57, "endOffset": 61}, {"referenceID": 18, "context": "For many other deterministic initialization methods, see [18], [19], [27] and the references quoted there.", "startOffset": 63, "endOffset": 67}, {"referenceID": 26, "context": "For many other deterministic initialization methods, see [18], [19], [27] and the references quoted there.", "startOffset": 69, "endOffset": 73}, {"referenceID": 27, "context": "The third drawback of k-means, that is, its sensitivity to outliers, can be addressed by using more robust proximity measure [28], such as the Mahalanobis distance and the L1 distance rather than the Euclidean distance.", "startOffset": 125, "endOffset": 129}, {"referenceID": 28, "context": "The outlier removal clustering algorithm [29] uses this idea and achieves a better performance than the original k-means method when dealing with overlapping clusters.", "startOffset": 41, "endOffset": 45}, {"referenceID": 29, "context": "Other outliers-removing cluster algorithms can be found in [30]and the references quoted there.", "startOffset": 59, "endOffset": 63}, {"referenceID": 9, "context": "k-medoids [10], as a variant of kmeans, overcomes this difficulty by restricting the centers to be the data samples themselves.", "startOffset": 10, "endOffset": 14}, {"referenceID": 30, "context": "It can be solved effectively (but slowly) by data partitioning around medoids (PAM) [31], or efficiently (but approximately optimally) by CLARA [31].", "startOffset": 84, "endOffset": 88}, {"referenceID": 30, "context": "It can be solved effectively (but slowly) by data partitioning around medoids (PAM) [31], or efficiently (but approximately optimally) by CLARA [31].", "startOffset": 144, "endOffset": 148}, {"referenceID": 15, "context": "Finally, the algorithm is guaranteed to converge [16] at a quadratic rate [17] to a local minima of the SSE, denoted as SSE\u2217.", "startOffset": 49, "endOffset": 53}, {"referenceID": 16, "context": "Finally, the algorithm is guaranteed to converge [16] at a quadratic rate [17] to a local minima of the SSE, denoted as SSE\u2217.", "startOffset": 74, "endOffset": 78}, {"referenceID": 9, "context": "k-medoids [10] has the same objective function (1) as kmeans.", "startOffset": 10, "endOffset": 14}, {"referenceID": 19, "context": "x-means [20] is an extension of k-means with the estimation of the number k of clusters.", "startOffset": 8, "endOffset": 12}, {"referenceID": 31, "context": "During the process, the Bayesian Information Criterion (BIC) [32] is applied to score the modal.", "startOffset": 61, "endOffset": 65}, {"referenceID": 21, "context": "dip-means [22] is another extension of k-means with the estimation of the number of clusters.", "startOffset": 10, "endOffset": 14}, {"referenceID": 6, "context": "Clustering by fast search and find of density peaks (CFSFDP) [7] is a novel clustering method.", "startOffset": 61, "endOffset": 64}, {"referenceID": 32, "context": "We use the kernel density estimation (KDE) [33] to compute the local densities.", "startOffset": 43, "endOffset": 47}, {"referenceID": 6, "context": "In addition, compared with the uniform kernel K(z) = [1/(b\u2212 a)]I{a\u2264z\u2264b} which is used in [7] (see (5)), the Gaussian kernel has a relatively higher value when |z| is small and thus keeps more local information near zero.", "startOffset": 89, "endOffset": 92}, {"referenceID": 0, "context": "By definition (10), \u03b3 lies in [0, 1] and is an increasing function of \u03c1 and \u03b4.", "startOffset": 30, "endOffset": 36}, {"referenceID": 19, "context": "However, it is not always easy to determine the best value of k [20].", "startOffset": 64, "endOffset": 68}, {"referenceID": 33, "context": "A practical way of solving this problem approximately will be the grid search method [34], in which various pairs of the (h, r) values are tried and the one that results in the maximum \u03c4\u2217 is picked.", "startOffset": 85, "endOffset": 89}, {"referenceID": 34, "context": "A dissimilarity measure is the inverse of a similarity measure [35], which is a real-word function that quantifies the similarity between two objects.", "startOffset": 63, "endOffset": 67}, {"referenceID": 35, "context": "If the dissimilarity measure is the L1 distance, k-medoids will get the same result as k-median [36].", "startOffset": 96, "endOffset": 100}, {"referenceID": 36, "context": "For manifold distributed data, the best choice for dissimilarity measures would be the manifold distance [37] which is usually approximated by the graph distance based on the neighborhood or the t-nearest-neighborhood (t-nn).", "startOffset": 105, "endOffset": 109}, {"referenceID": 37, "context": "Graphbased k-means [38] uses this measure.", "startOffset": 19, "endOffset": 23}, {"referenceID": 38, "context": "For images, one of the most effective similarity measures may be the complex wavelet structural similarity (CW-SSIM) index [39], which is robust to small rotations and translations of images.", "startOffset": 123, "endOffset": 127}, {"referenceID": 39, "context": "In [40], a combination of the manifold assumption and the CW-SSIM index is used for constructing a new manifold distance named geometric CW-SSIM distance, which shows a superior performance for visual object categorization tasks.", "startOffset": 3, "endOffset": 7}, {"referenceID": 21, "context": "For dip-means, we set the significance level \u03b1 = 0 for the dip test and the voting percentage vthd = 1% as in [22].", "startOffset": 110, "endOffset": 114}, {"referenceID": 6, "context": "For CFSFDP, we follow the suggestion in [7] to choose dc so that the average number of neighbors is around 1% to 2% of the total number of data points in the data set.", "startOffset": 40, "endOffset": 43}, {"referenceID": 40, "context": "Four different kinds of synthetic data sets are used: the A-sets [41] have different numbers of clusters, k, the S-sets [42] have different data distributions, the Dimsets vary with the dimensionality p and the Shape-sets [7] are of different shapes.", "startOffset": 65, "endOffset": 69}, {"referenceID": 41, "context": "Four different kinds of synthetic data sets are used: the A-sets [41] have different numbers of clusters, k, the S-sets [42] have different data distributions, the Dimsets vary with the dimensionality p and the Shape-sets [7] are of different shapes.", "startOffset": 120, "endOffset": 124}, {"referenceID": 6, "context": "Four different kinds of synthetic data sets are used: the A-sets [41] have different numbers of clusters, k, the S-sets [42] have different data distributions, the Dimsets vary with the dimensionality p and the Shape-sets [7] are of different shapes.", "startOffset": 222, "endOffset": 225}, {"referenceID": 42, "context": "The real world data sets include Handwritten Pendigits [43], Coil-20 [44], Coil-100 [45] and Olivetti Face Database [46].", "startOffset": 55, "endOffset": 59}, {"referenceID": 43, "context": "The real world data sets include Handwritten Pendigits [43], Coil-20 [44], Coil-100 [45] and Olivetti Face Database [46].", "startOffset": 69, "endOffset": 73}, {"referenceID": 44, "context": "The real world data sets include Handwritten Pendigits [43], Coil-20 [44], Coil-100 [45] and Olivetti Face Database [46].", "startOffset": 84, "endOffset": 88}, {"referenceID": 45, "context": "The real world data sets include Handwritten Pendigits [43], Coil-20 [44], Coil-100 [45] and Olivetti Face Database [46].", "startOffset": 116, "endOffset": 120}, {"referenceID": 0, "context": "In our experiments, the attributes are generally normalized into the interval [0, 1] using the min-max normalization.", "startOffset": 78, "endOffset": 84}, {"referenceID": 46, "context": "On the real world data sets, we consider the unsupervised classification task [47].", "startOffset": 78, "endOffset": 82}, {"referenceID": 6, "context": "This is slightly different from the results reported in [7].", "startOffset": 56, "endOffset": 59}, {"referenceID": 6, "context": "It should be pointed out that the results reported in [7] can be achieved with a very careful selection of parameters and with a prior knowledge on the data distribution, which we did not consider in this paper.", "startOffset": 54, "endOffset": 57}, {"referenceID": 43, "context": "We now consider the real world data set, Coil-20 [44], which is used for the task of unsupervised object clustering.", "startOffset": 49, "endOffset": 53}, {"referenceID": 6, "context": "-40 by CFSFDP with the CWSSIM index is around 30 [7], much close to k\u2217 compared with the previous methods.", "startOffset": 49, "endOffset": 52}, {"referenceID": 6, "context": "8% increasing and rf by 25% decreasing over the results reported in [7], where re = 22.", "startOffset": 68, "endOffset": 71}], "year": 2016, "abstractText": "The k-means algorithm is a widely used clustering method in pattern recognition and machine learning due to its simplicity to implement and low time complexity. However, it has the following main drawbacks: 1) the number of clusters, k, needs to be provided by the user in advance, 2) it can easily reach local minima with randomly selected initial centers, 3) it is sensitive to outliers, and 4) it can only deal with well separated hyperspherical clusters. In this paper, we propose a Local Density Peaks Searching (LDPS) initialization framework to address these issues. The LDPS framework includes two basic components: one of them is the local density that characterizes the density distribution of a data set, and the other is the local distinctiveness index (LDI) which we introduce to characterize how distinctive a data point is compared with its neighbors. Based on these two components, we search for the local density peaks which are characterized with high local densities and high LDIs to deal with the first two drawbacks of k-means. Moreover, we detect outliers characterized with low local densities but high LDIs, and exclude them out before clustering begins. Finally, we apply the LDPS initialization framework to k-medoids, which is a variant of k-means and chooses data samples as centers, with diverse similarity measures other than the Euclidean distance to fix the last drawback of k-means. Combining the LDPS initialization framework with k-means and k-medoids, we obtain two novel clustering methods called LDPS-means and LDPS-medoids, respectively. Experiments on synthetic data sets verify the effectiveness of the proposed methods, especially when the ground truth of the cluster number k is large. Further, experiments on several real world data sets, Handwritten Pendigits, Coil-20, Coil-100 and Olivetti Face Database, illustrate that our methods give a superior performance than the analogous approaches on both estimating k and unsupervised object categorization.", "creator": "LaTeX with hyperref package"}}}