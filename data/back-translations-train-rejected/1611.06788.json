{"id": "1611.06788", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Nov-2016", "title": "Bidirectional Tree-Structured LSTM with Head Lexicalization", "abstract": "Sequential LSTM has been extended to model tree structures, giving competitive results for a number of tasks. Existing methods model constituent trees by bottom-up combinations of constituent nodes, making direct use of input word information only for leaf nodes. This is different from sequential LSTMs, which contain reference to input words for each node. In this paper, we propose a method for automatic head-lexicalization for tree-structure LSTMs, propagating head words from leaf nodes to every constituent node. In addition, enabled by head lexicalization, we build a tree LSTM in the top-down direction, which corresponds to bidirectional sequential LSTM structurally. Experiments show that both extensions give better representations of tree structures. Our final model gives the best results on the Standford Sentiment Treebank and highly competitive results on the TREC question type classification task.", "histories": [["v1", "Mon, 21 Nov 2016 14:01:53 GMT  (541kb,D)", "http://arxiv.org/abs/1611.06788v1", "12 pages, 6 figures"]], "COMMENTS": "12 pages, 6 figures", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["zhiyang teng", "yue zhang"], "accepted": false, "id": "1611.06788"}, "pdf": {"name": "1611.06788.pdf", "metadata": {"source": "CRF", "title": "Bidirectional Tree-Structured LSTM with Head Lexicalization", "authors": ["Zhiyang Teng", "Yue Zhang"], "emails": ["teng@mymail.sutd.edu.sg", "zhang@sutd.edu.sg"], "sections": [{"heading": "1 Introduction", "text": "In fact, most people who fight for the rights of women and men are fighting for the rights of men and women, not for the rights of women and men, but for the rights of women and men who fight for the rights of men and women."}, {"heading": "2 Related Work", "text": "LSTM (Hochreiter and Schmidhuber, 1997) is a variant of RNN (Elman, 1990) to solve disappearing slopes in education. It has been widely adopted for NLP tasks such as parsing (Dyer et al., 2015) and machine translation (Bahdanau et al., 2014). We take the standard LSTM with peephole connections (Gers and Schmidhuber, 2000) as our baseline for sequencing models.There have been extensions of sequence-structured LSTMs for tree modeling (Tai et al., 2015; Le and Zuidema, 2015; Zhu et al, 2015b). The idea is similar to the expansion of recursive neural networks into recursive neural networks, where gates are used in recursive structures."}, {"heading": "3 Baselines", "text": "A sequence structure LSTM estimates a sequence of hidden state vectors specifying a sequence of vectors vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector"}, {"heading": "4 Our Model", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Head Lexicalization", "text": "We introduce a lexical input vector xt to calculate each cell vector. \u2212 As shown in Figure 3 (b) at the shaded nodes, the propagation mechanism xt is relatively parallel to the cell propagation mechanism. \u2212 In contrast, in the method of Zhu et al. (2015b) in Figure 3 (a), there is no input vector xt for non-leaf constituents. \u2212 There are several ways to choose a head flexicon for a given binary branched constituent. A simple baseline consists in selecting the head flexicon of the left child as the head (left-handedness). \u2212 Accordingly, an alternative is to use the right head flexicon for the head flexicon. However, these simple baseline lines may bring less advantages than the linguistically motivated head finding, since the head flexicon xxt is not as consistent as the xxxxx model of the ruling head. \u2212 These simple baseline lines, however, can easily bring the same advantages as the child flexlexx, the child flexlexx and the head lexx model, the same as the child flexx."}, {"heading": "4.2 Lexicalized Tree LSTM", "text": "Considering the head flexicon vectors for nodes, the tree LSTM of Zhu et al. (2015b) can be extended by using xt in the calculation of the corresponding ct. In particular, xt is used to estimate the input (it), output (ot) and forgetfulness (fRt and f L t) of gates: it = \u03c3 (WNhflh N t \u2212 1 + W N \u00b2 (WNhih N t \u2212 1 + W N \u2212 1) + bi) fLt = \u03c3 (WNhfrh N \u2212 1) {L, R} (WNhflh N t \u2212 1 + W N cfl cNt \u2212 1) + bfl) + fRt = \u03c3 (Wxfxt + 0) {L, R} (WNhfrh N \u2212 1) and W N (cfrc N \u2212 1) binded (Wxoxt) ot = xi (WNhoh N \u2212 1 + Wt \u2212 Wcogh) and WH \u2212 head only (WH \u2212 11)."}, {"heading": "4.3 Bidirectional Extensions", "text": "Given a sequence of input vectors [x1, x2,. ht, xn], a bidirectional sequential LSTM (Graves et al., 2013) calculates a bi-directional sequential LSTM (Graves et al.,) each containing two sets of hidden state vectors, [h-1, h-2,.., h-ho-n] and [h-h-n, h-h-n-n, h-n-h-h-h-h-h-h-h-h-h-h-h-h-h-h-h-h-h-h-h-h-h-h-h-h-h-h-h-h-h-h-h-h-h-h-h-h-h-h-h-h-h-h-h-h-h-h-h-h-h-h-h-h-h-h-h-h-h-h-h-h-h-h-h-h-h-h-h-h-h-h-h-h-h-h-h-h-h-h-h-h-h-h-h-h-h-h-h-h-h-h-h-h-h-h-h-h-h-h-h-h-h-h-h-h-h-h-h-h-h-h-h-h-h-h-h-h-h-h-h-h-h-h-h-h-h-h-h-h-h-h-h-h-h h-h-h-h-h-h-h-h-h-h-h-h-h-h-h-h-h-h-h h-h-h-h-h-h-h h-h-h h-h-h-h-h-h h h-h h-h h h-h-h h h-h h-h h h h h-h-h h h h-h-h-h h h-h-h h h h h-h h h-h h h h h h h-h h-h h h h h h"}, {"heading": "5 Classification", "text": "We apply the bidirectional tree LSTM to classification tasks where the input is a sentence with its binarized constituent tree and the output represents a discrete designation: the designation of the bottom-to-top hidden state vector of the root as h-ROOT vector, the top-to-bottom hidden state vector of the input words x1, x2,..., h-n as h-1, h-2,..., h-n we take the concatenation of h-ROOT, h-ROOT, h-ROOT and the average of h-1, h-2,..., h-n as the final representation h of the sentence: h = h-ROOT, h-ROOT, h-n-ROOT, h-n-i = 1 h-ROOT and the average of h-2,..."}, {"heading": "6 Training", "text": "We train our classifier to maximize the conditional log probability of gold labels on training samples. Formally, the training target is defined by L (\u03b8) = \u2212 | D | \u2211 i = 1 log pyi + \u03bb 2 | | \u03b8 | | 2, (18) where \u03b8 is the set of model parameters, \u03bb is a regularization parameter, yi is the gold label of the ith training sample and pyi is obtained according to Equation 17. For sequential LSTM models, we collect errors over each sequence. For tree LSTMs, we sum up errors on each node.The model parameters are optimized with ADAM (Kingma and Ba, 2014) without gradient clipping, with the standard hyper parameters of the AdamTrainer in the Dynet toolkites.2 We also use exposure parameters in each node.The model parameters are optimized with ADAM (Kingma and Ba, 2014) without the tool hyper-clipping parameters of the training AM."}, {"heading": "7 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "7.1 Data", "text": "The effectiveness of our model is tested on the basis of a mood classification task and a question-type classification."}, {"heading": "7.1.1 Sentiment Classification", "text": "For the classification of feelings, we use the same data settings as Zhu et al. (2015b). Specifically, the Stanford Sentiment Treebank (Socher et al., 2013b) is used to train our classification model, which contains sensations for film reviews. Each sentence is commented with a constituent tree. Each internal node corresponds to a phrase. Each node is manually assigned a holistic sentiment label from 0 to 4, each corresponding to five sensation classes: very negative, negative, neutral, positive and very positive.The root label represents the sentiment label of the entire sentence. We perform both a binary classification and a fine-grained classification. After previous work, we use labels of all phrases for training and testing. Gold standard tree structures are used for training and testing (Le and Zuidema, 2015; Li et al., 2015; Zhu et al., 2015b; Tai, 2015)."}, {"heading": "7.1.2 Question Type Classification", "text": "For the classification of the question types we use the TREC data (Li and Roth, 2002). Each training project in this data set contains a question set and the corresponding question type. We are working on the six-fold rough classification task, where the six question types are ENTY, HUM, LOC, DESC, NUMand ABBR, corresponding to ENTITY, HUMAN, LOCATION, DESCRIPTION, NUMERIC VALUE or ABBREVIATION. For example, the correct type for the phrase \"In which year did the Titanic sink?\" is of type NUM. The training set consists of 5,452 examples and the test set contains 500 examples. Since there is no development set, we follow Zhou et al. (2015) by randomly extracting 500 examples from the training set as a development set. Unlike the sentiment treebank, there is no commented tree for each set. Instead, we get an automatically parsed tree for each set, where the difference between one REC and one set is only an expression."}, {"heading": "7.2 Baselines", "text": "We look at two models for our baselines: the first is a bidirectional LSTM (BiLSTM) (Hochreiter and Schmidhuber, 1997; Graves et al., 2013); our bidirectional constituency Tree LSTM (BiConTree) is compared with the bidirectional sequential LSTM to investigate the effectiveness of the tree structure; for the mood task according to Tai et al. (2015) and Li et al. (2015) we convert the tree bank in sequences so that the bidirectional LSTM model can use any phrase span as a training example; the second base model is the bottom-up tree LSTM model by Zhu et al. (2015b); we create a contrast between this model and our lexicalized bidirectional models to show the effects of lexicalization of the head and information flow from top to bottom."}, {"heading": "7.3 Main Results", "text": "Table 1 shows the main results for the sentiment classification task, where RNTN represents the recursive neural tensor model of Socher et al. (2013b), ConTree and DepTree represent the constituency tree LSTMs and the dependence tree LSTMs. Our reimplementation of sequential bidirectional LSTM and constituent tree LSTM (Zhu et al., 2015b) yields results comparable to Zhu et al. (2015b)'s original implementation. After incorporating head lexicalization into our constituent tree LSTM, the fine-grained sentiment4https: / / github.com / SUTDNLP / ZPar, version 7.5 classification accuracy increases from 51.2 to 52.8, and binary sentification accuracy increases from 88.5 to 89.2, demonstrating the effectiveness of the head lexicalization mechanisms."}, {"heading": "7.4 Training Time and Model Size", "text": "The introduction of head lexicography and bidirectional extension to the model increases the complexity of the model. In this section we analyze our model in terms of training time and model size on the fine-grained mood classification task. We run all models with an i7-4790 3.60GHz CPU with a single thread. Table 4 shows the average runtime for different models over 30 iterations. The baseline ConTree model takes about 1.3 hours to finish the training process. ConTree + Lex takes about 1.5 times longer than ConTree. BiConTree takes about 3.2 hours, which is about 2.5 times longer than that of ConTree.Table 5 compares the model with the sizes. We did not count the number of parameters in the lookup table, as these parameters are the same for all models. Since the size of the LSTM models mainly depends on the dimensionality of the state vector h, we change the size of the model h for comparing the model size."}, {"heading": "7.5 Head Lexicalization Methods", "text": "In this experiment, we examine the effect of our head flexicalization method against heuristic ground rules. We consider three basic methods, namely L, R, and A. For L, a parent node accepts lexical information from its left child while ignoring the right child. Accordingly, a parent node accepts lexical information from its right child while ignoring the left child. For A, a parent node on average delivers the lexical vectors from the left and right child. Table 3 shows the accuracy on the test set, where G denotes our method described in Section 4.1. R delivers better results than L due to relatively stronger right-branched structures in this nodule. A simple average delivers similar results compared to the right branch. In contrast, G outperforms a method by looking at the relative weights of each branch according to the contexts at tree level."}, {"heading": "7.6 Error Analysis", "text": "Table 6 shows some sample sentences that were incorrectly predicted by the bottom-up tree model but were correctly labeled by our final model. Sentence 2 and Sentence 3 show the usefulness of top-down information for complex semantic structures where compositivity has subtle effects. Top-down LSTM enables models to correctly predict sentiment labels. Our final model improves the results for the \"very negative\" and \"very positive\" classes by 10% and 11%, respectively. It also increases the accuracy for sentences with negation (e.g. \"not,\" \"not\" and \"no\") by 4.4%. Figure 6 shows the accuracy distribution according to sentence length. We find that our model can improve the classification accuracy for longer sentences (> 30 words) by 3.5 points compared to the base model."}, {"heading": "7.7 Parser Reranking", "text": "In order to further investigate the effectiveness of automatically learned head information on a purely syntactical task, we also perform a simple parser re-ranking experiment using the standard PTB (Marcus et al., 1993) split (Collins, 2003), and the same off-the-shelf ZPar model is used for our baseline. A standard interpolated re-anchor (Zhu et al., 2015a; Zhou et al., 2016) is used for the evaluation of 8-best trees."}, {"heading": "8 Conclusion", "text": "We have shown that existing LSTM models of constituent tree structures are limited by the failure to take direct lexical input into account when calculating cell values for non-leaf constituents, and a headline lexicalization method is proposed to solve this problem. If we learn the heads of constituent components automatically using a neural model, our lexicalized tree LSTM is applicable to any binary branch tree in the form of CFG and is formalism-independent. In addition, lexical information about the root enables top-down extension of the model, resulting in a bi-directional constituent tree LSTM. Experiments on two well-known datasets show that lexicalization of the head improves the unidirectional tree LSTM model, and bidirectional tree LSTM delivers superior labeling results compared to both unidirectional tree LSTMs 2016 and bidirectional LSTM number 5."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "arXiv preprint arXiv:1409.0473.", "citeRegEx": "Bahdanau et al\\.,? 2014", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Parsing as language modeling", "author": ["Eugene Charniak"], "venue": null, "citeRegEx": "Charniak.,? \\Q2016\\E", "shortCiteRegEx": "Charniak.", "year": 2016}, {"title": "Parsing the wsj using ccg and log-linear models", "author": ["Stephen Clark", "James R. Curran."], "venue": "Proceedings of the 42nd Meeting of the Association for Computational Linguistics (ACL\u201904), Main Volume, pages 103\u2013 110, Barcelona, Spain, July.", "citeRegEx": "Clark and Curran.,? 2004", "shortCiteRegEx": "Clark and Curran.", "year": 2004}, {"title": "Head-driven statistical models for natural language parsing", "author": ["Michael Collins."], "venue": "Computational linguistics, 29(4):589\u2013637.", "citeRegEx": "Collins.,? 2003", "shortCiteRegEx": "Collins.", "year": 2003}, {"title": "A unified architecture for natural language processing: Deep neural networks with multitask learning", "author": ["Ronan Collobert", "Jason Weston."], "venue": "Proceedings of the 25th international conference on Machine learning, pages 160\u2013167. ACM.", "citeRegEx": "Collobert and Weston.,? 2008", "shortCiteRegEx": "Collobert and Weston.", "year": 2008}, {"title": "Transitionbased dependency parsing with stack long short-term memory", "author": ["Chris Dyer", "Miguel Ballesteros", "Wang Ling", "Austin Matthews", "Noah A. Smith."], "venue": "ACL.", "citeRegEx": "Dyer et al\\.,? 2015", "shortCiteRegEx": "Dyer et al\\.", "year": 2015}, {"title": "Recurrent neural network grammars", "author": ["Chris Dyer", "Adhiguna Kuncoro", "Miguel Ballesteros", "Noah A Smith."], "venue": "arXiv preprint arXiv:1602.07776.", "citeRegEx": "Dyer et al\\.,? 2016", "shortCiteRegEx": "Dyer et al\\.", "year": 2016}, {"title": "Finding structure in time", "author": ["Jeffrey L Elman."], "venue": "Cognitive science, 14(2):179\u2013211.", "citeRegEx": "Elman.,? 1990", "shortCiteRegEx": "Elman.", "year": 1990}, {"title": "Recurrent nets that time and count", "author": ["Felix A Gers", "J\u00fcrgen Schmidhuber."], "venue": "Neural Networks, 2000. IJCNN 2000, Proceedings of the IEEE-INNSENNS International Joint Conference on, volume 3, pages 189\u2013194. IEEE.", "citeRegEx": "Gers and Schmidhuber.,? 2000", "shortCiteRegEx": "Gers and Schmidhuber.", "year": 2000}, {"title": "Hybrid speech recognition with deep bidirectional lstm", "author": ["Alan Graves", "Navdeep Jaitly", "Abdel-rahman Mohamed."], "venue": "Automatic Speech Recognition and Understanding (ASRU), 2013 IEEE Workshop on, pages 273\u2013278. IEEE.", "citeRegEx": "Graves et al\\.,? 2013", "shortCiteRegEx": "Graves et al\\.", "year": 2013}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "Neural computation, 9(8):1735\u2013 1780.", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik P. Kingma", "Jimmy Ba."], "venue": "CoRR, abs/1412.6980.", "citeRegEx": "Kingma and Ba.,? 2014", "shortCiteRegEx": "Kingma and Ba.", "year": 2014}, {"title": "Accurate unlexicalized parsing", "author": ["Dan Klein", "Christopher D Manning."], "venue": "Proceedings of the 41st Annual Meeting on Association for Computational Linguistics-Volume 1, pages 423\u2013430. Association for Computational Linguistics.", "citeRegEx": "Klein and Manning.,? 2003", "shortCiteRegEx": "Klein and Manning.", "year": 2003}, {"title": "Compositional distributional semantics with long short term memory", "author": ["Phong Le", "Willem Zuidema"], "venue": null, "citeRegEx": "Le and Zuidema.,? \\Q2015\\E", "shortCiteRegEx": "Le and Zuidema.", "year": 2015}, {"title": "Learning question classifiers", "author": ["Xin Li", "Dan Roth."], "venue": "Proceedings of the 19th international conference on Computational linguistics-Volume 1, pages 1\u20137. Association for Computational Linguistics.", "citeRegEx": "Li and Roth.,? 2002", "shortCiteRegEx": "Li and Roth.", "year": 2002}, {"title": "When are tree structures necessary for deep learning of representations? In Empirical Methods in Natural Language Processing (EMNLP)", "author": ["Jiwei Li", "Minh-Thang Luong", "Dan Jurafsky", "Eudard Hovy"], "venue": null, "citeRegEx": "Li et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Li et al\\.", "year": 2015}, {"title": "Building a large annotated corpus of english: The penn treebank", "author": ["Mitchell P Marcus", "Mary Ann Marcinkiewicz", "Beatrice Santorini."], "venue": "Computational linguistics, 19(2):313\u2013330.", "citeRegEx": "Marcus et al\\.,? 1993", "shortCiteRegEx": "Marcus et al\\.", "year": 1993}, {"title": "Recurrent neural network based language model", "author": ["Tomas Mikolov", "Martin Karafi\u00e1t", "Lukas Burget", "Jan Cernock\u1ef3", "Sanjeev Khudanpur."], "venue": "INTERSPEECH, 2:3.", "citeRegEx": "Mikolov et al\\.,? 2010", "shortCiteRegEx": "Mikolov et al\\.", "year": 2010}, {"title": "Efficient estimation of word representations in vector space", "author": ["Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean."], "venue": "arXiv preprint arXiv:1301.3781.", "citeRegEx": "Mikolov et al\\.,? 2013", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "End-to-end Relation Extraction using LSTMs on Sequences and Tree Structures", "author": ["Makoto Miwa", "Mohit Bansal."], "venue": "ArXiv e-prints, abs/1601.00770, January.", "citeRegEx": "Miwa and Bansal.,? 2016", "shortCiteRegEx": "Miwa and Bansal.", "year": 2016}, {"title": "Global belief recursive neural networks", "author": ["Romain Paulus", "Richard Socher", "Christopher D Manning."], "venue": "Advances in Neural Information Processing Systems, pages 2888\u20132896.", "citeRegEx": "Paulus et al\\.,? 2014", "shortCiteRegEx": "Paulus et al\\.", "year": 2014}, {"title": "From symbolic to sub-symbolic information in question classification", "author": ["Joao Silva", "Lu\u0131\u0301sa Coheur", "Ana Cristina Mendes", "Andreas Wichert"], "venue": "Artificial Intelligence Review,", "citeRegEx": "Silva et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Silva et al\\.", "year": 2011}, {"title": "Parsing Natural Scenes and Natural Language with Recursive Neural Networks", "author": ["Richard Socher", "Cliff C. Lin", "Andrew Y. Ng", "Christopher D. Manning."], "venue": "Proceedings of the 26th International Conference on Machine Learning (ICML).", "citeRegEx": "Socher et al\\.,? 2011", "shortCiteRegEx": "Socher et al\\.", "year": 2011}, {"title": "Parsing with compositional vector grammars", "author": ["Richard Socher", "John Bauer", "Christopher D. Manning", "Ng Andrew Y."], "venue": "Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 455\u2013465, Sofia,", "citeRegEx": "Socher et al\\.,? 2013a", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Recursive deep models for semantic compositionality over a sentiment treebank", "author": ["Richard Socher", "Alex Perelygin", "Jean Wu", "Jason Chuang", "Christopher Manning", "Andrew Ng", "Christopher Potts."], "venue": "EMNLP.", "citeRegEx": "Socher et al\\.,? 2013b", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V Le."], "venue": "Advances in neural information processing systems, pages 3104\u20133112.", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Improved semantic representations from tree-structured long short-term memory networks", "author": ["Kai Sheng Tai", "Richard Socher", "Christopher D. Manning."], "venue": "Association for Computational Linguistics (ACL).", "citeRegEx": "Tai et al\\.,? 2015", "shortCiteRegEx": "Tai et al\\.", "year": 2015}, {"title": "Grammar as a foreign language", "author": ["Oriol Vinyals", "\u0141ukasz Kaiser", "Terry Koo", "Slav Petrov", "Ilya Sutskever", "Geoffrey Hinton."], "venue": "Advances in Neural Information Processing Systems, pages 2755\u20132763.", "citeRegEx": "Vinyals et al\\.,? 2015", "shortCiteRegEx": "Vinyals et al\\.", "year": 2015}, {"title": "Tree recurrent neural networks with application to language modeling", "author": ["Xingxing Zhang", "Liang Lu", "Mirella Lapata."], "venue": "CoRR, abs/1511.00060.", "citeRegEx": "Zhang et al\\.,? 2015", "shortCiteRegEx": "Zhang et al\\.", "year": 2015}, {"title": "A C-LSTM neural network for text classification", "author": ["Chunting Zhou", "Chonglin Sun", "Zhiyuan Liu", "Francis C.M. Lau."], "venue": "CoRR, abs/1511.08630.", "citeRegEx": "Zhou et al\\.,? 2015", "shortCiteRegEx": "Zhou et al\\.", "year": 2015}, {"title": "A search-based dynamic reranking model for dependency parsing", "author": ["Hao Zhou", "Yue Zhang", "Shujian Huang", "Junsheng Zhou", "Xin-Yu Dai", "Jiajun Chen."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long", "citeRegEx": "Zhou et al\\.,? 2016", "shortCiteRegEx": "Zhou et al\\.", "year": 2016}, {"title": "Fast and accurate shift-reduce constituent parsing", "author": ["Muhua Zhu", "Yue Zhang", "Wenliang Chen", "Min Zhang", "Jingbo Zhu"], "venue": null, "citeRegEx": "Zhu et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Zhu et al\\.", "year": 2013}, {"title": "A re-ranking model for dependency parser with recursive convolutional neural network", "author": ["Chenxi Zhu", "Xipeng Qiu", "Xinchi Chen", "Xuanjing Huang."], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th Inter-", "citeRegEx": "Zhu et al\\.,? 2015a", "shortCiteRegEx": "Zhu et al\\.", "year": 2015}, {"title": "Long short-term memory over tree structures", "author": ["Xiaodan Zhu", "Parinaz Sobhani", "Hongyu Guo."], "venue": "CoRR, abs/1503.04881.", "citeRegEx": "Zhu et al\\.,? 2015b", "shortCiteRegEx": "Zhu et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 4, "context": "Seminal work employs convolutional neural network (Collobert and Weston, 2008), recurrent neural network (Elman, 1990; Mikolov et al.", "startOffset": 50, "endOffset": 78}, {"referenceID": 7, "context": "Seminal work employs convolutional neural network (Collobert and Weston, 2008), recurrent neural network (Elman, 1990; Mikolov et al., 2010) and recursive neural network (Socher et al.", "startOffset": 105, "endOffset": 140}, {"referenceID": 17, "context": "Seminal work employs convolutional neural network (Collobert and Weston, 2008), recurrent neural network (Elman, 1990; Mikolov et al., 2010) and recursive neural network (Socher et al.", "startOffset": 105, "endOffset": 140}, {"referenceID": 22, "context": ", 2010) and recursive neural network (Socher et al., 2011) for sequence and tree modeling.", "startOffset": 37, "endOffset": 58}, {"referenceID": 25, "context": "Recently, Long ShortTerm Memories (LSTM) have received increasing research attention, giving significantly improved accuracies in a variety of sequence tasks (Sutskever et al., 2014; Bahdanau et al., 2014) compared to vanilla recurrent neural networks.", "startOffset": 158, "endOffset": 205}, {"referenceID": 0, "context": "Recently, Long ShortTerm Memories (LSTM) have received increasing research attention, giving significantly improved accuracies in a variety of sequence tasks (Sutskever et al., 2014; Bahdanau et al., 2014) compared to vanilla recurrent neural networks.", "startOffset": 158, "endOffset": 205}, {"referenceID": 28, "context": "Addressing diminishing gradients effectively, they have been extended to tree structures, achieving promising results for tasks such as syntactic language modeling (Zhang et al., 2015), sentiment analysis (Li et al.", "startOffset": 164, "endOffset": 184}, {"referenceID": 15, "context": ", 2015), sentiment analysis (Li et al., 2015; Zhu et al., 2015b; Le and Zuidema, 2015; Tai et al., 2015) and relation extraction (Miwa and Bansal, 2016).", "startOffset": 28, "endOffset": 104}, {"referenceID": 33, "context": ", 2015), sentiment analysis (Li et al., 2015; Zhu et al., 2015b; Le and Zuidema, 2015; Tai et al., 2015) and relation extraction (Miwa and Bansal, 2016).", "startOffset": 28, "endOffset": 104}, {"referenceID": 13, "context": ", 2015), sentiment analysis (Li et al., 2015; Zhu et al., 2015b; Le and Zuidema, 2015; Tai et al., 2015) and relation extraction (Miwa and Bansal, 2016).", "startOffset": 28, "endOffset": 104}, {"referenceID": 26, "context": ", 2015), sentiment analysis (Li et al., 2015; Zhu et al., 2015b; Le and Zuidema, 2015; Tai et al., 2015) and relation extraction (Miwa and Bansal, 2016).", "startOffset": 28, "endOffset": 104}, {"referenceID": 19, "context": ", 2015) and relation extraction (Miwa and Bansal, 2016).", "startOffset": 32, "endOffset": 55}, {"referenceID": 33, "context": "There are three existing methods for constituent tree LSTM (Zhu et al., 2015b; Tai et al., 2015; Le and Zuidema, 2015), which make essentially the same extension from sequence structure LSTMs.", "startOffset": 59, "endOffset": 118}, {"referenceID": 26, "context": "There are three existing methods for constituent tree LSTM (Zhu et al., 2015b; Tai et al., 2015; Le and Zuidema, 2015), which make essentially the same extension from sequence structure LSTMs.", "startOffset": 59, "endOffset": 118}, {"referenceID": 13, "context": "There are three existing methods for constituent tree LSTM (Zhu et al., 2015b; Tai et al., 2015; Le and Zuidema, 2015), which make essentially the same extension from sequence structure LSTMs.", "startOffset": 59, "endOffset": 118}, {"referenceID": 13, "context": ", 2015; Le and Zuidema, 2015), which make essentially the same extension from sequence structure LSTMs. We take the method of Zhu et al. (2015b) as our baseline.", "startOffset": 8, "endOffset": 145}, {"referenceID": 10, "context": "A contrast between the sequence structured LSTM of Hochreiter and Schmidhuber (1997) and the tree-structured LSTM of Zhu et al.", "startOffset": 51, "endOffset": 85}, {"referenceID": 10, "context": "A contrast between the sequence structured LSTM of Hochreiter and Schmidhuber (1997) and the tree-structured LSTM of Zhu et al. (2015b) is shown in Figure 1, which illustrates the input (x), cell (c) and hidden (h) nodes at a certain time step t.", "startOffset": 51, "endOffset": 136}, {"referenceID": 3, "context": "Research has shown that head word information can significantly improve the performance of syntactic parsing (Collins, 2003; Clark and Curran, 2004).", "startOffset": 109, "endOffset": 148}, {"referenceID": 2, "context": "Research has shown that head word information can significantly improve the performance of syntactic parsing (Collins, 2003; Clark and Curran, 2004).", "startOffset": 109, "endOffset": 148}, {"referenceID": 3, "context": "Traditional head-lexicalization relies on specific rules (Collins, 2003), typically extracting heads Figure 2: Head-Lexicalized Constituent Tree.", "startOffset": 57, "endOffset": 72}, {"referenceID": 9, "context": "This is analogous to the bidirectional extension of sequence structure LSTMs, which are commonly used for NLP tasks such as speech recognition (Graves et al., 2013), sentiment analysis (Tai et al.", "startOffset": 143, "endOffset": 164}, {"referenceID": 26, "context": ", 2013), sentiment analysis (Tai et al., 2015; Li et al., 2015) and machine translation (Sutskever et al.", "startOffset": 28, "endOffset": 63}, {"referenceID": 15, "context": ", 2013), sentiment analysis (Tai et al., 2015; Li et al., 2015) and machine translation (Sutskever et al.", "startOffset": 28, "endOffset": 63}, {"referenceID": 25, "context": ", 2015) and machine translation (Sutskever et al., 2014; Bahdanau et al., 2014) tasks.", "startOffset": 32, "endOffset": 79}, {"referenceID": 0, "context": ", 2015) and machine translation (Sutskever et al., 2014; Bahdanau et al., 2014) tasks.", "startOffset": 32, "endOffset": 79}, {"referenceID": 31, "context": "Results on a standard sentiment classification benchmark and a question type classification benchmark show that our tree LSTM structure gives significantly better accuracies compared with the method of Zhu et al. (2015b). We achieve the best reported results for sentiment classification.", "startOffset": 202, "endOffset": 221}, {"referenceID": 10, "context": "LSTM (Hochreiter and Schmidhuber, 1997) is a variation of RNN (Elman, 1990) to solve vanishing gradients in training.", "startOffset": 5, "endOffset": 39}, {"referenceID": 7, "context": "LSTM (Hochreiter and Schmidhuber, 1997) is a variation of RNN (Elman, 1990) to solve vanishing gradients in training.", "startOffset": 62, "endOffset": 75}, {"referenceID": 5, "context": "It has been widely adopted for NLP tasks, such as parsing (Dyer et al., 2015) and machine translation (Bahdanau et al.", "startOffset": 58, "endOffset": 77}, {"referenceID": 0, "context": ", 2015) and machine translation (Bahdanau et al., 2014).", "startOffset": 32, "endOffset": 55}, {"referenceID": 8, "context": "tions (Gers and Schmidhuber, 2000) as our baseline, which models sequences.", "startOffset": 6, "endOffset": 34}, {"referenceID": 26, "context": "There has been extensions of sequence-structured LSTMs for modeling trees (Tai et al., 2015; Le and Zuidema, 2015; Zhu et al., 2015b).", "startOffset": 74, "endOffset": 133}, {"referenceID": 13, "context": "There has been extensions of sequence-structured LSTMs for modeling trees (Tai et al., 2015; Le and Zuidema, 2015; Zhu et al., 2015b).", "startOffset": 74, "endOffset": 133}, {"referenceID": 33, "context": "There has been extensions of sequence-structured LSTMs for modeling trees (Tai et al., 2015; Le and Zuidema, 2015; Zhu et al., 2015b).", "startOffset": 74, "endOffset": 133}, {"referenceID": 13, "context": ", 2015; Le and Zuidema, 2015; Zhu et al., 2015b). The idea is similar to extending recursive neural network to recurrent neural network, where gates are used in recursive structures for alleviating diminishing gradients. While Tai et al. (2015) investigated tree-structured LSTMs for both dependency structures and constituent structures, and both for unrestricted trees and M-nary trees, Zhu et al.", "startOffset": 8, "endOffset": 245}, {"referenceID": 13, "context": ", 2015; Le and Zuidema, 2015; Zhu et al., 2015b). The idea is similar to extending recursive neural network to recurrent neural network, where gates are used in recursive structures for alleviating diminishing gradients. While Tai et al. (2015) investigated tree-structured LSTMs for both dependency structures and constituent structures, and both for unrestricted trees and M-nary trees, Zhu et al. (2015b) and Le and Zuidema (2015) focused on binary constituent trees.", "startOffset": 8, "endOffset": 408}, {"referenceID": 13, "context": ", 2015; Le and Zuidema, 2015; Zhu et al., 2015b). The idea is similar to extending recursive neural network to recurrent neural network, where gates are used in recursive structures for alleviating diminishing gradients. While Tai et al. (2015) investigated tree-structured LSTMs for both dependency structures and constituent structures, and both for unrestricted trees and M-nary trees, Zhu et al. (2015b) and Le and Zuidema (2015) focused on binary constituent trees.", "startOffset": 8, "endOffset": 434}, {"referenceID": 13, "context": ", 2015; Le and Zuidema, 2015; Zhu et al., 2015b). The idea is similar to extending recursive neural network to recurrent neural network, where gates are used in recursive structures for alleviating diminishing gradients. While Tai et al. (2015) investigated tree-structured LSTMs for both dependency structures and constituent structures, and both for unrestricted trees and M-nary trees, Zhu et al. (2015b) and Le and Zuidema (2015) focused on binary constituent trees. As discussed earlier, none of these existing methods make direct use of lexical input for composing constituent vectors. We take Zhu et al. (2015b) as our baseline.", "startOffset": 8, "endOffset": 619}, {"referenceID": 9, "context": "Bidirectional information has been leveraged to extend sequence LSTM (Graves et al., 2013).", "startOffset": 69, "endOffset": 90}, {"referenceID": 9, "context": "Bidirectional information has been leveraged to extend sequence LSTM (Graves et al., 2013). On the other hand, the aforementioned tree-LSTM models work bottom-up, without information flew from parents to children. Zhang et al. (2015) built a topdown tree-LSTM for dependency trees, but without a bottom-up component.", "startOffset": 70, "endOffset": 234}, {"referenceID": 9, "context": "Bidirectional information has been leveraged to extend sequence LSTM (Graves et al., 2013). On the other hand, the aforementioned tree-LSTM models work bottom-up, without information flew from parents to children. Zhang et al. (2015) built a topdown tree-LSTM for dependency trees, but without a bottom-up component. Paulus et al. (2014) made use of bidirectional information on binary trees, but for recursive neural networks only.", "startOffset": 70, "endOffset": 338}, {"referenceID": 9, "context": "Bidirectional information has been leveraged to extend sequence LSTM (Graves et al., 2013). On the other hand, the aforementioned tree-LSTM models work bottom-up, without information flew from parents to children. Zhang et al. (2015) built a topdown tree-LSTM for dependency trees, but without a bottom-up component. Paulus et al. (2014) made use of bidirectional information on binary trees, but for recursive neural networks only. The closest in spirit to our method, Miwa and Bansal (2016) adopted a bidirectional Tree LSTM model to jointly extract named entities and relations under dependency tree structure.", "startOffset": 70, "endOffset": 493}, {"referenceID": 18, "context": "For NLP, the input vectors are typically word embeddings (Mikolov et al., 2013), but can also include PoS embeddings, character embeddings or other types of information.", "startOffset": 57, "endOffset": 79}, {"referenceID": 31, "context": "The bottom-up Tree LSTM of Zhu et al. (2015b) extends the left-to-right sequence LSTM by splitting the previous state vector ht\u22121 into a left child state vector ht\u22121 and a right child state vector h R t\u22121, and the previous cell vector ct\u22121 into a left child cell vector ct\u22121 and a right child cell vector c R t\u22121, calculating ct as", "startOffset": 27, "endOffset": 46}, {"referenceID": 31, "context": "Figure 3: Contrast between Zhu et al. (2015b) (a) and this paper (b).", "startOffset": 27, "endOffset": 46}, {"referenceID": 3, "context": "Rather than selecting head lexicons using manually-defined head-finding rules, which are language- and formalism-dependent (Collins, 2003), we cast head finding as a part of the neural network model, learning the head lexicon of each constituent by a gated combination of head lexicons of its two children1.", "startOffset": 123, "endOffset": 138}, {"referenceID": 30, "context": "In contrast, the method of Zhu et al. (2015b) in Figure 3 (a) does not have the input vector xt for non-leaf constituents.", "startOffset": 27, "endOffset": 46}, {"referenceID": 31, "context": "Given head lexicon vectors for nodes, the Tree LSTM of Zhu et al. (2015b) can be extended by leveraging xt in calculating the corresponding ct.", "startOffset": 55, "endOffset": 74}, {"referenceID": 12, "context": "Typicial binarization methods, such as head binarization (Klein and Manning, 2003) , also rely on specific head-finding rules.", "startOffset": 57, "endOffset": 82}, {"referenceID": 9, "context": ", xn], a bidirectional sequential LSTM (Graves et al., 2013) computes two sets of hidden state vectors, [h\u03031, h\u03032, .", "startOffset": 39, "endOffset": 60}, {"referenceID": 11, "context": "The model parameters are optimized using ADAM (Kingma and Ba, 2014) without gradient clipping, with the default hyper-parameters of the AdamTrainer in the Dynet toolkits.", "startOffset": 46, "endOffset": 67}, {"referenceID": 11, "context": "The model parameters are optimized using ADAM (Kingma and Ba, 2014) without gradient clipping, with the default hyper-parameters of the AdamTrainer in the Dynet toolkits.2 We also use dropout (Srivastava et al., 2014) at lexical input embeddings with a fixed probability pdrop to avoid overfitting. pdrop is set to 0.5 for all tasks. Following Tai et al. (2015), Li et al.", "startOffset": 47, "endOffset": 362}, {"referenceID": 11, "context": "The model parameters are optimized using ADAM (Kingma and Ba, 2014) without gradient clipping, with the default hyper-parameters of the AdamTrainer in the Dynet toolkits.2 We also use dropout (Srivastava et al., 2014) at lexical input embeddings with a fixed probability pdrop to avoid overfitting. pdrop is set to 0.5 for all tasks. Following Tai et al. (2015), Li et al. (2015), Zhu et al.", "startOffset": 47, "endOffset": 380}, {"referenceID": 11, "context": "The model parameters are optimized using ADAM (Kingma and Ba, 2014) without gradient clipping, with the default hyper-parameters of the AdamTrainer in the Dynet toolkits.2 We also use dropout (Srivastava et al., 2014) at lexical input embeddings with a fixed probability pdrop to avoid overfitting. pdrop is set to 0.5 for all tasks. Following Tai et al. (2015), Li et al. (2015), Zhu et al. (2015b) and Le and Zuidema (2015), we use Glove-300d word embeddings3 to train our model.", "startOffset": 47, "endOffset": 400}, {"referenceID": 11, "context": "The model parameters are optimized using ADAM (Kingma and Ba, 2014) without gradient clipping, with the default hyper-parameters of the AdamTrainer in the Dynet toolkits.2 We also use dropout (Srivastava et al., 2014) at lexical input embeddings with a fixed probability pdrop to avoid overfitting. pdrop is set to 0.5 for all tasks. Following Tai et al. (2015), Li et al. (2015), Zhu et al. (2015b) and Le and Zuidema (2015), we use Glove-300d word embeddings3 to train our model.", "startOffset": 47, "endOffset": 426}, {"referenceID": 24, "context": "Specifically, the Stanford Sentiment Treebank (Socher et al., 2013b) is used to train our classification model, which contains sentiments for movie reviews.", "startOffset": 46, "endOffset": 68}, {"referenceID": 28, "context": "For sentiment classification, we use the same data settings as Zhu et al. (2015b). Specifically, the Stanford Sentiment Treebank (Socher et al.", "startOffset": 63, "endOffset": 82}, {"referenceID": 13, "context": "Gold-standard tree structures are used for training and testing (Le and Zuidema, 2015; Li et al., 2015; Zhu et al., 2015b; Tai et al., 2015).", "startOffset": 64, "endOffset": 140}, {"referenceID": 15, "context": "Gold-standard tree structures are used for training and testing (Le and Zuidema, 2015; Li et al., 2015; Zhu et al., 2015b; Tai et al., 2015).", "startOffset": 64, "endOffset": 140}, {"referenceID": 33, "context": "Gold-standard tree structures are used for training and testing (Le and Zuidema, 2015; Li et al., 2015; Zhu et al., 2015b; Tai et al., 2015).", "startOffset": 64, "endOffset": 140}, {"referenceID": 26, "context": "Gold-standard tree structures are used for training and testing (Le and Zuidema, 2015; Li et al., 2015; Zhu et al., 2015b; Tai et al., 2015).", "startOffset": 64, "endOffset": 140}, {"referenceID": 14, "context": "For the question type classification task, we use the TREC data (Li and Roth, 2002).", "startOffset": 64, "endOffset": 83}, {"referenceID": 29, "context": "Since there is no development set, we follow Zhou et al. (2015), randomly extracting 500 examples from the training set as a development set.", "startOffset": 45, "endOffset": 64}, {"referenceID": 10, "context": "The first is a bidirectional LSTM (BiLSTM) (Hochreiter and Schmidhuber, 1997; Graves et al., 2013).", "startOffset": 43, "endOffset": 98}, {"referenceID": 9, "context": "The first is a bidirectional LSTM (BiLSTM) (Hochreiter and Schmidhuber, 1997; Graves et al., 2013).", "startOffset": 43, "endOffset": 98}, {"referenceID": 9, "context": "The first is a bidirectional LSTM (BiLSTM) (Hochreiter and Schmidhuber, 1997; Graves et al., 2013). Our bidirectional constituency Tree LSTM (BiConTree) are compared with the bidirectional sequential LSTM to investigate the effectiveness of the tree structure. For the sentiment task, following Tai et al. (2015) and Li et al.", "startOffset": 78, "endOffset": 313}, {"referenceID": 9, "context": "The first is a bidirectional LSTM (BiLSTM) (Hochreiter and Schmidhuber, 1997; Graves et al., 2013). Our bidirectional constituency Tree LSTM (BiConTree) are compared with the bidirectional sequential LSTM to investigate the effectiveness of the tree structure. For the sentiment task, following Tai et al. (2015) and Li et al. (2015), we convert the treebank into sequences to allow the bidirectional LSTM model to make use of every phrase span as a training example.", "startOffset": 78, "endOffset": 334}, {"referenceID": 9, "context": "The first is a bidirectional LSTM (BiLSTM) (Hochreiter and Schmidhuber, 1997; Graves et al., 2013). Our bidirectional constituency Tree LSTM (BiConTree) are compared with the bidirectional sequential LSTM to investigate the effectiveness of the tree structure. For the sentiment task, following Tai et al. (2015) and Li et al. (2015), we convert the treebank into sequences to allow the bidirectional LSTM model to make use of every phrase span as a training example. The second baseline model is the bottom-up Tree LSTM model of Zhu et al. (2015b). We make a contrast between this model and our lexicalized bidirectional models to show the effects of adding head lexicalization and top-down information flow.", "startOffset": 78, "endOffset": 549}, {"referenceID": 33, "context": "Our reimplementation of sequential bidirectional LSTM and constituent Tree LSTM (Zhu et al., 2015b) gives comparable results to Zhu et al.", "startOffset": 80, "endOffset": 99}, {"referenceID": 22, "context": "Table 1 shows the main results for the sentiment classification task, where RNTN is the recursive neural tensor model of Socher et al. (2013b), ConTree and DepTree denote constituency Tree LSTMs and dependency Tree LSTMs, respectively.", "startOffset": 121, "endOffset": 143}, {"referenceID": 22, "context": "Table 1 shows the main results for the sentiment classification task, where RNTN is the recursive neural tensor model of Socher et al. (2013b), ConTree and DepTree denote constituency Tree LSTMs and dependency Tree LSTMs, respectively. Our reimplementation of sequential bidirectional LSTM and constituent Tree LSTM (Zhu et al., 2015b) gives comparable results to Zhu et al. (2015b)\u2019s original implementation.", "startOffset": 121, "endOffset": 383}, {"referenceID": 24, "context": "5 Model 5-class binary Root Phrase Root Phrase RNTN(Socher et al., 2013b) 45.", "startOffset": 51, "endOffset": 73}, {"referenceID": 15, "context": "6 BiLSTM(Li et al., 2015) 49.", "startOffset": 8, "endOffset": 25}, {"referenceID": 26, "context": "7 DepTree(Tai et al., 2015) 48.", "startOffset": 9, "endOffset": 27}, {"referenceID": 13, "context": "7 ConTree(Le and Zuidema, 2015) 49.", "startOffset": 9, "endOffset": 31}, {"referenceID": 33, "context": "0 ConTree(Zhu et al., 2015b) 50.", "startOffset": 9, "endOffset": 28}, {"referenceID": 15, "context": "ConTree(Li et al., 2015) 50.", "startOffset": 7, "endOffset": 24}, {"referenceID": 26, "context": "7 ConTree(Tai et al., 2015) 51.", "startOffset": 9, "endOffset": 27}, {"referenceID": 15, "context": "There is no significant difference between different models, as consistent with the observation of Li et al. (2015). To our knowledge, these are the best reported results for this sentiment classification task until now.", "startOffset": 99, "endOffset": 116}, {"referenceID": 21, "context": "4 SVM (Silva et al., 2011) 95.", "startOffset": 6, "endOffset": 26}, {"referenceID": 3, "context": "Interestingly, the lexical heads contain both syntactic and sentiment information: while some heads correspond well to Collins syntactic rules (Collins, 2003), others are driven by subjective words.", "startOffset": 143, "endOffset": 158}, {"referenceID": 31, "context": "5 points compared to the baseline ConTree LSTM of Zhu et al. (2015b), which demonstrates the strength of our model for handling long range information.", "startOffset": 50, "endOffset": 69}, {"referenceID": 31, "context": "Model F1 Baseline (Zhu et al. (2013)) 90.", "startOffset": 19, "endOffset": 37}, {"referenceID": 16, "context": "The standard PTB (Marcus et al., 1993) split (Collins, 2003) are used, and the same off-the-shelf ZPar model is adopted for our baseline.", "startOffset": 17, "endOffset": 38}, {"referenceID": 3, "context": ", 1993) split (Collins, 2003) are used, and the same off-the-shelf ZPar model is adopted for our baseline.", "startOffset": 14, "endOffset": 29}, {"referenceID": 32, "context": "A standard interpolated reranker (Zhu et al., 2015a; Zhou et al., 2016) is adopted for scoring 8-best trees.", "startOffset": 33, "endOffset": 71}, {"referenceID": 30, "context": "A standard interpolated reranker (Zhu et al., 2015a; Zhou et al., 2016) is adopted for scoring 8-best trees.", "startOffset": 33, "endOffset": 71}, {"referenceID": 3, "context": ", 1993) split (Collins, 2003) are used, and the same off-the-shelf ZPar model is adopted for our baseline. A standard interpolated reranker (Zhu et al., 2015a; Zhou et al., 2016) is adopted for scoring 8-best trees. For each tree y of sentence x, we follow Socher et al. (2013a) to define f(x, y; \u0398) as the sum of scores of each constituent node,", "startOffset": 15, "endOffset": 279}, {"referenceID": 27, "context": "(2013a), but is not as good as current state-of-theart models, including sequence-to-sequence based LSTM language models (Vinyals et al., 2015; Charniak, 2016) and recurrent neural network grammars (Dyer et al.", "startOffset": 121, "endOffset": 159}, {"referenceID": 1, "context": "(2013a), but is not as good as current state-of-theart models, including sequence-to-sequence based LSTM language models (Vinyals et al., 2015; Charniak, 2016) and recurrent neural network grammars (Dyer et al.", "startOffset": 121, "endOffset": 159}, {"referenceID": 6, "context": ", 2015; Charniak, 2016) and recurrent neural network grammars (Dyer et al., 2016), due to a low baseline oracle and simple reranking models5.", "startOffset": 62, "endOffset": 81}, {"referenceID": 19, "context": "Among neural rerankers, out model outperforms Socher et al. (2013a), but is not as good as current state-of-theart models, including sequence-to-sequence based LSTM language models (Vinyals et al.", "startOffset": 46, "endOffset": 68}, {"referenceID": 1, "context": "(2016) employs 2-layerd LSTMs with input and hidden dimensions of size 256 and 128, respectively, and Charniak (2016) use 3-layered LSTMs with both the input and hidden dimensions of size 1500.", "startOffset": 102, "endOffset": 118}], "year": 2016, "abstractText": "Sequential LSTM has been extended to model tree structures, giving competitive results for a number of tasks. Existing methods model constituent trees by bottom-up combinations of constituent nodes, making direct use of input word information only for leaf nodes. This is different from sequential LSTMs, which contain reference to input words for each node. In this paper, we propose a method for automatic head-lexicalization for tree-structure LSTMs, propagating head words from leaf nodes to every constituent node. In addition, enabled by head lexicalization, we build a tree LSTM in the top-down direction, which corresponds to bidirectional sequential LSTM structurally. Experiments show that both extensions give better representations of tree structures. Our final model gives the best results on the Standford Sentiment Treebank and highly competitive results on the TREC question type classification task.", "creator": "TeX"}}}