{"id": "1611.06791", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Nov-2016", "title": "Generalized Dropout", "abstract": "Deep Neural Networks often require good regularizers to generalize well. Dropout is one such regularizer that is widely used among Deep Learning practitioners. Recent work has shown that Dropout can also be viewed as performing Approximate Bayesian Inference over the network parameters. In this work, we generalize this notion and introduce a rich family of regularizers which we call Generalized Dropout. One set of methods in this family, called Dropout++, is a version of Dropout with trainable parameters. Classical Dropout emerges as a special case of this method. Another member of this family selects the width of neural network layers. Experiments show that these methods help in improving generalization performance over Dropout.", "histories": [["v1", "Mon, 21 Nov 2016 14:06:48 GMT  (48kb,D)", "http://arxiv.org/abs/1611.06791v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.AI cs.CV cs.NE", "authors": ["suraj srinivas", "r venkatesh babu"], "accepted": false, "id": "1611.06791"}, "pdf": {"name": "1611.06791.pdf", "metadata": {"source": "CRF", "title": "Generalized Dropout", "authors": ["Suraj Srinivas"], "emails": ["surajsrinivas@grads.cds.iisc.ac.in", "venky@cds.iisc.ac.in"], "sections": [{"heading": "1 Introduction", "text": "For large tasks such as image classification, the common practice in recent times has been to form large conventional neural networks (CNN). Even with large data sets, the risk of overuse is high because the size of the model is too large. As a result, strong regulatory mechanisms are needed to limit the complexity of these models. Dropout [2] is a stochastic regulator that has become widespread in recent times. However, the rule itself has been proposed as a heurist - with the aim of reducing co-adaptation among neurons. Behaviour was (and is) not well understood. Gal and Gharamani have shown that Dropout is implicitly capable of keeping Bayesian informants secret."}, {"heading": "2 Bayesian Neural Networks", "text": "In this section we will formally introduce the concept of BNNs and also discuss our proposed method. Let f (;; w) denote a function of the neural network with parameters w. For a given input x, the neural network y = f (x; w) generates a probability distribution over possible labels (by Softmax) for a classification problem. In view of the training data D, the parameter vector w is updated using the Bayes rule. P (y | x, w) = f (x; w) P (y | w) P (w) (w) (1) After calculating the posterior distribution, we draw conclusions about a new data point x as follows. Since the neural network generates a probability distribution over labels, P (y | x, w) = f (x; w) P (y | x, D) = w P (y | x, y)."}, {"heading": "2.1 Bayesian Neural Networks with Gates", "text": "We add multiplicative gates to each characteristic / neuron in the neural network, as described in Figure 1a. These gates modulate the output of each neuron. Let us also assume that they are in [0, 1]. Intuitively, these gates now control the relative meaning of each individual characteristic from the point of view of the next layer. This relative meaning may be fundamentally uncertain due to a particular characteristic. Therefore, it may be useful to imagine the gate parameters as random variables. However, we will now crystallize all these assumptions in the form of decisions for variable distribution. We will first put the previous hyperprevious pair above the gate parameters. \u2212 Therefore, it may be useful to consider the regular parameters as random variables."}, {"heading": "2.2 Generalized Dropout", "text": "Given all the assumptions and approximations discussed above, we will now write the complete objective function we want to solve. Since the varying distributions are for w and k delta distributions, we will now use w, k in our notations instead of \u00b51, \u00b52 to ensure simplicity."}, {"heading": "2.3 Architecture Learning", "text": "Srinivas and Babu [6] have recently introduced a method to learn the breadth and depth of neural network architectures, adding additional learnable parameters similar to those of gates. Moreover, their objective function in our notation has the following form: w *, k * = argmin w, k \u2212 logP (D | \u03b8s, w, k) + \u03bb1k (1 \u2212 k) + \u03bb3k (8), where \u03b8s = heaviside (k \u2212 0,5) Note that our objective function 7 is very similar to this when \u03b1, \u03b2 < 1 and \u03b1 < \u03b2, except that we use logk instead of k. Another difference is that they use a heaviside threshold to select a frame instead of sampling from a Bernoulli. We observe that this corresponds to taking the highest possible sample from the Bernoulli distribution. Given these similarities, we found that it is appropriate to rename the corresponding method with a chastic architecture, \u03b1 < 1."}, {"heading": "2.4 A Practitioner\u2019s Perspective", "text": "In this section, we will try to provide an intuitive explanation for Generalized Dropout + (0) the most encouraging values. Let's go back to Fig. 1a, each neuron is augmented with a gate that values between 0 and 1. This is forced by our regulators and by parameter clipping. During the run-up, we treat each of these gate values as probabilities and toss a coin with that probability. Issuing the coin toss is used to block / allow neuron outputs. As a result of learning, important characteristics tend to have higher probability values than unimportant characteristics. At test time, we do not perform sampling. Rather, we simply use the real probability values in the gate variables. This approximation - called re-scaling - is also used in classic dropouts. What do the various Generalized Dropout methods that have to do? Intuitively, they place limitations on the gate values (probabilities) that can be learned."}, {"heading": "2.5 Estimating gradients for binary stochastic gates", "text": "Considering our formalism of stochastic gate variables, it is unclear how to calculate error gradients through them. Bengio et al. [7] examined this problem for binary stochastic neurons and empirically confirmed the effectiveness of various solutions. They conclude that the simplest method of calculating gradients - the continuous estimator - works best overall, simply by multiplying through a stochastic neuron as if it were an identity function. If the sampling step is given by \u03b8-Bernoulli (k), then the grade president dk = 1 is used. Another consideration is to ensure that k is always in [0, 1] so that it is a valid Bernoulli parameter. Bengio et al. [7] use a sigmoid activation via k. Our experiments showed that clipping functions worked better. This can be imagined as a \"linearized\" sigmogradation."}, {"heading": "2.6 Applying to Convolution Layers", "text": "Suppose the output feature map of a revolutionary level is k \u00b7 k \u00b7 n, i.e. n feature maps of size k \u00b7 k. If we want to make an architectural selection such as Architecture Learning [6], we must select a subset of n feature maps. In this case, we only have n gate variables multiplied by the output of each feature map. If a gate is near zero and the output of the entire feature map is near zero at test time, we determine which of the n filters in the previous level are indispensable."}, {"heading": "3 Related Work", "text": "There is much work aimed at extending dropout. DropConnect [8] stochastically drops weights rather than neurons to achieve better accuracy in networks. As mentioned before, the use of the independence assumption for weights may not be correct. In fact, it turns out that DropConnect only works on fully interconnected layers. [9] What is striking is a version of dropout where the dropout rate depends on the output activation of a layer. Variational Dropout [10] suggests a Bayesian interpretation for Gaussian dropout instead of the canonical multiplicative dropout. By considering multiplicative dropouts, we establish important links to Architecture Learning / Neuron Pruning. Gal and Gharamani [3] showed a Bayesian interpretation for binary dropouts and show that test performance improves by turning away from Monte Carlo rather than re-scaling."}, {"heading": "4 Experiments", "text": "In this section, we will conduct experiments with the Generalized Dropout family to test its usefulness. First, we will perform a variety of analyses with the Generalized Dropout family. Later, we will examine some specific applications of this method. We will conduct experiments primarily with Theano [15] and lasagne."}, {"heading": "4.1 Analysis of Generalized Dropout", "text": "For the experiments with the MNIST dataset, we use the standard LeNet-like architecture [16], which consists of two 5 \u00d7 5 layers with 20 and 50 filters and two fully connected layers with 500 and 10 (initial layer) neurons. Although there is nothing special about this architecture, we simply use it as a standard network to analyze our method."}, {"heading": "4.1.1 Effect of data-size", "text": "We are investigating whether generalized dropout does indeed have an advantage over dropout in terms of accuracy. At this point, we are applying dropout and generalized dropout only to the last fully connected level. Our experiments show that for the network under consideration, the accuracies achieved by a generalized dropout method are not always strictly better than dropout, as shown in Figure 2a. This indicates that most of Dropout's regulatory power comes from the assumption of variational inference, and not from certain values of the dropout parameter. This is a surprising result that we will use to our advantage in the paper. However, we note that dropout + + (0) appears to be advantageous to small data quantities over dropout (Figure 2b), possibly because dropout + + (0) forces most (but not all) neurons to have a very small capacity due to the low value of the parameters."}, {"heading": "4.1.2 Effect of Layer-width", "text": "Inspired by the results above on dropout + + (0), we look at the relationship between the use of different layer widths for the fully connected layer and the learned gate parameters. Intuitively, of course, it is to assume that larger layers should learn lower gate values, while smaller layers should learn much higher values if we want the overall capacity of the layer to remain roughly the same. Our experiments confirm this intuition, as shown in Figure 2c.1 Note that in our notation, a high value of dropout + + indicates a high probability of retaining the neuron, as opposed to the commonly used notation for dropout. We also test whether this flexibility is reflected in higher accuracy numbers above a fixed dropout value, and we find that this is indeed the case. We find that for small layer widths dropout (at p = 0.5 for example) tends to remove too many neurons, while dropout + 2 matches its parameter values as shown in the layout + 2."}, {"heading": "4.1.3 Effect of Initialization", "text": "To test whether this also applies to the newly introduced Generalized Dropout parameters, we will try different initializations of the Generalized Dropout parameters. In this example, we will simply initialize all gates to a single constant value. As expected, choosing this initialization is much less critical than setting the dropout value, as shown in Figure 2. However, choosing the initialization affects the training time. As an example, it is empirically observed that dropout with p = 0.1 is much slower than p = 0.9. Therefore, it is helpful to have higher dropout rates to enable faster training. To support faster training in Dropout + +, we simply initialize with p = 1.0, i.e. starting with a network without dropout, and gradually learning how much dropout to add."}, {"heading": "4.1.4 Visualization of Learnt Parameters", "text": "Similar effects persist when we apply them to winding layers as well. Here, we visualize the learned parameters in winding layers. First, we add dropout + + only to the input layer. The resulting gate parameters are shown in Figure 2g. We observe a similar effect when we add dropout + + only to the first winding layer, as shown in Figure 2h, which shows the average gate map of all the Convolutionary Filters in that layer. In both cases, we observe that Dropout + + learns to selectively take care of the center of the image, not the corners. This has several advantages. First, by not looking at the corners of each feature, we can potentially reduce the evaluation time of the model. Second, this translation equivalence breaks implicitly in the windings, as in our case certain spatial positions are more important to the eye than it could be for other layers (if they are not helpful everywhere)."}, {"heading": "4.1.5 Architecture Selection", "text": "We will now attempt to automatically learn the required layer width of the network using stochastic architecture learning (SAL), the inherent assumption being that the original architecture is overcomplete and that a subset of neurons is sufficient to achieve similar performance. We will first learn the network parameters using the SAL regularizer, and later we will trim neurons with low gate parameters. Figure 2i shows that SAL learns gate parameters that are often close to 0 or 1, resulting in a much greater increase compared to the other methods. We will use this steep increase as a criterion to select the width of a layer. We observe that variation of the \u03b2 / \u03b1 parameter encourages the method to obtain smaller architectures, sometimes at the expense of accuracy, as shown in Table 1."}, {"heading": "4.2 Dropout++ on standard models", "text": "We will now move to larger networks to test the effectiveness of Dropout + +. Modern networks mainly use Dropout only in the fully connected layers, or simply not at all, due to very powerful regulators such as batch normalization. Here, we take such networks, simply add Dropout + + (flat) after each layer, and see if we get an increase in accuracy. We experiment per form with ResNet32, ResNet56, and a generic VGG-like network, all trained on the CIFAR-10 dataset. As shown in Table 2, the addition of Dropout + + is largely helpful for all three models."}, {"heading": "5 Conclusion", "text": "One group of methods in this family, Dropout + +, is an adaptive version of Dropout. Stochastic Architecture Learning is another group of methods that performs architecture selection. An uninformed selection of the dropout parameter usually harms performance. Dropout + + helps to set a useful parameter value, regardless of factors such as layer width and initialization. Experiments show that it is generally advantageous to simply add Dropout + + (flat) after each layer of a deep network."}], "references": [{"title": "Imagenet classification with deep convolutional neural networks", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E Hinton"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2012}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["Nitish Srivastava", "Geoffrey Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1929}, {"title": "Bayesian convolutional neural networks with bernoulli approximate variational inference", "author": ["Yarin Gal", "Zoubin Ghahramani"], "venue": "arXiv preprint arXiv:1506.02158,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2015}, {"title": "Pattern recognition", "author": ["Christopher M Bishop"], "venue": "Machine Learning,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2006}, {"title": "Predicting parameters in deep learning", "author": ["Misha Denil", "Babak Shakibi", "Laurent Dinh", "Nando de Freitas"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2013}, {"title": "Learning the architecture of deep neural networks", "author": ["Suraj Srinivas", "R Venkatesh Babu"], "venue": "arXiv preprint arXiv:1511.05497,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2015}, {"title": "Estimating or propagating gradients through stochastic neurons for conditional computation", "author": ["Yoshua Bengio", "Nicholas L\u00e9onard", "Aaron Courville"], "venue": "arXiv preprint arXiv:1308.3432,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2013}, {"title": "Regularization of neural networks using dropconnect", "author": ["Li Wan", "Matthew Zeiler", "Sixin Zhang", "Yann L Cun", "Rob Fergus"], "venue": "In Proceedings of the 30th International Conference on Machine Learning", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2013}, {"title": "Adaptive dropout for training deep neural networks", "author": ["Jimmy Ba", "Brendan Frey"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2013}, {"title": "Variational dropout and the local reparameterization trick", "author": ["Diederik P Kingma", "Tim Salimans", "Max Welling"], "venue": "arXiv preprint arXiv:1506.02557,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2015}, {"title": "Keeping the neural networks simple by minimizing the description length of the weights", "author": ["Geoffrey E Hinton", "Drew Van Camp"], "venue": "In Proceedings of the sixth annual conference on Computational learning theory,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1993}, {"title": "Practical variational inference for neural networks", "author": ["Alex Graves"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2011}, {"title": "Weight uncertainty in neural networks", "author": ["Charles Blundell", "Julien Cornebise", "Koray Kavukcuoglu", "Daan Wierstra"], "venue": "arXiv preprint arXiv:1505.05424,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2015}, {"title": "Probabilistic backpropagation for scalable learning of bayesian neural networks", "author": ["Jos\u00e9 Miguel Hern\u00e1ndez-Lobato", "Ryan P Adams"], "venue": "arXiv preprint arXiv:1502.05336,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2015}, {"title": "Theano: a cpu and gpu math expression compiler", "author": ["James Bergstra", "Olivier Breuleux", "Fr\u00e9d\u00e9ric Bastien", "Pascal Lamblin", "Razvan Pascanu", "Guillaume Desjardins", "Joseph Turian", "David Warde-Farley", "Yoshua Bengio"], "venue": "In Proceedings of the Python for scientific computing conference (SciPy),", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2010}, {"title": "Gradient-based learning applied to document recognition", "author": ["Yann LeCun", "L\u00e9on Bottou", "Yoshua Bengio", "Patrick Haffner"], "venue": "Proceedings of the IEEE,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1998}, {"title": "Deep residual learning for image recognition", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"], "venue": "arXiv preprint arXiv:1512.03385,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2015}, {"title": "Scalable bayesian optimization using deep neural networks", "author": ["Jasper Snoek", "Oren Rippel", "Kevin Swersky", "Ryan Kiros", "Nadathur Satish", "Narayanan Sundaram", "Md Patwary", "Mostofa Ali", "Ryan P Adams"], "venue": "arXiv preprint arXiv:1502.05700,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2015}, {"title": "Deepface: Closing the gap to human-level performance in face verification", "author": ["Yaniv Taigman", "Ming Yang", "Marc\u2019Aurelio Ranzato", "Lior Wolf"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2014}], "referenceMentions": [{"referenceID": 0, "context": "For large-scale tasks like image classification, the general practice in recent times [1] has been to train large Convolutional Neural Network (CNN) models.", "startOffset": 86, "endOffset": 89}, {"referenceID": 1, "context": "Dropout [2] is a stochastic regularizer that has been widely used in recent times.", "startOffset": 8, "endOffset": 11}, {"referenceID": 2, "context": "Gal and Gharamani [3] showed that dropout implicitly performs approximate Bayesian inference making it a Bayesian Neural Net.", "startOffset": 18, "endOffset": 21}, {"referenceID": 3, "context": "MCMC and Variational Inference (VI) [4] are two popular methods for performing these approximations.", "startOffset": 36, "endOffset": 39}, {"referenceID": 4, "context": "[5] explicitly show that parameters of a NN can be predicted given other parameters - hinting at the large amount of correlation present.", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "Let us also assume that they lie in [0, 1].", "startOffset": 36, "endOffset": 42}, {"referenceID": 0, "context": "5] Dropout++ [flat] Dropout++ [1] Dropout++ [0] Stochastic Architecture Learning", "startOffset": 30, "endOffset": 33}, {"referenceID": 5, "context": "3 Architecture Learning Srinivas and Babu [6] recently introduced a method to learn the width and depth of neural network architectures.", "startOffset": 42, "endOffset": 45}, {"referenceID": 6, "context": "[7] investigated this problem for binary stochastic neurons and empirically verified the efficacy of different solutions.", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "Another issue of consideration is that of ensuring that k always lies in [0, 1] so that it is a valid bernoulli parameter.", "startOffset": 73, "endOffset": 79}, {"referenceID": 6, "context": "[7] use a sigmoid activation over k.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "However, if we wish to perform architecture selection like Architecture Learning [6], we need to select a subset of the n feature maps.", "startOffset": 81, "endOffset": 84}, {"referenceID": 7, "context": "DropConnect [8] stochastically drops weights instead of neurons to obtain better accuracy on ensembles of networks.", "startOffset": 12, "endOffset": 15}, {"referenceID": 8, "context": "Standout [9] is a version of Dropout where the dropout rate depends on the output activations of a layer.", "startOffset": 9, "endOffset": 12}, {"referenceID": 9, "context": "Variational Dropout [10] proposes a Bayesian interpretation for Gaussian Dropout rather than the canonical multiplicative Dropout.", "startOffset": 20, "endOffset": 24}, {"referenceID": 2, "context": "Gal and Gharamani [3] showed a Bayesian interpretation for binary dropout and show that test performance improves by performing Monte-Carlo averaging rather than re-scaling.", "startOffset": 18, "endOffset": 21}, {"referenceID": 10, "context": "Hinton and Van Camp [11] first introduced variational inference for making Neural Networks Bayesian.", "startOffset": 20, "endOffset": 24}, {"referenceID": 11, "context": "Recent work by Graves [12] and Blundell et al.", "startOffset": 22, "endOffset": 26}, {"referenceID": 12, "context": "[13] further investigated this notion by using different priors and relevant approximations for large networks.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "Probabalistic Backpropagation [14] is an algorithm for inferring marginal posterior probabilities for special classes of Bayesian Neural Networks.", "startOffset": 30, "endOffset": 34}, {"referenceID": 14, "context": "We perform experiments primarily using Theano [15] and Lasagne.", "startOffset": 46, "endOffset": 50}, {"referenceID": 15, "context": "For the experiments on the MNIST dataset, we use the standard LeNet-like architecture [16], which consists of two 5 \u00d7 5 convolutional layers with 20 and 50 filters, and two fully connected layers with 500 and 10 (output layer) neurons.", "startOffset": 86, "endOffset": 90}, {"referenceID": 5, "context": "82 431k Architecture Learning [6] 20-50-20-10 0.", "startOffset": 30, "endOffset": 33}, {"referenceID": 16, "context": "8) ResNet-32 [17] 7.", "startOffset": 13, "endOffset": 17}, {"referenceID": 16, "context": "8 ResNet-56 [17] 6.", "startOffset": 12, "endOffset": 16}, {"referenceID": 17, "context": "18 GenericNet [18] 6.", "startOffset": 14, "endOffset": 18}, {"referenceID": 18, "context": "Such locally connected layers have been previously used in works such as DeepFace [19].", "startOffset": 82, "endOffset": 86}], "year": 2016, "abstractText": "Deep Neural Networks often require good regularizers to generalize well. Dropout is one such regularizer that is widely used among Deep Learning practitioners. Recent work has shown that Dropout can also be viewed as performing Approximate Bayesian Inference over the network parameters. In this work, we generalize this notion and introduce a rich family of regularizers which we call Generalized Dropout. One set of methods in this family, called Dropout++, is a version of Dropout with trainable parameters. Classical Dropout emerges as a special case of this method. Another member of this family selects the width of neural network layers. Experiments show that these methods help in improving generalization performance over Dropout.", "creator": "LaTeX with hyperref package"}}}