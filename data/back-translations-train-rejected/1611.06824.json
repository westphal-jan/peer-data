{"id": "1611.06824", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Nov-2016", "title": "Options Discovery with Budgeted Reinforcement Learning", "abstract": "We consider the problem of learning hierarchical policies for Reinforcement Learning able to discover options, an option corresponding to a sub-policy over a set of primitive actions. Different models have been proposed during the last decade that usually rely on a predefined set of options. We specifically address the problem of automatically discovering options in decision processes. We describe a new RL learning framework called Bi-POMDP, and a new learning model called Budgeted Option Neural Network (BONN) able to discover options based on a budgeted learning objective. Since Bi-POMDP are more general than POMDP, our model can also be used to discover options for classical RL tasks. The BONN model is evaluated on different classical RL problems, demonstrating both quantitative and qualitative interesting results.", "histories": [["v1", "Mon, 21 Nov 2016 15:05:55 GMT  (225kb,D)", "http://arxiv.org/abs/1611.06824v1", "Under review as a conference paper at ICLR 2017"], ["v2", "Tue, 24 Jan 2017 17:06:24 GMT  (273kb,D)", "http://arxiv.org/abs/1611.06824v2", "Under review as a conference paper at ICLR 2017"], ["v3", "Wed, 22 Feb 2017 13:12:33 GMT  (567kb,D)", "http://arxiv.org/abs/1611.06824v3", "Under review as a conference paper at IJCAI 2017"]], "COMMENTS": "Under review as a conference paper at ICLR 2017", "reviews": [], "SUBJECTS": "cs.LG cs.AI", "authors": ["aur\\'elia l\\'eon", "ludovic denoyer"], "accepted": false, "id": "1611.06824"}, "pdf": {"name": "1611.06824.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["BUDGETED REINFORCE", "Aur\u00e9lia L\u00e9on", "Ludovic Denoyer"], "emails": ["aurelia.leon@lip6.fr", "ludovic.denoyer@lip6.fr"], "sections": [{"heading": "1 INTRODUCTION", "text": "In fact, it is as if most people are able to understand themselves and what they are doing. (...) It is not as if people are able to understand and understand the world. (...) It is as if people are able to understand and understand the world. (...) It is as if people are able to understand and understand the world. (...) It is as if people are able to understand the world. (...) It is as if people in the world are able to understand the world. (...) It is as if people in the world and people in the world, in the world and in the world, in the world, in the world, in the world and in the world, in the world, in the world and in the world, in the world, in the world and in the world, and in the world, and in the world. (...) It is as if people are able to understand and understand the world."}, {"heading": "2 BACKGROUND", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 (PO-) MARKOV DECISION PROCESSES AND REINFORCEMENT LEARNING", "text": "Let us define an MDP as a set of states S, a discrete set of possible actionsA, a transition distribution P (st + 1 | st, at), and a reward function r (s, a). We assume that each state st is associated with an observation xt-Rn, and that xt is a partial view of st (i.e. POMDP), where n is the size of the observation space. Furthermore, we define PI (s) as the probability distribution over the possible initial states of the MDP. In a current course x1, a1, x2, a2,...., xt, a policy is defined by a probability distribution such as \u03c0 (x1, a1, x2,...., xt, a) = P (a | x1, a1, x2, a2,...., xt), which is the probability of any action a at a given time t, knowing the history of the agent."}, {"heading": "2.2 LEARNING WITH POLICY GRADIENT", "text": "Let us specify the discount factor, and R (s1, a1, s2, a2,...., sT) the discounted reward corresponding to the trajectory (s1, a1, s2,...., sT), so that: R (s1, a1, s2, a2,...., sT) = T \u2212 1 \u2211 t = 1 \u03b3t \u2212 1r (st, at) with T the size of the trajectories covered by the policy 3. We can define the amplification problem as an optimization problem so that the optimal policy \u03c0 is calculated by maximizing the expected reward J (\u03c0): J (\u03c0) = Es1 \u2248 PI (s1), a1,...., aT Expanded learning problems [R (s1, a1, s2, a2, a2,..., a2,... (sT) (1), s1 after PI and the actions are sampled on the basis of Procepts. < Various learning algorithms (1, 2, a2, a2...)"}, {"heading": "3 BUDGETED OPTION NEURAL NETWORK", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 BI-OBSERVATIONS POMDP", "text": "This general definition corresponds to many practical cases: for example, a robot that receives information from its camera (xt) but sometimes decides to perform a full scan of the room (yt); a user who drives a car (using xt) but decides to consult its map or GPS (yt); a virtual agent who makes decisions in a virtual world (based on xt) but can request instructions from a human (yt) etc. Note that the Bi-POMPD generalizes the POMDP case by choosing either yt = Celsius or yt = xt and xt = Celsius (see section 4)."}, {"heading": "3.2 THE BONN ARCHITECTURE", "text": "We now describe the budgeted neural network option, which is able to learn a policy in Bi-POMDP. This model consists of three components: the underlying idea is that the first component uses the additional observations to calculate which option to use, while the second component typically uses the basic observations xt and the last option selected to try out primitive actions; the third component, which is used to decide when to switch between the options. Therefore, a new option is acquired each time yt. Now, let us describe how each component works: (i) The first (or option model) is aimed at which option is applied depending on the observations yt collected about the states of the process. In our model, an option is represented by a vector representing the size of the option representation space. (ii) The choice of a new option (or option model) depends on which option is applied by the option:"}, {"heading": "3.3 BUDGETED LEARNING FOR OPTIONS DISCOVERY", "text": "Most of these techniques link the problem of option discovery with the problem of sub-target discovery, where different strategies are used to discover the sub-targets - see Botvinick et al. (2009) for a review of the linkages between cognitive research and hierarchical reinforcement learning. BONN's model is based on a different approach, where we assume that the discovery of options will lead to a good balance between policy efficiency and the cognitive efforts generated by such policy.The underlying idea is that a system learns relevant options when these options make it possible to reduce the cognitive efforts generated in solving the task without reducing the quality of the solution.Note that the reduction in cognitive efforts has already been studied in cognitive science (Kool & Botvinick, 2014), and very recently in the RL context (Bacon & Precup), but are defined differently."}, {"heading": "3.4 DISCOVERING A DISCRETE SET OF OPTIONS", "text": "In the previous sections, we considered that the option generated by the option model is a vector in a latent space RO. This differs slightly from the classical option definition, which usually assumes that an agent has a predefined \"catalog\" of possible subroutines, i.e. the option set is a finite discrete set. We propose a variant of the model in which the model learns a finite discrete set of options. Let's call K the (manually set) number of options that one wants to discover. Each option is associated with an (learned) embedding, which is called ok. The option model stores the various possible options and selects which of them an option is required each time. In this case, the option model is considered a stochastic model, capable of trying out an option index that denotes it in {1, 2,..., K} by using a multinomic distribution (on a so-called logmax calculation, it is considered to be able in this case)."}, {"heading": "4 EXPERIMENTS", "text": "The full details of the architecture used for the experiments are in the Appendix -.We have tested this architecture on 3 different types of environments and compared it with a recurrent policy gradient algorithm using a GRU-based neural network (R-PG): CartPole: This is the classic shopping cart pole environment as implemented in the OpenAI Gym4 platform, where the observations are (position, angle, speed, angular velocity), and the actions are right or left. The reward is + 1 for each time step without error. For BONN, the observation is only made by the option model i.e yt = (position, speed, angular velocity, angular velocity), the actor model that xt an empty observation = TIME at each time step. Lunar Lander: This environment corresponds to the Lunar Lander environment, the Lunar Lander environment proposed in OpenAI Gym, where observations describe the position, velocity, angle, angular velocity, angular velocity of the actor, angular velocity), the actuation time, the TIME-lext model = an observation."}, {"heading": "4.1 QUANTITATIVE ANALYSIS", "text": "We illustrate the quality of Bonn with cost / reward curves (see Figure 2), where the X axis corresponds to the number of times an option is calculated (normalizes w.r.t the size of the episode), while the Y axis corresponds to the general reward R (st, at), which is achieved at each level of stochasticity. Note: The cost / reward curves are achieved by calculating the Pareto front across all learned models at different cost levels. These curves were generated by learning our model at 0 and then by progressively increasing these costs, forcing the model to purchase yt more frequently and discover the options. Firstly, one can see that even with few options being calculated, the BONN model is able to obtain the same performance as the model."}, {"heading": "5 RELATED WORK", "text": "Hierarchical Reinforcement Learning (Dayan & Hinton, 1993; Dietterich, 1998; Parr & Russell, 1998) has seen the rise of many different types of work over the last decade because it is seen as a solution to long-term planning tasks and allows knowledge to be transferred between tasks. Many different models have been proposed, in which sub-tasks are a priori known as Dietterich (1998), which propose the MAXQ Method. The concept of the option was introduced by Sutton et al. (1999). In this architecture, each option consists of an initiation set, its own policy (on primitive actions or other options) and a termination function, which defines the likelihood of ending the option due to a particular state. This concept of options is at the core of many current articles, for example in Kulkarni et al. (2016), the Deep Q Learning Framework is expanded to integrate hierarchical value functions, using the intrinsic motivation to manually select the options policy, but the options must first be discovered in the course of these modelling options."}, {"heading": "6 CONCLUSION AND PERSPECTIVES", "text": "We have proposed a new model for learning options in the POMDP and the Bi-POMDP, where the agent can choose to acquire a more informative observation in each step of the time. The model is learned in a budgeted learning environment in which the acquisition of additional information and thus the use of a new option entails costs. The policy learned is a trade-off between the efficiency and the cognitive effort of the agent. In our environment, the options are handled through learned latent representations, and we have also proposed a discrete version of BONN in which the number of options remains constant. Experimental results show that the model is able to extract relevant options in complex environments. This work opens up different research directions. Another question is whether BONN can be applied in multiaspect-related learning problems (the MAZE environment, since the target position is randomly selected for each episode, can be considered a particularly simple multipurpose RL problem because each multiaspect-fund would be considered a different question in the MAN)."}], "references": [{"title": "The option-critic architecture", "author": ["Pierre-Luc Bacon", "Doina Precup"], "venue": "In NIPS Deep Reinforcement Learning Workshop,", "citeRegEx": "Bacon and Precup.,? \\Q2015\\E", "shortCiteRegEx": "Bacon and Precup.", "year": 2015}, {"title": "Hierarchically organized behavior and its neural foundations: A reinforcement-learning perspective", "author": ["Matthew Botvinick", "Yael Niv", "Andrew C. Barto"], "venue": null, "citeRegEx": "Botvinick et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Botvinick et al\\.", "year": 2009}, {"title": "Hierarchical multiscale recurrent neural networks", "author": ["Junyoung Chung", "Sungjin Ahn", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1609.01704,", "citeRegEx": "Chung et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Chung et al\\.", "year": 2016}, {"title": "Recurrent neural networks for adaptive feature acquisition", "author": ["Gabriella Contardo", "Ludovic Denoyer", "Thierry Arti\u00e8res"], "venue": "In International Conference on Neural Information Processing,", "citeRegEx": "Contardo et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Contardo et al\\.", "year": 2016}, {"title": "Feudal reinforcement learning. In Advances in neural information processing systems, pp. 271\u2013271", "author": ["Peter Dayan", "Geoffrey E Hinton"], "venue": null, "citeRegEx": "Dayan and Hinton.,? \\Q1993\\E", "shortCiteRegEx": "Dayan and Hinton.", "year": 1993}, {"title": "The maxq method for hierarchical reinforcement learning", "author": ["Thomas G Dietterich"], "venue": "In ICML,", "citeRegEx": "Dietterich.,? \\Q1998\\E", "shortCiteRegEx": "Dietterich.", "year": 1998}, {"title": "Sequential approaches for learning datum-wise sparse representations", "author": ["Gabriel Dulac-Arnold", "Ludovic Denoyer", "Philippe Preux", "Patrick Gallinari"], "venue": "Machine Learning,", "citeRegEx": "Dulac.Arnold et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Dulac.Arnold et al\\.", "year": 2012}, {"title": "Hierarchical solution of markov decision processes using macro-actions", "author": ["Milos Hauskrecht", "Nicolas Meuleau", "Leslie Pack Kaelbling", "Thomas Dean", "Craig Boutilier"], "venue": "In Proceedings of the Fourteenth conference on Uncertainty in artificial intelligence,", "citeRegEx": "Hauskrecht et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Hauskrecht et al\\.", "year": 1998}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "Kingma and Ba.,? \\Q2014\\E", "shortCiteRegEx": "Kingma and Ba.", "year": 2014}, {"title": "Actor-critic algorithms. In Advances in Neural Information Processing Systems 12, [NIPS Conference, Denver, Colorado, USA, November 29 - December", "author": ["Vijay R. Konda", "John N. Tsitsiklis"], "venue": null, "citeRegEx": "Konda and Tsitsiklis.,? \\Q1999\\E", "shortCiteRegEx": "Konda and Tsitsiklis.", "year": 1999}, {"title": "A labor/leisure tradeoff in cognitive control", "author": ["Wouter Kool", "Matthew Botvinick"], "venue": "Journal of Experimental Psychology: General,", "citeRegEx": "Kool and Botvinick.,? \\Q2014\\E", "shortCiteRegEx": "Kool and Botvinick.", "year": 2014}, {"title": "Hierarchical deep reinforcement learning: Integrating temporal abstraction and intrinsic motivation", "author": ["Tejas D Kulkarni", "Karthik R Narasimhan", "Ardavan Saeedi", "Joshua B Tenenbaum"], "venue": "arXiv preprint arXiv:1604.06057,", "citeRegEx": "Kulkarni et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Kulkarni et al\\.", "year": 2016}, {"title": "Human-level control through deep reinforcement learning", "author": ["Volodymyr Mnih", "Koray Kavukcuoglu", "David Silver", "Andrei A Rusu", "Joel Veness", "Marc G Bellemare", "Alex Graves", "Martin Riedmiller", "Andreas K Fidjeland", "Georg Ostrovski"], "venue": "Nature, 518(7540):529\u2013533,", "citeRegEx": "Mnih et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2015}, {"title": "Strategic attentive writer for learning macro-actions", "author": ["Volodymyr Mnih", "John Agapiou", "Simon Osindero", "Alex Graves", "Oriol Vinyals", "Koray Kavukcuoglu"], "venue": "arXiv preprint arXiv:1606.04695,", "citeRegEx": "Mnih et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2016}, {"title": "Reinforcement learning with hierarchies of machines", "author": ["Ronald Parr", "Stuart Russell"], "venue": "Advances in neural information processing systems,", "citeRegEx": "Parr and Russell.,? \\Q1998\\E", "shortCiteRegEx": "Parr and Russell.", "year": 1998}, {"title": "Between mdps and semi-mdps: A framework for temporal abstraction in reinforcement learning", "author": ["Richard S Sutton", "Doina Precup", "Satinder Singh"], "venue": "Artificial intelligence,", "citeRegEx": "Sutton et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Sutton et al\\.", "year": 1999}, {"title": "Deep reinforcement learning with double qlearning", "author": ["Hado Van Hasselt", "Arthur Guez", "David Silver"], "venue": "CoRR, abs/1509.06461,", "citeRegEx": "Hasselt et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hasselt et al\\.", "year": 2015}, {"title": "Recurrent policy gradients", "author": ["Daan Wierstra", "Alexander F\u00f6rster", "Jan Peters", "J\u00fcrgen Schmidhuber"], "venue": "Logic Journal of the IGPL,", "citeRegEx": "Wierstra et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Wierstra et al\\.", "year": 2010}], "referenceMentions": [{"referenceID": 12, "context": "These new methods include for example the DQN algorithm (Mnih et al., 2015) and its variants (Van Hasselt et al.", "startOffset": 56, "endOffset": 75}, {"referenceID": 17, "context": ", 2015), the use of recurrent architectures with policy gradient models (Wierstra et al., 2010), or even approaches like Guided Policy Search (Levine & Koltun, 2013) or actor-critic algorithms (Konda & Tsitsiklis, 1999).", "startOffset": 72, "endOffset": 95}, {"referenceID": 1, "context": "Research in cognitive science based on the study of human or animal behavior have long emphasized that the internal policy of such agents can be seen as a hierarchical process where solving a task is obtained by sequentially solving sub-tasks, each sub-task being treated by choosing a sequence of primitive actions (Botvinick et al., 2009).", "startOffset": 316, "endOffset": 340}, {"referenceID": 5, "context": "In the computer science domain, these researches have been echoed during the last decade with the apparition of the hierarchical reinforcement learning paradigm (Dayan & Hinton, 1993; Dietterich, 1998; Parr & Russell, 1998) and its generalization to options (Sutton et al.", "startOffset": 161, "endOffset": 223}, {"referenceID": 15, "context": "In the computer science domain, these researches have been echoed during the last decade with the apparition of the hierarchical reinforcement learning paradigm (Dayan & Hinton, 1993; Dietterich, 1998; Parr & Russell, 1998) and its generalization to options (Sutton et al., 1999).", "startOffset": 258, "endOffset": 279}, {"referenceID": 1, "context": "Most of these techniques associate the problem of option discovery with the problem of sub-goals discovery where different strategies are used to discover the sub-goals \u2013 see Botvinick et al. (2009) for a review on links between cognitive research and hierarchical reinforcement learning.", "startOffset": 175, "endOffset": 199}, {"referenceID": 3, "context": "We propose to integrate the acquisition cost C (or cognitive effort) in the learning objective, relying on the budgeted learning paradigm already explored in different RL-based applications (Contardo et al., 2016; Dulac-Arnold et al., 2012).", "startOffset": 190, "endOffset": 240}, {"referenceID": 6, "context": "We propose to integrate the acquisition cost C (or cognitive effort) in the learning objective, relying on the budgeted learning paradigm already explored in different RL-based applications (Contardo et al., 2016; Dulac-Arnold et al., 2012).", "startOffset": 190, "endOffset": 240}, {"referenceID": 15, "context": "Note that this environment is much more difficult than other 4-rooms problems (introduced by Sutton et al. (1999)) in others RL works, where there is only one or two goal(s), and that, in a more realistic way, the agent only observes the room he is in.", "startOffset": 93, "endOffset": 114}, {"referenceID": 5, "context": "Hierarchical Reinforcement Learning (Dayan & Hinton, 1993; Dietterich, 1998; Parr & Russell, 1998) has been the surge of many different works during the last decade because it is considered as one solution to solve long-range planning tasks and allows to transfer knowledge between tasks.", "startOffset": 36, "endOffset": 98}, {"referenceID": 7, "context": "Some models are focused on the problem of learning macro-actions (Hauskrecht et al., 1998; Mnih et al., 2016).", "startOffset": 65, "endOffset": 109}, {"referenceID": 13, "context": "Some models are focused on the problem of learning macro-actions (Hauskrecht et al., 1998; Mnih et al., 2016).", "startOffset": 65, "endOffset": 109}, {"referenceID": 2, "context": "Outside reinforcement learning, our work is also in relation with the Hierarchical Multiscale Recurrent Neural Networks (Chung et al., 2016) that discover hierarchical structure in sequences.", "startOffset": 120, "endOffset": 140}, {"referenceID": 4, "context": "Hierarchical Reinforcement Learning (Dayan & Hinton, 1993; Dietterich, 1998; Parr & Russell, 1998) has been the surge of many different works during the last decade because it is considered as one solution to solve long-range planning tasks and allows to transfer knowledge between tasks. Many different models have been proposed where subtasks are a priori known like Dietterich (1998) which proposes the MAXQ method.", "startOffset": 59, "endOffset": 387}, {"referenceID": 4, "context": "Hierarchical Reinforcement Learning (Dayan & Hinton, 1993; Dietterich, 1998; Parr & Russell, 1998) has been the surge of many different works during the last decade because it is considered as one solution to solve long-range planning tasks and allows to transfer knowledge between tasks. Many different models have been proposed where subtasks are a priori known like Dietterich (1998) which proposes the MAXQ method. The concept of option has been introduced by Sutton et al. (1999). In this architecture, each option consists of an initiation set, its own policy (over primitive actions or other options), and a termination function which defines the probability of ending the option given a certain state.", "startOffset": 59, "endOffset": 485}, {"referenceID": 4, "context": "Hierarchical Reinforcement Learning (Dayan & Hinton, 1993; Dietterich, 1998; Parr & Russell, 1998) has been the surge of many different works during the last decade because it is considered as one solution to solve long-range planning tasks and allows to transfer knowledge between tasks. Many different models have been proposed where subtasks are a priori known like Dietterich (1998) which proposes the MAXQ method. The concept of option has been introduced by Sutton et al. (1999). In this architecture, each option consists of an initiation set, its own policy (over primitive actions or other options), and a termination function which defines the probability of ending the option given a certain state. This concept of options is at the core of many recent articles, for example in Kulkarni et al. (2016), the Deep Q-Learning framework is extended to integrate hierarchical value functions using intrinsic motivation to learn the option policies.", "startOffset": 59, "endOffset": 812}, {"referenceID": 4, "context": "Hierarchical Reinforcement Learning (Dayan & Hinton, 1993; Dietterich, 1998; Parr & Russell, 1998) has been the surge of many different works during the last decade because it is considered as one solution to solve long-range planning tasks and allows to transfer knowledge between tasks. Many different models have been proposed where subtasks are a priori known like Dietterich (1998) which proposes the MAXQ method. The concept of option has been introduced by Sutton et al. (1999). In this architecture, each option consists of an initiation set, its own policy (over primitive actions or other options), and a termination function which defines the probability of ending the option given a certain state. This concept of options is at the core of many recent articles, for example in Kulkarni et al. (2016), the Deep Q-Learning framework is extended to integrate hierarchical value functions using intrinsic motivation to learn the option policies. But in these different models, the options have to be manually chosen a priori and are not discovered during the learning process. Still in the option framework, Daniel et al. learns options (both policies and termination probabilities) without supervision using the Expectation Maximization algorithm. More recently, Bacon & Precup (2015) does the same with an architecture close to an actor-critic algorithm where options are discrete.", "startOffset": 59, "endOffset": 1294}], "year": 2016, "abstractText": "We consider the problem of learning hierarchical policies for Reinforcement Learning able to discover options, an option corresponding to a sub-policy over a set of primitive actions. Different models have been proposed during the last decade that usually rely on a predefined set of options. We specifically address the problem of automatically discovering options in decision processes. We describe a new RL learning framework called Bi-POMDP, and a new learning model called Budgeted Option Neural Network (BONN) 1 able to discover options based on a budgeted learning objective. Since Bi-POMDP are more general than POMDP, our model can also be used to discover options for classical RL tasks. The BONN model is evaluated on different classical RL problems, demonstrating both quantitative and qualitative interesting results.", "creator": "LaTeX with hyperref package"}}}