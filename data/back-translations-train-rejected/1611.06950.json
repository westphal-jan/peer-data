{"id": "1611.06950", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Nov-2016", "title": "Statistical Learning for OCR Text Correction", "abstract": "The accuracy of Optical Character Recognition (OCR) is crucial to the success of subsequent applications used in text analyzing pipeline. Recent models of OCR post-processing significantly improve the quality of OCR-generated text, but are still prone to suggest correction candidates from limited observations while insufficiently accounting for the characteristics of OCR errors. In this paper, we show how to enlarge candidate suggestion space by using external corpus and integrating OCR-specific features in a regression approach to correct OCR-generated errors. The evaluation results show that our model can correct 61.5% of the OCR-errors (considering the top 1 suggestion) and 71.5% of the OCR-errors (considering the top 3 suggestions), for cases where the theoretical correction upper-bound is 78%.", "histories": [["v1", "Mon, 21 Nov 2016 19:00:32 GMT  (440kb)", "http://arxiv.org/abs/1611.06950v1", null]], "reviews": [], "SUBJECTS": "cs.CV cs.CL cs.LG", "authors": ["jie mei", "aminul islam", "yajing wu", "abidalrahman moh'd", "evangelos e milios"], "accepted": false, "id": "1611.06950"}, "pdf": {"name": "1611.06950.pdf", "metadata": {"source": "CRF", "title": "Statistical Learning for OCR Text Correction", "authors": ["Jie Mei", "Aminul Islam", "Yajing Wu", "Abidalrahman Moh\u2019d", "Evangelos E. Milios"], "emails": ["eem}@cs.dal.ca", "aminul@louisiana.edu"], "sections": [{"heading": null, "text": "ar Xiv: 161 1,06 950v 1 [cs.C V] 21 Nov 2"}, {"heading": "1 Introduction", "text": "In fact, the fact is that most of them will be able to move to a different world in which they are able than to another world in which they are able to move, in which they are able to move."}, {"heading": "2 Related Works", "text": "The literature of OCR research shows a rich family of models for correcting OCR-generated errors."}, {"heading": "3 Characteristics of OCR Errors", "text": "In practice, the word error rate of OCR engines is in the range of 7-16% (Santos et al., 1992; Jones et al., 1991), which is significantly higher than the 1.5-2.5% for handwriting (Wing and Baddeley, 1980; Mitton, 1987) and the 0.2-0.05% for editorial news delivery (Pollock and Zamora, 1984; Church and Gale, 1991). OCR-generated errors tend to have some distinct features that require techniques other than spell correction: Complex non-standard editing The human-generated spelling errors are character-level edits that can be categorized into one of the following four standard types: insertion, deletion, substitution, and transposition. The majority of spell-correction errors, about 80%, is a single editing of the intended word (Damerau, 1964) and tend to lie within a length difference (Kuk2b)."}, {"heading": "4 Proposed Model", "text": "In this section we describe in detail the processing steps of the proposed model. We use external resources during the proofreading process, including lexicographs and a word n-gram corpus. A lexicon 1 is a list of unique words, and a word n-gram corpus refers to a list of n-grams (i.e. n consecutive words) with observed frequency. Some examples of a word n-gram corpus are Google Book n-gram (Michel et al., 2011) and Google Web 1T 5-gram corpus (Brants and Franz, 2006). To note, we note English strings with text font (e.g. wc, s), vectors with bold lowercase letters (e.g. x), sentences with italics (e.g. C, E), scalars with lowercase English or Greek characters (e.g. y, \u03b1) and functions followed by parentheses (e.g. dist (BO), Score (disciplined).)"}, {"heading": "4.1 Error Detection", "text": "Since a correct word does not proceed to further correction steps, we want to set a weak recognition restriction to filter only very self-conscious words. We rely on the n-gram frequency to determine the correctness of a word. A word is recognized as an error if one of the following conditions is not met. \u2022 If we consider an ordinary word to be less likely to be an error word, the 1-gram frequency of a word should be greater than a frequency threshold. The frequency threshold varies with different word lengths. 1The term \"lexicon\" is usually used interchangeably with \"dictionary\" and \"word list\" in the literature. \u2022 A word is probably correct if that word occurs with its context in other places. We use a sliding window to construct n-gram contexts for a word. The frequency of a context in the n-gram corpus should be greater than a frequency threshold."}, {"heading": "4.2 Candidate Search", "text": "For each error, we select a candidate that contains all the words of the vocabulary within a limited number of character changes. To be precise, let's choose the symbol set, L-\u03a3 is a language dictionary. The candidate that is set for a detected error is: {wc-wc-wc-wc-L, dist (wc, we) \u2264 \u03b4}, (1) where dist (\u043a) is the minimum edit distance and \u03b4 is a distance threshold. As transmission errors are frequent in human-generated text, but rarely occur in OCR-generated text, we use Levenshtein removal (Levenshtein, 1966), which uses a simpler operation group without transposition."}, {"heading": "4.3 Feature Scoring", "text": "In this section we will discuss the contribution of the characteristics in candidate estimation and describe the scoring measures used in our model. (Levenshtein edit distance is a basic method to quantify the difference between two strings in spell correction.) We use Levenshtein edit distance (s1, s2) is the minimum number of editing operations required to transform from s1 to s2 (wc, we). (2) We use Levenshtein edit distance for the same reason as described above. The score function is as follows: Score (wc, we) = 1 \u2212 dist (wc, we) 1 (2) String similarity of the longest common subsequence (Allison and Dix, 1986) (LCS) is an alternative approach as edit distance in concordance of similar strings."}, {"heading": "4.4 Candidate Ranking", "text": "In order to train a regressor for correction, we label the applicant characteristics with 1 if a candidate is the intended correction, or 0 otherwise. Training data contains candidates with different errors, and there are more candidates with the designation 0 than 1. In order to deal with the unbalanced nature of the candidates, we weight the samples when calculating the training loss (D) = \u2211 e-E-E-CF ewc \u00b7 loss (xc, yc). (11) We count the number of samples with the designation 1 or 0. Then we use the ratio to weight for samples with the designation 1 and 1 for samples with the designation 0.Experimentally, we apply an AdaBoost.R2 (Freund und Schapire, 1997) model on decision trees with linear loss function."}, {"heading": "5 Evaluation", "text": "To better describe the error example, we use < wt \u2192 we > to represent the intended word wt, which is recognized as the error word that we are."}, {"heading": "5.1 Experimental Dataset", "text": "The OCR text was generated from the book \"Birds of Great Britain and Ireland\" (Butler et al., 1907) and made available to the public by the Biodiversity Heritage Library (BHL) for Europe2. The soil truth text text2http: / / www.biodiversitylibrary.org / item / 35947 # page / 13 / mode / 1upis based on an improved OCR output3 and adjusted manually to match with the original content of the whole book.This source image data of the book contains 460 page-separate files, where the main content is included in 211 pages. This book combines various fonts and layouts in the main text, resulting in erroneous OCR results. There are 2698 mismatched words between the soil truth text and the BHL digital OCR text, which are used e.g. as a basic truth error. The basic text contains 892 non-complex OCR results (OCR punctuation), which have less complex errors than the data rate (OCR punctuation)."}, {"heading": "5.2 Evaluation Setup", "text": "Walker and Amsler (2014) claim that the lexicon from a published dictionary has limited coverage on newswire vocabulary, and vice versa. Thus, we construct a linguistic lexicon with unigrams in the Google Web 1T n-gram corpus4. This corpus contains the frequency of unigrams (single words) to five grams, extracted from approximately 3http: / / www.bhle.eu / en / results-of-the-collaboration-ofbhl-europe-and-impact4https: / / catalog.ldc.upenn.edu / LDC2006T13mately 1 trillion word tokens extracted from publicly available web pages. Its unigram corpus is filtered with frequency no less than 200. We use five grams in Google Web 1T corpus for accurate and relaxed context matching.For lexicon existence feature, we use three lexicons to build two instances: (1) we have goophanger, not the digital, and vice versa)."}, {"heading": "5.3 Detection Evaluation", "text": "In all troubleshooting techniques, an undetected error will not enter the correction phase. We report on the error-detection confusion matrix in Table 2. The proposed model achieves 91.07% detection error across all error types. However, there are a significant number of errors that are correct words but are recognized as errors. When we use this type of error for training or testing, we use the word itself as the intended word for each error. Correction results for all error types are given in Section 5.4. For tokenization of the loud text, any tokenization approach is inevitably involved in the common word boundary problem (Kukich, 1992b), the correct boundary of errors is not properly identified, in both man-made (Kukich, 1992a) and OCR-generated text (Jones et al, 1991)."}, {"heading": "5.4 Correction Evaluation", "text": "We perform the following steps to create a training data set: First, we construct a candidate set for each error that contains the ten best candidates evaluated by each trait; then we select a subset of errors whose intended word is present in the candidate set; finally, we randomly select 80% errors and use their candidate sets for training; we train several AdaBoost regressors with different settings; and we apply 10-fold cross-validation to select the best setting for evaluating the remaining errors; we report the correction results for various error categories in Table 4. P @ n stands for precision in the top-n candidate suggestions, which calculates the ratio of the presence of intended words among the top-n candidates; the proposed model ranks the candidates according to a regression model and shows that more than 61.5% of the errors can be corrected; for 25.9% of the uncorrected errors, our model could provide the top three corrections."}, {"heading": "6 Discussion", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6.1 Selected Features", "text": "First, we examine how well the scoring functions could rank the intended words upwards without the regressor predicting them. For each detected error, we construct a candidate set that contains the 10 best candidates rated by each attribute, and check whether that candidate set contains a correction. Note that the scope of the candidate search is limited by the number of processing distances \u03b4 (by default in Eq.1), i.e. the intended words cannot be found for us if distlev (wt, we) > \u03b4. The results will be in Table 5. The model could locate the largest part of the correction among the top candidates with the sequence of all applied attributes. We observe that performance varies drastically for limited and unlimited errors, presumably because the attribute value for unlimited errors is inaccurate (e.g. the split part of a splitting error is counted as the context word in the context search). To get a better differentiation based on the intuitive strength of each of the attributes."}, {"heading": "6.2 Regression Model Selection", "text": "The same training and test data set as described in Section 5.4 is used for all models. As the upper limit of the correction rate within three processing intervals is 78% (in Table 5), all regressors achieve good results. As can be seen, ensemble methods such as Random Forest and AdaBoost are more robust than others when it comes to proposing suitable candidates."}, {"heading": "7 Conclusion", "text": "We introduce a statistical learning model for correcting OCR-generated errors. By integrating various features into a regression process, our model is able to select and classify candidates that resemble the error, are suitable for the domain and match the context. Evaluation results show that our model can correct 61.5% of errors and could provide a correction in the first three suggestions for 25.9% of uncorrected errors. That is, by proposing three candidates for each error, our model can correct 71.5% of errors within a theoretical correction ceiling of 78%."}], "references": [], "referenceMentions": [], "year": 2016, "abstractText": "The accuracy of Optical Character Recognition (OCR) is crucial to the success of subsequent applications used in text analyzing pipeline. Recent models of OCR post-processing significantly improve the quality of OCR-generated text, but are still prone to suggest correction candidates from limited observations while insufficiently accounting for the characteristics of OCR errors. In this paper, we show how to enlarge candidate suggestion space by using external corpus and integrating OCR-specific features in a regression approach to correct OCR-generated errors. The evaluation results show that our model can correct 61.5% of the OCR-errors (considering the top 1 suggestion) and 71.5% of the OCR-errors (considering the top 3 suggestions), for cases where the theoretical correction upper-bound is 78%.", "creator": "dvips(k) 5.991 Copyright 2011 Radical Eye Software"}}}