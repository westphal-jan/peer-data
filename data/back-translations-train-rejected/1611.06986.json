{"id": "1611.06986", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Nov-2016", "title": "Robust end-to-end deep audiovisual speech recognition", "abstract": "Speech is one of the most effective ways of communication among humans. Even though audio is the most common way of transmitting speech, very important information can be found in other modalities, such as vision. Vision is particularly useful when the acoustic signal is corrupted. Multi-modal speech recognition however has not yet found wide-spread use, mostly because the temporal alignment and fusion of the different information sources is challenging.", "histories": [["v1", "Mon, 21 Nov 2016 20:08:51 GMT  (7229kb)", "http://arxiv.org/abs/1611.06986v1", null]], "reviews": [], "SUBJECTS": "cs.CL cs.LG cs.SD", "authors": ["ramon sanabria", "florian metze", "fernando de la torre"], "accepted": false, "id": "1611.06986"}, "pdf": {"name": "1611.06986.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Ramon Sanabria", "Florian Metze", "Fernando De La Torre"], "emails": ["ramons@andrew.cmu.edu", "fmetze@andrew.cmu.edu", "ftorre@andrew.cmu.edu"], "sections": [{"heading": null, "text": "ar Xiv: 161 1.06 986v 1 [cs.C L] 21 Nov 2This paper introduces a continuous audiovisual speech recognition mechanism (AVSR) based on recursive neural networks (RNN) with a connectionist time classification (CTC) [1]. CTC generates sparse \"peak\" output activations and we analyze the differences in alignment of output targets (phonemes or visems) between pure audio, video and audiovisual feature representations. We present the first such experiments with the large vocabulary of the IBM ViaVoice database that surpass previously published approaches to telephone accuracy under clean and loud conditions. Index terms - audiovisual speech recognition, recursive neural networks, connectionist time classification"}, {"heading": "1. INTRODUCTION", "text": "In fact, most of them are people who are able to survive on their own, without being able to survive on their own."}, {"heading": "2. TECHNICAL BACKGROUND", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1. Architecture of AVSR systems", "text": "Traditionally, HMMs and Gaussian Mixture Models (GMMs) are used as a central learning structure for AVSR systems. HMMs normalize the timeline of input sequences and GMMs model the emission probability of each individual state of HMM. Traditional AVSR uses two methods to merge the two modalities: First, the early combination (feature fusion) of both feature vectors can be applied [10, 11]. In some cases, algorithms such as Principal Component Analysis (PCA) or Linear Discrimination Analysis (LDA) are used to reduce the dimensionality of these representations, which can lead to frame synchronization problems [10, 12, 11]. Second, a score combination (late fusion) is implemented to avoid such problems and even allows asynchronous Discrimination Analysis (LDA) in the state sequences of the two data streams."}, {"heading": "2.2. Audio and Video Feature Representation", "text": "Several phonetic studies have attempted to understand which are the most relevant features that can be extracted from the face to perform audiovisual speech recognition. According to [12], the position of the lips is a significant source of information in visual speech recognition. In addition to the position of the lips, researchers claim that the visibility of the teeth facilitates the process of guessing the sound produced. [16, 17, 18] conduct experiments showing that the entire face provides information about language. Traditionally, researchers use different processing and feature extraction methods to represent the features explained above, all based on the extraction of interest regions (ROIs) of each image in which the mouth and other parts of the face (e.g. jaw) are located. Various techniques are used to parameterise the ROIs, such as the use of the gray value of each pixel, the extraction of the value of each pixel between the frames or the statistical use of each part of the face."}, {"heading": "3. ARCHITECTURE OF OUR SYSTEM", "text": "We use the Eesen framework [20]. The acoustic model (AM) is composed of multiple stacked LSTM networks = vignettes and uses CTC as a loss function. This set-up allows our system to automatically align the sequence of vector representations and phoneme sequences. It is important to note that the system prints the additional CTC symbol \"blank\" most of the time. Instead of the HMM, a series of three weighted finite state transducers (WFSTs) can be used to model the sequence of a symbol and blank states that make up a token (phoneme or viseme), then the words and speech model (LM) during decoding. In our pipeline, four layers of RNNs are connected to build our AM. To provide the ability to learn more complex time sequences, we use bidirectional LM units [6] for our NNN."}, {"heading": "4. DATA AND FEATURES", "text": "The data set consists of 17111 utterances spoken by 26 different speakers looking directly at the camera with an estimated signal-to-noise ratio of 19.5 dB in the audio channel. However, the data was initially divided into 17111 utterances by 261 speakers for training (about 34.9 h) and 1893 by 26 speakers utterance (4.6 hours) for testing. However, we could only use 15,963 utterances for training and 1840 utterances for testing (see Section 2.2). We perform data augmentation that adds white noise to 10 different levels of the original audio signal that actually have an initial 19.5 dB SNR (office noise), generating different SNR (40dB to 20dB)."}, {"heading": "5. EXPERIMENTS", "text": "We conducted a pure audio recognition experiment using FBank + pitch coefficients and a domain internal language model as in [8] and achieved a WER of 11.8% under clean conditions using the entire database. In the following experiments, a subset of the training and test set was used, as explained in Figure 4, some data had to be removed. In order to reduce the language model distortion of this structure in the ViaVoice domain, we opted for a more general n-gram language model based on TED talks [23], which we reduced to the required vocabulary and used in the following experiments."}, {"heading": "5.1. Audio Results", "text": "As you can see in Table 1, FBank + Pitch leads to better results. This feature is used in the following multimodal experiment. Interestingly, relatively small differences in telephone accuracy lead to greater differences in test word error rate. Figure 2 shows the telephone error rate, Figure 3 the word error rate for different states of interference during the test, systems trained only with clean data, and all states of interference occurring in the test data (multi-conditioning training). \"Clean\" data is very consistent (the recording condition is identical for all speakers), so in this case multi-conditioning training does not improve over the baseline. These results are calculated using the reduced data set explained in Section 4. Experiments, however, show that with the characteristics of FBank Pitch we achieve 11.7% WHO with the full data set."}, {"heading": "5.2. Video Results", "text": "As you can see in Table 2, different combinations of visual characteristic representations were tested using visemen as target units. SIFTdescriptors perform particularly well."}, {"heading": "5.3. Audiovisual Results", "text": "Figures 2 and 3 show the advantages of training a model using data augmentation techniques and the benefits of training a multimodal system with audio and video functions. \"Full fusion\" (audio + landmarks + SIFT) models do not perform better with higher SNR than Audio + Landmarks models with higher SNR because the high dimensionality of the SIFT feature dominates the audio features and makes them less useful. We are currently experimenting with other dimensionality reduction techniques to solve this problem. Figure 4 shows the peaks CTC uses to label the units in each model after they have been caused by the codec and other factors, in the purely audiovisual, purely videovisual and audiovisual case. Several conclusions can be drawn from this figure. First, the video signal always precedes the audio signal. This finding supports the coarticulation [3] and the predictive coarticulation of the phone prior to the coarticulation of the [5] natural coarticulation of the mouth."}, {"heading": "6. CONCLUSIONS", "text": "In this paper, we demonstrated that end-to-end deep learning can be successfully applied to the problem of audiovisual (multimodal) speech recognition. Using the CTC loss function and early integration (feature fusion), our system achieves the lowest published word error rate in the large IBM ViaVoice database. We show that multi-conditioning training can be used to improve results on loud data, and that audiovisual fusion is expected to improve results under all conditions. Interestingly, the multimodal setting allows us to think about the inherent importance of the \"peaky\" output structure of CTC models and to examine how its location matches our intuition about the language production process."}, {"heading": "7. REFERENCES", "text": "[1] Alex Graves, Santiago Ferna \u0301 ndez, Faustino Gomez, and Juergen Schmidhuber 2001, \"Connectionist temporal classification: labeling unsegmented sequence data with recurrent neural networks,\" in Proceedings of the 23rd International Conference on Machine learning. [3] Raymond D Kent and Fred D Minifie, \"Coarticulation in recent speech production models,\" Journal of Phonetics, vol. 5, pp. 115-133, 1977. [4] Sarah Taylor, Barry-John Theobald, and Iain Matthews, \"The effect of speaking rate on audio and visual speech,\" in 2014 IEEomi International Conference on Acoustics, Speech and Signal Processing (ICASSP)."}], "references": [{"title": "Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks", "author": ["Alex Graves", "Santiago Fern\u00e1ndez", "Faustino Gomez", "J\u00fcrgen Schmidhuber"], "venue": "Proceedings of the 23rd international conference on Machine learning. ACM, 2006, pp. 369\u2013376.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2006}, {"title": "Hearing lips and seeing voices", "author": ["Harry McGurk", "John MacDonald"], "venue": "Nature, vol. 264, pp. 746\u2013748, 1976.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1976}, {"title": "Coarticulation in recent speech production models", "author": ["Raymond D Kent", "Fred D Minifie"], "venue": "Journal of Phonetics, vol. 5, no. 2, pp. 115\u2013133, 1977.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1977}, {"title": "The effect of speaking rate on audio and visual speech", "author": ["Sarah Taylor", "Barry-John Theobald", "Iain Matthews"], "venue": "2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2014, pp. 3037\u20133041.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2014}, {"title": "Temporal patterns of coarticulation: Lip rounding", "author": ["Fredericka Bell-Berti", "Katherine S Harris"], "venue": "The Journal of the Acoustical Society of America, vol. 71, no. 2, pp. 449\u2013454, 1982.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1982}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation, vol. 9, no. 8, pp. 1735\u20131780, 1997.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1997}, {"title": "Phoneme-to-viseme mapping for visual speech recognition", "author": ["Luca Cappelletta", "Naomi Harte"], "venue": "ICPRAM (2), 2012, pp. 322\u2013329.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2012}, {"title": "Large-vocabulary audio-visual speech recognition by machines and humans", "author": ["Gerasimos Potamianos", "Chalapathy Neti", "Giridharan Iyengar", "Eric Helmuth"], "venue": "INTERSPEECH. Citeseer, 2001, pp. 1027\u20131030.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2001}, {"title": "Deep multimodal learning for audio-visual speech recognition", "author": ["Youssef Mroueh", "Etienne Marcheret", "Vaibhava Goel"], "venue": "2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2015, pp. 2130\u20132134.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2015}, {"title": "Large vocabulary audio-visual speech recognition using the janus speech recognition toolkit", "author": ["Jan Kratt", "Florian Metze", "Rainer Stiefelhagen", "Alex Waibel"], "venue": "Joint Pattern Recognition Symposium. Springer, 2004, pp. 488\u2013495.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2004}, {"title": "Audio visual speech recognition", "author": ["Chalapathy Neti", "Gerasimos Potamianos", "Juergen Luettin", "Iain Matthews", "Herve Glotin", "Dimitra Vergyri", "June Sison", "Azad Mashari"], "venue": "Tech. Rep., IDIAP, 2000.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2000}, {"title": "Features for audio-visual speech recognition", "author": ["Iain Matthews"], "venue": "Ph.D. thesis, Citeseer,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1998}, {"title": "Asynchronous stream modeling for large vocabulary audiovisual speech recognition", "author": ["Juergen Luettin", "Gerasimos Potamianos", "Chalapathy Neti"], "venue": "Acoustics, Speech, and Signal Processing, 2001. Proceedings.(ICASSP\u201901). 2001 IEEE International Conference on. IEEE, 2001, vol. 1, pp. 169\u2013172.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2001}, {"title": "Multimodal deep learning", "author": ["Jiquan Ngiam", "Aditya Khosla", "Mingyu Kim", "Juhan Nam", "Honglak Lee", "Andrew Y Ng"], "venue": "Proceedings of the 28th international conference on machine learning (ICML-11), 2011, pp. 689\u2013696.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2011}, {"title": "Temporal multimodal learning in audiovisual speech recognition", "author": ["Di Hu", "Xuelong Li"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2016, pp. 3574\u20133582.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2016}, {"title": "The moving face during speech communication", "author": ["Eric Vatikiotis-Bateson"], "venue": "Hearing by eye II: Advances in the psychology of speechreading and auditory-visual speech, vol. 2, pp. 123, 1998.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1998}, {"title": "The dynamics of audiovisual behavior in speech", "author": ["Eric Vatikiotis-Bateson", "Kevin G Munhall", "Makoto Hirayama", "Y Victor Lee", "Demetri Terzopoulos"], "venue": "Speechreading by humans and machines, pp. 221\u2013232. Springer, 1996.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1996}, {"title": "Characterizing audiovisual information during speech", "author": ["Eric Vatikiotis-Bateson", "Kevin G Munhall", "Y Kasahara", "Frederique Garcia", "Hani Yehia"], "venue": "ICSLP, 1996.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1996}, {"title": "Audio-visual speech recognition using deep learning", "author": ["Kuniaki Noda", "Yuki Yamaguchi", "Kazuhiro Nakadai", "Hiroshi G Okuno", "Tetsuya Ogata"], "venue": "Applied Intelligence, vol. 42, no. 4, pp. 722\u2013737, 2015.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2015}, {"title": "Eesen: End-to-end speech recognition using deep rnn models and wfst-based decoding", "author": ["Yajie Miao", "Mohammad Gowayyed", "Florian Metze"], "venue": "2015 IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU). IEEE, 2015, pp. 167\u2013174.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2015}, {"title": "Intraface", "author": ["Fernando De la Torre", "Wen-Sheng Chu", "Xuehan Xiong", "Francisco Vicente", "Xiaoyu Ding", "Jeffrey Cohn"], "venue": "Automatic Face and Gesture Recognition (FG), 2015 11th IEEE International Conference and Workshops on. IEEE, 2015, vol. 1, pp. 1\u20138.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2015}, {"title": "Distinctive image features from scaleinvariant keypoints", "author": ["David G Lowe"], "venue": "International journal of computer vision, vol. 60, no. 2, pp. 91\u2013110, 2004.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2004}, {"title": "Scaling recurrent neural network language models", "author": ["Will Williams", "Niranjani Prasad", "David Mrva", "Tom Ash", "Tony Robinson"], "venue": "CoRR, vol. abs/1502.00512, 2015.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2015}, {"title": "Lipreading (speechreading)", "author": ["J Jeffers", "M Barley"], "venue": "Charles C. Thomas, Springfield, IL, p. b10, 1971.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 1971}], "referenceMentions": [{"referenceID": 0, "context": "This paper presents an end-to-end audiovisual speech recognizer (AVSR), based on recurrent neural networks (RNN) with a connectionist temporal classification (CTC) [1] loss function.", "startOffset": 164, "endOffset": 167}, {"referenceID": 1, "context": "It has been demonstrated by [2] that humans also tend to put their attention to other information channels (i.", "startOffset": 28, "endOffset": 31}, {"referenceID": 2, "context": "Specifically, it is shown by different linguistic studies such as [3] that the mouth shape towards an articulatory target modify the following phone.", "startOffset": 66, "endOffset": 69}, {"referenceID": 3, "context": "This effect is accentuated when the speaking rate of the speaker is high [4].", "startOffset": 73, "endOffset": 76}, {"referenceID": 4, "context": "Some practical studies such as [5] state that the coarticulation is speaker- and phoneme-dependent.", "startOffset": 31, "endOffset": 34}, {"referenceID": 5, "context": "We present an AVSR solution that does not require an HMM, but rather uses several layers of bi-directional long short-term memory (LSTM) [6] units as building blocks, followed by a CTC loss function for the output layer.", "startOffset": 137, "endOffset": 140}, {"referenceID": 6, "context": "In an audio-visual setting, these may correspond to the different times at which phonemes and their corresponding visemes (we follow [7] for the mapping) are observed, without requiring any manual labeling or input.", "startOffset": 133, "endOffset": 136}, {"referenceID": 7, "context": "This paper thus makes two main contributions: first, we demonstrate that CTC-based acoustic models can achieve state-of-the-art performance in audio-visual speech recognition tasks, as our system achieve comparable results with a traditional pre-Deep Learning baseline ( [8] report 11% Word Error Rate on the ViaVoice database) and outperforms recent cross-entropy trained DNN baseline [9] in terms of phoneme error rate.", "startOffset": 271, "endOffset": 274}, {"referenceID": 8, "context": "This paper thus makes two main contributions: first, we demonstrate that CTC-based acoustic models can achieve state-of-the-art performance in audio-visual speech recognition tasks, as our system achieve comparable results with a traditional pre-Deep Learning baseline ( [8] report 11% Word Error Rate on the ViaVoice database) and outperforms recent cross-entropy trained DNN baseline [9] in terms of phoneme error rate.", "startOffset": 386, "endOffset": 389}, {"referenceID": 9, "context": "Two ways of fusing both modalities are used in traditional AVSR: first, early combination (feature fusion) of both feature vectors can be applied [10, 11].", "startOffset": 146, "endOffset": 154}, {"referenceID": 10, "context": "Two ways of fusing both modalities are used in traditional AVSR: first, early combination (feature fusion) of both feature vectors can be applied [10, 11].", "startOffset": 146, "endOffset": 154}, {"referenceID": 9, "context": "This approach may lead to frame synchronization problems [10, 12, 11].", "startOffset": 57, "endOffset": 69}, {"referenceID": 11, "context": "This approach may lead to frame synchronization problems [10, 12, 11].", "startOffset": 57, "endOffset": 69}, {"referenceID": 10, "context": "This approach may lead to frame synchronization problems [10, 12, 11].", "startOffset": 57, "endOffset": 69}, {"referenceID": 12, "context": "In [13] for example both modalities are analyzed separately and later on the results of both are fused using a bias.", "startOffset": 3, "endOffset": 7}, {"referenceID": 13, "context": "In [14, 15, 9] present different recent deep-learning approaches to solve the AVSR problem.", "startOffset": 3, "endOffset": 14}, {"referenceID": 14, "context": "In [14, 15, 9] present different recent deep-learning approaches to solve the AVSR problem.", "startOffset": 3, "endOffset": 14}, {"referenceID": 8, "context": "In [14, 15, 9] present different recent deep-learning approaches to solve the AVSR problem.", "startOffset": 3, "endOffset": 14}, {"referenceID": 13, "context": "In [14] and [9], a joined (audio and video) representations using Deep Neural Networks (DNNs) is learned to perform word and phone recognition respectively.", "startOffset": 3, "endOffset": 7}, {"referenceID": 8, "context": "In [14] and [9], a joined (audio and video) representations using Deep Neural Networks (DNNs) is learned to perform word and phone recognition respectively.", "startOffset": 12, "endOffset": 15}, {"referenceID": 14, "context": "More recently, [15] present a Recurrent Temporal Multimodal Restricted Boltzmann Machine (RTMRB), which takes into consideration long-term dependencies and outperforms other non-temporal solutions.", "startOffset": 15, "endOffset": 19}, {"referenceID": 11, "context": "According to [12], lip position is a considerable source of information when performing visual-only speech recognition.", "startOffset": 13, "endOffset": 17}, {"referenceID": 11, "context": "In addition to the position of the lips, [12] state that teeth visibility eases the process of guessing the sound that was produced.", "startOffset": 41, "endOffset": 45}, {"referenceID": 15, "context": "Moreover, [16, 17, 18] conduct experiments where it is shown that the entire face provides information about speech.", "startOffset": 10, "endOffset": 22}, {"referenceID": 16, "context": "Moreover, [16, 17, 18] conduct experiments where it is shown that the entire face provides information about speech.", "startOffset": 10, "endOffset": 22}, {"referenceID": 17, "context": "Moreover, [16, 17, 18] conduct experiments where it is shown that the entire face provides information about speech.", "startOffset": 10, "endOffset": 22}, {"referenceID": 18, "context": "In the field of deep learning, [19] proposed a MSHMM infrastructure, which uses features extracted from a Convolutial Neural Network (CNN).", "startOffset": 31, "endOffset": 35}, {"referenceID": 13, "context": "1, [14, 9, 15] learn a joint feature representation using different DNNs approaches.", "startOffset": 3, "endOffset": 14}, {"referenceID": 8, "context": "1, [14, 9, 15] learn a joint feature representation using different DNNs approaches.", "startOffset": 3, "endOffset": 14}, {"referenceID": 14, "context": "1, [14, 9, 15] learn a joint feature representation using different DNNs approaches.", "startOffset": 3, "endOffset": 14}, {"referenceID": 19, "context": "We use the Eesen framework [20].", "startOffset": 27, "endOffset": 31}, {"referenceID": 5, "context": "To provide the ability of learning more complex time sequences we use bidirectional LSTM units [6] for our RNN.", "startOffset": 95, "endOffset": 98}, {"referenceID": 10, "context": "The IBM ViaVoice [11] data set is used to test and train the proposed pipelines.", "startOffset": 17, "endOffset": 21}, {"referenceID": 20, "context": "18 coordinate points that define the inner and outer profile of the mouth shape are extracted using IntraFace [21].", "startOffset": 110, "endOffset": 114}, {"referenceID": 21, "context": "A richer representation of the visual modality is achieved describing the mouth landmark points using a scale invariant local description (SIFT) [22].", "startOffset": 145, "endOffset": 149}, {"referenceID": 7, "context": "We perform a baseline audio-only recognition experiments with FBank + pitch coefficients and an in-domain language model as used in [8], and achieved a WER of 11.", "startOffset": 132, "endOffset": 135}, {"referenceID": 22, "context": "Also, in order to reduce the language model bias of this setup in the ViaVoice domain, we decided to switch to a more general n-gram language model based on TED talks [23], which we reduce to the required vocabulary and we use in the following experiments.", "startOffset": 167, "endOffset": 171}, {"referenceID": 23, "context": "Summary of the results obtained with the different visual feature representations, mapping phonemes to 12 visemes according to [24].", "startOffset": 127, "endOffset": 131}, {"referenceID": 2, "context": "This finding supports the coarticualation [3] and anticipatory coarticulation [5] studies of natural speech production, where is stated that speaker changes the mouth shape before pronounce the following phone.", "startOffset": 42, "endOffset": 45}, {"referenceID": 4, "context": "This finding supports the coarticualation [3] and anticipatory coarticulation [5] studies of natural speech production, where is stated that speaker changes the mouth shape before pronounce the following phone.", "startOffset": 78, "endOffset": 81}, {"referenceID": 2, "context": "This shows that the \u201cpeaky\u201d structure of CTC is well suited also to multi-modal fusion, and that more detailed studies should be performed in order to investigate the, presumably,speaker- and phoneme-dependent nature of coarticulation [3, 5].", "startOffset": 235, "endOffset": 241}, {"referenceID": 4, "context": "This shows that the \u201cpeaky\u201d structure of CTC is well suited also to multi-modal fusion, and that more detailed studies should be performed in order to investigate the, presumably,speaker- and phoneme-dependent nature of coarticulation [3, 5].", "startOffset": 235, "endOffset": 241}], "year": 2016, "abstractText": "Speech is one of the most effective ways of communication among humans. Even though audio is the most common way of transmitting speech, very important information can be found in other modalities, such as vision. Vision is particularly useful when the acoustic signal is corrupted. Multi-modal speech recognition however has not yet found wide-spread use, mostly because the temporal alignment and fusion of the different information sources is challenging. This paper presents an end-to-end audiovisual speech recognizer (AVSR), based on recurrent neural networks (RNN) with a connectionist temporal classification (CTC) [1] loss function. CTC creates sparse \u201cpeaky\u201d output activations, and we analyze the differences in the alignments of output targets (phonemes or visemes) between audio-only, video-only, and audio-visual feature representations. We present the first such experiments on the large vocabulary IBM ViaVoice database, which outperform previously published approaches on phone accuracy in clean and noisy conditions.", "creator": "LaTeX with hyperref package"}}}