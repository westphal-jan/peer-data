{"id": "1611.07174", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Nov-2016", "title": "Deep Recurrent Convolutional Neural Network: Improving Performance For Speech Recognition", "abstract": "Performance of end-to-end automatic speech recognition (ASR) systems can significantly be improved by the increasing large speech corpus and deeper neural network. Given the arising problem of training speed and recent success of deep convolutional neural network in ASR, we build a novel deep recurrent convolutional network for acoustic modeling and apply deep residual learning framework to it, our experiments show that it has not only faster convergence speed but better recognition accuracy over traditional deep convolutional recurrent network. We mainly compare convergence speed of two acoustic models, which are novel deep recurrent convolutional networks and traditional deep convolutional recurrent networks. With faster convergence speed, our novel deep recurrent convolutional networks can reach the comparable performance. We further show that applying deep residual learning can boost both convergence speed and recognition accuracy of our novel recurret convolutional networks. Finally, we evaluate all our experimental networks by phoneme error rate (PER) with newly proposed bidirectional statistical language model. Our evaluation results show that our model applied with deep residual learning can reach the best PER of 17.33% with fastest convergence speed in TIMIT database.", "histories": [["v1", "Tue, 22 Nov 2016 07:36:21 GMT  (463kb,D)", "http://arxiv.org/abs/1611.07174v1", "10pages, 13figures"], ["v2", "Tue, 27 Dec 2016 04:53:56 GMT  (469kb,D)", "http://arxiv.org/abs/1611.07174v2", "11 pages, 13 figures"]], "COMMENTS": "10pages, 13figures", "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["zewang zhang", "zheng sun", "jiaqi liu", "jingwen chen", "zhao huo", "xiao zhang"], "accepted": false, "id": "1611.07174"}, "pdf": {"name": "1611.07174.pdf", "metadata": {"source": "CRF", "title": "An Experimental Comparison of Deep Neural Networks for End-to-end Speech Recognition", "authors": ["Zewang Zhang", "Zheng Sun", "Jiaqi Liu", "Jingwen Chen"], "emails": ["iao}@{mail2,mail2,mail2,mail2,mail}.sysu.edu.cn", "huozhao@cupl.edu.cn"], "sections": [{"heading": null, "text": "This is an example of the high probability of using a statistical model of maximum decryption and maximum mutual information estimation for speech recognition [1], i.e. the use of the hidden Markov model (HMM) for speech recognition had also become prevalent several decades ago. [3] Traditionally, a statistical model of maximum perception of speech is also used for speech recognition. [4] With the spring-up of deep learning, deep mutual information estimation (DNN) with HMM states, it has been shown that the traditional Gaussian mixing models are surpassed [5], [7] so many new training tricks have been proposed to improve the performance of DNNs."}, {"heading": "II. REVIEW OF NEURAL NETWORK", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. ELU Nonlinearity", "text": "The most common functions applied to the performance of a neuron are ReLU [19] depending on its input with f (x) = max (0, x). ReLU performs better than traditional nonlinear functions such as Sigmoid or Tanh, because the mean value of ReLU activations is not zero, some neurons are always dead in practice during backpropagation. Exponential linear unit (ELU) was introduced in [20], in contrast to ReLU, ELU has negative values that press the mean value of activations close to zero, i.e. ELU can reduce the gap between the normal gradient and the natural gradient and thus speed up the training.The expression of the nonlinearity of ELU is shown below in equation??. f (x) = {x if x x > 0 \u03b1 (exp (x) \u2212 1) if x 6 0 (1) Because our work is based on deep CNNs, we can use ELU as a non-linear function to obtain the network data more quickly."}, {"heading": "B. Recurrent Neural Network", "text": "A kind of special forward-facing neural network is the recursive neural network (RNN). If RNN is unfolded in time, it can be considered a DNN with many successive layers. In contrast to the forward-facing neural network, RNN is used to build time dependence on input functions with internal memory units. RNN can receive input and generate output in each layer, the general architecture of RNN is represented in Figure 1. As shown in Figure 1, a very simple RNN consists of an input layer, a hidden layer, and an output layer. The recurring hidden layer is designed to pass forward information to backward-facing time steps. We can represent the internal relationship of a general RNN in equation (?). Ot = f (yt) = f (Whh: ht \u2212 1 + Wxh: stxt + dances) if the previous weight unit is covered between the forward layer, the ELx is covered."}, {"heading": "C. Convolutional Neural Network", "text": "Compared to fully networked neural networks and RNNs, CNNs proposed in [16] have much fewer parameters, making them easier to train. CNNs are similar to ordinary neural networks, they consist of trainable weights and biases, and they can also be stacked to the depth, which was successfully used in the ImageNet competition [32]. A typical CNN architecture consists of a revolutionary layer and a pool layer, which is shown in Figure 2. In most cases, a typical revolutionary layer contains several feature cards, each of which is a type of filter with common parameters. These filters are spatial and extend over the entire depth of the input volume. The pool layer is designed for reducing dimensionality and a fully connected layer, the probability distribution of all different classes can be generated. In our experimental model, we replaced the pool layer with the entire evolutionary layer, especially if we can coat the two dimensions in 1."}, {"heading": "III. REVIEW OF CONNECTIONIST TEMPORAL CLASSIFICATION", "text": "The basic idea of CTC is to interpret the results of the network as a probability distribution over all possible phenomena. However, given this distribution, we can derive the objective function of the sequence marker. Since the objective function is differentiable, we can train it by backward propagation by time algorithm. Using the probability distributions learned by deep CNNs, we would then use a CTC loss layer to finally output the phenomenon sequence. For a given input sequence, the goal of CTC is to minimize the average distance from the output sequence. Suppose L is an alphabet of all output phonemes, the CTC network has one more unit than there is."}, {"heading": "IV. BIDIRECTIONAL N-GRAM LANGUAGE MODEL", "text": "Both statistical speech modelling and the neural network have been successfully used in speech recognition [34], [35], [36], [37]. The traditional n-gram model always assumes that the probability of the current word only depends on the probability of the previous n-1 words, so we propose a new bidirectional n-gram model that takes into account the context probability of two sides. Pf (ph) = P (phn | phn \u2212 1,..., phn \u2212 1) (13) Second, we define an n-gram that goes from left to right in a sentence and obtain the forward probability for each phoneme in the face of a previous phoneme phrase."}, {"heading": "V. EXPERIMENTAL SETUP", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. Dataset", "text": "We conduct our speech recognition experiments using a commonly used language data set: TIMIT. TIMIT consists of 630 loudspeakers with 6300 expressions of different sexes and dialects. Each audio clip is followed by phonemes and sentence transcriptions, and we take the phonemes as floor captions. The output is the probability distribution of 63 captions, including 61 non-empty phonemes, a space and blank captions for CTC. Since a typical data set for machine learning contains training sets, validation sets and test sentences, we design the division of 6300 expressions of TIMIT into 5000 training explanations, 1000 validation 5 rectified sequences n-grams correct predicted sequences dcl dclae y y y y he dclerdclerdcl? er? he dclerdcl? dcl aeFig. 5."}, {"heading": "B. Feature Selection", "text": "At the pre-processing stage, each piece of speech is analyzed using a 25 ms hamming window with a fixed overlap of 10 ms. Each feature vector of a frame is calculated using a Fourier transformation-based filter bank analysis that includes 39 log energy coefficients distributed over 13 mel frequency receiver coefficients (MFCs), along with their first and second time derivatives. In addition, all feature data is normalized so that each vector dimension has a zero medium and unit variance. As the feature matrix of each audio language differs in time length, we set each feature matrix with zeros to a maximum length. Implementation We build our deep neural network models based on the open source library Lasagne, which is a lightweight library for building and training neural networks in Theano. We are accelerating our training on the Tesla, our Punigen 80."}, {"heading": "D. Training Details", "text": "Since the Adam Optimization Method [21] is computationally efficient, requires little memory allocation, and is well suited for training deep-learning models with large data, we use the Adam Method with a learning rate of 0.00005 at the beginning of the training. Instead of setting a learning rate of 0.00001, as stated in the paper, we find that a learning rate of 0.0005 can make divergence more stable in practice. As our experimental models are very deep, we would like to issue some regulations to avoid overadjustment. Conversely, batch normalization [18] has shown better regularization performance, but it would add additional parameters and requires strong data augmentation, which we would like to avoid. Instead, we add a fail layer after the recurring layers and after the first fully connected preshift to prevent it from overlapping."}, {"heading": "E. Evaluation", "text": "Since our proposed model is end-to-end and phoneme height, we use phoneme error rate (PER) to evaluate the result. PER is calculated after the CTC network has decrypted the entire output set into a sequence of phonemes. Then, we calculate the Damerau-Levenshtein distance between our predicted sequence and the truth sequence to get the error we made. The average number of errors over the length of the phoneme sequence is just our PER. Finally, we evaluate our model using the test set."}, {"heading": "VI. ARCHITECTURE", "text": "In order to explore the convergence properties of deep revolutionary networks with relapsed networks, we have conducted experiments with different configurations. In this section, we will discuss three typical experiments we have tried, namely novel deep relapsing revolutionary networks, traditional deep revolutionary relapsing networks, and residual networks. We will compare the convergence speed of these networks with different configurations, including the number of layers, the number of parameters, the number of character maps, and the application of residual learning."}, {"heading": "A. End-to-End Training", "text": "The traditional educational approach to acoustic modeling is based on the frame-by-frame cross-entropy of predicted output and true label, which requires practical alignment between input frames and output labels. To avoid such high labor costs, our approach uses the CTC-based dynamic decoding method, which can perform supervised learning based on sequence data and avoid alignment between input data and output labels. We choose a commonly used TIMIT dataset to work on our acoustic model. TIMIT contains 6300 expressions from 630 speakers in 8 dialects, each audio includes phonemes, word transcription, and the entire sentence. We select phoneme transcription as the labels for phoneme-level training. 6"}, {"heading": "B. Deep convolutional recurrent networks", "text": "1) Details of experimental models: Since many newer ASR systems use deep CNNs for acoustic modelling in ASR, deep CNNs are used in particular for feature pre-processing, followed by RNN CTC decoding networks. Conventional deep CNNs contain folding and pooling, but we find that pooling can be replaced in practice by folding with fewer feature cards. Inspired by this, we also build four deep Convolutionary Recursive Networks consisting of deep Constitutional Layers, four recursive Layers, and two fully connected forward Layers. They differ in the number of feature cards on the Convolutionary Layer. As shown in Figure 7, the two deeply connected Convolutionary Recursive Layers \"CR1\" and \"CR2\" differ in the number of feature cards in different layers."}, {"heading": "C. Deep recurrent convolutional networks", "text": "This year, it is more than ever before in the history of the city."}, {"heading": "D. Residual networks", "text": "In fact, it is the case that most of them are in a position to go to another world, in which they go to another world, in which they go to another world, in which they find themselves to another world, in which they find themselves to another world, in which they live to another world, in which they live to another world, in which they live to another world, in which they live to another world, in which they find themselves to another world, in which they live to another world, in which they live to themselves, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they do not, in which they live, in which they do not, in which they do not, in which they do not, in which they do not, in any other world, in any other world, in any other world, in which they do not exist."}, {"heading": "VII. EVALUATION", "text": "TIMIT is a small 16 kHz speech corpus with 6300 expressions, from which the validation set of 300 expressions and the test set of 300 expressions are derived. We use 63 phonemes as output labels, including 61 non-empty labels, a Space10 label and an empty label for CTC. After the acoustic modeling, a probability distribution of 63 labels is decoded from the CTC network into final phoneme sequences. Subsequently, our proposed bidirectional hybrid n-gram language model of phonemes, estimated from the entire training set, is used to correct the final sequence. \"Since our acoustic models have phoneme level, we evaluate them according to PER on the test set. Each experiment of an architecture has been performed several times, we present the minimal validation PER and the minimal test PER for each architecture in Table III.\" According to the evaluation result, our CER-resistant network will be 20.2% lower than the traditional CRER-71% improvement. \""}, {"heading": "VIII. CONCLUSIONS", "text": "Our work presents a detailed experimental comparison of three different acoustic models in speech recognition: traditional deep Convolutionary Recursive Networks, deep Recursive Convolutionary Networks, and deep Residual Networks. Traditional applications of deep Convolutionary Networks are used for pre-processing functions, followed by recurring layers and CTC decoding layers, but in practice it takes too long for them to converge. Our proposed Recursive Convolutionary Network incorporates recursive networks as feature preprocessing, and deep CNNs are designed to represent high-grade characteristic representations. Our experiments show that our novel Recursive Convolutionary Network can converge in a shorter time compared to traditional Recursive Networks and also achieve a comparable PER in the first half. Furthermore, we try to apply the rest of the learning framework in our acoustic models."}, {"heading": "ACKNOWLEDGMENT", "text": "The author thanks Chengyou Xie and Qionghaofeng Wu for helpful discussions about automatic speech recognition."}], "references": [{"title": "A Maximum Likelihood Approach to Continuous Speech Recognition.", "author": ["Ieee", "Lalit R. Bahl Member", "F.J.F. Ieee", "R.L. Mercer"], "venue": "Pattern Analysis & Machine Intelligence IEEE Transactions on 5.2(1983),", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1983}, {"title": "Maximum mutual information estimation of hidden Markov model parameters for speech recognition.", "author": ["L Bahl"], "venue": "IEEE International Conference on Acoustics", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1986}, {"title": "Hidden Markov Models for Speech Recognition.", "author": ["Michaelson", "By S", "M. Steedman"], "venue": "Technometrics", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2010}, {"title": "An Introduction to the Application of the Theory of Probabilistic Functions of a Markov Process to Automatic Speech Recognition.", "author": ["S.E. Levinson", "L.R. Rabiner", "M.M. Sondhi"], "venue": "Bell System Technical Journal", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1983}, {"title": "Context-Dependent Pre-trained Deep Neural Networks for Large Vocabulary Speech Recognition.", "author": ["Dahl", "George E"], "venue": "IEEE Transactions on Audio Speech & Language Processing", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2012}, {"title": "Deep Neural Networks for Acoustic Modeling in Speech Recognition.", "author": ["G Hinton"], "venue": "IEEE Signal Processing Magazine", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2012}, {"title": "Deep Neural Networks for Single-Channel Multi- Talker Speech Recognition.", "author": ["Weng", "Chao"], "venue": "Audio Speech & Language Processing IEEE/ACM Transactions on 23.10(2015),", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2015}, {"title": "Towards end-to-end speech recognition with recurrent neural networks.", "author": ["A. Graves", "N. Jaitly"], "venue": "International Conference on Machine Learning", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2014}, {"title": "Online Sequence Training of Recurrent Neural Networks with Connectionist Temporal Classification.", "author": ["Hwang", "Kyuyeon", "W. Sung"], "venue": "Computer Science", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2015}, {"title": "Adam and others, \u201cDeep speech: Scaling up end-to-end speech recognition", "author": ["Hannun", "Awni", "Case", "Carl", "Casper", "Jared", "Catanzaro", "Bryan", "Diamos", "Greg", "Elsen", "Erich", "Prenger", "Ryan", "Satheesh", "Sanjeev", "Sengupta", "Shubho", "Coates"], "venue": "arXiv preprint arXiv:1412.5567,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2014}, {"title": "Speech recognition with deep recurrent neural networks.", "author": ["A. Graves", "A.R. Mohamed", "G. Hinton"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2003}, {"title": "Framewise phoneme classification with bidirectional LSTM and other neural network architectures.", "author": ["Graves", "Alex", "J. Schmidhuber"], "venue": "Neural Networks", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2005}, {"title": "Long Short-Term Memory Based Recurrent Neural Network Architectures for Large Vocabulary Speech Recognition.", "author": ["Haim Sak", "Andrew Senior", "Franoise Beaufays"], "venue": "Science", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2014}, {"title": "Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks.", "author": ["Graves", "Alex"], "venue": "International Conference on Machine Learning ACM,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2006}, {"title": "Handwritten digit recognition with a back-propagation network", "author": ["Le Cun", "B Boser", "Denker", "John S", "D Henderson", "Howard", "Richard E", "W Hubbard", "Jackel", "Lawrence D"], "venue": "Advances in neural information processing systems,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1990}, {"title": "Dropout: a simple way to prevent neural networks from overfitting.", "author": ["Srivastava", "Nitish"], "venue": "Journal of Machine Learning Research", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2014}, {"title": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift.", "author": ["Ioffe", "Sergey", "C. Szegedy"], "venue": "Computer Science", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2015}, {"title": "Rectified Linear Units Improve Restricted Boltzmann Machines Vinod Nair.", "author": ["Nair", "Vinod", "G.E. Hinton"], "venue": "International Conference on Machine Learning", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2010}, {"title": "Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs).", "author": ["Clevert", "Djork-Arn", "T. Unterthiner", "S. Hochreiter"], "venue": "Computer Science", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2015}, {"title": "Adam: A method for stochastic optimization\u201d, arXiv preprint arXiv:1412.6980", "author": ["D. Kingma", "J. Ba"], "venue": null, "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2014}, {"title": "Convolutional Neural Networks for Speech Recognition.", "author": ["Abdel-Hamid", "Ossama"], "venue": "IEEE/ACM Transactions on Audio Speech & Language Processing", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2014}, {"title": "very Deep Convolutional Neural Networks for Robust Speech Recognition.", "author": ["Qian", "Yanmin", "Philip C. Woodland"], "venue": "arXiv preprint arXiv:1610.00277", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2016}, {"title": "Advances in Very Deep Convolutional Neural Networks for LVCSR.", "author": ["Sercu", "Tom", "Vaibhava Goel"], "venue": "arXiv preprint arXiv:1604.01792", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2016}, {"title": "Applying Convolutional Neural Networks concepts to hybrid NN-HMM model for speech recognition.", "author": ["O Abdel-Hamid"], "venue": null, "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2012}, {"title": "Deep convolutional neural networks for LVCSR.", "author": ["Sainath", "T. N"], "venue": null, "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2013}, {"title": "Convolutional Neural Networks for Speech Recognition.", "author": ["Abdel-Hamid", "Ossama"], "venue": "IEEE/ACM Transactions on Audio Speech & Language Processing", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2014}, {"title": "Analysis of cnn-based speech recognition system using raw speech as input.", "author": ["Palaz", "Dimitri", "Ronan Collobert"], "venue": "Proceedings of Interspeech. No. EPFL-CONF-210029", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2015}, {"title": "Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position.", "author": ["Fukushima", "Kunihiko"], "venue": "Biological Cybernetics", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 1980}, {"title": "Improvements to deep convolutional neural networks for LVCSR.", "author": ["Sainath", "T. N"], "venue": null, "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2013}, {"title": "Deep Residual Learning for Image Recognition", "author": ["He", "Kaiming"], "venue": "Computer Science,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2015}, {"title": "ImageNet Large Scale Visual Recognition Challenge", "author": ["Russakovsky", "Olga"], "venue": "International Journal of Computer Vision,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2015}, {"title": "Microsoft COCO: Common Objects", "author": ["Lin", "Tsung Yi"], "venue": "in Context\u201d,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2015}, {"title": "A variable-length category-based n-gram language model", "author": ["T.R. Niesler", "P.C. Woodland"], "venue": "Icassp IEEE,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 1996}, {"title": "Neural Probabilistic Language Models", "author": ["Bengio", "Yoshua"], "venue": "Journal of Machine Learning Research", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2001}, {"title": "Recurrent neural network based language model\u201d, INTERSPEECH 2010, Conference of the International Speech Communication Association, Makuhari", "author": ["Mikolov", "Tomas"], "venue": null, "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2010}, {"title": "NN-grams: Unifying neural network and ngram language models for Speech", "author": ["Damavandi", "Babak"], "venue": null, "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2016}], "referenceMentions": [{"referenceID": 0, "context": "Traditionally, a statistical model of maximum likelihood decoding and maximum mutual information estimation are used for speech recognition [1], [2], thus the use of hidden Markov model (HMM) for speech recognition had also become predominant several years ago [3], [4].", "startOffset": 140, "endOffset": 143}, {"referenceID": 1, "context": "Traditionally, a statistical model of maximum likelihood decoding and maximum mutual information estimation are used for speech recognition [1], [2], thus the use of hidden Markov model (HMM) for speech recognition had also become predominant several years ago [3], [4].", "startOffset": 145, "endOffset": 148}, {"referenceID": 2, "context": "Traditionally, a statistical model of maximum likelihood decoding and maximum mutual information estimation are used for speech recognition [1], [2], thus the use of hidden Markov model (HMM) for speech recognition had also become predominant several years ago [3], [4].", "startOffset": 261, "endOffset": 264}, {"referenceID": 3, "context": "Traditionally, a statistical model of maximum likelihood decoding and maximum mutual information estimation are used for speech recognition [1], [2], thus the use of hidden Markov model (HMM) for speech recognition had also become predominant several years ago [3], [4].", "startOffset": 266, "endOffset": 269}, {"referenceID": 4, "context": "With the spring-up of deep learning, deep neural network (DNN) with HMM states has been shown to outperform the traditional Gaussian mixture models [5], [6], [7], thus many new training tricks have been proposed to improve the performance of DNNs, such as powerful non-linear activation functions, layer-wise mini-batch training, dropout and fast gradient descent method [17], [19], [20], [21].", "startOffset": 148, "endOffset": 151}, {"referenceID": 5, "context": "With the spring-up of deep learning, deep neural network (DNN) with HMM states has been shown to outperform the traditional Gaussian mixture models [5], [6], [7], thus many new training tricks have been proposed to improve the performance of DNNs, such as powerful non-linear activation functions, layer-wise mini-batch training, dropout and fast gradient descent method [17], [19], [20], [21].", "startOffset": 153, "endOffset": 156}, {"referenceID": 6, "context": "With the spring-up of deep learning, deep neural network (DNN) with HMM states has been shown to outperform the traditional Gaussian mixture models [5], [6], [7], thus many new training tricks have been proposed to improve the performance of DNNs, such as powerful non-linear activation functions, layer-wise mini-batch training, dropout and fast gradient descent method [17], [19], [20], [21].", "startOffset": 158, "endOffset": 161}, {"referenceID": 15, "context": "With the spring-up of deep learning, deep neural network (DNN) with HMM states has been shown to outperform the traditional Gaussian mixture models [5], [6], [7], thus many new training tricks have been proposed to improve the performance of DNNs, such as powerful non-linear activation functions, layer-wise mini-batch training, dropout and fast gradient descent method [17], [19], [20], [21].", "startOffset": 371, "endOffset": 375}, {"referenceID": 17, "context": "With the spring-up of deep learning, deep neural network (DNN) with HMM states has been shown to outperform the traditional Gaussian mixture models [5], [6], [7], thus many new training tricks have been proposed to improve the performance of DNNs, such as powerful non-linear activation functions, layer-wise mini-batch training, dropout and fast gradient descent method [17], [19], [20], [21].", "startOffset": 377, "endOffset": 381}, {"referenceID": 18, "context": "With the spring-up of deep learning, deep neural network (DNN) with HMM states has been shown to outperform the traditional Gaussian mixture models [5], [6], [7], thus many new training tricks have been proposed to improve the performance of DNNs, such as powerful non-linear activation functions, layer-wise mini-batch training, dropout and fast gradient descent method [17], [19], [20], [21].", "startOffset": 383, "endOffset": 387}, {"referenceID": 19, "context": "With the spring-up of deep learning, deep neural network (DNN) with HMM states has been shown to outperform the traditional Gaussian mixture models [5], [6], [7], thus many new training tricks have been proposed to improve the performance of DNNs, such as powerful non-linear activation functions, layer-wise mini-batch training, dropout and fast gradient descent method [17], [19], [20], [21].", "startOffset": 389, "endOffset": 393}, {"referenceID": 7, "context": "Meanwhile, Deep Long Short-term Memory RNNs and deep bidirectional RNNs are purposed to exploit long time memory in ASR [8], [11], [12], [13], [14].", "startOffset": 120, "endOffset": 123}, {"referenceID": 9, "context": "Meanwhile, Deep Long Short-term Memory RNNs and deep bidirectional RNNs are purposed to exploit long time memory in ASR [8], [11], [12], [13], [14].", "startOffset": 125, "endOffset": 129}, {"referenceID": 10, "context": "Meanwhile, Deep Long Short-term Memory RNNs and deep bidirectional RNNs are purposed to exploit long time memory in ASR [8], [11], [12], [13], [14].", "startOffset": 131, "endOffset": 135}, {"referenceID": 11, "context": "Meanwhile, Deep Long Short-term Memory RNNs and deep bidirectional RNNs are purposed to exploit long time memory in ASR [8], [11], [12], [13], [14].", "startOffset": 137, "endOffset": 141}, {"referenceID": 12, "context": "Meanwhile, Deep Long Short-term Memory RNNs and deep bidirectional RNNs are purposed to exploit long time memory in ASR [8], [11], [12], [13], [14].", "startOffset": 143, "endOffset": 147}, {"referenceID": 8, "context": "Besides, sequence training of RNNs with connectionist temporal classification (CTC) has shown great performance in endto-end ASR [10], [15].", "startOffset": 129, "endOffset": 133}, {"referenceID": 13, "context": "Besides, sequence training of RNNs with connectionist temporal classification (CTC) has shown great performance in endto-end ASR [10], [15].", "startOffset": 135, "endOffset": 139}, {"referenceID": 27, "context": "CNN is an older deep neural network architecture [29], and has enjoyed the great popularity as a efficient approach in character recognition.", "startOffset": 49, "endOffset": 53}, {"referenceID": 23, "context": "Some researchers proposed CNN can be used in speech recognition, and it has been proved that deep CNN has better performance over general feed-forward neural network or GMM-HMM in several speech recognition tasks [25], [26], [27].", "startOffset": 213, "endOffset": 217}, {"referenceID": 24, "context": "Some researchers proposed CNN can be used in speech recognition, and it has been proved that deep CNN has better performance over general feed-forward neural network or GMM-HMM in several speech recognition tasks [25], [26], [27].", "startOffset": 219, "endOffset": 223}, {"referenceID": 25, "context": "Some researchers proposed CNN can be used in speech recognition, and it has been proved that deep CNN has better performance over general feed-forward neural network or GMM-HMM in several speech recognition tasks [25], [26], [27].", "startOffset": 225, "endOffset": 229}, {"referenceID": 20, "context": "[22] used one convolutional layer, one pooling layer and a few full-connected layers.", "startOffset": 0, "endOffset": 4}, {"referenceID": 26, "context": "Some researcher has shown that CNNbased speech recognition which uses raw speech as input can be more robust [28], and very deep CNNs has also been proved to show great performance in noisy speech recognition and Large Vocabulary Continuous Speech Recognition (LVCSR) tasks [23], [24], [26].", "startOffset": 109, "endOffset": 113}, {"referenceID": 21, "context": "Some researcher has shown that CNNbased speech recognition which uses raw speech as input can be more robust [28], and very deep CNNs has also been proved to show great performance in noisy speech recognition and Large Vocabulary Continuous Speech Recognition (LVCSR) tasks [23], [24], [26].", "startOffset": 274, "endOffset": 278}, {"referenceID": 22, "context": "Some researcher has shown that CNNbased speech recognition which uses raw speech as input can be more robust [28], and very deep CNNs has also been proved to show great performance in noisy speech recognition and Large Vocabulary Continuous Speech Recognition (LVCSR) tasks [23], [24], [26].", "startOffset": 280, "endOffset": 284}, {"referenceID": 24, "context": "Some researcher has shown that CNNbased speech recognition which uses raw speech as input can be more robust [28], and very deep CNNs has also been proved to show great performance in noisy speech recognition and Large Vocabulary Continuous Speech Recognition (LVCSR) tasks [23], [24], [26].", "startOffset": 286, "endOffset": 290}, {"referenceID": 28, "context": "kernels have recently been successfully applied in acoustic modeling in hybrid NN-HMM speech recognition system, and pooling layer has been proved to be replaced by full-connected convolutional layers and pooling has no highlights for LVCSR tasks [30].", "startOffset": 247, "endOffset": 251}, {"referenceID": 29, "context": "Deep residual learning framework can obtain compelling accuracy and good convergence performance in computer vision [31], which attributes to its identity mapping as the skip connection.", "startOffset": 116, "endOffset": 120}, {"referenceID": 17, "context": "The most common functions applied to a neuron\u2019s output is ReLU [19] as a function of its input with f(x) = max(0, x).", "startOffset": 63, "endOffset": 67}, {"referenceID": 18, "context": "Exponential linear unit (ELU) was introduced in [20], in contrast to ReLU, ELU has negative values which pushes the mean of activations close to zero, this is to say, ELU can decrease the gap between the normal gradient and unit natural gradient and, therefore, speed up training.", "startOffset": 48, "endOffset": 52}, {"referenceID": 14, "context": "Compared to standard fully-connected neural networks and RNNs, CNNs which are proposed in [16] have much fewer parameters so that they are easier to train.", "startOffset": 90, "endOffset": 94}, {"referenceID": 30, "context": "similar to ordinary neural networks, they are made of trainable weights and bias and they can also be stacked to a deep depth, which has been successfully applied in ImageNet competition [32].", "startOffset": 187, "endOffset": 191}, {"referenceID": 32, "context": "Statistical language modeling and neural network have both been successfully used in speech recognition [34], [35], [36], [37].", "startOffset": 104, "endOffset": 108}, {"referenceID": 33, "context": "Statistical language modeling and neural network have both been successfully used in speech recognition [34], [35], [36], [37].", "startOffset": 110, "endOffset": 114}, {"referenceID": 34, "context": "Statistical language modeling and neural network have both been successfully used in speech recognition [34], [35], [36], [37].", "startOffset": 116, "endOffset": 120}, {"referenceID": 35, "context": "Statistical language modeling and neural network have both been successfully used in speech recognition [34], [35], [36], [37].", "startOffset": 122, "endOffset": 126}, {"referenceID": 19, "context": "In detail, since the Adam optimization method [21] is computationally efficient, requires little memory allocation and is well suitable for training deep learning models with large data, we adopt the Adam method with a learning rate of 0.", "startOffset": 46, "endOffset": 50}, {"referenceID": 16, "context": "Recetnly, batch normalization [18] has shown a better regularization performance, however it would add extra parameters and needs heavy data augmentaion, which we would like to avoid.", "startOffset": 30, "endOffset": 34}, {"referenceID": 7, "context": "Some previous ASR systems use pure stacked RNNs for acoustic modeling [8], [11], and some recent ASR systems start to focus on taking some shallow CNNs as the stage of feature preprocessing in the bottom layers [25], [9].", "startOffset": 70, "endOffset": 73}, {"referenceID": 9, "context": "Some previous ASR systems use pure stacked RNNs for acoustic modeling [8], [11], and some recent ASR systems start to focus on taking some shallow CNNs as the stage of feature preprocessing in the bottom layers [25], [9].", "startOffset": 75, "endOffset": 79}, {"referenceID": 23, "context": "Some previous ASR systems use pure stacked RNNs for acoustic modeling [8], [11], and some recent ASR systems start to focus on taking some shallow CNNs as the stage of feature preprocessing in the bottom layers [25], [9].", "startOffset": 211, "endOffset": 215}, {"referenceID": 24, "context": "Third, as opposite to the traditionally used 6*6 or 3*4 filter and 1*3 pooling in speech recognition [26], we use the small filters of 3*3 to build the full convolutional layers with no pooling layer.", "startOffset": 101, "endOffset": 105}, {"referenceID": 29, "context": "Recently, a new residual learning framework has been proposed to ease the training of very deep convolutional neural networks, and deep residual networks [31] have been proved to improve convergence and higher accuracy in image classification with no more extra parameters.", "startOffset": 154, "endOffset": 158}, {"referenceID": 18, "context": "In general, h(xl) = xl is an identity mapping and f is an activation function, which we set to be elu [20] here.", "startOffset": 102, "endOffset": 106}, {"referenceID": 30, "context": "Although ResNets with over 100 layers have shown great accuracy for several challenging image classification tasks on ImageNet competitions [32] and MS COCO competitions [33], we also want to explore how the residual blocks behave in our experimental models.", "startOffset": 140, "endOffset": 144}, {"referenceID": 31, "context": "Although ResNets with over 100 layers have shown great accuracy for several challenging image classification tasks on ImageNet competitions [32] and MS COCO competitions [33], we also want to explore how the residual blocks behave in our experimental models.", "startOffset": 170, "endOffset": 174}], "year": 2016, "abstractText": "Performance of end-to-end automatic speech recognition (ASR) systems can significantly be improved by the increasing large speech corpus and deeper neural network. Given the arising problem of training speed and recent success of deep convolutional neural network in ASR, we build a novel deep recurrent convolutional network for acoustic modeling and apply deep residual learning framework to it, our experiments show that it has not only faster convergence speed but better recognition accuracy over traditional deep convolutional recurrent network. We mainly compare convergence speed of two acoustic models, which are novel deep recurrent convolutional networks and traditional deep convolutional recurrent networks. With faster convergence speed, our novel deep recurrent convolutional networks can reach the comparable performance. We further show that applying deep residual learning can boost both convergence speed and recognition accuracy of our novel recurret convolutional networks. Finally, we evaluate all our experimental networks by phoneme error rate (PER) with newly proposed bidirectional statistical language model. Our evaluation results show that our model applied with deep residual learning can reach the best PER of 17.33% with fastest convergence speed in TIMIT database.", "creator": "LaTeX with hyperref package"}}}