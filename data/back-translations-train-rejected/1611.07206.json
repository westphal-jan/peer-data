{"id": "1611.07206", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Nov-2016", "title": "Learning to Distill: The Essence Vector Modeling Framework", "abstract": "In the context of natural language processing, representation learning has emerged as a newly active research subject because of its excellent performance in many applications. Learning representations of words is a pioneering study in this school of research. However, paragraph (or sentence and document) embedding learning is more suitable/reasonable for some tasks, such as sentiment classification and document summarization. Nevertheless, as far as we are aware, there is relatively less work focusing on the development of unsupervised paragraph embedding methods. Classic paragraph embedding methods infer the representation of a given paragraph by considering all of the words occurring in the paragraph. Consequently, those stop or function words that occur frequently may mislead the embedding learning process to produce a misty paragraph representation. Motivated by these observations, our major contributions in this paper are twofold. First, we propose a novel unsupervised paragraph embedding method, named the essence vector (EV) model, which aims at not only distilling the most representative information from a paragraph but also excluding the general background information to produce a more informative low-dimensional vector representation for the paragraph. Second, in view of the increasing importance of spoken content processing, an extension of the EV model, named the denoising essence vector (D-EV) model, is proposed. The D-EV model not only inherits the advantages of the EV model but also can infer a more robust representation for a given spoken paragraph against imperfect speech recognition.", "histories": [["v1", "Tue, 22 Nov 2016 09:11:42 GMT  (410kb)", "http://arxiv.org/abs/1611.07206v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["kuan-yu chen", "shih-hung liu", "berlin chen", "hsin-min wang"], "accepted": false, "id": "1611.07206"}, "pdf": {"name": "1611.07206.pdf", "metadata": {"source": "CRF", "title": "Learning to Distill: The Essence Vector Modeling Framework", "authors": ["Kuan-Yu Chen", "Shih-Hung Liu", "Hsin-Min Wang"], "emails": ["kychen@iis.sinica.edu.tw", "journey@iis.sinica.edu.tw", "berlin@csie.ntnu.edu.tw", "whm@iis.sinica.edu.tw"], "sections": [{"heading": "1 Introduction", "text": "This year it has come to the point that it will be able to erenie.nlrrsVo rf\u00fc eid eerwtlrsrteeaeVnlrsrteeu to erenie.nlrsVo"}, {"heading": "2 Literature Review", "text": "In contrast to the extensive work on the development of various word embedding methods, there are relatively few studies that focus on learning paragraph representations unattended (Huang et al., 2013; Le and Mikolov, 2014; Chen et al., 2014; Palangi et al., 2015). Representative methods include the model of distributed memory (Le and Mikolov, 2014) and the model of the distributed dictionary bag (Le and Mikolov, 2014; Chen et al., 2014)."}, {"heading": "2.1 The Distributed Memory Model", "text": "The Distributed Storage Model (DM) is inspired and hybridized by the traditional Forward Neural Network Language Model (NNLM) (Bengio et al., 2003) and the recently proposed Word Bedding Methods (Mikolov et al., 2013). Formally, the objective function of the Forward NNNLM is to maximize the overall protocol probability, ("}, {"heading": "2.2 The Distributed Bag-of-Words Model", "text": "Unlike the DM model, a simplified version consists of relying only on paragraph representation to predict all the words occurring in the paragraph (Le and Mikolov, 2014; Chen et al., 2014). The training target function can then be defined by maximizing the predictive probabilities in all the words occurring in the paragraph: \"log = 1 T = 1. (4) Since the simplified model ignores the contextual words at the input level, the model is called the Distributed Bag-of-Words (DBOW) model. In addition to its conceptual simplicity, the DBOW model only needs to store the Softmax weights, while the DM model stores both Softmax weights and word vectors (Le and Mikolov, 2014)."}, {"heading": "3 Learning to Dist i l l", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 The Essence Vector Model", "text": "In fact, most of them are only a matter of time before an agreement is reached."}, {"heading": "3.2 The Denoising Essence Vector Model", "text": "The point is that we not only inherit the advantages of the EV model, but also a more robust representation for a given spoken paragraph that withstands the errors of the spoken transcripts. The core message is that the learned representation of a spoken paragraph can be interpreted as far as possible."}, {"heading": "4 Experimental Setup & Results", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Experiments on the EV Model for Sentiment Analysis", "text": "In fact, the fact is that most of them are able to hide, in the way that they are able, in the way that they are able, in the way that they are able, in the way that they are able, in the way that they are able, in the way that they are able, in the way that they are able, in the way that they are able to assert themselves, and in the way that they are able to put themselves in the position to put themselves in the position to survive."}, {"heading": "4.2 Experiments on the EV Model for Multi-Document Summarization", "text": "In this study, we conduct the experiments with DUC 2001, 2002 and 2004 datasets 2. All documents were compiled from peer peer peer peer and grouped into different thematic clusters; the summary was limited to 100 words for DUC 2001 and DUC 2002, and 665 bytes for DUC 2004. General background information was derived from LDC2 http: / / www-nlpir.nist.gov / projects / Gigaword corpus3 (including Associated Press Worldstream (AP), New York Times Service (NYT), and Xinhua News Agency (XIN). The most common belief in the document summary community is that relevance and redundancy are two key factors in generating a precise summary."}, {"heading": "4.3 Experiments on the D-EV Model for Spoken Document Summarization", "text": "This year, more than ever before in the history of a country which is not a country, but a country in which it is not a country, but a country in which it is a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country"}, {"heading": "DM 0.387 0.242 0.337", "text": "In the last series of experiments, we compare the above-mentioned results with those of several well-practiced, unsupervised summaries (Erkan and Radev, 2004) and the combinatorial optimization methods (i.e., the submodular method (Lin and Bilmes, 2010) and holistic programming (Riedhammer et al., 2010), including the ability to reduce redundant information in the submodular method and the ILP method."}, {"heading": "5 Conclusions", "text": "In this paper, we have proposed a novel framework for embedding paragraphs, embodied by the Essence Vector Model (EV) and the Denoising Essence Vector Model (D-EV), and taken a step forward to evaluate the proposed methods for benchmark classification and document summary. Experimental results show that the proposed framework is the most robust of all the methods (including several well-practiced or / and modern methods) compared in the paper, demonstrating the potential of the new paragraph to embed the framework. In future work, we will first focus on combining the (denoising) essence vector model with other summary methods. Furthermore, we will explore other effective ways to incorporate additional clues, such as speaker identities and relevance information, into the proposed framework."}], "references": [{"title": "A neural probabilistic language model", "author": ["Yoshua Bengio", "Rejean Ducharme", "Pascal Vincent", "Christian Jauvin."], "venue": "Journal of Machine Learning Research (3):1137\u20131155.", "citeRegEx": "Bengio et al\\.,? 2003", "shortCiteRegEx": "Bengio et al\\.", "year": 2003}, {"title": "Representation learning: a review and new perspectives", "author": ["Yoshua Bengio", "Aaron Courville", "Pascal Vincent."], "venue": "Pattern Analysis and Machine Intelligence, 35(8):1798\u20131828.", "citeRegEx": "Bengio et al\\.,? 2013", "shortCiteRegEx": "Bengio et al\\.", "year": 2013}, {"title": "Biographies, bollywood, boom-boxes and blenders: domain adaptation for sentiment classification", "author": ["John Blitzer", "Mark Dredze", "Fernando Pereira."], "venue": "Proceedings of ACL, pages 187\u2013 205.", "citeRegEx": "Blitzer et al\\.,? 2007", "shortCiteRegEx": "Blitzer et al\\.", "year": 2007}, {"title": "Learning Summary Prior Representation for Extractive Summarization", "author": ["Ziqiang Cao", "Furu Wei", "Sujian Li", "Wenjie Li", "Ming Zhou", "Houfeng Wang."], "venue": "Proceedings of ACL, pages 829\u2013833.", "citeRegEx": "Cao et al\\.,? 2015", "shortCiteRegEx": "Cao et al\\.", "year": 2015}, {"title": "The use of MMR, diversity based reranking for reordering documents and producing summaries", "author": ["Jaime Carbonell", "Jade Goldstein."], "venue": "Proceedings of SIGIR, pages 335\u2013336.", "citeRegEx": "Carbonell and Goldstein.,? 1998", "shortCiteRegEx": "Carbonell and Goldstein.", "year": 1998}, {"title": "LIBSVM: a library for support vector machines", "author": ["Chih-Chung Chang", "Chih-Jen Lin."], "venue": "ACM Transactions on Intelligent Systems and Technology, 2(27):1\u201327.", "citeRegEx": "Chang and Lin.,? 2011", "shortCiteRegEx": "Chang and Lin.", "year": 2011}, {"title": "I-vector based language modeling for spoken document retrieval", "author": ["Kuan-Yu Chen", "Hung-Shin Lee", "Hsin-Min Wang", "Berlin Chen."], "venue": "Proceedings of ICASSP, pages 7083\u20137088.", "citeRegEx": "Chen et al\\.,? 2014", "shortCiteRegEx": "Chen et al\\.", "year": 2014}, {"title": "Hierarchical Pitman-Yor-Dirichlet language model", "author": ["Jen-Tzung Chien."], "venue": "IEEE/ACM Transactions on Audio, Speech and Language Processing, 23(8): 1259\u20131272.", "citeRegEx": "Chien.,? 2015", "shortCiteRegEx": "Chien.", "year": 2015}, {"title": "A unified architecture for natural language processing: deep neural networks with multitask learning", "author": ["Ronan Collobert", "Jason Weston."], "venue": "Proceedings of ICML, pages 160\u2013167.", "citeRegEx": "Collobert and Weston.,? 2008", "shortCiteRegEx": "Collobert and Weston.", "year": 2008}, {"title": "LexRank: Graph-based lexical centrality as salience in text summarization", "author": ["Gunes Erkan", "Dragomir R. Radev."], "venue": "Journal of Artificial Intelligent Research, 22(1):457\u2013479.", "citeRegEx": "Erkan and Radev.,? 2004", "shortCiteRegEx": "Erkan and Radev.", "year": 2004}, {"title": "Fundamental technologies in modern speech recognition", "author": ["Sadaoki Furui", "Li Deng", "Mark Gales", "Hermann Ney", "Keiichi Tokuda."], "venue": "IEEE Signal Processing Magazine, 29(6):16\u201317.", "citeRegEx": "Furui et al\\.,? 2012", "shortCiteRegEx": "Furui et al\\.", "year": 2012}, {"title": "Generic text summarization using relevance measure and latent semantic analysis", "author": ["Yihong Gong", "Xin Liu."], "venue": "Proceedings of SIGIR, pages 19\u201325.", "citeRegEx": "Gong and Liu.,? 2001", "shortCiteRegEx": "Gong and Liu.", "year": 2001}, {"title": "Deep Learning", "author": ["Ian Goodfellow", "Yoshua Bengio", "Aaron Courville."], "venue": "Cambridge, MA: MIT Press.", "citeRegEx": "Goodfellow et al\\.,? 2016", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2016}, {"title": "Learning deep structured semantic models for web search using clickthrough data", "author": ["Po-Sen Huang", "Xiaodong He", "Jianfeng Gao", "Li Deng", "Alex Acero", "Larry Heck."], "venue": "Proceedings of CIKM, pages 2333\u20132338.", "citeRegEx": "Huang et al\\.,? 2013", "shortCiteRegEx": "Huang et al\\.", "year": 2013}, {"title": "Spoken Document Retrieval Using Multi-Level Knowledge and Semantic Verification", "author": ["Chien-Lin Huang", "Chung-Hsien Wu."], "venue": "IEEE Transactions on Audio, Speech, and Language Processing, 15(8): 2551\u20132560.", "citeRegEx": "Huang and Wu.,? 2007", "shortCiteRegEx": "Huang and Wu.", "year": 2007}, {"title": "Extractive summarization using continuous vector space models", "author": ["Mikael Kageback", "Olof Mogren", "Nina Tahmasebi", "Devdatt Dubhashi."], "venue": "Proceedings of CVSC, pages 31\u201339.", "citeRegEx": "Kageback et al\\.,? 2014", "shortCiteRegEx": "Kageback et al\\.", "year": 2014}, {"title": "ADAM: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba."], "venue": "Proceedings of ICLR, pages 1\u201315.", "citeRegEx": "Kingma and Ba.,? 2015", "shortCiteRegEx": "Kingma and Ba.", "year": 2015}, {"title": "Distributed representations of sentences and documents", "author": ["Quoc Le", "Tomas Mikolov."], "venue": "Proceedings of ICML, pages 1188\u20131196.", "citeRegEx": "Le and Mikolov.,? 2014", "shortCiteRegEx": "Le and Mikolov.", "year": 2014}, {"title": "ROUGE: Recall-oriented understudy for gisting evaluation", "author": ["Chin-Yew Lin."], "venue": "[Online]. Available: http://haydn.isi.edu/ROUGE/.", "citeRegEx": "Lin.,? 2003", "shortCiteRegEx": "Lin.", "year": 2003}, {"title": "Multi-document summarization via budgeted maximization of submodular functions", "author": ["Hui Lin", "Jeff Bilmes."], "venue": "Proceedings of NAACL HLT, pages 912\u2013920.", "citeRegEx": "Lin and Bilmes.,? 2010", "shortCiteRegEx": "Lin and Bilmes.", "year": 2010}, {"title": "Speech summarization", "author": ["Yang Liu", "Dilek Hakkani-Tur."], "venue": "Chapter 13 in Spoken Language Understanding: Systems for Extracting Semantic Information from Speech. G. Tur and R. D. Mori (Eds), New York: Wiley.", "citeRegEx": "Liu and Hakkani.Tur.,? 2011", "shortCiteRegEx": "Liu and Hakkani.Tur.", "year": 2011}, {"title": "Combining relevance language modeling and clarity measure for extractive speech summarization", "author": ["Shih-Hung Liu", "Kuan-Yu Chen", "Berlin Chen", "Hsin-Min Wang", "Hsu-Chun Yen", "Wen-Lian Hsu."], "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing, 23(6): 957\u2013969.", "citeRegEx": "Liu et al\\.,? 2015", "shortCiteRegEx": "Liu et al\\.", "year": 2015}, {"title": "Efficient estimation of word representations in vector space", "author": ["Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean."], "venue": "Proceedings of ICLR, pages 1\u201312.", "citeRegEx": "Mikolov et al\\.,? 2013", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Automatic summarization", "author": ["Ani Nenkova", "Kathleen McKeown."], "venue": "Foundations and Trends in Information Retrieval, 5(2\u20133): 103\u2013233.", "citeRegEx": "Nenkova and McKeown.,? 2011", "shortCiteRegEx": "Nenkova and McKeown.", "year": 2011}, {"title": "Speech technology and information access", "author": ["Mari Ostendorf."], "venue": "IEEE Signal Processing Magazine, 25(3):150\u2013152.", "citeRegEx": "Ostendorf.,? 2008", "shortCiteRegEx": "Ostendorf.", "year": 2008}, {"title": "Spoken document understanding and organization", "author": ["Lin-shan Lee", "Berlin Chen."], "venue": "IEEE Signal Processing Magazine, 22(5):42\u201360.", "citeRegEx": "Lee and Chen.,? 2005", "shortCiteRegEx": "Lee and Chen.", "year": 2005}, {"title": "Deep sentence embedding using the long short term memory network: analysis and application to information retrieval", "author": ["Hamid Palangi", "Li Deng", "Yelong Shen", "Jianfeng Gao", "Xiaodong He", "Jianshu Chen", "Xinying Song", "Rabab Ward."], "venue": "Proceedings of arXiv:1502.06922.", "citeRegEx": "Palangi et al\\.,? 2015", "shortCiteRegEx": "Palangi et al\\.", "year": 2015}, {"title": "A critical reassessment of evaluation baselines for speech summarization", "author": ["Gerald Penn", "Xiaodan Zhu."], "venue": "Proceedings of ACL, pages 470\u2013478.", "citeRegEx": "Penn and Zhu.,? 2008", "shortCiteRegEx": "Penn and Zhu.", "year": 2008}, {"title": "GloVe: Global vector for word representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher D. Manning."], "venue": "Proceedings of EMNLP, pages 1532\u20131543.", "citeRegEx": "Pennington et al\\.,? 2014", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Long story short - Global unsupervised models for keyphrase based meeting summarization", "author": ["Korbinian Riedhammer", "Benoit Favre", "Dilek Hakkani-Tur."], "venue": "Speech Communication, 52(10):801\u2013815.", "citeRegEx": "Riedhammer et al\\.,? 2010", "shortCiteRegEx": "Riedhammer et al\\.", "year": 2010}, {"title": "Clustering by fast search and find of density peaks", "author": ["Alex Rodriguez", "Alessandro Laio."], "venue": "Science, 344(6191): 1492\u20131496.", "citeRegEx": "Rodriguez and Laio.,? 2014", "shortCiteRegEx": "Rodriguez and Laio.", "year": 2014}, {"title": "Learning sentimentspecific word embedding for twitter sentiment classification", "author": ["Duyu Tang", "Furu Wei", "Nan Yang", "Ming Zhou", "Ting Liu", "Bing Qin."], "venue": "Proceedings of ACL, pages 1555\u20131565.", "citeRegEx": "Tang et al\\.,? 2014", "shortCiteRegEx": "Tang et al\\.", "year": 2014}, {"title": "Multi-document summarization using cluster-based link analysis", "author": ["Xiaojun Wan", "Jianwu Yang."], "venue": "Proceedings of SIGIR, pages 299\u2013306.", "citeRegEx": "Wan and Yang.,? 2008", "shortCiteRegEx": "Wan and Yang.", "year": 2008}, {"title": "MATBN: A Mandarin Chinese broadcast news corpus", "author": ["Hsin-Min Wang", "Berlin Chen", "Jen-Wei Kuo", "Shih-Sian Cheng."], "venue": "International Journal of Computational Linguistics and Chinese Language Processing, 10(2):219\u2013236.", "citeRegEx": "Wang et al\\.,? 2005", "shortCiteRegEx": "Wang et al\\.", "year": 2005}, {"title": "Clustering sentences with density peaks for multi-document summarization", "author": ["Yang Zhang", "Yunqing Xia", "Yi Liu", "Wenmin Wang."], "venue": "Proceedings of NAACL, pages 1262\u20131267.", "citeRegEx": "Zhang et al\\.,? 2015", "shortCiteRegEx": "Zhang et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "When it comes to the field of natural language processing (NLP), word embedding methods can be viewed as pioneering studies (Bengio et al., 2003; Mikolov et al., 2013; Pennington et al., 2014).", "startOffset": 124, "endOffset": 192}, {"referenceID": 22, "context": "When it comes to the field of natural language processing (NLP), word embedding methods can be viewed as pioneering studies (Bengio et al., 2003; Mikolov et al., 2013; Pennington et al., 2014).", "startOffset": 124, "endOffset": 192}, {"referenceID": 28, "context": "When it comes to the field of natural language processing (NLP), word embedding methods can be viewed as pioneering studies (Bengio et al., 2003; Mikolov et al., 2013; Pennington et al., 2014).", "startOffset": 124, "endOffset": 192}, {"referenceID": 8, "context": "By doing so, this thread of methods has recently enjoyed substantial success in many NLP-related tasks (Collobert and Weston, 2008; Tang et al., 2014; Kageback et al., 2014).", "startOffset": 103, "endOffset": 173}, {"referenceID": 31, "context": "By doing so, this thread of methods has recently enjoyed substantial success in many NLP-related tasks (Collobert and Weston, 2008; Tang et al., 2014; Kageback et al., 2014).", "startOffset": 103, "endOffset": 173}, {"referenceID": 15, "context": "By doing so, this thread of methods has recently enjoyed substantial success in many NLP-related tasks (Collobert and Weston, 2008; Tang et al., 2014; Kageback et al., 2014).", "startOffset": 103, "endOffset": 173}, {"referenceID": 13, "context": "Theoretically, paragraph-based representation learning is expected to be more suitable for such tasks as information retrieval, sentiment analysis and document summarization (Huang et al., 2013; Le and Mikolov, 2014; Palangi et al., 2015), to name but a few.", "startOffset": 174, "endOffset": 238}, {"referenceID": 17, "context": "Theoretically, paragraph-based representation learning is expected to be more suitable for such tasks as information retrieval, sentiment analysis and document summarization (Huang et al., 2013; Le and Mikolov, 2014; Palangi et al., 2015), to name but a few.", "startOffset": 174, "endOffset": 238}, {"referenceID": 26, "context": "Theoretically, paragraph-based representation learning is expected to be more suitable for such tasks as information retrieval, sentiment analysis and document summarization (Huang et al., 2013; Le and Mikolov, 2014; Palangi et al., 2015), to name but a few.", "startOffset": 174, "endOffset": 238}, {"referenceID": 25, "context": "Consequently, spoken content processing has become an important and urgent demand (Lee and Chen, 2005; Ostendorf, 2008; Liu and Hakkani-Tur, 2011).", "startOffset": 82, "endOffset": 146}, {"referenceID": 24, "context": "Consequently, spoken content processing has become an important and urgent demand (Lee and Chen, 2005; Ostendorf, 2008; Liu and Hakkani-Tur, 2011).", "startOffset": 82, "endOffset": 146}, {"referenceID": 20, "context": "Consequently, spoken content processing has become an important and urgent demand (Lee and Chen, 2005; Ostendorf, 2008; Liu and Hakkani-Tur, 2011).", "startOffset": 82, "endOffset": 146}, {"referenceID": 10, "context": "Obviously, speech is one of the most important sources of information about multimedia (Furui et al., 2012).", "startOffset": 87, "endOffset": 107}, {"referenceID": 13, "context": "In contrast to the large body of work on developing various word embedding methods, there are relatively few studies concentrating on learning paragraph representations in an unsupervised manner (Huang et al., 2013; Le and Mikolov, 2014; Chen et al., 2014; Palangi et al., 2015).", "startOffset": 195, "endOffset": 278}, {"referenceID": 17, "context": "In contrast to the large body of work on developing various word embedding methods, there are relatively few studies concentrating on learning paragraph representations in an unsupervised manner (Huang et al., 2013; Le and Mikolov, 2014; Chen et al., 2014; Palangi et al., 2015).", "startOffset": 195, "endOffset": 278}, {"referenceID": 6, "context": "In contrast to the large body of work on developing various word embedding methods, there are relatively few studies concentrating on learning paragraph representations in an unsupervised manner (Huang et al., 2013; Le and Mikolov, 2014; Chen et al., 2014; Palangi et al., 2015).", "startOffset": 195, "endOffset": 278}, {"referenceID": 26, "context": "In contrast to the large body of work on developing various word embedding methods, there are relatively few studies concentrating on learning paragraph representations in an unsupervised manner (Huang et al., 2013; Le and Mikolov, 2014; Chen et al., 2014; Palangi et al., 2015).", "startOffset": 195, "endOffset": 278}, {"referenceID": 17, "context": "Representative methods include the distributed memory model (Le and Mikolov, 2014) and the distributed bag-of-words model (Le and Mikolov, 2014; Chen et al.", "startOffset": 60, "endOffset": 82}, {"referenceID": 17, "context": "Representative methods include the distributed memory model (Le and Mikolov, 2014) and the distributed bag-of-words model (Le and Mikolov, 2014; Chen et al., 2014).", "startOffset": 122, "endOffset": 163}, {"referenceID": 6, "context": "Representative methods include the distributed memory model (Le and Mikolov, 2014) and the distributed bag-of-words model (Le and Mikolov, 2014; Chen et al., 2014).", "startOffset": 122, "endOffset": 163}, {"referenceID": 0, "context": "The distributed memory (DM) model is inspired and hybridized from the traditional feed-forward neural network language model (NNLM) (Bengio et al., 2003) and the recently proposed word", "startOffset": 132, "endOffset": 153}, {"referenceID": 22, "context": "embedding methods (Mikolov et al., 2013).", "startOffset": 18, "endOffset": 40}, {"referenceID": 17, "context": "Based on the NNLM, the notion underlying the DM model is that a given paragraph also contributes to the prediction of the next word, given its previous words in the paragraph (Le and Mikolov, 2014).", "startOffset": 175, "endOffset": 197}, {"referenceID": 17, "context": "Opposite to the DM model, a simplified version is to only rely on the paragraph representation to predict all of the words occurring in the paragraph (Le and Mikolov, 2014; Chen et al., 2014).", "startOffset": 150, "endOffset": 191}, {"referenceID": 6, "context": "Opposite to the DM model, a simplified version is to only rely on the paragraph representation to predict all of the words occurring in the paragraph (Le and Mikolov, 2014; Chen et al., 2014).", "startOffset": 150, "endOffset": 191}, {"referenceID": 17, "context": "In addition to being conceptually simple, the DBOW model only needs to store the softmax weights, whereas the DM model stores both softmax weights and word vectors (Le and Mikolov, 2014).", "startOffset": 164, "endOffset": 186}, {"referenceID": 12, "context": "The activation function used in the EV model is the hyperbolic tangent, except that the output layer in the decoder h(\u2219) is the softmax (Goodfellow et al., 2016), the cosine distance is used to calculate the attention coefficients, and the Adam (Kingma and Ba, 2015) is employed to solve the optimization problem.", "startOffset": 136, "endOffset": 161}, {"referenceID": 16, "context": ", 2016), the cosine distance is used to calculate the attention coefficients, and the Adam (Kingma and Ba, 2015) is employed to solve the optimization problem.", "startOffset": 91, "endOffset": 112}, {"referenceID": 2, "context": "Four widely-used benchmark multi-domain sentiment datasets are used in this study1 (Blitzer et al., 2007).", "startOffset": 83, "endOffset": 105}, {"referenceID": 5, "context": "The linear kernel SVM (Chang and Lin, 2011) is used as our classifier and all of the parameters are set to the default values.", "startOffset": 22, "endOffset": 43}, {"referenceID": 1, "context": "In this set of experiments, we first compare the EV model with PCA (Bengio et al., 2013), which is a standard dimension reduction method.", "startOffset": 67, "endOffset": 88}, {"referenceID": 1, "context": "It is worthy to note that PCA is a variation of an autoencoder (Bengio et al., 2013) method; thus it can be treated as our baseline system.", "startOffset": 63, "endOffset": 84}, {"referenceID": 30, "context": "In this paper, we leverage a density peaks clustering summarization method (Rodriguez and Laio, 2014; Zhang et al., 2015), which can take both relevance and redundancy information into account at the same time.", "startOffset": 75, "endOffset": 121}, {"referenceID": 34, "context": "In this paper, we leverage a density peaks clustering summarization method (Rodriguez and Laio, 2014; Zhang et al., 2015), which can take both relevance and redundancy information into account at the same time.", "startOffset": 75, "endOffset": 121}, {"referenceID": 34, "context": "Recently, the summarization method has proven its empirical effectiveness (Zhang et al., 2015).", "startOffset": 74, "endOffset": 94}, {"referenceID": 18, "context": "For evaluation, we adopt the widely-used automatic evaluation metric ROUGE (Lin, 2003), and take ROUGE-1 and ROUGE-2 (in F-scores) as the main measures following Cao et al.", "startOffset": 75, "endOffset": 86}, {"referenceID": 3, "context": "For evaluation, we adopt the widely-used automatic evaluation metric ROUGE (Lin, 2003), and take ROUGE-1 and ROUGE-2 (in F-scores) as the main measures following Cao et al., (2015).", "startOffset": 162, "endOffset": 181}, {"referenceID": 11, "context": "We compare the proposed EV model with two baseline systems (the vector space model (VSM) (Gong and Liu, 2001) and the LexRank (Erkan and Radev, 2004) method), the best peer systems (including Peer T, Peer 26, and Peer 65) participating DUC evaluations, and the recently elaborated DNN-based systems (including CNN and PriorSum) (Cao et al.", "startOffset": 89, "endOffset": 109}, {"referenceID": 9, "context": "We compare the proposed EV model with two baseline systems (the vector space model (VSM) (Gong and Liu, 2001) and the LexRank (Erkan and Radev, 2004) method), the best peer systems (including Peer T, Peer 26, and Peer 65) participating DUC evaluations, and the recently elaborated DNN-based systems (including CNN and PriorSum) (Cao et al.", "startOffset": 126, "endOffset": 149}, {"referenceID": 3, "context": "We compare the proposed EV model with two baseline systems (the vector space model (VSM) (Gong and Liu, 2001) and the LexRank (Erkan and Radev, 2004) method), the best peer systems (including Peer T, Peer 26, and Peer 65) participating DUC evaluations, and the recently elaborated DNN-based systems (including CNN and PriorSum) (Cao et al., 2015).", "startOffset": 328, "endOffset": 346}, {"referenceID": 3, "context": "We compare the proposed EV model with two baseline systems (the vector space model (VSM) (Gong and Liu, 2001) and the LexRank (Erkan and Radev, 2004) method), the best peer systems (including Peer T, Peer 26, and Peer 65) participating DUC evaluations, and the recently elaborated DNN-based systems (including CNN and PriorSum) (Cao et al., 2015). Owing to the space limitation, we omit the detailed introduction to these summarization methods; interested readers may refer to Penn and Zhu (2008), Liu and Hakkani-Tur (2011), Nenkova and McKeown (2011), and Cao et al.", "startOffset": 329, "endOffset": 497}, {"referenceID": 3, "context": "We compare the proposed EV model with two baseline systems (the vector space model (VSM) (Gong and Liu, 2001) and the LexRank (Erkan and Radev, 2004) method), the best peer systems (including Peer T, Peer 26, and Peer 65) participating DUC evaluations, and the recently elaborated DNN-based systems (including CNN and PriorSum) (Cao et al., 2015). Owing to the space limitation, we omit the detailed introduction to these summarization methods; interested readers may refer to Penn and Zhu (2008), Liu and Hakkani-Tur (2011), Nenkova and McKeown (2011), and Cao et al.", "startOffset": 329, "endOffset": 525}, {"referenceID": 3, "context": "We compare the proposed EV model with two baseline systems (the vector space model (VSM) (Gong and Liu, 2001) and the LexRank (Erkan and Radev, 2004) method), the best peer systems (including Peer T, Peer 26, and Peer 65) participating DUC evaluations, and the recently elaborated DNN-based systems (including CNN and PriorSum) (Cao et al., 2015). Owing to the space limitation, we omit the detailed introduction to these summarization methods; interested readers may refer to Penn and Zhu (2008), Liu and Hakkani-Tur (2011), Nenkova and McKeown (2011), and Cao et al.", "startOffset": 329, "endOffset": 553}, {"referenceID": 3, "context": "We compare the proposed EV model with two baseline systems (the vector space model (VSM) (Gong and Liu, 2001) and the LexRank (Erkan and Radev, 2004) method), the best peer systems (including Peer T, Peer 26, and Peer 65) participating DUC evaluations, and the recently elaborated DNN-based systems (including CNN and PriorSum) (Cao et al., 2015). Owing to the space limitation, we omit the detailed introduction to these summarization methods; interested readers may refer to Penn and Zhu (2008), Liu and Hakkani-Tur (2011), Nenkova and McKeown (2011), and Cao et al., (2015) for more in-depth elaboration.", "startOffset": 329, "endOffset": 577}, {"referenceID": 33, "context": "All of experiments are conducted on a Mandarin benchmark broadcast new corpus4 (Wang et al., 2005).", "startOffset": 79, "endOffset": 98}, {"referenceID": 7, "context": "The MATBN dataset is publicly available and has been widely used to evaluate several NLP-related tasks, including speech recognition (Chien, 2015), information retrieval (Huang and Wu, 2007) and summarization (Liu et al.", "startOffset": 133, "endOffset": 146}, {"referenceID": 14, "context": "The MATBN dataset is publicly available and has been widely used to evaluate several NLP-related tasks, including speech recognition (Chien, 2015), information retrieval (Huang and Wu, 2007) and summarization (Liu et al.", "startOffset": 170, "endOffset": 190}, {"referenceID": 21, "context": "The MATBN dataset is publicly available and has been widely used to evaluate several NLP-related tasks, including speech recognition (Chien, 2015), information retrieval (Huang and Wu, 2007) and summarization (Liu et al., 2015).", "startOffset": 209, "endOffset": 227}, {"referenceID": 18, "context": "For the assessment of summarization performance, we adopt the commonly-used ROUGE metric (Lin, 2003), and take ROUGE-1, ROUGE-2 and ROUGE-L (in F-scores) as the main measures.", "startOffset": 89, "endOffset": 100}, {"referenceID": 32, "context": ", the Markov random walk (MRW) method (Wan and Yang, 2008) and the LexRank method (Erkan and Radev, 2004)) and the combinatorial optimization methods (i.", "startOffset": 38, "endOffset": 58}, {"referenceID": 9, "context": ", the Markov random walk (MRW) method (Wan and Yang, 2008) and the LexRank method (Erkan and Radev, 2004)) and the combinatorial optimization methods (i.", "startOffset": 82, "endOffset": 105}, {"referenceID": 19, "context": ", the submodularitybased (SM) method (Lin and Bilmes, 2010) and the integer linear programming (ILP) method (Riedhammer et al.", "startOffset": 37, "endOffset": 59}, {"referenceID": 29, "context": ", the submodularitybased (SM) method (Lin and Bilmes, 2010) and the integer linear programming (ILP) method (Riedhammer et al., 2010)).", "startOffset": 108, "endOffset": 133}, {"referenceID": 9, "context": ", the Markov random walk (MRW) method (Wan and Yang, 2008) and the LexRank method (Erkan and Radev, 2004)) and the combinatorial optimization methods (i.e., the submodularitybased (SM) method (Lin and Bilmes, 2010) and the integer linear programming (ILP) method (Riedhammer et al., 2010)). Among them, the ability of reducing redundant information has been aptly incorporated into the submodular-based method and the ILP method. Interested readers may refer to Penn and Zhu (2008), Liu and Hakkani-Tur (2011), and Nenkova and McKeown (2011) for comprehensive reviews and new insights into the major methods that have been developed and applied with good success to a wide range of spoken document summarization tasks.", "startOffset": 83, "endOffset": 482}, {"referenceID": 9, "context": ", the Markov random walk (MRW) method (Wan and Yang, 2008) and the LexRank method (Erkan and Radev, 2004)) and the combinatorial optimization methods (i.e., the submodularitybased (SM) method (Lin and Bilmes, 2010) and the integer linear programming (ILP) method (Riedhammer et al., 2010)). Among them, the ability of reducing redundant information has been aptly incorporated into the submodular-based method and the ILP method. Interested readers may refer to Penn and Zhu (2008), Liu and Hakkani-Tur (2011), and Nenkova and McKeown (2011) for comprehensive reviews and new insights into the major methods that have been developed and applied with good success to a wide range of spoken document summarization tasks.", "startOffset": 83, "endOffset": 510}, {"referenceID": 9, "context": ", the Markov random walk (MRW) method (Wan and Yang, 2008) and the LexRank method (Erkan and Radev, 2004)) and the combinatorial optimization methods (i.e., the submodularitybased (SM) method (Lin and Bilmes, 2010) and the integer linear programming (ILP) method (Riedhammer et al., 2010)). Among them, the ability of reducing redundant information has been aptly incorporated into the submodular-based method and the ILP method. Interested readers may refer to Penn and Zhu (2008), Liu and Hakkani-Tur (2011), and Nenkova and McKeown (2011) for comprehensive reviews and new insights into the major methods that have been developed and applied with good success to a wide range of spoken document summarization tasks.", "startOffset": 83, "endOffset": 542}], "year": 2016, "abstractText": "In the context of natural language processing, representation learning has emerged as a newly active research subject because of its excellent performance in many applications. Learning representations of words is a pioneering study in this school of research. However, paragraph (or sentence and document) embedding learning is more suitable/reasonable for some tasks, such as sentiment classification and document summarization. Nevertheless, as far as we are aware, there is relatively less work focusing on the development of unsupervised paragraph embedding methods. Classic paragraph embedding methods infer the representation of a given paragraph by considering all of the words occurring in the paragraph. Consequently, those stop or function words that occur frequently may mislead the embedding learning process to produce a misty paragraph representation. Motivated by these observations, our major contributions in this paper are twofold. First, we propose a novel unsupervised paragraph embedding method, named the essence vector (EV) model, which aims at not only distilling the most representative information from a paragraph but also excluding the general background information to produce a more informative low-dimensional vector representation for the paragraph. We evaluate the proposed EV model on benchmark sentiment classification and multi-document summarization tasks. The experimental results demonstrate the effectiveness and applicability of the proposed embedding method. Second, in view of the increasing importance of spoken content processing, an extension of the EV model, named the denoising essence vector (D-EV) model, is proposed. The D-EV model not only inherits the advantages of the EV model but also can infer a more robust representation for a given spoken paragraph against imperfect speech recognition. The utility of the D-EV model is evaluated on a spoken document summarization task, confirming the practical merits of the proposed embedding method in relation to several wellpracticed and state-of-the-art summarization methods.", "creator": "Acrobat PDFMaker 15 Word \u7248"}}}