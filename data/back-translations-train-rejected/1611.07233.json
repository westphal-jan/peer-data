{"id": "1611.07233", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Nov-2016", "title": "CAS-CNN: A Deep Convolutional Neural Network for Image Compression Artifact Suppression", "abstract": "Lossy image compression algorithms are pervasively used to reduce the size of images transmitted over the web and recorded on data storage media. However, we pay for their high compression rate with visual artifacts degrading the user experience. Deep convolutional neural networks have become a widespread tool to address high-level computer vision tasks very successfully. Recently, they have found their way into the areas of low-level computer vision and image processing to solve regression problems mostly with relatively shallow networks.", "histories": [["v1", "Tue, 22 Nov 2016 10:11:58 GMT  (4710kb,D)", "http://arxiv.org/abs/1611.07233v1", "8 pages"]], "COMMENTS": "8 pages", "reviews": [], "SUBJECTS": "cs.CV cs.AI cs.GR cs.IR cs.MM", "authors": ["lukas cavigelli", "pascal hager", "luca benini"], "accepted": false, "id": "1611.07233"}, "pdf": {"name": "1611.07233.pdf", "metadata": {"source": "CRF", "title": "CAS-CNN: A Deep Convolutional Neural Network for Image Compression Artifact Suppression", "authors": ["Lukas Cavigelli", "Pascal Hager", "Luca Benini"], "emails": ["surname@iis.ee.ethz.ch"], "sections": [{"heading": null, "text": "This year, it has come to the point where it is only a matter of time before a decision is reached in which a decision is taken."}, {"heading": "II. RELATED WORK", "text": "Traditional approaches to suppressing compression artifacts can be divided into three categories. Different types of intelligent edgeaware denozing such as SA-DCT [15], [16], BM3D [17] were proposed to address this task during the late 2000s. In recent years, deficits such as ringing and blocking of very specific JPEG algorithms such as DicTV [18], RTF [19], S-D2 [21], DDDCN [22] have been dealt with very well. These algorithms explicitly attempt to reverse the effect of DCT domain quantification using learned words and quantizations using very specific compressors and quantization tables. This work was inspired by frames and blockages that are very specific."}, {"heading": "III. METHODOLOGY", "text": "We start with the basic concept of forming a deep ConvNet for a regression problem, as it was done for the related task of superresolution [14], [23] or other low-level computer vision operations such as optical flow estimation [13]. [25] The authors of ConvNets propose several new elements for artifact reduction: a residual architecture, an edge-stressed loss function, symmetrical weight initialization, and skip connections. All these elements were introduced to remove the obstacles preventing the training of deep networks for regression tasks. Inspired by deep neural networks such as FlowNet [13] and FCN [10], developed for optical flow estimation and semantic segmentation, we propose a neural network with hierarchical skip connections (cf. Section III-A) and a multi-scale loss function (cf. Section III-C) for compression suppression suppression of artefacts."}, {"heading": "A. Network Architecture", "text": "This year it is as far as never before in the history of the Federal Republic of Germany."}, {"heading": "B. Performance Metrics", "text": "The most common performance indicators for evaluating differences between images and many other signals are the mean square error (MSE) and the closely related peak signal-to-noise ratio (PSNR).The MSE is the pixel-by-pixel average of the square difference in intensity between the distorted and the reference frame.The PSNR is the MSE that is normalized to the maximum possible signal values typically expressed in decibels (dB).After [24], [25] with pixel values normalized to the range [0, 1], we use PSNR (X, X) = 10 log10 (1 / MSE (X, X))), (1) MSE (X, P) e (xp, p, p) 2 / | P | (2), where P is the set of pixel indexes (X, X), the reference image (X, X)."}, {"heading": "C. Loss Function", "text": "During the ConvNets training, we minimize the MSE criterion by penalizing deviations from the reference image by the square distance. However, as mentioned in Section III-A, in order to improve the training process, we include not only the results in full resolution, but also the results in low resolution within the network. The reference values for this are calculated by scanning the input image on average over 4, 16, and 64 pixels, respectively. Each of these results equally contributes to the total loss function in multiple scales (MS).We perform the training to convergence with this goal before removing the lower resolution images from the loss function and continuing the training for several eras to minimize the MSE only of the output image in full resolution (output loss), block the fine tuning (FT) of the network with this optimization function.In previous work, including an edged term in the loss function, we have decided not to introduce such a loss."}, {"heading": "D. Dataset", "text": "The authors of [25] show that this is the limiting factor for further improving their larger L8 network with 220k learned parameters. We do not want to limit the size of our network by the amount of available training data, especially since we do not need hard-to-obtain labels for this. Therefore, we use the large, well-known and publicly available ImageNet 2013 recognition dataset [35], which consists of 396k training and 20k validation color images of varying sizes. We take sections of 120 x 120 pixels from each image to generate our datasets, the color images are transformed into YCbCr space and only the luminance channel is used, and the input into the network is generated by compressing the resulting single-channel image."}, {"heading": "IV. RESULTS & DISCUSSION", "text": "This year, the time has come for us to be able to go to a place where we can go to a place where we can go to a place where we can go to a place where we can move."}, {"heading": "V. CONCLUSION", "text": "We presented a 12-layer, deep convolutionary neural network for artifact compression suppression in JPEG images with hierarchical skip connections and trained with a multi-scale loss function, resulting in a new, state-of-the-art ConvNet that increases up to 1.79 dB in PSNR over normal JPEG, and improves up to 0.36 dB over the best previous ConvNet result. We demonstrated that a network designed for a specific quality factor is resilient to the QF used, compresses the input image - a single network designed for QF 60 provides a PSNR gain of more than 1.5 dB over the wide QF range from 40 to 76. The results obtained are also qualitatively better than the existing ConvNets. The network is not tailored to the JPEG-specific compression algorithm and can therefore potentially be applied to a wide range of image compression algorithms."}, {"heading": "ACKNOWLEDGMENTS", "text": "The authors thank Thilo Weber and Jonas Wiesendanger for their preparatory work on this topic and armasuisse Science & Technology for funding this research."}], "references": [{"title": "Why is image quality assessment so difficult?", "author": ["Z. Wang", "A.C. Bovik", "L. Lu"], "venue": "IEEE Int. Conf. Acoust. Speech Signal Process.,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2002}, {"title": "Deep Convolution Networks for Compression Artifacts Reduction", "author": ["K. Yu", "C. Dong", "C.C. Loy", "X. Tang"], "venue": "arXiv:1608.02778, 2016.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2016}, {"title": "Survey of image compression algorithms in wireless sensor networks", "author": ["L. Chew", "L. Ang"], "venue": "2008 Int. Symp. Inf. Technol., pp. 1\u20139, 2008.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2008}, {"title": "HTTP Archive - Interesting Stats", "author": ["S. Souders"], "venue": "2016. [Online]. Available: http://httparchive.org/interesting.php", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2016}, {"title": "A lowpower wireless video sensor node for distributed object detection", "author": ["A. Kerhet", "M. Magno", "F. Leonardi", "A. Boni", "L. Benini"], "venue": "J. Real-Time Image Process., vol. 2, no. 4, pp. 331\u2013342, 2007.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2007}, {"title": "Energy Aware Lossless Data Compression", "author": ["K. Barr", "K. Asanovic"], "venue": "Proc. of MobiSys, no. May, 2003.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2003}, {"title": "Energy-aware data compression for Multi-Level Cell (MLC) flash memory", "author": ["Y. Joo", "Y. Cho", "D. Shin", "N. Chang"], "venue": "Proc. ACM/IEEE Des. Autom. Conf., 2007, pp. 716\u2013719.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2007}, {"title": "Deep Residual Learning for Image Recognition", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "arXiv:1512.03385, dec 2015.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks", "author": ["S. Ren", "K. He", "R. Girshick", "J. Sun"], "venue": "arXiv:1506.01497, 2015.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2015}, {"title": "Fully Convolutional Networks for Semantic Segmentation", "author": ["J. Long", "E. Shelhamer", "T. Darrell"], "venue": "Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2015. 3 http://iis.ee.ethz.ch/\u223clukasc/cascnn/", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "Accelerating Real-Time Embedded Scene Labeling with Convolutional Networks", "author": ["L. Cavigelli", "M. Magno", "L. Benini"], "venue": "Proc. ACM/IEEE Des. Autom. Conf., 2015.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "Saliency detection by multicontext deep learning", "author": ["R. Zhao", "W. Ouyang", "H. Li", "X. Wang"], "venue": "Proc. IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit., vol. 07-12-June, pp. 1265\u20131274, 2015.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2015}, {"title": "FlowNet: Learning Optical Flow with Convolutional Networks", "author": ["P. Fischer", "A. Dosovitskiy", "E. Ilg", "P. Haeusser", "C. Hazirbas", "V. Glokov", "P. Van der Smagt", "D. Cremers", "T. Brox"], "venue": "arXiv:15047.06852, 2015.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning a deep convolutional network for image super-resolution", "author": ["C. Dong", "C.C. Loy", "K. He", "X. Tang"], "venue": "Proc. Eur. Conf. Comput. Vis., pp. 184\u2013199, 2014.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2014}, {"title": "Pointwise shape-adaptive DCT for high-quality denoising and deblocking of grayscale and color images", "author": ["A. Foi", "V. Katkovnik", "K. Egiazarian"], "venue": "IEEE Trans. Image Process., vol. 16, no. 5, pp. 1395\u20131411, 2007.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2007}, {"title": "Pointwise shape-adaptive DCT for high-quality deblocking of compressed color images", "author": ["\u2014\u2014"], "venue": "Eur. Signal Process. Conf., 2006.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2006}, {"title": "Image denoising by sparse 3D transformation-domain collaborative filtering", "author": ["K. Dabov", "A. Foi", "V. Katkovnik"], "venue": "IEEE Trans. Image Process., vol. 16, no. 8, pp. 1\u201316, 2007.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2007}, {"title": "Reducing artifacts in JPEG decompression via a learned dictionary", "author": ["H. Chang", "M.K. Ng", "T. Zeng"], "venue": "IEEE Trans. Signal Process., vol. 62, no. 3, pp. 718\u2013728, 2014.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2014}, {"title": "Loss-Specific Training of Non- Parametric Image Restoration Models: A New State of the Art", "author": ["J. Jancsary", "S. Nowozin", "C. Rother"], "venue": "pp. 112\u2013125, 2012.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2012}, {"title": "Data-driven sparsity-based restoration of JPEG-compressed images in dual transform-pixel domain", "author": ["X. Liu", "X. Wu", "J. Zhou", "D. Zhao"], "venue": "pp. 5171\u20135178, 2015.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2015}, {"title": "D3: Deep Dual- Domain Based Fast Restoration of JPEG-Compressed Images", "author": ["Z. Wang", "D. Liu", "S. Chang", "Q. Ling", "T.S. Huang"], "venue": "IEEE Conf. Comput. Vis. Pattern Recognit., 2016.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2016}, {"title": "Building Dual-Domain Representations for Compression Artifacts Reduction", "author": ["J. Guo", "H. Chao"], "venue": "ECCV, 2016, pp. 628\u2013644.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2016}, {"title": "Deeply-Recursive Convolutional Network for Image Super-Resolution", "author": ["J. Kim", "J.K. Lee", "K.M. Lee"], "venue": "arXiv:1511.04491, 2015.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2015}, {"title": "Compression Artifacts Reduction by a Deep Convolutional Network", "author": ["C. Dong", "Y. Deng", "C.C. Loy", "X. Tang"], "venue": "2015 IEEE Int. Conf. Comput. Vis. IEEE, dec 2015, pp. 576\u2013584.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2015}, {"title": "Compression Artifacts Removal Using Convolutional Neural Networks", "author": ["P. Svoboda", "M. Hradis", "D. Barina", "P. Zemcik"], "venue": "J. WSCG, vol. 24, no. 2, pp. 63\u201372, 2016.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2016}, {"title": "LIVE image quality assessment database release 2", "author": ["H.R. Sheikh", "Z. Wang", "L. Cormack", "A.C. Bovik"], "venue": "2005.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2005}, {"title": "Learning Deconvolution Network for Semantic Segmentation", "author": ["H. Noh", "S. Hong", "B. Han"], "venue": "arXiv:1505.04366, vol. 1, 2015.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2015}, {"title": "Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "arXiv:1502.01852, 2015.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 1852}, {"title": "Visualizing and Understanding Convolutional Networks", "author": ["M.D. Zeiler", "R. Fergus"], "venue": "ECCV 2014, LNCS 8689, nov 2014, pp. 818\u2013833.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2014}, {"title": "Image quality assessment: From error visibility to structural similarity", "author": ["Z. Wang", "A.C. Bovik", "H.R. Sheikh", "E.P. Simoncelli"], "venue": "IEEE Trans. Image Process., vol. 13, no. 4, pp. 600\u2013612, 2004.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2004}, {"title": "Image information and visual quality", "author": ["H. Sheikh", "A. Bovik"], "venue": "IEEE Trans. Image Process., vol. 15, no. 2, pp. 430\u2013444, 2006.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2006}, {"title": "Digital Images and Human Vision", "author": ["B. Girod"], "venue": "A. B. Watson, Ed. Cambridge, MA, USA: MIT Press, 1993, ch. What\u2019s wro, pp. 207\u2013220.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 1993}, {"title": "Quality Assessment of Deblocked Images", "author": ["C. Yim", "A.C. Bovik"], "venue": "IEEE Trans. Image Process., vol. 20, no. 1, pp. 88\u201398, jan 2011.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2011}, {"title": "ImageNet: A large-scale hierarchical image database", "author": ["J. Deng", "W. Dong", "R. Socher", "L.-J. Li", "K. Li", "L. Fei-Fei"], "venue": "Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2009.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2009}, {"title": "Torch7: A Matlab-like Environment for Machine Learning", "author": ["R. Collobert"], "venue": "Adv. Neural Inf. Process. Syst. Work., 2011.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2011}, {"title": "cuDNN: Efficient Primitives for Deep Learning", "author": ["S. Chetlur", "C. Woolley", "P. Vandermersch", "J. Cohen", "J. Tran", "B. Catanzaro", "E. Shelhamer"], "venue": "arXiv:1410.0759, oct 2014.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 0, "context": "JPEG) [1].", "startOffset": 6, "endOffset": 9}, {"referenceID": 0, "context": "In practical uses, lossy compression schemes are often preferred on consumer devices for their much higher compression rate [1].", "startOffset": 124, "endOffset": 127}, {"referenceID": 1, "context": "These are not only unpleasant to see, but also have a negative impact on many low-level vision algorithms [2].", "startOffset": 106, "endOffset": 109}, {"referenceID": 2, "context": "Many compression algorithms rely on tiling the images into blocks, applying a sparsifying transform and re-quantization, followed by a generic loss-less data compression [3].", "startOffset": 170, "endOffset": 173}, {"referenceID": 3, "context": "JPEG has become the most widely accepted standard in lossy image compression [4], with many efficient software transcoders publicly available and specialized hardware accelerators deployed in many cameras.", "startOffset": 77, "endOffset": 80}, {"referenceID": 4, "context": "In such systems, the transmitting node is often battery-powered and thus heavily powerconstrained [5].", "startOffset": 98, "endOffset": 101}, {"referenceID": 2, "context": "Transmitting data is often the most expensive part in terms of energy, and strong compression can mitigate this by reducing the required transmit energy at the expense of introducing compression artifacts [3].", "startOffset": 205, "endOffset": 208}, {"referenceID": 5, "context": "Similar challenges are also seen in mobile devices storing data: size and cost constraints limit the amount of memory for data storage, and the energy available on such devices is depleted rapidly when writing to flash memory\u2014so much that it pays off to apply compression before writing to flash memory [6], [7].", "startOffset": 303, "endOffset": 306}, {"referenceID": 6, "context": "Similar challenges are also seen in mobile devices storing data: size and cost constraints limit the amount of memory for data storage, and the energy available on such devices is depleted rapidly when writing to flash memory\u2014so much that it pays off to apply compression before writing to flash memory [6], [7].", "startOffset": 308, "endOffset": 311}, {"referenceID": 2, "context": "On the processing site, these space and energy constraints are absent and much more computational power is available to decompress and possibly post-process the transmitted or stored images [3].", "startOffset": 190, "endOffset": 193}, {"referenceID": 7, "context": "Deep convolutional neural networks (ConvNets) have become an essential tool for computer vision, even exceeding human performance in tasks such as image classification [8], object detection [9], and semantic segmentation [10], [11].", "startOffset": 168, "endOffset": 171}, {"referenceID": 8, "context": "Deep convolutional neural networks (ConvNets) have become an essential tool for computer vision, even exceeding human performance in tasks such as image classification [8], object detection [9], and semantic segmentation [10], [11].", "startOffset": 190, "endOffset": 193}, {"referenceID": 9, "context": "Deep convolutional neural networks (ConvNets) have become an essential tool for computer vision, even exceeding human performance in tasks such as image classification [8], object detection [9], and semantic segmentation [10], [11].", "startOffset": 221, "endOffset": 225}, {"referenceID": 10, "context": "Deep convolutional neural networks (ConvNets) have become an essential tool for computer vision, even exceeding human performance in tasks such as image classification [8], object detection [9], and semantic segmentation [10], [11].", "startOffset": 227, "endOffset": 231}, {"referenceID": 11, "context": "In addition, they have also started to gain relevance for regression tasks in low-level image and video processing, computing saliency maps [12], optical flow fields [13] and single-image super-resolution images [14] with state-of-the-art performance.", "startOffset": 140, "endOffset": 144}, {"referenceID": 12, "context": "In addition, they have also started to gain relevance for regression tasks in low-level image and video processing, computing saliency maps [12], optical flow fields [13] and single-image super-resolution images [14] with state-of-the-art performance.", "startOffset": 166, "endOffset": 170}, {"referenceID": 13, "context": "In addition, they have also started to gain relevance for regression tasks in low-level image and video processing, computing saliency maps [12], optical flow fields [13] and single-image super-resolution images [14] with state-of-the-art performance.", "startOffset": 212, "endOffset": 216}, {"referenceID": 14, "context": "Various types of intelligent edgeaware denoising such as SA-DCT [15], [16], BM3D [17] have", "startOffset": 64, "endOffset": 68}, {"referenceID": 15, "context": "Various types of intelligent edgeaware denoising such as SA-DCT [15], [16], BM3D [17] have", "startOffset": 70, "endOffset": 74}, {"referenceID": 16, "context": "Various types of intelligent edgeaware denoising such as SA-DCT [15], [16], BM3D [17] have", "startOffset": 81, "endOffset": 85}, {"referenceID": 17, "context": "In recent years, dictionary-based sparse recovery algorithms such as DicTV [18], RTF [19], S-D2 [20], D [21], DDCN [22] have achieved outstanding results by directly addressing the deficiencies such as ringing and blocking very specific to JPEG.", "startOffset": 75, "endOffset": 79}, {"referenceID": 18, "context": "In recent years, dictionary-based sparse recovery algorithms such as DicTV [18], RTF [19], S-D2 [20], D [21], DDCN [22] have achieved outstanding results by directly addressing the deficiencies such as ringing and blocking very specific to JPEG.", "startOffset": 85, "endOffset": 89}, {"referenceID": 19, "context": "In recent years, dictionary-based sparse recovery algorithms such as DicTV [18], RTF [19], S-D2 [20], D [21], DDCN [22] have achieved outstanding results by directly addressing the deficiencies such as ringing and blocking very specific to JPEG.", "startOffset": 96, "endOffset": 100}, {"referenceID": 20, "context": "In recent years, dictionary-based sparse recovery algorithms such as DicTV [18], RTF [19], S-D2 [20], D [21], DDCN [22] have achieved outstanding results by directly addressing the deficiencies such as ringing and blocking very specific to JPEG.", "startOffset": 104, "endOffset": 108}, {"referenceID": 21, "context": "In recent years, dictionary-based sparse recovery algorithms such as DicTV [18], RTF [19], S-D2 [20], D [21], DDCN [22] have achieved outstanding results by directly addressing the deficiencies such as ringing and blocking very specific to JPEG.", "startOffset": 115, "endOffset": 119}, {"referenceID": 13, "context": "Several networks have shown to be very successful at this task, such as SRCNN [14] or DRCN [23].", "startOffset": 78, "endOffset": 82}, {"referenceID": 22, "context": "Several networks have shown to be very successful at this task, such as SRCNN [14] or DRCN [23].", "startOffset": 91, "endOffset": 95}, {"referenceID": 1, "context": "Recently, two important works have been published, which apply ConvNets for compression artifact suppression: ARCNN [2], [24] and the approach presented in [25].", "startOffset": 116, "endOffset": 119}, {"referenceID": 23, "context": "Recently, two important works have been published, which apply ConvNets for compression artifact suppression: ARCNN [2], [24] and the approach presented in [25].", "startOffset": 121, "endOffset": 125}, {"referenceID": 24, "context": "Recently, two important works have been published, which apply ConvNets for compression artifact suppression: ARCNN [2], [24] and the approach presented in [25].", "startOffset": 156, "endOffset": 160}, {"referenceID": 24, "context": "In [25] a residual structure extends the simple stacking of convolutional, non-linearity and pooling layers, such that the network is only trained to produce an increment compensating for the distortions.", "startOffset": 3, "endOffset": 7}, {"referenceID": 25, "context": "Testing of these networks was then performed on the LIVE1 dataset (29 images) [26] and, in case of AR-CNN, on the 5 test images of [15] and a self-collected dataset of 40 photographs from twitter as well.", "startOffset": 78, "endOffset": 82}, {"referenceID": 14, "context": "Testing of these networks was then performed on the LIVE1 dataset (29 images) [26] and, in case of AR-CNN, on the 5 test images of [15] and a self-collected dataset of 40 photographs from twitter as well.", "startOffset": 131, "endOffset": 135}, {"referenceID": 13, "context": "We start from the basic concept of training a deep ConvNet for a regression problem, as has been done for the related task of superresolution [14], [23] or other low-level computer vision operations such as optical flow estimation [13].", "startOffset": 142, "endOffset": 146}, {"referenceID": 22, "context": "We start from the basic concept of training a deep ConvNet for a regression problem, as has been done for the related task of superresolution [14], [23] or other low-level computer vision operations such as optical flow estimation [13].", "startOffset": 148, "endOffset": 152}, {"referenceID": 12, "context": "We start from the basic concept of training a deep ConvNet for a regression problem, as has been done for the related task of superresolution [14], [23] or other low-level computer vision operations such as optical flow estimation [13].", "startOffset": 231, "endOffset": 235}, {"referenceID": 24, "context": "The authors of [25] propose several new elements for artifact reduction ConvNets: A residual architecture, an edgeemphasized loss function, symmetric weight initialization, and skip connections.", "startOffset": 15, "endOffset": 19}, {"referenceID": 12, "context": "Taking inspiration from deep neural networks such as FlowNet [13] and FCN [10] developed for optical flow estimation and semantic segmentation respectively, we propose a neural network with hierarchical skip connections (cf.", "startOffset": 61, "endOffset": 65}, {"referenceID": 9, "context": "Taking inspiration from deep neural networks such as FlowNet [13] and FCN [10] developed for optical flow estimation and semantic segmentation respectively, we propose a neural network with hierarchical skip connections (cf.", "startOffset": 74, "endOffset": 78}, {"referenceID": 9, "context": "1We use the definition of full-convolution (also known as up-convolution, deconvolution, backwards convolution, or fractional-strided convolution) as described in [10], [27].", "startOffset": 163, "endOffset": 167}, {"referenceID": 26, "context": "1We use the definition of full-convolution (also known as up-convolution, deconvolution, backwards convolution, or fractional-strided convolution) as described in [10], [27].", "startOffset": 169, "endOffset": 173}, {"referenceID": 27, "context": "All these layers are followed by a Parametric Rectified Linear Unit (PReLU) [28] activation layer, where the slope for negative inputs is learned from data rather than pre-defined.", "startOffset": 76, "endOffset": 80}, {"referenceID": 27, "context": "These units have shown superior performance for ImageNet classification [28], reducing the issues of dead features [29].", "startOffset": 72, "endOffset": 76}, {"referenceID": 28, "context": "These units have shown superior performance for ImageNet classification [28], reducing the issues of dead features [29].", "startOffset": 115, "endOffset": 119}, {"referenceID": 24, "context": "We have found that learning a residual to the input image instead of the reconstructed image as suggested in previous work [25] did not improve the performance of the proposed ConvNet and thus do not include it in our network.", "startOffset": 123, "endOffset": 127}, {"referenceID": 23, "context": "Following [24], [25] with pixel values normalized to the", "startOffset": 10, "endOffset": 14}, {"referenceID": 24, "context": "Following [24], [25] with pixel values normalized to the", "startOffset": 16, "endOffset": 20}, {"referenceID": 0, "context": "range [0, 1], we use", "startOffset": 6, "endOffset": 12}, {"referenceID": 0, "context": "They are known to differ from perceived visual quality [1], [30]\u2013[32] but find wide-spread use due to their simplicity.", "startOffset": 55, "endOffset": 58}, {"referenceID": 29, "context": "They are known to differ from perceived visual quality [1], [30]\u2013[32] but find wide-spread use due to their simplicity.", "startOffset": 60, "endOffset": 64}, {"referenceID": 31, "context": "They are known to differ from perceived visual quality [1], [30]\u2013[32] but find wide-spread use due to their simplicity.", "startOffset": 65, "endOffset": 69}, {"referenceID": 29, "context": "A popular alternative is to use the structural similarity index (SSIM) [30], which is the mean of the product of three terms assessing similarity in luminance, contrast and structure over multiple localized windows.", "startOffset": 71, "endOffset": 75}, {"referenceID": 29, "context": "We use the Matlab implementation provided with [30] for evaluation and use the same parameters as related work [2], [24], [25]: K1 = 0.", "startOffset": 47, "endOffset": 51}, {"referenceID": 1, "context": "We use the Matlab implementation provided with [30] for evaluation and use the same parameters as related work [2], [24], [25]: K1 = 0.", "startOffset": 111, "endOffset": 114}, {"referenceID": 23, "context": "We use the Matlab implementation provided with [30] for evaluation and use the same parameters as related work [2], [24], [25]: K1 = 0.", "startOffset": 116, "endOffset": 120}, {"referenceID": 24, "context": "We use the Matlab implementation provided with [30] for evaluation and use the same parameters as related work [2], [24], [25]: K1 = 0.", "startOffset": 122, "endOffset": 126}, {"referenceID": 32, "context": "A third measure used in related work is the PSNR-B [33], which adds a (non-referenced) blocking effect factor (BEF) term to the MSE measure.", "startOffset": 51, "endOffset": 55}, {"referenceID": 24, "context": "In previous work, including an edge-emphasized term into the loss function has been proposed [25].", "startOffset": 93, "endOffset": 97}, {"referenceID": 1, "context": "Previous networks for compression artifact reduction were trained on the 400 train and test images of the BSDS500 dataset and tested on the 100 remaining validation images [2], [24], [25].", "startOffset": 172, "endOffset": 175}, {"referenceID": 23, "context": "Previous networks for compression artifact reduction were trained on the 400 train and test images of the BSDS500 dataset and tested on the 100 remaining validation images [2], [24], [25].", "startOffset": 177, "endOffset": 181}, {"referenceID": 24, "context": "Previous networks for compression artifact reduction were trained on the 400 train and test images of the BSDS500 dataset and tested on the 100 remaining validation images [2], [24], [25].", "startOffset": 183, "endOffset": 187}, {"referenceID": 24, "context": "The authors of [25] show that this is the limiting", "startOffset": 15, "endOffset": 19}, {"referenceID": 33, "context": "We thus use the large, widely-known and publicly available ImageNet 2013 detection dataset [35], which consists of 396k", "startOffset": 91, "endOffset": 95}, {"referenceID": 34, "context": "We use the Torch framework [36] with cuDNN v5.", "startOffset": 27, "endOffset": 31}, {"referenceID": 35, "context": "3 [37] for our evaluations.", "startOffset": 2, "endOffset": 6}, {"referenceID": 23, "context": "We use the same JPEG compressor as in AR-CNN [24] and Svoboda et al.", "startOffset": 45, "endOffset": 49}, {"referenceID": 24, "context": "[25] (i.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "791 SA-DCT [15] 28.", "startOffset": 11, "endOffset": 15}, {"referenceID": 1, "context": "809 AR-CNN [2] 29.", "startOffset": 11, "endOffset": 14}, {"referenceID": 24, "context": "823 L4 [25] 29.", "startOffset": 7, "endOffset": 11}, {"referenceID": 14, "context": "868 SA-DCT [15] 30.", "startOffset": 11, "endOffset": 15}, {"referenceID": 1, "context": "878 AR-CNN [2] 31.", "startOffset": 11, "endOffset": 14}, {"referenceID": 24, "context": "890 L4 [25] 31.", "startOffset": 7, "endOffset": 11}, {"referenceID": 24, "context": "890 L8 [25] 31.", "startOffset": 7, "endOffset": 11}, {"referenceID": 14, "context": "917 SA-DCT [15] 32.", "startOffset": 11, "endOffset": 15}, {"referenceID": 1, "context": "924 AR-CNN [2] 33.", "startOffset": 11, "endOffset": 14}, {"referenceID": 24, "context": "931 L4 [25] 33.", "startOffset": 7, "endOffset": 11}, {"referenceID": 24, "context": "19 dB over the L8 network [25], 0.", "startOffset": 26, "endOffset": 30}, {"referenceID": 24, "context": "The lighthouse3 image serves as a basis for this comparison and is the same one used in [25].", "startOffset": 88, "endOffset": 92}], "year": 2016, "abstractText": "Lossy image compression algorithms are pervasively used to reduce the size of images transmitted over the web and recorded on data storage media. However, we pay for their high compression rate with visual artifacts degrading the user experience. Deep convolutional neural networks have become a widespread tool to address high-level computer vision tasks very successfully. Recently, they have found their way into the areas of low-level computer vision and image processing to solve regression problems mostly with relatively shallow networks. We present a novel 12-layer deep convolutional network for image compression artifact suppression with hierarchical skip connections and a multi-scale loss function. We achieve a boost of up to 1.79 dB in PSNR over ordinary JPEG and an improvement of up to 0.36 dB over the best previous ConvNet result. We show that a network trained for a specific quality factor (QF) is resilient to the QF used to compress the input image\u2014a single network trained for QF 60 provides a PSNR gain of more than 1.5 dB over the wide QF range from 40 to 76.", "creator": "LaTeX with hyperref package"}}}