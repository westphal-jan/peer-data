{"id": "1611.07476", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Nov-2016", "title": "Singularity of the Hessian in Deep Learning", "abstract": "We look at the eigenvalues of the Hessian of a loss function before and after training. The eigenvalue distribution is seen to be composed of two parts, the bulk which is concentrated around zero, and the edges which are scattered away from zero. We present empirical evidence for the bulk indicating how over-parametrized the system is, and for the edges indicating the complexity of the input data.", "histories": [["v1", "Tue, 22 Nov 2016 19:24:49 GMT  (4520kb,D)", "http://arxiv.org/abs/1611.07476v1", "ICLR 2017 Submission on Nov 4, 2016"], ["v2", "Thu, 5 Oct 2017 13:28:50 GMT  (2514kb,D)", "http://arxiv.org/abs/1611.07476v2", "ICLR submission, 2016 - updated to match the openreview.net version"]], "COMMENTS": "ICLR 2017 Submission on Nov 4, 2016", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["levent sagun", "leon bottou", "yann lecun"], "accepted": false, "id": "1611.07476"}, "pdf": {"name": "1611.07476.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["DEEP LEARNING", "Levent Sagun"], "emails": ["sagun@cims.nyu.edu", "leon@bottou.org", "yann@cs.nyu.edu"], "sections": [{"heading": null, "text": "We look at the eigenvalues of the Hessian of a loss function before and after training. The eigenvalues distribution is considered to consist of two parts, the majority concentrated around zero and the edges scattered away from zero. We present empirical evidence for the majority, which indicates how over-parameterized the system is, and for the edges, which indicate the complexity of the input data."}, {"heading": "1 INTRODUCTION", "text": "Considering a (piecemeal) differentiable loss function and a gradient-based algorithm to minimize it, knowledge of second-order information about what the landscape looks like and how we could modify our algorithm to make it faster and find better solutions depends quite a bit. However, one of the biggest challenges with second-order optimization methods is accessing this second-order information itself. Especially in the area of deep learning, there have been many suggestions for speeding up the training using second-order information. Ngiam et al. (2011) has an in-depth review of some of the suggestions for approximating the Hessian to the loss function. Nevertheless, given the computational complexity of the problems at hand, we do not have much information about what the actual Hessian looks like. This work is part of a series of papers linking the data model algorithms with Sagun et al. (2014; 2015) and it builds on the intuition used in Lecun al. (1998) The totality of the architectural data we depend on the concentration of the data."}, {"heading": "2 MNIST EIGENVALUES ON A FULLY-CONNECTED MODEL", "text": "We calculate the exact Hessian loss function of a network with two hidden layers. The inputs consist of 28 \u0445 28 MNIST data, the network has two hidden layers with ReLU nonlinearity, the top layer has a softmax and a negative log probability loss function at the end. We record the histogram of Hessian eigenvalues for a different number of hidden units. Hessian is extremely unique, and increasing the number of units in hidden layers only contributes to the singularity of Hessian. ar Xiv: 161 1,07 476v 1 [cs.L G] 22 Nov 201 6"}, {"heading": "2.1 VARYING THE DATA", "text": "To show how the eigenvalue distribution can depend on the data itself, we maintain the same architecture and change the input into random patterns. First, a random point in the weight space is selected, and we calculate the Hesse without any training. After the training, the system goes so far that the norm of the gradient is close to zero. We calculate the exact Hesse again and draw the histogram of its eigenvalues."}, {"heading": "3 A SIMPLER CASE", "text": "In this section, we will repeat the same experiment in two-dimensional data, in an attempt to better understand the connection between the data and the spectrum of Hesse. We create two Gaussian blobs, centered on (1, 1) and (\u2212 1, \u2212 1), and first, we keep the standard deviation equal, and increase the network size. At the end of the training, the norm of gradient is in the order of 10 \u2212 4. There are two eigenvalues that are isolated, and away from the mass of the spectrum, combined with a negative log probability loss function. We train the system with descending gradient with constant increment size. At the end of the training, the norm of gradient is in the order of 10 \u2212 4. There are two eigenvalues that are isolated, and away from the mass of the spectrum. The increase in the network only adds the concentration of eigenvalues at and around zero. To give an insight into the way the Hessians themselves look, we complete the three systems mentioned above."}, {"heading": "4 CONCLUSION", "text": "Such a step was taken in Panageas & Piliouras (2016) to relax the isolated singularity state adopted in Lee et al. (2016). \u2022 There are still negative eigenvalues, even if they are small, suggesting that we can go beyond the classical concepts of basins when exploring the energy landscapes of loss functions. \u2022 The next obvious question is how to find low energy paths between such landscapes, even if they are small."}, {"heading": "ACKNOWLEDGMENTS", "text": "We would like to thank Afonso Bandeira, Yann Dauphin, Ruoyu Sun, Arthur Szlam and Soumith Chintala for valuable discussions. Part of the research was carried out when the first author was an intern at FAIR."}], "references": [{"title": "Train faster, generalize better: Stability of stochastic gradient descent", "author": ["Moritz Hardt", "Benjamin Recht", "Yoram Singer"], "venue": "arXiv preprint arXiv:1509.01240,", "citeRegEx": "Hardt et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hardt et al\\.", "year": 2015}, {"title": "Efficient backprop", "author": ["Yann Lecun", "Leon Bottou", "Genevieve B Orr", "Klaus-Robert M\u00fcller"], "venue": null, "citeRegEx": "Lecun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Lecun et al\\.", "year": 1998}, {"title": "Gradient descent converges to minimizers", "author": ["Jason D Lee", "Max Simchowitz", "Michael I Jordan", "Benjamin Recht"], "venue": "University of California, Berkeley,", "citeRegEx": "Lee et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Lee et al\\.", "year": 2016}, {"title": "On optimization methods for deep learning", "author": ["Jiquan Ngiam", "Adam Coates", "Ahbik Lahiri", "Bobby Prochnow", "Quoc V Le", "Andrew Y Ng"], "venue": "In Proceedings of the 28th International Conference on Machine Learning", "citeRegEx": "Ngiam et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Ngiam et al\\.", "year": 2011}, {"title": "Gradient descent only converges to minimizers: Nonisolated critical points and invariant regions", "author": ["Ioannis Panageas", "Georgios Piliouras"], "venue": "arXiv preprint arXiv:1605.00405,", "citeRegEx": "Panageas and Piliouras.,? \\Q2016\\E", "shortCiteRegEx": "Panageas and Piliouras.", "year": 2016}, {"title": "Explorations on high dimensional landscapes", "author": ["Levent Sagun", "V U\u011fur G\u00fcney", "G\u00e9rard Ben Arous", "Yann LeCun"], "venue": "ICLR 2015 Workshop Contribution,", "citeRegEx": "Sagun et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sagun et al\\.", "year": 2014}, {"title": "Universality in halting time and its applications in optimization", "author": ["Levent Sagun", "Thomas Trogdon", "Yann LeCun"], "venue": "arXiv preprint arXiv:1511.06444,", "citeRegEx": "Sagun et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sagun et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 2, "context": "Recent research suggest new insights into convergence properties of gradient based algorithms in non-convex systems (Lee et al., 2016; Hardt et al., 2015).", "startOffset": 116, "endOffset": 154}, {"referenceID": 0, "context": "Recent research suggest new insights into convergence properties of gradient based algorithms in non-convex systems (Lee et al., 2016; Hardt et al., 2015).", "startOffset": 116, "endOffset": 154}, {"referenceID": 0, "context": "Ngiam et al. (2011) has an in depth review of some of the proposals for approximating the Hessian of the loss function.", "startOffset": 0, "endOffset": 20}, {"referenceID": 0, "context": "(2014; 2015) and it builds on top of the intuition developed in Lecun et al. (1998). In this short note, we show how the data and the architecture depends on the eigenvalues of the Hessian of the loss function.", "startOffset": 64, "endOffset": 84}, {"referenceID": 2, "context": "One such step has been taken in Panageas & Piliouras (2016) in relaxing the isolated singularity condition that was assumed in Lee et al. (2016). From a practical point of view this has multiple implications: \u2022 The landscape may be flat beyond the notion of wide basins.", "startOffset": 127, "endOffset": 145}], "year": 2016, "abstractText": "We look at the eigenvalues of the Hessian of a loss function before and after training. The eigenvalue distribution is seen to be composed of two parts, the bulk which is concentrated around zero, and the edges which are scattered away from zero. We present empirical evidence for the bulk indicating how overparametrized the system is, and for the edges indicating the complexity of the input data.", "creator": "LaTeX with hyperref package"}}}