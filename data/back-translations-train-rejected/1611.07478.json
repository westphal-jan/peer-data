{"id": "1611.07478", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Nov-2016", "title": "An unexpected unity among methods for interpreting model predictions", "abstract": "Understanding why a model made a certain prediction is crucial in many data science fields. Interpretable predictions engender appropriate trust and provide insight into how the model may be improved. However, with large modern datasets the best accuracy is often achieved by complex models even experts struggle to interpret, which creates a tension between accuracy and interpretability. Recently, several methods have been proposed for interpreting predictions from complex models by estimating the importance of input features. Here, we present how a model-agnostic additive representation of the importance of input features unifies current methods. This representation is optimal, in the sense that it is the only set of additive values that satisfies important properties. We show how we can leverage these properties to create novel visual explanations of model predictions. The thread of unity that this representation weaves through the literature indicates that there are common principles to be learned about the interpretation of model predictions that apply in many scenarios.", "histories": [["v1", "Tue, 22 Nov 2016 19:30:28 GMT  (1028kb,D)", "https://arxiv.org/abs/1611.07478v1", "Short extended abstract"], ["v2", "Wed, 23 Nov 2016 06:44:36 GMT  (1224kb,D)", "http://arxiv.org/abs/1611.07478v2", "Presented at NIPS 2016 Workshop on Interpretable Machine Learning in Complex Systems"], ["v3", "Thu, 8 Dec 2016 08:24:15 GMT  (1224kb,D)", "http://arxiv.org/abs/1611.07478v3", "Presented at NIPS 2016 Workshop on Interpretable Machine Learning in Complex Systems"]], "COMMENTS": "Short extended abstract", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["scott lundberg", "su-in lee"], "accepted": false, "id": "1611.07478"}, "pdf": {"name": "1611.07478.pdf", "metadata": {"source": "CRF", "title": "An unexpected unity among methods for interpreting model predictions", "authors": ["Scott M. Lundberg"], "emails": ["slund1@cs.washington.edu", "suinlee@cs.washington.edu"], "sections": [{"heading": "An unexpected unity among methods for interpreting model predictions", "text": "Scott M. Lundberg University of Washingtonslund1 @ cs.washington.eduSu-In Lee University of Washingtonsuinlee @ cs.washington.eduUnderstanding why a model has made a particular prediction is critical in many areas of data science. Interpretable predictions generate reasonable confidence and provide insight into how the model can be improved. However, in large modern datasets, the best accuracy is often achieved by complex models that even experts struggle to interpret, creating a tension between accuracy and interpretability. Recently, several methods have been proposed for interpreting predictions from complex models by estimating the importance of input functions. Here, we present how a model agnostic addictive representation of the meaning of input functions combines current methods. This representation is optimal as it is the only set of additive values that fulfills important properties. We show how we can use these properties to create novel visual explanations of models."}, {"heading": "Introduction", "text": "This year, it has come to the point that it will be able to put itself at the top, \"he said in an interview with the German Press Agency.\" We have never hesitated so long, \"he said,\" but we are not yet able to do it as if we were able to do it. \""}, {"heading": "Expectation Shapley values and LIME", "text": "To understand why a model made a prediction, one needs to understand how a series of interpretable model inputs contributed to the prediction. (The original x-RP inputs can be difficult for a user to interpret, so a transformation to a new set of x-interpretable inputs is often required.) ES values contain x-hx (x), a binary vector of length M that represents when an input (or group of values) is known or missing. This mapping hx takes an arbitrary input space and converts it into an interpretable binary vector of the presence of features. For example, if the model is word input, x-hx could be a binary vector of our knowledge of word presence vs. absence. If the model is a vector of real evaluation measurements, x-horizontally could be a binary vector representing a group of measurements."}, {"heading": "Expectation Shapley values and DeepLIFT", "text": "DeepLIFT calculates the effects of input on the output of compositional models such as deep neural networks. The influence of input xj on model output y is denoted by Cxjy and f (x (0) + P \u2211 j = 1 Cxjy = f (x) (7), where x (0) is a \"reference input\" designed to represent typical input values. ES value implementations approach the effects of missing data taking into account expectations, so when interpreting x (0) as an estimate of E [x], DeepLIFT is an additive model of the same shape as ES values. To enable an efficient recursive calculation of Cxjy DeepLIFT, DeepLIFT assumes a linear composition rule that is equivalent to the linearization of the non-linear components of the neural network. Its backpropagation rules, which define how each component is linear, but we can interpret it as an intuitive WES component, are intuitive for us to interpret it."}, {"heading": "Visualization of Expectation Shapley values", "text": "The width of the segment corresponds to the ES value \u03c6i. Red bar segments correspond to inputs where \u03c6i > 0, and blue segments with inputs where \u03c6i < 0. The model output starts at the base \u03c60 = f (\u2205) in the middle and is then pushed to the right by the red bars in proportion to its length, or to the left by the blue bars. The final position of the model output is then equal to f (x) = \u2211 M i = 0 \u03c6i. While the explanation of a single prediction is very useful, we often want to understand how a model works across a dataset. To make this possible, we have designed a visualization based on rotating the individual forecast representation (Figure 3A) by 90 \u0445, and then stacking many horizontal predictions. By arranging the predictions by explaining the similarity, we can simply assume that the most popular gradient patterns for the individual, Figure 3A, are most interesting."}, {"heading": "Sample Efficiency and the Importance of the Shapley Kernel", "text": "Combining Shapley values from game theory with locally weighted linear models brings benefits to both concepts. Shapley values can be estimated more efficiently, and locally weighted linear models gain a theoretical justification for their weight core. Here, we briefly illustrate both the efficiency improvement for Shapley values and the importance of core selection for locally weighted linear models (Figure 4). Shapley values are classically defined by the effects of a feature when it is added to features that precede it in an order. Shapley value for this feature is the average effect across all possible arrangements: \u03c6i (f, x) = 1P!"}], "references": [{"title": "On pixel-wise explanations for non-linear classifier decisions by layerwise relevance propagation", "author": ["Sebastian Bach"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2015}, {"title": "Analysis of regression in game theory approach", "author": ["Stan Lipovetsky", "Michael Conklin"], "venue": "Applied Stochastic Models in Business and Industry", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2001}, {"title": " Why Should I Trust You?\": Explaining the Predictions of Any Classifier", "author": ["Marco Tulio Ribeiro", "Sameer Singh", "Carlos Guestrin"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2016}, {"title": "The Shapley value: essays in honor of Lloyd S", "author": ["Alvin E Roth"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1988}, {"title": "Not Just a Black Box: Learning Important Features Through Propagating Activation Differences", "author": ["Avanti Shrikumar"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2016}, {"title": "Explaining prediction models and individual predictions with feature contributions", "author": ["Erik \u0160trumbelj", "Igor Kononenko"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2014}, {"title": "Monotonic solutions of cooperative games", "author": ["H Peyton Young"], "venue": "Journal of Game Theory", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1985}], "referenceMentions": [{"referenceID": 2, "context": "Recent model-agnostic methods leverage this property by summarizing the behaviour of the complex models only with respect to a single prediction [3, 6].", "startOffset": 145, "endOffset": 151}, {"referenceID": 5, "context": "Recent model-agnostic methods leverage this property by summarizing the behaviour of the complex models only with respect to a single prediction [3, 6].", "startOffset": 145, "endOffset": 151}, {"referenceID": 5, "context": "Here, we extend a prediction explanation method based on game theory, specifically on the Shapley value, which describes a way to distribute the total gains to players, assuming they all collaborate [6].", "startOffset": 199, "endOffset": 202}, {"referenceID": 2, "context": "Intriguingly, ES values connect with and motivate several other current prediction explanation methods: LIME is a method for interpreting individual model predictions based on locally approximating the model around a given prediction [3].", "startOffset": 234, "endOffset": 237}, {"referenceID": 2, "context": "(2016) [3] can be viewed as approximations of ES values with a different weighting kernel defining locality.", "startOffset": 7, "endOffset": 10}, {"referenceID": 4, "context": "DeepLIFT was recently proposed as a recursive prediction explanation method for deep learning [5].", "startOffset": 94, "endOffset": 97}, {"referenceID": 0, "context": "Layer-wise relevance propagation is another method for interpreting the predictions of compositional models, such as deep learning [1].", "startOffset": 131, "endOffset": 134}, {"referenceID": 4, "context": "[5].", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "Shapley regression values are an approach to computing feature importance in the presence of multicollinearity [2].", "startOffset": 111, "endOffset": 114}, {"referenceID": 2, "context": "as finding an interpretable local model \u03be that minimizes the following objective function [3]:", "startOffset": 90, "endOffset": 93}, {"referenceID": 6, "context": "In 1985, Peyton Young demonstrated that there is only one set of values that satisfies the above assumptions and they are the Shapley values [7, 4].", "startOffset": 141, "endOffset": 147}, {"referenceID": 3, "context": "In 1985, Peyton Young demonstrated that there is only one set of values that satisfies the above assumptions and they are the Shapley values [7, 4].", "startOffset": 141, "endOffset": 147}, {"referenceID": 2, "context": "This optimality of ES values holds over a large class of possible models, including the examples used in the LIME paper that originally proposed this formalism [3].", "startOffset": 160, "endOffset": 163}, {"referenceID": 5, "context": "This leads to a natural estimation approach which involves taking the average over a small sample of all orderings [6].", "startOffset": 115, "endOffset": 118}], "year": 2016, "abstractText": "Understanding why a model made a certain prediction is crucial in many data science fields. Interpretable predictions engender appropriate trust and provide insight into how the model may be improved. However, with large modern datasets the best accuracy is often achieved by complex models even experts struggle to interpret, which creates a tension between accuracy and interpretability. Recently, several methods have been proposed for interpreting predictions from complex models by estimating the importance of input features. Here, we present how a model-agnostic additive representation of the importance of input features unifies current methods. This representation is optimal, in the sense that it is the only set of additive values that satisfies important properties. We show how we can leverage these properties to create novel visual explanations of model predictions. The thread of unity that this representation weaves through the literature indicates that there are common principles to be learned about the interpretation of model predictions that apply in many scenarios.", "creator": "LaTeX with hyperref package"}}}