{"id": "1611.07490", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Nov-2016", "title": "Can Co-robots Learn to Teach?", "abstract": "We explore beyond existing work on learning from demonstration by asking the question: Can robots learn to teach?, that is, can a robot autonomously learn an instructional policy from expert demonstration and use it to instruct or collaborate with humans in executing complex tasks in uncertain environments? In this paper we pursue a solution to this problem by leveraging the idea that humans often implicitly decompose a higher level task into several subgoals whose execution brings the task closer to completion. We propose Dirichlet process based non-parametric Inverse Reinforcement Learning (DPMIRL) approach for reward based unsupervised clustering of task space into subgoals. This approach is shown to capture the latent subgoals that a human teacher would have utilized to train a novice. The notion of action primitive is introduced as the means to communicate instruction policy to humans in the least complicated manner, and as a computationally efficient tool to segment demonstration data. We evaluate our approach through experiments on hydraulic actuated scaled model of an excavator and evaluate and compare different teaching strategies utilized by the robot.", "histories": [["v1", "Tue, 22 Nov 2016 19:56:27 GMT  (1038kb,D)", "http://arxiv.org/abs/1611.07490v1", "9 pages, conference"]], "COMMENTS": "9 pages, conference", "reviews": [], "SUBJECTS": "cs.RO cs.LG", "authors": ["harshal maske", "emily kieson", "girish chowdhary", "charles abramson"], "accepted": false, "id": "1611.07490"}, "pdf": {"name": "1611.07490.pdf", "metadata": {"source": "CRF", "title": "Can Co-robots Learn to Teach?", "authors": ["Harshal Maske", "Emily Kieson", "Girish Chowdhary", "Charles Abramson"], "emails": ["Champaign@illinois.edu,", "hmaske2@illinois.edu,", "girishc@illinois.edu}.", "{kieson@okstate.edu,", "charles.abramson@okstate.edu}."], "sections": [{"heading": null, "text": "The fact is that we are able to change and change the world, and that we are able to change the world in order to change it."}, {"heading": "II. BACKGROUND", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. Learning from Demonstration", "text": "Much of the existing work in LfD has focused on the problem of enabling the robot to learn a closed-loop policy to perform a series of actions on its own. LfD has been successful in teaching robot tennis wings [20], gaits [12] and complex helicopter maneuvers [1]. A review of various LfD techniques is available in [2]. In a subset of methods called Inverse Reinforcement Learning (IRL), the reward function is learned from a series of expert demonstrations [1]. Learning reward is argued by [11] to be equivalent to a high-level description of the task, which explains the expert's behavior in a richer sense than politics alone. Some recent work in LfD has focused on the automatic segmentation of demonstrations into easier reusable primitives [8], [5], [3], [15]."}, {"heading": "B. Bayesian Nonparametric Inverse Reinforcement Learning", "text": "This is an approach to automatically break down the reward function into a set of goals that were considered local reward functions, but it has been proposed [11]. We use the term subgoals execution in a particular sequence to perform a task as a key component of the guide model, based on research that deals with human expertise in complex environments [16], [6]. According to these results, people often form implicit decompositions of higher tasks into multiple subgoals, so that the execution of each subgoal brings the task closer to completion. In IRL [1] a Markov Decision Process (MDP) without the reward function R (s) i.e. MDP\\ R is a demonstration of state action, O = {s1, a1)."}, {"heading": "C. Dirichlet Process Means for Gaussian mixture model", "text": "The Dirichlet process (DP) is a well-known precursor to the parameters of a Gaussian mixing model when the number of mixing components a priori is not known. Kulis and Jordan [9] recently showed that the Gibbs sampling algorithm for the Dirichlet process mix approximates a hard cluster algorithm when the covariances of the Gaussian variables tend to zero. This results in a k-middle-like cluster lens which is calculated by min \u2211 k = 1 x x x x x x x x x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-c-x-x-x-x-x-x-x-c-x-c-x-c-x-x-c-x-c-x-x-x-x-x-x-x-x-x-x-x-x-x-c-c-c-c-x-c-c-x-x-x-x-c-x-x-c-x-x-x-x-x-x-x-x-x-x-x-x-c-c-c-c-c-c-c-x-x-x-x-c-x-x-x-x-x-x-x-c-x-x-x-x-x-x-x-x-x-x-x-x-c-c-c-c-c-c-c-c-c-c-c-x-x-x-x-x-x-x-c-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-c-c-c-c-c-c-c-c-c-c-c-c-c-c-c-c-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x"}, {"heading": "III. METHODOLOGY", "text": "In the LfD literature, the Dynamic Movement Primitive (DMP) Framework [7] is a predominant approach to mimicking demonstrations. Given the starting and finishing position in the task space, a robot uses the DMP to generate the trajectory (or statuses) for the position of the end effector and its mechanism, which is then mapped to a suitable pivot angle (or a series of actions a) using inverse kinematics. [17] We are interested in creating an instruction policy for operators who perform the task for which DMPs encoded in relation to the trajectory may not be the best solution. Tracking a particular trajectory (i.e. the amount of states) in 3-D space is a complex task for an operator and can therefore present difficulties in teaching. On the other hand, instructions regarding joystick movements are easily understandable to the operator and are probably the easiest means to communicate desired actions to operators."}, {"heading": "A. Action Primitives", "text": "The definition of action primitives is developed for articulated robots, but should clearly be generalizable to other types of robots. For an articulated robot, a revolutionary joint provides a uniaxial rotation function, so a articulated robot can assume three generally defined possible states: i) anti-clockwise rotation, ii) stationary or non-zero noise disturbance, or iii) clockwise rotation. Note that it is possible to add further resolution to this decomposition. Let rj be the variable that takes values from the set {1, 2, 3} corresponding to these states. Thus, rj = 1 implies that the articulated unit j rotates counter-clockwise and so on. Assigning values {1, 2, 3} is arbitrary. Mathematically, an action primitive defines the action of a robot in relation to variable actions."}, {"heading": "B. Action Primitive based Segmentation of Task demos", "text": "We now have to deal with how action bases can be used for the segmentation algorithms. Classifying action bases into action bases generates the necessary segmentation of action bases. Each segment is used as action basis for the segmentation algorithms. But first, we have to define the distribution for each action basis. Recall that an action basis is defined by the value of its variable rj for each action basis."}, {"heading": "D. Task-come-Instruction Model", "text": "This analogy is used as a building block for the instruction model that can be used by a robot to instruct or guide the human operator. Measures primitive based segmentation plays an important role in such a model, such as the transition between these action primitives results in a transfer from one sub-target to another. Further action primitives can be easily communicated to the human operator as they represent the activation of one or more actuators, which in turn are driven by specific joysticks. An example of action primitives and their corresponding communication in relation to joystick movements is presented via a simple graphical interface. We are now working out the model construction that is autonomously based on the sub-targets achieved using DPMIRL and the segments of this task based on it."}, {"heading": "IV. EXPERIMENTS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. Test Platform", "text": "To test the developed approach, exhaustive experiments were conducted on a 345D Wedico excavator model with a 4 d.o.f robotic arm controlled by a radio transmitter as shown in Figure 5. This model was constructed by Wedico, Germany (http: / / www.wedico.de /), which specializes in true-to-scale and fully functional hydraulic construction equipment. Ideally, the operator and the scoreboard shown in Figure 5 would be placed inside the excavator, but this was left for future work. However, the results reported here should be generalized and also directly applicable to tele-driven robotics. The Wedico excavator lacked articulated angle sensors and internal receipt, so all experiments were carried out within a motion detection system to ensure real-time data entry to the algorithm. In our experiments, we record the positions and speeds of the four actuators."}, {"heading": "B. Learning Instruction Policy", "text": "In fact, the fact is that we are able to hide, and that we are able to hide, we are going to be able to hide, \"he said."}, {"heading": "C. Testing Instruction Policy Model", "text": "We tested the effectiveness of the proposed instruction model by dividing newcomers into performing truck loading tasks in novel configurations.The process of instructing a human operator while performing a task is illustrated in Figure 1 (in green).At a given time, the task space configuration, consisting of end effector position, dirt pile and truck position, is made available to the co-robot. Co-robot then generates the action Primitive Zi\u03c4 using the instructive policy model based on the previous action Primitive Zi\u03c4 \u2212 1 (calculated from the common position history) and the most likely current objective Xi (subtarget closest to the current end effector position) using equations (10).Figure 4 shows the two different instruction strategies used by the robot to communicate the action primitively to the human operator."}, {"heading": "V. CONCLUSION", "text": "The main contribution of the presented paper is a generalizable and scalable technique that allows the robot to learn a teaching policy directly from expert demonstrations. The main advantage of using action primitives, which are typically parameterized in the trajectory, is that action primitives are easier to explain to the robot than complex multi-degree freedom robots. In addition, action primitives can be used as building blocks for a variety of similar tasks that use the same basic actions and can be generalized to robots with different scales but the same common space, like other excavators with different boom, bucket and arm lengths. Action primitives can also be used as building blocks for a variety of similar tasks that use the same basic actions and are transferred to robots with different scales but the same common space."}], "references": [{"title": "Apprenticeship learning via inverse reinforcement learning", "author": ["Pieter Abbeel", "Andrew Y Ng"], "venue": "In Proceedings of the twenty-first international conference on Machine learning,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2004}, {"title": "A survey of robot learning from demonstration", "author": ["Brenna D Argall", "Sonia Chernova", "Manuela Veloso", "Brett Browning"], "venue": "Robotics and Autonomous Systems,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2009}, {"title": "Learning from demonstration using a multi-valued function regressor for time-series data", "author": ["Jesse Butterfield", "Sarah Osentoski", "Graylin Jay", "Odest Chadwicke Jenkins"], "venue": "In 2010 10th IEEE-RAS International Conference on Humanoid Robots,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2010}, {"title": "Joint modeling of multiple related time series via the beta process", "author": ["Emily B Fox", "Erik B Sudderth", "Michael I Jordan", "Alan S Willsky"], "venue": "arXiv preprint arXiv:1111.4226,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2011}, {"title": "Incremental learning of subtasks from unsegmented demonstration", "author": ["Daniel H Grollman", "Odest Chadwicke Jenkins"], "venue": "In Intelligent Robots and Systems (IROS),", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2010}, {"title": "The development of human expertise in a complex environment", "author": ["Michael Harr\u00e9", "Terry Bossomaier", "Allan Snyder"], "venue": "Minds and Machines,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2011}, {"title": "Movement imitation with nonlinear dynamical systems in humanoid robots", "author": ["Auke Jan Ijspeert", "Jun Nakanishi", "Stefan Schaal"], "venue": "In Robotics and Automation,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2002}, {"title": "Robot learning from demonstration by constructing skill trees", "author": ["George Konidaris", "Scott Kuindersma", "Roderic Grupen", "Andrew Barto"], "venue": "The International Journal of Robotics Research,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2011}, {"title": "Revisiting k-means: New algorithms via bayesian nonparametrics", "author": ["Brian Kulis", "Michael I Jordan"], "venue": "In Proceedings of the 29th International Conference on Machine Learning", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2012}, {"title": "Scalable reward learning from demonstration", "author": ["Bernard Michini", "Mark Cutler", "Jonathan P How"], "venue": "In Robotics and Automation (ICRA),", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2013}, {"title": "Bayesian nonparametric reward learning from demonstration", "author": ["Bernard Michini", "Thomas J Walsh", "Ali-Akbar Agha-Mohammadi", "Jonathan P How"], "venue": "IEEE Transactions on Robotics,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2015}, {"title": "Learning from demonstration and adaptation of biped locomotion", "author": ["Jun Nakanishi", "Jun Morimoto", "Gen Endo", "Gordon Cheng", "Stefan Schaal", "Mitsuo Kawato"], "venue": "Robotics and Autonomous Systems,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2004}, {"title": "Apprenticeship learning using inverse reinforcement learning and gradient methods", "author": ["Gergely Neu", "Csaba Szepesv\u00e1ri"], "venue": "arXiv preprint arXiv:1206.5264,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2012}, {"title": "Incremental semantically grounded learning from demonstration", "author": ["Scott Niekum", "Sachin Chitta", "Andrew G Barto", "Bhaskara Marthi", "Sarah Osentoski"], "venue": "In Robotics: Science and Systems,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2013}, {"title": "Learning and generalization of complex tasks from unstructured demonstrations", "author": ["Scott Niekum", "Sarah Osentoski", "George Konidaris", "Andrew G Barto"], "venue": "In Intelligent Robots and Systems (IROS),", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2012}, {"title": "Problem solving and human expertise", "author": ["T.J. Nokes", "C.D. Schunn", "Michelene T.H. Chi"], "venue": "International Encyclopedia of Education,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2010}, {"title": "Learning and generalization of motor skills by learning from demonstration", "author": ["Peter Pastor", "Heiko Hoffmann", "Tamim Asfour", "Stefan Schaal"], "venue": "In Robotics and Automation,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2009}, {"title": "Bayesian inverse reinforcement learning", "author": ["Deepak Ramachandran", "Eyal Amir"], "venue": "Urbana, 51(61801):1\u20134,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2007}, {"title": "Maximum margin planning", "author": ["Nathan D Ratliff", "J Andrew Bagnell", "Martin A Zinkevich"], "venue": "In Proceedings of the 23rd international conference on Machine learning,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2006}, {"title": "Dynamic movement primitives-a framework for motor control in humans and humanoid robotics", "author": ["Stefan Schaal"], "venue": "In Adaptive Motion of Animals and Machines,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2006}, {"title": "A game-theoretic approach to apprenticeship learning", "author": ["Umar Syed", "Robert E Schapire"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2007}, {"title": "Maximum entropy inverse reinforcement learning", "author": ["Brian D Ziebart", "Andrew L Maas", "J Andrew Bagnell", "Anind K Dey"], "venue": "In AAAI,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2008}], "referenceMentions": [{"referenceID": 1, "context": "Learning from Demonstration has been widely studied in the context of robots learning to do a task from teacher demonstrations [2].", "startOffset": 127, "endOffset": 130}, {"referenceID": 19, "context": "LfD has been successful in teaching robots tennis swings [20], walking gaits [12], and complex helicopter maneuvers [1].", "startOffset": 57, "endOffset": 61}, {"referenceID": 11, "context": "LfD has been successful in teaching robots tennis swings [20], walking gaits [12], and complex helicopter maneuvers [1].", "startOffset": 77, "endOffset": 81}, {"referenceID": 0, "context": "LfD has been successful in teaching robots tennis swings [20], walking gaits [12], and complex helicopter maneuvers [1].", "startOffset": 116, "endOffset": 119}, {"referenceID": 1, "context": "A review of various LfD techniques is available in [2].", "startOffset": 51, "endOffset": 54}, {"referenceID": 0, "context": "In a subset of methods called Inverse Reinforcement Learning (IRL), the reward function is learned from a set of expert demonstrations [1].", "startOffset": 135, "endOffset": 138}, {"referenceID": 10, "context": "Learning reward is argued by [11] to be equivalent to high-level description of the task, that explains the expert\u2019s behavior in a richer sense than the policy alone.", "startOffset": 29, "endOffset": 33}, {"referenceID": 7, "context": "primitives [8], [5], [3], [15].", "startOffset": 11, "endOffset": 14}, {"referenceID": 4, "context": "primitives [8], [5], [3], [15].", "startOffset": 16, "endOffset": 19}, {"referenceID": 2, "context": "primitives [8], [5], [3], [15].", "startOffset": 21, "endOffset": 24}, {"referenceID": 14, "context": "primitives [8], [5], [3], [15].", "startOffset": 26, "endOffset": 30}, {"referenceID": 13, "context": "The work by [14] uses the Beta Process Autoregressive Hidden Markov Model (BPAR-HMM) developed by [4] to perform auto-segmentation of time series data available from multiple demonstrations.", "startOffset": 12, "endOffset": 16}, {"referenceID": 3, "context": "The work by [14] uses the Beta Process Autoregressive Hidden Markov Model (BPAR-HMM) developed by [4] to perform auto-segmentation of time series data available from multiple demonstrations.", "startOffset": 98, "endOffset": 101}, {"referenceID": 2, "context": "For sequencing [3], [5] developed novel non-parametric probabilistic models in contrast [14] developed a finite state", "startOffset": 15, "endOffset": 18}, {"referenceID": 4, "context": "For sequencing [3], [5] developed novel non-parametric probabilistic models in contrast [14] developed a finite state", "startOffset": 20, "endOffset": 23}, {"referenceID": 13, "context": "For sequencing [3], [5] developed novel non-parametric probabilistic models in contrast [14] developed a finite state", "startOffset": 88, "endOffset": 92}, {"referenceID": 21, "context": "Inverse Reinforcement Learning (IRL) is an LfD technique concerned with finding hidden reward function of an expert human demonstrator from the demonstrated state and action samples [22], [1].", "startOffset": 182, "endOffset": 186}, {"referenceID": 0, "context": "Inverse Reinforcement Learning (IRL) is an LfD technique concerned with finding hidden reward function of an expert human demonstrator from the demonstrated state and action samples [22], [1].", "startOffset": 188, "endOffset": 191}, {"referenceID": 10, "context": "Recently, an approach to solve IRL by automatically decomposing the reward function into a series of sugoals, which was viewed as local reward functions, was proposed [11].", "startOffset": 167, "endOffset": 171}, {"referenceID": 15, "context": "This is based on research that deals with human expertise in complex environment [16], [6].", "startOffset": 81, "endOffset": 85}, {"referenceID": 5, "context": "This is based on research that deals with human expertise in complex environment [16], [6].", "startOffset": 87, "endOffset": 90}, {"referenceID": 0, "context": "In IRL [1] a Markov Decision Process (MDP) without the reward function R(s) i.", "startOffset": 7, "endOffset": 10}, {"referenceID": 18, "context": "This ambiguity was resolved by restricting the reward function to be of certain form [19], [21], [13].", "startOffset": 85, "endOffset": 89}, {"referenceID": 20, "context": "This ambiguity was resolved by restricting the reward function to be of certain form [19], [21], [13].", "startOffset": 91, "endOffset": 95}, {"referenceID": 12, "context": "This ambiguity was resolved by restricting the reward function to be of certain form [19], [21], [13].", "startOffset": 97, "endOffset": 101}, {"referenceID": 17, "context": "Later [18] developed a standard Bayesian inference procedure to learn reward function.", "startOffset": 6, "endOffset": 10}, {"referenceID": 10, "context": "[11] developed Bayesian nonparametric IRL (BNIRL) that partitions the demonstration set", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "This likelihood term is evaluated using exponential rationality model (similar to that in [18]):", "startOffset": 90, "endOffset": 94}, {"referenceID": 9, "context": "This approximation to BNIRL [10], enables successful application of IRL to real-world learning scenario characterized by large state space such as quad-rotor domain.", "startOffset": 28, "endOffset": 32}, {"referenceID": 8, "context": "Recently Kulis and Jordan [9] have shown that the Gibbs sampling algorithm for the Dirichlet process mixture approaches a hard clustering algorithm when the covariances of the Gaussian variables tend to zero.", "startOffset": 26, "endOffset": 29}, {"referenceID": 6, "context": "In LfD literature dynamic movement primitive (DMP) framework [7] is a prevalent approach to imitate demon-", "startOffset": 61, "endOffset": 64}, {"referenceID": 16, "context": "a) using inverse kinematics [17] and thus the entire stateaction policy map.", "startOffset": 28, "endOffset": 32}, {"referenceID": 0, "context": "An example of action primitive a\u03c4 at a time \u03c4 for a robot with n = 3 revolute joints is a\u03c4 = [r1 = 1; r2 = 2; r3 = 2] or just a\u03c4 = [1, 2, 2] indicative of counterclockwise rotation for the first revolute joint.", "startOffset": 131, "endOffset": 140}, {"referenceID": 1, "context": "An example of action primitive a\u03c4 at a time \u03c4 for a robot with n = 3 revolute joints is a\u03c4 = [r1 = 1; r2 = 2; r3 = 2] or just a\u03c4 = [1, 2, 2] indicative of counterclockwise rotation for the first revolute joint.", "startOffset": 131, "endOffset": 140}, {"referenceID": 1, "context": "An example of action primitive a\u03c4 at a time \u03c4 for a robot with n = 3 revolute joints is a\u03c4 = [r1 = 1; r2 = 2; r3 = 2] or just a\u03c4 = [1, 2, 2] indicative of counterclockwise rotation for the first revolute joint.", "startOffset": 131, "endOffset": 140}, {"referenceID": 15, "context": "Such decomposition is implicit within the human mind, and humans are not always able to clearly explain how they arrived at the decomposition [16], [6].", "startOffset": 142, "endOffset": 146}, {"referenceID": 5, "context": "Such decomposition is implicit within the human mind, and humans are not always able to clearly explain how they arrived at the decomposition [16], [6].", "startOffset": 148, "endOffset": 151}, {"referenceID": 8, "context": "This simplification allows partitioning of stateaction pairs based on euclidean distance metric using Dirichlet process means [9].", "startOffset": 126, "endOffset": 129}, {"referenceID": 9, "context": "As proposed in [10], aCL is the action of a closed-loop controller that attempts to go from si to subgoal gzi .", "startOffset": 15, "endOffset": 19}, {"referenceID": 13, "context": "To test the developed approach exhaustive experiments were performed on a 1/14 scaled 345D Wedico excaTABLE I: Comparison of Action primitive based segmentation with BP-AR-HMM [14] in terms of computation time", "startOffset": 176, "endOffset": 180}, {"referenceID": 13, "context": "We compared our segmentation approach with that of Beta process auto-regressive HMM of [14] in terms of computation time (table I) required to segment on a i7-6700K CPU @4GHz and 24 GB RAM machine.", "startOffset": 87, "endOffset": 91}, {"referenceID": 10, "context": "We followed similar procedure using BNIRL approach [11] for comparison, and the result is shown in figure 7b.", "startOffset": 51, "endOffset": 55}], "year": 2016, "abstractText": "We explore beyond existing work on learning from demonstration by asking the question: \u201cCan robots learn to teach?\u201d, that is, can a robot autonomously learn an instructional policy from expert demonstration and use it to instruct or collaborate with humans in executing complex tasks in uncertain environments? In this paper we pursue a solution to this problem by leveraging the idea that humans often implicitly decompose a higher level task into several subgoals whose execution brings the task closer to completion. We propose Dirichlet process based non-parametric Inverse Reinforcement Learning (DPMIRL) approach for reward based unsupervised clustering of task space into subgoals. This approach is shown to capture the latent subgoals that a human teacher would have utilized to train a novice. The notion of \u201caction primitive\u201d is introduced as the means to communicate instruction policy to humans in the least complicated manner, and as a computationally efficient tool to segment demonstration data. We evaluate our approach through experiments on hydraulic actuated scaled model of an excavator and evaluate and compare different teaching strategies utilized by the robot.", "creator": "LaTeX with hyperref package"}}}