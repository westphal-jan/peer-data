{"id": "1611.07507", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Nov-2016", "title": "Variational Intrinsic Control", "abstract": "In this paper we introduce a new unsupervised reinforcement learning method for discovering the set of intrinsic options available to an agent. This set is learned by maximizing the number of different states an agent can reliably reach, as measured by the mutual information between the set of options and option termination states. To this end, we instantiate two policy gradient based algorithms, one that creates an explicit embedding space of options and one that represents options implicitly. The algorithms also provide an explicit measure of empowerment in a given state that can be used by an empowerment maximizing agent. The algorithm scales well with function approximation and we demonstrate the applicability of the algorithm on a range of tasks.", "histories": [["v1", "Tue, 22 Nov 2016 20:44:39 GMT  (1051kb,D)", "http://arxiv.org/abs/1611.07507v1", "15 pages, 6 figures"]], "COMMENTS": "15 pages, 6 figures", "reviews": [], "SUBJECTS": "cs.LG cs.AI", "authors": ["karol gregor", "danilo jimenez rezende", "daan wierstra"], "accepted": false, "id": "1611.07507"}, "pdf": {"name": "1611.07507.pdf", "metadata": {"source": "CRF", "title": "VARIATIONAL INTRINSIC CONTROL", "authors": ["Karol Gregor", "Danilo Rezende"], "emails": ["karolg@google.com", "danilor@google.com", "wierstra@google.com"], "sections": [{"heading": "1 INTRODUCTION", "text": "In fact, most of them are able to play by the rules that they have set themselves in order to play by the rules."}, {"heading": "2 INTRINSIC CONTROL AND THE MUTUAL INFORMATION PRINCIPLE", "text": "This year, the number of Indonesian Indonesian indonesian indonesian indonesian indonese.srsE \"sE tsi sla,\" so sasl sasd sda ireeSrpnlrsrteeeaSrpnlhsrsrsrsrrrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsrsr"}, {"heading": "3 INTRINSIC CONTROL WITH EXPLICIT OPTIONS", "text": "In this section, we provide a simple algorithm to maximize the possibilities of variation mentioned above. (sE) \"We have the possibility that we are able to decide.\" (sE) \"We have the possibility that we are able to decide.\" (sE) \"We have the possibility that we are able to decide.\" (sE) \"We have the possibility that we are able to decide.\" (sE) \"We have the possibility that we are able to be able.\" (sE) \"We have the possibility that we are able.\" (D) \"We have the possibility that we are able.\" (D) \"We have the possibility that we are able.\" (sE) \"We have the possibility that we are able.\" (D) \"We have the possibility that we are able.\" (D) \"We have the possibility that we are able.\""}, {"heading": "3.1 EXPERIMENTS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1.1 GRID WORLD", "text": "We demonstrate the behavior of the algorithm using a simple example of a two-dimensional grid world. The agent lives on a grid and has five actions - he can go up, down, right, left, and stop. The environment is noisy in the following way: After an agent takes a step, with a probability of 0.2, the agent is pushed in a random direction. We follow algorithm 1. We choose a discrete space of N = 30 options. We fix the option before pC to be uniform (over 30 values). Therefore, the goal is to learn a political option (a | s, ig) that would make the 30 options for as different states as possible. This is measured by the function q (b | sf), which is obtained from the state sf, trying to infer which option was followed. At the end of an episode, we receive an intrinsic reward rI = \u2212 log + log world to fix the negative states q = logN + log q (N because C is p1 / N)."}, {"heading": "3.1.2 \u2019DANGEROUS\u2019 GRID WORLD", "text": "The second environment is also a grid world, but with special characteristics. It consists of two parts: a narrow corridor connected to an open square (see Figure 2, top left), where the square is not only a simple grid world, but somewhat dangerous. There are two types of cells arranged as checkerboard grids (like on a checkerboard). On a partial grid, only the left and right actions actually move the agent to the neighboring states, and on the other partial grid only the up and down movements. If the agent selects an action that is not one of them, he falls into a state in which he is stuck for a long time. Furthermore, the step into an adjacent state occurs only with a certain probability. Therefore, if the agent does not observe the environment, he quickly loses the information about which partial grid he is on, and therefore inevitably falls into a state of low empowerment. To show this, we have calculated the exact empowerment values in each place for an open navigation result in the policy predicting the position of the 1."}, {"heading": "3.2 THE IMPORTANCE OF CLOSED LOOP POLICIES", "text": "Classical empowerment (Salge et al., 2014; Mohamed & Rezende, 2015) maximizes the mutual information between action sequences A = a1,.., aT and end states, that is, it maximizes the same objective function (3), but where action sequences A = A. This corresponds to maximizing action capacity through the space of open action sequences, that is, options where an actor first commits to a sequence of action and then blindly follows that sequence of action, regardless of what the environment is doing. In contrast, in a closed action sequence, each action is dependent on the current state. We show that the use of open action sequences can lead to a severe underestimation of action sequence in stochastic environments, which leads to action sequences aimed at achieving low action sequences. We demonstrate this effect in the \"dangerous network world,\" where action sequences in the environment, Section 3.1.2. When using open action sequences of options of length T, the actor becomes the center of action, the likelihood that the T expands the scope of action in the environment."}, {"heading": "3.3 ADVANTAGES AND DISADVANTAGES", "text": "The advantages of the algorithm 1) are: 1) it is relatively simple, 2) it uses closed-loop strategies, 3) it can use general functional approximation, 4) it is of course formulated with combinatorial option spaces, both discrete and continuous, and 5) it is model-free. The primary problem we have found with this algorithm is that it is difficult to make it work in practice with functional approximation. We suggest that there could be two reasons for this. First, the intrinsic reward is loud and changing as the agent learns, which makes it difficult for politics to learn. The algorithm worked the way it did in these simple environments described above, when we used linear functional approximation and a small, finite number of options. However, it failed when neural networks were substituted."}, {"heading": "4 INTRINSIC CONTROL WITH IMPLICIT OPTIONS", "text": "In order to address the learning difficulties of algorithm 1, we use the action space itself as an option space. This gives us the inference function q grounded goals, which makes it easier to train the policy. The elements of the algorithm are as follows: However, the steering capability before pC and policy \u03c0 in algorithm 1 simply becomes a policy that we should designate by \u03c0p (at | spt). Therefore, the action options taken by knowing about the final observation xf are calculated from (s p \u2212 1, xt, at \u2212 1). In our implementation, it is the state of a recursive network. The q function in algorithm 1 should derive the action options made by knowing about the final observation xf and thus q =. Where sq is its internal state calculated from (sqt \u2212 1, xt, xt xf). The logarithms of the action options at t effectively differ from each other - which can be distinguished by the final observation."}, {"heading": "4.1 EXPERIMENTS", "text": "We test this algorithm on several environments. The first is a grid world of size 25 \u00d7 25 with four spaces (see Figure 3 on the left (no action noise). A random action policy of length T leads to final states whose distance from the initial state is distributed approximately according to a Gaussian distribution of width \u0445 \u221a T within a room. For T, in the order of ambient size, such an actor rarely crosses another room because the narrow doors between the rooms. Figure 3 shows the trajectories learned by Algorithm 2. We see that they are actually extended and encompass large parts of the environment. In addition, many trajectories that pass through different rooms show no difficulty. This is interesting because while the Expanded q policy is conditioned to the final state, the actual one followed was given no idea of the final state that is explicitly reached. It implicitly learns to navigate through the doors to different parts of the environment.To maximize control, the distribution is even."}, {"heading": "4.2 ELEMENTS BEYOND AN AGENT\u2019S CONTROL", "text": "A dominant feature of the real world is that there are many elements that are beyond our control, such as falling leaves or traffic. One of the most important characteristics of these algorithms is that intrinsic options are things that an agent can actually control and, as such, need not model all the complexities of the real world - these algorithms are model-free. To demonstrate this property, we introduce environments with elements that are outside the control of an agent. The first environment is the same, above-used world with four rooms, but with two deflectors that move randomly at different input levels. These deflection mechanisms do not affect the agent, but the agent observes them. The agent must learn to ignore them. We find that the agent achieves the same amount of empowerments with and without the deflection mechanisms (see Figure 6 in Appendix 4). The second environment consists of pairs of MIST digits that form an image 28 (x) of 2."}, {"heading": "4.3 OPEN VS CLOSED LOOP OPTIONS", "text": "Next, we compare agents using open and closed loop options. We use the network environment, but we add the following noise: after an agent has taken a step, the environment pushes the agent in a random direction. While the closed loop policy can correct ambient noise, for example, by pursuing a strategy of always moving toward the target, an open loop policy agent has less and less information about where he is in the environment over time, and cannot reliably navigate toward the actual destination. We implement the open loop agent using algorithm 2, but we only feed the start and end states into the recurring network. Table 1 shows the comparison. We see that the closed loop agent actually performs much better."}, {"heading": "4.4 MAXIMIZING EXTRINSIC REWARD", "text": "While the primary focus of the work is on unsupervised control, we eventually offer a proof-of-principle experiment that shows that learned strategies can help in learning extrinsic reward. We consider the following situation: An agent is placed in an environment without being told what the goal is. The agent has the opportunity to explore the learned strategies and learn how to control the environment. After some time, the agent is notified of the goal as extrinsic reward rE, and he has only a limited time to collect as much reward as possible. There could be a number of ways to use the learned strategies \u03c0p and \u03c0q to maximize the extrinsic reward. In our case, we simply combine the intrinsic and extrinsic rewards and reinforce the policy \u03c0p in algorithm 2 with r = rI + \u03b1rE, where \u03b1 is a large constant (30 \u00d7 in our quadrilateral experiment) that we will reset clearly after a certain period of time (15 \u00d7 we use the different ones)."}, {"heading": "5 CONCLUSION", "text": "In this paper, we present a formalism of intrinsic control maximization for unattended option learning. Within this framework, we introduced two algorithms and analyzed them in a variety of experiments. We demonstrated the importance of closed-loop strategies in assessing empowerment. In addition, we demonstrated the usefulness of unattended learning and intrinsic control for extrinsic reward maximization."}, {"heading": "ACKNOWLEDGMENTS", "text": "We thank Shakir Mohamed, Frederic Besse, David Siver, Ivo Danihelka, Remi Munos, Ali Eslami, Tom Schaul, Nicolas Heess and Daniel Polani for their useful discussions and comments."}], "references": [{"title": "An algorithm for computing the capacity of arbitrary discrete memoryless channels", "author": ["Suguru Arimoto"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Arimoto.,? \\Q1972\\E", "shortCiteRegEx": "Arimoto.", "year": 1972}, {"title": "The option-critic architecture", "author": ["Pierre-Luc Bacon", "Doina Precup"], "venue": "In NIPS Deep Reinforcement Learning Workshop,", "citeRegEx": "Bacon and Precup.,? \\Q2015\\E", "shortCiteRegEx": "Bacon and Precup.", "year": 2015}, {"title": "Unifying count-based exploration and intrinsic motivation", "author": ["Marc G Bellemare", "Sriram Srinivasan", "Georg Ostrovski", "Tom Schaul", "David Saxton", "Remi Munos"], "venue": "arXiv preprint arXiv:1606.01868,", "citeRegEx": "Bellemare et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Bellemare et al\\.", "year": 2016}, {"title": "Computation of channel capacity and rate-distortion functions", "author": ["Richard Blahut"], "venue": "IEEE transactions on Information Theory,", "citeRegEx": "Blahut.,? \\Q1972\\E", "shortCiteRegEx": "Blahut.", "year": 1972}, {"title": "Deep learning. Book in preparation for MIT Press, 2016", "author": ["Ian Goodfellow", "Yoshua Bengio", "Aaron Courville"], "venue": "URL http://www.deeplearningbook.org", "citeRegEx": "Goodfellow et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2016}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "Hochreiter and Schmidhuber.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Variational information maximizing exploration", "author": ["Rein Houthooft", "Xi Chen", "Yan Duan", "John Schulman", "Filip De Turck", "Pieter Abbeel"], "venue": "arXiv preprint arXiv:1605.09674,", "citeRegEx": "Houthooft et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Houthooft et al\\.", "year": 2016}, {"title": "Empowerment: A universal agentcentric measure of control", "author": ["Alexander S Klyubin", "Daniel Polani", "Chrystopher L Nehaniv"], "venue": "IEEE Congress on Evolutionary Computation,", "citeRegEx": "Klyubin et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Klyubin et al\\.", "year": 2005}, {"title": "Hierarchical deep reinforcement learning: Integrating temporal abstraction and intrinsic motivation", "author": ["Tejas D Kulkarni", "Karthik R Narasimhan", "Ardavan Saeedi", "Joshua B Tenenbaum"], "venue": "arXiv preprint arXiv:1604.06057,", "citeRegEx": "Kulkarni et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Kulkarni et al\\.", "year": 2016}, {"title": "Time-regularized interrupting options", "author": ["Daniel J Mankowitz", "Timothy A Mann", "Shie Mannor"], "venue": "In Proceedings of the 31st International Conference on Machine Learning,", "citeRegEx": "Mankowitz et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Mankowitz et al\\.", "year": 2014}, {"title": "Automatic discovery of subgoals in reinforcement learning using diverse density", "author": ["Amy McGovern", "Andrew G Barto"], "venue": null, "citeRegEx": "McGovern and Barto.,? \\Q2001\\E", "shortCiteRegEx": "McGovern and Barto.", "year": 2001}, {"title": "Human-level control through deep reinforcement learning", "author": ["Volodymyr Mnih", "Koray Kavukcuoglu", "David Silver", "Andrei A Rusu", "Joel Veness", "Marc G Bellemare", "Alex Graves", "Martin Riedmiller", "Andreas K Fidjeland", "Georg Ostrovski"], "venue": "Nature, 518(7540):529\u2013533,", "citeRegEx": "Mnih et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2015}, {"title": "Variational information maximisation for intrinsically motivated reinforcement learning", "author": ["Shakir Mohamed", "Danilo Jimenez Rezende"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Mohamed and Rezende.,? \\Q2015\\E", "shortCiteRegEx": "Mohamed and Rezende.", "year": 2015}, {"title": "How can we define intrinsic motivation", "author": ["Pierre-Yves Oudeyer", "Frederic Kaplan"], "venue": "In Proc. 8th Int. Conf. Epigenetic Robot.: Modeling Cogn. Develop. Robot. Syst,", "citeRegEx": "Oudeyer and Kaplan,? \\Q2008\\E", "shortCiteRegEx": "Oudeyer and Kaplan", "year": 2008}, {"title": "Empowerment\u2013an introduction", "author": ["Christoph Salge", "Cornelius Glackin", "Daniel Polani"], "venue": "In Guided Self-Organization: Inception,", "citeRegEx": "Salge et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Salge et al\\.", "year": 2014}, {"title": "Universal value function approximators", "author": ["Tom Schaul", "Daniel Horgan", "Karol Gregor", "David Silver"], "venue": "In Proceedings of the 32nd International Conference on Machine Learning", "citeRegEx": "Schaul et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Schaul et al\\.", "year": 2015}, {"title": "Curious model-building control systems", "author": ["J\u00fcrgen Schmidhuber"], "venue": "In Neural Networks,", "citeRegEx": "Schmidhuber.,? \\Q1991\\E", "shortCiteRegEx": "Schmidhuber.", "year": 1991}, {"title": "Formal theory of creativity, fun, and intrinsic motivation (1990\u20132010)", "author": ["J\u00fcrgen Schmidhuber"], "venue": "IEEE Transactions on Autonomous Mental Development,", "citeRegEx": "Schmidhuber.,? \\Q2010\\E", "shortCiteRegEx": "Schmidhuber.", "year": 2010}, {"title": "Compositional planning using optimal option models", "author": ["David Silver", "Kamil Ciosek"], "venue": "arXiv preprint arXiv:1206.6473,", "citeRegEx": "Silver and Ciosek.,? \\Q2012\\E", "shortCiteRegEx": "Silver and Ciosek.", "year": 2012}, {"title": "Learning options in reinforcement learning", "author": ["Martin Stolle", "Doina Precup"], "venue": "In International Symposium on Abstraction, Reformulation, and Approximation,", "citeRegEx": "Stolle and Precup.,? \\Q2002\\E", "shortCiteRegEx": "Stolle and Precup.", "year": 2002}, {"title": "Reinforcement learning: An introduction, volume 1", "author": ["Richard S Sutton", "Andrew G Barto"], "venue": "MIT press Cambridge,", "citeRegEx": "Sutton and Barto.,? \\Q1998\\E", "shortCiteRegEx": "Sutton and Barto.", "year": 1998}, {"title": "Between mdps and semi-mdps: A framework for temporal abstraction in reinforcement learning", "author": ["Richard S Sutton", "Doina Precup", "Satinder Singh"], "venue": "Artificial intelligence,", "citeRegEx": "Sutton et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Sutton et al\\.", "year": 1999}, {"title": "Strategic attentive writer for learning macro-actions", "author": ["Alexander Vezhnevets", "Volodymyr Mnih", "John Agapiou", "Simon Osindero", "Alex Graves", "Oriol Vinyals", "Koray Kavukcuoglu"], "venue": "arXiv preprint arXiv:1606.04695,", "citeRegEx": "Vezhnevets et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Vezhnevets et al\\.", "year": 2016}, {"title": "Learning from delayed rewards", "author": [], "venue": "PhD thesis,", "citeRegEx": "Watkins.,? \\Q1989\\E", "shortCiteRegEx": "Watkins.", "year": 1989}], "referenceMentions": [{"referenceID": 21, "context": "This differs from the traditional approach to option learning where the goal is to find a small number of options that are useful for a particular task (Sutton et al., 1999; McGovern & Barto, 2001; Stolle & Precup, 2002; Silver & Ciosek, 2012; Kulkarni et al., 2016; Mankowitz et al., 2014; Vezhnevets et al., 2016; Bacon & Precup, 2015).", "startOffset": 152, "endOffset": 337}, {"referenceID": 8, "context": "This differs from the traditional approach to option learning where the goal is to find a small number of options that are useful for a particular task (Sutton et al., 1999; McGovern & Barto, 2001; Stolle & Precup, 2002; Silver & Ciosek, 2012; Kulkarni et al., 2016; Mankowitz et al., 2014; Vezhnevets et al., 2016; Bacon & Precup, 2015).", "startOffset": 152, "endOffset": 337}, {"referenceID": 9, "context": "This differs from the traditional approach to option learning where the goal is to find a small number of options that are useful for a particular task (Sutton et al., 1999; McGovern & Barto, 2001; Stolle & Precup, 2002; Silver & Ciosek, 2012; Kulkarni et al., 2016; Mankowitz et al., 2014; Vezhnevets et al., 2016; Bacon & Precup, 2015).", "startOffset": 152, "endOffset": 337}, {"referenceID": 22, "context": "This differs from the traditional approach to option learning where the goal is to find a small number of options that are useful for a particular task (Sutton et al., 1999; McGovern & Barto, 2001; Stolle & Precup, 2002; Silver & Ciosek, 2012; Kulkarni et al., 2016; Mankowitz et al., 2014; Vezhnevets et al., 2016; Bacon & Precup, 2015).", "startOffset": 152, "endOffset": 337}, {"referenceID": 15, "context": "The idea of goal and state embeddings, along with a universal value function for reaching these goals, was introduced in Schaul et al. (2015). This work allowed an agent to efficiently represent control over many goals and to generalize to new goals.", "startOffset": 121, "endOffset": 142}, {"referenceID": 14, "context": "The second scenario is that in which the long-term goal of the agent is to get to a state with a maximal set of available intrinsic options \u2013 the objective of empowerment (Salge et al., 2014).", "startOffset": 171, "endOffset": 191}, {"referenceID": 16, "context": "Let us compare this to the commonly used intrinsic motivation objective of maximizing the amount of model-learning progress, measured as the difference in compression of its experience before and after learning (Schmidhuber, 1991; 2010; Bellemare et al., 2016; Houthooft et al., 2016).", "startOffset": 211, "endOffset": 284}, {"referenceID": 2, "context": "Let us compare this to the commonly used intrinsic motivation objective of maximizing the amount of model-learning progress, measured as the difference in compression of its experience before and after learning (Schmidhuber, 1991; 2010; Bellemare et al., 2016; Houthooft et al., 2016).", "startOffset": 211, "endOffset": 284}, {"referenceID": 6, "context": "Let us compare this to the commonly used intrinsic motivation objective of maximizing the amount of model-learning progress, measured as the difference in compression of its experience before and after learning (Schmidhuber, 1991; 2010; Bellemare et al., 2016; Houthooft et al., 2016).", "startOffset": 211, "endOffset": 284}, {"referenceID": 2, "context": "Let us compare this to the commonly used intrinsic motivation objective of maximizing the amount of model-learning progress, measured as the difference in compression of its experience before and after learning (Schmidhuber, 1991; 2010; Bellemare et al., 2016; Houthooft et al., 2016). The empowerment objective differs from this in a fundamental manner: the primary goal is not to understand or predict the observations but to control the environment. This is an important point \u2013 agents can often control an environment perfectly well without much understanding, as exemplified by canonical model-free reinforcement learning algorithms (Sutton & Barto, 1998), where agents only model action-conditioned expected returns. Focusing on such understanding might significantly distract and impair the agent, as such reducing the control it achieves. Our algorithm can be viewed as learning to represent the intrinsic control space of an agent. Developing this space should be seen as acquiring universal knowledge useful for accomplishing a multitude of different tasks, such as maximizing extrinsic or intrinsic reward (see Oudeyer et al. (2008) for an overview and useful references).", "startOffset": 237, "endOffset": 1144}, {"referenceID": 4, "context": "Just like there are multiple methods and objectives for unsupervised learning (Goodfellow et al., 2016), we can devise multiple methods and objectives for unsupervised control.", "startOffset": 78, "endOffset": 103}, {"referenceID": 14, "context": "This information measure has been introduced in the empowerment literature before (Salge et al., 2014; Klyubin et al., 2005) along with methods for measuring it (such as Blahut (1972); Arimoto (1972)).", "startOffset": 82, "endOffset": 124}, {"referenceID": 7, "context": "This information measure has been introduced in the empowerment literature before (Salge et al., 2014; Klyubin et al., 2005) along with methods for measuring it (such as Blahut (1972); Arimoto (1972)).", "startOffset": 82, "endOffset": 124}, {"referenceID": 2, "context": ", 2005) along with methods for measuring it (such as Blahut (1972); Arimoto (1972)).", "startOffset": 53, "endOffset": 67}, {"referenceID": 0, "context": ", 2005) along with methods for measuring it (such as Blahut (1972); Arimoto (1972)).", "startOffset": 68, "endOffset": 83}, {"referenceID": 0, "context": ", 2005) along with methods for measuring it (such as Blahut (1972); Arimoto (1972)). Recently, Mohamed & Rezende (2015) proposed an algorithm that can utilize function approximation and deep learning techniques to operate in high-dimensional environments.", "startOffset": 68, "endOffset": 120}, {"referenceID": 23, "context": "The agent can use any reinforcement learning algorithm (Sutton & Barto, 1998), such as policy gradients (Williams, 1992) or Q-learning (Watkins, 1989; Werbos, 1977), to train a policy to maximize this reward.", "startOffset": 135, "endOffset": 164}, {"referenceID": 15, "context": "We could also use a universal value function approximation (Schaul et al., 2015).", "startOffset": 59, "endOffset": 80}, {"referenceID": 14, "context": "Classical empowerment (Salge et al., 2014; Mohamed & Rezende, 2015) maximizes mutual information between sequences of actions A = a1, .", "startOffset": 22, "endOffset": 67}, {"referenceID": 11, "context": "These problems are related to those in deep reinforcement learning (Mnih et al., 2015), where in order to make Q learning work well with function approximation, one needs to store a large number of experiences in memory and replay them.", "startOffset": 67, "endOffset": 86}], "year": 2016, "abstractText": "In this paper we introduce a new unsupervised reinforcement learning method for discovering the set of intrinsic options available to an agent. This set is learned by maximizing the number of different states an agent can reliably reach, as measured by the mutual information between the set of options and option termination states. To this end, we instantiate two policy gradient based algorithms, one that creates an explicit embedding space of options and one that represents options implicitly. The algorithms also provide an explicit measure of empowerment in a given state that can be used by an empowerment maximizing agent. The algorithm scales well with function approximation and we demonstrate the applicability of the algorithm on a range of tasks.", "creator": "LaTeX with hyperref package"}}}