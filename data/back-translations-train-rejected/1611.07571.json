{"id": "1611.07571", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Nov-2016", "title": "Quad-networks: unsupervised learning to rank for interest point detection", "abstract": "Several machine learning tasks require to represent the data using only a sparse set of interest points. An ideal detector is able to find the corresponding interest points even if the data undergo a transformation typical for a given domain. Since the task is of high practical interest in computer vision, many hand-crafted solutions were proposed. In this paper, we ask a fundamental question: can we learn such detectors from scratch? Since it is often unclear, what points are \"interesting\", human labelling cannot be used to find a truly unbiased solution. Therefore, the task requires an unsupervised formulation. We are the first to propose such a formulation: training a neural network to rank points in a transformation-invariant manner. Interest points are then extracted from the top/bottom quantiles of this ranking. We validate our approach on two tasks: standard RGB image interest point detection and challenging cross-modal interest point detection between RGB and depth images. We quantitatively show that our unsupervised method performs better or on-par with baselines.", "histories": [["v1", "Tue, 22 Nov 2016 22:46:17 GMT  (7464kb,D)", "https://arxiv.org/abs/1611.07571v1", null], ["v2", "Mon, 10 Apr 2017 21:15:18 GMT  (7521kb,D)", "http://arxiv.org/abs/1611.07571v2", "Accepted at CVPR 2017"]], "reviews": [], "SUBJECTS": "cs.CV cs.LG cs.NE", "authors": ["nikolay savinov", "akihito seki", "lubor ladicky", "torsten sattler", "marc pollefeys"], "accepted": false, "id": "1611.07571"}, "pdf": {"name": "1611.07571.pdf", "metadata": {"source": "CRF", "title": "Quad-networks: unsupervised learning to rank for interest point detection", "authors": ["Nikolay Savinov", "Akihito Seki", "L\u2019ubor Ladick\u00fd", "Torsten Sattler", "Marc Pollefeys"], "emails": ["nikolay.savinov@inf.ethz.ch,", "ladickyl@inf.ethz.ch,", "sattlert@inf.ethz.ch,", "marc.pollefeys@inf.ethz.ch,", "akihito.seki@toshiba.co.jp"], "sections": [{"heading": "1. Introduction", "text": "eSi rf\u00fc ide eeisrVnlrsreeeoteeteeteerrrrrf\u00fc ide eeisrsrteeeeoiiiiiiiiiiiiiiiueegtlrsrlrrteeoiuiuiuiuiuiuiueeglrrrrrrrrrrrrrsrrrrrrrteeeteeteerteersrrrrrrrrrrrrrsrrrrrrrlrrrrrrrrlrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrreeeeeeeeeeeeeeeteeteerrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrreeeeeeeeeeeeeeeeeeeeeeeeeteeeeeeeeeeeeeeeeeeeeeeeeeeteeeeteerrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr"}, {"heading": "2. Related work", "text": "In fact, most people who are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to fight, to fight, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to fight, to fight, to fight, to move, to move, to fight, to fight, to fight, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to"}, {"heading": "3. Detection by ranking", "text": "In this section, we present the problem of learning an interest point detector as the problem of learning d > pid objects after transformation. We consider interest points to be derived from the topmost / lowest quantities of a response function. If these quantities are preserved under certain transformation classes, we have a good detector: it recognizes the same points. For the quantities of the ranking to be obtained, we look for a ranking that is invariant for these transformations.Let's consider a set of D objects, where each object d \"D\" is a nine-dimensional point multiple (p1d,., p \"Nd\" d \"). Each point pid comes from a certain P of points. Each object d can transform from a specified T: D 7 \u2192 D. Each transformation t\" D \"preserves certain point cordencies: Some points in the object t (d) will correspond to points in the object d.\" Let's assume that there is a point in the object d. \""}, {"heading": "3.1. Ranking objective and optimization", "text": "Let us first introduce a rank matching function for quadruplets: R (pid, p j d, p i t (d), p j t (d) | w) = (H (pid | w) \u2212 H (p j d | w))) (H (p i t (d) | w) \u2212 H (p j t) > 0. (3) To give preference to this invariance, we assume that the rank invariance condition D and the transformation quantity T can be finite (for the sake of training) and minimize the target: L (w) = D (d), p (d)."}, {"heading": "4. Image interest point detector learning", "text": "It is a difficult task to learn detectors from scratch, as it is not trivial to formulate good detector criteria in the optimization framework. As examined by [22], a good detector should produce points of interest that are robust in terms of point of view / illumination changes (to detect the same points and continue to correspond to them) and sparse (to make feature matching feasible).In order to comply with the terminology previously introduced, d is an image, p is a position in the image represented by a patch, transformation is a point of view / illumination changes, and correspondence sets Cdt are patch-to-patch correspondences between images that observe the same 3D scene. It is typical of point of interest detectors to ensure sparseness in two ways: by maintaining the upper / lower quantities of the reaction function (contrast filtering) and by maintaining the local extreme of the reaction function (non-refiltering during observation)."}, {"heading": "4.1. Training", "text": "We need to take samples from both the image set D and the transformation set T in order to perform a training with the goal (7). Of course, we could take images and transformations from any available image set with matches, but this does not address two important questions: \u2022 How to achieve invariance exactly on the transformations we want? For example, most real images are captured correctly so that there is no relative rotation between them. However, we want our detector to be robust against cases where there is such a rotation. \u2022 How to extend the training images? For example, all objects in the training images could be well illuminated so that some objects are in the shadow while others are in the light. In this chapter, we will show how to achieve any goal by randomly transforming the training images."}, {"heading": "5. Experiments", "text": "Our objective function (7) is based on pairs of correspondences belonging to a type of space of interest. (An example of such a quadruple relationship is shown in Fig. 2) To train a detector, we must obtain these correspondences. We examined an RGB detector of ground-level correspondences (they come from the projection of laser scanned 3D dots onto images), a completely unattended RGB detector (correspondences are obtained by random distortion of images and changing illumination), a cross-modal RGB / depth detector (correspondences are achieved trivially as matching locations in viewy Kinect RGBD frames). We go on to describe the structure of these experiments. We focus on the most commonly used type of detectors: scale-space co-variant, rotation-invariant (although our method is suitable for any combination of detector variants / inventories)."}, {"heading": "5.1. RGB detector from ground-truth correspondences", "text": "In this experiment, we show how to use existing 3D data to determine correspondences for training a detector. Training. We used the DTU Robot Image Dataset [1]. It has 3D dots coming from a laser scanner, and camera positions that allow to project 3D dots into the image pairs and extract image fields centered on the projections. These projections form the correspondence pairs for the training. Testing. We used the Oxford VGG dataset [22], which is often selected for this type of evaluation. This dataset consists of 40 image pairs. NN architectures. In this experiment, we tested two NN architectures: a linear model (c (17, 1, 1, 1, 0) and a non-linear NN with a hidden layer (c (17, 1, 32, 0), e, f (32, 1)."}, {"heading": "5.2. Fully-unsupervised RGB detector", "text": "The aim of this experiment is to show that matches between soil and truth from an additional data source (such as 3D dots from a laser scanner) are not necessary to train a detector with our method. Instead, we can scan random transformations to obtain matches. In this experiment, we used only images from the DTU dataset with different illumination. To generate the match, a patch was randomly selected from an image and randomly transformed. We looked at affine distortions, conservation areas, along with illumination changes, uniformly sampled from those provided by the dataset. Affine distortions were parameterized as decay (\u03b1)."}, {"heading": "5.3. Cross-modal RGB/depth detector", "text": "In this experiment, we will show how to use our method to learn a cross-modal detector - a hard problem where we do not understand how to design a good solution by hand. We will learn a detector between RGB and depth images by training on the NYUv2 dataset [24]. Such a detector has an application in extending a monochrome 3D dot cloud with colors from a newly obtained image. Training. We will use 40 random frames from NYUv2, which contains vision-oriented Kinect RGBD frames (one RGB pixel is equivalent to a depth pixel in the same place). We will test 40 random frames from NYUv2 (independent of the training set). We evaluated the following architectures for the reaction function H: \u2022 Deep Convolutional Network (Deep Conv Net): (7, 1, 32, 3), b, 32, 3."}, {"heading": "6. Conclusion", "text": "The basic idea of the method is to generate a repeatable ranking of the points of the object and to use the upper / lower quantity of the ranking as interest points. We have shown how to learn such a detector for images. We show a superior or comparable performance of our method in relation to DoG in two different settings: learning the standard RGB detector from scratch and learning a detector that can be repeated between different modalities (RGB and depth of Kinect). Future work includes learning the descriptor together with our detector. Furthermore, it could be investigated whether our method can be applied to detection beyond images (e.g. to detect images of interest in videos).Recognition: This work is partly funded by the Swiss NSF Project 163910, the Max Planck CLS Fellowship and the Swiss CTI Project 17136.1 PFES-ES."}], "references": [{"title": "Interesting interest points", "author": ["H. Aan\u00e6s", "A.L. Dahl", "K. Steenstrup Pedersen"], "venue": "IJCV, 97:18\u201335", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2012}, {"title": "Multispectral image feature points", "author": ["C. Aguilera", "F. Barrera", "F. Lumbreras", "A.D. Sappa", "R. Toledo"], "venue": "Sensors, 12(9):12661\u201312672", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2012}, {"title": "Opencv library", "author": ["G. Bradski"], "venue": "Dr. Dobb\u2019s Journal of Software Tools", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2000}, {"title": "Fast and accurate deep network learning by exponential linear units (elus)", "author": ["D.-A. Clevert", "T. Unterthiner", "S. Hochreiter"], "venue": "arXiv preprint arXiv:1511.07289", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2015}, {"title": "Torch7: A matlab-like environment for machine learning", "author": ["R. Collobert", "K. Kavukcuoglu", "C. Farabet"], "venue": "BigLearn, NIPS Workshop", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2011}, {"title": "Unsupervised visual representation learning by context prediction", "author": ["C. Doersch", "A. Gupta", "A.A. Efros"], "venue": "ICCV, pages 1422\u20131430", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2015}, {"title": "Generative adversarial nets", "author": ["I. Goodfellow", "J. Pouget-Abadie", "M. Mirza", "B. Xu", "D. Warde-Farley", "S. Ozair", "A. Courville", "Y. Bengio"], "venue": "NIPS", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2014}, {"title": "A combined corner and edge detector", "author": ["C. Harris", "M. Stephens"], "venue": "Alvey Vision Conference", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1988}, {"title": "Algorithm as 136: A kmeans clustering algorithm", "author": ["J.A. Hartigan", "M.A. Wong"], "venue": "Journal of the Royal Statistical Society. Series C (Applied Statistics), 28(1):100\u2013108", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1979}, {"title": "Training products of experts by minimizing contrastive divergence", "author": ["G.E. Hinton"], "venue": "Neural Computation, 14(8):1771\u2013 1800", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2002}, {"title": "Reducing the dimensionality of data with neural networks", "author": ["G.E. Hinton", "R.R. Salakhutdinov"], "venue": "Science, 313(5786):504\u2013507", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2006}, {"title": "Principal component analysis", "author": ["I. Jolliffe"], "venue": "Wiley Online Library", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2002}, {"title": "Keypoint descriptors for matching across multiple image modalities and nonlinear intensity variations", "author": ["A. Kelman", "M. Sofka", "C.V. Stewart"], "venue": "2007 IEEE conference on computer vision and pattern recognition, pages 1\u20137. IEEE", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2007}, {"title": "Data-driven fluid simulations using regression forests", "author": ["L. Ladicky", "S. Jeong", "B. Solenthaler", "M. Pollefeys", "M. Gross"], "venue": "ACM TOG, 34(6):199", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning covariant feature detectors", "author": ["K. Lenc", "A. Vedaldi"], "venue": "arXiv preprint arXiv:1605.01224", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2016}, {"title": "Learning physical intuition of block towers by example", "author": ["A. Lerer", "S. Gross", "R. Fergus"], "venue": "arXiv preprint arXiv:1603.01312", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2016}, {"title": "Microsoft coco: Common objects in context", "author": ["T.-Y. Lin", "M. Maire", "S. Belongie", "J. Hays", "P. Perona", "D. Ramanan", "P. Doll\u00e1r", "C.L. Zitnick"], "venue": "In ECCV", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2014}, {"title": "Distinctive image features from scale-invariant keypoints", "author": ["D.G. Lowe"], "venue": "IJCV, 60(2):91\u2013110", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2004}, {"title": "Robust widebaseline stereo from maximally stable extremal regions", "author": ["J. Matas", "O. Chum", "M. Urban", "T. Pajdla"], "venue": "Image and Vision Computing, 22(10):761\u2013767", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2004}, {"title": "Finite mixture models", "author": ["G. McLachlan", "D. Peel"], "venue": "John Wiley & Sons", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2004}, {"title": "Scale & affine invariant interest point detectors", "author": ["K. Mikolajczyk", "C. Schmid"], "venue": "IJCV, 60(1):63\u201386", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2004}, {"title": "A comparison of affine region detectors", "author": ["K. Mikolajczyk", "T. Tuytelaars", "C. Schmid", "A. Zisserman", "J. Matas", "F. Schaffalitzky", "T. Kadir", "L. Van Gool"], "venue": "IJCV, 65(1-2):43\u201372", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2005}, {"title": "Wxbs: Wide baseline stereo generalizations", "author": ["D. Mishkin", "J. Matas", "M. Perdoch", "K. Lenc"], "venue": "arXiv preprint arXiv:1504.06603", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2015}, {"title": "Derek Hoiem and R", "author": ["P.K. Nathan Silberman"], "venue": "Fergus. Indoor segmentation and support inference from rgbd images. In ECCV", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2012}, {"title": "Unsupervised learning of visual representations by solving jigsaw puzzles", "author": ["M. Noroozi", "P. Favaro"], "venue": "arXiv preprint arXiv:1603.09246", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2016}, {"title": "Context encoders: Feature learning by inpainting", "author": ["D. Pathak", "P. Krahenbuhl", "J. Donahue", "T. Darrell", "A.A. Efros"], "venue": "arXiv preprint arXiv:1604.07379", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2016}, {"title": "Registering images to untextured geometry using average shading gradients", "author": ["T. Plotz", "S. Roth"], "venue": "In The IEEE International Conference on Computer Vision (ICCV),", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2015}, {"title": "Unsupervised representation learning with deep convolutional generative adversarial networks", "author": ["A. Radford", "L. Metz", "S. Chintala"], "venue": "arXiv preprint arXiv:1511.06434", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2015}, {"title": "Nonlinear dimensionality reduction by locally linear embedding", "author": ["S.T. Roweis", "L.K. Saul"], "venue": "Science, 290(5500):2323\u2013 2326", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2000}, {"title": "et al", "author": ["O. Russakovsky", "J. Deng", "H. Su", "J. Krause", "S. Satheesh", "S. Ma", "Z. Huang", "A. Karpathy", "A. Khosla", "M. Bernstein"], "venue": "Imagenet large scale visual recognition challenge. IJCV, 115(3):211\u2013252", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2015}, {"title": "Normalized cuts and image segmentation", "author": ["J. Shi", "J. Malik"], "venue": "PAMI, 22(8):888\u2013905", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2000}, {"title": "A global geometric framework for nonlinear dimensionality reduction", "author": ["J.B. Tenenbaum", "V. De Silva", "J.C. Langford"], "venue": "Science, 290(5500):2319\u20132323", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2000}, {"title": "Fast corner detection", "author": ["M. Trajkovi\u0107", "M. Hedley"], "venue": "Image and Vision Computing, 16(2):75\u201387", "citeRegEx": "33", "shortCiteRegEx": null, "year": 1998}, {"title": "Visualizing data using t-sne", "author": ["L. Van der Maaten", "G.E. Hinton"], "venue": "JMLR, 9(2579-2605):85,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2008}, {"title": "Tilde: A temporally invariant learned detector", "author": ["Y. Verdie", "K.M. Yi", "P. Fua", "V. Lepetit"], "venue": "The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 5279\u20135288. IEEE", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2015}, {"title": "Unsupervised learning of visual representations using videos", "author": ["X. Wang", "A. Gupta"], "venue": "ICCV", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning descriptors for object recognition and 3d pose estimation", "author": ["P. Wohlhart", "V. Lepetit"], "venue": "CVPR", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2015}, {"title": "Lift: Learned invariant feature transform", "author": ["K.M. Yi", "E. Trulls", "V. Lepetit", "P. Fua"], "venue": "arXiv preprint arXiv:1603.09114", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2016}, {"title": "Adadelta: an adaptive learning rate method", "author": ["M.D. Zeiler"], "venue": "arXiv preprint arXiv:1212.5701", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2012}], "referenceMentions": [{"referenceID": 29, "context": "Recently, more labelled data with millions of examples have become available (for example, Imagenet [30], Microsoft COCO [17]), which led to significant progress in supervised learning research.", "startOffset": 100, "endOffset": 104}, {"referenceID": 16, "context": "Recently, more labelled data with millions of examples have become available (for example, Imagenet [30], Microsoft COCO [17]), which led to significant progress in supervised learning research.", "startOffset": 121, "endOffset": 125}, {"referenceID": 26, "context": "Let\u2019s assume one wants to match new images to untextured parts of an existing 3D model [27].", "startOffset": 87, "endOffset": 91}, {"referenceID": 17, "context": "Some earlier works hand-crafted detectors like DoG [18].", "startOffset": 51, "endOffset": 55}, {"referenceID": 37, "context": "For example, LIFT [38] aims to extract a subset of DoG detections that are matched correctly in the later stages of the sparse 3D reconstruction.", "startOffset": 18, "endOffset": 22}, {"referenceID": 21, "context": "Transformations that should transform the result of a detector together with the transformation \u2014 and thus their parameters have to be estimated as latent variables \u2014 are called covariant [22].", "startOffset": 188, "endOffset": 192}, {"referenceID": 19, "context": "Currently, unsupervised learning comprises many directions: learning the distribution that best explains data (Gaussian Mixture Models learned via an EMalgorithm [20], Restricted Boltzmann Machines [10], Generative Adversarial Nets [7]), clustering, dimensionality reduction and unsupervised segmentation (kMeans [9], LLE [29], Isomap [32], PCA [12], Normalized Cuts [31], t-SNE [34]), learning to simulate task solvers (when a solution is provided by the solver and the task is automatically generated [14], [16]), and learning data representation suitable for further use in some other task (autoencoders [11], deep convolutional adversarial nets [28], learning by context prediction [6], learning from tracking in videos [36], metric learning [37], learning by predicting inpainting [26], learning by solving jigsaw puzzles [25]).", "startOffset": 162, "endOffset": 166}, {"referenceID": 9, "context": "Currently, unsupervised learning comprises many directions: learning the distribution that best explains data (Gaussian Mixture Models learned via an EMalgorithm [20], Restricted Boltzmann Machines [10], Generative Adversarial Nets [7]), clustering, dimensionality reduction and unsupervised segmentation (kMeans [9], LLE [29], Isomap [32], PCA [12], Normalized Cuts [31], t-SNE [34]), learning to simulate task solvers (when a solution is provided by the solver and the task is automatically generated [14], [16]), and learning data representation suitable for further use in some other task (autoencoders [11], deep convolutional adversarial nets [28], learning by context prediction [6], learning from tracking in videos [36], metric learning [37], learning by predicting inpainting [26], learning by solving jigsaw puzzles [25]).", "startOffset": 198, "endOffset": 202}, {"referenceID": 6, "context": "Currently, unsupervised learning comprises many directions: learning the distribution that best explains data (Gaussian Mixture Models learned via an EMalgorithm [20], Restricted Boltzmann Machines [10], Generative Adversarial Nets [7]), clustering, dimensionality reduction and unsupervised segmentation (kMeans [9], LLE [29], Isomap [32], PCA [12], Normalized Cuts [31], t-SNE [34]), learning to simulate task solvers (when a solution is provided by the solver and the task is automatically generated [14], [16]), and learning data representation suitable for further use in some other task (autoencoders [11], deep convolutional adversarial nets [28], learning by context prediction [6], learning from tracking in videos [36], metric learning [37], learning by predicting inpainting [26], learning by solving jigsaw puzzles [25]).", "startOffset": 232, "endOffset": 235}, {"referenceID": 8, "context": "Currently, unsupervised learning comprises many directions: learning the distribution that best explains data (Gaussian Mixture Models learned via an EMalgorithm [20], Restricted Boltzmann Machines [10], Generative Adversarial Nets [7]), clustering, dimensionality reduction and unsupervised segmentation (kMeans [9], LLE [29], Isomap [32], PCA [12], Normalized Cuts [31], t-SNE [34]), learning to simulate task solvers (when a solution is provided by the solver and the task is automatically generated [14], [16]), and learning data representation suitable for further use in some other task (autoencoders [11], deep convolutional adversarial nets [28], learning by context prediction [6], learning from tracking in videos [36], metric learning [37], learning by predicting inpainting [26], learning by solving jigsaw puzzles [25]).", "startOffset": 313, "endOffset": 316}, {"referenceID": 28, "context": "Currently, unsupervised learning comprises many directions: learning the distribution that best explains data (Gaussian Mixture Models learned via an EMalgorithm [20], Restricted Boltzmann Machines [10], Generative Adversarial Nets [7]), clustering, dimensionality reduction and unsupervised segmentation (kMeans [9], LLE [29], Isomap [32], PCA [12], Normalized Cuts [31], t-SNE [34]), learning to simulate task solvers (when a solution is provided by the solver and the task is automatically generated [14], [16]), and learning data representation suitable for further use in some other task (autoencoders [11], deep convolutional adversarial nets [28], learning by context prediction [6], learning from tracking in videos [36], metric learning [37], learning by predicting inpainting [26], learning by solving jigsaw puzzles [25]).", "startOffset": 322, "endOffset": 326}, {"referenceID": 31, "context": "Currently, unsupervised learning comprises many directions: learning the distribution that best explains data (Gaussian Mixture Models learned via an EMalgorithm [20], Restricted Boltzmann Machines [10], Generative Adversarial Nets [7]), clustering, dimensionality reduction and unsupervised segmentation (kMeans [9], LLE [29], Isomap [32], PCA [12], Normalized Cuts [31], t-SNE [34]), learning to simulate task solvers (when a solution is provided by the solver and the task is automatically generated [14], [16]), and learning data representation suitable for further use in some other task (autoencoders [11], deep convolutional adversarial nets [28], learning by context prediction [6], learning from tracking in videos [36], metric learning [37], learning by predicting inpainting [26], learning by solving jigsaw puzzles [25]).", "startOffset": 335, "endOffset": 339}, {"referenceID": 11, "context": "Currently, unsupervised learning comprises many directions: learning the distribution that best explains data (Gaussian Mixture Models learned via an EMalgorithm [20], Restricted Boltzmann Machines [10], Generative Adversarial Nets [7]), clustering, dimensionality reduction and unsupervised segmentation (kMeans [9], LLE [29], Isomap [32], PCA [12], Normalized Cuts [31], t-SNE [34]), learning to simulate task solvers (when a solution is provided by the solver and the task is automatically generated [14], [16]), and learning data representation suitable for further use in some other task (autoencoders [11], deep convolutional adversarial nets [28], learning by context prediction [6], learning from tracking in videos [36], metric learning [37], learning by predicting inpainting [26], learning by solving jigsaw puzzles [25]).", "startOffset": 345, "endOffset": 349}, {"referenceID": 30, "context": "Currently, unsupervised learning comprises many directions: learning the distribution that best explains data (Gaussian Mixture Models learned via an EMalgorithm [20], Restricted Boltzmann Machines [10], Generative Adversarial Nets [7]), clustering, dimensionality reduction and unsupervised segmentation (kMeans [9], LLE [29], Isomap [32], PCA [12], Normalized Cuts [31], t-SNE [34]), learning to simulate task solvers (when a solution is provided by the solver and the task is automatically generated [14], [16]), and learning data representation suitable for further use in some other task (autoencoders [11], deep convolutional adversarial nets [28], learning by context prediction [6], learning from tracking in videos [36], metric learning [37], learning by predicting inpainting [26], learning by solving jigsaw puzzles [25]).", "startOffset": 367, "endOffset": 371}, {"referenceID": 33, "context": "Currently, unsupervised learning comprises many directions: learning the distribution that best explains data (Gaussian Mixture Models learned via an EMalgorithm [20], Restricted Boltzmann Machines [10], Generative Adversarial Nets [7]), clustering, dimensionality reduction and unsupervised segmentation (kMeans [9], LLE [29], Isomap [32], PCA [12], Normalized Cuts [31], t-SNE [34]), learning to simulate task solvers (when a solution is provided by the solver and the task is automatically generated [14], [16]), and learning data representation suitable for further use in some other task (autoencoders [11], deep convolutional adversarial nets [28], learning by context prediction [6], learning from tracking in videos [36], metric learning [37], learning by predicting inpainting [26], learning by solving jigsaw puzzles [25]).", "startOffset": 379, "endOffset": 383}, {"referenceID": 13, "context": "Currently, unsupervised learning comprises many directions: learning the distribution that best explains data (Gaussian Mixture Models learned via an EMalgorithm [20], Restricted Boltzmann Machines [10], Generative Adversarial Nets [7]), clustering, dimensionality reduction and unsupervised segmentation (kMeans [9], LLE [29], Isomap [32], PCA [12], Normalized Cuts [31], t-SNE [34]), learning to simulate task solvers (when a solution is provided by the solver and the task is automatically generated [14], [16]), and learning data representation suitable for further use in some other task (autoencoders [11], deep convolutional adversarial nets [28], learning by context prediction [6], learning from tracking in videos [36], metric learning [37], learning by predicting inpainting [26], learning by solving jigsaw puzzles [25]).", "startOffset": 503, "endOffset": 507}, {"referenceID": 15, "context": "Currently, unsupervised learning comprises many directions: learning the distribution that best explains data (Gaussian Mixture Models learned via an EMalgorithm [20], Restricted Boltzmann Machines [10], Generative Adversarial Nets [7]), clustering, dimensionality reduction and unsupervised segmentation (kMeans [9], LLE [29], Isomap [32], PCA [12], Normalized Cuts [31], t-SNE [34]), learning to simulate task solvers (when a solution is provided by the solver and the task is automatically generated [14], [16]), and learning data representation suitable for further use in some other task (autoencoders [11], deep convolutional adversarial nets [28], learning by context prediction [6], learning from tracking in videos [36], metric learning [37], learning by predicting inpainting [26], learning by solving jigsaw puzzles [25]).", "startOffset": 509, "endOffset": 513}, {"referenceID": 10, "context": "Currently, unsupervised learning comprises many directions: learning the distribution that best explains data (Gaussian Mixture Models learned via an EMalgorithm [20], Restricted Boltzmann Machines [10], Generative Adversarial Nets [7]), clustering, dimensionality reduction and unsupervised segmentation (kMeans [9], LLE [29], Isomap [32], PCA [12], Normalized Cuts [31], t-SNE [34]), learning to simulate task solvers (when a solution is provided by the solver and the task is automatically generated [14], [16]), and learning data representation suitable for further use in some other task (autoencoders [11], deep convolutional adversarial nets [28], learning by context prediction [6], learning from tracking in videos [36], metric learning [37], learning by predicting inpainting [26], learning by solving jigsaw puzzles [25]).", "startOffset": 607, "endOffset": 611}, {"referenceID": 27, "context": "Currently, unsupervised learning comprises many directions: learning the distribution that best explains data (Gaussian Mixture Models learned via an EMalgorithm [20], Restricted Boltzmann Machines [10], Generative Adversarial Nets [7]), clustering, dimensionality reduction and unsupervised segmentation (kMeans [9], LLE [29], Isomap [32], PCA [12], Normalized Cuts [31], t-SNE [34]), learning to simulate task solvers (when a solution is provided by the solver and the task is automatically generated [14], [16]), and learning data representation suitable for further use in some other task (autoencoders [11], deep convolutional adversarial nets [28], learning by context prediction [6], learning from tracking in videos [36], metric learning [37], learning by predicting inpainting [26], learning by solving jigsaw puzzles [25]).", "startOffset": 649, "endOffset": 653}, {"referenceID": 5, "context": "Currently, unsupervised learning comprises many directions: learning the distribution that best explains data (Gaussian Mixture Models learned via an EMalgorithm [20], Restricted Boltzmann Machines [10], Generative Adversarial Nets [7]), clustering, dimensionality reduction and unsupervised segmentation (kMeans [9], LLE [29], Isomap [32], PCA [12], Normalized Cuts [31], t-SNE [34]), learning to simulate task solvers (when a solution is provided by the solver and the task is automatically generated [14], [16]), and learning data representation suitable for further use in some other task (autoencoders [11], deep convolutional adversarial nets [28], learning by context prediction [6], learning from tracking in videos [36], metric learning [37], learning by predicting inpainting [26], learning by solving jigsaw puzzles [25]).", "startOffset": 686, "endOffset": 689}, {"referenceID": 35, "context": "Currently, unsupervised learning comprises many directions: learning the distribution that best explains data (Gaussian Mixture Models learned via an EMalgorithm [20], Restricted Boltzmann Machines [10], Generative Adversarial Nets [7]), clustering, dimensionality reduction and unsupervised segmentation (kMeans [9], LLE [29], Isomap [32], PCA [12], Normalized Cuts [31], t-SNE [34]), learning to simulate task solvers (when a solution is provided by the solver and the task is automatically generated [14], [16]), and learning data representation suitable for further use in some other task (autoencoders [11], deep convolutional adversarial nets [28], learning by context prediction [6], learning from tracking in videos [36], metric learning [37], learning by predicting inpainting [26], learning by solving jigsaw puzzles [25]).", "startOffset": 724, "endOffset": 728}, {"referenceID": 36, "context": "Currently, unsupervised learning comprises many directions: learning the distribution that best explains data (Gaussian Mixture Models learned via an EMalgorithm [20], Restricted Boltzmann Machines [10], Generative Adversarial Nets [7]), clustering, dimensionality reduction and unsupervised segmentation (kMeans [9], LLE [29], Isomap [32], PCA [12], Normalized Cuts [31], t-SNE [34]), learning to simulate task solvers (when a solution is provided by the solver and the task is automatically generated [14], [16]), and learning data representation suitable for further use in some other task (autoencoders [11], deep convolutional adversarial nets [28], learning by context prediction [6], learning from tracking in videos [36], metric learning [37], learning by predicting inpainting [26], learning by solving jigsaw puzzles [25]).", "startOffset": 746, "endOffset": 750}, {"referenceID": 25, "context": "Currently, unsupervised learning comprises many directions: learning the distribution that best explains data (Gaussian Mixture Models learned via an EMalgorithm [20], Restricted Boltzmann Machines [10], Generative Adversarial Nets [7]), clustering, dimensionality reduction and unsupervised segmentation (kMeans [9], LLE [29], Isomap [32], PCA [12], Normalized Cuts [31], t-SNE [34]), learning to simulate task solvers (when a solution is provided by the solver and the task is automatically generated [14], [16]), and learning data representation suitable for further use in some other task (autoencoders [11], deep convolutional adversarial nets [28], learning by context prediction [6], learning from tracking in videos [36], metric learning [37], learning by predicting inpainting [26], learning by solving jigsaw puzzles [25]).", "startOffset": 786, "endOffset": 790}, {"referenceID": 24, "context": "Currently, unsupervised learning comprises many directions: learning the distribution that best explains data (Gaussian Mixture Models learned via an EMalgorithm [20], Restricted Boltzmann Machines [10], Generative Adversarial Nets [7]), clustering, dimensionality reduction and unsupervised segmentation (kMeans [9], LLE [29], Isomap [32], PCA [12], Normalized Cuts [31], t-SNE [34]), learning to simulate task solvers (when a solution is provided by the solver and the task is automatically generated [14], [16]), and learning data representation suitable for further use in some other task (autoencoders [11], deep convolutional adversarial nets [28], learning by context prediction [6], learning from tracking in videos [36], metric learning [37], learning by predicting inpainting [26], learning by solving jigsaw puzzles [25]).", "startOffset": 827, "endOffset": 831}, {"referenceID": 5, "context": "Designing such a task is non-trivial, therefore only few successful approaches exist (for example, [6]).", "startOffset": 99, "endOffset": 102}, {"referenceID": 17, "context": "These include the DoG detector [18], the Harris corner detector [8] and its affine-covariant version [21], the FAST corner detector [33] and the MSER detector [19].", "startOffset": 31, "endOffset": 35}, {"referenceID": 7, "context": "These include the DoG detector [18], the Harris corner detector [8] and its affine-covariant version [21], the FAST corner detector [33] and the MSER detector [19].", "startOffset": 64, "endOffset": 67}, {"referenceID": 20, "context": "These include the DoG detector [18], the Harris corner detector [8] and its affine-covariant version [21], the FAST corner detector [33] and the MSER detector [19].", "startOffset": 101, "endOffset": 105}, {"referenceID": 32, "context": "These include the DoG detector [18], the Harris corner detector [8] and its affine-covariant version [21], the FAST corner detector [33] and the MSER detector [19].", "startOffset": 132, "endOffset": 136}, {"referenceID": 18, "context": "These include the DoG detector [18], the Harris corner detector [8] and its affine-covariant version [21], the FAST corner detector [33] and the MSER detector [19].", "startOffset": 159, "endOffset": 163}, {"referenceID": 37, "context": "Most recently, there also emerged methods that do supervised learning building upon a hand-crafted solution: LIFT [38] aims to extract an SfMsurviving subset of DoG detections, TILDE [35] uses DoG for collecting the training set, [15] samples training points only where LoG filter gives large absolute-value response.", "startOffset": 114, "endOffset": 118}, {"referenceID": 34, "context": "Most recently, there also emerged methods that do supervised learning building upon a hand-crafted solution: LIFT [38] aims to extract an SfMsurviving subset of DoG detections, TILDE [35] uses DoG for collecting the training set, [15] samples training points only where LoG filter gives large absolute-value response.", "startOffset": 183, "endOffset": 187}, {"referenceID": 14, "context": "Most recently, there also emerged methods that do supervised learning building upon a hand-crafted solution: LIFT [38] aims to extract an SfMsurviving subset of DoG detections, TILDE [35] uses DoG for collecting the training set, [15] samples training points only where LoG filter gives large absolute-value response.", "startOffset": 230, "endOffset": 234}, {"referenceID": 26, "context": "Several works mention this complex issue ([27], [2], [13], [23]) but do not propose a general solution.", "startOffset": 42, "endOffset": 46}, {"referenceID": 1, "context": "Several works mention this complex issue ([27], [2], [13], [23]) but do not propose a general solution.", "startOffset": 48, "endOffset": 51}, {"referenceID": 12, "context": "Several works mention this complex issue ([27], [2], [13], [23]) but do not propose a general solution.", "startOffset": 53, "endOffset": 57}, {"referenceID": 22, "context": "Several works mention this complex issue ([27], [2], [13], [23]) but do not propose a general solution.", "startOffset": 59, "endOffset": 63}, {"referenceID": 21, "context": "As investigated by [22], a good detector should produce interest points that are robust to viewpoint/illumination changes (to detect the same points and further match them) and sparse (to make feature matching feasible).", "startOffset": 19, "endOffset": 23}, {"referenceID": 17, "context": "This pipeline is followed by many detectors including the popular DoG detector [18].", "startOffset": 79, "endOffset": 83}, {"referenceID": 17, "context": "Third, we do accurate localization based on the second-order Taylor expansion of the response function around potential interest points [18].", "startOffset": 136, "endOffset": 140}, {"referenceID": 21, "context": "For quantitative evaluation, we use the repeatability measure described in [22] (with the overlap threshold parameter equal 40%).", "startOffset": 75, "endOffset": 79}, {"referenceID": 3, "context": "\u2022 e for the ELU non-linearity function [4],", "startOffset": 39, "endOffset": 42}, {"referenceID": 12, "context": "1) with random rotations from [0, 2\u03c0] and random scale changes from [ 13 , 3].", "startOffset": 68, "endOffset": 77}, {"referenceID": 2, "context": "1) with random rotations from [0, 2\u03c0] and random scale changes from [ 13 , 3].", "startOffset": 68, "endOffset": 77}, {"referenceID": 38, "context": "To optimize the objective (7), we use the Adadelta algorithm [39], which is a version of gradient descent that chooses the gradient step size per-parameter automatically.", "startOffset": 61, "endOffset": 65}, {"referenceID": 4, "context": "We implement the model and optimization on a GPU (Nvidia Titan X) using the Torch7 framework [5].", "startOffset": 93, "endOffset": 96}, {"referenceID": 0, "context": "We used the DTU Robot Image Dataset [1].", "startOffset": 36, "endOffset": 39}, {"referenceID": 21, "context": "We used the Oxford VGG dataset [22], commonly chosen for this kind of evaluation.", "startOffset": 31, "endOffset": 35}, {"referenceID": 21, "context": "For that we use the same matching score as in [22], i.", "startOffset": 46, "endOffset": 50}, {"referenceID": 21, "context": "Matching score (the higher, the better) of DoG and our methods (Linear, Non-linear) on the benchmark from [22].", "startOffset": 106, "endOffset": 110}, {"referenceID": 0, "context": "1]) and large warps (s sample uniformly from [1, 2]).", "startOffset": 45, "endOffset": 51}, {"referenceID": 1, "context": "1]) and large warps (s sample uniformly from [1, 2]).", "startOffset": 45, "endOffset": 51}, {"referenceID": 21, "context": "We used the Oxford VGG dataset [22] (same as in the previous experiment).", "startOffset": 31, "endOffset": 35}, {"referenceID": 23, "context": "We learn a detector between RGB and depth images by training on the NYUv2 dataset [24].", "startOffset": 82, "endOffset": 86}, {"referenceID": 2, "context": "The DoG filter parameters default to the standard implementation [3].", "startOffset": 65, "endOffset": 68}], "year": 2017, "abstractText": "Several machine learning tasks require to represent the data using only a sparse set of interest points. An ideal detector is able to find the corresponding interest points even if the data undergo a transformation typical for a given domain. Since the task is of high practical interest in computer vision, many hand-crafted solutions were proposed. In this paper, we ask a fundamental question: can we learn such detectors from scratch? Since it is often unclear what points are \"interesting\", human labelling cannot be used to find a truly unbiased solution. Therefore, the task requires an unsupervised formulation. We are the first to propose such a formulation: training a neural network to rank points in a transformation-invariant manner. Interest points are then extracted from the top/bottom quantiles of this ranking. We validate our approach on two tasks: standard RGB image interest point detection and challenging cross-modal interest point detection between RGB and depth images. We quantitatively show that our unsupervised method performs better or on-par with baselines.", "creator": "LaTeX with hyperref package"}}}