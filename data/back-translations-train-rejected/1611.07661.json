{"id": "1611.07661", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Nov-2016", "title": "Multigrid Neural Architectures", "abstract": "We propose a multigrid extension of convolutional neural networks (CNNs). Rather than manipulating representations living on a single spatial grid, our network layers operate across scale space, on a pyramid of tensors. They consume multigrid inputs and produce multigrid outputs; convolutional filters themselves have both within-scale and cross-scale extent. This aspect is distinct from simple multiscale designs, which only process the input at different scales. Viewed in terms of information flow, a multigrid network passes messages across a spatial pyramid. As a consequence, receptive field size grows exponentially with depth, facilitating rapid integration of context. Most critically, multigrid structure enables networks to learn internal attention and dynamic routing mechanisms, and use them to accomplish tasks on which modern CNNs fail.", "histories": [["v1", "Wed, 23 Nov 2016 06:55:53 GMT  (815kb,D)", "http://arxiv.org/abs/1611.07661v1", "Code available:this http URL"], ["v2", "Thu, 11 May 2017 19:24:33 GMT  (818kb,D)", "http://arxiv.org/abs/1611.07661v2", "updated with ImageNet results; to appear at CVPR 2017"]], "COMMENTS": "Code available:this http URL", "reviews": [], "SUBJECTS": "cs.CV cs.LG cs.NE", "authors": ["tsung-wei ke", "michael maire", "stella x yu"], "accepted": false, "id": "1611.07661"}, "pdf": {"name": "1611.07661.pdf", "metadata": {"source": "CRF", "title": "Neural Multigrid", "authors": ["Tsung-Wei Ke", "Michael Maire", "Stella X. Yu"], "emails": ["twke@icsi.berkeley.edu", "mmaire@ttic.edu", "stellayu@berkeley.edu"], "sections": [{"heading": null, "text": "Experiments show far-reaching performance benefits of multigrid. In CIFAR image classification, switching from single to multigrid within standard CNN architectures improves accuracy with modest arithmetic and parameter gains. Multigrid is independent of other architectural decisions; we show synergistic results in combination with residual connections. Significant gains can be made in tasks requiring output per pixel. We show dramatic improvements over a synthetic semantic segmentation dataset. Remarkably, relatively flat multigrid networks can learn to perform spatial transformation tasks directly, while current CNNs fail to do so."}, {"heading": "1. Introduction", "text": "In fact, most of them are able to outdo themselves, and they are able to outdo themselves, \"he said in an interview with Welt am Sonntag.\" I don't think I'm able to outdo myself, \"he said.\" I don't think I'm able to outdo myself. \"He added,\" I don't think I'm able to outdo myself, but I don't think I'm able to outdo myself. \""}, {"heading": "2. Related Work", "text": "In the wake of AlexNet [18], research into CNNs in the field of computer vision has distilled some rules of thumb for their design. Small, e.g. 3 x 3 cm, spatial filters in many successive layers provide efficient parameter allocation [28, 30, 9]. Functional channels should increase with a reduction in spatial resolution [18, 28], e.g. a doubling as shown in Figure 1. Deeper networks are better as long as a means of overcoming disappearing slopes is built into the training process [30, 13] or the network itself [29, 9, 19, 12]. Width also matters [34]. Recent efforts to improve the computerized efficiency of CNNs, although too numerous to enumerate them fully, often rely on parameters [33, 3, 14] or precision [25, 4] reduction. We focus on various aspects of efficiency that, while traditionally influencing computer vision, are not yet coming into play."}, {"heading": "3. Multigrid Architectures", "text": "\"I don't think we will be able to find a solution,\" he told the German Press Agency in Berlin. \"I don't think we will be able to find a solution,\" he said. \"But I don't think we will engage in a solution.\" He added, \"I don't think we will agree on a solution.\" He added, \"I don't think we will engage in a solution.\" He added, \"I don't think we will engage in a solution.\" He added, \"I don't think we will engage in a solution.\" He added, \"I don't think we will engage in a solution.\""}, {"heading": "3.1. Pooling", "text": "Similarly, upsampling facilitates lateral communication from coarse to fine meshes. Instead of localizing these operations in some fixed stages of the pipeline, we actually insert them everywhere. However, their action is now lateral, between networks of different sizes in the same layer, rather than between networks of different sizes in two successive layers at depth. While Figure 1 is drawn with additional maximum pooling and subsampling levels that affect whole pyramids in depth, we also consider pipelines that do not shrink pyramids. Rather, they simply add an output to a particular network (and possibly trim a few networks outside its communication area).The success of this strategy may be motivation for rethinking the role of pooling in CNNs. Standard CNNs could actually misuse it as an explicit pooling mechanism to apply the communication mechanisms as summarizing mechanisms that prove to be perbilization mechanisms."}, {"heading": "3.2. Progressive Multigrid", "text": "If we think only of the communication pattern underlying a forward move through a multigride CNN, we find a strong analogy to the multi-scale multigride calculation scheme of Maire and Yu [22]. Although their proposed self-solver can be considered a linear diffusion process on a multi-scale pyramid in a different mathematical environment, we consider a multigride CNN to be a non-linear process on a similar pyramid with the same communication structure. The analogy is strong enough that we port its progressive multigride calculation scheme directly to our environment. A network layer corresponds to an iteration of a solver. Instead of starting work directly on the full pyramid, a progressive multigrider CNN can spend several layers processing only a coarse grid, followed by several additional layers processing a small, coarse and middle grid pyramid before the rest of the network works with the full pyramid."}, {"heading": "3.3. Model Zoo", "text": "Figure 3 shows the variety of network architectures we evaluate. For classification, we use a network with slight differences from those of Simonyan and Zisserman [28] as a starting point. In minor misuse of notation, we refer to the modified version in Figure 3 as VGG. Our 16-layer version, VGG-16, consists of 5 sections with different layers each, with pooling and subsampling between. A final Softmax layer produces class prediction (on CIFAR-100). The conventional layers use all 3 \u00d7 3 filters. We instantiate the variants with different depth by changing the number of layers per section (e.g. VGG-11, 4 for VGG-21)."}, {"heading": "4. Experiments", "text": "Our experiments focus on systematic research and evaluation of architectural design space. [1] The goal is to quantify the relative utility of multigrid and its synergistic combination with residual compounds, rather than achieving absolute performance for a particular dataset."}, {"heading": "4.1. Classification", "text": "We evaluate the array of network architectures listed in Table 1 against the task of CIFAR-100 [17] image classification. Data pre-processing and magnification. First, the data is brightened and then each image is magnified to 36 x 36. We use random 32 x 32 patches from the training set and the mean 32 x 32 patch from the test examples. Each training image is subject to random horizontal flipping. There are 50,000 training images and 10,000 test images. Training methods. We use SGD with training batch size 128, weight decay rate 0.0005 and 300 iterations per epoch for 200 epochs. For VGG, NMG and P-NMG, our learning rate starts at 0.1 and decays exponentially to 0.0001; while for RES, R-NMG and PR-NMG our learning rate starts at 0.1 with simultaneous quantification of inaccuracies and 0.2 decays every 60 epochs."}, {"heading": "4.2. Semantic Segmentation", "text": "We generate synthetic data for semantic segmentation in the following steps. First, we randomly select about 5 \u00b1 2 digits from the MNIST training set [20], with the number of digits following the Gaussian distribution with the mean 5 and the standard deviation 0.5. Each digit is recalculated by a uniform random factor between 0.5 and 1.25 of the original size and rotated by a uniform random angle between \u2212 60 and 60 \u00b0. We randomly select a location on a 64 \u00d7 64 screen for each transformed digit and stick it on the canvas to ensure that each newly added digit does not overlap with existing ones and by more than 30%. We generate 10,000 training images and 1,000 test images. For test images, we use digits from the MNIST test to verify any overlap with the training. In addition to the aforementioned networks, we consider a single digit number (SG) that represents a decubitation rate of NIS-1, but reflects all the NIS-P values most finely."}, {"heading": "4.3. Spatial Transformers", "text": "Jaderberg et al. [16] develop a system for reversing spatial distortions and translations by combining a neural network for estimating the parameters of transformation with a special unit for applying the inversion. We report here that such a division of the problem into two components seems necessary when using standard CNNs; they cannot fully learn the task. However, progressive multigride networks are capable of learning such tasks. Our synthetic dataset for spatial transformation is generated in the same way as that for semantic segmentation, except that (1) there is only one digit, (2) the conversion factor has a range of (0,5,1,5), and (3) there is an additional affine transformation with a uniformly random transformation angle in (60%, 60%). The task is no longer labeling, but rather the output of the original, pristine digital instance at the center of the channels."}, {"heading": "5. Conclusion", "text": "Our proposed multigride extension to CNNs leads to improved accuracy in classification and semantic segmentation tasks. Progressive multigride variants open a new avenue for optimizing the efficiency of CNNs. Multigrid seems unique in extending the range of tasks that CNNs can accomplish by integrating the ability to learn routing and attention mechanisms into the network structure. These new capabilities suggest that Multigrid could replace some of the ad hoc mechanisms in the current zoo of the CNN architecture. In summary, it can be speculated that multigride neural networks may also have broader implications for neuroscience. Forward-facing composition on a sequence of pyramids with multiple networks looks quite similar to combined bottom-up / up processing over a single larger structure when all neurons are embedded in the larger computorial substrate according to their spatial resolution, rather than their depth in the processing chain."}], "references": [{"title": "Segnet: A deep convolutional encoder-decoder architecture for image segmentation", "author": ["V. Badrinarayanan", "A. Kendall", "R. Cipolla"], "venue": "arXiv:1511.00561", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2015}, {"title": "Semantic image segmentation with deep convolutional nets and fully connected crfs", "author": ["L.-C. Chen", "G. Papandreou", "I. Kokkinos", "K. Murphy", "A.L. Yuille"], "venue": "ICLR", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2015}, {"title": "Compressing neural networks with the hashing trick", "author": ["W. Chen", "J.T. Wilson", "S. Tyree", "K.Q. Weinberger", "Y. Chen"], "venue": "arXiv:1504.04788", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2015}, {"title": "Binaryconnect: Training deep neural networks with binary weights during propagations", "author": ["M. Courbariaux", "Y. Bengio", "J.-P. David"], "venue": "NIPS", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2015}, {"title": "Depth map prediction from a single image using a multi-scale deep network", "author": ["D. Eigen", "C. Puhrsch", "R. Fergus"], "venue": "NIPS", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning hierarchical features for scene labeling", "author": ["C. Farabet", "C. Couprie", "L. Najman", "Y. LeCun"], "venue": "PAMI", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2013}, {"title": "Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position", "author": ["K. Fukushima"], "venue": "Biological Cybernetics", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1980}, {"title": "Hypercolumns for object segmentation and fine-grained localization", "author": ["B. Hariharan", "P. Arbelaez", "R. Girshick", "J. Malik"], "venue": "CVPR", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep residual learning for image recognition", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "CVPR", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2016}, {"title": "Identity mappings in deep residual networks", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "arXiv:1603.05027", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2016}, {"title": "Bottom-up and top-down reasoning with convolutional latent-variable models", "author": ["P. Hu", "D. Ramanan"], "venue": "CVPR", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2016}, {"title": "Densely connected convolutional networks", "author": ["G. Huang", "Z. Liu", "K.Q. Weinberger"], "venue": "arXiv:1608.06993", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2016}, {"title": "Deep networks with stochastic depth", "author": ["G. Huang", "Y. Sun", "Z. Liu", "D. Sedra", "K. Weinberger"], "venue": "arXiv:1603.09382", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2016}, {"title": "SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and <1MB model size", "author": ["F.N. Iandola", "M.W. Moskewicz", "K. Ashraf", "S. Han", "W.J. Dally", "K. Keutzer"], "venue": "arXiv:1602.07360", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2016}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["S. Ioffe", "C. Szegedy"], "venue": "ICML", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2015}, {"title": "Spatial transformer networks", "author": ["M. Jaderberg", "K. Simonyan", "A. Zisserman", "K. Kavukcuoglu"], "venue": "arXiv:1506.02025", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning multiple layers of features from tiny images", "author": ["A. Krizhevsky"], "venue": "Technical report", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2009}, {"title": "ImageNet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "NIPS", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2012}, {"title": "Fractalnet: Ultra-deep neural networks without residuals", "author": ["G. Larsson", "M. Maire", "G. Shakhnarovich"], "venue": "arXiv:1605.07648", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2016}, {"title": "Gradientbased learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE", "citeRegEx": "20", "shortCiteRegEx": null, "year": 1998}, {"title": "Fully convolutional networks for semantic segmentation", "author": ["J. Long", "E. Shelhamer", "T. Darrell"], "venue": "CVPR", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2015}, {"title": "Progressive multigrid eigensolvers for multiscale spectral segmentation", "author": ["M. Maire", "S.X. Yu"], "venue": "ICCV", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2013}, {"title": "Stacked hourglass networks for human pose estimation", "author": ["A. Newell", "K. Yang", "J. Deng"], "venue": "arXiv:1603.06937", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2016}, {"title": "A neurobiological model of visual attention and invariant pattern recognition based on dynamic routing of information", "author": ["B.A. Olshausen", "C.H. Anderson", "D.C.V. Essen"], "venue": "Journal of Neuroscience", "citeRegEx": "24", "shortCiteRegEx": null, "year": 1993}, {"title": "Xnornet: Imagenet classification using binary convolutional neural networks", "author": ["M. Rastegari", "V. Ordonez", "J. Redmon", "A. Farhadi"], "venue": "arXiv:1603.05279", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2016}, {"title": "U-net: Convolutional networks for biomedical image segmentation", "author": ["O. Ronneberger", "P. Fischer", "T. Brox"], "venue": "arXiv:1505.04597", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2015}, {"title": "Hierarchy and adaptivity in segmenting visual scenes", "author": ["E. Sharon", "M. Galun", "D. Sharon", "R. Basri", "A. Brandt"], "venue": "Nature, 442:810\u2013813", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2006}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "ICLR", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2015}, {"title": "Highway networks", "author": ["R.K. Srivastava", "K. Greff", "J. Schmidhuber"], "venue": "ICML", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2015}, {"title": "Going deeper with convolutions", "author": ["C. Szegedy", "W. Liu", "Y. Jia", "P. Sermanet", "S. Reed", "D. Anguelov", "D. Erhan", "V. Vanhoucke", "A. Rabinovich"], "venue": "CVPR", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2015}, {"title": "Robust real-time face detection", "author": ["P. Viola", "M.J. Jones"], "venue": "IJCV", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2004}, {"title": "Holistically-nested edge detection", "author": ["S. Xie", "Z. Tu"], "venue": "ICCV", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2015}, {"title": "N", "author": ["Z. Yang", "M. Moczulski", "M. Denil"], "venue": "de Freitas, A. J. Smola, L. Song, and Z. Wang. Deep fried convnets. ICCV", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2015}, {"title": "Wide residual networks", "author": ["S. Zagoruyko", "N. Komodakis"], "venue": "arXiv:1605.07146", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 6, "context": "Introduction Since Fukushima\u2019s neocognitron [7], the basic architectural design of convolutional neural networks has persisted in form similar to that shown in the top of Figure 1.", "startOffset": 44, "endOffset": 47}, {"referenceID": 19, "context": "Work following this mold includes LeNet [20], the breakthrough AlexNet [18], and the many architectural enhancements that followed, such as VGG [28], GoogLeNet [30], residual networks [9, 10, 34] and like [19, 12].", "startOffset": 40, "endOffset": 44}, {"referenceID": 17, "context": "Work following this mold includes LeNet [20], the breakthrough AlexNet [18], and the many architectural enhancements that followed, such as VGG [28], GoogLeNet [30], residual networks [9, 10, 34] and like [19, 12].", "startOffset": 71, "endOffset": 75}, {"referenceID": 27, "context": "Work following this mold includes LeNet [20], the breakthrough AlexNet [18], and the many architectural enhancements that followed, such as VGG [28], GoogLeNet [30], residual networks [9, 10, 34] and like [19, 12].", "startOffset": 144, "endOffset": 148}, {"referenceID": 29, "context": "Work following this mold includes LeNet [20], the breakthrough AlexNet [18], and the many architectural enhancements that followed, such as VGG [28], GoogLeNet [30], residual networks [9, 10, 34] and like [19, 12].", "startOffset": 160, "endOffset": 164}, {"referenceID": 8, "context": "Work following this mold includes LeNet [20], the breakthrough AlexNet [18], and the many architectural enhancements that followed, such as VGG [28], GoogLeNet [30], residual networks [9, 10, 34] and like [19, 12].", "startOffset": 184, "endOffset": 195}, {"referenceID": 9, "context": "Work following this mold includes LeNet [20], the breakthrough AlexNet [18], and the many architectural enhancements that followed, such as VGG [28], GoogLeNet [30], residual networks [9, 10, 34] and like [19, 12].", "startOffset": 184, "endOffset": 195}, {"referenceID": 33, "context": "Work following this mold includes LeNet [20], the breakthrough AlexNet [18], and the many architectural enhancements that followed, such as VGG [28], GoogLeNet [30], residual networks [9, 10, 34] and like [19, 12].", "startOffset": 184, "endOffset": 195}, {"referenceID": 18, "context": "Work following this mold includes LeNet [20], the breakthrough AlexNet [18], and the many architectural enhancements that followed, such as VGG [28], GoogLeNet [30], residual networks [9, 10, 34] and like [19, 12].", "startOffset": 205, "endOffset": 213}, {"referenceID": 11, "context": "Work following this mold includes LeNet [20], the breakthrough AlexNet [18], and the many architectural enhancements that followed, such as VGG [28], GoogLeNet [30], residual networks [9, 10, 34] and like [19, 12].", "startOffset": 205, "endOffset": 213}, {"referenceID": 20, "context": "For tasks requiring fine-scale output, such as semantic segmentation, this necessitates specialized designs for reintegrating spatial information [21, 8, 2, 1].", "startOffset": 146, "endOffset": 159}, {"referenceID": 7, "context": "For tasks requiring fine-scale output, such as semantic segmentation, this necessitates specialized designs for reintegrating spatial information [21, 8, 2, 1].", "startOffset": 146, "endOffset": 159}, {"referenceID": 1, "context": "For tasks requiring fine-scale output, such as semantic segmentation, this necessitates specialized designs for reintegrating spatial information [21, 8, 2, 1].", "startOffset": 146, "endOffset": 159}, {"referenceID": 0, "context": "For tasks requiring fine-scale output, such as semantic segmentation, this necessitates specialized designs for reintegrating spatial information [21, 8, 2, 1].", "startOffset": 146, "endOffset": 159}, {"referenceID": 29, "context": "This may be one reason extremely deep networks [30, 9, 19] appear necessary; many layers are needed to counteract inefficient signal propagation.", "startOffset": 47, "endOffset": 58}, {"referenceID": 8, "context": "This may be one reason extremely deep networks [30, 9, 19] appear necessary; many layers are needed to counteract inefficient signal propagation.", "startOffset": 47, "endOffset": 58}, {"referenceID": 18, "context": "This may be one reason extremely deep networks [30, 9, 19] appear necessary; many layers are needed to counteract inefficient signal propagation.", "startOffset": 47, "endOffset": 58}, {"referenceID": 23, "context": "[24], emerge within them.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "Related Work In wake of AlexNet [18], exploration of CNNs across computer vision has distilled some rules of thumb for their design.", "startOffset": 32, "endOffset": 36}, {"referenceID": 27, "context": "3\u00d7 3, spatial filters, in many successive layers make for efficient parameter allocation [28, 30, 9].", "startOffset": 89, "endOffset": 100}, {"referenceID": 29, "context": "3\u00d7 3, spatial filters, in many successive layers make for efficient parameter allocation [28, 30, 9].", "startOffset": 89, "endOffset": 100}, {"referenceID": 8, "context": "3\u00d7 3, spatial filters, in many successive layers make for efficient parameter allocation [28, 30, 9].", "startOffset": 89, "endOffset": 100}, {"referenceID": 17, "context": "Feature channels should increase with spatial resolution reduction [18, 28], e.", "startOffset": 67, "endOffset": 75}, {"referenceID": 27, "context": "Feature channels should increase with spatial resolution reduction [18, 28], e.", "startOffset": 67, "endOffset": 75}, {"referenceID": 29, "context": "Deeper networks are better, so long as a means of overcoming vanishing gradients is engineered into the training process [30, 13] or the network itself [29, 9, 19, 12].", "startOffset": 121, "endOffset": 129}, {"referenceID": 12, "context": "Deeper networks are better, so long as a means of overcoming vanishing gradients is engineered into the training process [30, 13] or the network itself [29, 9, 19, 12].", "startOffset": 121, "endOffset": 129}, {"referenceID": 28, "context": "Deeper networks are better, so long as a means of overcoming vanishing gradients is engineered into the training process [30, 13] or the network itself [29, 9, 19, 12].", "startOffset": 152, "endOffset": 167}, {"referenceID": 8, "context": "Deeper networks are better, so long as a means of overcoming vanishing gradients is engineered into the training process [30, 13] or the network itself [29, 9, 19, 12].", "startOffset": 152, "endOffset": 167}, {"referenceID": 18, "context": "Deeper networks are better, so long as a means of overcoming vanishing gradients is engineered into the training process [30, 13] or the network itself [29, 9, 19, 12].", "startOffset": 152, "endOffset": 167}, {"referenceID": 11, "context": "Deeper networks are better, so long as a means of overcoming vanishing gradients is engineered into the training process [30, 13] or the network itself [29, 9, 19, 12].", "startOffset": 152, "endOffset": 167}, {"referenceID": 33, "context": "Width also matters [34].", "startOffset": 19, "endOffset": 23}, {"referenceID": 32, "context": "Recent efforts to improve the computational efficiency of CNNs, though too numerous to list in full, often attack via parameter [33, 3, 14] or precision [25, 4] reduction.", "startOffset": 128, "endOffset": 139}, {"referenceID": 2, "context": "Recent efforts to improve the computational efficiency of CNNs, though too numerous to list in full, often attack via parameter [33, 3, 14] or precision [25, 4] reduction.", "startOffset": 128, "endOffset": 139}, {"referenceID": 13, "context": "Recent efforts to improve the computational efficiency of CNNs, though too numerous to list in full, often attack via parameter [33, 3, 14] or precision [25, 4] reduction.", "startOffset": 128, "endOffset": 139}, {"referenceID": 24, "context": "Recent efforts to improve the computational efficiency of CNNs, though too numerous to list in full, often attack via parameter [33, 3, 14] or precision [25, 4] reduction.", "startOffset": 153, "endOffset": 160}, {"referenceID": 3, "context": "Recent efforts to improve the computational efficiency of CNNs, though too numerous to list in full, often attack via parameter [33, 3, 14] or precision [25, 4] reduction.", "startOffset": 153, "endOffset": 160}, {"referenceID": 30, "context": "Viola and Jones [31].", "startOffset": 16, "endOffset": 20}, {"referenceID": 26, "context": "Multigrid methods have a history of use in image segmentation algorithms [27, 22].", "startOffset": 73, "endOffset": 81}, {"referenceID": 21, "context": "Multigrid methods have a history of use in image segmentation algorithms [27, 22].", "startOffset": 73, "endOffset": 81}, {"referenceID": 20, "context": "These include skip connections and upsampling [21, 2], hypercolumns [8], and, autoencoder-like, hourglass or U-shaped networks that re-", "startOffset": 46, "endOffset": 53}, {"referenceID": 1, "context": "These include skip connections and upsampling [21, 2], hypercolumns [8], and, autoencoder-like, hourglass or U-shaped networks that re-", "startOffset": 46, "endOffset": 53}, {"referenceID": 7, "context": "These include skip connections and upsampling [21, 2], hypercolumns [8], and, autoencoder-like, hourglass or U-shaped networks that re-", "startOffset": 68, "endOffset": 71}, {"referenceID": 8, "context": "Right: A building block (res-mg-unit) for residual connectivity [9] in multigrid networks.", "startOffset": 64, "endOffset": 67}, {"referenceID": 0, "context": "duce and re-expand spatial grids [1, 11, 23, 26].", "startOffset": 33, "endOffset": 48}, {"referenceID": 10, "context": "duce and re-expand spatial grids [1, 11, 23, 26].", "startOffset": 33, "endOffset": 48}, {"referenceID": 22, "context": "duce and re-expand spatial grids [1, 11, 23, 26].", "startOffset": 33, "endOffset": 48}, {"referenceID": 25, "context": "duce and re-expand spatial grids [1, 11, 23, 26].", "startOffset": 33, "endOffset": 48}, {"referenceID": 5, "context": "Even past multiscale CNN work, such as [6, 5, 32], does not consider embedded and continual cross-scale communication.", "startOffset": 39, "endOffset": 49}, {"referenceID": 4, "context": "Even past multiscale CNN work, such as [6, 5, 32], does not consider embedded and continual cross-scale communication.", "startOffset": 39, "endOffset": 49}, {"referenceID": 31, "context": "Even past multiscale CNN work, such as [6, 5, 32], does not consider embedded and continual cross-scale communication.", "startOffset": 39, "endOffset": 49}, {"referenceID": 27, "context": "This difference in information routing capability is particularly dramatic in light of the recent trend towards stacking many layers of small 3\u00d7 3 convolutional filters [28, 9].", "startOffset": 169, "endOffset": 176}, {"referenceID": 8, "context": "This difference in information routing capability is particularly dramatic in light of the recent trend towards stacking many layers of small 3\u00d7 3 convolutional filters [28, 9].", "startOffset": 169, "endOffset": 176}, {"referenceID": 0, "context": "In standard CNNs, this virtually guarantees that either very deep networks, or manually-added pooling and unpooling stages [1, 11, 23, 26], will be needed to propagate information across the pixel grid.", "startOffset": 123, "endOffset": 138}, {"referenceID": 10, "context": "In standard CNNs, this virtually guarantees that either very deep networks, or manually-added pooling and unpooling stages [1, 11, 23, 26], will be needed to propagate information across the pixel grid.", "startOffset": 123, "endOffset": 138}, {"referenceID": 22, "context": "In standard CNNs, this virtually guarantees that either very deep networks, or manually-added pooling and unpooling stages [1, 11, 23, 26], will be needed to propagate information across the pixel grid.", "startOffset": 123, "endOffset": 138}, {"referenceID": 25, "context": "In standard CNNs, this virtually guarantees that either very deep networks, or manually-added pooling and unpooling stages [1, 11, 23, 26], will be needed to propagate information across the pixel grid.", "startOffset": 123, "endOffset": 138}, {"referenceID": 8, "context": "Given recent widespread use of residual networks [9], we would be remiss not to consider a multigrid extension of them.", "startOffset": 49, "endOffset": 52}, {"referenceID": 8, "context": "The right side of Figure 2 shows a multigrid analogue of the original residual unit [9].", "startOffset": 84, "endOffset": 87}, {"referenceID": 14, "context": "Convolution acts jointly across the multigrid pyramid, while batch normalization (BN) [15] and ReLU apply separately to each grid in the pyramid.", "startOffset": 86, "endOffset": 90}, {"referenceID": 9, "context": "Similar extensions are possible with alternative residual units [10], but we leave such exploration to future work.", "startOffset": 64, "endOffset": 68}, {"referenceID": 25, "context": "Bottom: Semantic segmentation (SEG) and spatial transformation (SPT) architectures: U-NET [26], and our progressive multigrid alternative (P-NMG).", "startOffset": 90, "endOffset": 94}, {"referenceID": 21, "context": "Progressive Multigrid Thinking only of the communication pattern underlying a forward pass performed by a multigrid CNN, we find a strong analogy to the multiscale multigrid computational scheme of Maire and Yu [22].", "startOffset": 211, "endOffset": 215}, {"referenceID": 27, "context": "For classification, we take a network with minor differences from that of Simonyan and Zisserman [28] as our baseline.", "startOffset": 97, "endOffset": 101}, {"referenceID": 25, "context": "[26].", "startOffset": 0, "endOffset": 4}, {"referenceID": 25, "context": "Again, convolutional filters are 3 \u00d7 3, except for the layers immediately following upsampling; here we use 2 \u00d7 2 filters, following [26].", "startOffset": 133, "endOffset": 137}, {"referenceID": 16, "context": "Classification We evaluate the array of network architectures listed in Table 1 on the task of CIFAR-100 [17] image classification.", "startOffset": 105, "endOffset": 109}, {"referenceID": 19, "context": "We first pick randomly about 5\u00b12 digits from the training set of MNIST [20], where the number of digits follows the Gaussian distribution with mean value 5 and standard deviation 0.", "startOffset": 71, "endOffset": 75}, {"referenceID": 15, "context": "[16] engineer a system for inverting spatial distortions and translations by combining an neural network for estimating the parameters of the transformation with a dedicated unit for applying the inverse.", "startOffset": 0, "endOffset": 4}], "year": 2016, "abstractText": "We propose a multigrid extension of convolutional neural networks (CNNs). Rather than manipulating representations living on a single spatial grid, our network layers operate across scale space, on a pyramid of tensors. They consume multigrid inputs and produce multigrid outputs; convolutional filters themselves have both within-scale and cross-scale extent. This aspect is distinct from simple multiscale designs, which only process the input at different scales. Viewed in terms of information flow, a multigrid network passes messages across a spatial pyramid. As a consequence, receptive field size grows exponentially with depth, facilitating rapid integration of context. Most critically, multigrid structure enables networks to learn internal attention and dynamic routing mechanisms, and use them to accomplish tasks on which modern CNNs fail. Experiments demonstrate wide-ranging performance advantages of multigrid. On CIFAR image classification, flipping from single to multigrid within standard CNN architectures improves accuracy at modest compute and parameter increase. Multigrid is independent of other architectural choices; we show synergistic results in combination with residual connections. On tasks demanding per-pixel output, gains can be substantial. We show dramatic improvement on a synthetic semantic segmentation dataset. Strikingly, we show that relatively shallow multigrid networks can learn to directly perform spatial transformation tasks, where, in contrast, current CNNs fail. Together, our results suggest that continuous evolution of features on a multigrid pyramid could replace virtually all existing CNN designs.", "creator": "LaTeX with hyperref package"}}}