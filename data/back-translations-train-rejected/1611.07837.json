{"id": "1611.07837", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Nov-2016", "title": "Adaptive Feature Abstraction for Translating Video to Language", "abstract": "A new model for video captioning is developed, using a deep three-dimensional Convolutional Neural Network (C3D) as an encoder for videos and a Recurrent Neural Network (RNN) as a decoder for captions. We consider both \"hard\" and \"soft\" attention mechanisms, to adaptively and sequentially focus on different layers of features (levels of feature \"abstraction\"), as well as local spatiotemporal regions of the feature maps at each layer. The proposed approach is evaluated on three benchmark datasets: YouTube2Text, M-VAD and MSR-VTT. Along with visualizing the results and how the model works, these experiments quantitatively demonstrate the effectiveness of the proposed adaptive spatiotemporal feature abstraction for translating videos to sentences with rich semantics.", "histories": [["v1", "Wed, 23 Nov 2016 15:21:48 GMT  (4421kb,D)", "http://arxiv.org/abs/1611.07837v1", null]], "reviews": [], "SUBJECTS": "cs.CV cs.CL", "authors": ["yunchen pu", "martin renqiang min", "zhe gan", "lawrence carin"], "accepted": false, "id": "1611.07837"}, "pdf": {"name": "1611.07837.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Yunchen Pu", "Martin Renqiang Min"], "emails": ["yunchen.pu@duke.edu", "renqiang@nec-labs.com", "zhe.gan@duke.edu", "lcarin@duke.edu"], "sections": [{"heading": "1 INTRODUCTION", "text": "Videos represent the most widely used forms of data, and their accurate characterization is a major challenge for computer vision and machine learning. Generating a description of a video called video captioning is an important component of video analysis, inspired by the successful encoder decoder framework used in machine translation (Cho et al., 2014; Bahdanau et al., 2015; most recent work on video captioning (Donahue et al., 2015) and captioning (Kiros et al., 2014; CNN et al., 2015; Karpathy Li, 2015; Mao et al., 2015), employs a two-dimensional (2D) or three-dimensional (3D) Convolutional Network (CNN) as the encoder."}, {"heading": "2 RELATED WORK", "text": "Early work on video subtitling used a two-step approach, using role word recognition (e.g. subject, verb, and object) and rules for linguistic grammar. In such a work, the sentence for the video description is initially divided into several parts, each of which is oriented toward visual content. Rohrbach et al. (2013; 2014), for example, is used to select an appropriate level of sentence fragments. Statistical language models learned from large corpora text are used to translate the semantic representation into a grammatically correct sentence; in Guadarrama et al. (2013) semantic hierarchies are used to select an appropriate level of sentence fragments. Statistical language models learned from large corpora text are used to translate the semantic representation into a grammatically correct sentence."}, {"heading": "3 METHOD", "text": "Consider N-training videos, the n-th part of which is called X (n), with the accompanying caption Y (n). The length-Tn caption is represented with Y (n) = (y (n) 1,.., y (n) Tn), with y (n) t a 1-of-V (\"one hot\") encoding vector, with V the size of the vocabulary. For each video, the C3D feature extractor (Tran et al., 2015) produces a set of features A (n) = {a (n) 1,.., a (n) L, a (n) L + 1}, where {a) 1,.. a (n) L} are feature cards extracted from L layers, and a (n) L + 1 vector used from the top layer (n)."}, {"heading": "3.1 CAPTION MODEL", "text": "The tenth word in a caption, yt, is mapped to an M-dimensional vector, wt = Weyt, where we are a learned word bed matrix, i.e., wt is a column of We choose a column of We choose (1).The probability of caption Y = {yt} t = 1, T is defined asp (Y | A) = p (y1 | A), T = 2 p (yt | y < t, A). (1) Specifically, the first word y1 of p (y1 | A) = softmax (Vh1), where h1 = tanh (CaL + 1).Bias terms are omitted for simplification throughout the paper. All other words in the caption are then sequentially generated with an RNN until the final sentence symbol is explicitly generated."}, {"heading": "3.2 ATTENTION MECHANISM", "text": "We first define the attention mechanism, which we define as nlF -dimensional vector, to define a three-dimensional level. (D) Each attribute is a 4D tensor, with elements corresponding to two spatial coordinates (i.e., the number of dimensions of this tensor is searched within a given frame), the third tensor index in the frame index dimension, and a fourth dimension associated with the respective dimensions. (D) The number of dimensions of this tensor is shown within a given frame. (D) nlx \u00b7 nly \u00b7 nlz \u00b7 nlF, with the respective dimensions corresponding to the vertical, horizontal, horizontal, and filters. (e.g., nlF convolutionary filters at the layer l). Note that the dimensions nlx, n l, and l vary at the plane l (decreasing l) define an elector correspondingly as F, we define a Vai -nl."}, {"heading": "4 EXPERIMENTS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 DATASETS", "text": "We present results from three benchmark datasets: Microsoft Research Video Description Corpus (YouTube2Text) (Chen & Dolan, 2011), Montreal Video Annotation Dataset (M-VAD) (Torabi et al., 2015) and Microsoft Research Video to Text (MSR-VTT) (Xu et al., 2016).The Youtube2Text contains 1970 YouTube clips, and each video is annotated with about 40 sentences. To be fair, we use the same splits as in Venugopalan et al. (2015b), with 1200 videos for training, 100 videos for validation, and 670 videos for testing.The M-VAD is an extensive dataset for film description, consisting of 46587 film snippets annotated with 56752 sentences. We follow the setting in Torabi et al. (2015), with 36920 videos for training, 4950 videos for validation, and 47videos for the SR-5513 and SR-5513 tests for truth."}, {"heading": "4.2 TRAINING PROCEDURE", "text": "We consider the RGB frames of videos as input, and all videos are spatially enlarged to 112 \u00d7 112, at 2 frames per second; note that the time sampling rate of the videos is therefore consistent, but not necessarily the total number of frames. C3D (Tran et al., 2015) is pre-trained on Sports-1M dataset Karpathy et al. (2014), consisting of 1.1 million sports videos from 487 categories. We extract the features from four revolutionary layers and a fully connected layer, called Pool2, Pool3, Pool4, Pool5 and FC7 in C3D (Tran et al al al, 2015). The model architecture of C3D is appendix B. The core sizes of the revolutionary transformation in (4) are 7 \u00d7 7, 5 \u00d7 5 x 5 and 3 \u00d7 3 for the layer Pool2."}, {"heading": "4.3 EVALUATION", "text": "The widely used metrics BLEU (Papineni et al., 2002), METEOR (Banerjee & Lavie, 2005) and CIDER (Vedantam et al., 2015) are used to quantitatively evaluate the performance of our generation model for video labels and other models in literature. For each data set, we show three types of results, using a part or the entire model to illustrate the role of each component: 1. C3D fc7 + LSTM: The context vector zt is included, but only features extracted from the topmost, fully connected layer. No context vector zt arising from the middle revolutionary layer is generated. 2. Spatio-temporal attention + LSTM: The context vector zt is included, but only features extracted from a particular revolutionary layer."}, {"heading": "4.4 QUANTITATIVE RESULTS", "text": "Results are summarized in Tables 1 and 2, and we obtain state-of-the-art results for both Youtube2Text and M-VAD datasets. In the MSRVTT dataset, our results also significantly exceed the strong baselines and show the effectiveness of the proposed model. Note that there are no comparative results in the literature for MSR-VTT. As a reference with similar settings, Xu et al. (2016) achieved 39.9 for BLEU-4 and 29.3 for METEOR using the standard test set using C3D fc7 + LSTM, while state-of-the-art results were 40.8 for BLEU-4, 28.2 for METEOR and 44.8 for CIDEr by the team \"v2t Navigator\" from RUC and CMU. It is worth noting that our model consistently produces significant improvements over models with only spatial attention that perform better than those that we only combine with this upper layer, without fully integrating these 3D models."}, {"heading": "4.5 QUALITATIVE RESULTS", "text": "Following Xu et al. (2015), we visualize the attention components that our model learned on Youtube2Text in Figure 3. As shown in Figure 3, spatio-temporal attention matches the objects in the video well with the corresponding words. In addition, abstraction level attention tends to focus on low-level characteristics when the model creates a noun and high-level characteristics when creating an article or preposition. Further results are provided in Appendix A.1. Examples of captions generated from videos not seen on Youtube2Text are shown in Figure 4. We find the results with the attention of the abstraction level (indicated as \"soft attention\" or \"hard attention\") generally equal or better than the best results, compared to those that use only one specific feature of the convolutional layer (indicated as \"Pool2,\" etc.). They show the effectiveness of our attention at the multi-level abstraction (indicated as \"soft attention\" or \"hard attention\" in the caption)."}, {"heading": "5 CONCLUSION AND FUTURE WORK", "text": "We have proposed a novel video captioning model that adaptively selects / weights feature abstraction (CNN layer) and position within a layer-dependent feature map. We have implemented attention with both \"hard\" and \"soft\" mechanisms, with the latter typically delivering better performance. Our model performs excellently in generating video captions and has the ability to provide interpretable orientations that seem analogous to human perception. We have focused on analyzing videos and associated captions. Similar ideas could be applied to captions in the future. Additionally, CNN parameters were learned separately as a first step before analyzing captions."}, {"heading": "A MORE RESULTS", "text": "VISUALIZATION OF ATTENTION WEIGHTSA.2 GENERATED CAPTIONSPool2: Pool3: Pool5: Soft Attention: Soft Attention: a man smiles a man eats a banana Man talks a man eats a banana Pool2: Pool3: Pool4: Pool5: Soft Attention: a monkey does a monkey do martial arts.: a monkey fights a monkey fights a monkey fights a monkey. Pool3: Pool4: Pool5: Soft Attention: a monkey does a martial arts."}, {"heading": "B MODEL ARCHITECTURE OF C3D", "text": "In fact, most of them are able to survive themselves if they do not see themselves able to survive themselves, \"he told the German Press Agency in an interview with\" Welt am Sonntag \":\" I don't think they are able to survive themselves. \""}, {"heading": "C DIMENSIONS AND RECEPTIVE FIELD OF C3D FEATURES", "text": "The dimensions for features extracted from Pool2, Pool3, Pool4, and Pool5 are 28 \u00b7 28 \u00b7 N / 2 \u00b7 128, 14 \u00b7 14 \u00b7 N / 4 \u00b7 256, 7 \u2212 7 \u00b7 N / 8 \u00b7 512, and 4 \u2212 4 \u00b7 N / 16 \u00b7 512, respectively. N is the number of frames of the input video. After the revolutionary transformation, the dimensions will be all 4 \u2212 4 \u00b7 N / 16 \u00b7 512. To prove these features are spatially oriented, we first provide the receptive field for 3D Conversion Layer and 3D Conversion Layer. Leave Y = 3D Conversion Layer (X), where 3D Conversion Layer is to the Conversion Layer with kernel size 3 \u00b7 3 \u00b7 3 \u00b7 3 \u00b7 3. The features indexed by i = [ix, iiz] in Y."}], "references": [{"title": "Learning wake-sleep recurrent attention models", "author": ["J. Ba", "R. Grosse", "R. Salakhutdinov", "B. Frey"], "venue": "In NIPS,", "citeRegEx": "Ba et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ba et al\\.", "year": 2015}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["D. Bahdanau", "K. Cho", "Y. Bengio"], "venue": "In ICLR,", "citeRegEx": "Bahdanau et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Delving deeper into convolutional networks for learning video representations", "author": ["N. Ballas", "L. Yao", "C. Pal", "A. Courville"], "venue": "In ICLR,", "citeRegEx": "Ballas et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Ballas et al\\.", "year": 2016}, {"title": "Meteor: An automatic metric for mt evaluation with improved correlation with human judgments", "author": ["S. Banerjee", "A. Lavie"], "venue": "In ACL workshop,", "citeRegEx": "Banerjee and Lavie.,? \\Q2005\\E", "shortCiteRegEx": "Banerjee and Lavie.", "year": 2005}, {"title": "Importance weighted autoencoders", "author": ["Y. Burda", "R. Grosse", "R. Salakhutdinov"], "venue": "In ICLR,", "citeRegEx": "Burda et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Burda et al\\.", "year": 2016}, {"title": "Collecting highly parallel data for paraphrase evaluation", "author": ["D. Chen", "W.B. Dolan"], "venue": "In ACL,", "citeRegEx": "Chen and Dolan.,? \\Q2011\\E", "shortCiteRegEx": "Chen and Dolan.", "year": 2011}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "author": ["K. Cho", "B.V. Merrienboer", "C. Gulcehre", "D. Bahdanau", "F. Bougares", "H. Schwenk", "Y. Bengio"], "venue": "In EMNLP,", "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Torch7: A matlab-like environment for machine learning", "author": ["R. Collobert", "K. Kavukcuoglu", "C. Farabet"], "venue": "In BigLearn, NIPS Workshop,", "citeRegEx": "Collobert et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Long-term recurrent convolutional networks for visual recognition and description", "author": ["J. Donahue", "L. Anne Hendricks", "S. Guadarrama", "M. Rohrbach", "S. Venugopalan", "K. Saenko", "T. Darrell"], "venue": "In CVPR,", "citeRegEx": "Donahue et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Donahue et al\\.", "year": 2015}, {"title": "Youtube2text: Recognizing and describing arbitrary activities using semantic hierarchies and zero-shot recognition", "author": ["S. Guadarrama", "N. Krishnamoorthy", "G. Malkarnenkar", "S. Venugopalan", "R. Mooney", "T. Darrell", "K. Saenko"], "venue": "In ICCV,", "citeRegEx": "Guadarrama et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Guadarrama et al\\.", "year": 2013}, {"title": "Deep residual learning for image recognition", "author": ["K. He", "X. Zhang", "S. Ren", "Sun J"], "venue": null, "citeRegEx": "He et al\\.,? \\Q2016\\E", "shortCiteRegEx": "He et al\\.", "year": 2016}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural Computation,", "citeRegEx": "Hochreiter and Schmidhuber.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Deep visual-semantic alignments for generating image descriptions", "author": ["A. Karpathy", "F. Li"], "venue": "In CVPR,", "citeRegEx": "Karpathy and Li.,? \\Q2015\\E", "shortCiteRegEx": "Karpathy and Li.", "year": 2015}, {"title": "Large-scale video classification with convolutional neural networks", "author": ["A. Karpathy", "G. Toderici", "S. Shetty", "T. Leung", "R. Sukthankar", "Li Fei-Fei"], "venue": "In CVPR,", "citeRegEx": "Karpathy et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Karpathy et al\\.", "year": 2014}, {"title": "Adam: A method for stochastic optimization", "author": ["D. Kingma", "J. Ba"], "venue": "In ICLR,", "citeRegEx": "Kingma and Ba.,? \\Q2014\\E", "shortCiteRegEx": "Kingma and Ba.", "year": 2014}, {"title": "Multimodal neural language models", "author": ["R. Kiros", "R. Salakhutdinov", "R. Zemel"], "venue": "In ICML,", "citeRegEx": "Kiros et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kiros et al\\.", "year": 2014}, {"title": "Understanding deep image representations by inverting them", "author": ["A. Mahendran", "A. Vedaldi"], "venue": "In CVPR,", "citeRegEx": "Mahendran and Vedaldi.,? \\Q2015\\E", "shortCiteRegEx": "Mahendran and Vedaldi.", "year": 2015}, {"title": "Deep captioning with multimodal recurrent neural networks (m-RNN)", "author": ["J. Mao", "W. Xu", "Y. Yang", "J. Wang", "A. Yuille"], "venue": "In ICLR,", "citeRegEx": "Mao et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mao et al\\.", "year": 2015}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["T. Mikolov", "I. Sutskever", "K. Chen", "G.S. Corrado", "J. Dean"], "venue": "In NIPS,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Variational inference for monte carlo objectives", "author": ["A. Mnih", "D.J. Rezende"], "venue": "In ICML,", "citeRegEx": "Mnih and Rezende.,? \\Q2016\\E", "shortCiteRegEx": "Mnih and Rezende.", "year": 2016}, {"title": "Recurrent models of visual attention", "author": ["V. Mnih", "N. Heess", "A. Graves", "K. Kavukcuoglu"], "venue": "In NIPS,", "citeRegEx": "Mnih et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2014}, {"title": "Rectied linear units improve restricted boltzmann machines", "author": ["V. Nair", "G.E. Hinton"], "venue": "In ICML,", "citeRegEx": "Nair and Hinton.,? \\Q2010\\E", "shortCiteRegEx": "Nair and Hinton.", "year": 2010}, {"title": "Hierarchical recurrent neural encoder for video representation with application to captioning", "author": ["P. Pan", "Z. Xu", "Y. Yang", "F. Wu", "Y. Zhuang"], "venue": "In CVPR,", "citeRegEx": "Pan et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Pan et al\\.", "year": 2016}, {"title": "Jointly modeling embedding and translation to bridge video and language", "author": ["Y. Pan", "T. Mei", "T. Yao", "H. Li", "Y. Rui"], "venue": "In CVPR,", "citeRegEx": "Pan et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Pan et al\\.", "year": 2016}, {"title": "Bleu: a method for automatic evaluation of machine translation", "author": ["K. Papineni", "S. Roukos", "T. Ward", "W. Zhu"], "venue": "Transactions of the Association for Computational Linguistics,", "citeRegEx": "Papineni et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "Coherent multisentence video description with variable level of detail", "author": ["A. Rohrbach", "M. Rohrbach", "W. Qiu", "A. Friedrich", "M. Pinkal", "B. Schiele"], "venue": "GCPR", "citeRegEx": "Rohrbach et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Rohrbach et al\\.", "year": 2014}, {"title": "Translating video content to natural language descriptions", "author": ["M. Rohrbach", "W. Qiu", "I. Titov", "S. Thater", "M. Pinkal", "B. Schiele"], "venue": "In ICCV,", "citeRegEx": "Rohrbach et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Rohrbach et al\\.", "year": 2013}, {"title": "Exact solutions to the nonlinear dynamics of learning in deep linear neural networks", "author": ["A.M. Saxe", "J.L. McClelland", "S. Ganguli"], "venue": "In ICLR,", "citeRegEx": "Saxe et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Saxe et al\\.", "year": 2014}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["N. Srivastava", "G. Hinton", "A. Krizhevsky", "I. Sutskever", "R. Salakhutdinov"], "venue": null, "citeRegEx": "Srivastava et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 2014}, {"title": "Sequence to sequence learning with neural networks", "author": ["I. Sutskever", "O. Vinyals", "Q.V. Le"], "venue": "In NIPS,", "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Using descriptive video services to create a large data source for video annotation research", "author": ["A Torabi", "H Larochelle C Pal", "A Courville"], "venue": "In arXiv preprint arXiv:1503.01070,", "citeRegEx": "Torabi et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Torabi et al\\.", "year": 2015}, {"title": "Learning spatiotemporal features with 3d convolutional networks", "author": ["D. Tran", "L. Bourdev", "R. Fergus", "L. Torresani", "M. Paluri"], "venue": "In ICCV,", "citeRegEx": "Tran et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Tran et al\\.", "year": 2015}, {"title": "Cider: Consensus-based image description evaluation", "author": ["R. Vedantam", "Z.C. Lawrence", "D. Parikh"], "venue": "In CVPR,", "citeRegEx": "Vedantam et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Vedantam et al\\.", "year": 2015}, {"title": "Sequence to sequence-video to text", "author": ["S. Venugopalan", "M. Rohrbach", "J. Donahue", "R. Mooney", "T. Darrell", "K. Saenko"], "venue": "In ICCV,", "citeRegEx": "Venugopalan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Venugopalan et al\\.", "year": 2015}, {"title": "Translating videos to natural language using deep recurrent neural networks", "author": ["S. Venugopalan", "H. Xu", "J. Donahue", "M. Rohrbach", "R. Mooney", "K. Saenko"], "venue": "In NAACL,", "citeRegEx": "Venugopalan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Venugopalan et al\\.", "year": 2015}, {"title": "Show and tell: A neural image caption generator", "author": ["O. Vinyals", "A. Toshev", "S. Bengio", "D. Erhan"], "venue": "In CVPR,", "citeRegEx": "Vinyals et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Vinyals et al\\.", "year": 2015}, {"title": "Msr-vtt: A large video description dataset for bridging video and language", "author": ["J. Xu", "T. Mei", "T. Yao", "Y. Rui"], "venue": null, "citeRegEx": "Xu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2016}, {"title": "Show, attend and tell: Neural image caption generation with visual attention", "author": ["K. Xu", "J.L. Ba", "R. Kiros", "K. Cho", "A. Courville", "R. Salakhutdinov", "R.S. Zemel", "Y. Bengio"], "venue": "In ICML,", "citeRegEx": "Xu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2015}, {"title": "Video paragraph captioning using hierarchical recurrent neural networks", "author": ["H. Yu", "J. Wang", "Z. Huang", "Y. Yang", "W. Xu"], "venue": null, "citeRegEx": "Yu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Yu et al\\.", "year": 2016}, {"title": "Visualizing and understanding convolutional networks", "author": ["M. Zeiler", "R. Fergus"], "venue": "In ECCV,", "citeRegEx": "Zeiler and Fergus.,? \\Q2014\\E", "shortCiteRegEx": "Zeiler and Fergus.", "year": 2014}], "referenceMentions": [{"referenceID": 6, "context": "Inspired by the successful encoder-decoder framework used in machine translation (Cho et al., 2014; Bahdanau et al., 2015; Sutskever et al., 2014) and image caption generation (Kiros et al.", "startOffset": 81, "endOffset": 146}, {"referenceID": 1, "context": "Inspired by the successful encoder-decoder framework used in machine translation (Cho et al., 2014; Bahdanau et al., 2015; Sutskever et al., 2014) and image caption generation (Kiros et al.", "startOffset": 81, "endOffset": 146}, {"referenceID": 29, "context": "Inspired by the successful encoder-decoder framework used in machine translation (Cho et al., 2014; Bahdanau et al., 2015; Sutskever et al., 2014) and image caption generation (Kiros et al.", "startOffset": 81, "endOffset": 146}, {"referenceID": 15, "context": ", 2014) and image caption generation (Kiros et al., 2014; Vinyals et al., 2015; Karpathy & Li, 2015; Mao et al., 2015), most recent work on video captioning (Donahue et al.", "startOffset": 37, "endOffset": 118}, {"referenceID": 35, "context": ", 2014) and image caption generation (Kiros et al., 2014; Vinyals et al., 2015; Karpathy & Li, 2015; Mao et al., 2015), most recent work on video captioning (Donahue et al.", "startOffset": 37, "endOffset": 118}, {"referenceID": 17, "context": ", 2014) and image caption generation (Kiros et al., 2014; Vinyals et al., 2015; Karpathy & Li, 2015; Mao et al., 2015), most recent work on video captioning (Donahue et al.", "startOffset": 37, "endOffset": 118}, {"referenceID": 8, "context": ", 2015), most recent work on video captioning (Donahue et al., 2015; Venugopalan et al., 2015a;b; Pan et al., 2016b; Yu et al., 2016) employs a twodimensional (2D) or three-dimensional (3D) Convolutional Neural Network (CNN) as an encoder, mapping an input video to a compact feature-vector representation.", "startOffset": 46, "endOffset": 133}, {"referenceID": 38, "context": ", 2015), most recent work on video captioning (Donahue et al., 2015; Venugopalan et al., 2015a;b; Pan et al., 2016b; Yu et al., 2016) employs a twodimensional (2D) or three-dimensional (3D) Convolutional Neural Network (CNN) as an encoder, mapping an input video to a compact feature-vector representation.", "startOffset": 46, "endOffset": 133}, {"referenceID": 31, "context": "Such features have been shown to be effective for video representations, action recognition and scene understanding (Tran et al., 2015), by learning the spatiotemporal features that can provide better appearance and motion information.", "startOffset": 116, "endOffset": 135}, {"referenceID": 20, "context": "In addition, the proposed model is inspired by the recent success of attention-based models that mimic human perception (Mnih et al., 2014; Xu et al., 2015).", "startOffset": 120, "endOffset": 156}, {"referenceID": 37, "context": "In addition, the proposed model is inspired by the recent success of attention-based models that mimic human perception (Mnih et al., 2014; Xu et al., 2015).", "startOffset": 120, "endOffset": 156}, {"referenceID": 9, "context": "(2013; 2014) learn a Conditional Random Field (CRF) to infer high-level concepts such as object and action; in Guadarrama et al. (2013) semantic hierarchies are used to choose an appropriate level of sentence fragments.", "startOffset": 111, "endOffset": 136}, {"referenceID": 8, "context": "Given the sequence of features extracted from video frames, the video representation is then obtained by a CRF (Donahue et al., 2015), mean pooling (Venugopalan et al.", "startOffset": 111, "endOffset": 133}, {"referenceID": 38, "context": ", 2015b), weighted mean pooling with attention (Yu et al., 2016), or via the last hidden state of an RNN encoder (Venugopalan et al.", "startOffset": 47, "endOffset": 64}, {"referenceID": 31, "context": "(2016b), which jointly embedded the 2D CNN features and spatiotemporal features extracted from a 3D CNN (Tran et al., 2015).", "startOffset": 104, "endOffset": 123}, {"referenceID": 8, "context": "Donahue et al. (2015); Venugopalan et al.", "startOffset": 0, "endOffset": 22}, {"referenceID": 8, "context": "Donahue et al. (2015); Venugopalan et al. (2015a;b); Yu et al. (2016); Pan et al.", "startOffset": 0, "endOffset": 70}, {"referenceID": 8, "context": "Donahue et al. (2015); Venugopalan et al. (2015a;b); Yu et al. (2016); Pan et al. (2016a) applied a 2D CNN pretrained on ImageNet to video frames, with the top-layer output of the CNN used as features.", "startOffset": 0, "endOffset": 90}, {"referenceID": 8, "context": "Donahue et al. (2015); Venugopalan et al. (2015a;b); Yu et al. (2016); Pan et al. (2016a) applied a 2D CNN pretrained on ImageNet to video frames, with the top-layer output of the CNN used as features. Given the sequence of features extracted from video frames, the video representation is then obtained by a CRF (Donahue et al., 2015), mean pooling (Venugopalan et al., 2015b), weighted mean pooling with attention (Yu et al., 2016), or via the last hidden state of an RNN encoder (Venugopalan et al., 2015a). These works were followed by Pan et al. (2016b), which jointly embedded the 2D CNN features and spatiotemporal features extracted from a 3D CNN (Tran et al.", "startOffset": 0, "endOffset": 559}, {"referenceID": 2, "context": "The proposed model is related to Ballas et al. (2016), but distinct in important ways.", "startOffset": 33, "endOffset": 54}, {"referenceID": 2, "context": "The proposed model is related to Ballas et al. (2016), but distinct in important ways. The intermediate convolutional feature maps are leveraged, like Ballas et al. (2016), but an attention model is developed instead of the \u201cstack\u201d RNN in Ballas et al.", "startOffset": 33, "endOffset": 172}, {"referenceID": 2, "context": "The proposed model is related to Ballas et al. (2016), but distinct in important ways. The intermediate convolutional feature maps are leveraged, like Ballas et al. (2016), but an attention model is developed instead of the \u201cstack\u201d RNN in Ballas et al. (2016). In addition, a powerful decoder enhanced with two attention mechanisms is constructed for generating captions, while a simple RNN decoder is employed in Ballas et al.", "startOffset": 33, "endOffset": 260}, {"referenceID": 2, "context": "The proposed model is related to Ballas et al. (2016), but distinct in important ways. The intermediate convolutional feature maps are leveraged, like Ballas et al. (2016), but an attention model is developed instead of the \u201cstack\u201d RNN in Ballas et al. (2016). In addition, a powerful decoder enhanced with two attention mechanisms is constructed for generating captions, while a simple RNN decoder is employed in Ballas et al. (2016). Finally, we use features extracted from C3D instead of a 2D CNN.", "startOffset": 33, "endOffset": 435}, {"referenceID": 31, "context": "For each video, the C3D feature extractor (Tran et al., 2015) produces a set of features A = {a 1 , .", "startOffset": 42, "endOffset": 61}, {"referenceID": 31, "context": "However, the fully connected layer at the top, responsible for a L+1, assumes that a (n) L is of the same size for all videos (like the 16-frame-length videos in (Tran et al., 2015)).", "startOffset": 162, "endOffset": 181}, {"referenceID": 31, "context": "To account for variablelength videos, and maintain the same fully-connected layer at the top, we employ mean pooling to a (n) L , based on a window of length 16 (as in (Tran et al., 2015)) with an overlap of 8 frames.", "startOffset": 168, "endOffset": 187}, {"referenceID": 33, "context": "than using it at each time step of the RNN, as also found in Vinyals et al. (2015); Venugopalan et al.", "startOffset": 61, "endOffset": 83}, {"referenceID": 33, "context": "(2015); Venugopalan et al. (2015b). The transition function H(\u00b7) is implemented with Long Short-Term Memory (LSTM) (Hochreiter & Schmidhuber, 1997).", "startOffset": 8, "endOffset": 35}, {"referenceID": 1, "context": "Soft attention Following Bahdanau et al. (2015), we formulate the soft attention model by computing a weighted sum of the input features", "startOffset": 25, "endOffset": 48}, {"referenceID": 0, "context": "Inspired by importance sampling, the multi-sample stochastic lower bound has been recently used for latent variable models (Ba et al., 2015; Burda et al., 2016), defined as L(Y) = \u2211 m1:K p(m 1:K |A) [ log 1 K \u2211K k=1 p(Y|m,A) ] , (10)", "startOffset": 123, "endOffset": 160}, {"referenceID": 4, "context": "Inspired by importance sampling, the multi-sample stochastic lower bound has been recently used for latent variable models (Ba et al., 2015; Burda et al., 2016), defined as L(Y) = \u2211 m1:K p(m 1:K |A) [ log 1 K \u2211K k=1 p(Y|m,A) ] , (10)", "startOffset": 123, "endOffset": 160}, {"referenceID": 4, "context": "This lower bound is guaranteed to be tighter with the increase of the number of samples K (Burda et al., 2016), thus providing a better approximation of the objective function than (9).", "startOffset": 90, "endOffset": 110}, {"referenceID": 4, "context": "This lower bound is guaranteed to be tighter with the increase of the number of samples K (Burda et al., 2016), thus providing a better approximation of the objective function than (9). As shown in Mnih & Rezende (2016), the gradient of L(Y) with respect to the model parameters is \u2207L(Y) = \u2211 m1:K p(m 1:K |A) \u2211K k=1 [ L(m)\u2207 log p(m|A) + \u03c9k\u2207p(Y|m,A) ] , (11)", "startOffset": 91, "endOffset": 220}, {"referenceID": 30, "context": "We present results on three benchmark datasets: Microsoft Research Video Description Corpus (YouTube2Text) (Chen & Dolan, 2011), Montreal Video Annotation Dataset (M-VAD) (Torabi et al., 2015), and Microsoft Research Video to Text (MSR-VTT) (Xu et al.", "startOffset": 171, "endOffset": 192}, {"referenceID": 36, "context": ", 2015), and Microsoft Research Video to Text (MSR-VTT) (Xu et al., 2016).", "startOffset": 56, "endOffset": 73}, {"referenceID": 33, "context": "For fair comparison, we used the same splits as provided in Venugopalan et al. (2015b), with 1200 videos for training, 100 videos for validation, and 670 videos for testing.", "startOffset": 60, "endOffset": 87}, {"referenceID": 30, "context": "We follow the setting in Torabi et al. (2015), taking 36920 videos for training, 4950 videos for validation, and 4717 videos for testing.", "startOffset": 25, "endOffset": 46}, {"referenceID": 30, "context": "[1,2,3,4] represent Venugopalan et al. (2015a); Pan et al.", "startOffset": 20, "endOffset": 47}, {"referenceID": 21, "context": "(2015a); Pan et al. (2016b); Ballas et al.", "startOffset": 9, "endOffset": 28}, {"referenceID": 2, "context": "(2016b); Ballas et al. (2016); Yu et al.", "startOffset": 9, "endOffset": 30}, {"referenceID": 2, "context": "(2016b); Ballas et al. (2016); Yu et al. (2016), respectively.", "startOffset": 9, "endOffset": 48}, {"referenceID": 31, "context": "The C3D (Tran et al., 2015) is pretrained on Sports-1M dataset Karpathy et al.", "startOffset": 8, "endOffset": 27}, {"referenceID": 31, "context": "We extract the features from four convolutional layers and one fully connected layer, named as pool2, pool3, pool4, pool5 and fc-7 in the C3D (Tran et al., 2015), respectively.", "startOffset": 142, "endOffset": 161}, {"referenceID": 13, "context": ", 2015) is pretrained on Sports-1M dataset Karpathy et al. (2014), consisting of 1.", "startOffset": 43, "endOffset": 66}, {"referenceID": 27, "context": "All recurrent matrices in the LSTM are initialized with orthogonal initialization (Saxe et al., 2014).", "startOffset": 82, "endOffset": 101}, {"referenceID": 18, "context": "Word embedding vectors are initialized with the publicly available word2vec vectors that were trained on 100 billion words from Google News, which have dimensionality 300, and were trained using a continuous bag-of-words architecture (Mikolov et al., 2013).", "startOffset": 234, "endOffset": 256}, {"referenceID": 29, "context": "Gradients are clipped if the norm of the parameter vector exceeds 5 (Sutskever et al., 2014).", "startOffset": 68, "endOffset": 92}, {"referenceID": 28, "context": "We do not perform any datasetspecific tuning and regularization other than dropout (Srivastava et al., 2014) and early stopping on validation sets.", "startOffset": 83, "endOffset": 108}, {"referenceID": 7, "context": "All experiments are implemented in Torch (Collobert et al., 2011).", "startOffset": 41, "endOffset": 65}, {"referenceID": 17, "context": "Word embedding vectors are initialized with the publicly available word2vec vectors that were trained on 100 billion words from Google News, which have dimensionality 300, and were trained using a continuous bag-of-words architecture (Mikolov et al., 2013). The embedding vectors of words not present in the pre-trained set are initialized randomly. The number of hidden units in the LSTM is set as 512 and we use mini-batches of size 32. Gradients are clipped if the norm of the parameter vector exceeds 5 (Sutskever et al., 2014). The number of samples for multi-sample stochastic lower bound is set to 10. We do not perform any datasetspecific tuning and regularization other than dropout (Srivastava et al., 2014) and early stopping on validation sets. The Adam algorithm Kingma & Ba (2014) with learning rate 0.", "startOffset": 235, "endOffset": 795}, {"referenceID": 24, "context": "The widely used BLEU (Papineni et al., 2002), METEOR (Banerjee & Lavie, 2005) and CIDEr (Vedantam et al.", "startOffset": 21, "endOffset": 44}, {"referenceID": 32, "context": ", 2002), METEOR (Banerjee & Lavie, 2005) and CIDEr (Vedantam et al., 2015) metrics are employed to quantitatively evaluate the performance of our video caption generation model, and other models in the literature.", "startOffset": 51, "endOffset": 74}, {"referenceID": 6, "context": "To verify the effectiveness of our video caption generation model and C3D features, we also implement a strong baseline method based on the LSTM encoder-decoder network (Cho et al., 2014), where ResNet He et al.", "startOffset": 169, "endOffset": 187}, {"referenceID": 6, "context": "To verify the effectiveness of our video caption generation model and C3D features, we also implement a strong baseline method based on the LSTM encoder-decoder network (Cho et al., 2014), where ResNet He et al. (2016) is employed as the feature extractor on each frame.", "startOffset": 170, "endOffset": 219}, {"referenceID": 36, "context": "As a reference, with similar settings, Xu et al. (2016) using C3D fc7 + LSTM achieved 39.", "startOffset": 39, "endOffset": 56}, {"referenceID": 36, "context": "Following Xu et al. (2015), we visualize the attention components learned by our model on Youtube2Text in Figure 3.", "startOffset": 10, "endOffset": 27}], "year": 2016, "abstractText": "A new model for video captioning is developed, using a deep three-dimensional Convolutional Neural Network (C3D) as an encoder for videos and a Recurrent Neural Network (RNN) as a decoder for captions. We consider both \u201chard\u201d and \u201csoft\u201d attention mechanisms, to adaptively and sequentially focus on different layers of features (levels of feature \u201cabstraction\u201d), as well as local spatiotemporal regions of the feature maps at each layer. The proposed approach is evaluated on three benchmark datasets: YouTube2Text, M-VAD and MSR-VTT. Along with visualizing the results and how the model works, these experiments quantitatively demonstrate the effectiveness of the proposed adaptive spatiotemporal feature abstraction for translating videos to sentences with rich semantics.", "creator": "LaTeX with hyperref package"}}}