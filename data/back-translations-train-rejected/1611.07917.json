{"id": "1611.07917", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Nov-2016", "title": "Deep Restricted Boltzmann Networks", "abstract": "Building a good generative model for image has long been an important topic in computer vision and machine learning. Restricted Boltzmann machine (RBM) is one of such models that is simple but powerful. However, its restricted form also has placed heavy constraints on the models representation power and scalability. Many extensions have been invented based on RBM in order to produce deeper architectures with greater power. The most famous ones among them are deep belief network, which stacks multiple layer-wise pretrained RBMs to form a hybrid model, and deep Boltzmann machine, which allows connections between hidden units to form a multi-layer structure. In this paper, we present a new method to compose RBMs to form a multi-layer network style architecture and a training method that trains all layers jointly. We call the resulted structure deep restricted Boltzmann network. We further explore the combination of convolutional RBM with the normal fully connected RBM, which is made trivial under our composition framework. Experiments show that our model can generate descent images and outperform the normal RBM significantly in terms of image quality and feature quality, without losing much efficiency for training.", "histories": [["v1", "Tue, 15 Nov 2016 03:57:42 GMT  (2036kb,D)", "http://arxiv.org/abs/1611.07917v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["hengyuan hu", "lisheng gao", "quanbin ma"], "accepted": false, "id": "1611.07917"}, "pdf": {"name": "1611.07917.pdf", "metadata": {"source": "CRF", "title": "Deep Restricted Boltzmann Networks", "authors": ["Hengyuan Hu", "Lisheng Gao", "Quanbin Ma"], "emails": ["hengyuanhu@cmu.edu", "lishengg@andrew.cmu.edu", "quanbinm@andrew.cmu.edu"], "sections": [{"heading": "1. Introduction", "text": "Boltzmann Machine (BM) is a family of bidirectionally connected neural network models designed to learn unknown probabilistic distributions [2]. However, the original Boltzmann machine is rarely useful because its lateral connections between visible and hidden units make it mathematically impossible to train them. Limited Boltzmann Machine (RBM) [5] is proposed to address this problem, where the connection pattern in a Boltzmann machine is so limited that no lateral connections are allowed, making the learning process much more efficient while still having enough representational power to be a useful generative equality. Model [16] Several deeper architectures are later invented to address the problem that one layer of RBMs cannot model complicated probable distributions in practice. Two of the most successful are deep belief network (DBN) [6] and deep boltzmann (DM)."}, {"heading": "2. Background", "text": "In this section we will discuss the limited Boltzmann machine and its two most important multi-layered extensions, i.e. the Deep Belief Network and the Deep Boltzmann Machine. These three models are the foundations and inspirations of our new model. Therefore, it is crucial to understand them in order to recognize the differences and advantages of our new severely restricted Boltzmann networks."}, {"heading": "2.1. Restricted Boltzmann Machine", "text": "It contains a set of visible units v (0, 1) D, hidden units h (0, 1) P, where D and P are the numbersar Xiv: 161 1.07 917v 1 [cs.L G] 15 Nov 201 6of the visible and hidden units are each defined as: E (v, h) is defined as: E (v, h); the parameters involved are p = {W, b, c}, where the mutual weights, visible units are called \"bias and hidden units.\" The energy of a given state (v, h) is defined as: E (v, h) is defined as: - bTv \u2212 cTh \u2212 vTWh (1) = \u2212 jp = 1 bivi \u2212 j = 1 cjhj \u2212 j. The probability of a certain configuration of the visible state v in the model isp."}, {"heading": "2.2. Deep Belief Network", "text": "In a DBN, two adjacent layers are connected in the same way as in an RBM. The network is trained in greedy, layer-by-layer [6], with the bottom layer trained solely as an RBM and then fixed to train the next layer. After all layers are pre-trained, the resulting network contains an RBM on the top layer, while the remaining layers form a directed neural network. There is no common training for all layers in DBN. To generate images, we must first perform Gibbs sampling in the top layer of the RBM to convergence and then spread to the bottom layer. DBN has also been expanded to use revolutionary RBMs for better scalability and higher quality features [10]."}, {"heading": "2.3. Deep Boltzmann Machine", "text": "Deep Boltzmann machine [15], as shown in Figure 2b, is another special model in the Boltzmann machine family. Unlike RBM, which does not allow connections between hidden units, DBM introduces additional connections between latent variables in order to form a multi-layer structure in the hidden part of the Boltzmann machine. In contrast to RBM, which does not allow connections between hidden units, the energy of a DBM is defined as E (v, h1, h2; \u03b8) = \u2212 bTv \u2212 c (1) \u2212 c (2) Th (1) \u2212 vTW (1) \u2212 h (1) \u2212 h (1). (11) In view of the energy function, the conditional distributions of each layer can also be defined as: p (h (2) j = 1 | h (1) = 1) = \u03c3 (1) \u2212 h (1) \u2212 h (1) \u2212 h (1) \u2212 h (1) \u2212 h (1) \u2212 W (2) iW (2) (2) iW (2) (2) j (2) j (2), j (1 j (j), j (k (2 k (k) (2) j (c (c) (2), j (c (2) c (c)."}, {"heading": "3. Deep Restricted Boltzmann Network", "text": "However, it will be interesting to see if we can develop a new rule to stack the simplest RBMs in such a way that the resulting model can both produce better images and extract higher quality features. In this section, we will introduce the architecture of the severely restricted Boltzmann network and its training method. Furthermore, we will further enhance our architecture to support both RBMs and conventional RBMs."}, {"heading": "3.1. Architecture", "text": "As shown in Figure 3a, the hidden units of each RBM pass their values directly to the visible units of the next layer. Now, the hidden units on each layer are actually the visible units on the next layer, so in the following discussion we will not distinguish between v and h, while using only x (l) to indicate the state of the l-th layer for the sake of simplicity of notation. In other words, h (i) and v (i + 1) are unified as x (i + 1) in the following discussion because they have essentially the same value. Of course, x (0) denotes the input layer of the entire mesh. Since each layer is considered to be a separate RBM instead of a part of a DBM, the energy on each layer is so defined, i.e. each RBM is x, instead of the whole mesh."}, {"heading": "3.2. Training DRBN", "text": "All RBMs in the network are trained together with persistent contrastive divergences. Starting with the input of visible x (0), the model would first perform an upward trend according to Equation 16 with Bernoulli sampling at each level.The results obtained in this pass are called {x (l) | l = 0, 1, 2,..., L} and later used to calculate the date-pendent terminal. k Upward-downward are performed on the PCD particles.The values of these particles at each level during the last downward trend are called {x (l) pcd | l = 0, 1, 2,..., L} 1, which is then used to approximate the model-dependent terminal.These two processes are illustrated in Figure 3b. After obtaining x (l) s and x (l) pcds, we can calculate the gradients for each layer separately, using all of the sewing forces that are required in this formation at the same time."}, {"heading": "3.3. Extension with Convolutional RBM", "text": "The original RBM operates in a fully connected way, which means that it uses the entire input image as a one-dimensional array (W = W = defined) and ignores locality. Inspired by the enormous success of the Convolutionary Neural Network in computer vision [9, 4], we expand our method to use Convolutionary RBM under the same compositional framework. Constitutional RBM [10, 13] is conceptually the same as RBM, but \u2212 \u2212 weighted (filter) between all places on the input image and replaces the matrix multiplications happen during upward compiling with convolutions. For notation simplification, we assume that the visible units are of the shape (Nv, Nv) with 1 channel, the hidden units are derived from the shape (Nh, Nh) with C channels, and the convolution uses filters of size (Nw, Nw) with no other polarity and cannot be derived in any way."}, {"heading": "4. Experiments", "text": "We implement our model in Tensorflow [1]. With the help of its auto-differentiation functionality, we can simply define the loss function for each RBM asLoss (\u03b8 (l)) = 1M \u2211 i F (x (l) i) \u2212 1 N \u2211 j F (x (l) j) (24) and leave the rest to the auto-differential- and gradient-based optimizers. M, N in the previous equation denote the size of the minibatch or the number of PCD particles. In all the following experiments, we perform PCD for 5 iterations between each update and set the number of PCD particles to 100, i.e. PCD (5, 100) in algorithm 1. The generated images are the probabilities of the binary visible units, i.e. in the first layer, no samples are performed during the last downward run. The code for reproducing our test results is published."}, {"heading": "4.1. MNIST", "text": "This year is the highest in the history of the country."}, {"heading": "4.2. Weizmann Horses", "text": "We are testing our model on the Weizmann Horse Dataset [3], which includes 328 binary horse figures, characterized by real photographs, with a variety of different postures. The images also have a high variance, mainly due to their rich detail regarding the necks, feet and tails of the horses as shown later in Figure 5c. The high variety and low volume of the datasets make them a challenging task for the model to create realistic postures. The original images come in different resolutions, so we cut out the horses in the middle and adjust them to 32 pixels."}, {"heading": "5. Conclusion", "text": "In this paper, we have proposed a new way to assemble Boltzmann restricted machine and Boltzmann conventively restricted machine and form a deep network to improve their performance in both image generation and feature extraction. We have also developed a simple and intuitive training method that collectively optimizes all the RBMs in the network, which is good in practice. Our experiments with MNIST and Weizmann Horse data sets show that such composite architectures are good generative models and can extract useful features to facilitate supervised learning tasks such as classification. In the future, it would be interesting to see if this architecture can be used on real images and whether it can be generalized to use other Boltzmann machines, such as Boltzmann deep machines, as the base unit for each shift."}], "references": [{"title": "Software available from tensorflow.org", "author": ["M. Abadi", "A. Agarwal", "P. Barham", "E. Brevdo", "Z. Chen", "C. Citro", "G.S. Corrado", "A. Davis", "J. Dean", "M. Devin", "S. Ghemawat", "I. Goodfellow", "A. Harp", "G. Irving", "M. Isard", "Y. Jia", "R. Jozefowicz", "L. Kaiser", "M. Kudlur", "J. Levenberg", "D. Man\u00e9", "R. Monga", "S. Moore", "D. Murray", "C. Olah", "M. Schuster", "J. Shlens", "B. Steiner", "I. Sutskever", "K. Talwar", "P. Tucker", "V. Vanhoucke", "V. Vasudevan", "F. Vi\u00e9gas", "O. Vinyals", "P. Warden", "M. Wattenberg", "M. Wicke", "Y. Yu", "X. Zheng"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2015}, {"title": "Connectionist models and their implications: Readings from cognitive science. chapter A Learning Algorithm for Boltzmann Machines, pages 285\u2013307", "author": ["D.H. Ackley", "G.E. Hinton", "T.J. Sejnowski"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1988}, {"title": "Combining topdown and bottom-up segmentation", "author": ["E. Borenstein", "E. Sharon", "S. Ullman"], "venue": "In Proceedings of the 2004 Conference on Computer Vision and Pattern Recognition Workshop (CVPRW\u201904) Volume 4 - Volume 04,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2004}, {"title": "Deep residual learning for image recognition", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "CoRR, abs/1512.03385,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2015}, {"title": "Training products of experts by minimizing contrastive divergence", "author": ["G.E. Hinton"], "venue": "Neural Comput.,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2002}, {"title": "A fast learning algorithm for deep belief nets", "author": ["G.E. Hinton", "S. Osindero", "Y.-W. Teh"], "venue": "Neural Comput.,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2006}, {"title": "Reducing the dimensionality of data with neural networks", "author": ["G.E. Hinton", "R.R. Salakhutdinov"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2006}, {"title": "Adam: A method for stochastic optimization", "author": ["D.P. Kingma", "J. Ba"], "venue": "CoRR, abs/1412.6980,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2014}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2012}, {"title": "Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations", "author": ["H. Lee", "R. Grosse", "R. Ranganath", "A.Y. Ng"], "venue": "In Proceedings of the 26th Annual International Conference on Machine Learning,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2009}, {"title": "Connectionist learning of belief networks", "author": ["R.M. Neal"], "venue": "Artif. Intell.,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1992}, {"title": "The shape boltzmann machine: a strong model of object shape", "author": ["J.W.J.W.S.M. Eslami", "N. Heess"], "venue": "In Proc. Conf. Computer Vision and Pattern Recognition (to appear),", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2012}, {"title": "An efficient learning procedure for deep boltzmann machines", "author": ["R. Salakhutdinov", "G. Hinton"], "venue": "Neural Comput.,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2012}, {"title": "Deep boltzmann machines", "author": ["R. Salakhutdinov", "G.E. Hinton"], "venue": "In Proceedings of the Twelfth International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2009}, {"title": "Restricted boltzmann machines for collaborative filtering", "author": ["R. Salakhutdinov", "A. Mnih", "G. Hinton"], "venue": "In Proceedings of the 24th International Conference on Machine Learning,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2007}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "CoRR, abs/1409.1556,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2014}, {"title": "Training restricted boltzmann machines using approximations to the likelihood gradient", "author": ["T. Tieleman"], "venue": "In Proceedings of the 25th International Conference on Machine Learning,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2008}], "referenceMentions": [{"referenceID": 4, "context": "Restricted Boltzmann machine (RBM) [5] is one of such models that is simple but powerful.", "startOffset": 35, "endOffset": 38}, {"referenceID": 5, "context": "The most famous ones among them are deep belief network [6], which stacks multiple layer-wise pretrained RBMs to form a hybrid model, and deep Boltzmann machine [15], which allows connections between hidden units to form a multi-layer structure.", "startOffset": 56, "endOffset": 59}, {"referenceID": 13, "context": "The most famous ones among them are deep belief network [6], which stacks multiple layer-wise pretrained RBMs to form a hybrid model, and deep Boltzmann machine [15], which allows connections between hidden units to form a multi-layer structure.", "startOffset": 161, "endOffset": 165}, {"referenceID": 1, "context": "Boltzmann machine (BM) is a family of bidirectionally connected neural network models designed to learn unknown probabilistic distributions [2].", "startOffset": 140, "endOffset": 143}, {"referenceID": 4, "context": "Restricted Boltzmann machine (RBM) [5] is proposed to address this problem, where the connection pattern in a Boltzmann machine is restricted such that no lateral connections are allowed.", "startOffset": 35, "endOffset": 38}, {"referenceID": 14, "context": "model [16].", "startOffset": 6, "endOffset": 10}, {"referenceID": 5, "context": "Two of the most successful ones are deep belief network (DBN) [6, 7] and deep Boltzmann machine (DBM) [15].", "startOffset": 62, "endOffset": 68}, {"referenceID": 6, "context": "Two of the most successful ones are deep belief network (DBN) [6, 7] and deep Boltzmann machine (DBM) [15].", "startOffset": 62, "endOffset": 68}, {"referenceID": 13, "context": "Two of the most successful ones are deep belief network (DBN) [6, 7] and deep Boltzmann machine (DBM) [15].", "startOffset": 102, "endOffset": 106}, {"referenceID": 13, "context": "The resulted model is thus still a bipartite graph so that efficient learning can be conducted [15, 14].", "startOffset": 95, "endOffset": 103}, {"referenceID": 12, "context": "The resulted model is thus still a bipartite graph so that efficient learning can be conducted [15, 14].", "startOffset": 95, "endOffset": 103}, {"referenceID": 16, "context": "Persistent contrastive divergence (PCD) [18, 11] has been widely employed to estimate the second term.", "startOffset": 40, "endOffset": 48}, {"referenceID": 10, "context": "Persistent contrastive divergence (PCD) [18, 11] has been widely employed to estimate the second term.", "startOffset": 40, "endOffset": 48}, {"referenceID": 5, "context": "The network is trained in a greedy, layer-by-layer manner [6], where the bottom layer is trained alone as an RBM, and then fixed to train the next layer.", "startOffset": 58, "endOffset": 61}, {"referenceID": 9, "context": "DBN has also been extended to use convolutional RBMs for better scalability and higher quality features [10].", "startOffset": 104, "endOffset": 108}, {"referenceID": 13, "context": "Deep Boltzmann machine [15], as shown in Figure 2b, is another special model in the Boltzmann machine family.", "startOffset": 23, "endOffset": 27}, {"referenceID": 16, "context": "Further more, mean-field method and persistent contrastive divergence [18, 11] are employed to make the learning tractable [15, 14].", "startOffset": 70, "endOffset": 78}, {"referenceID": 10, "context": "Further more, mean-field method and persistent contrastive divergence [18, 11] are employed to make the learning tractable [15, 14].", "startOffset": 70, "endOffset": 78}, {"referenceID": 13, "context": "Further more, mean-field method and persistent contrastive divergence [18, 11] are employed to make the learning tractable [15, 14].", "startOffset": 123, "endOffset": 131}, {"referenceID": 12, "context": "Further more, mean-field method and persistent contrastive divergence [18, 11] are employed to make the learning tractable [15, 14].", "startOffset": 123, "endOffset": 131}, {"referenceID": 8, "context": "Inspired by the huge success of convolutional neural network in computer vision [9, 4], we extend our method to utilize convolutional RBM under the same composition framework.", "startOffset": 80, "endOffset": 86}, {"referenceID": 3, "context": "Inspired by the huge success of convolutional neural network in computer vision [9, 4], we extend our method to utilize convolutional RBM under the same composition framework.", "startOffset": 80, "endOffset": 86}, {"referenceID": 9, "context": "Convolutional RBM [10, 13] is conceptually the same as RBM, but shares weights (filters) among all location on the input image and replaces the matrix multiplications happen during the upward-downward computation with convolutions.", "startOffset": 18, "endOffset": 26}, {"referenceID": 11, "context": "Convolutional RBM [10, 13] is conceptually the same as RBM, but shares weights (filters) among all location on the input image and replaces the matrix multiplications happen during the upward-downward computation with convolutions.", "startOffset": 18, "endOffset": 26}, {"referenceID": 9, "context": "Previously, convolutional RBM has been studied in [10, 12], but with the focus on feature extraction instead of image generation.", "startOffset": 50, "endOffset": 58}, {"referenceID": 0, "context": "We implement our model in Tensorflow [1].", "startOffset": 37, "endOffset": 40}, {"referenceID": 15, "context": "These properties are different from those of convolution layers in discriminative models [17, 4] where performance generally benefits from smaller and denser filters.", "startOffset": 89, "endOffset": 96}, {"referenceID": 3, "context": "These properties are different from those of convolution layers in discriminative models [17, 4] where performance generally benefits from smaller and denser filters.", "startOffset": 89, "endOffset": 96}, {"referenceID": 7, "context": "We used Adam [8] with the default parameters to perform gradient descent.", "startOffset": 13, "endOffset": 16}, {"referenceID": 2, "context": "We further test our model on the Weizmann Horse dataset [3], which includes 328 binary horse figures segmented from real world photos, with quite a variety of different postures.", "startOffset": 56, "endOffset": 59}, {"referenceID": 11, "context": "Such pattern for model selection is also reflected in ShapeBM [13], whose implementation of deep Boltzmann machine on similar 32\u00d732 horse images is equivalent to using 500 filters of size 18\u00d718 with stride 14 in the first layer.", "startOffset": 62, "endOffset": 66}], "year": 2016, "abstractText": "Building a good generative model for image has long been an important topic in computer vision and machine learning. Restricted Boltzmann machine (RBM) [5] is one of such models that is simple but powerful. However, its restricted form also has placed heavy constraints on the model\u2019s representation power and scalability. Many extensions have been invented based on RBM in order to produce deeper architectures with greater power. The most famous ones among them are deep belief network [6], which stacks multiple layer-wise pretrained RBMs to form a hybrid model, and deep Boltzmann machine [15], which allows connections between hidden units to form a multi-layer structure. In this paper, we present a new method to compose RBMs to form a multi-layer network style architecture and a training method that trains all layers/RBMs jointly. We call the resulted structure deep restricted Boltzmann network. We further explore the combination of convolutional RBM with the normal fully connected RBM, which is made trivial under our composition framework. Experiments show that our model can generate descent images and outperform the normal RBM significantly in terms of image quality and feature quality, without losing much efficiency for training.", "creator": "LaTeX with hyperref package"}}}