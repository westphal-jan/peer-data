{"id": "1611.07941", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Nov-2016", "title": "Multi-Modal Mean-Fields via Cardinality-Based Clamping", "abstract": "Mean Field inference is central to statistical physics. It has attracted much interest in the Computer Vision community to efficiently solve problems expressible in terms of large Conditional Random Fields. However, since it models the posterior probability distribution as a product of marginal probabilities, it may fail to properly account for important dependencies between variables. We therefore replace the fully factorized distribution of Mean Field by a weighted mixture of such distributions, that similarly minimizes the KL-Divergence to the true posterior. By introducing two new ideas, namely, conditioning on groups of variables instead of single ones and using a parameter of the conditional random field potentials, that we identify to the temperature in the sense of statistical physics to select such groups, we can perform this minimization efficiently. Our extension of the clamping method proposed in previous works allows us to both produce a more descriptive approximation of the true posterior and, inspired by the diverse MAP paradigms, fit a mixture of Mean Field approximations. We demonstrate that this positively impacts real-world algorithms that initially relied on mean fields.", "histories": [["v1", "Wed, 23 Nov 2016 19:14:25 GMT  (2082kb,D)", "http://arxiv.org/abs/1611.07941v1", "Submitted for review to CVPR 2017"]], "COMMENTS": "Submitted for review to CVPR 2017", "reviews": [], "SUBJECTS": "cs.CV cs.AI", "authors": ["pierre baqu\\'e", "fran\\c{c}ois fleuret", "pascal fua"], "accepted": false, "id": "1611.07941"}, "pdf": {"name": "1611.07941.pdf", "metadata": {"source": "CRF", "title": "Multi-Modal Mean-Fields via Cardinality-Based Clamping", "authors": ["Pierre Baqu\u00e9", "Fran\u00e7ois Fleuret", "Pascal Fua"], "emails": ["firstname.lastname@epfl.ch"], "sections": [{"heading": null, "text": "We therefore replace the fully factorized distribution of the mean field with a weighted mix of such distributions, which similarly minimizes the KL divergence to the true rear field. By introducing two new ideas, namely conditioning to groups of variables instead of individual variables, and using a parameter of the conditional potentials of random fields that we identify in the sense of statistical physics to select such groups, we can efficiently perform this minimization. Our expansion of the clamping method proposed in previous work allows us both to generate a more descriptive approximation of the true rear field and, inspired by the various MAP paradigms, to adapt a mixture of approximations of the Mean Field. We show that this positively impacts on real algorithms that originally relied on midfields."}, {"heading": "1 Introduction", "text": "In fact, most of them will be able to move to a different world in which they are able to escape than to another world in which they are able to escape."}, {"heading": "2 Background and Related Work", "text": "Conditional random fields (CRFs) are often used to represent correlations between variables [36]. Midfield conclusions are a means to approach them efficiently."}, {"heading": "2.1 Conditional Random Fields", "text": "Let X = (X1,.., XN) represent hidden variables, and I an image proof. A CRF relates one to the other via a posterior probability distributionP (X | I) = exp (\u2212 E (X | I) \u2212 log (Z (I))), (1) where E (X | I) is an energy function that is the sum of the terms defined as potentials \u03c6c (\u00b7) defined on a series of graph cliques. C, log (Z (I) is the log partition function that normalizes the distribution. Henceforth, we will omit the dependency with respect to I."}, {"heading": "2.2 Mean Field Inference", "text": "In fact, it is that we see ourselves in a position to be in, and that we are in a position, we will be in a position, we will be in a position, we will be in a position."}, {"heading": "3 Motivation", "text": "To motivate our approach, we present here a toy example that illustrates a typical error mode of the standard MF technique that our method is designed to prevent. Fig. 1 shows a CRF in which each pixel represents a binary variable connected to its neighbors by attractive pairs of potentials. To illustrate, we divide the grid into four zones as follows: The attractive terms are weak on the left, but strong on the right. Similarly, the simple terms in the upper part favor the value of 1, while they are completely random in the lower part. The simple potentials are displayed in the upper left of Fig. 1, and the result of the standard MF approximation at the bottom in terms of the probability that the pixels are assigned the designation of 1. In the lower right corner of the grid, since the interaction potentials are strong, all pixels are ultimately assigned with a high probability of 1 by MF, where they might just as well be set to zero."}, {"heading": "4 Multi-Modal Mean Fields", "text": "In fact, most of them are able to abide by the rules that they apply in practice."}, {"heading": "5 Partitioning the State Space", "text": "In this section we describe the cardinality criterion by which we recursively split state spaces and explain why it enables an efficient optimization of the KL divergence KL (QMM-P), where QMM is the mixture of equation 4."}, {"heading": "5.1 Cardinality Based Clamping", "text": "The state space partitionXk, 1 \u2264 k \u2264 K presented above is at the center of our approach, and its quality and tractability depend crucially on how well chosen it is. In [37], each split is achieved by setting the value of a single binary variable to zero or one. In other words, when giving a set of states Xk to split, it is split into subsets X 1k = {x-Xk | xi = 0} and X 2k = {x-Xk | xi = 1}, where i is the index of a particular variable. However, to calculate a Mean Field Approximation to P in each of these subspaces, one only needs to perform a standard Mean Field Approximation, while the Qi probability assigned to the bracketed variable is either zero or one. However, this is a limitation on the large and dense CRFs used in practice."}, {"heading": "5.2 Instantiating the Multi-Modal Approximation", "text": "The cardinality clamping scheme introduced above yields a state space partitionXk, 1 \u2264 k \u2264 k (K). We now show that in such a partition, minimizing the KL divergence KL (QMM \u2211 P) becomes tractable using the multimodal approximation of Eq. 4 below the disjointness constraint. In practice, we loosen the constraint 5 to near disjointness while retaining the ability to calculate the KL divergence to O (log ()). Let m and q \u00b2 stand for all mk and qki parameters that appear in Eq. 4, and eliminate the need to restrict each individual variable tightly while resolving the optimization of the KL divergence to O (log ()."}, {"heading": "5.2.1 Handling Two Modes", "text": "First, let us consider the case where we create only two modes modelled by Q1 (x) = Q1i (xi) and Q2 (x) = Q2i (xi) and try to estimate the q 1 i probabilities, and the q 2 i probabilities are evaluated in a similar way. Let's remember from Section 5.2 that the Q1i must be such that the A1 term of Q10 is maximised under the near-disjointness scheme defined by Q1. Performing this maximisation using a standard Lagrangian dual procedure [8] requires the evaluation of the restriction and its derivatives. Despite the potentially exponentially large number of terms involved, we can do this in one of two ways. In both cases, the Lagrangian dual procedure [8] requires the evaluation of the restriction and its derivatives."}, {"heading": "5.2.2 Handling an Arbitrary Number of Nodes", "text": "Remember from section 5 that in general there can be an arbitrary number of modes. They correspond to the leaves of a binary tree created by a sequence of cardinal splits. Therefore, let us consider the mode k for 1 \u2264 k \u2264 K. Let B be the set of branching points on the way to it. Near-splitting 16 can only be enforced with | B | constraints. For each b-B constraint there is a list of variables ib1,... i b Lb, a list of values v b 1,..., v b Lb, a cardinality threshold Cb and a sign of inequality \u2265 b defining a constraint Qk (\u0432u = 1... Lb 1 (Xibu = v b b u) \u2265 b Cb) \u2264 (13) of the same form as that of Equation 12. It causes disconnection with all modes in the sub-tree on the side of the b that do not belong to the mode, but only with Eq-10, but with a constraint."}, {"heading": "6 Selecting Variables to Clamp", "text": "We now present an approach for selecting the variables i1,.., iL and the values v1,.., vL, which define the cardinality splits of Eq.6 and 7 based on phase transitions in the graphical model. To this end, we first introduce a temperature parameter into our model, which allows us to smooth out the probability distribution we want to approximate. This parameter [18], well known to physicists, was used in another context in the vision of [28]. We examine its influence on the corresponding MF approximation and how we can use the resulting behavior to select suitable values for our variables."}, {"heading": "6.1 Temperature and its Influence on Convexity", "text": "We take temperature T as a number that we use to redefine the probability distribution of equation 1 asPT (x) = 1 ZT e \u2212 1 T E (x), where ZT is the partition function that normalizes PT so that its integral is one. For T = 1, PT is reduced to P. Since T goes into infinity, it always results in the same maximum aposteriori value, but becomes increasingly smoother. When performing the MF approximation at high T, the first term of KL divergence dominates, convex negative entropy, and makes the problem convex. At decrease of T, the second term of KL divergence, the expected energy, dominant, the function ceases to be convex, and local minima can begin to appear. In the supplementary material, we introduce a physically inspired proof that in the case of a dense Gaussian CRF [23] we can approximate the CRF limit temperature and be more easily validated above the CRF temperature at the CRF-K, the temperature of the CRF-K being validated."}, {"heading": "6.2 Entropy-Based Splitting", "text": "To find it, we start at Tmax, a temperature high enough to make the KL divergence convex and gradually decrease it. For each successive temperature, we perform the MF approximation, starting with the estimate for the previous one, in order to speed up the calculation. If we look at the resulting approximate values starting from the lowest temperature values T = 1, a significant sign of increasing convexity is that the assignment of some variables that were very unambiguous suddenly becomes uncertain. Intuitively, this happens when the CRF terms that bind the variables are overcome by entropy terms, which increase uncertainty. In physical terms, this can be regarded as a local phase transition [18]."}, {"heading": "7 Results", "text": "We first use synthetic data to show that MMMF can better approximate a multimodal probability density function than the standard MMF and the most recent approach of [37], which also relies on terminals to explore multiple modes. We will then show that this leads to a real performance gain for two real algorithms - one for person recognition [13] and the other for segmentation [9, 40] - both based on a traditional Mean Field approach. We will make publicly available all of our code and test datasets introduced in Section 6. The parameters that MMMF controls are the number of modes we use, the cardinality threshold C for each split, the value of Equation 16, the entropy thresholds of Equation 15, and the temperature Tmax introduced in Section 6. In all of our experiments, we use the values = 10 \u2212 4, hlow = 0.3, and h0.7 to set the temperature Tlimit. As discussed in Maximum 6, if this maximum GF is equal to the CRIC, we can only hold the Tannsian temperature limit."}, {"heading": "7.1 Synthetic Data", "text": "To show that our approach minimizes KL divergence better than the two standard MF and the clamping of one of [37], we use the same experimental protocol to generate conditional random fields with random weights as in [12, 38, 37]. Our task is then to find the MMMF approach with lowest KL divergence for a given number of nodes. If this number is one, it is reduced to MF. Note that the authors of [37] are looking for an approximation of the log partition function, which is strictly the same as the minimization of KL divergence as shown in the supplementary material. As these are randomly selected positive and negative weights, this problem effectively mimics difficult real shapes with repellent terms, uncontrolled loops, and strong correlations. In Figure 2, we draw the KL divergence as the function of the number of mochmarks used for bending the standard."}, {"heading": "7.2 Multi-modal Probabilistic Occupancy Maps", "text": "This year it is so far that it is only a matter of time before it is so far, until it is so far."}, {"heading": "7.3 Multi-Modal Semantic Segmentation", "text": "This year it has come to the point that it will only be a matter of time before it will happen, until it does."}, {"heading": "8 Conclusion", "text": "We have shown that our MMMF approach makes it possible to add a structure to the standard MF approximation of CRFs and increase the performance of the dependent algorithms. In fact, our algorithm generates several alternative MF approximations with their assigned probabilities, effectively modelling complex situations in which more than one interpretation is possible. As MF has recently been integrated into structured learning architectures using the back-mean field method [11, 25, 41, 1], future work will also aim to replace MF with MMMF in this context."}, {"heading": "A Proofs for Multi-Modal Mean-Fields via CardinalityBased Clamping", "text": "This document provides technical details and evidence related to section 5. We first prove the approximation of the KL divergence used in Eq. (9) Then we show that the problem we are trying to solve in Eq, in Eq. 9, minimizing the KL divergence is actually equivalent to solving (37), namely the search for an approximation to the log divergence function. (8) Finally, we justify the Gaussian divergence, which in the case of large bracket groups in 5.2.1- (2).A.1 Minimizing the KL divergence Let us see how the KL divergence between QMM and P can be minimized with respect to the mk parameters and to the Qk distributions. (9) We reform the problem up to a constant approximation factor of the order ()."}, {"heading": "B Computing the Critical Temperature for the Dense Gaussian CRFs", "text": "We assume that the RGB distance between the pixels is uniform and equal to drgb. (...) We assume that the RGB derivative is constant with the values of the RGB kernels. (...) We assume that the RGB spacing between the pixels is uniform and equal to drgb. (...) We assume that the RGB derivative is uniform and equal to drgb. (...) We assume that the RGB spacing between the pixels is uniform and equal to drgb. (...) We assume that the RGB spacing between the pixels is equal to drgb. (...) We assume that the RGB spacing between the pixels is equal to drgb. (...) We assume that the RGB spacing between the pixels is uniform and equal to drgb."}, {"heading": "C K-Shortest Path algorithm for the Multi-Modal Probabilistic Occupancy Maps", "text": "Here we present the algorithms we use in each step to reconstruct the problem."}, {"heading": "D Pseudo-code for the Multi-Modal Mean-Fields algorithm", "text": "The results of the study show that the distribution differences between the individual distribution components are very different in the individual distribution components. Algorithm D summarizes the operations to divide the individual distribution components into two, or, in other words, to obtain the two additional constraints used to define the two newly created subgroups. Algorithm 2 summarizes the operations to obtain the multi-modal mean field distribution by covering the entire tree.In Algorithm 2, ConstraintTree, is considered as a tree in the form of a list of distribution components, each containing a branch point or sheet, except for the root, in a broad first order. The function mode pathto (nNode), returns the set of indices corresponding to the branch points on the branch point, or leaf with index nNode, including index nNode self.Function (Constract: An energy input: A)."}], "references": [], "referenceMentions": [], "year": 2016, "abstractText": "<lb>Mean Field inference is central to statistical physics. It has attracted much in-<lb>terest in the Computer Vision community to efficiently solve problems expressible<lb>in terms of large Conditional Random Fields. However, since it models the pos-<lb>terior probability distribution as a product of marginal probabilities, it may fail to<lb>properly account for important dependencies between variables.<lb>We therefore replace the fully factorized distribution of Mean Field by a weighted<lb>mixture of such distributions, that similarly minimizes the KL-Divergence to the<lb>true posterior. By introducing two new ideas, namely, conditioning on groups of<lb>variables instead of single ones and using a parameter of the conditional random<lb>field potentials, that we identify to the temperature in the sense of statistical physics<lb>to select such groups, we can perform this minimization efficiently. Our extension<lb>of the clamping method proposed in previous works allows us to both produce a<lb>more descriptive approximation of the true posterior and, inspired by the diverse<lb>MAP paradigms, fit a mixture of Mean Field approximations. We demonstrate that<lb>this positively impacts real-world algorithms that initially relied on mean fields.", "creator": "LaTeX with hyperref package"}}}