{"id": "1611.08002", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Nov-2016", "title": "Semantic Compositional Networks for Visual Captioning", "abstract": "A Semantic Compositional Network (SCN) is developed for image captioning, in which semantic concepts (i.e., tags) are detected from the image, and the probability of each tag is used to compose the parameters in a long short-term memory (LSTM) network. The SCN extends each weight matrix of the LSTM to an ensemble of tag-dependent weight matrices. The degree to which each member of the ensemble is used to generate an image caption is tied to the image-dependent probability of the corresponding tag. In addition to captioning images, we also extend the SCN to generate captions for video clips. We qualitatively analyze semantic composition in SCNs, and quantitatively evaluate the algorithm on three benchmark datasets: COCO, Flickr30k, and Youtube2Text. Experimental results show that the proposed method significantly outperforms prior state-of-the-art approaches, across multiple evaluation metrics.", "histories": [["v1", "Wed, 23 Nov 2016 21:22:22 GMT  (1235kb,D)", "http://arxiv.org/abs/1611.08002v1", null], ["v2", "Tue, 28 Mar 2017 18:33:51 GMT  (2051kb,D)", "http://arxiv.org/abs/1611.08002v2", "Accepted in CVPR 2017"]], "reviews": [], "SUBJECTS": "cs.CV cs.CL cs.LG", "authors": ["zhe gan", "chuang gan", "xiaodong he", "yunchen pu", "kenneth tran", "jianfeng gao", "lawrence carin", "li deng"], "accepted": false, "id": "1611.08002"}, "pdf": {"name": "1611.08002.pdf", "metadata": {"source": "CRF", "title": "Semantic Compositional Networks for Visual Captioning", "authors": ["Zhe Gan", "Chuang Gan", "Xiaodong He", "Yunchen Pu", "Kenneth Tran", "Jianfeng Gao", "Lawrence Carin", "Li Deng"], "emails": ["lcarin}@duke.edu,", "ganchuang1990@gmail.com", "deng}@microsoft.com"], "sections": [{"heading": "1. Introduction", "text": "Most of these approaches learn a probable model of caption based on an image or a video. However, most of these approaches are based on the fact that it is a real image. Most of these approaches are based on the fact that it is a real image. However, most of these approaches are inspired by the way it is used in the real world. Most of these approaches are shaped by the way it is used in the real world. Most of these approaches are inspired by the way it is used in the real world."}, {"heading": "2. Related work", "text": "We focus on the latest neural network-based literature for image editing, as it is most relevant to our work. Such models typically extract a visual vector for a video and send this vector to a language model for image editing. The differences between the different methods are mainly in the types of CNN architectures and language models. For example, the vanilla method was used for image editing, while the LSTM [13] was used for exploitation."}, {"heading": "3. Semantic compositional networks", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1. Review of RNN for image captioning", "text": "Consider an image I, with the accompanying caption X. First, we extract the vector v (I), which often represents the uppermost properties of a prefabricated CNN. For convenience, we leave out the explicit dependence on I and represent the visual vector v. The length-T label is typically represented as X = (x1,.., xT), with a text a 1-of-V (\"one hot\") encoding vector, with V the size of the vocabulary. The length T typically varies between different notions. The t-th word in a caption, xt, is linearly embedded in an nx-dimensional real vector wt = text, where we embed Rnx \u00b7 V is a word that embeds (learned) the matrix, i.e., wt is a column of We choose the vector x."}, {"heading": "3.2. Semantic concept detection", "text": "The SCN developed below is based on the recognition of semantic concepts, i.e. tags, in the image to be examined. To identify such tags from an image, we first select a series of tags from the caption. Following [11], we use the most commonly used K-words in the captions to determine the vocabulary of the tags, which includes the most common nouns, verbs, or adjectives. To predict semantic concepts in the test image, we treat this problem, motivated by [42], as a multi-level classification task. Suppose there are N training examples, and yi = [yi1,....) yiK is the label vector of the i-th image, where yik = 1 if the image is commented with tag k, and yik = 0 otherwise. Let vi and si represent the image feature vector and the semantic feature vector for the i-th image."}, {"heading": "3.3. SCN-RNN", "text": "The WN's. \"s.\" s. \"s.\" W \"s.\" s. \"W\" s. \"s.\" s. \"s.\" s. \"s.\" s. \"s.\" s. \"s.\" s. \"s.\" s. \"s.\" s. \"s.\" W \"s.\" s. \"s.\" s. \"s.\" s. \"s.\" s. \"s.\" s. \"s.\" s \"s.\" s. \"s.\" s. \"s.\" s. \"s.\" s \"s.\" s \"s.\" s \"s.\" s. \"s\" s. \"s.\" s. \"s.\" s \"s.\" s \"s.\" s \"s.\" s \"s.\" s \"s.\" s \"s.\" s. \"s.\" s \"s.\" s \"s\" s. \"s\" s \"s.\" s \"s\" s. \"s\" s \"s.\" s \"s\" s. \"s\" s \"s.\" s \"s\" s. \"s\" s \"s.\" s \"s\" s \"s.\" s \"s\" s \"s.\" W \"s\" s \"s\" s. \"s\" s \"s\" s \"s\" s. \"s\" s \"s\" s \"s\" s. \"s\" s \"s\" s \"s\" s. \"s\" s \"s\" s \"s\" s \"s.\" s \"s\" s \"s\" s. \"s\" s \"s\" s \"s\" s \"s\" s \"W\" s \"s.\" s \"s\" s \"s\" s \"s\" s. \"s\" s \"s\" s \"s\" s \"s\" s. \"s\" s \"s\" s \"s.\" s \"s\" s \"s\" s \"s\" s. \"s\" s \"s\" s. \"s\" s \"s\" s \"s\" s. \"s\" s \"s\" s \"s\" s. \"s.\" W \"s\" s \"s\" s. \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\""}, {"heading": "3.4. SCN-LSTM", "text": "RNNs with LSTM units [13] have become a popular architecture due to their representational power and effectiveness in mapping long-term dependencies. We generalize the SCN RNN model by using LSTM units. Specifically, we define ht = H (xt \u2212 1, ht \u2212 1, v, s) asit = \u03c3 (Wiax-i, t \u2212 1 + Uiah-i, t \u2212 1 + z), (13) f \u2212 t = \u03c3 (Wfax-f, t \u2212 1 + Ufah-f, t \u2212 1 + z), (14) ot = \u03c3 (Woax-o, t \u2212 1 + Uoah-o, t \u2212 1 + z), (15) c-t = Wcax-c, t \u2212 1 + Ucah-f-f, t \u2212 f-z), (16) ct = es c-t-c, (17) ht \u2212 tanh (ct)."}, {"heading": "3.5. Extension to video captioning", "text": "To effectively represent the spatio-temporal visual content of a video, we use a two-dimensional (2D) and a three-dimensional (3D) CNN to extract visual characteristics of video images / clips. Then, we perform a middle pooling process [40] over all 2D CNN characteristics and 3D CNN characteristics to generate two feature vectors (one from 2D CNN characteristics and the other from 3D CNN characteristics).The representation of each video, v, occurs by concatenating these two characteristics. Likewise, we obtain the semantic concept vectors by executing the semantic concept detector based on video representation. Once v and s are achieved, we use the same model proposed above for directly generating video captions as described in Figure 2 (b)."}, {"heading": "4. Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1. Datasets", "text": "We present the results on three benchmark datasets: COCO [23], Flickr30k [48] and Youtube2Text [4]. COCO and Flickr30k serve as captions and contain 123287 and 31783 images, respectively. Each image is accompanied by at least 5 captions. We use the same predefined splits as [16] for all datasets: on Flickr30k there are 1000 images for validation, 1000 for testing and the rest for training; and for COCO 5000 images are used for both validation and testing. We continued to test our model on the official COCO test set, which consists of 40775 images (human-made captions for this split are not publicly available) and evaluated our model on the COCO evaluation server. We also follow the publicly available code [16] to pre-process the captions, resulting in vocabulary sizes of 8791 and 7414 captions for COCO and FlickrCO 30k."}, {"heading": "4.2. Training procedure", "text": "For image rendering we take the output of 2048 - mode pool5 layer of ResNet-152 [12], pretrained on theImageNet dataset [31]. For video rendering, in addition to using 2D ResNet-152 to extract functions on each video image, we also use a 3D CNN (C3D) [37] to extract features on each video. C3D is pretrained on Sports-1M video dataset [17], and we take the output of the 4096-mode fc7 layer from C3D as video rendering. We consider the RGB frames of videos as input, with 2 frames per second. Each video frame is as 112 x 112 and 224 x 224 \u00d7 224 for the C3D and ResNet-152 function extractor, respectively. The C3D function extractor is applied on video clips of the length 16 frames (as in [17] with an overlap of 8 frames."}, {"heading": "4.3. Evaluation", "text": "The widely used metrics BLEU [29], METEOR [3], ROUGEL [22] and CIDER-D [38] are reported in our quantitative evaluation of the performance of the proposed model and the baselines contained in the literature. All metrics are calculated using the code published by the COCO evaluation server [5]. For COCO and Flickr30k datasets, in addition to comparing with results from previous work, we have also reimplemented strong baselines for comparison. The results of the caption are presented in Table 1. The models we implemented are the following: 1. LSTM-R / LSTM-T / LSTM-RT: R, T, RT denotes the use of different characteristics. Specifically, R denotes the visual characteristic vector ResNet, T denotes the tags (i.e. the semantic concept vector), and RT denotes the concatenation of R and T. The characteristics are entered in a standard model STM."}, {"heading": "4.4. Quantitative results", "text": "Performance on COCO and Flickr30k We first present results on the task of image captioning, summarized in Table 1. The use of tags (LSTM-T) provides better performance than leveraging visual features alone (LSTM-R). Combining both tags and visual features further horizonts performance, as expected. Combined with only feed the tags in the LSTM at the initial time step (LSTM-RT), LSTM-RT2 comes better results, as it input the tag function in each time step. Furthermore, the direct comparison between LSTM-RT2 and SCN-LSTM shows the advantage of our proposed model, suggesting that our approach is a better way to merge semantic concepts into the LSTM. We also report results that form an ensemble of 5 identical SCN-LSTM models trained with different initializations."}, {"heading": "4.5. Qualitative analysis", "text": "Figure 3 shows three examples to illustrate the semantic composition of the caption generation. Our model correctly describes the content of the image by using the correctly determined tags. By manually replacing certain tags, our model can smoothly adjust the caption. For example, by replacing the tag \"grass\" with \"bed,\" our model imagines \"a dog lying on a bed.\" Our model is also able to generate novel captions that are highly unlikely in real life. For example, by replacing the tag \"street\" and \"street\" with \"ocean,\" our model imagines \"a bus driving in the sea\"; in the right image, by replacing the tag \"field\" with \"snow,\" our model dreams become \"a group of zebras standing in the snow.\" SCN not only absorbs the tags well (and imagines the corresponding scenes), but also chooses the right functional words for different concepts to create syntactically correct captions."}, {"heading": "5. Conclusion", "text": "We have introduced Semantic Compositional Network (SCN), a new framework for effectively comprising the individual semantic meaning of tags for visual subtitling. The SCN extends each weight matrix of the traditional LSTM to a three-way matrix product, with one of these matrices depending on the derived tags. Consequently, the SCN can be considered an interplay of tag-dependent LSTM bases, with the contribution of each LSTM base unit proportional to the probability that the tag is present in the image. Experiments with three visual subtitling data confirm the superiority of the proposed approach."}], "references": [], "referenceMentions": [], "year": 2016, "abstractText": "A Semantic Compositional Network (SCN) is developed<lb>for image captioning, in which semantic concepts (i.e., tags)<lb>are detected from the image, and the probability of each tag<lb>is used to compose the parameters in a long short-term mem-<lb>ory (LSTM) network. The SCN extends each weight matrix of<lb>the LSTM to an ensemble of tag-dependent weight matrices.<lb>The degree to which each member of the ensemble is used<lb>to generate an image caption is tied to the image-dependent<lb>probability of the corresponding tag. In addition to caption-<lb>ing images, we also extend the SCN to generate captions for<lb>video clips. We qualitatively analyze semantic composition<lb>in SCNs, and quantitatively evaluate the algorithm on three<lb>benchmark datasets: COCO, Flickr30k, and Youtube2Text.<lb>Experimental results show that the proposed method signifi-<lb>cantly outperforms prior state-of-the-art approaches, across<lb>multiple evaluation metrics.", "creator": "LaTeX with hyperref package"}}}