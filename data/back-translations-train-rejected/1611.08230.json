{"id": "1611.08230", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Nov-2016", "title": "Learning Fast Sparsifying Transforms", "abstract": "Given a dataset, the task of learning a transform that allows sparse representations of the data bears the name of dictionary learning. In many applications, these learned dictionaries represent the data much better than the static well-known transforms (Fourier, Hadamard etc.). The main downside of learned transforms is that they lack structure and therefore they are not computationally efficient, unlike their classical counterparts. This posses several difficulties especially when using power limited hardware such as mobile devices, therefore discouraging the application of sparsity techniques in such scenarios. In this paper we construct orthonormal and non-orthonormal dictionaries that are factorized as a product of a few basic transformations. In the orthonormal case, we solve exactly the dictionary update problem for one basic transformation, which can be viewed as a generalized Givens rotation, and then propose to construct orthonormal dictionaries that are a product of these transformations, guaranteeing their fast manipulation. We also propose a method to construct fast square but non-orthonormal dictionaries that are factorized as a product of few transforms that can be viewed as a further generalization of Givens rotations to the non-orthonormal setting. We show how the proposed transforms can balance very well data representation performance and computational complexity. We also compare with classical fast and learned general and orthonormal transforms.", "histories": [["v1", "Thu, 24 Nov 2016 15:57:09 GMT  (350kb)", "https://arxiv.org/abs/1611.08230v1", null], ["v2", "Sun, 28 May 2017 13:09:45 GMT  (528kb)", "http://arxiv.org/abs/1611.08230v2", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["cristian rusu", "john thompson"], "accepted": false, "id": "1611.08230"}, "pdf": {"name": "1611.08230.pdf", "metadata": {"source": "CRF", "title": "Learning Fast Sparsifying Transforms", "authors": ["Cristian Rusu", "John Thompson"], "emails": ["john.thompson}@ed.ac.uk."], "sections": [{"heading": null, "text": "It is not the first time that we have to deal with the issue as it relates to the processing of data we have learned in practice in recent years. (It is not the first time that the processing of data we have learned in the last five years, in the last five years, in the last five years, in the last five years.) The key idea of this approach is not the use of data itself. (It is not the first time that the processing of data in the last five years, in the last five years, in the last five years, in the last five years, in the last five years, in the last five years, in the last five years, in the last five years, in the last five years, in the last five years, in the last five years, in the last five years, in the last five years, in the last five years, in the last five years, in the last five years, in the last five years. (It is the first time that the processing of data in the last five years, in the last five years, in the last five years, in the last five years, in the last five years, and in the United States.) It is the last time that we have taken in the last five years, in the last five years in the last five years, in the last five years, in the last five years, in the last five years, in the last five years, in the last five years, in the last five years, in the United States, in the last year, in the last five years, in the last five years, in the United States, in the last five years, and in the last five years, in the last five years, in the last year, in the last five years, in the United States, in the last year, in the last five years, in the last five years, in the last five years, in the last five years, and in the last five years, in the last five years, in the last years, in the last five years, in the last five years, in the last years, in the last five years, in the last five years, in the last five years, in the last years, in the last years, in the last years, in the last five years, in the last years, in the last five years, in the last years, in the last years, in the last five years, in the last years, in the last"}, {"heading": "II. A BRIEF DESCRIPTION OF DICTIONARY LEARNING OPTIMIZATION PROBLEMS", "text": "Given a real data set Y, Rn \u00b7 N and the splitter level s, the general dictionary learning problem consists of producing the factorization Y, DX, which is given by the optimization problem: minimize D, X, Y, DX, 2Fsubject to factorization 0, 1, 2, 3, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5"}, {"heading": "III. A BUILDING BLOCK FOR FAST TRANSFORMS", "text": "For indices (i, j), j > i and variables p, q, t, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c"}, {"heading": "IV. A METHOD FOR DESIGNING FAST ORTHOGONAL", "text": "TRANSFORMS: Gm-DLAIn this section, we propose a method called Gm-DLA to learn orthogonal dictionaries factored as the product of m G transformations (restricted R transformations)."}, {"heading": "A. An overview of G-transforms", "text": "We call Gij a G transformation, an orthogonal limited rtransform (3), which is parameterized only by c, d-R with c2 + d2 = 1, and the indexes (i, j), i 6 = j, so that the non-zero part of Gij, which corresponds to (4), is given by G-ij. (8) Classically, a Givens rotation is a matrix like in (3) with G-ij = [c-d-c] so that det (Gij) = 1, i.e. correct rotation matrices are orthogonal matrices with determinant one. These rotations are important, because any orthogonal dictionary of size n-n can be factored in a product of (n-d) Givens rotations. (n 2) Givens rotations [35] are because we are interested in the computational complexity of these structures, we can have both onal jjjj-type of the 38 rejvens-structure (n n). [Givens-size 2]"}, {"heading": "B. One G-transform as a dictionary", "text": "(1), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2, (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2, (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2),"}, {"heading": "C. A method for designing fast orthogonal transforms: Gm\u2013 DLA", "text": "In this paper we propose to construct an orthogonal problem. (17) The aim of this section is to propose a learning method that constructs such a transformation method. We fix the representations X and all G-transformations in (17) except the kth, denoted asGikjk. To define the objective function Y \u2212 UX-2F = Gimjm Y \u2212 Gimjm. (Gi1j1X) 2 F = Gimjm."}, {"heading": "V. A METHOD FOR DESIGNING FAST, GENERAL,", "text": "NON-ORTHOGONAL TRANSFORMS: Rm-DLAIn orthogonal dictionaries, the basic building blocks such as householder reflectors and givens rotations are readily available. This is not the case with general dictionaries. In this section, we propose a building block for non-orthogonal structures in Section A and then show how they can be used in a similar way to the G transformation to learn mathematically efficient square non-orthogonal dictionaries by deriving the Rm-DLA method in Section B."}, {"heading": "A. A building block for fast non-orthogonal transforms", "text": "We assume that there are no restrictions on the variables p, q, r, t (these are four degrees of freedom) and therefore Rij (3), which are no longer orthogonally able to solve the following optimization formula (i, j), R (2), D (2), D (2), D (2), D (2), D (2), D (2), D (2), D (2), D (2), D (2), D (2), D (2), D (2), D (2), D (2), D (2), D (2), D (2), D (2), D (2), D (2), D (2), D (2), D (2), D (2), D, D (2), D, D (2), D, D (2), D, D (2, D, D, D (2), D, D (2), D (2, D, D, D (2), D (2), D (2), D (2, D (2), D (2), D (2, D (2), D (2), D (2, D (2), D (2), D (2), D (2), D (2, D (2), D (2), D (2, D (2), D (2), D (2, D (2), D (2, D, D (2), D (2, D, D, D (2), D (2), D (2, D (2), D (2, D, D (2), D (2), D (2), D (2, D (2), D (2, D, D (2), D (2), D (2, D (2), D (2, D (2), D (2, D (2), D (2, D (2), D (2, D (2), D (2), D (2, D (2), D (2, D (2), D, D (2, D (2, D, D (2),"}, {"heading": "B. A method for designing fast general transforms: Rm\u2013DLA", "text": "\"We are not able to do what we want.\" \"We are not able to do what we want.\" \"We.\" \"We.\" \"We.\" \"We.\" \"We.\" \"We.\" \"We.\" \"We.\" \"We.\" \"We.\" \"We.\" \"We.\" \"\" We. \"\" \"\" We. \"\" \"\". \"\" \"We.\" \"\" \"\". \"\" \"\" We. \"\" \"\" \"..\" \"\" \"\".. \"\" \"\".. \"\" \"\".. \"\" \"\".. \"\" \"\" \"..\" \"\" \"..\" \".\" \"\" We. \"\" \".\" \"\" \"..\" \"\" \"\" \"..\" \"\" \"\".. \"\" \"\" \"..\" \"\" \"..\" \"\""}, {"heading": "VI. THE COMPUTATIONAL COMPLEXITY OF USING LEARNED TRANSFORMS", "text": "The total number of operations is dominated by the construction of projections in the dictatorial column space, which requires 2n2 operations per sample, the other operations depend on the cost of economy and the cost of savings. The computational complexity of operations is dominated by the construction of projections in the dictatorial column space, the 2n2 operations per sample. The other operations depend on the cost of economy s and the cost of savings. The computational complexity of operations is dominated by the construction of projections in the dictatorial column space, which performs 2n2 operations per sample."}, {"heading": "VII. EXPERIMENTAL RESULTS", "text": "In this sense, it is also necessary for us to be able to set out in search of new ways that enable us to change the world and to change the world. (...) In this sense, it is important for us to be able to change the world. (...) It is important for us to be able to change the world. (...) It is important for us to be able to change the world. (...) It is very important for us to be able to change the world. (...) It is very important for us to be able to change the world. (...) It is important for us to be able to change the world. (...) It is important for us to be able to change the world. (...) It is important for us to understand how to change the world. (...) It is necessary for us to understand how to change the world. \"(...) It is necessary for us to change the world.\""}, {"heading": "VIII. CONCLUSIONS", "text": "In this paper, we present practical procedures for learning square orthogonal and non-orthogonal dictionaries that are already included in a fixed number of computationally efficient blocks. We show how effective the dictionaries created with the proposed methods are on image data, where we compare them with the fast cosine transformation on the one hand and general nonorthogonal and orthogonal dictionaries on the other. We also show comparisons with a recently proposed method that constructs orthogonal dictionaries for household appliances. We show empirically that the proposed methods construct transformations that provide an improved balance between computational complexity and representational performance between the methods we are considering. We are able to construct transformations that exhibit lower computational efficiency and display errors due to performance constraints than the fast cosine transformation for image data. We expect that current work will be able to utilize learned transformations based on time-critical and low-complexity devices."}], "references": [{"title": "Dictionary learning", "author": ["I. Tosic", "P. Frossard"], "venue": "IEEE Signal Processing Magazine, vol. 28, no. 2, pp. 27\u201338, 2011.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2011}, {"title": "From sparse solutions of systems of equations to sparse modeling of signals and images", "author": ["A.M. Bruckstein", "D.L. Donoho", "M. Elad"], "venue": "SIAM Review, vol. 51, pp. 34\u201381, 2009.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2009}, {"title": "Image denoising via sparse and redundant representations over learned dictionaries", "author": ["M. Elad", "M. Aharon"], "venue": "IEEE Trans. Image Proc., vol. 15, no. 12, pp. 3736\u20133745, 2006.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2006}, {"title": "Low complexity hybrid sparse precoding and combining in millimeter wave MIMO systems", "author": ["C. Rusu", "R. Mendez-Rial", "N. Gonzalez-Prelcic", "R.W. Heath"], "venue": "Proc. IEEE ICC, 2015.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2015}, {"title": "Task-driven dictionary learning", "author": ["J. Mairal", "F. Bach", "J. Ponce"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 34, no. 4, pp. 791\u2013804, 2012.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2012}, {"title": "On the computational intractability of exact and approximate dictionary learning", "author": ["A.M. Tillmann"], "venue": "IEEE Signal Processing Letters, vol. 22, no. 1, pp. 45\u201349, 2014.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2014}, {"title": "Method of optimal directions for frame design", "author": ["K. Engan", "S.O. Aase", "J.H. Hus\u00f8y"], "venue": "Proc. IEEE ICASSP, 1999, pp. 2443\u20132446.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1999}, {"title": "K-SVD: An algorithm for designing overcomplete dictionaries for sparse representation", "author": ["M. Aharon", "M. Elad", "A.M. Bruckstein"], "venue": "IEEE Trans. Sig. Proc., vol. 54, no. 11, pp. 4311\u20134322, 2006.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2006}, {"title": "On the uniqueness of overcomplete dictionaries, and a practical way to retrieve them", "author": ["\u2014\u2014"], "venue": "Linear Algebra and Applications, vol. 416, pp. 48\u201367, 2006.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2006}, {"title": "Direct optimization of the dictionary learning problem", "author": ["A. Rakotomamonjy"], "venue": "IEEE Trans. Sig. Proc., vol. 61, no. 22, pp. 5495\u20135506, 2013.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2013}, {"title": "An algorithm for the machine calculation of complex Fourier series", "author": ["J. Cooley", "J. Tukey"], "venue": "Mathematics of Computation, vol. 19, no. 90, pp. 297\u2013301, 1965.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1965}, {"title": "Just relax: Convex programming methods for subset selection and sparse approximation", "author": ["J.A. Tropp"], "venue": "IEEE Trans. Inf. Theory, vol. 52, pp. 1030\u20131051, 2006.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2006}, {"title": "Greed is good: Algorithmic results for sparse approximation", "author": ["\u2014\u2014"], "venue": "IEEE Trans. Inf. Theory, vol. 50, pp. 2231\u20132242, 2004.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2004}, {"title": "Message-passing algorithms for compressed sensing", "author": ["D.L. Donoho", "A. Maleki", "A. Montanari"], "venue": "Proceedings of the National Academy of Sciences, vol. 106, no. 45, pp. 18 914\u201318 919, 2009.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2009}, {"title": "Efficient implementation of the K-SVD algorithm using batch orthogonal matching pursuit", "author": ["R. Rubinstein", "M. Zibulevsky", "M. Elad"], "venue": "CS Technion, 2008.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2008}, {"title": "Learning sparsifying transforms", "author": ["S. Ravishankar", "Y. Bresler"], "venue": "IEEE Trans. Signal Process., vol. 61, no. 5, pp. 1072\u20131086, 2013.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2013}, {"title": "l0 sparsifying transform learning with efficient optimal updates and convergence guarantees", "author": ["\u2014\u2014"], "venue": "IEEE Trans. Signal Process., vol. 63, no. 9, pp. 2389\u20132404, 2015.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2015}, {"title": "Fast approximation of rotations and Hessians matrices", "author": ["M. Mathieu", "Y. LeCun"], "venue": "arXiv:1404.7195, 2014.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2014}, {"title": "Double sparsity: learning sparse dictionaries for sparse signal approximation", "author": ["R. Rubinstein", "M. Zibulevsky", "M. Elad"], "venue": "IEEE Trans. Sig. Proc., vol. 58, no. 3, pp. 1553\u20131564, 2010.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2010}, {"title": "Chasing butterflies: In search of efficient dictionaries", "author": ["L.L. Magoarou", "R. Gribonval"], "venue": "2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), April 2015, pp. 3287\u20133291.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2015}, {"title": "Toward fast transform learning", "author": ["O. Chabiron", "F. Malgouyres", "J.-Y. Tourneret", "N. Dobigeon"], "venue": "Technical report, 2013.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2013}, {"title": "Explicit shift-invariant dictionary learning", "author": ["C. Rusu", "B. Dumitrescu", "S.A. Tsaftaris"], "venue": "IEEE Signal Proc. Let., vol. 21, no. 1, pp. 6\u20139, 2014.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2014}, {"title": "Block orthonormal overcomplete dictionary learning", "author": ["C. Rusu", "B. Dumitrescu"], "venue": "21st European Sig. Proc. Conf., 2013, pp. 1\u20135.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2013}, {"title": "Learning doubly sparse transforms for images", "author": ["S. Ravishankar", "Y. Bresler"], "venue": "IEEE Trans. Image Process., vol. 22, no. 12, pp. 4598\u20134612, 2013.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2013}, {"title": "Fast orthonormal sparsifying transforms based on Householder reflectors", "author": ["C. Rusu", "N. Gonzalez-Prelcic", "R. Heath"], "venue": "IEEE Trans. Sig. Process., vol. 64, no. 24, pp. 6589\u20136599, 2016.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2016}, {"title": "The JPEG still picture compression standard", "author": ["G.K. Wallace"], "venue": "IEEE Trans. Consumer Electronics, vol. 38, no. 1, 1992.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 1992}, {"title": "Learning sparsely used overcomplete dictionaries", "author": ["A. Agarwal", "A. Anandkumar", "P. Jain", "P. Netrapalli", "R. Tandon"], "venue": "JMLR Workshop and Conference Proceedings, vol. 35, 2014, pp. 1\u201315.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2014}, {"title": "Sparse orthonormal transforms for image compression", "author": ["O.G. Sezer", "O. Harmanci", "O.G. Guleryuz"], "venue": "Proc. IEEE ICIP, 2008, pp. 149\u2013 152.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2008}, {"title": "Approximation and compression with sparse orthonormal transforms", "author": ["O.G. Sezer", "O.G. Guleryuz", "Y. Altunbasak"], "venue": "IEEE Trans. Image Proc., vol. 24, no. 8, pp. 2328\u20132343, 2015.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2015}, {"title": "An EM-algorithm approach for the design of orthonormal bases adapted to sparse representations", "author": ["A. Dremeau", "C. Herzet"], "venue": "Proc. IEEE ICASSP, 2010, pp. 2046\u20132049.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2010}, {"title": "Learning efficient data representations with orthogonal sparse coding", "author": ["H. Schutze", "E. Barth", "T. Martinetz"], "venue": "IEEE Trans. Comp. Imaging, vol. 2, no. 3, pp. 177\u2013189, 2016.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2016}, {"title": "Learning unions of orthonormal bases with thresholded singular value decompositon", "author": ["S. Lesage", "R. Gribonval", "F. Bimbot", "L. Benaroya"], "venue": "Proc. IEEE ICASSP, 2005, pp. 293\u2013296.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2005}, {"title": "A generalized solution of the orthogonal Procrustes problem", "author": ["P. Schonemann"], "venue": "Psychometrika, vol. 31, no. 1, pp. 1\u201310, 1966.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 1966}, {"title": "Matrix Computations", "author": ["G.H. Golub", "C.F. Van Loan"], "venue": null, "citeRegEx": "35", "shortCiteRegEx": "35", "year": 1996}, {"title": "The sparse matrix transform for covariance estimation and analysis of high dimensional signals", "author": ["C. Guangzhi", "L.R. Bachega", "C.A. Bouman"], "venue": "IEEE Trans. Image Processing, vol. 20, no. 3, pp. 625\u2013640, 2011.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2011}, {"title": "Multiresolution matrix factorization", "author": ["R. Kondor", "N. Teneva", "V. Garg"], "venue": "Proc. of the 31st International Conference on Machine Learning, 2014, pp. 1620\u20131628.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2014}, {"title": "An initialization strategy for the dictionary learning problem", "author": ["C. Rusu", "B. Dumitrescu"], "venue": "Proc. IEEE ICASSP, 2014, pp. 6731\u20136735.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2014}, {"title": "Stagewise K-SVD to design efficient dictionaries for sparse representations", "author": ["\u2014\u2014"], "venue": "IEEE Signal Processing Letters, vol. 19, no. 10, pp. 631\u2013634, 2012.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2012}, {"title": "A fast computational algorithm for the discrete cosine transform", "author": ["W.-H. Chen", "C.H. Smith", "S.C. Fralick"], "venue": "IEEE Trans. Communications, vol. 25, no. 9, pp. 1004\u20131009, 1977.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 1977}, {"title": "Double sparsity: Learning sparse dictionaries for sparse signal approximation", "author": ["R. Rubinstein", "M. Zibulevsky", "M. Elad"], "venue": "IEEE Trans. Sig. Process., vol. 58, no. 3, pp. 1553\u20131564, 2010.", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2010}, {"title": "Fast design of efficient dictionaries for sparse representations", "author": ["C. Rusu"], "venue": "Proc. IEEE Machine Learning for Signal Processing, 2012, pp. 1\u20135.", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2012}], "referenceMentions": [{"referenceID": 0, "context": "INTRODUCTION Dictionary learning methods [1] represent a well-known class of algorithms that have seen many applications in signal processing [2], image processing [3], wireless communications [4] and machine learning [5].", "startOffset": 41, "endOffset": 44}, {"referenceID": 1, "context": "INTRODUCTION Dictionary learning methods [1] represent a well-known class of algorithms that have seen many applications in signal processing [2], image processing [3], wireless communications [4] and machine learning [5].", "startOffset": 142, "endOffset": 145}, {"referenceID": 2, "context": "INTRODUCTION Dictionary learning methods [1] represent a well-known class of algorithms that have seen many applications in signal processing [2], image processing [3], wireless communications [4] and machine learning [5].", "startOffset": 164, "endOffset": 167}, {"referenceID": 3, "context": "INTRODUCTION Dictionary learning methods [1] represent a well-known class of algorithms that have seen many applications in signal processing [2], image processing [3], wireless communications [4] and machine learning [5].", "startOffset": 193, "endOffset": 196}, {"referenceID": 4, "context": "INTRODUCTION Dictionary learning methods [1] represent a well-known class of algorithms that have seen many applications in signal processing [2], image processing [3], wireless communications [4] and machine learning [5].", "startOffset": 218, "endOffset": 221}, {"referenceID": 5, "context": "While the dictionary learning problem is NP-hard [6] in general, it has been extensively studied and several good algorithms to tackle it exist.", "startOffset": 49, "endOffset": 52}, {"referenceID": 6, "context": "Alternating minimization methods like the method of optimal directions (MOD) [7], K\u2013SVD [8], [9] and direct optimization [10] have been shown to work well in practice and also enjoy some theoretical performance guarantees.", "startOffset": 77, "endOffset": 80}, {"referenceID": 7, "context": "Alternating minimization methods like the method of optimal directions (MOD) [7], K\u2013SVD [8], [9] and direct optimization [10] have been shown to work well in practice and also enjoy some theoretical performance guarantees.", "startOffset": 88, "endOffset": 91}, {"referenceID": 8, "context": "Alternating minimization methods like the method of optimal directions (MOD) [7], K\u2013SVD [8], [9] and direct optimization [10] have been shown to work well in practice and also enjoy some theoretical performance guarantees.", "startOffset": 93, "endOffset": 96}, {"referenceID": 9, "context": "Alternating minimization methods like the method of optimal directions (MOD) [7], K\u2013SVD [8], [9] and direct optimization [10] have been shown to work well in practice and also enjoy some theoretical performance guarantees.", "startOffset": 121, "endOffset": 125}, {"referenceID": 10, "context": "directly using O(n log n) computations for example [11].", "startOffset": 51, "endOffset": 55}, {"referenceID": 11, "context": "In a general non-orthogonal (and even overcomplete) dictionary, special non-linear reconstruction methods such as l1 minimization [12], greedy approaches like orthogonal matching pursuit (OMP) [13] or variational Bayesian algorithms like approximate message passing (AMP) [14] need to be applied.", "startOffset": 130, "endOffset": 134}, {"referenceID": 12, "context": "In a general non-orthogonal (and even overcomplete) dictionary, special non-linear reconstruction methods such as l1 minimization [12], greedy approaches like orthogonal matching pursuit (OMP) [13] or variational Bayesian algorithms like approximate message passing (AMP) [14] need to be applied.", "startOffset": 193, "endOffset": 197}, {"referenceID": 13, "context": "In a general non-orthogonal (and even overcomplete) dictionary, special non-linear reconstruction methods such as l1 minimization [12], greedy approaches like orthogonal matching pursuit (OMP) [13] or variational Bayesian algorithms like approximate message passing (AMP) [14] need to be applied.", "startOffset": 272, "endOffset": 276}, {"referenceID": 14, "context": "For example, the classical OMP has complexityO(sn) [15] and, assuming that we are looking for sparse approximations with s \u226a n, it is in general computationally cheaper than l1 optimization.", "startOffset": 51, "endOffset": 55}, {"referenceID": 15, "context": "For the analysis dictionary, recent work based on transform learning [16], [17] has been proposed.", "startOffset": 69, "endOffset": 73}, {"referenceID": 16, "context": "For the analysis dictionary, recent work based on transform learning [16], [17] has been proposed.", "startOffset": 75, "endOffset": 79}, {"referenceID": 17, "context": "This connects our paper to previous work on approximating orthogonal (and symmetric) matrices [18] such that matrix-vector multiplications are computationally efficient.", "startOffset": 94, "endOffset": 98}, {"referenceID": 18, "context": "Previous work [19], [20], [21], [22], [23], [24], [25] in the literature has already proposed various structured dictionaries to cope with the high computational complexity of learned transforms.", "startOffset": 14, "endOffset": 18}, {"referenceID": 19, "context": "Previous work [19], [20], [21], [22], [23], [24], [25] in the literature has already proposed various structured dictionaries to cope with the high computational complexity of learned transforms.", "startOffset": 20, "endOffset": 24}, {"referenceID": 20, "context": "Previous work [19], [20], [21], [22], [23], [24], [25] in the literature has already proposed various structured dictionaries to cope with the high computational complexity of learned transforms.", "startOffset": 26, "endOffset": 30}, {"referenceID": 21, "context": "Previous work [19], [20], [21], [22], [23], [24], [25] in the literature has already proposed various structured dictionaries to cope with the high computational complexity of learned transforms.", "startOffset": 32, "endOffset": 36}, {"referenceID": 22, "context": "Previous work [19], [20], [21], [22], [23], [24], [25] in the literature has already proposed various structured dictionaries to cope with the high computational complexity of learned transforms.", "startOffset": 38, "endOffset": 42}, {"referenceID": 23, "context": "Previous work [19], [20], [21], [22], [23], [24], [25] in the literature has already proposed various structured dictionaries to cope with the high computational complexity of learned transforms.", "startOffset": 44, "endOffset": 48}, {"referenceID": 24, "context": "Specifically, [26] proposed to build an orthogonal dictionary composed of a product of a few Householder reflectors.", "startOffset": 14, "endOffset": 18}, {"referenceID": 24, "context": "We show numerically that the fast dictionaries proposed in this paper outperform those based on Householder reflectors [26] in terms of representation error, for the same computational complexity.", "startOffset": 119, "endOffset": 123}, {"referenceID": 25, "context": "Interestingly, we are able to provide numerical examples where the proposed fast orthogonal dictionaries have higher computational efficiency and provide better representation performance than the well-known discrete cosine transform (DCT), the transform at the heart of the jpeg compression standard [27].", "startOffset": 301, "endOffset": 305}, {"referenceID": 6, "context": "Several algorithms that work very well in practice exist [7] [8] [15] to solve this factorization problem.", "startOffset": 57, "endOffset": 60}, {"referenceID": 7, "context": "Several algorithms that work very well in practice exist [7] [8] [15] to solve this factorization problem.", "startOffset": 61, "endOffset": 64}, {"referenceID": 14, "context": "Several algorithms that work very well in practice exist [7] [8] [15] to solve this factorization problem.", "startOffset": 65, "endOffset": 69}, {"referenceID": 6, "context": "This alternating minimization approach proves to work very well experimentally [7], [8] and allows some theoretical insights [28].", "startOffset": 79, "endOffset": 82}, {"referenceID": 7, "context": "This alternating minimization approach proves to work very well experimentally [7], [8] and allows some theoretical insights [28].", "startOffset": 84, "endOffset": 87}, {"referenceID": 26, "context": "This alternating minimization approach proves to work very well experimentally [7], [8] and allows some theoretical insights [28].", "startOffset": 125, "endOffset": 129}, {"referenceID": 27, "context": "In this paper we also consider the dictionary learning problem (1) with an orthogonal dictionary Q \u2208 R [29] [30] [31] [32].", "startOffset": 103, "endOffset": 107}, {"referenceID": 28, "context": "In this paper we also consider the dictionary learning problem (1) with an orthogonal dictionary Q \u2208 R [29] [30] [31] [32].", "startOffset": 108, "endOffset": 112}, {"referenceID": 29, "context": "In this paper we also consider the dictionary learning problem (1) with an orthogonal dictionary Q \u2208 R [29] [30] [31] [32].", "startOffset": 113, "endOffset": 117}, {"referenceID": 30, "context": "In this paper we also consider the dictionary learning problem (1) with an orthogonal dictionary Q \u2208 R [29] [30] [31] [32].", "startOffset": 118, "endOffset": 122}, {"referenceID": 31, "context": "The orthogonal dictionary learning problem (which we call in this paper Q\u2013DLA) [33] is formulated as:", "startOffset": 79, "endOffset": 83}, {"referenceID": 11, "context": "Since the dictionary Q is orthogonal, the construction of X no longer involves l1 [12], OMP [13] or AMP [14] approaches as in (1), but reduces to X = Ts(Q Y), where Ts() is an operator that given an input vector zeros all entries except the largest s in magnitude and given an input matrix applies the same operation on each column in turn.", "startOffset": 82, "endOffset": 86}, {"referenceID": 12, "context": "Since the dictionary Q is orthogonal, the construction of X no longer involves l1 [12], OMP [13] or AMP [14] approaches as in (1), but reduces to X = Ts(Q Y), where Ts() is an operator that given an input vector zeros all entries except the largest s in magnitude and given an input matrix applies the same operation on each column in turn.", "startOffset": 92, "endOffset": 96}, {"referenceID": 13, "context": "Since the dictionary Q is orthogonal, the construction of X no longer involves l1 [12], OMP [13] or AMP [14] approaches as in (1), but reduces to X = Ts(Q Y), where Ts() is an operator that given an input vector zeros all entries except the largest s in magnitude and given an input matrix applies the same operation on each column in turn.", "startOffset": 104, "endOffset": 108}, {"referenceID": 32, "context": "To solve (2) for variable Q and fixed X, a problem also known as the orthogonal Procrustes problem [34], a closed form solution Q = UV is given by the singular value decomposition of YX = U\u03a3V .", "startOffset": 99, "endOffset": 103}, {"referenceID": 33, "context": "Givens rotations [35] which are all in fact constrained", "startOffset": 17, "endOffset": 21}, {"referenceID": 33, "context": "Givens rotations [35].", "startOffset": 17, "endOffset": 21}, {"referenceID": 34, "context": "Givens rotations have been previously used in matrix factorization applications [37], [38].", "startOffset": 80, "endOffset": 84}, {"referenceID": 35, "context": "Givens rotations have been previously used in matrix factorization applications [37], [38].", "startOffset": 86, "endOffset": 90}, {"referenceID": 32, "context": "This is a two dimensional Procrustes problem [34] whose optimum solution is G\u0303ij = UV T where Z{i,j} = U\u03a3V T .", "startOffset": 45, "endOffset": 49}, {"referenceID": 24, "context": "It has been shown in [26] that the reduction in the objective function of (10) when considering an orthogonal dictionary Gij given by the Procrustes solution is", "startOffset": 21, "endOffset": 25}, {"referenceID": 32, "context": "and then solve a Procrustes problem [34] to construct G\u0303i\u22c6j\u22c6 .", "startOffset": 36, "endOffset": 40}, {"referenceID": 24, "context": "There are some connections between the Householder [26] and the G-transform approaches.", "startOffset": 51, "endOffset": 55}, {"referenceID": 24, "context": "Following results from [26] we can also write", "startOffset": 23, "endOffset": 27}, {"referenceID": 36, "context": "It has been shown experimentally in the past [39], that a good initial orthogonal dictionary is to choose U from the singular value decomposition of the dataset Y = U\u03a3V .", "startOffset": 45, "endOffset": 49}, {"referenceID": 24, "context": "Compared to the Householder approach [26] we again expect Gm\u2013DLA to performs better since the optimization is made over two coordinates at a time.", "startOffset": 37, "endOffset": 41}, {"referenceID": 24, "context": "It has been shown in [26] that with this Q we have that T = YXQT = U\u03a3U is symmetric positive semidefinite.", "startOffset": 21, "endOffset": 25}, {"referenceID": 7, "context": "We can construct X in the same way as for Gm\u2013DLA from the singular value decomposition of the dataset or by running another dictionary learning algorithm (like the K\u2013SVD [8] for example) and use the X it constructs.", "startOffset": 170, "endOffset": 173}, {"referenceID": 14, "context": "The computational complexity of using a general nonorthogonal dictionary A of size n \u00d7 n in sparse recovery problems with Batch\u2013OMP [15] is", "startOffset": 132, "endOffset": 136}, {"referenceID": 14, "context": "Finally, with a dictionary D as (27) computed via Rm2\u2013 DLA the sparse approximation step via Batch\u2013OMP [15] takes", "startOffset": 103, "endOffset": 107}, {"referenceID": 38, "context": "We choose to compare the proposed methods on image data since in this setting fast transforms that perform very well, like the Discrete Cosine Transform (DCT) [41] for example, are available.", "startOffset": 159, "endOffset": 163}, {"referenceID": 31, "context": "Comparisons, in terms of relative representation errors (40), of Gm\u2013 DLA against the DCT, Q\u2013DLA [33], SK\u2013SVD [40] and Householder based orthogonal dictionaries [26] denoted here Hp\u2013DLA where p is the number of reflectors in the factorization of the dictionary.", "startOffset": 96, "endOffset": 100}, {"referenceID": 37, "context": "Comparisons, in terms of relative representation errors (40), of Gm\u2013 DLA against the DCT, Q\u2013DLA [33], SK\u2013SVD [40] and Householder based orthogonal dictionaries [26] denoted here Hp\u2013DLA where p is the number of reflectors in the factorization of the dictionary.", "startOffset": 109, "endOffset": 113}, {"referenceID": 24, "context": "Comparisons, in terms of relative representation errors (40), of Gm\u2013 DLA against the DCT, Q\u2013DLA [33], SK\u2013SVD [40] and Householder based orthogonal dictionaries [26] denoted here Hp\u2013DLA where p is the number of reflectors in the factorization of the dictionary.", "startOffset": 160, "endOffset": 164}, {"referenceID": 24, "context": "An interesting point of comparison is between the dictionaries constructed via Gm\u2013DLA and Hp\u2013DLA [26].", "startOffset": 97, "endOffset": 101}, {"referenceID": 38, "context": "The computational complexity of H3\u2013DLA approximately matches that of the DCT [41] (based on the FFT) while G85\u2013DLA is actually computationally simpler than the DCT.", "startOffset": 77, "endOffset": 81}, {"referenceID": 37, "context": "Figure 6 shows the performance of Rm\u2013DLA in a regime close to the results of the SK\u2013SVD dictionary learning method [40].", "startOffset": 115, "endOffset": 119}, {"referenceID": 39, "context": "Pareto curves for Rm\u2013DLA and the Sparse K\u2013SVD approach [42].", "startOffset": 55, "endOffset": 59}, {"referenceID": 39, "context": "In the last experimental setup we compare our Rm\u2013DLA with the previously proposed Sparse K\u2013SVD approach [42].", "startOffset": 104, "endOffset": 108}, {"referenceID": 37, "context": "Experimental insights into how the representation performance scales with the number of atoms in the dictionary are given in [40], [43].", "startOffset": 125, "endOffset": 129}, {"referenceID": 40, "context": "Experimental insights into how the representation performance scales with the number of atoms in the dictionary are given in [40], [43].", "startOffset": 131, "endOffset": 135}], "year": 2017, "abstractText": "Given a dataset, the task of learning a transform that allows sparse representations of the data bears the name of dictionary learning. In many applications, these learned dictionaries represent the data much better than the static well-known transforms (Fourier, Hadamard etc.). The main downside of learned transforms is that they lack structure and therefore they are not computationally efficient, unlike their classical counterparts. These posse several difficulties especially when using power limited hardware such as mobile devices, therefore discouraging the application of sparsity techniques in such scenarios. In this paper we construct orthogonal and nonorthogonal dictionaries that are factorized as a product of a few basic transformations. In the orthogonal case, we solve exactly the dictionary update problem for one basic transformation, which can be viewed as a generalized Givens rotation, and then propose to construct orthogonal dictionaries that are a product of these transformations, guaranteeing their fast manipulation. We also propose a method to construct fast square but nonorthogonal dictionaries that are factorized as a product of few transforms that can be viewed as a further generalization of Givens rotations to the non-orthogonal setting. We show how the proposed transforms can balance very well data representation performance and computational complexity. We also compare with classical fast and learned general and orthogonal transforms.", "creator": "LaTeX with hyperref package"}}}