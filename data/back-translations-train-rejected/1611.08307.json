{"id": "1611.08307", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Nov-2016", "title": "Learning Python Code Suggestion with a Sparse Pointer Network", "abstract": "To enhance developer productivity, all modern integrated development environments (IDEs) include code suggestion functionality that proposes likely next tokens at the cursor. While current IDEs work well for statically-typed languages, their reliance on type annotations means that they do not provide the same level of support for dynamic programming languages as for statically-typed languages. Moreover, suggestion engines in modern IDEs do not propose expressions or multi-statement idiomatic code. Recent work has shown that language models can improve code suggestion systems by learning from software repositories. This paper introduces a neural language model with a sparse pointer network aimed at capturing very long-range dependencies. We release a large-scale code suggestion corpus of 41M lines of Python code crawled from GitHub. On this corpus, we found standard neural language models to perform well at suggesting local phenomena, but struggle to refer to identifiers that are introduced many tokens in the past. By augmenting a neural language model with a pointer network specialized in referring to predefined classes of identifiers, we obtain a much lower perplexity and a 5 percentage points increase in accuracy for code suggestion compared to an LSTM baseline. In fact, this increase in code suggestion accuracy is due to a 13 times more accurate prediction of identifiers. Furthermore, a qualitative analysis shows this model indeed captures interesting long-range dependencies, like referring to a class member defined over 60 tokens in the past.", "histories": [["v1", "Thu, 24 Nov 2016 21:01:46 GMT  (741kb,D)", "http://arxiv.org/abs/1611.08307v1", "Under review as a conference paper at ICLR 2017"]], "COMMENTS": "Under review as a conference paper at ICLR 2017", "reviews": [], "SUBJECTS": "cs.NE cs.AI cs.CL cs.SE", "authors": ["avishkar bhoopchand", "tim rockt\\\"aschel", "earl barr", "sebastian riedel"], "accepted": false, "id": "1611.08307"}, "pdf": {"name": "1611.08307.pdf", "metadata": {"source": "CRF", "title": "LEARNING PYTHON CODE SUGGESTION WITH A SPARSE POINTER NETWORK", "authors": ["Avishkar Bhoopchand", "Tim Rockt\u00e4schel", "Earl Barr", "Sebastian Riedel"], "emails": ["avishkar.bhoopchand.15@ucl.ac.uk,", "t.rocktaschel@cs.ucl.ac.uk", "e.barr@cs.ucl.ac.uk", "s.riedel@cs.ucl.ac.uk"], "sections": [{"heading": "1 INTRODUCTION", "text": "In fact, the fact is that most of them will be able to be in the position in which they are able to hide."}, {"heading": "2 METHODS", "text": "First, we discuss neural language models before briefly describing how to add an attention mechanism to such a language model, and then we introduce a sparse attention mechanism for a pointer network that can exploit the abstract Python syntax tree of the current context for code suggestions."}, {"heading": "2.1 NEURAL LANGUAGE MODEL", "text": "Code suggestion can be addressed by a language model that measures the probability of observing a sequence of symbols in a Python program. For example, for the sequence S = a1,.., aN, the common probability of S is factored according to P\u03b8 (S) = P\u03b8 (a1) \u00b7 N = 2 P\u03b8 (at \u2212 1,.., a1) (1), where the parameters from a training corpus are estimated. However, in view of a sequence of Python symbols, we try to predict the next M symbols at + 1,.., at + M, which maximize the equation 1argmax at + 1,..., at + M P\u03b8 (a1,...), at + 1,.., at + M), the next M symbols at + M. In this work, we build on neural language models that use recurrent neural networks (RNNNNs) and long short-m memory (LHF, this fixed function 1997-STH)."}, {"heading": "2.2 ATTENTION", "text": "The idea behind it is to overcome the hidden state bottlenecks by reverting to the previous output vectors (2015b), as well as dual sequence modeling such as textual origin detection (Rockta \u00bc schel et al., 2016). The idea behind this is to overcome the hidden state bottlenecks by pointing to the previous output vectors. (2016) The idea behind it is that the distribution mechanisms are applied to the voice modeling of Cheng et al. (2016).Formell, an attention mechanism with a fixed memory mt of K-vectors-vectors-vectors-vectors-vectors-vectors-vectors-vectors-vectors-vectors-vectors-vectors-vectors-vectors-vectors-vectors-vectors-vectors-vectors-vectors-vectors-vectors-vectors-vectors-vectors-vectors-vectors-vectors-vectors-vectors-vectors-vectors-vectors-vectors-vectors-vectors-vectors-vectors-vectors-vectors-vectors-vectors-vectors-vectors-vectors-vectors-vectors-vectors-vectors-vectors-vectors-vectors-vectors-vectors-vectors-vectors-vectors-vectors-vectors-vectors-vectors-vectors-vectors-vectors-vectors-vectors-vectors-vectors-vectors-vectors-vectors-vectors-vectors-vectors-vectors-vectors-vectors-vectors-vectors-vectors-vectors-vectors-vectors-vectors-vectors-vectors-vectors-vectors-vectors-vectors-vectors-vectors-vectors-vectors-vec"}, {"heading": "2.3 SPARSE POINTER NETWORK", "text": "We are developing an attention mechanism that provides a filtered view of a large history of Python characters = global vocabulary. At any given time, memory consists of contextual representations of the previous K-signs introduced into history. This allows us to model far-reaching dependencies found in the identification reference and in a standard neural language model. The weighting of the two words is determined by a controller before it is used. Formally, the sparse pointer network operates on the basis of a weighted average of the sparse pointer network for identification references and a standard neural language model. The weighting of the two is determined by a controller argument."}, {"heading": "3 LARGE-SCALE PYTHON CORPUS", "text": "Previous work on code proposals has focused either on statically typed languages (especially Java) or trained on very small corpora. Therefore, we decided to collect a new, large-scale corpus of the dynamic programming language Python. According to the popularity website for programming languages Pypl (Carbonnelle, 2016), Python is the second most common language after Java. In terms of the number of repositories on the open source code repository GitHub, after JavaScript and Java (Zapponi, 2016), we have collected a corpus of 41 million lines of Python code from GitHub projects. Ideally, we would like this corpus to contain only high-quality Python code, as our language model learns to propose code based on which users write code. However, it is difficult to automatically assess what constitutes high-quality code. So we resort to the heuristic method that we can use codetic projects that are popular with codec projects, as there are two similar metrics to good ones)."}, {"heading": "3.1 NORMALIZATION OF IDENTIFIERS", "text": "Unsurprisingly, the long tail of words in the vocabulary consists of rare identifiers. To improve generalization, we normalize identifiers before adding the resulting token stream to our models. That is, we replace each identifier name with an anonymous identifier that specifies the identifier group (class, variable, argument, attribute, or function), coupled with a random number that makes the identifier unique in scope. Note that we only replace new identifiers defined within a file. Identification references to external APIs and libraries remain untouched. In line with the earlier creation of corpus for code suggestions (e.g. Khanh Dam et al., 2016; White et al., 2015), we replace numerical constant tokens with $NUM $, remove comments, format the code, and replace tokens that appear less than five times with $OV $(from the token)."}, {"heading": "4 EXPERIMENTS", "text": "Although previous work by White et al. (2015) found that a simple neural language model exceeds an n-gram model for code suggestions, we include a number of n-gram baselines to confirm this observation. Specifically, we use n-gram models for n-gram models for n-gram (3, 4, 5, 6) with modified Kneser-Ney smoothing (Kneser & Ney, 1995) from the Kyoto Language Modelling Toolkit (Neubig, 2012). We train the sparse pointer network with mini-batch SGD with a batch size of 30 and shortened backpropagation through time (Werbos, 1990) with a history of 20 identification representations. We use an initial learning rate of 0.7 and decay them by 0.9 after each epoch. As additional baselines, we test a neural language model with LSTM units with and without attention."}, {"heading": "5 RESULTS", "text": "The results are summarized in Table 2. We can confirm that neural models for code suggestion perform significantly better than language models. In addition, adding attention significantly improves the results (2.3 lower perplexity and 3.4 percentage points higher accuracy). Interestingly, this increase can be attributed to superior prediction of identifiers, which has increased from an accuracy of 2.1% to 21.4%. An LSTM with an attention window of 50 gives us the best accuracy for the top prediction. Further improvements in perplexity and accuracy of the top five predictions are achieved by using a sparse pointer network that uses a smaller memory of the last 20 identification representations."}, {"heading": "5.1 QUALITATIVE ANALYSIS", "text": "Figure 3a-d shows an example of a code proposal using an identifier. While the LSTM baseline is uncertain about the next token, we get a reasonable prediction by using attention or the sparse pointer network. The sparse pointer network offers more reasonable alternative suggestions beyond the correct top suggestion. Figure 3e-h shows the use case referring to a class attribute that was declared 67 tokens in the past. Only the sparse pointer network is a good suggestion. Furthermore, the attention weights in Figure 4 in the appendix show that this model distinguished attributes from other groups of identifiers."}, {"heading": "6 RELATED WORK", "text": "Much of this work is inspired by Hindle et al. (2012), who argued that real programs (a) drop snippets of code to reference variables. (b) LSTM Model. (c) LSTM w / Attention 50. (d) Sparse gram et al. (d) Sparse Pointer Network.into a much smaller space than the flexibility of programming languages enables them to capture the repeatability and predictable statistical properties of real programs using language models, whereas Tu et al. (2014) improves on Hindle et al.'s work by adding a cache mechanism that allows them to exploit and decouple the locality of the programming language.' s idea of adding a cache mechanism to the language model that was specifically designed for us to exploit the properties of the source code."}, {"heading": "7 CONCLUSIONS AND FUTURE WORK", "text": "In this thesis, we examined neural language models for code suggestions of the dynamically typed Python programming language. We published a corpus of 41M lines of Python searched from GitHub, and compared n-gram, standard models of neural language and attention. By using attention, we observed a more accurate prediction of identifiers of the order of magnitude. In addition, we proposed a sparse pointer network that can efficiently capture extensive dependencies by working only on a filtered view of a memory of previous identification representations. This model achieves the least confusion and the best accuracy among the top five predictions. We are interested in making the python corpus and the code for replication of our experiment work at http: / / github.com / uclmr / pycodecting. The methods presented were tested only on code suggestions within the same Python file. We are interested in building a codecode proposal based on the same level of the python file and on the codecode projects based on it, as well as in building a collection model within the file."}, {"heading": "ACKNOWLEDGMENTS", "text": "This work was supported by Microsoft Research through its doctoral fellowship program, an Allen Distinguished Investigator Award, and a Marie Curie Career Integration Award."}], "references": [{"title": "Tensorflow: A system for large-scale machine learning", "author": ["Mart\u0131\u0301n Abadi", "Paul Barham", "Jianmin Chen", "Zhifeng Chen", "Andy Davis", "Jeffrey Dean", "Matthieu Devin", "Sanjay Ghemawat", "Geoffrey Irving", "Michael Isard", "Manjunath Kudlur", "Josh Levenberg", "Rajat Monga", "Sherry Moore", "Derek Gordon Murray", "Benoit Steiner", "Paul A. Tucker", "Vijay Vasudevan", "Pete Warden", "Martin Wicke", "Yuan Yu", "Xiaoqiang Zhang"], "venue": "CoRR, abs/1605.08695,", "citeRegEx": "Abadi et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Abadi et al\\.", "year": 2016}, {"title": "Mining idioms from source code", "author": ["Miltiadis Allamanis", "Charles Sutton"], "venue": "In Proceedings of the 22Nd ACM SIGSOFT International Symposium on Foundations of Software Engineering,", "citeRegEx": "Allamanis and Sutton.,? \\Q2014\\E", "shortCiteRegEx": "Allamanis and Sutton.", "year": 2014}, {"title": "Mining source code repositories at massive scale using language modeling", "author": ["Miltiadis Allamanis", "Charles A. Sutton"], "venue": "MSR, pp. 207\u2013216. IEEE Computer Society,", "citeRegEx": "Allamanis and Sutton.,? \\Q2013\\E", "shortCiteRegEx": "Allamanis and Sutton.", "year": 2013}, {"title": "Learning natural coding conventions", "author": ["Miltiadis Allamanis", "Earl T. Barr", "Christian Bird", "Charles Sutton"], "venue": "In Proceedings of the 22Nd ACM SIGSOFT International Symposium on Foundations of Software Engineering,", "citeRegEx": "Allamanis et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Allamanis et al\\.", "year": 2014}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio"], "venue": "CoRR, abs/1409.0473,", "citeRegEx": "Bahdanau et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Pypl popularity of programming language. http://pypl.github.io/ PYPL.html, 2016", "author": ["Pierre Carbonnelle"], "venue": "URL http://pypl.github.io/PYPL.html. [Online; accessed 30August-2016]", "citeRegEx": "Carbonnelle.,? \\Q2016\\E", "shortCiteRegEx": "Carbonnelle.", "year": 2016}, {"title": "Long short-term memory-networks for machine reading", "author": ["Jianpeng Cheng", "Li Dong", "Mirella Lapata"], "venue": "In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Cheng et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Cheng et al\\.", "year": 2016}, {"title": "Contextual code completion using machine learning", "author": ["Subhasis Das", "Chinmayee Shah"], "venue": null, "citeRegEx": "Das and Shah.,? \\Q2015\\E", "shortCiteRegEx": "Das and Shah.", "year": 2015}, {"title": "Generating sequences with recurrent neural networks", "author": ["Alex Graves"], "venue": "CoRR, abs/1308.0850,", "citeRegEx": "Graves.,? \\Q2013\\E", "shortCiteRegEx": "Graves.", "year": 2013}, {"title": "Teaching machines to read and comprehend", "author": ["Karl Moritz Hermann", "Tom\u00e1s Kocisk\u00fd", "Edward Grefenstette", "Lasse Espeholt", "Will Kay", "Mustafa Suleyman", "Phil Blunsom"], "venue": "In Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems", "citeRegEx": "Hermann et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hermann et al\\.", "year": 2015}, {"title": "On the naturalness of software", "author": ["Abram Hindle", "Earl T. Barr", "Zhendong Su", "Mark Gabel", "Premkumar Devanbu"], "venue": "In Proceedings of the 34th International Conference on Software Engineering,", "citeRegEx": "Hindle et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hindle et al\\.", "year": 2012}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural Comput.,", "citeRegEx": "Hochreiter and Schmidhuber.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "On using very large target vocabulary for neural machine translation", "author": ["S\u00e9bastien Jean", "Kyunghyun Cho", "Roland Memisevic", "Yoshua Bengio"], "venue": "In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),", "citeRegEx": "Jean et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Jean et al\\.", "year": 2015}, {"title": "An empirical exploration of recurrent network architectures", "author": ["Rafal Jozefowicz", "Wojciech Zaremba", "Ilya Sutskever"], "venue": "Proceedings of the 32nd International Conference on Machine Learning", "citeRegEx": "Jozefowicz et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Jozefowicz et al\\.", "year": 2015}, {"title": "A deep language model for software code", "author": ["H. Khanh Dam", "T. Tran", "T. Pham"], "venue": "ArXiv e-prints,", "citeRegEx": "Dam et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Dam et al\\.", "year": 2016}, {"title": "Improved backing-off for m-gram language modeling", "author": ["R. Kneser", "H. Ney"], "venue": "In Acoustics, Speech, and Signal Processing,", "citeRegEx": "Kneser and Ney.,? \\Q1995\\E", "shortCiteRegEx": "Kneser and Ney.", "year": 1995}, {"title": "Latent predictor networks for code generation", "author": ["Wang Ling", "Edward Grefenstette", "Karl Moritz Hermann", "Tomas Kocisky", "Andrew Senior", "Fumin Wang", "Phil Blunsom"], "venue": "arXiv preprint arXiv:1603.06744,", "citeRegEx": "Ling et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Ling et al\\.", "year": 2016}, {"title": "Structured generative models of natural source code", "author": ["Chris J Maddison", "Daniel Tarlow"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Maddison and Tarlow.,? \\Q2014\\E", "shortCiteRegEx": "Maddison and Tarlow.", "year": 2014}, {"title": "Building a large annotated corpus of english: The penn treebank", "author": ["Mitchell P. Marcus", "Beatrice Santorini", "Mary Ann Marcinkiewicz"], "venue": "COMPUTATIONAL LINGUISTICS,", "citeRegEx": "Marcus et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Marcus et al\\.", "year": 1993}, {"title": "Kylm - the kyoto language modeling toolkit", "author": ["Graham Neubig"], "venue": "http://www.phontron.com/ kylm/,", "citeRegEx": "Neubig.,? \\Q2012\\E", "shortCiteRegEx": "Neubig.", "year": 2012}, {"title": "On the difficulty of training recurrent neural networks", "author": ["Razvan Pascanu", "Tomas Mikolov", "Yoshua Bengio"], "venue": "In Proceedings of the 30th International Conference on Machine Learning,", "citeRegEx": "Pascanu et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Pascanu et al\\.", "year": 2013}, {"title": "Reasoning about entailment with neural attention", "author": ["Tim Rockt\u00e4schel", "Edward Grefenstette", "Karl Moritz Hermann", "Tomas Kocisky", "Phil Blunsom"], "venue": "In ICLR,", "citeRegEx": "Rockt\u00e4schel et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Rockt\u00e4schel et al\\.", "year": 2016}, {"title": "Recurrent memory networks for language modeling", "author": ["Ke M. Tran", "Arianna Bisazza", "Christof Monz"], "venue": "In NAACL HLT", "citeRegEx": "Tran et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Tran et al\\.", "year": 2016}, {"title": "On the localness of software", "author": ["Zhaopeng Tu", "Zhendong Su", "Premkumar Devanbu"], "venue": "In Proceedings of the 22Nd ACM SIGSOFT International Symposium on Foundations of Software Engineering,", "citeRegEx": "Tu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Tu et al\\.", "year": 2014}, {"title": "Grammar as a foreign language", "author": ["Oriol Vinyals", "Lukasz Kaiser", "Terry Koo", "Slav Petrov", "Ilya Sutskever", "Geoffrey E. Hinton"], "venue": "In Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems", "citeRegEx": "Vinyals et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Vinyals et al\\.", "year": 2015}, {"title": "Backpropagation through time: what it does and how to do it", "author": ["Paul J Werbos"], "venue": "Proceedings of the IEEE,", "citeRegEx": "Werbos.,? \\Q1990\\E", "shortCiteRegEx": "Werbos.", "year": 1990}, {"title": "Toward deep learning software repositories", "author": ["Martin White", "Christopher Vendome", "Mario Linares-V\u00e1squez", "Denys Poshyvanyk"], "venue": "In Proceedings of the 12th Working Conference on Mining Software Repositories,", "citeRegEx": "White et al\\.,? \\Q2015\\E", "shortCiteRegEx": "White et al\\.", "year": 2015}, {"title": "Githut - programming languages and github. http://githut.info/, 2016", "author": ["Carlo Zapponi"], "venue": "URL http://githut.info/. [Online; accessed 19-August-2016]", "citeRegEx": "Zapponi.,? \\Q2016\\E", "shortCiteRegEx": "Zapponi.", "year": 2016}], "referenceMentions": [{"referenceID": 10, "context": "Recently, methods from statistical natural language processing (NLP) have been used to train code suggestion systems from code usage in large code repositories (Hindle et al., 2012; Allamanis & Sutton, 2013; Tu et al., 2014).", "startOffset": 160, "endOffset": 224}, {"referenceID": 23, "context": "Recently, methods from statistical natural language processing (NLP) have been used to train code suggestion systems from code usage in large code repositories (Hindle et al., 2012; Allamanis & Sutton, 2013; Tu et al., 2014).", "startOffset": 160, "endOffset": 224}, {"referenceID": 26, "context": "Neural language models for code suggestion (White et al., 2015; Das & Shah, 2015) have extended this line of work to capture more long-range dependencies.", "startOffset": 43, "endOffset": 81}, {"referenceID": 4, "context": "We investigate, for the first time, the use of attention (Bahdanau et al., 2014) for code suggestion and find that, despite a substantial improvement", "startOffset": 57, "endOffset": 80}, {"referenceID": 4, "context": "A straight-forward approach to capturing long-range dependencies is to use a neural attention mechanism (Bahdanau et al., 2014) on the previous K output vectors of the language model.", "startOffset": 104, "endOffset": 127}, {"referenceID": 4, "context": "Attention mechanisms have been successfully applied to sequence-to-sequence tasks such as machine translation (Bahdanau et al., 2014), question-answering (Hermann et al.", "startOffset": 110, "endOffset": 133}, {"referenceID": 9, "context": ", 2014), question-answering (Hermann et al., 2015), syntactic parsing (Vinyals et al.", "startOffset": 28, "endOffset": 50}, {"referenceID": 21, "context": ", 2015b), as well as dual-sequence modeling like recognizing textual entailment (Rockt\u00e4schel et al., 2016).", "startOffset": 80, "endOffset": 106}, {"referenceID": 4, "context": "A straight-forward approach to capturing long-range dependencies is to use a neural attention mechanism (Bahdanau et al., 2014) on the previous K output vectors of the language model. Attention mechanisms have been successfully applied to sequence-to-sequence tasks such as machine translation (Bahdanau et al., 2014), question-answering (Hermann et al., 2015), syntactic parsing (Vinyals et al., 2015b), as well as dual-sequence modeling like recognizing textual entailment (Rockt\u00e4schel et al., 2016). The idea is to overcome the hidden-state bottleneck by allowing referral back to previous output vectors. Recently, these mechanisms were applied to language modelling by Cheng et al. (2016) and Tran et al.", "startOffset": 105, "endOffset": 694}, {"referenceID": 4, "context": "A straight-forward approach to capturing long-range dependencies is to use a neural attention mechanism (Bahdanau et al., 2014) on the previous K output vectors of the language model. Attention mechanisms have been successfully applied to sequence-to-sequence tasks such as machine translation (Bahdanau et al., 2014), question-answering (Hermann et al., 2015), syntactic parsing (Vinyals et al., 2015b), as well as dual-sequence modeling like recognizing textual entailment (Rockt\u00e4schel et al., 2016). The idea is to overcome the hidden-state bottleneck by allowing referral back to previous output vectors. Recently, these mechanisms were applied to language modelling by Cheng et al. (2016) and Tran et al. (2016).", "startOffset": 105, "endOffset": 717}, {"referenceID": 5, "context": "According to the programming language popularity website Pypl (Carbonnelle, 2016), Python is the second most popular language after Java.", "startOffset": 62, "endOffset": 81}, {"referenceID": 27, "context": "It is also the 3rd most common language in terms of number of repositories on the open-source code repository GitHub, after JavaScript and Java (Zapponi, 2016).", "startOffset": 144, "endOffset": 159}, {"referenceID": 3, "context": "Similar to Allamanis & Sutton (2013) and Allamanis et al. (2014), we select Python projects with more than 100 stars, sort by the number of forks descending, and take the top 1000 projects.", "startOffset": 41, "endOffset": 65}, {"referenceID": 26, "context": "Consistent with previous corpus creation for code suggestion (e.g. Khanh Dam et al., 2016; White et al., 2015), we replace numerical constant tokens with $NUM$, remove comments, reformat the code, and replace tokens appearing less than five times with an $OOV$ (out of vocabulary) token.", "startOffset": 61, "endOffset": 110}, {"referenceID": 19, "context": "Specifically, we use n-gram models for n \u2208 {3, 4, 5, 6} with Modified Kneser-Ney smoothing (Kneser & Ney, 1995) from the Kyoto Language Modelling Toolkit (Neubig, 2012).", "startOffset": 154, "endOffset": 168}, {"referenceID": 25, "context": "We train the sparse pointer network using mini-batch SGD with a batch size of 30 and truncated backpropagation through time (Werbos, 1990) with a history of 20 identifier representations.", "startOffset": 124, "endOffset": 138}, {"referenceID": 24, "context": "Although previous work by White et al. (2015) already established that a simple neural language model outperforms an n-gram model for code suggestion, we include a number of n-gram baselines to confirm this observation.", "startOffset": 26, "endOffset": 46}, {"referenceID": 0, "context": "All neural language models were developed in TensorFlow (Abadi et al., 2016) and trained using cross-entropy loss.", "startOffset": 56, "endOffset": 76}, {"referenceID": 13, "context": "All models use an input and hidden size of 200, an LSTM forget gate bias of 1 (Jozefowicz et al., 2015), gradient norm clipping of 5 (Pascanu et al.", "startOffset": 78, "endOffset": 103}, {"referenceID": 20, "context": ", 2015), gradient norm clipping of 5 (Pascanu et al., 2013), and randomly initialized parameters in the interval (\u22120.", "startOffset": 37, "endOffset": 59}, {"referenceID": 12, "context": "Furthermore, we use a sampled softmax (Jean et al., 2015) with a log-uniform sampling distribution and a sample size of 1000.", "startOffset": 38, "endOffset": 57}, {"referenceID": 10, "context": "Much of this work is inspired by Hindle et al. (2012) who argued that real programs fall", "startOffset": 33, "endOffset": 54}, {"referenceID": 18, "context": "They achieved promising results on the standard Penn Treebank benchmark corpus (Marcus et al., 1993).", "startOffset": 79, "endOffset": 100}, {"referenceID": 18, "context": "Subsequently, Tu et al. (2014) improved upon Hindle et al.", "startOffset": 14, "endOffset": 31}, {"referenceID": 9, "context": "(2014) improved upon Hindle et al.\u2019s work by adding a cache mechanism that allowed them to exploit locality stemming from the specialisation and decoupling of program modules. Tu et al.\u2019s idea of adding a cache mechanism to the language model is specifically designed to exploit the properties of source code, and thus follows the same aim as the sparse attention mechanism introduced in this paper. While the majority of preceding work trained on small corpora, Allamanis & Sutton (2013) created a corpus of 352M lines of Java code which they analysed with n-gram language models.", "startOffset": 21, "endOffset": 489}, {"referenceID": 9, "context": "(2014) improved upon Hindle et al.\u2019s work by adding a cache mechanism that allowed them to exploit locality stemming from the specialisation and decoupling of program modules. Tu et al.\u2019s idea of adding a cache mechanism to the language model is specifically designed to exploit the properties of source code, and thus follows the same aim as the sparse attention mechanism introduced in this paper. While the majority of preceding work trained on small corpora, Allamanis & Sutton (2013) created a corpus of 352M lines of Java code which they analysed with n-gram language models. The size of the corpus allowed them to train a single language model that was effective across multiple different project domains. White et al. (2015) later demonstrated that neural language models outperform n-gram models for code suggestion.", "startOffset": 21, "endOffset": 733}, {"referenceID": 9, "context": "(2014) improved upon Hindle et al.\u2019s work by adding a cache mechanism that allowed them to exploit locality stemming from the specialisation and decoupling of program modules. Tu et al.\u2019s idea of adding a cache mechanism to the language model is specifically designed to exploit the properties of source code, and thus follows the same aim as the sparse attention mechanism introduced in this paper. While the majority of preceding work trained on small corpora, Allamanis & Sutton (2013) created a corpus of 352M lines of Java code which they analysed with n-gram language models. The size of the corpus allowed them to train a single language model that was effective across multiple different project domains. White et al. (2015) later demonstrated that neural language models outperform n-gram models for code suggestion. They compared various n-gram models (up to nine grams), including Tu et al.\u2019s cache model, with a basic RNN neural language model. Khanh Dam et al. (2016) compared White et al.", "startOffset": 21, "endOffset": 981}, {"referenceID": 6, "context": "The combination of lagged attention mechanisms with language modelling is inspired by Cheng et al. (2016) who equipped LSTM cells with a fixed-length memory tape rather than a single memory cell.", "startOffset": 86, "endOffset": 106}, {"referenceID": 6, "context": "The combination of lagged attention mechanisms with language modelling is inspired by Cheng et al. (2016) who equipped LSTM cells with a fixed-length memory tape rather than a single memory cell. They achieved promising results on the standard Penn Treebank benchmark corpus (Marcus et al., 1993). Similarly, Tran et al. added a memory block to LSTMs for language modelling of English, German and Italian and outperformed both n-gram and neural language models. Their memory encompasses representations of all possible words in the vocabulary rather than providing a sparse view as we do. An alternative to our purely lexical approach to code suggestion involves the use of probabilistic context-free grammars (PCFGs) which exploit the formal grammar specifications and well-defined, deterministic parsers available for source code. These were used by Allamanis & Sutton (2014) to extract idiomatic patterns from source code.", "startOffset": 86, "endOffset": 878}, {"referenceID": 16, "context": "Ling et al. (2016) recently used a pointer network to generate code from natural language descriptions.", "startOffset": 0, "endOffset": 19}, {"referenceID": 8, "context": ", models that provide a likely continuation of a partial token, using character language models (Graves, 2013).", "startOffset": 96, "endOffset": 110}], "year": 2016, "abstractText": "To enhance developer productivity, all modern integrated development environments (IDEs) include code suggestion functionality that proposes likely next tokens at the cursor. While current IDEs work well for statically-typed languages, their reliance on type annotations means that they do not provide the same level of support for dynamic programming languages as for statically-typed languages. Moreover, suggestion engines in modern IDEs do not propose expressions or multi-statement idiomatic code. Recent work has shown that language models can improve code suggestion systems by learning from software repositories. This paper introduces a neural language model with a sparse pointer network aimed at capturing very longrange dependencies. We release a large-scale code suggestion corpus of 41M lines of Python code crawled from GitHub. On this corpus, we found standard neural language models to perform well at suggesting local phenomena, but struggle to refer to identifiers that are introduced many tokens in the past. By augmenting a neural language model with a pointer network specialized in referring to predefined classes of identifiers, we obtain a much lower perplexity and a 5 percentage points increase in accuracy for code suggestion compared to an LSTM baseline. In fact, this increase in code suggestion accuracy is due to a 13 times more accurate prediction of identifiers. Furthermore, a qualitative analysis shows this model indeed captures interesting long-range dependencies, like referring to a class member defined over 60 tokens in the past.", "creator": "LaTeX with hyperref package"}}}