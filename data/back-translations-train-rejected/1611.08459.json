{"id": "1611.08459", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Nov-2016", "title": "Neural Machine Translation with Latent Semantic of Image and Text", "abstract": "Although attention-based Neural Machine Translation have achieved great success, attention-mechanism cannot capture the entire meaning of the source sentence because the attention mechanism generates a target word depending heavily on the relevant parts of the source sentence. The report of earlier studies has introduced a latent variable to capture the entire meaning of sentence and achieved improvement on attention-based Neural Machine Translation. We follow this approach and we believe that the capturing meaning of sentence benefits from image information because human beings understand the meaning of language not only from textual information but also from perceptual information such as that gained from vision. As described herein, we propose a neural machine translation model that introduces a continuous latent variable containing an underlying semantic extracted from texts and images. Our model, which can be trained end-to-end, requires image information only when training. Experiments conducted with an English--German translation task show that our model outperforms over the baseline.", "histories": [["v1", "Fri, 25 Nov 2016 14:10:39 GMT  (4451kb,D)", "http://arxiv.org/abs/1611.08459v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["joji toyama", "masanori misono", "masahiro suzuki", "kotaro nakayama", "yutaka matsuo"], "accepted": false, "id": "1611.08459"}, "pdf": {"name": "1611.08459.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Joji Toyama", "Masanori Misono", "Masahiro Suzuki", "Kotaro Nakayama", "Yutaka Matsuo"], "emails": ["toyama@weblab.t.u-tokyo.ac.jp", "misono@weblab.t.u-tokyo.ac.jp", "masa@weblab.t.u-tokyo.ac.jp", "k-nakayama@weblab.t.u-tokyo.ac.jp", "matsuo@weblab.t.u-tokyo.ac.jp"], "sections": [{"heading": "1 INTRODUCTION", "text": "Unlike statistical machine translation, which requires enormous phrase and rule tables, NMT requires much less memory. However, the most common model, NMT with attention (Bahdanau et al., 2015), requires the deficiency that the attention mechanism cannot grasp the entire meaning of a sentence because it generates a target word while it depends heavily on the relevant parts of the source set (Tu et al., 2016). To overcome this problem, variational image translation (VNMT), which exceeds NMT with attention, introduces a latent variable to grasp the underlying semantic language of source and target (Zhang et al., 2016), we follow the motivation of VNMT to grasp the underlying semantic language of a source."}, {"heading": "2 BACKGROUND", "text": "Our model is the extension of Variational Neural Machine Translation (VNMT) (Zhang et al., 2016). Our model is also considered as one of the multimodal translation models. In our model, UAE is used to introduce a latent variable."}, {"heading": "2.1 VARIATIONAL NEURAL MACHINE TRANSLATION", "text": "The VNMT translation model introduces a latent variable, and the architecture of this model shown in Figure 1 excludes the arrow \u03c0. This model consists of three parts: encoder, inferrer and decoder. In the encoder, both source and target are encoded from a semantic representation by bidirectional-recurrent neural networks (bidirectional RNN) and a semantic representation is generated. In the inferrer, a latent variable z is modeled from a semantic representation by introducing VAE. In the decoder, a latent variable z is integrated into the decoder of the Gated Recurrent Unit (GRU), and a translation is generated.Our model is followed by the architecture, except that the image is also encoded to obtain a latent variable z."}, {"heading": "2.2 MULTIMODAL TRANSLATION", "text": "The first work to examine multimodal translation is Elliott et al. (2015) and Hitschler & Riezler (2016), which was selected as a joint task in Workshop of Machine Translation 2016 (WMT161). Although several studies have been conducted (Caglayan et al., 2016; Huang et al., 2016; Calixto et al., 2016; Libovicky \u0301 et al., 2016; Rodr\u00ed guez Guasch & Costa-jussa, 2016; Shah et al., 2016), they do not show much improvement, especially in neural machine translation (Specia et al., 2016). Here, we are introducing end-to-end neural network translation models, such as our model.Caglayan et al. (2016): they integrate an image into an NMT decoder, simply employing context vectors and image functions extracted from the Resoural-Resourtional-U method."}, {"heading": "2.3 VARIATIONAL AUTO ENCODER", "text": "UAE was proposed in an earlier literature report Kingma et al. (2014); Rezende et al. (2014). In view of an observed variable x, UAE introduces a continuous latent variable z, assuming that x from e.g. UAE integrates p\u03b8 (x | z) and q\u03c6 (z | x) into an end-to-end neural network.The lower limit is shown below.LVAE = \u2212 DKL [q\u03c6 (z | x) | | p\u03b8 (z)] + Eq\u03c6 (z | x) [log p\u03b8 (x | z)] \u2264 log p\u03b8 (x) (1)"}, {"heading": "3 NEURAL MACHINE TRANSLATION WITH LATENT SEMANTIC OF IMAGE AND TEXT", "text": "We propose a neural machine translation model that explicitly contains a latent variable containing an underlying semantics extracted from text and image. This model can be considered an extension of VNMT by adding image information.Our model can be drawn as a graphical model in Figure 3. its lower limit is L = \u2212 DKL [q\u03c6 (z | x, y, \u03c0) | p\u03b8 (z | x)] + Eq\u03c6 (z | x, y, \u03c0)] [log p\u03b8 (y | z, x)], (2) where x, y, \u03c0, z each indicate the source, target, image and latent variable, and p\u03b8 respectively the previous distribution and approximate posterior distribution. Remarkable in equation is in equation. (2) that we want to model p (z | x, y, z) with the intractable model, image and latent variable attached, and therefore we attach the preceding distribution, y | qore."}, {"heading": "3.1 ENCODER", "text": "In the encoder, the semantic representation it receives from the image, source and destination, we propose several methods for encoding an image. We will show how these methods affect the translation result in the Experiment section. This representation is used in the Inferrer. This section links to the green part of Figure 1."}, {"heading": "3.1.1 TEXT ENCODING", "text": "Source and target are encoded in the same way as Bahdanau et al. (2015). The source is converted into a sequence of 1-of-k vector and embedded in Demb dimensions. We call it the source sequence. Subsequently, a source sequence is converted into bidirectional RNN. Representation hi is obtained by concatenating ~ hi and ~ hi: ~ hi = RNN (~ hi \u2212 1, Ewi), ~ hi = RNN (~ hi + 1, Ewi), hi = [~ hi; ~ hi], where Ewi is the embedded word in a source set, hi-hi-Rdh and ~-hi-R dh 2. It is performed by i = 0 to i = Tf, where Tf is the sequence length. GRU is implemented in bidirectional RNN so that it can achieve long-term dependence. Finally, we perform a media pooling by veering and get the source process applied as Thf = the target Thi."}, {"heading": "3.1.2 IMAGE ENCODING AND SEMANTIC REPRESENTATION", "text": "We use Convolutionary Neuronal Networks (CNN) to extract feature vectors from images. We propose several ways to extract image characteristics. Global (G) The image feature vector is extracted from the picture using a CNN. With this method we use a feature vector at the specified level as \u03c0. Then \u03c0 is added to the picture vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector vector"}, {"heading": "3.2 INFERRER", "text": "We model the posterior q\u03c6 (z | x, y, \u03c0) with the help of a neural mesh and also the previous p\u03b8 (z | x) with the help of a neural mesh. This section refers to the black and gray part of Figure 1."}, {"heading": "3.2.1 NEURAL POSTERIOR APPROXIMATOR", "text": "Therefore, we consider the modeling of an approximate posterior q\u03c6 (z | x, y, \u03c0) by introducing VAE. We assume that the posterior q\u03c6 (z | x, y, \u03c0) has the following form: q\u03c6 (z | x, y, \u03c0) = N (z; \u00b5 (x, y, \u03c0), \u03c3 (x, y, \u03c0) 2I). (4) The mean and the standard deviation of the approximate posterior are the outputs of neural networks. Starting from the varying neuronal encoder, a semantic representation vector, it will be activated on latent semantic space ashz = g (W (1) z + b (1) z-R (2), where W (1) z-Rdz \u00b7 (de) b (1) z-Rdz (\u00b7 g \u00b7) is an element that we activate by activating (W (1) s (1) er + b (1) z-R (2)."}, {"heading": "3.2.2 NEURAL PRIOR MODEL", "text": "We model the previous distribution p\u03b8 (z | x) as follows: p\u03b8 (z | x) = N (z; \u00b5 \"(x), \u03c3\" (x) 2 I). (7) \u00b5 \"and \u03c3\" are generated in the same way as in Section 3.2.1, except for the absence of y and \u03c0 as inputs. Due to the absence of representation vectors, the dimensions of the weight are in Equation (5) for the previous model W \"(1) z\" Rdz, \"b\" (1) z \"Rdz.\" We use a repair trick to obtain a representation of the latent variables z: h \"z = \u00b5 + \u03c3, \u0445 N (0, I). During the translation, h\" z \"is set as the mean of p\u03b8 (z | x). Then h\" z \"is projected onto the target area ash\" e = g (W (2) z \"z + b (2) z), where h\" e \"Rde\" (8) is then integrated into the machine code."}, {"heading": "3.3 DECODER", "text": "This section refers to the orange part of Figure 1. Given the source set x and the latent variables z, the decoder defines the probability of translation y asp (y | z, x) = T-j = 1 p (yj | y < j, z, x). (9) How we define the probability of translation y is essentially the same as VNMT, except for the use of conditional GRU instead of GRU. Conditional GRU includes two GRU and an attention mechanism. We integrate a latent variable z into the second GRU that we describe in the appendix."}, {"heading": "3.4 MODEL TRAINING", "text": "The Monte Carlo scanning method is used to approximate the expectation via the posterior equation (2), Eq\u03c6 (z | x, y, \u03c0) \u2248 1L \u2211 L = 1 log p\u03b8 (y | x, h (l) z), where L is the number of scans. The training target is defined as L (\u03b8, \u03c6) = \u2212 DKL [q\u03c6 (z | x, y, \u03c0) | | p\u03b8 (z | x)] + 1L L \u2211 l = 1 T \u2211 j = 1 log p\u03b8 (yj | y < j, x, h (l) z), (10), where hz = \u00b5 + \u03c3 \u00b7, \u0445 N (0, I). The first term, KL divergence, can be analytically calculated and is differentiable because both distributions are of Gaussian origin. The second term is also differentiable. Overall, the target L is differentiable."}, {"heading": "4 EXPERIMENTS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 EXPERIMENTAL SETUP", "text": "We used Multi30k et al., 2016) as a data set. Multi30k have an English description and an English description for each corresponding image. We treat 29,000 pairs as training data, 1,014 pairs as validation data, and 1,000 pairs as test data.Before training, punctuation normalization, and lowercase letters are applied to both English and German sentences by Moses (Koehn et al., 2007) scripts2. Compound-word splitting is only applied to German sentences with Sennrich et al. (2016) 3. Then tokenize sentences2 and use them as training data. We produce dictionaries from training data. The vocabulary will be 10,211 words for English and 13,180 words for German by compound-word splitting.Image features are extracted using VGG-19 CNN & Zisserman, 2014."}, {"heading": "4.2 RESULT", "text": "Table 1 shows test results. It shows that our models exceed the baseline in both METEOR and BLEU. Figure 4 shows the graph of the baseline METEOR score and our models during validation. Figure 5 shows the graph of the METEOR score and the length of the source set."}, {"heading": "4.3 QUANTITATIVE ANALYSIS", "text": "Table 1 shows that G performs best in proposed models. In G, we simply set the characteristic of the original image. In fact, the proposed model does not benefit from R-CNN, presumably because we are not very good at handling image sequences. G + O-AVG, for example, uses the average of several image characteristics, but it only makes the original image information unnecessarily confusing. Figure 4 also shows that G and G + O-AVG fluctuate more moderately than others. We find that G and G + O-AVG gain stability almost every time image information is added. If we look at the difference between the test value and the validation value for each model, Figure 4 also shows that G and G + O-AVG values fluctuate more moderately than others."}, {"heading": "4.4 QUALITATIVE ANALYSIS", "text": "The top 30 sentences that make the biggest difference between G and VNMT, we have presented to native German speakers and get the overall comments. They have not been informed by 7https: / / github.com / jhclark / multeval, we use meteor1.5 instead of meteor1.4, which is the default of MultEval.our model training with image in addition to the text. These comments are summarized in two general comments: First, that G translates the meaning of the source material more accurately than VNMT. Second, that our model has more grammatical errors than preposition errors or missing verbs compared to VNMT. We assume that these two comments are reasonable because G is trained with images that mainly have a representation of noun and not of verb, so can grasp the meaning of materials in sentences."}, {"heading": "5 CONCLUSION", "text": "As described here, we have proposed the neural machine translation model, which explicitly has a latent variable that includes semantics derived from both text and images. Our model exceeds the baseline in both METER and BLEU values. Experiments and analysis show that our model can produce more precise translations for short sentences. In qualitative analysis, we present that our model can translate nouns accurately, while our model makes grammatical errors."}, {"heading": "A DERIVATION OF LOWER BOUNDS", "text": "The lower limit of our model can be derived as follows: p (y | x) = p (y, z | x) dz = p (z | x) p (y | z, x) dzlog p (y | x) = log q (z | x, y, \u03c0) p (z | z | x, x) p (y | z, x) q (z | x, y, \u03c0) dz \u2265 q (z | x, y, \u03c0) + log p (z | x, x)) p (y | z, x) q (z | x, y, \u03c0) (logp (z | x, y) + log p (y | z, x)) dz = \u2212 DKL [q (z | x, y, \u03c0) | | p (z | x | x)] + Eq (z | x, y, \u03c0) [log p (y | z, x, x)] = L"}, {"heading": "B CONDITIONAL GRU", "text": "Conditional GRU is implemented in dl4mt. Caglayan et al. (2016) extends the Conditional GRU to enable it to receive image information as input. the first GRU calculates the intermediate representation s'j'j'j = (1 \u2212 o'j) s'j + o'j sj \u2212 1 (11) s'j = tanh (W'E'j \u2212 1] + r'j (U'sj \u2212 1))) (12) r \"j\" s (W'rE [yj \u2212 1] + U \"rsj \u2212 1) (W'rsj \u2212 1) (13) o'j \u2212 1) o\" j \"s (W'oE'j \u2212 1) + U\" osj \u2212 1) (14), where E \"Rdemb\" dt \"s is the target word embedding, s\" j \"j\" s \"s\" s \"Rdh\" (s), s \"j\" s \"s\" Rdh \"(dh\")."}, {"heading": "C TRAINING DETAIL", "text": "C.1 HYPERPARAMETERSTable 2 presents parameters that we use in the experiments. We found that the Multi30k dataset can be easily missed. Figure 8 and Figure 9 show training costs and validation curves of the METEOR result diagram of the two experimental settings of the NMT model. Table 3 shows the hyperparameters used in the experiments. Large decay sizes and small batch sizes ultimately result in the better METEOR values. Training is stopped if there are no validation cost improvements during the last 10 validations. 0 10000 20000 30000 30000 40000 50000 60000 Iteration0000 60000 70000 Iteration020406080100co stTraining Cost1 Figure 8: NMT training costs 0 10 20 30 40 50 60 70 Iteration (x 1000) 0102030405060M E T Ralidation METE10."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio"], "venue": "In ICLR,", "citeRegEx": "Bahdanau et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Perceptual symbol Systems", "author": ["Lawrence W. Barsalou"], "venue": "Behavioral and Brain Sciences,", "citeRegEx": "Barsalou.,? \\Q1999\\E", "shortCiteRegEx": "Barsalou.", "year": 1999}, {"title": "Does Multimodality Help Human and Machine for Translation and Image Captioning", "author": ["Ozan Caglayan", "Walid Aransa", "Yaxing Wang", "Marc Masana", "Mercedes Garc\u0131\u0301a-Mart\u0131\u0301nez", "Fethi Bougares", "Lo\u0131\u0308c Barrault", "Joost van de Weijer"], "venue": "WMT,", "citeRegEx": "Caglayan et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Caglayan et al\\.", "year": 2016}, {"title": "DCU-UvA Multimodal MT System Report", "author": ["Iacer Calixto", "Desmond Elliott", "Stella Frank"], "venue": "In Proceedings of the First Conference on Machine Translation,", "citeRegEx": "Calixto et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Calixto et al\\.", "year": 2016}, {"title": "Meteor Universal: Language Specific Translation Evaluation for Any Target Language", "author": ["Michael Denkowski", "Alon Lavie"], "venue": "In Proceedings of the EACL 2014 Workshop on Statistical Machine Translation,", "citeRegEx": "Denkowski and Lavie.,? \\Q2014\\E", "shortCiteRegEx": "Denkowski and Lavie.", "year": 2014}, {"title": "Multilingual Image Description with Neural Sequence Models", "author": ["D. Elliott", "S. Frank", "E. Hasler"], "venue": "ArXiv e-prints,", "citeRegEx": "Elliott et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Elliott et al\\.", "year": 2015}, {"title": "Fast R-CNN", "author": ["Ross Girshick"], "venue": "In ICCV,", "citeRegEx": "Girshick.,? \\Q2015\\E", "shortCiteRegEx": "Girshick.", "year": 2015}, {"title": "Deep Residual Learning for Image Recognition", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"], "venue": null, "citeRegEx": "He et al\\.,? \\Q2016\\E", "shortCiteRegEx": "He et al\\.", "year": 2016}, {"title": "Multimodal Pivots for Image Caption Translation", "author": ["Julian Hitschler", "Stefan Riezler"], "venue": "arXiv preprint arXiv:1601.03916,", "citeRegEx": "Hitschler and Riezler.,? \\Q2016\\E", "shortCiteRegEx": "Hitschler and Riezler.", "year": 2016}, {"title": "Attention-based Multimodal Neural Machine Translation", "author": ["Po-Yao Huang", "Frederick Liu", "Sz-Rung Shiang", "Jean Oh", "Chris Dyer"], "venue": "In WMT,", "citeRegEx": "Huang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2016}, {"title": "Semi-supervised Learning with Deep Generative Models", "author": ["Diederik P Kingma", "Shakir Mohamed", "Danilo Jimenez Rezende", "Max Welling"], "venue": "In NIPS,", "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Open Source Toolkit for Statistical Machine Translation", "author": ["Philipp Koehn", "Hieu Hoang", "Alexandra Birch", "Chris Callison-Burch", "Marcello Federico", "Nicola Bertoldi", "Brooke Cowan", "Wade Shen", "Christine Moran", "Richard Zens", "Chris Dyer", "Ond\u0159ej Bojar", "Alexandra Constantin", "Evan Herbst. Moses"], "venue": "In ACL,", "citeRegEx": "Koehn et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Koehn et al\\.", "year": 2007}, {"title": "CUNI System for WMT16 Automatic Post-Editing and Multimodal Translation Tasks", "author": ["Jind\u0159ich Libovick\u00fd", "Jind\u0159ich Helcl", "Marek Tlust\u00fd", "Ond\u0159ej Bojar", "Pavel Pecina"], "venue": "In Proceedings of the First Conference on Machine Translation,", "citeRegEx": "Libovick\u00fd et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Libovick\u00fd et al\\.", "year": 2016}, {"title": "BLEU: A Method for Automatic Evaluation of Machine Translation", "author": ["Kishore Papineni", "Salim Roukos", "Todd Ward", "Wei-Jing Zhu"], "venue": "In ACL,", "citeRegEx": "Papineni et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "Generative Adversarial Text to Image Synthesis", "author": ["Scott Reed", "Zeynep Akata", "Xinchen Yan", "Lajanugen Logeswaran", "Bernt Schiele", "Honglak Lee"], "venue": null, "citeRegEx": "Reed et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Reed et al\\.", "year": 2016}, {"title": "Stochastic Backpropagation and Approximate Inference in Deep Generative Models", "author": ["Danilo J. Rezende", "Shakir Mohamed", "Daan Wierstra"], "venue": "In ICML,", "citeRegEx": "Rezende et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Rezende et al\\.", "year": 2014}, {"title": "Costa-juss\u00e0. WMT 2016 Multimodal Translation System Description based on Bidirectional Recurrent Neural Networks with Double-Embeddings", "author": ["Sergio Rodr\u0131\u0301guez Guasch", "Marta R"], "venue": "In Proceedings of the First Conference on Machine Translation,", "citeRegEx": "Guasch and R.,? \\Q2016\\E", "shortCiteRegEx": "Guasch and R.", "year": 2016}, {"title": "Neural Machine Translation of Rare Words with Subword Units", "author": ["Rico Sennrich", "Barry Haddow", "Alexandra Birch"], "venue": "In ACL,", "citeRegEx": "Sennrich et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Sennrich et al\\.", "year": 2016}, {"title": "SHEF-Multimodal: Grounding Machine Translation on Images", "author": ["Kashif Shah", "Josiah Wang", "Lucia Specia"], "venue": "In Proceedings of the First Conference on Machine Translation,", "citeRegEx": "Shah et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Shah et al\\.", "year": 2016}, {"title": "Very Deep Convolutional Networks for Large-Scale Image Recognition", "author": ["Karen Simonyan", "Andrew Zisserman"], "venue": "CoRR, abs/1409.1556,", "citeRegEx": "Simonyan and Zisserman.,? \\Q2014\\E", "shortCiteRegEx": "Simonyan and Zisserman.", "year": 2014}, {"title": "A shared Task on Multimodal Machine Translation and Crosslingual Image Description", "author": ["Lucia Specia", "Stella Frank", "Khalil Simaan", "Desmond Elliott"], "venue": "In Proceedings of the First Conference on Machine Translation,", "citeRegEx": "Specia et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Specia et al\\.", "year": 2016}, {"title": "Sequence to Sequence Learning with Neural Networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V Le"], "venue": "In NIPS,", "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Modeling Coverage for Neural Machine Translation", "author": ["Zhaopeng Tu", "Zhengdong Lu", "Yang Liu", "Xiaohua Liu", "Hang Li"], "venue": "In ACL,", "citeRegEx": "Tu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Tu et al\\.", "year": 2016}, {"title": "Show, Attend and Tell: Neural Image Caption Generation with Visual Attention", "author": ["Kelvin Xu", "Jimmy Ba", "Ryan Kiros", "Kyunghyun Cho", "Aaron Courville", "Ruslan Salakhutdinov", "Richard S Zemel", "Yoshua Bengio"], "venue": "In CVPR,", "citeRegEx": "Xu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2015}, {"title": "Variational Neural Machine Translation", "author": ["Biao Zhang", "Deyi Xiong", "Jinsong Su"], "venue": "In EMNLP,", "citeRegEx": "Zhang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2016}, {"title": "CONDITIONAL GRU Conditional GRU is implemented in dl4mt. Caglayan et al. (2016) extends Conditional GRU to make it capable of receiving image information", "author": ["B L"], "venue": null, "citeRegEx": "L,? \\Q2016\\E", "shortCiteRegEx": "L", "year": 2016}], "referenceMentions": [{"referenceID": 21, "context": "Neural machine translation (NMT) has achieved great success in recent years (Sutskever et al., 2014; Bahdanau et al., 2015).", "startOffset": 76, "endOffset": 123}, {"referenceID": 0, "context": "Neural machine translation (NMT) has achieved great success in recent years (Sutskever et al., 2014; Bahdanau et al., 2015).", "startOffset": 76, "endOffset": 123}, {"referenceID": 0, "context": "However, the most standard model, NMT with attention (Bahdanau et al., 2015) entails the shortcoming that the attention mechanism cannot capture the entire meaning of a sentence because it generates a target word while depending heavily on the relevant parts of the source sentence (Tu et al.", "startOffset": 53, "endOffset": 76}, {"referenceID": 22, "context": ", 2015) entails the shortcoming that the attention mechanism cannot capture the entire meaning of a sentence because it generates a target word while depending heavily on the relevant parts of the source sentence (Tu et al., 2016).", "startOffset": 213, "endOffset": 230}, {"referenceID": 24, "context": "To overcome this problem, Variational Neural Machine Translation (VNMT), which outperforms NMT with attention introduces a latent variable to capture the underlying semantic from source and target (Zhang et al., 2016).", "startOffset": 197, "endOffset": 217}, {"referenceID": 1, "context": "For example, we human beings understand the meaning of language by linking perceptual information given by the surrounding environment and language (Barsalou, 1999).", "startOffset": 148, "endOffset": 164}, {"referenceID": 10, "context": "Our model includes an explicit latent variable z, which has underlying semantics extracted from text and images by introducing a Variational Autoencoder (VAE) (Kingma et al., 2014; Rezende et al., 2014).", "startOffset": 159, "endOffset": 202}, {"referenceID": 15, "context": "Our model includes an explicit latent variable z, which has underlying semantics extracted from text and images by introducing a Variational Autoencoder (VAE) (Kingma et al., 2014; Rezende et al., 2014).", "startOffset": 159, "endOffset": 202}, {"referenceID": 0, "context": ", 2014; Bahdanau et al., 2015). In contrast to statistical machine translation, which requires huge phrase and rule tables, NMT requires much less memory. However, the most standard model, NMT with attention (Bahdanau et al., 2015) entails the shortcoming that the attention mechanism cannot capture the entire meaning of a sentence because it generates a target word while depending heavily on the relevant parts of the source sentence (Tu et al., 2016). To overcome this problem, Variational Neural Machine Translation (VNMT), which outperforms NMT with attention introduces a latent variable to capture the underlying semantic from source and target (Zhang et al., 2016). We follow the motivation of VNMT, which is to capture underlying semantic of a source. Image information is related to language. For example, we human beings understand the meaning of language by linking perceptual information given by the surrounding environment and language (Barsalou, 1999). Although it is natural and easy for humans, it is difficult for computers to understand different domain\u2019s information integrally. Solving this difficult task might, however, bring great improvements in natural language processing. Several researchers have attempted to link language and images such as image captioning by Xu et al. (2015) or image generation from sentences by Reed et al.", "startOffset": 8, "endOffset": 1310}, {"referenceID": 0, "context": ", 2014; Bahdanau et al., 2015). In contrast to statistical machine translation, which requires huge phrase and rule tables, NMT requires much less memory. However, the most standard model, NMT with attention (Bahdanau et al., 2015) entails the shortcoming that the attention mechanism cannot capture the entire meaning of a sentence because it generates a target word while depending heavily on the relevant parts of the source sentence (Tu et al., 2016). To overcome this problem, Variational Neural Machine Translation (VNMT), which outperforms NMT with attention introduces a latent variable to capture the underlying semantic from source and target (Zhang et al., 2016). We follow the motivation of VNMT, which is to capture underlying semantic of a source. Image information is related to language. For example, we human beings understand the meaning of language by linking perceptual information given by the surrounding environment and language (Barsalou, 1999). Although it is natural and easy for humans, it is difficult for computers to understand different domain\u2019s information integrally. Solving this difficult task might, however, bring great improvements in natural language processing. Several researchers have attempted to link language and images such as image captioning by Xu et al. (2015) or image generation from sentences by Reed et al. (2016). They described the possibility of integral understanding of images and text.", "startOffset": 8, "endOffset": 1367}, {"referenceID": 13, "context": "Our model outperforms the baseline with two evaluation metrics: METEOR (Denkowski & Lavie, 2014) and BLEU (Papineni et al., 2002).", "startOffset": 106, "endOffset": 129}, {"referenceID": 24, "context": "Our model is the extension of Variational Neural Machine Translation (VNMT) (Zhang et al., 2016).", "startOffset": 76, "endOffset": 96}, {"referenceID": 2, "context": "Although several studies have been conducted (Caglayan et al., 2016; Huang et al., 2016; Calixto et al., 2016; Libovick\u00fd et al., 2016; Rodr\u0131\u0301guez Guasch & Costa-juss\u00e0, 2016; Shah et al., 2016), they do not show great improvement, especially in neural machine translation (Specia et al.", "startOffset": 45, "endOffset": 192}, {"referenceID": 9, "context": "Although several studies have been conducted (Caglayan et al., 2016; Huang et al., 2016; Calixto et al., 2016; Libovick\u00fd et al., 2016; Rodr\u0131\u0301guez Guasch & Costa-juss\u00e0, 2016; Shah et al., 2016), they do not show great improvement, especially in neural machine translation (Specia et al.", "startOffset": 45, "endOffset": 192}, {"referenceID": 3, "context": "Although several studies have been conducted (Caglayan et al., 2016; Huang et al., 2016; Calixto et al., 2016; Libovick\u00fd et al., 2016; Rodr\u0131\u0301guez Guasch & Costa-juss\u00e0, 2016; Shah et al., 2016), they do not show great improvement, especially in neural machine translation (Specia et al.", "startOffset": 45, "endOffset": 192}, {"referenceID": 12, "context": "Although several studies have been conducted (Caglayan et al., 2016; Huang et al., 2016; Calixto et al., 2016; Libovick\u00fd et al., 2016; Rodr\u0131\u0301guez Guasch & Costa-juss\u00e0, 2016; Shah et al., 2016), they do not show great improvement, especially in neural machine translation (Specia et al.", "startOffset": 45, "endOffset": 192}, {"referenceID": 18, "context": "Although several studies have been conducted (Caglayan et al., 2016; Huang et al., 2016; Calixto et al., 2016; Libovick\u00fd et al., 2016; Rodr\u0131\u0301guez Guasch & Costa-juss\u00e0, 2016; Shah et al., 2016), they do not show great improvement, especially in neural machine translation (Specia et al.", "startOffset": 45, "endOffset": 192}, {"referenceID": 20, "context": ", 2016), they do not show great improvement, especially in neural machine translation (Specia et al., 2016).", "startOffset": 86, "endOffset": 107}, {"referenceID": 7, "context": "They simply put source context vectors and image feature vectors extracted from ResNet-50\u2019s \u2018res4f relu\u2019 layer (He et al., 2016) into the decoder called multimodal conditional GRU.", "startOffset": 111, "endOffset": 128}, {"referenceID": 6, "context": "They extract prominent objects from the image by Region-based Convolutional Neural Networks (R-CNN) (Girshick, 2015).", "startOffset": 100, "endOffset": 116}, {"referenceID": 3, "context": "The first papers to study multimodal translation are Elliott et al. (2015) and Hitschler & Riezler (2016).", "startOffset": 53, "endOffset": 75}, {"referenceID": 3, "context": "The first papers to study multimodal translation are Elliott et al. (2015) and Hitschler & Riezler (2016). It was selected as a shared task in Workshop of Machine Translation 2016 (WMT161).", "startOffset": 53, "endOffset": 106}, {"referenceID": 2, "context": "Although several studies have been conducted (Caglayan et al., 2016; Huang et al., 2016; Calixto et al., 2016; Libovick\u00fd et al., 2016; Rodr\u0131\u0301guez Guasch & Costa-juss\u00e0, 2016; Shah et al., 2016), they do not show great improvement, especially in neural machine translation (Specia et al., 2016). Here, we introduce end-to-end neural network translation models like our model. Caglayan et al. (2016) integrate an image into an NMT decoder.", "startOffset": 46, "endOffset": 397}, {"referenceID": 2, "context": "Although several studies have been conducted (Caglayan et al., 2016; Huang et al., 2016; Calixto et al., 2016; Libovick\u00fd et al., 2016; Rodr\u0131\u0301guez Guasch & Costa-juss\u00e0, 2016; Shah et al., 2016), they do not show great improvement, especially in neural machine translation (Specia et al., 2016). Here, we introduce end-to-end neural network translation models like our model. Caglayan et al. (2016) integrate an image into an NMT decoder. They simply put source context vectors and image feature vectors extracted from ResNet-50\u2019s \u2018res4f relu\u2019 layer (He et al., 2016) into the decoder called multimodal conditional GRU. They demonstrate that their method does not surpass the text-only baseline: NMT with attention. Huang et al. (2016) integrate an image into a head of source words sequence.", "startOffset": 46, "endOffset": 734}, {"referenceID": 2, "context": "Although several studies have been conducted (Caglayan et al., 2016; Huang et al., 2016; Calixto et al., 2016; Libovick\u00fd et al., 2016; Rodr\u0131\u0301guez Guasch & Costa-juss\u00e0, 2016; Shah et al., 2016), they do not show great improvement, especially in neural machine translation (Specia et al., 2016). Here, we introduce end-to-end neural network translation models like our model. Caglayan et al. (2016) integrate an image into an NMT decoder. They simply put source context vectors and image feature vectors extracted from ResNet-50\u2019s \u2018res4f relu\u2019 layer (He et al., 2016) into the decoder called multimodal conditional GRU. They demonstrate that their method does not surpass the text-only baseline: NMT with attention. Huang et al. (2016) integrate an image into a head of source words sequence. They extract prominent objects from the image by Region-based Convolutional Neural Networks (R-CNN) (Girshick, 2015). Objects are then converted to feature vectors by VGG-19 (Simonyan & Zisserman, 2014) and are put into a head of source words sequence. They demonstrate that object extraction by R-CNN contributes greatly to the improvement. This model achieved the highest METEOR score in NMTbased models in WMT16, which we compare to our model in the experiment. We designate this model as CMU. Caglayan et al. (2016) argue that their proposed model did not achieve improvement because they failed to benefit from both text and images.", "startOffset": 46, "endOffset": 1311}, {"referenceID": 10, "context": "3 VARIATIONAL AUTO ENCODER VAE was proposed in an earlier report of the literature Kingma et al. (2014); Rezende et al.", "startOffset": 83, "endOffset": 104}, {"referenceID": 10, "context": "3 VARIATIONAL AUTO ENCODER VAE was proposed in an earlier report of the literature Kingma et al. (2014); Rezende et al. (2014). Given an observed variable x, VAE introduces a continuous latent variable z, with the assumption that x is generated from z.", "startOffset": 83, "endOffset": 127}, {"referenceID": 0, "context": "1 TEXT ENCODING The source and target are encoded in the same way as Bahdanau et al. (2015). The source is converted to a sequence of 1-of-k vector and is embedded to demb dimensions.", "startOffset": 69, "endOffset": 92}, {"referenceID": 11, "context": "Before training, punctuation normalization and lowercase are applied to both English and German sentences by Moses (Koehn et al., 2007) scripts2.", "startOffset": 115, "endOffset": 135}, {"referenceID": 6, "context": "To extract the object\u2019s region, we use Fast R-CNN (Girshick, 2015).", "startOffset": 50, "endOffset": 66}, {"referenceID": 4, "context": "1 EXPERIMENTAL SETUP We used Multi30k (Elliott et al., 2016) as the dataset. Multi30k have an English description and a German description for each corresponding image. We handle 29,000 pairs as training data, 1,014 pairs as validation data, and 1,000 pairs as test data. Before training, punctuation normalization and lowercase are applied to both English and German sentences by Moses (Koehn et al., 2007) scripts2. Compound-word splitting is conducted only to German sentences using Sennrich et al. (2016)3.", "startOffset": 39, "endOffset": 509}, {"referenceID": 0, "context": "Actually, dl4mt is fundamentally the same model as Bahdanau et al. (2015), except that its decoder employs conditional GRU6.", "startOffset": 51, "endOffset": 74}, {"referenceID": 9, "context": "The score of the CMU is from (Huang et al., 2016).", "startOffset": 29, "endOffset": 49}, {"referenceID": 9, "context": "Huang et al. (2016) states that their proposed model outperforms the baseline (NMT), but we do not have that observation.", "startOffset": 0, "endOffset": 20}], "year": 2016, "abstractText": "Although attention-based Neural Machine Translation have achieved great success, attention-mechanism cannot capture the entire meaning of the source sentence because the attention mechanism generates a target word depending heavily on the relevant parts of the source sentence. The report of earlier studies has introduced a latent variable to capture the entire meaning of sentence and achieved improvement on attention-based Neural Machine Translation. We follow this approach and we believe that the capturing meaning of sentence benefits from image information because human beings understand the meaning of language not only from textual information but also from perceptual information such as that gained from vision. As described herein, we propose a neural machine translation model that introduces a continuous latent variable containing an underlying semantic extracted from texts and images. Our model, which can be trained endto-end, requires image information only when training. Experiments conducted with an English\u2013German translation task show that our model outperforms over the baseline.", "creator": "LaTeX with hyperref package"}}}