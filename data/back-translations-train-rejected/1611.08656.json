{"id": "1611.08656", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-Nov-2016", "title": "Attention-based Memory Selection Recurrent Network for Language Modeling", "abstract": "Recurrent neural networks (RNNs) have achieved great success in language modeling. However, since the RNNs have fixed size of memory, their memory cannot store all the information about the words it have seen before in the sentence, and thus the useful long-term information may be ignored when predicting the next words. In this paper, we propose Attention-based Memory Selection Recurrent Network (AMSRN), in which the model can review the information stored in the memory at each previous time step and select the relevant information to help generate the outputs. In AMSRN, the attention mechanism finds the time steps storing the relevant information in the memory, and memory selection determines which dimensions of the memory are involved in computing the attention weights and from which the information is extracted.In the experiments, AMSRN outperformed long short-term memory (LSTM) based language models on both English and Chinese corpora. Moreover, we investigate using entropy as a regularizer for attention weights and visualize how the attention mechanism helps language modeling.", "histories": [["v1", "Sat, 26 Nov 2016 04:25:00 GMT  (255kb,D)", "http://arxiv.org/abs/1611.08656v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["da-rong liu", "shun-po chuang", "hung-yi lee"], "accepted": false, "id": "1611.08656"}, "pdf": {"name": "1611.08656.pdf", "metadata": {"source": "CRF", "title": "ATTENTION-BASED MEMORY SELECTION RECURRENT NETWORK FOR LANGUAGE MODELING", "authors": ["Da-Rong Liu", "Shun-Po Chuang", "Hung-yi Lee"], "emails": [], "sections": [{"heading": null, "text": "Index Terms - Language Modelling, Recurring Network, Attention Model"}, {"heading": "1. INTRODUCTION", "text": "This year, it has come to the point where it can only take a year to find a solution that is capable of finding a solution, that is capable of finding a solution that is capable of finding a solution, and that is capable of finding a solution that is capable of finding a solution, that is capable of finding a solution that is capable of finding a solution, that is capable of finding a solution that is capable of finding a solution, that is capable of finding a solution that is capable of finding a solution. \""}, {"heading": "2. ATTENTION-BASED MEMORY SELECTION RECURRENT NETWORK", "text": "The overall structure of the proposed Attention-based Memory Selection Recurrent Network (AMSRN) is shown in Fig. 1. AMSRNar Xiv: 161 1.08 656v 1 [cs.C L] 26 Nov 201 6 consists of two main parts: the typical LSTM described in Section 2.1, and the Attention Mechanism Module, which is stacked on LSTM in Section 2.2. The LSTM reads the entered word sequence and stores the hidden layer outputs generated at each time step. The Attention Mechanism Module takes the stored information as input and creates a vector relevant to predicting the next words. Subsequently, the relevant vector and the current LSTM hidden state are used to generate the distributions for the next words. In the Attention Mechanism Module, the memory selection is applied to the LSTM memory to determine which dimensions of the attention should be weighted by the M in order to calculate the relevant attention weights in the section of the STM."}, {"heading": "2.1. LSTM", "text": "The input of the LSTM consists of a sequence of words represented by 1-ofN encoding, {x1, x2, \u00b7 \u00b7 \u00b7 xt, \u00b7 \u00b7} at the bottom of Figure 1. At any time, step t (if the model has read the first t words of the sentence in question) is a d-dimensional vector, where d is the number of memory cells in the LSTM and ht would be stored for further use. Therefore, the stored information at the time step t (if the model has read the first t words in the sentence in question) is Mt, Mt = [h0, h1, \u00b7 ht \u2212 1], (1), where h0 is the initial state of the LSTM.Mt in (1) is a d \u00b7 t matrix that grows with increasing t. The attention module extracts the information from Mt."}, {"heading": "2.2. Attention Mechanism", "text": "In the Attention Mechanism, the memory selection module generates two d-dimensional vectors, wh1 and wh2, from the current LSTM state. wh1 and wh2 are used to select the stored information. Here, all elements in wh1 and wh2 are used to extract the relevant information, represented as d-dimensional vector rt, fromMt in (1). The model first creates a d-dimensional vector kt from the current hiddenstate ht ht ht ht as a \"key\" for generating attention weight, kt = Wkhht + bk, (2) where the d-dimensional vector rt rt rt and the d-dimensional vector bk are relevant."}, {"heading": "2.3. Memory Selection", "text": "In this paper, we examine three different ways to get wh1 in (3) and wh2 in (6): 1. wh1 and wh2 are generated independently of each other; the current state of LSTM, ht, is transferred to two fully interconnected layers with sigmoid activation function to generate wh1 and wh2 as below, wh1 = sigmoid (Whh1ht + bh1) wh2 = sigmoid (Whh2ht + bh2), Whh1, bh1, Whh2 and bh2 denoting the weights and distortions of the fully interconnected layer; 2. the two vectors wh1 and wh2 must be generated in the same way. wh1 is generated in the same way as the first approach, and the model simply sets wh2 = wh1.3. The only difference between the third and second approach is that we set wh1 = 1 \u2212 wh2, where 1 is a dimensional vector with all, and \"\u2212\" here is an elementary subtraction."}, {"heading": "2.4. Regularizer", "text": "In the training model, a regularization term is usually used to prevent overadjustment. Thus, for example, the two-norm of model parameters is widely used as a regularization term. Here, we examine whether the entropy of attention weights is used as a regularization term [28]. The purpose of using entropy as a regularization term is because only a portion of the information in the previous steps is relevant for predicting the next word. Therefore, the attention weights that extract useful information from the previous time steps are sparse. The entropy regularizer, in order to keep attention weights low, is as shown below. Lreg = \u2211 u \u0445 t = 1 t \u2212 1 \u0445 i = 0 \u2212 wti logwti, (7) where u is a sentence in the training corpus, and Tu is the length of and wti (in step 7, attention weight is reversed when reading the sentence of hi and wti)."}, {"heading": "3. EXPERIMENTS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1. Experimental Setups", "text": "We tested the proposed model on two English datasets and one Chinese dataset. The first dataset we used was the Penn Treebank Corpus [29], a widely used dataset for evaluating the effectiveness of a language model. It contains about 40K training sets, 3K validation sets and 4K test sets. The other English dataset we used came from the Corpus [30] switching board. The switching board is a telephone caller body that collects two-way telephone calls between speakers in the United States. We used about 945K sets for training, 10K for validation and about 5.2K for the test. For Chinese we used the Chinese Gigaword dataset [31] to evaluate the model. The Chinese Gigaword dataset consists of about 25K Chinese news articles. After the parse, there are 531K sets for training, and 10K for the validation of the following STM, we summarize the data sets for the following table 0K and the STM testing for about 26K."}, {"heading": "3.2. Memory Selection Methods", "text": "In this experiment, we will examine various methods of memory selection for generating wh1 and wh2, respectively; the three methods in subsection 2.3 are (1) generating wh1 and wh2 independently, (2) generating wh1 = wh2 and (3) generating wh1 = 1 \u2212 wh2; the results of the three methods are in Table 2. Generating wh1 and wh2 independently leads to the worst result (133.80), possibly because this approach requires two parameters compared to the other two parameters; limiting the equality of wh1 and wh2 is better than supplementing it (133.36 v.s. 133.62); the results suggest that the information for calculating similarity and information extraction is likely to be contained in the same dimension of the hidden states of the LSTM; since the second method achieves the best result, it is used in the following experiments."}, {"heading": "3.3. Comparison of Different Models", "text": "The experimental results of the different models are shown in Table 3. Columns (1), (2) and (3) are the results on Penn Treebank Corpus, Switchboard Corpus and Chinese Gigaword Dataset. Experiments were carried out step by step. First, a typical LSTM language model was trained, and PPLs of the LSTM model on the test sets are in row (A). Then, in row (B) the attention module was added on top of the LSTM, but without memory selection (or all elements in wh1 and wh2 are one) and entropy regulators. It is noted that the attention mechanism was helpful on both Penn Treebank and Switchboard. (Rows (B) v.s. (A) on columns (1) and (2)), but it does not improve the LSTM model on Chinese Gigaword (Rows) v.s."}, {"heading": "3.4. Analysis", "text": "To illustrate how the attention mechanism works, we visualize the attention weights in some sentences. First, we calculate the helplessness of each sentence in the Gigaword (Chinese) and Penn Treebank (English) datasets, and then select the sentences that have improved most by the proposed model (row (C) in Table 3) compared to the LSTM baselines. We selected ten sentences from the Gigaword (Chinese) and Penn Treebank (English) datasets and visualize and analyze the attention weights. Four examples are in Fig. 2. In Fig. 2, the arrows point to the words to be predicted, and we highlight the words whose attention weights are higher than a threshold when predicting the words with arrows. We found that a word under one of the following four conditions will have great attention: 1. Trigger (Example (a) in Fig. 2: When the information is repeated, the same part of the information is predicted in the model."}, {"heading": "4. CONCLUDING REMARKS", "text": "In this paper, we propose Attention-based Memory Selection Recurrent Network (AMSRN) for speech modeling and examine the integration of attention mechanism and LSTM. Results have been verified on two English corpora and a Chinese corpus. Results show that AMSRN consistently performs better than LSTM-based speech model, and memory selection is indispensable for the attention mechanism. We also visualize how the attention mechanism works in speech modeling. Some unresolved questions in this paper will be investigated in the future, such as the influence of language traits on the attention model."}, {"heading": "5. REFERENCES", "text": "[1] Jeffrey L Elman, \"Finding structure in time,\" Cognitive science, vol. 14, no. 2, pp. 179-211, 1990. [2] Ilya Sutskever, Oriol Vinyals, and Quoc V Le, \"Sequence to sequence learning with neural networks,\" in Advances in neural information processing systems, 2014, pp. 3104-3112. [3] Sepp Hochreiter and Juergen Schmidhuber, \"Long short-term memory,\" Neural computation, vol. 9, no. 8, pp. 1735-1780, 1997. [4] Junyoung Chung, Caglar Gulcehre, KyungHyun Cho Bengio, \"Empirical evaluation of gated recurrent neural networks on sequence modeling,\" arXiv preprint arXiv: 1412.3555, 2014."}], "references": [{"title": "Finding structure in time", "author": ["Jeffrey L Elman"], "venue": "Cognitive science, vol. 14, no. 2, pp. 179\u2013211, 1990.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1990}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V Le"], "venue": "Advances in neural information processing systems, 2014, pp. 3104\u20133112.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2014}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation, vol. 9, no. 8, pp. 1735\u20131780, 1997.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1997}, {"title": "Empirical evaluation of gated recurrent neural networks on sequence modeling", "author": ["Junyoung Chung", "Caglar Gulcehre", "KyungHyun Cho", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1412.3555, 2014.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2014}, {"title": "Neural turing machines", "author": ["Alex Graves", "Greg Wayne", "Ivo Danihelka"], "venue": "arXiv preprint arXiv:1410.5401, 2014.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2014}, {"title": "Effective approaches to attention-based neural machine translation", "author": ["Minh-Thang Luong", "Hieu Pham", "Christopher D Manning"], "venue": "arXiv preprint arXiv:1508.04025, 2015.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2015}, {"title": "Incorporating structural alignment biases into an attentional neural translation model", "author": ["Trevor Cohn", "Cong Duy Vu Hoang", "Ekaterina Vymolova", "Kaisheng Yao", "Chris Dyer", "Gholamreza Haffari"], "venue": "arXiv preprint arXiv:1601.01085, 2016.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2016}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1409.0473, 2014.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2014}, {"title": "Show, attend and tell: Neural image caption generation with visual attention", "author": ["Kelvin Xu", "Jimmy Ba", "Ryan Kiros", "Kyunghyun Cho", "Aaron Courville", "Ruslan Salakhutdinov", "Richard S Zemel", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1502.03044, vol. 2, no. 3, pp. 5, 2015.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2015}, {"title": "Aligning where to see and what to tell: image caption with region-based attention and scene factorization", "author": ["Junqi Jin", "Kun Fu", "Runpeng Cui", "Fei Sha", "Changshui Zhang"], "venue": "arXiv preprint arXiv:1506.06272, 2015.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "Stacked attention networks for image question answering", "author": ["Zichao Yang", "Xiaodong He", "Jianfeng Gao", "Li Deng", "Alex Smola"], "venue": "arXiv preprint arXiv:1511.02274, 2015.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "Abc-cnn: An attention based convolutional neural network for visual question answering", "author": ["Kan Chen", "Jiang Wang", "Liang-Chieh Chen", "Haoyuan Gao", "Wei Xu", "Ram Nevatia"], "venue": "arXiv preprint arXiv:1511.05960, 2015.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2015}, {"title": "Where to look: Focus regions for visual question answering", "author": ["Kevin J. Shih", "Saurabh Singh", "Derek Hoiem"], "venue": "The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2016.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2016}, {"title": "Human attention in visual question answering: Do humans and deep networks look at the same regions", "author": ["Abhishek Das", "Harsh Agrawal", "C Lawrence Zitnick", "Devi Parikh", "Dhruv Batra"], "venue": "arXiv preprint arXiv:1606.03556, 2016.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2016}, {"title": "Two decades of statistical language modeling: Where do we go from here", "author": ["Roni Rosenfeld"], "venue": "2000.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2000}, {"title": "Up from trigrams", "author": ["Fred Jelinek"], "venue": ".", "citeRegEx": "16", "shortCiteRegEx": null, "year": 0}, {"title": "Recurrent neural network based language model", "author": ["Tomas Mikolov"], "venue": ".", "citeRegEx": "17", "shortCiteRegEx": null, "year": 0}, {"title": "Extensions of recurrent neural network language model", "author": ["Tom\u00e1\u0161 Mikolov", "Stefan Kombrink", "Luk\u00e1\u0161 Burget", "Jan \u010cernock\u1ef3", "Sanjeev Khudanpur"], "venue": "2011 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2011, pp. 5528\u20135531.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2011}, {"title": "Feed forward pre-training for recurrent neural network language models", "author": ["Siva Reddy Gangireddy", "Fergus McInnes", "Steve Renals"], "venue": "2014.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2014}, {"title": "Efficient lattice rescoring using recurrent neural network language models", "author": ["Xunying Liu", "Yongqiang Wang", "Xie Chen", "Mark JF Gales", "Philip C Woodland"], "venue": "2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2014, pp. 4908\u20134912.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2014}, {"title": "Bilingual recurrent neural networks for improved statistical machine translation", "author": ["Bing Zhao", "Yik-Cheung Tam"], "venue": "Spoken Language Technology Workshop (SLT), 2014 IEEE. IEEE, 2014, pp. 66\u201370.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2014}, {"title": "Bayesian recurrent neural network for language modeling", "author": ["Jen-Tzung Chien", "Yuan-Chu Ku"], "venue": "IEEE transactions on neural networks and learning systems, vol. 27, no. 2, pp. 361\u2013374, 2016.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2016}, {"title": "Deep bi-directional recurrent networks over spectral windows", "author": ["Abdel-rahman Mohamed", "Frank Seide", "Dong Yu", "Jasha Droppo", "Andreas Stoicke", "Geoffrey Zweig", "Gerald Penn"], "venue": "2015 IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU). IEEE, 2015, pp. 78\u201383.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2015}, {"title": "Constructing long short-term memory based deep recurrent neural networks for large vocabulary speech recognition", "author": ["Xiangang Li", "Xihong Wu"], "venue": "2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2015, pp. 4520\u20134524.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2015}, {"title": "Long shortterm memory-networks for machine reading", "author": ["Jianpeng Cheng", "Li Dong", "Mirella Lapata"], "venue": "arXiv preprint arXiv:1601.06733, 2016.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2016}, {"title": "Recurrent memory network for language modeling", "author": ["Ke Tran", "Arianna Bisazza", "Christof Monz"], "venue": "arXiv preprint arXiv:1601.01272, 2016.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2016}, {"title": "Endto-end memory networks", "author": ["Sainbayar Sukhbaatar", "Jason Weston", "Rob Fergus"], "venue": "Advances in neural information processing systems, 2015, pp. 2440\u20132448.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2015}, {"title": "Semi-supervised learning by entropy minimization", "author": ["Yves Grandvalet", "Yoshua Bengio"], "venue": "Advances in neural information processing systems, 2004, pp. 529\u2013536.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2004}, {"title": "The penn treebank: an overview", "author": ["Ann Taylor", "Mitchell Marcus", "Beatrice Santorini"], "venue": "Treebanks, pp. 5\u201322. Springer, 2003.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2003}, {"title": "Switchboard: Telephone speech corpus for research and development", "author": ["John J Godfrey", "Edward C Holliman", "Jane McDaniel"], "venue": "Acoustics, Speech, and Signal Processing, 1992. ICASSP-92., 1992 IEEE International Conference on. IEEE, 1992, vol. 1, pp. 517\u2013520.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 1992}, {"title": "Chinese gigaword", "author": ["David Graff", "Ke Chen"], "venue": "LDC Catalog No.: LDC2003T09, ISBN, vol. 1, pp. 58563\u201358230, 2005.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2003}], "referenceMentions": [{"referenceID": 0, "context": "Recurrent neural networks (RNNs) [1] have been shown to perform well in many sequence modeling tasks[2].", "startOffset": 33, "endOffset": 36}, {"referenceID": 1, "context": "Recurrent neural networks (RNNs) [1] have been shown to perform well in many sequence modeling tasks[2].", "startOffset": 100, "endOffset": 103}, {"referenceID": 2, "context": "In RNNs, the gated memory cells like long short-term memory (LSTM) [3] and gated recurrent unit (GRU) [4] are widely used.", "startOffset": 67, "endOffset": 70}, {"referenceID": 3, "context": "In RNNs, the gated memory cells like long short-term memory (LSTM) [3] and gated recurrent unit (GRU) [4] are widely used.", "startOffset": 102, "endOffset": 105}, {"referenceID": 4, "context": "Neural Turing Machine (NTM) [5] is one of the examples.", "startOffset": 28, "endOffset": 31}, {"referenceID": 5, "context": "Attention mechanism shows promising results on many tasks including machine translation[6, 7, 8], caption generation[9, 10] and question answering [11, 12, 13, 14].", "startOffset": 87, "endOffset": 96}, {"referenceID": 6, "context": "Attention mechanism shows promising results on many tasks including machine translation[6, 7, 8], caption generation[9, 10] and question answering [11, 12, 13, 14].", "startOffset": 87, "endOffset": 96}, {"referenceID": 7, "context": "Attention mechanism shows promising results on many tasks including machine translation[6, 7, 8], caption generation[9, 10] and question answering [11, 12, 13, 14].", "startOffset": 87, "endOffset": 96}, {"referenceID": 8, "context": "Attention mechanism shows promising results on many tasks including machine translation[6, 7, 8], caption generation[9, 10] and question answering [11, 12, 13, 14].", "startOffset": 116, "endOffset": 123}, {"referenceID": 9, "context": "Attention mechanism shows promising results on many tasks including machine translation[6, 7, 8], caption generation[9, 10] and question answering [11, 12, 13, 14].", "startOffset": 116, "endOffset": 123}, {"referenceID": 10, "context": "Attention mechanism shows promising results on many tasks including machine translation[6, 7, 8], caption generation[9, 10] and question answering [11, 12, 13, 14].", "startOffset": 147, "endOffset": 163}, {"referenceID": 11, "context": "Attention mechanism shows promising results on many tasks including machine translation[6, 7, 8], caption generation[9, 10] and question answering [11, 12, 13, 14].", "startOffset": 147, "endOffset": 163}, {"referenceID": 12, "context": "Attention mechanism shows promising results on many tasks including machine translation[6, 7, 8], caption generation[9, 10] and question answering [11, 12, 13, 14].", "startOffset": 147, "endOffset": 163}, {"referenceID": 13, "context": "Attention mechanism shows promising results on many tasks including machine translation[6, 7, 8], caption generation[9, 10] and question answering [11, 12, 13, 14].", "startOffset": 147, "endOffset": 163}, {"referenceID": 14, "context": "The statistical models such as N-gram language model [15, 16] were widely used to solve this task.", "startOffset": 53, "endOffset": 61}, {"referenceID": 15, "context": "The statistical models such as N-gram language model [15, 16] were widely used to solve this task.", "startOffset": 53, "endOffset": 61}, {"referenceID": 16, "context": "Recently, RNNs are introduced in language modeling [17, 18, 19, 20, 21, 22, 23, 24] and have shown great improvement compared to the traditional counterpart.", "startOffset": 51, "endOffset": 83}, {"referenceID": 17, "context": "Recently, RNNs are introduced in language modeling [17, 18, 19, 20, 21, 22, 23, 24] and have shown great improvement compared to the traditional counterpart.", "startOffset": 51, "endOffset": 83}, {"referenceID": 18, "context": "Recently, RNNs are introduced in language modeling [17, 18, 19, 20, 21, 22, 23, 24] and have shown great improvement compared to the traditional counterpart.", "startOffset": 51, "endOffset": 83}, {"referenceID": 19, "context": "Recently, RNNs are introduced in language modeling [17, 18, 19, 20, 21, 22, 23, 24] and have shown great improvement compared to the traditional counterpart.", "startOffset": 51, "endOffset": 83}, {"referenceID": 20, "context": "Recently, RNNs are introduced in language modeling [17, 18, 19, 20, 21, 22, 23, 24] and have shown great improvement compared to the traditional counterpart.", "startOffset": 51, "endOffset": 83}, {"referenceID": 21, "context": "Recently, RNNs are introduced in language modeling [17, 18, 19, 20, 21, 22, 23, 24] and have shown great improvement compared to the traditional counterpart.", "startOffset": 51, "endOffset": 83}, {"referenceID": 22, "context": "Recently, RNNs are introduced in language modeling [17, 18, 19, 20, 21, 22, 23, 24] and have shown great improvement compared to the traditional counterpart.", "startOffset": 51, "endOffset": 83}, {"referenceID": 23, "context": "Recently, RNNs are introduced in language modeling [17, 18, 19, 20, 21, 22, 23, 24] and have shown great improvement compared to the traditional counterpart.", "startOffset": 51, "endOffset": 83}, {"referenceID": 24, "context": "The examples of integrating attention mechanism and LSTM-based RNN model for language modeling are Long Shortterm Memory-Network (LSTMN) [25] and Recurrent Memory Network (RMN) [26].", "startOffset": 137, "endOffset": 141}, {"referenceID": 25, "context": "The examples of integrating attention mechanism and LSTM-based RNN model for language modeling are Long Shortterm Memory-Network (LSTMN) [25] and Recurrent Memory Network (RMN) [26].", "startOffset": 177, "endOffset": 181}, {"referenceID": 25, "context": "is that in the attention models the memory used to generate attention distribution and the memory used to generate the final attention vector can be different [26, 27].", "startOffset": 159, "endOffset": 167}, {"referenceID": 26, "context": "is that in the attention models the memory used to generate attention distribution and the memory used to generate the final attention vector can be different [26, 27].", "startOffset": 159, "endOffset": 167}, {"referenceID": 27, "context": "Here we investigate to use the entropy of the attention weights as the regularization term [28].", "startOffset": 91, "endOffset": 95}, {"referenceID": 28, "context": "The first data set we used is the Penn Treebank Corpus [29], which is a widely used data set to evaluate the effectiveness of a language model.", "startOffset": 55, "endOffset": 59}, {"referenceID": 29, "context": "The other English data set we used is from the Switchboard Corpus[30].", "startOffset": 65, "endOffset": 69}, {"referenceID": 30, "context": "For Chinese, we used Chinese Gigaword data set[31] to evaluate the model.", "startOffset": 46, "endOffset": 50}, {"referenceID": 25, "context": "We further compare the proposed model with another two attention-based language model, recurrent memory network (RMN) [26] and Recurrent-Memory-Recurrent (RMR) [26].", "startOffset": 118, "endOffset": 122}, {"referenceID": 25, "context": "We further compare the proposed model with another two attention-based language model, recurrent memory network (RMN) [26] and Recurrent-Memory-Recurrent (RMR) [26].", "startOffset": 160, "endOffset": 164}, {"referenceID": 25, "context": "25 (E) RMN [26] 123.", "startOffset": 11, "endOffset": 15}, {"referenceID": 25, "context": "28 (F) RMR [26] 134.", "startOffset": 11, "endOffset": 15}], "year": 2016, "abstractText": "Recurrent neural networks (RNNs) have achieved great success in language modeling. However, since the RNNs have fixed size of memory, their memory cannot store all the information about the words it have seen before in the sentence, and thus the useful longterm information may be ignored when predicting the next words. In this paper, we propose Attention-based Memory Selection Recurrent Network (AMSRN), in which the model can review the information stored in the memory at each previous time step and select the relevant information to help generate the outputs. In AMSRN, the attention mechanism finds the time steps storing the relevant information in the memory, and memory selection determines which dimensions of the memory are involved in computing the attention weights and from which the information is extracted. In the experiments, AMSRN outperformed long short-term memory (LSTM) based language models on both English and Chinese corpora. Moreover, we investigate using entropy as a regularizer for attention weights and visualize how the attention mechanism helps language modeling.", "creator": "LaTeX with hyperref package"}}}