{"id": "1611.08666", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-Nov-2016", "title": "Training an Interactive Humanoid Robot Using Multimodal Deep Reinforcement Learning", "abstract": "Training robots to perceive, act and communicate using multiple modalities still represents a challenging problem, particularly if robots are expected to learn efficiently from small sets of example interactions. We describe a learning approach as a step in this direction, where we teach a humanoid robot how to play the game of noughts and crosses. Given that multiple multimodal skills can be trained to play this game, we focus our attention to training the robot to perceive the game, and to interact in this game. Our multimodal deep reinforcement learning agent perceives multimodal features and exhibits verbal and non-verbal actions while playing. Experimental results using simulations show that the robot can learn to win or draw up to 98% of the games. A pilot test of the proposed multimodal system for the targeted game---integrating speech, vision and gestures---reports that reasonable and fluent interactions can be achieved using the proposed approach.", "histories": [["v1", "Sat, 26 Nov 2016 06:25:08 GMT  (4821kb)", "http://arxiv.org/abs/1611.08666v1", "NIPS Workshop on Future of Interactive Learning Machines, 2016"]], "COMMENTS": "NIPS Workshop on Future of Interactive Learning Machines, 2016", "reviews": [], "SUBJECTS": "cs.LG cs.AI cs.RO", "authors": ["heriberto cuay\\'ahuitl", "guillaume couly", "cl\\'ement olalainty"], "accepted": false, "id": "1611.08666"}, "pdf": {"name": "1611.08666.pdf", "metadata": {"source": "CRF", "title": "Training an Interactive Humanoid Robot Using Multimodal Deep Reinforcement Learning", "authors": ["Heriberto Cuay\u00e1huitl"], "emails": ["HCuayahuitl@lincoln.ac.uk"], "sections": [{"heading": null, "text": "ar Xiv: 161 1.08 666v 1 [cs.L G] 26 N"}, {"heading": "1 Introduction", "text": "Interactive humanoid robots that perceive, act, communicate and learn at the same time are not only interesting for demonstrating robotic skills, but also have the potential to study the embodied human intelligence. This paper describes a small step in this direction, in which we equip a humanoid robot1 with multiple input and output modalities to play the game of zeros and crosses, also known as tic-tac-toe - see Figure 1. These modalities allow the robot to listen to human commands, view human gazes and human drawings in the targeted game, focus on the human actor, talk to human actors, draw zeros / crosses and learn from examples - all asynchronous. The latter ability (learning) requires very efficient forms of training to approach human behavior. While previous work robot learning from real interactions with the environment [10, 8, 25] - mostly verbal skills without having to learn simple autonomous behaviors themselves, 14 are different ways of learning."}, {"heading": "2 Learning Approach for Physical Human-Humanoid Interaction", "text": "Our approach uses two separate but interconnected learning tasks: first, perceiving learning to predict what is going on in the environment (in our case, moves), and second, learning to interact to decide what to do or say next. In this way, humanoid robots can learn to interact using words and moves (a more compact set of multimodal features) rather than words or language and pixels. Although our approach implies more efficient learning due to more compact environments, the latter state representation (raw multimodal features) remains to be explored in future work."}, {"heading": "2.1 Learning to Perceive with Deep Supervised Learning", "text": "We use the camera on the right arm of the robot to perceive symbols in the game grid. Rather, we use multiple images (one per position in the grid, 9 in our case) to detect new drawings that are used to generate moves. The robot continuously takes images and splits them into 9 images of 40 x 40 pixels, as shown in Figure 2.Given a dataset of the form D = (x1, y1),..., (xN, yN)}, where xi are n matrices of pixel-based characteristics and yj are class labels, the task is to match images on labels. In our case, the images have 40 x 40 pixels, and the labels are {\"circle,\" \"nothing.\" We use a deep superior classification to induce the function h."}, {"heading": "2.2 Learning to Interact with Deep Reinforcement Learning", "text": "The visual perceptions above plus speech-based perceptions (words with confidence values) are defined as input to a Q-Q learning agent to induce its behavior from interaction with the environment, mapping situations to actions by maximizing a long-term reward signal. [21, 22] An RL agent is typically characterized by: (i) a finite set of states S = {si}; (ii) a finite set of actions by the user is derived from who starts the game and with which symbol. When the robot starts the game, we assume that the robot circles and the users cross-sessessessesses.actions A = {aj}; (iii) a state transitional function T (s, a) specifying the next state s and the action a; (iv) a reward function R (s, a, s) specifying the reward that the agent is given for selecting actions."}, {"heading": "3 Experiments and Results", "text": "In this section, we apply the above approach and learning tools to a humanoid robot that learns to play the game of zeros and crosses."}, {"heading": "3.1 Integrated System", "text": "In fact, it is a way in which people are able to determine for themselves what they want and what they want."}, {"heading": "3.2 Experimental Results", "text": "While the integrated system is capable of producing reasonable and fluid interactions with human players (as seen in this video), we focus our evaluation on learning to perceive and learn to handle simulated interactions. However, this integrated system has been used in preliminary responses to seven independent human users, and all reported successful interactions they enjoyed. A comprehensive evaluation with multiple human players is considered a future work. The task of this learner is to classify the graduate images into three labels (circle, cross, nothing)."}, {"heading": "4 Related Work and Limitations", "text": "Recent developments in the field of deep learning have enabled the development of new interactive systems. For example, the previous interactive systems require a considerable effort in terms of the selection of people. The underlying learning processes are far from trivial."}, {"heading": "5 Concluding Remarks", "text": "Robot systems that interact with their environment through perception, action, communication and learning often face the challenge of how to bring these different concepts together. We describe a general approach to training a robot to interact with the world using multiple modalities. Instead of training the robot directly using raw pixels, the proposed approach simplifies the general learning task in two stages: perception learning and interaction learning. We tested our approach by training the humanoid robot Baxter to play the game of nothing and ticks. Our experimental results based on simulations show that learning to perceive reached 99.9% of classification accuracy and learning to interact reached a win / draw rate of 98%. A pilot test reported on reasonable interactions with the proposed approach in a multimodal integrated system. Furthermore, our system proved to be data-efficient due to the amount of data used to induce the simulated environment (108 seed images and 10 seed dialogues will initially be usable for seed management)."}], "references": [{"title": "A survey of robot learning from demonstration", "author": ["B.D. Argall", "S. Chernova", "M. Veloso", "B. Browning"], "venue": "Robot. Auton. Syst., 57(5):469\u2013483, May", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2009}, {"title": "Modular neural networks for learning contextdependent game strategies", "author": ["J.A. Boyan", "J.A. Boyan", "J.A. Boyan", "J.A. Boyan"], "venue": "Master\u2019s thesis,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1992}, {"title": "Robot learning from verbal interaction: A brief survey", "author": ["H. Cuay\u00e1huitl"], "venue": "4th International Symposium on New Frontiers in HRI,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2015}, {"title": "SimpleDS: A simple deep reinforcement learning dialogue system", "author": ["H. Cuay\u00e1huitl"], "venue": "CoRR, abs/1601.04574,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2016}, {"title": "Strategic dialogue management via deep reinforcement learning", "author": ["H. Cuay\u00e1huitl", "S. Keizer", "O. Lemon"], "venue": "CoRR, abs/1511.08099,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2015}, {"title": "Hierarchical reinforcement learning for situated natural language generation", "author": ["N. Dethlefs", "H. Cuay\u00e1huitl"], "venue": "Natural Language Engineering, 21(3):391\u2013435,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2015}, {"title": "Random forests for real time 3d face analysis", "author": ["G. Fanelli", "M. Dantone", "J. Gall", "A. Fossati", "L. Gool"], "venue": "Int. J. Comput. Vision, 101(3):437\u2013458, Feb.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2013}, {"title": "Reinforcement learning in robotics: A survey", "author": ["J. Kober", "J.A.D. Bagnell", "J. Peters"], "venue": "Intl Journal of Robotics Research,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2013}, {"title": "Deep learning", "author": ["Y. LeCun", "Y. Bengio", "G. Hinton"], "venue": "Nature, 521(7553):436\u2013444, 05", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2015}, {"title": "End-to-end training of deep visuomotor policies", "author": ["S. Levine", "C. Finn", "T. Darrell", "P. Abbeel"], "venue": "JMLR, 17(1):1334\u20131373, Jan.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2016}, {"title": "A review of verbal and non-verbal human\u2013robot interactive communication", "author": ["N. Mavridis"], "venue": "Robot. Auton. Sys., 63, Part 1:22 \u2013 35,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "Asynchronous methods for deep reinforcement learning", "author": ["V. Mnih", "A.P. Badia", "M. Mirza", "A. Graves", "T.P. Lillicrap", "T. Harley", "D. Silver", "K. Kavukcuoglu"], "venue": "CoRR, abs/1602.01783,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2016}, {"title": "Playing atari with deep reinforcement learning", "author": ["V. Mnih", "K. Kavukcuoglu", "D. Silver", "A. Graves", "I. Antonoglou", "D. Wierstra", "M. Riedmiller"], "venue": "NIPS Deep Learning Workshop.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2013}, {"title": "Human-level control through deep reinforcement learning", "author": ["V. Mnih", "K. Kavukcuoglu", "D. Silver", "A.A. Rusu", "J. Veness", "M.G. Bellemare", "A. Graves", "M. Riedmiller", "A.K. Fidjeland", "G. Ostrovski", "S. Petersen", "C. Beattie", "A. Sadik", "I. Antonoglou", "H. King", "D. Kumaran", "D. Wierstra", "S. Legg", "D. Hassabis"], "venue": "Nature, 518(7540):529\u2013533, 02", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2015}, {"title": "Rectified linear units improve restricted Boltzmann machines", "author": ["V. Nair", "G.E. Hinton"], "venue": "ICML,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2010}, {"title": "Multimodal deep learning", "author": ["J. Ngiam", "A. Khosla", "M. Kim", "J. Nam", "H. Lee", "A.Y. Ng"], "venue": "International Conference on Machine Learning ICML,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2011}, {"title": "Ros: an open-source robot operating system", "author": ["M. Quigley", "K. Conley", "B.P. Gerkey", "J. Faust", "T. Foote", "J. Leibs", "R. Wheeler", "A.Y. Ng"], "venue": "ICRA Workshop on Open Source Software,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2009}, {"title": "Training an artificial neural network to play tic-tac-toe", "author": ["S. Siegel"], "venue": "Technical report, University of Wisconsin-Madison,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2001}, {"title": "Multimodal learning with deep boltzmann machines", "author": ["N. Srivastava", "R. Salakhutdinov"], "venue": "Journal of Machine Learning Research, 15(1),", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2014}, {"title": "Temporal difference learning for the game tic-tac-toe 3d: Applying structure to neural networks", "author": ["M.V.D. Steeg", "M.M. Drugan", "M. Wiering"], "venue": "IEEE Symp. S. on Comp. Intelligence SSCI,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2015}, {"title": "Reinforcement Learning: An Introduction", "author": ["R. Sutton", "A. Barto"], "venue": "MIT Press,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1998}, {"title": "Algorithms for Reinforcement Learning", "author": ["C. Szepesv\u00e1ri"], "venue": "Morgan and Claypool Publishers,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2010}, {"title": "Pragmatic frames for teaching and learning in human\u2013robot interaction: Review and challenges", "author": ["A.-L. Vollmer", "B. Wrede", "K.J. Rohlfing", "P.-Y. Oudeyer"], "venue": "Frontiers in Neurorobotics, 10:10,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2016}, {"title": "Towards multimodal neural robot learning", "author": ["S. Wermter", "C. Weber", "M. Elshaw", "C. Panchev", "H.R. Erwin", "F. Pulverm\u00fcller"], "venue": "Robotics and Autonomous Systems, 47(2-3),", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2004}, {"title": "Towards vision-based deep reinforcement learning for robotic motion control", "author": ["F. Zhang", "J. Leitner", "M. Milford", "B. Upcroft", "P.I. Corke"], "venue": "CoRR, abs/1511.03791,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 9, "context": "While previous work have studied robot learning from real interactions with the environment [10, 8, 25]\u2014mostly without verbal abilities, training an autonomous agent to learn even simple behaviours can take large amounts of experience [14].", "startOffset": 92, "endOffset": 103}, {"referenceID": 7, "context": "While previous work have studied robot learning from real interactions with the environment [10, 8, 25]\u2014mostly without verbal abilities, training an autonomous agent to learn even simple behaviours can take large amounts of experience [14].", "startOffset": 92, "endOffset": 103}, {"referenceID": 24, "context": "While previous work have studied robot learning from real interactions with the environment [10, 8, 25]\u2014mostly without verbal abilities, training an autonomous agent to learn even simple behaviours can take large amounts of experience [14].", "startOffset": 92, "endOffset": 103}, {"referenceID": 13, "context": "While previous work have studied robot learning from real interactions with the environment [10, 8, 25]\u2014mostly without verbal abilities, training an autonomous agent to learn even simple behaviours can take large amounts of experience [14].", "startOffset": 235, "endOffset": 239}, {"referenceID": 23, "context": "Other previous works have addressed multimodal deep learning but in non-conversational settings [24, 16, 19].", "startOffset": 96, "endOffset": 108}, {"referenceID": 15, "context": "Other previous works have addressed multimodal deep learning but in non-conversational settings [24, 16, 19].", "startOffset": 96, "endOffset": 108}, {"referenceID": 18, "context": "Other previous works have addressed multimodal deep learning but in non-conversational settings [24, 16, 19].", "startOffset": 96, "endOffset": 108}, {"referenceID": 8, "context": "The labelling process is defined as h(x) = argmaxy f(x, y), where f is a scoring function using learnt features x derived from a Convolutional neural network [9].", "startOffset": 158, "endOffset": 161}, {"referenceID": 20, "context": "The visual perceptions above plus speech-based perceptions (words with confidence scores) are given as input to a reinforcement learning agent to induce its behaviour from interaction with the environment, where situations are mapped to actions by maximizing a long-term reward signal [21, 22].", "startOffset": 285, "endOffset": 293}, {"referenceID": 21, "context": "The visual perceptions above plus speech-based perceptions (words with confidence scores) are given as input to a reinforcement learning agent to induce its behaviour from interaction with the environment, where situations are mapped to actions by maximizing a long-term reward signal [21, 22].", "startOffset": 285, "endOffset": 293}, {"referenceID": 13, "context": "To induce the Q function above our agent approximates Q using a multilayer neural network as in [14].", "startOffset": 96, "endOffset": 100}, {"referenceID": 12, "context": "This process is implemented in the learning algorithm Deep Q-Learning with Experience Replay described in [13].", "startOffset": 106, "endOffset": 110}, {"referenceID": 6, "context": "(Right) 3D head tracking is used to observe changes in orientation based on patterns detected from the signals produced by [7].", "startOffset": 123, "endOffset": 126}, {"referenceID": 14, "context": "The hidden layers use RELU (Rectified Linear Units) activation functions to normalise their weights, see [15] for details.", "startOffset": 105, "endOffset": 109}, {"referenceID": 16, "context": "To do that we used both off-the-shelf components and components built specifically for our ROS-based [17] integrated system.", "startOffset": 101, "endOffset": 105}, {"referenceID": 6, "context": "To do that we extract patterns from depth-based sensory data using a Kinect sensor and the algorithm described in [7]\u2014see Figure 6 (Right).", "startOffset": 114, "endOffset": 117}, {"referenceID": 3, "context": "Interaction Manager The interaction manager, based on the publicly available SimpleDS tool5 [4], orchestrates the components above by continuously receiving speech-based and vision-based perceptions from the environment, and deciding what to do next and when.", "startOffset": 92, "endOffset": 95}, {"referenceID": 8, "context": "Now, deep supervised learners can be trained from learnt features [9], and deep reinforcement learners can be trained to induce their features and policy jointly [14, 12].", "startOffset": 66, "endOffset": 69}, {"referenceID": 13, "context": "Now, deep supervised learners can be trained from learnt features [9], and deep reinforcement learners can be trained to induce their features and policy jointly [14, 12].", "startOffset": 162, "endOffset": 170}, {"referenceID": 11, "context": "Now, deep supervised learners can be trained from learnt features [9], and deep reinforcement learners can be trained to induce their features and policy jointly [14, 12].", "startOffset": 162, "endOffset": 170}, {"referenceID": 24, "context": "For example, the robot described in [25] was trained to carry out target reaching from pixels, which was successful in simulation but failed when tested in the real environment.", "startOffset": 36, "endOffset": 40}, {"referenceID": 1, "context": "In addition, previous works teaching agents to play noughts and crosses assume perfectly drawn grids and symbols without any multimodal inputs and outputs [2, 18, 20].", "startOffset": 155, "endOffset": 166}, {"referenceID": 17, "context": "In addition, previous works teaching agents to play noughts and crosses assume perfectly drawn grids and symbols without any multimodal inputs and outputs [2, 18, 20].", "startOffset": 155, "endOffset": 166}, {"referenceID": 19, "context": "In addition, previous works teaching agents to play noughts and crosses assume perfectly drawn grids and symbols without any multimodal inputs and outputs [2, 18, 20].", "startOffset": 155, "endOffset": 166}, {"referenceID": 10, "context": "Training robots to perceive, act and communicate using multiple modalities is important to bring them to end users [11, 3].", "startOffset": 115, "endOffset": 122}, {"referenceID": 2, "context": "Training robots to perceive, act and communicate using multiple modalities is important to bring them to end users [11, 3].", "startOffset": 115, "endOffset": 122}, {"referenceID": 5, "context": "[6]) in the training process would contribute towards more natural interactions.", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "It would be interesting to incorporate other forms of learning to train or retrain the robot as it collects data from real interactions [1, 8].", "startOffset": 136, "endOffset": 142}, {"referenceID": 7, "context": "It would be interesting to incorporate other forms of learning to train or retrain the robot as it collects data from real interactions [1, 8].", "startOffset": 136, "endOffset": 142}, {"referenceID": 4, "context": "Sixth, it would be interesting to integrate more complex versions of noughts and crosses and other games using deep learning [5], which represents a niche for investigating more efficient learning.", "startOffset": 125, "endOffset": 128}, {"referenceID": 22, "context": "In addition, our system showed to be dataefficient due to the amount of data used to induce the simulated environment (108 seed images and 10 seed dialogues), which is relevant for trainable robot systems from example demonstrations with end users\u2014as pointed out by [23].", "startOffset": 266, "endOffset": 270}], "year": 2016, "abstractText": "Training robots to perceive, act and communicate using multiple modalities still represents a challenging problem, particularly if robots are expected to learn efficiently from small sets of example interactions. We describe a learning approach as a step in this direction, where we teach a humanoid robot how to play the game of noughts and crosses. Given that multiple multimodal skills can be trained to play this game, we focus our attention to training the robot to perceive the game, and to interact in this game. Our multimodal deep reinforcement learning agent perceives multimodal features and exhibits verbal and non-verbal actions while playing. Experimental results using simulations show that the robot can learn to win or draw up to 98% of the games. A pilot test of the proposed multimodal system for the targeted game\u2014integrating speech, vision and gestures\u2014reports that reasonable and fluent interactions can be achieved using the proposed approach.", "creator": "LaTeX with hyperref package"}}}