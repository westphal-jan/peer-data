{"id": "1611.08669", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-Nov-2016", "title": "Visual Dialog", "abstract": "We introduce the task of Visual Dialog, which requires an AI agent to hold a meaningful dialog with humans in natural, conversational language about visual content. Specifically, given an image, a dialog history, and a question about the image, the agent has to ground the question in image, infer context from history, and answer the question accurately. Visual Dialog is disentangled enough from a specific downstream task so as to serve as a general test of machine intelligence, while being grounded in vision enough to allow objective evaluation of individual responses and benchmark progress. We develop a novel two-person chat data-collection protocol to curate a large-scale Visual Dialog dataset (VisDial). Data collection is underway and on completion, VisDial will contain 1 dialog with 10 question-answer pairs on all ~200k images from COCO, with a total of 2M dialog question-answer pairs.", "histories": [["v1", "Sat, 26 Nov 2016 06:39:28 GMT  (6982kb,D)", "https://arxiv.org/abs/1611.08669v1", "22 pages, 16 figures"], ["v2", "Mon, 5 Dec 2016 02:00:49 GMT  (6982kb,D)", "http://arxiv.org/abs/1611.08669v2", "22 pages, 17 figures, Webpage:this http URL"], ["v3", "Fri, 21 Apr 2017 16:29:55 GMT  (9069kb,D)", "http://arxiv.org/abs/1611.08669v3", "22 pages, 17 figures, Webpage:this http URL"], ["v4", "Mon, 24 Apr 2017 02:10:49 GMT  (9069kb,D)", "http://arxiv.org/abs/1611.08669v4", "23 pages, 18 figures, CVPR 2017 camera-ready, results on VisDial v0.9 dataset, Webpage:this http URL"], ["v5", "Tue, 1 Aug 2017 22:04:37 GMT  (9068kb,D)", "http://arxiv.org/abs/1611.08669v5", "23 pages, 18 figures, CVPR 2017 camera-ready, results on VisDial v0.9 dataset, Webpage:this http URL"]], "COMMENTS": "22 pages, 16 figures", "reviews": [], "SUBJECTS": "cs.CV cs.AI cs.CL cs.LG", "authors": ["abhishek das", "satwik kottur", "khushi gupta", "avi singh", "deshraj yadav", "jos\\'e m f moura", "devi parikh", "dhruv batra"], "accepted": false, "id": "1611.08669"}, "pdf": {"name": "1611.08669.pdf", "metadata": {"source": "CRF", "title": "Visual Dialog", "authors": ["Abhishek Das", "Satwik Kottur", "Khushi Gupta", "Avi Singh", "Deshraj Yadav", "Jos\u00e9 M.F. Moura", "Devi Parikh", "Dhruv Batra"], "emails": ["dbatra}@gatech.edu", "moura}@andrew.cmu.edu", "avisingh@cs.berkeley.edu", "deshraj@vt.edu"], "sections": [{"heading": null, "text": "We present a family of neural encoder decoder models for visual dialogue with 3 encoders - Late Fusion, Hierarchical Recurrent Encoder and Memory Network - and 2 decoders (generative and discriminatory) that exceed a number of complex baselines. We propose a visual dialogue evaluation protocol, in which the AI agent is asked to sort a set of candidate responses and evaluate them using metrics such as the mean value of human response. We quantify the gap between machine and human performance in the Visual Dialogue task via human studies. Taken together, we demonstrate the first \"Visual Chatbot\"! Our dataset, code, trained models and visual chatbot are available at https: / / visualdialog.org."}, {"heading": "1. Introduction", "text": "That is why we are able to put ourselves in a situation in which we are able to plunge ourselves into a crisis, in which we are able, in which we are able, in which we are in and in which we are able to overcome it."}, {"heading": "2. Related Work", "text": "In fact, most of them will be able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move and to move."}, {"heading": "3. The Visual Dialog Dataset (VisDial)", "text": "We describe our VisDial dataset. We start by asking the chat interface and data collection process on AMT highly hidden questions, analyze the dataset, then discuss the evaluation protocol. In line with previous data collection efforts, we collect visual dialogue data about images from the Common Objects in Context (COCO) [32] dataset, which contains multiple objects in everyday scenes. The visual complexity of these images allows engaging and diverse conversations. Live chat interface. Good data for this task should include dialogs that have (1) time continuity, (2) grounding in the image, and (3) mixing natural \"conversation\" exchange. To elicit such responses, we pair 2 workers on AMT to chat with each other in real time (Fig. 3). Each worker was assigned a specific role."}, {"heading": "4. VisDial Dataset Analysis", "text": "We now analyze the v0.9 subset of our VisDial dataset - it contains 1 dialogue (10 QA pairs) on \u0445 123k images of COCO-train / val, a total of 1,232,870 QA pairs."}, {"heading": "4.1. Analyzing VisDial Questions", "text": "A major difference between VisDial and previous image question and answer datasets (VQA [6], Visual 7W [70], Baidu mQA [17]) is the absence of a \"visual priming bias\" in VisDial. In particular, in all previous datasets, subjects saw an image while asking questions about it. As analyzed in [3,19,69], this leads to a particular bias in the questions - people just ask: \"Is there a clock tower on the image that actually contains clock towers?\" This allows linguistic models to function remarkably well on VQA and leads to a bloated sense of progress [19, 69]. As a particularly perverse example of questions in the VQA datasets that start with \"See one.,\" we blindly answer \"Yes,\" without reading the rest of the question or looking at the associated image leads to an average accuracy of 87%!"}, {"heading": "4.2. Analyzing VisDial Answers", "text": "In contrast to previous data sets, the answers in VisDial are longer and more descriptive - mean length 2.9 words (VisDial) vs 1.1 (VQA), 2.0 (Visual 7W), 2.8 (Visual Madlibs). Fig.: 4b shows the cumulative coverage of all answers (yaxis) by the most common answers (x-axis). The difference between VisDial and VQA is strong - the top 1000 answers in VQA coincide with 83% of all answers, while in VisDial this number is only 63%. There is no significant strong tail in VisDial - most long strings are unique, and the coverage in Fig. 4b becomes a straight line with slope 1. Overall, there are 337,527 unique answers in VisDial v0.9."}, {"heading": "4.3. Analyzing VisDial Dialog", "text": "We analyze two quantitative statistics here. Coreference in dialogue. Since the language in VisDial is the result of a sequential conversation, it naturally contains pronouns - \"he,\" \"she,\" \"she,\" \"she,\" \"these,\" \"these,\" etc. A total of 38% of the questions, 19% of the answers, and almost all (98%) dialogues contain at least one pronoun, confirming that a machine has to overcome the ambiguities regarding this task in order to be successful. We find that the use of pronouns in the first round is low (as expected) and then picks up in frequency. A fine-grained perround analysis is available in the supplement. Temporal continuity in dialogue topics."}, {"heading": "4.4. VisDial Evaluation Protocol", "text": "A fundamental challenge in dialog systems is the evaluation. Similar to the state of things in the caption and machine translation, it is an open problem to automatically evaluate the quality of the answers in free form. Existing metrics such as BLEU, METEOR, ROUGE are known to correlate poorly with human judgment when evaluating dialog answers [33]. Instead of evaluating the results of a downstream task [9] or evaluating the entire conversation holistically (as in the target-free chitchat [5]), we evaluate the individual answers in each round (t = 1, 2,., 10) in a retrieval or multiple-choice task. Especially at test times, a VisDial system is provided with an image I, the \"bottom-truth\" dialog history (including the caption) C, (Q1, A1), (Qt \u2212 1, At \u2212 1), a list of N = 100 candidates answers."}, {"heading": "5. Neural Visual Dialog Models", "text": "This year, it is only a matter of time before an agreement is reached."}, {"heading": "6. Experiments", "text": "In fact, most people who are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move and to move."}, {"heading": "7. Conclusions", "text": "In summary, we present a new AI task - the Visual Dialogue, in which an AI agent must have a dialogue with a human about visual content. We develop a novel two-person chat data acquisition protocol to curate a large data set (VisDial), propose an on-demand evaluation protocol, and develop a family of encoder decoder models for Visual Dialogue. We quantify human performance in this task through human studies. Our results suggest that there is considerable room for improvement, and we believe that this task can serve as a testing ground for measuring progress towards visual intelligence."}, {"heading": "8. Acknowledgements", "text": "We thank Harsh Agrawal, Jiasen Lu for helping with AMT data collection; Xiao Lin, Latha Pemula for the model discussions; Marco Baroni, Antoine Bordes, Mike Lewis, Marc'Aurelio Ranzato for helpful discussions; we are grateful to the developers of Torch [2] for building an excellent framework, which has been partially funded by NSF-CAREER prizes to DB and DP, ONR-YIP prizes to DP and DB, ONR scholarships N00014-14-1-0679 to DB, a Sloan scholarship to DP, ARO-YIP prizes to DB and DP, an Allen Distinguished Investigator prize to DP from the Paul G. Allen Family Foundation, ICTAS Junior Faculty prizes to DB and DP, Google Faculty Research Awards to DP and DB, Amazon Academic Research Awards to DP and DB, AWS in Education Research Grant to DB and NVIDIA GPU-DB donations to DB."}, {"heading": "Appendix Overview", "text": "In fact, most of them are able to survive on their own if they do not put themselves in a position to survive on their own."}, {"heading": "B. Qualitative Examples from VisDial", "text": "Fig. 10 shows samples of dialogs from the VisDial dataset."}, {"heading": "C. Human-Machine Comparison", "text": "In fact, most of them are able to survive themselves if they do not see themselves able to survive and survive, \"he told the German Press Agency in an interview with\" Welt am Sonntag. \""}, {"heading": "E. Additional Analysis of VisDial", "text": "In this section, we present additional analyses that characterize our VisDial dataset. (a) Detailed instructions for Amazon Mechanical Turkers on our user interface (b) Left: What the questioner sees; Right: What the respondent see.E.1. Question and answer lengths Fig. 12 show question lengths by type and round. The average length of question by type is consistent across rounds. Questions that begin with \"any\" (\"any person?,\" any other fruit? \"etc.) tend to be shortest. Fig. 13 shows answer lengths by type of question they were said as an answer to and in round. In contrast to questions, there are significant differences in answer lengths. Answers to binary questions (\" any people?, \"\" can you see the dog? \"etc.) tend to be short, while answers to\" how \"and\" which \"questions tend to be more explanatory and long.\""}, {"heading": "F. Performance on VisDial v0.5", "text": "Tab. 5 shows the results for our proposed models and baselines on VisDial v0.5. A few important takeaway examples: First, as expected, all learning-based models perform significantly better than non-learning baselines. Second, all discriminatory models perform significantly better than generative models, which, as we discussed, can be tailored to the distortions in response options. This improvement comes with the significant caveat that they cannot generate reactions, and we recommend that the two decoders be considered as separate use cases. Third, our best generative and discriminatory models are MN-QIH-G with 0.44 MRR and MN-QIH-D with 0.53 MRR, which exceed a number of models and complex baselines. Fourth, we observe that models with H perform better than Q-only models, highlighting the importance of the story in VisDial. Fifth, models that look at MIH, both blind models (Q by at least 2% H Recoder in both)."}, {"heading": "G. Experimental Details", "text": "This year, the number of unemployed people slipping into unemployment has doubled in recent years, and the number of unemployed people slipping into unemployment has doubled in the last ten years."}], "references": [{"title": "Analyzing the Behavior of Visual Question Answering Models", "author": ["A. Agrawal", "D. Batra", "D. Parikh"], "venue": "EMNLP", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2016}, {"title": "Sort story: Sorting jumbled images and captions into stories", "author": ["H. Agrawal", "A. Chandrasekaran", "D. Batra", "D. Parikh", "M. Bansal"], "venue": "EMNLP", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2016}, {"title": "VQA: Visual Question Answering", "author": ["S. Antol", "A. Agrawal", "J. Lu", "M. Mitchell", "D. Batra", "C.L. Zitnick", "D. Parikh"], "venue": "ICCV", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2015}, {"title": "VizWiz: Nearly Real-time Answers to Visual Questions", "author": ["J.P. Bigham", "C. Jayant", "H. Ji", "G. Little", "A. Miller", "R.C. Miller", "R. Miller", "A. Tatarowicz", "B. White", "S. White", "T. Yeh"], "venue": "UIST", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2010}, {"title": "Largescale Simple Question Answering with Memory Networks", "author": ["A. Bordes", "N. Usunier", "S. Chopra", "J. Weston"], "venue": "arXiv preprint arXiv:1506.02075", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning End-to-End Goal- Oriented Dialog", "author": ["A. Bordes", "J. Weston"], "venue": "arXiv preprint arXiv:1605.07683", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2016}, {"title": "Resolving language and vision ambiguities together: Joint segmentation and prepositional attachment resolution in captioned scenes", "author": ["G. Christie", "A. Laddha", "A. Agrawal", "S. Antol", "Y. Goyal", "K. Kochersberger", "D. Batra"], "venue": "EMNLP", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2016}, {"title": "Chameleons in imagined conversations: A new approach to understanding coordination of linguistic style in dialogs", "author": ["C. Danescu-Niculescu-Mizil", "L. Lee"], "venue": "Proceedings of the Workshop on Cognitive Modeling and Computational Linguistics, ACL 2011", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2011}, {"title": "and D", "author": ["A. Das", "H. Agrawal", "C.L. Zitnick", "D. Parikh"], "venue": "Batra. Human Attention in Visual Question Answering: Do Humans and Deep Networks Look at the Same Regions? In EMNLP", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2016}, {"title": "GuessWhat?! Visual object discovery through multi-modal dialogue", "author": ["H. de Vries", "F. Strub", "S. Chandar", "O. Pietquin", "H. Larochelle", "A.C. Courville"], "venue": "In CVPR,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2017}, {"title": "Evaluating Prerequisite Qualities for Learning End-to-End Dialog Systems", "author": ["J. Dodge", "A. Gane", "X. Zhang", "A. Bordes", "S. Chopra", "A. Miller", "A. Szlam", "J. Weston"], "venue": "ICLR", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2016}, {"title": "Long-term Recurrent Convolutional Networks for Visual Recognition and Description", "author": ["J. Donahue", "L.A. Hendricks", "S. Guadarrama", "M. Rohrbach", "S. Venugopalan", "K. Saenko", "T. Darrell"], "venue": "CVPR", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2015}, {"title": "From Captions to Visual Concepts and Back", "author": ["H. Fang", "S. Gupta", "F.N. Iandola", "R.K. Srivastava", "L. Deng", "P. Doll\u00e1r", "J. Gao", "X. He", "M. Mitchell", "J.C. Platt", "C.L. Zitnick", "G. Zweig"], "venue": "CVPR", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}, {"title": "20 Figure 18: Selected examples of attention over history facts from our Memory Network encoder", "author": ["H. Gao", "J. Mao", "J. Zhou", "Z. Huang", "L. Wang", "W. Xu"], "venue": "The intensity of color in each row indicates the strength of attention placed on that round by the model. Are You Talking to a Machine? Dataset and Methods for Multilingual Image Question Answering. In NIPS", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2015}, {"title": "A Visual Turing Test for Computer Vision Systems", "author": ["D. Geman", "S. Geman", "N. Hallonquist", "L. Younes"], "venue": "PNAS", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2014}, {"title": "Making the v in vqa matter: Elevating the role of image understanding in visual question answering", "author": ["Y. Goyal", "T. Khot", "D. Summers-Stay", "D. Batra", "D. Parikh"], "venue": "CVPR", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2017}, {"title": "Deep Residual Learning for Image Recognition", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "CVPR", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2016}, {"title": "Teaching machines to read and comprehend", "author": ["K.M. Hermann", "T. Kocisky", "E. Grefenstette", "L. Espeholt", "W. Kay", "M. Suleyman", "P. Blunsom"], "venue": "NIPS", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2015}, {"title": "Segmentation from natural language expressions", "author": ["R. Hu", "M. Rohrbach", "T. Darrell"], "venue": "ECCV", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2016}, {"title": "Visual storytelling", "author": ["T.-H. Huang", "F. Ferraro", "N. Mostafazadeh", "I. Misra", "A. Agrawal", "J. Devlin", "R. Girshick", "X. He", "P. Kohli", "D. Batra", "L. Zitnick", "D. Parikh", "L. Vanderwende", "M. Galley", "M. Mitchell"], "venue": "NAACL HLT", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2016}, {"title": "Oriol Vinyals", "author": ["Q.V.L. Ilya Sutskever"], "venue": "Sequence to Sequence Learning with Neural Networks. In NIPS", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2014}, {"title": "and L", "author": ["A. Jabri", "A. Joulin"], "venue": "van der Maaten. Revisiting visual question answering baselines. In ECCV", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2016}, {"title": "et al", "author": ["A. Kannan", "K. Kurach", "S. Ravi", "T. Kaufmann", "A. Tomkins", "B. Miklos", "G. Corrado", "L. Luk\u00e1cs", "M. Ganea", "P. Young"], "venue": "Smart Reply: Automated Response Suggestion for Email. In KDD", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2016}, {"title": "Deep visual-semantic alignments for generating image descriptions", "author": ["A. Karpathy", "L. Fei-Fei"], "venue": "CVPR", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2015}, {"title": "Adam: A Method for Stochastic Optimization", "author": ["D. Kingma", "J. Ba"], "venue": "ICLR", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2015}, {"title": "What are you talking about? text-to-image coreference", "author": ["C. Kong", "D. Lin", "M. Bansal", "R. Urtasun", "S. Fidler"], "venue": "CVPR", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2014}, {"title": "An ISU dialogue system exhibiting reinforcement learning of dialogue policies: generic slot-filling in the TALK in-car system", "author": ["O. Lemon", "K. Georgila", "J. Henderson", "M. Stuttle"], "venue": "EACL", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2006}, {"title": "Deep Reinforcement Learning for Dialogue Generation", "author": ["J. Li", "W. Monroe", "A. Ritter", "M. Galley", "J. Gao", "D. Jurafsky"], "venue": "EMNLP", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2016}, {"title": "P", "author": ["T.-Y. Lin", "M. Maire", "S. Belongie", "J. Hays", "P. Perona", "D. Ramanan"], "venue": "Doll\u00c3\u00a1r, and C. L. Zitnick. Microsoft COCO: Common Objects in Context. In ECCV", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2014}, {"title": "How NOT To Evaluate Your Dialogue System: An Empirical Study of Unsupervised Evaluation Metrics for Dialogue Response Generation", "author": ["C.-W. Liu", "R. Lowe", "I.V. Serban", "M. Noseworthy", "L. Charlin", "J. Pineau"], "venue": "EMNLP", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2016}, {"title": "SSD: Single Shot MultiBox Detector", "author": ["W. Liu", "D. Anguelov", "D. Erhan", "C. Szegedy", "S. Reed", "C.-Y. Fu", "A.C. Berg"], "venue": "ECCV", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2016}, {"title": "The Ubuntu Dialogue Corpus: A Large Dataset for Research in Unstructured Multi-Turn Dialogue Systems", "author": ["R. Lowe", "N. Pow", "I. Serban", "J. Pineau"], "venue": "SIGDIAL", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2015}, {"title": "Deeper LSTM and Normalized CNN Visual Question Answering model", "author": ["J. Lu", "X. Lin", "D. Batra", "D. Parikh"], "venue": "https://github.com/VT-vision-lab/ VQA_LSTM_CNN", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2015}, {"title": "Hierarchical Question-Image Co-Attention for Visual Question Answering", "author": ["J. Lu", "J. Yang", "D. Batra", "D. Parikh"], "venue": "NIPS", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2016}, {"title": "A Multi-World Approach to Question Answering about Real-World Scenes based on Un-  certain Input", "author": ["M. Malinowski", "M. Fritz"], "venue": "NIPS", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2014}, {"title": "Ask your neurons: A neural-based approach to answering questions about images", "author": ["M. Malinowski", "M. Rohrbach", "M. Fritz"], "venue": "ICCV", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2015}, {"title": "Listen", "author": ["H. Mei", "M. Bansal", "M.R. Walter"], "venue": "attend, and walk: Neural mapping of navigational instructions to action sequences. In AAAI", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2016}, {"title": "Human-level control through deep reinforcement learning", "author": ["V. Mnih", "K. Kavukcuoglu", "D. Silver", "A.A. Rusu", "J. Veness", "M.G. Bellemare", "A. Graves", "M. Riedmiller", "A.K. Fidjeland", "G. Ostrovski", "S. Petersen", "C. Beattie", "A. Sadik", "I. Antonoglou", "H. King", "D. Kumaran", "D. Wierstra", "S. Legg", "D. Hassabis"], "venue": "Nature, 518(7540):529\u2013533,", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2015}, {"title": "Image-Grounded Conversations: Multimodal Context for Natural Question and Response Generation", "author": ["N. Mostafazadeh", "C. Brockett", "B. Dolan", "M. Galley", "J. Gao", "G.P. Spithourakis", "L. Vanderwende"], "venue": "arXiv preprint arXiv:1701.08251", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2017}, {"title": "Empirical methods for evaluating dialog systems", "author": ["T. Paek"], "venue": "Proceedings of the workshop on Evaluation for Language and Dialogue Systems-Volume 9", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2001}, {"title": "Flickr30k entities: Collecting region-to-phrase correspondences for richer imageto-sentence models", "author": ["B.A. Plummer", "L. Wang", "C.M. Cervantes", "J.C. Caicedo", "J. Hockenmaier", "S. Lazebnik"], "venue": "ICCV", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2015}, {"title": "and P", "author": ["P. Rajpurkar", "J. Zhang", "K. Lopyrev"], "venue": "Liang. SQuAD: 100,000+ Questions for Machine Comprehension of Text. In EMNLP", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2016}, {"title": "Linking people with \"their\" names using coreference resolution", "author": ["V. Ramanathan", "A. Joulin", "P. Liang", "L. Fei-Fei"], "venue": "ECCV", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2014}, {"title": "Question Relevance in VQA: Identifying Non-Visual And False-Premise Questions", "author": ["A. Ray", "G. Christie", "M. Bansal", "D. Batra", "D. Parikh"], "venue": "EMNLP", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2016}, {"title": "Exploring Models and Data for Image Question Answering", "author": ["M. Ren", "R. Kiros", "R. Zemel"], "venue": "NIPS", "citeRegEx": "49", "shortCiteRegEx": null, "year": 2015}, {"title": "Grounding of textual phrases in images by reconstruction", "author": ["A. Rohrbach", "M. Rohrbach", "R. Hu", "T. Darrell", "B. Schiele"], "venue": "ECCV", "citeRegEx": "50", "shortCiteRegEx": null, "year": 2016}, {"title": "A dataset for movie description", "author": ["A. Rohrbach", "M. Rohrbach", "N. Tandon", "B. Schiele"], "venue": "CVPR", "citeRegEx": "51", "shortCiteRegEx": null, "year": 2015}, {"title": "Generating Factoid Questions With Recurrent Neural Networks: The 30M Factoid Question-Answer Corpus", "author": ["I.V. Serban", "A. Garc\u00eda-Dur\u00e1n", "\u00c7. G\u00fcl\u00e7ehre", "S. Ahn", "S. Chandar", "A.C. Courville", "Y. Bengio"], "venue": "ACL", "citeRegEx": "52", "shortCiteRegEx": null, "year": 2016}, {"title": "Building End-To-End Dialogue Systems Using Generative Hierarchical Neural Network Models", "author": ["I.V. Serban", "A. Sordoni", "Y. Bengio", "A. Courville", "J. Pineau"], "venue": "AAAI", "citeRegEx": "53", "shortCiteRegEx": null, "year": 2016}, {"title": "A Hierarchical Latent Variable Encoder-Decoder Model for Generating Dialogues", "author": ["I.V. Serban", "A. Sordoni", "R. Lowe", "L. Charlin", "J. Pineau", "A. Courville", "Y. Bengio"], "venue": "arXiv preprint arXiv:1605.06069", "citeRegEx": "54", "shortCiteRegEx": null, "year": 2016}, {"title": "G", "author": ["D. Silver", "A. Huang", "C.J. Maddison", "A. Guez", "L. Sifre"], "venue": "Van Den Driessche, J. Schrittwieser, I. Antonoglou, 22  V. Panneershelvam, M. Lanctot, et al. Mastering the game of Go with deep neural networks and tree search. Nature, 529(7587):484\u2013489", "citeRegEx": "55", "shortCiteRegEx": null, "year": 2016}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "ICLR", "citeRegEx": "56", "shortCiteRegEx": null, "year": 2015}, {"title": "MovieQA: Understanding Stories in Movies through Question-Answering", "author": ["M. Tapaswi", "Y. Zhu", "R. Stiefelhagen", "A. Torralba", "R. Urtasun", "S. Fidler"], "venue": "CVPR", "citeRegEx": "57", "shortCiteRegEx": null, "year": 2016}, {"title": "Joint Video and Text Parsing for Understanding Events and Answering Queries", "author": ["K. Tu", "M. Meng", "M.W. Lee", "T.E. Choe", "S.C. Zhu"], "venue": "IEEE MultiMedia", "citeRegEx": "58", "shortCiteRegEx": null, "year": 2014}, {"title": "Sequence to Sequence - Video to Text", "author": ["S. Venugopalan", "M. Rohrbach", "J. Donahue", "R.J. Mooney", "T. Darrell", "K. Saenko"], "venue": "ICCV", "citeRegEx": "59", "shortCiteRegEx": null, "year": 2015}, {"title": "Translating Videos to Natural Language Using Deep Recurrent Neural Networks", "author": ["S. Venugopalan", "H. Xu", "J. Donahue", "M. Rohrbach", "R.J. Mooney", "K. Saenko"], "venue": "NAACL HLT", "citeRegEx": "60", "shortCiteRegEx": null, "year": 2015}, {"title": "A Neural Conversational Model", "author": ["O. Vinyals", "Q. Le"], "venue": "arXiv preprint arXiv:1506.05869", "citeRegEx": "61", "shortCiteRegEx": null, "year": 2015}, {"title": "Show and tell: A neural image caption generator", "author": ["O. Vinyals", "A. Toshev", "S. Bengio", "D. Erhan"], "venue": "CVPR", "citeRegEx": "62", "shortCiteRegEx": null, "year": 2015}, {"title": "Knowledge Guided Disambiguation for Large-Scale Scene Classification with Multi-Resolution CNNs", "author": ["L. Wang", "S. Guo", "W. Huang", "Y. Xiong", "Y. Qiao"], "venue": "arXiv preprint arXiv:1610.01119", "citeRegEx": "63", "shortCiteRegEx": null, "year": 2016}, {"title": "Towards AI-Complete Question Answering: A Set of Prerequisite Toy Tasks", "author": ["J. Weston", "A. Bordes", "S. Chopra", "T. Mikolov"], "venue": "ICLR", "citeRegEx": "65", "shortCiteRegEx": null, "year": 2016}, {"title": "Using Artificial Intelligence to Help Blind People \u2018See\u2019 Facebook", "author": ["S. Wu", "H. Pique", "J. Wieland"], "venue": "http://newsroom.fb.com/news/2016/04/using-artificialintelligence-to-help-blind-people-see-facebook/", "citeRegEx": "66", "shortCiteRegEx": null, "year": 2016}, {"title": "Stacked Attention Networks for Image Question Answering", "author": ["Z. Yang", "X. He", "J. Gao", "L. Deng", "A.J. Smola"], "venue": "CVPR", "citeRegEx": "67", "shortCiteRegEx": null, "year": 2016}, {"title": "Visual Madlibs: Fill in the blank Image Generation and Question Answering", "author": ["L. Yu", "E. Park", "A.C. Berg", "T.L. Berg"], "venue": "ICCV", "citeRegEx": "68", "shortCiteRegEx": null, "year": 2015}, {"title": "Yin and Yang: Balancing and Answering Binary Visual Questions", "author": ["P. Zhang", "Y. Goyal", "D. Summers-Stay", "D. Batra", "D. Parikh"], "venue": "CVPR", "citeRegEx": "69", "shortCiteRegEx": null, "year": 2016}, {"title": "Visual7W: Grounded Question Answering in Images", "author": ["Y. Zhu", "O. Groth", "M. Bernstein", "L. Fei-Fei"], "venue": "CVPR", "citeRegEx": "70", "shortCiteRegEx": null, "year": 2016}, {"title": "Measuring machine intelligence through visual question answering", "author": ["C.L. Zitnick", "A. Agrawal", "S. Antol", "M. Mitchell", "D. Batra", "D. Parikh"], "venue": "AI Magazine", "citeRegEx": "71", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 16, "context": "We are witnessing unprecedented advances in computer vision (CV) and artificial intelligence (AI) \u2013 from \u2018low-level\u2019 AI tasks such as image classification [20], scene recogni-", "startOffset": 155, "endOffset": 159}, {"referenceID": 58, "context": "tion [63], object detection [34] \u2013 to \u2018high-level\u2019 AI tasks such as learning to play Atari video games [42] and Go [55], answering reading comprehension questions by understanding short stories [21, 65], and even answering questions about images [6, 39, 49, 71] and videos [57, 58]!", "startOffset": 5, "endOffset": 9}, {"referenceID": 30, "context": "tion [63], object detection [34] \u2013 to \u2018high-level\u2019 AI tasks such as learning to play Atari video games [42] and Go [55], answering reading comprehension questions by understanding short stories [21, 65], and even answering questions about images [6, 39, 49, 71] and videos [57, 58]!", "startOffset": 28, "endOffset": 32}, {"referenceID": 37, "context": "tion [63], object detection [34] \u2013 to \u2018high-level\u2019 AI tasks such as learning to play Atari video games [42] and Go [55], answering reading comprehension questions by understanding short stories [21, 65], and even answering questions about images [6, 39, 49, 71] and videos [57, 58]!", "startOffset": 103, "endOffset": 107}, {"referenceID": 50, "context": "tion [63], object detection [34] \u2013 to \u2018high-level\u2019 AI tasks such as learning to play Atari video games [42] and Go [55], answering reading comprehension questions by understanding short stories [21, 65], and even answering questions about images [6, 39, 49, 71] and videos [57, 58]!", "startOffset": 115, "endOffset": 119}, {"referenceID": 17, "context": "tion [63], object detection [34] \u2013 to \u2018high-level\u2019 AI tasks such as learning to play Atari video games [42] and Go [55], answering reading comprehension questions by understanding short stories [21, 65], and even answering questions about images [6, 39, 49, 71] and videos [57, 58]!", "startOffset": 194, "endOffset": 202}, {"referenceID": 59, "context": "tion [63], object detection [34] \u2013 to \u2018high-level\u2019 AI tasks such as learning to play Atari video games [42] and Go [55], answering reading comprehension questions by understanding short stories [21, 65], and even answering questions about images [6, 39, 49, 71] and videos [57, 58]!", "startOffset": 194, "endOffset": 202}, {"referenceID": 2, "context": "tion [63], object detection [34] \u2013 to \u2018high-level\u2019 AI tasks such as learning to play Atari video games [42] and Go [55], answering reading comprehension questions by understanding short stories [21, 65], and even answering questions about images [6, 39, 49, 71] and videos [57, 58]!", "startOffset": 246, "endOffset": 261}, {"referenceID": 35, "context": "tion [63], object detection [34] \u2013 to \u2018high-level\u2019 AI tasks such as learning to play Atari video games [42] and Go [55], answering reading comprehension questions by understanding short stories [21, 65], and even answering questions about images [6, 39, 49, 71] and videos [57, 58]!", "startOffset": 246, "endOffset": 261}, {"referenceID": 44, "context": "tion [63], object detection [34] \u2013 to \u2018high-level\u2019 AI tasks such as learning to play Atari video games [42] and Go [55], answering reading comprehension questions by understanding short stories [21, 65], and even answering questions about images [6, 39, 49, 71] and videos [57, 58]!", "startOffset": 246, "endOffset": 261}, {"referenceID": 65, "context": "tion [63], object detection [34] \u2013 to \u2018high-level\u2019 AI tasks such as learning to play Atari video games [42] and Go [55], answering reading comprehension questions by understanding short stories [21, 65], and even answering questions about images [6, 39, 49, 71] and videos [57, 58]!", "startOffset": 246, "endOffset": 261}, {"referenceID": 52, "context": "tion [63], object detection [34] \u2013 to \u2018high-level\u2019 AI tasks such as learning to play Atari video games [42] and Go [55], answering reading comprehension questions by understanding short stories [21, 65], and even answering questions about images [6, 39, 49, 71] and videos [57, 58]!", "startOffset": 273, "endOffset": 281}, {"referenceID": 53, "context": "tion [63], object detection [34] \u2013 to \u2018high-level\u2019 AI tasks such as learning to play Atari video games [42] and Go [55], answering reading comprehension questions by understanding short stories [21, 65], and even answering questions about images [6, 39, 49, 71] and videos [57, 58]!", "startOffset": 273, "endOffset": 281}, {"referenceID": 3, "context": "\u2022 Aiding visually impaired users in understanding their surroundings [7] or social media content [66] (AI: \u2018John just uploaded a picture from his vacation in Hawaii\u2019, Human: \u2018Great, is he at the beach?\u2019, AI: \u2018No, on a mountain\u2019).", "startOffset": 69, "endOffset": 72}, {"referenceID": 60, "context": "\u2022 Aiding visually impaired users in understanding their surroundings [7] or social media content [66] (AI: \u2018John just uploaded a picture from his vacation in Hawaii\u2019, Human: \u2018Great, is he at the beach?\u2019, AI: \u2018No, on a mountain\u2019).", "startOffset": 97, "endOffset": 101}, {"referenceID": 36, "context": "search and rescue missions) where the operator may be \u2018situationally blind\u2019 and operating via language [40] (Human: \u2018Is there smoke in any room around you?\u2019, AI: \u2018Yes, in one room\u2019, Human: \u2018Go there and look for people\u2019).", "startOffset": 103, "endOffset": 107}, {"referenceID": 10, "context": "Goal-driven dialog is typically evaluated on task-completion rate (how frequently was the user able to book their flight) or time to task completion [14,44] \u2013 clearly, the shorter the dialog the better.", "startOffset": 149, "endOffset": 156}, {"referenceID": 39, "context": "Goal-driven dialog is typically evaluated on task-completion rate (how frequently was the user able to book their flight) or time to task completion [14,44] \u2013 clearly, the shorter the dialog the better.", "startOffset": 149, "endOffset": 156}, {"referenceID": 26, "context": "The former discourages taskengineered bots for \u2018slot filling\u2019 [30] and the latter discourages bots that put on a personality to avoid answering questions while keeping the user engaged [64].", "startOffset": 62, "endOffset": 66}, {"referenceID": 28, "context": "Upon completion1, VisDial will contain 1 dialog each (with 10 question-answer pairs) on \u223c140k images from the COCO dataset [32], for a total of \u223c1.", "startOffset": 123, "endOffset": 127}, {"referenceID": 2, "context": "When compared to VQA [6], VisDial studies a significantly richer task (dialog), overcomes a \u2018visual priming bias\u2019 in VQA (in VisDial, the questioner does not see the image), contains free-form longer answers, and is an order of magnitude larger.", "startOffset": 21, "endOffset": 24}, {"referenceID": 11, "context": "A number of problems at the intersection of vision and language have recently gained prominence \u2013 image captioning [15, 16, 27, 62], video/movie description [51, 59, 60], text-to-image coreference/grounding [10, 22, 29, 45, 47, 50], visual storytelling [4, 23], and of course, visual question answering (VQA) [3, 6, 12, 17, 19, 37\u201339, 49, 69].", "startOffset": 115, "endOffset": 131}, {"referenceID": 12, "context": "A number of problems at the intersection of vision and language have recently gained prominence \u2013 image captioning [15, 16, 27, 62], video/movie description [51, 59, 60], text-to-image coreference/grounding [10, 22, 29, 45, 47, 50], visual storytelling [4, 23], and of course, visual question answering (VQA) [3, 6, 12, 17, 19, 37\u201339, 49, 69].", "startOffset": 115, "endOffset": 131}, {"referenceID": 23, "context": "A number of problems at the intersection of vision and language have recently gained prominence \u2013 image captioning [15, 16, 27, 62], video/movie description [51, 59, 60], text-to-image coreference/grounding [10, 22, 29, 45, 47, 50], visual storytelling [4, 23], and of course, visual question answering (VQA) [3, 6, 12, 17, 19, 37\u201339, 49, 69].", "startOffset": 115, "endOffset": 131}, {"referenceID": 57, "context": "A number of problems at the intersection of vision and language have recently gained prominence \u2013 image captioning [15, 16, 27, 62], video/movie description [51, 59, 60], text-to-image coreference/grounding [10, 22, 29, 45, 47, 50], visual storytelling [4, 23], and of course, visual question answering (VQA) [3, 6, 12, 17, 19, 37\u201339, 49, 69].", "startOffset": 115, "endOffset": 131}, {"referenceID": 46, "context": "A number of problems at the intersection of vision and language have recently gained prominence \u2013 image captioning [15, 16, 27, 62], video/movie description [51, 59, 60], text-to-image coreference/grounding [10, 22, 29, 45, 47, 50], visual storytelling [4, 23], and of course, visual question answering (VQA) [3, 6, 12, 17, 19, 37\u201339, 49, 69].", "startOffset": 157, "endOffset": 169}, {"referenceID": 54, "context": "A number of problems at the intersection of vision and language have recently gained prominence \u2013 image captioning [15, 16, 27, 62], video/movie description [51, 59, 60], text-to-image coreference/grounding [10, 22, 29, 45, 47, 50], visual storytelling [4, 23], and of course, visual question answering (VQA) [3, 6, 12, 17, 19, 37\u201339, 49, 69].", "startOffset": 157, "endOffset": 169}, {"referenceID": 55, "context": "A number of problems at the intersection of vision and language have recently gained prominence \u2013 image captioning [15, 16, 27, 62], video/movie description [51, 59, 60], text-to-image coreference/grounding [10, 22, 29, 45, 47, 50], visual storytelling [4, 23], and of course, visual question answering (VQA) [3, 6, 12, 17, 19, 37\u201339, 49, 69].", "startOffset": 157, "endOffset": 169}, {"referenceID": 6, "context": "A number of problems at the intersection of vision and language have recently gained prominence \u2013 image captioning [15, 16, 27, 62], video/movie description [51, 59, 60], text-to-image coreference/grounding [10, 22, 29, 45, 47, 50], visual storytelling [4, 23], and of course, visual question answering (VQA) [3, 6, 12, 17, 19, 37\u201339, 49, 69].", "startOffset": 207, "endOffset": 231}, {"referenceID": 18, "context": "A number of problems at the intersection of vision and language have recently gained prominence \u2013 image captioning [15, 16, 27, 62], video/movie description [51, 59, 60], text-to-image coreference/grounding [10, 22, 29, 45, 47, 50], visual storytelling [4, 23], and of course, visual question answering (VQA) [3, 6, 12, 17, 19, 37\u201339, 49, 69].", "startOffset": 207, "endOffset": 231}, {"referenceID": 25, "context": "A number of problems at the intersection of vision and language have recently gained prominence \u2013 image captioning [15, 16, 27, 62], video/movie description [51, 59, 60], text-to-image coreference/grounding [10, 22, 29, 45, 47, 50], visual storytelling [4, 23], and of course, visual question answering (VQA) [3, 6, 12, 17, 19, 37\u201339, 49, 69].", "startOffset": 207, "endOffset": 231}, {"referenceID": 40, "context": "A number of problems at the intersection of vision and language have recently gained prominence \u2013 image captioning [15, 16, 27, 62], video/movie description [51, 59, 60], text-to-image coreference/grounding [10, 22, 29, 45, 47, 50], visual storytelling [4, 23], and of course, visual question answering (VQA) [3, 6, 12, 17, 19, 37\u201339, 49, 69].", "startOffset": 207, "endOffset": 231}, {"referenceID": 42, "context": "A number of problems at the intersection of vision and language have recently gained prominence \u2013 image captioning [15, 16, 27, 62], video/movie description [51, 59, 60], text-to-image coreference/grounding [10, 22, 29, 45, 47, 50], visual storytelling [4, 23], and of course, visual question answering (VQA) [3, 6, 12, 17, 19, 37\u201339, 49, 69].", "startOffset": 207, "endOffset": 231}, {"referenceID": 45, "context": "A number of problems at the intersection of vision and language have recently gained prominence \u2013 image captioning [15, 16, 27, 62], video/movie description [51, 59, 60], text-to-image coreference/grounding [10, 22, 29, 45, 47, 50], visual storytelling [4, 23], and of course, visual question answering (VQA) [3, 6, 12, 17, 19, 37\u201339, 49, 69].", "startOffset": 207, "endOffset": 231}, {"referenceID": 1, "context": "A number of problems at the intersection of vision and language have recently gained prominence \u2013 image captioning [15, 16, 27, 62], video/movie description [51, 59, 60], text-to-image coreference/grounding [10, 22, 29, 45, 47, 50], visual storytelling [4, 23], and of course, visual question answering (VQA) [3, 6, 12, 17, 19, 37\u201339, 49, 69].", "startOffset": 253, "endOffset": 260}, {"referenceID": 19, "context": "A number of problems at the intersection of vision and language have recently gained prominence \u2013 image captioning [15, 16, 27, 62], video/movie description [51, 59, 60], text-to-image coreference/grounding [10, 22, 29, 45, 47, 50], visual storytelling [4, 23], and of course, visual question answering (VQA) [3, 6, 12, 17, 19, 37\u201339, 49, 69].", "startOffset": 253, "endOffset": 260}, {"referenceID": 0, "context": "A number of problems at the intersection of vision and language have recently gained prominence \u2013 image captioning [15, 16, 27, 62], video/movie description [51, 59, 60], text-to-image coreference/grounding [10, 22, 29, 45, 47, 50], visual storytelling [4, 23], and of course, visual question answering (VQA) [3, 6, 12, 17, 19, 37\u201339, 49, 69].", "startOffset": 309, "endOffset": 342}, {"referenceID": 2, "context": "A number of problems at the intersection of vision and language have recently gained prominence \u2013 image captioning [15, 16, 27, 62], video/movie description [51, 59, 60], text-to-image coreference/grounding [10, 22, 29, 45, 47, 50], visual storytelling [4, 23], and of course, visual question answering (VQA) [3, 6, 12, 17, 19, 37\u201339, 49, 69].", "startOffset": 309, "endOffset": 342}, {"referenceID": 8, "context": "A number of problems at the intersection of vision and language have recently gained prominence \u2013 image captioning [15, 16, 27, 62], video/movie description [51, 59, 60], text-to-image coreference/grounding [10, 22, 29, 45, 47, 50], visual storytelling [4, 23], and of course, visual question answering (VQA) [3, 6, 12, 17, 19, 37\u201339, 49, 69].", "startOffset": 309, "endOffset": 342}, {"referenceID": 13, "context": "A number of problems at the intersection of vision and language have recently gained prominence \u2013 image captioning [15, 16, 27, 62], video/movie description [51, 59, 60], text-to-image coreference/grounding [10, 22, 29, 45, 47, 50], visual storytelling [4, 23], and of course, visual question answering (VQA) [3, 6, 12, 17, 19, 37\u201339, 49, 69].", "startOffset": 309, "endOffset": 342}, {"referenceID": 15, "context": "A number of problems at the intersection of vision and language have recently gained prominence \u2013 image captioning [15, 16, 27, 62], video/movie description [51, 59, 60], text-to-image coreference/grounding [10, 22, 29, 45, 47, 50], visual storytelling [4, 23], and of course, visual question answering (VQA) [3, 6, 12, 17, 19, 37\u201339, 49, 69].", "startOffset": 309, "endOffset": 342}, {"referenceID": 33, "context": "A number of problems at the intersection of vision and language have recently gained prominence \u2013 image captioning [15, 16, 27, 62], video/movie description [51, 59, 60], text-to-image coreference/grounding [10, 22, 29, 45, 47, 50], visual storytelling [4, 23], and of course, visual question answering (VQA) [3, 6, 12, 17, 19, 37\u201339, 49, 69].", "startOffset": 309, "endOffset": 342}, {"referenceID": 34, "context": "A number of problems at the intersection of vision and language have recently gained prominence \u2013 image captioning [15, 16, 27, 62], video/movie description [51, 59, 60], text-to-image coreference/grounding [10, 22, 29, 45, 47, 50], visual storytelling [4, 23], and of course, visual question answering (VQA) [3, 6, 12, 17, 19, 37\u201339, 49, 69].", "startOffset": 309, "endOffset": 342}, {"referenceID": 35, "context": "A number of problems at the intersection of vision and language have recently gained prominence \u2013 image captioning [15, 16, 27, 62], video/movie description [51, 59, 60], text-to-image coreference/grounding [10, 22, 29, 45, 47, 50], visual storytelling [4, 23], and of course, visual question answering (VQA) [3, 6, 12, 17, 19, 37\u201339, 49, 69].", "startOffset": 309, "endOffset": 342}, {"referenceID": 44, "context": "A number of problems at the intersection of vision and language have recently gained prominence \u2013 image captioning [15, 16, 27, 62], video/movie description [51, 59, 60], text-to-image coreference/grounding [10, 22, 29, 45, 47, 50], visual storytelling [4, 23], and of course, visual question answering (VQA) [3, 6, 12, 17, 19, 37\u201339, 49, 69].", "startOffset": 309, "endOffset": 342}, {"referenceID": 63, "context": "A number of problems at the intersection of vision and language have recently gained prominence \u2013 image captioning [15, 16, 27, 62], video/movie description [51, 59, 60], text-to-image coreference/grounding [10, 22, 29, 45, 47, 50], visual storytelling [4, 23], and of course, visual question answering (VQA) [3, 6, 12, 17, 19, 37\u201339, 49, 69].", "startOffset": 309, "endOffset": 342}, {"referenceID": 9, "context": "Concurrent with our work, two recent works [13, 43] have also begun studying visually-grounded dialog.", "startOffset": 43, "endOffset": 51}, {"referenceID": 38, "context": "Concurrent with our work, two recent works [13, 43] have also begun studying visually-grounded dialog.", "startOffset": 43, "endOffset": 51}, {"referenceID": 14, "context": "[18], who proposed a fairly restrictive \u2018Visual Turing Test\u2019 \u2013 a system that asks templated, binary questions.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "2) The dataset in [18] only contains street scenes, while our dataset has considerably more variety since it uses images from COCO [32].", "startOffset": 18, "endOffset": 22}, {"referenceID": 28, "context": "2) The dataset in [18] only contains street scenes, while our dataset has considerably more variety since it uses images from COCO [32].", "startOffset": 131, "endOffset": 135}, {"referenceID": 14, "context": "Moreover, our dataset is two orders of magnitude larger \u2013 2,591 images in [18] vs \u223c140k images, 10 question-answer pairs per image, total of \u223c1.", "startOffset": 74, "endOffset": 78}, {"referenceID": 47, "context": "Some recent large-scale datasets in this domain include the 30M Factoid Question-Answer corpus [52], 100K SimpleQuestions dataset [8], DeepMind Q&A dataset [21], the 20 artificial tasks in the bAbI dataset [65], and the SQuAD dataset for reading comprehension [46].", "startOffset": 95, "endOffset": 99}, {"referenceID": 4, "context": "Some recent large-scale datasets in this domain include the 30M Factoid Question-Answer corpus [52], 100K SimpleQuestions dataset [8], DeepMind Q&A dataset [21], the 20 artificial tasks in the bAbI dataset [65], and the SQuAD dataset for reading comprehension [46].", "startOffset": 130, "endOffset": 133}, {"referenceID": 17, "context": "Some recent large-scale datasets in this domain include the 30M Factoid Question-Answer corpus [52], 100K SimpleQuestions dataset [8], DeepMind Q&A dataset [21], the 20 artificial tasks in the bAbI dataset [65], and the SQuAD dataset for reading comprehension [46].", "startOffset": 156, "endOffset": 160}, {"referenceID": 59, "context": "Some recent large-scale datasets in this domain include the 30M Factoid Question-Answer corpus [52], 100K SimpleQuestions dataset [8], DeepMind Q&A dataset [21], the 20 artificial tasks in the bAbI dataset [65], and the SQuAD dataset for reading comprehension [46].", "startOffset": 206, "endOffset": 210}, {"referenceID": 41, "context": "Some recent large-scale datasets in this domain include the 30M Factoid Question-Answer corpus [52], 100K SimpleQuestions dataset [8], DeepMind Q&A dataset [21], the 20 artificial tasks in the bAbI dataset [65], and the SQuAD dataset for reading comprehension [46].", "startOffset": 260, "endOffset": 264}, {"referenceID": 5, "context": "While some of the earliest developed chatbots were rule-based [64], end-to-end learning based approaches are now being actively explored [9, 14, 26, 31, 53, 54, 61].", "startOffset": 137, "endOffset": 164}, {"referenceID": 10, "context": "While some of the earliest developed chatbots were rule-based [64], end-to-end learning based approaches are now being actively explored [9, 14, 26, 31, 53, 54, 61].", "startOffset": 137, "endOffset": 164}, {"referenceID": 22, "context": "While some of the earliest developed chatbots were rule-based [64], end-to-end learning based approaches are now being actively explored [9, 14, 26, 31, 53, 54, 61].", "startOffset": 137, "endOffset": 164}, {"referenceID": 27, "context": "While some of the earliest developed chatbots were rule-based [64], end-to-end learning based approaches are now being actively explored [9, 14, 26, 31, 53, 54, 61].", "startOffset": 137, "endOffset": 164}, {"referenceID": 48, "context": "While some of the earliest developed chatbots were rule-based [64], end-to-end learning based approaches are now being actively explored [9, 14, 26, 31, 53, 54, 61].", "startOffset": 137, "endOffset": 164}, {"referenceID": 49, "context": "While some of the earliest developed chatbots were rule-based [64], end-to-end learning based approaches are now being actively explored [9, 14, 26, 31, 53, 54, 61].", "startOffset": 137, "endOffset": 164}, {"referenceID": 56, "context": "While some of the earliest developed chatbots were rule-based [64], end-to-end learning based approaches are now being actively explored [9, 14, 26, 31, 53, 54, 61].", "startOffset": 137, "endOffset": 164}, {"referenceID": 31, "context": "A recent large-scale conversation dataset is the Ubuntu Dialogue Corpus [35], which contains about 500K dialogs extracted from the Ubuntu channel on Internet Relay Chat (IRC).", "startOffset": 72, "endOffset": 76}, {"referenceID": 29, "context": "[33] perform a study of problems in existing evaluation protocols for free-form dialog.", "startOffset": 0, "endOffset": 4}, {"referenceID": 28, "context": "Consistent with previous data collection efforts, we collect visual dialog data on images from the Common Objects in Context (COCO) [32] dataset, which contains multiple objects in everyday scenes.", "startOffset": 132, "endOffset": 136}, {"referenceID": 2, "context": "Unlike VQA [6], answers are not restricted to be short or concise, instead workers are encouraged to reply as naturally and \u2018conversationally\u2019 as possible.", "startOffset": 11, "endOffset": 14}, {"referenceID": 2, "context": "One key difference between VisDial and previous image question-answering datasets (VQA [6], Visual 7W [70], Baidu mQA [17]) is the lack of a \u2018visual priming bias\u2019 in VisDial.", "startOffset": 87, "endOffset": 90}, {"referenceID": 64, "context": "One key difference between VisDial and previous image question-answering datasets (VQA [6], Visual 7W [70], Baidu mQA [17]) is the lack of a \u2018visual priming bias\u2019 in VisDial.", "startOffset": 102, "endOffset": 106}, {"referenceID": 13, "context": "One key difference between VisDial and previous image question-answering datasets (VQA [6], Visual 7W [70], Baidu mQA [17]) is the lack of a \u2018visual priming bias\u2019 in VisDial.", "startOffset": 118, "endOffset": 122}, {"referenceID": 0, "context": "As analyzed in [3,19,69], this leads to a particular bias in the questions \u2013 people only ask \u2018Is there a clocktower in the picture?\u2019 on pictures actually containing clock towers.", "startOffset": 15, "endOffset": 24}, {"referenceID": 15, "context": "As analyzed in [3,19,69], this leads to a particular bias in the questions \u2013 people only ask \u2018Is there a clocktower in the picture?\u2019 on pictures actually containing clock towers.", "startOffset": 15, "endOffset": 24}, {"referenceID": 63, "context": "As analyzed in [3,19,69], this leads to a particular bias in the questions \u2013 people only ask \u2018Is there a clocktower in the picture?\u2019 on pictures actually containing clock towers.", "startOffset": 15, "endOffset": 24}, {"referenceID": 15, "context": "This allows language-only models to perform remarkably well on VQA and results in an inflated sense of progress [19, 69].", "startOffset": 112, "endOffset": 120}, {"referenceID": 63, "context": "This allows language-only models to perform remarkably well on VQA and results in an inflated sense of progress [19, 69].", "startOffset": 112, "endOffset": 120}, {"referenceID": 43, "context": "See [48] for a related, but complementary effort on question relevance in VQA.", "startOffset": 4, "endOffset": 8}, {"referenceID": 2, "context": "In VQA, binary questions are simply those with \u2018yes\u2019, \u2018no\u2019, \u2018maybe\u2019 as answers [6].", "startOffset": 79, "endOffset": 82}, {"referenceID": 2, "context": "Binary answers in VQA are biased towards \u2018yes\u2019 [6, 69] \u2013 61.", "startOffset": 47, "endOffset": 54}, {"referenceID": 63, "context": "Binary answers in VQA are biased towards \u2018yes\u2019 [6, 69] \u2013 61.", "startOffset": 47, "endOffset": 54}, {"referenceID": 29, "context": "Existing metrics such as BLEU, METEOR, ROUGE are known to correlate poorly with human judgement in evaluating dialog responses [33].", "startOffset": 127, "endOffset": 131}, {"referenceID": 5, "context": "Instead of evaluating on a downstream task [9] or holistically evaluating the entire conversation (as in goal-free chitchat [5]), we evaluate individual responses at each round (t = 1, 2, .", "startOffset": 43, "endOffset": 46}, {"referenceID": 21, "context": "As a result, such models do not exploit the biases in option creation and typically underperform models that do [25], but it is debatable whether exploiting such biases is really indicative of progress.", "startOffset": 112, "endOffset": 116}, {"referenceID": 51, "context": "In all cases, we represent I via the `2-normalized activations from the penultimate layer of VGG-16 [56].", "startOffset": 100, "endOffset": 104}, {"referenceID": 49, "context": "Thus, similar to [54], as shown in Fig.", "startOffset": 17, "endOffset": 21}, {"referenceID": 5, "context": "In the language of Memory Network [9], this is a \u20181-hop\u2019 encoding.", "startOffset": 34, "endOffset": 37}, {"referenceID": 61, "context": "Finally, we adapt several (near) state-of-art VQA models (SAN [67], HieCoAtt [37]) to Visual Dialog.", "startOffset": 62, "endOffset": 66}, {"referenceID": 33, "context": "Finally, we adapt several (near) state-of-art VQA models (SAN [67], HieCoAtt [37]) to Visual Dialog.", "startOffset": 77, "endOffset": 81}, {"referenceID": 32, "context": "Note that our LF-QI-D model is similar to that in [36].", "startOffset": 50, "endOffset": 54}, {"referenceID": 2, "context": "In this section, we lay out an exhaustive list of differences between VisDial and image question-answering datasets, with the VQA dataset [6] serving as the representative.", "startOffset": 138, "endOffset": 141}, {"referenceID": 34, "context": "DAQUAR [38] 12,468 1,447 11.", "startOffset": 7, "endOffset": 11}, {"referenceID": 62, "context": "4% Visual Madlibs [68] 56,468 9,688 4.", "startOffset": 18, "endOffset": 22}, {"referenceID": 44, "context": "9% COCO-QA [49] 117,684 69,172 8.", "startOffset": 11, "endOffset": 15}, {"referenceID": 13, "context": "0% 100% Baidu [17] 316,193 316,193 VQA [6] 614,163 204,721 6.", "startOffset": 14, "endOffset": 18}, {"referenceID": 2, "context": "0% 100% Baidu [17] 316,193 316,193 VQA [6] 614,163 204,721 6.", "startOffset": 39, "endOffset": 42}, {"referenceID": 64, "context": "Visual7W [70] 327,939 47,300 6.", "startOffset": 9, "endOffset": 13}, {"referenceID": 2, "context": "Such a line of questioning does not exist in the VQA dataset, where the subjects were shown the questions already asked about an image, and explicitly instructed to ask about different entities [6].", "startOffset": 194, "endOffset": 197}, {"referenceID": 7, "context": "In particular, we compare VisDial, VQA, and Cornell Movie-Dialogs Corpus [11].", "startOffset": 73, "endOffset": 77}, {"referenceID": 20, "context": "For the purpose of our analysis, we pick the popular sequence-to-sequence (Seq2Seq) language model [24] and use the perplexity of this model trained on different datasets as a measure of temporal structure in a dataset.", "startOffset": 99, "endOffset": 103}, {"referenceID": 2, "context": "One key difference between VisDial and previous image question answering datasets (VQA [6], Visual 7W [70], Baidu mQA [17]) is the lack of a \u2018visual priming bias\u2019 in VisDial.", "startOffset": 87, "endOffset": 90}, {"referenceID": 64, "context": "One key difference between VisDial and previous image question answering datasets (VQA [6], Visual 7W [70], Baidu mQA [17]) is the lack of a \u2018visual priming bias\u2019 in VisDial.", "startOffset": 102, "endOffset": 106}, {"referenceID": 13, "context": "One key difference between VisDial and previous image question answering datasets (VQA [6], Visual 7W [70], Baidu mQA [17]) is the lack of a \u2018visual priming bias\u2019 in VisDial.", "startOffset": 118, "endOffset": 122}, {"referenceID": 63, "context": "As described in [69], this leads to a particular bias in the questions \u2013 people only ask \u2018Is there a clocktower in the picture?\u2019 on pictures actually containing clock towers.", "startOffset": 16, "endOffset": 20}, {"referenceID": 63, "context": "This allows languageonly models to perform remarkably well on VQA and results in an inflated sense of progress [69].", "startOffset": 111, "endOffset": 115}, {"referenceID": 43, "context": "See [48] for a related, but complementary effort on question relevance in VQA.", "startOffset": 4, "endOffset": 8}, {"referenceID": 2, "context": "In VQA, binary questions are simply those with \u2018yes\u2019, \u2018no\u2019, \u2018maybe\u2019 as answers [6].", "startOffset": 79, "endOffset": 82}, {"referenceID": 2, "context": "Binary answers in VQA are biased towards \u2018yes\u2019 [6,69] \u2013 61.", "startOffset": 47, "endOffset": 53}, {"referenceID": 63, "context": "Binary answers in VQA are biased towards \u2018yes\u2019 [6,69] \u2013 61.", "startOffset": 47, "endOffset": 53}, {"referenceID": 24, "context": "We use Adam [28]", "startOffset": 12, "endOffset": 16}], "year": 2017, "abstractText": "We introduce the task of Visual Dialog, which requires an AI agent to hold a meaningful dialog with humans in natural, conversational language about visual content. Specifically, given an image, a dialog history, and a question about the image, the agent has to ground the question in image, infer context from history, and answer the question accurately. Visual Dialog is disentangled enough from a specific downstream task so as to serve as a general test of machine intelligence, while being grounded in vision enough to allow objective evaluation of individual responses and benchmark progress. We develop a novel two-person chat data-collection protocol to curate a large-scale Visual Dialog dataset (VisDial). VisDial v0.9 has been released and contains 1 dialog with 10 question-answer pairs on \u223c120k images from COCO, with a total of\u223c1.2M dialog questionanswer pairs. We introduce a family of neural encoder-decoder models for Visual Dialog with 3 encoders \u2013 Late Fusion, Hierarchical Recurrent Encoder and Memory Network \u2013 and 2 decoders (generative and discriminative), which outperform a number of sophisticated baselines. We propose a retrievalbased evaluation protocol for Visual Dialog where the AI agent is asked to sort a set of candidate answers and evaluated on metrics such as mean-reciprocal-rank of human response. We quantify gap between machine and human performance on the Visual Dialog task via human studies. Putting it all together, we demonstrate the first \u2018visual chatbot\u2019! Our dataset, code, trained models and visual chatbot are available on https://visualdialog.org.", "creator": "LaTeX with hyperref package"}}}