{"id": "1611.08696", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-Nov-2016", "title": "Optimizing Expectation with Guarantees in POMDPs (Technical Report)", "abstract": "A standard objective in partially-observable Markov decision processes (POMDPs) is to find a policy that maximizes the expected discounted-sum payoff. However, such policies may still permit unlikely but highly undesirable outcomes, which is problematic especially in safety-critical applications. Recently, there has been a surge of interest in POMDPs where the goal is to maximize the probability to ensure that the payoff is at least a given threshold, but these approaches do not consider any optimization beyond satisfying this threshold constraint. In this work we go beyond both the \"expectation\" and \"threshold\" approaches and consider a \"guaranteed payoff optimization (GPO)\" problem for POMDPs, where we are given a threshold $t$ and the objective is to find a policy $\\sigma$ such that a) each possible outcome of $\\sigma$ yields a discounted-sum payoff of at least $t$, and b) the expected discounted-sum payoff of $\\sigma$ is optimal (or near-optimal) among all policies satisfying a). We present a practical approach to tackle the GPO problem and evaluate it on standard POMDP benchmarks.", "histories": [["v1", "Sat, 26 Nov 2016 10:55:40 GMT  (338kb,D)", "http://arxiv.org/abs/1611.08696v1", null], ["v2", "Sun, 29 Jan 2017 13:31:54 GMT  (338kb,D)", "http://arxiv.org/abs/1611.08696v2", null]], "reviews": [], "SUBJECTS": "cs.AI cs.GT", "authors": ["krishnendu chatterjee", "petr novotn\\'y", "guillermo a p\\'erez", "jean-fran\\c{c}ois raskin", "{\\dj}or{\\dj}e \\v{z}ikeli\\'c"], "accepted": false, "id": "1611.08696"}, "pdf": {"name": "1611.08696.pdf", "metadata": {"source": "CRF", "title": "Optimizing Expectation with Guarantees in POMDPs (Technical Report)", "authors": ["Krishnendu Chatterjee", "Petr Novotn\u00fd", "Guillermo A. P\u00e9rez", "Jean-Fran\u00e7ois Raskin"], "emails": ["krishnendu.chatterjee@ist.ac.at,", "pnovotny@ist.ac.at", "jraskin@ulb.ac.be,", "gperezme@ulb.ac.be", "dz277@cam.ac.uk"], "sections": [{"heading": "1 Introduction", "text": "This year, it has reached the point where it will be able to retaliate."}, {"heading": "2 Preliminaries", "text": "During this work we follow the standard notations (PO) for MDP from [Put05, Lit96]."}, {"heading": "2.1 POMDPs", "text": "We designate the number of all probability distributions on a finite set X, i.e. all functions f: X (0.1) in such a way that every single number (x) = 1. For f: D (X) we are supported by Supp (f), i.e. the sentence {x: X (x) > 0). Definition 1: D (S) is a probable transitional function that gives a state and a probability distribution over the successor states, r: S, O) where S is a finite group of states, A is a finite alphabet of actions, B: S \u00b7 A (S) is a probability transitional function that gives a probability distribution over the successor states, r: S \u00d7 A \u2192 R is a reward function, Z is a finite series of observations, O: S \u2192 D (Z) is a probable observation function that assigns each state to a distribution over the observations."}, {"heading": "3 Policies for GPO Problem", "text": "It is well known that doctrines of faith form a sufficient statistic of history to attain the optimum expected value, i.e., there is always only one guaranteed policy based on faith. (That is, a policy that strives for such a policy for every story.) \"We.\" (\"We.\"). (\"We.\"). (\"We.\"). (\"We.\"). (\"We.\"). (\"We.\"). (\"We.\"). (\"We.\"). (\"We.\". (\"We.\"). (\"We.\"). (\"We.\"). (\"We.\"). (\"We.\" (\"We.\"). (\"We.\" (.). (.). (\"We.\" (.). (.). (.). (. \"We. (.). (.). (. (.). (.\" We. \"(.). (. (.). (.\" We. \"(.). (. (.). (. (.). (.\" We. (.). (. (.). (. (.). (. \"We. (.). (.\" (.). (. \"We.\" (. \"(.). (.\" (.). (. \"(.). (.\" (.). \"We. (. (.). (. (.). (.). (. (.). (. (.). (.).\" We. (. (. (. (.). (.). (. (.). (.). (. (.). (. (.). (. (.). (.). (. (. (. (.). (. (.). (. (.). (.). (. (. (. (.). (.). (. (.). (.). (. (.). (. (.). (. (.). (.). (. (. (.). (. (.). (. (. (.).). (. (.)."}, {"heading": "4 Computing Future Values", "text": "The threshold condition in the GPO problem is global, i.e. it talks about all runs that are compatible with a policy. Therefore, it is unlikely that solving the GPO problem can be led to a purely online method that only allows local approaches to the policy. (In this section we will show how to calculate future values in an offline process.) This requires a global analysis of the processing steps that either B = Supp or there is a story in which B = Supp can be supported only with faith, rather than with faith. (Belief) Supports VBelief Supports VBelief. A belief in support of B '2S is valid if there is a story. (Supp) Only valid supports can be encountered during the planning process, and so we only need to calculate future values. We denote of VBelief Supports VBelSup (P) to support the establishment of valid beliefs."}, {"heading": "5 Solving the GPO problem", "text": "We solve the GPO problem by fixing the partially observable Monte Carlo Planning (POMCP) in a tree (SV10].POMCP is an online planning method that aims at selecting the best action in each decision-making epoch given the current story h. In each epoch, POMCP performs a number of finite horizon simulations based on the belief in a local approximation of the optimal expected value function. Each simulation extends to the story h by selecting actions according to certain rules until the horizon is reached. The payout of the path produced is then evaluated and the result is used to update the optimal value approximation. After all the simulations have been performed, the best action is played according to the estimated values, a new observation is made, and the process continues as superficial.1Since the number 1 \u2212 and the size of the data structure can be exponential, POCP stores the information gained in a tree."}, {"heading": "6 Experiments", "text": "We tested our algorithm on two classic sets of benchmarks. The first, Hallway, was introduced in [LCK95]. In one corridor POMDP, a robot navigates through a grid world of walls and traps. We have considered variants in which traps cause irreparable damage and another in which they simply \"rotate\" the robot - making it more uncertain about its current position in the network. In addition, we have run our algorithm on RockSample POMDPs. The latter corresponds to the classic scenario first described in [SS04]. (We use a slight customization with a single imprecise scanning action.) Our experimental results are summarized in Figure 2 and Table 1.Test Environment Specifications: (1.) CPU: 6-core Intel Zeon, 3.33 GHz, 6 cores, 6 cores; (2.) Memory: 256 KB of L2 cache, 12 MB of L3 cache, 32 GB; 3. OS 7.X expected in Mac OS 7.X."}, {"heading": "7 Discussion", "text": "In this work, we have provided a practical solution to the GPO problem: our algorithm, G-POMCP, allows us to develop a policy that ensures a worst-case discount value while optimizing the expected payout. We have implemented G-POMCP and evaluated its performance against classic families of benchmarks. Our experiments show that our approach is efficient, although the exact GPO problem is fundamentally more complicated."}, {"heading": "Acknowledgements", "text": "The research that led to these results was supported by the Austrian Science Fund (FWF) NFN Grant No. S11407-N23 (RiSE / SHiNE), two ERC Starting Grants (279307: Graph Games, 279499: inVEST), the Vienna Science and Technology Fund (WWTF) through the ICT15-003 project and the People Programme (Marie Curie Actions) of the Seventh Framework Programme of the European Union (FP7 / 2007-2013) under the REA Funding Agreement No. [291734]."}, {"heading": "A Examples of Section 2", "text": "Here is presented a detailed analysis of all possible policies, and the best policy with regard to the optimized expected disbursement. First, it should be noted that a policy is unambiguously determined if the first action taken is clearly in the amount [m1, m2, sense]. Otherwise, it is possible to carry out actions n times until they are successfully achieved. In the following, the expected disbursements are calculated for each of the cases listed above. \u2022 \u03c31: m1 actions taken firsteValP (m1) = 0.9 GOP [1] = 0.9 \u22c5, sense] [1-2] = 0.9 actions taken [1-p] = 0.9 actions taken [1-p] = 0.9 actions taken [1-fold] = 0.9 actions [1-fold implementation] = 0.9 actions taken [1-fold implementation] = 0.9 actions taken [1-fold implementation] = 0.9 actions taken [1-implementation]."}, {"heading": "B On the assumption of observable rewards (Section 4)", "text": "If the rewards of a given POMDP cannot be observed, calculating future values is at least as difficult as solving the problem of the discounted target sum, a long-standing problem in machine theory related to other open problems in algebra [BHO15]. For POMDPs with non-observable rewards, there is an easy way to get a sub-approximation of fVal. Following the value iteration algorithm for discount games outlined in Section 4 and detailed in [HM15], it is possible to get the exact future values. In addition, it is easy to see that the functions generated by the algorithm move ever closer to the actual future values. Therefore, stopping the iteration around each i value leads to the desired sub-approximation. (Note that the reward function must assign a non-negative value to each transition."}, {"heading": "C Formal Proof of Lemma 1 and Theorem 1", "text": "In this section we argue that, for POMDPs with observable rewards, we can reduce the calculation of a policy with a worst-case value above a certain threshold. < p > p > p > p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p and p = p = p = p = p = p and p = p = p = p and p = p = p is simple: we will construct a weighted arena in which the states from a number of possible states with the same observation and the new transition model with non-zero probability in the POMDP will allow us to observe the fact that in a POMDP all possible states with the same observation will represent the actual state of the system with the same observation and the new transition model with non-zero probability in the POMDP. This subset construction captures the fact that in a POMDP all possible states with the same observation will allow us to observe the actual state of the Poppy system with the same observation, the fact that MDP has no obtainable POsitions, and the fact that MDP will allow us to lose the Poppy information."}, {"heading": "D Formal Proof of Proposition 1", "text": "Suppose we got a POMDP P = (S, A, \u03b4, r, Z, O, \u03bb) with observable rewards and we constructed the corresponding weighted arena for each B-2S. (i) Then any policy that is sufficiently safe for t would meet the wValP standard. (ii) In addition, a policy for t fVal is -safe if and only if wValP standard is met for t-2S standard. (ii) For point (ii) we refer the reader to [BMR14], where the authors show that playing fVal -safe for t in discounted sum games is sufficient and necessary to be at least t-safe."}, {"heading": "E Open Theoretical Problems", "text": "A lower limit on the arithmetical complexity of this problem would entail a lower limit on the universality of discounted sum machines, which is open [CDH10]. In the other direction, an upper limit (i.e. an algorithm or some kind of decisiveness result) would result in an upper limit on the goal of discounted sum problems [BHO15], which has proven to be more general than some important open problems in mathematics and computer science. The exact GPO problem (i.e. not the approximation we are achieving in this paper) is also open, even for fully observable MDPs. Note: If the threshold given for the worst case is actually the future value of the initial state, then we could construct a subgraph of decisions that meets the equation from the system (2), and be sure that it is a complete representation of all strategies that reach the optimal value."}], "references": [{"title": "pages 750\u2013761", "author": ["U. Boker", "T.A. Henzinger", "J. Otop. The Target Discounted-Sum Problem. In LICS"], "venue": "July", "citeRegEx": "BHO15", "shortCiteRegEx": null, "year": 2015}, {"title": "In CSL-LICS", "author": ["V\u00e9ronique Bruy\u00e8re", "No\u00e9mie Meunier", "Jean-Fran\u00e7ois Raskin. Secure equilibria in weighted games"], "venue": "pages 26:1\u201326:26,", "citeRegEx": "BMR14", "shortCiteRegEx": null, "year": 2014}, {"title": "pages 1\u201314", "author": ["Krishnendu Chatterjee", "Laurent Doyen. The complexity of partial-observation parity games. In LPAR"], "venue": "Springer,", "citeRegEx": "CD10", "shortCiteRegEx": null, "year": 2010}, {"title": "ACM Transactions on Computational Logic", "author": ["Krishnendu Chatterjee", "Laurent Doyen", "Thomas A. Henzinger. Quantitative languages"], "venue": "11(4),", "citeRegEx": "CDH10", "shortCiteRegEx": null, "year": 2010}, {"title": "Technical note", "author": ["Axel Haddad", "Benjamin Monmege. Why Value Iteration Runs in Pseudo-Polynomial Time for Discounted-Payoff Games"], "venue": "Universit\u00e9 libre de Bruxelles,", "citeRegEx": "HM15", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "More precisely, if the rewards of a given POMDP are not observable, the computation of future values is at least as hard as solving the target discounted sum problem, a long-standing open problem in automata theory related to other open problems in algebra [BHO15].", "startOffset": 257, "endOffset": 264}], "year": 2017, "abstractText": "A standard objective in partially-observable Markov decision processes (POMDPs) is to find a policy that maximizes the expected discounted-sum payoff. However, such policies may still permit unlikely but highly undesirable outcomes, which is problematic especially in safety-critical applications. Recently, there has been a surge of interest in POMDPs where the goal is to maximize the probability to ensure that the payoff is at least a given threshold, but these approaches do not consider any optimization beyond satisfying this threshold constraint. In this work we go beyond both the \u201cexpectation\u201d and \u201cthreshold\u201d approaches and consider a \u201cguaranteed payoff optimization (GPO)\u201d problem for POMDPs, where we are given a threshold t and the objective is to find a policy \u03c3 such that a) each possible outcome of \u03c3 yields a discounted-sum payoff of at least t, and b) the expected discounted-sum payoff of \u03c3 is optimal (or near-optimal) among all policies satisfying a). We present a practical approach to tackle the GPO problem and evaluate it on standard POMDP benchmarks.", "creator": "TeX"}}}