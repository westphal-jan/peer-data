{"id": "1611.08930", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Nov-2016", "title": "Deep attractor network for single-microphone speaker separation", "abstract": "Despite the overwhelming success of deep learning in various speech processing tasks, the problem of separating simultaneous speakers in a mixture remains challenging. Two major difficulties in such systems are the arbitrary source permutation and unknown number of sources in the mixture. We propose a novel deep learning framework for single channel speech separation by creating attractor points in high dimensional embedding space of the acoustic signals which pull together the time-frequency bins corresponding to each source. Attractor points in this study are created by finding the centroids of the sources in the embedding space, which are subsequently used to determine the similarity of each bin in the mixture to each source. The network is then trained to minimize the reconstruction error of each source by optimizing the embeddings. The proposed model is different from prior works in that it implements an end-to-end training, and it does not depend on the number of sources in the mixture. Two strategies are explored in the test time, K-means and fixed attractor points, where the latter requires no post-processing and can be implemented in real-time. We evaluated our system on Wall Street Journal dataset and show 5.49\\% improvement over the previous state-of-the-art methods.", "histories": [["v1", "Sun, 27 Nov 2016 22:47:23 GMT  (2571kb,D)", "http://arxiv.org/abs/1611.08930v1", "2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"], ["v2", "Tue, 28 Mar 2017 03:15:07 GMT  (2826kb,D)", "http://arxiv.org/abs/1611.08930v2", "2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"]], "COMMENTS": "2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)", "reviews": [], "SUBJECTS": "cs.SD cs.LG", "authors": ["zhuo chen", "yi luo", "nima mesgarani"], "accepted": false, "id": "1611.08930"}, "pdf": {"name": "1611.08930.pdf", "metadata": {"source": "CRF", "title": "DEEP ATTRACTOR NETWORK FOR SINGLE-MICROPHONE SPEAKER SEPARATION", "authors": ["Zhuo Chen", "Yi Luo", "Nima Mesgarani"], "emails": [], "sections": [{"heading": null, "text": "In fact, we are in a situation in which we are able to change the world, in which we are able to change the world, and in which we are able to change the world in order to change it. \""}, {"heading": "2.1. Model", "text": "neuralF is able to reactivate the mixture from the year 2000 to the year 2000, in the way in which it is able, in the way in which it is able, in the way in which it is able, in the way in which it is able, in the way in which it is able, in the way in which it is in the way in which it is able, in the way in which it is able, in the way in which it is able, in the way in which it is able, in the way in which it is able, in the way in which it is able to remain in the world in which it is living."}, {"heading": "2.2. Estimation of attractor points", "text": "One possibility is to use a weighted average. As the attractors are the center of gravity of the source, we can only include the embedding of the most noticeable T-F containers, resulting in a more robust estimate. We examine this strategy using an amplitude threshold in the estimation of the attractor. Alternatively, we can also use a neural network model to determine the representative embedding for each source, an idea that has similarities to encoder decoder attention networks [12, 13]. During the test period, since the true mapping of Y is unknown, we insert two strategies for creating the attractor points. The first is similar to the strategy in DC, where the centers are found using a post-K mean algorithm. The second method is based on the observation that the position of the attractors in the embedding space is relatively stable. This observation is shown in Figure 1, where each pair of points corresponds to the principle used for two different pairs in each of the figures and is used in the mixture of 1."}, {"heading": "2.3. Relation with DC and PIT", "text": "The objective function of DC is shown in Equation 4, where Y is the indicator function corresponding to a binary mask, and V the learned embedding is: L = p, t, t, t, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c"}, {"heading": "3.1. Experimental setup", "text": "We evaluate our proposed model for the task of single-channel overlapping of two loudspeakers. We use the corpus introduced in [8], which includes a 30-hour training set and a 10-hour validation set generated by randomly selecting statements from different speakers in the Wall Street Journal (WSJ0), and mix them at various signal-to-noise ratios (SNR) randomly selected between 0 dB and 10 dB. 5-hour weighting sets are generated similarly to the above by using statements from 16 invisible speakers from the WSJ0 dataset."}, {"heading": "3.2. Separation examples", "text": "Figure 1 shows an example of a mixture, the difference between the two speakers in the mixture and the separate spectrograms of the two speakers with DANet. Also shown in Figure 1 is the embedding of the mixture, which is projected onto its first main components. In Figure 1, each point represents a T-F container in the embedding space. Red and blue dots correspond to the T-F containers in which speakers have one or two correspondingly higher energies.The position of the attractors is characterized by x. It shows that two symmetrical attractor centers are formed, each corresponding to one of the speakers in the mixture. A clear boundary can be observed in the figure, which shows that the network has successfully pulled the two speakers apart in the direction of their respective attractor points. Figure 2 shows the location of the attractors for 10,000 mixing examples, which are mapped onto the 3-dimensional space for visualization purposes."}, {"heading": "3.3. Results", "text": "In fact, it is such that it is a way in which people are able to survive themselves, and in which people are able to survive themselves. \"(S. S. S. S. S. S., S. S. S., S. S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S.,"}], "references": [{"title": "Contextdependent pre-trained deep neural networks for largevocabulary speech recognition", "author": ["G.E. Dahl", "D. Yu", "L. Deng", "A. Acero"], "venue": "IEEE Trans. on Audio, Speech and Language Processing, 2012, vol. 20.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2012}, {"title": "Long short-term memory recurrent neural network architectures for large scale acoustic modeling", "author": ["S. Has\u0306im", "Senior A.", "F. Beaufays"], "venue": "Interspeech 2014. ISCA, 2014.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2014}, {"title": "Acoustic modelling with cd-ctc-smbr lstm rnns", "author": ["A. Senior", "S. Has\u0306im", "F.C. Quitry", "T.N. Sainath", "K. Rao"], "venue": "ASRU 2015. IEEE, 2015.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2015}, {"title": "Speech enhancement and recognition using multi-task learning of long short-term memory recurrent neural networks", "author": ["Zhuo Chen", "Shinji Watanabe", "Hakan Erdo\u011fan", "John R Hershey"], "venue": "ISCA, 2015.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2015}, {"title": "The merl/sri system for the 3rd chime challenge using beamforming, robust feature extraction, and advanced speech recognition", "author": ["Takaaki Hori", "Zhuo Chen", "Hakan Erdogan", "John R Hershey", "Jonathan Le Roux", "Vikramjit Mitra", "Shinji Watanabe"], "venue": "2015 IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU). IEEE, 2015, pp. 475\u2013481.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2015}, {"title": "Speech enhancement based on deep denoising autoencoder", "author": ["X. Lu", "Y. Tsao", "S. Matsuda", "C. Hori"], "venue": "2013, pp. 436\u2013440.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2013}, {"title": "Some experiments on the recognition of speech, with one and with two ears", "author": ["E Colin Cherry"], "venue": "The Journal of the acoustical society of America, vol. 25, no. 5, pp. 975\u2013979, 1953.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1953}, {"title": "Deep clustering: Discriminative embeddings for segmentation and separation", "author": ["John R Hershey", "Zhuo Chen", "Jonathan Le Roux", "Shinji Watanabe"], "venue": "2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2016, pp. 31\u2013 35.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2016}, {"title": "Permutation invariant training of deep models for speaker-independent multi-talker speech separation", "author": ["Dong Yu", "Morten Kolb\u00e6k", "Zheng-Hua Tan", "Jesper Jensen"], "venue": "arXiv preprint arXiv:1607.00325, 2016.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2016}, {"title": "Single-channel multi-speaker separation using deep clustering", "author": ["Yusuf Isik", "Jonathan Le Roux", "Zhuo Chen", "Shinji Watanabe", "John R Hershey"], "venue": "arXiv preprint arXiv:1607.02173, 2016.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2016}, {"title": "Human adults and human infants show a perceptual magnet effect", "author": ["Patricia K. Kuhl"], "venue": "Perception & psychophysics, 50.2 (1991): 93-107.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1991}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1409.0473, 2014.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2014}, {"title": "Describing multimedia content using attention-based encoder-decoder networks", "author": ["Kyunghyun Cho", "Aaron Courville", "Yoshua Bengio"], "venue": "IEEE Transactions on Multimedia, vol. 17, no. 11, pp. 1875\u20131886, 2015.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1875}, {"title": "Long shortterm memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation, vol. 9, no. 8, pp. 1735\u20131780, 1997.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1997}, {"title": "Lecture 6.5rmsprop", "author": ["Tijmen Tieleman", "Geoffrey Hinton"], "venue": "COURSERA: Neural networks for machine learning, 2012.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2012}], "referenceMentions": [{"referenceID": 0, "context": "Despite the recent advances in deep learning methods for various speech processing tasks such as automatic recognition[1, 2, 3], enhancement[4, 5, 6], speech separation remains unresolved.", "startOffset": 118, "endOffset": 127}, {"referenceID": 1, "context": "Despite the recent advances in deep learning methods for various speech processing tasks such as automatic recognition[1, 2, 3], enhancement[4, 5, 6], speech separation remains unresolved.", "startOffset": 118, "endOffset": 127}, {"referenceID": 2, "context": "Despite the recent advances in deep learning methods for various speech processing tasks such as automatic recognition[1, 2, 3], enhancement[4, 5, 6], speech separation remains unresolved.", "startOffset": 118, "endOffset": 127}, {"referenceID": 3, "context": "Despite the recent advances in deep learning methods for various speech processing tasks such as automatic recognition[1, 2, 3], enhancement[4, 5, 6], speech separation remains unresolved.", "startOffset": 140, "endOffset": 149}, {"referenceID": 4, "context": "Despite the recent advances in deep learning methods for various speech processing tasks such as automatic recognition[1, 2, 3], enhancement[4, 5, 6], speech separation remains unresolved.", "startOffset": 140, "endOffset": 149}, {"referenceID": 5, "context": "Despite the recent advances in deep learning methods for various speech processing tasks such as automatic recognition[1, 2, 3], enhancement[4, 5, 6], speech separation remains unresolved.", "startOffset": 140, "endOffset": 149}, {"referenceID": 6, "context": "Two main difficulties hinder the efficacy of deep learning algorithms to tackle the famous \u201ccocktail party problem\u201d[7].", "startOffset": 115, "endOffset": 118}, {"referenceID": 7, "context": "The first difficulty is referred as the \u201cpermutation problem\u201d [8].", "startOffset": 62, "endOffset": 65}, {"referenceID": 7, "context": "Two deep learning based methods have been proposed to resolve these problems, which are known as \u201ddeep clustering (DC)[8]\u201d and \u201dpermutation invariant training (PIT)[9]\u201d.", "startOffset": 118, "endOffset": 121}, {"referenceID": 8, "context": "Two deep learning based methods have been proposed to resolve these problems, which are known as \u201ddeep clustering (DC)[8]\u201d and \u201dpermutation invariant training (PIT)[9]\u201d.", "startOffset": 164, "endOffset": 167}, {"referenceID": 9, "context": "Minimizing the separation error is done with an unfolding clustering system and a second network, which is trained iteratively and stage by stage to ensure convergence [10].", "startOffset": 168, "endOffset": 172}, {"referenceID": 7, "context": "PIT was first proposed in [8], and was later shown to have comparable performance as DC [9].", "startOffset": 26, "endOffset": 29}, {"referenceID": 8, "context": "PIT was first proposed in [8], and was later shown to have comparable performance as DC [9].", "startOffset": 88, "endOffset": 91}, {"referenceID": 10, "context": "The term attractor refers to the well-studied perceptual effects in human speech perception which suggest that the brain circuits create perceptual attractors (magnets) that warp the stimulus space such that to draws the sound that is closest to it, a phenomenon that is called Perceptual Magnet Effect [11].", "startOffset": 303, "endOffset": 307}, {"referenceID": 0, "context": "This particular metric uses the inner product followed by a sigmoid function which monotonically scales the masks between [0, 1].", "startOffset": 122, "endOffset": 128}, {"referenceID": 11, "context": "Alternatively, a neural network model may also be used to pick the representative embedding for each source, an idea which shares similarities with encoder-decoder attention networks [12, 13].", "startOffset": 183, "endOffset": 191}, {"referenceID": 12, "context": "Alternatively, a neural network model may also be used to pick the representative embedding for each source, an idea which shares similarities with encoder-decoder attention networks [12, 13].", "startOffset": 183, "endOffset": 191}, {"referenceID": 3, "context": "On the other hand, when the attractor vectors are considered as free parameters in the network, DANet reduces to a classification network [4, 5], and Equation 1 becomes a fullyconnected layer.", "startOffset": 138, "endOffset": 144}, {"referenceID": 4, "context": "On the other hand, when the attractor vectors are considered as free parameters in the network, DANet reduces to a classification network [4, 5], and Equation 1 becomes a fullyconnected layer.", "startOffset": 138, "endOffset": 144}, {"referenceID": 7, "context": "We use the corpus introduced in [8], which contains a 30 h training set and a 10 h validation set generated by randomly selecting utterances from different speakers in the Wall Street Journal (WSJ0) training set si tr s, and mixing them at various signal-to-noise ratios (SNR) randomly chosen between 0 dB and 10 dB.", "startOffset": 32, "endOffset": 35}, {"referenceID": 13, "context": "The network contains 4 Bi-directional LSTM [14] layers with 600 hidden units in each layer.", "startOffset": 43, "endOffset": 47}, {"referenceID": 14, "context": "RMSprop algorithm [15] is used for training with an exponential learning rate decaying strategy, where the learning rate starts at 10\u22124 and ends at 3\u00d7 10\u22126.", "startOffset": 18, "endOffset": 22}], "year": 2016, "abstractText": "Despite the overwhelming success of deep learning in various speech processing tasks, the problem of separating simultaneous speakers in a mixture remains challenging. Two major difficulties in such systems are the arbitrary source permutation and unknown number of sources in the mixture. We propose a novel deep learning framework for single channel speech separation by creating attractor points in high dimensional embedding space of the acoustic signals which pull together the time-frequency bins corresponding to each source. Attractor points in this study are created by finding the centroids of the sources in the embedding space, which are subsequently used to determine the similarity of each bin in the mixture to each source. The network is then trained to minimize the reconstruction error of each source by optimizing the embeddings. The proposed model is different from prior works in that it implements an end-to-end training, and it does not depend on the number of sources in the mixture. Two strategies are explored in the test time, K-means and fixed attractor points, where the latter requires no post-processing and can be implemented in real-time. We evaluated our system on Wall Street Journal dataset and show 5.49% improvement over the previous state-of-the-art methods.", "creator": "LaTeX with hyperref package"}}}