{"id": "1611.08944", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Nov-2016", "title": "Nonparametric General Reinforcement Learning", "abstract": "Reinforcement learning (RL) problems are often phrased in terms of Markov decision processes (MDPs). In this thesis we go beyond MDPs and consider RL in environments that are non-Markovian, non-ergodic and only partially observable. Our focus is not on practical algorithms, but rather on the fundamental underlying problems: How do we balance exploration and exploitation? How do we explore optimally? When is an agent optimal? We follow the nonparametric realizable paradigm.", "histories": [["v1", "Mon, 28 Nov 2016 00:36:40 GMT  (195kb,D)", "http://arxiv.org/abs/1611.08944v1", "PhD thesis"]], "COMMENTS": "PhD thesis", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["jan leike"], "accepted": false, "id": "1611.08944"}, "pdf": {"name": "1611.08944.pdf", "metadata": {"source": "META", "title": "Nonparametric General Reinforcement Learning", "authors": ["Jan Leike"], "emails": [], "sections": [{"heading": null, "text": "Nonparametric General Reinforcement LearningJan LeikeA doctoral thesis submitted for the doctorate of philosophy at the Australian National University in November 2016ar Xiv: 161 1,08 944v 1 [cs.A I] 2 8N ov2 016 \u00a9 Jan LeikeThis thesis is licensed under the Creative Commons Attribution 4.0 International LicenseNo reinforcement learners were injured in the making of these thesis.Unless otherwise specified, this is my own original thesis. Jan Leike 29 November 2016"}, {"heading": "Acknowledgements", "text": "There are many without whom this thesis would not have been possible. I sincerely hope that this site is not the way in which they learn how grateful I am to them. I especially thank them for making me be absolutely rigorous in my mathematical arguments, and of course for developing the theory of universal AI, without which this thesis would not exist. I could not imagine that the Australian National University would grant me scholarships that pursue my academic interests without restriction and without financial worries. Csaba Szepesv\u00e1ri and the University of Alberta for hosting three months.. Matthias Heizmann and the University of Freiburg for hosting me."}, {"heading": "1 Introduction 1", "text": "1.1 Reinforcement Learning.................................................................................................................................................................................."}, {"heading": "2 Preliminaries 15", "text": "2.1 Measurement theory............................................. 16 2.2 Stochastic processes......................................................................................................"}, {"heading": "3 Learning 23", "text": "The question of how to proceed is not yet decided. It is not yet decided how to proceed. It is not yet decided how to proceed."}, {"heading": "4 Acting 49", "text": "For example, when it comes to how relations between countries develop."}, {"heading": "5 Optimality 67", "text": "......................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................"}, {"heading": "6 Computability 101", "text": "6.1. Background on computing..............................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................."}, {"heading": "7 The Grain of Truth Problem 127", "text": ". Reflective oracle..............................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................."}, {"heading": "8 Conclusion 147", "text": "The results from the different areas: among others Figures 1,1. Selection. Atari. List. 2600 Video Games... Brighter. Brighter. Brighter. Brighter. Brighter. Brighter. Brighter. Brighter. Brighter. Brighter. Brighter. Brighter. Brighter. Brighter. Brighter. Brighter. Brighter. Brighter. Brighter. Brighter. Brighter. Brighter. Brighter. Brighter. Brighter. Brighter. Brighter. Brighter. Brighter. Brighter. Brighter. Brighter. Brighter. Brighter. Brighter. Brighter. Brighter. Brighter. Brighter. Brighter. Brighter. Brighter. Brighter. Brighter. Brighter. Brighter. Brighter. Brighter. Brighter. Brighter. Brighter."}, {"heading": "2 Introduction", "text": "We believe that significant progress can be made on one or more of these issues if a carefully selected group of scientists work together for a summer. In retrospect, this proposal reads far too optimistic and disappointment was inevitable. Making progress on these issues proved much more difficult than promised, and in recent decades, any discussion of HLAI research has been avoided by serious researchers in the field. This gap has been largely filled by contortions that further tarnished the reputation of HLAI research. However, this trend has recently reversed: Chalmers (2010), Hutter (2012a), Schmidhuber (2012), Bostrom (2014), Hawking, Tegmark, Russell and Wilczek (2014), Shanahan (2015), and Walton (2016) are well-known scientists who are seriously discussing the prospect of HLAI."}, {"heading": "1.1 Reinforcement Learning", "text": "The best formal model for strong AI currently exists in the form of Reinforcement Measures (RL). Reinforcement studies that act in an unknown environment through trial and error (Sutton and Barto, 1998; Szepesv\u00e1ri, 2010; Wiering and van Otterlo, 2012). Without knowledge of the structure of the environment, an actor must learn what he has to do through the carrot and stick approach: he receives a reward in the form of numerical feedback that shows how well he behaves, and the actor has to find out what he is doing."}, {"heading": "1.1.1 Narrow Reinforcement Learning", "text": "In the literature on enhancing learning, it is typically assumed that the environment is a Markov decision-making process (MDP), i.e., the next perception depends only on the last perception and action and is independent of the rest of the story (see Section 4.1.3). In an MDP, perceptions are commonly referred to as states. This attitude is well analyzed (Puterman, 2014; Bertsekas and Tsitsiklis, 1995; Sutton and Barto, 1998), and there are a variety of algorithms known to learn the MDP asymptotically, such as TD learning (Sutton, 1988) and Q learning (Watkins and Dayan, 1992). In addition, various guarantees of learning have been demonstrated for MDPs in literature. First, there are limits to the agent's regret, the difference between the rewards obtained and the rewards of optimal policies."}, {"heading": "4 Introduction", "text": "With its emphatic TD algorithm that converges outside politics, there is no guarantee of convergence for nonlinear functional approximation. Historical successes of amplification learning include the autonomous helicopter pilot (Kim et al., 2003) and TD gammon, a backgammon algorithm that learns through self-play (Tesauro, 1995), similar to AlphaGo (Silver et al., 2016)."}, {"heading": "1.1.2 Deep Q-Networks", "text": "In fact, most of them are able to survive on their own, without being able to survive on their own. (...) Most of them are able to survive on their own. (...) Most of them are not able to survive on their own. (...) Most of them are able to survive on their own. (...) Most of them are able to survive on their own. (...) Most of them are able to survive on their own. (...) Most of them are able to survive on their own. (...) Most of them are able to survive on their own. (...) Most of them are able to survive on their own. (...) Most of them are able to survive on their own. (...) Most of them are able to survive on their own."}, {"heading": "6 Introduction", "text": "In the real world, however, some actions are irreversibly bad. If the robot goes down a cliff, it cannot learn from the mistake. \u2022 The real world is full of potentially fatal mistakes (e.g. crossing the road at the wrong time) and the natural mistakes are amplified by the training. \u2022 Kulkarni et al. (2016) introduce a hierarchical approach based on intrinsic motivation to improve the exploration of DQN and manage to score in Montezuma's Revenge. However, their approach relies on quite a bit of visual pre-processing and domain knowledge. \u2022 Non-ergodicity. When the agent loses in an Atari game, he always gets the same game. From the agent's perspective, he has not really failed, but is only put back into the starting state."}, {"heading": "1.1.3 General Reinforcement Learning", "text": "A theory of strong AI cannot fulfill some of the typical assumptions. Environments are partially observable, so we are dealing with partially observable Markov decision-making processes (POMDPs). The state space of POMDP does not have to be endless. Moreover, the environment cannot allow recovery from mistakes: we do not assume that the state of POMDPs can be reached from any other state. So, in general, our environments are infinite, not ergodic POMDPs. Table 1.1 lists the assumptions that are typical, but we do not make.Learning POMDPs is a lot more difficult, and only partially successful attempts have been made: through predictive state representations. 2003, 2004, and Bayesian Methods (Doshi-Velez, 2012)."}, {"heading": "1.2 Contribution", "text": "The goal of this thesis is not to increase AI capability and implement our results. As such, we are not trying to improve on the state of the art, and we are not trying to deduce practical algorithms. Instead, the focus of this thesis is to broaden our understanding of general amplification problems and therefore to ignore strong AI techniques. How a future implementation of strong AI will actually work is in the realm of speculation at this time. Therefore, we should make as few and as weak assumptions as possible. We ignore computer-related limitations to focus on the underlying problems. Of course, this is unrealistic, with unlimited computing power many traditional AI problems will become trivial: playing chess, Go, or backgammon can be solved by full expansion of the game tree. But the general RL problem will not be trivial: the agent must learn the environment and the balance between exploration and exploitation."}, {"heading": "10 Introduction", "text": "One insight from this thesis is that in practice, it is quite possible that the people who are able to survive themselves are also able to survive themselves. In practice, this means that the people who are able to survive themselves are not able to survive themselves. In practice, it is so that they are able to survive themselves. In practice, it is so that the people who are able to survive themselves are able to survive themselves. In practice, it is so that they are able to survive themselves. In practice, it is so that the people who are able to survive themselves are able to survive themselves. In practice, it is so that the people who are able to survive themselves are able to survive themselves. In practice, it is so that they are able to survive themselves."}, {"heading": "1.3 Thesis Outline", "text": "This work is based on the work of Leike and Hutter (2014a, b, 2015a, b, c, 2016); Leike et al. (2016a, b). During my doctoral thesis, I was also involved in the publications Leike and Heizmann (2014a, b, 2015); Heizmann et al. (2015, 2016), which are based on my research on termination analysis (in collaboration with Matthias Heizmann), Daswani and Leike (2015) (co-authored with Mayank Daswani in equal parts), Everitt et al. (2015) (co-authored with Tom Everitt in equal parts), Filan et al. (2016) (written by Daniel Filan as part of his honorary work supervised by Marcus Hutter and me)."}, {"heading": "12 Introduction", "text": "Leike and Hutter (2014a, 2015d) are related to the main approach of this dissertation, so the results are mentioned only in passing. A list of the theses written during my doctorate is given in Table 1.3 on page 14, with a corresponding chapter outline in Table 1.2. The core of our paper is found in chapters 5, 6 and 7. Each chapter begins with a quotation. If this is not blatantly obvious: these are false quotations, a desperate attempt to make the thesis less dry and humourless. None of the quotations was actually given by the person to whom they are attributed (to our knowledge)."}, {"heading": "14 Introduction", "text": "In recent years, it has been shown that people in the United States and in other parts of the world are able to understand themselves and understand what they are doing. [1] In recent years, it has been shown that people in the United States of America are able to understand themselves. [2] In recent years, it has been shown that people in the United States are able to understand themselves. [3] In recent years, it has been shown that people in the United States of America are able to understand themselves. [4] In recent years, the role of the individual has shifted. \"[4] In recent years, the people in the United States of America and in the United States of America and in the United States of America and in the United States of America and in the United States of America and in the United States of America and in the United States of America and in the United States of America and in the United States of America and in the United States of America and in the United States of America and in the United States of America and in the United States of America and in the United States of America and in the United States of America and in the United States of America and in the United States of America and in the United States of America have pushed the role.\""}, {"heading": "16 Preliminaries", "text": "We use x v y to indicate that x is a prefix of y, i.e. x = y1: | x |. Our examples often (implicitly) include the binary alphabet {0, 1}. In this case, we define the functions One, Zeros: X \u0445 \u2192 N, which count the number of ones and zeros in a string. Computability. A function f: X \u0445 \u2192 R is lower semicomputable iff the set {(x, q). X \u0445 Q | f (x) > q} is recursively enumerable. If f and \u2212 f are lower semicomputable, then f is called computable. See Section 6.1.2 for more computability definitions. Asymptotic notation. Let us leave f, g: N \u2192 R \u2265 0."}, {"heading": "2.1 Measure Theory", "text": "For an imputable set of real numbers that we apply, the set of probability distributions across space (B) is uncalculable (like the set of all infinite strings X), we must apply the machinery of measurement theory. This section provides a precise introduction to measurement theory; see Durrett (2010) for a comprehensive measurement (like the set of all infinite strings X). Let us apply a specified number of measurement variables. The specified F-2b algebra is a number of computational variables above the value F, (b) A-F implies an arrangement of A-F, and (c) for each imputable number of measurement variables A0, A1,. F, the Union i F. For a specified A-2b algebra, we define the smallest (A) to be the inclusion (in relation to the inclusion)."}, {"heading": "18 Preliminaries", "text": "X \u221e are measurable (assuming the axiom of choice). Although we do this for reasons of legibility, it should be noted that among some axioms that are compatible with the Zermelo-Fraenkel set theory, especially the determinism axiom, all subsets of X \u221e are measurable."}, {"heading": "2.2 Stochastic Processes", "text": "This section introduces some terms about sequences of random variables.Definition 2.4 (Stochastic Convergence) = > > > each year. (Xt) t \"N\" is a stochastic process, iff \"s is a random variable for each t.\" (Xt) t \"N\" (Martchy sequence, iff \"s is a constant c\" R. \"(Xt\" s sequence of random variables Convergence is much more subtle and there are several different terms of convergence. (Martchy sequence, iff \"s sequence.\"). (Xt \"s sequence of random variables Convergence is much more subtle and there are different terms of convergence.) Definition 2.5 (Stochastic Convergence)."}, {"heading": "2.3 Information Theory", "text": "This section introduces the terms entropy and two terms of distance between probability measures: KL divergence and total variation distance. Definition 2.9 (entropy). Definition 2.9 (x) > 0p (x) log p (x). Definition 2.10 (KL divergence). Let P, Q be two measures and let m \u00b2 N be a predictive time step. Kullback Leibler divergence (KL divergence) of P and Q between the time steps t and m is defined as KLm (P, Q | x < t): m \u00b2 Xm \u2212 t + 1 P (x1: m | x < t) logP (x1: m | x < Gibett) Q (X: < variation \u00b2 Q \u00b2 x) is always a divergence (K \u00b2)."}, {"heading": "20 Preliminaries", "text": "In contrast to the KL divergence, the total deviation fulfils the distance axioms: symmetry (D (P, Q) = D (Q, P)), identity of the indistinguishables (D (P, Q) = 0 if and only if P = Q) and the triangular inequality (D (P, Q) + D (Q, R) \u2265 D (P, R))). The following problem shows that the total deviation can be used for bound expectation differences. Lemma 2.12 (Total Variation Bound on the Expectation): For a random variable X with 0 \u2264 X \u2264 1 and two probabilities P and Q-EP [X] \u2212 EQ [X] \u2264 D (P, Q)."}, {"heading": "2.4 Algorithmic Information Theory", "text": "A universal Turing machine (UTM) is a Turing machine that can simulate all other Turing machines. Formally speaking, a Turing machine is a UTM iff for each Turing machine. There is a binary string p (referred to as a program), so U (p, x) = T (x) for all x (x) of a string x (x) of a string x (x) is the same as the output of T when executed on x. We assume that the set of programs on U is prefix-free. The Kolmogorov complexity K (x) of a string x is the length of the shortest program on U that x and then stops: K (x): A monotonous Turing machine is a turing machine with a one-way tape, one-way tape and one-way tape."}, {"heading": "22 Preliminaries", "text": "We use the prediction of the length of x: if x = then \u03bdnorm () = 1 = \u03bd (), and other wisse\u03bdnorm (xa) = \u03bdnorm (x) \u03bd (xa) \u2211 b \u00b2 X \u03bd (xb) \u2265 \u03bd (x) \u03bd (xa) \u2211 b \u00b2 X \u03bd (x) \u03bd (xa) \u03bd (x) = \u03bd (xa) = \u03bd (xa).The first inequality holds by induction hypothesis and the second inequality utilizes the fact that \u03bd is a semimeasure.Chapter 3LearningThe problem of induction is essentially solved. - David HumeMachine Learning Deniers for the process of learning models and / or making predictions of (large) data sets that are typically independent and identically distributed (i.i.d.); see Bishop (2006) and Hastie et al. (2009) In this chapter we do not make the prediction i.i.i."}, {"heading": "24 Learning", "text": "In general, the unrealizable case is more difficult, but Ryabko (2011) argues that on some issues both cases coincide. After introducing the formal structure in Section 3.1, we discuss several examples of learning distributions and terms that relate the learning distribution to the process that generates the data in Section 3.2. in Section 3.3, we combine these terms with, among other things, the theory of martyr processes. Section 3.6 links the results of the first sections with the learning framework developed by Solomonoff (1964, 1978), Hutter (2001b, 2005, 2007b) and Schmidhuber (2002). This framework relies, among other things, on results from algorithmic information theory and computational theory to learn a predictable distribution quickly and effectively. It is unpredictable (see Section 6.2), but can serve as the gold standard for learning."}, {"heading": "3.1 Setup", "text": "In fact, it is such that we cannot control the distribution of people in the world in which we are located. < < < < < < < < < < < < < < < < < < < < \"\" \"\" \"\", \"\" \"\" \"\" \"\" \"\" \"\" \",\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\", \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \",\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\", \"\" \"\" \"\" \"\" \"\" \"\" \",\" \"\" \"\" \"\" \"\" \"\" \"\" \"\", \"\" \"\" \"\" \"\" \"\" \",\" \"\" \"\" \"\" \"\" \"\" \"\" \",\" \"\" \"\" \"\" \"\" \"\", \"\" \"\" \"\" \"\" \"\" \"\" \"\", \"\" \"\" \"\" \"\" \"\" \"\" \"\" \",\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\"."}, {"heading": "3.2 Compatibility", "text": "In this section, we define dominance, absolute continuity, dominance with coefficients, weak dominance and local absolute continuity in decreasing order of strength. These terms make the relationship between the two probability measures P and Q precise. We also give examples of different decisions for the learning algorithm Q.In our examples, we often rely on the following process. Example 3.2 (Bernoulli process): Let's say X = {0, 1}. For a real number r [0, 1], we define the Bernoulli process with the parameter r as Bernoulli (r) (x): = rones (x) (1 \u2212 r) zeros (x).Note that Bernoulli (1 / 2) = \u03bb, the Lebesgue measure from Example 2.15. 3Definition 3.3 (Dominance): The measure Q dominates P (Q) iff there is a constant c > 0 so that Q (x) exceeds cx (P)."}, {"heading": "26 Learning", "text": "(X) D (S) D (S) D (S) D (S) D (S) D (S) D (S) D (S) D (S) D (S) D (S) D (S) D (S) D (S) D (S) D (S) D (S) D (S) D (S) D (S) D (S) D (S) D (S) D (S) D (S) D (S) D (S) D (S) D (S) D (S) D (S) D (S) D (S) D (S) D (S) D (S) D (S) D (S) D (S) D (S) D (S) D (S) D (S) D (S) D) D (S) D (S) D (S) D (S) D (S) D (S) D D (S) D D (S) D D (S) D (S) D (S) D (S) D (S) D (S) D (S) D (S) D (S) D (S) D (S) D (S) D (S) D (S) D (S) D (S) D (S) D (S) D (S) D (S) D (S) D (S) D (S) D (S) D (S) D (S) D (S) D (S) D (S) D (S) D (S) D (S) D (S) D (S) D (S) D (S) D (S) D (S) D (S) D (S) D (S) D (S) D (S) D (S) D (S) D (S) D (S) D (S) D (S) D (S) D (S) D (S) D (S) D (S) D (S) D (S) D (S) D (S) D (S) D (S) D (S) D (S) D (S) D (S) D (S) D (S) D (S) D (S) D (S) D (S) D (S) D (S) D (S) D ("}, {"heading": "28 Learning", "text": "It is not as if this is a real value, but a real value, which is related to the real values of the real world in relation to the real values of the real world in relation to the real values of the real world in relation to the real values of the real world in relation to the real realities of the real world in relation to the real realities of the real world in relation to the real realities of the real world in relation to the real realities of the real world in relation to the real realities of the real world in relation to the real realities of the real world in relation to the real realities of the real world in relation to the real realities of the real world in relation to the real realities of the real world in relation to the real realities of the real world in relation to the real realities of the real world in relation to the real realities of the real world in relation to the real realities of the real world in relation to the real realities of the real world in relation to the real realities of the real world in relation to the real realities of the real world in relation to the real realities of the real world in relation to the real realities of the real world in relation to the real realities of the real world in relation to the real realities of the real world in relation to the real realities of the real world in relation to the real realities of the real world in relation to the real world in relation to the real realities of the real realities of the real world in relation to the real realities of the real world in relation to the real realities of the real world in relation to the real realities of the real world in relation to the real realities of the real world in relation to the real realities of the real world in relation to the real realities of the real world in relation to the real realities of the real world in relation to the real realities of the real world in relation to the real world in relation to the real realities of the real world in relation to the real realities of the real realities of the real world in relation to the real realities of the real world in relation to the real realities of the real world in relation to the real world in relation to the real realities of the real world in relation to the real realities of the real realities of the real world in relation to the real realities of the real realities of"}, {"heading": "3.3 Martingales", "text": "The following two theorems represent the relationship between probability measurements on infinite strings and martyrdom. For two probability measurements P and Q, the quotient Q / P is a non-negative P martyrdom if Q is locally absolutely continuous in relation to P. Conversely, for each non-negative P martyrdom there is a probability measure Q L P, so that martyrdom is almost certainly a multiple of Q / P."}, {"heading": "30 Learning", "text": "X. X. X. X. X. (X). X. (X). X. (X). X. (X). X. (X). (X). (X). (X). (X). (X). (X). (X). (X). (X). (X). (X). (X). (X). (X). (X). (X). (X). (P). (X). (X). (X). (X). (X). (X). (X). (X). (X). (X). (X). (X). X. (X). X. (X). (X). (X). (X). (X). (X). (X). (X). (X). (X). (X. (X). (X. (X). (X). (X. (X). (X. (X). (X). (X. (X). (X). (X. (X). (X). (X. (X). (X). (X. (X). (X). (X. (X). (X). (X. (X). (X). (X). (X. (X). (X). (X). (X). (X). (X. (X). (X). (X. (X). (X). (X. (X). (X). (X). (X). (X). (X. (X). (X). (X. (X). (X. (X). (X. (X). (X. (X). (X). (X. (X). (X). (X). (X. (X). (X. (X). (X. (X). (X). (X. (X). (X. (X). (X). (X. (X). (X). (X. (X). (X). (X). (X."}, {"heading": "32 Learning", "text": "(b) Proven by Hutter (2009a, Lem. 3i). (c) Analogous to the detection of (a). (d) If Q P weakly dominates, we get \u2212 log Yt-o (t) according to note 3,9. Together with Y0 = 1 we get \u2212 log Yt = 1 k = 0 \u2212 log (Yk + 1 / Yk). (e) Let x-x be any finite string. If Q L P and P (x) > 0, then Q (x) > 0, then Q (x) > 0 and thus Q (x) / P (x) > 0. (e) Let x-X be any finite string. If Q L P and P (x) > 0, then Q (x) > 0, then Q (x) / P (x) > 0."}, {"heading": "3.4 Merging", "text": "If Q is capable of learning, it should use the order x drawn by P to shift its views more strongly toward P. Specifically, we want Q (\u00b7 | x < t) \u2248 P (\u00b7 | x < t) for big. In the rest of this chapter, we make this notion of proximity precise and discuss various conditions for Q that are sufficient for learning. Strong fusion implies that belief in each hypothesis merges, which is very strong since hypotheses can talk about tail events: events that are independent of an infinite initial part of the infinite sequence (such as event A in note 3.17). Weak fusion only takes into account hypotheses about the next two symbols, and almost weak fusion allows Q to deviate from P in a vanishing fraction of the time. Much of this section is based on Kalai and Teacher (1994) and Teacher and Smorodinsky (1996)."}, {"heading": "3.4.1 Strong Merging", "text": "Definition 3.24 (Strong Merging).Q merges strongly with P iff D \u221e (P, Q | x < t).Theorem 3.25 (Absolute Continuity \u21d2 Strong Merging; Blackwell and Dubins, 1962).Theorem 3.4 (Strong Merging; Blackwell and Dubins, 1962).If P is absolutely continuous in relation to Q, then Q merges strongly with P. Example 3.26 (The Black Ravens 2; Rathmanner and Hutter, 2011, Sec. 7.4).Recall the black raven problem from Example 3.1. Let Q be a learning distribution that dominates the true distribution P, such a bayesian mixture (Example 3.4).By Proposition 3.16a we get Q P, and hence Q merges strong to P by Theorem 3.25."}, {"heading": "3.4.2 Weak Merging", "text": "In definition 3.24, superiority extends to all measurable sets A, F. Instead, we can limit the superiority to the next symbols. This is known as weak fusion. Definition 3.29 (Weak fusion). Q weakly merges with P iff for each d, N, Dt + d (Q, P | x < t) \u2192 0 as t, P - almost certainly. The following problem results in an equivalent formulation for weak fusion. Lemma 3.30 (Teacher and Smorodinsky, 1996, Rem. 5). Q weakly merges with P when and only when Dt (Q, P | x < t) \u2192 0 as t, P -almost certainly. Unfortunately, weak dominance is not enough for weak fusion (Teacher and Smorodinsky, 1996, Ex. 10). We need the following stronger condition, which proves to be (almost) necessary."}, {"heading": "34 Learning", "text": "The following is a reversal of Theorem 3.31.Theorem 3.33 (Kalai and Teacher, 1994, Prop. 5b): If Q fuses weakly with P, then Yt + 1 / Yt \u2192 1 in P probability. Unfortunately, weak dominance is not enough to guarantee weak fusing. Example 3.34 (Weak dominance; Weakening merging; Ryabko and Hutter, 2007, Prop. 7). Leave X = {0, 1} and let f be any slow monotonous growth functions with f (t). Define P (1): = 1, the sequence (ti) i-N so that f (ti + 1) \u2265 2f (ti), andQ (xt | x < t): = 1 2 if t = ti for some i-N, 1 if t 6 = ti and xt = 1, and 0 otherwise."}, {"heading": "3.4.3 Almost Weak Merging", "text": "The following definition is based on Lehrer and Smorodinsky (1996, Def. 10).Definition 3.35 (Almost weak merger).Q almost weakly fuses with P iff for each d-N1t-K = 1 Dt + d (Q, P | x < t) \u2192 0 as t \u2192 \u221e P -almost secure.There is also an analogy to Lemma 3.30 for almost weak merger in the sense that we can put equivalent d = 0 (Lehrer and Smorodinsky, 1996, Rem. 6).Remark 3.36 (Weak merger and merging in KL divergence).From Lemma 2.13 it follows that weak merger is implied by KLd (P, Q | x) \u2192 0 P -almost certainly and almost weak merger with Yak (Merging and merging in KL divergence)."}, {"heading": "3.5 Predicting", "text": "In Section 3.4, we wanted Q to acquire the correct beliefs about P. In this section, we use the accuracy of our beliefs to predict individual symbols. We deduce limits from the number of errors Q makes when trying to predict a string drawn from P. Because the data drawn from P is stochastical, we cannot expect to make an infinite number of errors. Even the perfect forecaster who knows P generally makes an infinite number of errors. For example, when trying to predict the Lebesgue measurement (Example 2.15), we regret expecting to make half an error in each step of time. Instead, we ask for the asymptotic error rate of a predictor based on Q compared to a predictor based on P. Let xRt be the t symbol predicted by the probability measurement R according to the maximum probability."}, {"heading": "36 Learning", "text": "Example 3.41 (Good Prediction Regret; Merging / Compatibility): Good prediction repentance does not imply (weak / strong) merger or (weak) dominance: Allow P: = Bernoulli (1 / 3) and Q: = Bernoulli (1 / 4). Obviously, P and Q do not merge (weak) or dominate each other. However, a P-based predictor always predicts 0, and the same is true for a Q-based predictor. Therefore, the prediction repentance EQt \u2212 EPt is always 0. 3Example 3.42 (Adversarial Sequence; Legg, 2006, Lem. 4). No learning distribution Q will learn to predict everything. We can always define a Q-adversarial sequence z1: \u221e recursively after death: = {0 if Q (0 | z < t) < 1 if Q (0 | z < zzzz) is a prediction."}, {"heading": "3.5.1 Dominance", "text": "We start with the prediction confirmed by Hutter (2001b) if the learning distribution Q = 3.5 Q Q dominates the true distribution P. < < p > p > p > p > p > p > p > p (Hutter, 2007b, Eq. 5 & 8) For all P and Q, \u221a EPEQn \u2212 p = 2KLn (P, Q) The following prediction about EPP \u2212 p is then easy, but it is a factor worse than the limit set by Hutter (2005, Thm. 3.36).Corollary 3.44 (Expected Prediction Regret).For all P and Q, 0 \u2264 EP \u2212 EPn \u2212 p = 2KLn \u2264 2KLn (P, Q) + 2KLn (P, Q) EPEPEPEPEPn.Proof."}, {"heading": "38 Learning", "text": "According to the following theorem, the least favorable case limits are reached only if P (xt | x < t) q = \u03b5 (g) is sufficiently close to 1 / 2.Theorem 3.48 (Expected prediction remorse for non-uniform measurements). If X = {0, 1} and there is an increase > 0, so that | P (xt | x < t) \u2212 1 / 2 | \u2265 \u03b5 for all x1: t \u00b2 X, thenEP [EQt \u2212 EPt] \u2264 KLt (P, Q) \u03b5.Proof. Remember the definition of entropy in nature: Ent (p) \u2212 p: = \u2212 p \u2212 p \u2212 isp \u2212 isp \u2212 isp \u2212 isp \u2212 p \u2212 p \u2212 p \u2212 p = p \u2212 p \u2212 p (p \u2212 p) = p \u2212 p \u2212 ln) 2.You can check that f (p) \u2265 Entrop (p); p \u2212 isp \u2212 isp \u2212 isp \u2212 isf \u2212 p \u2212 p = p \u2212 \u2212 p \u2212 p \u2212 p \u2212 p = p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p."}, {"heading": "3.5.2 Absolute Continuity", "text": "Theorem 3.49 (prediction with absolute continuity) If Q P = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = = = = = = = = = = = = = = = = = = = = = = = = = = = = Q = Q = = = = = Q = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7"}, {"heading": "40 Learning", "text": "E [EQt \u2212 EPt] are not comparable to the near-safe limit given in Theorem 3.49: for a sequence of non-negative (unlimited) random variables, convergence means on average no near-safe convergence (Stoyanov, 2013, 14.7) or vice versa (Stoyanov, 2013, 14.8ii). If Q P, X = {0, 1}, and there is an improved prediction bound for P -probability 1 | P (xt | x < t) \u2212 1 / 2 | \u2265 convergence for all t-N (prediction repentance for non-uniform metrics), then P -almost certainly is EQt \u2212 EPt O (1). Proof: If | P \u2212 probability 1 / 2 | is high enough, then Qit is N, then P -almost certainly is another change (EPt \u2012 O (1).evidence."}, {"heading": "3.5.3 Dominance with Coefficients", "text": "Lemma 3.52 (KL Divergence and dominance with coefficients). If Q P dominates with coefficients f, then KLt (Q, P) \u2264 ln f (t).Evidence. Analogous to (3,5).From this can be derived an analogous regret, which is bound to the sequence 3.44.Conclusion 3.53 (Expected prediction Regret for dominance with coefficients). If Q P dominates with coefficients f, we get sublinear regret. \u00a7 3.6 Learning with Algorithmic Information Theory 41Episode 3.54 (Sublinear Prediction Regret for Weak Dominance)."}, {"heading": "3.6 Learning with Algorithmic Information Theory", "text": "Algorithmic information theory provides a theoretical framework for applying probability theory from the previous sections. In the following, we discuss Solomonov's famous theory of induction (Section 3.6.1), the speed before it (Section 3.6.2), and learning with a universal compression algorithm (Section 3.6.3)."}, {"heading": "3.6.1 Solomonoff Induction", "text": "Solomonoff (1964, 1978) proposed a theory of learning, also known as universal induction or Solomonoff induction. It includes Ockham's razor by favouring simple explanations over complex ones, and Epicurus \"principle of multiple explanations by never discarding possible explanations. See Rathmanner and Hutter (2011) for a very readable introduction to Solomonoff's theory and its philosophical motivations, and Sterkenburg (2016) for a critique of its optimality. At the heart of this theory is Solomonoff's distribution M as defined in Example 3.5. SinceM dominates all lower semicomputable metrics, we get all the results of the merger and prediction from Section 3.4 and Section 3.5: if we draw a string from each calculable metric P, M comes to the right belief for each hypothesis."}, {"heading": "42 Learning", "text": "It is not the first time that the EU Commission and the EU Commission have agreed on a common line. (...) It is not the first time that the EU Commission has adopted a common line. (...) It is the second time that the EU has adopted a common line. (...) It is not the first time that the EU has adopted a common line. (...) It is the second time that the EU has adopted a common line. (...) It is the second time that the EU has adopted a common line. (...) It is the second time that the EU has adopted a common line. (...) It is the third time that the EU has adopted a common line. (...) It is the third time that the EU has adopted a common line. (...) It is the third time that the EU has adopted a common line. (...) It is the third time that the EU has adopted a common line. (...) It is the third time that the EU has adopted a common line. (...) It is the third time that the EU has adopted a common line."}, {"heading": "3.6.2 The Speed Prior", "text": "Solomonoff's previous M is incalculable (theorem 6,3); a calculable alternative is the velocity given in Example 3,11. In this section we give merge and prediction results for SKt, a velocity formally defined by Filan et al. (2016) in Example 3,11. It differs slightly from the velocity previously defined by Schmidhuber (2002), but for the latter no compatibility properties are known for non-deterministic measurements. Definition 3,60 (Estimable in polynomial time). A function f: X-R is estimable in polynomial time iff there is a function g: X-R compatible in polynomial time, so that f - = g.For a measurement P-estimable in polynomial time, the velocity before SKt P with coefficients polynomial dominates in | x-logP (x) (Filan et al., 2016, Eq 12)."}, {"heading": "3.6.3 Universal Compression", "text": "Solomonoff's distribution can be approximated using a standard compression algorithm, motivated by the similarity M (x) \u2248 2 \u2212 Km (x), where Km denotes monotonous Kolmogorov complexity. Km is a universal compressor that compresses at least as well as any other recursively enumerated program. G\u00e1cs (1983) shows that the similarity M \u2248 2 \u2212 Km is not equality, but the difference between \u2212 logM and Km is very small: the best-known lower limit is due."}, {"heading": "44 Learning", "text": "Regarding Day (2011), which shows that Km (x) > \u2212 logM (x) + O (log log | x |) for an infinite number of x * X *. Nevertheless, 2 \u2212 Km dominates every computable metric (Li and Vit\u00e1nyi, 2008, Thm. 4.5.4 and Lem. 4.5.6ii (d); originally proved by Levin, 1973).Therefore, all the strong results that apply to solomonotic processing (prediction regret and strong merging) also apply to compression: we use Theorem 3.25 and Corollary 3.44 to obtain the following results. See Hutter (2006a) for further discussion on the use of the universal compressor Km for learning. The following 3.63 (Strong Merging for Universal Compression). The distribution 2 \u2212 Km (x) merges strongly with each compressible metric."}, {"heading": "3.7 Summary", "text": "The question of whether learning succeeds depends ultimately on the rate at which the non-negative P -martingale Q / P has absolute continuity (in general) 3.3.0 (in the drawing of P) 3.0 (in the drawing of P) 3.0 (in the drawing of P) 3.0 (in the drawing of P) 4.0 (in the drawing of P) 4.0 (in the drawing of Q / P) 4.0 (in the drawing of D) 4.0 (in the drawing of D) 4.0 (in the drawing of D) 4.0 (in the drawing of D) 4.0 (in the drawing of D) 4.0 (in the drawing of D) 4.0 (in the drawing of D) 4.0 (in the drawing of D) 4.0 (in the drawing of D) 4.0 (in the drawing of D) 4.0 (in the drawing of D) 4.0 (in the drawing of D) 4.0 (in the drawing of H).0 (in the drawing of H).0 (in the drawing of H).1 (in the drawing of H).0 (in the drawing of H).0 (in the drawing of H).0 (in the drawing of H).0 (in the drawing of H).0 (in the drawing of H).0 (in the drawing of H).0 (in the drawing of D) 4.0 (in the drawing of D) 4.0 (in the drawing of D) 4.0 (in the drawing of D) 4.0 (in the drawing of D) 4.0 (in the drawing of D) 4.0 (in the drawing) 4.0 (in the drawing of D) 4.0 (in the drawing of D) 4.0 (in the drawing (in the drawing of D) 4.0 (in the drawing) 4.0 (in the drawing of D) 4.0 (in the drawing) (in the drawing of D) 4.0 (in the drawing (in the drawing of D) 4.0 (in the drawing).0 (in the drawing).0 (in the drawing).0 (in the drawing of D).0 (in the drawing).0 (in the drawing of D).0 (in the drawing).0 (in the drawing).0 (in the drawing of D).0 (in the drawing).0).0 (in the drawing"}, {"heading": "48 Learning", "text": "And this solution to the confirmation paradox is known as the Standard Bayesian Solution. Vranas (2004) shows that this solution is consistent with the assertion that blackness is equally likely, regardless of whether it is H. P (black | H) \u2248 P (black).The following is a very precise example of the Standard Bayesian Solution of Good (1967): There are two possible worlds, the first has 100 black ravens and a million other birds, while the second has 1000 black ravens, a white raven and a million other birds. Now, we are drawing a bird that turns out to be a black raven. Contrary to what Nicod's criterion claims, this is strong evidence that we are in the second world, not black ravens and a million other birds."}, {"heading": "I ought never to act except in such a way that I could also will that my maxim should", "text": "In this chapter, we move on to the active environment: we look at an agent acting in an unknown environment in order to achieve a goal. In our case, this goal is to maximize the reward, but this is known as reinforcement. Where this reward signal originates, we do not care. In this thesis, we look at the general problem of reinforcing learning, where we do not make several of the typical simplistic assumptions (see Table 1.1). Environments are only partially observable, have an infinite number of states and may contain traps from which the agent cannot escape."}, {"heading": "50 Acting", "text": "Effective exploration is carried out by knowledge-seeking agents; these agents ignore the rewards and simply concentrate on exploration.This chapter introduces the central concepts of general reinforcement learning. It is largely based on Hutter (2005) and Lattimore (2013).Section 4.1 describes the general learning problem of reinforcement, discusses discounting (Section 4.1.1), our implicit assumptions (Section 4.1.2) and typical environmental classes (Section 4.1.3).Section 4.2 discusses the value function and its properties. In Section 4.3 we introduce the agents: AIXI (Section 4.3.1), knowledge-seeking agents (Section 4.3.2), BayesExp (Section 4.3.3) and Thompson sampling (Section 4.3.4)."}, {"heading": "4.1 The General Reinforcement Learning Problem", "text": "In Enhancing Learning, an agent interacts with an environment: in the time step t'N, the agent receives either a reward or a reward. < < < < < < < < History is an element of the (A \u00b7 E) story and lists the actions that the agent has taken and the perceptions he has received. We use \u00e6 A \u00b7 E to denote an interaction cycle, and \u00e6 < t = \u00e61\u00e62.. \u00e6t \u2212 1 to denote a history of length t \u2212 1. For our agent, the story is a sufficient statistic about the past, and in general amplification of learning, there is no simpler sufficient statistic. For example, we consider the agent as a robot that interacts with the real world. Its actions move the motors in its limbo and wheels and send data packets over a network connection."}, {"heading": "52 Acting", "text": "Problem 4.2 is deliberately kept vague: it does not say how we should balance achieving more rewards in some environments and achieving less in others. In other words, we leave open what is an optimal solution to the general learning problem of amplification. This turns out to be a notoriously difficult question that we will take in Chapter 5.As promised in the title of this paper, we will take the non-parametric approach. For the rest of this thesis, fixM is an arbitrary group of environments. While the true environment is unknown, we assume that it belongs to Class M (the realizable case). As long as Class M is sufficiently large (like the class of all calculable environments), this assumption is weak. Some typical decisions are made in Section 4.1.3.Our agent-environment setup, shown in Figure 4.1, is known as a dualistic model: The agent is different from the environment and influences it only by his actions."}, {"heading": "4.1.1 Discounting", "text": "To get around this technical problem, we let our agent prioritize the present over the future. This is done with a discrete function that quantifies how much the agent rewards now over subsequent rewards. Definition 4.3 (Discreet function): A discrete function is a function. Definition 4.3 (Discreet function) is a function. Definition 4.3 (Discreet function) is a function. Definition 4.3 (Discreet function) is a function. Definition 4.3 (Discreet function) is a function. (Discreet function). (Discreet function). (Discreet function). (Discreet function). (Discreet function). (Discreet function). (Discreet function). (Discreet function.). (Discreet function.). (Discreet function.). (Discreet function. (Discreet function.). (Discreet function. (Discreet function.). (Discreet function. (Discreet function.). (Discreet function. (Discreet function.). (Discreet function. (Discreet function.). (Discreet function.). (Discreet function. (Discreet function.). (Discreet function. (Discreet function.). (Discreet function. (Discreet function.). (Discreet function.). (Discreet function. (Discreet function.). (Discreet function.). (Discreet function. (Discreet function.).). (Discreet function. (Discreet function.). (Discreet function. (Discreet function.). (Discreet function. (Discreet function.). (Discreet function. (Discreet function.). (Discreet function.). (Discreet function. (Discreet function.). (Discreet function. (Discreet function. (.). (Discreet function. (Discreet function.). (Discreet function. (Discreet function. (Discreet function.). (Discreet function. (Discreet function.). (Discreet function"}, {"heading": "4.1.2 Implicit Assumptions", "text": "(a) The discount function \u03b3 is calculable. (b) Rewards are between 0 and 1. (c) The amount of actions A and the amount of perceptions E. This is important for Chapter 6 and Chapter 7, where we analyze the compatibility of optimal strategies. Note that all discount functions specified in Table 4.1 are compatible. Assumption 4.6a is a technical assumption that ensures that discounted reward sums are calculable. We can use rewards 7 rescale > crt + d for any compatibility of optimal strategies. Note that all discount functions specified in Table 4.1 are compatible. (Assumption 4.6b could be loosened to require only that rewards are limited."}, {"heading": "54 Acting", "text": "Note that finite spaces of action and perception are very natural, as they ensure that our agent receives and transmits only a finite amount of information at each step of the time. This is consistent with the problems faced by strong artificial intelligence: the agent must memorize important information and act sequentially. Assumption 4.6b, Assumption 4.6c, and the fact that the discount function is a summable guarantee that there is an optimal deterministic policy for each environment according to Lattimore and Hutter (2014, Thm. 10). It would be interesting to loosen these assumptions while preserving the existence of optimal strategies or at least semi-optimal strategies (e.g. using compact measures and perceiving spaces)."}, {"heading": "4.1.3 Typical Environment Classes", "text": "The simplest amplification problems are many-armed bandits or drag this uncertainty to the highest level? \"\" The simplest amplification problems complicate learning. \"\" The simplest amplification problems. \"\" The simplest amplification problems. \"\" The simplest amplification problems. \"\" The simplest amplification problems. \"\" The simplest amplification problems. \"\" \"The simplest amplification problems.\" \"\" The simplest amplification problems. \"\" \"\" The simplest amplification problems. \"\" \"\" The simplest amplification problems........ \"\" \"\" \"The simplest amplification problems...........\" \"\" \"\" \"The simplest amplification problems............\" \"\" \"\" The simplest amplification problems.............. \"\" \"\" \"\" \"The simplest amplification problems.................\" \"\" \"\" \"\" \"\" \"\"..... \"\" \"\" \"\" \".......\" \"\" \"\" \"\" \"..........\" \"\" \"\" \"\""}, {"heading": "56 Acting", "text": "Formally, we define setMCCSLSC as a set of environments that have lower semicomputable contextual chronological semimeasures, and MCCMcomp as a set of environments that are computable chronological contextual measures. Note that for chronological contextual semimeasures, it makes a difference whether \u03bd (\u00b7 \u27e9 a1: \u221e) is lower semicomputable or the conditionals \u03bd (\u00b7 | \u00e6 < tat), the latter implying the former, but not vice versa."}, {"heading": "4.2 The Value Function", "text": "The value of a policy in an environment is the future expected discounted reward when we follow a specific policy in a given environment, due to the past. Since this value captures exactly what our agent is trying to maximize, we prefer strategies whose value is high. < p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p p = p p = p p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p."}, {"heading": "4.2.1 Optimal Policies", "text": "An optimal policy is a policy that achieves the highest value: Definition 4,12 (Optimal policy; Hutter, 2005, Def. 5,19 & 5,30). A policy is optimal in the environmental field. (Constance-optimal) iff for all historical developments. Definition 4,12 (Optimal policy; Hutter, 2005, Def. 5,19 & 5,30). A policy is optimal in the environmental field. (Constance-optimal) iff for all historical developments. Definition 4,12 (Optimal policy; Hutter, 2005, Def. 5,19 & 5,30). Definition 4,12 (Optimal policy; Constance-optimal) iff for all historical developments. (at | \u00e6 < t) = 1 for a constant optimal policy. (As opposed to a semi-authoritative measure)."}, {"heading": "58 Acting", "text": "Definition 4.13 (\u03b5-optimal policy): A policy \u03c0 is \u03b5-optimal in the environmental sphere \u0432 iff V \u0445 \u03bd (\u00e6 < t) \u2212 V \u03c0\u03bd (\u00e6 < t) < \u03b5 for all histories \u00e6 < t \u0445 (A \u00b7 E) \u0445.A policy \u03c0 that achieves optimal t0 value, V \u03c0\u03bd () = V \u0445 \u03bd (), adopts m-optimal measures in relation to any history that can be achieved by \u03c0 in \u03bd. However, this does not apply to \u03b5-optimal policy: a policy that is \u03b5-optimal at t = 0 is not necessarily \u03b5-optimal in later stages."}, {"heading": "4.2.2 Properties of the Value Function", "text": "The following two Lemmas are by Hutter (A | < T) = A < Z < Z < Z < T) < Z (Z) < Z (Z) < Z (Z) < Z (Z) < Z (Z) < Z (Z) < Z (Z) < Z); Z (Z); Z (Z); Z (Z); Z); Z (Z); Z (Z); Z (Z); Z (Z); Z (Z (Z); Z (Z); Z (Z); Z (Z); Z (Z); Z (Z); Z (Z); Z; Z; Z); Z (Z; Z; Z; Z); Z (Z; Z); Z (Z); Z); Z (Z) (Z); Z); Z (Z); Z (Z); Z (Z); Z (Z); Z (Z); Z (Z); Z (Z); Z (Z); Z (Z); Z; Z; Z; Z; Z; Z; Z; Z); Z; Z); Z (Z); Z (Z); Z (Z); Z); Z (Z); Z (Z); Z (Z); Z (Z; Z; Z); Z; Z; Z; Z; Z; Z; Z); Z; Z; Z); Z (Z); Z (Z; Z; Z; Z); Z; Z; Z; Z; Z); Z (Z); Z; Z); Z (Z; Z; Z); Z (Z); Z (Z); Z; Z; Z); Z (Z); Z (Z); Z; Z); Z (Z); Z (Z); Z (Z); Z (Z); Z (Z); Z (Z); Z; Z; Z); Z; Z); Z (Z); Z (Z); Z (Z); Z (Z); Z (Z); Z); Z); Z (Z (Z); Z (Z); Z); Z (Z"}, {"heading": "4.2.3 On-Policy Value Convergence", "text": "Convergence of political values refers to the fact that if we use a learning distribution \u03c1 to learn in the environment \u00b5, and \u03c1\u03c0 merges with \u00b5\u03c0 in the sense discussed in Section 3.4, then V \u03c0\u03c1 converges to V \u03c0\u00b5, i.e. if we use \u03c1, we learn to estimate values correctly. A weaker variant of the following theorem has been demonstrated by Hutter (2005, Thm. 5.36)."}, {"heading": "60 Acting", "text": "Theorem 4.19 (On-Policy Value Convergence): < < < < T) < T) < T (0) T (0) T (0) T (0) T (0) T (0) T (0) T (1) T (0) T (0) T (0) T (0) T (1) T (0) T (0) T (0) T) T (0) T) T (0) T (0) T) T (1) T) T (1) T) T (T) T (1) T) T (1) T) T (1) T) T (T) T (1) T (T) T (0) T) T) T (0) T) T (0) T) T (T) T (1) T) T) T (\"T."}, {"heading": "62 Acting", "text": "Conclusion 4.23 (On-Policy Value Convergence for the Speed Prior): If the effective horizon is limited, then for any environment that can be estimated in polynomial time, and for any policy \u03c0, 1t \u2211 k = 1 (V \u03c0SKt (\u00e6 < k) \u2212 V \u03c0 \u00b5 (\u00e6 < k)) \u2192 0 as t \u2192 \u221e \u00b5\u03c0-almost certain. Proof. As a result 3.61, the velocity before SKt merges almost faintly with any measure estimated in polynomial time. Therefore, we can apply the theorem 4.19c."}, {"heading": "4.3 The Agents", "text": "If we knew the true environment, we would choose the \u00b5-optimal policy, the policy that maximizes the expected \u00b5 rebates. But, in general, we do not know the true environment, and the difficult part of reinforcement learning is learning the environment while trying to collect rewards. In this section, we present a number of actors who are trying to solve the general problem of reinforcement learning (problem 4.2)."}, {"heading": "4.3.1 Bayes", "text": "A Bayes optimal policy with respect to the previous policy is the policy. < B > Q = universal mix defined in Section 4.2.3. There may be one or more Bayes strategies. < B < B & B: one or more Bayes strategies. < B & B: one or more Bayes strategies. < B & B: one or more Bayes strategies. < B & B: one or more Bayes strategies. < B & B: one or more Bayes strategies. < B < B: one or more strategies. < B & B: one or more strategies. < B < B: one or more Bayes strategies. < B & B: one or more Bayes strategies. < B < B: one or more Bayes strategies. < B: one or more Bayes strategies. < B: one or more strategies. < B: one or more strategies. < B: one or more strategies. < B: one or more strategies. < B: one or more strategies. < B: one or more strategies. < B: an optimal strategies. < B < B: an optimal strategy. < B < B: an optimal strategy. < B: an optimal strategy. < B: an optimal strategy. < B: an optimal strategy. < B: an optimal strategy. < B: an optimal strategy. < B: an optimal strategy. < B: an optimal. < B: an optimal strategy. < B: an optimal strategy. < B: an optimal policy. < B: an optimal policy. < B: an optimal policy. < B: an optimal policy. < B: an optimal policy. < B: an optimal policy. < B: an optimal one. < B: an optimal policy. < B: an optimal one. < B: an optimal policy. < B: an optimal policy."}, {"heading": "4.3.2 Knowledge-Seeking Agents", "text": "In this section, we discuss two variants of knowledge-seeking agents: entropy-seeking agents introduced by Orseau (2011, 2014a), and information-seeking agents introduced by Orseau et al. (2013). The entropy-seeking agent maximizes the gain of Shannon entropy, while the information-seeking agent maximizes the expected gain in information. These quantities are expressed in different value functions. In places where confusion may occur, we call the value function V \u03c0\u03bd from definition 4.10 the value-seeking function. In this section, we use a finite horizon m < \u221e (possibly depending on time step t): The knowledge-seeking agent maximizes the entropy / information received up to time step m. We implicitly assume that m (as a function of t) is predictable. Furthermore, in this section, we assume that the Bayesian mix measure is a sixth and not a function < this sect-max. < an example: 2014p; a semimax."}, {"heading": "64 Acting", "text": "The entropy search for value is the Bayes expectation of the \u2212 log-in-order. Orseau (2011, 2014a) also considers a related value function based on the expectation that we will not discuss here.Definition 4.26 (Information search value function; Orseau et al., 2013, Def. 1). The information-seeking value of a policy based on history is < t isV. Definition 4.26 (Information search value function; Orseau et al., 2013, Def. 1). Definition search value of a policy based on given history. < t isV. Definition Entrop. / V."}, {"heading": "4.3.3 BayesExp", "text": "Lattimore (2013, Thm. 5,6) defines BayesExp by combining AIXI with the information-seeking agent. BayesExp switches between phases of exploration and phases of exploitation: Let it be a monotonous, decreasing sequence of positive reals, so \u03b5t \u2192 0 as t \u2192 \u221e. If the optimal information-seeking value V * IG is greater than \u03b5t, BayesExp begins an exploration phase, otherwise an exploitation phase begins. During an exploration phase, BayesExp pursues an optimal information policy for a \u03b5t-effective horizon. During an exploitation phase, BayesExp pursues an optimal reward-seeking policy for one step (see Algorithm 1). Algorithm 1 BayesExp policy \u03c0BE (Lattimore, 2013, Alg. 2)."}, {"heading": "4.3.4 Thompson Sampling", "text": "Thompson sampling, also known as posterior sampling or the Bayesian control rule, was originally proposed by Thompson (1933) as a bandit algorithm. It is easy to implement and often achieves quite good results (Chapelle and Li, 2011); for multi-armed bandits, it achieves optimum regret (Agrawal and Goyal, 2011; Kaufmann et al., 2012); Thompson sampling has also been discussed for MDPs (Strens, 2000; Dearden et al., 1998) and Bayesian and frequent regrets have been limited (Osband et al., 2013; Gopalan and Mannor, 2015); for general RL Thompson sampling was first proposed by Ortega and Braun (2010), with each time being rehearsed anew. Strens (2000) suggests following the optimal policy for an episode or \"in connection with the number of state transitions.\""}, {"heading": "66 Acting", "text": "Environmental policy is an optimal policy. < V \u00b2 an optimal policy. < V \u00b2 an optimal policy. < V \u00b2 an optimal policy. < V \u00b2 an optimal policy. < V \u00b2 an optimal policy. < V \u00b2 an optimal policy. < V \u00b2 an optimal policy. < V \u00b2 an optimal policy. < V \u00b2 an optimal policy. < V \u00b2 an optimal policy. < V \u00b2 an optimal policy. < V \u00b2 an optimal policy. < V \u00b2 an optimal policy. < V \u00b2 an optimal policy. < V \u00b2 an optimal policy."}, {"heading": "68 Optimality", "text": "This means that the value of the policy adjusts to the optimal value (Ps < t) < t) \u2212 V \u03c0\u00b5 (\u00e6 < t) < t > 0 as asymptotic optimality (Hutter, 2005, Sec. 5.3.4).The value of the policy converts to the optimal value (S < t) \u2212 V \u03c0\u00b5 (S < t) < t > 0 as asymptotic optimality (Hutter, 2005, Sec. 5.3.4).The difference between the reward sum of the policy and the best policy in hindsight grows sublinear: sup."}, {"heading": "5.1 Pareto Optimality", "text": "In this section, we show that Pareto-optimism is not a useful criterion for optimism, because for every environmental class that MCCMcomp contains, all policies are Pareto-optimism. Definition 5,1 (Pareto-optimism; Hutter, 2005, Def. 5,22). A policy \u03c0 is Pareto-optimism. The literature delivers the following result: Theorem 5,2 (AIXI is Pareto-optimism; Hutter, 2002a, Thm. 2). Any optimal policy is Pareto-optimism and V-optimism policy is Pareto-optimism. The literature delivers the following result: Theorem 5,2 (AIXI is Pareto-optimism; Hutter, 2002a, Thm. 2). Any optimal policy is Pareto-optimism and V-optimism policy."}, {"heading": "70 Optimality", "text": "The environment is predictable, even if politics is not: For a set story \u00e6 < t and action \u03b1 there is a program that calculates \u00b5; hence \u00b5-MCCMcomp. We get the following value difference for policies \u03c0 and zipp: V \u03c0\u00b5 () \u2212 V \u03c0 () = E\u03c0\u00b5 [k \u2212 1 \u2211 t = 1 \u03b3trt + \u221e t = k \u03b3trt] \u2212 E\u03c0 [k \u2212 1 \u0445 t = 1 \u03b3trt \u2212 \u221e t = (Phillips < k) = (Phillips < k). < k). < k). < k). < p < k). < k). < k). < k). < p < k) = (Phillips < k). < p < p < p < k)."}, {"heading": "5.2 Bad Priors", "text": "In this section, we give three examples of universal priorities that cause an AIXI to behave drastically. In the case of a limited horizon, indifference to AIXI makes all measures equally advantageous (Section 5.2.1). The dogmatic prior keeps AIXI adhering to a given predictable policy \u03c0 as long as the expected future benefits do not fall too close to zero (Section 5.2.2). The G\u00f6del prior prevents AIXItl from taking measures (Section 5.2.3)."}, {"heading": "5.2.1 The Indifference Prior", "text": "The following theorem constructs the indifference to it, which results in a Bayesian mixture that leads to an increase in bindings for the first m steps. (< m) In this case, AIXI's behavior only depends on how we break the Argmax bonds. (Theorem 5.4 (Indifference before). (Theorem 5.4) If there is an m that is m = 0, then there is a Bayesian mixture that all policies are such that all policies are \"-optimal.Proof.\" First, we assume that the scope of action is binary, A = {0, 1}. (Let's use the reference UTM and the definition of the UTM U \u2032 byU \u2032 (s < mp, a1: t): = U (p, a1: t xor s1: t), where s & ltlt; m: the binary chain of length \u2212 1 and sk: = 0 for k( U)."}, {"heading": "5.2.2 The Dogmatic Prior", "text": "In this section, we define a universal prior who assigns a very high probability of going to hell (reward 0 forever) if we deviate from a given calculable policy. Therefore, for a Bayesian agent like AIXI, it is only worthwhile to depart from politics \u03c0 if the agent believes that the prospects of following it are already very poor. We call this before the dogmatic prior, because the fear of going to hell corresponds to any \"dogmatic ideology.\" AIXI will only erupt when he expects that the future payout will be very small; in this case, the agent has not much to lose. Theoretical 5.5 (dogmatic prior): Unless any deterministic policy, let alone any Bayesian mix via MCCSLSC, and let it happen > 0. There is a Bayesian mix so that the agent behaves in accordance with any story <"}, {"heading": "72 Optimality", "text": "The proof of theory 5.5. We assume that a slightly greater weight than a slight loss arises without assuming otherwise. < for each environment (MCCSLSC) we define the environment (E < k) if we (E < k) if we (E < k) if we (E < k) if we (E < k) if we (E < k) if we (E) if we (E), not (E), not (E), not (E), not (E), not (E), not (E), not (E), not (E), not (E), not (E), not (E), not (E), not (E), not (E), not (E), not (E), not (E)."}, {"heading": "5.2.3 The G\u00f6del Prior", "text": "This section introduces a stipulation that prevents any fixed formal system from making statements about the result of virtually endless calculations. It is named after G\u00f6del (1931), who famously showed that for every sufficiently rich formal system there are statements that it can neither prove nor refute."}, {"heading": "74 Optimality", "text": "H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-H.-"}, {"heading": "5.3 Bayes Optimality", "text": "The goal of the Legg-Hutter Intelligence Measure is to formalize mathematically the intuitive notion of intelligence. Legg and Hutter (2007a) collect different definitions of intelligence in many academic areas and distill them into the following statement (Legg and Hutter, 2007b). Intelligence measures an agent's ability to achieve goals in a wide range of environments. This definition is formalized as the following. Definition 5.9 (Legg-Hutter Intelligence; Legg and Hutter, 2007b, Sec. 3.3)). The (Legg-Hutter) intelligence of a policy is weighted by the previous w. Legg and Hutter (2007b) consider a subclass of MCCSLSC, the class of compatible measures along with a Solomonoff value that is achieved across all environments."}, {"heading": "76 Optimality", "text": "Proposition 5.10 (Bayes Optimality = Maximum Intelligence).Proposition 5.10 (Bayes Optimality = Maximum Intelligence).Proposition 5.10 (Bayes Optimality = Maximum Intelligence).Proposition 5.10 (Bayes Optimality = Maximum Intelligence).Proposition 5.10 (Bayes Optimum Intelligence).Proposition 5.10 (Bayes Optimum Intelligence). Proposition 5.10 (Bayes Optimum Intelligence). Proposition 5.10 (Bayes Optimum Intelligence). Proposition 6.10 (VII Optimum Intelligence). Proposition 5.10 (Bayes Optimum Intelligence). Proposition 5.10 (Bayes Optimum Intelligence). Proposition 5.10 (Bayes Optimum Intelligence). Proposition 5.10 (VII Optimum Intelligence). Proposition 5.10 (VII Optimum Intelligence). Proposition 5.10 (Optimum Intelligence). Proposition 5.10 (VII Optimum Intelligence)."}, {"heading": "78 Optimality", "text": "Along with the 4.16 result, < < < / p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p"}, {"heading": "5.4 Asymptotic Optimality", "text": "An asymptotic optimal policy is one that learns to act optimally in any environment. < p > p > p > p > p > p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p"}, {"heading": "5.4.1 Bayes", "text": "In this section, we list two results from the literature on the asymptotic optimality of Bayes \"optimal policy: The following negative result is due to Orseau (2010, 2013). Theorem 5.22 (Bayes is not asymptotically optimal in the general environment; Orseau, 2013, Thm. 4). For each classM MCCMcomp no Bayes optimal policy \u03c0 is asymptotically optimal: There is an environment \u00b5 and a time step t0 \u0445 N, so that the current strategy is ultimately sufficient and that any additional exploration is not worth its expected benefit. If the environment changes thereafter, the Bayes agent acts suboptimally. Proof: Without loss of universality, a Bayesian agent decides that the current strategy is good enough and that any additional exploration is not worth the expected reward."}, {"heading": "82 Optimality", "text": "The duration of the state sequence is defined as a 1 / t effective horizon, n: = Ht (1 / t), in which t is the time step in which the agent leaves the state s0. Since the discount function \u03b3 is calculable by assuming 4.6a, the agent of Bayes when acting in this area is infinitely frequent. Let's < t be a story in which the agent is s0 and takes action. Then, the agent of Bayes is 1 / t. Through the political value of convergence (Corollary 4.20), V."}, {"heading": "5.4.2 BayesExp", "text": "The definition of BayesExp is given in Section 4.3.3. In this subsection, we cite a result from Lattimore (2013) that motivated the definition of BayesExp 5.24 (BayesExp is weakly asymptotically optimal; Lattimore, 2013, Thm. 5.6), which is the policy of algorithm 1. If Ht (\u03b5) grows in t and Ht (\u03b5t) / \u03b5t (t) monotonously, then for all environments \u00b5-M1t-k = 1 (V-\u00b5 (\u00e6 < k) \u2212 V-BE\u00b5 (\u00e6 < k) \u2192 0 as t-p-t / \u03b5t-t-t-t-t (t) almost certainly. If the horizon grows sublinear (Ht-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-"}, {"heading": "5.4.3 Thompson Sampling", "text": "In this section, we demonstrate that the Thompson sampling policy defined in Section 4.3.4 is asymptotically optimal. < Ortega and Braun (2010) demonstrate that the probabilities of action of the Thompson sampling policy approximate to the probability of action of the optimal policy, but require a finite environmental class and two (arguably quite strong) technical assumptions about the behavior of the subordinate distribution (similar to ergodicity) and the similarity of environmental conditions in the class. Our convergence results do not require these assumptions. Theorem 5.25 (Thompson sampling, however, is asymptotically optimal in Mean). For all environments \u00b5 M, E\u03c0T\u00b5 [V sampling policy]. < t) \u2212 V sampling policy does not require these assumptions."}, {"heading": "86 Optimality", "text": "We assume that this sum is sufficient in all cases. < < < < < < < < < T) D (< T) D (< T) (< T) (< T) (< T) (< T) (< T) (< T) (< T) (< T) (< T) (< T) (< T) (< T) (< T) (< T) (< T) (< T) (< T) (< T) (< T) (< T) (< T) (\")."}, {"heading": "88 Optimality", "text": "What along with Lemma 4.17 and the fact that the reward in [0, 1] implies that it is at least (\u03b2 < t) - V, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p,"}, {"heading": "5.4.4 Almost Sure in Ces\u00e0ro Average vs. in Mean", "text": "It may seem that the convergence of averages is more natural than the convergence of Ces\u00e0ro's averages of weak asymptotic optimality. However, the two views are not so fundamentally different because they both allow an infinite number of errors (actions that cause V \u0445 \u00b5 \u2212 V \u03c0\u00b5 to be large)."}, {"heading": "90 Optimality", "text": "Errors as long as their probability converges to zero; weak asymptotic optimality allows bad errors as long as the total time spent on bad errors grows sublinearly. Note that, according to Example 5.19, bad errors are infinitely often necessary for asymptotic optimality. Theorem 5.24 shows that weak asymptotic optimality is possible in any countable class of stochastic environments. However, this requires the additional condition that the effective horizon grows sublinearly, Ht (\u03b5t), while Theorem 5.25 does not require a condition for discounting. Generally, weak asymptotic optimality and asymptotic optimality are incomparable on average, because the terms convergence for (limited) random variables are incomparable. First, for deterministic sequences (i.e. deterministic strategies in deterministic environments), convergence is on average equal to (regular) convergence, which is impossible by 5.20."}, {"heading": "5.5 Regret", "text": "Regret is how many expected rewards the agent forfeits by not following the best informed policy. < Regret a policy \u03c0 in the environment \u00b5 isRm (\u03c0, \u00b5): = sup. < Regret \u00b7 \u00b7 p = 1 rt] \u2212 p > p > p > p > p > p = 1 rt].Note: Regret is not discounted and is not always negative. Furthermore, the scope of possible different policy measures for the first m measures is limited and we assumed that the set of measures A and the set of perceptions E will be limited (Assumption 4.6c), so that the supremum is always achieved by some policy (not necessarily the micro-optimal policy, because politics uses discounting).Different problem classes have different rates of regret, depending on the structure and difficulty of the problem class. Multi-armed bandits offer a (worst case) regret regarding the number of weapons (Bubeck and Bianchi, 2012)."}, {"heading": "5.5.1 Sublinear Regret in Recoverable Environments", "text": "This subsection is dedicated to the following theorem, which combines asymptotic optimality on average with sublinear regret."}, {"heading": "92 Optimality", "text": "The question we have to ask ourselves is whether we will be able to find a solution with which we will not grow too fast. (c) Ht (n) Ht (n) s (n) s (n) s (n) s (n) s (n) s (n) s (n) s (n) s (n) s (n) s (n) s (n) s (n) s (n) s (n) s (n) s (n) s (n) s (n) s (n) s (n) s (n) s (n) s (n) s (n) s (n) s (n) s (n) s (n) s (n) s (n) n (n) s (n) n (n) s n (n) n (n) s n (n (n) n (n (n) n (n (n) n (n (n) n (n (n) n (n (n) n (n) n (n (n) n (n) n (n) n (n (n) n (n) n (n) n (n) n (n) n (n (n) n (n) n (n) n (n (n) n) n (n (n) n (n) n (n) n (n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n) n (n (n) n) n (n) n (n (n) n) n (n (n) n (n (n) n) n (n (n) n) n (n) n) n (n) n (n) n) n (n (n (n) n) n (n) n) n (n) n (n) n (n (n) n (n) n) n) n (n (n) n (n) n) n (n (n (n) n (n) n) n) n (n (n) n (n) n (n (n) n) n (n (n) n) n (n) n (n) n (n) n (n) n (n) n) n (n"}, {"heading": "94 Optimality", "text": "And using the assumption (5.8) and the assumption (5.9), we want to show that we are not in a position. < t0 + m \u00b2 t = 4.0 (4.9). < t0 + 4.0 (4.9). < t0 + 4.0 (4.1). < t0 (4.9). < t0 (4.9). < t0 (4.9). < t0 (4.0 + 1). < t0 (4.9). < t0 (4.9). < t0 (4.9). < t0 (4.9)."}, {"heading": "5.5.2 Regret of the Optimal Policy and Thompson sampling", "text": "The following immediate consequences result from consequence 5.37 (Sublinear regret about the optimal discount policy): If the discount function \u03b3 fulfills assumption 5.34 and the environment \u00b5 fulfills the utilization assumption, then Rm (\u03c0 \u0445 \u00b5, \u00b5).Proof. From theorem 5.33 onwards, since the policy \u0445 \u00b5 (trivial) is asymptotically optimal in {\u00b5}. If the environment does not fulfill the utilization assumption, regret can be linear even with the optimal policy: The optimal policy maximizes the discounted rewards."}, {"heading": "96 Optimality", "text": "Conclusion 5.38 (Sublinear regret for Thompson sampling). If the discount function \u03b3 fulfills assumption 5.34 and the environment \u00b5-M fulfills the recovery assumption, then Rm (\u03c0T, \u00b5) and o (m) for the Thompson sampling policy \u03c0T. Proof. from Theorem 5.25 and Theorem 5.33."}, {"heading": "5.6 Discussion", "text": "Because of this, our agents learn very efficiently and we can focus on how to balance exploration and exploitation, so what is the best balance?"}, {"heading": "5.6.1 The Optimality of AIXI", "text": "Maximizing expected rewards in accordance with any positive predictions does not result in sufficient exploration to achieve asymptotic optimality (Theorem 5.XI); the previous prediction is maintained indefinitely; however, for poor predictions, this can cause serious malfunctions: the dogmatic predictions defined in Section 5.2.2 may prevent a Bayean agent from taking a single exploratory action; exploration is limited to cases where the expected future payout falls under some predetermined points > 0. However, this problem can be mitigated by adding an additional exploration component to BayXI: Lattimore (2013) shows that BayesExp is weakly asymptotically optimal (Theorem 5.24).Instead, we can ask the following weaker questions: Will AIXI succeed in any (earthbound) finite state (PO) MDP, bandit problem or sequence prediction task?"}, {"heading": "5.6.2 Natural Universal Turing Machines", "text": "The choice of the UTM has long been a big open question in algorithmic information theory. Kolmogorov complexity of a string depends on this choice. However, there are invariance theorems (Li and Vit\u00e1nyi, 2008, Thm. 2.1.1 & Thm. 3.1.1) that state that the change in the UTM changes Kolmogorov complexity only by a constant. If one uses the Solomonoff before M to predict a deterministic, calculable binary sequence, the number of false predictions is limited by the Kolmogorov complexity of the sequence (episode 3.56). Due to the inventory theory, changing the UTM changes the number of errors only by a constant. In this sense, compression and prediction work for any choice of the UTM. For AIXI, there can be no complexity of the sequence (episode 3.56)."}, {"heading": "98 Optimality", "text": "Unfortunately, there is no stationary distribution. Alternatively, we could demand that the UTM U \u2032 that we use for the universal prior has a small compiler on the reference machine U (Hutter, 2005, p. 35). Furthermore, we could demand the opposite that the reference machine U has a small compiler on U \u2032. The idea is that this should limit the degree of bias that can be introduced by defining a UTM that has very small programs for very complicated and \"unusual\" environments. Unfortunately, this forces the choice of the UTM only on the reference machine. Table 5.2 lists the compiler sizes of the UTMs constructed in this thesis."}, {"heading": "5.6.3 Asymptotic Optimality", "text": "We discussed two asymptotically optimal strategies. BayesExp is weakly asymptotically optimal when the horizon exhibits sublinear growth (theorem 5.24) and Thompson's sampling is asymptotically optimal on average (theorem 5.25). Both strategies commit to exploration in multiple steps. As explained in Example 5.19: In order to achieve asymptotic optimality, the drug must explore an infinite number of times for an entire effective horizon. This is why weak asymptotic optimality is impossible when the horizon grows linearly (theorem 5.21): When the drug investigates for an entire effective horizon, it corrupts a significant fraction of the average. Thompson sampling examines whenever it draws a bad environment. BayesExp examines whether the maximum expected information gain is above a certain threshold. Both strategies commit to exploration for the entire effective horizon."}, {"heading": "5.6.4 The Quest for Optimality", "text": "This year, it is only a matter of time before an agreement is reached."}, {"heading": "6.1 Background on Computability", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6.1.1 The Arithmetical Hierarchy", "text": "A set A N is a set 0n if there is a quantifier-free formula in which Qn = N is even, Qn = if n is odd (Nies, 2009, Def. 1.4.10). (We can also imagine a set A N as a calculable relationship.) It can be shown that we add any limited quantifiers and duplicated quantifiers of the same type without changing the classification of A. The formula to the right of (6,1) is a quantifiable formula and its negation is a quantifiable formula."}, {"heading": "6.1.2 Computability of Real-valued Functions", "text": "We fix some encodings of rational numbers into binary strings and an encoding of binary strings into natural numbers."}, {"heading": "6.2 The Complexity of Solomonoff Induction", "text": "In this section we derive the calculation results for Solomonoff as shown in Table 6.1.Since M (n) is lower semicomputable (n), Mnorm (n) is limited computable by Lemma 6.2 (c) and (d). If we find the conditional probability M (xy) / M (x) for finite strings x, y). Because M (x) > 0 for all finite strings x) > 0 for all finite strings x (c), this quotient x is, we must find the conditional probability M (xy) / M (x) for finite strings x, y (X). Because M (x) > 0 for all finite strings x)."}, {"heading": "6.3 The Complexity of AINU, AIMU, and AIXI", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6.3.1 Upper Bounds", "text": "In this section, we derive upper limits on the computability of AINU, AIMU, and AIXI. (With the exception of Corollary 6.14, all results in this section generally apply to any other formula defined in the arithmetic hierarchy of AINU, AIMU, and AIXI. (For the rest of this chapter, we assume that politics is deterministic, that is, can be expressed as relationships over (A \u00d7 E). (These relationships are easily identifiable with sets of natural numbers by encoding history in a natural number.) From now on, on this translation of politics into sets of natural numbers, we are done implicitly wherever necessary. (Lemma 6.7) These relationships are explicitly identified with sets of natural numbers."}, {"heading": "6.3.2 Lower Bounds", "text": "We continue to show that the boundaries of the previous section are the best we can hope for. In environmental classes in which bindings must be broken, AINU must solve the following problems: < # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #"}, {"heading": "6.4 Iterative Value Function", "text": "In this section we discuss the problems with this definition. In order to avoid confusion with the recursive value function, we rename the iterative value function with W. 1Definition 6.18 (Iterative value function).Hutter, 2005, Def. 5.30). The iterative value function 1In Leike and Hutter (2015a) is to undo the use of symbols V and W. Value of a policy in an environment given is < t isW lengthens < t isW; t < t < t < t < t < t < t < t < t < t < t < t < t < t < t < t < t < t < t &ltp; t &ltp; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p"}, {"heading": "6.5 The Complexity of Knowledge-Seeking", "text": "Consider the definition of the optimal entropy-seeking value V \u0445, mEnt and the optimal information-seeking value V \u0445, mIG from Section 4.3.2. Based on the results from Section 6.3, we can show that \u03b5-optimal knowledge-seeking agents reach their calculable limits, and optimal knowledge-seeking agents reach their calculable limits. This follows from Lemma 6.2 (c-e) from the time they act, and w are lower semicomputable values. Conclusion 6.25 (Computability of Knowledge-Seeking Policies): For entropy-seeking and information-seeking agents, there are limitless-computable-optimal policies, and vice versa. Conclusions from Theory 6.24, Theory 6.11.Note: For entropy-seeking agents and information-seeking agents, there are limitless-optimal policies, and vice versa."}, {"heading": "6.6 A Limit Computable Weakly Asymptotically Optimal Agent", "text": "According to Theorem 6.16, optimal reward policies are generally < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < &lt"}, {"heading": "6.7 Discussion", "text": "We have shown that conditional M and Mnorm can only be calculated to a limited extent (which we do not know). In some cases, standardized priors have advantages. As illustrated in Example 4.27, unstandardized priors can assign entropy-seeking agent errors from the probability that they are based on finite knowledge."}, {"heading": "7.1 Reflective Oracles", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "7.1.1 Definition", "text": "First, we combine semidimensions as defined in Definition 2.14 with Turing machines. In Chapter 2, we use monotonous Turing machines, which naturally correspond to lower semicomputable semidimensions (Li and Vit\u00e1nyi, 2008, Sec. 4.5.2), which describe the distribution that occurs when piping fair coins tips over into the monotonous machine. Here, we take a different type of Turing machine, which has an unlimited number of uniform random coin rotations. Let T denote the set of all probabilistic Turing machines that make some input into X and query an oracle (formally defined below). We take a Turing machine T, which can access a semi-uniform number of random coin rotations."}, {"heading": "7.1.2 A Limit Computable Reflective Oracle", "text": "The proof for Theorem 7.5 by Fallenstein et al. (2015c, App. B) is not constructive and uses the axiom of choice. In Section 7.1.3 we give new evidence for the existence of reflective oracles and provide a construction that there is a reflective oracle that is computable. 132 The Grain of Truth Problem Theorem 7.7 (A Limit Computable Reflective Oracle) There is a reflective oracle that is computable. This theorem has the immediate consequence that reflective oracles cannot be used as braking oracles. At first, this result may seem surprising: according to the definition of reflective oracles, they make concrete statements about the outcome of probabilistic turing machines. However, the fact that the oracle is actually so far away some of the time that a holding can no longer be decided by the oracle."}, {"heading": "7.1.3 Proof of Theorem 7.7", "text": "The idea for the proof of theorem 7.7 is to construct an algorithm that triggers an infinite series of sub-oracles that converge to form a reflective oracle in the boundary zone. (...) The definition of sub-oracles is estimable, so that we can assume that from the first sub-oracles to the multiplications of 2 \u2212 k in [0, 1]: O k in [0, 1]: O k in [0, 1]: O k in [0, 1]: O k in [0, 1]: O k in [0, 2]: O k in [0, 2]., qk in the definition of 2 \u2212 k]."}, {"heading": "7.2 A Grain of Truth", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "7.2.1 Reflective Bayesian Agents", "text": "From now on, we assume that the action space A: = \u00b2, \u03b2 \u00b2, is two-dimensional. < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < <"}, {"heading": "7.2.2 Reflective-Oracle-Computable Policies", "text": "This subsection is devoted to the following result, which has already been presented by Fallenstein et al. (2015a, Alg. 6) but has not been proven. It contrasts the results with arbitrary semicomputable environments where optimal strategies are unpredictable (see Section 6.3). Theorem 7.19 (Optimal strategies are oracle-computable). < MOrefl, there is an equally optimal (stochastic) answer for any commerce-computable policy. To prove theorem 7.19, we need the following lemma.Lemma 7.20 (Reflective-oracle-computable-computable-computable-value). Note that although deterministically optimal strategies always exist, these strategies are usually not reflective-oracle-computable.V To prove theorem 7.20 (Reflective-oracle-computable-computable-computable-value)."}, {"heading": "7.2.3 Solution to the Grain of Truth Problem", "text": "Proposition 7.18 and Theorem 7.19 together provide the necessary ingredients to solve the truth problem (Problem 7.1).Conclusion 7.21 (Solving the truth problem).For each lower semicomputable before w = MOrefl, the Bayes Oracle Oracle Reflectorate is calculable where the Bayes mixture corresponds to the w. Evidence defined in (7.2).From Proposition 7.18 and Theorem 7.19 it follows that the environmental MOrefl contains every reflective-oracle-computable modification of the Bayes Oracle with respect to optimal policies, especially including predictable multi-agent environments containing other Bayesian agents above the ClassMOrefl. Thus, each Bayesian agent has a grain of truth above the ClassMOrefl, although the environment may contain other Bayesian agents of equal power."}, {"heading": "7.3 Multi-Agent Environments", "text": "In a multi-agent environment, there are n agents who perform sequential actions from the finite space of action. In each time step t = 1, 2,.. the environment receives an action ait138 The grain of truth problem of agent i and outputs n perceptions e1t,..., ent. E, one for each agent. Each perceptual capacity eit = (o i t, r i t) contains an observation and a reward. [0, 1] It is important that agent i sees only his own action ait and his own perception (see Figure 7.2). We use the short-term note: (a1t,.,.,.) and a reward. & lt., and denote. < t = ai1ei1."}, {"heading": "7.4 Informed Reflective Agents", "text": "Let's be a multi-agent environment and a multi-agent environment. Let's be a multi-agent environment and a multi-agent environment. Let's leave the multi-agent environment and the policy. Let's leave the multi-agent environment. Let's leave the multi-agent environment. Let's leave the multi-agent environment. Let's leave the multi-agent environment. Let's leave the multi-agent environment. Let's leave the multi-agent environment. Let's leave the multi-agent environment. Let's leave the multi-agent environment. Let's leave the multi-agent environment and the policy. Let's leave. Let's leave the multi-agent environment. Let's leave the multi-agent environment. Let's leave the multi-agent environment. Let's leave the multi-agent environment. Let's leave the multi-agent environment."}, {"heading": "7.5 Learning Reflective Agents", "text": "Since our class MOrefl solves the grain-of-truth problem, the result of Kalai and Lehrer (1993) is immediately implicit that for all Bayesian agents \u03c01,.., \u03c0n interacting agents in an infinitely repetitive game, and for all those who must know the game, and also all other agents, almost certainly a t0-Nash policy is that for those who cannot understand politics, politics is an \"expeditive\" best answer. However, this depends on the important fact that every agent must know the game, and also that all other agents are Bayesian agents. Otherwise, convergence to a \"Far Nash\" equilibrium policy may fail, as exemplified by the following example.At the base of the construction is a dogmatic anticipation (Section 5.2.2).A dogmatic anticipation sign is very likely to go to hell (reward 0 eternal) if the agent deviates from a given compatible Bayesian policy, therefore it is only an agent's policy."}, {"heading": "7.6 Impossibility Results", "text": "Suppose we play an infinitely repetitive game in which no agent has a weakly dominant plot and the pure action maxmin reward is strictly lower than the minimum reward. However, the impossibility result of Neighbor (1997, 2005) states that there is no class of strategy so that the following are satisfied at the same time. \u2022 Learning Ability: Each agent learns to predict the actions of the other agent. \u2022 Caution and symmetry: The set is closed under simple political modifications, such as renaming measures. \u2022 Purity: There is such a > 0 that for every stochastic political action there is a deterministic policy that provides for such an action when the other measures are taken. (\u00e6 < t) = a, then we will take these measures. \u2022 Consistency: Each agent always has the best answer available. <"}, {"heading": "7.7 Discussion", "text": "This chapter presents the class of all reflecting persons with a problem. This class solves the problem of truthfulness (problem 7.1), because it contains (any calculable modification of) what concerns people as a whole. (Example 7.29) But if every person pursues a policy that is asymptomatic, then it is also able to change the world. (Example 7.29) But if every person pursues a policy that is asymptomatic, then that is the policy of Section 4.3.4), then the agents are able to play suboptimally. (Example 7.30 and Corollary 7.31)"}], "references": [{"title": "Analysis of Thompson sampling for the multi-armed bandit problem", "author": ["Shipra Agrawal", "Navin Goyal"], "venue": "In Conference on Learning Theory,", "citeRegEx": "Agrawal and Goyal.,? \\Q2011\\E", "shortCiteRegEx": "Agrawal and Goyal.", "year": 2011}, {"title": "AI researchers on AI risk", "author": ["Scott Alexander"], "venue": "http://slatestarcodex.com/2015/05/ 22/ai-researchers-on-ai-risk/,", "citeRegEx": "Alexander.,? \\Q2015\\E", "shortCiteRegEx": "Alexander.", "year": 2015}, {"title": "Interruptibility and corrigibility for AIXI and Monte Carlo agents. 2016", "author": ["Stuart Armstrong", "Laurent Orseau"], "venue": null, "citeRegEx": "Armstrong and Orseau.,? \\Q2016\\E", "shortCiteRegEx": "Armstrong and Orseau.", "year": 2016}, {"title": "Bayesianism, infinite decisions, and binding", "author": ["Frank Arntzenius", "Adam Elga", "John Hawthorne"], "venue": "Mind, 113(450):251\u2013283,", "citeRegEx": "Arntzenius et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Arntzenius et al\\.", "year": 2004}, {"title": "Near-optimal regret bounds for reinforcement learning", "author": ["Peter Auer", "Thomas Jaksch", "Ronald Ortner"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Auer et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Auer et al\\.", "year": 2009}, {"title": "Near-optimal regret bounds for reinforcement learning", "author": ["Peter Auer", "Thomas Jaksch", "Ronald Ortner"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Auer et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Auer et al\\.", "year": 2010}, {"title": "Residual algorithms: Reinforcement learning with function approximation", "author": ["Leemon Baird"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Baird.,? \\Q1995\\E", "shortCiteRegEx": "Baird.", "year": 1995}, {"title": "Universal upper bound on the entropy-to-energy ratio for bounded systems", "author": ["Jacob D Bekenstein"], "venue": "Physical Review D,", "citeRegEx": "Bekenstein.,? \\Q1981\\E", "shortCiteRegEx": "Bekenstein.", "year": 1981}, {"title": "The arcade learning environment: An evaluation platform for general agents", "author": ["Marc G Bellemare", "Yavar Naddaf", "Joel Veness", "Michael Bowling"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Bellemare et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bellemare et al\\.", "year": 2013}, {"title": "Increasing the action gap: New operators for reinforcement learning", "author": ["Marc G Bellemare", "Georg Ostrovski", "Arthur Guez", "Philip S Thomas", "R\u00e9mi Munos"], "venue": "In AAAI,", "citeRegEx": "Bellemare et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Bellemare et al\\.", "year": 2016}, {"title": "Learning long-term dependencies with gradient descent is difficult", "author": ["Yoshua Bengio", "Patrice Simard", "Paolo Frasconi"], "venue": "IEEE Transactions on Neural Networks,", "citeRegEx": "Bengio et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 1994}, {"title": "Dynamic Programming and Optimal Control", "author": ["Dimitri P Bertsekas", "John Tsitsiklis"], "venue": "Athena Scientific,", "citeRegEx": "Bertsekas and Tsitsiklis.,? \\Q1995\\E", "shortCiteRegEx": "Bertsekas and Tsitsiklis.", "year": 1995}, {"title": "Pattern Recognition and Machine Learning", "author": ["Christopher M Bishop"], "venue": null, "citeRegEx": "Bishop.,? \\Q2006\\E", "shortCiteRegEx": "Bishop.", "year": 2006}, {"title": "Merging of opinions with increasing information", "author": ["David Blackwell", "Lester Dubins"], "venue": "The Annals of Mathematical Statistics,", "citeRegEx": "Blackwell and Dubins.,? \\Q1962\\E", "shortCiteRegEx": "Blackwell and Dubins.", "year": 1962}, {"title": "Existential risks", "author": ["Nick Bostrom"], "venue": "Journal of Evolution and Technology,", "citeRegEx": "Bostrom.,? \\Q2002\\E", "shortCiteRegEx": "Bostrom.", "year": 2002}, {"title": "Ethical issues in advanced artificial intelligence. Science Fiction and Philosophy: From Time Travel to Superintelligence", "author": ["Nick Bostrom"], "venue": null, "citeRegEx": "Bostrom.,? \\Q2003\\E", "shortCiteRegEx": "Bostrom.", "year": 2003}, {"title": "Superintelligence: Paths, Dangers, Strategies", "author": ["Nick Bostrom"], "venue": null, "citeRegEx": "Bostrom.,? \\Q2014\\E", "shortCiteRegEx": "Bostrom.", "year": 2014}, {"title": "Strategic implications of openness in AI development", "author": ["Nick Bostrom"], "venue": "Technical report, Future of Humanity Institute,", "citeRegEx": "Bostrom.,? \\Q2016\\E", "shortCiteRegEx": "Bostrom.", "year": 2016}, {"title": "Rational and convergent learning in stochastic games", "author": ["Michael Bowling", "Manuela Veloso"], "venue": "In International Joint Conference on Artificial Intelligence,", "citeRegEx": "Bowling and Veloso.,? \\Q2001\\E", "shortCiteRegEx": "Bowling and Veloso.", "year": 2001}, {"title": "Regret analysis of stochastic and nonstochastic multi-armed bandit problems", "author": ["S\u00e9bastien Bubeck", "Cesa-Nicol\u00f2 Bianchi"], "venue": "Foundations and Trends in Machine Learning,", "citeRegEx": "Bubeck and Bianchi.,? \\Q2012\\E", "shortCiteRegEx": "Bubeck and Bianchi.", "year": 2012}, {"title": "A comprehensive survey of multiagent reinforcement learning", "author": ["Lucian Busoniu", "Robert Babuska", "Bart De Schutter"], "venue": "IEEE Transactions on Systems, Man, and Cybernetics, Part C: Applications and Reviews,", "citeRegEx": "Busoniu et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Busoniu et al\\.", "year": 2008}, {"title": "Prediction, Learning, and Games", "author": ["Nicolo Cesa-Bianchi", "G\u00e1bor Lugosi"], "venue": null, "citeRegEx": "Cesa.Bianchi and Lugosi.,? \\Q2006\\E", "shortCiteRegEx": "Cesa.Bianchi and Lugosi.", "year": 2006}, {"title": "The singularity: A philosophical analysis", "author": ["David Chalmers"], "venue": "Journal of Consciousness Studies,", "citeRegEx": "Chalmers.,? \\Q2010\\E", "shortCiteRegEx": "Chalmers.", "year": 2010}, {"title": "An empirical evaluation of Thompson sampling", "author": ["Olivier Chapelle", "Lihong Li"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Chapelle and Li.,? \\Q2011\\E", "shortCiteRegEx": "Chapelle and Li.", "year": 2011}, {"title": "Definability of truth in probabilistic logic", "author": ["Paul Christiano", "Eliezer Yudkowsky", "Marcello Herreshoff", "Mihaly Barasz"], "venue": "Technical report, Machine Intelligence Research Institute,", "citeRegEx": "Christiano et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Christiano et al\\.", "year": 2013}, {"title": "Elements of Information Theory", "author": ["Thomas M Cover", "Joy A Thomas"], "venue": null, "citeRegEx": "Cover and Thomas.,? \\Q2006\\E", "shortCiteRegEx": "Cover and Thomas.", "year": 2006}, {"title": "Generic Reinforcement Learning Beyond Small MDPs", "author": ["Mayank Daswani"], "venue": "PhD thesis, Australian National University,", "citeRegEx": "Daswani.,? \\Q2015\\E", "shortCiteRegEx": "Daswani.", "year": 2015}, {"title": "A definition of happiness for reinforcement learning agents", "author": ["Mayank Daswani", "Jan Leike"], "venue": "In Artificial General Intelligence,", "citeRegEx": "Daswani and Leike.,? \\Q2015\\E", "shortCiteRegEx": "Daswani and Leike.", "year": 2015}, {"title": "Ethical guidelines for a superintelligence", "author": ["Ernest Davis"], "venue": "Technical report,", "citeRegEx": "Davis.,? \\Q2014\\E", "shortCiteRegEx": "Davis.", "year": 2014}, {"title": "Increasing the gap between descriptional complexity and algorithmic probability", "author": ["Adam Day"], "venue": "Transactions of the American Mathematical Society,", "citeRegEx": "Day.,? \\Q2011\\E", "shortCiteRegEx": "Day.", "year": 2011}, {"title": "Bayesian Q-learning", "author": ["Richard Dearden", "Nir Friedman", "Stuart Russell"], "venue": "In AAAI,", "citeRegEx": "Dearden et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Dearden et al\\.", "year": 1998}, {"title": "Learning what to value", "author": ["Daniel Dewey"], "venue": "In Artificial General Intelligence,", "citeRegEx": "Dewey.,? \\Q2011\\E", "shortCiteRegEx": "Dewey.", "year": 2011}, {"title": "Bayesian Nonparametric Approaches for Reinforcement Learning in Partially Observable Domains", "author": ["Finale Doshi-Velez"], "venue": "PhD thesis, Massachusetts Institute of Technology,", "citeRegEx": "Doshi.Velez.,? \\Q2012\\E", "shortCiteRegEx": "Doshi.Velez.", "year": 2012}, {"title": "Probability: Theory and Examples", "author": ["Rick Durrett"], "venue": null, "citeRegEx": "Durrett.,? \\Q2010\\E", "shortCiteRegEx": "Durrett.", "year": 2010}, {"title": "The singularity controversy", "author": ["Amnon H Eden"], "venue": "Technical report, Sapience Project,", "citeRegEx": "Eden.,? \\Q2016\\E", "shortCiteRegEx": "Eden.", "year": 2016}, {"title": "Singularity Hypotheses: A Scientific and Philosophical Assessment", "author": ["Amnon H Eden", "James H Moor", "Johnny H S\u00f8raker", "Eric Steinhart", "editors"], "venue": null, "citeRegEx": "Eden et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Eden et al\\.", "year": 2013}, {"title": "Avoiding wireheading with value reinforcement learning", "author": ["Tom Everitt", "Marcus Hutter"], "venue": "In Artificial General Intelligence,", "citeRegEx": "Everitt and Hutter.,? \\Q2016\\E", "shortCiteRegEx": "Everitt and Hutter.", "year": 2016}, {"title": "Sequential extensions of causal and evidential decision theory", "author": ["Tom Everitt", "Jan Leike", "Marcus Hutter"], "venue": "In Algorithmic Decision Theory,", "citeRegEx": "Everitt et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Everitt et al\\.", "year": 2015}, {"title": "Self-modification of policy and utility function in rational agents", "author": ["Tom Everitt", "Daniel Filan", "Mayank Daswani", "Marcus Hutter"], "venue": "In Artificial General Intelligence,", "citeRegEx": "Everitt et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Everitt et al\\.", "year": 2016}, {"title": "Reflective variants of Solomonoff induction and AIXI", "author": ["Benja Fallenstein", "Nate Soares", "Jessica Taylor"], "venue": "In Artificial General Intelligence. Springer,", "citeRegEx": "Fallenstein et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Fallenstein et al\\.", "year": 2015}, {"title": "Reflective oracles: A foundation for game theory in artificial intelligence", "author": ["Benja Fallenstein", "Jessica Taylor", "Paul F Christiano"], "venue": "In Logic, Rationality, and Interaction,", "citeRegEx": "Fallenstein et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Fallenstein et al\\.", "year": 2015}, {"title": "Reflective oracles: A foundation for classical game theory", "author": ["Benja Fallenstein", "Jessica Taylor", "Paul F Christiano"], "venue": "Technical report, Machine Intelligence Research Institute,", "citeRegEx": "Fallenstein et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Fallenstein et al\\.", "year": 2015}, {"title": "Agents using speed priors", "author": ["Daniel Filan"], "venue": "Honours thesis,", "citeRegEx": "Filan.,? \\Q2015\\E", "shortCiteRegEx": "Filan.", "year": 2015}, {"title": "Loss bounds and time complexity for speed priors", "author": ["Daniel Filan", "Jan Leike", "Marcus Hutter"], "venue": "In Artificial Intelligence and Statistics,", "citeRegEx": "Filan et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Filan et al\\.", "year": 2016}, {"title": "Learning to communicate to solve riddles with deep distributed recurrent Q-networks", "author": ["Jakob N Foerster", "Yannis M Assael", "Nando de Freitas", "Shimon Whiteson"], "venue": "Technical report, University of Oxford,", "citeRegEx": "Foerster et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Foerster et al\\.", "year": 2016}, {"title": "On the impossibility of predicting the behavior of rational agents", "author": ["Dean P Foster", "H Peyton Young"], "venue": "Proceedings of the National Academy of Sciences,", "citeRegEx": "Foster and Young.,? \\Q2001\\E", "shortCiteRegEx": "Foster and Young.", "year": 2001}, {"title": "Bandit processes and dynamic allocation indices", "author": ["John Gittins"], "venue": "Journal of the Royal Statistical Society. Series B (Methodological),", "citeRegEx": "Gittins.,? \\Q1979\\E", "shortCiteRegEx": "Gittins.", "year": 1979}, {"title": "The paradox of confirmation", "author": ["Irving John Good"], "venue": "British Journal for the Philosophy of Science,", "citeRegEx": "Good.,? \\Q1960\\E", "shortCiteRegEx": "Good.", "year": 1960}, {"title": "Speculations concerning the first ultraintelligent machine", "author": ["Irving John Good"], "venue": "Advances in Computers,", "citeRegEx": "Good.,? \\Q1965\\E", "shortCiteRegEx": "Good.", "year": 1965}, {"title": "The white shoe is a red herring", "author": ["Irving John Good"], "venue": "The British Journal for the Philosophy of Science,", "citeRegEx": "Good.,? \\Q1967\\E", "shortCiteRegEx": "Good.", "year": 1967}, {"title": "Deep learning. Book in preparation for MIT Press, http://www.deeplearningbook.org, 2016", "author": ["Ian Goodfellow", "Yoshua Bengio", "Aaron Courville"], "venue": null, "citeRegEx": "Goodfellow et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2016}, {"title": "Thompson sampling for learning parameterized Markov decision processes", "author": ["Aditya Gopalan", "Shie Mannor"], "venue": "In Conference on Learning Theory,", "citeRegEx": "Gopalan and Mannor.,? \\Q2015\\E", "shortCiteRegEx": "Gopalan and Mannor.", "year": 2015}, {"title": "Reinforcement learning with function approximation converges to a region", "author": ["Geoffrey J Gordon"], "venue": "In Advanced in Neural Information Processing Systems,", "citeRegEx": "Gordon.,? \\Q2001\\E", "shortCiteRegEx": "Gordon.", "year": 2001}, {"title": "The Minimum Description Length Principle", "author": ["Peter D. Gr\u00fcnwald"], "venue": null, "citeRegEx": "Gr\u00fcnwald.,? \\Q2007\\E", "shortCiteRegEx": "Gr\u00fcnwald.", "year": 2007}, {"title": "On the relation between descriptional complexity and algorithmic probability", "author": ["P\u00e9ter G\u00e1cs"], "venue": "Theoretical Computer Science,", "citeRegEx": "G\u00e1cs.,? \\Q1983\\E", "shortCiteRegEx": "G\u00e1cs.", "year": 1983}, {"title": "\u00dcber formal unentscheidbare S\u00e4tze der Principia Mathematica und verwandter Systeme I", "author": ["Kurt G\u00f6del"], "venue": "Monatshefte fu\u0308r Mathematik und Physik,", "citeRegEx": "G\u00f6del.,? \\Q1931\\E", "shortCiteRegEx": "G\u00f6del.", "year": 1931}, {"title": "Deep recurrent Q-learning for partially observable MDPs", "author": ["Matthew Hausknecht", "Peter Stone"], "venue": "In 2015 AAAI Fall Symposium Series,", "citeRegEx": "Hausknecht and Stone.,? \\Q2015\\E", "shortCiteRegEx": "Hausknecht and Stone.", "year": 2015}, {"title": "Transcending complacency on superintelligent machines. http://www.huffingtonpost.com/ stephen-hawking/artificial-intelligence_b_5174265.html", "author": ["Stephen Hawking", "Max Tegmark", "Stuart Russell", "Frank Wilczek"], "venue": null, "citeRegEx": "Hawking et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Hawking et al\\.", "year": 2014}, {"title": "Universal semimeasures: An introduction", "author": ["Nicholas J Hay"], "venue": "Master\u2019s thesis, University of Auckland,", "citeRegEx": "Hay.,? \\Q2007\\E", "shortCiteRegEx": "Hay.", "year": 2007}, {"title": "Memory-based control with recurrent neural networks", "author": ["Nicolas Heess", "Jonathan J Hunt", "Timothy P Lillicrap", "David Silver"], "venue": "Technical report, Google DeepMind,", "citeRegEx": "Heess et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Heess et al\\.", "year": 2015}, {"title": "Deep reinforcement learning from self-play in imperfect-information games", "author": ["Johannes Heinrich", "David Silver"], "venue": "Technical report, DeepMind,", "citeRegEx": "Heinrich and Silver.,? \\Q2016\\E", "shortCiteRegEx": "Heinrich and Silver.", "year": 2016}, {"title": "Ultimate Automizer with array interpolation (competition contribution)", "author": ["Matthias Heizmann", "Daniel Dietsch", "Jan Leike", "Betim Musa", "Andreas Podelski"], "venue": "In Tools and Algorithms for the Construction and Analysis of Systems,", "citeRegEx": "Heizmann et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Heizmann et al\\.", "year": 2015}, {"title": "Ultimate Automizer with two-track proofs (competition contribution)", "author": ["Matthias Heizmann", "Daniel Dietsch", "Marius Greitschus", "Jan Leike", "Betim Musa", "Claus Sch\u00e4tzle", "Andreas Podelski"], "venue": "In Tools and Algorithms for the Construction and Analysis of Systems,", "citeRegEx": "Heizmann et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Heizmann et al\\.", "year": 2016}, {"title": "Studies in the logic of confirmation (I.)", "author": ["Carl G Hempel"], "venue": "Mind, pages 1\u201326,", "citeRegEx": "Hempel.,? \\Q1945\\E", "shortCiteRegEx": "Hempel.", "year": 1945}, {"title": "The white shoe: No red herring", "author": ["Carl G Hempel"], "venue": "The British Journal for the Philosophy of Science,", "citeRegEx": "Hempel.,? \\Q1967\\E", "shortCiteRegEx": "Hempel.", "year": 1967}, {"title": "A theory of universal artificial intelligence based on algorithmic complexity", "author": ["Marcus Hutter"], "venue": "Technical report,", "citeRegEx": "Hutter.,? \\Q2000\\E", "shortCiteRegEx": "Hutter.", "year": 2000}, {"title": "Universal sequential decisions in unknown environments", "author": ["Marcus Hutter"], "venue": "In European Workshop on Reinforcement Learning,", "citeRegEx": "Hutter.,? \\Q2001\\E", "shortCiteRegEx": "Hutter.", "year": 2001}, {"title": "New error bounds for Solomonoff prediction", "author": ["Marcus Hutter"], "venue": "Journal of Computer and System Sciences,", "citeRegEx": "Hutter.,? \\Q2001\\E", "shortCiteRegEx": "Hutter.", "year": 2001}, {"title": "Self-optimizing and Pareto-optimal policies in general environments based on Bayes-mixtures", "author": ["Marcus Hutter"], "venue": "In Computational Learning Theory,", "citeRegEx": "Hutter.,? \\Q2002\\E", "shortCiteRegEx": "Hutter.", "year": 2002}, {"title": "The fastest and shortest algorithm for all well-defined problems", "author": ["Marcus Hutter"], "venue": "International Journal of Foundations of Computer Science,", "citeRegEx": "Hutter.,? \\Q2002\\E", "shortCiteRegEx": "Hutter.", "year": 2002}, {"title": "A gentle introduction to the universal algorithmic agent AIXI", "author": ["Marcus Hutter"], "venue": "Technical report, IDSIA,", "citeRegEx": "Hutter.,? \\Q2003\\E", "shortCiteRegEx": "Hutter.", "year": 2003}, {"title": "Sequential predictions based on algorithmic complexity", "author": ["Marcus Hutter"], "venue": "Journal of Computer and System Sciences,", "citeRegEx": "Hutter.,? \\Q2006\\E", "shortCiteRegEx": "Hutter.", "year": 2006}, {"title": "General discounting versus average reward. In Algorithmic Learning Theory, pages 244\u2013258", "author": ["Marcus Hutter"], "venue": null, "citeRegEx": "Hutter.,? \\Q2006\\E", "shortCiteRegEx": "Hutter.", "year": 2006}, {"title": "50\u2019000\u20ac prize for compressing human knowledge", "author": ["Marcus Hutter"], "venue": "http://prize. hutter1.net/,", "citeRegEx": "Hutter.,? \\Q2006\\E", "shortCiteRegEx": "Hutter.", "year": 2006}, {"title": "Universal algorithmic intelligence: A mathematical top\u2192down approach", "author": ["Marcus Hutter"], "venue": "In Artificial General Intelligence,", "citeRegEx": "Hutter.,? \\Q2007\\E", "shortCiteRegEx": "Hutter.", "year": 2007}, {"title": "On universal prediction and Bayesian confirmation", "author": ["Marcus Hutter"], "venue": "Theoretical Computer Science,", "citeRegEx": "Hutter.,? \\Q2007\\E", "shortCiteRegEx": "Hutter.", "year": 2007}, {"title": "Discrete MDL predicts in total variation", "author": ["Marcus Hutter"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Hutter.,? \\Q2009\\E", "shortCiteRegEx": "Hutter.", "year": 2009}, {"title": "Open problems in universal induction & intelligence", "author": ["Marcus Hutter"], "venue": null, "citeRegEx": "Hutter.,? \\Q2009\\E", "shortCiteRegEx": "Hutter.", "year": 2009}, {"title": "Feature dynamic Bayesian networks", "author": ["Marcus Hutter"], "venue": "In Artificial General Intelligence,", "citeRegEx": "Hutter.,? \\Q2009\\E", "shortCiteRegEx": "Hutter.", "year": 2009}, {"title": "Feature reinforcement learning: Part I: Unstructured MDPs", "author": ["Marcus Hutter"], "venue": "Journal of Artificial General Intelligence,", "citeRegEx": "Hutter.,? \\Q2009\\E", "shortCiteRegEx": "Hutter.", "year": 2009}, {"title": "Can intelligence explode", "author": ["Marcus Hutter"], "venue": "Journal of Consciousness Studies,", "citeRegEx": "Hutter.,? \\Q2012\\E", "shortCiteRegEx": "Hutter.", "year": 2012}, {"title": "One decade of universal artificial intelligence", "author": ["Marcus Hutter"], "venue": "In Theoretical Foundations of Artificial General Intelligence,", "citeRegEx": "Hutter.,? \\Q2012\\E", "shortCiteRegEx": "Hutter.", "year": 2012}, {"title": "Extreme state aggregation beyond MDPs", "author": ["Marcus Hutter"], "venue": "In Algorithmic Learning Theory. Springer,", "citeRegEx": "Hutter.,? \\Q2014\\E", "shortCiteRegEx": "Hutter.", "year": 2014}, {"title": "On semimeasures predicting Martin-L\u00f6f random sequences", "author": ["Marcus Hutter", "Andrej A. Muchnik"], "venue": "Theoretical Computer Science,", "citeRegEx": "Hutter and Muchnik.,? \\Q2007\\E", "shortCiteRegEx": "Hutter and Muchnik.", "year": 2007}, {"title": "Probability Theory: The Logic of Science", "author": ["Edwin T Jaynes"], "venue": null, "citeRegEx": "Jaynes.,? \\Q2003\\E", "shortCiteRegEx": "Jaynes.", "year": 2003}, {"title": "On the Sample Complexity of Reinforcement Learning", "author": ["Sham Machandranath Kakade"], "venue": "PhD thesis,", "citeRegEx": "Kakade.,? \\Q2003\\E", "shortCiteRegEx": "Kakade.", "year": 2003}, {"title": "Rational learning leads to Nash equilibrium", "author": ["Ehud Kalai", "Ehud Lehrer"], "venue": "Econometrica, pages 1019\u20131045,", "citeRegEx": "Kalai and Lehrer.,? \\Q1993\\E", "shortCiteRegEx": "Kalai and Lehrer.", "year": 1993}, {"title": "Weak and strong merging of opinions", "author": ["Ehud Kalai", "Ehud Lehrer"], "venue": "Journal of Mathematical Economics,", "citeRegEx": "Kalai and Lehrer.,? \\Q1994\\E", "shortCiteRegEx": "Kalai and Lehrer.", "year": 1994}, {"title": "Thompson sampling: An asymptotically optimal finite-time analysis", "author": ["Emilie Kaufmann", "Nathaniel Korda", "R\u00e9mi Munos"], "venue": "In Algorithmic Learning Theory,", "citeRegEx": "Kaufmann et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Kaufmann et al\\.", "year": 2012}, {"title": "Autonomous helicopter flight via reinforcement learning", "author": ["HJ Kim", "Michael I Jordan", "Shankar Sastry", "Andrew Y Ng"], "venue": "In Advances in Neural Information Processing Systems, page None,", "citeRegEx": "Kim et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Kim et al\\.", "year": 2003}, {"title": "Introduction to Metamathematics", "author": ["Stephen Cole Kleene"], "venue": "Wolters-Noordhoff Publishing,", "citeRegEx": "Kleene.,? \\Q1952\\E", "shortCiteRegEx": "Kleene.", "year": 1952}, {"title": "Hierarchical deep reinforcement learning: Integrating temporal abstraction and intrinsic motivation", "author": ["Tejas D Kulkarni", "Karthik R Narasimhan", "Ardavan Saeedi", "Joshua B Tenenbaum"], "venue": "Technical report, Massachusetts Institute of Technology,", "citeRegEx": "Kulkarni et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Kulkarni et al\\.", "year": 2016}, {"title": "The Singularity is Near: When Humans Transcend Biology", "author": ["Ray Kurzweil"], "venue": "Viking Books,", "citeRegEx": "Kurzweil.,? \\Q2005\\E", "shortCiteRegEx": "Kurzweil.", "year": 2005}, {"title": "Theory of General Reinforcement Learning", "author": ["Tor Lattimore"], "venue": "PhD thesis, Australian National University,", "citeRegEx": "Lattimore.,? \\Q2013\\E", "shortCiteRegEx": "Lattimore.", "year": 2013}, {"title": "Regret analysis of the finite-horizon Gittins index strategy for multiarmed bandits", "author": ["Tor Lattimore"], "venue": "In Conference on Learning Theory,", "citeRegEx": "Lattimore.,? \\Q2016\\E", "shortCiteRegEx": "Lattimore.", "year": 2016}, {"title": "Asymptotically optimal agents", "author": ["Tor Lattimore", "Marcus Hutter"], "venue": "In Algorithmic Learning Theory,", "citeRegEx": "Lattimore and Hutter.,? \\Q2011\\E", "shortCiteRegEx": "Lattimore and Hutter.", "year": 2011}, {"title": "PAC bounds for discounted MDPs", "author": ["Tor Lattimore", "Marcus Hutter"], "venue": "In Algorithmic Learning Theory,", "citeRegEx": "Lattimore and Hutter.,? \\Q2012\\E", "shortCiteRegEx": "Lattimore and Hutter.", "year": 2012}, {"title": "On Martin-L\u00f6f convergence of Solomonoff\u2019s mixture", "author": ["Tor Lattimore", "Marcus Hutter"], "venue": "In Theory and Applications of Models of Computation,", "citeRegEx": "Lattimore and Hutter.,? \\Q2013\\E", "shortCiteRegEx": "Lattimore and Hutter.", "year": 2013}, {"title": "General time consistent discounting", "author": ["Tor Lattimore", "Marcus Hutter"], "venue": "Theoretical Computer Science,", "citeRegEx": "Lattimore and Hutter.,? \\Q2014\\E", "shortCiteRegEx": "Lattimore and Hutter.", "year": 2014}, {"title": "On Martin-L\u00f6f (non-)convergence of Solomonoff\u2019s universal mixture", "author": ["Tor Lattimore", "Marcus Hutter"], "venue": "Theoretical Computer Science,", "citeRegEx": "Lattimore and Hutter.,? \\Q2015\\E", "shortCiteRegEx": "Lattimore and Hutter.", "year": 2015}, {"title": "Universal prediction of selected bits", "author": ["Tor Lattimore", "Marcus Hutter", "Vaibhav Gavane"], "venue": "In Algorithmic Learning Theory,", "citeRegEx": "Lattimore et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Lattimore et al\\.", "year": 2011}, {"title": "Is there an elegant universal theory of prediction", "author": ["Shane Legg"], "venue": "In Algorithmic Learning Theory,", "citeRegEx": "Legg.,? \\Q2006\\E", "shortCiteRegEx": "Legg.", "year": 2006}, {"title": "Machine Super Intelligence", "author": ["Shane Legg"], "venue": "PhD thesis, University of Lugano,", "citeRegEx": "Legg.,? \\Q2008\\E", "shortCiteRegEx": "Legg.", "year": 2008}, {"title": "A collection of definitions of intelligence", "author": ["Shane Legg", "Marcus Hutter"], "venue": "Frontiers in Artificial Intelligence and Applications,", "citeRegEx": "Legg and Hutter.,? \\Q2007\\E", "shortCiteRegEx": "Legg and Hutter.", "year": 2007}, {"title": "Universal intelligence: A definition of machine intelligence", "author": ["Shane Legg", "Marcus Hutter"], "venue": "Minds & Machines,", "citeRegEx": "Legg and Hutter.,? \\Q2007\\E", "shortCiteRegEx": "Legg and Hutter.", "year": 2007}, {"title": "An approximation of the universal intelligence measure. In Algorithmic Probability and Friends", "author": ["Shane Legg", "Joel Veness"], "venue": "Bayesian Prediction and Artificial Intelligence,", "citeRegEx": "Legg and Veness.,? \\Q2013\\E", "shortCiteRegEx": "Legg and Veness.", "year": 2013}, {"title": "Merging and learning. Statistics, Probability and Game Theory, pages", "author": ["Ehud Lehrer", "Rann Smorodinsky"], "venue": null, "citeRegEx": "Lehrer and Smorodinsky.,? \\Q1996\\E", "shortCiteRegEx": "Lehrer and Smorodinsky.", "year": 1996}, {"title": "Ranking templates for linear loops. In Tools and Algorithms for the Construction and Analysis of Systems, pages 172\u2013186", "author": ["Jan Leike", "Matthias Heizmann"], "venue": null, "citeRegEx": "Leike and Heizmann.,? \\Q2014\\E", "shortCiteRegEx": "Leike and Heizmann.", "year": 2014}, {"title": "Geometric series as nontermination arguments for linear lasso programs", "author": ["Jan Leike", "Matthias Heizmann"], "venue": "Technical report, University of Freiburg,", "citeRegEx": "Leike and Heizmann.,? \\Q2014\\E", "shortCiteRegEx": "Leike and Heizmann.", "year": 2014}, {"title": "Ranking templates for linear loops", "author": ["Jan Leike", "Matthias Heizmann"], "venue": "Logical Methods in Computer Science,", "citeRegEx": "Leike and Heizmann.,? \\Q2015\\E", "shortCiteRegEx": "Leike and Heizmann.", "year": 2015}, {"title": "Geometric nontermination arguments. 2016", "author": ["Jan Leike", "Matthias Heizmann"], "venue": "Under preparation", "citeRegEx": "Leike and Heizmann.,? \\Q2016\\E", "shortCiteRegEx": "Leike and Heizmann.", "year": 2016}, {"title": "Indefinitely oscillating martingales", "author": ["Jan Leike", "Marcus Hutter"], "venue": "In Algorithmic Learning Theory, pages 321\u2013335,", "citeRegEx": "Leike and Hutter.,? \\Q2014\\E", "shortCiteRegEx": "Leike and Hutter.", "year": 2014}, {"title": "Indefinitely oscillating martingales", "author": ["Jan Leike", "Marcus Hutter"], "venue": "Technical report, Australian National University,", "citeRegEx": "Leike and Hutter.,? \\Q2014\\E", "shortCiteRegEx": "Leike and Hutter.", "year": 2014}, {"title": "On the computability of AIXI", "author": ["Jan Leike", "Marcus Hutter"], "venue": "In Uncertainty in Artificial Intelligence,", "citeRegEx": "Leike and Hutter.,? \\Q2015\\E", "shortCiteRegEx": "Leike and Hutter.", "year": 2015}, {"title": "On the computability of Solomonoff induction and knowledge-seeking", "author": ["Jan Leike", "Marcus Hutter"], "venue": "In Algorithmic Learning Theory,", "citeRegEx": "Leike and Hutter.,? \\Q2015\\E", "shortCiteRegEx": "Leike and Hutter.", "year": 2015}, {"title": "Bad universal priors and notions of optimality", "author": ["Jan Leike", "Marcus Hutter"], "venue": "In Conference on Learning Theory, pages 1244\u20131259,", "citeRegEx": "Leike and Hutter.,? \\Q2015\\E", "shortCiteRegEx": "Leike and Hutter.", "year": 2015}, {"title": "Solomonoff induction violates Nicod\u2019s criterion", "author": ["Jan Leike", "Marcus Hutter"], "venue": "In Algorithmic Learning Theory,", "citeRegEx": "Leike and Hutter.,? \\Q2015\\E", "shortCiteRegEx": "Leike and Hutter.", "year": 2015}, {"title": "On the computability of Solomonoff induction and AIXI", "author": ["Jan Leike", "Marcus Hutter"], "venue": null, "citeRegEx": "Leike and Hutter.,? \\Q2016\\E", "shortCiteRegEx": "Leike and Hutter.", "year": 2016}, {"title": "Thompson sampling is asymptotically optimal in general environments", "author": ["Jan Leike", "Tor Lattimore", "Laurent Orseau", "Marcus Hutter"], "venue": "In Uncertainty in Artificial Intelligence,", "citeRegEx": "Leike et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Leike et al\\.", "year": 2016}, {"title": "A formal solution to the grain of truth problem", "author": ["Jan Leike", "Jessica Taylor", "Benya Fallenstein"], "venue": "In Uncertainty in Artificial Intelligence,", "citeRegEx": "Leike et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Leike et al\\.", "year": 2016}, {"title": "On the notion of a random sequence", "author": ["Leonid A Levin"], "venue": "Soviet Mathematics Doklady,", "citeRegEx": "Levin.,? \\Q1973\\E", "shortCiteRegEx": "Levin.", "year": 1973}, {"title": "An Introduction to Kolmogorov Complexity and Its Applications", "author": ["Ming Li", "Paul M.B. Vit\u00e1nyi"], "venue": "Texts in Computer Science. Springer,", "citeRegEx": "Li and Vit\u00e1nyi.,? \\Q2008\\E", "shortCiteRegEx": "Li and Vit\u00e1nyi.", "year": 2008}, {"title": "State of the art control of Atari games using shallow reinforcement learning", "author": ["Yitao Liang", "Marlos C Machado", "Erik Talvitie", "Michael Bowling"], "venue": "In Autonomous Agents and Multiagent Systems,", "citeRegEx": "Liang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Liang et al\\.", "year": 2016}, {"title": "Continuous control with deep reinforcement learning", "author": ["Timothy P Lillicrap", "Jonathan J Hunt", "Alexander Pritzel", "Nicolas Heess", "Tom Erez", "Yuval Tassa", "David Silver", "Daan Wierstra"], "venue": "In International Conference on Learning Representations,", "citeRegEx": "Lillicrap et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Lillicrap et al\\.", "year": 2016}, {"title": "The paradox of confirmation", "author": ["John L Mackie"], "venue": "British Journal for the Philosophy of Science,", "citeRegEx": "Mackie.,? \\Q1963\\E", "shortCiteRegEx": "Mackie.", "year": 1963}, {"title": "On the undecidability of probabilistic planning and infinite-horizon partially observable Markov decision problems", "author": ["Omid Madani", "Steve Hanks", "Anne Condon"], "venue": "In AAAI,", "citeRegEx": "Madani et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Madani et al\\.", "year": 1999}, {"title": "On the undecidability of probabilistic planning and related stochastic optimization problems", "author": ["Omid Madani", "Steve Hanks", "Anne Condon"], "venue": "Artificial Intelligence,", "citeRegEx": "Madani et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Madani et al\\.", "year": 2003}, {"title": "Optimality criteria in reinforcement learning", "author": ["Sridhar Mahadevan"], "venue": "In AAAI Fall Symposium on Learning Complex Behaviors in Adaptive Intelligent Systems,", "citeRegEx": "Mahadevan.,? \\Q1996\\E", "shortCiteRegEx": "Mahadevan.", "year": 1996}, {"title": "Inductive logic and the ravens paradox", "author": ["Patrick Maher"], "venue": "Philosophy of Science,", "citeRegEx": "Maher.,? \\Q1999\\E", "shortCiteRegEx": "Maher.", "year": 1999}, {"title": "Emphatic temporal-difference learning", "author": ["A Rupam Mahmood", "Huizhen Yu", "Martha White", "Richard Sutton"], "venue": "Technical report, University of Alberta,", "citeRegEx": "Mahmood et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mahmood et al\\.", "year": 2015}, {"title": "The maximum speed of dynamical evolution", "author": ["Norman Margolus", "Lev B Levitin"], "venue": "Physica D: Nonlinear Phenomena,", "citeRegEx": "Margolus and Levitin.,? \\Q1998\\E", "shortCiteRegEx": "Margolus and Levitin.", "year": 1998}, {"title": "Death and suicide in universal artificial intelligence", "author": ["Jarryd Martin", "Tom Everitt", "Marcus Hutter"], "venue": "In Artificial General Intelligence,", "citeRegEx": "Martin et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Martin et al\\.", "year": 2016}, {"title": "A proposal for the Dartmouth summer research project on artificial intelligence", "author": ["John McCarthy", "Marvin Minsky", "Nathaniel Rochester", "Claude Shannon"], "venue": null, "citeRegEx": "McCarthy et al\\.,? \\Q1955\\E", "shortCiteRegEx": "McCarthy et al\\.", "year": 1955}, {"title": "The role of absolute continuity in \u201cmerging of opinions\u201d and \u201crational learning", "author": ["Ronald I Miller", "Chris William Sanchirico"], "venue": "Games and Economic Behavior,", "citeRegEx": "Miller and Sanchirico.,? \\Q1999\\E", "shortCiteRegEx": "Miller and Sanchirico.", "year": 1999}, {"title": "Playing Atari with deep reinforcement learning", "author": ["Volodymyr Mnih", "Koray Kavukcuoglu", "David Silver", "Alex Graves", "Ioannis Antonoglou", "Daan Wierstra", "Martin Riedmiller"], "venue": "Technical report, Google DeepMind,", "citeRegEx": "Mnih et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2013}, {"title": "Human-level control through deep reinforcement learning", "author": ["Volodymyr Mnih", "Koray Kavukcuoglu", "David Silver", "Andrei A Rusu", "Joel Veness", "Marc G Bellemare", "Alex Graves", "Martin Riedmiller", "Andreas K Fidjeland", "Georg Ostrovski", "Stig Petersen", "Charles Beattie", "Amir Sadik", "Ioannis Antonoglou", "Helen King", "Dharshan Kumaran", "Daan Wierstra", "Shane Legg", "Demis Hassabis"], "venue": "Nature, 518(7540):529\u2013533,", "citeRegEx": "Mnih et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2015}, {"title": "Asynchronous methods for deep reinforcement learning", "author": ["Volodymyr Mnih", "Adri\u00e0 Puigdom\u00e8nech Badia", "Mehdi Mirza", "Alex Graves", "Timothy P Lillicrap", "Tim Harley", "David Silver", "Koray Kavukcuoglu"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Mnih et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2016}, {"title": "Intelligence explosion: Evidence and import", "author": ["Luke Muehlhauser", "Anna Salamon"], "venue": "In Singularity Hypotheses,", "citeRegEx": "Muehlhauser and Salamon.,? \\Q2012\\E", "shortCiteRegEx": "Muehlhauser and Salamon.", "year": 2012}, {"title": "Complexity of finite-horizon Markov decision process problems", "author": ["Martin Mundhenk", "Judy Goldsmith", "Christopher Lusena", "Eric Allender"], "venue": "Journal of the ACM,", "citeRegEx": "Mundhenk et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Mundhenk et al\\.", "year": 2000}, {"title": "Stationary algorithmic probability", "author": ["Markus M\u00fcller"], "venue": "Theoretical Computer Science,", "citeRegEx": "M\u00fcller.,? \\Q2010\\E", "shortCiteRegEx": "M\u00fcller.", "year": 2010}, {"title": "Future progress in artificial intelligence: A survey of expert opinion", "author": ["Vincent C M\u00fcller", "Nick Bostrom"], "venue": "Fundamental Issues of Artificial Intelligence,", "citeRegEx": "M\u00fcller and Bostrom.,? \\Q2016\\E", "shortCiteRegEx": "M\u00fcller and Bostrom.", "year": 2016}, {"title": "Prediction, optimization, and learning", "author": ["John H Nachbar"], "venue": "in repeated games. Econometrica,", "citeRegEx": "Nachbar.,? \\Q1997\\E", "shortCiteRegEx": "Nachbar.", "year": 1997}, {"title": "Massively parallel methods for deep reinforcement learning", "author": ["Arun Nair", "Praveen Srinivasan", "Sam Blackwell", "Cagdas Alcicek", "Rory Fearon", "Alessandro De Maria", "Vedavyas Panneershelvam", "Mustafa Suleyman", "Charles Beattie", "Stig Petersen", "Shane Legg", "Volodymyr Mnih", "Koray Kavukcuoglu", "David Silver"], "venue": "Technical report, Google DeepMind,", "citeRegEx": "Nair et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Nair et al\\.", "year": 2015}, {"title": "Is A.I. an existential threat to humanity? https://www.quora.com/ Is-A-I-an-existential-threat-to-humanity/answer/Andrew-Ng, January 2016", "author": ["Andrew Ng"], "venue": null, "citeRegEx": "Ng.,? \\Q2016\\E", "shortCiteRegEx": "Ng.", "year": 2016}, {"title": "Competing with an infinite set of models in reinforcement learning", "author": ["Phuong Nguyen", "Odalric-Ambrym Maillard", "Daniil Ryabko", "Ronald Ortner"], "venue": "In Artificial Intelligence and Statistics,", "citeRegEx": "Nguyen et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Nguyen et al\\.", "year": 2013}, {"title": "Le Probl\u00e8me Logique de L\u2019Induction", "author": ["Jean Nicod"], "venue": "Presses Universitaires de France,", "citeRegEx": "Nicod.,? \\Q1961\\E", "shortCiteRegEx": "Nicod.", "year": 1961}, {"title": "Computability and Randomness", "author": ["Andr\u00e9 Nies"], "venue": null, "citeRegEx": "Nies.,? \\Q2009\\E", "shortCiteRegEx": "Nies.", "year": 2009}, {"title": "Positive reinforcement produced by electrical stimulation of septal area and other regions of rat brain", "author": ["James Olds", "Peter Milner"], "venue": "Journal of Comparative and Physiological Psychology,", "citeRegEx": "Olds and Milner.,? \\Q1954\\E", "shortCiteRegEx": "Olds and Milner.", "year": 1954}, {"title": "The basic AI drives", "author": ["Stephen M Omohundro"], "venue": "In Artificial General Intelligence,", "citeRegEx": "Omohundro.,? \\Q2008\\E", "shortCiteRegEx": "Omohundro.", "year": 2008}, {"title": "Optimality issues of universal greedy agents with static priors", "author": ["Laurent Orseau"], "venue": "In Algorithmic Learning Theory,", "citeRegEx": "Orseau.,? \\Q2010\\E", "shortCiteRegEx": "Orseau.", "year": 2010}, {"title": "Universal knowledge-seeking agents", "author": ["Laurent Orseau"], "venue": null, "citeRegEx": "Orseau.,? \\Q2011\\E", "shortCiteRegEx": "Orseau.", "year": 2011}, {"title": "Asymptotic non-learnability of universal agents with computable horizon functions", "author": ["Laurent Orseau"], "venue": "Theoretical Computer Science,", "citeRegEx": "Orseau.,? \\Q2013\\E", "shortCiteRegEx": "Orseau.", "year": 2013}, {"title": "Universal knowledge-seeking agents", "author": ["Laurent Orseau"], "venue": "Theoretical Computer Science,", "citeRegEx": "Orseau.,? \\Q2014\\E", "shortCiteRegEx": "Orseau.", "year": 2014}, {"title": "The multi-slot framework: A formal model for multiple, copiable AIs", "author": ["Laurent Orseau"], "venue": "In Artificial General Intelligence,", "citeRegEx": "Orseau.,? \\Q2014\\E", "shortCiteRegEx": "Orseau.", "year": 2014}, {"title": "Teleporting universal intelligent agents", "author": ["Laurent Orseau"], "venue": "In Artificial General Intelligence,", "citeRegEx": "Orseau.,? \\Q2014\\E", "shortCiteRegEx": "Orseau.", "year": 2014}, {"title": "Safely interruptible agents", "author": ["Laurent Orseau", "Stuart Armstrong"], "venue": "In Uncertainty in Artificial Intelligence,", "citeRegEx": "Orseau and Armstrong.,? \\Q2016\\E", "shortCiteRegEx": "Orseau and Armstrong.", "year": 2016}, {"title": "Self-modification and mortality in artificial agents", "author": ["Laurent Orseau", "Mark Ring"], "venue": "In Artificial General Intelligence,", "citeRegEx": "Orseau and Ring.,? \\Q2011\\E", "shortCiteRegEx": "Orseau and Ring.", "year": 2011}, {"title": "Space-time embedded intelligence", "author": ["Laurent Orseau", "Mark Ring"], "venue": "In Artificial General Intelligence,", "citeRegEx": "Orseau and Ring.,? \\Q2012\\E", "shortCiteRegEx": "Orseau and Ring.", "year": 2012}, {"title": "Memory issues of intelligent agents", "author": ["Laurent Orseau", "Mark Ring"], "venue": "In Artificial General Intelligence,", "citeRegEx": "Orseau and Ring.,? \\Q2012\\E", "shortCiteRegEx": "Orseau and Ring.", "year": 2012}, {"title": "Universal knowledge-seeking agents for stochastic environments", "author": ["Laurent Orseau", "Tor Lattimore", "Marcus Hutter"], "venue": "In Algorithmic Learning Theory,", "citeRegEx": "Orseau et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Orseau et al\\.", "year": 2013}, {"title": "A minimum relative entropy principle for learning and acting", "author": ["Pedro A Ortega", "Daniel A Braun"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Ortega and Braun.,? \\Q2010\\E", "shortCiteRegEx": "Ortega and Braun.", "year": 2010}, {"title": "Generalized Thompson sampling for sequential decision-making and causal inference", "author": ["Pedro A Ortega", "Daniel A Braun"], "venue": "Complex Adaptive Systems Modeling,", "citeRegEx": "Ortega and Braun.,? \\Q2014\\E", "shortCiteRegEx": "Ortega and Braun.", "year": 2014}, {"title": "More) efficient reinforcement learning via posterior sampling", "author": ["Ian Osband", "Dan Russo", "Benjamin van Roy"], "venue": "In Neural Information Processing Systems,", "citeRegEx": "Osband et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Osband et al\\.", "year": 2013}, {"title": "The complexity of Markov decision processes", "author": ["Christos H Papadimitriou", "John N Tsitsiklis"], "venue": "Mathematics of Operations Research,", "citeRegEx": "Papadimitriou and Tsitsiklis.,? \\Q1987\\E", "shortCiteRegEx": "Papadimitriou and Tsitsiklis.", "year": 1987}, {"title": "Markov Decision Processes", "author": ["Martin L Puterman"], "venue": null, "citeRegEx": "Puterman.,? \\Q2014\\E", "shortCiteRegEx": "Puterman.", "year": 2014}, {"title": "A philosophical treatise of universal induction", "author": ["Samuel Rathmanner", "Marcus Hutter"], "venue": "Entropy, 13(6):1076\u20131136,", "citeRegEx": "Rathmanner and Hutter.,? \\Q2011\\E", "shortCiteRegEx": "Rathmanner and Hutter.", "year": 2011}, {"title": "Delusion, survival, and intelligent agents", "author": ["Mark Ring", "Laurent Orseau"], "venue": "In Artificial General Intelligence,", "citeRegEx": "Ring and Orseau.,? \\Q2011\\E", "shortCiteRegEx": "Ring and Orseau.", "year": 2011}, {"title": "Diffusions, Markov Processes, and Martingales: Volume 1, Foundations", "author": ["Chris Rogers", "David Williams"], "venue": null, "citeRegEx": "Rogers and Williams.,? \\Q1994\\E", "shortCiteRegEx": "Rogers and Williams.", "year": 1994}, {"title": "Research priorities for robust and beneficial artificial intelligence", "author": ["Stuart Russell", "Daniel Dewey", "Max Tegmark"], "venue": "Technical report, Future of Life Institute,", "citeRegEx": "Russell et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Russell et al\\.", "year": 2015}, {"title": "Artificial Intelligence. A Modern Approach", "author": ["Stuart J Russell", "Peter Norvig"], "venue": null, "citeRegEx": "Russell and Norvig.,? \\Q2010\\E", "shortCiteRegEx": "Russell and Norvig.", "year": 2010}, {"title": "Characterizing predictable classes of processes", "author": ["Daniil Ryabko"], "venue": "In Uncertainty in Artificial Intelligence,", "citeRegEx": "Ryabko.,? \\Q2009\\E", "shortCiteRegEx": "Ryabko.", "year": 2009}, {"title": "On finding predictors for arbitrary families of processes", "author": ["Daniil Ryabko"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Ryabko.,? \\Q2010\\E", "shortCiteRegEx": "Ryabko.", "year": 2010}, {"title": "On the relation between realizable and nonrealizable cases of the sequence prediction problem", "author": ["Daniil Ryabko"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Ryabko.,? \\Q2011\\E", "shortCiteRegEx": "Ryabko.", "year": 2011}, {"title": "On sequence prediction for arbitrary measures", "author": ["Daniil Ryabko", "Marcus Hutter"], "venue": "In IEEE International Symposium on Information Theory, pages 2346\u20132350,", "citeRegEx": "Ryabko and Hutter.,? \\Q2007\\E", "shortCiteRegEx": "Ryabko and Hutter.", "year": 2007}, {"title": "Predicting non-stationary processes", "author": ["Daniil Ryabko", "Marcus Hutter"], "venue": "Applied Mathematics Letters,", "citeRegEx": "Ryabko and Hutter.,? \\Q2008\\E", "shortCiteRegEx": "Ryabko and Hutter.", "year": 2008}, {"title": "Purely epistemic Markov decision processes", "author": ["R\u00e9gis Sabbadin", "J\u00e9r\u00f4me Lang", "Nasolo Ravoanjanahry"], "venue": "In AAAI,", "citeRegEx": "Sabbadin et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Sabbadin et al\\.", "year": 2007}, {"title": "Prioritized experience replay", "author": ["Tom Schaul", "John Quan", "Ioannis Antonoglou", "David Silver"], "venue": "In International Conference on Learning Representations,", "citeRegEx": "Schaul et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Schaul et al\\.", "year": 2016}, {"title": "The speed prior: A new simplicity measure yielding near-optimal computable predictions", "author": ["J\u00fcrgen Schmidhuber"], "venue": "In Computational Learning Theory,", "citeRegEx": "Schmidhuber.,? \\Q2002\\E", "shortCiteRegEx": "Schmidhuber.", "year": 2002}, {"title": "Philosophers & futurists, catch up", "author": ["J\u00fcrgen Schmidhuber"], "venue": "Journal of Consciousness Studies,", "citeRegEx": "Schmidhuber.,? \\Q2012\\E", "shortCiteRegEx": "Schmidhuber.", "year": 2012}, {"title": "Deep learning in neural networks: An overview", "author": ["J\u00fcrgen Schmidhuber"], "venue": "Neural Networks,", "citeRegEx": "Schmidhuber.,? \\Q2015\\E", "shortCiteRegEx": "Schmidhuber.", "year": 2015}, {"title": "The Technological Singularity", "author": ["Murray Shanahan"], "venue": null, "citeRegEx": "Shanahan.,? \\Q2015\\E", "shortCiteRegEx": "Shanahan.", "year": 2015}, {"title": "Programming a computer for playing chess", "author": ["Claude E Shannon"], "venue": "The London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science,", "citeRegEx": "Shannon.,? \\Q1950\\E", "shortCiteRegEx": "Shannon.", "year": 1950}, {"title": "Multiagent Systems: Algorithmic, GameTheoretic, and Logical Foundations", "author": ["Yoav Shoham", "Kevin Leyton-Brown"], "venue": null, "citeRegEx": "Shoham and Leyton.Brown.,? \\Q2009\\E", "shortCiteRegEx": "Shoham and Leyton.Brown.", "year": 2009}, {"title": "Learning predictive state representations", "author": ["Satinder Singh", "Michael L Littman", "Nicholas K Jong", "David Pardoe", "Peter Stone"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Singh et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Singh et al\\.", "year": 2003}, {"title": "Predictive state representations: A new theory for modeling dynamical systems", "author": ["Satinder Singh", "Michael R James", "Matthew R Rudary"], "venue": "In Uncertainty in Artificial Intelligence,", "citeRegEx": "Singh et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Singh et al\\.", "year": 2004}, {"title": "Formalizing two problems of realistic world-models", "author": ["Nate Soares"], "venue": "Technical report, Machine Intelligence Research Institute,", "citeRegEx": "Soares.,? \\Q2015\\E", "shortCiteRegEx": "Soares.", "year": 2015}, {"title": "Aligning superintelligence with human interests: A technical research agenda", "author": ["Nate Soares", "Benja Fallenstein"], "venue": "Technical report, Machine Intelligence Research Institute,", "citeRegEx": "Soares and Fallenstein.,? \\Q2014\\E", "shortCiteRegEx": "Soares and Fallenstein.", "year": 2014}, {"title": "A formal theory of inductive inference. Parts 1 and 2", "author": ["Ray Solomonoff"], "venue": "Information and Control,", "citeRegEx": "Solomonoff.,? \\Q1964\\E", "shortCiteRegEx": "Solomonoff.", "year": 1964}, {"title": "Complexity-based induction systems: Comparisons and convergence theorems", "author": ["Ray Solomonoff"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Solomonoff.,? \\Q1978\\E", "shortCiteRegEx": "Solomonoff.", "year": 1978}, {"title": "Responses to catastrophic AGI risk: A survey", "author": ["Kaj Sotala", "Roman V Yampolskiy"], "venue": "Physica Scripta,", "citeRegEx": "Sotala and Yampolskiy.,? \\Q2014\\E", "shortCiteRegEx": "Sotala and Yampolskiy.", "year": 2014}, {"title": "Putnam\u2019s diagonal argument and the impossibility of a universal learning machine", "author": ["Tom F Sterkenburg"], "venue": "Technical report, Centrum Wiskunde & Informatica,", "citeRegEx": "Sterkenburg.,? \\Q2016\\E", "shortCiteRegEx": "Sterkenburg.", "year": 2016}, {"title": "Counterexamples in Probability", "author": ["Jordan M Stoyanov"], "venue": "Courier Corporation,", "citeRegEx": "Stoyanov.,? \\Q2013\\E", "shortCiteRegEx": "Stoyanov.", "year": 2013}, {"title": "A Bayesian framework for reinforcement learning", "author": ["Malcolm Strens"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Strens.,? \\Q2000\\E", "shortCiteRegEx": "Strens.", "year": 2000}, {"title": "Consistency of feature Markov processes", "author": ["Peter Sunehag", "Marcus Hutter"], "venue": "In Algorithmic Learning Theory,", "citeRegEx": "Sunehag and Hutter.,? \\Q2010\\E", "shortCiteRegEx": "Sunehag and Hutter.", "year": 2010}, {"title": "Optimistic agents are asymptotically optimal", "author": ["Peter Sunehag", "Marcus Hutter"], "venue": "In Australasian Joint Conference on Artificial Intelligence,", "citeRegEx": "Sunehag and Hutter.,? \\Q2012\\E", "shortCiteRegEx": "Sunehag and Hutter.", "year": 2012}, {"title": "Optimistic AIXI", "author": ["Peter Sunehag", "Marcus Hutter"], "venue": "In Artificial General Intelligence,", "citeRegEx": "Sunehag and Hutter.,? \\Q2012\\E", "shortCiteRegEx": "Sunehag and Hutter.", "year": 2012}, {"title": "Rationality, optimism and guarantees in general reinforcement learning", "author": ["Peter Sunehag", "Marcus Hutter"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Sunehag and Hutter.,? \\Q2015\\E", "shortCiteRegEx": "Sunehag and Hutter.", "year": 2015}, {"title": "Learning to predict by the methods of temporal differences", "author": ["Richard Sutton"], "venue": "Machine Learning,", "citeRegEx": "Sutton.,? \\Q1988\\E", "shortCiteRegEx": "Sutton.", "year": 1988}, {"title": "Reinforcement Learning: An Introduction", "author": ["Richard S. Sutton", "Andrew G. Barto"], "venue": null, "citeRegEx": "Sutton and Barto.,? \\Q1998\\E", "shortCiteRegEx": "Sutton and Barto.", "year": 1998}, {"title": "The paradoxes of confirmation: A survey", "author": ["Richard G Swinburne"], "venue": "American Philosophical Quarterly,", "citeRegEx": "Swinburne.,? \\Q1971\\E", "shortCiteRegEx": "Swinburne.", "year": 1971}, {"title": "Algorithms for Reinforcement Learning", "author": ["Csaba Szepesv\u00e1ri"], "venue": null, "citeRegEx": "Szepesv\u00e1ri.,? \\Q2010\\E", "shortCiteRegEx": "Szepesv\u00e1ri.", "year": 2010}, {"title": "Temporal difference learning and TD-Gammon", "author": ["Gerald Tesauro"], "venue": "Communications of the ACM,", "citeRegEx": "Tesauro.,? \\Q1995\\E", "shortCiteRegEx": "Tesauro.", "year": 1995}, {"title": "On the likelihood that one unknown probability exceeds another in view of the evidence of two samples", "author": ["William R Thompson"], "venue": null, "citeRegEx": "Thompson.,? \\Q1933\\E", "shortCiteRegEx": "Thompson.", "year": 1933}, {"title": "An analysis of temporal-difference learning with function approximation", "author": ["John N Tsitsiklis", "Benjamin Van Roy"], "venue": "IEEE Transactions on Automatic Control,", "citeRegEx": "Tsitsiklis and Roy.,? \\Q1997\\E", "shortCiteRegEx": "Tsitsiklis and Roy.", "year": 1997}, {"title": "Introduction to Nonparametric Estimation", "author": ["Alexandre Tsybakov"], "venue": null, "citeRegEx": "Tsybakov.,? \\Q2008\\E", "shortCiteRegEx": "Tsybakov.", "year": 2008}, {"title": "Deep reinforcement learning with double Q-learning", "author": ["Hado van Hasselt", "Arthur Guez", "David Silver"], "venue": "In AAAI,", "citeRegEx": "Hasselt et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Hasselt et al\\.", "year": 2016}, {"title": "A MonteCarlo AIXI approximation", "author": ["Joel Veness", "Kee Siong Ng", "Marcus Hutter", "William Uther", "David Silver"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Veness et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Veness et al\\.", "year": 2011}, {"title": "Compress and control", "author": ["Joel Veness", "Marc G Bellemare", "Marcus Hutter", "Alvin Chua", "Guillaume Desjardins"], "venue": "In AAAI,", "citeRegEx": "Veness et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Veness et al\\.", "year": 2015}, {"title": "The coming technological singularity", "author": ["Vernor Vinge"], "venue": "Vision 21: Interdisciplinary Science and Engineering in the Era of Cyberspace,", "citeRegEx": "Vinge.,? \\Q1993\\E", "shortCiteRegEx": "Vinge.", "year": 1993}, {"title": "Normalized information distance", "author": ["Paul MB Vit\u00e1nyi", "Frank J Balbach", "Rudi L Cilibrasi", "Ming Li"], "venue": "In Information Theory and Statistical Learning,", "citeRegEx": "Vit\u00e1nyi et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Vit\u00e1nyi et al\\.", "year": 2009}, {"title": "Bayesian reinforcement learning", "author": ["Nikos Vlassis", "Mohammad Ghavamzadeh", "Shie Mannor", "Pascal Poupart"], "venue": "Reinforcement Learning,", "citeRegEx": "Vlassis et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Vlassis et al\\.", "year": 2012}, {"title": "Hempel\u2019s raven paradox: A lacuna in the standard Bayesian solution", "author": ["Peter BM Vranas"], "venue": "The British Journal for the Philosophy of Science,", "citeRegEx": "Vranas.,? \\Q2004\\E", "shortCiteRegEx": "Vranas.", "year": 2004}, {"title": "The singularity may never be near", "author": ["Toby Walsh"], "venue": "Technical report,", "citeRegEx": "Walsh.,? \\Q2016\\E", "shortCiteRegEx": "Walsh.", "year": 2016}, {"title": "Dueling network architectures for deep reinforcement learning", "author": ["Ziyu Wang", "Nando de Freitas", "Tom Schaul", "Matteo Hessel", "Hado van Hasselt", "Marc Lanctot"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Wang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2016}, {"title": "Random walk\u20141-dimensional", "author": ["Eric W Weisstein"], "venue": "In MathWorld\u2014A Wolfram Web Resource. Wolfram Research, Inc.,", "citeRegEx": "Weisstein.,? \\Q2002\\E", "shortCiteRegEx": "Weisstein.", "year": 2002}, {"title": "Non-)equivalence of universal priors", "author": ["Ian Wood", "Peter Sunehag", "Marcus Hutter"], "venue": "In Solomonoff 85th Memorial Conference,", "citeRegEx": "Wood et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Wood et al\\.", "year": 2011}, {"title": "On convergence of emphatic temporal-difference learning", "author": ["Huizhen Yu"], "venue": "In Conference on Learning Theory,", "citeRegEx": "Yu.,? \\Q2015\\E", "shortCiteRegEx": "Yu.", "year": 2015}, {"title": "Creating friendly AI 1.0: The analysis and design of benevolent goal architectures", "author": ["Eliezer Yudkowsky"], "venue": "Technical report, Singularity Institute for Artificial Intelligence,", "citeRegEx": "Yudkowsky.,? \\Q2001\\E", "shortCiteRegEx": "Yudkowsky.", "year": 2001}, {"title": "Artificial intelligence as a positive and negative factor in global risk. In Global Catastrophic Risks, pages 308\u2013345", "author": ["Eliezer Yudkowsky"], "venue": null, "citeRegEx": "Yudkowsky.,? \\Q2008\\E", "shortCiteRegEx": "Yudkowsky.", "year": 2008}, {"title": "Graying the black box: Understanding DQNs", "author": ["Tom Zahavy", "Nir Ben Zrihem", "Shie Mannor"], "venue": "Technical report, Israel Institute of Technology,", "citeRegEx": "Zahavy et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Zahavy et al\\.", "year": 2016}, {"title": "A universal algorithm for sequential data compression", "author": ["Jacob Ziv", "Abraham Lempel"], "venue": "IEEE Transactions on information theory,", "citeRegEx": "Ziv and Lempel.,? \\Q1977\\E", "shortCiteRegEx": "Ziv and Lempel.", "year": 1977}], "referenceMentions": [{"referenceID": 169, "context": "After the early enthusiastic decades, research in artificial intelligence (AI) now mainly aims at specific domains: playing games, mining data, processing natural language, recognizing objects in images, piloting robots, filtering email, and many others (Russell and Norvig, 2010).", "startOffset": 254, "endOffset": 280}, {"referenceID": 132, "context": "The goal of developing HLAI has a long tradition in AI research and was explicitly part of the 1956 Dartmouth conference that gave birth to the field of AI (McCarthy et al., 1955):", "startOffset": 156, "endOffset": 179}, {"referenceID": 18, "context": "However, this trend has recently been reverted: Chalmers (2010), Hutter (2012a), Schmidhuber (2012), Bostrom (2014), Hawking, Tegmark, Russell, and Wilczek (2014), Shanahan (2015), and Walsh (2016) are well-known scientists discussing the prospect of HLAI seriously.", "startOffset": 48, "endOffset": 64}, {"referenceID": 18, "context": "However, this trend has recently been reverted: Chalmers (2010), Hutter (2012a), Schmidhuber (2012), Bostrom (2014), Hawking, Tegmark, Russell, and Wilczek (2014), Shanahan (2015), and Walsh (2016) are well-known scientists discussing the prospect of HLAI seriously.", "startOffset": 48, "endOffset": 80}, {"referenceID": 18, "context": "However, this trend has recently been reverted: Chalmers (2010), Hutter (2012a), Schmidhuber (2012), Bostrom (2014), Hawking, Tegmark, Russell, and Wilczek (2014), Shanahan (2015), and Walsh (2016) are well-known scientists discussing the prospect of HLAI seriously.", "startOffset": 48, "endOffset": 100}, {"referenceID": 14, "context": "However, this trend has recently been reverted: Chalmers (2010), Hutter (2012a), Schmidhuber (2012), Bostrom (2014), Hawking, Tegmark, Russell, and Wilczek (2014), Shanahan (2015), and Walsh (2016) are well-known scientists discussing the prospect of HLAI seriously.", "startOffset": 101, "endOffset": 116}, {"referenceID": 14, "context": "However, this trend has recently been reverted: Chalmers (2010), Hutter (2012a), Schmidhuber (2012), Bostrom (2014), Hawking, Tegmark, Russell, and Wilczek (2014), Shanahan (2015), and Walsh (2016) are well-known scientists discussing the prospect of HLAI seriously.", "startOffset": 101, "endOffset": 163}, {"referenceID": 14, "context": "However, this trend has recently been reverted: Chalmers (2010), Hutter (2012a), Schmidhuber (2012), Bostrom (2014), Hawking, Tegmark, Russell, and Wilczek (2014), Shanahan (2015), and Walsh (2016) are well-known scientists discussing the prospect of HLAI seriously.", "startOffset": 101, "endOffset": 180}, {"referenceID": 14, "context": "However, this trend has recently been reverted: Chalmers (2010), Hutter (2012a), Schmidhuber (2012), Bostrom (2014), Hawking, Tegmark, Russell, and Wilczek (2014), Shanahan (2015), and Walsh (2016) are well-known scientists discussing the prospect of HLAI seriously.", "startOffset": 101, "endOffset": 198}, {"referenceID": 198, "context": "Reinforcement learning studies algorithms that learn to act in an unknown environment through trial and error (Sutton and Barto, 1998; Szepesv\u00e1ri, 2010; Wiering and van Otterlo, 2012).", "startOffset": 110, "endOffset": 183}, {"referenceID": 200, "context": "Reinforcement learning studies algorithms that learn to act in an unknown environment through trial and error (Sutton and Barto, 1998; Szepesv\u00e1ri, 2010; Wiering and van Otterlo, 2012).", "startOffset": 110, "endOffset": 183}, {"referenceID": 198, "context": "Typically, the policy is slowly improved while learning, like SARSA (Sutton and Barto, 1998).", "startOffset": 68, "endOffset": 92}, {"referenceID": 164, "context": "This setting is well-analyzed (Puterman, 2014; Bertsekas and Tsitsiklis, 1995; Sutton and Barto, 1998), and there is a variety of algorithms that are known to learn the MDP asymptotically, such as TD learning (Sutton, 1988) and Q-learning (Watkins and Dayan, 1992).", "startOffset": 30, "endOffset": 102}, {"referenceID": 11, "context": "This setting is well-analyzed (Puterman, 2014; Bertsekas and Tsitsiklis, 1995; Sutton and Barto, 1998), and there is a variety of algorithms that are known to learn the MDP asymptotically, such as TD learning (Sutton, 1988) and Q-learning (Watkins and Dayan, 1992).", "startOffset": 30, "endOffset": 102}, {"referenceID": 198, "context": "This setting is well-analyzed (Puterman, 2014; Bertsekas and Tsitsiklis, 1995; Sutton and Barto, 1998), and there is a variety of algorithms that are known to learn the MDP asymptotically, such as TD learning (Sutton, 1988) and Q-learning (Watkins and Dayan, 1992).", "startOffset": 30, "endOffset": 102}, {"referenceID": 197, "context": "This setting is well-analyzed (Puterman, 2014; Bertsekas and Tsitsiklis, 1995; Sutton and Barto, 1998), and there is a variety of algorithms that are known to learn the MDP asymptotically, such as TD learning (Sutton, 1988) and Q-learning (Watkins and Dayan, 1992).", "startOffset": 209, "endOffset": 223}, {"referenceID": 4, "context": "Lattimore and Hutter (2012) use the algorithm UCRL\u03b3 (Auer et al., 2009) with geometric discounting with discount rate \u03b3 and derive the currently best-known PAC bound of \u00d5(\u2212T/(\u03b52(1 \u2212 \u03b3)3) log \u03b4) where T is the number of non-zero transitions in the MDP.", "startOffset": 52, "endOffset": 71}, {"referenceID": 198, "context": "In these cases, function approximation can be used to learn an approximation to the value function (Sutton and Barto, 1998).", "startOffset": 99, "endOffset": 123}, {"referenceID": 203, "context": "Linear function approximation is known to converge for several on-policy algorithms (Tsitsiklis and Roy, 1997; Sutton, 1988; Gordon, 2001), but proved tricky for off-policy algorithms (Baird, 1995).", "startOffset": 84, "endOffset": 138}, {"referenceID": 197, "context": "Linear function approximation is known to converge for several on-policy algorithms (Tsitsiklis and Roy, 1997; Sutton, 1988; Gordon, 2001), but proved tricky for off-policy algorithms (Baird, 1995).", "startOffset": 84, "endOffset": 138}, {"referenceID": 52, "context": "Linear function approximation is known to converge for several on-policy algorithms (Tsitsiklis and Roy, 1997; Sutton, 1988; Gordon, 2001), but proved tricky for off-policy algorithms (Baird, 1995).", "startOffset": 84, "endOffset": 138}, {"referenceID": 6, "context": "Linear function approximation is known to converge for several on-policy algorithms (Tsitsiklis and Roy, 1997; Sutton, 1988; Gordon, 2001), but proved tricky for off-policy algorithms (Baird, 1995).", "startOffset": 184, "endOffset": 197}, {"referenceID": 4, "context": "Auer et al. (2009) derive the regret bound \u00d5(dS \u221a At) for ergodic MDPs where d is the diameter of the MDP (how many steps a policy needs on average to get from one state of the MDP to any other), S is the number of states, A is the number of actions, and t is the number of time steps the algorithm runs.", "startOffset": 0, "endOffset": 19}, {"referenceID": 4, "context": "Auer et al. (2009) derive the regret bound \u00d5(dS \u221a At) for ergodic MDPs where d is the diameter of the MDP (how many steps a policy needs on average to get from one state of the MDP to any other), S is the number of states, A is the number of actions, and t is the number of time steps the algorithm runs. Second, given \u03b5 and \u03b4, a reinforcement learning algorithm is said to have sample complexity C(\u03b5, \u03b4) iff it is \u03b5-suboptimal for at most C(\u03b5, \u03b4) time steps with probability at least 1 \u2212 \u03b4 (probably approximately correct, PAC). For MDPs the first sample complexity bounds were due to Kakade (2003). Lattimore and Hutter (2012) use the algorithm UCRL\u03b3 (Auer et al.", "startOffset": 0, "endOffset": 600}, {"referenceID": 4, "context": "Auer et al. (2009) derive the regret bound \u00d5(dS \u221a At) for ergodic MDPs where d is the diameter of the MDP (how many steps a policy needs on average to get from one state of the MDP to any other), S is the number of states, A is the number of actions, and t is the number of time steps the algorithm runs. Second, given \u03b5 and \u03b4, a reinforcement learning algorithm is said to have sample complexity C(\u03b5, \u03b4) iff it is \u03b5-suboptimal for at most C(\u03b5, \u03b4) time steps with probability at least 1 \u2212 \u03b4 (probably approximately correct, PAC). For MDPs the first sample complexity bounds were due to Kakade (2003). Lattimore and Hutter (2012) use the algorithm UCRL\u03b3 (Auer et al.", "startOffset": 0, "endOffset": 629}, {"referenceID": 4, "context": "Auer et al. (2009) derive the regret bound \u00d5(dS \u221a At) for ergodic MDPs where d is the diameter of the MDP (how many steps a policy needs on average to get from one state of the MDP to any other), S is the number of states, A is the number of actions, and t is the number of time steps the algorithm runs. Second, given \u03b5 and \u03b4, a reinforcement learning algorithm is said to have sample complexity C(\u03b5, \u03b4) iff it is \u03b5-suboptimal for at most C(\u03b5, \u03b4) time steps with probability at least 1 \u2212 \u03b4 (probably approximately correct, PAC). For MDPs the first sample complexity bounds were due to Kakade (2003). Lattimore and Hutter (2012) use the algorithm UCRL\u03b3 (Auer et al., 2009) with geometric discounting with discount rate \u03b3 and derive the currently best-known PAC bound of \u00d5(\u2212T/(\u03b52(1 \u2212 \u03b3)3) log \u03b4) where T is the number of non-zero transitions in the MDP. Typically, algorithms for MDPs rely on visiting every state multiple times (or even infinitely often), which becomes infeasible for large state spaces (e.g. a video game screen consisting of millions of pixels). In these cases, function approximation can be used to learn an approximation to the value function (Sutton and Barto, 1998). Linear function approximation is known to converge for several on-policy algorithms (Tsitsiklis and Roy, 1997; Sutton, 1988; Gordon, 2001), but proved tricky for off-policy algorithms (Baird, 1995). A recent breakthrough was made by Mahmood et al. (2015) and Yu (2015)", "startOffset": 0, "endOffset": 1445}, {"referenceID": 4, "context": "Auer et al. (2009) derive the regret bound \u00d5(dS \u221a At) for ergodic MDPs where d is the diameter of the MDP (how many steps a policy needs on average to get from one state of the MDP to any other), S is the number of states, A is the number of actions, and t is the number of time steps the algorithm runs. Second, given \u03b5 and \u03b4, a reinforcement learning algorithm is said to have sample complexity C(\u03b5, \u03b4) iff it is \u03b5-suboptimal for at most C(\u03b5, \u03b4) time steps with probability at least 1 \u2212 \u03b4 (probably approximately correct, PAC). For MDPs the first sample complexity bounds were due to Kakade (2003). Lattimore and Hutter (2012) use the algorithm UCRL\u03b3 (Auer et al., 2009) with geometric discounting with discount rate \u03b3 and derive the currently best-known PAC bound of \u00d5(\u2212T/(\u03b52(1 \u2212 \u03b3)3) log \u03b4) where T is the number of non-zero transitions in the MDP. Typically, algorithms for MDPs rely on visiting every state multiple times (or even infinitely often), which becomes infeasible for large state spaces (e.g. a video game screen consisting of millions of pixels). In these cases, function approximation can be used to learn an approximation to the value function (Sutton and Barto, 1998). Linear function approximation is known to converge for several on-policy algorithms (Tsitsiklis and Roy, 1997; Sutton, 1988; Gordon, 2001), but proved tricky for off-policy algorithms (Baird, 1995). A recent breakthrough was made by Mahmood et al. (2015) and Yu (2015)", "startOffset": 0, "endOffset": 1459}, {"referenceID": 89, "context": "Among the historical successes of reinforcement learning is autonomous helicopter piloting (Kim et al., 2003) and TD-Gammon, a backgammon algorithm that learned through self-play (Tesauro, 1995), similar to AlphaGo (Silver et al.", "startOffset": 91, "endOffset": 109}, {"referenceID": 201, "context": ", 2003) and TD-Gammon, a backgammon algorithm that learned through self-play (Tesauro, 1995), similar to AlphaGo (Silver et al.", "startOffset": 77, "endOffset": 92}, {"referenceID": 65, "context": "This approach to general AI is in accordance with the definition of intelligence given by Legg and Hutter (2007b):", "startOffset": 99, "endOffset": 114}, {"referenceID": 8, "context": "\u2019 A popular such selection is the Atari 2600 video game console (Bellemare et al., 2013).", "startOffset": 64, "endOffset": 88}, {"referenceID": 179, "context": "1 DQN rides the wave of success of deep learning (LeCun et al., 2015; Schmidhuber, 2015; Goodfellow et al., 2016).", "startOffset": 49, "endOffset": 113}, {"referenceID": 50, "context": "1 DQN rides the wave of success of deep learning (LeCun et al., 2015; Schmidhuber, 2015; Goodfellow et al., 2016).", "startOffset": 49, "endOffset": 113}, {"referenceID": 9, "context": "Since the introduction of DQN there have been numerous improvements on this algorithm: increasing the gap on the Q-values of different actions (Bellemare et al., 2016), training in parallel (Nair et al.", "startOffset": 143, "endOffset": 167}, {"referenceID": 142, "context": ", 2016), training in parallel (Nair et al., 2015; Mnih et al., 2016), improvements to the experience replay mechanism (Schaul et al.", "startOffset": 30, "endOffset": 68}, {"referenceID": 136, "context": ", 2016), training in parallel (Nair et al., 2015; Mnih et al., 2016), improvements to the experience replay mechanism (Schaul et al.", "startOffset": 30, "endOffset": 68}, {"referenceID": 176, "context": ", 2016), improvements to the experience replay mechanism (Schaul et al., 2016), generalization to continuous action spaces (Lillicrap et al.", "startOffset": 57, "endOffset": 78}, {"referenceID": 123, "context": ", 2016), generalization to continuous action spaces (Lillicrap et al., 2016), solve the overestimation problem (van Hasselt et al.", "startOffset": 52, "endOffset": 76}, {"referenceID": 213, "context": ", 2016), and improvements to the neural network architecture (Wang et al., 2016).", "startOffset": 61, "endOffset": 80}, {"referenceID": 8, "context": "Since the introduction of DQN there have been numerous improvements on this algorithm: increasing the gap on the Q-values of different actions (Bellemare et al., 2016), training in parallel (Nair et al., 2015; Mnih et al., 2016), improvements to the experience replay mechanism (Schaul et al., 2016), generalization to continuous action spaces (Lillicrap et al., 2016), solve the overestimation problem (van Hasselt et al., 2016), and improvements to the neural network architecture (Wang et al., 2016). TheQvalues learned by DQN\u2019s neural networks are intransparent to inspection; Zahavy et al. (2016) use visualization techniques on the Q-value networks.", "startOffset": 144, "endOffset": 602}, {"referenceID": 8, "context": "Since the introduction of DQN there have been numerous improvements on this algorithm: increasing the gap on the Q-values of different actions (Bellemare et al., 2016), training in parallel (Nair et al., 2015; Mnih et al., 2016), improvements to the experience replay mechanism (Schaul et al., 2016), generalization to continuous action spaces (Lillicrap et al., 2016), solve the overestimation problem (van Hasselt et al., 2016), and improvements to the neural network architecture (Wang et al., 2016). TheQvalues learned by DQN\u2019s neural networks are intransparent to inspection; Zahavy et al. (2016) use visualization techniques on the Q-value networks. Finally, Liang et al. (2016) managed to reproduce DQN\u2019s success using only linear function approximation (no neural networks).", "startOffset": 144, "endOffset": 685}, {"referenceID": 59, "context": "An obvious approach to equip DQN with memory is to use recurrent neural networks instead of simple feedforward neural networks (Heess et al., 2015).", "startOffset": 127, "endOffset": 147}, {"referenceID": 10, "context": "However, it is currently unclear whether recurrent neural networks are powerful enough to learn long-term dependencies in the data (Bengio et al., 1994).", "startOffset": 131, "endOffset": 152}, {"referenceID": 55, "context": "Hausknecht and Stone (2015) show that this enables the agent to play the games when using only a single frame as input.", "startOffset": 0, "endOffset": 28}, {"referenceID": 159, "context": "by knowledge-seeking agents (Orseau, 2011, 2014a; Orseau et al., 2013).", "startOffset": 28, "endOffset": 70}, {"referenceID": 148, "context": "In this setting, the most efficient way to get rewards is to modify the reward mechanism to always provide the maximal reward (Omohundro, 2008; Ring and Orseau, 2011; Bostrom, 2014).", "startOffset": 126, "endOffset": 181}, {"referenceID": 166, "context": "In this setting, the most efficient way to get rewards is to modify the reward mechanism to always provide the maximal reward (Omohundro, 2008; Ring and Orseau, 2011; Bostrom, 2014).", "startOffset": 126, "endOffset": 181}, {"referenceID": 16, "context": "In this setting, the most efficient way to get rewards is to modify the reward mechanism to always provide the maximal reward (Omohundro, 2008; Ring and Orseau, 2011; Bostrom, 2014).", "startOffset": 126, "endOffset": 181}, {"referenceID": 87, "context": "Kulkarni et al. (2016) introduce a hierarchical approach based on intrinsic motivation to improve DQN\u2019s exploration and manage to score points in Montezuma\u2019s Revenge.", "startOffset": 0, "endOffset": 23}, {"referenceID": 14, "context": "In this setting, the most efficient way to get rewards is to modify the reward mechanism to always provide the maximal reward (Omohundro, 2008; Ring and Orseau, 2011; Bostrom, 2014). Consequently the agent no longer pursues the designers\u2019 originally intended goals and instead only attempts to protect its own existence. The name wireheading was established by analogy to a biology experiment by Olds and Milner (1954) in which rats had a wire embedded into the reward center of their brain that they could then stimulate by the push of a button.", "startOffset": 167, "endOffset": 419}, {"referenceID": 32, "context": ", 2003, 2004), and Bayesian methods (Doshi-Velez, 2012).", "startOffset": 36, "endOffset": 55}, {"referenceID": 193, "context": "The quest for a good cost function for feature maps remains unsuccessful thus far (Sunehag and Hutter, 2010; Daswani, 2015).", "startOffset": 82, "endOffset": 123}, {"referenceID": 26, "context": "The quest for a good cost function for feature maps remains unsuccessful thus far (Sunehag and Hutter, 2010; Daswani, 2015).", "startOffset": 82, "endOffset": 123}, {"referenceID": 165, "context": "Based in algorithmic information theory, Solomonoff\u2019s prior draws from famous insights by William of Ockham, Sextus Epicurus, Alan Turing, and Andrey Kolmogorov (Rathmanner and Hutter, 2011).", "startOffset": 161, "endOffset": 190}, {"referenceID": 95, "context": "A typical optimality property in general reinforcement learning is asymptotic optimality (Lattimore and Hutter, 2011): as time progresses the agent converges to achieve the same rewards as the optimal policy.", "startOffset": 89, "endOffset": 117}, {"referenceID": 197, "context": "Asymptotic optimality is usually what is meant by \u201cQ-learning converges\u201d (Watkins and Dayan, 1992) or \u201cTD learning converges\u201d (Sutton, 1988).", "startOffset": 126, "endOffset": 140}, {"referenceID": 26, "context": "The quest for a good cost function for feature maps remains unsuccessful thus far (Sunehag and Hutter, 2010; Daswani, 2015). However, Hutter (2014) managed to derive strong bounds relating the optimal value function of the aggregated MDP to the value function of the original process even if the latter violates the Markov condition.", "startOffset": 109, "endOffset": 148}, {"referenceID": 26, "context": "The quest for a good cost function for feature maps remains unsuccessful thus far (Sunehag and Hutter, 2010; Daswani, 2015). However, Hutter (2014) managed to derive strong bounds relating the optimal value function of the aggregated MDP to the value function of the original process even if the latter violates the Markov condition. A full theoretical approach to the general reinforcement learning problem is given by Hutter (2000, 2001a, 2002a, 2003, 2005, 2007a, 2012b). He introduces the Bayesian RL agent AIXI building on the theory of sequence prediction by Solomonoff (1964, 1978). Based in algorithmic information theory, Solomonoff\u2019s prior draws from famous insights by William of Ockham, Sextus Epicurus, Alan Turing, and Andrey Kolmogorov (Rathmanner and Hutter, 2011). AIXI uses Solomonoff\u2019s prior over the class of all computable environments and acts to maximize Bayes-expected rewards. We formally introduce Solomonoff\u2019s theory of induction in Chapter 3 and AIXI in Section 4.3.1. See also Legg (2008) for an accessible introduction to AIXI.", "startOffset": 109, "endOffset": 1018}, {"referenceID": 65, "context": "1) Here, intelligence refers to an agent that optimizes towards some goal in accordance with the definition by Legg and Hutter (2007b). For learning we distinguish two (very related) aspects: (1) arriving at accurate beliefs about the future and (2) making accurate predictions about the future.", "startOffset": 120, "endOffset": 135}, {"referenceID": 135, "context": ", 2011, 2015) are easily outperformed by neural-network-based approaches (Mnih et al., 2015).", "startOffset": 73, "endOffset": 92}, {"referenceID": 65, "context": "Hutter (2002a) showed that AIXI is Pareto optimal, balanced Pareto optimal, and self-optimizing.", "startOffset": 0, "endOffset": 15}, {"referenceID": 65, "context": "Hutter (2002a) showed that AIXI is Pareto optimal, balanced Pareto optimal, and self-optimizing. Orseau (2013) established that AIXI does not achieve asymptotic optimality in all computable environments (making the self-optimizing result inapplicable to this general environment class).", "startOffset": 0, "endOffset": 111}, {"referenceID": 42, "context": "Chapter Publication(s) Chapter 1 Chapter 2 Chapter 3 with links to Leike and Hutter (2014a, 2015d); Filan et al. (2016) Chapter 4 Chapter 5 Leike and Hutter (2015c); Leike et al.", "startOffset": 100, "endOffset": 120}, {"referenceID": 42, "context": "Chapter Publication(s) Chapter 1 Chapter 2 Chapter 3 with links to Leike and Hutter (2014a, 2015d); Filan et al. (2016) Chapter 4 Chapter 5 Leike and Hutter (2015c); Leike et al.", "startOffset": 100, "endOffset": 165}, {"referenceID": 42, "context": "Chapter Publication(s) Chapter 1 Chapter 2 Chapter 3 with links to Leike and Hutter (2014a, 2015d); Filan et al. (2016) Chapter 4 Chapter 5 Leike and Hutter (2015c); Leike et al. (2016a) Chapter 6 Leike and Hutter (2015b,a, 2016) Chapter 7 Leike et al.", "startOffset": 100, "endOffset": 187}, {"referenceID": 42, "context": "Chapter Publication(s) Chapter 1 Chapter 2 Chapter 3 with links to Leike and Hutter (2014a, 2015d); Filan et al. (2016) Chapter 4 Chapter 5 Leike and Hutter (2015c); Leike et al. (2016a) Chapter 6 Leike and Hutter (2015b,a, 2016) Chapter 7 Leike et al. (2016b) Chapter 8 Appendix A Leike and Hutter (2014b)", "startOffset": 100, "endOffset": 261}, {"referenceID": 42, "context": "Chapter Publication(s) Chapter 1 Chapter 2 Chapter 3 with links to Leike and Hutter (2014a, 2015d); Filan et al. (2016) Chapter 4 Chapter 5 Leike and Hutter (2015c); Leike et al. (2016a) Chapter 6 Leike and Hutter (2015b,a, 2016) Chapter 7 Leike et al. (2016b) Chapter 8 Appendix A Leike and Hutter (2014b)", "startOffset": 100, "endOffset": 307}, {"referenceID": 45, "context": "Only small classes are known to have a grain of truth and the literature contains several related impossibility results (Nachbar, 1997, 2005; Foster and Young, 2001).", "startOffset": 120, "endOffset": 165}, {"referenceID": 26, "context": "(2015, 2016) based on my research in termination analysis (in collaboration with Matthias Heizmann), Daswani and Leike (2015) (coauthored with Mayank Daswani in equal parts), Everitt et al.", "startOffset": 101, "endOffset": 126}, {"referenceID": 26, "context": "(2015, 2016) based on my research in termination analysis (in collaboration with Matthias Heizmann), Daswani and Leike (2015) (coauthored with Mayank Daswani in equal parts), Everitt et al. (2015) (co-authored with Tom Everitt in equal parts), Filan et al.", "startOffset": 101, "endOffset": 197}, {"referenceID": 26, "context": "(2015, 2016) based on my research in termination analysis (in collaboration with Matthias Heizmann), Daswani and Leike (2015) (coauthored with Mayank Daswani in equal parts), Everitt et al. (2015) (co-authored with Tom Everitt in equal parts), Filan et al. (2016) (written by Daniel Filan as part of his honour\u2019s thesis supervised by Marcus Hutter and me).", "startOffset": 101, "endOffset": 264}, {"referenceID": 26, "context": "(2015, 2016) based on my research in termination analysis (in collaboration with Matthias Heizmann), Daswani and Leike (2015) (coauthored with Mayank Daswani in equal parts), Everitt et al. (2015) (co-authored with Tom Everitt in equal parts), Filan et al. (2016) (written by Daniel Filan as part of his honour\u2019s thesis supervised by Marcus Hutter and me). Leike and Hutter (2016) is", "startOffset": 101, "endOffset": 381}, {"referenceID": 31, "context": "We recommend to consult Wasserman (2004) on statistics, Durrett (2010) on probability theory and stochastic processes, Cover and Thomas (2006) on information theory, Li and Vit\u00e1nyi (2008) on algorithmic information theory, Russell and Norvig (2010) on artificial intelligence, Bishop (2006) and Hastie et al.", "startOffset": 56, "endOffset": 71}, {"referenceID": 24, "context": "We recommend to consult Wasserman (2004) on statistics, Durrett (2010) on probability theory and stochastic processes, Cover and Thomas (2006) on information theory, Li and Vit\u00e1nyi (2008) on algorithmic information theory, Russell and Norvig (2010) on artificial intelligence, Bishop (2006) and Hastie et al.", "startOffset": 119, "endOffset": 143}, {"referenceID": 24, "context": "We recommend to consult Wasserman (2004) on statistics, Durrett (2010) on probability theory and stochastic processes, Cover and Thomas (2006) on information theory, Li and Vit\u00e1nyi (2008) on algorithmic information theory, Russell and Norvig (2010) on artificial intelligence, Bishop (2006) and Hastie et al.", "startOffset": 119, "endOffset": 188}, {"referenceID": 24, "context": "We recommend to consult Wasserman (2004) on statistics, Durrett (2010) on probability theory and stochastic processes, Cover and Thomas (2006) on information theory, Li and Vit\u00e1nyi (2008) on algorithmic information theory, Russell and Norvig (2010) on artificial intelligence, Bishop (2006) and Hastie et al.", "startOffset": 119, "endOffset": 249}, {"referenceID": 12, "context": "We recommend to consult Wasserman (2004) on statistics, Durrett (2010) on probability theory and stochastic processes, Cover and Thomas (2006) on information theory, Li and Vit\u00e1nyi (2008) on algorithmic information theory, Russell and Norvig (2010) on artificial intelligence, Bishop (2006) and Hastie et al.", "startOffset": 277, "endOffset": 291}, {"referenceID": 12, "context": "We recommend to consult Wasserman (2004) on statistics, Durrett (2010) on probability theory and stochastic processes, Cover and Thomas (2006) on information theory, Li and Vit\u00e1nyi (2008) on algorithmic information theory, Russell and Norvig (2010) on artificial intelligence, Bishop (2006) and Hastie et al. (2009) on machine learning, Sutton and Barto (1998) on reinforcement learning, and Hutter (2005) and Lattimore (2013) on general reinforcement learning.", "startOffset": 277, "endOffset": 316}, {"referenceID": 12, "context": "We recommend to consult Wasserman (2004) on statistics, Durrett (2010) on probability theory and stochastic processes, Cover and Thomas (2006) on information theory, Li and Vit\u00e1nyi (2008) on algorithmic information theory, Russell and Norvig (2010) on artificial intelligence, Bishop (2006) and Hastie et al. (2009) on machine learning, Sutton and Barto (1998) on reinforcement learning, and Hutter (2005) and Lattimore (2013) on general reinforcement learning.", "startOffset": 277, "endOffset": 361}, {"referenceID": 12, "context": "We recommend to consult Wasserman (2004) on statistics, Durrett (2010) on probability theory and stochastic processes, Cover and Thomas (2006) on information theory, Li and Vit\u00e1nyi (2008) on algorithmic information theory, Russell and Norvig (2010) on artificial intelligence, Bishop (2006) and Hastie et al. (2009) on machine learning, Sutton and Barto (1998) on reinforcement learning, and Hutter (2005) and Lattimore (2013) on general reinforcement learning.", "startOffset": 277, "endOffset": 406}, {"referenceID": 12, "context": "We recommend to consult Wasserman (2004) on statistics, Durrett (2010) on probability theory and stochastic processes, Cover and Thomas (2006) on information theory, Li and Vit\u00e1nyi (2008) on algorithmic information theory, Russell and Norvig (2010) on artificial intelligence, Bishop (2006) and Hastie et al. (2009) on machine learning, Sutton and Barto (1998) on reinforcement learning, and Hutter (2005) and Lattimore (2013) on general reinforcement learning.", "startOffset": 277, "endOffset": 427}, {"referenceID": 33, "context": "This section provides a concise introduction to measure theory; see Durrett (2010) for an extensive treatment.", "startOffset": 68, "endOffset": 83}, {"referenceID": 21, "context": "learning has focused on predicting individual symbols and bounds on the number of prediction errors (Hutter, 2001b, 2005; Cesa-Bianchi and Lugosi, 2006), and the results on merging are from the game theory literature (Blackwell and Dubins, 1962; Kalai and Lehrer, 1994; Lehrer and Smorodinsky, 1996).", "startOffset": 100, "endOffset": 152}, {"referenceID": 13, "context": "learning has focused on predicting individual symbols and bounds on the number of prediction errors (Hutter, 2001b, 2005; Cesa-Bianchi and Lugosi, 2006), and the results on merging are from the game theory literature (Blackwell and Dubins, 1962; Kalai and Lehrer, 1994; Lehrer and Smorodinsky, 1996).", "startOffset": 217, "endOffset": 299}, {"referenceID": 87, "context": "learning has focused on predicting individual symbols and bounds on the number of prediction errors (Hutter, 2001b, 2005; Cesa-Bianchi and Lugosi, 2006), and the results on merging are from the game theory literature (Blackwell and Dubins, 1962; Kalai and Lehrer, 1994; Lehrer and Smorodinsky, 1996).", "startOffset": 217, "endOffset": 299}, {"referenceID": 106, "context": "learning has focused on predicting individual symbols and bounds on the number of prediction errors (Hutter, 2001b, 2005; Cesa-Bianchi and Lugosi, 2006), and the results on merging are from the game theory literature (Blackwell and Dubins, 1962; Kalai and Lehrer, 1994; Lehrer and Smorodinsky, 1996).", "startOffset": 217, "endOffset": 299}, {"referenceID": 21, "context": "A well-known approach to the nonrealizable case is prediction with expert advice (Cesa-Bianchi and Lugosi, 2006), which we do not con-", "startOffset": 81, "endOffset": 112}, {"referenceID": 12, "context": "); see Bishop (2006) and Hastie et al.", "startOffset": 7, "endOffset": 21}, {"referenceID": 12, "context": "); see Bishop (2006) and Hastie et al. (2009). In this chapter we do not make the i.", "startOffset": 7, "endOffset": 46}, {"referenceID": 152, "context": "Generally, the nonrealizable case is harder, but Ryabko (2011) argues that for some problems, both cases coincide.", "startOffset": 49, "endOffset": 63}, {"referenceID": 65, "context": "6 connects the results from the first sections to the learning framework developed by Solomonoff (1964, 1978), Hutter (2001b, 2005, 2007b), and Schmidhuber (2002) (among others).", "startOffset": 111, "endOffset": 163}, {"referenceID": 65, "context": "1 (The Black Ravens; Rathmanner and Hutter, 2011, Sec. 7.4). If we live in a world in which all ravens are black, how can we learn this fact? Since at every time step we have observed only a finite subset of the (possibly infinite) set of all ravens, how can we confidently state anything about all ravens? We formalize this problem in line with Rathmanner and Hutter (2011, Sec. 7.4) and Leike and Hutter (2015d). We define two predicates, blackness B and ravenness R.", "startOffset": 36, "endOffset": 414}, {"referenceID": 65, "context": "It is very easy to derive from a countable set of distributions, and it has been considered extensively in the literature (Solomonoff, 1964; Jaynes, 2003; Hutter, 2005, . . . ). Ryabko (2009) shows that even for uncountably infinite classes, if there are good predictors, then a Bayesian mixture over a countable subclass asymptotically also does well.", "startOffset": 155, "endOffset": 192}, {"referenceID": 65, "context": "It is very easy to derive from a countable set of distributions, and it has been considered extensively in the literature (Solomonoff, 1964; Jaynes, 2003; Hutter, 2005, . . . ). Ryabko (2009) shows that even for uncountably infinite classes, if there are good predictors, then a Bayesian mixture over a countable subclass asymptotically also does well. Example 3.5 (Solomonoff Prior). Solomonoff (1964) defines a distribution M over X ] that assigns to a string x the probability that the universal monotone Turing machine U outputs x when fed with fair coin flips on the input tape.", "startOffset": 155, "endOffset": 403}, {"referenceID": 65, "context": "See Rathmanner and Hutter (2011) for a discussion on the philosophical underpinnings of Solomonoff\u2019s prior.", "startOffset": 19, "endOffset": 33}, {"referenceID": 65, "context": "Ryabko and Hutter (2007, 2008) consider the following definition. It is analogous to Definition 3.3, except that the constant c is allowed to depend on time. Definition 3.10 (Dominance with Coefficients; Ryabko and Hutter, 2008, Def. 2). The measure Q dominates P with coefficients f (Q \u2265 P/f) iff Q(x) \u2265 P (x)/f(|x|) for all x \u2208 X \u2217. If Q dominates P with coefficients f and f grows subexponentially (f \u2208 o(exp)), then Q weakly dominates P by Remark 3.9. Example 3.11 (Speed Prior). Schmidhuber (2002) defines a variant of Solomonoff\u2019s priorM that penalizes programs by their running time, called the speed prior.", "startOffset": 11, "endOffset": 503}, {"referenceID": 53, "context": "14 (The Minimum Description Length Principle; Gr\u00fcnwald, 2007).", "startOffset": 3, "endOffset": 61}, {"referenceID": 53, "context": "14 (The Minimum Description Length Principle; Gr\u00fcnwald, 2007). Let M be a countable set of probability measures on (X,F\u221e) and let K : M \u2192 [0, 1] be a function such that \u2211 P\u2208M 2 \u2212K(P ) \u2264 1 called regularizer. Following notation from Hutter (2009a), we define for each x \u2208 X \u2217 the minimal description length model as MDL := arg min P\u2208M {\u2212 logP (x) +K(P )}.", "startOffset": 46, "endOffset": 247}, {"referenceID": 86, "context": "Much of this section is based on Kalai and Lehrer (1994) and Lehrer and Smorodinsky (1996).", "startOffset": 33, "endOffset": 57}, {"referenceID": 86, "context": "Much of this section is based on Kalai and Lehrer (1994) and Lehrer and Smorodinsky (1996).", "startOffset": 33, "endOffset": 91}, {"referenceID": 13, "context": "25 (Absolute Continuity\u21d2 Strong Merging; Blackwell and Dubins, 1962).", "startOffset": 3, "endOffset": 68}, {"referenceID": 13, "context": "The following theorem is the famous merging of opinions theorem by Blackwell and Dubins (1962). Theorem 3.", "startOffset": 67, "endOffset": 95}, {"referenceID": 65, "context": "More generally, we could also follow Hutter (2001b) and phrase predictive performance in terms of loss: given a loss function ` : X \u00d7X \u2192 R the predictor Q suffers an (instantaneous) loss of `(xQt , xt ) in time step t.", "startOffset": 37, "endOffset": 52}, {"referenceID": 65, "context": "1 Dominance We start with the prediction regret bounds proved by Hutter (2001b) in case the learning distribution Q dominates the true distribution P .", "startOffset": 65, "endOffset": 80}, {"referenceID": 214, "context": "We have (Weisstein, 2002)", "startOffset": 8, "endOffset": 25}, {"referenceID": 133, "context": "The proof idea is inspired by Miller and Sanchirico (1999). We think of P and Q as two players in a zero-sum betting game.", "startOffset": 30, "endOffset": 59}, {"referenceID": 65, "context": "See Rathmanner and Hutter (2011) for a very readable introduction to Solomonoff\u2019s theory and its philosophical motivations and Sterkenburg (2016) for a critique of its optimality.", "startOffset": 19, "endOffset": 33}, {"referenceID": 65, "context": "See Rathmanner and Hutter (2011) for a very readable introduction to Solomonoff\u2019s theory and its philosophical motivations and Sterkenburg (2016) for a critique of its optimality.", "startOffset": 19, "endOffset": 146}, {"referenceID": 42, "context": "In this section we state merging and prediction results for SKt, a speed prior introduced by Filan et al. (2016) formally defined in Example 3.", "startOffset": 93, "endOffset": 113}, {"referenceID": 42, "context": "In this section we state merging and prediction results for SKt, a speed prior introduced by Filan et al. (2016) formally defined in Example 3.11. It is slightly different from the speed prior defined by Schmidhuber (2002), but for the latter no compatibility properties are known for nondeterministic measures.", "startOffset": 93, "endOffset": 223}, {"referenceID": 54, "context": "G\u00e1cs (1983) shows that the similarity M \u2248 2\u2212Km is not an equality.", "startOffset": 0, "endOffset": 12}, {"referenceID": 29, "context": "to Day (2011) who shows that Km(x) > \u2212 logM(x) +O(log log |x|) for infinitely many x \u2208 X \u2217.", "startOffset": 3, "endOffset": 14}, {"referenceID": 29, "context": "to Day (2011) who shows that Km(x) > \u2212 logM(x) +O(log log |x|) for infinitely many x \u2208 X \u2217. Nevertheless, 2\u2212Km dominates every computable measure (Li and Vit\u00e1nyi, 2008, Thm. 4.5.4 and Lem. 4.5.6ii(d); originally proved by Levin, 1973). Hence all the strong results that hold for Solomonoff induction (prediction regret and strong merging) also hold for compression: we apply Theorem 3.25 and Corollary 3.44 to get the following results. See Hutter (2006a) for further discussion on using the universal compressor Km for learning.", "startOffset": 3, "endOffset": 456}, {"referenceID": 65, "context": "In this spirit, the Hutter prize is awarded for the compression of a 100MB excerpt from the English Wikipedia (Hutter, 2006c). Practical compression algorithms (such as the algorithm by Ziv and Lempel (1977) used in gzip) are not universal.", "startOffset": 20, "endOffset": 208}, {"referenceID": 65, "context": "In this spirit, the Hutter prize is awarded for the compression of a 100MB excerpt from the English Wikipedia (Hutter, 2006c). Practical compression algorithms (such as the algorithm by Ziv and Lempel (1977) used in gzip) are not universal. Hence they do not dominate every computable distribution. As with the speed prior, what matters is the rate at which Yt = Q(x1:t)/P (x1:t) goes to 0, i.e., does the compressor weakly dominate the true distribution in the sense of Definition 3.8? Veness et al. (2015) successfully apply the Lempel-Ziv compression algorithm as a learning algorithm for reinforcement learning; however, some preprocessing of the data is required.", "startOffset": 20, "endOffset": 508}, {"referenceID": 65, "context": "In this spirit, the Hutter prize is awarded for the compression of a 100MB excerpt from the English Wikipedia (Hutter, 2006c). Practical compression algorithms (such as the algorithm by Ziv and Lempel (1977) used in gzip) are not universal. Hence they do not dominate every computable distribution. As with the speed prior, what matters is the rate at which Yt = Q(x1:t)/P (x1:t) goes to 0, i.e., does the compressor weakly dominate the true distribution in the sense of Definition 3.8? Veness et al. (2015) successfully apply the Lempel-Ziv compression algorithm as a learning algorithm for reinforcement learning; however, some preprocessing of the data is required. More remotely, Vit\u00e1nyi et al. (2009) use standard compression algorithms to classify mammal genomes, languages, and classical music.", "startOffset": 20, "endOffset": 706}, {"referenceID": 13, "context": "3i) Blackwell and Dubins (1962)", "startOffset": 4, "endOffset": 32}, {"referenceID": 63, "context": "The paradox of confirmation, also known as Hempel\u2019s paradox (Hempel, 1945), relies on the following three principles.", "startOffset": 60, "endOffset": 74}, {"referenceID": 63, "context": "The paradox of confirmation has been discussed extensively in the literature on the philosophy of science (Hempel, 1945; Good, 1960; Mackie, 1963; Good, 1967; Hempel, 1967; Maher, 1999; Vranas, 2004); see Swinburne (1971) for a survey.", "startOffset": 106, "endOffset": 199}, {"referenceID": 47, "context": "The paradox of confirmation has been discussed extensively in the literature on the philosophy of science (Hempel, 1945; Good, 1960; Mackie, 1963; Good, 1967; Hempel, 1967; Maher, 1999; Vranas, 2004); see Swinburne (1971) for a survey.", "startOffset": 106, "endOffset": 199}, {"referenceID": 124, "context": "The paradox of confirmation has been discussed extensively in the literature on the philosophy of science (Hempel, 1945; Good, 1960; Mackie, 1963; Good, 1967; Hempel, 1967; Maher, 1999; Vranas, 2004); see Swinburne (1971) for a survey.", "startOffset": 106, "endOffset": 199}, {"referenceID": 49, "context": "The paradox of confirmation has been discussed extensively in the literature on the philosophy of science (Hempel, 1945; Good, 1960; Mackie, 1963; Good, 1967; Hempel, 1967; Maher, 1999; Vranas, 2004); see Swinburne (1971) for a survey.", "startOffset": 106, "endOffset": 199}, {"referenceID": 64, "context": "The paradox of confirmation has been discussed extensively in the literature on the philosophy of science (Hempel, 1945; Good, 1960; Mackie, 1963; Good, 1967; Hempel, 1967; Maher, 1999; Vranas, 2004); see Swinburne (1971) for a survey.", "startOffset": 106, "endOffset": 199}, {"referenceID": 128, "context": "The paradox of confirmation has been discussed extensively in the literature on the philosophy of science (Hempel, 1945; Good, 1960; Mackie, 1963; Good, 1967; Hempel, 1967; Maher, 1999; Vranas, 2004); see Swinburne (1971) for a survey.", "startOffset": 106, "endOffset": 199}, {"referenceID": 211, "context": "The paradox of confirmation has been discussed extensively in the literature on the philosophy of science (Hempel, 1945; Good, 1960; Mackie, 1963; Good, 1967; Hempel, 1967; Maher, 1999; Vranas, 2004); see Swinburne (1971) for a survey.", "startOffset": 106, "endOffset": 199}, {"referenceID": 124, "context": "Support for Nicod\u2019s criterion is not uncommon (Mackie, 1963; Hempel, 1967; Maher, 1999) and no consensus is in sight.", "startOffset": 46, "endOffset": 87}, {"referenceID": 64, "context": "Support for Nicod\u2019s criterion is not uncommon (Mackie, 1963; Hempel, 1967; Maher, 1999) and no consensus is in sight.", "startOffset": 46, "endOffset": 87}, {"referenceID": 128, "context": "Support for Nicod\u2019s criterion is not uncommon (Mackie, 1963; Hempel, 1967; Maher, 1999) and no consensus is in sight.", "startOffset": 46, "endOffset": 87}, {"referenceID": 47, "context": "A Bayesian reasoner might be tempted to argue that a green apple does confirm the hypothesis H, but only to a small degree, since there are vastly more non-black objects than ravens (Good, 1960).", "startOffset": 182, "endOffset": 194}, {"referenceID": 47, "context": "The paradox of confirmation has been discussed extensively in the literature on the philosophy of science (Hempel, 1945; Good, 1960; Mackie, 1963; Good, 1967; Hempel, 1967; Maher, 1999; Vranas, 2004); see Swinburne (1971) for a survey.", "startOffset": 121, "endOffset": 222}, {"referenceID": 179, "context": "Vranas (2004) shows that this solution is equivalent to the assertion that blackness is equally probable regardless of whether H holds: P (black|H) \u2248 P (black).", "startOffset": 0, "endOffset": 14}, {"referenceID": 47, "context": "The following is a very concise example against the standard Bayesian solution by Good (1967): There are two possible worlds, the first has 100 black ravens and a million other birds, while the second has 1000 black ravens, one white raven, and a million other birds.", "startOffset": 82, "endOffset": 94}, {"referenceID": 47, "context": "The following is a very concise example against the standard Bayesian solution by Good (1967): There are two possible worlds, the first has 100 black ravens and a million other birds, while the second has 1000 black ravens, one white raven, and a million other birds. Now we draw a bird uniformly at random, and it turns out to be a black raven. Contrary to what Nicod\u2019s criterion claims, this is strong evidence that we are in fact in the second world, and in this world non-black ravens exist. For another, more intuitive example: Suppose you do not know anything about ravens and you have a friend who collects atypical objects. If you see a black raven in her collection, surely this would not increase your belief in the hypothesis that all ravens are black. In Leike and Hutter (2015d) we investigate the paradox of confirmation in the context of Solomonoff induction.", "startOffset": 82, "endOffset": 792}, {"referenceID": 65, "context": "It is mostly based on Hutter (2005) and Lattimore (2013).", "startOffset": 22, "endOffset": 36}, {"referenceID": 65, "context": "It is mostly based on Hutter (2005) and Lattimore (2013). Section 4.", "startOffset": 22, "endOffset": 57}, {"referenceID": 65, "context": "Equivalently, Hutter (2005) defines environments as chronological contextual semimeasures.", "startOffset": 14, "endOffset": 28}, {"referenceID": 65, "context": "Hutter (2005) calls them chronological conditional semimeasures.", "startOffset": 0, "endOffset": 14}, {"referenceID": 37, "context": "Decision making in the physicalistic model is still underdeveloped; see Everitt et al. (2015) and Orseau and Ring (2012a).", "startOffset": 72, "endOffset": 94}, {"referenceID": 37, "context": "Decision making in the physicalistic model is still underdeveloped; see Everitt et al. (2015) and Orseau and Ring (2012a). In this thesis we restrict ourselves to the dualistic model.", "startOffset": 72, "endOffset": 122}, {"referenceID": 65, "context": "For a discussion of general discounting we refer the reader to Lattimore and Hutter (2014).", "startOffset": 77, "endOffset": 91}, {"referenceID": 3, "context": "Moreover, with unbounded rewards there all kinds of pathological problems where defining optimal actions is no longer straightforward; see Arntzenius et al. (2004) for a discussion.", "startOffset": 139, "endOffset": 164}, {"referenceID": 19, "context": "Bandits exist in many flavors; see Bubeck and Bianchi (2012) for a survey.", "startOffset": 35, "endOffset": 61}, {"referenceID": 198, "context": "Much of today\u2019s literature on reinforcement learning focuses on MDPs (Sutton and Barto, 1998).", "startOffset": 69, "endOffset": 93}, {"referenceID": 135, "context": "While they have a huge state space2 they can still be learned using Q-learning with function approximation (Mnih et al., 2015).", "startOffset": 107, "endOffset": 126}, {"referenceID": 7, "context": "For any physical system of finite volume and finite (average) energy, the amount of information it can contain is finite (Bekenstein, 1981), and so is the number of state transitions per unit of time (Margolus and Levitin, 1998).", "startOffset": 121, "endOffset": 139}, {"referenceID": 130, "context": "For any physical system of finite volume and finite (average) energy, the amount of information it can contain is finite (Bekenstein, 1981), and so is the number of state transitions per unit of time (Margolus and Levitin, 1998).", "startOffset": 200, "endOffset": 228}, {"referenceID": 65, "context": "12 (Optimal Policy; Hutter, 2005, Def. 5.19 & 5.30). A policy \u03c0 is optimal in environment \u03bd (\u03bd-optimal) iff for all histories \u03c0 attains the optimal value: V \u03c0 \u03bd (\u00e6<t) = V \u2217 \u03bd (\u00e6<t) for all \u00e6<t \u2208 (A\u00d7E)\u2217. The action at is an optimal action iff \u03c0\u2217 \u03bd(at | \u00e6<t) = 1 for some \u03bd-optimal policy \u03c0\u2217 \u03bd . Following the tradition of Hutter (2005), AINU denotes a \u03bd-optimal policy for the environment \u03bd \u2208 MCCS LSC and AIMU denotes an \u03bc-optimal policy for the environment \u03bc \u2208MCCM comp that is a measure (as opposed to a semimeasure).", "startOffset": 20, "endOffset": 335}, {"referenceID": 215, "context": "5) can be defined equivalently according to (Wood et al., 2011) \u03be(e<t \u2016 a<t) := \u2211", "startOffset": 44, "endOffset": 63}, {"referenceID": 94, "context": "This strategy even achieves the optimal asymptotic regret bounds (Lattimore, 2016).", "startOffset": 65, "endOffset": 82}, {"referenceID": 45, "context": "For multi-armed bandits, Gittins (1979) achieved a breakthrough with an index strategy that enables the computation of the optimal policy by computing one quantity for each arm independently of the rest.", "startOffset": 25, "endOffset": 40}, {"referenceID": 45, "context": "For multi-armed bandits, Gittins (1979) achieved a breakthrough with an index strategy that enables the computation of the optimal policy by computing one quantity for each arm independently of the rest. This strategy even achieves the optimal asymptotic regret bounds (Lattimore, 2016). Larger classes have also been attempted: using Monte-Carlo tree search, Veness et al. (2011) approximate the Bayes optimal policy in the class of all context trees.", "startOffset": 25, "endOffset": 381}, {"referenceID": 32, "context": "Doshi-Velez (2012) uses Bayesian techniques to learn infinite-state POMDPs.", "startOffset": 0, "endOffset": 19}, {"referenceID": 32, "context": "Doshi-Velez (2012) uses Bayesian techniques to learn infinite-state POMDPs. See Vlassis et al. (2012) for a survey on Bayesian techniques in RL.", "startOffset": 0, "endOffset": 102}, {"referenceID": 149, "context": "In this section we discuss two variants of knowledge-seeking agents: entropy-seeking agents introduced by Orseau (2011, 2014a) and information-seeking agents introduced by Orseau et al. (2013). The entropy-seeking agent maximizes the Shannon entropy gain, while the information-seeking agent maximizes the expected information gain.", "startOffset": 106, "endOffset": 193}, {"referenceID": 23, "context": "It is easy to implement and often achieves quite good results (Chapelle and Li, 2011).", "startOffset": 62, "endOffset": 85}, {"referenceID": 0, "context": "In multi-armed bandits it attains optimal regret (Agrawal and Goyal, 2011; Kaufmann et al., 2012).", "startOffset": 49, "endOffset": 97}, {"referenceID": 88, "context": "In multi-armed bandits it attains optimal regret (Agrawal and Goyal, 2011; Kaufmann et al., 2012).", "startOffset": 49, "endOffset": 97}, {"referenceID": 192, "context": "Thompson sampling has also been discussed for MDPs (Strens, 2000; Dearden et al., 1998) and Bayesian and frequentist regret bounds have been established (Osband et al.", "startOffset": 51, "endOffset": 87}, {"referenceID": 30, "context": "Thompson sampling has also been discussed for MDPs (Strens, 2000; Dearden et al., 1998) and Bayesian and frequentist regret bounds have been established (Osband et al.", "startOffset": 51, "endOffset": 87}, {"referenceID": 162, "context": ", 1998) and Bayesian and frequentist regret bounds have been established (Osband et al., 2013; Gopalan and Mannor, 2015).", "startOffset": 73, "endOffset": 120}, {"referenceID": 51, "context": ", 1998) and Bayesian and frequentist regret bounds have been established (Osband et al., 2013; Gopalan and Mannor, 2015).", "startOffset": 73, "endOffset": 120}, {"referenceID": 193, "context": "4 Thompson Sampling Thompson sampling, also known as posterior sampling or the Bayesian control rule, was originally proposed by Thompson (1933) as a bandit algorithm.", "startOffset": 2, "endOffset": 145}, {"referenceID": 0, "context": "In multi-armed bandits it attains optimal regret (Agrawal and Goyal, 2011; Kaufmann et al., 2012). Thompson sampling has also been discussed for MDPs (Strens, 2000; Dearden et al., 1998) and Bayesian and frequentist regret bounds have been established (Osband et al., 2013; Gopalan and Mannor, 2015). For general RL Thompson sampling was first suggested by Ortega and Braun (2010) with resampling at every time step.", "startOffset": 50, "endOffset": 381}, {"referenceID": 0, "context": "In multi-armed bandits it attains optimal regret (Agrawal and Goyal, 2011; Kaufmann et al., 2012). Thompson sampling has also been discussed for MDPs (Strens, 2000; Dearden et al., 1998) and Bayesian and frequentist regret bounds have been established (Osband et al., 2013; Gopalan and Mannor, 2015). For general RL Thompson sampling was first suggested by Ortega and Braun (2010) with resampling at every time step. Strens (2000) proposes following the optimal policy for one episode or \u201crelated to the number of state transitions the agent is likely to need to plan ahead\u201d.", "startOffset": 50, "endOffset": 431}, {"referenceID": 127, "context": "See also Mahadevan (1996) for a discussion of notions of optimality in MDPs.", "startOffset": 9, "endOffset": 26}, {"referenceID": 65, "context": "1 (Pareto Optimality; Hutter, 2005, Def. 5.22). A policy \u03c0 is Pareto optimal in the set of environmentsM iff there is no policy \u03c0\u0303 such that V \u03c0\u0303 \u03bd ( ) \u2265 V \u03c0 \u03bd ( ) for all \u03bd \u2208M and V \u03c0\u0303 \u03c1 ( ) > V \u03c0 \u03c1 ( ) for at least one \u03c1 \u2208M. The literature provides the following result. Theorem 5.2 (AIXI is Pareto Optimal; Hutter, 2002a, Thm. 2). Every \u03be-optimal policy is Pareto optimal inMCCS LSC . The following theorem was proved for deterministic policies in Leike and Hutter (2015c). Here we extend it to stochastic policies.", "startOffset": 22, "endOffset": 476}, {"referenceID": 65, "context": "The following proof was adapted from Leike and Hutter (2015c) to work for environment classes that do not contain the Bayesian mixture.", "startOffset": 47, "endOffset": 62}, {"referenceID": 55, "context": "3 The G\u00f6del Prior This section introduces a prior that prevents any fixed formal system from making any statements about the outcome of all but finitely many computations. It is named after G\u00f6del (1931) who famously showed that for any sufficiently rich formal system there are statements that it can neither prove nor disprove.", "startOffset": 6, "endOffset": 203}, {"referenceID": 65, "context": "The aim of the Legg-Hutter intelligence measure is to formalize the intuitive notion of intelligence mathematically. Legg and Hutter (2007a) collect various definitions of intelligence across many academic fields and destill it into the following statement (Legg and Hutter, 2007b) Intelligence measures an agent\u2019s ability to achieve goals in a wide range of environments.", "startOffset": 20, "endOffset": 141}, {"referenceID": 65, "context": "The Legg-Hutter intelligence of a policy \u03c0 is the t0-value that \u03c0 achieves across all environments from the classM weighted by the prior w. Legg and Hutter (2007b) consider a subclass ofMCCS LSC , the class of computable measures together with a Solomonoff prior w(\u03bd) = 2\u2212K(\u03bd) and do not use discounting explicitly.", "startOffset": 9, "endOffset": 164}, {"referenceID": 105, "context": "It is natural to fix the policy random that takes actions uniformly at random to have an intelligence score of 1/2 by choosing a \u2018symmetric\u2019 universal prior (Legg and Veness, 2013).", "startOffset": 157, "endOffset": 180}, {"referenceID": 65, "context": "Next, we state two negative results about asymptotic optimality proved by Lattimore and Hutter (2011). It is important to emphasize that Theorem 5.", "startOffset": 88, "endOffset": 102}, {"referenceID": 93, "context": "In this subsection we state a result by Lattimore (2013) that motivated the definition of BayesExp.", "startOffset": 40, "endOffset": 57}, {"referenceID": 160, "context": "Ortega and Braun (2010) prove that the action probabilities of Thompson sampling converge to the action probability of the optimal policy almost surely, but require a finite environment class and two (arguably quite strong) technical assumptions on the behavior of the posterior distribution (akin to ergodicity) and the similarity of environments in the class.", "startOffset": 0, "endOffset": 24}, {"referenceID": 19, "context": "Multi-armed bandits provide a (problemindependent) worst-case regret bound of \u03a9( \u221a km) where k is the number of arms (Bubeck and Bianchi, 2012).", "startOffset": 117, "endOffset": 143}, {"referenceID": 5, "context": "In MDPs the lower bound is \u03a9( \u221a SAdm) where S is the number of states, A the number of actions, and d the diameter of the MDP (Auer et al., 2010).", "startOffset": 126, "endOffset": 145}, {"referenceID": 144, "context": "For a countable class of environments given by state representation functions that map histories to MDP states, a regret of \u00d5(m2/3) is achievable assuming the resulting MDP is weakly communicating (Nguyen et al., 2013).", "startOffset": 197, "endOffset": 218}, {"referenceID": 93, "context": "However, this problem can be alleviated by adding an extra exploration component to AIXI: Lattimore (2013) shows that BayesExp is weakly asymptotically optimal (Theorem 5.", "startOffset": 90, "endOffset": 107}, {"referenceID": 55, "context": "5), and the G\u00f6del prior (Theorem 5.8) should be considered unnatural. But what are other desirable properties of a UTM? A remarkable but unsuccessful attempt to find natural UTMs is due to M\u00fcller (2010). It takes the probability that one universal machine simulates another according", "startOffset": 12, "endOffset": 203}, {"referenceID": 93, "context": "25) and BayesExp (Lattimore, 2013), but not AIXI (Orseau, 2013) sublinear regret impossible in general environments, but possible with recoverability (Theorem 5.", "startOffset": 17, "endOffset": 34}, {"referenceID": 151, "context": "25) and BayesExp (Lattimore, 2013), but not AIXI (Orseau, 2013) sublinear regret impossible in general environments, but possible with recoverability (Theorem 5.", "startOffset": 49, "endOffset": 63}, {"referenceID": 163, "context": "For MDPs, planning is already P-complete for finite and infinite horizons (Papadimitriou and Tsitsiklis, 1987).", "startOffset": 74, "endOffset": 110}, {"referenceID": 138, "context": "The existence of a policy whose expected value exceeds a given threshold is PSPACE-complete (Mundhenk et al., 2000), even for purely epistemic POMDPs in which actions do not change the hidden state (Sabbadin et al.", "startOffset": 92, "endOffset": 115}, {"referenceID": 175, "context": ", 2000), even for purely epistemic POMDPs in which actions do not change the hidden state (Sabbadin et al., 2007).", "startOffset": 90, "endOffset": 113}, {"referenceID": 65, "context": "In contrast, Hutter (2005) defines the value function as the limit of the iterative value function.", "startOffset": 13, "endOffset": 27}, {"referenceID": 65, "context": "We gave a different, more complicated proof in Leike and Hutter (2015b). The following, much simpler and more elegant proof is due to Sterkenburg (2016, Prop.", "startOffset": 57, "endOffset": 72}, {"referenceID": 65, "context": "In Leike and Hutter (2015a) the use of the symbols V and W is reversed.", "startOffset": 13, "endOffset": 28}, {"referenceID": 131, "context": "of ending is a philosophical debate we do not want to engage in here; see Martin et al. (2016) for a discussion.", "startOffset": 74, "endOffset": 95}, {"referenceID": 86, "context": "Kalai and Lehrer (1993) show that in infinitely repeated games Bayesian agents converge to an \u03b5-Nash equilibrium as long as each agent assigns positive prior probability to the other agents\u2019 policies (a grain of truth).", "startOffset": 0, "endOffset": 24}, {"referenceID": 24, "context": "Alternating strategies like TitForTat (cooperate first, then play The idea for reflective oracles was developed by Jessica Taylor and Benya Fallenstein based on ideas by Paul Christiano (Christiano et al., 2013).", "startOffset": 186, "endOffset": 211}, {"referenceID": 24, "context": "Alternating strategies like TitForTat (cooperate first, then play The idea for reflective oracles was developed by Jessica Taylor and Benya Fallenstein based on ideas by Paul Christiano (Christiano et al., 2013). Reflective oracles were first described in Fallenstein et al. (2015a). The proof of Theorem 7.", "startOffset": 187, "endOffset": 283}, {"referenceID": 45, "context": "Foster and Young (2001) and Nachbar (1997, 2005) prove several impossibility results on the grain of truth problem that identify properties that cannot be simultaneously satisfied for classes that allow a grain of truth (see Section 7.", "startOffset": 0, "endOffset": 24}, {"referenceID": 161, "context": "This is not the first time Thompson sampling is used in game theory (Ortega and Braun, 2014), but the first time to show that it achieves such general positive results.", "startOffset": 68, "endOffset": 92}, {"referenceID": 20, "context": "The same problem occurs in multi-agent reinforcement learning (Busoniu et al., 2008).", "startOffset": 62, "endOffset": 84}, {"referenceID": 18, "context": "Assuming convergence to a stationary policy is a necessary criterion to enable all agents to learn, but the process is unstable for many reinforcement learning algorithms and only empirical positive results are known (Bowling and Veloso, 2001).", "startOffset": 217, "endOffset": 243}, {"referenceID": 86, "context": "Since our classMrefl solves the grain of truth problem, the result by Kalai and Lehrer (1993) immediately implies that for any Bayesian agents \u03c01, .", "startOffset": 70, "endOffset": 94}, {"referenceID": 45, "context": "Foster and Young (2001) present a condition that makes convergence to a Nash equilibrium impossible: if the player\u2019s rewards are perturbed by a small real number drawn from some continuous density \u03bd, then for \u03bd-almost all realizations the players do not learn to predict each other and do not converge to a Nash equilibrium.", "startOffset": 0, "endOffset": 24}, {"referenceID": 45, "context": "Foster and Young (2001) present a condition that makes convergence to a Nash equilibrium impossible: if the player\u2019s rewards are perturbed by a small real number drawn from some continuous density \u03bd, then for \u03bd-almost all realizations the players do not learn to predict each other and do not converge to a Nash equilibrium. For example, in a matching pennies game, rational agents randomize only if the (subjective) values of both actions are exactly equal. But this happens only with \u03bd-probability zero, since \u03bd is a density. Thus with \u03bd-probability one the agents do not randomize. If the agents do not randomize, they either fail to learn to predict each other, or they are not acting rationally according to their beliefs: otherwise they would seize the opportunity to exploit the other player\u2019s deterministic action. But this does not contradict our convergence result: the class Mrefl is countable and each \u03bd \u2208 Mrefl has positive prior probability. Perturbation of rewards with arbitrary real numbers is not possible. Even more, the argument given by Foster and Young (2001) cannot work in our setting: the Bayesian mixture \u03be mixes over \u03bbT for all probabilistic Turing machines T .", "startOffset": 0, "endOffset": 1082}, {"referenceID": 140, "context": "But research on AI has progressed steadily over the last decades and there is good reason to believe that we will be able to build HLAI eventually, and even sooner than most people think (M\u00fcller and Bostrom, 2016).", "startOffset": 187, "endOffset": 213}, {"referenceID": 48, "context": "Once we engineer HLAI, it seems unlikely that progress is going to stop; why not build even smarter machines? This could lead to an intelligence explosion (Good, 1965; Vinge, 1993; Kurzweil, 2005; Chalmers, 2010; Hutter, 2012a; Schmidhuber, 2012; Muehlhauser and Salamon, 2012; Eden et al., 2013; Shanahan, 2015; Eden, 2016; Walsh, 2016): a (possibly very rapid) increase in intelligence, e.", "startOffset": 155, "endOffset": 337}, {"referenceID": 208, "context": "Once we engineer HLAI, it seems unlikely that progress is going to stop; why not build even smarter machines? This could lead to an intelligence explosion (Good, 1965; Vinge, 1993; Kurzweil, 2005; Chalmers, 2010; Hutter, 2012a; Schmidhuber, 2012; Muehlhauser and Salamon, 2012; Eden et al., 2013; Shanahan, 2015; Eden, 2016; Walsh, 2016): a (possibly very rapid) increase in intelligence, e.", "startOffset": 155, "endOffset": 337}, {"referenceID": 92, "context": "Once we engineer HLAI, it seems unlikely that progress is going to stop; why not build even smarter machines? This could lead to an intelligence explosion (Good, 1965; Vinge, 1993; Kurzweil, 2005; Chalmers, 2010; Hutter, 2012a; Schmidhuber, 2012; Muehlhauser and Salamon, 2012; Eden et al., 2013; Shanahan, 2015; Eden, 2016; Walsh, 2016): a (possibly very rapid) increase in intelligence, e.", "startOffset": 155, "endOffset": 337}, {"referenceID": 22, "context": "Once we engineer HLAI, it seems unlikely that progress is going to stop; why not build even smarter machines? This could lead to an intelligence explosion (Good, 1965; Vinge, 1993; Kurzweil, 2005; Chalmers, 2010; Hutter, 2012a; Schmidhuber, 2012; Muehlhauser and Salamon, 2012; Eden et al., 2013; Shanahan, 2015; Eden, 2016; Walsh, 2016): a (possibly very rapid) increase in intelligence, e.", "startOffset": 155, "endOffset": 337}, {"referenceID": 178, "context": "Once we engineer HLAI, it seems unlikely that progress is going to stop; why not build even smarter machines? This could lead to an intelligence explosion (Good, 1965; Vinge, 1993; Kurzweil, 2005; Chalmers, 2010; Hutter, 2012a; Schmidhuber, 2012; Muehlhauser and Salamon, 2012; Eden et al., 2013; Shanahan, 2015; Eden, 2016; Walsh, 2016): a (possibly very rapid) increase in intelligence, e.", "startOffset": 155, "endOffset": 337}, {"referenceID": 137, "context": "Once we engineer HLAI, it seems unlikely that progress is going to stop; why not build even smarter machines? This could lead to an intelligence explosion (Good, 1965; Vinge, 1993; Kurzweil, 2005; Chalmers, 2010; Hutter, 2012a; Schmidhuber, 2012; Muehlhauser and Salamon, 2012; Eden et al., 2013; Shanahan, 2015; Eden, 2016; Walsh, 2016): a (possibly very rapid) increase in intelligence, e.", "startOffset": 155, "endOffset": 337}, {"referenceID": 35, "context": "Once we engineer HLAI, it seems unlikely that progress is going to stop; why not build even smarter machines? This could lead to an intelligence explosion (Good, 1965; Vinge, 1993; Kurzweil, 2005; Chalmers, 2010; Hutter, 2012a; Schmidhuber, 2012; Muehlhauser and Salamon, 2012; Eden et al., 2013; Shanahan, 2015; Eden, 2016; Walsh, 2016): a (possibly very rapid) increase in intelligence, e.", "startOffset": 155, "endOffset": 337}, {"referenceID": 180, "context": "Once we engineer HLAI, it seems unlikely that progress is going to stop; why not build even smarter machines? This could lead to an intelligence explosion (Good, 1965; Vinge, 1993; Kurzweil, 2005; Chalmers, 2010; Hutter, 2012a; Schmidhuber, 2012; Muehlhauser and Salamon, 2012; Eden et al., 2013; Shanahan, 2015; Eden, 2016; Walsh, 2016): a (possibly very rapid) increase in intelligence, e.", "startOffset": 155, "endOffset": 337}, {"referenceID": 34, "context": "Once we engineer HLAI, it seems unlikely that progress is going to stop; why not build even smarter machines? This could lead to an intelligence explosion (Good, 1965; Vinge, 1993; Kurzweil, 2005; Chalmers, 2010; Hutter, 2012a; Schmidhuber, 2012; Muehlhauser and Salamon, 2012; Eden et al., 2013; Shanahan, 2015; Eden, 2016; Walsh, 2016): a (possibly very rapid) increase in intelligence, e.", "startOffset": 155, "endOffset": 337}, {"referenceID": 212, "context": "Once we engineer HLAI, it seems unlikely that progress is going to stop; why not build even smarter machines? This could lead to an intelligence explosion (Good, 1965; Vinge, 1993; Kurzweil, 2005; Chalmers, 2010; Hutter, 2012a; Schmidhuber, 2012; Muehlhauser and Salamon, 2012; Eden et al., 2013; Shanahan, 2015; Eden, 2016; Walsh, 2016): a (possibly very rapid) increase in intelligence, e.", "startOffset": 155, "endOffset": 337}, {"referenceID": 14, "context": "But research on AI has progressed steadily over the last decades and there is good reason to believe that we will be able to build HLAI eventually, and even sooner than most people think (M\u00fcller and Bostrom, 2016). The advent of strong AI would be the biggest event in human history. Potential benefits are huge, as the new level of automation would free us from any kind of undesirable labor. But there is no reason to believe that humans are at the far end of the intelligence spectrum. Rather, humans barely cross the threshold for general intelligence to be able to use language and do science. Once we engineer HLAI, it seems unlikely that progress is going to stop; why not build even smarter machines? This could lead to an intelligence explosion (Good, 1965; Vinge, 1993; Kurzweil, 2005; Chalmers, 2010; Hutter, 2012a; Schmidhuber, 2012; Muehlhauser and Salamon, 2012; Eden et al., 2013; Shanahan, 2015; Eden, 2016; Walsh, 2016): a (possibly very rapid) increase in intelligence, e.g., through self-amplification effects from AIs improving themselves (if doing AI research is one of humans\u2019 capabilities, then a machine that can do everything humans can do can also do AI research). Once machine intelligence is above or far above human level, machines would steer the course of history. There is no reason to believe that machines would be adversarial to us, but nevertheless humanity\u2019s fate might rest at the whims of the machines, just as chimpanzees today have no say in the large-scale events on this planet. Bostrom (2002) defines: Existential risk \u2014 One where an adverse outcome would either annihilate Earth-originating intelligent life or permanently and drastically curtail its potential.", "startOffset": 199, "endOffset": 1537}, {"referenceID": 14, "context": "But research on AI has progressed steadily over the last decades and there is good reason to believe that we will be able to build HLAI eventually, and even sooner than most people think (M\u00fcller and Bostrom, 2016). The advent of strong AI would be the biggest event in human history. Potential benefits are huge, as the new level of automation would free us from any kind of undesirable labor. But there is no reason to believe that humans are at the far end of the intelligence spectrum. Rather, humans barely cross the threshold for general intelligence to be able to use language and do science. Once we engineer HLAI, it seems unlikely that progress is going to stop; why not build even smarter machines? This could lead to an intelligence explosion (Good, 1965; Vinge, 1993; Kurzweil, 2005; Chalmers, 2010; Hutter, 2012a; Schmidhuber, 2012; Muehlhauser and Salamon, 2012; Eden et al., 2013; Shanahan, 2015; Eden, 2016; Walsh, 2016): a (possibly very rapid) increase in intelligence, e.g., through self-amplification effects from AIs improving themselves (if doing AI research is one of humans\u2019 capabilities, then a machine that can do everything humans can do can also do AI research). Once machine intelligence is above or far above human level, machines would steer the course of history. There is no reason to believe that machines would be adversarial to us, but nevertheless humanity\u2019s fate might rest at the whims of the machines, just as chimpanzees today have no say in the large-scale events on this planet. Bostrom (2002) defines: Existential risk \u2014 One where an adverse outcome would either annihilate Earth-originating intelligent life or permanently and drastically curtail its potential. Existential risks are events that have the power to extinguish human life as we know it. Examples are cosmic events such as an asteroid colliding with Earth. But cosmic events are unlikely on human timescales compared to human-made existential risks from nuclear weapons, synthetic biology, and nanotechnology. It is possible that artificial intelligence also falls into this category. Vinge (1993) was the first person to recognize this:", "startOffset": 199, "endOffset": 2106}, {"referenceID": 16, "context": "Bostrom (2003) picked up on this issue very early and gave the topic credibility through his well-researched and carefully written book Superintelligence (Bostrom, 2014).", "startOffset": 154, "endOffset": 169}, {"referenceID": 57, "context": "In 2014 high-profile scientists such as Stephen Hawking, Max Tegmark, Stuart Russell, and Frank Wilczek have warned against the dangers posed by AI (Hawking et al., 2014).", "startOffset": 148, "endOffset": 170}, {"referenceID": 186, "context": "Moreover, the Future of Life Institute and the Machine Intelligence Research Institute have formulated concrete technical research priorities to make AI more robust and beneficial (Soares and Fallenstein, 2014; Russell et al., 2015).", "startOffset": 180, "endOffset": 232}, {"referenceID": 168, "context": "Moreover, the Future of Life Institute and the Machine Intelligence Research Institute have formulated concrete technical research priorities to make AI more robust and beneficial (Soares and Fallenstein, 2014; Russell et al., 2015).", "startOffset": 180, "endOffset": 232}, {"referenceID": 13, "context": "Bostrom (2003) picked up on this issue very early and gave the topic credibility through his well-researched and carefully written book Superintelligence (Bostrom, 2014).", "startOffset": 0, "endOffset": 15}, {"referenceID": 1, "context": "(See also Alexander (2015) for a collection of positions by prominent AI researchers.", "startOffset": 10, "endOffset": 27}, {"referenceID": 14, "context": "But, despite the name, OpenAI is not committed to publish all of their research freely, and Bostrom (2016) argues that unrestricted publication might not be the best idea.", "startOffset": 92, "endOffset": 107}, {"referenceID": 14, "context": "But, despite the name, OpenAI is not committed to publish all of their research freely, and Bostrom (2016) argues that unrestricted publication might not be the best idea. Despite all of the recent efforts in AI safety research, critical voices within the AI community remain. Prominently, Davis (2014), Ng (2016), Walsh (2016), and Lawrence (2016) have proposed counterarguments that range from \u2018HLAI is so far away that any worry is misplaced\u2019 to claims that \u2018the safety problem would not be so hard\u2019.", "startOffset": 92, "endOffset": 303}, {"referenceID": 14, "context": "But, despite the name, OpenAI is not committed to publish all of their research freely, and Bostrom (2016) argues that unrestricted publication might not be the best idea. Despite all of the recent efforts in AI safety research, critical voices within the AI community remain. Prominently, Davis (2014), Ng (2016), Walsh (2016), and Lawrence (2016) have proposed counterarguments that range from \u2018HLAI is so far away that any worry is misplaced\u2019 to claims that \u2018the safety problem would not be so hard\u2019.", "startOffset": 92, "endOffset": 314}, {"referenceID": 14, "context": "But, despite the name, OpenAI is not committed to publish all of their research freely, and Bostrom (2016) argues that unrestricted publication might not be the best idea. Despite all of the recent efforts in AI safety research, critical voices within the AI community remain. Prominently, Davis (2014), Ng (2016), Walsh (2016), and Lawrence (2016) have proposed counterarguments that range from \u2018HLAI is so far away that any worry is misplaced\u2019 to claims that \u2018the safety problem would not be so hard\u2019.", "startOffset": 92, "endOffset": 328}, {"referenceID": 14, "context": "But, despite the name, OpenAI is not committed to publish all of their research freely, and Bostrom (2016) argues that unrestricted publication might not be the best idea. Despite all of the recent efforts in AI safety research, critical voices within the AI community remain. Prominently, Davis (2014), Ng (2016), Walsh (2016), and Lawrence (2016) have proposed counterarguments that range from \u2018HLAI is so far away that any worry is misplaced\u2019 to claims that \u2018the safety problem would not be so hard\u2019.", "startOffset": 92, "endOffset": 349}, {"referenceID": 14, "context": "But, despite the name, OpenAI is not committed to publish all of their research freely, and Bostrom (2016) argues that unrestricted publication might not be the best idea. Despite all of the recent efforts in AI safety research, critical voices within the AI community remain. Prominently, Davis (2014), Ng (2016), Walsh (2016), and Lawrence (2016) have proposed counterarguments that range from \u2018HLAI is so far away that any worry is misplaced\u2019 to claims that \u2018the safety problem would not be so hard\u2019. See Sotala and Yampolskiy (2014) for a discussion.", "startOffset": 92, "endOffset": 537}, {"referenceID": 31, "context": "In particular, value learning (Dewey, 2011), self-reflection (Soares, 2015; Fallenstein et al.", "startOffset": 30, "endOffset": 43}, {"referenceID": 185, "context": "In particular, value learning (Dewey, 2011), self-reflection (Soares, 2015; Fallenstein et al., 2015b), self-modification (Orseau and Ring, 2011, 2012a; Everitt et al.", "startOffset": 61, "endOffset": 102}, {"referenceID": 38, "context": ", 2015b), self-modification (Orseau and Ring, 2011, 2012a; Everitt et al., 2016), interruptibility (Orseau and Armstrong, 2016; Armstrong and Orseau, 2016), decision theory (Everitt et al.", "startOffset": 28, "endOffset": 80}, {"referenceID": 155, "context": ", 2016), interruptibility (Orseau and Armstrong, 2016; Armstrong and Orseau, 2016), decision theory (Everitt et al.", "startOffset": 26, "endOffset": 82}, {"referenceID": 2, "context": ", 2016), interruptibility (Orseau and Armstrong, 2016; Armstrong and Orseau, 2016), decision theory (Everitt et al.", "startOffset": 26, "endOffset": 82}, {"referenceID": 37, "context": ", 2016), interruptibility (Orseau and Armstrong, 2016; Armstrong and Orseau, 2016), decision theory (Everitt et al., 2015), memory manipulation (Orseau and Ring, 2012b), wireheading (Ring and Orseau, 2011; Everitt and Hutter, 2016), and questions of identity (Orseau, 2014b,c).", "startOffset": 100, "endOffset": 122}, {"referenceID": 166, "context": ", 2015), memory manipulation (Orseau and Ring, 2012b), wireheading (Ring and Orseau, 2011; Everitt and Hutter, 2016), and questions of identity (Orseau, 2014b,c).", "startOffset": 67, "endOffset": 116}, {"referenceID": 36, "context": ", 2015), memory manipulation (Orseau and Ring, 2012b), wireheading (Ring and Orseau, 2011; Everitt and Hutter, 2016), and questions of identity (Orseau, 2014b,c).", "startOffset": 67, "endOffset": 116}, {"referenceID": 57, "context": "We built on top of Hutter\u2019s theory of universal artificial intelligence. Chapter 3 discussed the formal theory of learning. Chapter 4 presented several approaches to acting in unknown environments (Bayes, Thompson sampling, knowledge-seeking agents, and BayesExp). Chapter 5 analysed these approaches and discussed notions of optimality and principled problems with acting under uncertainty in general environment. Chapter 6 provided the mathematical tools to analyze the computational properties of these models. Finally, Chapter 7 solved the grain of truth problem, which lead to convergence to Nash equilibria in unknown general multi-agent environments. Our work is theoretical by nature and there is still a long way to go until these results make their way into applications. But a solution in principle is a crucial first step towards solving a problem in practice. Consider the research paper by Shannon (1950) on how to solve chess in principle.", "startOffset": 19, "endOffset": 919}], "year": 2016, "abstractText": "Reinforcement learning problems are often phrased in terms of Markov decision processes (MDPs). In this thesis we go beyond MDPs and consider reinforcement learning in environments that are non-Markovian, non-ergodic and only partially observable. Our focus is not on practical algorithms, but rather on the fundamental underlying problems: How do we balance exploration and exploitation? How do we explore optimally? When is an agent optimal? We follow the nonparametric realizable paradigm: we assume the data is drawn from an unknown source that belongs to a known countable class of candidates. First, we consider the passive (sequence prediction) setting, learning from data that is not independent and identically distributed. We collect results from artificial intelligence, algorithmic information theory, and game theory and put them in a reinforcement learning context: they demonstrate how an agent can learn the value of its own policy. Next, we establish negative results on Bayesian reinforcement learning agents, in particular AIXI. We show that unlucky or adversarial choices of the prior cause the agent to misbehave drastically. Therefore Legg-Hutter intelligence and balanced Pareto optimality, which depend crucially on the choice of the prior, are entirely subjective. Moreover, in the class of all computable environments every policy is Pareto optimal. This undermines all existing optimality properties for AIXI. However, there are Bayesian approaches to general reinforcement learning that satisfy objective optimality guarantees: We prove that Thompson sampling is asymptotically optimal in stochastic environments in the sense that its value converges to the value of the optimal policy. We connect asymptotic optimality to regret given a recoverability assumption on the environment that allows the agent to recover from mistakes. Hence Thompson sampling achieves sublinear regret in these environments. AIXI is known to be incomputable. We quantify this using the arithmetical hierarchy, and establish upper and corresponding lower bounds for incomputability. Further, we show that AIXI is not limit computable, thus cannot be approximated using finite computation. However there are limit computable \u03b5-optimal approximations to AIXI. We also derive computability bounds for knowledge-seeking agents, and give a limit computable weakly asymptotically optimal reinforcement learning agent. Finally, our results culminate in a formal solution to the grain of truth problem: A Bayesian agent acting in a multi-agent environment learns to predict the other agents\u2019 policies if its prior assigns positive probability to them (the prior contains a grain of truth). We construct a large but limit computable class containing a grain of truth and show that agents based on Thompson sampling over this class converge to play \u03b5-Nash equilibria in arbitrary unknown computable multi-agent environments.", "creator": "LaTeX with hyperref package"}}}