{"id": "1611.08998", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Nov-2016", "title": "DeepSetNet: Predicting Sets with Deep Neural Networks", "abstract": "This paper addresses the task of set prediction using deep learning. This is important because the output of many computer vision tasks, including image tagging and object detection, are naturally expressed as sets of entities rather than vectors. As opposed to a vector, the size of a set is not fixed in advance, and it is invariant to the ordering of entities within it. We define a likelihood for a set distribution and learn its parameters using a deep neural network. We also derive a loss for predicting a discrete distribution corresponding to set cardinality. Set prediction is demonstrated on the problems of multi-class image classification and pedestrian detection. Our approach yields state-of-the-art results in both cases on standard datasets.", "histories": [["v1", "Mon, 28 Nov 2016 06:42:56 GMT  (12333kb,D)", "http://arxiv.org/abs/1611.08998v1", null], ["v2", "Fri, 2 Dec 2016 06:18:14 GMT  (9136kb,D)", "http://arxiv.org/abs/1611.08998v2", null], ["v3", "Mon, 12 Dec 2016 01:10:13 GMT  (9136kb,D)", "http://arxiv.org/abs/1611.08998v3", null], ["v4", "Fri, 31 Mar 2017 06:45:52 GMT  (11218kb,D)", "http://arxiv.org/abs/1611.08998v4", null], ["v5", "Fri, 11 Aug 2017 02:52:36 GMT  (11229kb,D)", "http://arxiv.org/abs/1611.08998v5", "Accepted in IEEE International Conference on Computer Vision (ICCV), Venice, 2017, (Spotlight)"]], "reviews": [], "SUBJECTS": "cs.CV cs.AI cs.LG", "authors": ["s hamid rezatofighi", "vijay kumar b g", "anton milan", "ehsan abbasnejad", "anthony dick", "ian reid"], "accepted": false, "id": "1611.08998"}, "pdf": {"name": "1611.08998.pdf", "metadata": {"source": "CRF", "title": "DeepSetNet: Predicting Sets with Deep Neural Networks", "authors": ["Seyed Hamid Rezatofighi", "Vijay Kumar", "B G Anton", "Milan Ehsan Abbasnejad", "Anthony Dick", "Ian Reid"], "emails": ["hamid.rezatofighi@adelaide.edu.au"], "sections": [{"heading": "1. Introduction", "text": "In fact, most of them will be able to play by the rules they have established in the past."}, {"heading": "2. Related Work", "text": "A sudden success in several applications, including speech recognition [13], machine translation [32], and multi-person image classification [17], has triggered the use of deep learning methods in many areas of research. Deep convolutional (CNN) and recurring (RNN) neural networks surpass traditional approaches in areas such as semantic segmentation [3], image caption [16], or object recognition [20]. We will briefly review some of the most recent approaches to image classification and object recognition and show their limitations. However, image or scene classification is a basic task of understanding photographs, with the goal of predicting a scene label for a particular image. Early datasets, such as Caltech-101 [7], contained a single object and could easily be described by a category."}, {"heading": "3. Random Vectors vs. Random Finite Sets", "text": "In statistics, a continuous random variable y is a variable that takes up an infinite number of possible values. A continuous random vector can be defined by stacking several continuous random variables into a vector, Y = (y1, \u00b7 \u00b7, ym).The mathematical function that describes the possible values of a continuous random vector and the associated random probabilities is so known as the probability density function (PDF) p (Y) that the number of constitutive variables of random finite set (RFS) Y is a finite set of estimated random variables Y = {y1, \u00b7, ym}.The main difference between an RFS and a random vector is that the number of constituent variables are more random and the variables themselves random and disordered magnitude. A statistical function that describes a limited variable Y is a compatible probability function that we call the probability unit."}, {"heading": "4. Deep Set Network", "text": "Let's start with the definition of a training set D = {Yi, xi}, in which each training sample i = 1,..., n is a pair consisting of an input character xi-Rl and an output (or label) sentence Yi = {y1, y2,..., ym}, yk-Rd. In the following, we drop the instance index i for better readability. Note that m: = | Y | denotes the cardinality of the set Y. The probability of a set Y is defined as: p (Y, w, x) = p (m | w, x) \u00b7 m! \u00b7 p (y1, y2, \u00b7, ym | \u03b8, x), (3), in which the parameters of yk estimate the distribution of the set elements for a fixed cardinality, while w represents the collection of parameters that estimate the cardinality distribution of the set elements."}, {"heading": "4.1. Posterior distribution", "text": "To learn the parameters \u03b8 and w, we first define the posterior distribution over them asp (\u03b8, w | D), p (\u03b8, w), p (\u03b8), p (w), p (xi) and p (p). (5) 1This is also known as the spatial distribution of points in point processing statistics. A closed form solution for the integral in equation. (20) can be obtained by using conjugate priors: m \u00b2 P (m; \u03bb), p (x, w), \u03b2 (x, w)), \u03b1 (x, w), \u03b2 (x, w) > 0 \u00b2 x, w \u00b2 N (\u03b8; 0, conceptual 21I), ammerial N (w)."}, {"heading": "4.2. Learning", "text": "For the sake of simplicity, we use a point estimate for the rear p (\u03b8, w | D) \u03b2 (\u03b2 \u03b2) q categories, i.e. p (\u03b8, w | D) = \u03b4 (\u03b8 = \u03b8, w = w \u043a | D), whereby the optimization problem in Eq. (24) can be calculated with the following MAP estimator. \u00b7 The parameters \u03b8, w *) = arg max \u03b8, w log (p *, w | D) Note: The optimization problem in Eq. (24) can be \u00b7 decomposed. Therefore, we can learn them independently."}, {"heading": "4.3. Inference", "text": "After using the learned parameters of the network (w *, \u03b8 *) for a test attribute x *, we use an MAP estimate to generate a specified output asY * = arg max Y p (Y * | D, x *), (11) where p (Y * | D, x *) = p (Y * | \u03b8, w | D) p (\u03b8, w | D) d\u03b8dwand p (\u03b8, w | D) = \u043c (\u03b8, w | w *), (12) where p (m | w *, x *) = NB (Y * | D, x *) must be calculated, we must first calculate the mode m * of the cardinality distribution m * = arg max m (m | w *, x *), (12) where p (m | w *, x *, x *) = NB (m; \u03b1 *, x *), 11 + \u03b2 (w *, x *). Then we calculate the community distribution m (\u00b7 \u00b2), the \u00b7 m \u00b2, that \u00b7, the \u00b7 m \u00b2, the predistributions m \u00b2, the m \u00b2, the m \u00b2, the m \u00b2, the m \u00b2, the m \u00b2, the m \u00b2, the m \u00b2 of the m, the m, the m \u00b2 of the m, the m *)."}, {"heading": "5. Experimental Results", "text": "In order to evaluate the performance of our proposed deep-set network, we are conducting experiments on two different and relevant applications: classification of multi-label images and pedestrian detection."}, {"heading": "5.1. Multi-label Image Classification", "text": "In this section, we will evaluate our approach to the task of multi-label image classification. In contrast to the more common and more studied problems of (single-label) image classification, the task here will be more likely to be to label a photo with an arbitrary, a-priori unknown number of tags. In this experiment, we will conduct experiments on two standard benchmarks, the PASCAL VOC 2007 dataset [6] and the Microsoft Common Objects in Context (MS COCO) dataset [19]. In this experiment, we will build on the 16 layers of the VGG network [30], pre-trained on the ImageNet datasets of 2012. We will adapt VGG for our purpose by modifying the last fully connected prediction layer to predict 20 classes for PASCAL VOC, and 80 classes for MS COCO. We will then match the entire network using two commonly used losses for multi-label classification, 38, and cross-ropy classification (entropy) dynamics."}, {"heading": "5.2. Pedestrian Detection", "text": "To demonstrate the universality of our approach, we are testing it on a completely different setting of the pedestrian architecture. We are conducting experiments on two widely used datasets, Caltech Pedestrians [5] and MOT16, the 2016 edition of the MOTChallenge benchmark [22]. To increase the status-of-the-art value of pedestrian detection, we chose the leading detector in each frame. To learn the cardinality distribution, we use a multi-dimensional, deep image as network input, constructed by overlaying all regional suggestions and their results generated by MS-CNN detector (before not maximum suppression). We found that this input provides a stronger signal than the raw RGB images, delivering better results. We are building on the top of the well-known Alexa network architecture that we are generating from MS-CNN (before not maximum suppression approach). We found that this input provides a stronger signal than the raw RGB images that deliver better results."}, {"heading": "6. Conclusion", "text": "To achieve this goal, we derived a loss for predicting discrete distribution over the specified cardinality, which enabled us to use standard back propagation to train a deep network to predict specified values. We demonstrated the effectiveness of this approach in multi-class classification and pedestrian detection, and achieved state-of-the-art results in both applications. As our network is independently trained, it can be trivially applied to any existing classifier or detector to further improve performance. In the future, we plan to extend our model to multi-class cardinality estimation and extend its application to general object detectors. Another potential avenue could be to exploit the Bajesian nature of the model to incorporate uncertainties, rather than relying solely on the MAP estimation. Finally, we have considered only specified results. We believe that a promising direction for follow-up work is to apply our idea to specified input factors, such as grading problems."}, {"heading": "A. Background on Finite Set Statistics", "text": "In this section, we review some basic mathematical background information on this topic of statistics. In conventional statistics theory, a continuous random variable y is a variable that can assume an infinite number of possible values. A continuous random vector can be defined by stacking several continuous random variables into a fixed length vector, Y = (Y1, \u00b7, ym). The mathematical function that describes the possible values of a continuous random vector and the common probabilities associated with it is known as a probability function (PDF) p (Y), which is defined so that a random finite set (RFS) Y is a finite set that describes a finite set of estimated random variables Y = {y1, \u00b7 \u00b7, ym}. The main difference between an RFS and a random vector is that the fixed number of conventional variables is itself defined."}, {"heading": "B. Deep Set Network", "text": "Let's start with the definition of a training set D = \"Yi,\" whereby each training sample i = \"Y\" (\"Y\") (\"Y\") (\"Y\") (\"W\") (\"W\") (\"W\") (\"W\") (\"W\") (\"W\") (\"W\") (\"W\") (\"W\") (\"W\") (\"W\") (\"W\") (\"W\") (\"W\") (\"W\") (\"W\") (\"W\" W \") (\" W \"W\") (\"W\" W \"W\") (\"W\" W \"W\") (W \"W\" W \"W\") (W \"W\" W \") (W\" W \"W\") (W \"W\") (W \"W\" (W \") (W\" W \") (W\" W W (W \"W) (W\" W \") (W\" W \"W (W\") (W \"W\") (W \"W\") (W \"W\" W \"(W\" W \") (W\" W \") (W\" W \"(W\" W \"W\") \"(W\" W \"(W\" W \"W\") \"(W\" W \"(W\" W \"W\") \"(W\" W \"(W\" W \"W\" W \")\" (W \"W\" (W \"W\" W \"W\" (W \"W\") (W \"W\" W \"(W\" W \"W\" W \"W\") \"(W\" W \"W\" (W \"W\" W \"W\" W \") (W\" W \"W\" (W \"W\" W \"W\" W \"W\") (W \"W\" W \"W\" (W \"W\" W \"W\") (W \"W\" W \"W\" W \"W\") (W \"W\" W \"(W\" W \"W\") (W \"W\" W \"W\" W \"(W\" W \"W\" W \"W\") (W \"W\" W \"W\" W \"W\") (W \"W\" W \"W\" W \"W\") (W \"W\" W \"W\" W \"W\" W \""}, {"heading": "C. Further Experimental Results", "text": "Here we show additional evaluation diagrams and qualitative results, which could not be included in the main paper. Multi-stage image classification. Figure 8 shows further results for a successful image marking. Figure 9 indicates some interesting errors and incorrect predictions. Pedestrian detection. ROC curves on two detection data sets are shown in Figure 7. Qualitative results of pedestrian detection are shown in Figure 9."}], "references": [{"title": "Pedestrian detection at 100 frames per second", "author": ["R. Benenson", "M. Mathias", "R. Timofte", "L.V. Gool"], "venue": "CVPR", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2012}, {"title": "A unified multi-scale deep convolutional neural network for fast object detection", "author": ["Z. Cai", "Q. Fan", "R. Feris", "N. Vasconcelos"], "venue": "ECCV", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2016}, {"title": "Semantic image segmentation with deep convolutional nets and fully connected CRFs", "author": ["L.-C. Chen", "G. Papandreou", "I. Kokkinos", "K. Murphy", "A.L. Yuille"], "venue": "ICLR,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2015}, {"title": "Histograms of oriented gradients for human detection", "author": ["N. Dalal", "B. Triggs"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2005}, {"title": "Pedestrian detection: An evaluation of the state of the art", "author": ["P. Doll\u00e1r", "C. Wojek", "B. Schiele", "P. Perona"], "venue": "IEEE T. Pattern Anal. Mach. Intell., 34,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2012}, {"title": "The PASCAL Visual Object Classes Challenge", "author": ["M. Everingham", "L. Van Gool", "C.K.I. Williams", "J. Winn", "A. Zisserman"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2007}, {"title": "One-shot learning of object categories", "author": ["L. Fei-Fei", "R. Fergus", "P. Perona"], "venue": "IEEE T. Pattern Anal. Mach. Intell., 28(4):594\u2013611, Apr.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2006}, {"title": "Object detection with discriminatively trained part based models", "author": ["P.F. Felzenszwalb", "R.B. Girshick", "D. McAllester", "D. Ramanan"], "venue": "IEEE T. Pattern Anal. Mach. Intell., 32(9):1627\u20131645,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2010}, {"title": "Are we ready for autonomous driving? The KITTI Vision Benchmark Suite", "author": ["A. Geiger", "P. Lenz", "R. Urtasun"], "venue": "CVPR", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2012}, {"title": "Fast R-CNN", "author": ["R. Girshick"], "venue": "ICCV", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep convolutional ranking for multilabel image annotation", "author": ["Y. Gong", "Y. Jia", "T. Leung", "A. Toshev", "S. Ioffe"], "venue": "arXiv preprint arXiv:1312.4894,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2013}, {"title": "Deep convolutional ranking for multilabel image annotation", "author": ["Y. Gong", "Y. Jia", "T. Leung", "A. Toshev", "S. Ioffe"], "venue": "CoRR, abs/1312.4894,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2013}, {"title": "Speech recognition with deep recurrent neural networks", "author": ["A. Graves", "A.-r. Mohamed", "G.E. Hinton"], "venue": "In IEEE International Conference on Acoustics, Speech and Signal Processing,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2013}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural Comput., 9(8):17351780, Nov.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1997}, {"title": "A convnet for nonmaximum suppression", "author": ["J. Hosang", "R. Benenson", "B. Schiele"], "venue": "B. Rosenhahn and B. Andres, editors, gcpr-2016, volume 9796 of Lecture Notes in Computer Science, pages 192\u2013204, Hannover, Germany,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2016}, {"title": "Densecap: Fully convolutional localization networks for dense captioning", "author": ["J. Johnson", "A. Karpathy", "L. Fei-Fei"], "venue": "CVPR", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2016}, {"title": "ImageNet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "In NIPS*2012,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2012}, {"title": "Individualness and determinantal point processes for pedestrian detection", "author": ["D. Lee", "G. Cha", "M.-H. Yang", "S. Oh"], "venue": "ECCV", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2016}, {"title": "Microsoft COCO: Common objects in context", "author": ["T.-Y. Lin", "M. Maire", "S. Belongie", "L. Bourdev", "R. Girshick", "J. Hays", "P. Perona", "D. Ramanan", "C.L. Zitnick", "P. Doll\u00e1r"], "venue": "arXiv:1405.0312 [cs], May", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2014}, {"title": "SSD: Single shot multibox detector", "author": ["W. Liu", "D. Anguelov", "D. Erhan", "C. Szegedy", "S. Reed", "C.- Y. Fu", "A.C. Berg"], "venue": "In ECCV 2016, Lecture Notes in Computer Science,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2016}, {"title": "Statistical multisource-multitarget information fusion, volume 685", "author": ["R.P. Mahler"], "venue": "Artech House Boston,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2007}, {"title": "Mot16: A benchmark for multi-object tracking", "author": ["A. Milan", "L. Leal-Taix\u00e9", "I. Reid", "S. Roth", "K. Schindler"], "venue": "arXiv:1603.00831 [cs], Mar.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2016}, {"title": "Deep decision network for multi-class image classification", "author": ["V.N. Murthy", "V. Singh", "T. Chen", "R. Manmatha", "D. Comaniciu"], "venue": "CVPR 2016, June", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2016}, {"title": "Learning multi-domain convolutional neural networks for visual tracking", "author": ["H. Nam", "B. Han"], "venue": "CVPR", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2016}, {"title": "Weakly- and semi-supervised learning of a deep convolutional network for semantic image segmentation", "author": ["G. Papandreou", "L.-C. Chen", "K.P. Murphy", "A.L. Yuille"], "venue": "ICCV 2015, Dec.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2015}, {"title": "Efficient point process inference for large-scale object detection", "author": ["T.T. Pham", "S. Hamid Rezatofighi", "I. Reid", "T.-J. Chin"], "venue": "CVPR", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2016}, {"title": "Faster r-cnn: Towards real-time object detection with region proposal networks", "author": ["S. Ren", "K. He", "R. Girshick", "J. Sun"], "venue": "In NIPS*2015", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2015}, {"title": "Imagenet large scale visual recognition challenge", "author": ["O. Russakovsky", "J. Deng", "H. Su", "J. Krause", "S. Satheesh", "S. Ma", "Z. Huang", "A. Karpathy", "A. Khosla", "M. Bernstein", "A.C. Berg", "L. Fei-Fei"], "venue": "Int. J. Comput. Vision, 115(3):211\u2013252,", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2015}, {"title": "OverFeat: Integrated recognition, localization and detection using convolutional networks", "author": ["P. Sermanet", "D. Eigen", "X. Zhang", "M. Mathieu", "R. Fergus", "Y. LeCun"], "venue": "CoRR, abs/1312.6229,", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2013}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "CoRR, abs/1409.1556,", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2014}, {"title": "End-to-end people detection in crowded scenes", "author": ["R. Stewart", "M. Andriluka", "A.Y. Ng"], "venue": "CVPR", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2016}, {"title": "Sequence to sequence learning with neural networks", "author": ["I. Sutskever", "O. Vinyals", "Q.V. Le"], "venue": "In NIPS*2014,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2014}, {"title": "Going deeper with convolutions", "author": ["C. Szegedy", "W. Liu", "Y. Jia", "P. Sermanet", "S. Reed", "D. Anguelov", "D. Erhan", "V. Vanhoucke", "A. Rabinovich"], "venue": "CoRR, abs/1409.4842,", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2014}, {"title": "Order matters: Sequence to sequence for sets", "author": ["O. Vinyals", "S. Bengio", "M. Kudlur"], "venue": "arXiv:1511.06391 [cs, stat], Nov.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2015}, {"title": "Robust real-time face detection", "author": ["P. Viola", "M.J. Jones"], "venue": "Int. J. Comput. Vision, 57(2):137\u2013154, May", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2004}, {"title": "New features and insights for pedestrian detection", "author": ["S. Walk", "N. Majer", "K. Schindler", "B. Schiele"], "venue": "CVPR", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2010}, {"title": "CNN-RNN: A unified framework for multi-label image classification", "author": ["J. Wang", "Y. Yang", "J. Mao", "Z. Huang", "C. Huang", "W. Xu"], "venue": "CVPR, June", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2016}, {"title": "CNN: Single-label to multi-label", "author": ["Y. Wei", "W. Xia", "J. Huang", "B. Ni", "J. Dong", "Y. Zhao", "S. Yan"], "venue": "CoRR, abs/1406.5726,", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2014}, {"title": "Visualizing and understanding convolutional networks", "author": ["M.D. Zeiler", "R. Fergus"], "venue": "ECCV", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2014}], "referenceMentions": [{"referenceID": 24, "context": "Deep neural networks have state-of-the-art performance for many computer vision problems, including semantic segmentation [25], visual tracking [24], image captioning [16], scene classification [17], and object detection [20].", "startOffset": 122, "endOffset": 126}, {"referenceID": 23, "context": "Deep neural networks have state-of-the-art performance for many computer vision problems, including semantic segmentation [25], visual tracking [24], image captioning [16], scene classification [17], and object detection [20].", "startOffset": 144, "endOffset": 148}, {"referenceID": 15, "context": "Deep neural networks have state-of-the-art performance for many computer vision problems, including semantic segmentation [25], visual tracking [24], image captioning [16], scene classification [17], and object detection [20].", "startOffset": 167, "endOffset": 171}, {"referenceID": 16, "context": "Deep neural networks have state-of-the-art performance for many computer vision problems, including semantic segmentation [25], visual tracking [24], image captioning [16], scene classification [17], and object detection [20].", "startOffset": 194, "endOffset": 198}, {"referenceID": 19, "context": "Deep neural networks have state-of-the-art performance for many computer vision problems, including semantic segmentation [25], visual tracking [24], image captioning [16], scene classification [17], and object detection [20].", "startOffset": 221, "endOffset": 225}, {"referenceID": 16, "context": "Modern approaches typically address this by a series of convolutional layers, followed by a number of fully connected layers, which are finally mapped to predict a fixedsized vector [17, 30, 33].", "startOffset": 182, "endOffset": 194}, {"referenceID": 29, "context": "Modern approaches typically address this by a series of convolutional layers, followed by a number of fully connected layers, which are finally mapped to predict a fixedsized vector [17, 30, 33].", "startOffset": 182, "endOffset": 194}, {"referenceID": 32, "context": "Modern approaches typically address this by a series of convolutional layers, followed by a number of fully connected layers, which are finally mapped to predict a fixedsized vector [17, 30, 33].", "startOffset": 182, "endOffset": 194}, {"referenceID": 27, "context": "1,000 for the ImageNet challenge [28].", "startOffset": 33, "endOffset": 37}, {"referenceID": 1, "context": "), and what is (a) Proposals (b) MS-CNN [2] (c) Our result", "startOffset": 40, "endOffset": 43}, {"referenceID": 10, "context": "Another strategy to account for multiple classes is to fix the number to a certain value for all test instances, and report precision and recall by counting false positive and false negative predictions, as was done in [11, 38].", "startOffset": 219, "endOffset": 227}, {"referenceID": 36, "context": "Another strategy to account for multiple classes is to fix the number to a certain value for all test instances, and report precision and recall by counting false positive and false negative predictions, as was done in [11, 38].", "startOffset": 219, "endOffset": 227}, {"referenceID": 3, "context": "The most common approach is to assign a confidence score to a number of region candidates [4, 8, 10, 27], which are typically selected heuristically by thresholding and non-maxima suppression.", "startOffset": 90, "endOffset": 104}, {"referenceID": 7, "context": "The most common approach is to assign a confidence score to a number of region candidates [4, 8, 10, 27], which are typically selected heuristically by thresholding and non-maxima suppression.", "startOffset": 90, "endOffset": 104}, {"referenceID": 9, "context": "The most common approach is to assign a confidence score to a number of region candidates [4, 8, 10, 27], which are typically selected heuristically by thresholding and non-maxima suppression.", "startOffset": 90, "endOffset": 104}, {"referenceID": 26, "context": "The most common approach is to assign a confidence score to a number of region candidates [4, 8, 10, 27], which are typically selected heuristically by thresholding and non-maxima suppression.", "startOffset": 90, "endOffset": 104}, {"referenceID": 12, "context": "A sudden success in multiple applications including voice recognition [13], machine translation [32] and image classification [17], has sparked the deployment of deep learning methods throughout numerous research areas.", "startOffset": 70, "endOffset": 74}, {"referenceID": 31, "context": "A sudden success in multiple applications including voice recognition [13], machine translation [32] and image classification [17], has sparked the deployment of deep learning methods throughout numerous research areas.", "startOffset": 96, "endOffset": 100}, {"referenceID": 16, "context": "A sudden success in multiple applications including voice recognition [13], machine translation [32] and image classification [17], has sparked the deployment of deep learning methods throughout numerous research areas.", "startOffset": 126, "endOffset": 130}, {"referenceID": 2, "context": "Deep convolutional (CNN) and recurrent (RNN) neural networks now outperform traditional approaches in tasks like semantic segmentation [3], image captioning [16] or object detection [20].", "startOffset": 135, "endOffset": 138}, {"referenceID": 15, "context": "Deep convolutional (CNN) and recurrent (RNN) neural networks now outperform traditional approaches in tasks like semantic segmentation [3], image captioning [16] or object detection [20].", "startOffset": 157, "endOffset": 161}, {"referenceID": 19, "context": "Deep convolutional (CNN) and recurrent (RNN) neural networks now outperform traditional approaches in tasks like semantic segmentation [3], image captioning [16] or object detection [20].", "startOffset": 182, "endOffset": 186}, {"referenceID": 6, "context": "Early datasets, such as Caltech-101 [7], mostly contained one single object and could easily be described by one category.", "startOffset": 36, "endOffset": 39}, {"referenceID": 16, "context": "Consequently, a large body of literature focused on single-class prediction [17, 29, 40, 23].", "startOffset": 76, "endOffset": 92}, {"referenceID": 28, "context": "Consequently, a large body of literature focused on single-class prediction [17, 29, 40, 23].", "startOffset": 76, "endOffset": 92}, {"referenceID": 38, "context": "Consequently, a large body of literature focused on single-class prediction [17, 29, 40, 23].", "startOffset": 76, "endOffset": 92}, {"referenceID": 22, "context": "Consequently, a large body of literature focused on single-class prediction [17, 29, 40, 23].", "startOffset": 76, "endOffset": 92}, {"referenceID": 11, "context": "[12] combine deep CNNs with a top-k approximate ranking loss to predict multiple labels.", "startOffset": 0, "endOffset": 4}, {"referenceID": 37, "context": "[39] propose a Hypotheses-Pooling architecture that is specifically designed to handle multi-label output.", "startOffset": 0, "endOffset": 4}, {"referenceID": 36, "context": "[38] proposed a model that combines CNNs and RNNs (convolutional and recurrent networks) to predict a number classes in a sequential manner.", "startOffset": 0, "endOffset": 4}, {"referenceID": 33, "context": "[34].", "startOffset": 0, "endOffset": 4}, {"referenceID": 36, "context": "it may contain the same element multiple times), such that post-processing or heuristics such as beam search must be employed [35, 38].", "startOffset": 126, "endOffset": 134}, {"referenceID": 34, "context": "Traditional approaches follow the slidingwindow paradigm [36, 4, 37, 8, 1], where each possible (or rather plausible) image region is scored independently to contain or not to contain a person.", "startOffset": 57, "endOffset": 74}, {"referenceID": 3, "context": "Traditional approaches follow the slidingwindow paradigm [36, 4, 37, 8, 1], where each possible (or rather plausible) image region is scored independently to contain or not to contain a person.", "startOffset": 57, "endOffset": 74}, {"referenceID": 35, "context": "Traditional approaches follow the slidingwindow paradigm [36, 4, 37, 8, 1], where each possible (or rather plausible) image region is scored independently to contain or not to contain a person.", "startOffset": 57, "endOffset": 74}, {"referenceID": 7, "context": "Traditional approaches follow the slidingwindow paradigm [36, 4, 37, 8, 1], where each possible (or rather plausible) image region is scored independently to contain or not to contain a person.", "startOffset": 57, "endOffset": 74}, {"referenceID": 0, "context": "Traditional approaches follow the slidingwindow paradigm [36, 4, 37, 8, 1], where each possible (or rather plausible) image region is scored independently to contain or not to contain a person.", "startOffset": 57, "endOffset": 74}, {"referenceID": 9, "context": "More recent methods, such as Fast R-CNN [10] or the single-shot multi-box detector (SSD) [20] learn the relevant image features rather than manually engineering them, but retain the sliding window approach.", "startOffset": 40, "endOffset": 44}, {"referenceID": 19, "context": "More recent methods, such as Fast R-CNN [10] or the single-shot multi-box detector (SSD) [20] learn the relevant image features rather than manually engineering them, but retain the sliding window approach.", "startOffset": 89, "endOffset": 93}, {"referenceID": 30, "context": "[31] perform end-to-end head detection by predicting the bounding boxes sequentially using an LSTM [14].", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[31] perform end-to-end head detection by predicting the bounding boxes sequentially using an LSTM [14].", "startOffset": 99, "endOffset": 103}, {"referenceID": 25, "context": "[26] and Lee et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "[18] formulate NMS as a global optimisation problem while Hosang et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[15] propose to learn the NMS algorithm end-to-end using CNNs.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "The factor m! = \u220fm k=1 k appears because the probability density for a set {y1, \u00b7 \u00b7 \u00b7 , ym} must be equally distributed among all the m! possible permutations of the vector [21].", "startOffset": 173, "endOffset": 177}, {"referenceID": 5, "context": "We perform experiments on two standard benchmarks, the PASCAL VOC 2007 dataset [6] and the Microsoft Common Objects in Context (MS COCO) dataset [19].", "startOffset": 79, "endOffset": 82}, {"referenceID": 18, "context": "We perform experiments on two standard benchmarks, the PASCAL VOC 2007 dataset [6] and the Microsoft Common Objects in Context (MS COCO) dataset [19].", "startOffset": 145, "endOffset": 149}, {"referenceID": 36, "context": "In this experiment, similar to [38], we build on the 16-layers VGG network [30], pretrained on the 2012 ImageNet dataset.", "startOffset": 31, "endOffset": 35}, {"referenceID": 29, "context": "In this experiment, similar to [38], we build on the 16-layers VGG network [30], pretrained on the 2012 ImageNet dataset.", "startOffset": 75, "endOffset": 79}, {"referenceID": 10, "context": "We then fine-tune the entire network for each of these datasets using two commonly used losses for multi-label classification, softmax and binary cross-entropy (BCE)2 [11, 38].", "startOffset": 167, "endOffset": 175}, {"referenceID": 36, "context": "We then fine-tune the entire network for each of these datasets using two commonly used losses for multi-label classification, softmax and binary cross-entropy (BCE)2 [11, 38].", "startOffset": 167, "endOffset": 175}, {"referenceID": 36, "context": "However, it does not perform as well as softmax and binary cross-entropy for the used datasets [38].", "startOffset": 95, "endOffset": 99}, {"referenceID": 10, "context": "To evaluate the performance of the classifiers and our deep set network, we employ the commonly used evaluation metrics for multi-label image classification [11, 38]: precision and recall of the generated labels per-class (C-P and C-R) and overall (O-P and O-R).", "startOffset": 157, "endOffset": 165}, {"referenceID": 36, "context": "To evaluate the performance of the classifiers and our deep set network, we employ the commonly used evaluation metrics for multi-label image classification [11, 38]: precision and recall of the generated labels per-class (C-P and C-R) and overall (O-P and O-R).", "startOffset": 157, "endOffset": 165}, {"referenceID": 10, "context": "To generate the predicted labels for a particular image , we perform a forward pass of the CNN and choose top-k labels according to their scores similar to [11, 38].", "startOffset": 156, "endOffset": 164}, {"referenceID": 36, "context": "To generate the predicted labels for a particular image , we perform a forward pass of the CNN and choose top-k labels according to their scores similar to [11, 38].", "startOffset": 156, "endOffset": 164}, {"referenceID": 5, "context": "The Pascal Visual Object Classes (VOC) [6] benchmark is one of the most widely used datasets for detection and classification.", "startOffset": 39, "endOffset": 42}, {"referenceID": 18, "context": "Another popular benchmark for image captioning, recognition, and segmentation is the recent Microsoft Common Objects in Context (MS-COCO) [19].", "startOffset": 138, "endOffset": 142}, {"referenceID": 36, "context": "We also outperform the state-of-the art multi-label classifier CNN-RNN [38], for the reported value of k = 3 in Table 1.", "startOffset": 71, "endOffset": 75}, {"referenceID": 4, "context": "We perform experiments on two widely used datasets, Caltech Pedestrians [5] and MOT16, the 2016 edition of the MOTChallenge benchmark [22].", "startOffset": 72, "endOffset": 75}, {"referenceID": 21, "context": "We perform experiments on two widely used datasets, Caltech Pedestrians [5] and MOT16, the 2016 edition of the MOTChallenge benchmark [22].", "startOffset": 134, "endOffset": 138}, {"referenceID": 1, "context": "To push the state-of-the art performance on pedestrian detection, we chose the leading detector on Caltech Pedestrians, which is the recent the multi-scale deep CNN approach (MS-CNN) [2].", "startOffset": 183, "endOffset": 186}, {"referenceID": 16, "context": "We build on top of the well-known AlexNet [17] architecture, and replace the first convolutional layer with a single channel filter and last fully connected layer with 2 layers output followed by two weighted sigmoid activation function, similar to the case above (cf .", "startOffset": 42, "endOffset": 46}, {"referenceID": 30, "context": "in [31].", "startOffset": 3, "endOffset": 7}, {"referenceID": 36, "context": "9 CNN-RNN [38] k=3 66.", "startOffset": 10, "endOffset": 14}, {"referenceID": 4, "context": "To quantify the detection performance, we adapt the same evaluation metrics and follow the protocols used on the Caltech detection benchmark [5].", "startOffset": 141, "endOffset": 144}, {"referenceID": 4, "context": "Caltech Pedestrians [5] is a de-facto standard benchmark for pedestrian detection.", "startOffset": 20, "endOffset": 23}, {"referenceID": 1, "context": "We use the MS-CNN [2] network model and its parameters learned on the Caltech training set as \u03b8\u2217 in Eq.", "startOffset": 18, "endOffset": 21}, {"referenceID": 4, "context": "[5]).", "startOffset": 0, "endOffset": 3}, {"referenceID": 21, "context": "Additionally, we evaluate the MSCNN detector and DeepSet in significantly more crowded and more challenging sequences from MOT16 dataset [22].", "startOffset": 137, "endOffset": 141}, {"referenceID": 8, "context": "We only learn the cardinality network w\u2217 on training set and we use the MS-CNN network model and its parameters learned on the KITTI dataset [9] as \u03b8\u2217 in Eq.", "startOffset": 141, "endOffset": 144}, {"referenceID": 1, "context": "MOT16 MS-CNN [2] 51.", "startOffset": 13, "endOffset": 16}], "year": 2016, "abstractText": "This paper addresses the task of set prediction using deep learning. This is important because the output of many computer vision tasks, including image tagging and object detection, are naturally expressed as sets of entities rather than vectors. As opposed to a vector, the size of a set is not fixed in advance, and it is invariant to the ordering of entities within it. We define a likelihood for a set distribution and learn its parameters using a deep neural network. We also derive a loss for predicting a discrete distribution corresponding to set cardinality. Set prediction is demonstrated on the problems of multi-class image classification and pedestrian detection. Our approach yields state-of-theart results in both cases on standard datasets.", "creator": "LaTeX with hyperref package"}}}