{"id": "1611.09268", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Nov-2016", "title": "MS MARCO: A Human Generated MAchine Reading COmprehension Dataset", "abstract": "This paper presents our recent work on the design and development of a new, large scale dataset, which we name MS MARCO, for MAchine Reading COmprehension.This new dataset is aimed to overcome a number of well-known weaknesses of previous publicly available datasets for the same task of reading comprehension and question answering. In MS MARCO, all questions are sampled from real anonymized user queries. The context passages, from which answers in the dataset are derived, are extracted from real web documents using the most advanced version of the Bing search engine. The answers to the queries are human generated. Finally, a subset of these queries has multiple answers. We aim to release one million queries and the corresponding answers in the dataset, which, to the best of our knowledge, is the most comprehensive real-world dataset of its kind in both quantity and quality. We are currently releasing 100,000 queries with their corresponding answers to inspire work in reading comprehension and question answering along with gathering feedback from the research community.", "histories": [["v1", "Mon, 28 Nov 2016 18:14:11 GMT  (1852kb,D)", "http://arxiv.org/abs/1611.09268v1", null], ["v2", "Tue, 29 Nov 2016 02:39:53 GMT  (1852kb,D)", "http://arxiv.org/abs/1611.09268v2", null]], "reviews": [], "SUBJECTS": "cs.CL cs.IR", "authors": ["tri nguyen", "mir rosenberg", "xia song", "jianfeng gao", "saurabh tiwary", "rangan majumder", "li deng"], "accepted": false, "id": "1611.09268"}, "pdf": {"name": "1611.09268.pdf", "metadata": {"source": "CRF", "title": "MS MARCO: A Human Generated MAchine Reading COmprehension Dataset", "authors": ["Tri Nguyen", "Mir Rosenberg", "Xia Song", "Jianfeng Gao", "Saurabh Tiwary", "Rangan Majumder", "Li Deng"], "emails": ["deng}@microsoft.com"], "sections": [{"heading": "1 Introduction", "text": "This year it is so far that it will only be a matter of time before an agreement is reached."}, {"heading": "2 Related Work", "text": "The ImageNet dataset [10] is one of the best known for advances in image classification and detection, inspiring new classes of deep learning algorithms [22] [13] [15]. Reading comprehension and answering open domain questions are among these domains that existing systems still cannot solve [31]. Since they can be generated automatically, they can be large enough to apply modern data-intensive models."}, {"heading": "3 The MS MARCO Dataset", "text": "To ensure true Machine Reading Comprehension (RC), we start with QA as the initial problem to be solved. Our introduction included some of the key benefits of making very large RC or QA datasets freely available, which only include real-world questions and human crowdsourcing answers compared to artificially generated data. Given these advantages, our goal is that MS MARCO [5] - a large-scale, real, and human-generated QA dataset - will become a key vehicle to enable researchers to deliver many more AI breakthroughs in the future, just like ImageNet [10], which enables understanding of images. In addition, building an RC-oriented dataset will help us understand a coherent but complex RC problem while we learn about all the infrastructure parts needed to achieve such a large query set of a million queries that will help advance the research community."}, {"heading": "3.1 Dataset Structure and Building Process", "text": "This year it is more than ever before."}, {"heading": "4 Experimental Results", "text": "In this section, we present our results through a series of experiments designed to present the characteristics of the MS-MARCO dataset. As we discussed in Section 3, human judgments are accumulated to allow the dataset to grow to the expected scale. Along the timeline, various snapshots of the dataset are taken and used in carefully designed experiments for validation and insight. As datasets evolve, the finalized experimental results may differ from the complete dataset, but we expect observations and conclusions to be reasonably representative. We group the queries in MS-MARCO datasets into different categories based on their answer types, as described in Section 3.1. The complexity of the answers varies greatly from category to category. For example, the answers to yes / no questions are simple binary. The answers to entity questions can be a single name or expression, like the answer \"Rome\" for the query \"What is the capital of Italy.\""}, {"heading": "4.1 Generative Model Experiments", "text": "Recurring Neural Networks (RNNs) are able to predict future elements based on previous sequences. They are often used as a generative language model for various NLP tasks, such as machine translation [7], response to queries [16], etc. In this QA experimental setup, we mainly aim to train and evaluate such generative models that predict human-generated responses to queries and / or context-dependent passages as model input. Table 5 shows the quality of the results of these models using the ROUGE-L metric. While the passages provided in MS MARCO generally contain useful information for given queries, the way in which the problem is generated makes it relatively difficult for simple generative models to achieve great results."}, {"heading": "4.2 Cloze-Style Model Experiments", "text": "In this test, a model attempts to predict missing symbols in a partially predetermined text sequence by reading context texts that contain potentially helpful information. CNN and Daily Mail Dataset is one of the most commonly used Cloze-style QA datasets. Recently, significant progress has been made in various model proposals in the participating Cloze-style test competition for these datasets. In this section, we present the performance of two machine-read understanding models that use both CNN test datasets and an MS-MARCO subset. Subset is filtered by numerical response type category for which a Cloze-style test is applicable.We show model accuracy numbers for both datasets in Table 7 and precision curves to remember MS-MARCO subset in Figure 1."}, {"heading": "5 Summary and Future Work", "text": "The MS-MARCO dataset described in this paper provides training data with question-and-answer pairs in which only a single answer text is provided via crowdsourcing, and this simplicity makes evaluation relatively easy. However, in the real world, multiple and equally valid answers to a single question are possible, similar to machine translation, where multiple types of translation are equally valid. Our immediate future work is to enrich the test set of the current dataset by providing multiple answers. We plan to add 1000 to 5000 such multiple answers to the dataset described in this paper. Subsequent evaluation experiments to compare single vs. multiple answers will be conducted to understand whether the model we have a better resolution with multiple answers. The evaluation metric may be the same METEOR as described in the previous experiments. While MS-MARCO will address a number of undesirable properties of the existing RC and QA datasets, in particular the need to overcome the requirement that aggregate answers be limited to a single set of questions."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1409.0473,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2014}, {"title": "Meteor: An automatic metric for mt evaluation with improved correlation with human judgments. In Proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine translation", "author": ["Satanjeev Banerjee", "Alon Lavie"], "venue": "and/or summarization,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2005}, {"title": "Context-dependent pre-trained deep neural networks for largevocabulary speech recognition", "author": ["G. Dahl", "D. Yu", "L. Deng", "A. Acero"], "venue": "IEEE Transactions on Audio, Speech, and Language Processing,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2012}, {"title": "Challenges in adopting speech recognition", "author": ["L. Deng", "XD Huang"], "venue": "Communications of the ACM,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2004}, {"title": "Deep Learning: Methods and Applications", "author": ["L. Deng", "D. Yu"], "venue": "NOW Publishers, New York,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2014}, {"title": "Rich feature hierarchies for accurate object detection and semantic segmentation", "author": ["Ross Girshick", "Jeff Donahue", "Trevor Darrell", "Jitendra Malik"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2014}, {"title": "Hybrid computing using a neural network with dynamic external memory. Nature, 2016", "author": ["Alex Graves", "Greg Wayne", "Malcolm Reynolds", "Tim Harley", "Ivo Danihelka", "Agnieszka Grabska-Barwi\u0144ska", "Sergio G\u00f3mez Colmenarejo", "Edward Grefenstette", "Tiago Ramalho", "John Agapiou", "Adri\u00e0 Puigdom\u00e8nech Badia", "Karl Moritz Hermann", "Yori Zwols", "Georg Ostrovski", "Adam Cain", "Helen King", "Christopher Summerfield", "Phil Blunsom", "Koray Kavukcuoglu", "Demis Hassabis"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2016}, {"title": "Deep residual learning for image recognition", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2015}, {"title": "Teaching machines to read and comprehend", "author": ["Karl Moritz Hermann", "Tom\u00e1\u0161 Kocisk\u00fd", "Edward Grefenstette", "Lasse Espeholt", "Will Kay", "Mustafa Suleyman", "Phil Blunsom"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2015}, {"title": "The goldilocks principle: Reading children\u2019s books with explicit memory representations", "author": ["Felix Hill", "Antoine Bordes", "Sumit Chopra", "Jason Weston"], "venue": null, "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2015}, {"title": "Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups", "author": ["G. Hinton", "L. Deng", "D. Yu", "G. Dalh", "A. Mohamed"], "venue": "IEEE Signal Processing Magazine,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2012}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1997}, {"title": "Learning deep structured semantic models for web search using clickthrough data", "author": ["Po-Sen Huang", "Xiaodong He", "Jianfeng Gao", "Li Deng", "Alex Acero", "Larry Heck"], "venue": "In Proceedings of the 22nd ACM international conference on Conference on information & knowledge management,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2013}, {"title": "Text understanding with the attention sum reader network", "author": ["Rudolf Kadlec", "Martin Schmid", "Ondrej Bajgar", "Jan Kleindienst"], "venue": "arXiv preprint arXiv:1603.01547,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2016}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E. Hinton"], "venue": null, "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2012}, {"title": "Rouge: A package for automatic evaluation of summaries. In Text summarization branches out", "author": ["Chin-Yew Lin"], "venue": "Proceedings of the ACL-04 workshop,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2004}, {"title": "Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting on association for computational linguistics, pages 311\u2013318", "author": ["Kishore Papineni", "Salim Roukos", "Todd Ward", "Wei-Jing Zhu"], "venue": "Association for Computational Linguistics,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2002}, {"title": "Squad: 100,000+ questions for machine comprehension", "author": ["Pranav Rajpurkar", "Jian Zhang", "Konstantin Lopyrev", "Percy Liang"], "venue": null, "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2016}, {"title": "Mctest: A challenge dataset for the open-domain machine comprehension of text", "author": ["Matthew Richardson", "Christopher J.C. Burges", "Erin Renshaw"], "venue": null, "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2013}, {"title": "Reasonet: Learning to stop reading in machine comprehension", "author": ["Yelong Shen", "Po-Sen Huang", "Jianfeng Gao", "Weizhu Chen"], "venue": "arXiv preprint arXiv:1609.05284,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2016}, {"title": "End-to-end memory networks", "author": ["Sainbayar Sukhbaatar", "Jason Weston", "Rob Fergus"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2015}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V. Le"], "venue": "CoRR, abs/1409.3215,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2014}, {"title": "Towards ai-complete question answering: A set of prerequisite toy", "author": ["Jason Weston", "Antoine Bordes", "Sumit Chopra", "Alexander M. Rush", "Bart van Merrienboer", "Armand Joulin", "Tomas Mikolov"], "venue": null, "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2015}, {"title": "Wikiqa: A challenge dataset for open-domain question answering", "author": ["Yi Yang", "Wen tau Yih", "Christopher Meek"], "venue": null, "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2015}], "referenceMentions": [{"referenceID": 10, "context": "Such agents can have tremendous value for consumers because they can power personal assistants such as Cortana [3], Siri [6], Alexa [1], or Google Assistant [4] found on phones or headless devices like Amazon Echo [2], all of which have been facilitated by recent advances in deep speech recognition technology [18, 9].", "startOffset": 311, "endOffset": 318}, {"referenceID": 2, "context": "Such agents can have tremendous value for consumers because they can power personal assistants such as Cortana [3], Siri [6], Alexa [1], or Google Assistant [4] found on phones or headless devices like Amazon Echo [2], all of which have been facilitated by recent advances in deep speech recognition technology [18, 9].", "startOffset": 311, "endOffset": 318}, {"referenceID": 7, "context": "5 million labeled examples and 1000 object categories which has led to better than human level performance on object classification from images [15].", "startOffset": 144, "endOffset": 148}, {"referenceID": 3, "context": "Another example is the very large speech databases collected over 20 years by DARPA that enabled successes of deep learning in speech recognition [11].", "startOffset": 146, "endOffset": 150}, {"referenceID": 18, "context": "For example, some are not large enough to train deep models [27], and others are larger but are synthetic.", "startOffset": 60, "endOffset": 64}, {"referenceID": 14, "context": "The ImageNet dataset [10] is one of the best known for enabling advances in image classification and detection and inspired new classes of deep learning algorithms [22] [13] [15].", "startOffset": 164, "endOffset": 168}, {"referenceID": 5, "context": "The ImageNet dataset [10] is one of the best known for enabling advances in image classification and detection and inspired new classes of deep learning algorithms [22] [13] [15].", "startOffset": 169, "endOffset": 173}, {"referenceID": 7, "context": "The ImageNet dataset [10] is one of the best known for enabling advances in image classification and detection and inspired new classes of deep learning algorithms [22] [13] [15].", "startOffset": 174, "endOffset": 178}, {"referenceID": 22, "context": "domain question answering is one of those domains existing systems still struggle to solve [31].", "startOffset": 91, "endOffset": 95}, {"referenceID": 8, "context": "created a corpus of cloze style questions from CNN / Daily News summaries [16] and Hill et al.", "startOffset": 74, "endOffset": 78}, {"referenceID": 9, "context": "has built the Children\u2019s Book Test [17].", "startOffset": 35, "endOffset": 39}, {"referenceID": 22, "context": "[31].", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "MCTest is a challenging dataset which contains 660 stories created by crowdworkers, 4 questions per story, and 4 answer choices per question [27], but real-world QA systems needs to go beyond multiple choice answers or selecting from known responses.", "startOffset": 141, "endOffset": 145}, {"referenceID": 23, "context": "WikiQA is another set which includes 3047 questions [32].", "startOffset": 52, "endOffset": 56}, {"referenceID": 17, "context": "A more recently introduced reading comprehension dataset is the Stanford Question Answering Dataset (SQuAD) [26] which consists of 107785 question/answer pairs from 536 articles where the answer is span of paragraph.", "startOffset": 108, "endOffset": 112}, {"referenceID": 15, "context": "2, we use accuracy and precision-recall to measure the quality of the numeric answers, and apply metrics like ROUGE-L [23] and phrasing-aware evaluation framework [24] for long textual answers.", "startOffset": 118, "endOffset": 122}, {"referenceID": 16, "context": "These metrics are simple modifications to metrics like BLEU [25] and METEOR [8], and are shown to achieve better correlation with human judgments.", "startOffset": 60, "endOffset": 64}, {"referenceID": 1, "context": "These metrics are simple modifications to metrics like BLEU [25] and METEOR [8], and are shown to achieve better correlation with human judgments.", "startOffset": 76, "endOffset": 79}, {"referenceID": 0, "context": "It is often used as a generative language model for various NLP tasks, such as machine translation [7], query answering [16], etc.", "startOffset": 99, "endOffset": 102}, {"referenceID": 8, "context": "It is often used as a generative language model for various NLP tasks, such as machine translation [7], query answering [16], etc.", "startOffset": 120, "endOffset": 124}, {"referenceID": 21, "context": "\u2022 Sequence-to-Sequence (Seq2Seq) Model: Seq2Seq [30] model is one of the most commonly used RNN models.", "startOffset": 48, "endOffset": 52}, {"referenceID": 21, "context": "We trained a vanilla Seq2Seq model similar to the one described in [30] with query as source sequence and answer as target sequence.", "startOffset": 67, "endOffset": 71}, {"referenceID": 20, "context": "\u2022 Memory Networks Model: End-to-End Memory Networks [29] was proposed for and has shown good performance in QA task for its ability of learning memory representation of contextual information.", "startOffset": 52, "endOffset": 56}, {"referenceID": 12, "context": "This is a variant of [20] where we use LSTM [19] in place of Multilayer Perceptron (MLP).", "startOffset": 21, "endOffset": 25}, {"referenceID": 11, "context": "This is a variant of [20] where we use LSTM [19] in place of Multilayer Perceptron (MLP).", "startOffset": 44, "endOffset": 48}, {"referenceID": 13, "context": "\u2022 Attention Sum Reader (AS Reader): AS Reader [21] is a simple model that uses attention to directly pick the answer from the context.", "startOffset": 46, "endOffset": 50}, {"referenceID": 19, "context": "\u2022 ReasoNet: ReasoNet [28] also relies on attention, but is also a dynamic multi-turn model that attempts to exploit and reason over the relation among queries, contexts and answers.", "startOffset": 21, "endOffset": 25}, {"referenceID": 4, "context": "Currently, much of the successes of deep learning has been demonstrated in classification tasks [12].", "startOffset": 96, "endOffset": 100}, {"referenceID": 6, "context": "While recent work is moving towards this important direction [14], how to develop new deep learning methods towards human-like natural language understanding and reasoning, and how to design more advanced datasets to evaluate and facilitate this research is our longer-term goal.", "startOffset": 61, "endOffset": 65}], "year": 2016, "abstractText": "This paper presents our recent work on the design and development of a new, large scale dataset, which we name MS MARCO, for MAchine Reading COmprehension. This new dataset is aimed to overcome a number of well-known weaknesses of previous publicly available datasets for the same task of reading comprehension and question answering. In MS MARCO, all questions are sampled from real anonymized user queries. The context passages, from which answers in the dataset are derived, are extracted from real web documents using the most advanced version of the Bing search engine. The answers to the queries are human generated. Finally, a subset of these queries has multiple answers. We aim to release one million queries and the corresponding answers in the dataset, which, to the best of our knowledge, is the most comprehensive real-world dataset of its kind in both quantity and quality. We are currently releasing 100,000 queries with their corresponding answers to inspire work in reading comprehension and question answering along with gathering feedback from the research community.", "creator": "LaTeX with hyperref package"}}}