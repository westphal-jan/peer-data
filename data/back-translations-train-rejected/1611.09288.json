{"id": "1611.09288", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Nov-2016", "title": "Dense Prediction on Sequences with Time-Dilated Convolutions for Speech Recognition", "abstract": "In computer vision pixelwise dense prediction is the task of predicting a label for each pixel in the image. Convolutional neural networks achieve good performance on this task, while being computationally efficient. In this paper we carry these ideas over to the problem of assigning a sequence of labels to a set of speech frames, a task commonly known as framewise classification. We show that dense prediction view of framewise classification offers several advantages and insights, including computational efficiency and the ability to apply batch normalization. When doing dense prediction we pay specific attention to strided pooling in time and introduce an asymmetric dilated convolution, called time-dilated convolution, that allows for efficient and elegant implementation of pooling in time. We show that by using time-dilated convolutions with a very deep VGG-style CNN with batch normalization, we achieve best published single model accuracy result on the switchboard-2000 benchmark dataset.", "histories": [["v1", "Mon, 28 Nov 2016 19:09:58 GMT  (112kb,D)", "https://arxiv.org/abs/1611.09288v1", "Appeared at NIPS 2016 End-to-end Learning for Speech and Audio Processing Workshop"], ["v2", "Wed, 14 Dec 2016 08:27:41 GMT  (112kb,D)", "http://arxiv.org/abs/1611.09288v2", "Appeared at NIPS 2016 End-to-end Learning for Speech and Audio Processing Workshop"]], "COMMENTS": "Appeared at NIPS 2016 End-to-end Learning for Speech and Audio Processing Workshop", "reviews": [], "SUBJECTS": "cs.CL cs.LG cs.NE", "authors": ["tom sercu", "vaibhava goel"], "accepted": false, "id": "1611.09288"}, "pdf": {"name": "1611.09288.pdf", "metadata": {"source": "CRF", "title": "Dense Prediction on Sequences with Time-Dilated Convolutions for Speech Recognition", "authors": ["Tom Sercu", "Vaibhava Goel"], "emails": ["Tom.Sercu1@ibm.com", "vgoel@us.ibm.com"], "sections": [{"heading": "1 Introduction", "text": "In recent years, we have seen tremendous success in both computer vision [2, 3, 4] and speech recognition [5, 6, 7]. Many computer education problems fall into one of two problem types: the first is classification, where a single label is produced per image, the second is pixel-by-pixel prediction, where a label is produced for each pixel in the image. Examples of dense predictions are semantic segmentation, depth mapping, optical flow, normal prediction of surfaces, etc. Efficient revolutionary architectures make it possible to produce a full image size, rather than forecasting the values for each pixel separately from a small patch centered around the pixel. In this paper, we argue that we view acoustic modeling in language as a dense prediction task on sequences."}, {"heading": "2 Related work: Pooling in CNNs for dense prediction on images", "text": "Indeed, it is not surprising that in recent years, many people who work for the rights of minorities have fought for the rights of minorities and minorities, for the rights of minorities and for the rights of minorities, for the rights of minorities, for the rights of minorities, for the rights of minorities, for the rights of minorities, for the rights of minorities, for the rights of minorities and for the rights of minorities, for the rights of minorities and for the rights of minorities, for the rights of minorities, for the rights of minorities and for the rights of minorities, for the rights of minorities and for the rights of minorities, for the rights of minorities and for the rights of minorities, for the rights of minorities and for the rights of minorities. \""}, {"heading": "3 Time-dilated convolutions", "text": "The current work [7, 8] shows a significant increase in performance through the use of time pooling in the time during cross entropy training, but sequence training is prohibitively expensive, since an expression must be spliced in uttLen independent windows. By adapting the term dense prediction, we propose to allow pooling in time while maintaining efficient full statement processing by using an asymmetrical version of the spatial dilated convolution with dilation in the direction of time, but not in the frequency direction, which we appropriately call time-dilated convolutions. The problem with stricted pooling in time is that the length of the output sequence is shorter than the length of the input sequence with a factor of 2 (p), assuming the pooling levels with Stride 2."}, {"heading": "4 Experiments and results", "text": "We trained a VGG-style CNN [4] in the hybrid NN / HMM setting on the 2000h Switchboard + Fisher Dataset. The architecture and training method is similar to our earlier papers [7, 8], and is based on the setup described in [21]. Our input functions are VTLN-distorted logmaps with \u2206, the results are 32k bound CD states from forced orientation. Table 1 fully specifies the CNN pool when training on windows and predicting the center frame. Corresponding to the observations in [8], we do not pad in time, Layer Output: Vmaps \u00d7 f \u00d7 T Input Pool 3 \u00d7 64 \u00d7 48 conv 7 \u00d7 64 \u00d7 42 pool 2 conv 3 \u00d7 42 conv 3 \u00d7 3 \u00d7 64 conv 3 \u00d7 32 conv 3 \u00d7 32 \u00d7 40 conv 3 \u00d7 3xtureov 3 x 64 \u00d7 16,16,16,16,16,4 \u00d7 4,4 \u00d7 4,4 \u00d7 254,4 \u00d7 2016."}, {"heading": "5 Relation to other models", "text": "In fact, this multi-level SBN architecture is a special case of a time-dictated confrontation. DNN networks (SBN) [23, 24, 25] or hierarchical bottleneck networks [26] are an influential acoustic model in hybrid NN / HMM speech recognition. SBNs are typically seen as two consecutive DNNs, each stage being discriminatively trained with a bottleneck (small hidden layer), the first DNN seeing the input characteristics, while the second DNN receives the bottleneck characteristics from the first DNN as input. Typically, the second DNN gets 5 bottleneck characteristics with a step 5, i.e. characteristics from position {\u2212 10, \u2212 5, 10} relative to the center [25], it has been pointed out that this SBN structure is conventional and can be propagated back through both stages. In fact, this multi-level SBN architecture is a specific case of a confrontational time."}, {"heading": "6 Conclusion", "text": "We drew the parallel between dense prediction in computer vision and frame-by-frame sequence labeling, both in the HMM / NN setting and in the end-to-end setting, which gave us the tool (time-extended turns) to timely adapt pooling to CNN acoustic models while maintaining efficient processing and batch normalization at full utterance. On Hub5 \"00, we lowered the WHO from 9.4% in the previous work to 8.5%, a relative improvement of 10%. With a large (36M n-gram) language model, we achieve 7.7% WHO, the best single-pass performance reported so far."}], "references": [{"title": "Gradient-based learning applied to document recognition", "author": ["Yann LeCun", "L\u00e9on Bottou", "Yoshua Bengio", "Patrick Haffner"], "venue": "Proceedings of the IEEE, vol. 86, no. 11, pp. 2278\u20132324, 1998.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1998}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E Hinton"], "venue": "Advances in neural information processing systems, 2012, pp. 1097\u20131105.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2012}, {"title": "Overfeat: Integrated recognition, localization and detection using convolutional networks", "author": ["Pierre Sermanet", "David Eigen", "Xiang Zhang", "Micha\u00ebl Mathieu", "Rob Fergus", "Yann LeCun"], "venue": "arXiv:1312.6229, 2013.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2013}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["Karen Simonyan", "Andrew Zisserman"], "venue": "CoRR arXiv:1409.1556, 2014.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2014}, {"title": "Applying convolutional neural networks concepts to hybrid nn-hmm model for speech recognition", "author": ["Ossama Abdel-Hamid", "Abdel-rahman Mohamed", "Hui Jiang", "Gerald Penn"], "venue": "Proc. ICASSP, 2012.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2012}, {"title": "Deep convolutional neural networks for lvcsr", "author": ["Tara N Sainath", "Abdel-rahman Mohamed", "Brian Kingsbury", "Bhuvana Ramabhadran"], "venue": "Proc. ICASSP, 2013.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2013}, {"title": "Very deep multilingual convolutional neural networks for lvcsr", "author": ["Tom Sercu", "Christian Puhrsch", "Brian Kingsbury", "Yann LeCun"], "venue": "Proc. ICASSP, 2016.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2016}, {"title": "Advances in very deep convolutional neural networks for lvcsr", "author": ["Tom Sercu", "Vaibhava Goel"], "venue": "Proc. Interspeech, 2016.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2016}, {"title": "Scene parsing with multiscale feature learning, purity trees, and optimal covers", "author": ["Cl\u00e9ment Farabet", "Camille Couprie", "Laurent Najman", "Yann LeCun"], "venue": "Proc. ICML, 2012.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2012}, {"title": "Fully convolutional networks for semantic segmentation", "author": ["Jonathan Long", "Evan Shelhamer", "Trevor Darrell"], "venue": "CVPR, 2015.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "Segnet: A deep convolutional encoder-decoder architecture for image segmentation", "author": ["Vijay Badrinarayanan", "Alex Kendall", "Roberto Cipolla"], "venue": "arXiv:1511.00561, 2015.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "Adaptive deconvolutional networks for mid and high level feature learning", "author": ["Matthew D Zeiler", "Graham W Taylor", "Rob Fergus"], "venue": "2011 International Conference on Computer Vision. IEEE, 2011, pp. 2018\u20132025.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2011}, {"title": "Highly efficient forward and backward propagation of convolutional neural networks for pixelwise classification", "author": ["Hongsheng Li", "Rui Zhao", "Xiaogang Wang"], "venue": "arXiv:1412.4526, 2014.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2014}, {"title": "Multi-scale context aggregation by dilated convolutions", "author": ["Fisher Yu", "Vladlen Koltun"], "venue": "proc ICLR, 2016.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2016}, {"title": "Wavenet: A generative model for raw audio", "author": ["Aaron van den Oord", "Sander Dieleman", "Heiga Zen", "Karen Simonyan", "Oriol Vinyals", "Alex Graves", "Nal Kalchbrenner", "Andrew Senior", "Koray Kavukcuoglu"], "venue": "arXiv:1609.03499, 2016.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2016}, {"title": "Deep speech 2: End-to-end speech recognition in english and mandarin", "author": ["Dario Amodei", "Rishita Anubhai", "Eric Battenberg", "Carl Case", "Jared Casper", "Bryan Catanzaro", "Jingdong Chen", "Mike Chrzanowski", "Adam Coates", "Greg Diamos"], "venue": "CoRR arXiv:1512.02595, 2015.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}, {"title": "Convolutional, long short-term memory, fully connected deep neural networks", "author": ["Tara N Sainath", "Oriol Vinyals", "Andrew Senior", "Ha\u015fim Sak"], "venue": "proc. ICASSP, 2015.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2015}, {"title": "Joint training of convolutional and non-convolutional neural networks", "author": ["Hagen Soltau", "George Saon", "Tara N Sainath"], "venue": "to Proc. ICASSP, 2014.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2014}, {"title": "The ibm 2016 english conversational telephone speech recognition system", "author": ["George Saon", "Tom Sercu", "Steven Rennie", "Hong-Kwang J Kuo"], "venue": "Proc. Interspeech, 2016.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2016}, {"title": "The microsoft 2016 conversational speech recognition system", "author": ["W Xiong", "J Droppo", "X Huang", "F Seide", "M Seltzer", "A Stolcke", "D Yu", "G Zweig"], "venue": "arXiv:1609.03528, 2016.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2016}, {"title": "The ibm 2015 english conversational telephone speech recognition system", "author": ["George Saon", "Hong-Kwang J Kuo", "Steven Rennie", "Michael Picheny"], "venue": "Proc. Interspeech, 2015.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2015}, {"title": "Achieving human parity in conversational speech recognition", "author": ["W Xiong", "J Droppo", "X Huang", "F Seide", "M Seltzer", "A Stolcke", "D Yu", "G Zweig"], "venue": "arXiv:1610.05256, 2016.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2016}, {"title": "Investigation into bottle-neck features for meeting speech recognition", "author": ["Frantisek Grezl", "Martin Karafi\u00e1t", "Lukas Burget"], "venue": "Proc. Interspeech, 2009.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2009}, {"title": "Convolutive bottleneck network features for lvcsr", "author": ["Karel Vesel\u1ef3", "Martin Karafi\u00e1t", "Franti\u0161ek Gr\u00e9zl"], "venue": "ASRU, 2011.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2011}, {"title": "Adaptation of multilingual stacked bottle-neck neural network structure for new language", "author": ["Frantisek Gr\u00e9zl", "Martin Karafi\u00e1t", "Karel Vesel\u1ef3"], "venue": "Proc. ICASSP, 2014.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2014}, {"title": "Hierarchical bottle neck features for lvcsr", "author": ["Christian Plahl", "Ralf Schl\u00fcter", "Hermann Ney"], "venue": "Interspeech, 2010, pp. 1197\u20131200.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2010}, {"title": "Wav2letter: an end-to-end convnet-based speech recognition system", "author": ["Ronan Collobert", "Christian Puhrsch", "Gabriel Synnaeve"], "venue": "arXiv:1609.03193, 2016. 5", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 0, "context": "Deep convolutional networks [1] have seen tremendous sucess both in computer vision [2, 3, 4] and speech recognition [5, 6, 7] over the last years.", "startOffset": 28, "endOffset": 31}, {"referenceID": 1, "context": "Deep convolutional networks [1] have seen tremendous sucess both in computer vision [2, 3, 4] and speech recognition [5, 6, 7] over the last years.", "startOffset": 84, "endOffset": 93}, {"referenceID": 2, "context": "Deep convolutional networks [1] have seen tremendous sucess both in computer vision [2, 3, 4] and speech recognition [5, 6, 7] over the last years.", "startOffset": 84, "endOffset": 93}, {"referenceID": 3, "context": "Deep convolutional networks [1] have seen tremendous sucess both in computer vision [2, 3, 4] and speech recognition [5, 6, 7] over the last years.", "startOffset": 84, "endOffset": 93}, {"referenceID": 4, "context": "Deep convolutional networks [1] have seen tremendous sucess both in computer vision [2, 3, 4] and speech recognition [5, 6, 7] over the last years.", "startOffset": 117, "endOffset": 126}, {"referenceID": 5, "context": "Deep convolutional networks [1] have seen tremendous sucess both in computer vision [2, 3, 4] and speech recognition [5, 6, 7] over the last years.", "startOffset": 117, "endOffset": 126}, {"referenceID": 6, "context": "Deep convolutional networks [1] have seen tremendous sucess both in computer vision [2, 3, 4] and speech recognition [5, 6, 7] over the last years.", "startOffset": 117, "endOffset": 126}, {"referenceID": 7, "context": "\u2022 Batch normalization can easily be adopted during sequence training (or end to end training), which we will show gives strong improvements (as outlined in [8]).", "startOffset": 156, "endOffset": 159}, {"referenceID": 8, "context": "In [9], an image is processed at three different scales with three different CNNs, after which the output feature maps are merged.", "startOffset": 3, "endOffset": 6}, {"referenceID": 9, "context": "The Fully Convolutional Networks (FCNs) from [10] use a VGG classification network as basis, introducing skip connections to merge hi-res lower layers with upsampled low-res layers from deeper in the network.", "startOffset": 45, "endOffset": 49}, {"referenceID": 10, "context": "SegNet [11] uses a encoder-decoder structure, in which upsampling is done with max-unpooling [12], i.", "startOffset": 7, "endOffset": 11}, {"referenceID": 11, "context": "SegNet [11] uses a encoder-decoder structure, in which upsampling is done with max-unpooling [12], i.", "startOffset": 93, "endOffset": 97}, {"referenceID": 2, "context": "A second way of using CNNs with strided pooling for dense prediction was proposed in [3]: at every pooling layer with stride s\u00d7 s, the input is duplicated s\u00d7 s times, but shifted with offset (\u2206x,\u2206y) \u2208 [0 .", "startOffset": 85, "endOffset": 88}, {"referenceID": 9, "context": "This was called filter rarefaction in [10], introduced as \u201cd-regularly sparse kernels\u201d in [13], and dubbed spatial dilated convolutions in [14].", "startOffset": 38, "endOffset": 42}, {"referenceID": 12, "context": "This was called filter rarefaction in [10], introduced as \u201cd-regularly sparse kernels\u201d in [13], and dubbed spatial dilated convolutions in [14].", "startOffset": 90, "endOffset": 94}, {"referenceID": 13, "context": "This was called filter rarefaction in [10], introduced as \u201cd-regularly sparse kernels\u201d in [13], and dubbed spatial dilated convolutions in [14].", "startOffset": 139, "endOffset": 143}, {"referenceID": 2, "context": "It was noted [3, 10] that this method is equivalent to shift-and-interleave, though more intuitive.", "startOffset": 13, "endOffset": 20}, {"referenceID": 9, "context": "It was noted [3, 10] that this method is equivalent to shift-and-interleave, though more intuitive.", "startOffset": 13, "endOffset": 20}, {"referenceID": 14, "context": "The recent WaveNet work [15] uses dilated convolutions for a generative model of audio.", "startOffset": 24, "endOffset": 28}, {"referenceID": 4, "context": "Previous work on CNNs for acoustic modeling [5, 6] eliminated the possibility of strided pooling in time because of the downsampling effect.", "startOffset": 44, "endOffset": 50}, {"referenceID": 5, "context": "Previous work on CNNs for acoustic modeling [5, 6] eliminated the possibility of strided pooling in time because of the downsampling effect.", "startOffset": 44, "endOffset": 50}, {"referenceID": 6, "context": "Recent work [7, 8] shows a significant performance boost by using pooling in time during cross-entropy training, however sequence training is prohibitively expensive since an utterance has to be spliced into uttLen independent windows.", "startOffset": 12, "endOffset": 18}, {"referenceID": 7, "context": "Recent work [7, 8] shows a significant performance boost by using pooling in time during cross-entropy training, however sequence training is prohibitively expensive since an utterance has to be spliced into uttLen independent windows.", "startOffset": 12, "endOffset": 18}, {"referenceID": 15, "context": "For recurrent end-to-end networks typically a factor 4 size reduction is accepted [16, 17] which limits the number of pooling layers to 2, while in the hybrid NN/HMM framework, pooling is not acceptable.", "startOffset": 82, "endOffset": 90}, {"referenceID": 16, "context": "For recurrent end-to-end networks typically a factor 4 size reduction is accepted [16, 17] which limits the number of pooling layers to 2, while in the hybrid NN/HMM framework, pooling is not acceptable.", "startOffset": 82, "endOffset": 90}, {"referenceID": 12, "context": "We tackle this problem with a 1D version of sparse kernels [13], or equivalently spatial dilated convolutions [14].", "startOffset": 59, "endOffset": 63}, {"referenceID": 13, "context": "We tackle this problem with a 1D version of sparse kernels [13], or equivalently spatial dilated convolutions [14].", "startOffset": 110, "endOffset": 114}, {"referenceID": 12, "context": "This dilating procedure is how a VGG classification network is adapted for semantic segmentation [13, 14].", "startOffset": 97, "endOffset": 105}, {"referenceID": 13, "context": "This dilating procedure is how a VGG classification network is adapted for semantic segmentation [13, 14].", "startOffset": 97, "endOffset": 105}, {"referenceID": 6, "context": "This allows to combine the performance gains of pooling [7], while maintaining the computational efficiency and ability to apply batch normalization [8].", "startOffset": 56, "endOffset": 59}, {"referenceID": 7, "context": "This allows to combine the performance gains of pooling [7], while maintaining the computational efficiency and ability to apply batch normalization [8].", "startOffset": 149, "endOffset": 152}, {"referenceID": 3, "context": "We trained a VGG style CNN [4] in the hybrid NN/HMM setting on the 2000h Switchboard+Fisher dataset.", "startOffset": 27, "endOffset": 30}, {"referenceID": 6, "context": "The architecture and training method is similar to our earlier papers [7, 8], and is based on the setup described in [21].", "startOffset": 70, "endOffset": 76}, {"referenceID": 7, "context": "The architecture and training method is similar to our earlier papers [7, 8], and is based on the setup described in [21].", "startOffset": 70, "endOffset": 76}, {"referenceID": 20, "context": "The architecture and training method is similar to our earlier papers [7, 8], and is based on the setup described in [21].", "startOffset": 117, "endOffset": 121}, {"referenceID": 7, "context": "Corresponding to the observations in [8], we do not pad in time,", "startOffset": 37, "endOffset": 40}, {"referenceID": 17, "context": "SWB CH XE ST XE ST Classic 512 CNN [18] 12.", "startOffset": 35, "endOffset": 39}, {"referenceID": 18, "context": "4 IBM 2016 RNN+VGG+LSTM [19] 8.", "startOffset": 24, "endOffset": 28}, {"referenceID": 19, "context": "4 \u2020 MSR 2016 ResNet * [20] 8.", "startOffset": 22, "endOffset": 26}, {"referenceID": 19, "context": "9 MSR 2016 LACE * [20] 8.", "startOffset": 18, "endOffset": 22}, {"referenceID": 19, "context": "6 MSR 2016 BLSTM * [20] 8.", "startOffset": 19, "endOffset": 23}, {"referenceID": 18, "context": "7 VGG (pool, inefficient) [19] 10.", "startOffset": 26, "endOffset": 30}, {"referenceID": 7, "context": "0 VGG (no pool) [8] 10.", "startOffset": 16, "endOffset": 19}, {"referenceID": 7, "context": "7 VGG-10 + BN (no pool) [8] 10.", "startOffset": 24, "endOffset": 27}, {"referenceID": 20, "context": "Table 2: Results with small LM (4M n-grams) SWB CH IBM 2015 DNN+RNN+CNN [21] 8.", "startOffset": 72, "endOffset": 76}, {"referenceID": 18, "context": "3 \u2020 IBM 2016 RNN+VGG+LSTM [19] 7.", "startOffset": 26, "endOffset": 30}, {"referenceID": 19, "context": "7 \u2020 MSR 2016 ResNet [20] 8.", "startOffset": 20, "endOffset": 24}, {"referenceID": 19, "context": "8 MSR 2016 LACE [20] 8.", "startOffset": 16, "endOffset": 20}, {"referenceID": 19, "context": "8 MSR 2016 BLSTM [20] 8.", "startOffset": 17, "endOffset": 21}, {"referenceID": 6, "context": "We use the data balancing from [7] with exponent \u03b3 = 0.", "startOffset": 31, "endOffset": 34}, {"referenceID": 19, "context": "Note that the baselines from [20] use slightly smaller LMs: 3.", "startOffset": 29, "endOffset": 33}, {"referenceID": 21, "context": "We note that one typically does subsequent rescoring with more advanced language models like RNN or LSTM LMs; this way in [22] a single model performance of 6.", "startOffset": 122, "endOffset": 126}, {"referenceID": 22, "context": "Stacked bottleneck networks (SBN) [23, 24, 25] or hierarchical bottleneck networks [26] are a influential acoustic model in hybrid NN/HMM speech recognition.", "startOffset": 34, "endOffset": 46}, {"referenceID": 23, "context": "Stacked bottleneck networks (SBN) [23, 24, 25] or hierarchical bottleneck networks [26] are a influential acoustic model in hybrid NN/HMM speech recognition.", "startOffset": 34, "endOffset": 46}, {"referenceID": 24, "context": "Stacked bottleneck networks (SBN) [23, 24, 25] or hierarchical bottleneck networks [26] are a influential acoustic model in hybrid NN/HMM speech recognition.", "startOffset": 34, "endOffset": 46}, {"referenceID": 25, "context": "Stacked bottleneck networks (SBN) [23, 24, 25] or hierarchical bottleneck networks [26] are a influential acoustic model in hybrid NN/HMM speech recognition.", "startOffset": 83, "endOffset": 87}, {"referenceID": 24, "context": "features from position {\u221210,\u22125, 0, 5, 10} relative to the center [25].", "startOffset": 65, "endOffset": 69}, {"referenceID": 23, "context": "In [24], it was pointed out that this SBN is convolutional and one can backpropagate through both stages together.", "startOffset": 3, "endOffset": 7}, {"referenceID": 16, "context": "Both the CLDNN architecture [17] and Deep Speech 2 (DS2) [16] combine a convolutional network as first stage with LSTM and fully connected (DNN) output layers.", "startOffset": 28, "endOffset": 32}, {"referenceID": 15, "context": "Both the CLDNN architecture [17] and Deep Speech 2 (DS2) [16] combine a convolutional network as first stage with LSTM and fully connected (DNN) output layers.", "startOffset": 57, "endOffset": 61}, {"referenceID": 26, "context": "In Wav2Letter [27], a competitive end-to-end model is presented which is fully convolutional.", "startOffset": 14, "endOffset": 18}, {"referenceID": 26, "context": "Both DS2 and Wav2Letter do a certain amount of downsampling through pooling or striding, which can be accepted when training with a CTC (or AutoSeg [27]) criterion since it doesn\u2019t require the output to be the same length as the input.", "startOffset": 148, "endOffset": 152}], "year": 2016, "abstractText": "In computer vision pixelwise dense prediction is the task of predicting a label for each pixel in the image. Convolutional neural networks achieve good performance on this task, while being computationally efficient. In this paper we carry these ideas over to the problem of assigning a sequence of labels to a set of speech frames, a task commonly known as framewise classification. We show that dense prediction view of framewise classification offers several advantages and insights, including computational efficiency and the ability to apply batch normalization. When doing dense prediction we pay specific attention to strided pooling in time and introduce an asymmetric dilated convolution, called time-dilated convolution, that allows for efficient and elegant implementation of pooling in time. We show results using time-dilated convolutions in a very deep VGG-style CNN with batch normalization on the Hub5 Switchboard-2000 benchmark task. With a big n-gram language model, we achieve 7.7% WER which is the best single model single-pass performance reported so far.", "creator": "LaTeX with hyperref package"}}}