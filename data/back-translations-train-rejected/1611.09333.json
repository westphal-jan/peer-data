{"id": "1611.09333", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Nov-2016", "title": "Dictionary Learning with Equiprobable Matching Pursuit", "abstract": "Sparse signal representations based on linear combinations of learned atoms have been used to obtain state-of-the-art results in several practical signal processing applications. Approximation methods are needed to process high-dimensional signals in this way because the problem to calculate optimal atoms for sparse coding is NP-hard. Here we study greedy algorithms for unsupervised learning of dictionaries of shift-invariant atoms and propose a new method where each atom is selected with the same probability on average, which corresponds to the homeostatic regulation of a recurrent convolutional neural network. Equiprobable selection can be used with several greedy algorithms for dictionary learning to ensure that all atoms adapt during training and that no particular atom is more likely to take part in the linear combination on average. We demonstrate via simulation experiments that dictionary learning with equiprobable selection results in higher entropy of the sparse representation and lower reconstruction and denoising errors, both in the case of ordinary matching pursuit and orthogonal matching pursuit with shift-invariant dictionaries. Furthermore, we show that the computational costs of the matching pursuits are lower with equiprobable selection, leading to faster and more accurate dictionary learning algorithms.", "histories": [["v1", "Mon, 28 Nov 2016 20:38:52 GMT  (164kb,D)", "http://arxiv.org/abs/1611.09333v1", "8 pages, 8 figures"]], "COMMENTS": "8 pages, 8 figures", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["fredrik sandin", "sergio martin-del-campo"], "accepted": false, "id": "1611.09333"}, "pdf": {"name": "1611.09333.pdf", "metadata": {"source": "CRF", "title": "Dictionary Learning with Equiprobable Matching Pursuit", "authors": ["Fredrik Sandin", "Sergio Martin-del-Campo"], "emails": ["fredrik.sandin@ltu.se"], "sections": [{"heading": null, "text": "This year it has come to the point that it will only be a matter of time before it will happen, until it does."}, {"heading": "II. DICTIONARY LEARNING METHOD", "text": "The signal we want to adapt to the signal is in principle x (t) and can be multidimensional, but we are comparing it with each other. (2) The signal we want to adapt to the signal is multidimensional. (3) The signal we want to adapt to the signal is multidimensional. (3) The signal we want to adapt to the signal is multidimensional. (4) The signal we want to adapt to the signal is multidimensional. (4) The signal we want to adapt to the signal is multidimensional. (4) The signal we want to adapt to the signal we want to adapt to the signal is multidimensional. (4) The signal we want to adapt to the signal is multidimensional. (4) The signal we want to adapt to the signal is multidimensional."}, {"heading": "III. RESULTS", "text": "We study the effects of equivalent atom selection on the learned dictionary and the sparse approximation with numerical experiments using two different signals. One signal is a 155-second 44.1 kHz rock music track with text [35] and the second signal is a 26-second 48 kHz recording of zebra finch song phrases [36]. The dictionary learning method is defined by equivalent (11) with stride length \u03b7 = 10 \u2212 6. We do not observe significant improvements in the resulting model accuracy based on other \u03b7 values and our qualitative analysis does not depend on fine-tuning this hyperparameter. The getdata () function is defined in such a way that each block of data (xn in algorithm 2) is sampled from a random location in the dataset for five seconds."}, {"heading": "A. Model accuracy and rate of convergence", "text": "In the first experiment, we examine the signal-to-noise ratio (SNR) of the sparse approximation of rock music with an average selection probability of p = 0.05, see Figure 3. The resulting dictionaries with 32 atoms are shown in Figure 2. The equivalent selection reduces the initial selection rate of the dictionary, but also leads to improved convergence time and accuracy of the sparse model. For example, the accuracy of the E-MP-based approximation exceeds that of the OMP-based approximation after about 1000 seconds of learning. This is remarkable considering that the computing costs of E-MP are lower than MP and OMP (more details below). At 64 atoms, a longer learning time is required to achieve a comparable SNR, but after about 2000 seconds of learning the SNR exceeds that of models with 32 atoms. Note that atoms are immutable."}, {"heading": "B. Effect of varying atom selection probability", "text": "Next, we examine the sensitivity of the model to variations in the average probability of atomic selection, S. Using dictionaries we learned from 1500 seconds of rock music, we calculate the SNR of rock music's sparse approximation to different values of p, see Figure 4. This result shows that models based on dictionaries learned with E-MP and E-OMP degrade in a straight line when the sparsity of the model changes. Learned dictionaries generalize to such different conditions and are not \"upgraded\" to a certain value of p. Therefore, equivalent atomic selection / activation may be feasible even with low-performance computer substrates such as neuromorphic chips, where the average value of p cannot be precisely defined due to device malfunction and noise."}, {"heading": "C. Denoising", "text": "Minor approximations of signals based on learned dictionaries are useful to solve problems, since atoms are a repetitive additive structure, not noise-like components of the signal that mostly end in the model residue (t). We are investigating the denocialization capability of E- MP and E-OMP-based models by adding Gaussian noise to the rock music signal and comparing the model SNR for different noise levels to p = 0.05, see Figure 5. The noise level is quantified in relation to the ratio of the standard deviation of additive Gaussian noise \u03c3n to the standard deviation of the signal, \u03c3s. As expected, the OMP-based model provides denocidation results that, at least at moderate noise level, are much better than MP, \u03c3n / \u03c3s \u2264 0.1. The precision achieved with E-MP and E-OMP is comparable to the OMP model showing a superior noise level, such as a high selection of OMP and OMP."}, {"heading": "D. Computational cost", "text": "The experiments are performed with a 2.3 GHz Intel Core i7 processor core and an aC + + implementation of the algorithms executed in Matlab. We calculate the average core time per signal sample and iteration of matching tracking, see Figure 6. This definition implies that the displayed processing time should be multiplied by the number of samples squared with the average selection probability p to obtain the actual computing time. We find that an equivalent selection is advantageous in terms of computing costs. In particular, MP is more expensive than E-MP, and OMP is more expensive than E-OMP. Furthermore, the processing time of E-OMP with a window length of 200 kilosamples is comparable to that of MP-based dictionary learning. Note that these results are obtained with the efficient implementations of MP and local OMP, which is why the calculation cost per sample is comparable to the window length."}, {"heading": "E. Entropy of sparse representation", "text": "The number of atoms selected per second, however, depends on whether the signal is processed, and the method and manner in which it is used also depends on the constant rate of atomic selection. Illustrated in the figure are also the constant rate of atomic selection events for E-MP and E-OMP, which are illustrated with an average selection probability of p = 0.05 for both rock music and birdsong. Illustrated in the figure are also the constant rate of atomic selection events for E-MP and E-OMP, which are mentioned atoms with an average selection probability of p = 0.05, while some atoms are selected more typically than others, while some atoms are not selected and are learned at all. In contrast, the average probability of selecting each atom is constant for E-MP and E-OMP."}, {"heading": "IV. DISCUSSION", "text": "In this context, it should be noted that this project is a project which is, first and foremost, a project which responds to the needs of the people."}, {"heading": "ACKNOWLEDGMENT", "text": "This work was inspired by discussions at the CapoCaccia Neuromorphic Engineering Workshops in 2015 and 2016. F.S. particularly appreciates the discussions with Prof. Christopher Kello on cortical criticality and the lectures by Prof. Yves Fre'gnac on receptive fields, which highlighted a knowledge gap between models and observed properties of cells in the visual cortex. F.S. is supported by a Gunnar O'quist Fellowship of Kempe Foundations and S.M.C. is funded by SKF-LTU University Technology Center."}], "references": [{"title": "A Wavelet Tour of Signal Processing: The Sparse Way, 3rd ed", "author": ["S. Mallat"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2008}, {"title": "From sparse solutions of systems of equations to sparse modeling of signals and images", "author": ["A. Bruckstein", "D. Donoho", "M. Elad"], "venue": "SIAM Review, vol. 51, no. 1, pp. 34\u201381, 2009.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2009}, {"title": "Sparse and redundant representations: from theory to applications in signal and image processing", "author": ["M. Elad"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2010}, {"title": "The cosparse analysis model and algorithms", "author": ["S. Nam", "M. Davies", "M. Elad", "R. Gribonval"], "venue": "Applied and Computational Harmonic Analysis, vol. 34, no. 1, pp. 30\u201356, 2013.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2013}, {"title": "Dictionaries for sparse representation modeling", "author": ["R. Rubinstein", "A. Bruckstein", "M. Elad"], "venue": "Proceedings of the IEEE, vol. 98, no. 6, pp. 1045\u20131057, June 2010.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2010}, {"title": "Dictionary learning", "author": ["I. Tosic", "P. Frossard"], "venue": "Signal Processing Magazine, IEEE, vol. 28, no. 2, pp. 27\u201338, March 2011.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2011}, {"title": "Gradient pursuits", "author": ["T. Blumensath", "M. Davies"], "venue": "IEEE Transactions on Signal Processing, vol. 56, no. 6, pp. 2370\u20132382, 2008.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2008}, {"title": "Blind source separation by sparse decomposition in a signal dictionary", "author": ["M. Zibulevsky", "B.A. Pearlmutter"], "venue": "Neural Computation, vol. 13, no. 4, pp. 863\u2013882, 2016/02/04 2001. [Online]. Available: http://dx.doi.org/10.1162/089976601300014385", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2016}, {"title": "Underdetermined blind source separation using sparse representations", "author": ["P. Bofill", "M. Zibulevsky"], "venue": "Signal Processing, vol. 81, no. 11, pp. 2353 \u2013 2362, 2001. [Online]. Available: http://www.sciencedirect.com/ science/article/pii/S0165168401001207", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2001}, {"title": "Compressed sensing", "author": ["D.L. Donoho"], "venue": "IEEE Trans. Inform. Theory, vol. 52, pp. 1289\u20131306, 2006.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2006}, {"title": "Towards a mathematical theory of super-resolution", "author": ["E.J. Cands", "C. Fernandez-Granda"], "venue": "Communications on Pure and Applied Mathematics, vol. 67, no. 6, pp. 906\u2013956, 2014. [Online]. Available: http://dx.doi.org/10.1002/cpa.21455", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2014}, {"title": "Supervised dictionary learning", "author": ["J. Mairal", "J. Ponce", "G. Sapiro", "A. Zisserman", "F.R. Bach"], "venue": "Advances in Neural Information Processing Systems 21, D. Koller, D. Schuurmans, Y. Bengio, and L. Bottou, Eds. Curran Associates, Inc., 2009, pp. 1033\u20131040. [Online]. Available: http://papers.nips.cc/paper/3448-supervised-dictionary-learning.pdf", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2009}, {"title": "On the computational intractability of exact and approximate dictionary learning", "author": ["A.M. Tillmann"], "venue": "IEEE Signal Processing Letters, vol. 22, no. 1, pp. 45\u201349, Jan 2015.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "Emergence of simple-cell receptive field properties by learning a sparse code for natural images", "author": ["B. Olshausen", "D. Field"], "venue": "Nature, vol. 381, pp. 607\u2013609, 1996.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1996}, {"title": "Learning overcomplete representations", "author": ["M.S. Lewicki", "T.J. Sejnowski"], "venue": "Neural Computation, vol. 12, no. 2, pp. 337\u2013365, March 2000.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2000}, {"title": "Dictionary learning algorithms for sparse representation", "author": ["K. Kreutz-Delgado", "J.F. Murray", "B.D. Rao", "K. Engan", "T.-W. Lee", "T.J. Sejnowski"], "venue": "Neural Computation, vol. 15, no. 2, pp. 349\u2013396, 2016/02/03 2003. [Online]. Available: http://dx.doi.org/10. 1162/089976603762552951", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2016}, {"title": "An online algorithm for distributed dictionary learning", "author": ["S. Chouvardas", "Y. Kopsinis", "S. Theodoridis"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2015 IEEE International Conference on, April 2015, pp. 3292\u20133296.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2015}, {"title": "On-chip sparse learning acceleration with cmos and resistive synaptic devices", "author": ["J. sun Seo", "B. Lin", "M. Kim", "P.-Y. Chen", "D. Kadetotad", "Z. Xu", "A. Mohanty", "S. Vrudhula", "S. Yu", "J. Ye", "Y. Cao"], "venue": "Nanotechnology, IEEE Transactions on, vol. 14, no. 6, pp. 969\u2013979, Nov 2015.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2015}, {"title": "Adaptation of the simple or complex nature of v1 receptive fields to visual statistics", "author": ["J. Fournier", "C. Monier", "M. Pananceau", "Y. Fregnac"], "venue": "Nature Neuroscience, vol. 14, no. 8, pp. 1053\u20131060, Aug 2011.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2011}, {"title": "Spatiotemporal receptive fields of barrel cortex revealed by reverse correlation of synaptic input", "author": ["A. Ramirez", "E.A. Pnevmatikakis", "J. Merel", "L. Paninski", "K.D. Miller", "R.M. Bruno"], "venue": "Nature Neuroscience, vol. 17, no. 6, pp. 866\u2013875, Jun 2014.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2014}, {"title": "Hidden complexity of synaptic receptive fields in cat v1", "author": ["J. Fournier", "C. Monier", "M. Levy", "O. Marre", "K. Sri", "Z.F. Kisvrday", "Y. Frgnac"], "venue": "The Journal of Neuroscience, vol. 34, no. 16, pp. 5515\u20135528, 2014.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2014}, {"title": "Visual adaptation as optimal information transmission", "author": ["M.J. Wainwright"], "venue": "Vision Research, vol. 39, no. 23, pp. 3960 \u2013 3974, 1999.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 1999}, {"title": "The criticality hypothesis: how local cortical networks might optimize information processing", "author": ["J.M. Beggs"], "venue": "Philosophical Transactions of the Royal Society of London A: Mathematical, Physical and Engineering Sciences, vol. 366, no. 1864, pp. 329\u2013343, 2008. [Online]. Available: http://rsta.royalsocietypublishing.org/content/366/1864/329", "citeRegEx": "24", "shortCiteRegEx": null, "year": 1864}, {"title": "Being critical of criticality in the brain", "author": ["J.M. Beggs", "N. Timme"], "venue": "Frontiers in Physiology, vol. 3, no. 163, 2012. [Online]. Available: http://www.frontiersin.org/fractal physiology/10.3389/fphys. 2012.00163/abstract", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2012}, {"title": "The functional benefits of criticality in the cortex", "author": ["W.L. Shew", "D. Plenz"], "venue": "The Neuroscientist, vol. 19, no. 1, pp. 88\u2013100, 2013. [Online]. Available: http://nro.sagepub.com/content/19/1/88.abstract", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2013}, {"title": "Quasicritical brain dynamics on a nonequilibrium widom line", "author": ["R.V. Williams-Garc\u0131\u0301a", "M. Moore", "J.M. Beggs", "G. Ortiz"], "venue": "Phys. Rev. E, vol. 90, p. 062714, Dec 2014. [Online]. Available: http://link.aps.org/doi/10.1103/PhysRevE.90.062714", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2014}, {"title": "Efficient auditory coding", "author": ["E. Smith", "M.S. Lewicki"], "venue": "Nature, no. 7079, pp. 978\u2013982, 02.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 0}, {"title": "Efficient coding of time-relative structure using spikes", "author": ["E. Smith", "M. Lewicki"], "venue": "Neural Computation, vol. 17, no. 1, pp. 19\u201345, 2005.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2005}, {"title": "Matching pursuits with time-frequency dictionaries", "author": ["S. Mallat", "Z. Zhang"], "venue": "IEEE T Signal Proces, 1993.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 1993}, {"title": "Orthogonal matching pursuit: recursive function approximation with applications to wavelet decomposition", "author": ["Y. Pati", "R. Rezaiifar", "P. Krishnaprasad"], "venue": "Signals, Systems and Computers, 1993. 1993 Conference Record of The Twenty-Seventh Asilomar Conference on, Nov 1993, pp. 40\u201344 vol.1.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 1993}, {"title": "A low complexity orthogonal matching pursuit for sparse signal approximation with shift-invariant dictionaries", "author": ["B. Mailhe", "R. Gribonval", "F. Bimbot", "P. Vandergheynst"], "venue": "Acoustics, Speech and Signal Processing, 2009. ICASSP 2009. IEEE International Conference on, April 2009, pp. 3445\u20133448.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2009}, {"title": "Probabilistic framework for the adaptation and comparison of image codes", "author": ["M.S. Lewicki", "B.A. Olshausen"], "venue": "J. Opt. Soc. Am. A, vol. 16, no. 7, pp. 1587\u20131601, Jul 1999. [Online]. Available: http://josaa.osa.org/abstract.cfm?URI=josaa-16-7-1587", "citeRegEx": "33", "shortCiteRegEx": null, "year": 1999}, {"title": "MPTK: Matching Pursuit made tractable", "author": ["S. Krstulovic", "R. Gribonval"], "venue": "Proc. Int. Conf. Acoust. Speech Signal Process. (ICASSP\u201906), vol. 3, May 2006, pp. III\u2013496 \u2013 III\u2013499.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2006}, {"title": "After the last", "author": ["The Red Thread"], "venue": "licensed under CC BY-NC 3.0. [Online]. Available: http://freemusicarchive.org", "citeRegEx": "35", "shortCiteRegEx": null, "year": 0}, {"title": "Song phrases of Zebra Finch", "author": ["M. Anderson"], "venue": "cat. nr. XC287103. [Online]. Available: www.xeno-canto.org/287103/download", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2871}, {"title": "Inhibitory interneurons decorrelate excitatory cells to drive sparse code formation in a spiking model of v1", "author": ["P.D. King", "J. Zylberberg", "M.R. DeWeese"], "venue": "Journal of Neuroscience, vol. 33, no. 13, pp. 5475\u20135485, 2013.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2013}, {"title": "Nonlinear hebbian learning as a unifying principle in receptive field formation", "author": ["C.S.N. Brito", "W. Gerstner"], "venue": "2016. [Online]. Available: http://arxiv.org/abs/1601.00701", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2016}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["N. Srivastava", "G. Hinton", "A. Krizhevsky", "I. Sutskever", "R. Salakhutdinov"], "venue": "Journal of Machine Learning Research, vol. 15, pp. 1929\u20131958, 2014.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 1929}], "referenceMentions": [{"referenceID": 0, "context": "Sparse representation models [1], [2], [3], [4] and related machine learning algorithms [5], [6] have proven remarkably successful at extracting useful information from complex highdimensional signals, for example in the context of denoising [7], under-determined source separation [8], [9], compressed sensing [10], super-resolution sensing [11], and classification [12].", "startOffset": 29, "endOffset": 32}, {"referenceID": 1, "context": "Sparse representation models [1], [2], [3], [4] and related machine learning algorithms [5], [6] have proven remarkably successful at extracting useful information from complex highdimensional signals, for example in the context of denoising [7], under-determined source separation [8], [9], compressed sensing [10], super-resolution sensing [11], and classification [12].", "startOffset": 34, "endOffset": 37}, {"referenceID": 2, "context": "Sparse representation models [1], [2], [3], [4] and related machine learning algorithms [5], [6] have proven remarkably successful at extracting useful information from complex highdimensional signals, for example in the context of denoising [7], under-determined source separation [8], [9], compressed sensing [10], super-resolution sensing [11], and classification [12].", "startOffset": 39, "endOffset": 42}, {"referenceID": 3, "context": "Sparse representation models [1], [2], [3], [4] and related machine learning algorithms [5], [6] have proven remarkably successful at extracting useful information from complex highdimensional signals, for example in the context of denoising [7], under-determined source separation [8], [9], compressed sensing [10], super-resolution sensing [11], and classification [12].", "startOffset": 44, "endOffset": 47}, {"referenceID": 4, "context": "Sparse representation models [1], [2], [3], [4] and related machine learning algorithms [5], [6] have proven remarkably successful at extracting useful information from complex highdimensional signals, for example in the context of denoising [7], under-determined source separation [8], [9], compressed sensing [10], super-resolution sensing [11], and classification [12].", "startOffset": 88, "endOffset": 91}, {"referenceID": 5, "context": "Sparse representation models [1], [2], [3], [4] and related machine learning algorithms [5], [6] have proven remarkably successful at extracting useful information from complex highdimensional signals, for example in the context of denoising [7], under-determined source separation [8], [9], compressed sensing [10], super-resolution sensing [11], and classification [12].", "startOffset": 93, "endOffset": 96}, {"referenceID": 6, "context": "Sparse representation models [1], [2], [3], [4] and related machine learning algorithms [5], [6] have proven remarkably successful at extracting useful information from complex highdimensional signals, for example in the context of denoising [7], under-determined source separation [8], [9], compressed sensing [10], super-resolution sensing [11], and classification [12].", "startOffset": 242, "endOffset": 245}, {"referenceID": 7, "context": "Sparse representation models [1], [2], [3], [4] and related machine learning algorithms [5], [6] have proven remarkably successful at extracting useful information from complex highdimensional signals, for example in the context of denoising [7], under-determined source separation [8], [9], compressed sensing [10], super-resolution sensing [11], and classification [12].", "startOffset": 282, "endOffset": 285}, {"referenceID": 8, "context": "Sparse representation models [1], [2], [3], [4] and related machine learning algorithms [5], [6] have proven remarkably successful at extracting useful information from complex highdimensional signals, for example in the context of denoising [7], under-determined source separation [8], [9], compressed sensing [10], super-resolution sensing [11], and classification [12].", "startOffset": 287, "endOffset": 290}, {"referenceID": 9, "context": "Sparse representation models [1], [2], [3], [4] and related machine learning algorithms [5], [6] have proven remarkably successful at extracting useful information from complex highdimensional signals, for example in the context of denoising [7], under-determined source separation [8], [9], compressed sensing [10], super-resolution sensing [11], and classification [12].", "startOffset": 311, "endOffset": 315}, {"referenceID": 10, "context": "Sparse representation models [1], [2], [3], [4] and related machine learning algorithms [5], [6] have proven remarkably successful at extracting useful information from complex highdimensional signals, for example in the context of denoising [7], under-determined source separation [8], [9], compressed sensing [10], super-resolution sensing [11], and classification [12].", "startOffset": 342, "endOffset": 346}, {"referenceID": 11, "context": "Sparse representation models [1], [2], [3], [4] and related machine learning algorithms [5], [6] have proven remarkably successful at extracting useful information from complex highdimensional signals, for example in the context of denoising [7], under-determined source separation [8], [9], compressed sensing [10], super-resolution sensing [11], and classification [12].", "startOffset": 367, "endOffset": 371}, {"referenceID": 12, "context": "However, the dictionary learning problem is NP-hard and it is also hard to find approximate solutions near the optimal sparsity level [13].", "startOffset": 134, "endOffset": 138}, {"referenceID": 13, "context": "The development of dictionary learning methods was stimulated by results presented in the mid \u201990s by Olshausen and Field [14], [15], which demonstrate that atoms similar to the receptive fields of cells in visual cortex can be learned from natural images by imposing a few general optimization conditions, including sparsity and statistical independence of atoms.", "startOffset": 122, "endOffset": 126}, {"referenceID": 1, "context": "Since that time several probabilistic dictionary learning and sparse coding methods have been developed [2], [5], aiming for a dictionary that either maximizes the", "startOffset": 104, "endOffset": 107}, {"referenceID": 4, "context": "Since that time several probabilistic dictionary learning and sparse coding methods have been developed [2], [5], aiming for a dictionary that either maximizes the", "startOffset": 109, "endOffset": 112}, {"referenceID": 14, "context": "likelihood of the data, as for example in [16], or the posterior probability of the dictionary, as in [17].", "startOffset": 42, "endOffset": 46}, {"referenceID": 15, "context": "likelihood of the data, as for example in [16], or the posterior probability of the dictionary, as in [17].", "startOffset": 102, "endOffset": 106}, {"referenceID": 16, "context": "Recent developments include extensions of dictionary learning methods to distributed systems [18] and low-power hardware [19].", "startOffset": 93, "endOffset": 97}, {"referenceID": 17, "context": "Recent developments include extensions of dictionary learning methods to distributed systems [18] and low-power hardware [19].", "startOffset": 121, "endOffset": 125}, {"referenceID": 18, "context": "For instance, neurons demonstrate a form of homeostatic adaptation of the functional properties of the network to the ongoing changes in the statistical structure of the sensory input [20], [21], [22], which may be related to optimal encoding [23] of the input by dynamic adaptation of the receptive fields.", "startOffset": 184, "endOffset": 188}, {"referenceID": 19, "context": "For instance, neurons demonstrate a form of homeostatic adaptation of the functional properties of the network to the ongoing changes in the statistical structure of the sensory input [20], [21], [22], which may be related to optimal encoding [23] of the input by dynamic adaptation of the receptive fields.", "startOffset": 190, "endOffset": 194}, {"referenceID": 20, "context": "For instance, neurons demonstrate a form of homeostatic adaptation of the functional properties of the network to the ongoing changes in the statistical structure of the sensory input [20], [21], [22], which may be related to optimal encoding [23] of the input by dynamic adaptation of the receptive fields.", "startOffset": 196, "endOffset": 200}, {"referenceID": 21, "context": "For instance, neurons demonstrate a form of homeostatic adaptation of the functional properties of the network to the ongoing changes in the statistical structure of the sensory input [20], [21], [22], which may be related to optimal encoding [23] of the input by dynamic adaptation of the receptive fields.", "startOffset": 243, "endOffset": 247}, {"referenceID": 22, "context": "Furthermore, there is a notion that homeostatic mechanisms serve to maintain the dynamics of cortical networks at a critical point [24], [25] where the dynamic ar X iv :1 61 1.", "startOffset": 131, "endOffset": 135}, {"referenceID": 23, "context": "Furthermore, there is a notion that homeostatic mechanisms serve to maintain the dynamics of cortical networks at a critical point [24], [25] where the dynamic ar X iv :1 61 1.", "startOffset": 137, "endOffset": 141}, {"referenceID": 24, "context": "range and information processing capacity are optimal [26], [27].", "startOffset": 54, "endOffset": 58}, {"referenceID": 25, "context": "range and information processing capacity are optimal [26], [27].", "startOffset": 60, "endOffset": 64}, {"referenceID": 26, "context": "The starting point is the dictionary learning method introduced by Smith and Lewicki [28], where the sparse code is generated with Matching Pursuit (MP) [29], [30] and the shiftinvariant dictionary is updated with probabilistic gradient ascent on the likelihood of the data [16].", "startOffset": 85, "endOffset": 89}, {"referenceID": 27, "context": "The starting point is the dictionary learning method introduced by Smith and Lewicki [28], where the sparse code is generated with Matching Pursuit (MP) [29], [30] and the shiftinvariant dictionary is updated with probabilistic gradient ascent on the likelihood of the data [16].", "startOffset": 153, "endOffset": 157}, {"referenceID": 28, "context": "The starting point is the dictionary learning method introduced by Smith and Lewicki [28], where the sparse code is generated with Matching Pursuit (MP) [29], [30] and the shiftinvariant dictionary is updated with probabilistic gradient ascent on the likelihood of the data [16].", "startOffset": 159, "endOffset": 163}, {"referenceID": 14, "context": "The starting point is the dictionary learning method introduced by Smith and Lewicki [28], where the sparse code is generated with Matching Pursuit (MP) [29], [30] and the shiftinvariant dictionary is updated with probabilistic gradient ascent on the likelihood of the data [16].", "startOffset": 274, "endOffset": 278}, {"referenceID": 28, "context": "In addition to MP we consider dictionary learning with Orthogonal MP (OMP) [30], [31], which in the case of shift-invariant dictionaries is applicable to high-dimensional signals in the form of local OMP [32].", "startOffset": 75, "endOffset": 79}, {"referenceID": 29, "context": "In addition to MP we consider dictionary learning with Orthogonal MP (OMP) [30], [31], which in the case of shift-invariant dictionaries is applicable to high-dimensional signals in the form of local OMP [32].", "startOffset": 81, "endOffset": 85}, {"referenceID": 30, "context": "In addition to MP we consider dictionary learning with Orthogonal MP (OMP) [30], [31], which in the case of shift-invariant dictionaries is applicable to high-dimensional signals in the form of local OMP [32].", "startOffset": 204, "endOffset": 208}, {"referenceID": 12, "context": "This problem is NP-hard [13] and cannot be solved explicitly.", "startOffset": 24, "endOffset": 28}, {"referenceID": 31, "context": "two-step iteration process is used [33]: A) Encoding step; optimize the sparse representation of the signal x(t) with MP or OMP and a constant dictionary \u03a6.", "startOffset": 35, "endOffset": 39}, {"referenceID": 30, "context": "MP optimizes the parameters \u03c4i,j and ai,j of the most recently selected atom, while local OMP [32] re-optimizes all ai,j for selected atoms with overlapping support in each iteration (OMP compensates for the interference between atom instances).", "startOffset": 94, "endOffset": 98}, {"referenceID": 30, "context": "Our implementation of MP and OMP are based on efficient computational methods like those described in [32] and [34].", "startOffset": 102, "endOffset": 106}, {"referenceID": 32, "context": "Our implementation of MP and OMP are based on efficient computational methods like those described in [32] and [34].", "startOffset": 111, "endOffset": 115}, {"referenceID": 28, "context": "The atom selection rule defined above is optimal for sparse decomposition of a signal with a constant dictionary [30], but it does not imply optimal dictionary learning.", "startOffset": 113, "endOffset": 117}, {"referenceID": 30, "context": "The resulting four matching pursuits are summarized in Table I and Algorithm 1, which is a straightforward extension of the local OMP algorithm presented in [32] to equiprobable atom selection.", "startOffset": 157, "endOffset": 161}, {"referenceID": 14, "context": "that is generated by the matching pursuit [16], [28], [29].", "startOffset": 42, "endOffset": 46}, {"referenceID": 26, "context": "that is generated by the matching pursuit [16], [28], [29].", "startOffset": 48, "endOffset": 52}, {"referenceID": 27, "context": "that is generated by the matching pursuit [16], [28], [29].", "startOffset": 54, "endOffset": 58}, {"referenceID": 26, "context": "Under these assumptions the atoms can be optimized by performing gradient ascent on the approximate log data probability [28].", "startOffset": 121, "endOffset": 125}, {"referenceID": 33, "context": "1 kHz rock music track with lyrics [35] and the second signal is a 26 seconds long 48 kHz recording of Zebra Finch song phrases [36].", "startOffset": 35, "endOffset": 39}, {"referenceID": 34, "context": "1 kHz rock music track with lyrics [35] and the second signal is a 26 seconds long 48 kHz recording of Zebra Finch song phrases [36].", "startOffset": 128, "endOffset": 132}, {"referenceID": 26, "context": "We extend a well-known dictionary learning and sparse representation model [28] with a basic homeostatic regulation mechanism.", "startOffset": 75, "endOffset": 79}, {"referenceID": 35, "context": "that the information entropy of such sparse representations is sub-optimal by construction, and partially also by the central role of homeostatic regulation in cortical networks and spiking neural network models of sensory areas, see for example [37], [38].", "startOffset": 246, "endOffset": 250}, {"referenceID": 36, "context": "that the information entropy of such sparse representations is sub-optimal by construction, and partially also by the central role of homeostatic regulation in cortical networks and spiking neural network models of sensory areas, see for example [37], [38].", "startOffset": 252, "endOffset": 256}, {"referenceID": 28, "context": "The sparse representations are generated with Matching Pursuit (MP) [30], Local Orthogonal MP (OMP) [32] and the homeostatic extensions Equiprobable MP (E\u2013MP) and Equiprobable OMP (E\u2013OMP) introduced here.", "startOffset": 68, "endOffset": 72}, {"referenceID": 30, "context": "The sparse representations are generated with Matching Pursuit (MP) [30], Local Orthogonal MP (OMP) [32] and the homeostatic extensions Equiprobable MP (E\u2013MP) and Equiprobable OMP (E\u2013OMP) introduced here.", "startOffset": 100, "endOffset": 104}, {"referenceID": 37, "context": "In principle the equiprobable selection mechanism resembles a dropout [39] mechanism where the probability of dropout dynamically depends on the selection rate of each atom.", "startOffset": 70, "endOffset": 74}], "year": 2016, "abstractText": "Sparse signal representations based on linear combinations of learned atoms have been used to obtain state-ofthe-art results in several practical signal processing applications. Approximation methods are needed to process high-dimensional signals in this way because the problem to calculate optimal atoms for sparse coding is NP-hard. Here we study greedy algorithms for unsupervised learning of dictionaries of shiftinvariant atoms and propose a new method where each atom is selected with the same probability on average, which corresponds to the homeostatic regulation of a recurrent convolutional neural network. Equiprobable selection can be used with several greedy algorithms for dictionary learning to ensure that all atoms adapt during training and that no particular atom is more likely to take part in the linear combination on average. We demonstrate via simulation experiments that dictionary learning with equiprobable selection results in higher entropy of the sparse representation and lower reconstruction and denoising errors, both in the case of ordinary matching pursuit and orthogonal matching pursuit with shift-invariant dictionaries. Furthermore, we show that the computational costs of the matching pursuits are lower with equiprobable selection, leading to faster and more accurate dictionary learning algorithms.", "creator": "LaTeX with hyperref package"}}}