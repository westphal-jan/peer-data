{"id": "1611.09345", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Nov-2016", "title": "Unifying Multi-Domain Multi-Task Learning: Tensor and Neural Network Perspectives", "abstract": "Multi-domain learning aims to benefit from simultaneously learning across several different but related domains. In this chapter, we propose a single framework that unifies multi-domain learning (MDL) and the related but better studied area of multi-task learning (MTL). By exploiting the concept of a \\emph{semantic descriptor} we show how our framework encompasses various classic and recent MDL/MTL algorithms as special cases with different semantic descriptor encodings. As a second contribution, we present a higher order generalisation of this framework, capable of simultaneous multi-task-multi-domain learning. This generalisation has two mathematically equivalent views in multi-linear algebra and gated neural networks respectively. Moreover, by exploiting the semantic descriptor, it provides neural networks the capability of zero-shot learning (ZSL), where a classifier is generated for an unseen class without any training data; as well as zero-shot domain adaptation (ZSDA), where a model is generated for an unseen domain without any training data. In practice, this framework provides a powerful yet easy to implement method that can be flexibly applied to MTL, MDL, ZSL and ZSDA.", "histories": [["v1", "Mon, 28 Nov 2016 20:59:18 GMT  (1309kb,D)", "http://arxiv.org/abs/1611.09345v1", "Invited book chapter"]], "COMMENTS": "Invited book chapter", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["yongxin yang", "timothy m hospedales"], "accepted": false, "id": "1611.09345"}, "pdf": {"name": "1611.09345.pdf", "metadata": {"source": "CRF", "title": "Unifying Multi-Domain Multi-Task Learning: Tensor and Neural Network Perspectives", "authors": ["Yongxin Yang", "Timothy M. Hospedales"], "emails": ["yongxin.yang@qmul.ac.uk,", "t.hospedales@qmul.ac.uk"], "sections": [{"heading": "1 Introduction", "text": "In fact, the fact is that most of them are able to survive themselves, and that they are able to survive themselves, \"he told the Deutsche Presse-Agentur in an interview with\" Welt am Sonntag \":\" I don't think we will be able to change the world. \"Indeed,\" I don't think we will be able to change the world. \""}, {"heading": "2 Methodology \u2013 Single Output Models", "text": "We start from the linear model assumption based on the most common MTL / MDL methods [Argyriou et al., 2008, Ji and Ye, 2009] - i.e. that a domain / task corresponds to a univariate linear regression or binary classification problem. In the case where a domain / task requires more than that (e.g. a multi-class problem), Section 3. We also assume that the domains / tasks are homogeneous, i.e. that different domains / tasks have model representations and instance function vectors of the same size."}, {"heading": "2.1 Formulation: Vector-Matrix Inner Product", "text": "Suppose we have M domains (tasks), and the ith domain has Ni-q instances. We denote the D-dimensional attribute vector of the jth instance in the ith domain (task) and the associated B-dimensional semantic descriptor12 by the pair {x (i) j, z (i)} Nij = 1} M i = 1 and the associated designation {{y (i) j} Ni j = 1. All weight vectors w (task) i can be stacked into a single weight matrix W (task). Without loss of generality, we minimize the empirical risk for all domains (tasks), argmin W \u00b2 1 MM \u00b2 i = 1 (1 Ni Ni \u00b2 j = 1 (task) j (i) j (i) j (i) j (i), y (i) j))."}, {"heading": "2.2 Semantic Descriptor Design", "text": "In this case, however, the length of the descriptor and the number of unique domains (tasks) is equal to B = 1, z (2) T, z (3) = 2, z (2) T if there are M = 3 domains / tasks. In this case, the length of the descriptor and the number of unique domains (tasks) is equal to B = 1, and the stack of all zi vectors (denotes Z = 2), z (2). The length of the task and the number of unique domains (tasks) are equal to B = M, and the stack of all zi vectors (denotes Z = 1), z (2). The meaning of the domain Z = 2 is an M \u00d7 M identity matrix. Learning the \"weight generation function\" is then equivalent to independent learning per domain."}, {"heading": "2.3 Unification of Existing Algorithms", "text": "In addition to enforcing local and common components [Evgeniou and Pontil, 2004, thumb \u0301 III, 2007] (which we interpret as a uniform + constant descriptor), many studies achieve information exchange by enforcing a lower factorization structure on W (the D \u00d7 T stack of weight vectors for each task). Popular options for modeling W \u0432 are: (i) adding a regulation that encourages being a low-level matrix (ii) being an explicit low-level factorization."}, {"heading": "2.4 A Two-sided Network View", "text": "By replacing Eq. 6 with Eq. 2, we see that the prediction is given by i) j = x (i) jT PQz (i) (7). A mathematically equivalent neural network model is illustrated in Fig. 3. The introduction of this NN leads to a better understanding of our framework and the methods it contains. The left side can be understood as a global representational learning network - xT P - and the right side can be considered as a network that generates a model (the weight vector zT QT) for the task / domain encoded by z. This interpretation enables existing MTL / MDL models to be implemented as neural networks - and to be conveniently optimized by standard neural network libraries: by setting inputs appropriately and activating appropriate weight regulation (Table 1)."}, {"heading": "2.5 Application to Zero-Shot Learning", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.5.1 Zero-Shot Recognition", "text": "It has been extensively studied in areas such as character [Larochelle et al., 2008] and object recognition [Lampert et al., 2009, Socher et al., 2013] Most existing ZSL methods follow data flows constructed either as: X \u2192 Z \u2192 Y [Palatucci et al., 2009] or Z + + X / / Y [Larochelle et al., 2008, Frome et al., 2013], where Z are some \"semantic descriptors,\" e.g. attributes [Lampert et al., 2009] or semantic word vectors [Socher et al., 2013]. In our framework, SL et al., where Z are some \"semantic descriptors,\" e.g.: attributes [Lampert et al., 2009] or semantic word vectors [Socher et al., 2013]."}, {"heading": "2.5.2 Zero-Shot Domain Adaptation", "text": "Beyond the traditional ZSL, we generalize the concept of zero-shot learning of tasks to zero-shot learning of domains. In this context, zero-shot means that no training data for the target domain was seen before testing. The challenge is to construct a good model for a novel test domain based solely on its semantic descriptor [Yang and Hospedales, 2015, Yang and Hospedales, 2016b]. We call this problem a zero-shot domain adaptation4 (ZSDA) ZSDA is possible with a distributed and not a unified domain descriptor, as in practice only a subset of domains is needed to effectively learn Qs. Thus, a model w (\u0435) suitable for an invisible domain can be constructed without training data - by applying its semantic descriptor z (\u0445) to the model matrix (w = specific domain) that is actually created."}, {"heading": "3 Methodology \u2013 Multiple Output Models", "text": "For example, suppose we have M = 2-digit datasets (domains): MNIST and USPS. For each MNIST or USPS image, a D-dimensional character vector is extracted. The task is to classify the image from 0 to 9, and therefore we have D \u00b7 C (C = 10) model parameter for each dataset. Therefore, the complete model for all digits and datasets should contain D \u00b7 C \u00b7 M parameters. We refer to this setting of multiple domains, each of which has multiple tasks, as multi-domain multi-task learning. In some recent literature [Romera Paredes et al., 2013], a similar setting is called multi-task multilinear learning."}, {"heading": "3.1 Formulation: Vector-Tensor Inner Product", "text": "To adapt this idea to this new situation, we propose W (i) = f (z (i)) = W \u00b7 3 z (i) (8), where \u00d7 n indicates the n-mode product of a tensor and vector (this is also called tensor contraction in some studies, since it is a generalization of the internal product for vectors and / or matrices).The generated model is now a weight matrix W (i) and not a vector w (i).The weight generation function is now parameterized by a third-order tensor W of size D \u00b7 C \u00b7 B, and it synthesizes the model matrix for the ith domain by matching the tensor W (i) with its B-dimensional semantic sliptor z (i)."}, {"heading": "3.2 Tensor (De)composition", "text": "Remember that the key intuition of many classic (matrix-based) multi-task learning methods is to use the information exchange triggered by the factorisation of series rankings, i.e. the composition of the weight matrix for all tasks W, of factor matrices P and Q. For multi-output MDL, we aim to extend this idea to the factorisation of the weight sensor W. Unlike the matrices, there are several approaches to factorisation of tensors, including CP [Hitchcock, 1927], Tucker [Tucker, 1966] and Tensor-Train [Oseledets, 2011] decompositions."}, {"heading": "3.2.1 CP decomposition", "text": "For a third-order tensor W of size D \u00b7 C \u00b7 B, however, the decomposition of rank K CP is: (Wd, c, b = K \u2211 k = 1U (D) k, d U (C) k, c U (B) k, b (10) W = K \u2211 k = 1U (D) k, \u00b7 U (C) k, \u00b7 U (B) k, \u00b7 (11), where it is an external product. The factor matrices U (D), U (C) and U (B) have the respective sizes K \u00b7 D, K \u00b7 C and K \u00b7 B. If a data point x and its associated descriptor z5, a C-dimensional vector y (e.g. C = 10 is the values of 10 digits for the MNIST / USPS example) is obtained from Eq. 11 in equivalent. 9 and a certain reorganization, we obtain signature z (C) as a signature value for the MS / IST value (e.g. VT = NSPY)."}, {"heading": "3.2.2 Tucker decomposition", "text": "With the same magnitude tensor W, Tucker outputs a core tensor S of size KD \u00b7 KC \u00b7 KB and 3 matrices U (D) of size KD \u00b7 D, U (C) of size KC \u00b7 C and U (B) of size KB \u00b7 B, so that, for example, we get the prediction x in domain / task zy = (U (D) kB, b (15) W = S \u00b7 1U (D) \u00d7 2U (C) \u00d7 3U (B) (16) if we replace equation 16 with equation 9."}, {"heading": "3.2.3 TT decomposition", "text": "With the same tensor W, tensor-train (TT) decomposition produces two matrices U (D) of size D \u00b7 KD and U (B) of size KB \u00b7 B and a tensor of third order S of size KD \u00b7 C \u00b7 KB, so that Wd, c, b = KD \u2211 kD = 1KB \u2211 kB = 1 U (D) d, kDSkD, c, kBU (B) kB, b, (19) W = S \u00b7 1U (D) T \u00b7 3U (B). (20) Replacing equation 20 in equation 9, one obtains the MDL / MTL prediction = (U (D) Tx) (U (B) z))) S T (2) (21), where S (2) is the unfolding of S, which is a C \u00b7 KDKB matrix, and its transpose S T (2) is a matrix of size KD (2) (21), where S (2) is the unfolding of T (T)."}, {"heading": "3.3 Gated Neural Network Architectures", "text": "This year it is more than ever before."}, {"heading": "4 Experiments", "text": "We will examine three types of experiments on object recognition (Section 4.1), surveillance image analysis (Section 4.2), and person recognition / soft biometrics (Section 4.3). The first detection experiment follows the traditional definition of domains / tasks as atomic units, and the last experiments explore the potential benefits of informative domain descriptors, including zero-shot domain adaptation. Implementation We implement our framework with TensorFlow [Abadi et al., 2015], taking over the neural network interpretation of each method, allowing easy optimization with SGD-based back propagation."}, {"heading": "4.1 Multi-domain Multi-task Object Recognition", "text": "In this area, we assume conventional atomic domains (i.e. domain descriptors are simply indicators rather than distributed codes), but we are exploring a multi-domain multi-task setting (MDMTL). Thus, there is a multi-class problem within each domain, and our method (Section 3) uses the exchange of information about both domains and tasks. To deal with multi-class recognition within each domain, it generalizes the existing vector MTL methods and implements a matrix-generated function (Section 3)."}, {"heading": "4.2 Surveillance Image Classification", "text": "In monitoring analysis, there are many covariants such as season, working day / holiday and day / night. Each of these factors influences the distribution of image functions and therefore introduces a domain shift. In this section, we examine the potential for multi-domain learning with distributed domain descriptors (Section 2) to improve performance by modelling the factorial structure in domain shift. Furthermore, we show the potential of ZSDA to apply a system to adapt to new conditions for which no training data.Data We consider the monitoring image classification proposed by [Hoffman et al., 2014]. This is a binary classification of each frame in a 12-day monitoring stream as empty or containing cars."}, {"heading": "4.3 Gait-based Soft-Biometrics and Recognition", "text": "However, they are a challenge, especially when there are multiple covariants such as viewing angle and accessory status (e.g. object transmission). In contrast, forming a model for each covariate combination could facilitate the use of a camera with a calibration step to specify covariates such as viewing angle, but no data acquisition or retraining. Data we study using CASIA gait analysis datasets could facilitate the use of a camera with a single calibration step to specify covariates such as viewing angle, but no data acquisition or retraining. Applying our framework to this setting using the CASIA gait analysis dataset [Zheng et al., 2011] with data from 124 individuals at 11 viewing angles. Each person has three situations: normal (\"nm\") and wears a coat (\"bg\")."}, {"heading": "5 Conclusion", "text": "In this chapter, we discussed multi-domain learning, a bidirectional generalization of domain matching, and zero-shot domain matching, an alternative to domain generalization approaches. We introduced a semantic domain / task descriptor to unify various existing multi-task / multi-domain algorithms within a single matrix factor framework. In order to go beyond the single output problems considered by previous methods, we generalized this framework to tensor factorization, enabling knowledge exchange for methods parameterized by matrices and not by vectors. This allows multi-domain learning for multi-output problems or concurrent multi-task multi-domain learning. All of these approaches turn out to be equivalent interpretations as neural networks that allow easy implementation and optimization of this to-end-promising network]."}], "references": [], "referenceMentions": [], "year": 2016, "abstractText": "Multi-domain learning aims to benefit from simultaneously learning across several different but related domains. In this chapter, we propose a single framework that unifies multi-domain learning (MDL) and the related but better studied area of multi-task learning (MTL). By exploiting the concept of a semantic descriptor we show how our framework encompasses various classic and recent MDL/MTL algorithms as special cases with different semantic descriptor encodings. As a second contribution, we present a higher order generalisation of this framework, capable of simultaneous multi-task-multi-domain learning. This generalisation has two mathematically equivalent views in multi-linear algebra and gated neural networks respectively. Moreover, by exploiting the semantic descriptor, it provides neural networks the capability of zero-shot learning (ZSL), where a classifier is generated for an unseen class without any training data; as well as zero-shot domain adaptation (ZSDA), where a model is generated for an unseen domain without any training data. In practice, this framework provides a powerful yet easy to implement method that can be flexibly applied to MTL, MDL, ZSL and ZSDA.", "creator": "LaTeX with hyperref package"}}}