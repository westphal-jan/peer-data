{"id": "1611.09405", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Nov-2016", "title": "An End-to-End Architecture for Keyword Spotting and Voice Activity Detection", "abstract": "We propose a single neural network architecture for two tasks: on-line keyword spotting and voice activity detection. We develop novel inference algorithms for an end-to-end Recurrent Neural Network trained with the Connectionist Temporal Classification loss function which allow our model to achieve high accuracy on both keyword spotting and voice activity detection without retraining. In contrast to prior voice activity detection models, our architecture does not require aligned training data and uses the same parameters as the keyword spotting model. This allows us to deploy a high quality voice activity detector with no additional memory or maintenance requirements.", "histories": [["v1", "Mon, 28 Nov 2016 22:03:22 GMT  (42kb,D)", "http://arxiv.org/abs/1611.09405v1", "NIPS 2016 End-to-End Learning for Speech and Audio Processing Workshop"]], "COMMENTS": "NIPS 2016 End-to-End Learning for Speech and Audio Processing Workshop", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["chris lengerich", "awni hannun"], "accepted": false, "id": "1611.09405"}, "pdf": {"name": "1611.09405.pdf", "metadata": {"source": "CRF", "title": "An End-to-End Architecture for Keyword Spotting and Voice Activity Detection", "authors": ["Chris Lengerich", "Awni Hannun"], "emails": ["chris@mindori.com", "awni@mindori.com"], "sections": [{"heading": "1 Introduction", "text": "Keyword spotting (KWS) is a language task that requires the recognition of a specific word in an audio signal that is commonly used as the \"wake-up call\" of a speech recognition system with a large vocabulary (LV). Voice Activity Detection (VAD) requires the recognition of human speech in the signal, often for the purpose of end-time recognition in a large speech recognition system. Both tasks are challenging due to computational constraints and noisy environments. To limit computing costs, VAD models often rely on handcrafted functions and require training separate from KWS models. Instead, we propose a single end-to-end neural network architecture for both KWS and VAD. We develop novel inference algorithms that allow us to perform KWS and VAD tasks without retraining. Our model surpasses both baselines, is trained only on unaligned character levels, and requires training for only one architecture."}, {"heading": "2 Related Work", "text": "In this thesis, we develop the model and follow-up procedure for the CHPS and VAD tasks. A thorough discussion of the advantages of this model for LVCSR can be found in [1]. A CTC architecture at the character level was also recently adopted for keyword spotting [10], where it exceeded a DNN-HMM baseline, while a CTC architecture at the word level was used for keyword spotting in [5]. Traditional VAD architectures trade accuracy for low computing costs, as they have historically been developed for very low resource environments. However, some simple and efficient techniques include a threshold for the energy of the audio signal, a threshold for the number of zeros [11] or combinations of these features, as they are not normally robust to non-stationary environments."}, {"heading": "3 Model", "text": "For a general keyword spotter, we model p (k | x), where k is a keyword and x is a window of speech. For VAD, we use the same distribution and simply put k on the empty string.We use the Connectionist Temporal Classification [6] (CTC) objective function to train an RNN on a corpus of expression and transcription pairs. The CTC target gives us the probability of an arbitrary label string for a given utterance. We do not need an alignment, since CTC efficiently calculates the number of dots over all possible alignments. The objective function for an utterance x and the corresponding transcription \"is aligned bypCTC ('| x) = \u2211 s task (', T) T-t p (st | x). (1) The alignment (\u00b7) function calculates the amount of possible alignments of speech over the T-time steps of the keyword operator CTC string operator, minus the string of the string of the W, we allow the combination of words and the W to determine."}, {"heading": "3.1 Network Architecture", "text": "The network accepts as input a spectrogram calculated from the raw waveform sampled at 8kHz. The first layer is a two-dimensional folding with a step of three [1]. For the next three layers of the network we use gated recurrent RNN layers [3] [4]. The last layer is a one-dimensional affine transformation followed by a softmax. The network directly outputs letters in the alphabet, including spaces and spaces."}, {"heading": "3.2 Inference", "text": "In order to reduce the sensitivity of the algorithm to the parameter window size, we propose a modification of the CTC scoring algorithm shown above. Instead of scoring k itself according to the model, for a certain keyword k, we evaluate the regular expression [\u0435k0] * k [\u0435kn \u2212 1] *, where k0 or kn \u2212 1 are the first or last character of k, respectively. This is described in algorithm 1. Calculation of the VAD score is reduced to summing the log probabilities of the empty character above the window of the language frame: log p (speech | xt: t + w) = 1 \u2212 t + w \u2211 i = t log pi (| xt: t + w) (2)."}, {"heading": "4 Experiments", "text": "The model parameters are optimized with stochastic gradient drop for 50 epochs and a minibatch size of 256. We sort examples so that the minibatch consists of expressions of similar length for computing power. Learning rate and impulse parameters are chosen to optimize the speed of convergence. We use 32 filters in all models. Algorithm 1 Calculate the score of a keyword, k, taking into account the CTC output probabilities, P. The parameter l is the keyword inserted at the beginning, end, and between each pair of letters of k. Function SCOREKEYWORD (l, P) \u2212 p \u2212 p is the keyword if it is inserted at the beginning, end, and between each pair of letters of k. Function SCOREKEYWORD (l, P) S \u00b2 Size (l) \u2212 p \u2212 p is the keyword if it is inserted at the end, and between each letter pair of k \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p if it is inserted at the end, ORK = \u03b1p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p if it is at the end."}, {"heading": "4.1 Data", "text": "The data used to train the model consists of two sets of data. The first set of data consists of a corpus of 526K transcribed expressions collected on Android phones via an assistant-like application; the second set consists of 1544 spoken examples of the keyword, in this case \"Olivia.\" The model is trained on both sets of data at the same time. We do not need to prepare for the large corpus before fine-tuning it. We also use a collection of about one hundred hours of sounds and music downloaded from the Internet to generate synthetic loud examples of the keyword and empty noise clips. In training with the noisy data, we replicate each keyword ten times, each time with a random noise clip. We also use a corpus of 57K randomly sampled noise clips with an blank label as a filler. The KWS model is evaluated on a test set of 550 positive examples (e.g. with the keyword \"Olivia\" and a large label containing 5000 examples)."}, {"heading": "4.2 Results", "text": "Our KWS base line is a DNN keyword spotter from kitt.ai 2. Our VAD base line is the WebRTC VAD codec3 with an image size of 30ms. Our model of 3 layers of size 256 outperforms both baselines. 2https: / / github.com / putty-AI / snowboy 3https: / / github.com / wiseman / py-webrtcvadAt a fixed false positive rate of 5%, our model achieves a real positive rate of 98.1% keyword spotting compared to baseline 96.2%. For the VAD task, our model achieves a true positive rate of 99.8% versus 44.6% for baseline at the same false positive rate. This large delta can be attributed to the considerable difference in the representation power of a large-parameter neural model compared to the small parameter size of VM planes, as well as differences in the type and volume of the training data."}, {"heading": "5 Conclusion", "text": "The model is easy to learn and, unlike previous VAD models, does not require alignment or frame-by-frame labeling. We propose modified inference algorithms for KWS and VAD from the basic CTC scoring algorithm that allow the model to perform both tasks. Although our model is efficient, the use of neural compression techniques could further enhance performance and is an interesting area for future work."}], "references": [{"title": "Deep speech 2: End-to-end speech recognition in english and mandarin", "author": ["D. Amodei", "R. Anubhai", "E. Battenberg", "C. Case", "J. Casper", "B. Catanzaro", "J. Chen", "M. Chrzanowski", "A. Coates", "G. Diamos", "E. Elsen", "J. Engel", "L. Fan", "C. Fougner", "T. Han", "A.Y. Hannun", "B. Jun", "P. LeGresley", "L. Lin", "S. Narang", "A.Y. Ng", "S. Ozair", "R. Prenger", "J. Raiman", "S. Satheesh", "D. Seetapun", "S. Sengupta", "Y. Wang", "Z. Wang", "C. Wang", "B. Xiao", "D. Yogatama", "J. Zhan", "Z. Zhu"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2016}, {"title": "Small-footprint keyword spotting using deep neural networks", "author": ["G. Chen", "C. Parada", "G. Heigold"], "venue": "In ICASSP,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2014}, {"title": "Learning phrase representations using RNN encoder-decoder for statistical machine translation", "author": ["K. Cho", "B. van Merrienboer", "\u00c7. G\u00fcl\u00e7ehre", "F. Bougares", "H. Schwenk", "Y. Bengio"], "venue": "In EMNLP,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2014}, {"title": "Empirical evaluation of gated recurrent neural networks on sequence modeling", "author": ["J. Chung", "\u00c7. G\u00fcl\u00e7ehre", "K. Cho", "Y. Bengio"], "venue": "In NIPS Deep Learning Workshop,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2014}, {"title": "An application of recurrent neural networks to discriminative keyword spotting", "author": ["S. Fern\u00e1ndez", "A. Graves", "J. Schmidhuber"], "venue": "In Proceedings of the 17th International Conference on Artificial Neural Networks,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2007}, {"title": "Connectionist temporal classification: Labelling unsegmented sequence data with recurrent neural networks", "author": ["A. Graves", "S. Fern\u00e1ndez", "F. Gomez", "J. Schmidhuber"], "venue": "In ICML,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2006}, {"title": "Towards End-to-End Speech Recognition with Recurrent Neural Networks", "author": ["A. Graves", "N. Jaitly"], "venue": "In ICML,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2014}, {"title": "Deep speech: Scaling up end-to-end speech recognition", "author": ["A.Y. Hannun", "C. Case", "J. Casper", "B. Catanzaro", "G. Diamos", "E. Elsen", "R. Prenger", "S. Satheesh", "S. Sengupta", "A. Coates", "A.Y. Ng"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2014}, {"title": "Recurrent neural networks for voice activity detection", "author": ["T. Hughes", "K. Mierle"], "venue": "In ICASSP,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2013}, {"title": "Online keyword spotting with a character-level recurrent neural network", "author": ["K. Hwang", "M. Lee", "W. Sung"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2015}, {"title": "A study of endpoint detection algorithms in adverse conditions: incidence on a dtw and hmm recognizer", "author": ["J.-C. Junqua", "B. Reaves", "B. Mak"], "venue": "In EUROSPEECH,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1991}], "referenceMentions": [{"referenceID": 6, "context": "The model is based on work in end-to-end speech recognition which uses the Connectionist Temporal Classification loss function coupled with deep Recurrent Neural Networks [7, 8].", "startOffset": 171, "endOffset": 177}, {"referenceID": 7, "context": "The model is based on work in end-to-end speech recognition which uses the Connectionist Temporal Classification loss function coupled with deep Recurrent Neural Networks [7, 8].", "startOffset": 171, "endOffset": 177}, {"referenceID": 0, "context": "A thorough treatment of the benefits of this model for LVCSR is given in [1].", "startOffset": 73, "endOffset": 76}, {"referenceID": 9, "context": "A character-level CTC architecture was also recently adopted for keyword spotting [10], where it outperformed a DNN-HMM baseline, while a word-level CTC architecture was used for keyword spotting in [5].", "startOffset": 82, "endOffset": 86}, {"referenceID": 4, "context": "A character-level CTC architecture was also recently adopted for keyword spotting [10], where it outperformed a DNN-HMM baseline, while a word-level CTC architecture was used for keyword spotting in [5].", "startOffset": 199, "endOffset": 202}, {"referenceID": 10, "context": "Some simple and efficient techniques include a threshold on the energy of the audio signal, a threshold on the number of zero-crossings [11] or combinations of these features, however, these methods are typically not robust to nonstationary environments.", "startOffset": 136, "endOffset": 140}, {"referenceID": 8, "context": "Neural architectures have been proposed for VAD, notably the RNN architecture in [9], however that approach relied on frame-aligned labels.", "startOffset": 81, "endOffset": 84}, {"referenceID": 5, "context": "We use the Connectionist Temporal Classification [6] (CTC) objective function to train an RNN on a corpus of utterance and transcription pairs.", "startOffset": 49, "endOffset": 52}, {"referenceID": 0, "context": "The first layer is a 2-dimensional convolution with a stride of three [1].", "startOffset": 70, "endOffset": 73}, {"referenceID": 2, "context": "For the next three layers of the network we use gated recurrent RNN layers [3][4].", "startOffset": 75, "endOffset": 78}, {"referenceID": 3, "context": "For the next three layers of the network we use gated recurrent RNN layers [3][4].", "startOffset": 78, "endOffset": 81}, {"referenceID": 1, "context": "5M trainable parameters, comparable to other neural network-based KWS approaches [2], and has been deployed to a modern smartphone.", "startOffset": 81, "endOffset": 84}], "year": 2016, "abstractText": "We propose a single neural network architecture for two tasks: on-line keyword spotting and voice activity detection. We develop novel inference algorithms for an end-to-end Recurrent Neural Network trained with the Connectionist Temporal Classification loss function which allow our model to achieve high accuracy on both keyword spotting and voice activity detection without retraining. In contrast to prior voice activity detection models, our architecture does not require aligned training data and uses the same parameters as the keyword spotting model. This allows us to deploy a high quality voice activity detector with no additional memory or maintenance requirements.", "creator": "LaTeX with hyperref package"}}}