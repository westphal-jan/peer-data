{"id": "1611.09441", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-Nov-2016", "title": "Sentiment Analysis for Twitter : Going Beyond Tweet Text", "abstract": "Analysing sentiment of tweets is important as it helps to determine the users' opinion. Knowing people's opinion is crucial for several purposes starting from gathering knowledge about customer base, e-governance, campaigning and many more. In this report, we aim to develop a system to detect the sentiment from tweets. We employ several linguistic features along with some other external sources of information to detect the sentiment of a tweet. We show that augmenting the 140 character-long tweet with information harvested from external urls shared in the tweet as well as Social Media features enhances the sentiment prediction accuracy significantly.", "histories": [["v1", "Tue, 29 Nov 2016 00:22:13 GMT  (22kb)", "http://arxiv.org/abs/1611.09441v1", null]], "reviews": [], "SUBJECTS": "cs.CL cs.SI", "authors": ["lahari poddar", "kishaloy halder", "xianyan jia"], "accepted": false, "id": "1611.09441"}, "pdf": {"name": "1611.09441.pdf", "metadata": {"source": "CRF", "title": "Sentiment Analysis for Twitter : Going Beyond Tweet Text", "authors": ["Lahari Poddar", "Kishaloy Halder", "Xianyan Jia"], "emails": ["jiaxiany}@comp.nus.edu.sg"], "sections": [{"heading": null, "text": "ar Xiv: 161 1.09 441v 1 [cs.C L] 29 Nov 201 6"}, {"heading": "1. INTRODUCTION", "text": "Analysis of the mood from the text is a well-known NLP problem. There are several state-of-the-art tools that can achieve this with reasonable accuracy. However, most existing tools work well with well-formatted text. In the case of tweets, the content generated by the user is short, loud and in many cases (about 30%) does not follow the correct grammatical structure. In addition, we are developing new features in this project to include the styles that stand out in short, informal tweets from the user. We achieve an F1 accuracy of 71.3% for predicting the mood of tweets in our data set. We also propose a method to discover new sentiments from the tweets. In Section 2, we present the analysis of the data set. We describe the data processing we have described in Section 4, how we tuned the system to accuracy in Section 3, Section 5."}, {"heading": "2. DATA-ANALYSIS", "text": "Due to the nature of this microblogging service (quick and short messages), people use abbreviations, make spelling mistakes, use emoticons and other characters that express special meanings. Below is a brief terminology associated with tweets: \u2022 Emoticons: These are facial expressions that are represented by punctuation and letters. \u2022 Url: Due to the brevity of the tweet, people use external links to refer to other users on the microblog. \u2022 Hashtags: Users frequently use hashtags to highlight topics, primarily to increase the visibility of their tweets. \u2022 Url: Due to the myopia of the tweet, people use external links to provide additional information (to support their tweet). \u2022 Our data set contains tweets about \"Obamacare Care\" in the United States, collected in March 2010. It is divided into three subsets (train, dev and test). Some tweets are manually commented on with one of the following classes: unsafe, unsafe, and unfavorable."}, {"heading": "3. DATA PRE-PROCESSING", "text": "Since tweets are informal in nature, some pre-processing is required. Consider the following tweet. \"# Healthcare # Ins. Cigna disputes # MD prescribed # tx"}, {"heading": "2 customers 20% of the time. - http://bit.ly/5PoQfo #HCR #Passit #ILDems #p2 PLS RT\u201d", "text": "It is difficult to understand the content of the tweet unless it is normalized. We process all tweets in the following steps."}, {"heading": "3.1 Normalization", "text": "Normalization occurs as follows: (1) Removal of patterns such as \"RT,\" \"@ user name,\" url. (2) Tokenizing tweet text using NLTK [1] word tokenizer. (3) Use of NLTK's stop word list to remove it from the tweet text. (4) Correction of informal / misspelled words using normalization dictionary [2]. For example, \"foundation\" for \"foudation,\" \"forgot\" for \"forgt.\" (5) Extension of abbreviations using slang dictionary. For example, \"btw\" is expanded to \"by the way.\" (6) Removal of emoticons. However, we keep the number of positive and negative emoticons in each tweet as a function. We use the Emoticon dictionary (Table 2) presented in [3]."}, {"heading": "3.2 Hashtag Segmentation", "text": "We segment a hashtag into meaningful English phrases. The \"#\" character is removed from the tweet text. For example, # killthebill is converted to kill the bill. To achieve this, we use a dictionary of English words. We recursively split the hashtagged phrase into segments and match the segments in the dictionary until we get a complete set of meaningful words. This is important because many users tend to post tweets that express the actual message of the tweet in the form of concise hashtagged phrases.1Slang Dictionary - Text Slang & Internet Slang Words. http: / / www.noslang.com / dictionary /"}, {"heading": "3.3 Processing URLs", "text": "The URLs embedded in the tweet are a good source of additional context to the actual short message content. Sometimes, tweets are too concise to understand simply because of the text content. However, if a URLs are embedded in the tweet, this can help us understand the context - perhaps the opinion expressed. To use this additional source of information, we identify all the URLs contained in the tweets and search the web pages using AlchemyAPI2. The API only retrieves the body of the article on a web page. We analyze the article texts later to get more context for the tweet."}, {"heading": "4. ALGORITHMIC FRAMEWORK", "text": "We use a supervised learning model that uses the manually labeled data as a training set and a collection of handmade features. In this section, we describe the features and classification model used in this task."}, {"heading": "4.1 Feature Extraction", "text": "Table 3 presents the features we use in our experiment. We have used some basic features (commonly used for text classification tasks) as well as some advanced features that are suitable for this particular area."}, {"heading": "4.1.1 Basic Features", "text": "We use two basic features: (1) Parts of the language (POS tags): We use NLTK's POS tagger to highlight Tweet texts. (2) Previous polarity of words: We use a polarity dictionary [4] to determine the previous polarity of words. The dictionary contains positive, negative and neutral words along with their polarity strength (weak or strong). The polarity of a word depends on its POS tag. For example, the word \"excuse\" is negative when used as a \"noun\" or \"adjective,\" but it has a positive meaning when used as a \"verb.\" We use the tags produced by NLTK postagger while selecting the previous polarity of a word from the dictionary. We also use stems (Porter Stemmer implementation of NLTK) as we perform the dictionary search to increase the number of positive, weak words and weak words in the dictionary."}, {"heading": "4.1.2 Advanced Features", "text": "(1) Emoticons: We use the emoticon dictionary of [3] and count the positive and negative emoticons for each tweet. (2) The mood of the tweets: Since almost all articles may be written in well-formatted English, we analyze the mood of the first paragraph of the article using2http: / / www.alchemyapi.com / apiTable 3: FeaturesBasic POS tag f1 # of noun, adj, adv, verbWord Polarity f2 # of Strong Positive Words, Strong Negative Words # of Weak Positive Words, Weak Negative WordsAdvanced Twitter specific f3 Whether the tweet is a retweet or not, mentions of the user emoticon f4 # positiveEmoticons, negativeEmoticons, negativeEmoticons f5 Break of the positive and senticons of the neutral Url page."}, {"heading": "4.2 Classifier", "text": "We are experimenting with the following set of classifiers for machine learning. We train the model with manually labeled data and use the characteristics described above to predict mood. We consider only positive, negative and neutral classes. (1) Naive Bayes: Naive Bayes has been one of the most widely used classifiers for text classification problems over the years. Naive Bayes classifier assumes that the value of a certain trait is independent of the value of another trait, since the class variable is predetermined. This assumption of independence makes the classifier both simple and scalable. Bayes classifier assigns a class name to a certain trait, following the equation: y = argmax k,..., K} p (Ck) n, i = 1p (xi | Ck). (1) Assumptions about the distribution of characteristics define the event model of the Naive Bayes classifier."}, {"heading": "4.3 Parameter Tuning", "text": "Parameter tuning or hyperparameter optimization is an important step in model selection as it prevents the model from overadjusting and optimizes the performance of a model on an independent dataset. We perform hyperparameter optimization by performing grid search, i.e. an exhaustive search through a manually defined subset of hyperparameter space for a learning algorithm. We perform grid search and set the \"best parameters\" by performing cross-validation on the training set and verifying the improvement in accuracy on the validation set. Finally, we use the model with the best hyperparameters to make predictions about the test set."}, {"heading": "5. EVALUATION AND ANALYSIS", "text": "Table 4 shows the test results when features are added step by step. We start with our base model (with only POS tag features and word polarity features) and then add several sets of features. First, we add emoticon features, it does not have much effect. This is reasonable as only 8 positive emoticons and 3 negative emoticons are detected (Table 1) of 40049 tokens. Therefore, the importance of emoticon in this data set can be neglected. Then, we add hashtag and capitalization features and achieve an overall gain of 2% over the base model. By adding the mood characteristics from URL articles, we achieve an overall improvement of 6% over the baseline. Other Twitter-specific features and user characteristics improve the f1 by 12%. Finally, we add the TF-IDF feature, and the result improves significantly, and our mood classifier achieves the best classification results with an F1-type of accuracy of tweets we see in the table of 69% of the classification results we show for different classes that work best."}, {"heading": "5.1 Comparison with Stanford Sentiment Analysis Tool", "text": "In this section, we compare the performance of our framework with an open-source state-of-the-art mood analysis tool. As a starting point, we choose the Stanford coreNLP package, which uses recursive, deep models for mood analysis, and achieves good accuracy (\u0445 85%) for formal corpora [5]. However, for loud and informal texts such as tweets, its performance drops sharply. We compare the performance of the Stanford coreNLP tool to the test data.Comparing Table 5 with Table 4, we find that our framework significantly outperforms Stanford coreNLP (\u0445 20%), owing to the fact that Stanford coreNLP is unable to handle text with much noise, lack of formality, and abbreviations, demonstrating the effectiveness of our framework."}, {"heading": "6. ENHANCEMENTS", "text": "In addition to sentiment prediction, we also present some enhancements to our system."}, {"heading": "6.1 Harvest New Sentiment Terms", "text": "We have used a static dictionary to determine the previous polarity of a word, which helps us to detect the general mood of a sentence, but the use of words varies according to the medium of conversation (e.g.: informal social media, blogs, news media), context and topic. In dynamic media such as Twitter, where word mixing and word mixing are often used in a positive sense, it is not enough to have a static dictionary of fixed polarity words. To obtain temporal and top-specific sentiment terms, we use the tweets that are classified by our classifier. We consider the words that occur in positive, neutral and negative tweets to be negative. A word that very often occurs in tweets with positive (negative) terms is negative."}, {"heading": "6.2 Predicting Strength of Sentiment", "text": "Apart from predicting the sensitivity class of the tweets, we are also interested in predicting the strength or intensity of the mood associated with it. Consider the following tweets: \u2022 t1: \"GO TO YOUR US REPS OFFICE ON SATURDAY AND SAY VOTE NO! ON # HCR # Obama # cnn # killthebill # p2 # msnbc # foxnews # congress # tcot\" \u2022 t2: \"Thank God the Democratic Party is not too big to fail. # tcot # hcr\" Although both tweets have a negative sentiment toward \"Obamacare,\" the intensity is not the same in both. The first tweet (t1) is quite aggressive, while the other (t2) is not so much. Here, we propose a technique to predict the strength of the sensation. We consider only a few characteristics from the tweet to do so. If our classifier predicts the sensation as neutral, we predict that the sensation is free."}, {"heading": "7. CONCLUSION", "text": "We have shown that the use of external knowledge outside of the tweet text (of URLs landing pages) and of user functions can significantly improve performance. We have presented experimental results and a comparison with modern tools. We have presented two advanced functionalities, namely the discovery of new terms from the tweet and the prediction of the strength of feeling. Due to the lack of marked data, we could not discuss the accuracy of these two improvements. In the future, we plan to use them as a feedback mechanism for classifying new tweets."}, {"heading": "8. REFERENCES", "text": "[1] Steven Bird. Nltk: the natural language toolkit. In Proceedings of the COLING / ACL on Interactive presentation sessions, pp. 69-72. Association for Computational Linguistics, 2006. [2] Bo Han, Paul Cook, and Timothy Baldwin. Automatically constructing a normalization dictionary for microblogs. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pp. 421-432, Jeju Island, Korea, July 2012. Association for Computational Linguistics. [3] Geetika Vashisht and Sangharsh Thakur. Facebook as corpus for emoticons-based sentiment analysis. [4] Theresa Wilson, Janyce Wiebe, and Paul Hoffmann. Recognizing contextual polarity in phrase-level sentiments analysis. In Proceedings of the conference on human language technology and empical analysis 2005, pp. Christopher Way347, Nay347."}], "references": [{"title": "Nltk: the natural language toolkit", "author": ["Steven Bird"], "venue": "In Proceedings of the COLING/ACL on Interactive presentation sessions,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2006}, {"title": "Automatically constructing a normalisation dictionary for microblogs", "author": ["Bo Han", "Paul Cook", "Timothy Baldwin"], "venue": "In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2012}, {"title": "Recognizing contextual polarity in phrase-level sentiment analysis. In Proceedings of the conference on human language technology and empirical methods in natural language processing, pages 347\u2013354", "author": ["Theresa Wilson", "Janyce Wiebe", "Paul Hoffmann"], "venue": "Association for Computational Linguistics,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2005}, {"title": "Recursive deep models for semantic compositionality over a sentiment treebank", "author": ["Richard Socher", "Alex Perelygin", "Jean Y Wu", "Jason Chuang", "Christopher D Manning", "Andrew Y Ng", "Christopher Potts"], "venue": "In Proceedings of the conference on empirical methods in natural language processing (EMNLP),", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2013}], "referenceMentions": [{"referenceID": 0, "context": "(2) Tokenizing tweet text using NLTK[1] word tokenizer.", "startOffset": 36, "endOffset": 39}, {"referenceID": 1, "context": "(4) Rectifying informal/misspelled words using normalization dictionary [2].", "startOffset": 72, "endOffset": 75}, {"referenceID": 0, "context": "(1) Parts of Speech (POS) tags: We use the POS tagger of NLTK to tag the tweet texts [1].", "startOffset": 85, "endOffset": 88}, {"referenceID": 2, "context": "(2) Prior polarity of the words: We use a polarity dictionary [4] to get the prior polarity of words.", "startOffset": 62, "endOffset": 65}, {"referenceID": 3, "context": "Standford Sentiment Analysis tool[5].", "startOffset": 33, "endOffset": 36}, {"referenceID": 3, "context": "It uses recursive deep models to do sentiment analysis and achieves good accuracy (\u223c 85%) for formal corpora [5].", "startOffset": 109, "endOffset": 112}, {"referenceID": 0, "context": "Once calculated, we normalize the score within [0-5].", "startOffset": 47, "endOffset": 52}, {"referenceID": 1, "context": "Once calculated, we normalize the score within [0-5].", "startOffset": 47, "endOffset": 52}, {"referenceID": 2, "context": "Once calculated, we normalize the score within [0-5].", "startOffset": 47, "endOffset": 52}, {"referenceID": 3, "context": "Once calculated, we normalize the score within [0-5].", "startOffset": 47, "endOffset": 52}], "year": 2016, "abstractText": "Analysing sentiment of tweets is important as it helps to determine the users\u2019 opinion. Knowing people\u2019s opinion is crucial for several purposes starting from gathering knowledge about customer base, e-governance, campaignings and many more. In this report, we aim to develop a system to detect the sentiment from tweets. We employ several linguistic features along with some other external sources of information to detect the sentiment of a tweet. We show that augmenting the 140 character-long tweet with information harvested from external urls shared in the tweet as well as Social Media features enhances the sentiment prediction accuracy significantly.", "creator": "LaTeX with hyperref package"}}}