{"id": "1611.09461", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-Nov-2016", "title": "Cost-Sensitive Reference Pair Encoding for Multi-Label Learning", "abstract": "We propose a novel cost-sensitive multi-label classification algorithm called cost-sensitive random pair encoding (CSRPE). CSRPE reduces the cost-sensitive multi-label classification problem to many cost-sensitive binary classification problems through the label powerset approach followed by the classic one-versus-one decomposition. While such a naive reduction results in exponentially-many classifiers, we resolve the training challenge of building the many classifiers by random sampling, and the prediction challenge of voting from the many classifiers by nearest-neighbor decoding through casting the one-versus-one decomposition as a special case of error-correcting code. Extensive experimental results demonstrate that CSRPE achieves stable convergence and reaches better performance than other ensemble-learning and error-correcting-coding algorithms for multi-label classification. The results also justify that CSRPE is competitive with state-of-the-art cost-sensitive multi-label classification algorithms for cost-sensitive multi-label classification.", "histories": [["v1", "Tue, 29 Nov 2016 02:41:28 GMT  (2153kb,D)", "http://arxiv.org/abs/1611.09461v1", null], ["v2", "Fri, 18 Aug 2017 07:26:05 GMT  (5624kb,D)", "http://arxiv.org/abs/1611.09461v2", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["yao-yuan yang", "kuan-hao huang", "chih-wei chang", "hsuan-tien lin"], "accepted": false, "id": "1611.09461"}, "pdf": {"name": "1611.09461.pdf", "metadata": {"source": "CRF", "title": "Cost-Sensitive Random Pair Encoding for Multi-Label Classification", "authors": ["Yao-Yuan Yang", "Chih-Wei Chang", "Hsuan-Tien Lin"], "emails": ["b01902066@ntu.edu.tw", "cwchang@cs.cmu.edu", "htlin@csie.ntu.edu.tw"], "sections": [{"heading": null, "text": "Keywords multi-label \u00b7 cost-sensitive \u00b7 classification"}, {"heading": "1 Introduction", "text": "This year is the highest in the history of the country."}, {"heading": "2 Preliminary", "text": "(.). (.). (.). \"(.).\" (.). \"(.).\" (.). \"(.).\" (.). \"(.).\" (.). \"It is as if men are able to decide whether they want or not. (.).\" (.). \"It is as if they do not want.\" (.). \"(.).\" (.). \"(.).\" (.). \"(.).\" (.). \"(.).\" (.). \"(.).\" (.). \"(.).\" (.). \"(.).\" (.). \"(.).\" (.). \"(.).\" (.). \"(.).\" (.). \"(.).\" (.). \"(.).\" (.). \"(.).\" (.). \"(.).\" (.). \"(.).\" (.). \"(.).\" (.). \"(.).\" (.). \"(.).).\" (.). \"(.).).\" (.). \"(.).).\" (.). \"(.).).\" (.). \"(.).).\" (.). \"(.).).\" (.). \"(.).).\" (.).). \"(.).\" (. \"(.).).\" (.). \"(.).).\" (.).). \"(.\" (.).). \"(.).\" (.).). \"(.).\" (.).). \"(.).\" (.). \"(.).).\" (.).). \"(.).\" (.). \"(.).\" (.).). \"(.).).\" (.).). \"(.).).\" (.).). \"(.).\" (.).). \"(.).).\""}, {"heading": "3 Proposed Approach", "text": "The proposed CSMLC algorithms follow the ML-ECC system, in which encoding and decoding takes place between two different approaches. (Although we apply the costs in the encoding phase in the encoding phase in the encoding phase in the encoding phase in the encoding phase in the encoding phase, we do not have to apply the cost information in the encoding phase in the encoding phase in the encoding phase, but apply the usual decoding steps in the encoding phase. As a result, the prediction stage can be much more efficient and faster.Hamming decoding enjoys theoretical warranty regarding 0 / 1 loss in the multiclass."}, {"heading": "4 EXPERIMENTS", "text": "The suggestions mentioned are primarily capable of establishing themselves in the region, namely in the areas in which they are located and in which they are located."}, {"heading": "5 Conclusions", "text": "CSRPE uses the redundancy of CSOVO and derives from the multi-class classification algorithm CSOVO. To decode CSRPE, the traditional ECOC Hamming20 Yao-Yuan Yang et al. is sufficient to perform well in cost-sensitive settings. In the experiments with the real data sets, CSRPE is able to provide a more stable performance, in addition to shortening the encoded vector, other encoding methods such as REP, RREP, HAMR in f1 score, rank loss, accuracy score, and connections in hammering loss.CSRPE also produces a more stable performance by increasing air quality in the coding efficiency of REP, RREFT, HAMR in f1 score, rank loss, accuracy score, and connections in hammering losses."}], "references": [{"title": "Reducing multiclass to binary: A unifying approach for margin classifiers", "author": ["EL Allwein", "RE Schapire", "Y Singer"], "venue": "The Journal of Machine Learning Research", "citeRegEx": "Allwein et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Allwein et al\\.", "year": 2001}, {"title": "Near-optimal hashing algorithms for approximate nearest neighbor in high dimensions", "author": ["A Andoni", "P Indyk"], "venue": null, "citeRegEx": "Andoni and Indyk,? \\Q2006\\E", "shortCiteRegEx": "Andoni and Indyk", "year": 2006}, {"title": "Multidimensional binary search trees used for associative searching", "author": ["JL Bentley"], "venue": null, "citeRegEx": "Bentley,? \\Q1975\\E", "shortCiteRegEx": "Bentley", "year": 1975}, {"title": "Bagging predictors. Machine learning", "author": ["L Breiman"], "venue": null, "citeRegEx": "Breiman,? \\Q1996\\E", "shortCiteRegEx": "Breiman", "year": 1996}, {"title": "Classification and Regression Trees", "author": ["L Breiman", "JH Friedman", "RA Olshen", "CJ Stone"], "venue": null, "citeRegEx": "Breiman et al\\.,? \\Q1984\\E", "shortCiteRegEx": "Breiman et al\\.", "year": 1984}, {"title": "Bayes optimal multilabel classification via probabilistic classifier chains", "author": ["K Dembczynski", "W Cheng", "E H\u00fcllermeier"], "venue": null, "citeRegEx": "Dembczynski et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Dembczynski et al\\.", "year": 2010}, {"title": "Consistent multilabel ranking through univariate losses", "author": ["K Dembczynski", "W Kotlowski", "E H\u00fcllermeier"], "venue": "Proceedings of the 29th International Conference on Machine Learning", "citeRegEx": "Dembczynski et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Dembczynski et al\\.", "year": 2012}, {"title": "An exact algorithm for f-measure maximization", "author": ["KJ Dembczynski", "W Waegeman", "W Cheng", "E H\u00fcllermeier"], "venue": "Advances in neural information processing systems,", "citeRegEx": "Dembczynski et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Dembczynski et al\\.", "year": 2011}, {"title": "Solving multiclass learning problems via errorcorrecting output codes. Journal of artificial intelligence research", "author": ["TG Dietterich", "G Bakiri"], "venue": null, "citeRegEx": "Dietterich and Bakiri,? \\Q1995\\E", "shortCiteRegEx": "Dietterich and Bakiri", "year": 1995}, {"title": "Metacost: A general method for making classifiers costsensitive", "author": ["PM Domingos"], "venue": null, "citeRegEx": "Domingos,? \\Q1999\\E", "shortCiteRegEx": "Domingos", "year": 1999}, {"title": "A kernel method for multi-labelled classification", "author": ["A Elisseeff", "J Weston"], "venue": null, "citeRegEx": "Elisseeff and Weston,? \\Q2001\\E", "shortCiteRegEx": "Elisseeff and Weston", "year": 2001}, {"title": "LIBLINEAR: A library for large linear classification", "author": ["R Fan", "K Chang", "C Hsieh", "X Wang", "C Lin"], "venue": "Journal of Machine Learning Research", "citeRegEx": "Fan et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Fan et al\\.", "year": 2008}, {"title": "Multilabel classification using error-correcting codes of hard or soft bits", "author": ["CS Ferng", "HT Lin"], "venue": "IEEE Transactions on Neural Networks and Learning Systems", "citeRegEx": "Ferng and Lin,? \\Q2013\\E", "shortCiteRegEx": "Ferng and Lin", "year": 2013}, {"title": "Multilabel text classification for automated tag suggestion", "author": ["I Katakis", "G Tsoumakas", "I Vlahavas"], "venue": "ECML PKDD discovery challenge", "citeRegEx": "Katakis et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Katakis et al\\.", "year": 2008}, {"title": "Condensed filter tree for cost-sensitive multi-label classification", "author": ["C Li", "H Lin"], "venue": "Proceedings of the 31th International Conference on Machine Learning,", "citeRegEx": "Li and Lin,? \\Q2014\\E", "shortCiteRegEx": "Li and Lin", "year": 2014}, {"title": "Reduction from cost-sensitive multiclass classification to one-versusone binary classification", "author": ["H Lin"], "venue": "ACML", "citeRegEx": "Lin,? \\Q2014\\E", "shortCiteRegEx": "Lin", "year": 2014}, {"title": "New algorithms for efficient high-dimensional nonparametric classification", "author": ["T Liu", "AW Moore", "A Gray"], "venue": "The Journal of Machine Learning Research", "citeRegEx": "Liu et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2006}, {"title": "Fastxml: a fast, accurate and stable tree-classifier for extreme multi-label learning", "author": ["Y Prabhu", "M Varma"], "venue": "In: International Conference on Knowledge Discovery", "citeRegEx": "Prabhu and Varma,? \\Q2014\\E", "shortCiteRegEx": "Prabhu and Varma", "year": 2014}, {"title": "Classifier chains for multi-label classification. Machine learning", "author": ["J Read", "B Pfahringer", "G Holmes", "E Frank"], "venue": null, "citeRegEx": "Read et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Read et al\\.", "year": 2011}, {"title": "Multi-label classification: An overview", "author": ["G Tsoumakas", "I Katakis"], "venue": "International Journal of Data Warehousing and Mining", "citeRegEx": "Tsoumakas and Katakis,? \\Q2007\\E", "shortCiteRegEx": "Tsoumakas and Katakis", "year": 2007}, {"title": "Random k -labelsets: An ensemble method for multilabel classification", "author": ["G Tsoumakas", "IP Vlahavas"], "venue": null, "citeRegEx": "Tsoumakas and Vlahavas,? \\Q2007\\E", "shortCiteRegEx": "Tsoumakas and Vlahavas", "year": 2007}, {"title": "Mulan: A java library for multi-label learning", "author": ["G Tsoumakas", "E Spyromitros-Xioufis", "J Vilcek", "I Vlahavas"], "venue": "Journal of Machine Learning Research", "citeRegEx": "Tsoumakas et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Tsoumakas et al\\.", "year": 2011}], "referenceMentions": [{"referenceID": 10, "context": "genes with some functional classes (Elisseeff and Weston, 2001).", "startOffset": 35, "endOffset": 63}, {"referenceID": 19, "context": "For instance, Binary Relevance (BR; Tsoumakas and Katakis, 2007) learns a per-label binary classifier to predict the label\u2019s relevance, and naturally targets at the Hamming loss.", "startOffset": 31, "endOffset": 64}, {"referenceID": 20, "context": "Random k-labelsets (RAKEL; Tsoumakas and Vlahavas, 2007) learns an ensemble of LP classifiers, each only on a few labels, and can be explained as optimizing the 0/1 loss from the Multi-label Error-correcting Code framework (ML-ECC; Ferng and Lin, 2013).", "startOffset": 19, "endOffset": 56}, {"referenceID": 12, "context": "Random k-labelsets (RAKEL; Tsoumakas and Vlahavas, 2007) learns an ensemble of LP classifiers, each only on a few labels, and can be explained as optimizing the 0/1 loss from the Multi-label Error-correcting Code framework (ML-ECC; Ferng and Lin, 2013).", "startOffset": 223, "endOffset": 252}, {"referenceID": 17, "context": "FastXML (Prabhu and Varma, 2014) is designed to optimize the normalized discounted cumulative gain, a popular criterion for information retrieval.", "startOffset": 8, "endOffset": 32}, {"referenceID": 14, "context": "Multi-label classification algorithms that automatically adapt to different evaluation criteria to improve the performance are called cost-sensitive multi-label classification (CSMLC) algorithms, as defined by Li and Lin (2014). A classic algorithm called Probabilistic Classifier Chain (PCC) (Dembczynski et al, 2010), an algorithm that extends from CC, is a state-of-the-art CSMLC algorithm.", "startOffset": 210, "endOffset": 228}, {"referenceID": 14, "context": "Another state-of-the-art CSMLC algorithm is Condense Filter Tree (CFT) (Li and Lin, 2014), which is designed by converting the CSMLC problem to a cost-sensitive multi-class classification problem with a n\u00e4\u0131ve Label Powerset (LP) reduction.", "startOffset": 71, "endOffset": 89}, {"referenceID": 9, "context": "opens a route of connecting CSMLC with the rich literature of cost-sensitive multiclass classification (Domingos, 1999; Beygelzimer et al, 2009; Lin, 2014), but results in exponentially many hyperclasses with respect to the number of labels.", "startOffset": 103, "endOffset": 155}, {"referenceID": 15, "context": "opens a route of connecting CSMLC with the rich literature of cost-sensitive multiclass classification (Domingos, 1999; Beygelzimer et al, 2009; Lin, 2014), but results in exponentially many hyperclasses with respect to the number of labels.", "startOffset": 103, "endOffset": 155}, {"referenceID": 19, "context": "Multi-label classification with per-label classifiers, such as BR (Tsoumakas and Katakis, 2007) and CFT (Li and Lin, 2014), are generally considered the simpler algorithms in multi-label classification.", "startOffset": 66, "endOffset": 95}, {"referenceID": 14, "context": "Multi-label classification with per-label classifiers, such as BR (Tsoumakas and Katakis, 2007) and CFT (Li and Lin, 2014), are generally considered the simpler algorithms in multi-label classification.", "startOffset": 104, "endOffset": 122}, {"referenceID": 12, "context": "Multi-Label Error-Correcting Code (ML-ECC) (Ferng and Lin, 2013) is such an algorithm.", "startOffset": 43, "endOffset": 64}, {"referenceID": 12, "context": "While ML-ECC is successful in terms of the hamming loss and the 0/1 loss (Ferng and Lin, 2013), it cannot easily adapt to other evaluation criteria and hence is not a CSMLC algorithm.", "startOffset": 73, "endOffset": 94}, {"referenceID": 8, "context": "It is well-known that OVO can be seen as a special case of ECC (Dietterich and Bakiri, 1995).", "startOffset": 63, "endOffset": 92}, {"referenceID": 15, "context": "In addition, OVO has been extended to some multi-class cost-sensitive classification algorithms like CostSensitive One-versus-one (CSOVO), which is reported to perform better than FT for cost-sensitive multi-class classification (Lin, 2014).", "startOffset": 229, "endOffset": 240}, {"referenceID": 14, "context": "More specifically, we follow the route of LP (Li and Lin, 2014) to reduce the CSMLC problem to a cost-sensitive multi-class classification problem.", "startOffset": 45, "endOffset": 63}, {"referenceID": 14, "context": "We follow (Li and Lin, 2014) for the definition of the multi-label classification problem (MLC) and cost-sensitive multi-label classification problem (CSMLC).", "startOffset": 10, "endOffset": 28}, {"referenceID": 19, "context": "For MLC, there already exists some basic approaches such as Binary Relevance (BR) (Tsoumakas and Katakis, 2007) and label powerset (LP) (Tsoumakas et al, 2010).", "startOffset": 82, "endOffset": 111}, {"referenceID": 20, "context": "RAndom K-labeLsets (RAKEL) (Tsoumakas and Vlahavas, 2007) is proposed to strike a balance between these two methods.", "startOffset": 27, "endOffset": 57}, {"referenceID": 12, "context": "Multi-Label Error-Correcting Code (ML-ECC) (Ferng and Lin, 2013) proposed a more sophisticated framework of using error correcting code (ECC).", "startOffset": 43, "endOffset": 64}, {"referenceID": 14, "context": "Condense Filter Tree (CFT) (Li and Lin, 2014) is the first algorithm to proposed as a CSMLC algorithm, which is able to extend to arbitrary cost function with ease.", "startOffset": 27, "endOffset": 45}, {"referenceID": 15, "context": "In multi-class setting Cost-Sensitive One-versus-one (CSOVO) (Lin, 2014), extends OVO approach to cost-sensitive setting.", "startOffset": 61, "endOffset": 72}, {"referenceID": 2, "context": "For example, KD Tree (Bentley, 1975) and Ball Tree (Liu et al, 2006).", "startOffset": 21, "endOffset": 36}, {"referenceID": 1, "context": "is larger than the normal size where KD Tree or Ball Tree can work efficiently, approximated nearest neighbor algorithms like locality-sensitive hashing (LSH) (Andoni and Indyk, 2006) can be adapted.", "startOffset": 159, "endOffset": 183}, {"referenceID": 3, "context": "There are multiple repeated bits in REP and HAMR, so the base learners are trained with bagging (Breiman, 1996).", "startOffset": 96, "endOffset": 111}], "year": 2016, "abstractText": "We propose a novel cost-sensitive multi-label classification algorithm called cost-sensitive random pair encoding (CSRPE). CSRPE reduces the costsensitive multi-label classification problem to many cost-sensitive binary classification problems through the label powerset approach followed by the classic oneversus-one decomposition. While such a n\u00e4\u0131ve reduction results in exponentiallymany classifiers, we resolve the training challenge of building the many classifiers by random sampling, and the prediction challenge of voting from the many classifiers by nearest-neighbor decoding through casting the one-versus-one decomposition as a special case of error-correcting code. Extensive experimental results demonstrate that CSRPE achieves stable convergence and reaches better performance than other ensemble-learning and error-correcting-coding algorithms for multi-label classification. The results also justify that CSRPE is competitive with state-of-the-art cost-sensitive multi-label classification algorithms for cost-sensitive multi-label classification.", "creator": "LaTeX with hyperref package"}}}