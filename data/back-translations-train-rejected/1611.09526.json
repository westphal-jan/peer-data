{"id": "1611.09526", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-Nov-2016", "title": "Learning Filter Banks Using Deep Learning For Acoustic Signals", "abstract": "Designing appropriate features for acoustic event recognition tasks is an active field of research. Expressive features should both improve the performance of the tasks and also be interpret-able. Currently, heuristically designed features based on the domain knowledge requires tremendous effort in hand-crafting, while features extracted through deep network are difficult for human to interpret. In this work, we explore the experience guided learning method for designing acoustic features. This is a novel hybrid approach combining both domain knowledge and purely data driven feature designing. Based on the procedure of log Mel-filter banks, we design a filter bank learning layer. We concatenate this layer with a convolutional neural network (CNN) model. After training the network, the weight of the filter bank learning layer is extracted to facilitate the design of acoustic features. We smooth the trained weight of the learning layer and re-initialize it in filter bank learning layer as audio feature extractor. For the environmental sound recognition task based on the Urban- sound8K dataset, the experience guided learning leads to a 2% accuracy improvement compared with the fixed feature extractors (the log Mel-filter bank). The shape of the new filter banks are visualized and explained to prove the effectiveness of the feature design process.", "histories": [["v1", "Tue, 29 Nov 2016 08:46:26 GMT  (2117kb,D)", "http://arxiv.org/abs/1611.09526v1", null]], "reviews": [], "SUBJECTS": "cs.SD cs.AI", "authors": ["shuhui qu", "juncheng li", "wei dai", "samarjit das"], "accepted": false, "id": "1611.09526"}, "pdf": {"name": "1611.09526.pdf", "metadata": {"source": "CRF", "title": "LEARNING FILTER BANKS USING DEEP LEARNING FOR ACOUSTIC SIGNALS", "authors": ["Shuhui Qu", "Juncheng Li", "Wei Dai", "Samarjit Das"], "emails": ["shuhuiq@stanford.edu,", "billy.li@us.bosch.com,", "wdai@cs.cmu.edu,", "samarjit.das@us.bosch.com"], "sections": [{"heading": null, "text": "This year, it has reached the stage where it will be able to take the lead."}, {"heading": "5.1. Experiment Result", "text": "The result is in Table 2. The proposed method could result in a modest 0.4% improvement in classification accuracy. We take out the weight of the learning layer of the filter bank and use the Savitzky-Golay function [18] to smooth it. We then re-initialize the filter bank layer with the smoothed weight. After retraining the model, the accuracy is improved by 1.5%. We have not taped the 4-second clip for the 22.5 kHz tone, but expect improvements compared to the 8 kHz result. We are also testing the learning layer of the filter bank proposed in [2], but the accuracy is lower than with other baselines. This could be caused by the complex nonlinearity of this layer and our estimation of the mean and standard deviation of the input. To our knowledge, our method achieves the highest accuracy of the Urbansound8K data. We also find that the sampling rate of the sound impairs the detection stratum."}, {"heading": "5.2. Filter Bank Analysis", "text": "The purpose of this work is to understand the mechanism of the filter bank and further facilitate the design of the filter banks, which are primarily needed for better functional extractors. Here, we visualize the filter banks from the triangular window and the smoothed weight from the trained filter bank learning layer, which is trained in the second row of 1-9 in the following image information. As we can see, the first few learned filter banks (first row) are compliant with the triangular filter banks, which means that these triangular filter banks capture most information in a low frequency range. However, in the second row, we notice that the learned filter banks are activated by 0.4kHz to 0.5kHz and learn 0.75kHz, while the frequency between 0.6 and 0.7 kHz is less interesting. In the triangular windows, the bandwidth of the filters increases as the frequency increases."}], "references": [{"title": "A dataset and taxonomy for urban sound research", "author": ["Justin Salamon", "Christopher Jacoby", "Juan Pablo Bello"], "venue": "Proceedings of the 22nd ACM international conference on Multimedia. ACM, 2014, pp. 1041\u20131044.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning filter banks within a deep neural network framework", "author": ["Tara N Sainath", "Brian Kingsbury", "Abdel-rahman Mohamed", "Bhuvana Ramabhadran"], "venue": "Automatic Speech Recognition and Understanding (ASRU), 2013 IEEE Workshop on. IEEE, 2013, pp. 297\u2013 302.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2013}, {"title": "Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups", "author": ["Geoffrey Hinton", "Li Deng", "Dong Yu", "George E Dahl", "Abdel-rahman Mohamed", "Navdeep Jaitly", "Andrew Senior", "Vincent Vanhoucke", "Patrick Nguyen", "Tara N Sainath"], "venue": "IEEE Signal Processing Magazine, vol. 29, no. 6, pp. 82\u201397, 2012.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2012}, {"title": "Deep learning", "author": ["Yann LeCun", "Yoshua Bengio", "Geoffrey Hinton"], "venue": "Nature, vol. 521, no. 7553, pp. 436\u2013 444, 2015.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2015}, {"title": "Improving deep neural networks for lvcsr using rectified linear units and dropout", "author": ["George E Dahl", "Tara N Sainath", "Geoffrey E Hinton"], "venue": "2013 IEEE International Conference on Acoustics, Speech and Signal Processing. IEEE, 2013, pp. 8609\u20138613.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2013}, {"title": "Deep residual learning for image recognition", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"], "venue": "arXiv preprint arXiv:1512.03385, 2015.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2015}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["Karen Simonyan", "Andrew Zisserman"], "venue": "arXiv preprint arXiv:1409.1556, 2014.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning the speech frontend with raw waveform cldnns", "author": ["Tara N Sainath", "Ron J Weiss", "Andrew Senior", "Kevin W Wilson", "Oriol Vinyals"], "venue": "Proc. Interspeech, 2015.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "Convolutional networks for images, speech, and time series", "author": ["Yann LeCun", "Yoshua Bengio"], "venue": "The handbook of brain theory and neural networks, vol. 3361, no. 10, pp. 1995, 1995.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1995}, {"title": "A tutorial on hidden markov models and selected applications in speech recognition", "author": ["Lawrence R Rabiner"], "venue": "Proceedings of the IEEE, vol. 77, no. 2, pp. 257\u2013286, 1989.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1989}, {"title": "The aurora experimental framework for the performance evaluation of speech recognition systems under noisy conditions", "author": ["Hans-G\u00fcnter Hirsch", "David Pearce"], "venue": "ASR2000-Automatic Speech Recognition: Challenges for the new Millenium ISCA Tutorial and Research Workshop (ITRW), 2000.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2000}, {"title": "Unsupervised feature learning for audio classification using convolutional deep belief networks", "author": ["Honglak Lee", "Peter Pham", "Yan Largman", "Andrew Y Ng"], "venue": "Advances in neural information processing systems, 2009, pp. 1096\u20131104.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2009}, {"title": "Cepstral analysis synthesis on the mel frequency scale", "author": ["Satoshi Imai"], "venue": "Acoustics, Speech, and Signal Processing, IEEE International Conference on ICASSP\u201983. IEEE, 1983, vol. 8, pp. 93\u201396.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1983}, {"title": "librosa: 0.4.1", "author": ["Brian McFee", "Matt McVicar", "Colin Raffel", "Dawen Liang", "Oriol Nieto", "Eric Battenberg", "Josh Moore", "Dan Ellis", "Ryuichi YAMAMOTO", "Rachel Bittner", "Douglas Repetto", "Petr Viktorin", "Joo Felipe Santos", "Adrian Holovaty"], "venue": "Oct. 2015.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2015}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba"], "venue": "arXiv preprint arXiv:1412.6980, 2014.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2014}, {"title": "Empirical evaluation of rectified activations in convolutional network", "author": ["Bing Xu", "Naiyan Wang", "Tianqi Chen", "Mu Li"], "venue": "CoRR, vol. abs/1505.00853, 2015.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}, {"title": "Environmental sound classification with convolutional neural networks", "author": ["Karol J Piczak"], "venue": "2015 IEEE 25th International Workshop on Machine Learning for Signal Processing (MLSP). IEEE, 2015, pp. 1\u20136.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2015}, {"title": "What is a savitzky-golay filter?[lecture notes", "author": ["Ronald W Schafer"], "venue": "IEEE Signal Processing Magazine, vol. 28, no. 4, pp. 111\u2013117, 2011.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2011}], "referenceMentions": [{"referenceID": 0, "context": "For the environmental sound recognition task based on the Urbansound8K dataset [1], the experience guided learning leads to a 2% accuracy improvement compared with the fixed feature extractors (the log Mel-filter bank).", "startOffset": 79, "endOffset": 82}, {"referenceID": 1, "context": "Meanwhile, since this process is separate from the classification process [2], heuristically designed features do not always contain enough information to obtain a high classification accuracy.", "startOffset": 74, "endOffset": 77}, {"referenceID": 2, "context": "Thanks to the development of deep learning methods and rich dataset for sound, deep learning is increasingly becoming a popular candidate for acoustic recognition tasks [3, 4, 5].", "startOffset": 169, "endOffset": 178}, {"referenceID": 3, "context": "Thanks to the development of deep learning methods and rich dataset for sound, deep learning is increasingly becoming a popular candidate for acoustic recognition tasks [3, 4, 5].", "startOffset": 169, "endOffset": 178}, {"referenceID": 4, "context": "Thanks to the development of deep learning methods and rich dataset for sound, deep learning is increasingly becoming a popular candidate for acoustic recognition tasks [3, 4, 5].", "startOffset": 169, "endOffset": 178}, {"referenceID": 5, "context": "Recently, CNN has shown the superior performance in feature extraction and classification in visual [6, 7] and acoustic domain [8], especially in speech recognition [7, 8, 6, 9].", "startOffset": 100, "endOffset": 106}, {"referenceID": 6, "context": "Recently, CNN has shown the superior performance in feature extraction and classification in visual [6, 7] and acoustic domain [8], especially in speech recognition [7, 8, 6, 9].", "startOffset": 100, "endOffset": 106}, {"referenceID": 7, "context": "Recently, CNN has shown the superior performance in feature extraction and classification in visual [6, 7] and acoustic domain [8], especially in speech recognition [7, 8, 6, 9].", "startOffset": 127, "endOffset": 130}, {"referenceID": 6, "context": "Recently, CNN has shown the superior performance in feature extraction and classification in visual [6, 7] and acoustic domain [8], especially in speech recognition [7, 8, 6, 9].", "startOffset": 165, "endOffset": 177}, {"referenceID": 7, "context": "Recently, CNN has shown the superior performance in feature extraction and classification in visual [6, 7] and acoustic domain [8], especially in speech recognition [7, 8, 6, 9].", "startOffset": 165, "endOffset": 177}, {"referenceID": 5, "context": "Recently, CNN has shown the superior performance in feature extraction and classification in visual [6, 7] and acoustic domain [8], especially in speech recognition [7, 8, 6, 9].", "startOffset": 165, "endOffset": 177}, {"referenceID": 8, "context": "Recently, CNN has shown the superior performance in feature extraction and classification in visual [6, 7] and acoustic domain [8], especially in speech recognition [7, 8, 6, 9].", "startOffset": 165, "endOffset": 177}, {"referenceID": 1, "context": "These features are not optimized for a particular audio recognition task at hand, and thus might not lead to high accuracy [2].", "startOffset": 123, "endOffset": 126}, {"referenceID": 0, "context": "Urbansound8K sound [1] increases at least 1.", "startOffset": 19, "endOffset": 22}, {"referenceID": 9, "context": "[10] provided a detailed implementation of the Hidden Markov Model (HMM) on speech recognition by using the Linear predictive coding (LPC) features.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[11] applied the HMM model on the MFCC features for speech recognition.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[12] applied convolutional deep belief networks to audio data and evaluated them on various audio classification tasks by using the MFCC feature.", "startOffset": 0, "endOffset": 4}, {"referenceID": 1, "context": "[2] thus proposed a filter learning layer to adaptively learn filter banks from the spectrum, and obtained good result in speech recognition.", "startOffset": 0, "endOffset": 3}, {"referenceID": 12, "context": "The mechanism of the filter bank learning layer is similar to the design of log-mel spectrogram [13], which has been widely used in automatic speech recognition.", "startOffset": 96, "endOffset": 100}, {"referenceID": 1, "context": "Different from [2]\u2019s work, our filter bank learning layer does not require estimating the mean and standard deviation of the input beforehand.", "startOffset": 15, "endOffset": 18}, {"referenceID": 0, "context": "In this study, the training of the CNN model is performed on the natural sounds dataset , the Urbansound8k [1].", "startOffset": 107, "endOffset": 110}, {"referenceID": 13, "context": "After that, we take the power spectrogram of the sound by using libROSA [14](nfft equals to sampling rate, default hop length).", "startOffset": 72, "endOffset": 76}, {"referenceID": 6, "context": "We build two CNN architectures, one is deep VGG architecture [7] while the other one is shallow as shown in Fig.", "startOffset": 61, "endOffset": 64}, {"referenceID": 14, "context": "The optimizer is default Adam optimizer [15] with learning rate 0.", "startOffset": 40, "endOffset": 44}, {"referenceID": 15, "context": "After each layer, we apply leakyrelu[16] with parameter Table 1.", "startOffset": 36, "endOffset": 40}, {"referenceID": 0, "context": "The baseline is around 70% [1] by using svm with rbf kernel and 73.", "startOffset": 27, "endOffset": 30}, {"referenceID": 16, "context": "7% [17].", "startOffset": 3, "endOffset": 7}, {"referenceID": 1, "context": "We also test the [2]\u2019s filter bank learning layer for comparison.", "startOffset": 17, "endOffset": 20}, {"referenceID": 17, "context": "We take out the weight of the filter bank learning layer and use the Savitzky-Golay function [18] to smooth it.", "startOffset": 93, "endOffset": 97}, {"referenceID": 1, "context": "We also test the filter bank learning layer proposed in [2], but the accuracy is lower than other baselines.", "startOffset": 56, "endOffset": 59}], "year": 2016, "abstractText": "Designing appropriate features for acoustic event recognition tasks is an active field of research. Expressive features should both improve the performance of the tasks and also be interpret-able. Currently, heuristically designed features based on the domain knowledge requires tremendous effort in hand-crafting, while features extracted through deep network are difficult for human to interpret. In this work, we explore the experience guided learning method for designing acoustic features. This is a novel hybrid approach combining both domain knowledge and purely data driven feature designing. Based on the procedure of log Mel-filter banks, we design a filter bank learning layer. We concatenate this layer with a convolutional neural network (CNN) model. After training the network, the weight of the filter bank learning layer is extracted to facilitate the design of acoustic features. We smooth the trained weight of the learning layer and re-initialize it in filter bank learning layer as audio feature extractor. For the environmental sound recognition task based on the Urbansound8K dataset [1], the experience guided learning leads to a 2% accuracy improvement compared with the fixed feature extractors (the log Mel-filter bank). The shape of the new filter banks are visualized and explained to prove the effectiveness of the feature design process.", "creator": "LaTeX with hyperref package"}}}