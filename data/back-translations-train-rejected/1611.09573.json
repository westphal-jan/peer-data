{"id": "1611.09573", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-Nov-2016", "title": "Learning Concept Hierarchies through Probabilistic Topic Modeling", "abstract": "With the advent of semantic web, various tools and techniques have been introduced for presenting and organizing knowledge. Concept hierarchies are one such technique which gained significant attention due to its usefulness in creating domain ontologies that are considered as an integral part of semantic web. Automated concept hierarchy learning algorithms focus on extracting relevant concepts from unstructured text corpus and connect them together by identifying some potential relations exist between them. In this paper, we propose a novel approach for identifying relevant concepts from plain text and then learns hierarchy of concepts by exploiting subsumption relation between them. To start with, we model topics using a probabilistic topic model and then make use of some lightweight linguistic process to extract semantically rich concepts. Then we connect concepts by identifying an \"is-a\" relationship between pair of concepts. The proposed method is completely unsupervised and there is no need for a domain specific training corpus for concept extraction and learning. Experiments on large and real-world text corpora such as BBC News dataset and Reuters News corpus shows that the proposed method outperforms some of the existing methods for concept extraction and efficient concept hierarchy learning is possible if the overall task is guided by a probabilistic topic modeling algorithm.", "histories": [["v1", "Tue, 29 Nov 2016 11:28:59 GMT  (571kb)", "http://arxiv.org/abs/1611.09573v1", null]], "reviews": [], "SUBJECTS": "cs.AI cs.CL cs.IR", "authors": ["v s anoop", "s asharaf", "p deepak"], "accepted": false, "id": "1611.09573"}, "pdf": {"name": "1611.09573.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["anoop.res15@iiitmk.ac.in", "asharaf.s@iiitmk.ac.in", "deepaksp@acm.org"], "sections": [{"heading": null, "text": "ar Xiv: 161 1.09 573v 1 [cs.A I] 2 9N ov2 016 Learning Concept Hierarchies through Probabilistic Topic ModelingV S Anoopa, S Asharafb, Deepak PcaData Engineering Lab, Indian Institute of Information Technology and Management - Kerala (IIITM-K), Thiruvananthapuram 695 581, India, Contact: anoop.res15 @ iiitmk.ac.inbIndian Institute of Information Technology and Management - Kerala (IITM-K), Thiruvananthapuram 695 581, India, Contact: asharaf.s @ iiitmk.ac.incQueen's University, Belfast, UK, Contact: deepaksp @ acm.org gWith the advent of semantic web, various tools and techniques are introduced to present and organizing knowledge."}, {"heading": "1. INTRODUCTION", "text": "In this context, it should be noted that this is a very complex matter."}, {"heading": "2. PROBLEM DEFINITION AND RELATED WORK", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1. Problem Definition", "text": "Considering a large body of unstructured text documents, our problem is to automatically generate concept hierarchies using a lightweight linguistic process. 2. Can our proposed method learn a hierarchy of such concepts that include a subsumption relationship between them that is important for automated ontology generation? 3. Can our topic extract and learn concept hierarchies better than existing algorithms from a large but unstructured text corpus? Many recent papers have been reported in this direction, in which many algorithms have been proposed to extract semantically rich concepts from simple text. In the following section, we recognize some past literature that deals with methods close to our proposed algorithm. Notations used in this essay: To help, some commonly used notations are used in this Table 1, which are used in the rest of the paper."}, {"heading": "2.2. Related Work", "text": "This year is the highest in the history of the country."}, {"heading": "3. BACKGROUND : LATENT DIRICHLET ALLOCATION (LDA)", "text": "A good number of subject modeling algorithms have been introduced in the recent past, which essentially vary in their functioning with the assumptions they use for statistical processing. An automated document indexing method, based on a latent class model for factor analysis of counter data in latent semantic space, was introduced by Thomas Hofman. [18] This generative data model, called Probabilistic Latent Semantic Indexing (PLSI), which is considered an alternative to basic latent semantic indexing, has a strong statistical basis. PLSI's basic assumption is that every word in a document corresponds to only one topic. Later, lead et. al. [19] introduced a new topic known as latent dirichlet allocation (LDA), which is more efficient and attractive than PLSI. This model assumes that a document contains multiple topics and such topics, using a Dirichlet Prior process."}, {"heading": "4. PROPOSED APPROACH", "text": "In the field of word processing, theme models, or specifically probabilistic theme models, are a set of algorithms that have been widely recognized for their ability to harness hidden thematic information from vast archives of text data. Word processors make extensive use of theme modeling algorithms such as Latent Semantic Analysis (LSA) [20], Probabilistic Latent Semantic Indexing (pLSI) [21], Latent Dirichlet Allocation (LDA) [22], etc., to extract themes or so-called \"topics\" from high-dimensional, unstructured data. Of all these algorithms, LDA has received a lot of attention in the recent past and is widely used for its simplicity of implementation and potential applications. Although the power of the LDA algorithm has been widely utilized for the use of themes, very few studies have been developed to map these statistically outdated themes for semantically rich concepts."}, {"heading": "4.1. Concept Extraction", "text": "In this module, we introduce a topic to the concept mapping process for using potential concepts from statistically calculated topics generated by the LDA algorithm. The first step of the proposed framework deals with the pre-processing of data intended to remove unwanted and irrelevant data and noises, and the latent dirichlet allocation algorithm is executed via this pre-processed data, which in turn generates topics through the statistical process. A total of 50 topics were extracted by tuning the parameters of the LDA algorithm. As soon as we received the sufficient topics for the experiment, we created a topic - a document cluster by grouping the documents that generated such a topic, and the same process was performed for all topics.Now, we are introducing a new weighting scheme called tf \u2212 itf (Term frequency - inverse topic frequency), which is used to determine a highly contributing topic."}, {"heading": "4.2. Concept Hierarchy Learning", "text": "In this module we conduct hierarchical organization of debt concepts using a kind of co-algorithm 1 concept extraction 1: procedure ExtraktConcepts (tc) 2: 0, create Ctd 3: 0, calculate tf \u2212 itf weight 4: 0, select n words with highest tf \u2212 itf 5: S [] = sentences with highest tf \u2212 itf words 6: POS tag (S) 7: W [] = (NNP, NNS, NN, JJ) 8: MWc [] = noun + noun | adj + noun 9: while | MWc \u2212 itf words 6 = 0 do10: termCount (MW), if Tc > 0 then 12: Add MW into C 13: Remove MW into C 13: Remove MW from MWc 14: Fetch next MW from MWc 15: else 16: Remove MW from MWc 17: Fethiera here."}, {"heading": "5. EXPERIMENTAL SETUP", "text": "This section focuses on the implementation details of our proposed framework and concept extraction and hierarchy learning procedures are discussed in detail."}, {"heading": "5.1. Concept Extraction", "text": "This module focuses on tasks such as data collection and pre-processing, topic modeling, topic document clustering, tf-itf weighting, sentence extraction and POS tagging, linguistic pre-processing, etc. for the identification of concepts and a detailed explanation of the individual steps is given below."}, {"heading": "5.1.1. Dataset Collection and Preprocessing", "text": "We use publicly available datasets such as Reuters Corpus Volume 1 dataset [24] and BBC News Dataset [25] for the experiment. Reuters is the world's largest international news agency and provides various news and related information via its website, videos, interactive television and mobile platforms. Reuters Corpus Volume 1 is in XML format and is freely available for research purposes. Text messages are extracted through thorough pre-processing such as removing XML tags, URLs and other special symbols, and then created a new dataset exclusively for our experiment. BBC provides two benchmark news article datasets that are freely available for machine learning. The general BBC dataset consists of 2225 text documents that belong directly from its website to stories in five areas such as business, entertainment, politics, sports and technology, from 2004 to 2005."}, {"heading": "5.1.2. Topic Modeling", "text": "The Latent Dirichlet Allocation (LDA) algorithm was applied to the pre-processed dataset to use the topics for this experiment, setting the number of iterations to 300, as Gibbs scanning method normally approaches the target distribution after 300 iterations, setting the number of topics to 50, and showing a snapshot of 5 topics we randomly selected in Table 2."}, {"heading": "5.1.3. Topic - Document Clustering", "text": "In this step, we look at each topic and then group and group the 50 top documents that contributed to the creation of this specific topic. This was done for all 50 topics of our choice. As a result, we received 50 such clusters that contain documents that created the topics."}, {"heading": "5.1.4. TF-ITF Weighting", "text": "Here we calculate the tf \u2212 itf (term frequency \u2212 inverse topic frequency) weighting of each word in each topic using Equation (3), Equation (4), and Equation (5) to find common topic words in the collection. Table 2 also shows topic words along with their tf-itf weight."}, {"heading": "5.1.5. Sentence Extraction & POS Tagging", "text": "In the Sentence Extraction step, we look at topic words with the highest tf-itf weight and then extract sentences that contain those topic words from the topic - document clusters. Subsequently, a portion of the language marking was performed to identify words that are marked as nouns and adjectives from these sentences, since our goal is to extract potential \"concepts\" from the repository. Natural Language Toolkit (NLTK) [23], which contains libraries for the Natural Language Processing for Python programming language, was used for this experiment."}, {"heading": "5.1.6. Linguistic Processing & Concept Identification", "text": "All words marked as nouns (NN / NNP / NNS) and adjectives (YY) are filtered out and all possible combinations of noun + noun, adjective + noun and (noun / adjective) + noun are presented in Table 3. The term number for each of these multiword terms is then calculated based on the original corpus and a positive term number implies that the corresponding multiword term can be a potential concept and we eliminate the term if we get a zero number. This process repeats for all plurals we have filtered out."}, {"heading": "5.2. Concept Hierarchy Learning", "text": "The Conceptual Hierarchy learning module focuses on the use of a subsumption hierarchy [5], which represents an \"actual\" relationship between the concepts identified by the proposed algorithm. Subsumption relationship is simple, but is considered an important type of relationship in any ontological structure, and we calculate two probability conditions for the same. In both concepts, we first calculate P (C1 | C2) and then P (C2 | C1) to establish a subsumption relationship, with the former probability being 1 and the latter less than 1. In other words, C1 subsumes C2 if the documents in which C2 occurs are a subset of the documents in which C1 occurs. For example, if we consider two concepts that choose Internet and network connection, the proposed method calculates P (Dialup Internet | Network connection) and P (Network connection | Dial \u2212 up Internet) and determines that the number of documents in which this subset of documents exists is a dialup."}, {"heading": "6. EVALUATION OF RESULTS", "text": "We have first created a man-made concept archive and kept it for verification against the man-made concepts. Precision calculates the fraction of the man-made concepts that are also man-made and reminds us of concepts extracted from a proposed algorithm that was also man-made. In gathering information, it is estimated that it is difficult to achieve high precision and retrieval at the same time, and with a measure called F1 we can balance the two. What is really positive here is the number of overlapping concepts between man-made concepts and concepts extracted from our proposed algorithm, false positive is the number of extracted concepts that are not really man-made concepts, and false negative is the number of man-made concepts that are overlooked by the concept-extracted concepts."}, {"heading": "7. CONCLUSIONS AND FUTURE WORK", "text": "This paper proposed a new framework for extracting realistic concepts from a large collection of unstructured text documents guided by a probabilistic modelling algorithm. Proposed methodology also deals with learning a subsumption hierarchy that exploits the \"is-one\" relationships between identified concepts widely used in ontology generation. Experiments with large datasets such as Reuters and BBC News Corpus show that the proposed methodology exceeds some of the already available algorithms and enables better concept identification with this framework. Due to the promising end results, we are mainly interested in the directions of measuring the scalability of the proposed framework by using larger datasets. Apart from the basic submission hierarchy, which represents an \"actual-one\" relationship, our future work will be to automate other relationships that exist between concepts, so that such a framework can complete the ongeneration process."}, {"heading": "12 V S Anoop, S Asharaf and Deepak P", "text": "Anoop V S is a full-time Ph.D Research Scholar at the Data Engineering Lab, Indian Institute of Information Technology and Management - Kerala (IIITM-K), Thiruvananthapuram, India. In 2014, he earned his Master in Computer Applications (MCA) -from IGNOU and Master of Philosophy in Computer Science from the Cochin University of Science and Technology (CUSAT), Kerala. He has several publications in international journals, conference volumes and book chapters. His research interests include Information Retrieval, Text Mining and NLP.Asharaf S is an Associate Professor at the Indian Institute of Information Technology and Management - Kerala (IIITM-K), Thiruvananthapuram, India. He holds a Ph.D. and Master of Engineering in Computer Science and Engineering -from the Indian Institute of Science, Bangalore. His areas of interest include algorithms, business models and software systems related to data mining, data analysis, information evaluation, computer science, computer science and computer science - from the Indian Institute of Science."}], "references": [{"title": "Semi-automated Ontology Creation for Semantic Search in Business Process Exploration", "author": ["Pospiech", "Sebastian", "Martin Pelke", "Robert Mertens"], "venue": "IEEE Tenth International Conference on Semantic Computing (ICSC).,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2016}, {"title": "Exploring events and distributed representations of text in multidocument summarization", "author": ["Marujo", "Lus"], "venue": "Knowledge-Based Systems,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2016}, {"title": "Aspect term extraction for sentiment analysis in large movie reviews us ing Gini Index feature selection method and SVM classifier", "author": ["AS Manek", "PD Shenoy", "MC Mohan", "KR. Venugopal"], "venue": "World Wide Web,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2016}, {"title": "Probabilistic topic models", "author": ["M Steyvers", "T. Griffiths"], "venue": "Handbook of latent semantic analysis.,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2007}, {"title": "Deriving concept hierarchies from text", "author": ["M Sanderson", "B. Croft"], "venue": "InProceedings of the 22nd annual international ACM SIGIR conference on Research and development in information retrieval,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1999}, {"title": "A phrase-discovering topic model using hierarchical pitman-yor processes", "author": ["RV Lindsey", "WP Headden III", "MJ. Stipicevic"], "venue": "InProceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, (pp. 214-222),", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2012}, {"title": "Scalable topical phrase mining from text corpora", "author": ["A El-Kishky", "Y Song", "C Wang", "CR Voss", "J. Han"], "venue": "Proceedings of the VLDB Endowment.,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2014}, {"title": "Topical n-grams: Phrase and topic discovery, with an application to information retrieval", "author": ["X Wang", "A McCallum", "X. Wei"], "venue": "InSeventh IEEE International Conference on Data Mining (ICDM", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2007}, {"title": "ACE: improving search engines via Automatic Concept Extraction", "author": ["Ramirez PM", "Mattmann CA"], "venue": "InInformation Reuse and Integration,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2004}, {"title": "Learning algorithms for keyphrase extraction", "author": ["Turney PD"], "venue": "Information Retrieval.,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2000}, {"title": "Towards the web of concepts: Extracting concepts from large datasets", "author": ["A Parameswaran", "H Garcia-Molina", "A. Rajaraman"], "venue": "Proceedings of the VLDB Endowment.,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2010}, {"title": "Automated concept extraction from plain text", "author": ["B Gelfand", "M Wulfekuler", "PunchWF"], "venue": "AAAI", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1998}, {"title": "A graph-based approach to commonsense concept extraction and semantic similarity detection", "author": ["D Rajagopal", "E Cambria", "D Olsher", "K. Kwok"], "venue": "In Proceedings of the 22nd interna Learning Concept Hierarchies through Probabilistic Topic Modeling  11 tional conference on World Wide Web companion,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2013}, {"title": "Learning user information interests through extraction of semantically significant phrases", "author": ["B Krulwich", "C. Burkey"], "venue": "InProceedings of the AAAI spring symposium on machine learning in information access,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1996}, {"title": "KEA: Practical automatic keyphrase extraction", "author": ["IH Witten", "GW Paynter", "E Frank", "C Gutwin", "CG. Nevill-Manning"], "venue": "InProceedings of the fourth ACM conference on Digital libraries.,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1999}, {"title": "KPSpotter: a flexible information gain-based keyphrase extraction system", "author": ["M Song", "IY Song", "X. Hu"], "venue": "InProceedings of the 5th ACM international workshop on Web information and data management.,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2003}, {"title": "Automatic recognition of multi-word terms", "author": ["K Frantzi", "S Ananiadou", "H. Mima"], "venue": "the c-value/nc-value method. International Journal on Digital Libraries.,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2000}, {"title": "Probabilistic latent semantic indexing", "author": ["T. Hofmann"], "venue": "InProceedings of the 22nd annual international ACM SIGIR conference on Research and development in information retrieval,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1999}, {"title": "Latent dirichlet allocation", "author": ["DM Blei", "AY Ng", "MI. Jordan"], "venue": "Journal of machine Learning research.,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2003}, {"title": "Latent semantic analysis", "author": ["Dumais ST"], "venue": "Annual review of information science and technology.,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2004}, {"title": "Probabilistic latent semantic indexing", "author": ["T. Hofmann"], "venue": "InProceedings of the 22nd annual international ACM SIGIR conference on Research and development in information retrieval,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 1999}, {"title": "Latent dirichlet allocation", "author": ["DM Blei", "AY Ng", "MI. Jordan"], "venue": "Journal of machine Learning research.,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2003}, {"title": "NLTK: the natural language toolkit", "author": ["S. Bird"], "venue": "InProceedings of the COLING/ACL on Interactive presentation sessions,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2006}, {"title": "RCV1: A New Benchmark Collection for Text Categorization Research", "author": ["Lewis D", "Y Yang", "T Rose", "F. Li"], "venue": "Journal of Machine Learn ing Research,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2004}, {"title": "Practical solutions to the problem of diagonal dominance in kernel document clustering", "author": ["D Greene", "P. Cunningham"], "venue": "InProceedings of the 23rd international conference on Machine learning,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2006}], "referenceMentions": [{"referenceID": 0, "context": "Extraction of such entities are non-trivial for applications such as automated ontology generation [1], document summarization [2] and aspect oriented sentiment analysis [3] to name a few.", "startOffset": 99, "endOffset": 102}, {"referenceID": 1, "context": "Extraction of such entities are non-trivial for applications such as automated ontology generation [1], document summarization [2] and aspect oriented sentiment analysis [3] to name a few.", "startOffset": 127, "endOffset": 130}, {"referenceID": 2, "context": "Extraction of such entities are non-trivial for applications such as automated ontology generation [1], document summarization [2] and aspect oriented sentiment analysis [3] to name a few.", "startOffset": 170, "endOffset": 173}, {"referenceID": 3, "context": "Models such as Probabilistic topic models [4] and Latent Dirichlet Allocation (LDA) are some such flavors of topic modeling that attained significant attention.", "startOffset": 42, "endOffset": 45}, {"referenceID": 4, "context": "Then we make use of a subsumption relation [5] (\u201dis-a\u201d) to connect concepts which are related thus forms a hierarchy of concepts.", "startOffset": 43, "endOffset": 46}, {"referenceID": 5, "context": "Phrase discovering topic model [6] that uses pitman-yor process and TopMine [7] were two notable works that proposed algorithms for mining topical phrases from text documents.", "startOffset": 31, "endOffset": 34}, {"referenceID": 6, "context": "Phrase discovering topic model [6] that uses pitman-yor process and TopMine [7] were two notable works that proposed algorithms for mining topical phrases from text documents.", "startOffset": 76, "endOffset": 79}, {"referenceID": 7, "context": "Another work which uses topic models for generating multi-word phrases was the topical n-gram [8].", "startOffset": 94, "endOffset": 97}, {"referenceID": 8, "context": "Automatic Concept Extractor (ACE), a system specifically designed for extracting concepts from HTML pages and making use of the text body and some visual clues on HTML tags for identifying potential concepts was proposed by Ramirez and Mattmann [9].", "startOffset": 245, "endOffset": 248}, {"referenceID": 9, "context": "Turney[10] proposed another system named GenEx, which employed a genetic algorithm supported rule learning mechanism for concept extraction.", "startOffset": 6, "endOffset": 10}, {"referenceID": 10, "context": "[11] which uses techniques similar to association rule mining.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[12] for concept extraction from plain text and used these to form a closely tied semantic relations graph for representing relationships between them.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[13] introduced another graph based approach for commonsense concept extraction and detection of semantic similarity among those concepts.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "Another work reported in this domain is the method proposed by Krulwich and Burkey [14] which uses a simple heuristics rule based approach to extract key phrases from document by considering visual clues such as the usage of bold and italic characters as features.", "startOffset": 83, "endOffset": 87}, {"referenceID": 14, "context": "al[15] was reported in the concept extraction literatures which creates a Naive Bayes learning model with known key phrases extracted from training documents and uses this model for inferring key phrases from new set of documents.", "startOffset": 2, "endOffset": 6}, {"referenceID": 15, "context": "[16] proposed a method which uses the information gain measure for ranking candidate key phrases based on some distance and tf-idf features which was first introduced in [15].", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[16] proposed a method which uses the information gain measure for ranking candidate key phrases based on some distance and tf-idf features which was first introduced in [15].", "startOffset": 170, "endOffset": 174}, {"referenceID": 16, "context": "[17] which extracts multi-word terms from medical documents and named as C/NC method.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "An automated document indexing method based on a latent class model for factor analysis of count data in the latent semantic space has been introduced by Thomas Hofman [18].", "startOffset": 168, "endOffset": 172}, {"referenceID": 18, "context": "[19] introduced a new topic modeling algorithm known as Latent Dirichlet Allocation (LDA) which is more efficient and attractive than PLSI.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "The process of generating a document with n words by LDA can be described as follows[19]:", "startOffset": 84, "endOffset": 88}, {"referenceID": 18, "context": "The parameter \u03b2 is a hyper-parameter determining the number of times words are sampled from a topic [19], before any word of the corpus is observed.", "startOffset": 100, "endOffset": 104}, {"referenceID": 19, "context": "researchers are making use of topic modeling algorithms such as Latent Semantic Analysis (LSA) [20], Probabilistic Latent Semantic Indexing (pLSI) [21], Latent Dirichlet Allocation (LDA) [22] etc extensively for bringing out the themes or so called \u201dtopics\u201d from high dimensional unstructured data.", "startOffset": 95, "endOffset": 99}, {"referenceID": 20, "context": "researchers are making use of topic modeling algorithms such as Latent Semantic Analysis (LSA) [20], Probabilistic Latent Semantic Indexing (pLSI) [21], Latent Dirichlet Allocation (LDA) [22] etc extensively for bringing out the themes or so called \u201dtopics\u201d from high dimensional unstructured data.", "startOffset": 147, "endOffset": 151}, {"referenceID": 21, "context": "researchers are making use of topic modeling algorithms such as Latent Semantic Analysis (LSA) [20], Probabilistic Latent Semantic Indexing (pLSI) [21], Latent Dirichlet Allocation (LDA) [22] etc extensively for bringing out the themes or so called \u201dtopics\u201d from high dimensional unstructured data.", "startOffset": 187, "endOffset": 191}, {"referenceID": 23, "context": "Dataset Collection and Preprocessing We are using publicly available datasets such as Reuters Corpus Volume 1 dataset[24] and BBC News Dataset[25] for the experiment.", "startOffset": 117, "endOffset": 121}, {"referenceID": 24, "context": "Dataset Collection and Preprocessing We are using publicly available datasets such as Reuters Corpus Volume 1 dataset[24] and BBC News Dataset[25] for the experiment.", "startOffset": 142, "endOffset": 146}, {"referenceID": 22, "context": "For this experiment, Natural Language Toolkit (NLTK) [23] has been used which contains libraries for Natural Language Processing for Python programming language.", "startOffset": 53, "endOffset": 57}, {"referenceID": 4, "context": "Concept Hierarchy Learning Concept hierarchy learning module concentrates on leveraging a subsumption hierarchy[5] depicting an \u201dis-a\u201d relation between the concepts identified by the proposed algorithm.", "startOffset": 111, "endOffset": 114}], "year": 2016, "abstractText": "With the advent of semantic web, various tools and techniques have been introduced for presenting and organizing knowledge. Concept hierarchies are one such technique which gained significant attention due to its usefulness in creating domain ontologies that are considered as an integral part of semantic web. Automated concept hierarchy learning algorithms focus on extracting relevant concepts from unstructured text corpus and connect them together by identifying some potential relations exist between them. In this paper, we propose a novel approach for identifying relevant concepts from plain text and then learns hierarchy of concepts by exploiting subsumption relation between them. To start with, we model topics using a probabilistic topic model and then make use of some lightweight linguistic process to extract semantically rich concepts. Then we connect concepts by identifying an \u201dis-a\u201d relationship between pair of concepts. The proposed method is completely unsupervised and there is no need for a domain specific training corpus for concept extraction and learning. Experiments on large and real-world text corpora such as BBC News dataset and Reuters News corpus shows that the proposed method outperforms some of the existing methods for concept extraction and efficient concept hierarchy learning is possible if the overall task is guided by a probabilistic topic modeling algorithm.", "creator": "LaTeX with hyperref package"}}}