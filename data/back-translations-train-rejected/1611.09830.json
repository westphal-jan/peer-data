{"id": "1611.09830", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-Nov-2016", "title": "NewsQA: A Machine Comprehension Dataset", "abstract": "We present NewsQA, a challenging machine comprehension dataset of over 100,000 question-answer pairs. Crowdworkers supply questions and answers based on a set of over 10,000 news articles from CNN, with answers consisting in spans of text from the corresponding articles. We collect this dataset through a four-stage process designed to solicit exploratory questions that require reasoning. A thorough analysis confirms that NewsQA demands abilities beyond simple word matching and recognizing entailment. We measure human performance on the dataset and compare it to several strong neural models. The performance gap between humans and machines (25.3% F1) indicates that significant progress can be made on NewsQA through future research. The dataset is freely available at datasets.maluuba.com/NewsQA.", "histories": [["v1", "Tue, 29 Nov 2016 20:38:07 GMT  (760kb,D)", "http://arxiv.org/abs/1611.09830v1", "Under review for ICLR 2016"], ["v2", "Thu, 22 Dec 2016 18:12:57 GMT  (760kb,D)", "http://arxiv.org/abs/1611.09830v2", "Under review for ICLR 2016"], ["v3", "Tue, 7 Feb 2017 16:27:59 GMT  (760kb,D)", "http://arxiv.org/abs/1611.09830v3", null]], "COMMENTS": "Under review for ICLR 2016", "reviews": [], "SUBJECTS": "cs.CL cs.AI", "authors": ["adam trischler", "tong wang", "xingdi yuan", "justin harris", "alessandro sordoni", "philip bachman", "kaheer suleman"], "accepted": false, "id": "1611.09830"}, "pdf": {"name": "1611.09830.pdf", "metadata": {"source": "CRF", "title": "NEWSQA: A MACHINE COMPREHENSION DATASET", "authors": ["Adam Trischler", "Tong Wang", "Xingdi Yuan", "Justin Harris", "Alessandro Sordoni", "Philip Bachman", "Kaheer Suleman"], "emails": ["k.suleman}@maluuba.com"], "sections": [{"heading": null, "text": "We present NewsQA, a sophisticated machine understanding dataset with over 100,000 question-and-answer pairs. Crowdworkers provide questions and answers based on a set of over 10,000 CNN news articles, the answers to which consist of passages from the corresponding articles. We collect this dataset in a four-step process designed to answer explorative questions that require reasoning. In-depth analysis confirms that NewsQA requires skills that go beyond simple word comparisons and correlation recognition. We measure human performance in the dataset and compare it to several strong neural models. The performance gap between humans and machines (25.3% F1) indicates that future research can make significant progress in NewsQA. The datasets.maluuba.com / NewsQA is freely available."}, {"heading": "1 INTRODUCTION", "text": "In this sense, the understanding of written language by machines, at an almost human level, is too great to allow a broad class of artificial intelligence applications. In human students, we evaluate reading comprehension by asking questions based on a passage of text and then evaluating a student's answers. Such comprehension tests are attractive because they are objectively graded and measure a range of important skills, from basic knowledge to causal reasoning to conclusions (Richardson et al., 2013). To teach the competence of machines, the research community has taken a similar approach to machine understanding (MC). In recent years, we have seen the publication of a variety of MC datasets, which generally consist of (document, question, answer) three-dimensional to be used in a supervised learning framework. Existing datasets vary in size, difficulty and collection methods used by Rajpurkar et al."}, {"heading": "2 RELATED DATASETS", "text": "NewsQA follows in the tradition of several current understanding datasets. These differ in size, difficulty and survey methodology, and each has its own distinctive features. We agree with Bajgar et al. (2016), who said that \"models could certainly benefit from the widest possible collection of data sets.\" We discuss this collection below."}, {"heading": "2.1 MCTEST", "text": "MCTest (Richardson et al., 2013) is a crowdsourced collection of 660 children's stories at elementary school level with related questions and answers. The stories are fictitious to ensure that the answer must be found in the text itself and is carefully limited to what a young child can understand. Each question contains a set of 4 candidate answers ranging from single words to full explanatory sentences. Questions are designed to require rudimentary reasoning and synthesis of information across sentences, which makes the data set quite difficult. To make matters worse, the size of the data set limits the formation of expressive statistical models. Nevertheless, newer understanding models have performed well at MCTest (Sachan et al., 2015; Wang et al., 2015), including a highly structured neural model (Trischler et al., 2016a)."}, {"heading": "2.2 CNN/DAILY MAIL", "text": "The CNN / Daily Mail corpus (Hermann et al., 2015) consists of news articles scraped from the media with corresponding Cloze-style questions. Cloze questions are synthetically constructed by removing a single unit of abstract summarizing points that accompany each article (presumably written by human authors). Therefore, determining the correct answer is usually based on recognizing the textual entanglement between article and question. These units within an article are identified and anonymized in a pre-processing stage and form the group of candidate answers; compared to NewsQA, where answers often contain longer phrases and no candidates are given. Since the Cloze process runs automatically, it is easy to collect a significant amount of data to support in-depth approaches: CNN / Daily Mail contains about 1.4 million question-answer pairs. However, Chen et al. (2016) have shown that the task requires limited reasoning and, in fact, the most powerful 2016 models (Kleet, 6c and Trial)."}, {"heading": "2.3 CHILDREN\u2019S BOOK TEST", "text": "The Children's Book Test (CBT) (Hill et al., 2016) was conducted using a process similar to that used by CNN / Daily Mail. Text passages are 20-sentence extracts from children's books available through Project Gutenberg; questions are generated by deleting a single word in the next (i.e. 21st) sentence. As a result, the CBT evaluates word predictions based on context. It is a comprehension task in that understanding is likely to be necessary for this prediction, but understanding may be insufficient and other mechanisms may be more important."}, {"heading": "2.4 BOOKTEST", "text": "Bajgar et al. (2016) convincingly argue that, because existing data sets are not large enough, we have not yet reached the full capacity of existing understanding models. As a remedy, they present BookTest. This is an extension of the designated entity and common noun layers of CBT that increases their size by more than 60-fold. Bajgar et al. (2016) show that training with the extended data set leads to a model that matches human performance on CBT. This is impressive and suggests that much can be gained from more data, but we reiterate our concerns about the relevance of predicting stories as a task of understanding. We also want to promote more efficient learning from less data."}, {"heading": "2.5 SQUAD", "text": "The most closely related understanding dataset to NewsQA is SQuAD (Rajpurkar et al., 2016), which consists of natural language questions asked by crowdworkers about paragraphs of Wikipedia articles with a high PageRank. As in NewsQA, each answer consists of a span of the corresponding paragraph and no candidates are provided. Despite the effort of manual labeling, the size of SQuAD is significant and lends itself to in-depth learning approaches: 107,785 question-answer pairs based on 536 articles. SQuAD is a challenging comprehension task where humans far outperform machines. Authors measured human accuracy at 90.5% F1 (we measured human F1 at 82.0% using a different methodology), while the strongest published model to date reached only 70.0% F1 (Wang & Jiang, 2016b)."}, {"heading": "3 COLLECTION METHODOLOGY", "text": "We captured NewsQA in a four-step process: article curation, question sourcing, answer sourcing, and validation. We also applied a post-processing step with consolidation of response agreements and merging to improve the usability of the dataset."}, {"heading": "3.1 ARTICLE CURATION", "text": "We source articles from CNN according to the script created by Hermann et al. (2015) for CNN / Daily Mail. From the returned set of 90,266 articles, we randomly select 12,744 articles covering a wide range of topics, including politics, business, and current events. Articles are randomly divided into a training set (90%), a development set (5%), and a test set (5%)."}, {"heading": "3.2 QUESTION SOURCING", "text": "Like Richardson et al. (2013), we want to encourage thinking in understanding models. We are also interested in questions that in some ways model human curiosity and reflect actual human uses of information search. Along a similar line, we consider it an important (though hitherto overlooked) ability of an understanding model to recognize when given information is insufficient, so we are also interested in questions that may not have sufficient evidence in the text. Our phase of questioning was designed to ask questions of this kind and was deliberately separated from the phase of answering for the same reason. Questioners (a specific group of crowdworkers) only see the headline of a news article and its summary points (also available on CNN); they do not see the full article itself; they are asked to formulate a question from that incomplete information."}, {"heading": "3.3 ANSWER SOURCING", "text": "A second group of crowdworkers (respondents) provide answers. Although this separation of question and answer increases overall cognitive stress, we assume that such a relief would encourage more complex questions for questioners. Respondents are given a full article along with a crowdsourced question and are entrusted with determining the answer. They can also reject the question as nonsensical or choose a zero answer if the article contains insufficient information. Answers are given by clicking and highlighting words in the article, while instructions encourage the group of respondents to persist in a single continuous span (again, we include a sample request in the appendix). For each question, we ask answers from multiple crowdworkers with the aim of reaching an agreement between at least two respondents."}, {"heading": "3.4 VALIDATION", "text": "Crowdsourcing is a powerful tool, but it is not without its dangers. To obtain a data set of the highest possible quality, we use a validation process that mitigates some of these problems. In validation, a third group of crowdworkers sees the full article, a question and the unique answers to this question. We assign these employees to choose the best answer from the candidate list or reject all answers. In these questions, validation was performed after the previous phase without an answer agreement, which accounts for 43.2% of all questions."}, {"heading": "3.5 ANSWER MARKING AND CLEANUP", "text": "After validation, 86.0% of all questions in NewsQA have answers that have been agreed upon by at least two separate crowdworkers - either in the first stage of the answer or in the selection of the top answers. This improves the quality of the data set. We choose to include the questions in the corpus without agreed answers, but they are specially marked. Such questions could be treated as zero answers and used to train models that are aware of poorly asked questions. As a final cleanup step, we combine answer spans that are less than 3 words apart (punctuation is discounted). We find that 5.68% of answers consist of multiple spans, while 71.3% of multispans are within the threshold of 3 words. If we look at the data more closely, the multi-track answers are often lists."}, {"heading": "4 DATA ANALYSIS", "text": "We provide in-depth analysis of NewsQA to demonstrate its challenge and usefulness as a benchmark for machine understanding, focusing on the types of answers that appear in the data set and the various forms of reasoning required to solve them."}, {"heading": "4.1 ANSWER TYPES", "text": "Following Rajpurkar et al. (2016), we categorize the answers according to their linguistic type (see Table 1). This categorization is based on Stanford CoreNLP to generate constituency parses, POS tags, and NER tags for response spans (see Rajpurkar et al. (2016) for more details).The table shows that the majority of responses (23.1%) are common noun sentences, according to which the answers are fairly evenly distributed between the person (14.4%), sentence sentences (14.0%), numerical (12.0%), and others (11.1%).The answers in NewsQA are clearly linguistically different.The ratio in Table 1 only takes into account cases where there is a response span, and the addition of this sentence includes questions with an agreed null word (5.8% of the full corpus) and answers without consent after validation (4.5% of the full corpus)."}, {"heading": "4.2 REASONING TYPES", "text": "The forms of reasoning required to solve NewsQA directly affect the skills that the models learn from the dataset. We have layered types of reasoning based on a variation of the taxonomy presented by Chen et al. (2016) in their analysis of the CNN / Daily Mail dataset. Types are as follows, in ascending order of difficulty: 1. Word Matching: Important words in the question coincide exactly with words in the response span, so that a keyword search algorithm could do well in that subset. 2. Paraphrase: A single sentence in the article contains or paraphrases the question. Paraphrase recognition may require synonymy and word knowledge. 3. Conclusion: The answer must be derived from incomplete information in the article or by recognizing conceptual overlapping. This is typically based on world knowledge. 4. Synthesis: The answer can only be derived by synthesis of information distributed over multiple sentences."}, {"heading": "5 BASELINE MODELS", "text": "We test the performance of three understanding systems on NewsQA: human data analysts and two neural models; the first is Wang & Jiang's match LSTM (mLSTM) system (2016b); the second is a mathematically cheaper model of our own design, which we describe below, but ignore the personal details of our analysts."}, {"heading": "5.1 MATCH-LSTM", "text": "There are three stages involved in the mLSTM model: First, LSTM networks encode the document and the question (represented by GloVe Word Embeddings (Pennington et al., 2014) as hidden state sequences; second, an mLSTM network (Wang & Jiang, 2016a) compares the document encodings with the question encodings; this network processes the document sequentially and with each token using an attention mechanism to obtain a weighted vector representation of the question; the weighted combination is linked to the encoding of the current token and fed into a standard LSTM; and finally, a pointer network uses the hidden states of the mLSTM to select the boundaries of the response span. We refer the reader to Wang & Jiang (2016a; b) for full details. At the time of writing, mLSTM is on SQuAD state of the art (see Table 3), so it is more seamless to QA."}, {"heading": "5.2 THE BILINEAR ANNOTATION RE-ENCODING BOUNDARY (BARB) MODEL", "text": "We are the first to deal with the question of whether we need to deal with the question of whether we need to deal with the question of whether we need to deal with the question of whether we even agree with the question we want to answer with the question. (We are the second to deal with the question) (We are the third to deal with the question). (We are the third to deal with the question of whether we disagree with the question we want to answer with the question.) (We are the third to deal with the question.) (We are the third to deal with the question.) (We are the third to deal with the question). (We are the third to deal with the question.) (We are both to deal with the question.) (We are the third to deal with the question of the question.) (We are the third to deal with the question of the question.) (We are the third to deal with the question of the question.) (We are the third to deal with the question of the question. (We are the third to deal with the question.) (We are the third to deal with the question of the question.) (We are the third to deal with the question of the question. (We are the third to deal with the question.) (We are the third to deal with the question of the question.) (We are the third to deal with the question of the question of the question.) (We are the third to deal with the question of the question of the question. (We are the third to deal with the third to the question.) (We are the third to deal with the question of the question of the question of the question. (We are the third to deal with the question of the question.). (We are the third to deal with the question of the question of the question of the question of the question of the question. (We are the third to the third to deal with the question.) (We are the question of the question of the third to the question of the question of the question of the question of the question. (We are the third to the question of the question of the question.). (We are the third to the third to deal with the question of the question of the question of the question of the question of the question of the question of the question. (We are the question of the third to the question of the question of the"}, {"heading": "6 EXPERIMENTS2", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6.1 HUMAN EVALUATION", "text": "The exact match (EM) of our students is relatively low at 55.0%. This is because in many cases there are several ways to select the same answer, e.g. \"1996\" versus \"1996.\" We also compared human performance for answers that matched with and without validation, and found a difference of only 1.4 percentage points F1. This indicates that our validation phase delivers good quality answers. The original SQuAD assessment of human performance compares different answers given by crowdworkers; for a more accurate comparison with NewsQA, we replicated our human test with the same \"students.\" We measured their responses with the second group of crowdsourced responses in the SQuAD development group, as in Rajpurkar et al. (2016). Our students scored 82.0% F1."}, {"heading": "6.2 MODEL PERFORMANCE", "text": "The performance (measured by EM and F1 with the official SQuAD evaluation script) of the basic models and humans is shown in Table 3. For NewsQA, we use the same hyperparameters that gave the best performance on SQuAD. In NewsQA, the gap between man and machine is an astonishing 25.3 points F1 - much larger than the gap on SQuAD (11.1% and 19.6%, respectively, depending on the human evaluation method).The gaps indicate a large margin for improvement through automated methodologies.Figure 1 stratifies model performance by response type (left) and argumentation type (right) as defined in Sections 4.1 and 4.2 respectively. The answer type stratification suggests that the model points better to named units. On the other hand, the argumentation type stratification shows that questions that require conclusions and synthesis are not surprisingly difficult for the model."}, {"heading": "6.3 SENTENCE-LEVEL SCORING", "text": "We propose a simple sentence-level subtask to quantify the relative difficulty of NewsQA 0.0 3.0 3.0 3.0 3.0 3.0 3.0 3.0 3.0 3.0 3.0 3.0 3.0 3.0 3.0 3.0. In the face of a document and a question, the goal is to find the sentence that contains the answer2All experiments in this section, use the subset of NewsQA records with answer agreements (92,487 samples for training, 5,103 for validation and 5,251 for exam). We abandon the challenge to identify the unanswerable questions for future work."}, {"heading": "7 CONCLUSION", "text": "We have introduced a challenging new understanding dataset: NewsQA. We have gathered over 100,000 examples of NewsQA with the help of teams of crowdworkers who read CNN articles or highlights differently, ask questions about them, and find answers. Our methodology delivers different types of answers and a significant proportion of questions that require some thinking skills, making the corpus challenging, as confirmed by the large performance gap between humans and deep neural models (25.3 percentage points F1). Due to its size and complexity, NewsQA represents a significant expansion of the existing corpus of understanding datasets. We hope that our corpus will drive further advances in machine understanding and guide the development of artificial intelligence."}, {"heading": "ACKNOWLEDGMENTS", "text": "The authors thank Cag G\u00fcl\u00e7ehre, Sandeep Subramanian and Saizheng Zhang for helpful discussions and Pranav Subramani for the prints."}, {"heading": "B DATA COLLECTION USER INTERFACE", "text": "Here we present the user interfaces used in question sourcing, answer sourcing and question-answer validation."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio"], "venue": null, "citeRegEx": "Bahdanau et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Embracing data abundance: Booktest dataset for reading comprehension", "author": ["Ondrej Bajgar", "Rudolf Kadlec", "Jan Kleindienst"], "venue": "arXiv preprint arXiv:1610.00956,", "citeRegEx": "Bajgar et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Bajgar et al\\.", "year": 2016}, {"title": "Theano: a CPU and GPU math expression compiler", "author": ["J. Bergstra", "O. Breuleux", "F. Bastien", "P. Lamblin", "R. Pascanu", "G. Desjardins", "J. Turian", "D. WardeFarley", "Y. Bengio"], "venue": "Proc. of SciPy,", "citeRegEx": "Bergstra et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Bergstra et al\\.", "year": 2010}, {"title": "A thorough examination of the cnn / daily mail reading comprehension", "author": ["Danqi Chen", "Jason Bolton", "Christopher D. Manning"], "venue": null, "citeRegEx": "Chen et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2016}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["Xavier Glorot", "Yoshua Bengio"], "venue": "In Aistats,", "citeRegEx": "Glorot and Bengio.,? \\Q2010\\E", "shortCiteRegEx": "Glorot and Bengio.", "year": 2010}, {"title": "Teaching machines to read and comprehend", "author": ["Karl Moritz Hermann", "Tomas Kocisky", "Edward Grefenstette", "Lasse Espeholt", "Will Kay", "Mustafa Suleyman", "Phil Blunsom"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Hermann et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hermann et al\\.", "year": 2015}, {"title": "The goldilocks principle: Reading children\u2019s books with explicit memory representations", "author": ["Felix Hill", "Antoine Bordes", "Sumit Chopra", "Jason Weston"], "venue": null, "citeRegEx": "Hill et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Hill et al\\.", "year": 2016}, {"title": "Text understanding with the attention sum reader network", "author": ["Rudolf Kadlec", "Martin Schmid", "Ondrej Bajgar", "Jan Kleindienst"], "venue": "arXiv preprint arXiv:1603.01547,", "citeRegEx": "Kadlec et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Kadlec et al\\.", "year": 2016}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba"], "venue": "ICLR,", "citeRegEx": "Kingma and Ba.,? \\Q2015\\E", "shortCiteRegEx": "Kingma and Ba.", "year": 2015}, {"title": "On the difficulty of training recurrent neural networks", "author": ["Razvan Pascanu", "Tomas Mikolov", "Yoshua Bengio"], "venue": "ICML (3),", "citeRegEx": "Pascanu et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Pascanu et al\\.", "year": 2013}, {"title": "Glove: Global vectors for word representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher D Manning"], "venue": "In EMNLP,", "citeRegEx": "Pennington et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Squad: 100,000+ questions for machine comprehension of text", "author": ["Pranav Rajpurkar", "Jian Zhang", "Konstantin Lopyrev", "Percy Liang"], "venue": "arXiv preprint arXiv:1606.05250,", "citeRegEx": "Rajpurkar et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Rajpurkar et al\\.", "year": 2016}, {"title": "Mctest: A challenge dataset for the open-domain machine comprehension of text", "author": ["Matthew Richardson", "Christopher JC Burges", "Erin Renshaw"], "venue": "In EMNLP,", "citeRegEx": "Richardson et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Richardson et al\\.", "year": 2013}, {"title": "Learning answerentailing structures for machine comprehension", "author": ["Mrinmaya Sachan", "Avinava Dubey", "Eric P Xing", "Matthew Richardson"], "venue": "In Proceedings of ACL,", "citeRegEx": "Sachan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sachan et al\\.", "year": 2015}, {"title": "Exact solutions to the nonlinear dynamics of learning in deep linear neural networks", "author": ["Andrew M Saxe", "James L McClelland", "Surya Ganguli"], "venue": "arXiv preprint arXiv:1312.6120,", "citeRegEx": "Saxe et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Saxe et al\\.", "year": 2013}, {"title": "Iterative alternating neural attention for machine reading", "author": ["Alessandro Sordoni", "Philip Bachman", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1606.02245,", "citeRegEx": "Sordoni et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Sordoni et al\\.", "year": 2016}, {"title": "A parallelhierarchical model for machine comprehension on sparse data", "author": ["Adam Trischler", "Zheng Ye", "Xingdi Yuan", "Jing He", "Philip Bachman", "Kaheer Suleman"], "venue": "In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "Trischler et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Trischler et al\\.", "year": 2016}, {"title": "Natural language comprehension with the epireader", "author": ["Adam Trischler", "Zheng Ye", "Xingdi Yuan", "Kaheer Suleman"], "venue": "In EMNLP,", "citeRegEx": "Trischler et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Trischler et al\\.", "year": 2016}, {"title": "Machine comprehension with syntax, frames, and semantics", "author": ["Hai Wang", "Mohit Bansal", "Kevin Gimpel", "David McAllester"], "venue": "In Proceedings of ACL, Volume 2: Short Papers,", "citeRegEx": "Wang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2015}, {"title": "Learning natural language inference with lstm", "author": ["Shuohang Wang", "Jing Jiang"], "venue": "NAACL,", "citeRegEx": "Wang and Jiang.,? \\Q2016\\E", "shortCiteRegEx": "Wang and Jiang.", "year": 2016}, {"title": "Machine comprehension using match-lstm and answer pointer", "author": ["Shuohang Wang", "Jing Jiang"], "venue": "arXiv preprint arXiv:1608.07905,", "citeRegEx": "Wang and Jiang.,? \\Q2016\\E", "shortCiteRegEx": "Wang and Jiang.", "year": 2016}], "referenceMentions": [{"referenceID": 12, "context": "Such comprehension tests are appealing because they are objectively gradable and may measure a range of important abilities, from basic understanding to causal reasoning to inference (Richardson et al., 2013).", "startOffset": 183, "endOffset": 208}, {"referenceID": 12, "context": "(2016), most suffer from one of two shortcomings: those that are designed explicitly to test comprehension (Richardson et al., 2013) are too small for training data-intensive deep learning models, while those that are sufficiently large for deep learning (Hermann et al.", "startOffset": 107, "endOffset": 132}, {"referenceID": 5, "context": ", 2013) are too small for training data-intensive deep learning models, while those that are sufficiently large for deep learning (Hermann et al., 2015; Hill et al., 2016; Bajgar et al., 2016) are generated synthetically, yielding questions that are not posed in natural language and that may not test comprehension directly (Chen et al.", "startOffset": 130, "endOffset": 192}, {"referenceID": 6, "context": ", 2013) are too small for training data-intensive deep learning models, while those that are sufficiently large for deep learning (Hermann et al., 2015; Hill et al., 2016; Bajgar et al., 2016) are generated synthetically, yielding questions that are not posed in natural language and that may not test comprehension directly (Chen et al.", "startOffset": 130, "endOffset": 192}, {"referenceID": 1, "context": ", 2013) are too small for training data-intensive deep learning models, while those that are sufficiently large for deep learning (Hermann et al., 2015; Hill et al., 2016; Bajgar et al., 2016) are generated synthetically, yielding questions that are not posed in natural language and that may not test comprehension directly (Chen et al.", "startOffset": 130, "endOffset": 192}, {"referenceID": 3, "context": ", 2016) are generated synthetically, yielding questions that are not posed in natural language and that may not test comprehension directly (Chen et al., 2016).", "startOffset": 140, "endOffset": 159}, {"referenceID": 5, "context": "CNN articles were chosen as the source material because they have been used in the past (Hermann et al., 2015) and, in our view, machine comprehension systems are particularly suited to high-volume, rapidly changing information sources like news.", "startOffset": 88, "endOffset": 110}, {"referenceID": 7, "context": "Existing datasets vary in size, difficulty, and collection methodology; however, as pointed out by Rajpurkar et al. (2016), most suffer from one of two shortcomings: those that are designed explicitly to test comprehension (Richardson et al.", "startOffset": 99, "endOffset": 123}, {"referenceID": 1, "context": ", 2016; Bajgar et al., 2016) are generated synthetically, yielding questions that are not posed in natural language and that may not test comprehension directly (Chen et al., 2016). More recently, Rajpurkar et al. (2016) sought to overcome these deficiencies with their crowdsourced dataset, SQuAD.", "startOffset": 8, "endOffset": 221}, {"referenceID": 14, "context": "As Trischler et al. (2016a), Chen et al.", "startOffset": 3, "endOffset": 28}, {"referenceID": 3, "context": "(2016a), Chen et al. (2016), and others have argued, it is important for datasets to be sufficiently challenging to teach models the abilities we wish them to learn.", "startOffset": 9, "endOffset": 28}, {"referenceID": 3, "context": "(2016a), Chen et al. (2016), and others have argued, it is important for datasets to be sufficiently challenging to teach models the abilities we wish them to learn. Thus, in line with Richardson et al. (2013), our goal with NewsQA was to construct a corpus of questions that necessitates reasoning mechanisms, such as synthesis of information across different parts of an article.", "startOffset": 9, "endOffset": 210}, {"referenceID": 1, "context": "We agree with Bajgar et al. (2016) who have said \u201cmodels could certainly benefit from as diverse a collection of datasets as possible.", "startOffset": 14, "endOffset": 35}, {"referenceID": 12, "context": "MCTest (Richardson et al., 2013) is a crowdsourced collection of 660 elementary-level children\u2019s stories with associated questions and answers.", "startOffset": 7, "endOffset": 32}, {"referenceID": 13, "context": "Nevertheless, recent comprehension models have performed well on MCTest (Sachan et al., 2015; Wang et al., 2015), including a highly structured neural model (Trischler et al.", "startOffset": 72, "endOffset": 112}, {"referenceID": 18, "context": "Nevertheless, recent comprehension models have performed well on MCTest (Sachan et al., 2015; Wang et al., 2015), including a highly structured neural model (Trischler et al.", "startOffset": 72, "endOffset": 112}, {"referenceID": 5, "context": "The CNN/Daily Mail corpus (Hermann et al., 2015) consists of news articles scraped from those outlets with corresponding cloze-style questions.", "startOffset": 26, "endOffset": 48}, {"referenceID": 3, "context": "However, Chen et al. (2016) demonstrated that the task requires only limited reasoning and, in", "startOffset": 9, "endOffset": 28}, {"referenceID": 7, "context": "fact, performance of the strongest models (Kadlec et al., 2016; Trischler et al., 2016b; Sordoni et al., 2016) nearly matches that of humans.", "startOffset": 42, "endOffset": 110}, {"referenceID": 15, "context": "fact, performance of the strongest models (Kadlec et al., 2016; Trischler et al., 2016b; Sordoni et al., 2016) nearly matches that of humans.", "startOffset": 42, "endOffset": 110}, {"referenceID": 6, "context": "The Children\u2019s Book Test (CBT) (Hill et al., 2016) was collected using a process similar to that of CNN/Daily Mail.", "startOffset": 31, "endOffset": 50}, {"referenceID": 7, "context": "(2016) demonstrate that training on the augmented dataset yields a model (Kadlec et al., 2016) that matches human performance on CBT.", "startOffset": 73, "endOffset": 94}, {"referenceID": 11, "context": "The comprehension dataset most closely related to NewsQA is SQuAD (Rajpurkar et al., 2016).", "startOffset": 66, "endOffset": 90}, {"referenceID": 5, "context": "We retrieve articles from CNN using the script created by Hermann et al. (2015) for CNN/Daily Mail.", "startOffset": 58, "endOffset": 80}, {"referenceID": 12, "context": "Like Richardson et al. (2013) we want to encourage reasoning in comprehension models.", "startOffset": 5, "endOffset": 30}, {"referenceID": 11, "context": "Following Rajpurkar et al. (2016), we categorize answers based on their linguistic type (see Table 1).", "startOffset": 10, "endOffset": 34}, {"referenceID": 11, "context": "Following Rajpurkar et al. (2016), we categorize answers based on their linguistic type (see Table 1). This categorization relies on Stanford CoreNLP to generate constituency parses, POS tags, and NER tags for answer spans (see Rajpurkar et al. (2016) for more details).", "startOffset": 10, "endOffset": 252}, {"referenceID": 3, "context": "We stratified reasoning types using a variation on the taxonomy presented by Chen et al. (2016) in their analysis of the CNN/Daily Mail dataset.", "startOffset": 77, "endOffset": 96}, {"referenceID": 10, "context": "First, LSTM networks encode the document and question (represented by GloVe word embeddings (Pennington et al., 2014)) as sequences of hidden states.", "startOffset": 92, "endOffset": 117}, {"referenceID": 0, "context": "A bidirectional GRU network (Bahdanau et al., 2015) takes in di and encodes contextual states hi \u2208 R1 for the document.", "startOffset": 28, "endOffset": 51}, {"referenceID": 11, "context": "We measured their answers against the second group of crowdsourced responses in SQuAD\u2019s development set, as in Rajpurkar et al. (2016). Our students scored 82.", "startOffset": 111, "endOffset": 135}], "year": 2016, "abstractText": "We present NewsQA, a challenging machine comprehension dataset of over 100,000 question-answer pairs. Crowdworkers supply questions and answers based on a set of over 10,000 news articles from CNN, with answers consisting in spans of text from the corresponding articles. We collect this dataset through a fourstage process designed to solicit exploratory questions that require reasoning. A thorough analysis confirms that NewsQA demands abilities beyond simple word matching and recognizing entailment. We measure human performance on the dataset and compare it to several strong neural models. The performance gap between humans and machines (25.3% F1) indicates that significant progress can be made on NewsQA through future research. The dataset is freely available at datasets.maluuba.com/NewsQA.", "creator": "LaTeX with hyperref package"}}}