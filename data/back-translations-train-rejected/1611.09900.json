{"id": "1611.09900", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-Nov-2016", "title": "Context-aware Natural Language Generation with Recurrent Neural Networks", "abstract": "This paper studied generating natural languages at particular contexts or situations. We proposed two novel approaches which encode the contexts into a continuous semantic representation and then decode the semantic representation into text sequences with recurrent neural networks. During decoding, the context information are attended through a gating mechanism, addressing the problem of long-range dependency caused by lengthy sequences. We evaluate the effectiveness of the proposed approaches on user review data, in which rich contexts are available and two informative contexts, sentiments and products, are selected for evaluation. Experiments show that the fake reviews generated by our approaches are very natural. Results of fake review detection with human judges show that more than 50\\% of the fake reviews are misclassified as the real reviews, and more than 90\\% are misclassified by existing state-of-the-art fake review detection algorithm.", "histories": [["v1", "Tue, 29 Nov 2016 21:45:42 GMT  (3089kb,D)", "http://arxiv.org/abs/1611.09900v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["jian tang", "yifan yang", "sam carton", "ming zhang", "qiaozhu mei"], "accepted": false, "id": "1611.09900"}, "pdf": {"name": "1611.09900.pdf", "metadata": {"source": "CRF", "title": "Context-aware Natural Language Generation with Recurrent Neural Networks", "authors": ["Jian Tang", "Yifan Yang", "Sam Carton", "Ming Zhang", "Qiaozhu Mei"], "emails": ["tangjianpku@gmail.com,", "yang1fan2@gmail.com,", "cs@pku.edu.cn,", "qmei}@umich.edu"], "sections": [{"heading": "Introduction", "text": "In fact, the fact is that most of them will be able to demonstrate that they are able, that they are able to achieve their goals, and that they are able to achieve their goals."}, {"heading": "Related Work", "text": "Approaches to natural language generation can be roughly divided into two categories: the classical rules-based or template-based approaches, and the recent approaches with recurring neural networks that automatically learn the natural language generator from the data. Classical approaches usually define some rules or templates (Cheyer and Guzzoni 2014) by humans, but these are very brittle and difficult to generalize to solve different tasks and domains. Although there are some newer approaches aimed at learning the template structures from large amounts of corpus (Oh and Rudnicky 2000), the training data is very expensive to obtain and the final generation process still requires additional human craftsmanship characteristics."}, {"heading": "Problem Definition", "text": "Natural languages are usually associated with rich context information, e.g. time, place, which provide clues as to how natural languages are generated. In this work, we examine the context-sensitive generation of natural languages. In view of the context information, we want to generate the corresponding natural languages. First, we formally define the contexts as follows: Definition 1 (Contexts.) The contexts of natural languages refer to the situations in which they are generated. Each context is defined as a high-dimensional vector. Natural languages contexts can be either discrete or continuous characteristics. For example, the context can be a specific user or place; it can also be a continuous feature vector generated from other sources. For discrete characteristics, the context is usually presented with one-dimensional representations. Formally, we formulate our problem as follows: Definition 2 (Context-sensitive generation of natural language) Given a number of contexts {= Total C}, the words are a sequence to which K is suggested."}, {"heading": "Model", "text": "In this section, we present our proposed approaches to producing natural language in specific contexts. We first introduce the relapsing neural networks (RNN), which are very effective models for text generation, and then present our proposed approaches, which map a set of contexts to a text sequence."}, {"heading": "Recurrent Neural Network", "text": "The recursive neural network (RNN) models the generative process of sequence data = multiple functions that effectively group the information into a hidden state (a continuous representation) and then generate a new sample according to a probability distribution specified by the hidden state. Specifically, the hidden state ht from the sequence x1, x2,.., xt is recursively updated as: ht = f (ht \u2212 1, xt), (1) where f (\u00b7, \u00b7) is normally a nonlinear transformation, e.g. ht = tanh (Uht \u2212 1 + V xt))) (U, V are transformation matrices). The hidden state ht ht \u2212 bsums up the information of the entire sequences x1, x2,., xt, and the probability of generating the next words p (xt + 1 | x \u2264 t) is defined as asp (xt + 1 | x \u2264 t) = p (xt + 1 | exht (OTP + 1xt)."}, {"heading": "C2S: Contexts to Sequences", "text": "As already mentioned, natural languages are usually generated in certain contexts. (Instead of modelling the probability of observing a text sequence p (~ x), however, we are more interested in the probability of observing the x-th contexts, i.e., the conditional probability p (~ x). In this part, we present two generative models for modelling the conditional probabilities p (~ x | C), which are based on recurrent neural networks. Encoder. Our framework is based on the encoder decoder framework (Cho et al. 2014). The main idea is to encode the context information into a continuous semantic representation and then decode the semantic representation into a text sequence. We first introduce how the contexts of natural languages are encoded into a semantic representation. We represent the contexts as Set C = {~ ci} i = 1, K, where the semantic representation is converted into a text sequence."}, {"heading": "Experiments", "text": "In this section, we evaluate the effectiveness of the C2S and gC2S approach based on user rating data. Various tasks are evaluated, including voice modeling, detection of fake ratings by human judges or existing state-of-the-art algorithm for detecting fake ratings, mood classification for both real and fake ratings. First, we present the data sets to be used."}, {"heading": "Data Sets", "text": "We select the user review data because it contains rich contextual information, such as user, time, feelings, products. We select the sentiment ratings (from 1 to 5) and product IDs as contextual information that we believe are the most important factors influencing the review content. We use data from two websites: Amazon1 and TripAdvisor2. For the Amazon data, we select three most popular domains, including book, electronics and film; for the TripAdvisor data, it is the hotel domain. We select the most popular 20,000 words as a vocabulary and reviews that contain unknown words of more than 100 words in the Amazon data and more than 300 words in the TripAdvisor data, all of which are removed. All of the data is split into moves, validation, test data according to the 18: 1: 1 ratio. End data statistics are summarized in Table 1."}, {"heading": "Language Modeling", "text": "We start with the task of language modeling. Table 2 compares the performance of language modeling with the RNN, C2S, and gC2S approach. First, both the C2S and gC2S appear to outperform the vanilla RNN without context information either with the sentiment context, the product context, or their combination, showing that contextual information is actually helpful for understanding and predicting natural language. Second, the product context appears to be more informative than the sentiment context for language modeling, regardless of which contexts are used. It could be that there are many words in the ratings that are relevant to the product information. Second, when comparing the C2S and gC2S models, the gC2S model consistently outperforms the C2S model, regardless of which contexts are used. As explained earlier, this is because in the C2S model the contextual availability of the C2S model is significantly higher than the C2S-2S contextual availability and consequence."}, {"heading": "Fake Review Detection", "text": "To further evaluate the effectiveness of the gC2S model for generating natural language, we choose the task of detecting fake reviews, which aims to classify whether the reviews are written by real users or generated by the gC2S model. Real reviews are treated as positive, and fake reviews are treated as negative. However, for the rating data, we randomly select some products that have at least two real reviews for each rating scale in the Amazon data and one rating for each rating scale in the TripAdvisor data. Table 3 summarizes the number of reviews in each domain. Human Evaluation uses the Amazon Mechanical Turk to evaluate whether the reviews are not fake. We divide all the data into different batches. Each batch contains twenty reviews about the same product. We show the reviews of the products, the sentiment rating and the rating content for users to assess whether the reviews are written by real users or not."}, {"heading": "Sentiment Classification", "text": "The above results show that in certain contexts, the gC2S model is capable of generating very natural ratings that cannot be distinguished from real ratings, but how well do the generated ratings reflect the context information, such as how well the generated ratings express mood polarity? Therefore, in this part, we compare the results of the mood ratings for both the fake and the real ratings. There are two types of mood classification: finer granularity, i.e. feelings with five different ratings, and binary classification, where ratings with 4 and 5 ratings are treated as positive and ratings with 1 and 2 as negative. To perform the classification, we randomly sample 100,000 real ratings from the training data for the Mood Classifier. As for the rating data, a fake rating is generated for each test value according to its context. Table 6 and 7 summarize the results of the fineness or mood classification."}, {"heading": "Conclusion", "text": "In this paper, we examined the context-sensitive generation of natural language. We proposed two approaches, C2S and gC2S, which encode the contexts into semantic representations and then decipher the representations in text sequences.The gC2S model significantly outperforms the C2S model because it adds jump links between the context representations and the words in the sequences, allowing the information from the contexts to directly influence the generation of words.We evaluated our approaches in terms of user rating data. Experimental results show that more than 50% of the fake reviews generated by our approach are misclassified by human judges, and more than 90% of the reviews are misclassified by existing algorithms for detecting fake valuations.In the future, we plan to improve more context information, e.g. the user, the detailed descriptions of products, product prices, to integrate our approaches, and our approaches to misclassify our approaches to detect false ratings. In the future, we plan to evaluate more context information, e.g. the detailed descriptions of products, product prices, in terms based on the different event descriptions, and the different descriptions of the subject matter, and our approaches to evaluate the different scenarios in each case based on the different event descriptions."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["D. Bahdanau", "K. Cho", "Y. Bengio"], "venue": "arXiv preprint arXiv:1409.0473.", "citeRegEx": "Bahdanau et al\\.,? 2014", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Generating sentences from a continuous space", "author": ["S.R. Bowman", "L. Vilnis", "O. Vinyals", "A.M. Dai", "R. Jozefowicz", "S. Bengio"], "venue": "arXiv preprint arXiv:1511.06349.", "citeRegEx": "Bowman et al\\.,? 2015", "shortCiteRegEx": "Bowman et al\\.", "year": 2015}, {"title": "Context-aware query classification", "author": ["H. Cao", "D.H. Hu", "D. Shen", "D. Jiang", "J.-T. Sun", "E. Chen", "Q. Yang"], "venue": "Proceedings of the 32nd international ACM SIGIR conference on Research and development in information retrieval, 3\u201310. ACM.", "citeRegEx": "Cao et al\\.,? 2009", "shortCiteRegEx": "Cao et al\\.", "year": 2009}, {"title": "Method and apparatus for building an intelligent automated assistant", "author": ["A. Cheyer", "D. Guzzoni"], "venue": "US Patent 8,677,377.", "citeRegEx": "Cheyer and Guzzoni,? 2014", "shortCiteRegEx": "Cheyer and Guzzoni", "year": 2014}, {"title": "Generating sequences with recurrent neural networks", "author": ["A. Graves"], "venue": "arXiv preprint arXiv:1308.0850.", "citeRegEx": "Graves,? 2013", "shortCiteRegEx": "Graves", "year": 2013}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural computation 9(8):1735\u20131780.", "citeRegEx": "Hochreiter and Schmidhuber,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber", "year": 1997}, {"title": "Visualizing and understanding recurrent networks", "author": ["A. Karpathy", "J. Johnson", "F.-F. Li"], "venue": "arXiv preprint arXiv:1506.02078.", "citeRegEx": "Karpathy et al\\.,? 2015", "shortCiteRegEx": "Karpathy et al\\.", "year": 2015}, {"title": "A probabilistic approach to spatiotemporal theme pattern mining on weblogs", "author": ["Q. Mei", "C. Liu", "H. Su", "C. Zhai"], "venue": "Proceedings of the 15th international conference on World Wide Web, 533\u2013542. ACM.", "citeRegEx": "Mei et al\\.,? 2006", "shortCiteRegEx": "Mei et al\\.", "year": 2006}, {"title": "Context dependent recurrent neural network language model", "author": ["T. Mikolov", "G. Zweig"], "venue": "SLT, 234\u2013239.", "citeRegEx": "Mikolov and Zweig,? 2012", "shortCiteRegEx": "Mikolov and Zweig", "year": 2012}, {"title": "Stochastic language generation for spoken dialogue systems", "author": ["A.H. Oh", "A.I. Rudnicky"], "venue": "Proceedings of the 2000 ANLP/NAACL Workshop on Conversational systems-Volume 3, 27\u201332. Association for Computational Linguistics.", "citeRegEx": "Oh and Rudnicky,? 2000", "shortCiteRegEx": "Oh and Rudnicky", "year": 2000}, {"title": "Finding deceptive opinion spam by any stretch of the imagination", "author": ["M. Ott", "Y. Choi", "C. Cardie", "J.T. Hancock"], "venue": "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume 1, 309\u2013319. Association for Compu-", "citeRegEx": "Ott et al\\.,? 2011", "shortCiteRegEx": "Ott et al\\.", "year": 2011}, {"title": "A neural attention model for abstractive sentence summarization", "author": ["A.M. Rush", "S. Chopra", "J. Weston"], "venue": "arXiv preprint arXiv:1509.00685.", "citeRegEx": "Rush et al\\.,? 2015", "shortCiteRegEx": "Rush et al\\.", "year": 2015}, {"title": "A neural network approach to context-sensitive generation of conversational responses", "author": ["A. Sordoni", "M. Galley", "M. Auli", "C. Brockett", "Y. Ji", "M. Mitchell", "J.-Y. Nie", "J. Gao", "B. Dolan"], "venue": "arXiv preprint arXiv:1506.06714.", "citeRegEx": "Sordoni et al\\.,? 2015", "shortCiteRegEx": "Sordoni et al\\.", "year": 2015}, {"title": "Generating text with recurrent neural networks", "author": ["I. Sutskever", "J. Martens", "G.E. Hinton"], "venue": "Proceedings of the 28th International Conference on Machine Learning (ICML-11), 1017\u20131024.", "citeRegEx": "Sutskever et al\\.,? 2011", "shortCiteRegEx": "Sutskever et al\\.", "year": 2011}, {"title": "Larger-context language modelling", "author": ["T. Wang", "K. Cho"], "venue": "arXiv preprint arXiv:1511.03729.", "citeRegEx": "Wang and Cho,? 2015", "shortCiteRegEx": "Wang and Cho", "year": 2015}, {"title": "Semantically conditioned lstm-based natural language generation for spoken dialogue systems", "author": ["T.-H. Wen", "M. Gasic", "N. Mrksic", "P.-H. Su", "D. Vandyke", "S. Young"], "venue": "arXiv preprint arXiv:1508.01745.", "citeRegEx": "Wen et al\\.,? 2015b", "shortCiteRegEx": "Wen et al\\.", "year": 2015}, {"title": "Show, attend and tell: Neural image caption generation with visual attention", "author": ["K. Xu", "J. Ba", "R. Kiros", "A. Courville", "R. Salakhutdinov", "R. Zemel", "Y. Bengio"], "venue": "arXiv preprint arXiv:1502.03044.", "citeRegEx": "Xu et al\\.,? 2015", "shortCiteRegEx": "Xu et al\\.", "year": 2015}, {"title": "Chinese poetry generation with recurrent neural networks", "author": ["X. Zhang", "M. Lapata"], "venue": "EMNLP, 670\u2013680.", "citeRegEx": "Zhang and Lapata,? 2014", "shortCiteRegEx": "Zhang and Lapata", "year": 2014}], "referenceMentions": [{"referenceID": 4, "context": "Natural language generation is potentially useful in a variety of applications such as natural language understanding (Graves 2013), response generation in dialogue systems (Wen et al.", "startOffset": 118, "endOffset": 131}, {"referenceID": 12, "context": "Natural language generation is potentially useful in a variety of applications such as natural language understanding (Graves 2013), response generation in dialogue systems (Wen et al. 2015a; 2015b; Sordoni et al. 2015), text summarization (Rush, Chopra, and Weston 2015), machine translation (Bahdanau, Cho, and Bengio 2014) and image caption (Xu et al.", "startOffset": 173, "endOffset": 219}, {"referenceID": 16, "context": "2015), text summarization (Rush, Chopra, and Weston 2015), machine translation (Bahdanau, Cho, and Bengio 2014) and image caption (Xu et al. 2015).", "startOffset": 130, "endOffset": 146}, {"referenceID": 3, "context": "Traditional approaches usually generate languages according to some rules or templates designed by humans (Cheyer and Guzzoni 2014), which are specific for some tasks and domains and difficult to generalize to other tasks and domains.", "startOffset": 106, "endOffset": 131}, {"referenceID": 4, "context": "Recently, recurrent neural networks (RNNs) have been proved to very effective in natural language generation (Graves 2013; Sutskever, Martens, and Hinton 2011; Bowman et al. 2015).", "startOffset": 109, "endOffset": 179}, {"referenceID": 1, "context": "Recently, recurrent neural networks (RNNs) have been proved to very effective in natural language generation (Graves 2013; Sutskever, Martens, and Hinton 2011; Bowman et al. 2015).", "startOffset": 109, "endOffset": 179}, {"referenceID": 5, "context": "Though traditional RNNs suffer from the problem of gradient vanishing or exploding, the long-short term memory (LSTM) (Hochreiter and Schmidhuber 1997) unit effectively addresses this problem and is able to capture the long-range dependency in natural languages.", "startOffset": 118, "endOffset": 151}, {"referenceID": 4, "context": "RNNs with LSTM have shown very promising results on various data sets with different structures including Wikipedia articles (Graves 2013), linux source codes (Karpathy, Johnson, and Li 2015), scientific papers (Karpathy, Johnson, and Li 2015), NSF abstracts (Karpathy, Johnson, and Li 2015).", "startOffset": 125, "endOffset": 138}, {"referenceID": 7, "context": "Indeed, contexts have been proved to be very useful for various natural language processing tasks such as topic extraction (Mei et al. 2006), text classification (Cao et al.", "startOffset": 123, "endOffset": 140}, {"referenceID": 2, "context": "2006), text classification (Cao et al. 2009) and language modelingmikolov2012context.", "startOffset": 27, "endOffset": 44}, {"referenceID": 3, "context": "Classical approaches usually define some rules or templates (Cheyer and Guzzoni 2014) by humans, which are very brittle and hard to generalize to different tasks and domains.", "startOffset": 60, "endOffset": 85}, {"referenceID": 9, "context": "Though there are some recent approaches aiming to learn the template structures from large amounts of corpus (Oh and Rudnicky 2000), the training data is very expensive to obtain and the final generation process still requires additional human handcrafted features.", "startOffset": 109, "endOffset": 131}, {"referenceID": 4, "context": "(Graves 2013) studied sequence generation, including text, using recurrent neural networks (RNN) with long-short term memory unit.", "startOffset": 0, "endOffset": 13}, {"referenceID": 1, "context": "(Bowman et al. 2015) investigated generating sentences from continuous semantic spaces with a variational auto-encoder, in which RNN is used for both the encoder and the encoder.", "startOffset": 0, "endOffset": 20}, {"referenceID": 8, "context": "(Mikolov and Zweig 2012) studied language modeling by adding the topical features of preceding words as contexts.", "startOffset": 0, "endOffset": 24}, {"referenceID": 14, "context": "(Wang and Cho 2015) exploited the preceding words in larger windows for language modeling.", "startOffset": 0, "endOffset": 19}, {"referenceID": 12, "context": "There are also some related work of response generation in conversation systems (Sordoni et al. 2015; Wen et al. 2015b), in which the conversation history are treated as contexts.", "startOffset": 80, "endOffset": 119}, {"referenceID": 15, "context": "There are also some related work of response generation in conversation systems (Sordoni et al. 2015; Wen et al. 2015b), in which the conversation history are treated as contexts.", "startOffset": 80, "endOffset": 119}, {"referenceID": 5, "context": "(Hochreiter and Schmidhuber 1997) effectively addresses this problem through the long-short term memory (LSTM) unit.", "startOffset": 0, "endOffset": 33}, {"referenceID": 4, "context": "There are usually two types of approaches for natural language generation: beam search (Bahdanau, Cho, and Bengio 2014), which is widely used in neural machine translation, and random sample (Graves 2013).", "startOffset": 191, "endOffset": 204}, {"referenceID": 10, "context": "Table 5: Results of fake review detection in TripAdvisor with the approach in (Ott et al. 2011).", "startOffset": 78, "endOffset": 95}, {"referenceID": 10, "context": "We adopt the approach in (Ott et al. 2011), which trains a classifier with 800 real reviews from TripAdvisor and 800 fake reviews written by the Amazon Mechanical Turkers3.", "startOffset": 25, "endOffset": 42}, {"referenceID": 10, "context": "close to the best results according to (Ott et al. 2011).", "startOffset": 39, "endOffset": 56}], "year": 2016, "abstractText": "This paper studied generating natural languages at particular contexts or situations. We proposed two novel approaches which encode the contexts into a continuous semantic representation and then decode the semantic representation into text sequences with recurrent neural networks. During decoding, the context information are attended through a gating mechanism, addressing the problem of long-range dependency caused by lengthy sequences. We evaluate the effectiveness of the proposed approaches on user review data, in which rich contexts are available and two informative contexts, sentiments and products, are selected for evaluation. Experiments show that the fake reviews generated by our approaches are very natural. Results of fake review detection with human judges show that more than 50% of the fake reviews are misclassified as the real reviews, and more than 90% are misclassified by existing state-of-the-art fake review detection algorithm.", "creator": "TeX"}}}