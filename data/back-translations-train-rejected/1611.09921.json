{"id": "1611.09921", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-Nov-2016", "title": "Less is More: Learning Prominent and Diverse Topics for Data Summarization", "abstract": "Statistical topic models efficiently facilitate the exploration of large-scale data sets. Many models have been developed and broadly used to summarize the semantic structure in news, science, social media, and digital humanities. However, a common and practical objective in data exploration tasks is not to enumerate all existing topics, but to quickly extract representative ones that broadly cover the content of the corpus, i.e., a few topics that serve as a good summary of the data. Most existing topic models fit exactly the same number of topics as a user specifies, which have imposed an unnecessary burden to the users who have limited prior knowledge. We instead propose new models that are able to learn fewer but more representative topics for the purpose of data summarization. We propose a reinforced random walk that allows prominent topics to absorb tokens from similar and smaller topics, thus enhances the diversity among the top topics extracted. With this reinforced random walk as a general process embedded in classical topic models, we obtain \\textit{diverse topic models} that are able to extract the most prominent and diverse topics from data. The inference procedures of these diverse topic models remain as simple and efficient as the classical models. Experimental results demonstrate that the diverse topic models not only discover topics that better summarize the data, but also require minimal prior knowledge of the users.", "histories": [["v1", "Tue, 29 Nov 2016 22:24:30 GMT  (6346kb,D)", "http://arxiv.org/abs/1611.09921v1", null], ["v2", "Thu, 1 Dec 2016 02:45:34 GMT  (6345kb,D)", "http://arxiv.org/abs/1611.09921v2", null]], "reviews": [], "SUBJECTS": "cs.LG cs.CL cs.IR", "authors": ["jian tang", "cheng li", "ming zhang", "qiaozhu mei"], "accepted": false, "id": "1611.09921"}, "pdf": {"name": "1611.09921.pdf", "metadata": {"source": "CRF", "title": "Less is More: Learning Prominent and Diverse Topics for Data Summarization", "authors": ["Jian Tang", "Ming Zhang", "Qiaozhu Mei"], "emails": ["tangjian@net.pku.edu.cn", "mzhang@net.pku.edu.cn", "qmei@umich.edu"], "sections": [{"heading": null, "text": "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Text MiningGeneral Terms Algorithms, ExperimentationKeywords Data Summary, Topic Modeling, Diversity, Random Walk. This study is conducted when the first author attends the University of Michigan. Permission to make digital or printed copies of all or part of this work for personal or student use is granted at no charge, provided that copies are not made or distributed for profit or commercial purposes, and that copies bear this notice and the full quote on the first page."}, {"heading": "1. INTRODUCTION", "text": "This year, it has come to the point where there is only one occasion when there is a scandal, and that is when there is a scandal."}, {"heading": "2. RELATED WORK", "text": "This year, it is only a matter of time before there is an agreement, until there is an agreement."}, {"heading": "3. PROBLEM DEFINITION", "text": "???????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????"}, {"heading": "4. DIVERSE TOPIC MODELING", "text": "In most existing topic models or mixture models, the topics or mixture components are usually distributed i.e. according to a previous distribution. To address this problem, the diversity or redundancy of the topics must be taken into account. Recent work replaces the independent assumption that priority topics prefer diversified topics (e.g. [21, 28]), usually by a regulatory term covering the entire range of topics. A sentence containing similar topics is punished. Although these treatments can infer more diverse topics than the classical models, they still make the same assumption about the number of topics, and multiple topics are still used to represent each underlying topic. A more reasonable way is to merge similar topics into larger topics, so that the best placed topics become smaller and can absorb this topic with others."}, {"heading": "4.1 Reinforced Random Walk", "text": "Specifically, we are introducing a \"social\" network of topics, or a theme network, which is an undirected graph G = (V, E), where V is the set of topics and there is an edge e (i, j) between each topic pair (i, j). The weight of e (i, j), w (i, j) is defined as the topic of cosmic similarity between the word distributions \u03c6j, i.e., w (i, j) is the ability of each topic pair (i, j). The weight of e (i, j), w (i, j) is defined as the topic of cosmic similarity between the word distributions \u03c6j, i.e., w (i, j) is the ability of each topic (i). We are formulating the inclusion process among the topics as a random process after the word token on the topic network. In particular, if a token belongs to another topic, we can say that a token of this topic is absorbed by the other topic."}, {"heading": "4.2 Diverse Topic Models", "text": "In the E-step it calculates the posterior distribution of the theme of each token, i.e. p-z-z, i.e. p-z. The PLSA models are based on the current models of the respective models."}, {"heading": "4.3 Discussion", "text": "We discuss some practical problems of the two models. Convergence. In the two models above, there are no explicit objective functions after incorporation of the enhanced random walk process within the EM or Gibbs scanning process. We prove empirically that both the number of active topics and the data probability will converge (see Figure 2 (d) and 2 (h)). We leave the theoretical justification of the convergence as future work. How can the parameter \u03b1 be set? The parameter \u03b1 controls the transition probability to the neighboring topics or the staying on its own. It assumes a similar effect to the step size in the gradient descend method and therefore a small \u03b1 (e.g. 0.1) can be used in practice. How can the parameters \u03b3 and K be set? We show empirically that the performance of the two models is not sensitive to the parameters \u03b3 and K (see Figure 5 and 6). In practice, the data can be set so that K can be signed."}, {"heading": "5. EXPERIMENT", "text": "In this section we will evaluate the effectiveness of our proposed different topic models and evaluate the performance of four real data sets."}, {"heading": "5.1 Datasets", "text": "4CONF. We start with a small dataset, the 4CONF dataset as in [16]. The dataset consists of papers published in four conferences, including KDD, SIGIR, NIPS and WWW. Each document corresponds to an author by summarizing the titles of the author's papers. Stop Words and words that appear in less than 10 documents are removed. This small dataset allows us to interpret the topics intuitively and visually. 20NG. These are the widely used 20 newsgroup data in text mining. Stop Words and words that occur in less than 20 documents are removed. We stab 1,000 documents from the dataset as holdout. WIKIPEDIA. These include 10,000 articles randomly sampled from 4,636,797 Wikipedia articles in English. Stop Words and words that occur in less than 100 documents are removed. We also hold a sample of 1,000 articles. The larger dataset consists of all the BLP."}, {"heading": "5.2 Evaluation Metrics", "text": "In order to provide a good summary of the data, the topics must be highly coherent and also provide a high coverage of the information in the original data. We introduce metrics to assess the semantic coherence and coverage of the topics accordingly; the quality of a topic is measured by the semantic coherence of the word distribution, while the coverage of the information of a number of topics is measured by their forward-looking performance on the holdout dataset, measuring the well-accepted perplexity metric.Topic Semantic coherence. We measure the semantic quality of the topics by the semantic coherence of the topics. In [19] Newman et al. measures the semantic coherence of each topic as the average point-by-point mutual information (PMI) of each word pair among the best-rated words in the topic. Specifically, the semantic coherence of the topics-choice is chosen."}, {"heading": "5.3 Algorithms for Comparison", "text": "We compare the following algorithms to select the top K topics for the summary of the data. \u2022 PLSA / LDA. The classic PLSA or LDA model is used directly to learn exactly K topics. \u2022 PLSA / LDA-TopK. We first train PLSA / LDA with a large number of K topics and then select the top K topics with the largest sizes. \u2022 PLSA / LDA-MMR-TopK. PLSA / LDA is used to train a large number of K topics. Subsequently, the MR algorithm [6] is used to select the top K topics from the K topics. Although the MMR algorithm is proposed in a query-dependent setting, we adapt it to our scenario. The relevance between the query and each topic is measured because the coverage of this topic in the entire dataset covers the top K topics, and the similarity between the individual topics is calculated as the cosmic similarity of the topics distributions."}, {"heading": "5.4 Learning Behavior of Diverse Topic Models", "text": "This year we have it in our hands until we are able to convince our customers, \"he said."}, {"heading": "5.5 Evaluation for Data Summarization", "text": "eiD nlrrsrteeaeVnlrteeeirrrlrlllllrrrrrrrr\u00fc ide rrf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf"}, {"heading": "5.6 Parameter Sensitivity", "text": "In this part, we examine the performance sensitivity of 1low granularity topics, which tend to produce a larger PMIDivPLSA and DivLDA for summarizing the data.r.t The parameters \u03b3 and K. The parameter \u03b3 controls the intensity of amplification by topic size to the transition probabilities between topics. The greater \u03b3 is, the more likely the larger topics will absorb the smaller topics. Therefore, the model will return fewer, but larger topics with a larger \u03b3, which tend to have a higher information coverage and lower semantic coherence. This can be observed from the results shown in Figure 5, in which the performances are presented for both DivPLSA and DivLDA. We can see that the performances as a whole are not sensitive to \u03b3."}, {"heading": "5.7 Summarization for DBLP", "text": "Finally, we provide a summary for the entire DBLP dataset with our various topic models. The DivPLSA model is applied to the dataset with K = 100 and \u03b3 = 1.1 and ends with 36 topics. Table 3 shows the most prominent 10 topics in the dataset."}, {"heading": "6. CONCLUSION AND FUTURE WORK", "text": "In this paper, we proposed two different topic models, DivPLSA and DivLDA, to learn the prominent and diverse topics for summarizing the data; the two models are based on a reinforced random walk within the topic network, which allows the prominent topics to include tokens from smaller and similar topics and improves the diversity of the extracted topics; the inference procedures for the two models remain as simple and efficient as the classical ones and are suitable for big data analysis; experiments on four real data sets demonstrate the effectiveness of the two models for summarizing the data; the future work covers two areas: First, we plan to investigate the theoretical convergence of the two different topic models; second, the convergence of the two models is empirically supported by the probability of the training data and the number of active topics; and we believe that there is a fundamental objective function that compromises between the probability of data and the diversity of the topics; and second, we plan to apply the reinforced marker scenarios as an uncommon outcome in the various scenarios."}, {"heading": "7. REFERENCES", "text": "[1] R. Agrawal, S. Gollapudi, A. Halverson, and S. Ieong.Diversifying search results. In Proceedings of the Second ACM International Conference on Web Search and Data Mining, pp. 5-14. ACM, 2009. [2] A. Ahmed, M. Aly, J. Gonzalez, S. Narayanamurthy, and A. J. Smola. Scalable inference in latent variable models. In Proceedings of the fifth ACM international conference on Web search and data mining, pp. 123-132. ACM, 2012. [3] A. Ahmed, Y. Low, M. Aly, V. Josifovski, and A. J. Smola. Scalable mixed inference of dynamic user interests for behavioral targeting. In KDD, pp. 114-122, 2011. [4] D. M. Lead. Probabilistic models topic."}], "references": [{"title": "Diversifying search results", "author": ["R. Agrawal", "S. Gollapudi", "A. Halverson", "S. Ieong"], "venue": "In Proceedings of the Second ACM International Conference on Web Search and Data Mining,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2009}, {"title": "Scalable inference in latent variable models", "author": ["A. Ahmed", "M. Aly", "J. Gonzalez", "S. Narayanamurthy", "A.J. Smola"], "venue": "In Proceedings of the fifth ACM international conference on Web search and data mining,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2012}, {"title": "Scalable distributed inference of dynamic user interests for behavioral targeting", "author": ["A. Ahmed", "Y. Low", "M. Aly", "V. Josifovski", "A.J. Smola"], "venue": "In KDD,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2011}, {"title": "Probabilistic topic models", "author": ["D.M. Blei"], "venue": "Communications of the ACM,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2012}, {"title": "Latent dirichlet allocation", "author": ["D.M. Blei", "A.Y. Ng", "M.I. Jordan"], "venue": "Journal of machine Learning research,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2003}, {"title": "The use of mmr, diversity-based reranking for reordering documents and producing summaries", "author": ["J. Carbonell", "J. Goldstein"], "venue": "In Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1998}, {"title": "Lexrank: Graph-based lexical centrality as salience in text summarization", "author": ["G. Erkan", "D.R. Radev"], "venue": "J. Artif. Intell. Res.(JAIR),", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2004}, {"title": "Finding scientific topics", "author": ["T.L. Griffiths", "M. Steyvers"], "venue": "Proceedings of the National academy of Sciences of the United States of America,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2004}, {"title": "Probabilistic latent semantic analysis", "author": ["T. Hofmann"], "venue": "In Proceedings of the Fifteenth conference on Uncertainty in artificial intelligence,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1999}, {"title": "Clustering short status messages: A topic model based approach", "author": ["A. Karandikar"], "venue": "PhD thesis, University of Maryland,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2010}, {"title": "Disclda: Discriminative learning for dimensionality reduction and classification", "author": ["S. Lacoste-Julien", "F. Sha", "M.I. Jordan"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2008}, {"title": "Pachinko allocation: Dag-structured mixture models of topic correlations", "author": ["W. Li", "A. McCallum"], "venue": "In Proceedings of the 23rd international conference on Machine learning,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2006}, {"title": "Joint sentiment/topic model for sentiment analysis", "author": ["C. Lin", "Y. He"], "venue": "In Proceedings of the 18th ACM conference on Information and knowledge management,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2009}, {"title": "Diversifying query suggestion results", "author": ["H. Ma", "M.R. Lyu", "I. King"], "venue": "In Proc. of AAAI,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2010}, {"title": "Drl: Distributed recursive (graph", "author": ["S. Martin", "W. Brown", "R. Klavans", "K. Boyack"], "venue": "layout. SAND2008-2936J: Sandia National Laboratories,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2008}, {"title": "Topic modeling with network regularization", "author": ["Q. Mei", "D. Cai", "D. Zhang", "C. Zhai"], "venue": "In Proceedings of the 17th international conference on World Wide Web,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2008}, {"title": "Divrank: the interplay of prestige and diversity in information networks", "author": ["Q. Mei", "J. Guo", "D. Radev"], "venue": "In Proceedings of the 16th ACM SIGKDD international conference on Knowledge discovery and data mining,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2010}, {"title": "Topic sentiment mixture: modeling facets and opinions in weblogs", "author": ["Q. Mei", "X. Ling", "M. Wondra", "H. Su", "C. Zhai"], "venue": "In Proceedings of the 16th international conference on World Wide Web,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2007}, {"title": "Automatic evaluation of topic coherence. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 100\u2013108", "author": ["D. Newman", "J.H. Lau", "K. Grieser", "T. Baldwin"], "venue": "Association for Computational Linguistics,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2010}, {"title": "Vertex-reinforced random walk", "author": ["R. Pemantle"], "venue": "Probability Theory and Related Fields,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1992}, {"title": "Repulsive mixtures", "author": ["F. Petralia", "V. Rao", "D.B. Dunson"], "venue": "In NIPS, pages 1898\u20131906,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2012}, {"title": "Clustering the tagged web", "author": ["D. Ramage", "P. Heymann", "C.D. Manning", "H. Garcia-Molina"], "venue": "In Proceedings of the Second ACM International Conference on Web Search and Data Mining,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2009}, {"title": "Arnetminer: Extraction and mining of academic social networks", "author": ["J. Tang", "J. Zhang", "L. Yao", "J. Li", "L. Zhang", "Z. Su"], "venue": "In KDD\u201908,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2008}, {"title": "Hierarchical dirichlet processes", "author": ["Y.W. Teh", "M.I. Jordan", "M.J. Beal", "D.M. Blei"], "venue": "Journal of the american statistical association,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2006}, {"title": "Medlda: maximum margin supervised topic models for regression and classification", "author": ["J. Zhu", "A. Ahmed", "E.P. Xing"], "venue": "In Proceedings of the 26th Annual International Conference on Machine Learning,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2009}, {"title": "Improving diversity in ranking using absorbing random walks", "author": ["X. Zhu", "A.B. Goldberg", "J. Van Gael", "D. Andrzejewski"], "venue": "In HLT-NAACL,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2007}, {"title": "Improving recommendation lists through topic diversification", "author": ["C.-N. Ziegler", "S.M. McNee", "J.A. Konstan", "G. Lausen"], "venue": "In Proceedings of the 14th international conference on World Wide Web,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2005}, {"title": "Priors for diversity in generative latent variable models", "author": ["J. Zou", "R. Adams"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2012}], "referenceMentions": [{"referenceID": 3, "context": "Statistical topic models [4], e.", "startOffset": 25, "endOffset": 28}, {"referenceID": 8, "context": ", the probabilistic latent semantic analysis (PLSA) [9] and the latent Dirichlet allocation (LDA) [5], have been widely recognized as effective tools to assist the real users in understanding and exploring the data.", "startOffset": 52, "endOffset": 55}, {"referenceID": 4, "context": ", the probabilistic latent semantic analysis (PLSA) [9] and the latent Dirichlet allocation (LDA) [5], have been widely recognized as effective tools to assist the real users in understanding and exploring the data.", "startOffset": 98, "endOffset": 101}, {"referenceID": 10, "context": "The discovered topics can be further utilized in other data mining tasks such as classification [11, 25], clustering [10, 22], sentiment analysis [18, 13], and user modeling [3].", "startOffset": 96, "endOffset": 104}, {"referenceID": 24, "context": "The discovered topics can be further utilized in other data mining tasks such as classification [11, 25], clustering [10, 22], sentiment analysis [18, 13], and user modeling [3].", "startOffset": 96, "endOffset": 104}, {"referenceID": 9, "context": "The discovered topics can be further utilized in other data mining tasks such as classification [11, 25], clustering [10, 22], sentiment analysis [18, 13], and user modeling [3].", "startOffset": 117, "endOffset": 125}, {"referenceID": 21, "context": "The discovered topics can be further utilized in other data mining tasks such as classification [11, 25], clustering [10, 22], sentiment analysis [18, 13], and user modeling [3].", "startOffset": 117, "endOffset": 125}, {"referenceID": 17, "context": "The discovered topics can be further utilized in other data mining tasks such as classification [11, 25], clustering [10, 22], sentiment analysis [18, 13], and user modeling [3].", "startOffset": 146, "endOffset": 154}, {"referenceID": 12, "context": "The discovered topics can be further utilized in other data mining tasks such as classification [11, 25], clustering [10, 22], sentiment analysis [18, 13], and user modeling [3].", "startOffset": 146, "endOffset": 154}, {"referenceID": 2, "context": "The discovered topics can be further utilized in other data mining tasks such as classification [11, 25], clustering [10, 22], sentiment analysis [18, 13], and user modeling [3].", "startOffset": 174, "endOffset": 177}, {"referenceID": 23, "context": "Sophisticated treatments have to be applied to topic models in order to relax this assumption, which lead to various nonparametric [24] and hierarchical versions of the models [12] that are much more complicated.", "startOffset": 131, "endOffset": 135}, {"referenceID": 11, "context": "Sophisticated treatments have to be applied to topic models in order to relax this assumption, which lead to various nonparametric [24] and hierarchical versions of the models [12] that are much more complicated.", "startOffset": 176, "endOffset": 180}, {"referenceID": 6, "context": "RELATED WORK Our work presents a good analogy to extractive text summarization [7], the goal of which is to construct a short summary of an individual document or multiple documents by extracting the most representative sentences in the document(s).", "startOffset": 79, "endOffset": 82}, {"referenceID": 5, "context": "The diversity among the results has been recognized to be important in not only text summarization [6], but also many other applications including Web search engines [1], query suggestion [14], ranking in information networks [17], and recommender systems [27].", "startOffset": 99, "endOffset": 102}, {"referenceID": 0, "context": "The diversity among the results has been recognized to be important in not only text summarization [6], but also many other applications including Web search engines [1], query suggestion [14], ranking in information networks [17], and recommender systems [27].", "startOffset": 166, "endOffset": 169}, {"referenceID": 13, "context": "The diversity among the results has been recognized to be important in not only text summarization [6], but also many other applications including Web search engines [1], query suggestion [14], ranking in information networks [17], and recommender systems [27].", "startOffset": 188, "endOffset": 192}, {"referenceID": 16, "context": "The diversity among the results has been recognized to be important in not only text summarization [6], but also many other applications including Web search engines [1], query suggestion [14], ranking in information networks [17], and recommender systems [27].", "startOffset": 226, "endOffset": 230}, {"referenceID": 26, "context": "The diversity among the results has been recognized to be important in not only text summarization [6], but also many other applications including Web search engines [1], query suggestion [14], ranking in information networks [17], and recommender systems [27].", "startOffset": 256, "endOffset": 260}, {"referenceID": 5, "context": "In [6], Carbonell et al.", "startOffset": 3, "endOffset": 6}, {"referenceID": 25, "context": "In [26], a\u201csoft\u201dversion of MMR algorithm called Grasshopper based on absorbing random walk is proposed.", "startOffset": 3, "endOffset": 7}, {"referenceID": 16, "context": "In [17], Mei et al.", "startOffset": 3, "endOffset": 7}, {"referenceID": 20, "context": "Another related direction is to learn diversified mixture components in area of mixture models [21, 28].", "startOffset": 95, "endOffset": 103}, {"referenceID": 27, "context": "Another related direction is to learn diversified mixture components in area of mixture models [21, 28].", "startOffset": 95, "endOffset": 103}, {"referenceID": 20, "context": "In [21], Petralia and Rao proposed a repulsive mixture prior which penalizes redundant components.", "startOffset": 3, "endOffset": 7}, {"referenceID": 27, "context": "prior for mixture components with a determinantal point process (DPP), which defines a probability measure over the entire set of probability distributions [28].", "startOffset": 156, "endOffset": 160}, {"referenceID": 20, "context": ", [21, 28]), usually through a regularization term over the entire set of topics.", "startOffset": 2, "endOffset": 10}, {"referenceID": 27, "context": ", [21, 28]), usually through a regularization term over the entire set of topics.", "startOffset": 2, "endOffset": 10}, {"referenceID": 19, "context": "The above random walk process is related to the stochastic process vertex-reinforced random walk [20] and DivRank [17], in both of which the transition probabilities among the vertices in the network are reinforced by the number of visits to the vertices.", "startOffset": 97, "endOffset": 101}, {"referenceID": 16, "context": "The above random walk process is related to the stochastic process vertex-reinforced random walk [20] and DivRank [17], in both of which the transition probabilities among the vertices in the network are reinforced by the number of visits to the vertices.", "startOffset": 114, "endOffset": 118}, {"referenceID": 7, "context": "Collapsed Gibbs sampling [8] algorithm is widely used for the inference due to its simplicity and effectiveness.", "startOffset": 25, "endOffset": 28}, {"referenceID": 4, "context": "\u2022 for the latest assignment k\u0302, increment counts and sums: + + ndk\u0302, + + nk\u0302w,+ + nk\u0302; update the transition probability among the topics according to (2); optimize the parameter ~ \u03b1 according to [5]; end", "startOffset": 196, "endOffset": 199}, {"referenceID": 0, "context": "\u03b3 can be usually set within [1, 2] for DivPLSA and [0.", "startOffset": 28, "endOffset": 34}, {"referenceID": 1, "context": "\u03b3 can be usually set within [1, 2] for DivPLSA and [0.", "startOffset": 28, "endOffset": 34}, {"referenceID": 1, "context": ", the yahoo-LDA model in [2].", "startOffset": 25, "endOffset": 28}, {"referenceID": 15, "context": "We start with a small data set, the 4CONF data set as in [16].", "startOffset": 57, "endOffset": 61}, {"referenceID": 22, "context": "The larger data set consists of all papers with abstracts in the computer science bibliography as in [23].", "startOffset": 101, "endOffset": 105}, {"referenceID": 18, "context": "In [19], Newman et al.", "startOffset": 3, "endOffset": 7}, {"referenceID": 5, "context": "Then the MMR algorithm [6] is used to select top-K topics from the K topics.", "startOffset": 23, "endOffset": 26}, {"referenceID": 16, "context": "PLSA/LDA is used to train a large number of K topics, and then the DivRank algorithm [17] is used to select top-K topics.", "startOffset": 85, "endOffset": 89}, {"referenceID": 14, "context": "A DrL [15] layout algorithm is applied on this bipartite network to calculate the two-dimensional coordinates for both", "startOffset": 6, "endOffset": 10}], "year": 2016, "abstractText": "Statistical topic models efficiently facilitate the exploration of large-scale data sets. Many models have been developed and broadly used to summarize the semantic structure in news, science, social media, and digital humanities. However, a common and practical objective in data exploration tasks is not to enumerate all existing topics, but to quickly extract representative ones that broadly cover the content of the corpus, i.e., a few topics that serve as a good summary of the data. Most existing topic models fit exactly the same number of topics as a user specifies, which have imposed an unnecessary burden to the users who have limited prior knowledge. We instead propose new models that are able to learn fewer but more representative topics for the purpose of data summarization. We propose a reinforced random walk that allows prominent topics to absorb tokens from similar and smaller topics, thus enhances the diversity among the top topics extracted. With this reinforced random walk as a general process embedded in classical topic models, we obtain diverse topic models that are able to extract the most prominent and diverse topics from data. The inference procedures of these diverse topic models remain as simple and efficient as the classical models. Experimental results demonstrate that the diverse topic models not only discover topics that better summarize the data, but also require minimal prior knowledge of the users.", "creator": "LaTeX with hyperref package"}}}