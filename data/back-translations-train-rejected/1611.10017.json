{"id": "1611.10017", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Nov-2016", "title": "Fast Supervised Discrete Hashing and its Analysis", "abstract": "In this paper, we propose a learning-based supervised discrete hashing method. Binary hashing is widely used for large-scale image retrieval as well as video and document searches because the compact representation of binary code is essential for data storage and reasonable for query searches using bit-operations. The recently proposed Supervised Discrete Hashing (SDH) efficiently solves mixed-integer programming problems by alternating optimization and the Discrete Cyclic Coordinate descent (DCC) method. We show that the SDH model can be simplified without performance degradation based on some preliminary experiments; we call the approximate model for this the \"Fast SDH\" (FSDH) model. We analyze the FSDH model and provide a mathematically exact solution for it. In contrast to SDH, our model does not require an alternating optimization algorithm and does not depend on initial values. FSDH is also easier to implement than Iterative Quantization (ITQ). Experimental results involving a large-scale database showed that FSDH outperforms conventional SDH in terms of precision, recall, and computation time.", "histories": [["v1", "Wed, 30 Nov 2016 06:35:39 GMT  (2982kb)", "http://arxiv.org/abs/1611.10017v1", "12 pages"]], "COMMENTS": "12 pages", "reviews": [], "SUBJECTS": "cs.CV cs.LG cs.MM", "authors": ["gou koutaki", "keiichiro shirai", "mitsuru ambai"], "accepted": false, "id": "1611.10017"}, "pdf": {"name": "1611.10017.pdf", "metadata": {"source": "CRF", "title": "Fast Supervised Discrete Hashing and its Analysis", "authors": ["Gou Koutaki", "Keiichiro Shirai", "Mitsuru Ambai"], "emails": ["koutaki@cs.kumamoto-u.ac.jp", "keiichi@shinshu-u.ac.jp", "manbai@d-itlab.co.jp"], "sections": [{"heading": null, "text": "ar Xiv: 161 1.10 017v 1 [cs.C V] 30 Nov 2"}, {"heading": "1. Introduction", "text": "This year, it is as far as ever in the history of the city, where it is as far as never before."}, {"heading": "1.1. Contributions and advantages", "text": "In this study, we first analyze the SDH model and point out that it can be simplified without performance degradation, based on some preliminary experiments. We refer to the approximate model as the Fast SDH Model (FSDH). We analyze the FSDH model and provide a mathematically accurate solution to it. Simplification of the model is validated by experimenting with several large data sets. The advantages of the proposed method are the following: \u2022 Unlike SDH, it does not require alternating optimizations or hyperparameters and is not initially value dependent. \u2022 It is easier to implement than ITQ and efficient in terms of computing time. FSDH can be implemented in three lines on MATLAB. \u2022 High bit scalability: its learning time and performance do not depend on code length. \u2022 It has better precision and memory than other state-of-the-art, monitored hashing methods."}, {"heading": "1.2. Related work", "text": "As described below, the SDH model presents a matrix factorization problem: F = W B. The popular form of this problem is singular value decomposition (SVD) [8], and if W and B are unrestricted, the householder method is used for the calculation. If W \u2265 0 is used, the non-negative matrix factorization (NMF) [4]. In the case of the SDH model, B is fully bound to {\u2212 1, 1} and W. In a similar problem, Slawski et al. proposed a matrix factorization with binary components [31] and demonstrated an application for DNA analysis in cancer research. B is limited to {0, 1} and indicates non-methylated / methylated DNA sequences. In addition, a similar model was proposed in the display electronics. Koutaki suggested a binary continuous decomposition for matrix matrix displays [15 projection images] are placed in this projection D and D multiple images."}, {"heading": "2. Supervised Discrete Hashing (SDH) Model", "text": "In this section we present the supervised discrete hash model (SDH). Let xi-RM be a feature vector and enter a series of N (\u2265 M) training samples X = [x1,.., xN] and RM \u00b7 N. Then consider binary label information yi-RM-N that corresponds to xi, where C is the number of categories to classify. Set the kth element to 1, [yi] k = 1 and the other elements to 0 indicates that the i-th vector belongs to the class. By linking N samples of yi horizontally, a label matrix Y = [y1,.., yN] and {0, 1} C \u00b7 N is constructed."}, {"heading": "2.1. Binary code assignment to each sample", "text": "Through the horizontal concatenation of N samples of bi, a binary matrix B = [b1,.., bN] is constructed. The binary code bi is calculated asbi = sgn (P'xi), (1) where P'RM \u00b7 L (hence P'RL \u00b7 M) is a linear transformation matrix and sgn (\u00b7) is the drawing function. The main objective of SDH is to determine the matrix P from training samples X. In practice, feature vectors {xi} are transformed by pre-processing. Therefore, we refer to the original feature vectors xorii and the transformed feature vectors xii."}, {"heading": "2.2. Preprocessing: Kernel transformation", "text": "The original feature vectors of training samples xorii (i = 1,..., N) are converted into the feature vectors xi-RM by means of the following kernel transformation \u03a6: xi = \u03a6 (x-ori i) = [exp (\u2212 ichori i-a1-2\u03c3),.., exp (\u2212 ichori i-am-2\u03c3)], (2) where am is an anchor vector obtained by random sampling of the original feature vectors, am = xorirand then the transformed feature vectors are bundled in the matrix form X = [x1,.., xN]."}, {"heading": "2.3. Classification model", "text": "After the binary encoding by (1) we assume that a good binary code classifies the class and formulate the following simple linear classification model: y-i = W-bi, (3) where W-RL-C is a weight matrix and y-i an estimated markup vector. As mentioned above, its maximum index, argmink [y-i] k, indicates the assigned class of xi."}, {"heading": "2.4. Optimization of SDH", "text": "The SDH problem is defined as the following minimization problem: min B, W, P-Y \u2212 W B-Q (B-Q-2 + E-W-2 + E-P-X-2, (4), where B-P is the Frobenius standard, and \u03bb-Y \u2212 W-0 are equilibrium parameters. (5) However, due to the difficulty of the optimization, it is sufficient to calculate P, i.e. if P is reached, B can be achieved by (1), and W can be achieved by the following simple equation of the lowest squares: W = (BB + I) \u2212 1 BY. (5) Due to the difficulty of the optimization, the optimization problem of (4) is usually divided into three steps of optimization of W-B-i and W-P-Qi."}, {"heading": "3. Discussion of the SDH Model", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1. 0-1 integer quadratic programming problem", "text": "In this method, one onebit element of bi is optimized while the other L \u2212 1 bits are fixed; the l \u2212 th bit bl is optimized asbl = \u2212 sgn (2 \u0445 i6 = l Qi, lbi + fl). (11) Then, all bits l = 1,.., L are optimized, and this procedure is repeated several times. Furthermore, the DCC method tends to yield a local minimum due to its greed. To improve it, Shen et al. suggest using a proximal operation of convex optimization [29]. Branch \u2212 and bound method In the case of a large number of bits L \u2265 32, the solution (10) is precisely difficult because this problem is NP-hard. However, there are a few efficient methods to solve the 0 \u2212 1 integer square programming problem."}, {"heading": "3.2. Alternating optimization and initial value dependence", "text": "Even if we optimize the binary optimization in (10), the resulting binary codes B are not always optimal because they depend on the other fixed variables W and P. In addition, alternate optimization tends to cause a serious problem: A solution depends on the initial values and may fall to a local minimum during iterations, even if each step of F-Step, W-Step and B-Step provides the optimal solution. Figure 1 shows an example of the optimization result for a simple version of the SDH model in (4) with a small number of bits (L, C, N) = (16, 10, 10). In this case, an exact solution is known and its minimum value is 0.94 (green line in Figure 1). DCC (red lines) returns results for 10 randomized initial conditions. The full search (blue lines) returns the results of an exact full search in B-Step, with 216 = 536 nodes searched."}, {"heading": "4. Proposed Fast SDH Model", "text": "We introduce a new hash model by approximating the SDH model using the following assumptions: A1: The number of bits L of the binary code is a power of 2: L = 2l.A2: The number of bits is greater than the number of classes: L \u2265 C.A3: Simple labeling proble.A4: \"W Y\" 2, \"\" P X \"2 in (8).Note that the assumptions A1 \u00b2 A3 also become the limits of the proposed model. In A4, SDH recommends that the parameter can be set to a very small value, such as\" C \"10 \u2212 5 [28]. In practice, the assumptions are\" W \u00b2 Y \u00b2 2; 31.53 and \"P \u00b2 X \u00b2 2; 0.013 in the CIFAR-10 dataset. In addition, the parameter\" C \"can be set to a very small value, such as.\""}, {"heading": "4.1. Analytical solutions of FSDH model", "text": "From Proposition 4.1 we have found that it is sufficient to determine the binary code for each class. Furthermore, we can select the optimal binary codes according to the FSDH approximation as follows: Lemma 4.2 If f (xi) is convex, the solution of min {xi} N \u2211 if (xi) s.t. N \u2211 ixi = L (15) is obtained by the mean xi = L / N (i = 1,.., N).Proof See Appendix A.Theorem 4.3 An analytical solution of FSDH B \u2032 is obtained as a Hadamard matrix. Proof Using the FSDH approximation and label representations in (13), the SDH model in (4) becomesmin B \u2032, W \u00b2 B \u2032 2 + ctuonal W matrix."}, {"heading": "4.2. Implementation of FSDH", "text": "Figure 3 shows an example of B \u2032 and B. A Hadamard matrix of size 2k \u00d7 2k can be constructed recursively using Sylvester's method [33] asH2 = [1 1 1 \u2212 1], H2k = [H2k \u2212 1 H2k \u2212 1 \u2212 H2k \u2212 1] (k \u2265 2). (18) In addition, Hadamard matrices of order 12 and 20 were constructed by Hadamard transformation [10]. Fortunately, Sylvester's method is sufficient in most cases when using binary hashing, since L = 16, 32, 64, 128, 256 and 512 bits are commonly used."}, {"heading": "4.3. Analysis of bias term of FSDH", "text": "We have already shown that B minimizes from the Hadamard matrix two terms: B = B = B = B = B = B = B = B (B + K).We assume that samples (B + K).B + B (K).B (K).B (K).B (K).B (K).B (K).B (K).B (K).B (K).B (K).B (K).B (K).B (K).B (K).B (K).B (K).B.B (K).B.B.B).B (K).B (K).B (K).B (K).B (K).B (K).B (K).B).B (K).B (K).B (K).B (K).B (K).B (K).B (K).B (K).B (K).B (K).B (K).B (K).B (K).B (K).B (K).B (K).B (K).B (K).B (K).B (K).B (K).B (K).B (K).B (K).B (K).B (K).B (K).B (K).B (K).B (K).B (.B (K).B (K).B (K).B (.B (.B).B (K).B (K).B (K).B (.B (K).B (.B).B (.B (.B).B (K).B (.B (.B).B (K).B (K).B (K).B (.B (.B).B (K).B (.B (.B).B (.B).B (.B).B (K).B (.B).B (.B (K).B (.B).B (.B).B (.B).B (K).B).B"}, {"heading": "5. Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1. Datasets", "text": "We tested the proposed method on three large-format image datasets: CIFAR-10 [16] 1, SUN-397 [36] 2 and MNIST [18] 3. Characteristic vectors of all datasets were normalized, and a multi-dimensional NUS-WIDE dataset was not included due to the limitation that the proposed method can only be applied to single-brand issues. CIFAR-10 includes labeled subsets of 60,000 images. In this test, we used 512-dimensional GIST features [25] extracted from the images. N = 59,000 training samples and 1,000 test samples were used for evaluation, the number of classes was C = 10 and included \"airplane,\" \"car,\" \"bird,\" \"etc.SUN-397 is a large-format image dataset for scene detection."}, {"heading": "5.2. Comparative methods and settings", "text": "All methods were implemented in MATLAB R2012b and tested on an Intel i7-4770 @ 3.4 GHz CPU with DDR3 SDRAM @ 32 GB.CCA-ITQ and LSH: State-of-the-art binary hashing methods. They can be converted into verified binary hashing methods using pre-processing function vectors X using label information. Canonical correlation analysis (CCA transformation) was performed and feature vectors were normalized and set to zero. They generated the projection matrix P and 3000 binary codes were verified by (1).COSDISH is a recently proposed verified hashing method."}, {"heading": "5.3. Results and discussion", "text": "Precision and retrieval were calculated by calculating the hamming distance between the training samples and the test samples with a hamming radius of 2. Figure 6 shows the results in terms of precision, retrieval and the mean precision (MAP) of the hamming ranking for all methods and the three sets of data. Code lengths of L = 16, 32, 64, 96 and 128 were evaluated. CIFAR-10: COSDISH shows the best MAP. FSDH5000 returned the best precision and retrieval. Although COSDISH showed a satisfactory MAP, the precision was low. In SDH and FSDH, the number of anchor points improved performance. As the code length increased, SDFSH decreased precision. FSDH, however, obtained a high precision and retrieval. This is a significant advantage of the proposed method. Generally, the increase in code length indicates that FSH codelength tends to decrease FSH precision with FSH."}, {"heading": "5.3.1 Validation of FSDH approximation", "text": "Table 1 shows the comparison results of SDH1000 with \u03bd = 10 \u2212 5 and SDH1000 with \u03bd = 0. For all datasets, 6https: / / github.com / goukoutaki / FSDH, the results of SDH1000 and SDH1000 were almost identical with \u03bd = 0. Table 2 shows that SDH1000 was suitable for all datasets with L = 64 and after optimization in both W'Y-2 and P'X-2. This means that the FSDH approximation was suitable for monitored hashing."}, {"heading": "5.3.2 Computation time", "text": "Table 3 shows the calculation time of each method for CIFAR-10. The time for FSDH1000 was almost identical to that of CCA = ITQ. As the number of anchors increased, the computing time for SDH and FSDH increased. The computing time for SDH and COSDISH increased with the code length. The number of iterations of the DCC method depended on the code length."}, {"heading": "5.3.3 Bit scalability and larger classes", "text": "Table 4 shows the comparison results in terms of computation time and performance with a wide range of code lengths L = 32 \u0445 1024 for the CIFAR-10 dataset. N = 10,000 training samples, 1,000 test samples and 1,000 anchors were used. FSDH's computation time was almost identical in terms of code length, as the main computation in FSDH included matrix multiplication and inversion (XX) \u2212 1 of (6). In practice, the inverse matrix was not calculated directly, and Cholesky decomposition was performed. On the contrary, the computation time for SDH was increased exponentially and accuracy decreased, meaning that the DCC method fell into local minimums in cases of large code lengths. Generally, large bits are useful for a large number of classes. Table 5 shows the results of larger classes of the SUN dataset dataset.L = 512 bits, C = 397 classes, and FSH with high code lengths are comparable."}, {"heading": "6. Conclusion", "text": "In this paper, we simplified the SDH model to an FSDH model by approximating the bias term, and provided accurate solutions for the proposed FDSH model. FSDH approximation was validated by comparing experiments with the SDH model. FSDH is easy to implement and has surpassed several state-of-the-art monitored hashing methods. FSDH can maintain performance without losing precision, especially in large code lengths. In future work, we intend to apply this idea to other hashing models.Appendix"}, {"heading": "A. Proof of Lemma 4.2", "text": "The optimization problem in (15) is known as resource allocation problem [1, 5, 13]. Here we present a simple proof for the solution.The gradient vector of the object function [1 xi = L] can be considered as a surface equation in a N-dimensional space (x1, x2,..., f \u2032 (xN), where f \u2032 (\u00b7) is the differentiated version of f (\u00b7) and the ith element (the gradient in the i-th direction) is given by the equality xi (xj) f (xj) = equality in the i-th direction."}, {"heading": "B. Complete data of the experimental results", "text": "B.1. Recall, Precision and MAP Tables 6 \u0445 8 show retrieval, precision and MAP for all records when L = 16, 32, 64, 96 and 128. These were calculated by showing the hamming distance between the training samples and the test samples with a hamming radius of 2.B.2. ROC curveFigure 6 shows precision callback ROC curves based on the hamming distance ranking for all records when L = 16 \u0445 128."}, {"heading": "C. Loss comparison", "text": "We define W loss and P loss of the SDH model in (4) as follows: W loss = EY-W-B-2, P loss = EB-P-X-2. (31) Tables 9-11 show each loss of SDH and FSDH after optimization for the CIFAR-10, SUN-10 and MNIST records. As described in paragraph 4.1, FSDH can accurately minimize W loss. Therefore, FSDH results in a lower value of W loss than SDH for all records. Furthermore, FSDH can also reduce P loss as described in paragraph 4.3. For CIFAR-10, SDH leads to a lower value of P loss than FSDH. For SUN-10 and MNIST, FSDH leads to a lower value of P loss than SDH."}], "references": [{"title": "Applied dynamic programming", "author": ["R.E. Bellman", "S.E. Dreyfus"], "venue": "Princeton University Press,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1962}, {"title": "Supervised hashing with error correcting codes", "author": ["F. Cakir", "S. Sclaroff"], "venue": "ACM MM, pages 785\u2013788,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2014}, {"title": "BRIEF: binary robust independent elementary features", "author": ["M. Calonder", "V. Lepetit", "C. Strecha", "P. Fua"], "venue": "ECCV, pages 778\u2013792,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2010}, {"title": "Generalizednnonnegative matrix approximations with Bregman divergences", "author": ["I.S. Dhillon", "S. Sra"], "venue": "NIPS, pages 283\u2013290,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2005}, {"title": "The art and theory of dynamic programming", "author": ["S.E. Dreyfus", "A.M. Law"], "venue": "Academic Press,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1977}, {"title": "Similarity search in high dimensions via hashing", "author": ["A. Gionis", "P. Indyk", "R. Motwani"], "venue": "Int. Conf. Very Large Data Bases (VLDB), pages 518\u2013529,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1999}, {"title": "Fast and compact hamming distance index", "author": ["S. Gog", "R. Venturini"], "venue": "Int. ACM SIGIR Conf. Research & Devel. Info. Retriev., pages 285\u2013294,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2016}, {"title": "Matrix computations (3rd ed.)", "author": ["G.H. Golub", "C.F. Van Loan"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1996}, {"title": "Iterative quantization: a procrustean approach to learning binary codes for large-scale image retrieval", "author": ["Y. Gong", "S. Lazebnik", "A. Gordo", "F. Perronnin"], "venue": "IEEE T. PAMI, 35(12):2916\u20132929,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2013}, {"title": "R\u00e9solution d\u0301une question relative aux d\u00e9terminants", "author": ["J. Hadamard"], "venue": "Bulletin Sci. Math., 17:240\u2013246,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1893}, {"title": "Spherical hashing: Binary code embedding with hyperspheres", "author": ["J.P. Heo", "Y. Lee", "J. He", "S.F. Chang", "S.E. Yoon"], "venue": "IEEE T. PAMI, 37(11):2304\u20132316,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "Relations between two sets of variables", "author": ["H. Hotelling"], "venue": "Biometrika, pages 312\u2013377,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1936}, {"title": "Resource allocation problems: algorithms approaches", "author": ["T. Ibaraki", "N. Katoh"], "venue": "The MIT Press,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1988}, {"title": "Column sampling based discrete supervised hashing", "author": ["W.C. Kang", "W.J. Li", "Z.H. Zhou"], "venue": "AAAI, pages 1230\u20131236,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2016}, {"title": "Binary continuous image decomposition for multi-view display", "author": ["G. Koutaki"], "venue": "ACM TOG, 35(4):69:1\u201369:12,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2016}, {"title": "Learning multiple layers of features from tiny images", "author": ["A. Krizhevsky"], "venue": "Technical report, Univ. Toronto,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2009}, {"title": "Learning to hash with binary reconstructive embeddings", "author": ["B. Kulis", "T. Darrell"], "venue": "NIPS, pages 1042\u2013 1050,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2009}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. Lecun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proc. IEEE, pages 2278\u20132324,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1998}, {"title": "Learning hash functions using column generation", "author": ["X. Li", "G. Lin", "C. Shen", "A. van den Hengel", "A. Dick"], "venue": "In ICML,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2013}, {"title": "Fast supervised hashing with decision trees for high-dimensional data", "author": ["G. Lin", "C. Shen", "Q. Shi", "A. van den Hengel", "D. Suter"], "venue": "In CVPR,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1978}, {"title": "Discrete graph hashing", "author": ["W. Liu", "C. Mu", "S. Kumar", "S.-F. Chang"], "venue": "NIPS, pages 3419\u20133427,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2014}, {"title": "Hashing with graphs", "author": ["W. Liu", "J. Wang", "S. fu Chang"], "venue": "In ICML,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2011}, {"title": "Supervised hashing with kernels", "author": ["W. Liu", "J. Wang", "R. Ji", "Y.-G. Jiang", "S.-F. Chang"], "venue": "CVPR, pages 2074\u20132081,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2012}, {"title": "Supervised discriminative hashing for compact binary codes", "author": ["V.A. Nguyen", "J. Lu", "M.N. Do"], "venue": "ACM MM, pages 989\u2013992,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2014}, {"title": "Modeling the shape of the scene: a holistic representation of the spatial envelope", "author": ["A. Oliva", "A. Torralba"], "venue": "IJCV, 42(3):145\u2013175,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2001}, {"title": "Theory of linear and integer programming", "author": ["A. Schrijver"], "venue": "John Wiley & Sons, Inc.,", "citeRegEx": "26", "shortCiteRegEx": null, "year": 1986}, {"title": "Learning binary codes for maximum inner product search", "author": ["F. Shen", "W. Liu", "S. Zhang", "Y. Yang", "H. Tao Shen"], "venue": "ICCV, pages 4148\u20134156,", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2015}, {"title": "Supervised discrete hashing", "author": ["F. Shen", "C. Shen", "W. Liu", "H. Tao Shen"], "venue": "CVPR, pages 37\u201345,", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2015}, {"title": "A fast optimization method for general binary code learning", "author": ["F. Shen", "X. Zhou", "Y. Yang", "J. Song", "H.T. Shen", "D. Tao"], "venue": "IEEE T. Image Process. (TIP), 25(12):5610\u20135621,", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2016}, {"title": "Kernel-based supervised discrete hashing for image retrieval", "author": ["X. Shi", "F. Xing", "J. Cai", "Z. Zhang", "Y. Xie", "L. Yang"], "venue": "ECCV, pages 419\u2013433,", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2016}, {"title": "Matrix factorization with binary components", "author": ["M. Slawski", "M. Hein", "P. Lutsik"], "venue": "NIPS, pages 3210\u2013 3218,", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2013}, {"title": "Top rank supervised binary coding for visual search", "author": ["D. Song", "W. Liu", "R. Ji", "D.A. Meyer", "J.R. Smith"], "venue": "ICCV, pages 1922\u20131930,", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2015}, {"title": "Thoughts on inverse orthogonal matrices, simultaneous sign successions, and tessellated pavements in two or more colours, with applications to Newton\u2019s rule, ornamental tile-work, and the theory of numbers", "author": ["J. Sylvester"], "venue": "Philos. Magazine, 34:461\u2013475,", "citeRegEx": "33", "shortCiteRegEx": null, "year": 1867}, {"title": "Supervised quantization for similarity search", "author": ["X. Wang", "T. Zhang", "G.-J. Qi", "J. Tang", "J. Wang"], "venue": "CVPR, pages 2018\u20132026,", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2016}, {"title": "Spectral hashing", "author": ["Y. Weiss", "A. Torralba", "R. Fergus"], "venue": "NIPS, pages 1753\u20131760,", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2009}, {"title": "SUN database: large-scale scene recognition from abbey to zoo", "author": ["J. Xiao", "J. Hays", "K.A. Ehinger", "A. Oliva", "A. Torralba"], "venue": "CVPR, pages 3485\u20133492,", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2010}], "referenceMentions": [{"referenceID": 5, "context": "Binary hashing is an important technique for computer vision, machine learning, and large-scale image/video/document retrieval [6, 9, 17, 19, 24, 27, 28].", "startOffset": 127, "endOffset": 153}, {"referenceID": 8, "context": "Binary hashing is an important technique for computer vision, machine learning, and large-scale image/video/document retrieval [6, 9, 17, 19, 24, 27, 28].", "startOffset": 127, "endOffset": 153}, {"referenceID": 16, "context": "Binary hashing is an important technique for computer vision, machine learning, and large-scale image/video/document retrieval [6, 9, 17, 19, 24, 27, 28].", "startOffset": 127, "endOffset": 153}, {"referenceID": 18, "context": "Binary hashing is an important technique for computer vision, machine learning, and large-scale image/video/document retrieval [6, 9, 17, 19, 24, 27, 28].", "startOffset": 127, "endOffset": 153}, {"referenceID": 23, "context": "Binary hashing is an important technique for computer vision, machine learning, and large-scale image/video/document retrieval [6, 9, 17, 19, 24, 27, 28].", "startOffset": 127, "endOffset": 153}, {"referenceID": 26, "context": "Binary hashing is an important technique for computer vision, machine learning, and large-scale image/video/document retrieval [6, 9, 17, 19, 24, 27, 28].", "startOffset": 127, "endOffset": 153}, {"referenceID": 27, "context": "Binary hashing is an important technique for computer vision, machine learning, and large-scale image/video/document retrieval [6, 9, 17, 19, 24, 27, 28].", "startOffset": 127, "endOffset": 153}, {"referenceID": 2, "context": "Furthermore, it is easy to compare a query in binary code with a binary code in a database because the Hamming distance between them can be computed efficiently by using bitwise operations that are part of the instruction set of any modern CPU [3, 7].", "startOffset": 244, "endOffset": 250}, {"referenceID": 6, "context": "Furthermore, it is easy to compare a query in binary code with a binary code in a database because the Hamming distance between them can be computed efficiently by using bitwise operations that are part of the instruction set of any modern CPU [3, 7].", "startOffset": 244, "endOffset": 250}, {"referenceID": 5, "context": "Locality-sensitive hashing (LSH) [6] is one of most popular methods.", "startOffset": 33, "endOffset": 36}, {"referenceID": 8, "context": "Iterative quantization (ITQ) [9] is another state-of-the-art binary hashing method.", "startOffset": 29, "endOffset": 32}, {"referenceID": 16, "context": "Binary hashing can be roughly classified into two types: unsupervised hashing [17, 22, 21, 27, 11, 35] and supervised hashing.", "startOffset": 78, "endOffset": 102}, {"referenceID": 21, "context": "Binary hashing can be roughly classified into two types: unsupervised hashing [17, 22, 21, 27, 11, 35] and supervised hashing.", "startOffset": 78, "endOffset": 102}, {"referenceID": 20, "context": "Binary hashing can be roughly classified into two types: unsupervised hashing [17, 22, 21, 27, 11, 35] and supervised hashing.", "startOffset": 78, "endOffset": 102}, {"referenceID": 26, "context": "Binary hashing can be roughly classified into two types: unsupervised hashing [17, 22, 21, 27, 11, 35] and supervised hashing.", "startOffset": 78, "endOffset": 102}, {"referenceID": 10, "context": "Binary hashing can be roughly classified into two types: unsupervised hashing [17, 22, 21, 27, 11, 35] and supervised hashing.", "startOffset": 78, "endOffset": 102}, {"referenceID": 34, "context": "Binary hashing can be roughly classified into two types: unsupervised hashing [17, 22, 21, 27, 11, 35] and supervised hashing.", "startOffset": 78, "endOffset": 102}, {"referenceID": 11, "context": "For example, canonical correlation analysis (CCA) [12] can transform feature vectors to maximize inter-class variation and minimize intra-class variation according to label information.", "startOffset": 50, "endOffset": 54}, {"referenceID": 22, "context": "Kernel-based supervised hashing (KSH) [23] uses spectral relaxation to optimize the cost function through a sign function.", "startOffset": 38, "endOffset": 42}, {"referenceID": 29, "context": "KSH has also been improved to kernel-based supervised discrete hashing (KSDH) [30].", "startOffset": 78, "endOffset": 82}, {"referenceID": 23, "context": "Supervised Discriminative Hashing [24] decomposes training samples into inter and intra samples.", "startOffset": 34, "endOffset": 38}, {"referenceID": 13, "context": "Column sampling-based discrete supervised hashing (COSDISH) [14] uses column sampling based on semantic similarity, and decomposes the problem into a sub-problem to simplify solution.", "startOffset": 60, "endOffset": 64}, {"referenceID": 27, "context": "The optimization of binary codes leads to a mixedinteger programming problem involving integer and noninteger variables, which is an NP-hard problem in general [28].", "startOffset": 160, "endOffset": 164}, {"referenceID": 25, "context": ", a linear programming problem [26].", "startOffset": 31, "endOffset": 35}, {"referenceID": 27, "context": "This relaxation significantly simplifies the problem, but is known to affect classification performance [28].", "startOffset": 104, "endOffset": 108}, {"referenceID": 27, "context": "Recent research has introduced a type of supervised discrete hashing (SDH) [28, 34] that directly learns binary codes without relaxation.", "startOffset": 75, "endOffset": 83}, {"referenceID": 33, "context": "Recent research has introduced a type of supervised discrete hashing (SDH) [28, 34] that directly learns binary codes without relaxation.", "startOffset": 75, "endOffset": 83}, {"referenceID": 7, "context": "The popular form of this problem is singular value decomposition (SVD) [8], and when W and B are unconstrained, the Householder method is used for computation.", "startOffset": 71, "endOffset": 74}, {"referenceID": 3, "context": "When W \u2265 0, nonnegative matrix factorization (NMF) is used [4].", "startOffset": 59, "endOffset": 62}, {"referenceID": 30, "context": "proposed matrix factorization with binary components [31] and showed an application to DNA analysis for cancer research.", "startOffset": 53, "endOffset": 57}, {"referenceID": 14, "context": "Koutaki proposed binary continuous decomposition for multi-view displays [15].", "startOffset": 73, "endOffset": 77}, {"referenceID": 28, "context": "proposed using a proximal operation of convex optimization [29].", "startOffset": 59, "endOffset": 63}, {"referenceID": 14, "context": "In [15], Koutaki used a branch-and-bound method to solve the problem.", "startOffset": 3, "endOffset": 7}, {"referenceID": 27, "context": "In A4, SDH recommends that the parameter \u03bd be set to a very small value, such as \u03bd = 10\u22125 [28].", "startOffset": 90, "endOffset": 94}, {"referenceID": 32, "context": "A Hadamard matrix of size 2k\u00d72k can be constructed recursively by Sylvester\u2019s method [33] as", "startOffset": 85, "endOffset": 89}, {"referenceID": 9, "context": "Furthermore, Hadamard matrices of orders 12 and 20 were constructed by Hadamard transformation [10].", "startOffset": 95, "endOffset": 99}, {"referenceID": 15, "context": "We tested the proposed method on three large-scale image datasets: CIFAR-10 [16] 1, SUN-397 [36] 2, and MNIST [18] 3.", "startOffset": 76, "endOffset": 80}, {"referenceID": 35, "context": "We tested the proposed method on three large-scale image datasets: CIFAR-10 [16] 1, SUN-397 [36] 2, and MNIST [18] 3.", "startOffset": 92, "endOffset": 96}, {"referenceID": 17, "context": "We tested the proposed method on three large-scale image datasets: CIFAR-10 [16] 1, SUN-397 [36] 2, and MNIST [18] 3.", "startOffset": 110, "endOffset": 114}, {"referenceID": 24, "context": "In this test, we used 512-dimensional GIST features [25] extracted from the images.", "startOffset": 52, "endOffset": 56}, {"referenceID": 13, "context": "The proposed method was compared with four stateof-the-art supervised hashing methods: CCA-ITQ, CCALSH, SDH, and COSDISH [14].", "startOffset": 121, "endOffset": 125}, {"referenceID": 19, "context": "442 3542 FSH [20] 0.", "startOffset": 13, "endOffset": 17}, {"referenceID": 1, "context": "142 29624 LSVM-b [2] 0.", "startOffset": 17, "endOffset": 20}, {"referenceID": 31, "context": "042 Top-RSBC+SGD [32] 0.", "startOffset": 17, "endOffset": 21}, {"referenceID": 19, "context": "For reference, we refer to the results of fast supervised hashing (FSH), LSVM-b and TopRSBC+SGD which are reprinted from [20, 2, 32].", "startOffset": 121, "endOffset": 132}, {"referenceID": 1, "context": "For reference, we refer to the results of fast supervised hashing (FSH), LSVM-b and TopRSBC+SGD which are reprinted from [20, 2, 32].", "startOffset": 121, "endOffset": 132}, {"referenceID": 31, "context": "For reference, we refer to the results of fast supervised hashing (FSH), LSVM-b and TopRSBC+SGD which are reprinted from [20, 2, 32].", "startOffset": 121, "endOffset": 132}, {"referenceID": 0, "context": "The optimization problem in (15) is known as the resource allocation problem [1, 5, 13].", "startOffset": 77, "endOffset": 87}, {"referenceID": 4, "context": "The optimization problem in (15) is known as the resource allocation problem [1, 5, 13].", "startOffset": 77, "endOffset": 87}, {"referenceID": 12, "context": "The optimization problem in (15) is known as the resource allocation problem [1, 5, 13].", "startOffset": 77, "endOffset": 87}], "year": 2016, "abstractText": "In this paper, we propose a learning-based supervised discrete hashing method. Binary hashing is widely used for large-scale image retrieval as well as video and document searches because the compact representation of binary code is essential for data storage and reasonable for query searches using bit-operations. The recently proposed Supervised Discrete Hashing (SDH) efficiently solves mixed-integer programming problems by alternating optimization and the Discrete Cyclic Coordinate descent (DCC) method. We show that the SDH model can be simplified without performance degradation based on some preliminary experiments; we call the approximate model for this the \u201cFast SDH\u201d (FSDH) model. We analyze the FSDH model and provide a mathematically exact solution for it. In contrast to SDH, our model does not require an alternating optimization algorithm and does not depend on initial values. FSDH is also easier to implement than Iterative Quantization (ITQ). Experimental results involving a large-scale database showed that FSDH outperforms conventional SDH in terms of precision, recall, and computation time.", "creator": "LaTeX with hyperref package"}}}