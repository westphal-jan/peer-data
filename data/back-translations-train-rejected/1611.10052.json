{"id": "1611.10052", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Nov-2016", "title": "Performance Tuning of Hadoop MapReduce: A Noisy Gradient Approach", "abstract": "Hadoop MapReduce is a framework for distributed storage and processing of large datasets that is quite popular in big data analytics. It has various configuration parameters (knobs) which play an important role in deciding the performance i.e., the execution time of a given big data processing job. Default values of these parameters do not always result in good performance and hence it is important to tune them. However, there is inherent difficulty in tuning the parameters due to two important reasons - firstly, the parameter search space is large and secondly, there are cross-parameter interactions. Hence, there is a need for a dimensionality-free method which can automatically tune the configuration parameters by taking into account the cross-parameter dependencies. In this paper, we propose a novel Hadoop parameter tuning methodology, based on a noisy gradient algorithm known as the simultaneous perturbation stochastic approximation (SPSA). The SPSA algorithm tunes the parameters by directly observing the performance of the Hadoop MapReduce system. The approach followed is independent of parameter dimensions and requires only $2$ observations per iteration while tuning. We demonstrate the effectiveness of our methodology in achieving good performance on popular Hadoop benchmarks namely \\emph{Grep}, \\emph{Bigram}, \\emph{Inverted Index}, \\emph{Word Co-occurrence} and \\emph{Terasort}. Our method, when tested on a 25 node Hadoop cluster shows 66\\% decrease in execution time of Hadoop jobs on an average, when compared to the default configuration. Further, we also observe a reduction of 45\\% in execution times, when compared to prior methods.", "histories": [["v1", "Wed, 30 Nov 2016 08:52:11 GMT  (245kb,D)", "https://arxiv.org/abs/1611.10052v1", null], ["v2", "Fri, 16 Dec 2016 09:45:04 GMT  (245kb,D)", "http://arxiv.org/abs/1611.10052v2", null]], "reviews": [], "SUBJECTS": "cs.DC cs.LG", "authors": ["sandeep kumar", "sindhu padakandla", "chandrashekar l", "priyank parihar", "k gopinath", "shalabh bhatnagar"], "accepted": false, "id": "1611.10052"}, "pdf": {"name": "1611.10052.pdf", "metadata": {"source": "CRF", "title": "Performance Tuning of Hadoop MapReduce: A Noisy Gradient Approach", "authors": ["Sandeep Kumar", "Sindhu Padakandla"], "emails": ["iisc.csa.priyank.parihar}@gmail.com,", "shalabh}@csa.iisc.ernet.in"], "sections": [{"heading": null, "text": "Tags Hadoop Performance Tuning, Simultaneous Perturbation Stochastic Approximation"}, {"heading": "1. INTRODUCTION", "text": "This year, as never before in the history of a country in which it is a country in which it is a country in which it is a country in which it is a country in which it is a country, a country in which it is a country, a country in which it is a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country"}, {"heading": "1.1 Our Contribution", "text": "Our goal is to introduce users to a new method that is simple to implement and effective at the same time. The highlights of our SPSA-based approach are the following: \u2022 Mathematical model: The methodology we propose uses the observations of the Hadoop system and does not require a mathematical model. This is desirable because mathematical models developed for older versions are not transferable to newer versions of Hadoop. \u2022 Dimension-free nature: SPSA is designed to handle complex search spaces. Thus, unlike [7], no reduction in the search space is required. \u2022 Parametric dependencies: Unlike a variety of standard optimization methods based on clever heuristics, our SPSA-based method calculates the degree of gradation and therefore takes into account the cross-parameter interactions in the underlying problem. \u2022 Performance: With the SPSA algorithm, we simultaneously adjust parameters from 66% to 66% for the execution of our method."}, {"heading": "1.2 Organisation of the Paper", "text": "In the next section, we will describe the Hadoop architecture, its analysis of the data flow, and point out the importance and role of some of the configuration parameters. Afterwards, in Section 3, we will discuss the work involved and compare it with our approach. We will provide a detailed description of our SPSAbased approach in Section 4. In Section 5, we will discuss the specific details of implementing the SPSA algorithm to adjust the Hadoop parameters. We will describe the experimental setup and present the results in Section 6. Section 7 concludes the article and suggests future improvements."}, {"heading": "2. HADOOP", "text": "Hadoop is an open source implementation of MapReduce [10], which has grown in popularity in recent years as it can be used over hardware. Hadoop consists of two main components, MapReduce and Hadoop Distributed File System (HDFS). HDFS is used to store data and MapReduce to perform calculations on the data. First we discuss HDFS and then MapReduce. Then we describe the analysis of the data flow in Hadoop with the aim of illustrating the importance of the various parameters."}, {"heading": "2.1 Hadoop Distributed File System", "text": "Hadoop uses HDFS to store input and output data for the MapReduce applications. HDFS provides interfaces for applications to move closer to the data location [2] because moving data is costly compared to moving small MapReduce codes. It is fault tolerant and optimized for storing large amounts of data.An HDFS cluster (see [31]) consists of a single NameNode, a master server, and multiple slave dataNodes. The dataNodes, usually one per node, store the data actually used for the calculation. These manage the memory attached to the node on which they run. Internally, a file is divided into one or more data blocks (the block size is controlled by dfs.block.size) and these blocks are stored in a set of dataNodes. They are responsible for handling read and write requests to the clients of the file system by accessing the file node and file system name space."}, {"heading": "2.2 MapReduce", "text": "In MapReduce version 1 (v1), the JobTracker, which normally runs on a dedicated node, is responsible for executing and monitoring jobs in the cluster. It plans to map and reduce tasks to be performed on the nodes in the cluster, which are monitored by a corresponding TaskTracker running on that particular node. Each TaskTracker sends the progress of the corresponding map or periodically reduces the task to JobTracker. Hadoop MapReduce version 2 (v2, also known as Yet Another Resource Negotiator (YARN) [30]) has a different architecture. It has a Resource Manager and NodeManager instead of JobTracker and TaskTracker. Unlike Application Map, the tasks of resource and job management management management are distributed among resource managers and application masters (a process created for each job). The job submitted by a client is based, for example, on an application linked to a benchmark."}, {"heading": "2.3 MapReduce Data Flow Analysis", "text": "The reactionary rhapsodies in the reactionary world of the reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary, reactionary,"}, {"heading": "3. RELATED WORK", "text": "This year, more than ever before in the history of the country in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a city and in which it is a country."}, {"heading": "3.1 Motivation for Our Approach", "text": "The contrast between earlier approaches to parameter optimization in Hadoop and our approach is shown in Figure 2. In [14], the optimization is based on the what-if machine, which uses a mixture of simulation and model-based estimation. In this case, the cost model F is high-dimensional, nonlinear, nonconvex, and multimodal. In [32], authors use available literature to reduce the parameter space, and they use simulated annealing to find the correct parameter setting in reduced space. We observe that gathering statistics and building a precise model requires a certain amount of expertise. Also, mathematical models developed for an older version may fail for the newer versions, as Hadoop continues to evolve. In the worst case, mathematical models for some components of Hadoop may not be well defined. The effect of cross-parameter interactions is significant, and therefore it may be possible to design a good idea as large as the search space."}, {"heading": "4. AUTOMATIC PARAMETER TUNING", "text": "The performance of various complex systems such as traffic control [25], steering of unmanned aircraft [6], remote sensing [9], communications in satellites [13] and airlines [16] depends on a number of adjustable parameters. Parameter tuning is difficult in such cases due to bottlenecks, namely the black box nature of the problem and the curse of dimensionality, i.e. the complexity of the search space. In this section we will discuss the general issue behind the methods that resolve these bottlenecks, and their relevance to the problem of adjusting hadoop parameters."}, {"heading": "4.1 Bottlenecks in Parameter Tuning", "text": "In many complex systems, the exact nature of the power dependence on the parameters is not explicitly known, i.e. the power cannot be expressed as an analytical function of the parameters. As a result, the parameter setting that provides the best performance cannot be calculated apriori. However, the performance of the system for each given parameter setting can be observed either by the system or a simulator of the system. In such a scenario, one can resort to black box / simulation-based optimization methods that adjust the parameters based on the results observed by the system / simulator without knowing their internal functioning. Figure 3 is a scheme for visualizing the black box optimization method. Here, the black box optimization scheme determines the current value of the parameters based on the past observations. The way in which past observation is used to calculate the current parameter setting, the density of the individual parameters varies across methods. An important search for the black box observation number is the number of observation bits, not the number of the quantum optimization method increases in the context of the number of observation bits."}, {"heading": "4.2 Noisy Gradient based optimization", "text": "In order to take into account the interactions of the cross parameters, one must use the sensitivity of the power measurement with respect to each of the parameters in a given parameter setting. This sensitivity is formally known as the gradient of the power measurement in a given setting. It is important to note that only O (n) observations are needed to calculate the gradient of a function at a given point. However, even O (n) calculations are not desirable if each observation itself is costly. Consider the noise gradient scheme given in (1) below. Presence n + 1 = presence of the gradient \u2212 n (n + Mn), (1) where n = 1, 2. denotes the iteration number, the fn-Rn denotes the gradients of the function f, Mn-Rn is a zero mean noise sequence, and \u00a4n is the step variable. Fig. 4 represents an intuitive picture of how a noisy gradient algorithm works."}, {"heading": "4.3 Simultaneous Perturbation Stochastic Approximation (SPSA)", "text": "We use the following notation: 1. \u03b8. X. \u2212 Rn denotes the tunable parameter. Here n is the dimension of the parameter space. Furthermore, it is assumed that X is a compact and convex subset of Rn.2. If x. \u2212 Rn is any vector, then x. (i.) carefully denotes its coordinate, i.e. x. (1.),... f. (n.) denotes the power of the system for parameters. Let us f. a smooth and differentiable function of the phenomenon. 4. f. (.) = (.) regardless of the fact that f."}, {"heading": "4.4 Noisy Gradient Recovery from Random Perturbations", "text": "Suppose the one-sided SPSA algorithm receives for each small positive constant an estimate of the gradient according to the equation below (3)."}, {"heading": "4.5 Convergence Analysis", "text": "The SPSA algorithm (algorithm 1) uses an estimate of the noise gradient (in line 6) and takes a step in the negative gradient direction with each iteration to minimize the cost function. Noise gradient update can be rewritten to: \"+ 1 + 1 =\" \"), where Mn + 1 =\" fn-E \"(in line 6) and\" fn-E \"(in line 5) is a related difference sequence under the order of the fields\" Fn \"(in line 5), where Mn + 1 =\" fn-E \"(in line 5) is an associated sequence of the difference sources in the order of the fields\" Fn \"(in line 5),\" m-m, \"m < n\" n, n, \"n,\" n, \"n,\" n, \"n,\" n, \"n,\" \"n,\" n, \"n\" n \"and\" (in line 1) a slight deviation from the \"6,\" n."}, {"heading": "5. APPLYING SPSA TO HADOOP PARAMETER TUNING", "text": "The SPSA algorithm was presented in its general form in Algorithm 1. We will now discuss the specific details that appropriately apply the SPSA algorithm to the problem of parameter setting in Hadoop."}, {"heading": "5.1 Mapping the Parameters", "text": "The SPSA algorithm requires that each of the parameter components has a real value, i.e. in algorithm 1, a set of Rn parameters is required that the SPSA algorithm can tune, and a mapping that adapts these Rn parameters to the Hadoop parameters. To make things clear, we introduce the following notation: 1. The Hadoop parameters are denoted by PhenomenH, and the Rn parameters tuned by SPSA by PhenomenA 1,2. Si denotes the set of values that the i-th Hadoop-Pa rameter can take."}, {"heading": "5.2 Perturbation Sequences and Step-Sizes", "text": "In practice, however, a constant step quantity can be used, since one gets closer to the desired value in a finite number of phenomena, which are usually not able to arrive at the result in Theorem 1. (6) The conditions for the step quantities are asymptotic and must be adhered to in order to arrive at the result in Theorem 1. (1) In practice, however, a constant step quantity can be used, since one gets closer to the desired value in a finite number of phenomena that are not capable (1)."}, {"heading": "6. EXPERIMENTAL EVALUATION", "text": "We use Hadoop versions 1.0.3 and 2.7 in our experiments. The SPSA algorithm described in Section 4.5 is implemented as a process executed in the Resource Manager (and / or NameNode). First, we justify the selection of parameters to be fine-tuned in our experiments, and then provide details of the implementation."}, {"heading": "6.1 Parameter Selection", "text": "As discussed in Section 2, based on the analysis of the data flow of Hadoop MapReduce and the Hadoop manual [4], we identify 11 parameters that have a critical impact on the operation of HDFS and the map / reduce operations; the list of important parameters resulting from the analysis of the MapReduce implementation is in Table 1. Numerous parameters of Hadoop are related to accounting tasks, while some other parameters are related to the performance of underlying operating system tasks. For example, mapred.child.java.opts is a parameter related to Hadoop's Java Virtual Machine (JVM). We avoid tuning those parameters that are best left to optimize low-level operating systems. Instead, we agree parameters that are directly Hadoop-dependent, such as the number of reducers, I / O usage parameters, etc., but we avoid tuning those parameters that are best left to optimize low-level operating systems."}, {"heading": "6.2 Cluster Setup", "text": "Our Hadoop cluster consists of 25 nodes. Each node has an 8-core Intel Xeon E3, 2.50 GHz processor, 3.5 TB hard disk, 16 GB of memory, 1 MB L2 cache, 8 MB L3 cache. One node works as a name node and the remaining nodes are used as data nodes. For optimization and evaluation purposes, we set the number of card slots to 3 and reduce the number of card slots to 2 per node. Therefore, in a single wave of processing card jobs, the cluster can process 24 x 3 = 72 card tasks and reduce 24 x 2 = 48 tasks (see [31] for more details). HDFS block replication has been set to 2. In our experiments, we use a dedicated Hadoop cluster that is not shared with other applications."}, {"heading": "6.3 Benchmark Applications", "text": "To evaluate the performance of the tuning algorithm, we customize representative MapReduce applications. The applications we use are in Table 1. Terasort application takes a text file as input and sorts it. It consists of three components - TeraGen - that generate the input data for the sorting algorithm, TeraSort - an algorithm that implements sorting and TeraValidation - validates the sorted output data. The Grep application searches for a specific pattern in a given input file. The Word Cooccurrence application counts the number of occurrences of a certain word in an input file (can be any text format). Bigram application counts all unique sets of two consecutive words in a set of documents, while the Inverted Index application generates word for document indexing from a list of documents. Word Cood ccurrence application is a popular Natural Language textual processing program that calculates the large word cource."}, {"heading": "6.4 SPSA Iterations", "text": "SPSA is an iterative algorithm and it runs a Hadoop job with different configurations. We refer to these iterations such as the optimization or the learning phase. The algorithm even converts to an optimal value of the configuration parameters. The performance metric (the execution time of the job), which corresponds to the convergent parameter vector, is optimal for the corresponding application. During our evaluations we have seen that the SPSA converts within 20 - 30 iterations and makes two observations within each iteration, i.e. it performs Hadoop job twice taking into account the total number of Hadoop runs during the optimization phase to 40 - 60. It is of utmost importance that the optimization phase is fast, otherwise it can overshadow the advantages it provides. To ensure that the optimization phase is fast, we run the Hadoop jobs on a subset of the work setup."}, {"heading": "6.5 Optimization Settings", "text": "To evaluate the performance of SPSA on various benchmarks, two waves of card tasks were ensured during the execution of jobs. Furthermore, we selected workloads so that the execution time with standard configuration is at least 10 minutes. This was done to avoid the scenario where job setup and cleanup time overshadow the actual runtime and there is virtually nothing for SPSA to optimize. In the cases of Bigram and Inverted Index benchmark executions, we observed that even with small amounts of data, the job completion time is high (as they are reducing operations), so we used small input data files. Using small-format input data files, the absence of two waves of map tasks resulted in these applications taking precedence over the reduction of operations, the absence of two waves of card tasks did not generate much of a hurdle. We optimize Terasort with a Bigram split dataset ranging from Gre30 GB to 200GB to 8p 1GB, and InCo-200GB."}, {"heading": "6.6 Comparison with Related Work", "text": "We compare our method with previous work in the literature on Hadoop Performance Tuning. Specifically, we look at Starfish [15] and Profiling and Performance Analysisbased System (PPABS) [32] frameworks. We briefly describe these methods in Section 3. Starfish is designed for Hadoop version 1 only, while PPABS also works with the latest versions. Therefore, we use both versions of Hadoop in our experiments. To execute Starfish, we use the executable file hosted by the authors of [15] to profile the jobs that are executed on part-time loads. We then obtain the execution time of new jobs by executing the jobs with parameters provided by Starfish. To test PPABS, we collect data sets as described in [32], cluster them and find optimized parameters (using simulated annealing) for each cluster. Each new job is then assigned to a cluster and executed with the parameters optimized for that cluster."}, {"heading": "6.7 Discussion of Results", "text": "It is important to note that the jumps in the plots are due to the noisy nature of the gradient estimate and that they eventually die after a sufficiently large number of iterations. For inverted index benchmarks, the reduction is 80% when compared with the default settings. In the case of Words Cooccurence, the reduction is 22% when compared with the default settings, of 40-60% when compared with the Starfish Optimizer. Inverted index benchmarks are 80% when compared with the default settings. In the case of Worts Coccurence benchmarks, the reduction is 22% when compared with the default settings. SPSA finds the optimal configuration among the parameters."}, {"heading": "6.8 Advantages of SPSA", "text": "The above discussion shows that SPSA performs well when it comes to optimizing Hadoop parameters. We highlight other advantages (see also Table 2) by using our proposed method: 1. Most of the profiling-based methods (Starfish, MROnline, etc.) use the internal Hadoop structure to set \"markers\" for accurately profiling a job. 2. Minor changes in the source code render them useless (clearly Starfish only supports Hadoop versions < 1.0.3). SPSA does not rely on Hadoop's internal structure and only observes the final execution time of the job, which is easy to achieve. 2. The profile version of Hadoop is as mentioned above, profiling-based methods are highly dependent on mapping the source code and any changes in the Hadoop source code."}, {"heading": "7. CONCLUSIONS AND FUTURE WORK", "text": "Hadoop Framework presents a large number of tunable parameters to the user. Although the default setting for these parameters is known, it is important to optimize these parameters to achieve better performance. However, manual adjustment of these parameters is difficult due to the complex nature of the search space and the pronounced effect of cross-sectional interactions, which requires an automatic tuning mechanism. Previous attempts at automatic adjustment have adopted a mathematical model-based approach and have relied on a pre-optimization parameter reduction. As Hadoop continues to evolve, the mathematical models may fail for later versions, and given the level of cross-sectional interaction, it is a good idea to keep as many parameters as possible. In this paper, we proposed a tuning method based on simultaneous disruption of stochastic approximation (SPSA), whereby the key features of the SPSA-based scheme included its ability to use observations of a real system and its insensitivity to the number of parameters."}, {"heading": "8. REFERENCES", "text": "[1] Cloudera, 7 tips for improved MapReduceperformance. http: / / blog.cloudera.com / 2009 / 12 / 7-tips-for-improved-mapreduce-performance /. [2] HDFS Architecture. http: / / hadoop.apache.org / docs / en / r2.4.1 / hadoop-project-dist / hdfsDesign.html. [3] Microsoft, Hadoop Job Optimization. https: / / msdn.microsoft.com / en-us / dn197899.aspx. [4] Parameter manual. https: / hadoop.apache.org / r2.6.0 / hadoop-mapreduce-client / hadoop-reduced-client-core / mapred-default.xml."}], "references": [{"title": "Adaptive autonomous soaring of multiple uavs using simultaneous perturbation stochastic approximation", "author": ["C. Antal", "O. Granichin", "S. Levi"], "venue": "IEEE Conference on Decision and Control (CDC),", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2010}, {"title": "Towards automatic optimization of mapreduce programs", "author": ["S. Babu"], "venue": "In In SoCC, pages 137\u2013142,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2010}, {"title": "Stochastic Approximation: A Dynamical Systems Viewpoint", "author": ["V.S. Borkar"], "venue": "TRIM,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2008}, {"title": "Multiresolution registration of remote sensing imagery by optimization of mutual information using a stochastic gradient", "author": ["A.A. Cole-Rhodes", "K.L. Johnson", "J. LeMoigne", "I. Zavorin"], "venue": "Image Processing, IEEE Transactions on,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2003}, {"title": "MapReduce: Simplified Data", "author": ["J. Dean", "S. Ghemawat"], "venue": "Processing on Large Clusters. Commun. ACM,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2008}, {"title": "Jellyfish: Online performance tuning with adaptive configuration and elastic container in hadoop yarn", "author": ["X. Ding", "Y. Liu", "D. Qian"], "venue": "In Parallel and Distributed Systems (ICPADS),", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2015}, {"title": "Predicting and Optimizing System Utilization and Performance via Statistical Machine Learning", "author": ["A.S. Ganapathi"], "venue": "PhD thesis, EECS Department,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2009}, {"title": "SPSA-based step tracking algorithm for mobile {DBS} reception", "author": ["L. Hao", "M. Yao"], "venue": "Simulation Modelling Practice and Theory,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2011}, {"title": "Profiling, what-if analysis, and cost-based optimization of mapreduce programs", "author": ["H. Herodotou", "S. Babu"], "venue": "PVLDB, 4(11):1111\u20131122,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2011}, {"title": "Starfish: A self-tuning system for big data analytics", "author": ["H. Herodotou", "H. Lim", "G. Luo", "N. Borisov", "L. Dong", "F.B. Cetin", "S. Babu"], "venue": "In In CIDR,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2011}, {"title": "Simulation optimization of airline delay with constraints", "author": ["D.W. Hutchison", "S.D. Hill"], "venue": "In Simulation Conference,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2001}, {"title": "Towards optimizing hadoop provisioning", "author": ["K. Kambatla", "A. Pathak", "H. Pucha"], "venue": "in the cloud. HotCloud,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2009}, {"title": "An analysis of traces from a production mapreduce cluster", "author": ["S. Kavulya", "J. Tan", "R. Gandhi", "P. Narasimhan"], "venue": "In Proceedings of the 2010 10th IEEE/ACM  International Conference on Cluster, Cloud and Grid Computing,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2010}, {"title": "Stochastic Approximation Methods for Constrained and Unconstrained Systems", "author": ["H.J.D. Kushner", "Clark"], "venue": null, "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1978}, {"title": "Quasi-newton smoothed functional algorithms for unconstrained and constrained simulation optimization", "author": ["K. Lakshmanan", "S. Bhatnagar"], "venue": "Computational Optimization and Applications,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2016}, {"title": "Aroma: Automated resource allocation and configuration of mapreduce environment in the cloud", "author": ["P. Lama", "X. Zhou"], "venue": "In Proceedings of the 9th International Conference on Autonomic Computing,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2012}, {"title": "MROnline: MapReduce Online Performance Tuning", "author": ["M. Li", "L. Zeng", "S. Meng", "J. Tan", "L. Zhang", "A.R. Butt", "N. Fuller"], "venue": "In Proceedings of the 23rd International Symposium on High-performance Parallel and Distributed Computing,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2014}, {"title": "Robust adaptive pole placement for linear time-varying systems", "author": ["Y. Li", "H.-F. Chen"], "venue": "IEEE transactions on automatic control,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 1996}, {"title": "A comparison of approaches to large-scale data analysis", "author": ["A. Pavlo", "E. Paulson", "A. Rasin", "D.J. Abadi", "D.J. DeWitt", "S. Madden", "M. Stonebraker"], "venue": "In Proceedings of the 2009 ACM SIGMOD International Conference on Management of Data,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2009}, {"title": "Threshold tuning using stochastic optimization for graded signal control", "author": ["L. Prashanth", "S. Bhatnagar"], "venue": "Vehicular Technology, IEEE Transactions on,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2012}, {"title": "Adaptive system optimization using random directions stochastic approximation", "author": ["L.A. Prashanth", "S. Bhatnagar", "M.C. Fu", "S. Marcus"], "venue": "IEEE Transactions on Automatic Control,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2016}, {"title": "Multivariate stochastic approximation using a simultaneous perturbation gradient approximation", "author": ["J.C. Spall"], "venue": "Automatic Control, IEEE Transactions on,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 1992}, {"title": "Multivariate stochastic approximation using a simultaneous perturbation gradient approximation", "author": ["J.C. Spall"], "venue": "IEEE Transactions on Automatic Control,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 1992}, {"title": "Convergence rates of perturbation-analysis-robbins-monro-single-run algorithms for single server queues", "author": ["Q.-Y. Tang", "H.-F. Chen", "Z.-J. Han"], "venue": "IEEE Transactions on Automatic Control,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 1997}, {"title": "Baldeschwieler. Apache Hadoop YARN: Yet Another Resource Negotiator", "author": ["V.K. Vavilapalli", "A.C. Murthy", "C. Douglas", "S. Agarwal", "M. Konar", "R. Evans", "T. Graves", "J. Lowe", "H. Shah", "S. Seth", "B. Saha", "C. Curino", "O. O\u2019Malley", "S. Radia", "B. Reed"], "venue": "In Proceedings of the 4th Annual Symposium on Cloud Computing,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2013}, {"title": "Hadoop: The Definitive Guide, Storage and  Analysis at Internet Scale, 4th Edition", "author": ["T. White"], "venue": null, "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2015}, {"title": "A self-tuning system based on application profiling and performance analysis for optimizing hadoop mapreduce cluster configuration", "author": ["D. Wu", "A.S. Gokhale"], "venue": "In 20th Annual International Conference on High Performance Computing,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2013}], "referenceMentions": [{"referenceID": 18, "context": "Currently available parallel processing systems are database systems [24] like Teradata, Aster Data, Vertica etc.", "startOffset": 69, "endOffset": 73}, {"referenceID": 4, "context": "MapReduce[10] is one-such programming model.", "startOffset": 9, "endOffset": 13}, {"referenceID": 25, "context": "Apache Hadoop[31] is an open-source implementation of MapReduce written in Java for distributed storage and processing of very large data sets on clusters built using commodity hardware.", "startOffset": 13, "endOffset": 17}, {"referenceID": 11, "context": "The problem of Hadoop performance being limited by the parameter configuration was recognized in [17].", "startOffset": 97, "endOffset": 101}, {"referenceID": 18, "context": "Unlike SQL, MapReduce jobs cannot be modeled using a small and finite space of relational operators [24].", "startOffset": 100, "endOffset": 104}, {"referenceID": 11, "context": "The necessity for tuning of Hadoop parameters was first emphasized in [17], which proposed a method to determine the optimum configuration given a set of computing resources.", "startOffset": 70, "endOffset": 74}, {"referenceID": 9, "context": "Recent efforts in the direction of automatic tuning of the Hadoop parameters include Starfish[15], AROMA[21], MROnline[22], PPABS [32] and JellyFish [11].", "startOffset": 93, "endOffset": 97}, {"referenceID": 15, "context": "Recent efforts in the direction of automatic tuning of the Hadoop parameters include Starfish[15], AROMA[21], MROnline[22], PPABS [32] and JellyFish [11].", "startOffset": 104, "endOffset": 108}, {"referenceID": 16, "context": "Recent efforts in the direction of automatic tuning of the Hadoop parameters include Starfish[15], AROMA[21], MROnline[22], PPABS [32] and JellyFish [11].", "startOffset": 118, "endOffset": 122}, {"referenceID": 26, "context": "Recent efforts in the direction of automatic tuning of the Hadoop parameters include Starfish[15], AROMA[21], MROnline[22], PPABS [32] and JellyFish [11].", "startOffset": 130, "endOffset": 134}, {"referenceID": 5, "context": "Recent efforts in the direction of automatic tuning of the Hadoop parameters include Starfish[15], AROMA[21], MROnline[22], PPABS [32] and JellyFish [11].", "startOffset": 149, "endOffset": 153}, {"referenceID": 11, "context": "We observe that collecting statistical data to create virtual profiles and estimating execution time using mathematical model (as in [17, 15, 11, 21, 22, 32]) requires significant level of expertise which might not be available always.", "startOffset": 133, "endOffset": 157}, {"referenceID": 9, "context": "We observe that collecting statistical data to create virtual profiles and estimating execution time using mathematical model (as in [17, 15, 11, 21, 22, 32]) requires significant level of expertise which might not be available always.", "startOffset": 133, "endOffset": 157}, {"referenceID": 5, "context": "We observe that collecting statistical data to create virtual profiles and estimating execution time using mathematical model (as in [17, 15, 11, 21, 22, 32]) requires significant level of expertise which might not be available always.", "startOffset": 133, "endOffset": 157}, {"referenceID": 15, "context": "We observe that collecting statistical data to create virtual profiles and estimating execution time using mathematical model (as in [17, 15, 11, 21, 22, 32]) requires significant level of expertise which might not be available always.", "startOffset": 133, "endOffset": 157}, {"referenceID": 16, "context": "We observe that collecting statistical data to create virtual profiles and estimating execution time using mathematical model (as in [17, 15, 11, 21, 22, 32]) requires significant level of expertise which might not be available always.", "startOffset": 133, "endOffset": 157}, {"referenceID": 26, "context": "We observe that collecting statistical data to create virtual profiles and estimating execution time using mathematical model (as in [17, 15, 11, 21, 22, 32]) requires significant level of expertise which might not be available always.", "startOffset": 133, "endOffset": 157}, {"referenceID": 26, "context": "Further, given the presence of cross-parameter interaction it is a good idea to retain as many parameters as possible (as opposed to reducing the parameters [32]) in the tuning phase.", "startOffset": 157, "endOffset": 161}, {"referenceID": 21, "context": "In this paper, we present a novel tuning methodology based on a noisy gradient method known as the simultaneous perturbation stochastic approximation (SPSA) algorithm [27].", "startOffset": 167, "endOffset": 171}, {"referenceID": 1, "context": "Thus, unlike [7] reducing the search space is not a requirement.", "startOffset": 13, "endOffset": 16}, {"referenceID": 9, "context": "Further, we also observe a reduction of 45% in execution times, when compared to prior [15] methods.", "startOffset": 87, "endOffset": 91}, {"referenceID": 4, "context": "Hadoop is an open source implementation of the MapReduce[10], which has gained a huge amount of popularity in recent years as it can be used over commodity hardware.", "startOffset": 56, "endOffset": 60}, {"referenceID": 25, "context": "A HDFS cluster (see [31]) consists of a single NameNode, a master server, and multiple slave DataNodes.", "startOffset": 20, "endOffset": 24}, {"referenceID": 24, "context": "Hadoop MapReduce version 2 (v2, also known as Yet Another Resource Negotiator (YARN)[30]) has a different architecture.", "startOffset": 84, "endOffset": 88}, {"referenceID": 6, "context": "Some early works [12, 18] have focussed on analysing the MapReduce performance and not addressed the problem of parameter tuning.", "startOffset": 17, "endOffset": 25}, {"referenceID": 12, "context": "Some early works [12, 18] have focussed on analysing the MapReduce performance and not addressed the problem of parameter tuning.", "startOffset": 17, "endOffset": 25}, {"referenceID": 6, "context": "The authors in [12] develop models for predicting performance of Hive queries and ETL (Extract Transform Load) kind of MapReduce jobs.", "startOffset": 15, "endOffset": 19}, {"referenceID": 12, "context": "MapReduce logs of a M45 supercomputing cluster (released by Yahoo!) are analysed in [18].", "startOffset": 84, "endOffset": 88}, {"referenceID": 12, "context": "Based on this categorization, [18] suggests improvements in Hadoop MapReduce which can mitigate performance bottlenecks and reduce job failures.", "startOffset": 30, "endOffset": 34}, {"referenceID": 9, "context": "Attempts toward building an optimizer for hadoop performance started with Starfish[15].", "startOffset": 82, "endOffset": 86}, {"referenceID": 9, "context": "In Starfish [15, 14], a Profiler collects detailed statistical information (like data flow and cost statistics) from unmodified Mapreduce program during full or partial execution.", "startOffset": 12, "endOffset": 20}, {"referenceID": 8, "context": "In Starfish [15, 14], a Profiler collects detailed statistical information (like data flow and cost statistics) from unmodified Mapreduce program during full or partial execution.", "startOffset": 12, "endOffset": 20}, {"referenceID": 15, "context": "Works following Starfish are [21, 32].", "startOffset": 29, "endOffset": 37}, {"referenceID": 26, "context": "Works following Starfish are [21, 32].", "startOffset": 29, "endOffset": 37}, {"referenceID": 15, "context": "In the online phase [21] trains a SVM which makes accurate and fast prediction of a job\u2019s performance for various configuration parameters and input data sizes.", "startOffset": 20, "endOffset": 24}, {"referenceID": 26, "context": "In [32], the optimal parameter configuration for every cluster is obtained through simulated annealing, albeit for a reduced parameter search space.", "startOffset": 3, "endOffset": 7}, {"referenceID": 16, "context": "An online MapReduce performance tuner (MROnline) is developed in [22].", "startOffset": 65, "endOffset": 69}, {"referenceID": 24, "context": "It is desgined and implemented on YARN [30] (described in Section 2).", "startOffset": 39, "endOffset": 43}, {"referenceID": 8, "context": "In [14], the optimization is based on the what-if engine which uses a mix of simulation and model-based estimation.", "startOffset": 3, "endOffset": 7}, {"referenceID": 26, "context": "In [32], authors make use of available knowledge from literature in order to reduce the parameter space and they make use of simulated annealing to find the right parameter setting in the reduced space.", "startOffset": 3, "endOffset": 7}, {"referenceID": 19, "context": "The performance of various complex systems such as traffic control [25], unmanned aerial vehicle (UAV) control [6], remote sensing [9], communication in satellites [13] and airlines [16] depends on a set of tunable parameters (denoted by \u03b8).", "startOffset": 67, "endOffset": 71}, {"referenceID": 0, "context": "The performance of various complex systems such as traffic control [25], unmanned aerial vehicle (UAV) control [6], remote sensing [9], communication in satellites [13] and airlines [16] depends on a set of tunable parameters (denoted by \u03b8).", "startOffset": 111, "endOffset": 114}, {"referenceID": 3, "context": "The performance of various complex systems such as traffic control [25], unmanned aerial vehicle (UAV) control [6], remote sensing [9], communication in satellites [13] and airlines [16] depends on a set of tunable parameters (denoted by \u03b8).", "startOffset": 131, "endOffset": 134}, {"referenceID": 7, "context": "The performance of various complex systems such as traffic control [25], unmanned aerial vehicle (UAV) control [6], remote sensing [9], communication in satellites [13] and airlines [16] depends on a set of tunable parameters (denoted by \u03b8).", "startOffset": 164, "endOffset": 168}, {"referenceID": 10, "context": "The performance of various complex systems such as traffic control [25], unmanned aerial vehicle (UAV) control [6], remote sensing [9], communication in satellites [13] and airlines [16] depends on a set of tunable parameters (denoted by \u03b8).", "startOffset": 182, "endOffset": 186}, {"referenceID": 21, "context": "The SPSA algorithm [27] computes the gradient of a function with only 2 or fewer perturbations.", "startOffset": 19, "endOffset": 23}, {"referenceID": 17, "context": "Then for any small positive constant \u03b4 > 0, the one-sided SPSA algorithm [23, 29] obtains an estimate of the gradient according to equation (3) given below.", "startOffset": 73, "endOffset": 81}, {"referenceID": 23, "context": "Then for any small positive constant \u03b4 > 0, the one-sided SPSA algorithm [23, 29] obtains an estimate of the gradient according to equation (3) given below.", "startOffset": 73, "endOffset": 81}, {"referenceID": 2, "context": "The iterative update in (5) is known as a stochastic approximation [8] recursion.", "startOffset": 67, "endOffset": 70}, {"referenceID": 13, "context": "191196 of [19].", "startOffset": 10, "endOffset": 14}, {"referenceID": 25, "context": "Hence, in a single wave of Map jobs processing, the cluster can process 24 \u00d7 3 = 72 map tasks and 24 \u00d7 2 = 48 reduce tasks (for more details see [31]).", "startOffset": 145, "endOffset": 149}, {"referenceID": 22, "context": "Theoretical justification for net improvements to efficiency by such gradient averaging is given in [28].", "startOffset": 100, "endOffset": 104}, {"referenceID": 9, "context": "Specifically, we look at Starfish[15] as well as Profiling and Performance Analysisbased System (PPABS) [32] frameworks.", "startOffset": 33, "endOffset": 37}, {"referenceID": 26, "context": "Specifically, we look at Starfish[15] as well as Profiling and Performance Analysisbased System (PPABS) [32] frameworks.", "startOffset": 104, "endOffset": 108}, {"referenceID": 9, "context": "To run Starfish, we use the executable hosted by the authors of [15] to profile the jobs run on partial workloads.", "startOffset": 64, "endOffset": 68}, {"referenceID": 26, "context": "For testing PPABS, we collect datasets as described in [32], cluster them and find optimized parameters (using simulated annealing) for each cluster.", "startOffset": 55, "endOffset": 59}, {"referenceID": 14, "context": "Further, other simulation optimization algorithms like [20, 26] can be applied to the problem of Hadoop parameter tuning.", "startOffset": 55, "endOffset": 63}, {"referenceID": 20, "context": "Further, other simulation optimization algorithms like [20, 26] can be applied to the problem of Hadoop parameter tuning.", "startOffset": 55, "endOffset": 63}], "year": 2016, "abstractText": "Hadoop MapReduce is a framework for distributed storage and processing of large datasets that is quite popular in big data analytics. It has various configuration parameters (knobs) which play an important role in deciding the performance i.e., the execution time of a given big data processing job. Default values of these parameters do not always result in good performance and hence it is important to tune them. However, there is inherent difficulty in tuning the parameters due to two important reasons firstly, the parameter search space is large and secondly, there are cross-parameter interactions. Hence, there is a need for a dimensionality-free method which can automatically tune the configuration parameters by taking into account the cross-parameter dependencies. In this paper, we propose a novel Hadoop parameter tuning methodology, based on a noisy gradient algorithm known as the simultaneous perturbation stochastic approximation (SPSA). The SPSA algorithm tunes the parameters by directly observing the performance of the Hadoop MapReduce system. The approach followed is independent of parameter dimensions and requires only 2 observations per iteration while tuning. We demonstrate the effectiveness of our methodology in achieving good performance on popular Hadoop benchmarks namely Grep, Bigram, Inverted Index, Word Co-occurrence and Terasort. Our method, when tested on a 25 node Hadoop cluster shows 66% decrease in execution time of Hadoop jobs on an average, when compared to the default configuration. Further, we also observe a reduction of 45% in execution times, when compared to prior methods.", "creator": "LaTeX with hyperref package"}}}