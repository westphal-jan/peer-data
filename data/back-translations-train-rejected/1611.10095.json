{"id": "1611.10095", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Nov-2016", "title": "System-Generated Requests for Rewriting Proposals", "abstract": "We present an online deliberation system using mutual evaluation in order to collaboratively develop solutions. Participants submit their proposals and evaluate each other's proposals; some of them may then be invited by the system to rewrite 'problematic' proposals. Two cases are discussed: a proposal supported by many, but not by a given person, who is then invited to rewrite it for making yet more acceptable; and a poorly presented but presumably interesting proposal. The first of these cases has been successfully implemented. Proposals are evaluated along two axes-understandability (or clarity, or, more generally, quality), and agreement. The latter is used by the system to cluster proposals according to their ideas, while the former is used both to present the best proposals on top of their clusters, and to find poorly written proposals candidates for rewriting. These functionalities may be considered as important components of a large scale online deliberation system.", "histories": [["v1", "Wed, 30 Nov 2016 11:29:25 GMT  (362kb)", "http://arxiv.org/abs/1611.10095v1", "9 pages, 1 figure, presented at e-Part 2011 conference"]], "COMMENTS": "9 pages, 1 figure, presented at e-Part 2011 conference", "reviews": [], "SUBJECTS": "cs.AI cs.CY cs.HC cs.SI", "authors": ["pietro speroni di fenizio", "cyril velikanov"], "accepted": false, "id": "1611.10095"}, "pdf": {"name": "1611.10095.pdf", "metadata": {"source": "CRF", "title": "SYSTEM-GENERATED REQUESTS FOR REWRITING PROPOSALS", "authors": ["Pietro Speroni di Fenizio", "Cyril Velikanov"], "emails": ["(speroni@dei.uc.pt", "(cvelikanov@gmail.com),"], "sections": [{"heading": null, "text": "SYSTEM-GENERATED REQUIREMENTS FOR REVOLUTIONS Speroni di Fenizio1, Cyril Velikanov2We present an online reflection system based on mutual evaluation to jointly develop solutions. Participants submit their proposals and evaluate each other's proposals; some of them may then be asked by the system to rewrite \"problematic\" proposals. Two cases are discussed: a proposal supported by many but not by a specific person who is then asked to rewrite it to make it more acceptable; and a poorly presented but probably interesting proposal. The first of these cases has been successfully implemented. Proposals are evaluated along two axes - comprehensibility (or clarity or general quality) and consent. The latter is used by the system to cluster proposals according to their ideas, while the former is used both to present the best proposals at the top of their clusters and to rewrite poorly written proposals for the rewrite."}, {"heading": "1. Introduction", "text": "Our study presented in this paper refers to open online consultation and open online collaboration. In the context of eParticipation, online consultation is indeed an essential aspect; while online collaboration should also be considered essential if we want eParticipation to be targeted, that is, that we look for a solution to a given problem and are productive, that is, that it leads to jointly agreed results. As eParticipation remains a rather vague concept, we will start by better delineating the types of activities we are considering in this paper, bearing in mind that a particular eParticipation activity (project, campaign) involves a number of active participants, i.e. participants who can write their own contributions, but who can at least actively read and evaluate each other's contributions (aka evaluate). A contribution is essentially a proposal (how to solve the problem to be discussed) or a comment on one or more other contributions (suggestions or comments). Other types of contributions may indeed also be considered, but we are only interested in them once, but only to a lesser extent."}, {"heading": "1.1 Clustering of Contributions", "text": "One of our basic assumptions is that an open online consultation on an important social problem, if it is expected to be targeted and productive, can attract a really large number of 1 Pietro Speroni di Fenizio (speroni @ dei.uc.pt), CISUC, Department of Informatics Engineering, University of Coimbra, P\u00f3lo II, 3030-290 Coimbra, Portugal2 Cyril Velikanov (cvelikanov @ gmail.com), \"Memorial,\" Malyi Karetnyi per. 12, Moscow 127051, Russia, and PoliTech Institute, 67 Saint Bernard St., Brussels 1060, Belgium active participants (in order of thousands, or tens of thousands or more), in which case it is referred to as a mass online consultation (MOD) that is unable to write a large number of contributions (suggestions and comments), all directly or indirectly related to the same problem."}, {"heading": "1.2 Ranking of Contributions Within a Cluster", "text": "The task of pooling proposals should result in a grouping of proposals that are similar or compatible in the eyes of the participants who have read (and evaluated) them. Some of these clusters can be quite large and therefore require a finer analysis. On the other hand, a particular proposal containing an idea can present the idea in a more or less clear, concise and argumentative way. At the other end of this \"quality scale,\" we can have many poorly formulated proposals, ranging from barely intelligible or completely opaque texts. In the latter case, such a quality ranking, if carried out by participants and aggregated through the support system, can help participants navigate through a large number of proposals. As for texts that seemed incomprehensible to most of their readers, such a text may nevertheless have been sufficiently well understood by some of them, who may therefore have been able to make another judgment on the ideas contained therein - in the form of a simple (dis-) agreement or comment. In fact, it would be very desirable for the rest of the idea to be partially or partially distributed or poorly."}, {"heading": "1.3 Collaboration by Rewriting Proposals", "text": "Let us now turn to online cooperation - which can be seen either as an integral part of a targeted online consultation or as a separate task. However, in the first case, it is hard to imagine that a very large number of participants could be able to work productively together, for example, in preparing a joint \"final proposal\" from a set of \"original\" proposals. A more workable solution would be to set up within a large advisory community one or more online working groups of limited size (about 30 people) to work on a common proposal. In such a case, as in the case of a standalone collaborative working group, there should be an established mechanism to support the working group's efforts to reach agreement on a generally accepted final text by rewriting the input texts, perhaps even several periods of time."}, {"heading": "1.4 System-Requested User Actions", "text": "In each of the cases listed above, it seems necessary for the support system to support the consultation and / or cooperation process by requiring that certain actions be carried out by specific participants at specific times. These actions may include, in particular: (1) additional evaluation (evaluation) of a particular contribution (proposal or comment) that has not yet been seen sufficiently to reliably assign it to a cluster and place it within that cluster; (2) reformulation of a poorly written proposal that may nevertheless be of interest to the community; (3) reformulation of a controversial proposal to make it more acceptable to a larger part of the community or a working group. The idea of system-related user actions has several aspects that will be discussed in the following sections. First, we need an algorithm for selecting the most necessary actions (s) to be requested by the system at a given time."}, {"heading": "2. Rewriting Proposals by Those Who Disagree With Them", "text": "The idea of loading the system with the task of rewriting specific proposals from certain participants with which they disagree was implemented by one of the authors in a web-based collaboration tool \"Vilfredo Goes to Athens\" (http: / / vilfredo.org). The system is actually open to participation; its description can be in [2], the participants can specify a problem, and then move their own proposals (how to solve the problem) and evaluate the proposal of the other. The system uses a human-based genetic algorithm (HBGA), in which the entire process can have several cycles, each consisting of an aproposocial phase and an evaluation phase in which a new generation of proposals is written, based on, in turn, by the other."}, {"heading": "3. Clustering, Ranking, and System-Requested Appraisal Actions", "text": "Our next step concerns a mass online consultation support system (MOD), the necessity of which has been discussed in the introductory chapter of this paper, which will include an algorithm for clustering and ranking participants \"contributions - suggestions, comments and more - based on individual ratings (ratings) of each entry by a limited number of other participants. To ensure a more objective evaluation of each entry, especially immediately after it is written and uploaded to the system, participants who come online are invited to evaluate proposals that the system presents to them randomly, so that each proposal is evaluated on average the same number of times and everyone receives at least a predefined minimum number of ratings. Furthermore, at this stage, no participant should be able to see the scores of other participants to avoid unconscious biases. If, at the end of this period of\" blind reviews, \"some proposals have not been read sufficiently, they may be considered equivalent for additional evaluation among the currently selected participants, with these contributions possibly selected randomly."}, {"heading": "3.1 Managing Public Responsiveness to System Requests", "text": "Experience from existing projects, which involve user actions to evaluate (evaluate) texts and other articles and / or label them, shows that people are typically active enough to perform such actions at their own discretion. It remains to be seen how much the people involved in future large-scale eParticipation activities would respond to the same type of actions if they are not voluntary but requested by the system. We can expect a sufficient level of responsiveness, since the ability to participate and collaborate is the company itself. However, if the public does not respond sufficiently, the support system could optionally set and manage specific incentives for each participant. For example, the system could maintain two activity counterpoints for each participant, one for his / her voluntary actions, in particular for writing one's own contributions, another for actions requested by the system. If the second counter is too low, new contributions by that participant will be temporarily blocked until a sufficient number of actions requested by the system are considered inappropriate by the system, i.e. a system that the system itself may require this moment to be inappropriate or that the specific actions may be required by the system."}, {"heading": "3.2 Two-Parameter Appraisal: How Well Presented, How Much Do I Agree", "text": "Our algorithm of clustering and ranking the participants \"contributions is based on their double evaluation, first by a few randomly selected participants (peer reviewers), then by the one who wants to read and rate a given contribution. Presumably, the latter (discretionary assessors) are supported by the support system when looking for contributions that are potentially most interesting to them, and this system-related selection is made by using the said algorithm, as briefly explained below; see [3] for more details.Thus, each participant, when reading a contribution, is expected to rate it on two different scales, one for the quality of the contribution, one for the reader's approval of the ideas contained therein. The quality of a given text is considered to be its intrinsic quality, making it attractive for objective evaluation by the reader; while the agreement is actually a fully subjective one, as it depends on the beliefs, beliefs, etc."}, {"heading": "3.3 System Requests for Additional Appraisals", "text": "At some point, our algorithm might find that there is not enough data to decide which cluster to go to on a given proposal (or perhaps it should create a new cluster); or to better distinguish the difference between two proposals or two clusters. In such cases, the system may require additional ratings from participants. However, at this stage, not all participants are equal in their ability to make a rating that will be critical to the system. If, for example, participant U has not read and rated either A or B, he should do so for both so that the system increases his knowledge of (dis-) similarities between A and B. On the other hand, if V has rated A but not B or vice versa, the system may ask them to rate the other, and thus obtain valuable information by requesting only one measure, rather than two in the case of U."}, {"heading": "4. Rewriting Poorly Written but Agreeable Proposals", "text": "Let us now discuss in more detail a very simple case of two-parameter evaluation or evaluation, in which each participant is asked to characterise a proposal with only one of three discrete values: \"agree,\" \"disagree\" and \"do not understand.\" Despite the simplicity and simplicity of this method, it is still a \"two-parameter\" proposal, and it can provide the system with fairly rich information both about the proposals on the table and about the community of participants."}, {"heading": "4.1 Agreement Depends Upon Understanding.", "text": "Although the quality in which a proposal is made, and how much a person agrees with it, is considered independent, they are not, because only in so far as we understand a proposal can we agree or disagree with it. We cannot agree or disagree on something we do not understand. These two metrics are a bit like light and color. If there is no light, we cannot perceive color, and it does not make sense to talk about the bandwidth of a light source if there is no light source. So, the brighter a light source is, the better we can distinguish different colors. Here, we are looking at only two \"colors\" (agree, disagree), and the brightness would show how clearly we understand them (see Figure 1 below; however, the printed version does not show colors). So the simplest form of evaluation that can still work in our system should have three discrete options: \"agree,\" \"\" do not agree, \"and\" do not understand. \""}, {"heading": "4.2 Using the Above Two Measures.", "text": "Once the measures are taken and users begin to evaluate the existing proposals, the system recovers a lot of useful information, much more than what can be deduced from an ordinary \"linear\" evaluation, even from a multi-stage one. The most obvious information that can be collected is firstly, to what extent a proposal is understandable and secondly, to what extent people agree with it. But there is also less obvious data that can be collected, such as: which users are able to make proposals that are generally understandable; which users understand and agree on a particular proposal; which proposals have the same user base (in other words, which proposals are accepted by the same users)."}, {"heading": "4.3 Using Agreement Data for Clustering", "text": "Suppose we have n proposals and m user. Let Kn be the fully weighted undirected graph with n nodes and n * (n-1) / 2 edges. Let EAB be the edge from node A to node B. We can assign to the EAB a weight indicating how many people who voted for A also voted for B, and how many people who voted for B. Thus, the weight can be defined as W = | A-B, that is, as W = | A-B |, i.e. the cardinality of (A-average B) divided by the cardinality of (A-union B); it is a value between 0 and 1, where W = 0 if no one voted for A and B, and W = 1 if everyone who voted for A voted for B and vice versa. There are many simple ways to bundle the nodes into such a weighted graph, where W = 0 if no one voted for A and B, and everyone who voted for A, and vice versa."}, {"heading": "4.4 Using Quality Evaluation", "text": "While the evaluation of conformity can and should be used to pool proposals, the evaluation of quality (i.e., clarity in our case, i.e., how comprehensible they are) can be used for another purpose. Namely, by counting the percentage of participants who did not understand a given proposal, the system can find out which ones are not clearly written, and then ask someone among the participants to rewrite them into obscure proposals. We do not think it is practical to suggest to an author to rewrite his or her own proposal, because we assume that each author has already done his or her best. However, we also do not simply want to ask a random user to rewrite a proposal. Instead, the person who needs to rewrite a proposal should understand the proposal, (b) be able to rewrite (suggestions) well, and possibly better than the original author, and (c) agree with the proposal that is not strictly necessary."}], "references": [{"title": "Procedures and Methods for Cross-community Online Deliberation, in: eJournal of eDemocracy & Open Government (JeDem", "author": ["C. Velikanov"], "venue": "Danube University,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2010}, {"title": "Don't Vote, Evolve", "author": ["P. Speroni di Fenizio", "D. Paterson"], "venue": "in: Proceedings of the Second international conference on eParticipation (ePart", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2010}, {"title": "Mutual Moderation and Appraisal of Contributions in eParticipation", "author": ["C. Velikanov"], "venue": "in: Proceedings of the eDem 2010 Conference, Austrian Computer Society, Vienna,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2010}, {"title": "Graph clustering, in: Computer Science Review, Volume 1, Issue", "author": ["S.E. Schaeffer"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2007}, {"title": "Introduction to Algorithms (3rd ed.)", "author": ["Th.H. Cormen", "Ch.E. Leiserson", "Ronald L. Rivest", "Clifford Stein"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2009}, {"title": "Pareto Optimality in Coevolutionary Learning, in: Advances in Artificial Life", "author": ["G. Sevan", "J.B. Ficici. Pollack"], "venue": "Proceedings of the 6th European ECAL Conference,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2001}], "referenceMentions": [{"referenceID": 0, "context": "We refer to [1] for a more detailed rationale.", "startOffset": 12, "endOffset": 15}, {"referenceID": 1, "context": "The website is actually open for participation; its description can be found in [2].", "startOffset": 80, "endOffset": 83}, {"referenceID": 5, "context": "The selection is done by extracting a Pareto front [7] of the proposals.", "startOffset": 51, "endOffset": 54}, {"referenceID": 2, "context": "tion will be made by using said algorithm, as briefly explained below; see [3] for more details.", "startOffset": 75, "endOffset": 78}, {"referenceID": 4, "context": "There are many straightforward ways to cluster the nodes in such a weighted graph; the simplest one is to delete every edge that has a weight less than x (see [5]).", "startOffset": 159, "endOffset": 162}, {"referenceID": 3, "context": "[4]).", "startOffset": 0, "endOffset": 3}], "year": 2011, "abstractText": "We present an online deliberation system using mutual evaluation in order to collaboratively develop solutions. Participants submit their proposals and evaluate each other\u2019s proposals; some of them may then be invited by the system to rewrite \u201cproblematic\u201d proposals. Two cases are discussed: a proposal supported by many, but not by a given person, who is then invited to rewrite it for making yet more acceptable; and a poorly presented but presumably interesting proposal. The first of these cases has been successfully implemented. Proposals are evaluated along two axes\u2014understandability (or clarity, or, more generally, quality), and agreement. The latter is used by the system to cluster proposals according to their ideas, while the former is used both to present the best proposals on top of their clusters, and to find poorly written proposals candidates for rewriting. These functionalities may be considered as important components of a large scale online deliberation system.", "creator": "Pages"}}}