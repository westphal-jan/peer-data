{"id": "1611.10120", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Nov-2016", "title": "Fusion of EEG and Musical Features in Continuous Music-emotion Recognition", "abstract": "Emotion estimation in music listening is confronting challenges to capture the emotion variation of listeners. Recent years have witnessed attempts to exploit multimodality fusing information from musical contents and physiological signals captured from listeners to improve the performance of emotion recognition. In this paper, we present a study of fusion of signals of electroencephalogram (EEG), a tool to capture brainwaves at a high-temporal resolution, and musical features at decision level in recognizing the time-varying binary classes of arousal and valence. Our empirical results showed that the fusion could outperform the performance of emotion recognition using only EEG modality that was suffered from inter-subject variability, and this suggested the promise of multimodal fusion in improving the accuracy of music-emotion recognition.", "histories": [["v1", "Wed, 30 Nov 2016 12:24:57 GMT  (245kb,D)", "http://arxiv.org/abs/1611.10120v1", "The short version of this paper is accepted to appear as an abstract in the proceedings of AAAI-17 (student abstract and poster program)"]], "COMMENTS": "The short version of this paper is accepted to appear as an abstract in the proceedings of AAAI-17 (student abstract and poster program)", "reviews": [], "SUBJECTS": "cs.AI cs.HC", "authors": ["nattapong thammasan", "ken-ichi fukui", "masayuki numao"], "accepted": false, "id": "1611.10120"}, "pdf": {"name": "1611.10120.pdf", "metadata": {"source": "CRF", "title": "Fusion of EEG and Musical Features in Continuous Music-emotion Recognition", "authors": ["Nattapong Thammasan", "Ken-ichi Fukui", "Masayuki Numao"], "emails": ["nattapong@ai.sanken.osaka-u.ac.jp"], "sections": [{"heading": null, "text": "In this paper, we present a study on the fusion of signals from the electroencephalogram (EEG), a tool for capturing brain waves with high temporal resolution, and musical characteristics at the decision level in detecting the temporally varying binary classes of arousal and valence. Our empirical results showed that fusion could only outperform the performance of emotion detection through EEG modality, which suffered from variability between subjects, and this suggested the promise of multimodal fusion to improve the accuracy of detection of music and emotions."}, {"heading": "1 Introduction", "text": "In fact, most people who are able to move are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move and to move."}, {"heading": "2 Research Methodology", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Experimental Protocol", "text": "Subsequently, the songs were presented as synthetic sounds, separating the MIDI files from the lyrics; the MIDI files also made it possible to study the music that is considered to be future work; the songs in the library were between 73 and 147 s long; the MIDI files were separated from the lyrics; and the MIDI files made it possible to explore and develop the music."}, {"heading": "2.2 EEG Features", "text": "In this paper, we applied the fractal dimension (FD) approach to extract characteristics from EEG signals due to their simplicity and excellent performance in previous affective computer studies [17, 18]. The fractal dimension is a non-negative real value that quantifies the complexity and irregularity of data and can be used to demonstrate the complexity of a time-varying EEG signal. We applied Higuchi algorithm [4] to derive the FD value from each specific window of EEG signals in this study. Previous studies reported that asymmetries of characteristics extracted from symmetrical electrode pairs could be used as additional informative characteristics to classify emotional states [17,18]. Therefore, we also added asymmetry indices to our original EEG characteristic set by summarizing the differential asymmetries of five left-right electrode pairs in one EEG-computed feature set."}, {"heading": "2.3 Musical Features", "text": "To extract the emotional expression in the music, we used MIRtoolbox version 1.6.1 [11], a MATLAB toolbox that provides an integrated set of functions to extract musical characteristics from audio files. First, our MIDI files were converted to WAV format at a sampling rate of 44.1 kHz to be compatible with the toolbox. Then, in a specific window, we extracted the high-level musical characteristics using the features function. 2 / 8Then, we calculated the averages of the characteristics of each window using the Mirmean function to summarize the characteristics of the characteristics in the window. A summary of the musical characteristics can be found in Table 1. Some of the characteristics were selected after previous work [12]."}, {"heading": "2.4 Feature-level Multimodal Fusion of EEG and Musical Features", "text": "In decision-level fusion, the classification of the individual modalities is processed independently of each other and the results of the classifiers are later combined to achieve final results. In this work, we first classify EEEG and music modalities individually and then combine the classification outputs linearly. In binary classification, pxEEG and p x music [0, 1] are to denote the classification outputs of the EEEG and music modality respectively for class x [1, 2] and then the probability of the output class, namely pxmultimodal, is specified for class x bypxmultimodal = \u03b1p x EEG + (1 \u2212 \u03b1) pxmusic, (1) where \u03b1 is the weighting factor that fulfils 0 \u2264 1 and determines how the EEG modality contributes to the final decision. Although decision-level fusion allows asynchronous integration of different modalities, we have used synchronous mode, using both the EEG and the diesel for a decision-level comparison between the window size and the music-1."}, {"heading": "2.5 Emotion Classification and Evaluation", "text": "Despite the spatial continuity of the excitation-valence space, most of the recent attempts to evaluate subjects to classify emotional states of EEG signals, the individual subjects were then classified. http: / / overall, emotions were simply performed as classification and not as regression [8,12]. For the sake of simplicity, the problem of binary emotion classification was also addressed in our thesis by categorizing valence into positive and negative classes and classifying excitation into high and low excitation classes. Due to its success in the literature [6, 14], support vector machine (SVM) based on Gaussian radial basic function (kernel scale = 3) was used to classify emotional classes. The SVM classifier was built by MATLAB Statistics and Machine Learning Toolbox1.Emotion classification model can be either subjectspecific or generalized. In other words, the strategizing we were looking for can be performed independently of either the one or both of the subjects."}, {"heading": "3 Results", "text": "First, we examined the results of the subject-dependent and subject-independent classification by comparing decision fusion (DLF), EEG unimodality (EEG), music unimodality (MF), and random level (chance). In decision fusion, we used two different weighting factors (\u03b1), 0.45 (DLF MF), and 0.55 (DLF EEG) to investigate the effects of weight difference on classification performance. Then, we continued to analyze decision confusion, focusing primarily on the weighting factors. As some processes were based on randomization (10-fold cross-validation and the final decision on merger at decision level), the classification was repeated five times and we derived the average across all repetitions. The average degree of confidence of agreement in the commenting of these remaining subjects was 2.4063 (SD = 0.6565), suggesting that the respondents \"choice of songs in our data set was different from the cultural ones."}, {"heading": "3.1 Results of Subject-dependent and Subject-independent Classification", "text": "The averaged subject-dependent classification accuracies of the EEG signals between subjects who used sliding windows of different sizes are shown in Table 2 and the corresponding MCCs in Figure 1. According to the results, music unimodality achieved the best performance in both excitation and valence classification regardless of the window size. Interestingly, the merging of the EEG modality with the music modality achieved the best results in almost all cases. Table 3 and Figure 2 summarize the averaged subject-independent EEG classification accuracies or MCCs. As can be seen, the music modality achieved significantly better performance than other modalities. Interestingly, the EEG modality provided the worst results in each case. Our results suggested that the interindividual variation of the EEG signals could have a negative effect on the EEG classification. Therefore, the inclusion of the EEG signals could not perceive the influence of the independent EG modalities on the characteristics of the EEG EEG, which did not enhance the EEG characteristics noticeably."}, {"heading": "3.2 Analysis of Contribution of Each Modality in Decision-level Fusion", "text": "The literature [8, 9] and the above-mentioned results showed that the difference in the contribution of the individual modalities could influence the results of the merger at the decision level. We therefore analysed the effects of weighting factors (\u03b1 in Eq.1) on the classification in detail by varying the factor from 0 (equivalent to music modality) to 1 (equivalent to EEG uni-modality) in a step of 0.025. Sliding window size was set to 2 s for the subject-dependent classification and 9 s for the subject-independent classification, as the sizes in earlier sections were predominantly high in performance. Results (Figure 3) show that the classification performance decreased with the increase in the contribution of EEG characteristics (namely the shift of \u03b1 from 0 to 1), especially in the subject-independent excitation classification, indicating that music modality played a more important role in the emotion classification. Nevertheless, the higher weighting factors could be higher in the corresponding grades."}, {"heading": "4 Discussion and Conclusion", "text": "We have presented a study on multimodality using EEG and musical characteristics in conjunction with continuous emotion recognition. In this study, we investigated the different size of the sliding window, the dependence of subjects on classification models and the contribution of individual modalities. Empirically, the EEG modality suffered from variation of EEG signals between subjects and the merging of the music modality with EEG characteristics could slightly increase emotion recognition. Future research is encouraged to investigate subjective factors in variation and to offer possible solutions, such as calibration or normalization by individuals. However, the system cannot fully rely on the uniformity of music, which is based on the assumption that emotions during listening to music are subjective. Complete abolition of the EEG modality would have negative effects on the construction of the practical emotion recognition model. Nevertheless, the results would lead to a potential application in solving the cold-start problem, based on the assumption that emotions during listening to music are subjective. The complete abolition of the EEG modality would have negative effects on the construction of the practical emotion recognition model. Nevertheless, the results would rely on a potential application of the cold-start problem based on the assumption that emotions during listening to music are subjective are subjective."}], "references": [{"title": "EEGLAB, SIFT, NFT, BCILAB, and ERICA: New tools for advanced EEG processing", "author": ["A. Delorme", "T. Mullen", "C. Kothe", "Z.A. Acar", "N. Bigdely-Shamlo", "A. Vankov", "S. Makeig"], "venue": "Computational Intelligence and Neuroscience,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2011}, {"title": "A review and meta-analysis of multimodal affect detection systems", "author": ["S.K. D\u2019mello", "J. Kory"], "venue": "ACM Computing Surveys,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2015}, {"title": "Categorical and dimensional affect analysis in continuous input: Current trends and future directions", "author": ["H. Gunes", "B. Schuller"], "venue": "Image and Vision Computing,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2013}, {"title": "Approach to an irregular time series on the basis of the fractal theory", "author": ["T. Higuchi"], "venue": "Physica D,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1988}, {"title": "Emotion recognition based on physiological changes in music listening", "author": ["J. Kim", "E. Andre"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2008}, {"title": "A review on the computational methods for emotional state estimation from the human EEG", "author": ["M.K. Kim", "M. Kim", "E. Oh", "S.P. Kim"], "venue": "Computational and Mathematical Methods in Medicine,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2013}, {"title": "Brain correlates of music-evoked emotions", "author": ["S. Koelsch"], "venue": "Nature Reviews Neuroscience,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2014}, {"title": "DEAP: A database for emotion analysis using physiological signals", "author": ["S. Koelstra", "C. Muhl", "M. Soleymani", "J.S. Lee", "A. Yazdani", "T. Ebrahimi", "T. Pun", "A. Nijholt", "I. Patras"], "venue": "IEEE Transactions on Affective Computing,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2012}, {"title": "Fusion of facial expressions and EEG for implicit affective tagging", "author": ["S. Koelstra", "I. Patras"], "venue": "Image and Vision Computing,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2013}, {"title": "An exploratory study of musical emotions and psychophysiology", "author": ["C.L. Krumhansl"], "venue": "Canadian Journal of Experimental Psychology,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1997}, {"title": "MIR in Matlab (II): A matlab toolbox for music information retrieval", "author": ["O. Lartillot", "P. Toiviainen"], "venue": "In Proceedings of the 8th International Conference on Music Information Retrieval,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2007}, {"title": "Fusion of electroencephalogram dynamics and musical contents for estimating emotional responses in music listening", "author": ["Y.P. Lin", "Y.H. Yang", "T.P. Jung"], "venue": "Frontiers in Neuroscience,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2014}, {"title": "Comparison of the predicted and observed secondary structure of t4 phage lysozyme", "author": ["B.W. Matthews"], "venue": "Biochimica et Biophysica Acta (BBA) - Protein Structure,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1975}, {"title": "Improving multilabel analysis of music titles: A large-scale validation of the correction approach", "author": ["F. Pachet", "P. Roy"], "venue": "IEEE Transactions on Audio, Speech, and Language Processing,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2009}, {"title": "A circumplex model of affect", "author": ["J.A. Russell"], "venue": "Journal of Personality and Social Psychology,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1980}, {"title": "Analysis of EEG signals and facial expressions for continuous emotion detection", "author": ["M. Soleymani", "S. Asghari-Esfeden", "Y. Fu", "M. Pantic"], "venue": "IEEE Transactions on Affective Computing,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2016}, {"title": "Real-time EEG-based emotion recognition for music therapy", "author": ["O. Sourina", "Y. Liu", "M.K. Nguyen"], "venue": "Journal on Multimodal User Interfaces,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2012}, {"title": "Continuous music-emotion recognition based on electroencephalogram", "author": ["N. Thammasan", "K. Moriyama", "K. Fukui", "M. Numao"], "venue": "IEICE Transactions on Information and Systems,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2016}, {"title": "Multimodal fusion framework: A multiresolution approach for emotion classification and recognition from physiological signals", "author": ["G.K. Verma", "U.S. Tiwary"], "venue": "Part 1:162\u2013172,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2014}, {"title": "Machine recognition of music emotion: A review", "author": ["Y.H. Yang", "H.H. Chen"], "venue": "ACM Transactions on Intelligent Systems and Technology,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2012}], "referenceMentions": [{"referenceID": 19, "context": "Recognizing human emotion during music listening is attracting widespread interest in the field of music information retrieval for many years [20] because it could enable a variety of application including music therapy, automatic music composition, and multimedia tagging.", "startOffset": 142, "endOffset": 146}, {"referenceID": 9, "context": "Since the discovery of the relation between music-induced emotion and physiological patterns [10], bodily signals directly recorded from listeners have been employed to model emotional response to music [5].", "startOffset": 93, "endOffset": 97}, {"referenceID": 4, "context": "Since the discovery of the relation between music-induced emotion and physiological patterns [10], bodily signals directly recorded from listeners have been employed to model emotional response to music [5].", "startOffset": 203, "endOffset": 206}, {"referenceID": 5, "context": "Among these attempts, an electroencephalogram (EEG), a tool to capture brainwaves, is a popularly adopted tool because of its excellent temporal resolution, cost effectiveness and fruitfulness of electrical activities nearby the brain, which is the center of emotion processing [6].", "startOffset": 278, "endOffset": 281}, {"referenceID": 2, "context": "In recent years, researchers have emphasized the importance of continuous emotion recognition over the course of time in response to multimedia stimuli [3] (not limited to music stimuli).", "startOffset": 152, "endOffset": 155}, {"referenceID": 15, "context": "Recent works have been proposed to track time-varying emotion continuously annotated by users in response to music videos [16] and songs [18] using EEG dynamics.", "startOffset": 122, "endOffset": 126}, {"referenceID": 17, "context": "Recent works have been proposed to track time-varying emotion continuously annotated by users in response to music videos [16] and songs [18] using EEG dynamics.", "startOffset": 137, "endOffset": 141}, {"referenceID": 1, "context": "Recent efforts to reinforce the emotion recognition model include using EEG features in conjunction with other information sources [2], such as facial expression [9], and peripheral signals [8, 19].", "startOffset": 131, "endOffset": 134}, {"referenceID": 8, "context": "Recent efforts to reinforce the emotion recognition model include using EEG features in conjunction with other information sources [2], such as facial expression [9], and peripheral signals [8, 19].", "startOffset": 162, "endOffset": 165}, {"referenceID": 7, "context": "Recent efforts to reinforce the emotion recognition model include using EEG features in conjunction with other information sources [2], such as facial expression [9], and peripheral signals [8, 19].", "startOffset": 190, "endOffset": 197}, {"referenceID": 18, "context": "Recent efforts to reinforce the emotion recognition model include using EEG features in conjunction with other information sources [2], such as facial expression [9], and peripheral signals [8, 19].", "startOffset": 190, "endOffset": 197}, {"referenceID": 11, "context": "Based on this concept, the only literature work (to our best knowledge) using EEG signals reported that the fusion of EEG dynamics and musical contents at feature level could improve music-emotion classification results [12].", "startOffset": 220, "endOffset": 224}, {"referenceID": 14, "context": "To represent emotional state systematically, we adopted arousal-valence emotion model [15] that is one of the most commonly used models in the affective computing discipline.", "startOffset": 86, "endOffset": 90}, {"referenceID": 6, "context": "The positions of the selected electrodes were nearby the frontal lobe, which is believed to play a crucial role in emotion regulation [7].", "startOffset": 134, "endOffset": 137}, {"referenceID": 0, "context": "We also employed EEGLAB toolbox [1] to remove eye-movement artifacts from the acquired EEG signals based on the independent component analysis (ICA) approach.", "startOffset": 32, "endOffset": 35}, {"referenceID": 16, "context": "In this work, we applied the fractal dimension (FD) approach to extract features from EEG signals due to its simplicity and excellent performance in previous affective computing studies [17, 18].", "startOffset": 186, "endOffset": 194}, {"referenceID": 17, "context": "In this work, we applied the fractal dimension (FD) approach to extract features from EEG signals due to its simplicity and excellent performance in previous affective computing studies [17, 18].", "startOffset": 186, "endOffset": 194}, {"referenceID": 3, "context": "We applied Higuchi algorithm [4] to derive FD value from each particular window of EEG signals in this study.", "startOffset": 29, "endOffset": 32}, {"referenceID": 16, "context": "Previous studies reported that asymmetries of features extracted from symmetric electrode pairs could be used as additional informative features to classify emotional states [17,18].", "startOffset": 174, "endOffset": 181}, {"referenceID": 17, "context": "Previous studies reported that asymmetries of features extracted from symmetric electrode pairs could be used as additional informative features to classify emotional states [17,18].", "startOffset": 174, "endOffset": 181}, {"referenceID": 10, "context": "1 [11], which is a MATLAB toolbox that offers an integrated set of functions to extract musical features from audio files.", "startOffset": 2, "endOffset": 6}, {"referenceID": 11, "context": "The features were selected by partly following the previous work [12].", "startOffset": 65, "endOffset": 69}, {"referenceID": 0, "context": "For binary classification, let pEEG and p x music \u2208 [0, 1] denote the classifier outputs of EEG and music modality respectively for class x \u2208 {1, 2}.", "startOffset": 52, "endOffset": 58}, {"referenceID": 7, "context": "Despite the spatial continuity of arousal-valence space, most of recent attempts to estimate emotional states from EEG signals simply performed emotion recognition as classification rather than regression [8,12].", "startOffset": 205, "endOffset": 211}, {"referenceID": 11, "context": "Despite the spatial continuity of arousal-valence space, most of recent attempts to estimate emotional states from EEG signals simply performed emotion recognition as classification rather than regression [8,12].", "startOffset": 205, "endOffset": 211}, {"referenceID": 5, "context": "Because of its success in literature [6, 14], support vector machine (SVM) based on Gaussian radial basis kernel function (kernel scale = 3) was used to classify emotional classes.", "startOffset": 37, "endOffset": 44}, {"referenceID": 13, "context": "Because of its success in literature [6, 14], support vector machine (SVM) based on Gaussian radial basis kernel function (kernel scale = 3) was used to classify emotional classes.", "startOffset": 37, "endOffset": 44}, {"referenceID": 0, "context": "Prior to classification, each feature was independently normalized to the range of [0, 1] using the min-max algorithm; we performed the normalization within a subject for subject-dependent classification and across all subjects for subject-independent classification.", "startOffset": 83, "endOffset": 89}, {"referenceID": 12, "context": "In addition to accuracy, we also used Matthews correlation coefficient (MCC) [13], which is a measure to reflect classification performance with consideration of class imbalance.", "startOffset": 77, "endOffset": 81}, {"referenceID": 7, "context": "It was suggested from the literature [8, 9] and the above results that the difference in the contribution of each modality could influence results of decision-level fusion.", "startOffset": 37, "endOffset": 43}, {"referenceID": 8, "context": "It was suggested from the literature [8, 9] and the above results that the difference in the contribution of each modality could influence results of decision-level fusion.", "startOffset": 37, "endOffset": 43}], "year": 2016, "abstractText": "* nattapong@ai.sanken.osaka-u.ac.jp Abstract Emotion estimation in music listening is confronting challenges to capture the emotion variation of listeners. Recent years have witnessed attempts to exploit multimodality fusing information from musical contents and physiological signals captured from listeners to improve the performance of emotion recognition. In this paper, we present a study of fusion of signals of electroencephalogram (EEG), a tool to capture brainwaves at a high-temporal resolution, and musical features at decision level in recognizing the time-varying binary classes of arousal and valence. Our empirical results showed that the fusion could outperform the performance of emotion recognition using only EEG modality that was suffered from inter-subject variability, and this suggested the promise of multimodal fusion in improving the accuracy of music-emotion recognition.", "creator": "LaTeX with hyperref package"}}}