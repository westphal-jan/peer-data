{"id": "1611.10176", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Nov-2016", "title": "Effective Quantization Methods for Recurrent Neural Networks", "abstract": "Reducing bit-widths of weights, activations, and gradients of a Neural Network can shrink its storage size and memory usage, and also allow for faster training and inference by exploiting bitwise operations. However, previous attempts for quantization of RNNs show considerable performance degradation when using low bit-width weights and activations. In this paper, we propose methods to quantize the structure of gates and interlinks in LSTM and GRU cells. In addition, we propose balanced quantization methods for weights to further reduce performance degradation. Experiments on PTB and IMDB datasets confirm effectiveness of our methods as performances of our models match or surpass the previous state-of-the-art of quantized RNN.", "histories": [["v1", "Wed, 30 Nov 2016 14:33:08 GMT  (32kb)", "http://arxiv.org/abs/1611.10176v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.CV", "authors": ["qinyao he", "he wen", "shuchang zhou", "yuxin wu", "cong yao", "xinyu zhou", "yuheng zou"], "accepted": false, "id": "1611.10176"}, "pdf": {"name": "1611.10176.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Shuchang Zhou", "Yuxin Wu", "Cong Yao", "Xinyu Zhou", "Yuheng Zou"], "emails": ["zouyuheng}@megvii.com"], "sections": [{"heading": null, "text": "ar Xiv: 161 1,10 176v 1 [cs.L G] 30 Nov 201 6"}, {"heading": "1 INTRODUCTION", "text": "To mitigate these requirements, many methods have been proposed, both from a hardware and software perspective (Farabet et al., 2011; Pham et al., 2012; Chen et al.). To mitigate these requirements, many methods are proposed, both from a hardware and software perspective (Farabet et al., 2012; Pham et al.)."}, {"heading": "2 QUANTIZATION METHODS", "text": "In this section, we outline several quantization methods. W.l.above, we assume that the input for quantization is a matrix X, unless otherwise specified. If all inputs of X are made in narrow intervals [0, 1], we define the k-bit uniform quantization Qk as follows:.Qk (X) = 12k \u2212 1 (2k \u2212 1) X + 12, 0 \u2264 xij \u2264 1-i, j. (2) However, derivatives of this quantization function are almost always zero. We apply the method of the Straight-through estimator (STE) (Hinton et al., 2012b; Bengio et al., 2013) to work around this problem."}, {"heading": "2.1 DETERMINISTIC QUANTIZATION", "text": "If the input in X is not restricted at the closed interval [0, 1], an affine transformation must be applied before using the function Qk. A simple transformation can be performed with a minimum and maximum of X to obtain X, the standardized version of X: X = X \u2212 \u03b2\u03b1\u03b1 = max (X) \u2212 min (X) \u03b2 = min (X). After quantification, we can apply a reverse affine transformation to approximate the original values. Overall, the quantified result is: Qdetk (X) = \u03b1Qk (X) + \u03b2 \u2248 X 1https: / / github.com / hqythu / bit-rnn"}, {"heading": "2.2 BALANCED DETERMINISTIC QUANTIZATION", "text": "When quantifying values, it may be desirable that the quantified values have balanced distributions (Q = X) in order to take full advantage of the available parameter space (Q = 0). Normally, this is not possible because the distribution of the input values is already fixed. In particular, we first introduce a different standardization that affects the distribution of the quantified values and define a balanced quantization method Q (2). We show that we can define a more even distribution of the quantified values as follows: X = Clip (X = Clip)."}, {"heading": "2.3 QUANTIZATION OF WEIGHTS", "text": "Weights in neural networks are sometimes known to have a bell-like distribution around zero, similar to the normal distribution. Therefore, we can assume that X has a symmetrical distribution around 0, and apply the above equation for a balanced quantization asscale = mean (abs (X)) \u0445 2.5Qbeam (X) = Qk (Xscale) \u0445 X.To include the quantization in the calculation curve of a neural network, we apply STE to the entire expression and not only to Qk itself. Forward: q \u2190 Qbeam (p) Backward: \u2202 c \u2202 p \u2202 c \u2202 q.The special thing about the balanced quantization method Qbeam is that it generally distorts the extreme values due to the cutting in formula 4, which generally contribute more to the calculated sums of internal products."}, {"heading": "2.4 QUANTIZATION OF ACTIVATIONS", "text": "The quantization of the activation follows the method in Zhou et al. (2016), provided that the output of the previous layer has gone through a limited activation function h, and we will apply the quantization directly to it. In fact, we note that adding a scaling term containing a mean or maximum value to the activations may affect the prediction accuracy. There is a design selection about what range of the quantified value should be. Another option is the symmetrical distribution by 0. In this selection, the inputs are limited to [\u2212 0.5, 0.5] by the activation function and then shifted 0.5 to the right before being fed into Qk and then shifted backward.Xq = Qk (X + 0.5) \u2212 0.5 Another option is the use of a value range of [0, 1] which is closer to the value range of the ReLU activation function. In this selection, we can apply Qk directly. Of course, with the frequently used syetrical tannic, it appears in R1 with \u2212 1 activation."}, {"heading": "3 QUANTIZATION OF RECURRENT NEURAL NETWORK", "text": "In this section we explain in detail our design considerations for the quantification of recurrent neural networks. Unlike normal neural networks, recursive neural networks, in particular Long Short Term Memory (Hochreiter & Schmidhuber, 1997) and Gated Recurrent Unit (Chung et al., 2014), have a subtle and delicate structure that makes their quantization more complex and requires more careful consideration. Nevertheless, the most important algorithm is the same as Algorithm 1 in Zhou et al. (2016)."}, {"heading": "3.1 DROPOUT", "text": "Since fully connected layers have a large number of parameters, they are known to be prone to overfit (Srivastava et al., 2014). There are several FC-like structures in an RNN, such as the input, output and transition matrices in RNN cells (such as GRU and LSTM) and the last FC layer for Softmax classification. Failure technique, where some of the characteristics randomly drop to 0 during training, also proves to be an effective method to alleviate overfit in RNN (Zaremba et al., 2014). Since the failed activations are zero, it is necessary to have zero values in the quantified range. To achieve symmetrical quantification in the range [\u2212 0.5, 0.5] 0 does not exist in the range Qk (X + 0.5) \u2212 0.5. Therefore, we use [0, 1] as the range of quantified values when a dropout is required."}, {"heading": "3.2 EMBEDDING LAYER", "text": "For tasks related to processing natural language, the input words represented by ID's are embedded in a low-dimensional space before they are embedded in RNNs. The word embedding matrix is in R | V | \u00b7 N, where | V | is the size of the vocabulary and N is the length of the embedded vectors. Quantifying the weights in embedding layers turns out to be different from quantifying the weights in FC layers. In fact, the weights of the embedding layers actually behave like activations: a certain line is selected and fed to the next layer, so the quantization method should be the same as that of activations and not that of weights. Similarly, as suspenders can be applied to the results of embedding layers, it is necessary to bind the values to [0, 1] when embedding matrices."}, {"heading": "3.3 QUANTIZATION OF GRU", "text": "The basic structure of the GRU cell can be described as follows: zt = \u03c3 (Wz \u00b7 1, xt] rt = \u03c3 (Wr \u2212 1, xt]) h = tanh (W \u00b7 [rt \u0445 ht \u2212 1, xt] ht = (1 \u2212 zt)) ht = (1 \u2212 1 + zt \u0432 (1 \u2212 1, xt] h, where \u03c3 stands for the sigmoid function. Recall that we benefit from the speed advantage of bit convolution, we must make the two matrix inputs for multiplication in low bit form so that the dot product can be calculated by bit-by-bit operation. For pure forward neural networks, since the convolutions take up the most computing time, we can obtain decent acceleration by quantifying the inputs of convolutions and their weights."}, {"heading": "3.4 QUANTIZATION OF LSTM", "text": "The structure of LSTM can be described as follows: ft = \u03c3 (Wf \u00b7 [ht \u2212 1, xt] + bf) it = \u03c3 (Wi \u00b7 [ht \u2212 1, xt] + bi) C \u0442t = tanh (WC \u00b7 [ht \u2212 1, xt] + bi) Ct = ft \u0445 Ct \u2212 1 + it \u043a C \u0442t ot = \u03c3 (Wo \u00b7 [ht \u2212 1, xt] + bo) ht = ot \u0445 tanh (Ct) Unlike GRU, Ct cannot simply be quantified because the value is infinite by not using an activation function such as tanh and the sigmoid function. This difficulty stems from the structural design and cannot be alleviated without introducing additional possibilities for clipping value ranges. However, it can be stated that the calculations with Ct are all elemental multiplications and additions that can take much less time than the calculation of matrix products. For this reason, we leave Ct in the flow form \u2212 maxed."}, {"heading": "4 EXPERIMENT RESULTS", "text": "We evaluate the quantified RNN models based on two tasks: language modeling and sentence classification."}, {"heading": "4.1 EXPERIMENTS ON PENN TREEBANK DATASET", "text": "For speech modeling, we use Penn Treebank dataset (Taylor et al., 2003), which contains 10K unique words. We download the data from Tomas Mikolov's website2. To be fair, in the following experiments we all use a hidden layer of 300 hidden units, which is the same setting as Hubara et al. (2016a). At the entrance to the network, a word embedding layer is used, whose weights are re-formed from scratch. Performance is measured in perplexity per word (PPW). In experiments, we find orders of magnitude of values in dense matrices or fully connected layers explode when using small bit width and lead to overmatch and divergence. This can be achieved by adding tanh to restrict the ranges of values or adding weight deviations for regulation. Our result is in accordance with (Hubara et al al, 2016a), where they can achieve weight-activations with less stress and 4-loads."}, {"heading": "4.2 EXPERIMENTS ON PENN IMDB DATASETS", "text": "We are conducting further experiments on sentence classification with IMDB data sets (Maas et al., 2011), filling or cutting each sentence to 500 words, text embedding vectors of length 512 and a single recurring layer of 512 hidden neurons. All models are trained with ADAM (Kingma & Ba, 2014) Learning Rules with learning rate 10 \u2212 3. Since IMDB is a relatively simple data set, we do not detect any performance decrease even when quantified to 1-bit weights and 2-bit activations."}, {"heading": "4.3 EFFECTS OF BALANCED DISTRIBUTION", "text": "All of the above experiments show that balanced quantization yields better results than unbalanced counterparts, especially in quantification with 2-bit weights. However, for 4-bit weights there is no clear gap between mean scaling and maximum scaling (i.e. balanced and unbalanced quantization), indicating that more effective methods of quantification with 4-bit must be discovered."}, {"heading": "5 CONCLUSION AND FUTURE WORK", "text": "The method we propose for the balanced quantization of weights can bring about a balanced distribution of the quantified weight value in order to maximize the utilization of the parameter space. It can also be applied to the quantification of CNN. As a future work, first, the method for quantifying the balanced weight if the bit width is more than 2 is still to be found. Second, we have observed some difficulties in quantifying the cell paths in LSTM that produce unlimited values. One way to address this problem is to introduce new scaling schemes to quantify the activations that can handle unlimited values. Finally, since we have observed that GRU and LSTM have different properties in quantification, it remains to be shown whether there are more efficient recurring structures specifically designed to facilitate quantification."}], "references": [{"title": "Tensorflow: Large-scale machine learning on heterogeneous systems", "author": ["Mart\u0131n Abadi", "Ashish Agarwal", "Paul Barham", "Eugene Brevdo", "Zhifeng Chen", "Craig Citro", "Greg S Corrado", "Andy Davis", "Jeffrey Dean", "Matthieu Devin"], "venue": null, "citeRegEx": "Abadi et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Abadi et al\\.", "year": 2015}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1409.0473,", "citeRegEx": "Bahdanau et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Estimating or propagating gradients through stochastic neurons for conditional computation", "author": ["Yoshua Bengio", "Nicholas L\u00e9onard", "Aaron Courville"], "venue": "arXiv preprint arXiv:1308.3432,", "citeRegEx": "Bengio et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2013}, {"title": "Diannao: A small-footprint high-throughput accelerator for ubiquitous machine-learning", "author": ["Tianshi Chen", "Zidong Du", "Ninghui Sun", "Jia Wang", "Chengyong Wu", "Yunji Chen", "Olivier Temam"], "venue": "In ACM Sigplan Notices,", "citeRegEx": "Chen et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2014}, {"title": "Compressing neural networks with the hashing trick", "author": ["Wenlin Chen", "James T Wilson", "Stephen Tyree", "Kilian Q Weinberger", "Yixin Chen"], "venue": "arXiv preprint arXiv:1504.04788,", "citeRegEx": "Chen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2015}, {"title": "Dadiannao: A machine-learning supercomputer", "author": ["Yunji Chen", "Tao Luo", "Shaoli Liu", "Shijin Zhang", "Liqiang He", "Jia Wang", "Ling Li", "Tianshi Chen", "Zhiwei Xu", "Ninghui Sun"], "venue": "In Proceedings of the 47th Annual IEEE/ACM International Symposium on Microarchitecture,", "citeRegEx": "Chen et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2014}, {"title": "An exploration of parameter redundancy in deep networks with circulant projections", "author": ["Yu Cheng", "Felix X Yu", "Rogerio S Feris", "Sanjiv Kumar", "Alok Choudhary", "Shi-Fu Chang"], "venue": "In Proceedings of the IEEE International Conference on Computer Vision, pp. 2857\u20132865,", "citeRegEx": "Cheng et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Cheng et al\\.", "year": 2015}, {"title": "Empirical evaluation of gated recurrent neural networks on sequence modeling", "author": ["Junyoung Chung", "Caglar Gulcehre", "KyungHyun Cho", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1412.3555,", "citeRegEx": "Chung et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chung et al\\.", "year": 2014}, {"title": "Large-scale fpga-based convolutional networks. Scaling up Machine Learning: Parallel and Distributed Approaches", "author": ["Cl\u00e9ment Farabet", "Yann LeCun", "Koray Kavukcuoglu", "Eugenio Culurciello", "Berin Martini", "Polina Akselrod", "Selcuk Talay"], "venue": null, "citeRegEx": "Farabet et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Farabet et al\\.", "year": 2011}, {"title": "Compressing deep convolutional networks using vector quantization", "author": ["Yunchao Gong", "Liu Liu", "Ming Yang", "Lubomir Bourdev"], "venue": "arXiv preprint arXiv:1412.6115,", "citeRegEx": "Gong et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Gong et al\\.", "year": 2014}, {"title": "Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding", "author": ["Song Han", "Huizi Mao", "William J Dally"], "venue": "arXiv preprint arXiv:1510.00149,", "citeRegEx": "Han et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Han et al\\.", "year": 2015}, {"title": "Learning both weights and connections for efficient neural network", "author": ["Song Han", "Jeff Pool", "John Tran", "William Dally"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Han et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Han et al\\.", "year": 2015}, {"title": "Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups", "author": ["Geoffrey Hinton", "Li Deng", "Dong Yu", "George E Dahl", "Abdel-rahman Mohamed", "Navdeep Jaitly", "Andrew Senior", "Vincent Vanhoucke", "Patrick Nguyen", "Tara N Sainath"], "venue": "Signal Processing Magazine, IEEE,", "citeRegEx": "Hinton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2012}, {"title": "Neural networks for machine learning", "author": ["Geoffrey Hinton", "Nitsh Srivastava", "Kevin Swersky"], "venue": "Coursera, video lectures,", "citeRegEx": "Hinton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2012}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "Hochreiter and Schmidhuber.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Quantized neural networks: Training neural networks with low precision weights and activations", "author": ["Itay Hubara", "Matthieu Courbariaux", "Daniel Soudry", "Ran El-Yaniv", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1609.07061,", "citeRegEx": "Hubara et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Hubara et al\\.", "year": 2016}, {"title": "Speeding up convolutional neural networks with low rank expansions", "author": ["Max Jaderberg", "Andrea Vedaldi", "Andrew Zisserman"], "venue": "arXiv preprint arXiv:1405.3866,", "citeRegEx": "Jaderberg et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Jaderberg et al\\.", "year": 2014}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "Kingma and Ba.,? \\Q2014\\E", "shortCiteRegEx": "Kingma and Ba.", "year": 2014}, {"title": "Imagenet classification with deep convolutional neural networks. In Advances in neural information processing", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E Hinton"], "venue": null, "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Learning word vectors for sentiment analysis", "author": ["Andrew L. Maas", "Raymond E. Daly", "Peter T. Pham", "Dan Huang", "Andrew Y. Ng", "Christopher Potts"], "venue": "In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,", "citeRegEx": "Maas et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Maas et al\\.", "year": 2011}, {"title": "Another comment on ocinneide", "author": ["Colin Mallows"], "venue": "The American Statistician,", "citeRegEx": "Mallows.,? \\Q1991\\E", "shortCiteRegEx": "Mallows.", "year": 1991}, {"title": "Recurrent neural networks with limited numerical precision", "author": ["Joachim Ott", "Zhouhan Lin", "Ying Zhang", "Shih-Chii Liu", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1608.06902,", "citeRegEx": "Ott et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Ott et al\\.", "year": 2016}, {"title": "Neuflow: Dataflow vision processing system-on-a-chip", "author": ["Phi-Hung Pham", "Darko Jelaca", "Clement Farabet", "Berin Martini", "Yann LeCun", "Eugenio Culurciello"], "venue": "In Circuits and Systems (MWSCAS),", "citeRegEx": "Pham et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Pham et al\\.", "year": 2012}, {"title": "Xnor-net: Imagenet classification using binary convolutional neural networks", "author": ["Mohammad Rastegari", "Vicente Ordonez", "Joseph Redmon", "Ali Farhadi"], "venue": "arXiv preprint arXiv:1603.05279,", "citeRegEx": "Rastegari et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Rastegari et al\\.", "year": 2016}, {"title": "Mastering the game of go with deep neural networks and tree", "author": ["David Silver", "Aja Huang", "Chris J Maddison", "Arthur Guez", "Laurent Sifre", "George Van Den Driessche", "Julian Schrittwieser", "Ioannis Antonoglou", "Veda Panneershelvam", "Marc Lanctot"], "venue": "search. Nature,", "citeRegEx": "Silver et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Silver et al\\.", "year": 2016}, {"title": "Dropout: a simple way to prevent neural networks from overfitting", "author": ["Nitish Srivastava", "Geoffrey E Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Srivastava et al\\.,? \\Q1929\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 1929}, {"title": "Going deeper with convolutions", "author": ["Christian Szegedy", "Wei Liu", "Yangqing Jia", "Pierre Sermanet", "Scott Reed", "Dragomir Anguelov", "Dumitru Erhan", "Vincent Vanhoucke", "Andrew Rabinovich"], "venue": "arXiv preprint arXiv:1409.4842,", "citeRegEx": "Szegedy et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Szegedy et al\\.", "year": 2014}, {"title": "The penn treebank: an overview", "author": ["Ann Taylor", "Mitchell Marcus", "Beatrice Santorini"], "venue": "In Treebanks,", "citeRegEx": "Taylor et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Taylor et al\\.", "year": 2003}, {"title": "Recurrent neural network regularization", "author": ["Wojciech Zaremba", "Ilya Sutskever", "Oriol Vinyals"], "venue": "arXiv preprint arXiv:1409.2329,", "citeRegEx": "Zaremba et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zaremba et al\\.", "year": 2014}, {"title": "Accelerating very deep convolutional networks for classification and detection", "author": ["Xiangyu Zhang", "Jianhua Zou", "Kaiming He", "Jian Sun"], "venue": "IEEE transactions on pattern analysis and machine intelligence,", "citeRegEx": "Zhang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2015}, {"title": "Dorefa-net: Training low bitwidth convolutional neural networks with low bitwidth gradients", "author": ["Shuchang Zhou", "Yuxin Wu", "Zekun Ni", "Xinyu Zhou", "He Wen", "Yuheng Zou"], "venue": "arXiv preprint arXiv:1606.06160,", "citeRegEx": "Zhou et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Zhou et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 18, "context": "Deep Neural Networks have become important tools for modeling nonlinear functions in applications like computer vision (Krizhevsky et al., 2012), speech recognition (Hinton et al.", "startOffset": 119, "endOffset": 144}, {"referenceID": 1, "context": ", 2012a), natural language processing (Bahdanau et al., 2014), and computer games (Silver et al.", "startOffset": 38, "endOffset": 61}, {"referenceID": 24, "context": ", 2014), and computer games (Silver et al., 2016).", "startOffset": 28, "endOffset": 49}, {"referenceID": 18, "context": "However, inference and training of a DNN may involve up to billions of operations for inputs likes images (Krizhevsky et al., 2012; Szegedy et al., 2014).", "startOffset": 106, "endOffset": 153}, {"referenceID": 26, "context": "However, inference and training of a DNN may involve up to billions of operations for inputs likes images (Krizhevsky et al., 2012; Szegedy et al., 2014).", "startOffset": 106, "endOffset": 153}, {"referenceID": 6, "context": ", 2015b;a), circulant matrix (Cheng et al., 2015), low rank (Jaderberg et al.", "startOffset": 29, "endOffset": 49}, {"referenceID": 16, "context": ", 2015), low rank (Jaderberg et al., 2014; Zhang et al., 2015), vector quantization (Gong et al.", "startOffset": 18, "endOffset": 62}, {"referenceID": 29, "context": ", 2015), low rank (Jaderberg et al., 2014; Zhang et al., 2015), vector quantization (Gong et al.", "startOffset": 18, "endOffset": 62}, {"referenceID": 9, "context": ", 2015), vector quantization (Gong et al., 2014), and hash trick (Chen et al.", "startOffset": 29, "endOffset": 48}, {"referenceID": 4, "context": ", 2014), and hash trick (Chen et al., 2015) etc.", "startOffset": 24, "endOffset": 43}, {"referenceID": 23, "context": "Another line of research tries to reduce bit-width of weights and activations of a DNN by quantization to low bit-width numbers (Rastegari et al., 2016; Hubara et al., 2016b; Zhou et al., 2016; Hubara et al., 2016a).", "startOffset": 128, "endOffset": 215}, {"referenceID": 30, "context": "Another line of research tries to reduce bit-width of weights and activations of a DNN by quantization to low bit-width numbers (Rastegari et al., 2016; Hubara et al., 2016b; Zhou et al., 2016; Hubara et al., 2016a).", "startOffset": 128, "endOffset": 215}, {"referenceID": 21, "context": "(Ott et al., 2016) claims that the weight binarization method does not work with RNNs, and introduces weight ternarization and leaves activations as floating point numbers.", "startOffset": 0, "endOffset": 18}, {"referenceID": 2, "context": "We adopt the \u201cstraight-through estimator\u201d (STE) method (Hinton et al., 2012b; Bengio et al., 2013) to circumvent this problem.", "startOffset": 55, "endOffset": 98}, {"referenceID": 20, "context": "We note that when a distribution has bounded variance \u03c3, the mean \u03bc approximates the median m as there is an inequality bounding the difference(Mallows, 1991): |\u03bc\u2212m| \u2264 \u03c3.", "startOffset": 143, "endOffset": 158}, {"referenceID": 23, "context": "It should be noted that for 1-bit quantization (binarization), the scaling factor should be 2 mean(|X|), which can be proved to be optimal in the sense of reconstruction error measured by Frobenius norm, as in (Rastegari et al., 2016).", "startOffset": 210, "endOffset": 234}, {"referenceID": 30, "context": "Quantization of activation follows the method in Zhou et al. (2016), assuming output of the previous layer has passed through a bounded activation function h, and we will apply quantization directly", "startOffset": 49, "endOffset": 68}, {"referenceID": 7, "context": "Different from plain feed forward neural network, recurrent neural networks, especially Long Short Term Memory (Hochreiter & Schmidhuber, 1997) and Gated Recurrent Unit (Chung et al., 2014), have subtle and delicately designed structure, which makes their quantization more complex and need more careful considerations.", "startOffset": 169, "endOffset": 189}, {"referenceID": 7, "context": "Different from plain feed forward neural network, recurrent neural networks, especially Long Short Term Memory (Hochreiter & Schmidhuber, 1997) and Gated Recurrent Unit (Chung et al., 2014), have subtle and delicately designed structure, which makes their quantization more complex and need more careful considerations. Nevertheless, the major algorithm is the same as Algorithm 1 in Zhou et al. (2016).", "startOffset": 170, "endOffset": 403}, {"referenceID": 28, "context": "The dropout technique, which randomly dropping a portion of features to 0 at training time, turns out be also an effective way of alleviating overfitting in RNN (Zaremba et al., 2014).", "startOffset": 161, "endOffset": 183}, {"referenceID": 27, "context": "For language modeling we use Penn Treebank dataset (Taylor et al., 2003), which contains 10K unique words.", "startOffset": 51, "endOffset": 72}, {"referenceID": 15, "context": "For fair comparison, in the following experiments, our model all use one hidden layer with 300 hidden units, which is the same setting as Hubara et al. (2016a). A word embedding layer is used at the input side of the network whose weights are trained from scratch.", "startOffset": 138, "endOffset": 160}, {"referenceID": 19, "context": "We do further experiments on sentence classification using IMDB datasets (Maas et al., 2011).", "startOffset": 73, "endOffset": 92}], "year": 2016, "abstractText": "Reducing bit-widths of weights, activations, and gradients of a Neural Network can shrink its storage size and memory usage, and also allow for faster training and inference by exploiting bitwise operations. However, previous attempts for quantization of RNNs show considerable performance degradation when using low bit-width weights and activations. In this paper, we propose methods to quantize the structure of gates and interlinks in LSTM and GRU cells. In addition, we propose balanced quantization methods for weights to further reduce performance degradation. Experiments on PTB and IMDB datasets confirm effectiveness of our methods as performances of our models match or surpass the previous stateof-the-art of quantized RNN.", "creator": "LaTeX with hyperref package"}}}