{"id": "1612.00132", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Dec-2016", "title": "CDVAE: Co-embedding Deep Variational Auto Encoder for Conditional Variational Generation", "abstract": "Problems such as predicting an optical flow field (Y) for an image (X) are ambiguous: many very distinct solutions are good. Representing this ambiguity requires building a conditional model P(Y|X) of the prediction, conditioned on the image. It is hard because training data usually does not contain many different flow fields for the same image. As a result, we need different images to share data to produce good models. We demonstrate an improved method for building conditional models, the Co-Embedding Deep Variational Auto Encoder. Our CDVAE exploits multiple encoding and decoding layers for both X and Y. These are tied during training to produce a model of the joint distribution P(X, Y), which provides the necessary smoothing. Our tying procedure is designed to yield a conditional model easy at test time. We demonstrate our model on three example tasks using real data: image saturation adjustment, image relighting, and motion prediction. We describe quantitative evaluation metrics to evaluate ambiguous generation results. Our results quantitatively and qualitatively advance the state of the art.", "histories": [["v1", "Thu, 1 Dec 2016 03:40:42 GMT  (8992kb,D)", "https://arxiv.org/abs/1612.00132v1", null], ["v2", "Tue, 28 Mar 2017 03:21:34 GMT  (7823kb,D)", "http://arxiv.org/abs/1612.00132v2", null]], "reviews": [], "SUBJECTS": "cs.CV cs.AI cs.GR", "authors": ["jiajun lu", "aditya deshpande", "david forsyth"], "accepted": false, "id": "1612.00132"}, "pdf": {"name": "1612.00132.pdf", "metadata": {"source": "CRF", "title": "CDVAE: Co-embedding Deep Variational Auto Encoder for Conditional Variational Generation", "authors": ["Jiajun Lu", "Aditya Deshpande", "David Forsyth"], "emails": ["daf}@illinois.edu"], "sections": [{"heading": "1. Introduction", "text": "In fact, most of them will be able to play by the rules that they have shown in recent years, and they will be able to play by the rules that they have shown."}, {"heading": "2. Related Work", "text": "Generating a spatial field with a complex spatial structure from an image is an established problem. Important examples of applications where prediction is naturally ambiguous include coloring [4, 9, 29], style transfer [5], prediction of temporal transformations [30], prediction of motion fields [19, 25, 26], and prediction of future frames [24]. This is usually treated as a regression problem; current technology uses deep networks to learn properties. However, predicting the expected value of a conditional distribution by regression works poorly, as the expected value of a multimodal distribution may have a low probability. While one could mitigate the distribution (e.g. [29]), the ideal is to obtain different solutions. One strategy is to draw examples from a generative model. Generative models of images that show current problems of dimension can be addressed with latent variable models that vary (the models E)."}, {"heading": "3. Method", "text": "Our CDVAE consists of two deeply varying auto encoders (DVAE) [16] and a mixture density network (MDN) [2]. An overview of the CDVAE model is shown in Figure 3.Our architecture is we use two DVAEs to embed the conditioning image xc (or X from Section 1) and the generated image xg (or Y from Section 1) in two low-dimensional latent variables (or code spaces) zc and zg. The generated image corresponds to the output spatial field viz. saturation or shading etc. and the conditioning image is the input image viz intensity or albedo etc. Next, we regulate the latent variables zc with the embedding guide (or metric constraints) so that the similarity in the input space is maintained (Section 3.2).Since our problem is ambiguous, the conditional distribution between zg and zc is."}, {"heading": "3.1. CDVAE Loss", "text": "In CDVAE, we use two multi-layer variant auto encoders (DVAE), one for xc and one for xg. In addition, we have an MDN that models the relationship between the embedding zc and zg. LCDV AE, which corresponds to the CDVAE community model P (xc, xg), is a combination of the two DVAE models and the conditional probability model of MDN. If we assume in equations 1 and 25 that it is possible to encode xc without seeing xg, then we can use the auxiliary sample distributions Q = P (zc | xc) P (zg | xg) for our CDVAE model. If P (zg, zc) P (zg) P (zc) in equation 25, we can extract the negative terms of the equation (DVVAc) and the equation distributions (DVAc)."}, {"heading": "3.2. Embedding Guidance: Preventing code collapse", "text": "Vanilla DVAEs have difficulty learning a code space that encodes the complex spatial structure of the output, and the code space that a DVAE learns seems to be underdetermined, especially for large and complex datasets. This is a common error mode of VAEs. For our conditional models, it is desirable for codes for \"similar\" inputs to be nearby, and codes for \"very different\" inputs to be further apart. This discourages the method of grouping very different inputs. It also prevents similar images from having different codes and thus avoids faulty smoothing of the model (see Figure 2). We perform the codes (on multiple levels) so that they are similar to a precalculated embedding. Our precalculated embedding is such that it maintains the similarity observed in the input domain."}, {"heading": "3.3. Post Processing", "text": "Current deep generative models process only small images, in our case 32 \u00d7 32, and they produce results without high spatial frequencies. We post process-generated images for viewing at high resolution (not used in any quantitative evaluation). Our post-processing upsamples results in a resolution of 512 \u00d7 512, with more detail. We aggressively upsample the generated fields using the approach in [10], which preserves edges during upsampling. Specifically, the method represents a high-resolution field around a weight and an orientation value at each sample point; these parameters control a basic function that is placed at each sample, and the field at each test point is a sum of the values of the basic functions at that point. The sum of basic functions includes only these basic functions in the same image segment. Write interp (w, \u03b8; S (I)))) for the high-resolution field generated by interpolating a weight vector w and a vector of orientation functions."}, {"heading": "4. Applications", "text": "We apply our methods to two different ambiguous tasks, each of which allows both quantitative and qualitative evaluations. Photo-Relighting (or Reshading): In this application, we forecast new shading fields for images that are consistent with a relic version of the scene. We break the image down into albedo (the conditioning image xc) and shading (the generated image xg). In real images, shading by albedo is quite limited, because the Albedo image contains semantic information such as scene categories, object layout and potential light sources. A scene can be illuminated in many ways, so illumination is a multimodal problem. We use the MS-COCO dataset to illuminate (Section 5.1). Image Repair: Here, we forecast new saturation fields for color images, i.e. we modify the saturation for some or all objects in the image. We use the color field inserted into the RGB and the color field inserted in the HSV, and use the color field inserted into the advertising space."}, {"heading": "5. Results", "text": "To evaluate the effectiveness of our method, we compare with current strong methods (Section 5.3) and evaluate various variants of our method. We perform quantitative and qualitative comparisons to photo illumination and image saturation applications. Quantitative results (Section 5.2, Figure 5) are calculated from network output, without any post-processing. Images shown for qualitative evaluation of saturation and resaturation (Section 5.4, Figures 1, 6, 8 and 7) are post-processed using the steps described in Section 3.3. We scale down all input images to 32 x 32 dimensions, and all neural network operations prior to post-processing are performed at this image size. Once our CDVAE model has generated samples, our post-processing takes them to a resolution of 512 x 512."}, {"heading": "5.1. Datasets", "text": "MS-COCO: We use the MS-COCO dataset for our two tasks, photo-retrieval and image-retrieval. It is a wild dataset (as opposed to the structured face data commonly used by generative models) and has a complex spatial structure. Typically, such data is a challenge for UAE-based methods. We use train2014 (80K images) for model training and sample 6400 images from val2014 (40K images) for testing. For photo-retrieval, the intrinsic image-decomposition method of [1] is used to obtain albedo and shade images. For image-retrieval, we transform the image from the RGB space to the HSV space."}, {"heading": "5.2. Quantitative Metrics and Evaluation", "text": "We take 100 samples for each conditional model and calculate the minimum per pixel error to the ground truth fields. A better model will produce smaller values because it will produce some samples closer to the ground truth.Variance: A key objective of our work is to generate multiple solutions. However, no current evaluation regime can say whether a pool of different solutions is correct. We opt for the strictly weaker proxy of measuring variance in the pool, on the reasonable assumption that different predictions for our problems must have high variance.Procedures that produce predictions with small deviations are clearly unacceptable. It is clearly not enough to produce only deviations - we want the pool to contain appealing different predictions. To assess this, we rely on qualitative assessments (Figures 6, 7, 8).The supplementary materials contain many other qualitative results. To calculate deviations, we should obtain a minimum of deviations (32) to obtain deviations (4) as deviations."}, {"heading": "5.3. Baseline Methods", "text": "The Next Neighbor (NN): We perform the search for the closest neighbor (NN) in xc space and return the corresponding xg as multiple output. Gaussian smoothing is applied to the returned xg to remove inconsistent radio frequency signals. Since our training data does not have explicit oneto-many correspondences, NN is a naturally strong baseline. It is a non-parametric method to model the multimodal distribution by borrowing spatial output fields (we also smooth these) from nearby input images. The conditional variable autoencoder: We implement a CVAE similar to [25]. We cannot use it because its architecture is specific for predicting coarse motion paths. Our decoder is modeled on the DCGAN architecture by Radford et al. [14] With 5 devolution layers, we use it in conformity with the CD64 and it is in conformity with the measurement VE."}, {"heading": "5.4. Qualitative Evaluation", "text": "Figure 6 shows our qualitative comparisons with other methods for the two tasks. In both examples, we create plausible and diverse scenes based on \"similar\" images and a saturated image. Image Restoration: Further results for image restoration with our CDVAE (12 Gaussian nuclei) and embedding guidelines are shown in Figure 7. For each input image, we take four samples for saturation fields from our conditional model. The varied results of saturation matching show that our model effectively learns multimodal saturation distributions. Our automatic saturation adjustment produces appealing photos. We demonstrate artistic stylization / editing using our automated method.Photo Relighting: In Figure 8, we show additional photo search results from our method, in which we obtain important light results from light source E."}, {"heading": "6. Discussion", "text": "Our CDVAE delivers good results in terms of quality and quantity, but there are still some limitations, some of which are due to the limitations of generative VAE models. Variational auto-encoders and their variants, for example, smooth out their results, resulting in a loss of spatial structure. Our latent multi-layered Gaussian variable Original Sample 1 Example 2 Example 3 Example 4 can capture more complex structures, but we overlook the finer details compared to the truth. Second, like all current generative models, our model is applied to low-resolution images, which means that much of the structural and semantic information that is important for good results is lost. Finally, our model has no spatial hierarchy. Rough to fine hierarchy on the generative and conditional side would probably enable us to achieve results with more detail."}, {"heading": "7. Conclusion", "text": "Our common model provides a conditional model that can be easily sampled, allowing us to train with scattered data based on a common model. We have demonstrated our approach to producing photorealistic reproduced and saturated images. We propose a metric regularization of code space that prevents code breakdown, and in the future this regularization can be studied in the context of other generative models."}, {"heading": "8. Appendix", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "8.1. Architecture Details", "text": "Our CDVAE has a different architecture from a CVAE. The detailed architecture of our CDVAE can be found in Table 8.1. Write lin for the input layer and lout for the output layer, fc stands for a completely connected layer, mean is the mean of the Gaussian distribution of the code space, var is the variance of the Gaussian distribution of the code space. Sample is the process of sampling the Gaussian distribution with mean and var. lout is sampled by mean4 and var. lout is sampled by mean4 and var. we use L2 regulation (or weight decay) for the parameters of the MDN model. The learning rate is set to 5 x 10 \u2212 5 and we use the ADAM optimizer. We first set the reconstruction costs high, LPP embedding costs high and MDN costs low. We keep this setting and tensile for 100 epochs. For the next 200 epochs we gradually lower the embedding costs and finally increase the MDN costs."}, {"heading": "8.2. DVAE", "text": "The difference between DVAE [16] and VAE [7] consists of several layers of Gaussian latent variables. To generate a sample from the model, we start from the topmost layer (L) by drawing from a Gaussian distribution to zc, L.P (zc, L) = N (zc, L | 0, I) (8) The mean and variance of the Gaussian distributions on each bottom layer is formed by a nonlinear transformation of the sample from the topmost lay. \u00b5c, i = f\u00b5c, i (zc, i + 1) (9) \u03c32c, i (zc, i + 1) (10), where f represents the multilayer perceptrons."}, {"heading": "8.2.1 Inference", "text": "DVAE with multiple layers of dependent stochastic variables is difficult to learn, which limits the improvements achieved with these highly expressive models. LVAE [20] recursively corrects the generative distribution by a data-dependent approximate probability in a process similar to the current ladder network. It uses a more deeply distributed hierarchy of latent variables and captures more complex structures. We follow this work for xc, write \u00b5c, p, i and \u03c32c, p, i for the mean and variance at the i-level of the generative side, write \u00b5c, q, i and \u03c32c, q, q, i for the mean and the variance at the i-level of the side of the inference. This changes the notation in the previous part on the generative side.Pp (zc) = Pp (zc) = Pp (zc) = Pp (zc) = Pp (zc, L \u2212 1 \u00b2 i i, i \u00b2 i, p \u00b2 i, p \u00b2 i), p \u00b2 zp = 1 Pp (zc), c c c = c, c (c, c, c \u00b2, c, zc \u00b2 i, i, zi, zi \u00b2 i, p \u00b2 i, zp, zc, zc, z2, c, zc, c \u00b2 i, z2, c, z2, c, c \u00b2 i, zi, zp \u00b2 i."}, {"heading": "8.3. Joint Models", "text": "First, we prove that if the common probability is independent, we get two separate DVAEs; then we prove the derivatives for a common model with a non-independent common probability."}, {"heading": "8.3.1 Separate DVAEs", "text": "Section 3 of the paper shows that the joint probability P (xc, xg) in the CDVAE model isP (xc, xg) = VP (xc | zc) P (xg | zg) P (zg, zc) dzgdzc (25) If zc and zg are independent, P (zg, zc) = P (zg) P (zc), and Equation 25 can be transformed P (xc, xg) = VP (xc | zc) P (xg | zg) P (xg | zg) P (zg) P (zg) P (zg) dzc (P (xg | zc) P (zg) P (zg) P (zg) P (zg) P (zg) P (zg) (zg) P (zg) for model (xg) (Axc) and DVc (xP) for P (xc)."}, {"heading": "8.3.2 Joint Model Derivation", "text": "Section 2 of the paper indicates that we have objective function for VAE asVAE (\u03b8) = QQ = | QQ (QQ) = \u2211 data [EQ logP (x | z) \u2212 D (Q | P (z)))] (27), where Q = P (z | x) \u2212 D (Q | | P (zc, zg)]] (28) can be written, where Q = P (zc, zg | xg, zg) = \u2211 data [EQ logP (x \u2212 zc, zg) \u2212 zc, zg \u2212 zc, zg \u2212 zc, then the variation distribution Q = P (zc, zg) P (zg, zg)]] (28), where Q = P (zc, zg | xc, xc), zc, zc \u2212 zc, zc, zc, zg, zg, zg, zg, zg, zg, zg, zg, and then the variation distribution Q = P (zc | xc) P (zg) (zc, zg), Qc, xc, xc, Qxc."}, {"heading": "8.4. Embedding Influence", "text": "We compare the results with embedding aids and without embedding aids. Reshading comparisons can be found in Figure 9. Reshading results without embedding aids tend to show less diversity, more errors and artifacts. Reshading comparisons can be found in Figure 10. Results of reshading without embedding aids tend to show limited diversity and lead to less graphic results."}, {"heading": "8.5. Quantitative Results", "text": "The detailed quantitative evaluation results for photorecognition are in Table 2 and image saturation in Table 3. Tables contain the best error ground experiments with different sample numbers. As the sample count increases, the error falls off quickly at the beginning and then stabilizes. Our CDVAEs are consistently better than other methods. The second part of both tables are average deviations over 100 samples. We only report the final deviation as it almost does not change with the sample count."}, {"heading": "8.6. Qualitative Results", "text": "We will add more qualitative results and comparisons in this section. Results and comparisons in the field of photolight can be found in Figure 11, 12. Results in the field of photolighting with CGAN tend to be less diverse and less reasonable; results with CPixel tend to be extreme and random, and they also have less spatial structures; results with CVAE suffer from mode collapse and have limited variety. Results in image saturation and comparison can be found in Figure 13, 14. Results in the field of image saturation with CGAN tend to ignore image content and like coincidences and generate different artifacts; results with CPixel tend to be extreme and either go like coincidences or into mode collapse; results with CVAE have limited variety and produce more artifacts."}], "references": [{"title": "Intrinsic images in the wild", "author": ["S. Bell", "K. Bala", "N. Snavely"], "venue": "ACM Trans. Graph.,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2014}, {"title": "Mixture density networks", "author": ["C.M. Bishop"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1994}, {"title": "Learning diverse image colorization", "author": ["A. Deshpande", "J. Lu", "M. Yeh", "D.A. Forsyth"], "venue": "CoRR, abs/1612.01958,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2016}, {"title": "Learning large-scale automatic image colorization", "author": ["A. Deshpande", "J. Rock", "D. Forsyth"], "venue": "In Proceedings of the IEEE International Conference on Computer Vision,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2015}, {"title": "Image style transfer using convolutional neural networks", "author": ["L.A. Gatys", "A.S. Ecker", "M. Bethge"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, Jun 2016", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2016}, {"title": "DRAW: A recurrent neural network for image generation", "author": ["K. Gregor", "I. Danihelka", "A. Graves", "D.J. Rezende", "D. Wierstra"], "venue": "In Proceedings of the 32nd International Conference on Machine Learning, ICML 2015,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2015}, {"title": "Auto-encoding variational bayes", "author": ["D.P. Kingma", "M. Welling"], "venue": "International Conference on Learning Representations (ICLR),", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2014}, {"title": "Deep convolutional inverse graphics network", "author": ["T.D. Kulkarni", "W.F. Whitney", "P. Kohli", "J. Tenenbaum"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2015}, {"title": "Learning representations for automatic colorization", "author": ["G. Larsson", "M. Maire", "G. Shakhnarovich"], "venue": "In European Conference on Computer Vision (ECCV),", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2016}, {"title": "Sparse depth super resolution", "author": ["J. Lu", "D. Forsyth"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2015}, {"title": "Conditional generative adversarial nets", "author": ["M. Mirza", "S. Osindero"], "venue": "arXiv preprint arXiv:1411.1784,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2014}, {"title": "Plug & play generative networks: Conditional iterative generation of images in latent space", "author": ["A. Nguyen", "J. Yosinski", "Y. Bengio", "A. Dosovitskiy", "J. Clune"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2017}, {"title": "Locality preserving projections", "author": ["X. Niyogi"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2004}, {"title": "Unsupervised representation learning with deep convolutional generative adversarial networks", "author": ["A. Radford", "L. Metz", "S. Chintala"], "venue": "arXiv preprint arXiv:1511.06434,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2015}, {"title": "Learning what and where to draw", "author": ["S.E. Reed", "Z. Akata", "S. Mohan", "S. Tenka", "B. Schiele", "H. Lee"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2016}, {"title": "Stochastic backpropagation and approximate inference in deep generative models", "author": ["D.J. Rezende", "S. Mohamed", "D. Wierstra"], "venue": "ICML\u201914, pages II\u20131278\u2013II\u20131286. JMLR.org,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2014}, {"title": "Trajectory mixture density networks with multiple mixtures for acoustic-articulatory inversion", "author": ["K. Richmond"], "venue": "In  International Conference on Nonlinear Speech Processing,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2007}, {"title": "Learning structured output representation using deep conditional generative models", "author": ["K. Sohn", "H. Lee", "X. Yan"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2015}, {"title": "Ladder variational autoencoders", "author": ["C.K. S\u00f8nderby", "T. Raiko", "L. Maal\u00f8e", "S.K. S\u00f8nderby", "O. Winther"], "venue": "In Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2016}, {"title": "Learning stochastic feedforward neural networks", "author": ["Y. Tang", "R.R. Salakhutdinov"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2013}, {"title": "Deep architectures for articulatory inversion", "author": ["B. Uria", "I. Murray", "S. Renals", "K. Richmond"], "venue": "In Proceedings of Interspeech,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2012}, {"title": "Conditional image generation with pixelcnn decoders", "author": ["A. van den Oord", "N. Kalchbrenner", "O. Vinyals", "L. Espeholt", "A. Graves", "K. Kavukcuoglu"], "venue": "CoRR, abs/1606.05328,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2016}, {"title": "Anticipating visual representations from unlabeled video", "author": ["C. Vondrick", "H. Pirsiavash", "A. Torralba"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2016}, {"title": "An uncertain future: Forecasting from static images using variational autoencoders", "author": ["J. Walker", "C. Doersch", "A. Gupta", "M. Hebert"], "venue": "In European Conference on Computer Vision,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2016}, {"title": "Visual dynamics: Probabilistic future frame synthesis via cross convolutional networks", "author": ["T. Xue", "J. Wu", "K. Bouman", "B. Freeman"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2016}, {"title": "Unsupervised diverse colorization via generative adversarial networks", "author": ["W.Z. Yun Cao", "Zhiming Zhou", "Y. Yu"], "venue": "CoRR, abs/1702.06674,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2017}, {"title": "Deep mixture density networks for acoustic modeling in statistical parametric speech synthesis", "author": ["H. Zen", "A. Senior"], "venue": "In 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2014}, {"title": "Colorful image colorization", "author": ["R. Zhang", "P. Isola", "A.A. Efros"], "venue": "ECCV, 2016", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2016}, {"title": "Learning temporal transformations from time-lapse videos", "author": ["Y. Zhou", "T.L. Berg"], "venue": "In European Conference on Computer Vision,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2016}], "referenceMentions": [{"referenceID": 17, "context": "There are many motion fields consistent with an image [19, 25, 26].", "startOffset": 54, "endOffset": 66}, {"referenceID": 23, "context": "There are many motion fields consistent with an image [19, 25, 26].", "startOffset": 54, "endOffset": 66}, {"referenceID": 24, "context": "There are many motion fields consistent with an image [19, 25, 26].", "startOffset": 54, "endOffset": 66}, {"referenceID": 2, "context": "Similarly, there are many shading fields consistent with the layout of an image (Figure 1); many good ways to colorize an image [3, 27, 29]; many possible ways to adjust the saturation of an image (Figure 1); many possible long term futures for an image frame [30]; and so on.", "startOffset": 128, "endOffset": 139}, {"referenceID": 25, "context": "Similarly, there are many shading fields consistent with the layout of an image (Figure 1); many good ways to colorize an image [3, 27, 29]; many possible ways to adjust the saturation of an image (Figure 1); many possible long term futures for an image frame [30]; and so on.", "startOffset": 128, "endOffset": 139}, {"referenceID": 27, "context": "Similarly, there are many shading fields consistent with the layout of an image (Figure 1); many good ways to colorize an image [3, 27, 29]; many possible ways to adjust the saturation of an image (Figure 1); many possible long term futures for an image frame [30]; and so on.", "startOffset": 128, "endOffset": 139}, {"referenceID": 28, "context": "Similarly, there are many shading fields consistent with the layout of an image (Figure 1); many good ways to colorize an image [3, 27, 29]; many possible ways to adjust the saturation of an image (Figure 1); many possible long term futures for an image frame [30]; and so on.", "startOffset": 260, "endOffset": 264}, {"referenceID": 6, "context": "variational auto-encoder (VAE) of [7], to produce a conditional VAE (CVAE) [19, 25, 26].", "startOffset": 34, "endOffset": 37}, {"referenceID": 17, "context": "variational auto-encoder (VAE) of [7], to produce a conditional VAE (CVAE) [19, 25, 26].", "startOffset": 75, "endOffset": 87}, {"referenceID": 23, "context": "variational auto-encoder (VAE) of [7], to produce a conditional VAE (CVAE) [19, 25, 26].", "startOffset": 75, "endOffset": 87}, {"referenceID": 24, "context": "variational auto-encoder (VAE) of [7], to produce a conditional VAE (CVAE) [19, 25, 26].", "startOffset": 75, "endOffset": 87}, {"referenceID": 14, "context": "Recent work shows that better generative models are obtained by conditioning on textembeddings [15] or pre-trained features from visual recognition network [12].", "startOffset": 95, "endOffset": 99}, {"referenceID": 11, "context": "Recent work shows that better generative models are obtained by conditioning on textembeddings [15] or pre-trained features from visual recognition network [12].", "startOffset": 156, "endOffset": 160}, {"referenceID": 1, "context": "To ensure that F (u; c(x)) will vary for similar c(x), we use a Mixture Density Network (MDN) [2].", "startOffset": 94, "endOffset": 97}, {"referenceID": 3, "context": "Important application examples, where the prediction is naturally ambiguous, include colorization [4, 9, 29], style transfer [5], temporal transformations prediction [30], predicting motion fields [19, 25, 26], and predicting future frames [24].", "startOffset": 98, "endOffset": 108}, {"referenceID": 8, "context": "Important application examples, where the prediction is naturally ambiguous, include colorization [4, 9, 29], style transfer [5], temporal transformations prediction [30], predicting motion fields [19, 25, 26], and predicting future frames [24].", "startOffset": 98, "endOffset": 108}, {"referenceID": 27, "context": "Important application examples, where the prediction is naturally ambiguous, include colorization [4, 9, 29], style transfer [5], temporal transformations prediction [30], predicting motion fields [19, 25, 26], and predicting future frames [24].", "startOffset": 98, "endOffset": 108}, {"referenceID": 4, "context": "Important application examples, where the prediction is naturally ambiguous, include colorization [4, 9, 29], style transfer [5], temporal transformations prediction [30], predicting motion fields [19, 25, 26], and predicting future frames [24].", "startOffset": 125, "endOffset": 128}, {"referenceID": 28, "context": "Important application examples, where the prediction is naturally ambiguous, include colorization [4, 9, 29], style transfer [5], temporal transformations prediction [30], predicting motion fields [19, 25, 26], and predicting future frames [24].", "startOffset": 166, "endOffset": 170}, {"referenceID": 17, "context": "Important application examples, where the prediction is naturally ambiguous, include colorization [4, 9, 29], style transfer [5], temporal transformations prediction [30], predicting motion fields [19, 25, 26], and predicting future frames [24].", "startOffset": 197, "endOffset": 209}, {"referenceID": 23, "context": "Important application examples, where the prediction is naturally ambiguous, include colorization [4, 9, 29], style transfer [5], temporal transformations prediction [30], predicting motion fields [19, 25, 26], and predicting future frames [24].", "startOffset": 197, "endOffset": 209}, {"referenceID": 24, "context": "Important application examples, where the prediction is naturally ambiguous, include colorization [4, 9, 29], style transfer [5], temporal transformations prediction [30], predicting motion fields [19, 25, 26], and predicting future frames [24].", "startOffset": 197, "endOffset": 209}, {"referenceID": 22, "context": "Important application examples, where the prediction is naturally ambiguous, include colorization [4, 9, 29], style transfer [5], temporal transformations prediction [30], predicting motion fields [19, 25, 26], and predicting future frames [24].", "startOffset": 240, "endOffset": 244}, {"referenceID": 18, "context": "Each DVAE has two layers of latent gaussian variables and we use the ladder VAE architecture of [20].", "startOffset": 96, "endOffset": 100}, {"referenceID": 27, "context": "While one might temper the distribution (eg [29]), the ideal is to obtain multiple different solutions.", "startOffset": 44, "endOffset": 48}, {"referenceID": 6, "context": "Generative models of images present problems of dimension; these can be addressed with latent variable models, leading to the variational autoencoder (VAE) of [7].", "startOffset": 159, "endOffset": 162}, {"referenceID": 6, "context": "Current generative models are reliable in simple domains (handwritten digits [7, 18]; faces [7, 8, 16]; and CIFAR images [6]) but less so for general images.", "startOffset": 77, "endOffset": 84}, {"referenceID": 6, "context": "Current generative models are reliable in simple domains (handwritten digits [7, 18]; faces [7, 8, 16]; and CIFAR images [6]) but less so for general images.", "startOffset": 92, "endOffset": 102}, {"referenceID": 7, "context": "Current generative models are reliable in simple domains (handwritten digits [7, 18]; faces [7, 8, 16]; and CIFAR images [6]) but less so for general images.", "startOffset": 92, "endOffset": 102}, {"referenceID": 15, "context": "Current generative models are reliable in simple domains (handwritten digits [7, 18]; faces [7, 8, 16]; and CIFAR images [6]) but less so for general images.", "startOffset": 92, "endOffset": 102}, {"referenceID": 5, "context": "Current generative models are reliable in simple domains (handwritten digits [7, 18]; faces [7, 8, 16]; and CIFAR images [6]) but less so for general images.", "startOffset": 121, "endOffset": 124}, {"referenceID": 15, "context": "Improvements are available using multiple layers of latent variables (a DVAE) [16].", "startOffset": 78, "endOffset": 82}, {"referenceID": 13, "context": "Models can be trained with adversarial loss [14].", "startOffset": 44, "endOffset": 48}, {"referenceID": 18, "context": "The ladder VAE imposes both top-down and bottom up variational distributions for more efficient training [20].", "startOffset": 105, "endOffset": 109}, {"referenceID": 19, "context": "give a multimodal conditional model [21], but the conditioning variables are binary.", "startOffset": 36, "endOffset": 40}, {"referenceID": 17, "context": "A conditional variational autoencoder (CVAE) [19] conditions the decoder model on a continuous representation produced by a network applied to x.", "startOffset": 45, "endOffset": 49}, {"referenceID": 23, "context": "This approach has been demonstrated on motion prediction problems [25, 30].", "startOffset": 66, "endOffset": 74}, {"referenceID": 28, "context": "This approach has been demonstrated on motion prediction problems [25, 30].", "startOffset": 66, "endOffset": 74}, {"referenceID": 1, "context": "We use the mixture density network (MDN) in our models to capture the underlying multimodal distribution [2].", "startOffset": 105, "endOffset": 108}, {"referenceID": 16, "context": "MDNs have been successfully applied to articulatory-acoustic inversion mapping [17, 22] and speech synthesis [28].", "startOffset": 79, "endOffset": 87}, {"referenceID": 20, "context": "MDNs have been successfully applied to articulatory-acoustic inversion mapping [17, 22] and speech synthesis [28].", "startOffset": 79, "endOffset": 87}, {"referenceID": 26, "context": "MDNs have been successfully applied to articulatory-acoustic inversion mapping [17, 22] and speech synthesis [28].", "startOffset": 109, "endOffset": 113}, {"referenceID": 15, "context": "Our CDVAE consists of two deep variational auto encoders (DVAE) [16] and a mixture density network (MDN) [2].", "startOffset": 64, "endOffset": 68}, {"referenceID": 1, "context": "Our CDVAE consists of two deep variational auto encoders (DVAE) [16] and a mixture density network (MDN) [2].", "startOffset": 105, "endOffset": 108}, {"referenceID": 12, "context": "We use Niyogi [13] to build an embedding which preserves metric relations between data points.", "startOffset": 14, "endOffset": 18}, {"referenceID": 12, "context": "Our input feature vector to [13] is constructed in three parts.", "startOffset": 28, "endOffset": 32}, {"referenceID": 18, "context": "We use an inference method [20] that co-relates the latent variables of the encoder and the decoder.", "startOffset": 27, "endOffset": 31}, {"referenceID": 9, "context": "We aggressively upsample the generated fields with the approach in [10], which preserves edges during upsampling.", "startOffset": 67, "endOffset": 71}, {"referenceID": 0, "context": "For photo relighting, intrinsic image decomposition method from [1] is used to obtain albedo and shading images.", "startOffset": 64, "endOffset": 67}, {"referenceID": 23, "context": "Error-of-Best to ground truth: We follow the motion prediction work [25] to use error-of-best to ground truth as an evaluation metric.", "startOffset": 68, "endOffset": 72}, {"referenceID": 23, "context": "Conditional variational autoencoder: We implement a CVAE similar to [25].", "startOffset": 68, "endOffset": 72}, {"referenceID": 23, "context": "We cannot use [25] since their architecture is specific to prediction of coarse motion trajectories.", "startOffset": 14, "endOffset": 18}, {"referenceID": 13, "context": "[14] with 5 deconvolution layers, and we use codes of dimension 64 (to be consistent with CDVAE).", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "We use the same strategy as [25], i.", "startOffset": 28, "endOffset": 32}, {"referenceID": 10, "context": "Conditional GAN: CGAN [11] is another conditional image generation model.", "startOffset": 22, "endOffset": 26}, {"referenceID": 21, "context": "Conditional PixelCNN (CPixel): Conditional PixelCNN [23] uses masked and gated convolutions (sigmoid and tanh activations layers multiplied).", "startOffset": 52, "endOffset": 56}, {"referenceID": 5, "context": "Therefore, PixelCNN (CPixel) feasibly approximates the compute intensive recurrent architectures [6] for image generation.", "startOffset": 97, "endOffset": 100}, {"referenceID": 15, "context": "The difference between DVAE [16] and VAE [7] is multiple layers of gaussian latent variables.", "startOffset": 28, "endOffset": 32}, {"referenceID": 6, "context": "The difference between DVAE [16] and VAE [7] is multiple layers of gaussian latent variables.", "startOffset": 41, "endOffset": 44}, {"referenceID": 18, "context": "LVAE [20] recursively corrects the generative distribution by a data dependent approximate likelihood in a process resembling the recent Ladder Network.", "startOffset": 5, "endOffset": 9}], "year": 2017, "abstractText": "Problems such as predicting a new shading field (Y ) for an image (X) are ambiguous: many very distinct solutions are good. Representing this ambiguity requires building a conditional model P (Y |X) of the prediction, conditioned on the image. Such a model is difficult to train, because we do not usually have training data containing many different shadings for the same image. As a result, we need different training examples to share data to produce good models. This presents a danger we call \u201ccode space collapse\u201d \u2014 the training procedure produces a model that has a very good loss score, but which represents the conditional distribution poorly. We demonstrate an improved method for building conditional models by exploiting a metric constraint on training data that prevents code space collapse. We demonstrate our model on two example tasks using real data: image saturation adjustment, image relighting. We describe quantitative metrics to evaluate ambiguous generation results. Our results quantitatively and qualitatively outperform different strong baselines.", "creator": "LaTeX with hyperref package"}}}