{"id": "1612.00148", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Dec-2016", "title": "Domain Adaptation for Named Entity Recognition in Online Media with Word Embeddings", "abstract": "Content on the Internet is heterogeneous and arises from various domains like News, Entertainment, Finance and Technology. Understanding such content requires identifying named entities (persons, places and organizations) as one of the key steps. Traditionally Named Entity Recognition (NER) systems have been built using available annotated datasets (like CoNLL, MUC) and demonstrate excellent performance. However, these models fail to generalize onto other domains like Sports and Finance where conventions and language use can differ significantly. Furthermore, several domains do not have large amounts of annotated labeled data for training robust Named Entity Recognition models. A key step towards this challenge is to adapt models learned on domains where large amounts of annotated training data are available to domains with scarce annotated data.", "histories": [["v1", "Thu, 1 Dec 2016 05:08:53 GMT  (2454kb,D)", "http://arxiv.org/abs/1612.00148v1", "12 pages, 3 figures, 8 tables arxiv preprint"]], "COMMENTS": "12 pages, 3 figures, 8 tables arxiv preprint", "reviews": [], "SUBJECTS": "cs.CL cs.IR", "authors": ["vivek kulkarni", "yashar mehdad", "troy chevalier"], "accepted": false, "id": "1612.00148"}, "pdf": {"name": "1612.00148.pdf", "metadata": {"source": "CRF", "title": "Domain Adaptation for Named Entity Recognition in Online Media with Word Embeddings", "authors": ["Vivek Kulkarni", "Yashar Mehdad", "Troy Chevalier"], "emails": ["{vvkulkarni@cs.stonybrook.edu,", "ymehdad@yahoo-inc.com,", "troyc@yahoo-inc.com}"], "sections": [{"heading": null, "text": "In this paper, we propose methods for effectively adapting models learned on a domain to other domains. First, we analyze the linguistic variation that exists between domains to identify key linguistic insights that can enhance performance between domains. We suggest methods to capture domain-specific semantics of word use in addition to global semantics. Then, we show how to effectively use this domain-specific knowledge to learn NER models that surpass previous baselines in domain adaptation, which was done when the author was a research intern at Yahoo."}, {"heading": "1 Introduction", "text": "In fact, it is the case that most of them are in a position to go into a different world, in which they are able to move, in which they move, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they, in which they live, in which they live, in which they, in which they are able to move, in which they are able to move, in which they are able to move, in which they live in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they are able"}, {"heading": "2 Methods", "text": "In this section, we propose (a) two methods for modeling domain-specific word semantics to explicitly capture linguistic differences between domains, and (b) two methods that use domain-specific word embedding to learn robust named entity recognition models for different domains that use domain adaptation."}, {"heading": "2.1 Domain Specific Linguistic Variation", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "DOMAINDIST", "text": "Faced with a corpus C with K domains and the vocabulary V, we try to learn a domain-specific word that embeds \u03c6k: V 7 \u2192 Rd using a neural language model in which k domains and vocabulary occur. We apply the method described in (Bamman et al., 2014; Kulkarni et al., 2015b) to learn domain-specific word embeddings. 1 We briefly describe this approach as relevant to domain-specific embeddings. For each word w-V, the model (1) learns a global embeddings of the word \u03b4MAIN (w) for the word that ignores all domain-specific keywords and (2) a differentiated embeddings of the word \u03b4k (w) that encodes deviations from the global embeddings for w domains specifically k. The domain-specific embeddings of the word k (the word) is calculated as the word \u03b4k (w) (the word)."}, {"heading": "DOMAINSENSE", "text": "Here we outline another method to capture semantic variations in word use in domains. We model the problem as follows: \u2022 Sense Specific Embeddings We assume that each word w has potentially S senses, in which we try not only to learn an embedding for each sense of w, but also to derive what these senses are from the corpus C. \u2022 Sense Proportions in Domains The use of w in each domain d can be characterized by a probability distribution between the derived senses of w. To learn, we use the Adaptive Skip-gram Model, which is from (Bartunov et al., 2015) to automatically infer the different sensations of word w."}, {"heading": "Error Bounds on Estimated JS Divergence", "text": "Note that for each given word w, the empirical probability estimate calculated as Pr (\u03c0di (w) = s) is estimated based on its use in the example corpus and is therefore a random variable. Since this probability estimate is further used to calculate the JS divergence between the sense distributions across two domains (di, dj), this estimate is also a random variable and shows variance. We now offer theoretical limits to the standard deviation of the calculated Jennsen-Shannon divergence. Lemma 1. Let na (w) and nb (w) be the total number of occurrences of a word w in the domains da and db respectively. The standard deviation in the JS divergence of the sense distribution of w in this domain pair is O (1) na (w) + 1 (w) proof. We provide the proof in the supplementary material."}, {"heading": "2.2 Domain Adaptation for NER", "text": "In the previous section, we described methods for capturing domain-specific linguistic variations in word semantics / usage by learning domain-specific word embeddings. In this section, we outline how we learn NER models for the various domains using such word embeddings as traits. As in previous work, we treat NER as a sequence marker problem. For training, we use CRFsuite (Okazaki, 2007) using the L-BFGS algorithm. We use a BILOU identifier scheme. The traits we use are in Table 2. Our main traits are tokens and word embeddings within a small window of the target token. We examine the use of various types of embeddings listed below: \u2022 Generic Word2vec embeddings: We learn generic embeddings of ski programs using English Wikipedia.2Our reported results (see Figure 3), in which JS divergences are small in both domains."}, {"heading": "DOMAINEMBNER", "text": "In this context, we are interested in a named entity recognition system for domain T. However, the training data available for domain T is scarce, but we have access to a source domain S, for which we have a large number of training examples. We would like to use a simple method for this task by learning a model that uses the large amount of training data from the source domain S and adapts it to work well on the target domain T. There are a number of methods for the task of supervised domain adjustment (Jiang, 2008). We will use a simple method for this task, which is outlined below: 1. Combine the training data from S and T. Again, note that | S | > | T | works well in our setting. 2. Extract the features outlined for the training of the CRF model."}, {"heading": "ACTIVEDOMAINEMBNER", "text": "In this section we describe how to learn a named entity recognition system, assuming that weAlgorithm 1 ACTIVEDOMAINEMBNER (S, T, B, k) Input: S: Training data for NER in the source domain, T: Unlabeled data for the task of NER in the target domain which is separate and distinct from the final test set. B: Number of actively labeled examples, k: Batch size of actively labeled examples. Output: M: NER model 1: C \u2190 S 2: repeat 3: Learn a model M using C 4: Evaluate M on T. 5: E \u2190 Sort the weighted phrases of T in ascending order of model security (probability) and remove the least trusted examples. 6: Ask an expert to label each example in E and label it to C. 7: C \u2190 C, E 8: until | C | \u2265 | S | + B 9: return Mno eled data training in the domain."}, {"heading": "3 Datasets", "text": "In this section, we outline details of the data sets we are considering for our experiments. Our data sets can be divided into two categories: (a) Unlabeled data for learning word embedings and (b) Labeled data for the task of NER, which we describe below."}, {"heading": "3.1 Unlabeled Data", "text": "We use the following unmarked data sets for learning word embedding. We consider (a) all English Wikipedia sentences (b) a random sample of 1 million articles from Yahoo! Finance, which restricts our language to English only, and (c) a random sample of 1 million articles from Yahoo! Sports, which restricts our language to English only."}, {"heading": "3.2 Labeled Data", "text": "We also use marked data sets for the learning task of NER models, which we summarize in Table 3."}, {"heading": "4 Experiments", "text": "Here we briefly describe the results of our experiments on (a) Domain Specific Linguistic Variation and (b) Domain Adaptation for Named Entity Recognition."}, {"heading": "4.1 Domain Specific Linguistic Variation", "text": "Table 4 shows some of the semantic differences in word usage captured by DOMAINDIST. Note that the method is capable of capturing words such as quotes, overtime, hurdles with alternative meanings (semantics) in one area. For example, the word hurdles means challenges in the financial world, but a kind of athletic race in sports. In addition to capturing words that differ in semantics, note that DOMAINDIST also reveals different semantic uses of units, as in Figure 1. In the financial sphere, Anthem refers to a health insurance fund, but Anthem in sports refers mainly to a song like a team anthem. Figure 3 shows a number of words recognized by DOMAINSENSE. Again, it should be noted that we are able to capture domain-specific differences between words (both entities and non-entities). In addition, DOMAINSENSE is able to quantify the percentage of each word in different roles."}, {"heading": "4.2 Domain Adaptation for Named Entity Recognition", "text": "In this section we report on the results of using our DOMAINDIST and DOMAINSENSE word embedding for the task of Named Entity Recognition on Finance and Sports Domains in the domain customization setting as in Section 2. We also outline the basic methods to which we compare below:"}, {"heading": "Baseline methods", "text": "Since our setting is setting the domain adjustment for the named entity recognition, we consider several competing baselines for this task: \u2022 CoNLL-only model: We are looking at a simple baseline in which we train an NER model using only CoNLL data and generic Wikipedia embedding without adjusting to the target domain. \u2022 Feature Subsetting: This domain adjustment method attempts to punish features that exhibit large discrepancies between source and target domains (Satpal and Sarawagi, 2007). It is worth noting that this model presents the task of NER as a classification problem rather than a structured prediction problem. 3 \u2022 Online-FLORS: FLORS learns robust representations of each word based on distributions and counting to increase performance across domains and treats the tagging problem as a classification problem. We also use a random sample of 100K unidentified domains to use the wildness of each domain, etc."}, {"heading": "Results and Discussion", "text": "Table 5 shows the performance of our methods and other baselines on Finance and Sports. First, note that a CoNLL-only model without domain adaptation leads to poor performance. Domain adaptation methods such as feature subsetting and FLORS, which define the Named Entity Recognition model as a specific bed-bed problem rather than performing a sequence prediction problem, are even worse. In contrast, FEMA, which learns the dense representations of CRF functions that can then be used to learn a more robust CRF model (more suitable for domain adaptation), shows a significantly improved F1 score of 67.70 on Finance and 82.48 on Sport respectively. Empirically, we observe that the use of DOMAINSENSE embeddings improves performance compared to the ConLL model, but does not perform as well on this task (especially in Finance)."}, {"heading": "5 Related Work", "text": "Related work is divided into two areas: (a) Socio-variational linguistics and (b) Domain Adaptation. (Socio-variational linguistics) Several papers examine how language varies according to geography and time (Eisenstein et al., 2010; Eisenstein et al., 2011; Bamman et al., 2014; Bamman et al., 2014; Kulkarni et al., 2015a; Kenter et al., 2015; Gonc'alves and Sa \"nchez, 2014; Kulkarni et al., 2015b; Cook et al., 2014; Chantal and Lapata, 2016; Hamilton et al., 2016). Unlike these studies, our work attempts to identify semantic changes in word meaning (usage) in different areas, with a focus on improving the performance of an NLP task such as the NER. The methods outlined in (Kulkarni et al al al al al al al al al al al al al., 2015a)."}, {"heading": "6 Conclusions", "text": "In this paper, we propose methods for recognizing and analyzing semantic differences in word usage in multiple domains. Our methods capture domain-specific cues by capturing word embedding from unlabeled text and scaling to large datasets on a web scale. In addition, we outline methods that effectively utilize such domain-specific linguistic variations and knowledge to enhance performance in NLP tasks such as named entity recognition in areas with scarce training data and domain adaptations. Our methods not only exceed previous competitive baselines, but also require a very small number of manually annotated sentences in the target domain to achieve competitive performance. We believe that our work creates the conditions for new directions and further research into applications that effectively model linguistic variation in different domains to improve the performance, applicability, and usability of NLP systems for analyzing diverse content on the Internet."}, {"heading": "Acknowledgments", "text": "We thank Akshay Soni, Swayambhoo Jain, Aasish Pappu andKapil Thadani for valuable insights and discussions."}, {"heading": "A Supplemental Material", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A.1 Domain Disambiguation", "text": "In fact, it is so that it is a matter of a way in which people see themselves able to put themselves in the world, in which they are able to understand the world, and in which they are able to change the world. (...) It is as if people in the world were able to change the world. (...) It is as if they were able to change the world. (...) It is as if the world in the world of the world of the world were able to change the world. (...) It is as if they were able to change the world. (...) It is as if the world in the world of the world of the world to live in the world of the world, in the world of the world we live, in the world in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which, in which, in which, in which, in which, in which, in which, in which, in which, in which, in which, in which, in which, in which, in which, in which, in which, in which, in which, in which, in which, in which, in the world in which, in which, in which, in which, in which, in which, in which, in which, in which, in which, in which, in which, in which, in the world in which, in which, in which, in which, in which, in which, in which, in which, in which, in which, in which, in which, in which, in which, in which, in which, in which, in which, in the world, in which, in which, in which, in which, in the world in which, in which, in which, in which, in which, in which, in which, in which, in the world, in which, in which, in which, in the world, in which, in which, in which, in which, in which, in the world in which, in which, in which, in which, in which, in which, in which, in which, in which, in which, in which, in the world, in which, in which, in which, in which, in which, in the, in which, in which, in which, in the world in which, in which, in which, in"}], "references": [{"title": "Polyglotner: Massive multilingual named entity recognition", "author": ["Al-Rfou et al.2015] Rami Al-Rfou", "Vivek Kulkarni", "Bryan Perozzi", "Steven Skiena"], "venue": null, "citeRegEx": "Al.Rfou et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Al.Rfou et al\\.", "year": 2015}, {"title": "A framework for learning predictive structures from multiple tasks and unlabeled data", "author": ["Ando", "Zhang2005] Rie Kubota Ando", "Tong Zhang"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Ando et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Ando et al\\.", "year": 2005}, {"title": "Gender identity and lexical variation in social media", "author": ["David Bamman"], "venue": "Journal of Sociolinguistics", "citeRegEx": "Bamman,? \\Q2014\\E", "shortCiteRegEx": "Bamman", "year": 2014}, {"title": "Distributed representations of geographically situated language", "author": ["Bamman et al.2014] David Bamman", "Chris Dyer", "Noah A. Smith"], "venue": "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics", "citeRegEx": "Bamman et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bamman et al\\.", "year": 2014}, {"title": "Breaking sticks and ambiguities with adaptive skipgram", "author": ["Dmitry Kondrashkin", "Anton Osokin", "Dmitry Vetrov"], "venue": "arXiv preprint arXiv:1502.07257", "citeRegEx": "Bartunov et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bartunov et al\\.", "year": 2015}, {"title": "Domain adaptation with structural correspondence learning", "author": ["Blitzer et al.2006] John Blitzer", "Ryan McDonald"], "venue": "In Proceedings of the 2006 conference on empirical methods in natural language processing,", "citeRegEx": "Blitzer and McDonald,? \\Q2006\\E", "shortCiteRegEx": "Blitzer and McDonald", "year": 2006}, {"title": "Marginalized denoising autoencoders for domain adaptation", "author": ["Chen et al.2012] Minmin Chen", "Zhixiang Xu"], "venue": "arXiv preprint arXiv:1206.4683", "citeRegEx": "Chen and Xu,? \\Q2012\\E", "shortCiteRegEx": "Chen and Xu", "year": 2012}, {"title": "The expressive power of word embeddings", "author": ["Chen et al.2013] Yanqing Chen", "Bryan Perozzi", "Rami Al-Rfou", "Steven Skiena"], "venue": "arXiv preprint arXiv:1301.3226", "citeRegEx": "Chen et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2013}, {"title": "Named entity recognition: a maximum entropy approach using global information", "author": ["Chieu", "Ng2002] Hai Leong Chieu", "Hwee Tou Ng"], "venue": "In Proceedings of the 19th international conference on Computational linguistics-Volume", "citeRegEx": "Chieu et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Chieu et al\\.", "year": 2002}, {"title": "Natural language processing (almost) from scratch. JMLR", "author": ["Jason Weston", "L\u00e9on Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa"], "venue": null, "citeRegEx": "Collobert et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Novel wordsense identification", "author": ["Cook et al.2014] Paul Cook", "Jey Han Lau", "Diana McCarthy", "Timothy Baldwin"], "venue": null, "citeRegEx": "Cook et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cook et al\\.", "year": 2014}, {"title": "A latent variable model for geographic lexical variation", "author": ["Brendan O\u2019Connor", "Noah A Smith", "Eric P Xing"], "venue": null, "citeRegEx": "Eisenstein et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Eisenstein et al\\.", "year": 2010}, {"title": "Discovering sociolinguistic associations with structured sparsity", "author": ["Noah A Smith"], "venue": "ACL-HLT", "citeRegEx": "Eisenstein and Smith,? \\Q2011\\E", "shortCiteRegEx": "Eisenstein and Smith", "year": 2011}, {"title": "Regularized multi\u2013task learning", "author": ["Evgeniou", "Pontil2004] Theodoros Evgeniou", "Massimiliano Pontil"], "venue": "In Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining,", "citeRegEx": "Evgeniou et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Evgeniou et al\\.", "year": 2004}, {"title": "Named entity recognition through classifier combination", "author": ["Florian et al.2003] Radu Florian", "Abe Ittycheriah"], "venue": "In Proceedings of the seventh conference on Natural language learning at HLTNAACL 2003-Volume", "citeRegEx": "Florian and Ittycheriah,? \\Q2003\\E", "shortCiteRegEx": "Florian and Ittycheriah", "year": 2003}, {"title": "A bayesian model of diachronic meaning change. Transactions of the Association for Computational Linguistics", "author": ["Frermann", "Lapata2016] Lea Frermann", "Mirella Lapata"], "venue": null, "citeRegEx": "Frermann et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Frermann et al\\.", "year": 2016}, {"title": "Crowdsourcing dialect characterization through twitter", "author": ["Gon\u00e7alves", "S\u00e1nchez2014] Bruno Gon\u00e7alves", "David S\u00e1nchez"], "venue": null, "citeRegEx": "Gon\u00e7alves et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Gon\u00e7alves et al\\.", "year": 2014}, {"title": "Diachronic word embeddings reveal statistical laws of semantic change", "author": ["Jure Leskovec", "Dan Jurafsky"], "venue": "arXiv preprint arXiv:1605.09096", "citeRegEx": "Hamilton et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Hamilton et al\\.", "year": 2016}, {"title": "Bidirectional LSTM-CRF models for sequence tagging. CoRR, abs/1508.01991", "author": ["Huang et al.2015] Zhiheng Huang", "Wei Xu", "Kai Yu"], "venue": null, "citeRegEx": "Huang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2015}, {"title": "Instance weighting for domain adaptation in nlp", "author": ["Jiang", "Zhai2007] Jing Jiang", "ChengXiang Zhai"], "venue": "In ACL,", "citeRegEx": "Jiang et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Jiang et al\\.", "year": 2007}, {"title": "Domain adaptation in natural language processing. ProQuest", "author": ["Jing Jiang"], "venue": null, "citeRegEx": "Jiang.,? \\Q2008\\E", "shortCiteRegEx": "Jiang.", "year": 2008}, {"title": "Ad hoc monitoring of vocabulary shifts over time", "author": ["Kenter et al.2015] Tom Kenter", "Melvin Wevers", "Pim Huijnen"], "venue": "In CIKM. ACM", "citeRegEx": "Kenter et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kenter et al\\.", "year": 2015}, {"title": "Temporal analysis of language through neural language models", "author": ["Kim et al.2014] Yoon Kim", "Yi-I Chiu", "Kentaro Hanaki"], "venue": null, "citeRegEx": "Kim et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kim et al\\.", "year": 2014}, {"title": "Statistically significant detection of linguistic change", "author": ["Rami Al-Rfou", "Bryan Perozzi", "Steven Skiena"], "venue": null, "citeRegEx": "Kulkarni et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kulkarni et al\\.", "year": 2015}, {"title": "2015b. Freshman or fresher? quantifying the geographic variation of internet", "author": ["Bryan Perozzi", "Steven Skiena"], "venue": null, "citeRegEx": "Kulkarni et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kulkarni et al\\.", "year": 2015}, {"title": "Literature survey: domain adaptation algorithms for natural language processing", "author": ["Qi Li"], "venue": null, "citeRegEx": "Li.,? \\Q2012\\E", "shortCiteRegEx": "Li.", "year": 2012}, {"title": "Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781", "author": ["Kai Chen", "Greg Corrado", "Jeffrey Dean"], "venue": null, "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Crfsuite: a fast implementation of conditional random fields (crfs)", "author": ["Naoaki Okazaki"], "venue": null, "citeRegEx": "Okazaki.,? \\Q2007\\E", "shortCiteRegEx": "Okazaki.", "year": 2007}, {"title": "Domain adaptation of conditional probability models via feature subsetting", "author": ["Satpal", "Sunita Sarawagi"], "venue": "In European Conference on Principles of Data Mining and Knowledge Discovery,", "citeRegEx": "Satpal et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Satpal et al\\.", "year": 2007}, {"title": "Flors: Fast and simple domain adaptation for part-of-speech tagging. Transactions of the Association for Computational Linguistics, 2:15\u201326", "author": ["Schnabel", "Sch\u00fctze2014] Tobias Schnabel", "Hinrich Sch\u00fctze"], "venue": null, "citeRegEx": "Schnabel et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Schnabel et al\\.", "year": 2014}, {"title": "Unsupervised multi-domain adaptation with feature embeddings", "author": ["Yang", "Eisenstein2015] Yi Yang", "Jacob Eisenstein"], "venue": null, "citeRegEx": "Yang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2015}, {"title": "Online updating of word representations for part-of-speech tagging", "author": ["Yin et al.2016] Wenpeng Yin", "Tobias Schnabel", "Hinrich Sch\u00fctze"], "venue": "arXiv preprint arXiv:1604.00502", "citeRegEx": "Yin et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Yin et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 9, "context": "For example, most competitive Named Entity Recognition systems are trained on large amounts of labeled data from a given domain (like CoNLL or MUC) and evaluated on a held out test set drawn from the same domain (Florian et al., 2003; Chieu and Ng, 2002; Ando and Zhang, 2005; Collobert et al., 2011; Huang et al., 2015).", "startOffset": 212, "endOffset": 320}, {"referenceID": 18, "context": "For example, most competitive Named Entity Recognition systems are trained on large amounts of labeled data from a given domain (like CoNLL or MUC) and evaluated on a held out test set drawn from the same domain (Florian et al., 2003; Chieu and Ng, 2002; Ando and Zhang, 2005; Collobert et al., 2011; Huang et al., 2015).", "startOffset": 212, "endOffset": 320}, {"referenceID": 7, "context": "With recent advances in representation learning, word embeddings have been shown to be very useful features for several NLP tasks like POS Tagging, NER, and Sentiment Analysis (Chen et al., 2013; Al-Rfou et al., 2015; Collobert et al., 2011).", "startOffset": 176, "endOffset": 241}, {"referenceID": 0, "context": "With recent advances in representation learning, word embeddings have been shown to be very useful features for several NLP tasks like POS Tagging, NER, and Sentiment Analysis (Chen et al., 2013; Al-Rfou et al., 2015; Collobert et al., 2011).", "startOffset": 176, "endOffset": 241}, {"referenceID": 9, "context": "With recent advances in representation learning, word embeddings have been shown to be very useful features for several NLP tasks like POS Tagging, NER, and Sentiment Analysis (Chen et al., 2013; Al-Rfou et al., 2015; Collobert et al., 2011).", "startOffset": 176, "endOffset": 241}, {"referenceID": 3, "context": "We apply the method discussed in (Bamman et al., 2014; Kulkarni et al., 2015b) to learn domain specific word embeddings.", "startOffset": 33, "endOffset": 78}, {"referenceID": 3, "context": "1 we differentiate ourselves from (Bamman et al., 2014; Kulkarni et al., 2015b) by outlining a probabilistic method that uses this model to disambiguate the domain given a phrase that outlines the usage of a word w.", "startOffset": 34, "endOffset": 79}, {"referenceID": 4, "context": "To learn sense specific embeddings, we use the Adaptive Skip-gram model proposed by (Bartunov et al., 2015) to automatically infer (a) the different senses a word w exhibits (b) a probability distribution \u03c0(w) over the the different senses a word exhibits in the corpus and (c) an embedding for each sense of the word.", "startOffset": 84, "endOffset": 107}, {"referenceID": 4, "context": "Disambiguate each occurrence of w in di and dj using the method described by (Bartunov et al., 2015).", "startOffset": 77, "endOffset": 100}, {"referenceID": 27, "context": "To train, we use CRFsuite (Okazaki, 2007) with L-BFGS algorithm.", "startOffset": 26, "endOffset": 41}, {"referenceID": 20, "context": "There exist a number of methods for the task of supervised domain adaptation (Jiang, 2008).", "startOffset": 77, "endOffset": 90}, {"referenceID": 31, "context": "We consider a scalable version of FLORS (Yin et al., 2016).", "startOffset": 40, "endOffset": 58}, {"referenceID": 26, "context": "\u2022 FEMA: FEMA (Yang and Eisenstein, 2015) learns low dimensional embeddings of the features used in a CRF model by using a variant of the Skipgram Model (Mikolov et al., 2013).", "startOffset": 152, "endOffset": 174}, {"referenceID": 11, "context": "Socio-variational linguistics Several works study how language varies according to geography and time (Eisenstein et al., 2010; Eisenstein et al., 2011; Bamman and others, 2014; Bamman et al., 2014; Kim et al., 2014; Kulkarni et al., 2015a; Kenter et al., 2015; Gon\u00e7alves and S\u00e1nchez, 2014; Kulkarni et al., 2015b; Cook et al., 2014; Frermann and Lapata, 2016; Hamilton et al., 2016).", "startOffset": 102, "endOffset": 383}, {"referenceID": 3, "context": "Socio-variational linguistics Several works study how language varies according to geography and time (Eisenstein et al., 2010; Eisenstein et al., 2011; Bamman and others, 2014; Bamman et al., 2014; Kim et al., 2014; Kulkarni et al., 2015a; Kenter et al., 2015; Gon\u00e7alves and S\u00e1nchez, 2014; Kulkarni et al., 2015b; Cook et al., 2014; Frermann and Lapata, 2016; Hamilton et al., 2016).", "startOffset": 102, "endOffset": 383}, {"referenceID": 22, "context": "Socio-variational linguistics Several works study how language varies according to geography and time (Eisenstein et al., 2010; Eisenstein et al., 2011; Bamman and others, 2014; Bamman et al., 2014; Kim et al., 2014; Kulkarni et al., 2015a; Kenter et al., 2015; Gon\u00e7alves and S\u00e1nchez, 2014; Kulkarni et al., 2015b; Cook et al., 2014; Frermann and Lapata, 2016; Hamilton et al., 2016).", "startOffset": 102, "endOffset": 383}, {"referenceID": 21, "context": "Socio-variational linguistics Several works study how language varies according to geography and time (Eisenstein et al., 2010; Eisenstein et al., 2011; Bamman and others, 2014; Bamman et al., 2014; Kim et al., 2014; Kulkarni et al., 2015a; Kenter et al., 2015; Gon\u00e7alves and S\u00e1nchez, 2014; Kulkarni et al., 2015b; Cook et al., 2014; Frermann and Lapata, 2016; Hamilton et al., 2016).", "startOffset": 102, "endOffset": 383}, {"referenceID": 10, "context": "Socio-variational linguistics Several works study how language varies according to geography and time (Eisenstein et al., 2010; Eisenstein et al., 2011; Bamman and others, 2014; Bamman et al., 2014; Kim et al., 2014; Kulkarni et al., 2015a; Kenter et al., 2015; Gon\u00e7alves and S\u00e1nchez, 2014; Kulkarni et al., 2015b; Cook et al., 2014; Frermann and Lapata, 2016; Hamilton et al., 2016).", "startOffset": 102, "endOffset": 383}, {"referenceID": 17, "context": "Socio-variational linguistics Several works study how language varies according to geography and time (Eisenstein et al., 2010; Eisenstein et al., 2011; Bamman and others, 2014; Bamman et al., 2014; Kim et al., 2014; Kulkarni et al., 2015a; Kenter et al., 2015; Gon\u00e7alves and S\u00e1nchez, 2014; Kulkarni et al., 2015b; Cook et al., 2014; Frermann and Lapata, 2016; Hamilton et al., 2016).", "startOffset": 102, "endOffset": 383}, {"referenceID": 3, "context": "The methods outlined in (Kulkarni et al., 2015b; Bamman et al., 2014) are most closely related to our work.", "startOffset": 24, "endOffset": 69}, {"referenceID": 3, "context": "First, we show that the model proposed in (Bamman et al., 2014) is not only useful for learning domain specific word embeddings but the model itself can be utilized for the task of domain disambiguation by using an often neglected set of model parameters (the output vectors of the model).", "startOffset": 42, "endOffset": 63}, {"referenceID": 3, "context": "While the methods outlined in (Kulkarni et al., 2015b; Bamman et al., 2014) capture domain specific differences, they do not explicitly model the fact that words have multiple senses and their usage in a domain is a mixture of different proportions over these senses which can be explicitly quantified.", "startOffset": 30, "endOffset": 75}, {"referenceID": 20, "context": "Finally an excellent survey of various domain adaptation algorithms for NLP is provided by (Jiang, 2008; Li, 2012).", "startOffset": 91, "endOffset": 114}, {"referenceID": 25, "context": "Finally an excellent survey of various domain adaptation algorithms for NLP is provided by (Jiang, 2008; Li, 2012).", "startOffset": 91, "endOffset": 114}], "year": 2016, "abstractText": "Content on the Internet is heterogeneous and arises from various domains like News, Entertainment, Finance and Technology. Understanding such content requires identifying named entities (persons, places and organizations) as one of the key steps. Traditionally Named Entity Recognition (NER) systems have been built using available annotated datasets (like CoNLL, MUC) and demonstrate excellent performance. However, these models fail to generalize onto other domains like Sports and Finance where conventions and language use can differ significantly. Furthermore, several domains do not have large amounts of annotated labeled data for training robust Named Entity Recognition models. A key step towards this challenge is to adapt models learned on domains where large amounts of annotated training data are available to domains with scarce annotated data. In this paper, we propose methods to effectively adapt models learned on one domain onto other domains using distributed word representations. First we analyze the linguistic variation present across domains to identify key linguistic insights that can boost performance across domains. We propose methods to capture domain specific semantics of word usage in addition to global semantics. We then demonstrate how to effectively use such domain specific knowledge to learn NER models that outperform previous baselines in the domain adaptation setting. \u2217This work was done when the author was a research intern at Yahoo. \u2217\u00a9 2016 This is the authors draft of the work. It is posted here for your personal use. Not for redistribution.", "creator": "LaTeX with hyperref package"}}}