{"id": "1612.00155", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Dec-2016", "title": "Adversarial Images for Variational Autoencoders", "abstract": "We investigate adversarial attacks for autoencoders. We propose a procedure that distorts the input image to mislead the autoencoder in reconstructing a completely different target image. We attack the internal latent representations, attempting to make the adversarial input produce an internal representation as similar as possible as the target's. We find that autoencoders are much more robust to the attack than classifiers: while some examples have tolerably small input distortion, and reasonable similarity to the target image, there is a quasi-linear trade-off between those aims. We report results on MNIST and SVHN datasets, and also test regular deterministic autoencoders, reaching similar conclusions in all cases. Finally, we show that the usual adversarial attack for classifiers, while being much easier, also presents a direct proportion between distortion on the input, and misdirection on the output. That proportionality however is hidden by the normalization of the output, which maps a linear layer into non-linear probabilities.", "histories": [["v1", "Thu, 1 Dec 2016 05:59:57 GMT  (1331kb,D)", "http://arxiv.org/abs/1612.00155v1", "Workshop on Adversarial Training, NIPS 2016, Barcelona, Spain"]], "COMMENTS": "Workshop on Adversarial Training, NIPS 2016, Barcelona, Spain", "reviews": [], "SUBJECTS": "cs.NE cs.CV cs.LG", "authors": ["pedro tabacof", "julia tavares", "eduardo valle"], "accepted": false, "id": "1612.00155"}, "pdf": {"name": "1612.00155.pdf", "metadata": {"source": "CRF", "title": "Adversarial Images for Variational Autoencoders", "authors": ["Pedro Tabacof", "Julia Tavares"], "emails": ["dovalle}@dca.fee.unicamp.br"], "sections": [{"heading": "1 Introduction", "text": "In fact, it is in such a way that most of them will be able to put themselves in a different world, in which they are able to put themselves in a position than in a world in which they are able to live and live. In the other world, it is in such a way that they are able to put themselves in a world, in which they are one world, in which they are able to put themselves in a world, in which they are able to live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, they live, in which they live, in which they live, in which they live, they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live,"}, {"heading": "2 Autoencoders and Variational Autoencoders", "text": "The model consists of two parts: an encoder that maps the input into the latent representation; and a decoder that maps such a representation as close to the input as possible. In regular autocoders, the training function can be as simple as the \"2-distance between input and output. Famous variants include autocoders that map and denoise such representation as close to the input as possible, using implicit autocoders that feed implicit noise into the input while maintaining the original input-input-input-input-input input while maintaining the original input-input-input input."}, {"heading": "3 Adversarial Images for Autoencoders", "text": "If the attack succeeds, people should hardly be able to distinguish between the contrary and the regular input [2, 4]. However, we can be even stricter and only allow a distortion below the input quantization noise [3, 9]. To create contrary images for classification, one can maximize the misdirection toward a certain wrong label [2, 4] or away from the correct [3]. Distortion can be minimized [2, 4] or limited to being small [3, 9]. Finally, it is often required that the images remain within their valid space (i.e., no pixels \"below black or above white\"). In the autocoders, there is not a single class output to misclassify, but an entire image output to cramp."}, {"heading": "4 Data and Methods", "text": "In fact, it is a way in which people are able to determine for themselves what they want and what they want."}, {"heading": "5 Results and Discussion", "text": "In fact, it is such that most of them are able to move into another world, in which they are able to move, in which they are able to stay, in which they are able to move, in which they are able to live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they are able to"}, {"heading": "6 Conclusion", "text": "We have shown that there is a linear trade-off between the degree to which the opposing input resembles the original input and the degree to which the opposing reconstruction resembles the target reconstruction - shattering the hope that a small change in the input could lead to drastic changes in reconstruction. Surprisingly, such a linear trade-off also occurs in hostile attacks on classification networks when we \"undo\" the nonlinearity of the last layer. In the future, we intend to extend our empirical results to datasets with larger inputs and more complex networks (e.g. ImageNet) - as well as to various auto-encoder architectures. For example, the DRAW variation auto-code [18] uses feedback from the reconstruction error to improve reconstruction - and may therefore be more robust to attacks."}, {"heading": "Acknowledgments", "text": "We thank the Brazilian agencies CAPES, CNPq and FAPESP for their financial support. We thank NVIDIA Corporation for their support by donating the Tesla K40 GPU used for this research. Eduardo Valle is partially supported by a Google Awards LatAm 2016 scholarship and a CNPq PQ-2 scholarship (311486 / 2014-2)."}], "references": [{"title": "A note on the evaluation of generative models", "author": ["Lucas Theis", "A\u00e4ron van den Oord", "Matthias Bethge"], "venue": "arXiv preprint arXiv:1511.01844,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2015}, {"title": "Intriguing properties of neural networks", "author": ["Christian Szegedy", "Wojciech Zaremba", "Ilya Sutskever", "Joan Bruna", "Dumitru Erhan", "Ian Goodfellow", "Rob Fergus"], "venue": "arXiv preprint arXiv:1312.6199,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2013}, {"title": "Explaining and harnessing adversarial examples", "author": ["Ian J Goodfellow", "Jonathon Shlens", "Christian Szegedy"], "venue": "arXiv preprint arXiv:1412.6572,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2014}, {"title": "Exploring the space of adversarial images", "author": ["Pedro Tabacof", "Eduardo Valle"], "venue": "arXiv preprint arXiv:1510.05328,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2015}, {"title": "Distributional smoothing by virtual adversarial examples", "author": ["Takeru Miyato", "Shin-ichi Maeda", "Masanori Koyama", "Ken Nakae", "Shin Ishii"], "venue": "arXiv preprint arXiv:1507.00677,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2015}, {"title": "Towards deep neural network architectures robust to adversarial examples", "author": ["Shixiang Gu", "Luca Rigazio"], "venue": "arXiv preprint arXiv:1412.5068,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2014}, {"title": "Generative adversarial nets", "author": ["Ian Goodfellow", "Jean Pouget-Abadie", "Mehdi Mirza", "Bing Xu", "David Warde-Farley", "Sherjil Ozair", "Aaron Courville", "Yoshua Bengio"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2014}, {"title": "Adversarial manipulation of deep representations", "author": ["Sara Sabour", "Yanshuai Cao", "Fartash Faghri", "David J Fleet"], "venue": "arXiv preprint arXiv:1511.05122,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2015}, {"title": "Sparse autoencoder", "author": ["Andrew Ng"], "venue": "CS294A Lecture notes,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2011}, {"title": "Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion", "author": ["Pascal Vincent", "Hugo Larochelle", "Isabelle Lajoie", "Yoshua Bengio", "Pierre-Antoine Manzagol"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2010}, {"title": "Learning deconvolution network for semantic segmentation", "author": ["Hyeonwoo Noh", "Seunghoon Hong", "Bohyung Han"], "venue": "In Computer Vision (ICCV),", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2015}, {"title": "Auto-encoding variational bayes", "author": ["Diederik P Kingma", "Max Welling"], "venue": "arXiv preprint arXiv:1312.6114,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2013}, {"title": "Semi-supervised learning with deep generative models", "author": ["Diederik P Kingma", "Shakir Mohamed", "Danilo Jimenez Rezende", "Max Welling"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2014}, {"title": "Attend, infer, repeat: Fast scene understanding with generative models", "author": ["SM Eslami", "Nicolas Heess", "Theophane Weber", "Yuval Tassa", "Koray Kavukcuoglu", "Geoffrey E Hinton"], "venue": "arXiv preprint arXiv:1603.08575,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2016}, {"title": "Variational dropout and the local reparameterization trick", "author": ["Diederik P Kingma", "Tim Salimans", "Max Welling"], "venue": "arXiv preprint arXiv:1506.02557,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2015}, {"title": "Unsupervised representation learning with deep convolutional generative adversarial networks", "author": ["Alec Radford", "Luke Metz", "Soumith Chintala"], "venue": "arXiv preprint arXiv:1511.06434,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2015}, {"title": "Draw: A recurrent neural network for image generation", "author": ["Karol Gregor", "Ivo Danihelka", "Alex Graves", "Daan Wierstra"], "venue": "arXiv preprint arXiv:1502.04623,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2015}, {"title": "Towards conceptual compression", "author": ["Karol Gregor", "Frederic Besse", "Danilo Jimenez Rezende", "Ivo Danihelka", "Daan Wierstra"], "venue": "arXiv preprint arXiv:1604.08772,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2016}, {"title": "Stochastic backpropagation and approximate inference in deep generative models", "author": ["Danilo Jimenez Rezende", "Shakir Mohamed", "Daan Wierstra"], "venue": "arXiv preprint arXiv:1401.4082,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2014}, {"title": "The mnist database of handwritten digits", "author": ["Yann LeCun", "Corinna Cortes", "Christopher JC Burges"], "venue": null, "citeRegEx": "21", "shortCiteRegEx": "21", "year": 1998}, {"title": "Reading digits in natural images with unsupervised feature learning", "author": ["Yuval Netzer", "Tao Wang", "Adam Coates", "Alessandro Bissacco", "Bo Wu", "Andrew Y Ng"], "venue": null, "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2011}, {"title": "Lasagne: First release", "author": ["Sander Dieleman", "Jan Schl\u00fcter", "Colin Raffel", "Eben Olson", "S\u00f8ren Kaae S\u00f8nderby", "Daniel Nouri", "Daniel Maturana", "Martin Thoma", "Eric Battenberg", "Jack Kelly"], "venue": null, "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2015}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2014}, {"title": "Deconvolutional networks", "author": ["Matthew D Zeiler", "Dilip Krishnan", "Graham W Taylor", "Rob Fergus"], "venue": "In Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2010}], "referenceMentions": [{"referenceID": 0, "context": "Evaluating generative models is hard [1], there are no clear-cut success criteria for autoencoder reconstruction, and therefore, neither for the attack.", "startOffset": 37, "endOffset": 40}, {"referenceID": 1, "context": "[2] introduced adversarial images, showing how to force a deep network to misclassify an image by applying nearly imperceptible distortions.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[3] exploited the linear nature of deep convolutional networks to both attempt explaining how adversarial samples arise, and to propose a much faster technique to create them.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "Tabacof and Valle [4] explored", "startOffset": 18, "endOffset": 21}, {"referenceID": 2, "context": "Training models to resist adversarial attacks was advanced as a form of regularization [3, 5].", "startOffset": 87, "endOffset": 93}, {"referenceID": 4, "context": "Training models to resist adversarial attacks was advanced as a form of regularization [3, 5].", "startOffset": 87, "endOffset": 93}, {"referenceID": 5, "context": "[6] used autoencoders to pre-process the input and try to reinforce the network against adversarial attacks, finding that although in some cases resistance improved, attacks with small distortions remained possible.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "A more recent trend is training adversarial models, in which one attempts to generate \u201cartificial\u201d samples (from a generative model) and the other attempts to recognize those samples [7].", "startOffset": 183, "endOffset": 186}, {"referenceID": 5, "context": "Although autoencoders appear in the literature of adversarial images as an attempt to obtain robustness to the attacks [6], and in the literature of adversarial training as models that can be trained with the technique [8], we are unaware of any attempts to create attacks targeted to them.", "startOffset": 119, "endOffset": 122}, {"referenceID": 7, "context": "[9] show that adversarial attacks can not only lead to mislabelling, but also manipulate the internal representations of the network.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "Famous variants include sparse autoencoders, which use `1-regularization [10], and denoising autoencoders, which use implicit regularization by feeding noise to the input, while keeping the original input in the reconstruction loss term [11].", "startOffset": 73, "endOffset": 77}, {"referenceID": 9, "context": "Famous variants include sparse autoencoders, which use `1-regularization [10], and denoising autoencoders, which use implicit regularization by feeding noise to the input, while keeping the original input in the reconstruction loss term [11].", "startOffset": 237, "endOffset": 241}, {"referenceID": 10, "context": ", a segmentation map) [12].", "startOffset": 22, "endOffset": 26}, {"referenceID": 11, "context": "A modern variant of growing popularity, variational autoencoders [13] interpret the latent representation through a Bayesian lens, thus offering a theoretical foundation for the reconstruction and regularization objectives.", "startOffset": 65, "endOffset": 69}, {"referenceID": 11, "context": "The representation prior p(z) is often the standard normal N (0, I) [13], but might be instead a discrete distribution (e.", "startOffset": 68, "endOffset": 72}, {"referenceID": 12, "context": "Bernoulli) [14], or even some distribution with geometric interpretation (\u201cwhat\u201d and \u201cwhere\u201d latent variables) [15].", "startOffset": 11, "endOffset": 15}, {"referenceID": 13, "context": "Bernoulli) [14], or even some distribution with geometric interpretation (\u201cwhat\u201d and \u201cwhere\u201d latent variables) [15].", "startOffset": 111, "endOffset": 115}, {"referenceID": 11, "context": "As the prior and the approximated posterior are normal distributions, their KL divergence has analytic form [13].", "startOffset": 108, "endOffset": 112}, {"referenceID": 14, "context": "We can use the reparameterization trick to reduce the variance of the gradient estimator [16].", "startOffset": 89, "endOffset": 93}, {"referenceID": 11, "context": "The encoder and the decoder may be any neural network: a multilayer perceptron [13], a convolutional network [17], or even LSTMs.", "startOffset": 79, "endOffset": 83}, {"referenceID": 15, "context": "The encoder and the decoder may be any neural network: a multilayer perceptron [13], a convolutional network [17], or even LSTMs.", "startOffset": 109, "endOffset": 113}, {"referenceID": 16, "context": "The latter are a recent development \u2014 recurrent variational autoencoders \u2014 which use soft attention to encode and decode patches from the input image [18, 19].", "startOffset": 150, "endOffset": 158}, {"referenceID": 17, "context": "The latter are a recent development \u2014 recurrent variational autoencoders \u2014 which use soft attention to encode and decode patches from the input image [18, 19].", "startOffset": 150, "endOffset": 158}, {"referenceID": 18, "context": "Simulating a chain of samples from the latent variables and likelihood allows to denoise images, or to impute missing data (inpaint images) [20].", "startOffset": 140, "endOffset": 144}, {"referenceID": 15, "context": "The latent variables of a variational autoencoder also allow visual analogy and interpolation [17].", "startOffset": 94, "endOffset": 98}, {"referenceID": 1, "context": "If the attack is successful, humans should hardly be able to distinguish between the adversarial and the regular inputs [2, 4].", "startOffset": 120, "endOffset": 126}, {"referenceID": 3, "context": "If the attack is successful, humans should hardly be able to distinguish between the adversarial and the regular inputs [2, 4].", "startOffset": 120, "endOffset": 126}, {"referenceID": 2, "context": "We can be even more strict, and only allow a distortion below the input quantization noise [3, 9].", "startOffset": 91, "endOffset": 97}, {"referenceID": 7, "context": "We can be even more strict, and only allow a distortion below the input quantization noise [3, 9].", "startOffset": 91, "endOffset": 97}, {"referenceID": 1, "context": "To build adversarial images for classification, one can maximize the misdirection towards a certain wrong label [2, 4] or away from the correct one [3].", "startOffset": 112, "endOffset": 118}, {"referenceID": 3, "context": "To build adversarial images for classification, one can maximize the misdirection towards a certain wrong label [2, 4] or away from the correct one [3].", "startOffset": 112, "endOffset": 118}, {"referenceID": 2, "context": "To build adversarial images for classification, one can maximize the misdirection towards a certain wrong label [2, 4] or away from the correct one [3].", "startOffset": 148, "endOffset": 151}, {"referenceID": 1, "context": "The distortion can be minimized [2, 4] or constrained to be small [3, 9].", "startOffset": 32, "endOffset": 38}, {"referenceID": 3, "context": "The distortion can be minimized [2, 4] or constrained to be small [3, 9].", "startOffset": 32, "endOffset": 38}, {"referenceID": 2, "context": "The distortion can be minimized [2, 4] or constrained to be small [3, 9].", "startOffset": 66, "endOffset": 72}, {"referenceID": 7, "context": "The distortion can be minimized [2, 4] or constrained to be small [3, 9].", "startOffset": 66, "endOffset": 72}, {"referenceID": 19, "context": "We worked on the binarized MNIST [21] and SVHN datasets [22].", "startOffset": 33, "endOffset": 37}, {"referenceID": 20, "context": "We worked on the binarized MNIST [21] and SVHN datasets [22].", "startOffset": 56, "endOffset": 60}, {"referenceID": 11, "context": "Following literature [13], we modeled pixel likelihoods as independent Bernoullis (for binary images), or as independent normals (for RGB images).", "startOffset": 21, "endOffset": 25}, {"referenceID": 21, "context": "We used Parmesan and Lasagne [23] for the implementation1.", "startOffset": 29, "endOffset": 33}, {"referenceID": 22, "context": "We extract the gradients of the lower bound using automatic differentiation and maximize it using stochastic gradient ascent via the ADAM algorithm [24].", "startOffset": 148, "endOffset": 152}, {"referenceID": 23, "context": "We parameterized the encoder and decoder as fully-connected networks in the MNIST case, and as convolutional and deconvolutional [25] networks in the SVHN case.", "startOffset": 129, "endOffset": 133}, {"referenceID": 3, "context": "4) may be chosen by bisection as the smallest constant that still leads to success [4].", "startOffset": 83, "endOffset": 86}, {"referenceID": 2, "context": "[3] and Sabour et al.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[9] optimize differently, choosing for \u2206 an `\u221e-norm constrained to make the distortion imperceptible, while maximizing the misdirection.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[3] suggested that the linearity of deep models make them susceptible to adversarial attacks.", "startOffset": 0, "endOffset": 3}, {"referenceID": 16, "context": "For example, the DRAW variational autoencoder [18] uses feedback from the reconstruction error to improve the reconstruction \u2014 and thus could be more robust to attacks.", "startOffset": 46, "endOffset": 50}], "year": 2016, "abstractText": "We investigate adversarial attacks for autoencoders. We propose a procedure that distorts the input image to mislead the autoencoder in reconstructing a completely different target image. We attack the internal latent representations, attempting to make the adversarial input produce an internal representation as similar as possible as the target\u2019s. We find that autoencoders are much more robust to the attack than classifiers: while some examples have tolerably small input distortion, and reasonable similarity to the target image, there is a quasi-linear trade-off between those aims. We report results on MNIST and SVHN datasets, and also test regular deterministic autoencoders, reaching similar conclusions in all cases. Finally, we show that the usual adversarial attack for classifiers, while being much easier, also presents a direct proportion between distortion on the input, and misdirection on the output. That proportionality however is hidden by the normalization of the output, which maps a linear layer into non-linear probabilities.", "creator": "LaTeX with hyperref package"}}}