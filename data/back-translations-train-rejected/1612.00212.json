{"id": "1612.00212", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Dec-2016", "title": "Training Bit Fully Convolutional Network for Fast Semantic Segmentation", "abstract": "Fully convolutional neural networks give accurate, per-pixel prediction for input images and have applications like semantic segmentation. However, a typical FCN usually requires lots of floating point computation and large run-time memory, which effectively limits its usability. We propose a method to train Bit Fully Convolution Network (BFCN), a fully convolutional neural network that has low bit-width weights and activations. Because most of its computation-intensive convolutions are accomplished between low bit-width numbers, a BFCN can be accelerated by an efficient bit-convolution implementation. On CPU, the dot product operation between two bit vectors can be reduced to bitwise operations and popcounts, which can offer much higher throughput than 32-bit multiplications and additions.", "histories": [["v1", "Thu, 1 Dec 2016 11:56:15 GMT  (1730kb,D)", "http://arxiv.org/abs/1612.00212v1", null]], "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["he wen", "shuchang zhou", "zhe liang", "yuxiang zhang", "dieqiao feng", "xinyu zhou", "cong yao"], "accepted": false, "id": "1612.00212"}, "pdf": {"name": "1612.00212.pdf", "metadata": {"source": "CRF", "title": "Training Bit Fully Convolutional Network for Fast Semantic Segmentation", "authors": ["He Wen", "Shuchang Zhou", "Zhe Liang", "Yuxiang Zhang", "Cong Yao"], "emails": ["yaocong}@megvii.com"], "sections": [{"heading": "Introduction", "text": "This year it has come to the point that it is a purely reactionary, reactionary, reactionary, reactionary and reactionary project."}, {"heading": "Related Work", "text": "Semantic segmentation helps the computer understand the structure of the images and usually serves as the basis for other computer vision applications. Current state-of-the-art semantic segmentation networks are largely completely revolutionary networks (Long, Shelhamer and Darrell 2015) and adopt the architecture of the encoder decoder with multi-level refinement (Badrinarayanan, Handa and Cipolla 2015). To achieve the best performance, powerful classification models are often embedded as part of the FCNs, which, together with large decoders, drive up computational complexity. To further refine the results from neural networks, CRFs are often used in post-processing to improve local predictions (Chen et al. 2014a) by reconstructing boundaries more accurately. As CRF can be integrated with most methods as a post-processing step, which contributes little to our main topic, it is not discussed."}, {"heading": "Method", "text": "In this section, we first present the design of our fully twisted bit network and then propose our method for forming a BFCN."}, {"heading": "Network design", "text": "A standard approach to semantic segmentation includes a feature extractor to generate feature maps from the input image, and windings with upsampling operations to predict pro-pixel labels from those feature maps. We use ResNet as a feature extractor and adopt the multi-resolution reconstruction structure of Laplacian Reconstruction and Refinement (Ghiasi and Fowlkes 2016) to perform a pro-pixel classification across feature maps at different scales (see Figure 1). However, while the network works well in full precision, we observe a large loss of accuracy in conversion to small-bit width, suggesting that this architecture is not suitable for a low-bit-width network. To solve this problem, we are evaluating various variants of BFCN to find out the cause of performance degeneration."}, {"heading": "Bit-width allocation", "text": "It is important to decide how many bits to allocate for weight and function boards, since bit width has a decisive influence on both the performance and computational complexity of a network. Since our goal is to accelerate semantic segmentation networks without losing much power, we must carefully and wisely.First, we note that it has been observed (Gupta et al. 2015) that 8-bit fixed point quantification is sufficient for a network to achieve almost the same performance as 32-bit floating point counters. Therefore, we focus our attention on bit widths of less than eight, which can provide us with further acceleration.To extend bit conversion to m-bit weights and n-bit function boards, we note that: A \u00b7 B = (A0 + 2A1 + 2mAm +), that we can allocate the bit (B0 + 2nBn) = A0B0 + 2 + Bjj + BAij..."}, {"heading": "Bit-width decay", "text": "To support this observation, we are conducting a simple experiment by training a 2-bit network initialized by a pre-trained network with different bit counts, and the training process (Figure 3) shows that networks initialized by a smaller bit width converge faster, a phenomenon that can be explained by looking at the errors in quantization. Obviously, a quantization step with higher initial precision introduced larger errors, and as a result, the model benefits less from initialization. However, introducing intermediate steps can help solve it, since networks with narrower bit widths tend to be more similar and thus more noise tolerant in truncating the bit width. Our experiments show that BFCN cannot recover very well from the quantization loss if it is directly initialized by fully precise models. To expand the idea of using intermediate steps of quantity width, we need to reduce the quantity width."}, {"heading": "Experiments", "text": "In this section we first describe the evaluated data sets and the experimental setup and then demonstrate the results of our method. Please note that we perform most of our experiments in our in-house machine learning system."}, {"heading": "Datasets", "text": "The PASCAL VOC 2012 dataset for semantic segmentation consists of 1464 labeled images for training and 1449 for validation. There are 20 categories that need to be predicted, including airplane, bus, chair, sofa, etc. All images in the dataset are no larger than 500 x 500. In accordance with the convention of literature (Long, Shelhamer and Darrell 2015; Wu, Shen and Hengel 2016), we use the extended dataset (Hariharan et al. 2011), which provides us with a total of 10582 images for training. We have also used reflection, resizing and random cropping to expand the training data. The Cityscapes dataset consists of 2975 street photos with fine annotations for training and 500 images for validation. There are 19 classes of 7 categories where the total resolution of 36xU images is measured."}, {"heading": "Experiment Setup", "text": "All experiments are initialized by an ImageNet ResNet-50 with limited activations and weights. We then use stochastic gradients that descend with a pulse of 0.9 to set the BFCN to semantic segmentation data. Since predicting higher resolution characteristics in laplac reconstruction and refinement structure depends on predicting lower resolutions, we use incremental losses to train the network. First, we only define losses on 32x upsampling branch and refine the network to convergence. Then, losses of 16x, 8x and 4x upsampling keys are added successively. To overcome the enormous imbalance of classes in cityscapes datasets, we use a class weighting scheme introduced by ENet, defined as Wclass = 1 / ln (c + pclass). We select c = 1.4 to bound class weights in [1], 3."}, {"heading": "32 / 32 69.8% -", "text": "First, we evaluate the impact of different bit-width allocations on the PASCAL VOC 2012 dataset (see Table 5).We observe how the performance of network degeneration decreases as bit-width decreases, which is our intuition. While the 8-8 model performs exactly the same as the full-precision model, a decrease in bit-width from 4-4 to 2-2 continuously leads to performance degeneration. Performance degeneration is initially low compared to bit-width savings, but suddenly becomes negligible by 4-4. We also discover that mapping different bit-widths to weights and activations harms performance compared to equally assigned models of equal complexity. From the results, we conclude that 4-4 and 2-2 in different scenes are advantageous choices. The 4-4-4 model can offer comparable performance with complete precision model, but with a significant 75% resource savings compared to 8-8 in FP8, GA-2 can only offer comparable performance."}, {"heading": "Results of bit-width decay", "text": "We then show how the bit-width drop affects the performance of networks on PASCAL VOC 2012. Table 6 shows that the bit-width drop contributes to better performance compared to directly truncating the bit-width. In addition, we evaluate the influence of the \"decay rate,\" i.e. the number of bits in one step. For a decay rate of r, we have kW = c \u2212 r \u00b7 t and kA = c \u2212 r \u00b7 t after t decay, with c = 8 being the initial bit-width. The results of different decay rates are also shown in Table 6.We find that we can achieve almost the same performance with a decay rate of less than 2, but an increase to 3 results in a sudden drop in performance."}, {"heading": "Analysis of class-wise results", "text": "In PASCAL VOC 2012, we find that in fine-grained classes such as cars and buses, cats and dogs, BFCN is less powerful than its 32-bit counterpart, but in classes such as sofas and bicycles, even 2-BFCN outperforms the full-precision network. This can be seen more clearly in cityscapes data sets: classes with low mean IoU values in the full-precision network deteriorate after quantification (such as walls and tracks), while these large, frequent classes such as skies and cars remain almost identical in accuracy. Observation is consistent with our intuition that a quantified network with low bit width is generally less powerful and therefore harder to train for difficult tasks."}, {"heading": "Analysis of run-time performance", "text": "We have implemented a custom runtime on the arm, and all of our results on the CPU are measured directly at runtime. We note that a single precision operation in terms of resource usage is equivalent to 1024 bitops on the FPGA, and approximately 18 bitops on the CPU corresponding to the inference speed measured in our custom runtime. Therefore, a network with m-bit weights and n-bit activations is expected to be 18 m x n faster than its 32-bit counterpart without overheads. As shown in Table 8, our 1-2 BFCN can run 7.8x faster than a fully accurate network with only 1 / 32 memory size."}, {"heading": "Discussion", "text": "In Figure 4, we present some sample editions of PASCAL VOC 2012. Predictions show that BFCN performs well on simple tasks, but for difficult tasks, which consist largely of small objects or rare classes such as bottles and sofas, BFCN will fail and perform poorly. It also appears that BFCN has difficulty reconstructing fine structures of the input image. However, low bit-width networks rarely misrepresent the entire object, making them effective in real-world applications."}, {"heading": "Conclusion and Future Work", "text": "In this paper, we propose methods for building fully revolutionary bit networks that use low bit-width weights and activations to accelerate inference speed and reduce memory consumption; we also propose a novel method to train a low-bit-width network that gradually reduces bit-width to reduce the performance losses resulting from quantization; as a result, we are able to train efficient, low-bit-width scene networks without losing much power; and low-bit-width networks are particularly friendly to hardware implementations such as FPGA, as low-bit-width multipliers typically require orders of magnitude resources; a better base model can be used for future work; and CRF and other techniques can be integrated into BFCN for even better performance; and we point out that our methods for developing and forming low-bit-width networks can also be applied to other related tasks such as object recognition and punching segmentation."}], "references": [{"title": "Segnet: A deep convolutional encoder-decoder architecture for robust semantic pixel-wise labelling", "author": ["Handa Badrinarayanan", "V. Cipolla 2015] Badrinarayanan", "A. Handa", "R. Cipolla"], "venue": "arXiv preprint arXiv:1505.07293", "citeRegEx": "Badrinarayanan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Badrinarayanan et al\\.", "year": 2015}, {"title": "Semantic image segmentation with deep convolutional nets and fully connected crfs", "author": ["Chen"], "venue": "arXiv preprint arXiv:1412.7062", "citeRegEx": "Chen,? \\Q2014\\E", "shortCiteRegEx": "Chen", "year": 2014}, {"title": "Diannao: A small-footprint high-throughput accelerator for ubiquitous machine-learning", "author": ["Chen"], "venue": "In ACM Sigplan Notices,", "citeRegEx": "Chen,? \\Q2014\\E", "shortCiteRegEx": "Chen", "year": 2014}, {"title": "Dadiannao: A machine-learning supercomputer", "author": ["Chen"], "venue": "In Microarchitecture (MICRO),", "citeRegEx": "Chen,? \\Q2014\\E", "shortCiteRegEx": "Chen", "year": 2014}, {"title": "The cityscapes dataset for semantic urban scene understanding", "author": ["Cordts"], "venue": "In Proc. of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)", "citeRegEx": "Cordts,? \\Q2016\\E", "shortCiteRegEx": "Cordts", "year": 2016}, {"title": "Training deep neural networks with low precision multiplications", "author": ["Bengio Courbariaux", "M. David 2014] Courbariaux", "Y. Bengio", "J.-P. David"], "venue": "arXiv preprint arXiv:1412.7024", "citeRegEx": "Courbariaux et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Courbariaux et al\\.", "year": 2014}, {"title": "The pascal visual object classes challenge: A retrospective", "author": ["Everingham"], "venue": null, "citeRegEx": "Everingham,? \\Q2015\\E", "shortCiteRegEx": "Everingham", "year": 2015}, {"title": "Cnp: An fpga-based processor for convolutional networks", "author": ["Farabet"], "venue": "In 2009 International Conference on Field Programmable Logic and Applications,", "citeRegEx": "Farabet,? \\Q2009\\E", "shortCiteRegEx": "Farabet", "year": 2009}, {"title": "Large-scale fpga-based convolutional networks", "author": ["Farabet"], "venue": "Machine Learning on Very Large Data Sets", "citeRegEx": "Farabet,? \\Q2011\\E", "shortCiteRegEx": "Farabet", "year": 2011}, {"title": "Laplacian reconstruction and refinement for semantic segmentation", "author": ["Ghiasi", "G. Fowlkes 2016] Ghiasi", "C.C. Fowlkes"], "venue": "CoRR abs/1605.02264", "citeRegEx": "Ghiasi et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Ghiasi et al\\.", "year": 2016}, {"title": "Compressing deep convolutional networks using vector quantization", "author": ["Gong"], "venue": "arXiv preprint arXiv:1412.6115", "citeRegEx": "Gong,? \\Q2014\\E", "shortCiteRegEx": "Gong", "year": 2014}, {"title": "Deep learning with limited numerical precision", "author": ["Gupta"], "venue": "arXiv preprint arXiv:1502.02551", "citeRegEx": "Gupta,? \\Q2015\\E", "shortCiteRegEx": "Gupta", "year": 2015}, {"title": "Learning both weights and connections for efficient", "author": ["Han"], "venue": null, "citeRegEx": "Han,? \\Q2015\\E", "shortCiteRegEx": "Han", "year": 2015}, {"title": "Semantic contours from inverse detectors", "author": ["Hariharan"], "venue": "In 2011 International Conference on Computer Vision,", "citeRegEx": "Hariharan,? \\Q2011\\E", "shortCiteRegEx": "Hariharan", "year": 2011}, {"title": "Deep residual learning for image recognition", "author": ["He"], "venue": "arXiv preprint arXiv:1512.03385", "citeRegEx": "He,? \\Q2015\\E", "shortCiteRegEx": "He", "year": 2015}, {"title": "Densebox: Unifying landmark localization with end to end object detection", "author": ["Huang"], "venue": "arXiv preprint arXiv:1509.04874", "citeRegEx": "Huang,? \\Q2015\\E", "shortCiteRegEx": "Huang", "year": 2015}, {"title": "Densecap: Fully convolutional localization networks for dense captioning", "author": ["Karpathy Johnson", "J. Fei-Fei 2015] Johnson", "A. Karpathy", "L. Fei-Fei"], "venue": "arXiv preprint arXiv:1511.07571", "citeRegEx": "Johnson et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Johnson et al\\.", "year": 2015}, {"title": "Bitwise neural networks. arXiv preprint arXiv:1601.06071", "author": ["Kim", "M. Smaragdis 2016] Kim", "P. Smaragdis"], "venue": null, "citeRegEx": "Kim et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Kim et al\\.", "year": 2016}, {"title": "Pvanet: Deep but lightweight neural networks for real-time object detection", "author": ["Kim"], "venue": "arXiv preprint arXiv:1608.08021", "citeRegEx": "Kim,? \\Q2016\\E", "shortCiteRegEx": "Kim", "year": 2016}, {"title": "Imagenet classification with deep convolutional neural networks. In Advances in neural information processing systems, 1097\u20131105", "author": ["Sutskever Krizhevsky", "A. Hinton 2012] Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": null, "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Fully convolutional networks for semantic segmentation", "author": ["Shelhamer Long", "J. Darrell 2015] Long", "E. Shelhamer", "T. Darrell"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "Long et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Long et al\\.", "year": 2015}, {"title": "Enet: A deep neural network architecture for real-time semantic segmentation", "author": ["Paszke"], "venue": "arXiv preprint arXiv:1606.02147", "citeRegEx": "Paszke,? \\Q2016\\E", "shortCiteRegEx": "Paszke", "year": 2016}, {"title": "Neuflow: Dataflow vision processing system-on-a-chip", "author": ["Pham"], "venue": "In Circuits and Systems (MWSCAS),", "citeRegEx": "Pham,? \\Q2012\\E", "shortCiteRegEx": "Pham", "year": 2012}, {"title": "Xnor-net: Imagenet classification using binary convolutional neural networks. arXiv preprint arXiv:1603.05279", "author": ["Rastegari"], "venue": null, "citeRegEx": "Rastegari,? \\Q2016\\E", "shortCiteRegEx": "Rastegari", "year": 2016}, {"title": "Faster r-cnn: Towards real-time object detection with region proposal networks. In Advances in neural information processing", "author": ["Ren"], "venue": null, "citeRegEx": "Ren,? \\Q2015\\E", "shortCiteRegEx": "Ren", "year": 2015}, {"title": "Very deep convolutional networks for largescale image recognition", "author": ["Simonyan", "K. Zisserman 2014] Simonyan", "A. Zisserman"], "venue": "CoRR abs/1409.1556", "citeRegEx": "Simonyan et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Simonyan et al\\.", "year": 2014}, {"title": "Improving the speed of neural", "author": ["Senior Vanhoucke", "V. Mao 2011] Vanhoucke", "A. Senior", "M.Z. Mao"], "venue": null, "citeRegEx": "Vanhoucke et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Vanhoucke et al\\.", "year": 2011}, {"title": "High-performance semantic segmentation using very deep fully convolutional networks. arXiv preprint arXiv:1604.04339", "author": ["Shen Wu", "Z. Hengel 2016] Wu", "C. Shen", "A. v. d. Hengel"], "venue": null, "citeRegEx": "Wu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Wu et al\\.", "year": 2016}, {"title": "Optimizing fpga-based accelerator design for deep convolutional neural networks", "author": ["Zhang"], "venue": "In Proceedings of the 2015 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays,", "citeRegEx": "Zhang,? \\Q2015\\E", "shortCiteRegEx": "Zhang", "year": 2015}, {"title": "Accelerating very deep convolutional networks for classification and detection", "author": ["Zhang"], "venue": null, "citeRegEx": "Zhang,? \\Q2015\\E", "shortCiteRegEx": "Zhang", "year": 2015}, {"title": "Dorefa-net: Training low bitwidth convolutional neural networks with low bitwidth gradients. arXiv preprint arXiv:1606.06160", "author": ["Zhou"], "venue": null, "citeRegEx": "Zhou,? \\Q2016\\E", "shortCiteRegEx": "Zhou", "year": 2016}], "referenceMentions": [{"referenceID": 17, "context": "Some methods (Paszke et al. 2016; Kim et al. 2016) are proposed to reduce demand of computation resources of FCN by simplifying or redesigning the architecture of network.", "startOffset": 13, "endOffset": 50}], "year": 2016, "abstractText": "Fully convolutional neural networks give accurate, per-pixel prediction for input images and have applications like semantic segmentation. However, a typical FCN usually requires lots of floating point computation and large run-time memory, which effectively limits its usability. We propose a method to train Bit Fully Convolution Network (BFCN), a fully convolutional neural network that has low bit-width weights and activations. Because most of its computation-intensive convolutions are accomplished between low bit-width numbers, a BFCN can be accelerated by an efficient bit-convolution implementation. On CPU, the dot product operation between two bit vectors can be reduced to bitwise operations and popcounts, which can offer much higher throughput than 32-bit multiplications and additions. To validate the effectiveness of BFCN, we conduct experiments on the PASCAL VOC 2012 semantic segmentation task and Cityscapes. Our BFCN with 1-bit weights and 2-bit activations, which runs 7.8x faster on CPU or requires less than 1% resources on FPGA, can achieve comparable performance as the 32-bit counterpart. Introduction Deep convolutional neural networks (DCNN), with its recent progress, has considerably changed the landscape of computer vision (Krizhevsky, Sutskever, and Hinton 2012) and many other fields. To achieve close to state-of-the-art performance, a DCNN usually has a lot of parameters and high computational complexity, which may easily overwhelm resource capability of embedded devices. Substantial research efforts have been invested in speeding up DCNNs on both general-purpose (Vanhoucke, Senior, and Mao 2011; Gong et al. 2014; Han et al. 2015) and specialized computer hardware (Farabet et al. 2009; Farabet et al. 2011; Pham et al. 2012; Chen et al. 2014b; Chen et al. 2014c; Zhang et al. 2015a). Recent progress in using low bit-width networks has considerably reduced parameter storage size and computation burden by using 1-bit weight and low bit-width activations. In particular, in BNN (Kim and Smaragdis 2016) and XNOR-net (Rastegari et al. 2016), during the forward pass the most computationally expensive convolutions can Copyright c \u00a9 2017, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. network VOC12 Cityscapes speedup 32-bit FCN 69.8% 62.1% 1x 2-bit BFCN 67.0% 60.3% 4.1x 1-2 BFCN 62.8% 57.4% 7.8x Table 1: Summary results of our BFCNs. Performance measure in mean IoU. be done by combining xnor and popcount operations, thanks to the following equivalence when x and y are bit vectors:", "creator": "LaTeX with hyperref package"}}}