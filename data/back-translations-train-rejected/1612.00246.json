{"id": "1612.00246", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Dec-2016", "title": "Multilingual Multiword Expressions", "abstract": "The project aims to provide a semi-supervised approach to identify Multiword Expressions in a multilingual context consisting of English and most of the major Indian languages. Multiword expressions are a group of words which refers to some conventional or regional way of saying things. If they are literally translated from one language to another the expression will lose its inherent meaning.", "histories": [["v1", "Thu, 1 Dec 2016 14:01:00 GMT  (3728kb)", "http://arxiv.org/abs/1612.00246v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["lahari poddar"], "accepted": false, "id": "1612.00246"}, "pdf": {"name": "1612.00246.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Lahari Poddar", "Pushpak Bhattacharyya", "Manish Shrivastava", "Dhirendra Singh", "Kashiviswanatha Sarma", "Samir Janardan"], "emails": [], "sections": [{"heading": null, "text": "In fact, the fact is that most of them are able to assert themselves, that they are able to assert themselves, that they are able to achieve their goals."}, {"heading": "Acknowledgement", "text": "I would like to take this opportunity to sincerely thank Prof. Pushpak Bhattacharyya for his insights, constant support and support, and his leadership has been my primary source of motivation. I would like to thank Munish Minia for providing his report and work. I am grateful to Manish Shrivastava, Dhirendra Singh, Kashiviswanatha Sarma, Samir Janardan Sohoni for their valuable insights and intellectual contribution to the project. I am grateful to all members of CFILT in IIT Bombay for their strong interest and contributions, directly or indirectly, to the project. It is the in-depth discussion and brainstorming analysis conducted at our weekly meetings that has kept me motivated and deeply interested in this topic. Finally, I would like to thank my parents and friends who have always had faith in me and motivated me to set out these possible.viiTable of Contents."}, {"heading": "1.1. Introduction to Multiword Expressions", "text": "These are arbitrary word combinations that occur very frequently in the natural language and are therefore also referred to as collocations. In Word1,7, about 41% of entries are multiwords. Informal, the multiword refers to a phrase, when it is literally translated from one language to another, it loses its inherent meaning. Consider the term \"strong coffee\" as an example of collocation. We always use the adjective \"strong\" to describe coffee or tea, but not its other synonyms (say \"powerful\"), whereas in the case of \"drugs,\" powerful drug \"is more commonly used than\" strong drug. \"There is no specific reason why one representation or interpretation should be chosen over the other. It is exactly as it is. Multiwords are random, arbitrary and idiosyncratic. They are completely language dependent and transparent only to a native speaker."}, {"heading": "4. Down to earth person", "text": "[Bengali], [Bengali], [Bengali], [Bengali], [Bengali], [Bengali], [Bengali], [Bengali], [Bengali], [Bengali], [Bengali], [Bengali], [Bengali], [Bengali], [Bengali], [Bengali], [Bengali], [Bengali], [Bengali], [Bengali], [Bengali], [Bengali], [Bengali]."}, {"heading": "1.1. Motivation", "text": "In fact, most of them are able to move to another world, in which they are able to move to another world, in which they are able to move to another world, in which they are able to move."}, {"heading": "1.2. Organization of the report", "text": "In this chapter, we have introduced the concept of MWEs and explained the various fields of Natural Language Processing that must take care of the knowledge of multiword expressions. In Chapter 2, we first formally define the MWEs. We also present a classification of MWEs according to various criteria and the necessary and sufficient condition for an expression to be classified as MWE. In Chapter 3, we describe the different methods for extracting MWEs from a corpus and classifying them as a statistical method and knowledge-based method. In this chapter, another contemporary ongoing project in this field of research on the extraction of MWEs is described. Chapter 4 describes my contribution to the development of the MWE Engine in IIT Bombay. Chapter 5 describes some experiments that I have conducted and the ongoing research activity related to the project at IIT Bombay. Chapter 6 gives the evaluation of the performance of the system."}, {"heading": "2.1. Features of Multiword Expressions", "text": "There are certain characteristics that a group of words must have in order to be treated as collocation; the most important characteristics are: \u2022 Non-composition: the meaning of a full multiword expression can be determined entirely by the meaning of its constituents; the meaning of the expression may be completely different from its constituents (the idiom that overturns the bucket means dying), or it may contain an additional element or an inline meaning that cannot be predicted from the parts (reverting to number one means reverting to the place from which one started). 6 \u2022 Non-substitutability: The constituents of a multiword expression cannot be replaced by one of its synonyms without distorting the meaning of the expression, even though they refer to the same concept."}, {"heading": "2.2. Types of MWEs", "text": "They are either syntactically idiosyncratic or semantically non-degradable. They are either syntactically idiosyncratic or semantically non-degradable. \"These expressions are completely frozen and are not subject to any modification of general pictorial terms. Examples of this are: in short, ad hoc, of semi-fixed expressions: these kinds of expressions defy the general conventions of grammar and compositional interpretations. These expressions are completely frozen and are not subject to any modification of general pictorial forms of expression. They have limitations on word components and compositional interpretations. These expressions are completely unpronounced and are not subject to any modification in general pictorial places. Semi-fixed expressions: These kinds of expressions have limitations on word commands and compositional interpretations of formulation, but they are subject to some form of lexical variations."}, {"heading": "2.3. Classification of MWEs", "text": "In the second half of the second half of the second half of the second half of the second half of the second half of the second half of the second half of the second half of the second half of the second half of the second half of the second half of the second half of the second half of the second half of the second half of the second half of the second half of the second half of the second half of the second half of the second half of the second half of the second half of the second half of the second half of the second half of the second half of the second half of the second half of the second half of the second half of the second half of the second half of the second half of the second half of the second half of the second half of the second half of the second half of the second half of the second half of the second half of the second half of the second half of the second half of the second half of the second half of the second half of the second half of the second half of the second half of the second half of the second half of the second half of the second half of the second half of the second half of the second half of the second half of the second half of the second half of the second half of the second half of the second half of the second half of the second half of the second half of the second half of the second half of the second half of the second half of the second half of the second half of the second half of the second half of the second half of the second half of the second half of the second half of the second half of the second half of the second half of the second half of the second half of the second half of the second half of the second half of the second half of the second of the second half of the second half of the second half of the second half of the second of the second of the second half of the second half of the second half of the second of the second half of the second of the second of the second of the second half of the second of the second of the second of the second of the second of the second half of the second of the second of the second of the second of the second of the second of the second of the second of the second of the second of the second of the second of the second of the second of the second of the second of the second of the second of the second of the second of the second of the second of the second of the second of the"}, {"heading": "2.4. Necessary and Sufficient Conditions for MWE", "text": "In this section we describe the standard agreed upon during the Kashmir Multiword Workshop 2011. The necessary and sufficient requirements for classification as MWE are as follows:"}, {"heading": "2.4.1 Necessary Conditions", "text": "For a word sequence to be an MWE, it must be separated by spaces / separators. Example: Explanation: Indhiya kirikket ani Gloss: India Cricket Team Translation: Indian Cricket Team"}, {"heading": "2.4.2 Sufficient Conditions", "text": "The sufficient prerequisites for being an expression that can be classified as MWE are: 1. The non-composiality of the meaning of the MWE, i.e. the meaning of a MWE, cannot be derived from its components. Examples: The non-composiality of the meaning of the MWE, i.e. the meaning of a MWE, cannot be derived from its components. Examples: cevttu kimda plidaru15Gloss: a lawyer sitting under the tree Translation: an idle person 2. The fixity of the expression, i.e. the components of the MWE cannot be replaced by its synonyms or other words. Examples: Correct: Lifetime imprisonment * False: Thank you * False: Abundant thanks"}, {"heading": "2.5. MWE Extraction tasks", "text": "Armed with the background knowledge of the definition and characteristics of mutiword expressions, we have tried to classify them into different classes according to the most appropriate extraction approach. Figure 2.1 shows the classification. As we can see, both statistical and rule-based approaches are necessary to solve this problem. The following figure also shows the stack of NLP tools that must be used to identify different classes of multiword expressions. NLPMLString + MorphologyPOS tagging POS tagging + WordnetPOS tagging + ListChunking ParsingRules OnomaetopicReduplication (tik, chham chham chham chham) NonOnomaetopic Reduplication (ghar ghar) Semantic Relation (synonym, antonym, hypernym) (raat din, dhan doulat, chaye paani) Noncontainingsomesomesomeor StatithWE (MaliververyWE)"}, {"heading": "3.1. Approaches by various researchers", "text": "In this section, we will give an overview of the different approaches that have been tested by different researchers over the years to extract multiword expressions from a text. Methods vary greatly, some of them have adopted a linguistic approach, others have used statistical techniques, and still others have made use of the open source resources available to us to solve the problem."}, {"heading": "3.1.1 Rule Based Approaches", "text": "There are a whole range of approaches that try to recognize multiwords by using the rules that they make in the first place."}, {"heading": "3.1.1.1. Identification of Reduplication in Bengali", "text": "In fact, most of them are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move and to move."}, {"heading": "3.1.1.2. Detecting noun compounds and light verb constructions", "text": "The authors have described some rules-based methods for recognizing noun combinations and light verb constructions in running texts [25].Noun combinations are productive, i.e. new nominal combinations are formed all the time in linguistic usage, which means that they cannot be fully listed in a dictionary (e.g. \"Give a Skype call\" based on \"Give a Call\").Light verb combinations are syntactically very flexible, they can manifest in various forms: the verb can be bent, the noun can occur in its plural form, and the noun can be modified. The nominal component and the verbal component are not even coherent (e.g. \"He gave me a very helpful piece of advice.\""}, {"heading": "3.1.2 Statistical Methods for Multiwords Extraction", "text": "A number of basic statistical methods can be used to extract collocations from a given corpus [7]. The corpus used to perform the experiments was a collection of collocations for four months consisting of 14 million words. Let's look at these methods and their corresponding applications for extracting multiwords. This difficulty can easily be overcome by applying a simple 22 heuristic by retrieving the most common bigrams in the corpora. But this naive approach has produced a lot of insignificant bigrams that are very common (of the, in-the, etc.) This difficulty can easily be overcome by bypassing the candidate phrases through a POS tagger and including the combinations that have the probability of being phrases. POStag structures that are zero: AN, NN, ANN, NN, NPN, etc."}, {"heading": "3.1.2.5.1. The t-test", "text": "The t-test looks at the mean and variance of a sample, where the null hypothesis is that the sample is drawn from a distribution of mean. It calculates the difference between the observed and expected mean, scaled by the variance of the data, and tells us how likely it is to get a sample of that mean and variance (or a more extreme mean and deviation), provided that the sample follows the normal distribution.ns xt / = 2 24Where 2s is the sample variance, N is the sample size, the mean of the distribution. If the statistics are large enough, we can reject the null hypothesis that the words are related. Thus, new cases occur 15,828 times in the corpus, companies 4,675 times, and there are a total of 14,307,668 brands. New companies occur 8 times among the 14,307,668 cases that do not occur independently."}, {"heading": "3.1.2.5.2. Hypothesis Testing of Differences", "text": "Figure 3.3 shows the words that occur significantly more frequently with strong (the first ten words) and strong (the last ten words). The formula of the basic t-test is modified as 2 2 21 2 121 / / = nsns xxt 25The application for this form of t-test is lexicography. Such data is useful for a lexicographer who wants to write precise dictionary entries that point out the difference between strong and powerful.3.1.2.5.3. Persons Chi-Square Test The t-test assumes that the probability of occurrence is approximately normally distributed, which is generally not true. It is an alternative test that does not depend on the assumption of normality. The core of the test is to compare the observed frequencies with the frequencies expected for independence. If the difference between observed and expected frequencies is large, then we can reject the jiji hypothesis for the values observed."}, {"heading": "3.1.2.5.4. Likelihood Ratio", "text": "This test simply produces a number that tells us how much more likely one hypothesis is than the other. So, it is more useful than any other form of hypotheses testing. Also, probability ratios are more suitable for sparse data than the Chi-Square test.For the application of probability testing, let's consider the following two hypotheses (= =):) (= =) | (: 2) | (=) | (: 112 21 121212wwPHypothesiswwPpwwPwwPHypothesisHypothesis1 is a formalization of independence, whereas Hypothesis2 is a formalization of dependency. We calculate the log probability ratio as: 27The Figure 3.5 shows the top bigrams consisting of powerful if it is probability ratio.This approach is most useful for the discovery of subject-specific collocations. It can be used to compare a general text with a domain-specific text. 3.1.6 Mutual, we can derive a word from another one where the information theory is another."}, {"heading": "3.1.3 Word Association Measures", "text": "This is one of the very early attempts at collocation extraction by Kenneth Church and Pattrick Hanks (1990). [6] They generalized the idea of collocation to include coexistence, meaning that two words occur together when they occur very frequently in the same documents. For example, doctor and nurse or doctor and hospital are highly interconnected because they occur very frequently in a text. Information theory measure, the mutual information, was used to measure the norms of word association from a corpus, and then the collocations were produced."}, {"heading": "3.1.3.1. Word Association And Psycholinguistics", "text": "Word association norms are an important factor in psycholinguistic research. Informally, a person reacts more quickly to a word hospital if they have ever come into contact with a highly associated word doctor. In a psycholinguistic experiment, a few thousand people were asked to write down a word that comes to mind after each of the 200 words they were given. This was an empirical method of measuring word associations."}, {"heading": "3.1.3.2. Information Theoretic Measure", "text": "Mutual information: If two words (x, y) have their probability of occurrence as P (x) and P (y), then their mutual information is defined as follows:) (), (=), (2 yPxP yxPlogyxIinformally compares the probability that x and y appear together with the probability that x and y occur independently of each other. On the other hand, if there is an association between x and y) (), (yPxPyxP, i.e. 0), (yxI.30The word probabilities P (x) and P (y) are estimated by calculating the number of observations of x and y in a corpus (normalized by N, the size of the corpus), (yxI.30The word probabilities P (x) and P (y) by calculating the number of observations of x and y in a corpus."}, {"heading": "3.1.3.3. Lexico-Syntactic Regularities", "text": "s estimates) 10 * 70 / (7,3), (10 * 556) (, 10 * 250) (666 offsetP offsetP Mutual information for off is: 6,1) () (), (=); (2 offPsetP offsetPlogoffsetIFrom the above value we can conclude that the association between set and off is quite large (62 i.e. 64 times greater than chance).3.1.3.4. Meaning of word association This was a pioneering approach to the extraction of word associations. It broadened the psycholinguistic concept of the word association norm to an information theoretical measure of mutual information. Informally, it helped us predict which word to look for when we came across a word."}, {"heading": "3.1.4 Retrieving Collocations From Text : XTRACT", "text": "Frank Smadja has implemented a number of statistical techniques and developed a lexicographic tool, Xtract, to retrieve collocations from the text. [20] As already mentioned, the definition of collocation varies from author to author. According to the author, collocations have the following characteristics: \u2022 Arbitrary: They cannot be translated directly from one language to another because they are difficult to produce from a logical perspective. \u2022 Domain-dependent: There are expressions that make sense only in a particular domain. These collocations will be unknown to someone unfamiliar with the domain. \u2022 Recurrent: collocations are not unusual or random coexistence of words, they very often occur in a particular context. \u2022 Contiguous lexical clusters: The encounter with a word or part of a collocation often indicates the likelihood of hitting the rest of the collocation as well."}, {"heading": "3.1.4.1. Xtract: The lexicographic tool for collocation extraction", "text": "The three stages of analysis are: \u2022 First stage: statistical measures are used to retrieve wise lexical relations from a corpus pair. \u2022 Second stage: uses the output bigrams (the 1st stage) to produce collocations of n-grams.32. \u2022 Third stage: adds syntactical information to collocations retrieved in the first stage and filters inappropriate ones.The experiments were conducted on a 10 million word corpus of stock exchange messages."}, {"heading": "3.1.4.1.3. Xtract: Stage Three", "text": "In stage three of the Xtract, the collocations produced in stage one are analyzed, and the syntactic relationship between them is established, otherwise they are discarded. 1. Create concordances: Given a pair of words and the distance of the two words, produce all the sentences they contain in the specific position.2. Parse: Create a set of syntactic labels for each sentence 3. Labels and filters: Count the frequency of any labeling of the bigram (w, wi) and accept if and only ifTtilabelp) =] [(where T is a threshold that is defined manually during the execution of the experiment. For example, if after the first two steps of the Xtract the collocation decision is made, then it is identified as a verb-object collocation in the third stage. If no such relationship can be established, such collocations are rejected."}, {"heading": "3.1.4.2. Analysis Of Xtract", "text": "One observation that emerges from the results of Xtract is that the extracted collocations have a domain dependency, so the domain and size of the corpus have a strong influence on the type of collocations that are extracted from it. This paper shows a method in which the author finds out the syntax of the collocations that explain the syntactical relationships between the collocations."}, {"heading": "3.1.5.1.4. Intersection Graphs", "text": "When trying to determine significant collocations, the concept nodes for the synonyms are drawn. They have one or more common senses. A candidate phrase is supposed to have a collocation preference if it expresses more preference for one word than for the other to represent the same sense. This is indicated by a directed preference arc in the collocation graph and the arc first crosses the given word. This is shown as an example of emotional baggage and emotional baggage in Figure 3.11.383.1.5.2. Poly collocations It may be possible that a word in more than one of its synsets expresses preference for another word. These are referred to as poly collocations. Depending on whether or not sensory information is available, different configurations are possible for the collocation graph. 3.1.5.3. Anti-collocations A synonym used in relation to a specific target phrase can be divided into three disjunctive sentences: \u2022 The target words that are frequently used with the target language (the ones that are very common with the target language) are."}, {"heading": "3.1.5.4. Formalization", "text": "The algorithm takes a sequence of bigrams 21, pp... Np as input. \u2022 The frequency number for each such pair is defined as follows:), = (=), (1 = bapbaC i ni, where, (x) = 1, if x is true and 0, if x is false. \u2022 The frequency set of a word w is defined as follows: 390} >), (: {=) (vwcvwcs \u2022 ordering is defined as a set of synsets, W. The frequency of collocation of a word w of a candidate is defined as follows: 2} | >) (: | {=) (wcsSWSwCCS Thus, each collocation synthesis S of a candidate (for one word w) consists of at least two elements whose frequency is not zero. \u2022 The most common elements of a synthesis and their frequency are defined as follows:), (=), (= \"vwargmaxcfvgmaxmaxmaxmaxmaxmaxcmaxmaxmaxmaxmaxmaxmaxmaxmaxmaxmaxmaxmaxmaxmaxmaxmaxmaxmaxmaxmaxmaxmaxmaxmaxmaxmaxmaxmaxmaxmaxmaxmaxmaxmaxmaxmaxmaxmaxmaxmaxmaxmaxmaxmaxmaxmaxmaxmaxmaxmaxmaxmaxmaxmaxmaxmaxmaxmaxmaxmaxmaxmaxmaxmaxmaxmaxmaxmaxmaxmaxmaxmaxmaxmaxmaxmaxmaxmaxmaxmaxmaxmaxmaxx, where the algorithm is the most common. \u2022"}, {"heading": "3.1.5.5. Analysis", "text": "The idea presented in the paper looks promising, and since the work is on a semantic level, it is more intuitive and easier to relate to the functioning of a human mind in reality. Future work must focus on improving the basic algorithm in certain aspects: \u2022 The idea of the synonym set can be extended to concepts. \u2022 Experiments must be performed for synsets other than nouns. \u2022 Morphological processing must be performed. \u2022 Some thesaurs can be used together with words."}, {"heading": "3.1.6 Verb Phrase Idiomatic Expressions", "text": "An idiom can be defined as a linguistic form or expression of a given language that is grammatically peculiar to itself or cannot be understood due to the individual meanings of its elements. For example, broadly speaking, one spills the beans, shoots the breeze, breaks the ice, etc. These are very typical of a language and evolve over time. Even within a language, they vary from one dialect to another. Idioms do not follow any general conventions within their class. Likewise, some of them may allow a form of verbal inflection (a shot in the other breeze), while some may be completely fixed (now and then). Perfectly grammatical idioms are difficult to identify as idioms with idiosyncratic meaning as opposed to their similar structures (shoot the breeze and shoot the bird). The authors have examined two closely related problems facing the appropriate treatment of verb-noun-idiomatic combinations (where the noun is the direct object of the verb)."}, {"heading": "3.1.6.1. Recognizing VNICs", "text": "Although overall VNICs vary in their degree of flexibility, they are in contrast to compositional phrases (which are more lexical productive and occur in a wider range of syntactic forms), so the degree of lexical and syntactical flexibility of a given verb-noun combination can be used to determine the degree of idiomaticism of the expression. Authors have attempted to measure the lexical and syntactical fixation of an expression through a statistical approach to determine whether it is an idiom or not."}, {"heading": "3.1.6.2. Analysis", "text": "Idioms are a very interesting part of natural language, but due to their peculiarity and arbitrariness they have long been excluded by NLP researchers, who have tried to provide an effective mechanism for dealing with a widely documented and common cross-linguistic class of idioms, i.e. VNICs. They have thoroughly studied several linguistic properties of VNICs that distinguish them from similar literal expressions. Novel techniques for translating such characteristics into measures that predict the idiomatic level of verb-noun combinations have also been proposed."}, {"heading": "3.2. Study of an Ongoing Project: MWEToolkit", "text": "Multiword Expression Toolkit (mwetoolkit) was developed for type and language-independent MWE identification [4]. It is a hybrid system for recognizing multiwords from a body that uses rules-based as well as statistical association measures. It is open source software that can be downloaded from sf.net / projects /.41."}, {"heading": "3.2.1 MWEToolkit System Architecture", "text": "Faced with a text corpora, the toolkit filters out the MWE candidates from the corpora. The various phases available in the toolkit to achieve this goal are: 1. Preparing the corpus: Preparing the corpus for lowercase conversion, lemmatization, and POS tagging (with tree tagger).2. Extract \"ngrams\" depending on the predefined POS patterns. 3. For each of these bigrams, their number of corpses and the number of web pages (number of pages on which the respective bigram is present) are taken into account using Google and Yahoo4. Use some association measures (statistically) to filter out the candidates. \u2212 i. The corpus containing the N-word tokens is indexed, and the token counts (number of tokens) are estimated from this index. Using the index, individual word counts (w1), c (w2), wc (wc)... wwn (total number) and 1wn (1) can be calculated."}, {"heading": "3.2.2 Using Web as corpora", "text": "Another novel aspect of the system is that it uses the web number of MWEs as a feature of its machine learning model. Let's take a closer look and analyze the advantages and disadvantages of using the web as a corpus. Problems: The web number is \"estimated\" or \"approximate\" as the number of pages, whereas standard corpus numbers are the exact number of occurrences of the n-gram. In the web number, the occurrences of an n-gram are not exactly calculated in relation to the occurrences of the (n \u2212 1) -grams from which it is composed. For example, the n-gram \"the man\" can appear on 200,000 pages, while the words \"the\" and \"the man\" appear on 1,000,000 and 200,000 pages respectively, implying that the word \"the human\" does not appear with any word other than \"the.\" Unlike the size of a standard corpus, which can be easily assembled, it is very difficult to estimate how many pages exist on the web and especially because this number is a problem."}, {"heading": "4.1. System Overview", "text": "The overview of the MWE extraction pipeline is shown in Figure 4.1. The POS-marked corpus is fed to the system. Regular Expression (RegEx) filter filters out the MWE candidates on the basis of pre-defined regular expression patterns. The Partial Reduplication Filter is used to detect occurrences of both significant and non-significant partial reduplication. The filtered candidates are passed to the linguistic filter, which itself consists of three filters, namely Vector Verb & Verbalizer Filter, Named Entity Filter and Syllabic Separation Filter.o If the candidate belongs to the Verb + Verb Category or Noun + Verb Category, the candidate is passed to the very first filter of the linguistic filter, namely Vector Verb & Verbalizer Candidate Filter, Named Entity Filter and Syllabic Separation Filter Entity Filter. To remove the filter of the Candidate Filter of the Candidate Category Entity Filter, the Candidate Filter of the Candidate is passed to the Candidate Filter of the Candidate Filter of the Candidate. To remove the filter of the Candidate of the Candidate of the Candidate of the Candidate Filter of the Candidate of the Candidate of the Candidate of the Candidate Filter of the Candidate."}, {"heading": "4.1.1 System Features", "text": "1. This is a fully multilingual system that requires only raw text as input and can extract the multilingual expressions from the text.2. It is highly scalable for large amounts of data without requiring large amounts of memory, using Lucene's index as its data structure.3. A multilingual web-based service has been developed, hosted at IITBombay, to facilitate the use of this engine by all research groups throughout India."}, {"heading": "4.1.2 Integrated Resources", "text": "Linguistic Resources: Lists of vector verbs, verbalizers and named entities are required for each language to apply the linguistic filter. WORDS: The system is integrated with Princeton WORDS (English) and Indo WORDS (Indian languages) to detect semantic relationships between the components of a candidate."}, {"heading": "4.2. MWE Extraction Engine Pipeline", "text": "In this section, the different stages of the pipeline are explained in detail, the required input for each stage, the output produced at the end of that stage and its functionalities are analyzed."}, {"heading": "4.2.1 Regular Expression Filter", "text": "This is the first filter in the MWE extraction pipeline. It takes the POS marked data as input and searches for some predefined patterns. Since multi-word expressions are inherently very heterogeneous, the goal of this step is to generate as many candidates as possible. As this is the first filter of the motor, the goal here is to reduce the false negatives as much as possible."}, {"heading": "4.2.1.1. Reduplication", "text": "47Patterns: word1 _ POS1 word2 _ POS2 where word1 = word2 Example: Knock _ VB knock _ VBGloss: Knocking on the door"}, {"heading": "4.2.1.2. Compound Noun", "text": "Pattern: word1 _ noun word2 _ noun, where noun = NN | NNP | NNC | NNPC Example: railway _ NN station _ NNGloss: railway stationund: railway station\u043e\u0442 _ NN \u0435\u043d\u044b\u0448\u043e\u0431\u043d\u0435\u0435 _ NN (Hindi) Transliteration: Shiva mandirGloss: Shiva templeTranslation: Temple of Shiva"}, {"heading": "4.2.1.3. Compound Verb", "text": "Composite verbs are formed when two verbs occur one after the other in a sentence. Pattern: word1 _ verb word2 _ verb, where verb = VB | VBD | VBG | VBN | VBZ | VBZ | VM Example: HB-verb word2 _ verb, where verb = VB | VBD | VBN | VBZ | VM Example: HB-VM (Hindi) Transliteration: chala gayaGloss: disappeared"}, {"heading": "4.2.1.4. Conjunct Verb", "text": "Syntactically conjunctive verbs are noun + verb combinations. Pattern: word1 _ noun word2 _ verb Example: approximativityFor trigrams, quadragrams, pentagrams: salaah denaGloss: advice giveTranslation: to give adviceFor trigrams, quadragrams, pentagrams:"}, {"heading": "4.2.1.5. Noun Compounds", "text": "Noun combinations can merge and form a new, larger noun combination. Therefore, all noun combinations have been considered. Example: Science Fiction Author"}, {"heading": "4.2.1.6. Adjective + Noun Compounds", "text": "It has been observed that a noun, preceded by an adjective, is very often a terminology or a collocation. Example: red blood cell"}, {"heading": "4.2.2 Linguistic Filter", "text": "Linguistic Filter uses linguistic knowledge to filter out the MWE candidates. These filters are created on the basis of language observations and MWE analysis and are therefore language-dependent."}, {"heading": "4.2.2.1. Vector Verb + Verbalizer Verb Filter", "text": "The linguistic composition of compound verb and conjunct verb is compound verb = verb 1 + verb 2 = polar verb + vector verb as well as vector verb: (polar verb) + (vector verb) Transliteration: has uthna Translation: laughter gloss: laugh upConjunct verb = noun + verb = noun + verbalizer verb + verbalizer verb: (noun) + (verbalizer) Translation: salah dena Translation: consulting gloss: advice both verb, vector verb and verbalizer verb, are limited in number for Indian languages. In the case of a compound verb and conjunct verb candidate to be an MWE, its vector verb or verb must be removed from the list of vector verbs or verbalizer verbs defined in the language."}, {"heading": "4.2.2.2. Named Entity Filter", "text": "Named entities are a very important part of MWEs. As we switch from Bi-gram to Pent-gram in Indian languages, the likelihood of MWE candidate being a Named Entity increases, and most of the Quad Gram and Pent Gram candidates that are extracted are Named Entities. Example: Indira Gandhi International Airport Bi-Gram and Tri-Gram - these Named Entity candidates cause many false positive candidates. Since Named Entity candidates are nouns + noun combinations that are combined with the other nouns that come immediately after them or just before them, false positive candidates for MWE arise. Named entities filter out the candidates that have a part of a Named Entity in them. A lower score is given to these words during the ranking. The purpose of the Named Entity filter is to filter out false positive candidates caused by Named Entities and give them a lower weight during the filtering of the Rankings."}, {"heading": "4.2.2.3. Hyphenation Filter", "text": "50 Example: Hyphens Translation: Chatur-Chalak Gloss: smart-clever Translation: smartThese words are considered as a single word by the RegEx filter, but are mainly MWEs that, in combination with the other words, generate false positive applications. The motivation of this filter is to filter the words with hyphens in them to reduce false negatives, since most words with hyphens are the MWEs. These words are given a higher weight age during the ranking, all three of which form the linguistic filter that reduces the number of false positive MWE candidates and provides MWE candidates who have a high probability of being an MWE."}, {"heading": "4.2.3 Complex Predicate Filter", "text": "A complex predicate is a verb, an adjective, or an adverb, followed by a single verb that behaves like a single unit of the verb. \"Most of the work done in the literature to recognize CPs is language-dependent and uses a list of light associations for the specific language.\" We use a fully automated, language-independent methodology based on a lexical resource (IndoWordation) to recognize conjunctural verbs. \"Complex predicates are abundant in Indo-Aryan languages and their characteristics are also similar. The method described in this section is generic and is suitable for all of these languages."}, {"heading": "4.2.4 Semantic Filter", "text": "This module is used to check the semantic relationship between the components of a bigram. WORDS is used as the primary resource.For a bigram, we search the WORDS of that language to look for any synsets that contain the words. After retrieving the synsets, we check if there is a synonymous, antonymous relationship between them. To check whether the words belong to the same class or not, we check if they are sister words, i.e. if they share a common direct hypernym."}, {"heading": "4.2.4.1. Synonym", "text": "(Hindi) translation: rishte naate Gloss: relationship connection"}, {"heading": "4.2.4.2. Antonym", "text": "amp # 160; amp # 160; amp # 160; amp # 160; amp # 160; amp # 160; amp # 160; amp # 160; amp # 160; amp # 160; amp # 160; & # 160; & # 160; & # 160; & # 160; & # 160; & # 160; & # 160; & # 160; & # 160; & # 160; & # 160; & # 160; & # 160; & # 160; & # 160; & # 160; & # 160; & # 160; & # 160; & # 160; & # 160; & # 160; & # 160; & # 160; & # 160; & # 160; & # 160; & # 160; & # 160; & # 160; & # 160; & # 160; & # 160; & # 160; & # 160; & # 160;"}, {"heading": "4.2.4.3. Sister Words", "text": "Hi-Chi-Chi (Hindi) translation: bhai bahen Gloss: brother sister translation: Everyone54"}, {"heading": "4.2.5 Partial Reduplication Filter", "text": "It is the case in which a word is partially duplicated. - It can be further divided into two categories. - It is the case in which a word is partially duplicated. - It can be divided into two categories. - It is the case in which a word is partially duplicated. - It is the case in which a word is partially duplicated. - It is the case in which a word is partially duplicated. - It is the case in which a word is divided into two categories. - It is the case in which both words have a matching suffix (which implies that they form a rhyming pair) and both words are meaningful. - It is the case in which the words exist in a lexical database (such as, wordnet). Ideally, words that form a meaningful partial reduction, but it is difficult for a POS tagger to identify such expressions correctly. - If we include the restriction to POS tags, it reduces the abbreviation from the tag."}, {"heading": "4.2.6 Statistical Filter", "text": "We use statistical methods to detect collocations. Collocations are the type of multiword expressions that are statistically peculiar to a language and are not subject to any linguistic rule. We use three statistical methods to detect collocations, namely the bi-directional log likelihood ratio 56 cube coefficient Normalized Point-wise Mutual Information."}, {"heading": "4.2.6.1. Normalized Point-wise Mutual Information", "text": "Point-wise Mutual Information is used to score the MWE candidates on the basis of the shared information is calculated using the PMI formulas mentioned below: (,) = () * Where, I (w1, w2) is the amount of mutual information between words w1 and w2, p (w1w2) is the probability of w1 and w2 occurring together, p (w1) * p (w2) is the probability of words occurring in the corpus independently of each other. PMI is roughly a measure of how much one word tells us about the other and gives an idea of the \"dependence\" between words. If we look more closely at the formula, we will see that the two words completely depend on each other i.e. w1 occurs only when it is followed by w2, the PMI formula reduces to, () () ()."}, {"heading": "4.2.6.1.5. Observation Results for NPMI", "text": "English Corpus Hindi Corpus Bengali Corpushelter skelter. English Corpus Hindi Corpus Bengali Corpushelter skelter. English Corpus Corpus Hindi Corpus Corpus Bengali Corpushelter skelter, English Corpus Corpus Corpus Hindi Corpus Corpus Bengali Corpushelter skelter, English Corpus Corpus Corpus Corpus Corpus Bengali Corpushelter skelter, English Corpus Corpus Corpus Corpus Corpus Corpus Corpus Corpushelter skelter."}, {"heading": "4.2.6.2. Bi-Directional Log-Likelihood Algorithm", "text": "Log likelihood ratio (LLR) is a hypothesis used to test whether the constituent words of an expression are interdependent or independent. Two hypotheses (independence and dependence) are taken into account for the frequency of occurrence of a bigram (w1, w2): Hypothesis 1: P (w2 | w1) = p = P (w2 | w1) Hypothesis 2: P (w2 | w1) = p1 p2 = P (w2 | w1) Hypothesis 1 indicates the probability of occurrence of w2 independently of w1, whereas Hypothesis 2 measures the probability of occurrence of w1. The ratio of these two probabilities tells us which hypothesis is more likely.) () (21 22 HLHLLLlobell The problem with the prior art is that it only takes into account the dependence of these two probabilities on formula 1 and not on dependence of 1 (1)."}, {"heading": "4.2.6.2.1. Bi-gram Bi-directional Log-Likelihood Ratio", "text": "W1w2 should be the bi-gram, the probabilities p1, p2, p3 and p4 are calculated as follows: = (|) 59 = (| ~) = (|) = (| ~) Hypothesis 1: p1 = p2 Hypothesis 2: p1 \u0445 p2 Hypothesis 3: p3 = p4 Hypothesis 4: p3 \u0445 p4 The log likelihood value is calculated as follows: = () () + () ()) / 2We expand the formulas of (n-1) gram to find the value for n-gram."}, {"heading": "4.2.6.2.2. Tri-gram Bi-directional Log-Likelihood Ratio", "text": "W1w2 w3 is the tri-gram, the probabilities p1, p2, p3 and p4 are calculated as follows: = (|) = (| ~ ()) = (|) = (|) = (|) = (| ()) The log likelihood score is calculated as follows: = (() () + () ()) / 2"}, {"heading": "4.2.6.2.3. Quad-gram Bi-directional Log-Likelihood Ratio", "text": "Let w1w2 w3 w4 be the quad gram, the probabilities are calculated as follows: = (|) = (| ~ ()) = (|) = (|) = (|) = (| ~ ()) So the log likelihood value is calculated as follows: = () () + () ()) / 2"}, {"heading": "4.2.6.2.4. Pent-gram Bi-directional Log-Likelihood Ratio", "text": "Let w1w2 w3 w4 w5 be the pentagram, the probabilities p1 and p2 are calculated as follows: 60 = (|) = (| ~ ()) = (|) = (|) = (|) = (| ~ ()) The log likelihood score is calculated as follows: = (() () + () ()) / 2"}, {"heading": "4.2.6.2.5. Observation Results for Bi-directional Log-Likelihood Ratio", "text": "Table 4.3 shows the 10 best candidates extracted from the English, Hindi and Bengali corpus using the bidirectional LLR method."}, {"heading": "4.2.6.3. Dice Coefficient", "text": "The S\u00f8rensen cube index or cube coefficient is a statistic to compare the similarity of two samples. We use cube coefficients to detect collocations in the following way: 61 = () () + () Where, c (w1w2) the number (frequency) of the bigram w1w2 in the corpusc gives (w1) the number w1 and c (w2) the number w2 in the corpus."}, {"heading": "4.2.6.3.1. Dice Coefficient for Bigrams", "text": "For two words w1 and w2 in the corpus, their cube coefficient is calculated as follows: = () () + () Where c (w1w2) equals the number (frequency) of the bigram w1w2 in the corpus, c (w1) equals the number w1 and c (w2) equals the number w2 in the corpus"}, {"heading": "4.2.6.3.2. Dice Coefficient for Tri-grams", "text": "For three words w1, w2 and w3 in the corpus, their cube coefficient is calculated as follows: = () () + () Where c (w1w2 w3) is the number (frequency) of the trigram w1w2w3 in the corpus c (w1 w2) is the number of the bigram w1 w2 and c (w2 w3) is the number of the bigram w2w3 in the corpus"}, {"heading": "4.2.6.3.3. Dice Coefficient for Quad-grams", "text": "For four words w1, w2, w3 and w4 in the corpus, their cube coefficient is calculated as follows: = () () + () Where c (w1w2 w3 w4) is the number (frequency) of the quadrant w1w2w3w4 in the corpus c (w1 w2 w3) is the number of the trigram w1 w2 w3 and c (w2 w3 w4) is the number of the trigram w2w3 w4 in the corpus"}, {"heading": "4.2.6.3.4. Dice Coefficient for Pent-grams", "text": "For five words w1, w2, w3, w4 and w5 in the corpus, their cube coefficient is calculated as follows: = () () + () Where c (w1w2 w3 w4 w5) the number (frequency) of the accumulated gram w1w2w3w4 w5 in the corpus62c (w1 w2 w3 w4) the number of the quadrant w1 w2 w3 w4 and c (w2 w3 w4 w5) the number of the quadrant w2w3 w4 w5 in the corpus"}, {"heading": "4.2.6.3.5. Observation Results for Dice Coefficient", "text": "Table 4.4 shows the 10 best candidates extracted from the English, Hindi, Bengali corpus using the cube coefficient method."}, {"heading": "4.2.6.4. Combining Different Filter\u2019s Scores", "text": "In order to combine the values of PMI, log likelihood and cube coefficient, their independent values are first normalized, with normalization occurring in an interval of zero to one, dividing the score of each candidate by the maximum score achieved in the respective algorithm. After normalization, all three values are added for each MWE candidate, and the list is sorted by combination to generate a combined leader.63"}, {"heading": "4.2.7 Manual Evaluation", "text": "This is the final phase of the pipeline. Lexicographers evaluate the generated ranking to determine whether a candidate is really MWE or not. False positives are discarded by the lexicographers and the true MWEs are added to the dictionary. There is also an option for lexicographers to detect false negatives and add them to the dictionary. Therefore, the MWEs added to the dictionary are all validated by lexicographers and can thus serve as the gold standard."}, {"heading": "4.2.8 Universal Web Service", "text": "The MWE engine is an offline Java application that had to be installed and used locally. In collaboration with Goa University, a multilingual web service has been developed to develop an online interface to use the engine hosted at IIT Bombay. This web service will be available to all research groups across India. Different language groups can upload their corpus and query the MWE extraction engine to extract MWE candidates from it. Results produced by the MWE extraction engine can be validated later by them. In the future, there will also be arrangements for lexicographers to be able to enter the true meaning of a non-compositional MWE so that these MWE can be stored in lexical databases. Depending on the true meaning of an MWE, the expression should be associated with the appropriate synset of ordinances."}, {"heading": "5.1. Using Parallel Corpora", "text": "Multiword expressions are idiosyncrasies of a language or a conventional way of expressing things in a particular language. It is quite fascinating to see how multiword expressions behave in different languages. Whether a multiword expression is translated into a single word in another language, or whether it has a literal translation of its components, or whether, in the absence of such a concept in the other language or its corresponding expression in the other language, it is also a MWE!"}, {"heading": "5.1.1 Multilingual Aspects of Multiword Expressions", "text": "We examined an English-Hindi parallel corpus and found the following types of transformations that perform multiwords in different languages: English Expression Hindi Expression Transliteration GlossTranssliterationSocial Services, an organized activity to improve the situation of disadvantaged people in the societyService Record RecordExpansionDay Care, short: ExpansionDay Care."}, {"heading": "5.1.2 Motivation", "text": "To store an expression in a lexical database, we need both the expression and its meaning. Manually adding all the multiword expressions of a language to a resource is both expensive and time-consuming. However, extracting such expressions from the corpus is a challenge, and the \"automatic understanding\" of their meaning is quite trivial. The motivation behind using parallel corpora is to extract multiword expressions of a language using another language, and also to retrieve the meaning of the expression from parallel corpora. The underlying assumption of alignment approaches in a language to MWE extraction is that MWEs are aligned across languages in a way that is different from compositional expressions. Word alignment can be used in many ways to detect word alignments that are literally not aligned to the 1 source: MWWE extraction is the multiword length that is not aligned to the 1 in general."}, {"heading": "5.1.3 Automatically adding MWEs to Wordnet", "text": "Our primary motivation behind the use of the word alignment is to automatically retrieve the meaning of the expression and store it accordingly in a lexical resource. Consider the following example: The automatic translation of this expression into the word network: dam toda Gloss: breath break Translation: the \"automatic translation\" is an idiom in Hindi meaning \"die.\" If we want to store this expression in the word network, we should store it in the synset of \"math\" (marna), which also means \"die.\" In the parallel corpora, if we have a pair of sentences, such as the death of his grandfather, we must store it in the word network."}, {"heading": "5.1.4 Case Study", "text": "In this section, we present a case study we conducted during our investigation of the multilingual aspects of mwe. We present the steps we followed, our observation and the conclusion we drew from the experiment. 1) Consider a Hindi-English parallel corpora 2) Run the MWE extraction machine (described in Chapter 4) on one page. 3) Run an automatic word alignment machine to align parallel sentences 4) Call the translations (marked by word alignment) of the multiword expressions (extracted by the MWE extraction machine) We performed the above experiment by running the MWE extraction machine on the English side of the company and examined the Hindi translation equivalents of the English \"collocations.\" Below are the results from the 100 best collocations in English: English Collocation Hindi Expression TranssliterationCollocation - > Single Word (20% of the company's translation engine)."}, {"heading": "5.2. Investigating Sanskrit Traditions", "text": "Since Sanskrit is a very old and grammatically enriched language, there are many rules when and why words combine to form a new expression. There is a concept of \"samasa,\" in which words combine under certain conditions and the resulting expression acquires an exocentric meaning, i.e. a meaning that is completely different from its constituent words."}, {"heading": "5.2.1 \u092c\u0939\u0941 \u0940 \u0939: \u0938\u092e\u093e\u0938\u0903 (Exocentric Compounds)", "text": "This is similar to the case of non-compositional multiwords due to the fact that the meaning of the resulting expression lies outside the meaning of the constitutional words. This is also referred to as exocentric compounds. Syntactically, this category is also known as non-compositional multiwords (BahubrihiSamasa), which consist of two types: 69, that the meaning of the resulting expression lies outside the meaning of the constitutional composition. (Samanadhikarana Bahubrihi Bhavi Bhavi Bhavi Bhavi Bhavi Bhavi Bhavi Bhavi Bhavi Bhavi Bhavi Bhavi Bhavi Bhavi Bhavi Bhavi Bhavi Bhavi Bhavi Bhavi Bhavi Bhavi Bhavi Bhavi Bhavi Bhavi Bhavi Bhavi Bhavi Bhavi Bhavi Bhavi Bhavi Bhavi Bhavi Bhavi Bhavi Bhavi Bhavi Bhavi Bhavi Bhavi Bhavi Bhavi Bhavi Bhavi Bhavi Bhavi Bhavi Bhavi Bhavi Bhavi Bhavi Bhavi Bhavi Bhavi Bhavi Bhavi Bhavi Bhavi Bhavi Bhavi Bhavi Bhavi Bhavi Bhavi Bhavi Bhavi"}, {"heading": "5.2.3 \u0924 \u092a\u0941 \u0937\u0903 \u0938\u092e\u093e\u0938\u0903 (Determinative Compounds)", "text": "The meaning of the compound is endocentric, i.e. compositional, and can be derived from the meaning of the constituents. The constituents of the compound must be in the same inflective case, i.e. they must be in the same case, i.e. in the same case (Prathama). Example: o-ergo-adhanam gloss: (Sanskrit) Transliteration: nilmegh gloss: blue cloud gloss: vidyadhanam gloss: (Sanskrit) Trans literation: vidyadhanam gloss: (Sanskrit) Trans literation: vidyadhanam gloss: (Sanskrit) 71"}, {"heading": "5.2.4 \u0935 \u0935 (Coordinating compounds)", "text": "Apart from Sanskrit, Dvandvas are widely used in some languages such as Chinese, Japanese and some modern Indian languages such as Hindi and Urdu, but less common in English. [30] Apart from the Dvandva connections in Sanskrit, they can largely be divided into the following types: The last members govern the sex and the influences on the entire composition. [30] Apart from the Hindi Indian languages (Itaretara dvandva), this is an enumerative compound word whose meaning refers to all its constituent members; the last member governs the sex and the influences on the entire composition. Example: The Indian language (Itaretara dvandva) is a composite word: Rama-Lakshmanau Gloss: Rama and Lakshmana transliteration (Sanskrit)."}, {"heading": "6.1. Evaluation of MWE engine on English Corpus", "text": "The size of the corpus is 440,1696 words. We first applied the Regular Expression Filter to the English corpus to narrow down these words by selecting only patterns that can form multi-word expressions. We skip the filters of reduplication, partial reduplication, and complex predicates, as this phenomenon is typical of Indian languages and is not observed in English. We apply the semantic filter, which is based on Princeton Words, to detect semantic relationships, and then statistical filters to determine collocations.74 Table 6.2 shows the precision values of semantics, hyphenation, and statistical filters.The statistical methods produce rankings of collocations. We evaluated the 200 best candidates for each method and showed average precision. A comparative evaluation of the three different statistical metrics was created in Table 6.3, which shows the variation of the precision values more accurately than the size of the candidates. We have evaluated the 200 best candidates for each method and shown average precision, and a comparative evaluation of the three different statistical metrics was created in Table 6.3, which shows the variation of the precision values of the candidates as the size of collocations."}, {"heading": "6.2. Evaluation of MWE engine on Hindi Corpus", "text": "The Hindi corpus used for the evaluation is parallel to the English corpus, whose evaluation was presented in the previous subsection, and thus belongs to the same domain and has 4605343 words. First, a regular expression filter is used to narrow down the candidates. Reduplication filters and partial reduction filters are used, followed by the hyphenation and semantic filters. The complex predicate filter is used to detect conjunction verbs. Finally, statistical measures are used to detect collocations. 75Table 6.4 shows the precision values of semantics, hyphenation and statistical filters. The statistical methods produce rankings of collocations. We evaluated the 200 best candidates for each method and showed the average precision. A comparative evaluation of the three different statistical measures was made in Table 6.5, which shows the variation of precision values as the increase in candidate size.Table 6.6 shows the accuracy of the verb determinations of more complex predicates, as there are actually few verbs that can be found in verbs."}, {"heading": "6.3. Evaluation of MWE engine on Bengali Corpus", "text": "The Bengali corpus used for evaluation has 1891 words. First, a regular expression filter is used to narrow down the candidates. Reduplication filters and partial reduplication filters are used, followed by the hyphenation filter and semantic filters. Finally, statistical measures are used to detect collocations. Table 6.7 shows the precision values of reduplication, partial reduplication, semantics, hyphenation and statistical filters. Statistical methods produce rankings of collocations. We have evaluated the 200 best candidates for each method and have shown the average precision. A comparative evaluation of the three different statistical measures was made in Table 6.8, which shows the variation of precision values as candidate size. As we can see, the bidirectional log-likelihood ratio for each method works slightly better than other two methods for Bigram's universality, while Common-Prognosis-Diversity-Promising Universality Universality is Universatile."}, {"heading": "7.1. Motivation", "text": "Lexical resources play an important role in the processing of natural languages. In recent decades, there has been an enormous growth in the development of lexical resources such as WordPress, Wikipedia, Ontologies, etc. These resources vary considerably in structure and representation formalism.In order to develop applications that can use different resources, it is essential to combine these heterogeneous resources and develop a common framework of representation.However, differences in the coding of knowledge and multilingualism are the most important obstacles to the development of such a framework. Especially in a multilingual country such as India, information is available in many different languages. In order to exchange information across cultures and languages, it is essential to create architecture to share different lexical resources in different languages. In this report, we present IndoNet1, a lexical resource created by merging word networks from 18 different Indian languages."}, {"heading": "7.2. Related Work", "text": "This year it is more than ever before."}, {"heading": "7.3. IndoNet", "text": "IndoNet uses a common concept hierarchy to link different heterogeneous lexical resources. As shown in Figure 7.1, concepts from different word networks, the Universal Dictionary and the Upper Ontology merge into a common concept hierarchy. Figure 7.1 shows how concepts from English WordNet (EWN), Hindi Wordnet (HWN), Upper Ontology (SUMO) and the UniversalWord Dictionary (UWD) are linked by a common concept hierarchy (CCH). In this section, details of the Common Concept Hierarchy and LMF coding for different resources are explained."}, {"heading": "7.3.1 Common Concept Hierarchy (CCH)", "text": "An element of a common concept hierarchy is defined as < sinid _ 1, sinid _ 2,..., uwid, sumoid > where sinid _ i is the synset ID of the word it wordnet, uw _ id is the universal word ID, and sumo _ id is the SUMO designation of the concept. Unlike ILI, hypernymia-hyponymy relationships are merged from different 81 wordnets to construct the concept hierarchy."}, {"heading": "7.3.1.1. LMF for Wordnet", "text": "However, IndoWORDS encodes more lexical relationships than EuroWORDS. We have expanded the WORDS-LMF to include the following relationships: antagonism, gradation, hypernymia, meronym, troponymy, entropy, and cross-section of language connections for skills and abilities."}, {"heading": "7.3.1.2. LMF for Universal Word Dictionary", "text": "A universal word consists of a keyword and a list of constraints that provide a unique meaning for the UW. In our architecture, we allow each sense of a keyword to have more than one set of constraints (defined by different UW dictionaries) and are associated with multiple-language lemmas with a confidence value, allowing us to merge multiple UW dictionaries and render them in LMF format. We introduce four new LMF classes: Restrictions, Restriction, Lemmas and Lemma, and add new attributes, keywords and mapping scores to existing LMF classes. Figure 7.2 shows an example of the LMF representation of the UW Dictionary. Currently, the dictionary is created by merging two dictionaries, UW + + [3] and CFILT Hin-UW. Lemms from different languages are mapped to universal words and stored under the Lemmas class 82."}, {"heading": "7.3.1.3. LMF to link ontology with Common Concept Hierarchy", "text": "Figure 7.3 shows an example of an LMF representation of CCH. The interlingual pivot point is represented by SenseAxis. Concepts in different resources are linked to the SenseAxis in such a way that concepts linked to the same SenseAxis transport the same sense.Using the LMF class Monolingual ExternalRefs, ontology can be integrated with a monolingual lexicon. To share an ontology among multilingual resources, we modify the original core package of LMF. As shown in Figure 7.3, a SUMO term is divided into multiple lexicographs via SenseAxis. SUMO is linked to a concept hierarchy using the following relationships: antonym, hypernym, instance, and equivalent.83 To support these relationships, a Reltype attribute is added to the interlingual sense class.83"}, {"heading": "7.4. Observation", "text": "The terminology hierarchy contains 53848 terms, which are divided among the words of Indian languages, SUMO and Universal Word Dictionary. Of the total of 53848 terms, 21984 are associated with SUMO, 34114 with HWN and 44119 with UW. Among these 12,254 terms are between UW and SUMO and 21984 12,254 with WORDS and SUMO.POS HWN UW SUMO CCH Adjective 5532 2865 3140 5193 Adverb 380 2697 249 2813 noun 25721 32831 16889 39620 Verb 2481 5726 1706 6222 Total 34114 44119 21984 53848 Table 7.1: Statistics of the concept Linkages84This generates a multilingual semantic lexicon, which covers the semantic relations between terms of different languages."}, {"heading": "7.5. Conclusion", "text": "We have presented a multilingual lexical resource for Indian languages. The proposed architecture addresses the \"lexical gap\" and \"structural divergence\" between languages by building a common concept hierarchy. To encode this resource in LMF, we have developed standards to represent the UW in LMF.IndoNet as the largest multilingual resource covering 18 languages from 3 different language families, and it is possible to link or merge other standardized lexical resources with it. As the Universal Word dictionary is an integral part of the system, it can be used for UNL-based machine translation tasks. Ontological structure of the system can be used for multilingual information retrieval and extraction. In the future, we aim to address ontological problems of the common concept hierarchy and integrate domain ontologies with the system."}, {"heading": "8.1. Construction of the Generic Stemmer", "text": "In this section we discuss the construction of the tribe. IndoWordsis a lexical database for 18 Indian languages. We use the words from wordnet as a dictionary to search for a lemm.When an input word is found, we follow the following steps to search for its tribe and lemmings: i) Read the input word and the input language ii) Search for the word in the word network of the corresponding language iii) When the word is found in the wordneta. Type the word both as a lemm. and as a trunk b. Exitive) Search for the string with maximum match in words. a. Output maxMatch as a trunk b. Find all the words that have maxMatch as a prefix and type them as a lemm.) Ask the user if the output is correct. If so, then leave b. Otherwise, remove the last character from maxMatch and go to step ia.86."}, {"heading": "8.1.1 Storing Wordnet in a Trie", "text": "Searching for words in WordsWords requires storing them in the memory of the database in order to work efficiently. Since WordsWords is a huge lexical database containing thousands of words, it is very inefficient to read the database files for each word several times."}, {"heading": "8.1.2 Why Trie?", "text": "In computer science, a trie, also called a digital tree or prefix tree, is an ordered tree data structure that is used to store a dynamic set in which the keys are usually strings. [23] Unlike a binary search tree, no node in the tree stores the key associated with that node; instead, its position in the tree defines the key to which it is linked. All the descendants of a node have a common prefix of the string associated with that node, and the root is associated with the empty string. Values are usually not assigned to each node, only with leaves and some inner nodes corresponding to the keys of interest.The term trie comes from the retrievaluation. Trie is a very useful data structure for retrieving the string. Looking up data in a trie is faster, O (m) time (where m is the length of a search string), compared to an imperfect hash table."}, {"heading": "8.1.3 Structure of the Trie", "text": "The words from the word network are stored in a data structure, together with their part-of-speech tag and the synset id.87 Figure 8.1 shows an example of how the words are stored in the word mesh. Each leaf node contains a Boolean marker to mark \"end of the word.\" A non-leaf node could also be an \"end of the word,\" as shown in Figure.The nodes that set the EOW flag to \"true\" also store the part-of-speech tag and the synset ID (s) of the word. For example, if we consider the entered word as \"end of the word\" (chalti), we start searching from the root of the word and try to assign a character at each level. If we look at the snippet of the trick shown in Figure 8.1, we fit the character... \""}, {"heading": "8.2. GUI of the Stemmer", "text": "In order to facilitate easy use of the trunk, it is equipped with a graphical user interface developed using the Java Swing API. Integrated with IndoW, the system can act as a generic family tree for 18 Indian languages, allowing the user to select the input language from a drop-down menu of all languages and enter an input word. The system displays the family tree and the lemmings for the input word in different text fields and asks the user if the correct family tree and problem have been found. If the user believes that the output is incorrect, he can trace back his feedback via the radio buttons and the system and generate a new set of results by going one level into the depth of the trunk. This is a completely user-driven system and the tracing mechanism can continue until the system reaches the root of the trie.Figure 8.2 shows a screenshot of the GUI of the generic parent language. \"The input language is...\""}, {"heading": "8.3. Integration with Multilingual IndoWordnet Search", "text": "The generic family tree that has been developed can not only serve as a stand-alone system, but can also be integrated with a number of applications to increase its efficiency and ease of use. Since the entire word network is stored in a three-dimensional data structure, this library can very efficiently serve as a spell checker or automatic completion tool for word network search. Indian languages are rich in morphology, but morphological variations are not stored in the word network. It is not possible to store all the morphological variations of each word in the word network, but many applications may require the search for a transformed word (e.g.: an application that reads a corpus and uses word networks for some analysis of the words).90There is a multilingual web service for searching for Indo WORDS. We have integrated our system into the web service to provide automatic word suggestions. When a word is entered for search, it is searched in the trie.If the word exists in WORD, a search is automatically found, a search for a complete match of the word is made, and a search for the match of the page it."}, {"heading": "9.1. Conclusion", "text": "Through this report, we have established the term Multiword Expressions and the meaning of these terms in all areas of Natural Language Processing. In this paper, we have outlined a thorough literature review. We have discussed various definitions of MWE Expressions given by different researchers, and their types and characteristics. We have also mentioned some of the outstanding MWE Extraction Approaches carried out by different researchers. There is an ongoing project in research laboratories for MWE Extraction, we have described such a system in detail. We have presented the MWE Extraction Machine developed at IIT Bombay. It has a pipeline consisting of different filters to detect MWE from a corpus. For MWE Extraction, a web service is being developed that operates with the engine working in the backend. The web service is located on a fully multilingual platform where various language research groups in India can upload their corpus and extract MWE's. It will also help us to create the Gold Standard, which will also be used to lexWE Standard."}, {"heading": "9.2. Future Work", "text": "The future work of the project was divided into three sub-sections depending on the complexity and importance for the project."}, {"heading": "9.2.1 Short Term Future Work", "text": "As already mentioned, multiword expressions must be stored in lexical resources. In the case of non-compositional multiwords, it is extremely difficult (if not impossible) to recognize the meaning automatically. In the case of manual validation, we must store the meaning of such expressions so that we can associate them with the corresponding synsets of words."}, {"heading": "9.2.2 Medium Term Future Work", "text": "To detect such a pattern, we need some syntactical information, such as phrase boundaries in sentences. We plan to integrate a chunker with the existing model to detect non-contiguous multiword expressions."}, {"heading": "9.2.3 Long Term Future Work", "text": "Building a Machine Learning Classification Model The recognition of MWEs can be considered a classification problem that can be solved by a standard machine learning model, as it has a range of features and training data. To train such a model, we need some high-quality gold-standard training data that is not yet available. We hope that with the web service we will be able to collect manually annotated data from across the nation and thus develop a machine learning model that is linked to this pipeline.93"}], "references": [{"title": "July) LinkedData", "author": ["Tim Berners-Lee"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2007}, {"title": "IndoWordNet", "author": ["Pushpak Bhattacharyya"], "venue": "LREC, 2010.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2010}, {"title": "Using Wordnet for Building an Interlingual Dictionary", "author": ["I Boguslavsky", "J Bekios", "J Cardenosa", "C Gallardo"], "venue": "Fifth International Conference Information Research and Applications, 2007.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2007}, {"title": "mwetoolkit: a Framework for Multiword Expression Identification", "author": ["Ramisch Carlos", "Aline Villavicencio", "Christian Boitet"], "venue": "In Proc. of the Seventh LREC (LREC 2010), 2010.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2010}, {"title": "Identification of Reduplication in Bengali Corpus and their Semantic Analysis : A Rule Based Approach", "author": ["Tanmoy Chakraborty", "Sivaji Bandyopadhyay"], "venue": "Proceedings of the Multiword Expressions: From Theory to Applications, 2010.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2010}, {"title": "Word association norms,mutual information, and lexicography", "author": ["K. Church", "P. Hanks"], "venue": "Computational Linguistics, 1990.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1990}, {"title": "Accurate Methods for the Statistics of Surprise and Coincidence", "author": ["Ted Dunning"], "venue": "Computational Linguistics, 1993.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1993}, {"title": "Automatically Constructing a Lexicon of Verb Phrase Idiomatic Combinations", "author": ["Afsaneh Fazly", "Suzanne Stevenson"], "venue": "EACL, 2006.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2006}, {"title": "WordNet: An Electronic Lexical Database.", "author": ["Christiane Fellbaum"], "venue": "Bradford Books,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1998}, {"title": "Multilingual resources for NLP in the lexical markup framework (LMF)", "author": ["Gil Francopoulo"], "venue": "Language Resources and Evaluation, 2009.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2009}, {"title": "Standardizing wordnets in the ISO standard LMF: Wordnet-LMF for GermaNet", "author": ["Verena Henrich", "Erhard Hinrichs"], "venue": "Proceedings of the 23rd International Conference on Computational Linguistics, COLING, 2010.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2010}, {"title": "Multi-lingual Multiword Expression", "author": ["Munish Minia"], "venue": "DDP Thesis 2012.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2012}, {"title": "Linking Lexicons and Ontologies: MappingWordNet to the Suggested Upper Merged Ontology", "author": ["Ian Niles", "Adam Pease"], "venue": "Proceedings Of The 2003 International Conference  94  On Information And Knowledge Engineering, 2003.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2003}, {"title": "Using conceptual similarity for collocation extraction", "author": ["Darren Pearce"], "venue": "Proceedings of the Fourth annual CLUK colloquium, 2001.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2001}, {"title": "Towards a standard upper ontology", "author": ["Ian Niles", "Adam Pease"], "venue": "Proceedings of the international conference on Formal Ontology in Information Systems, 2001.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2001}, {"title": "Formal ontology as interlingua: The SUMO and WordNet linking project and global wordnet", "author": ["Adam Pease", "Christiane Fellbaum"], "venue": "Ontology and Lexicon, A Natural Language Processing perspective, 2010.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2010}, {"title": "Multiword Expressions in the wild? mwetoolkit comes in handy", "author": ["Carlos Ramischy", "Aline Villavicencio", "Christian Boitet"], "venue": "COLING, 2010.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2010}, {"title": "Multiword Expressions:A pain in the neck for NLP", "author": ["Ivan Sag", "Timothy Baldwin", "Francis Bond", "Ann Copestake", "Dan Flickinger"], "venue": "CICLing. Springer, 2002.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2002}, {"title": "Retrieving collocations from text: Xtract", "author": ["Frank Smadja"], "venue": "Computational Linguistics, 1993.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 1993}, {"title": "Wordnet-LMF: fleshing out a standardized format for wordnet interoperability", "author": ["Claudia Soria", "Monica Monachini", "Piek Vossen"], "venue": "Proceedings of the 2009 international workshop on Intercultural collaboration, 2009.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2009}, {"title": "YAGO: A Core of Semantic Knowledge Unifying Wordnet and Wikipedia", "author": ["Fabian M. Suchanek", "Gjergji Kasneci", "Gerhard Weikum"], "venue": "ACM, 2007.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2007}, {"title": "The UNL- a Gift for the Millenium", "author": ["H. Uchida", "M. Zhu", "T. Della Senta"], "venue": "United Nations University Press, 1999.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 1999}, {"title": "Detecting noun compounds and light verb constructions: a contrastive study", "author": ["Veronika Vincze", "Istvan Nagy T", "Gabor Berend"], "venue": "Proceedings of the Workshop on Multiword Expressions: from Parsing and Generation to the Real World (MWE 2011), 2011.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2011}, {"title": "Challenges for a multilingual wordnet", "author": ["Christiane Fellbaum", "Piek Vossen"], "venue": "Language Resources and Evaluation, 2012.  95", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2012}, {"title": "EuroWordNet: a multilingual database with lexical semantic networks", "author": ["Piek Vossen"], "venue": "1998.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 1998}, {"title": "IndoNet: A Multilingual Lexical Knowledge Network for Indian Languages, Association for Computational Linguistics", "author": ["PUBLICATION Brijesh Bhatt", "Lahari Poddar", "Pushpak Bhattacharyya"], "venue": null, "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2013}], "referenceMentions": [{"referenceID": 18, "context": "7 : Producing concordances for \u201cthe Dow Jones Industrial Average\u201d[20].", "startOffset": 65, "endOffset": 69}, {"referenceID": 18, "context": "8: Producing the \u201cNYSE's composite index of all its listed common stocks \u201c [20] .", "startOffset": 75, "endOffset": 79}, {"referenceID": 13, "context": "9: Collocational Information for 'baggage' and 'luggage'[14] .", "startOffset": 56, "endOffset": 60}, {"referenceID": 17, "context": "[19] \u2022 Idiosyncratic interpretations that cross word boundaries (or spaces) [18] \u2022 Recurrent combinations of words that co-occur more frequently than chance, often with non-compositional meaning[20] \u2022 A pair of words is considered to be a collocation if one of the words significantly prefers a particular lexical realization of the concept the other represents[14]", "startOffset": 76, "endOffset": 80}, {"referenceID": 18, "context": "[19] \u2022 Idiosyncratic interpretations that cross word boundaries (or spaces) [18] \u2022 Recurrent combinations of words that co-occur more frequently than chance, often with non-compositional meaning[20] \u2022 A pair of words is considered to be a collocation if one of the words significantly prefers a particular lexical realization of the concept the other represents[14]", "startOffset": 194, "endOffset": 198}, {"referenceID": 13, "context": "[19] \u2022 Idiosyncratic interpretations that cross word boundaries (or spaces) [18] \u2022 Recurrent combinations of words that co-occur more frequently than chance, often with non-compositional meaning[20] \u2022 A pair of words is considered to be a collocation if one of the words significantly prefers a particular lexical realization of the concept the other represents[14]", "startOffset": 361, "endOffset": 365}, {"referenceID": 17, "context": "The classification as described in [18] is given below.", "startOffset": 35, "endOffset": 39}, {"referenceID": 22, "context": "[25]", "startOffset": 0, "endOffset": 4}, {"referenceID": 4, "context": "Reduplications have been categorized into 2 levels in [5], namely Expression Level and Sense Level.", "startOffset": 54, "endOffset": 57}, {"referenceID": 22, "context": "Detecting noun compounds and light verb constructions The authors have described some rule based methods to detect noun compounds and light verb constructions in running texts [25].", "startOffset": 176, "endOffset": 180}, {"referenceID": 6, "context": "2 Statistical Methods for Multiwords Extraction A number of basic statistical methods can be used for extracting collocations from a given corpus [7] [19].", "startOffset": 146, "endOffset": 149}, {"referenceID": 5, "context": "3 Word Association Measures This is one of the very early attempts at collocation extraction by Kenneth Church and Pattrick Hanks (1990) [6].", "startOffset": 137, "endOffset": 140}, {"referenceID": 18, "context": "4 Retrieving Collocations From Text : XTRACT Frank Smadja has implemented a set of statistical techniques and developed a lexicographic tool, Xtract to retrieve collocations from text [20].", "startOffset": 184, "endOffset": 188}, {"referenceID": 18, "context": "7 : Producing concordances for \u201cthe Dow Jones Industrial Average\u201d[20]", "startOffset": 65, "endOffset": 69}, {"referenceID": 18, "context": "8: Producing the \u201cNYSE's composite index of all its listed common stocks \u201c [20]", "startOffset": 75, "endOffset": 79}, {"referenceID": 13, "context": "5 Collcation Extraction By Conceptual Similarity This is a method suggetsed in [14] where the author uses Wordnet to find out the conceptual similarity between different words.", "startOffset": 79, "endOffset": 83}, {"referenceID": 13, "context": "9: Collocational Information for 'baggage' and 'luggage'[14]", "startOffset": 56, "endOffset": 60}, {"referenceID": 7, "context": "The authors have looked into two closely related problems confronting the appropriate treatment of Verb-Noun Idiomatic Combinations(where the noun is the direct object of the verb) [8]: \u2022 The problem of determining their degree of flexibility \u2022 The problem of determining their level of idiomaticity", "startOffset": 181, "endOffset": 184}, {"referenceID": 3, "context": "Study of an Ongoing Project: MWEToolkit Multiword Expression Toolkit (mwetoolkit) is developed for type and language-independent MWE identification [4].", "startOffset": 148, "endOffset": 151}, {"referenceID": 21, "context": "Universal Word Dictionary[24] and an upper ontology, SUMO[15]", "startOffset": 25, "endOffset": 29}, {"referenceID": 14, "context": "Universal Word Dictionary[24] and an upper ontology, SUMO[15]", "startOffset": 57, "endOffset": 61}, {"referenceID": 12, "context": "Suggested Upper Merged Ontology (SUMO) is the largest freely available ontology which is linked to the entire English WordNet [13].", "startOffset": 126, "endOffset": 130}, {"referenceID": 9, "context": "IndoNet is encoded in Lexical Markup Framework (LMF), an ISO standard (ISO-24613) for encoding lexical resources[10].", "startOffset": 112, "endOffset": 116}, {"referenceID": 8, "context": "Though most of the wordnets are built by following the standards laid by English Wordnet [9], their conceptualizations differ because of the differences in lexicalization of concepts across languages.", "startOffset": 89, "endOffset": 92}, {"referenceID": 15, "context": "[16]", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "79 The challenge of constructing a unified multilingual resource was first addressed in EuroWordNet [27].", "startOffset": 100, "endOffset": 104}, {"referenceID": 23, "context": "While ILI allows each language wordnet to preserve its semantic structure, it has two basic drawbacks as described in [26].", "startOffset": 118, "endOffset": 122}, {"referenceID": 9, "context": "KYOTO project used Lexical Markup Framework (LMF) [10] as a representation language.", "startOffset": 50, "endOffset": 54}, {"referenceID": 9, "context": "`LMF provides a common model for the creation and use of lexical resources, to manage the exchange of data among these resources, and to enable the merging of a large number of individual electronic resources to form extensive global electronic resources[10].", "startOffset": 254, "endOffset": 258}, {"referenceID": 19, "context": "WordNet-LMF was proposed to represent wordnets in LMF format [21].", "startOffset": 61, "endOffset": 65}, {"referenceID": 10, "context": "It has further been modified to accommodate lexical relations[11].", "startOffset": 61, "endOffset": 65}, {"referenceID": 19, "context": "LMF for Wordnet We have adapted the Wordnet-LMF, as specified in [21].", "startOffset": 65, "endOffset": 69}, {"referenceID": 2, "context": "At present, the dictionary is created by merging two dictionaries, UW++ [3] and CFILT Hin-UW.", "startOffset": 72, "endOffset": 75}], "year": 2013, "abstractText": "Noun \u2022 Action \u2022 Psychological Feature VOS Conjunct Verb", "creator": null}}}