{"id": "1612.00334", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Dec-2016", "title": "A Theoretical Framework for Robustness of (Deep) Classifiers against Adversarial Examples", "abstract": "Recent literature has pointed out that machine learning classifiers, including deep neural networks (DNN), are vulnerable to adversarial samples that are maliciously created inputs that force a machine learning classifier to produce wrong output labels. Multiple studies have tried to analyze and thus harden machine classifiers under such adversarial noise (AN). However, they are mostly empirical and provide little understanding of the underlying principles that enable evaluation of the robustness of a classier against AN. This paper proposes a unified framework using two metric spaces to evaluate classifiers' robustness against AN and provides general guidance for hardening such classifiers. The central idea of our work is that for a certain classification task, the robustness of a classifier $f_1$ against AN is decided by both $f_1$ and its oracle $f_2$ (like human annotator of that specific task). In particular: (1) By adding oracle $f_2$ into the framework, we provide a general definition of the adversarial sample problem. (2) We theoretically formulate a definition that decides whether a classifier is always robust against AN (strong-robustness); (3) Using two metric spaces ($X_1,d_1$) and ($X_2,d_2$) defined by $f_1$ and $f_2$ respectively, we prove that the topological equivalence between ($X_1,d_1$) and ($X_2,d_2$) is sufficient in deciding whether $f_1$ is strong-robust at test time, or not; (5) By training a DNN classifier using the Siamese architecture, we propose a new defense strategy \"Siamese training\" to intuitively approach topological equivalence between ($X_1,d_1$) and ($X_2,d_2$). Experimental results show that Siamese training helps multiple DNN models achieve better accuracy compared to previous defense strategies in an adversarial setting. DNN models after Siamese training exhibit better robustness than the state-of-the-art baselines.", "histories": [["v1", "Thu, 1 Dec 2016 16:20:39 GMT  (978kb,D)", "http://arxiv.org/abs/1612.00334v1", "20 pages , submitting to ICLR 2017"], ["v2", "Mon, 5 Dec 2016 17:07:35 GMT  (1311kb,D)", "http://arxiv.org/abs/1612.00334v2", "20 pages , submitting to ICLR 2017"], ["v3", "Tue, 17 Jan 2017 22:23:55 GMT  (2899kb,D)", "http://arxiv.org/abs/1612.00334v3", "30 pages , submitting to ICLR 2017"], ["v4", "Sat, 21 Jan 2017 16:37:24 GMT  (2908kb,D)", "http://arxiv.org/abs/1612.00334v4", "30 pages , submitting to ICLR 2017"], ["v5", "Thu, 26 Jan 2017 15:32:06 GMT  (2918kb,D)", "http://arxiv.org/abs/1612.00334v5", "30 pages , submitting to ICLR 2017"], ["v6", "Wed, 1 Feb 2017 17:30:50 GMT  (2922kb,D)", "http://arxiv.org/abs/1612.00334v6", "30 pages , submitting to ICLR 2017"], ["v7", "Thu, 2 Feb 2017 14:39:50 GMT  (2922kb,D)", "http://arxiv.org/abs/1612.00334v7", "30 pages , submitting to ICLR 2017"], ["v8", "Fri, 3 Feb 2017 16:06:39 GMT  (2924kb,D)", "http://arxiv.org/abs/1612.00334v8", "30 pages , submitting to ICLR 2017"], ["v9", "Mon, 27 Feb 2017 20:18:26 GMT  (3233kb,D)", "http://arxiv.org/abs/1612.00334v9", "35 pages , submitting to ICLR 2017"], ["v10", "Thu, 9 Mar 2017 22:00:56 GMT  (3218kb,D)", "http://arxiv.org/abs/1612.00334v10", "35 pages , submitting to ICLR 2017"], ["v11", "Thu, 27 Apr 2017 14:36:40 GMT  (3029kb,D)", "http://arxiv.org/abs/1612.00334v11", "38 pages , ICLR 2017 Workshop Track"], ["v12", "Wed, 27 Sep 2017 16:02:48 GMT  (3236kb,D)", "http://arxiv.org/abs/1612.00334v12", "38 pages , ICLR 2017 Workshop Track"]], "COMMENTS": "20 pages , submitting to ICLR 2017", "reviews": [], "SUBJECTS": "cs.LG cs.CR cs.CV", "authors": ["beilun wang", "ji gao", "yanjun qi"], "accepted": false, "id": "1612.00334"}, "pdf": {"name": "1612.00334.pdf", "metadata": {"source": "CRF", "title": "(DEEP) CLASSIFIERS UNDER ADVERSARIAL NOISE", "authors": ["Beilun Wang", "Ji Gao", "Yanjun Qi"], "emails": ["bw4mw@virginia.edu", "jg6yd@virginia.edu", "yanjun@virginia.edu"], "sections": [{"heading": null, "text": "An adversarial sample is often generated by adding adversarial noise (AN) to a normal sample. Recent literature has suggested that machine learning classifiers, including deep neural networks (DNN), are susceptible to AN. Several studies have attempted to analyze the robustness of classifiers under AN, and thus harden it. However, they are predominantly empirical and provide little understanding of the underlying principles that allow an assessment of the robustness of a classifier against AN. This paper proposes a unified framework that uses two metric spaces to evaluate the robustness of classifiers against AN, and provides general guidelines for the hardening of such classifiers. The central idea of our work is that for a specific classification task the robustness of a classifier against AN is decided."}, {"heading": "1 INTRODUCTION", "text": "In fact, it is a way in which people are able to determine for themselves what they want and what they want."}, {"heading": "2 ORACLE MATTERS FOR DEFINING ADVERSARIAL SAMPLES AT TEST TIME", "text": "This section provides a formal definition of the enemy attack at test time by including the term \"oracle.\" To overcome this problem, we first define a general formulation for the \"enemy noise problem\" 1, which refers to the consistency between the classifier and the oracle. All previous definitions of \"enemy examples\" are special cases in our formulation.Table 1 contains a list of important notations that we use in our thesis. For a specific classification task, an learned classifier f1: X \u2192 Y is used, where X = Rp represents the sample space and Y the output space. Since f1 is a classifier, the output space Y is a categorical group. Let x be an example drawing from sample space X."}, {"heading": "2.1 BACKGROUND: PREVIOUS FORMULATIONS OF MACHINE LEARNING CLASSIFIER UNDER ADVERSARIAL NOISE AT TEST TIME", "text": "However, in order to attack such a predictive function, the basic idea proposed in the most recent literature remains almost classified, which is to create a misclassified sample x by interfering with a correctly classified sample x, with a contrary disturbance (noise) r, so that: f1 (x) 6 = f1 (x) r = 0 (x) r = 0 (x) r = x (x).1 We use \"contrary disturbances,\" \"adversarial attacks in the test period,\" \"adversarial attacks in the test period,\" \"adversarial sample,\" which is interchangeable in the rest of this paper. If x and x \"is defined in a vector space, r = x \u2212 x.\" Otherwise, the meaning of r depends on the specific data type, x and x \"to 3. Of course, the attackers want to control the size of the disturbance r to ensure that the sample x remains as close as possible to the original sample x."}, {"heading": "2.2 BACKGROUND: PREVIOUS STUDIES GENERATING \u201cADVERSARIAL SAMPLES\u201d FOR DNN", "text": "To deceive the classifiers at the test date, several approaches have been implemented to create \"hostile disturbances\" by solving Eq. (2,2). Here we summarize three typical attack studies: Gradient ascent method (Biggio et al., 2013) The easiest way to solve Eq. (2,2) is by ascent gradients. (2,3) 3To minimize the magnitude of the disturbance and maximize the opposite effect, noise should follow the direction of the gradient. Therefore, the disturbance r is calculated as: r = xLf1 (x) (2,3)."}, {"heading": "2.3 A NOVEL FORMULATION OF \u201cADVERSARIAL SAMPLES\u201d: ORACLE MATTERS", "text": "As we have already explained, we overlook the meaning of the oracle function f2 and have the assumption of a distance function in Eq. (2.2) We think that the distance function used in Eq. (2.2) is actually defined d2 in the attribute space of the oracle f2. Instead of relying on d2 to indirectly consider f2, we propose the following uniform definition of \"Adversarial Test Example\" for a classification task. In this paper, the oracle is defined as a decision process that provides the basic truth for a specific classification task. Oracle classifier f2 () assumes that the correct answers are given each time. In the image classification task, the oracle could be a group of human annotators that provides the basic truth. In the malicious detection task, the oracle decides whether a file is malicious or benign. Furthermore, we separate function 2 \u2192 Xg2 where the Xg2 is presented."}, {"heading": "3 USING TWO METRIC SPACES TO UNDERSTAND STRONG-ROBUSTNESS OF MACHINE LEARNING CLASSIFIER AGAINST ADVERSARIAL NOISE", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 METRIC SPACES AND TOPOLOGICAL EQUIVALENCE OF TWO METRIC SPACES", "text": "Now let us briefly introduce the concept of metric space and topological equivalence. A metric on a proposition / space X is a function d: X \u00d7 X \u2192 [0, \u221e] with four properties: (1) Non-negativity, (2) identity of indistinguishable, (3) symmetry and (4) triangular inequality. In machine learning, for example, the most common metric is Euclidean distance. Kernel-based methods such as SVM, kernel regression and Gaussian process look at samples in a reproducing Hilbert space (RKHS). Metrics in an RKHS are, of course, defined as: d2 (x, y) = K (x, x) + K (y, y) \u2212 2K (x, y), here K (x, y) is a core function. As we have already explained, the two metric spaces in our problem (X1, d1) and (X2, d2).topological problems represent x \u2032 (the x \u00b2, a small pair x \u00b2)."}, {"heading": "A function or mapping h(\u00b7) from one topological space to another is continuous if the inverse image", "text": "\"We,\" \"we,\" \"we,\" \"we,\" \"we,\" \"we,\" \"we,\" \"we,\" \"we,\" \"we,\" \"we,\" \"we,\" \"we,\" \"we,\" \"we,\" \"we,\" \"we,\" \"we,\" \"we,\" \"we,\" \"we,\" \"we,\" \"we,\" \"we,\" \"we,\" \"\" we, \"\" we, \"\" we, \"\" we, \"\" we, \"\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we,\" we, \"we"}, {"heading": "3.4 FINER TOPOLOGY TO DEFINE A SUFFICIENT AND NECESSARY CONDITION FOR THE STRONG-ROBUSTNESS", "text": "We briefly reviewed the concept of metric space in section 3.1 and proposed the associated theorems (3.3) in section 3.3. Our previous evidence is based on feature spaces X1 and X2. However, we can also provide direct proof of the original space X. However, the crucial problem of the original example space X is that it is difficult to precisely define a metric on the original feature space. Now, we expand the results with a more general concept of pseudometric space. Space (X) is a pseudometric space or generalized metric space. It is worth pointing out that the generalized metric space is a specific case of topological space and metric space.We can proceed from a common machine learning."}, {"heading": "3.5 A CASE STUDY: WHY THEOREM 3.4 IS IMPORTANT ?", "text": "So if an unrelated feature is selected in the feature selection step, regardless of how hard the model is trained, it is not strong-robust. Actually, we want to show that a model trained in this way is often susceptible to enemies. Here's an example. Figure 2 shows a situation where the oracle uses only one feature to classify objects. A machine classifier uses an additional, unrelated feature and successfully classifies all objects. However, in the real case, such attacks can be very simple by simply inserting words with a very small font size into a spam email that is invisible to a human annotator. In the illustration, the red circle is one such opponent that changes the prediction but is very close to the original sample in the oracle space. In real cases, such attacks can include words with a very small font size in a spam email that is invisible to a human annotator."}, {"heading": "3.6 A CASE STUDY: ARE DEEP NEURAL NETS STRONGLY ROBUST ?", "text": "In this section, we apply our theory to deep neural networks (DNN) classifiers. Specifically, we find that (i) DNNs are not robust against adversarial attacks (ii) this weakness is an important limitation and should therefore be improved to obtain better models. Researchers have proposed various adversarial attacks on deep neural networks (e.g., (Szegedy et al., 2013; Nguyen et al., 2015; Er et al., 2015; Papernot et al, 2016a; Moosavi-Dezfooli et al., Papernot et al, 2015b)). Here, we focus on the image classification task with symbols defined as follows: \u2022 f1 (\u00b7) is a DNN classifier with multiple layers, including linear pereptron layers that we activate."}, {"heading": "4 QUANTIFYING AND IMPROVING WEAK-ROBUSTNESS OF DNN AGAINST ADVERSARIAL SAMPLES", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 A NOVEL MEASURE: ADVERSARIAL ROBUSTNESS OF CLASSIFIERS (ARC) TO QUANTIFY MODEL ROBUSTNESS AGAINST ADVERSARIAL TEST SAMPLES", "text": "Based on the previous analysis, we define a measure of the robustness of machine learning against adversarial learning. This is because it is a very strong condition that requires a complete understanding of the oracle. However, it is also important to measure the robustness of machine learning if it is not strong-robust. In this section, we propose a quantitative measure to capture how robust a model is based on performance against adversarial attacks. We show that this measure can also measure the robustness of a particular machine learning model when it reaches the maximum. (1 when it is categorized to [0, 1]. Definition 4.1. Adversarial robustness for classifiers (ARC) We can measure the robustness of a particular machine learning model when it reaches the maximum. (1 when it is categorized to [0, 1].) Definition 4.1. Adversarial robustness for classifiers (ARC) We can measure the robustness of a particular machine learning model when it is based on the difficulty of the attackers to adversarial samples (x) from 2.x (2) for robustness in Eq."}, {"heading": "4.2 A NOVEL HARDENING APPROACH : TO IMPROVE DNN\u2019S ADVERSARIAL-ROBUSTNESS WITH \u201cSIAMESE TRAINING\u201d", "text": "In this section, we propose a method: Siamese training. In this method, we first create random disturbed samples. By penalizing the difference in the mean output layers between the disturbed sample and the original sample, we can move two feature spaces X1 and X2 to be similar, thus increasing the robustness of the model. Experimental results show that our method performs better than the basic methods in adversarial settings. Siamese network (Bromley et al., 1993) is a traditional approach to nonlinear embedding methods. This method has long been used in many areas, including face recognition (Krizhevsky et al., 2012) and dimension reduction (Hadsell et al., 2006). A sketch of the Siamese architecture is shown in sub-figure (a) of Figure 4. Essentially, a Siamese network contains two copies of a model that has the same weights."}, {"heading": "4.2.1 USING \u201cSIAMESE TRAINING\u201d TO IMPROVE ADVERSARIAL-ROBUSTNESS AND ACCURACY OF THE DNN TOGETHER", "text": "If we want to achieve a good classifier that has high accuracy and robustness, we should work together to maximize accuracy and robustness, because we want to train a DNN model to have a better 4.5-based ARCA in our case. Argmax wARCA (f1 (\u00b7; w))) = argmax wlog (ARCA (f1 (\u00b7; w)))) = argmax wlog (Accuracy (f1 (\u00b7; w))) + log (ARC \u221e (f1 (\u00b7; w)))) (4,5) Inspired by this, we should minimize two loss functions L1 and L2 at the same time. L1 is the loss function used in training to control accuracy, and a new L2 limits robustness. Based on Equation (4.5) we have: argmin wL1 (f (\u00b7; w))) + Lamam2 (f (\u00b7; w))) (4.6) By using the middle layer of the net (Size 1) we get (Size)."}, {"heading": "5 EXPERIMENT", "text": "It is not only a question of quality, but also of quality, quality, quality and expression of the results that we have made in recent years. (1) It is a question of quality, quality, quality, quality and expression. (2) It is a question of quality and expression. (2) It is a question of quality and expression. (4) It is a question of expression. (5) It is a question of expression. (5) It is a question of expression. (4) It is a question of expression. (5) It is a question of expression. (5) It is a question of expression. (4) It is a question of expression. (4) It is a question of expression. (4) It is a question of expression. (5) It is a question. (5) It is a question. (5) It is a question. (5) It is a question. (5) It is a question. (5) It is a question. (5) It is a question."}, {"heading": "6 CONCLUSION", "text": "This work focuses on providing a theoretical framework for understanding the robustness of learning-based classifiers, especially DNN against such adversaries during the test period. By studying the topology between two metric spaces corresponding to the predictor and the oracle, we develop several theoretical conditions that can determine whether a classifier is (strong) against adversary samples. Subsequently, using theoretical results, a novel measure called ARC and a new hardening strategy called \"Siam Training\" are proposed to improve the robustness of DNN models. Empirically, the results of two benchmark datasets across multiple DNN models have shown strong improvements in \"Siam Training\" over other state-of-the-art defense strategies."}, {"heading": "7 APPENDIX OF PROBLEM FORMULATION", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "7.1 PREVIOUS DEFINITIONS OF ADVERSARIAL ATTACKS AT TESTING TIME", "text": "In this section we discuss earlier definitions of enemy attacks. (Biggio et al., 2013) use this formula argminx \u2032 (f1 (x \u2032) s.t. d2 (x, x \u2032) < dmax f1 (x) > 0 (7,1), and (Lowd & Meek, 2005) use this formula argmin x \u2032 (d2 (x, x \u2032) s.t. f1 (x \u2032) < 0 f1 (x) > 0 (7,2).Here your max is a constant."}, {"heading": "7.2 A GENERAL DEFINITION OF ADVERSARIAL TEST SAMPLE PROBLEM", "text": "Most previous studies have focused on classification tasks, but other machine learning tasks also suffer from hostile attacks. We can easily extend our definition to general machine learning situations. Definition 7.1. Problem of the contrary test sample: In an original sample x, we find a sample x, so 0 < \u03b4, < c < 1, | f1 (x) \u2212 f1 (x) | > \u03b4 and | f2 (x) \u2212 f2 (x) | <."}, {"heading": "7.3 A GENERAL DEFINITION OF STRONG-ROBUSTNESS", "text": "Definition 7.2. Robustness In a test sample x-Rna.e. there is 0 < \u03b4 < c < c < 1 and | f2 (x) \u2212 f2 (x) | < 0 < \u03b4 < c < 1 such that | f1 (x) \u2212 f1 (x) | < \u03b4. Then we call the learned predictor f1 (\u00b7) strong-robust against enemy attacks at test time."}, {"heading": "7.4 A COUNTER-EXAMPLE OF THE CONTINUITY ASSUMPTION", "text": "A counter example can be the following: Suppose a predictor function f (x) = 1p (x, y = 1) > 12 for a classification problem, where p (x, y = 1) is the probability density function. Suppose the p (x, y = 1) is a continuous a.e. function, we define a probability density function p0 as: p0 (x) = {p (x), x-R\\ Q0, x-Q (7.3) Note that p0 is still the probability density function. However, f0 (x) = 1p0 (x, y = 1) > 12 in the set {x | f (x) = 1} is nowhere continuous. This is an extreme case, but it shows that a learned probability function may not be continuous a.e."}, {"heading": "7.5 MORE ABOUT A.E.CONTINUITY ASSUMPTION", "text": "Lemma 7.3. If the a.e. continuity assumption does not apply, there is a standard that does not deviate from zero, so that for each test sample x, x, s.t. f1 (x) 6 = f1 (x, x) d1 (x, x) < \u03b4 (7.4) proof is available. Without it, one can easily find a very similar sample x for each test sample x (i.e. for each small, d1 (x, x) < \u03b4), so that | f1 (x) \u2212 f1 (x) | >. For classification problems, this means that f1 (x) 6 = f1 (x) (i.e. there are very similar pairs of two samples x and x \u00b2, which have different designations for most x-X1 values). Lemma (7.3) shows that f1 is not robust against random noise if we do not assume that f1 is continuous."}, {"heading": "7.6 MOST CLASSIFIERS SATISFY THE CONTINUITY ASSUMPTION", "text": "For example, \u2022 logistic regression for text categorization with a bag of word representation. A classifier with a discrete feature representation is inherently continuous. Since {x \u2032 | d1 (x, x \u2032) < \u03b4, x 6 = x \u2032} = \u2205 is when \u03b4 are small and x, x \u2032 are discrete. Therefore, logistic regression with a bag of word representation is a continuous predictor. \u2022 Support vector machine with a continuous feature representation. Suppose we define the d21 (x, x \u2032) = k (x, x \u2032) \u2212 2k (x, x \u2032).The support vector engine is a linear classification engine with d1. Therefore, the SVM prediction function is continuous with d1."}, {"heading": "8 APPENDIX: TWO METRIC SPACES FOR MODEL ROBUSTNESS AGAINST ADVERSARIAL NOISE", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "8.1 MORE ABOUT METRIC SPACE", "text": "It is not necessary to use the metric as a similarity measurement in machine learning and statistical methods in the original attribute space Rn. For example, people sometimes implement a feature selection at the beginning of data analysis. This improves accuracy and speeds up the method, but projects the original sample into an embedded attribute space Rn \u2032. One problem is that the distance function d used in learning is not a metric. This is because x, y and Rn are such that d (x, y) = 0, but x 6 = y (ProjRn \u2032 (x) = ProjRn \u2032 (y). Thus, d does not fit the (2) identity of the space Rn not recognizable. A distance function that satisfies the property (1), (3) and (4), but is not (2) defined as \"pseudometric.\" In summary, the distance used in machine learning is a pseudometric, or more precisely a suitable attribute embedded in a metric."}, {"heading": "8.2 PROOFS FOR THEOREMS", "text": "In this section we provide the proofs for Theorem (3,3), Corollary (3,4), Theorem (3,6) and Corollary (3,7). We first prove Theorem (3,6) and Corollary (3,7). Since \"topological equivalence\" is a stronger condition than \"finer topology,\" Theorem (3,3) and Corollary (3,4) are straightforward. Proof of Theorem (3,6) Proof. Let us leave S1 = {B1 (x,)} and S2 = {B2 (x,)}, with B1 (x,) = {y | d (x, y) <} and B2 (x, y) < then S1 (y, y) <} and S2 = {B2 (x,). Then S1 and S2 (x)."}, {"heading": "8.3 A CASE STUDY: PREVIOUS GRADIENT-DRIVEN ATTACKS ON THE DIFFERENTIABLE PREDICTOR FUNCTIONS", "text": "Recent studies (Biggio et al., 2013; Szegedy et al., 2013) chose a supporting vector machine model with a continuous representation of features or a deep neural network. They generate the evasive samples by solving the following two convex optimization problems: x \u2032 = argmin y f1 (y) Subject: d (y, x) \u2264 \u03b4 (8.1) x \u2032 = argmin y (y, x) Subject: f1 (y) \u2264 0 (8.2) These two formulations are the dual form of each other. Previous studies choose the descent of gradients to solve this problem. The main problem of this attack strategy is that it relies on the gradient information. An efficient solution to prevent this type of attack is the concealment of gradient information. Although there are some studies that use the uncompromised gradient (Xu et al., 2016), it is difficult to estimate the grades."}], "references": [{"title": "Data poisoning attacks against autoregressive models", "author": ["Scott Alfeld", "Xiaojin Zhu", "Paul Barford"], "venue": null, "citeRegEx": "Alfeld et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Alfeld et al\\.", "year": 2016}, {"title": "Can machine learning be secure", "author": ["Marco Barreno", "Blaine Nelson", "Russell Sears", "Anthony D Joseph", "J Doug Tygar"], "venue": "In Proceedings of the 2006 ACM Symposium on Information, computer and communications security,", "citeRegEx": "Barreno et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Barreno et al\\.", "year": 2006}, {"title": "The Security of Machine Learning", "author": ["Marco Barreno", "Blaine Nelson", "Anthony D Joseph", "JD Tygar"], "venue": "Machine Learning,", "citeRegEx": "Barreno et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Barreno et al\\.", "year": 2010}, {"title": "Adversarial pattern classification using multiple classifiers and randomisation", "author": ["Battista Biggio", "Giorgio Fumera", "Fabio Roli"], "venue": "URL http://link.springer.com/chapter/10", "citeRegEx": "Biggio et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Biggio et al\\.", "year": 2008}, {"title": "Evasion attacks against machine learning at test time", "author": ["Battista Biggio", "Igino Corona", "Davide Maiorca", "Blaine Nelson", "Nedim \u0160rndi\u0107", "Pavel Laskov", "Giorgio Giacinto", "Fabio Roli"], "venue": "In Machine Learning and Knowledge Discovery in Databases,", "citeRegEx": "Biggio et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Biggio et al\\.", "year": 2013}, {"title": "Poisoning complete-linkage hierarchical clustering", "author": ["Battista Biggio", "Samuel Rota Bul\u00f2", "Ignazio Pillai", "Michele Mura", "Eyasu Zemene Mequanint", "Marcello Pelillo", "Fabio Roli"], "venue": null, "citeRegEx": "Biggio et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Biggio et al\\.", "year": 2014}, {"title": "Differentially-and non-differentially-private random decision trees", "author": ["Mariusz Bojarski", "Anna Choromanska", "Krzysztof Choromanski", "Yann LeCun"], "venue": "arXiv preprint arXiv:1410.6973,", "citeRegEx": "Bojarski et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bojarski et al\\.", "year": 2014}, {"title": "Signature verification using a \u201csiamese\u201d time delay neural network", "author": ["Jane Bromley", "James W Bentz", "L\u00e9on Bottou", "Isabelle Guyon", "Yann LeCun", "Cliff Moore", "Eduard S\u00e4ckinger", "Roopak Shah"], "venue": "International Journal of Pattern Recognition and Artificial Intelligence,", "citeRegEx": "Bromley et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Bromley et al\\.", "year": 1993}, {"title": "Stackelberg Games for Adversarial Prediction Problems", "author": ["Michael Br\u00fcckner", "Tobias Scheffer"], "venue": "In 17th ACM SIGKDD Conference on Knowledge Discovery and Data Mining,", "citeRegEx": "Br\u00fcckner and Scheffer.,? \\Q2011\\E", "shortCiteRegEx": "Br\u00fcckner and Scheffer.", "year": 2011}, {"title": "Defensive distillation is not robust to adversarial examples", "author": ["Nicholas Carlini", "David Wagner"], "venue": "arXiv preprint arXiv:1607.04311,", "citeRegEx": "Carlini and Wagner.,? \\Q2016\\E", "shortCiteRegEx": "Carlini and Wagner.", "year": 2016}, {"title": "Towards evaluating the robustness of neural networks", "author": ["Nicholas Carlini", "David Wagner"], "venue": "arXiv preprint arXiv:1608.04644,", "citeRegEx": "Carlini and Wagner.,? \\Q2016\\E", "shortCiteRegEx": "Carlini and Wagner.", "year": 2016}, {"title": "Prediction, learning, and games", "author": ["Nicolo Cesa-Bianchi", "G\u00e1bor Lugosi"], "venue": null, "citeRegEx": "Cesa.Bianchi and Lugosi.,? \\Q2006\\E", "shortCiteRegEx": "Cesa.Bianchi and Lugosi.", "year": 2006}, {"title": "Large-scale malware classification using random projections and neural networks", "author": ["George E Dahl", "Jack W Stokes", "Li Deng", "Dong Yu"], "venue": "In ICASSP,", "citeRegEx": "Dahl et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Dahl et al\\.", "year": 2013}, {"title": "Fitting Time Series Models to Nonstationary Processes", "author": ["Rainer Dahlhaus"], "venue": "The Annals of Statistics,", "citeRegEx": "Dahlhaus.,? \\Q1997\\E", "shortCiteRegEx": "Dahlhaus.", "year": 1997}, {"title": "Adversarial classification", "author": ["Nilesh Dalvi", "Pedro Domingos", "Sumit Sanghai", "Deepak Verma"], "venue": "In Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining,", "citeRegEx": "Dalvi et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Dalvi et al\\.", "year": 2004}, {"title": "Learning to Classify with Missing and Corrupted Features", "author": ["Ofer Dekel", "Ohad Shamir", "Lin Xiao"], "venue": "Machine Learning,", "citeRegEx": "Dekel et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Dekel et al\\.", "year": 2010}, {"title": "Imagenet: A large-scale hierarchical image database", "author": ["Jia Deng", "Wei Dong", "Richard Socher", "Li-Jia Li", "Kai Li", "Li Fei-Fei"], "venue": "In Computer Vision and Pattern Recognition,", "citeRegEx": "Deng et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Deng et al\\.", "year": 2009}, {"title": "Privacy aware learning", "author": ["John C Duchi", "Michael I Jordan", "Martin J Wainwright"], "venue": "Journal of the ACM (JACM),", "citeRegEx": "Duchi et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Duchi et al\\.", "year": 2014}, {"title": "Differential Privacy", "author": ["Cynthia Dwork"], "venue": "In Encyclopedia of Cryptography and Security,", "citeRegEx": "Dwork.,? \\Q2011\\E", "shortCiteRegEx": "Dwork.", "year": 2011}, {"title": "Fundamental limits on adversarial robustness", "author": ["Alhussein Fawzi", "Omar Fawzi", "Pascal Frossard"], "venue": "In Proceedings of ICML, Workshop on Deep Learning, number EPFL-CONF-214923,", "citeRegEx": "Fawzi et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Fawzi et al\\.", "year": 2015}, {"title": "Real analysis: modern techniques and their applications", "author": ["Gerald B Folland"], "venue": null, "citeRegEx": "Folland.,? \\Q2013\\E", "shortCiteRegEx": "Folland.", "year": 2013}, {"title": "Nightmare at Test Time: Robust Learning by Feature Deletion", "author": ["Amir Globerson", "Sam Roweis"], "venue": "In 23rd International Conference on Machine Learning,", "citeRegEx": "Globerson and Roweis.,? \\Q2006\\E", "shortCiteRegEx": "Globerson and Roweis.", "year": 2006}, {"title": "Explaining and harnessing adversarial examples", "author": ["Ian J Goodfellow", "Jonathon Shlens", "Christian Szegedy"], "venue": "arXiv preprint arXiv:1412.6572,", "citeRegEx": "Goodfellow et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2014}, {"title": "Adversarial perturbations against deep neural networks for malware classification", "author": ["Kathrin Grosse", "Nicolas Papernot", "Praveen Manoharan", "Michael Backes", "Patrick McDaniel"], "venue": "arXiv preprint arXiv:1606.04435,", "citeRegEx": "Grosse et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Grosse et al\\.", "year": 2016}, {"title": "Towards Deep Neural Network Architectures Robust to Adversarial Examples", "author": ["Shixiang Gu", "Luca Rigazio"], "venue": "[cs],", "citeRegEx": "Gu and Rigazio.,? \\Q2014\\E", "shortCiteRegEx": "Gu and Rigazio.", "year": 2014}, {"title": "Dimensionality reduction by learning an invariant mapping", "author": ["Raia Hadsell", "Sumit Chopra", "Yann LeCun"], "venue": "IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR\u201906),", "citeRegEx": "Hadsell et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Hadsell et al\\.", "year": 2006}, {"title": "DeepSpeech: Scaling up end-to-end speech recognition", "author": ["Awni Hannun", "Carl Case", "Jared Casper", "Bryan Catanzaro", "Greg Diamos", "Erich Elsen", "Ryan Prenger", "Sanjeev Satheesh", "Shubho Sengupta", "Adam Coates", "others"], "venue": "arXiv preprint arXiv:1412.5567,", "citeRegEx": "Hannun et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Hannun et al\\.", "year": 2014}, {"title": "Deep residual learning for image recognition", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"], "venue": "arXiv preprint arXiv:1512.03385,", "citeRegEx": "He et al\\.,? \\Q2015\\E", "shortCiteRegEx": "He et al\\.", "year": 2015}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "Hochreiter and Schmidhuber.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Adversarial machine learning", "author": ["Ling Huang", "Anthony D Joseph", "Blaine Nelson", "Benjamin IP Rubinstein", "JD Tygar"], "venue": "In 4th ACM Workshop on Security and Artificial Intelligence,", "citeRegEx": "Huang et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2011}, {"title": "Robust convolutional neural networks under adversarial noise", "author": ["Jonghoon Jin", "Aysegul Dundar", "Eugenio Culurciello"], "venue": "arXiv preprint arXiv:1511.06306,", "citeRegEx": "Jin et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Jin et al\\.", "year": 2015}, {"title": "Evasion and hardening of tree ensemble classifiers", "author": ["Alex Kantchelian", "JD Tygar", "Anthony D Joseph"], "venue": "arXiv preprint arXiv:1509.07892,", "citeRegEx": "Kantchelian et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kantchelian et al\\.", "year": 2015}, {"title": "Learning multiple layers of features from tiny images", "author": ["Alex Krizhevsky", "Geoffrey Hinton"], "venue": null, "citeRegEx": "Krizhevsky and Hinton.,? \\Q2009\\E", "shortCiteRegEx": "Krizhevsky and Hinton.", "year": 2009}, {"title": "ImageNet Classification with Deep Convolutional Neural Networks", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E Hinton"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Adversarial examples in the physical world", "author": ["Alexey Kurakin", "Ian Goodfellow", "Samy Bengio"], "venue": "arXiv preprint arXiv:1607.02533,", "citeRegEx": "Kurakin et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Kurakin et al\\.", "year": 2016}, {"title": "Differentially private distributed online learning", "author": ["Chencheng Li", "Pan Zhou"], "venue": "arXiv preprint arXiv:1505.06556,", "citeRegEx": "Li and Zhou.,? \\Q2015\\E", "shortCiteRegEx": "Li and Zhou.", "year": 2015}, {"title": "Mining adversarial patterns via regularized loss minimization", "author": ["Wei Liu", "Sanjay Chawla"], "venue": "Machine learning,", "citeRegEx": "Liu and Chawla.,? \\Q2010\\E", "shortCiteRegEx": "Liu and Chawla.", "year": 2010}, {"title": "Adversarial learning", "author": ["Daniel Lowd", "Christopher Meek"], "venue": "In Proceedings of the eleventh ACM SIGKDD international conference on Knowledge discovery in data mining,", "citeRegEx": "Lowd and Meek.,? \\Q2005\\E", "shortCiteRegEx": "Lowd and Meek.", "year": 2005}, {"title": "The security of latent dirichlet allocation", "author": ["Shike Mei", "Xiaojin Zhu"], "venue": null, "citeRegEx": "Mei and Zhu.,? \\Q2015\\E", "shortCiteRegEx": "Mei and Zhu.", "year": 2015}, {"title": "Some submodular data-poisoning attacks on machine learners", "author": ["Shike Mei", "Xiaojin Zhu"], "venue": null, "citeRegEx": "Mei and Zhu.,? \\Q2015\\E", "shortCiteRegEx": "Mei and Zhu.", "year": 2015}, {"title": "Under review as a conference", "author": ["Takeru Miyato", "Shin-ichi Maeda", "Koyama Masanori"], "venue": "ICLR\u2019 16,", "citeRegEx": "Miyato et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Miyato et al\\.", "year": 2016}, {"title": "Deep neural networks are easily fooled: High confidence predictions for unrecognizable", "author": ["Anh Nguyen", "Jason Yosinski", "Jeff Clune"], "venue": null, "citeRegEx": "Nguyen et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Nguyen et al\\.", "year": 2016}, {"title": "Distillation as a defense to adversarial perturbations against", "author": ["Nicolas Papernot", "Patrick McDaniel", "Xi Wu", "Somesh Jha", "Ananthram Swami"], "venue": null, "citeRegEx": "Papernot et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Papernot et al\\.", "year": 2015}, {"title": "The limitations of deep learning", "author": ["Nicolas Papernot", "Patrick McDaniel", "Somesh Jha", "Matt Fredrikson", "Z Berkay Celik", "Ananthram Swami"], "venue": null, "citeRegEx": "Papernot et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Papernot et al\\.", "year": 2016}, {"title": "A differentially private stochastic gradient descent algorithm for multiparty classification", "author": ["Arun Rajkumar", "Shivani Agarwal"], "venue": "IEEE European Symposium on Security and Privacy (EuroS&P),", "citeRegEx": "Rajkumar and Agarwal.,? \\Q2016\\E", "shortCiteRegEx": "Rajkumar and Agarwal.", "year": 2016}, {"title": "Practical Evasion of a Learning-Based Classifier: A Case Study", "author": ["N. Rndic", "P. Laskov"], "venue": "Conference on Artificial Intelligence and Statistics,", "citeRegEx": "Rndic and Laskov.,? \\Q2012\\E", "shortCiteRegEx": "Rndic and Laskov.", "year": 2012}, {"title": "Adversarial manipulation of deep", "author": ["Sara Sabour", "Yanshuai Cao", "Fartash Faghri", "David J Fleet"], "venue": null, "citeRegEx": "Sabour et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sabour et al\\.", "year": 2014}, {"title": "Intriguing properties", "author": ["Christian Szegedy", "Wojciech Zaremba", "Ilya Sutskever", "Joan Bruna", "Dumitru Erhan", "Ian Goodfellow", "Rob Fergus"], "venue": null, "citeRegEx": "Szegedy et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Szegedy et al\\.", "year": 2014}, {"title": "Adversarial reinforcement learning", "author": ["William Uther", "Manuela Veloso"], "venue": "neural networks. arXiv preprint arXiv:1312.6199,", "citeRegEx": "Uther and Veloso.,? \\Q2013\\E", "shortCiteRegEx": "Uther and Veloso.", "year": 2013}, {"title": "Extracting and composing robust features with denoising", "author": ["Unpublished", "1997. Pascal Vincent", "Hugo Larochelle", "Yoshua Bengio", "Pierre-Antoine Manzagol"], "venue": null, "citeRegEx": "Unpublished et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Unpublished et al\\.", "year": 1997}, {"title": "Is feature selection secure against training data", "author": ["Huang Xiao", "Battista Biggio", "Gavin Brown", "Giorgio Fumera", "Claudia Eckert", "Fabio Roli"], "venue": "Proceedings of the 25th international conference on Machine learning,", "citeRegEx": "Xiao et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Xiao et al\\.", "year": 2008}, {"title": "Crypto-nets: Neural networks", "author": ["Pengtao Xie", "Misha Bilenko", "Tom Finley", "Ran Gilad-Bachrach", "Kristin Lauter", "Michael Naehrig"], "venue": "Proceedings of the 32nd International Conference on Machine Learning (ICML-15),", "citeRegEx": "Xie et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Xie et al\\.", "year": 2015}, {"title": "Wide residual networks", "author": ["Sergey Zagoruyko", "Nikos Komodakis"], "venue": "arXiv preprint arXiv:1605.07146,", "citeRegEx": "2016", "shortCiteRegEx": "2016", "year": 2016}], "referenceMentions": [{"referenceID": 33, "context": "As a result, DNNs have been demonstrated to perform exceptionally well on multiple machine learning tasks (Krizhevsky et al., 2012; Hannun et al., 2014), some of which are increasingly security-sensitive (Microsoft Corporation, 2015; Dahl et al.", "startOffset": 106, "endOffset": 152}, {"referenceID": 26, "context": "As a result, DNNs have been demonstrated to perform exceptionally well on multiple machine learning tasks (Krizhevsky et al., 2012; Hannun et al., 2014), some of which are increasingly security-sensitive (Microsoft Corporation, 2015; Dahl et al.", "startOffset": 106, "endOffset": 152}, {"referenceID": 12, "context": ", 2014), some of which are increasingly security-sensitive (Microsoft Corporation, 2015; Dahl et al., 2013; Sharif et al., 2016; Moosavi-Dezfooli et al., 2016; Papernot et al., 2016b).", "startOffset": 59, "endOffset": 183}, {"referenceID": 22, "context": "A so-called \u201cadversarial sample\u201d (Goodfellow et al., 2014; Szegedy et al., 2013) is crafted by adding a carefully chosen adversarial perturbation (i.", "startOffset": 33, "endOffset": 80}, {"referenceID": 29, "context": "Investigating the behavior of machine learning systems in adversarial environments is an emerging topic (Huang et al., 2011; Barreno et al., 2006; 2010; Globerson & Roweis, 2006; Biggio et al., 2013; Kantchelian et al., 2015; Zhang et al., 2015).", "startOffset": 104, "endOffset": 245}, {"referenceID": 1, "context": "Investigating the behavior of machine learning systems in adversarial environments is an emerging topic (Huang et al., 2011; Barreno et al., 2006; 2010; Globerson & Roweis, 2006; Biggio et al., 2013; Kantchelian et al., 2015; Zhang et al., 2015).", "startOffset": 104, "endOffset": 245}, {"referenceID": 4, "context": "Investigating the behavior of machine learning systems in adversarial environments is an emerging topic (Huang et al., 2011; Barreno et al., 2006; 2010; Globerson & Roweis, 2006; Biggio et al., 2013; Kantchelian et al., 2015; Zhang et al., 2015).", "startOffset": 104, "endOffset": 245}, {"referenceID": 31, "context": "Investigating the behavior of machine learning systems in adversarial environments is an emerging topic (Huang et al., 2011; Barreno et al., 2006; 2010; Globerson & Roweis, 2006; Biggio et al., 2013; Kantchelian et al., 2015; Zhang et al., 2015).", "startOffset": 104, "endOffset": 245}, {"referenceID": 0, "context": "A few recent papers (Alfeld et al., 2016; Mei & Zhu, 2015b; Biggio et al., 2014; 2012; Mei & Zhu, 2015a) have considered the problem of an adversary being able to pollute the training data with the goal of influencing learning systems including support vector machines (SVM), autoregressive models and topic models.", "startOffset": 20, "endOffset": 104}, {"referenceID": 5, "context": "A few recent papers (Alfeld et al., 2016; Mei & Zhu, 2015b; Biggio et al., 2014; 2012; Mei & Zhu, 2015a) have considered the problem of an adversary being able to pollute the training data with the goal of influencing learning systems including support vector machines (SVM), autoregressive models and topic models.", "startOffset": 20, "endOffset": 104}, {"referenceID": 22, "context": "Related studies (Szegedy et al., 2013; Goodfellow et al., 2014; Xu et al., 2016; Kantchelian et al., 2015; Rndic & Laskov, 2014; Biggio et al., 2013) assume the adversary does not have an opportunity to influence the training data, but instead finds \u201cadversarial samples\u201d to evade a trained classifier like DNN, SVM or random forest.", "startOffset": 16, "endOffset": 149}, {"referenceID": 31, "context": "Related studies (Szegedy et al., 2013; Goodfellow et al., 2014; Xu et al., 2016; Kantchelian et al., 2015; Rndic & Laskov, 2014; Biggio et al., 2013) assume the adversary does not have an opportunity to influence the training data, but instead finds \u201cadversarial samples\u201d to evade a trained classifier like DNN, SVM or random forest.", "startOffset": 16, "endOffset": 149}, {"referenceID": 4, "context": "Related studies (Szegedy et al., 2013; Goodfellow et al., 2014; Xu et al., 2016; Kantchelian et al., 2015; Rndic & Laskov, 2014; Biggio et al., 2013) assume the adversary does not have an opportunity to influence the training data, but instead finds \u201cadversarial samples\u201d to evade a trained classifier like DNN, SVM or random forest.", "startOffset": 16, "endOffset": 149}, {"referenceID": 17, "context": "(3) Privacy-aware machine learning (Duchi et al., 2014; Xie et al., 2014; Bojarski et al., 2014; Stoddard et al., 2014; Li & Zhou, 2015; Rajkumar & Agarwal, 2012; Dwork, 2011; Nock et al., 2015) is another important category relevant to data security in machine learning systems.", "startOffset": 35, "endOffset": 194}, {"referenceID": 6, "context": "(3) Privacy-aware machine learning (Duchi et al., 2014; Xie et al., 2014; Bojarski et al., 2014; Stoddard et al., 2014; Li & Zhou, 2015; Rajkumar & Agarwal, 2012; Dwork, 2011; Nock et al., 2015) is another important category relevant to data security in machine learning systems.", "startOffset": 35, "endOffset": 194}, {"referenceID": 18, "context": "(3) Privacy-aware machine learning (Duchi et al., 2014; Xie et al., 2014; Bojarski et al., 2014; Stoddard et al., 2014; Li & Zhou, 2015; Rajkumar & Agarwal, 2012; Dwork, 2011; Nock et al., 2015) is another important category relevant to data security in machine learning systems.", "startOffset": 35, "endOffset": 194}, {"referenceID": 22, "context": "Then (Goodfellow et al., 2014) tries to clarify that the primary cause of such vulnerability is the linear nature of DNNs.", "startOffset": 5, "endOffset": 30}, {"referenceID": 19, "context": "More subsequent papers (Fawzi et al., 2015; Papernot et al., 2015a; Sabour et al., 2015; Nguyen et al., 2015) have explored other ways of adversarial manipulations on DNN outputs or deep representations recently.", "startOffset": 23, "endOffset": 109}, {"referenceID": 30, "context": "For instance, denoising NN architectures (Vincent et al., 2008; Gu & Rigazio, 2014; Jin et al., 2015) can discover more robust features by using noise corrupted version of inputs as training samples.", "startOffset": 41, "endOffset": 101}, {"referenceID": 40, "context": "More recent techniques incorporate smoothness penalty (Miyato et al., 2016; Zheng et al., 2016) or layer-wise penalty (Carlini & Wagner, 2016b) as a regularization term in the loss function to promote the smoothness of the DNN model distributions.", "startOffset": 54, "endOffset": 95}, {"referenceID": 2, "context": "(Barreno et al., 2010) and Biggio et al.", "startOffset": 0, "endOffset": 22}, {"referenceID": 3, "context": "(Biggio et al., 2008) propose a method to introduce some randomness in the selection of classification boundaries; (2) A few recent studies (Xiao et al.", "startOffset": 0, "endOffset": 21}, {"referenceID": 14, "context": "Related efforts include perfect information assumption (Dalvi et al., 2004), assuming a polynomial number of membership queries (Lowd & Meek, 2005), formalizing as a two-person sequential Stackelberg game (Br\u00fcckner & Scheffer, 2011; Liu & Chawla, 2010), a min-max strategy (training a classifier with best performance under the worst noise) (Dekel et al.", "startOffset": 55, "endOffset": 75}, {"referenceID": 15, "context": ", 2004), assuming a polynomial number of membership queries (Lowd & Meek, 2005), formalizing as a two-person sequential Stackelberg game (Br\u00fcckner & Scheffer, 2011; Liu & Chawla, 2010), a min-max strategy (training a classifier with best performance under the worst noise) (Dekel et al., 2010; Globerson & Roweis, 2006), exploring online and non-stationary learning by (Dahlhaus, 1997; Cesa-Bianchi & Lugosi, 2006), and formalizing as an adversarial reinforcement learning problem (Uther & Veloso, 1997).", "startOffset": 273, "endOffset": 319}, {"referenceID": 13, "context": ", 2010; Globerson & Roweis, 2006), exploring online and non-stationary learning by (Dahlhaus, 1997; Cesa-Bianchi & Lugosi, 2006), and formalizing as an adversarial reinforcement learning problem (Uther & Veloso, 1997).", "startOffset": 83, "endOffset": 128}, {"referenceID": 20, "context": "almost everywhere (Folland, 2013)2", "startOffset": 18, "endOffset": 33}, {"referenceID": 22, "context": "Previous researchers have used different distance functions to measure the perturbation size, including `2-norm, `1-norm, `0-norm and `\u221e-norm (Goodfellow et al., 2014; Szegedy et al., 2013; Grosse et al., 2016; Kantchelian et al., 2015).", "startOffset": 142, "endOffset": 236}, {"referenceID": 23, "context": "Previous researchers have used different distance functions to measure the perturbation size, including `2-norm, `1-norm, `0-norm and `\u221e-norm (Goodfellow et al., 2014; Szegedy et al., 2013; Grosse et al., 2016; Kantchelian et al., 2015).", "startOffset": 142, "endOffset": 236}, {"referenceID": 31, "context": "Previous researchers have used different distance functions to measure the perturbation size, including `2-norm, `1-norm, `0-norm and `\u221e-norm (Goodfellow et al., 2014; Szegedy et al., 2013; Grosse et al., 2016; Kantchelian et al., 2015).", "startOffset": 142, "endOffset": 236}, {"referenceID": 4, "context": "Most previous definitions about attacking machine learning models (Biggio et al., 2013; Lowd & Meek, 2005) view generating an adversarial samples as a constrained optimization problem.", "startOffset": 66, "endOffset": 106}, {"referenceID": 4, "context": "For instance, the authors of (Biggio et al., 2013) assume that d2(x, x\u2032) = ||r||2.", "startOffset": 29, "endOffset": 50}, {"referenceID": 22, "context": "(Goodfellow et al., 2014) assumes that d2(x, x\u2032) = ||r||\u221e.", "startOffset": 0, "endOffset": 25}, {"referenceID": 22, "context": "f1 d2 (Goodfellow et al., 2014) Convolutional neural networks `\u221e (Szegedy et al.", "startOffset": 6, "endOffset": 31}, {"referenceID": 4, "context": ", 2013) Convolutional neural networks `2 (Biggio et al., 2013) Support vector machine `2 (Kantchelian et al.", "startOffset": 41, "endOffset": 62}, {"referenceID": 31, "context": ", 2013) Support vector machine `2 (Kantchelian et al., 2015) Decision tree and Random forest `2, `1, `\u221e (Grosse et al.", "startOffset": 34, "endOffset": 60}, {"referenceID": 23, "context": ", 2015) Decision tree and Random forest `2, `1, `\u221e (Grosse et al., 2016) Convolutional neural networks `0", "startOffset": 51, "endOffset": 72}, {"referenceID": 4, "context": "Gradient ascent method (Biggio et al., 2013) The easiest way to solve Eq.", "startOffset": 23, "endOffset": 44}, {"referenceID": 22, "context": "Fast gradient sign method (Goodfellow et al., 2014) Fast gradient sign method view d2 as the `\u221e-norm.", "startOffset": 26, "endOffset": 51}, {"referenceID": 34, "context": "A recent paper (Kurakin et al., 2016) shows that adversarial examples generated by fast gradient sign method works are misclassified even perceived through cameras.", "startOffset": 15, "endOffset": 37}, {"referenceID": 4, "context": "is a hidden assumption made by most previous studies(Biggio et al., 2013; Lowd & Meek, 2005).", "startOffset": 52, "endOffset": 92}, {"referenceID": 27, "context": ", (Szegedy et al., 2013; Nguyen et al., 2015; He et al., 2015; Papernot et al., 2016a; Moosavi-Dezfooli et al., 2015; Papernot et al., 2015b)).", "startOffset": 2, "endOffset": 141}, {"referenceID": 27, "context": "The model we uses is a 200-layer residual network(He et al., 2015) on Imagenet dataset(Deng et al.", "startOffset": 49, "endOffset": 66}, {"referenceID": 16, "context": ", 2015) on Imagenet dataset(Deng et al., 2009).", "startOffset": 27, "endOffset": 46}, {"referenceID": 7, "context": "Siamese network (Bromley et al., 1993) is a traditional approach of non-linear embedding methods.", "startOffset": 16, "endOffset": 38}, {"referenceID": 33, "context": "This method has long been used in many fields, including face recognition (Krizhevsky et al., 2012) and dimension reduction (Hadsell et al.", "startOffset": 74, "endOffset": 99}, {"referenceID": 25, "context": ", 2012) and dimension reduction (Hadsell et al., 2006).", "startOffset": 32, "endOffset": 54}], "year": 2017, "abstractText": "Adversarial samples are maliciously created inputs that force a machine learning classifier to produce wrong output labels. An adversarial sample is often generated by adding adversarial noise (AN) to a normal sample. Recent literature has pointed out that machine learning classifiers, including deep neural networks (DNN), are vulnerable to AN. Multiple studies have tried to analyze and thus harden machine classifiers under AN. However, they are mostly empirical and provide little understanding of the underlying principles that enable evaluation of the robustness of a classier against AN. This paper proposes a unified framework using two metric spaces to evaluate classifiers\u2019 robustness against AN and provides general guidance for hardening such classifiers. The central idea of our work is that for a certain classification task, the robustness of a classifier f1 against AN is decided by both f1 and its oracle f2 (like human annotator of that specific task). In particular: (1) By adding oracle f2 into the framework, we provide a general definition of the adversarial sample problem. (2) We theoretically formulate a definition that decides whether a classifier is always robust against AN (strongrobustness); (3) Using two metric spaces (X1, d1) and (X2, d2) defined by f1 and f2 respectively, we prove that the topological equivalence between (X1, d1) and (X2, d2) is sufficient in deciding whether f1 is strong-robust at test time, or not; (4) Then a novel measure referred to Adversarial Robustness of Classifier (ARC) is defined to quantify the robustness of a classifier against AN; (5) By training a DNN classifier using the Siamese architecture, we propose a new defense strategy \u201cSiamese training\u201d to intuitively approach topological equivalence between (X1, d1) and (X2, d2). Experimental results show that Siamese training helps multiple DNN models achieve better accuracy compared to previous defense strategies in an adversarial setting. Using the proposed measure ARC, DNN models after Siamese training exhibit better robustness than the state-of-the-art baselines.", "creator": "LaTeX with hyperref package"}}}