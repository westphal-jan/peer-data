{"id": "1612.00370", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Dec-2016", "title": "Improved Image Captioning via Policy Gradient optimization of SPIDEr", "abstract": "In this paper, we propose a novel training procedure for image captioning models based on policy gradient methods. This allows us to directly optimize for the metrics of interest, rather than just maximizing likelihood of human generated captions. We show that by optimizing for standard metrics such as BLEU, CIDEr, METEOR and ROUGE, we can develop a system that improve on the metrics and ranks first on the MSCOCO image captioning leader board, even though our CNN-RNN model is much simpler than state of the art models. We further show that by also optimizing for the recently introduced SPICE metric, which measures semantic quality of captions, we can produce a system that significantly outperforms other methods as measured by human evaluation. Finally, we show how we can leverage extra sources of information, such as pre-trained image tagging models, to further improve quality", "histories": [["v1", "Thu, 1 Dec 2016 18:10:48 GMT  (2714kb,D)", "http://arxiv.org/abs/1612.00370v1", "Under review at CVPR 2017"], ["v2", "Wed, 14 Dec 2016 08:36:58 GMT  (2707kb,D)", "http://arxiv.org/abs/1612.00370v2", "Under review at CVPR 2017"], ["v3", "Sat, 18 Mar 2017 09:24:38 GMT  (4951kb,D)", "http://arxiv.org/abs/1612.00370v3", "Under review at ICCV 2017"]], "COMMENTS": "Under review at CVPR 2017", "reviews": [], "SUBJECTS": "cs.CV cs.CL", "authors": ["siqi liu", "zhenhai zhu", "ning ye", "sergio guadarrama", "kevin murphy"], "accepted": false, "id": "1612.00370"}, "pdf": {"name": "1612.00370.pdf", "metadata": {"source": "CRF", "title": "Optimization of image description metrics using policy gradient methods", "authors": ["Siqi Liu", "Zhenhai Zhu", "Ning Ye", "Sergio Guadarrama", "Kevin Murphy"], "emails": ["siqi.liu@cs.ox.ac.uk", "zhenhai@google.com", "nye@google.com", "sguada@google.com", "kpmurphy@google.com"], "sections": [{"heading": "1 Introduction", "text": "In fact, it is as if most people are able to know themselves and understand what they are doing. (...) In fact, it is as if people are able to know and understand themselves. \"(...) In fact, it is as if people are able to know themselves.\" (...) \"It is as if people are able to know themselves.\" (...) \"It is as if people are able to know themselves.\" (...) \"It is as if people are able to know themselves.\" (...) \"It is as if people are able to know themselves.\" (...) It is as if people are able to know themselves. \"(...)\" It is as if people are able to know themselves. \"(...) It is as if people are able to know themselves. (...) It is as if people are able to know themselves. (...) It is as if people are able to know themselves. (...) It is as if people are able to know themselves."}, {"heading": "2 Related work", "text": "There is an extensive work on caption that is too large to review here. See [5] for a current summary. The most relevant piece of previous work is [21], which proposed one of the first encoder decoder networks for caption, called \"Show and Tell\" (ST), also known as \"Neural Image Captioner\" (NIC). Since our contribution is orthogonal to the underlying model architecture, we use the same ST model for most of these papers. Numerous extensions of the basic encoder decoder frame have been proposed, many of which attempt to enrich the rendering of the encoder so that the image is not only represented by the global features of a CNN. A common approach is the use of an attention mechanism that allows the decoder to focus on certain parts of the input image. [24] This approach is used in [24] those who suggest the \"Show, Attend and Tell\" model, see also an optimization of the upper level of the object, [25] for an extension to the newer one."}, {"heading": "3 Methods", "text": "In this section, we will explain our approach in detail. First, we will discuss the policy gradient algorithm that can optimize any type of reward function, then we will discuss which reward function to use, then we will discuss the model itself, which is a standard CNN-RNN, and finally, we will discuss an extension of the basic model that uses a pre-trained image marker for improved performance."}, {"heading": "3.1 Training using policy gradient", "text": "example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example example"}, {"heading": "3.2 Reward Functions for the Policy Gradient", "text": "We can use our PG method to optimize many different reward functions. Frequent choices include BLEU, CIDER, METEOR, and ROUGE. Code for all of these metrics is included as part of the MSCOCO evaluation tool.1 We chose to use a weighted combination of all of these. Since these metrics are not on the same scale, we chose in our experiments the set of weights so that all metrics are roughly the same magnitude. In1 https: / / github.com / tylin / coco-caption.particular, we chose 0.5, 0.5, 1.0, 1.0, 5.0, 2.0 for BLEU-1, BLEU-2, BLEU-3, BLEU-4, METEOR, and ROUGE respectively. Optimizing this weighted combination of BCMR gives state-of-the-art results on the MSCOCO test group, as we have in Section 4.2.A problem with human metrics, hence the MBICE is not related to them."}, {"heading": "3.3 Encoder-decoder architecture", "text": "We use a CNN RNN architecture similar to that proposed in the original show-tell paper [21]. A high-level diagram is shown in Figure 2. Each symbol in the vocabulary is embedded as a 512-dimensional vector whose values are randomly initialized.The CNN encoder is implemented as a single-layer Inception V3 [17] network that is randomly initialized on ImageNet3.The2 https: / / github.com / peteanderson80 / SPICE. 3We used the open source implementation available under: RNN decoder is a single-layer LSTM with a state size of 512 units that was randomly initialized. \u2212 Each image is encoded by Inception V3 as a dense feature vector of dimension 2, 048, which is then projected onto the 512 dimension with a linear layer and as the initial state of the RNN decoder, we always apply the truth point to the RPG."}, {"heading": "3.4 Adding visual tags", "text": "In this section we describe a simple extension of the basic model from section 3.3. Specifically, we use an internal image marking system that is trained on a large but noisily labeled record called JFT (see [6] for details). This system provides a vector of the probability values p = {p0, p1,..., pN} for a set of N = 3, 713 classes in which JFT [0, 1] indicates the probability of the presence of a visual attribute of the class i in the picture. Typically, 60-70 tags have a non-zero rating for each image. See Figure 1 for some sample editions from this system. A high-ranking diagram of this architecture indicates the presence of a visual attribute of the class in the picture. Typically, 60-70 tags have a non-zero rating for each picture. See Figure 1 for some sample editions from this system. A high-ranking diagram of this architecture Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Score Sc"}, {"heading": "4 Results", "text": "In this section, we report on the results obtained by various methods on the MSCOCO ICE dataset. These are 82,081 training images and 40,137 validation images, each with at least 5 subtitles. Following standard practice for methods evaluated on the MSCOCO test server, we keep a small subset of 1,665 validation images available for hyperparameter tuning and use the remaining combined training and validation sets for training. We process the text data by replacing words that occur less than 4 times in the 82k training set with UNK, resulting in a vocabulary size of 8,855 (identical to the vocabulary used in [21]. During training, we keep all captions at their maximum lengths. During testing, the generated sequences are truncated to 30 symbols in all experiments. We report the results of the following 7 systems: Maximum probability of the MSCOCO ICE show: Er Tell model was trained."}, {"heading": "4.1 Qualitative Analysis", "text": "We see that PG-SPICE tends to produce ungrammatic sentences, with many repeating phrases. This is because SPICE measures how well the scene graph generated by a sentence matches the Ground Truth graph, but is relatively insensitive to syntactical quality. However, we also see that combining SPICE with CIDER gives us much better results."}, {"heading": "4.2 Results using automatic metrics on MSCOCO", "text": "In this section, we quantitatively evaluate the methods using the MSCOCO online evaluation server 4.Table 2 shows the performance of various state-of-the-art models in the caption literature. In particular, we show the most powerful models according to the official MSCOCO C-5 ranking at the time of writing (November 2016). We also report the results of 6 experiments that we have conducted, which all of our models except PG-SPICE.We start by discussing the results of our base model ST without using the Image Tagger. We see that all PG methods exceed MLE training. We also see that PG methods significantly exceed all previous methods, including those that use more complex models, such as those based on attention (Montreal / Toronto, ATT, Review Net), those that use more complex decoders (Berkeley LRCN), and those that use high-grade visual attributes (MSEr @ SCSRA, ATT).Our image processing method does not exceed the MPG available directly for MPG [also for MPG]."}, {"heading": "4.3 Results using human metrics on MSCOCO", "text": "Knowing that BCMR metrics are not well correlated with human metrics of quality, we have decided to evaluate the methods using human guidebooks. In particular, we use a crowd-sourcing platform that uses rating agencies that have previously had experience evaluating captions and other computer vision models. We showed each image-capture pair to 3 different raters, and asked them to rate it on a scale of 4 points, depending on whether the caption is \"bad,\" \"good\" or \"excellent.\" 5 We then take the majority to get the final rating. If no majority is found, the rating is considered unknown, and that image is excluded from the analysis. To simplify the presentation, we focus on measuring the percentage of captions that are classified as \"not bad,\" which we call the union of \"okay,\" \"\" \"good\" and \"excellent.\""}, {"heading": "5 Conclusion and future work", "text": "In this paper, we have shown that using methods to optimize the political gradient to optimize the BCMR and SPICE metrics results in much better performance than the standard maximum probability training. However, we have also seen that none of these existing metrics is perfect; in fact, in order to achieve our best results (as judged by humans), we had to combine the SPIC and CIDER metrics. In future work, we would be pleased 6 For example, some captions contain repeated words, such as \"this is a picture of a modern kitchen.\" Others contain typos, such as \"a blue and white truck with pants in its flatbed.\" And some do not make semantic sense, such as \"A crowd parked near a double-decker bus.\" to explore the possibility of using GANs [28] to automatically learn to distinguish good from bad captions, without having to examine the quality of metric metric metric metric metric metrics as an alternative to the explicit method of this MPG]."}], "references": [{"title": "Spice: Semantic propositional image caption evaluation", "author": ["P. Anderson", "B. Fernando", "M. Johnson", "S. Gould"], "venue": "In ECCV, 2016", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2016}, {"title": "An Actor-Critic algorithm for sequence", "author": ["D. Bahdanau", "P. Brakel", "K. Xu", "A. Goyal", "R. Lowe", "J. Pineau", "A. Courville", "Y. Bengio"], "venue": "prediction. Arxiv,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2016}, {"title": "METEOR: An automatic metric for MT evaluation with improved correlation with human judgments. In Proc. ACL workshop on intrinsic and extrinsic evaluation measures for machine translation", "author": ["S. Banerjee", "A. Lavie"], "venue": "and/or summarization,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2005}, {"title": "Scheduled sampling for sequence prediction with recurrent neural networks", "author": ["S. Bengio", "O. Vinyals", "N. Jaitly", "N. Shazeer"], "venue": "In NIPS,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2015}, {"title": "Automatic description generation from images: A survey of models, datasets, and evaluation measures", "author": ["R. Bernardi", "R. Cakici", "D. Elliott", "A. Erdem", "E. Erdem", "N. Ikizler-Cinbis", "F. Keller", "A. Muscat", "B. Plank"], "venue": "J. of AI Research,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2016}, {"title": "Information-theoretical label embeddings for large-scale image classification", "author": ["F. Chollet"], "venue": "Arxiv, 19 July 2016", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2016}, {"title": "Long-term recurrent convolutional networks for visual recognition and description", "author": ["J. Donahue", "L. Anne Hendricks", "S. Guadarrama", "M. Rohrbach", "S. Venugopalan", "K. Saenko", "T. Darrell"], "venue": "In CVPR,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2015}, {"title": "Generating visual explanations", "author": ["L.A. Hendricks", "Z. Akata", "M. Rohrbach", "J. Donahue", "B. Schiele", "T. Darrell"], "venue": "ECCV, 2016", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2016}, {"title": "How (not) to train your generative model: Scheduled sampling, likelihood", "author": ["F. Husz\u00e1r"], "venue": "adversary? Arxiv,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2015}, {"title": "Automatic evaluation of summaries using n-gram co-occurrence statistics", "author": ["C.-Y. Lin", "E. Hovy"], "venue": "In NAACL, pages 71\u201378,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2003}, {"title": "Microsoft  Under review as a conference paper at CVPR 2017 COCO: Common objects in context", "author": ["T.-Y. Lin", "M. Maire", "S. Belongie", "J. Hays", "P. Perona", "D. Ramanan", "P. Doll\u00e1r", "C. Lawrence Zitnick"], "venue": "In ECCV,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2014}, {"title": "Reward augmented maximum likelihood for neural structured prediction", "author": ["M. Norouzi", "S. Bengio", "Z. Chen", "N. Jaitly", "M. Schuster", "Y. Wu", "D. Schuurmans"], "venue": "In NIPS,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2016}, {"title": "BLEU: A method for automatic evaluation of machine translation", "author": ["K. Papineni", "S. Roukos", "T. Ward", "W.-J. Zhu"], "venue": "In Proc. ACL,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2002}, {"title": "Sequence level training with recurrent neural networks", "author": ["M. Ranzato", "S. Chopra", "M. Auli", "W. Zaremba"], "venue": "Arxiv,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2015}, {"title": "Minimum risk training for neural machine translation", "author": ["S. Shen", "Y. Cheng", "Z. He", "W. He", "H. Wu", "M. Sun", "Y. Liu"], "venue": "In Proc. ACL,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2016}, {"title": "Policy gradient methods for reinforcement learning with function approximation", "author": ["R.S. Sutton", "D. Mc Allester", "S. Singh", "Y. Mansour"], "venue": "In NIPS,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1999}, {"title": "Rethinking the inception architecture for computer vision", "author": ["C. Szegedy", "V. Vanhoucke", "S. Ioffe", "J. Shlens", "Z. Wojna"], "venue": "arXiv preprint arXiv:1512.00567,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2015}, {"title": "Rich image captioning in the wild", "author": ["K. Tran", "X. He", "L. Zhang", "J. Sun", "C. Carapcea", "C. Thrasher", "C. Buehler", "C. Sienkiewicz"], "venue": "In CVPR,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2016}, {"title": "Cider: Consensus-based image description evaluation", "author": ["R. Vedantam", "C. Lawrence Zitnick", "D. Parikh"], "venue": "In CVPR, pages 4566\u20134575,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2015}, {"title": "Captioning images with diverse", "author": ["S. Venugopalan", "L.A. Hendricks", "M. Rohrbach", "R. Mooney", "T. Darrell", "K. Saenko"], "venue": "objects. Arxiv,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2016}, {"title": "Show and tell: A neural image caption generator", "author": ["O. Vinyals", "A. Toshev", "S. Bengio", "D. Erhan"], "venue": "In CVPR,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2015}, {"title": "Simple statistical gradient-following algorithms for connectionist reinforcement learning", "author": ["R.J. Williams"], "venue": "Machine Learning J., 8(3-4):229\u2013256,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 1992}, {"title": "What value high level concepts in vision to language problems", "author": ["Q. Wu", "C. Shen", "A. van den Hengel", "L. Liu", "A. Dick"], "venue": "In CVPR,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2016}, {"title": "Show, attend and tell: Neural image caption generation with visual attention", "author": ["K. Xu", "J. Ba", "R. Kiros", "K. Cho", "A. Courville", "R. Salakhutdinov", "R. Zemel", "Y. Bengio"], "venue": "In ICML,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2015}, {"title": "Review networks for caption generation", "author": ["Z. Yang", "Y. Yuan", "Y. Wu", "R. Salakhutdinov", "W.W. Cohen"], "venue": "In NIPS,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2016}, {"title": "Boosting image captioning with attributes", "author": ["T. Yao", "Y. Pan", "Y. Li", "Z. Qiu", "T. Mei"], "venue": "In OpenReview,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2016}, {"title": "Image captioning with semantic attention", "author": ["Q. You", "H. Jin", "Z. Wang", "C. Fang", "J. Luo"], "venue": "arXiv preprint arXiv:1603.03925,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2016}, {"title": "SeqGAN: Sequence generative adversarial nets with policy gradient", "author": ["L. Yu", "W. Zhang", "J. Wang", "Y. Yu"], "venue": "Arxiv, 18 Sept. 2016", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2016}, {"title": "Reinforcement learning neural turing machines-revised", "author": ["W. Zaremba", "I. Sutskever"], "venue": "arXiv preprint arXiv:1505.00521,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2015}], "referenceMentions": [{"referenceID": 4, "context": "There are many ways to perform this task (see [5] for a recent review).", "startOffset": 46, "endOffset": 49}, {"referenceID": 13, "context": "This discrepancy has been called \u201cexposure bias\u201d [14], and can lead to poor performance at inference time.", "startOffset": 49, "endOffset": 53}, {"referenceID": 3, "context": "One approach to mitigate this, known as \u201cscheduled sampling\u201d, was proposed in [4]; however, this method has been shown to be statistically inconsistent [9], although it improves model performance in practice.", "startOffset": 78, "endOffset": 81}, {"referenceID": 8, "context": "One approach to mitigate this, known as \u201cscheduled sampling\u201d, was proposed in [4]; however, this method has been shown to be statistically inconsistent [9], although it improves model performance in practice.", "startOffset": 152, "endOffset": 155}, {"referenceID": 12, "context": "A better approach is to optimize for the same metrics that we use at test time, such as BLEU [13], CIDEr [19], METEOR [3], and ROUGE [10]; we shall call these metrics \u201cBCMR\u201d for short.", "startOffset": 93, "endOffset": 97}, {"referenceID": 18, "context": "A better approach is to optimize for the same metrics that we use at test time, such as BLEU [13], CIDEr [19], METEOR [3], and ROUGE [10]; we shall call these metrics \u201cBCMR\u201d for short.", "startOffset": 105, "endOffset": 109}, {"referenceID": 2, "context": "A better approach is to optimize for the same metrics that we use at test time, such as BLEU [13], CIDEr [19], METEOR [3], and ROUGE [10]; we shall call these metrics \u201cBCMR\u201d for short.", "startOffset": 118, "endOffset": 121}, {"referenceID": 9, "context": "A better approach is to optimize for the same metrics that we use at test time, such as BLEU [13], CIDEr [19], METEOR [3], and ROUGE [10]; we shall call these metrics \u201cBCMR\u201d for short.", "startOffset": 133, "endOffset": 137}, {"referenceID": 15, "context": "[16] to optimize them, by treating the score of a candidate sentence as analogous to a reward signal in a reinforcement learning setting.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "The idea of using PG to optimize BLEU score for image captioning has been previously explored in [14].", "startOffset": 97, "endOffset": 101}, {"referenceID": 21, "context": "These authors used a special case of PG known as REINFORCE [22], which they combined with MLE to create a learning method called \u201cMIXER\u2019.", "startOffset": 59, "endOffset": 63}, {"referenceID": 27, "context": "First, we use an improved implementation of PG, which estimates the expected future reward of each intermediate action using Monte Carlo rollouts, as in [28].", "startOffset": 153, "endOffset": 157}, {"referenceID": 10, "context": "The proposed training algorithm is able to surpass the previous state of the art on the MSCOCO captioning leaderboard [11], despite the simplicity of the captioning model itself (Section 4).", "startOffset": 118, "endOffset": 122}, {"referenceID": 0, "context": "Second, we extend our technique to directly optimize the recently introduced SPICE metric [1], which has been shown to correlate much more closely with human judgement of semantic quality than previous BCMR metrics.", "startOffset": 90, "endOffset": 93}, {"referenceID": 5, "context": "As a proof of concept, we show how to integrate an image tagging system [6], which has been pre-trained on a large quantity of noisily labeled web images, as an additional input to the decoder (beyond just the CNN features).", "startOffset": 72, "endOffset": 75}, {"referenceID": 4, "context": "See [5] for a recent summary.", "startOffset": 4, "endOffset": 7}, {"referenceID": 20, "context": "The most relevant piece of prior work is [21], who proposed one of the first encoder-decoder networks for image captioning, called \u201cShow and Tell\u201d (ST), also known as \u201dNeural Image Captioner\u201d (NIC).", "startOffset": 41, "endOffset": 45}, {"referenceID": 23, "context": "This approach is used in [24], who proposed the \u201cShow, Attend and Tell\u201d model.", "startOffset": 25, "endOffset": 29}, {"referenceID": 24, "context": "See also [25] for a more recent extension, that applies an attentional RNN on top of the encoder before passing to the decoder.", "startOffset": 9, "endOffset": 13}, {"referenceID": 22, "context": "This usually takes the form of an image tagger [23, 26], but can also use specialized modules, such as object detectors [20], or face detection and landmark recognition [18].", "startOffset": 47, "endOffset": 55}, {"referenceID": 25, "context": "This usually takes the form of an image tagger [23, 26], but can also use specialized modules, such as object detectors [20], or face detection and landmark recognition [18].", "startOffset": 47, "endOffset": 55}, {"referenceID": 19, "context": "This usually takes the form of an image tagger [23, 26], but can also use specialized modules, such as object detectors [20], or face detection and landmark recognition [18].", "startOffset": 120, "endOffset": 124}, {"referenceID": 17, "context": "This usually takes the form of an image tagger [23, 26], but can also use specialized modules, such as object detectors [20], or face detection and landmark recognition [18].", "startOffset": 169, "endOffset": 173}, {"referenceID": 11, "context": "[12, 28, 2, 15]).", "startOffset": 0, "endOffset": 15}, {"referenceID": 27, "context": "[12, 28, 2, 15]).", "startOffset": 0, "endOffset": 15}, {"referenceID": 1, "context": "[12, 28, 2, 15]).", "startOffset": 0, "endOffset": 15}, {"referenceID": 14, "context": "[12, 28, 2, 15]).", "startOffset": 0, "endOffset": 15}, {"referenceID": 7, "context": "Recent work in visual explaination such as [8] employed REINFORCE to optimize sequence level reward such that generated captions are class discriminative, which is orthogonal to our goal.", "startOffset": 43, "endOffset": 46}, {"referenceID": 13, "context": "As far as we know, the only paper to explore objectives beyond maximum likelihood for image captioning is the MIXER method of [14].", "startOffset": 126, "endOffset": 130}, {"referenceID": 15, "context": "To compute the gradient of J(\u03b8), we can use the policy gradient theorem from [16].", "startOffset": 77, "endOffset": 81}, {"referenceID": 1, "context": "In the special case of deterministic transition functions, this theorem simplifies as shown below (see [2] for a proof): \u2207\u03b8V\u03b8(s0) = Eg1:T \uf8ee\uf8f0 T \u2211", "startOffset": 103, "endOffset": 106}, {"referenceID": 27, "context": "For this, we will follow [28] and use Monte Carlo rollouts.", "startOffset": 25, "endOffset": 29}, {"referenceID": 28, "context": "Here, we simply refer to prior work ([29], [22]) for a full derivation of this property.", "startOffset": 37, "endOffset": 41}, {"referenceID": 21, "context": "Here, we simply refer to prior work ([29], [22]) for a full derivation of this property.", "startOffset": 43, "endOffset": 47}, {"referenceID": 20, "context": "Figure 2: Model architecture of Show and Tell image captioning system [21].", "startOffset": 70, "endOffset": 74}, {"referenceID": 0, "context": "One problem with the BCMR metrics is that they are not well correlated with human judgment individually [1].", "startOffset": 104, "endOffset": 107}, {"referenceID": 0, "context": "We therefore also tried optimizing the recently introduced SPICE metric [1], which better reflects human estimates of quality.", "startOffset": 72, "endOffset": 75}, {"referenceID": 20, "context": "We use a CNN-RNN architecture similar to the one proposed in the original Show-Tell paper [21].", "startOffset": 90, "endOffset": 94}, {"referenceID": 16, "context": "The encoder CNN is implemented as an Inception-V3 [17] network pretrained on ImageNet3.", "startOffset": 50, "endOffset": 54}, {"referenceID": 5, "context": "In particular, we leverage an in-house image tagging system, which is trained on a large, but noisily labeled, dataset called JFT (see [6] for details).", "startOffset": 135, "endOffset": 138}, {"referenceID": 0, "context": ", pN} for a set of N = 3, 713 classes, where pi \u2208 [0, 1] indicates the probability of presence of a visual attribute of class i in the image.", "startOffset": 50, "endOffset": 56}, {"referenceID": 25, "context": "Our attribute augmented model is similar to LSTM-A5, recently introduced in [26].", "startOffset": 76, "endOffset": 80}, {"referenceID": 22, "context": "(See also [23] for some related work on using visual tags for image captioning.", "startOffset": 10, "endOffset": 14}, {"referenceID": 20, "context": "We preprocess the text data by lower casing, and replacing words which occur less than 4 times in the 82k training set with UNK; this results in a vocabulary size of 8,855 (identical to the one used in [21]).", "startOffset": 202, "endOffset": 206}, {"referenceID": 20, "context": "Note that our implementation gives similar results to [21], which uses the same model, but was trained with scheduled sampling.", "startOffset": 54, "endOffset": 58}, {"referenceID": 13, "context": "Our PG method also outperforms MIXER [14], which is the only prior work (to the best of our knowledge) which uses PG for image captioning.", "startOffset": 37, "endOffset": 41}, {"referenceID": 25, "context": "MSM@MSRA [26] 0.", "startOffset": 9, "endOffset": 13}, {"referenceID": 24, "context": "330 Review Net [25] 0.", "startOffset": 15, "endOffset": 19}, {"referenceID": 26, "context": "313 ATT [27] 0.", "startOffset": 8, "endOffset": 12}, {"referenceID": 20, "context": "316 Google [21] 0.", "startOffset": 11, "endOffset": 15}, {"referenceID": 6, "context": "309 Berkeley LRCN [7] 0.", "startOffset": 18, "endOffset": 21}, {"referenceID": 13, "context": "306 MIXER* [14] 0.", "startOffset": 11, "endOffset": 15}, {"referenceID": 23, "context": "292 Montreal/Toronto [24] 0.", "startOffset": 21, "endOffset": 25}, {"referenceID": 27, "context": "to explore the possibility of using sequence GANs [28] to automatically learn to distinguish good from bad captions, without having to specify the quality metric explicitly.", "startOffset": 50, "endOffset": 54}, {"referenceID": 1, "context": "For example, we might investigate an actor-critic method, similar to [2], or the ML-PG hybrid described in [12].", "startOffset": 69, "endOffset": 72}, {"referenceID": 11, "context": "For example, we might investigate an actor-critic method, similar to [2], or the ML-PG hybrid described in [12].", "startOffset": 107, "endOffset": 111}, {"referenceID": 19, "context": "[20]).", "startOffset": 0, "endOffset": 4}], "year": 2016, "abstractText": "In this paper, we propose a novel training procedure for image captioning models based on policy gradient methods. This allows us to directly optimize for the metrics of interest, rather than just maximizing likelihood of human generated captions. We show that by optimizing for standard metrics such as BLEU, CIDEr, METEOR and ROUGE, we can develop a system that improve on the metrics and ranks first on the MSCOCO image captioning leader board, even though our CNN-RNN model is much simpler than state of the art models. We further show that by also optimizing for the recently introduced SPICE metric, which measures semantic quality of captions, we can produce a system that significantly outperforms other methods as measured by human evaluation. Finally, we show how we can leverage extra sources of information, such as pre-trained image tagging models, to further improve quality.", "creator": "LaTeX with hyperref package"}}}