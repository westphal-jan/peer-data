{"id": "1612.00385", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Dec-2016", "title": "Temporal Attention-Gated Model for Robust Sequence Classification", "abstract": "Typical techniques for sequence classification are designed for well-segmented sequences which has been edited to remove noisy or irrelevant parts. Therefore, such methods cannot be easily applied on noisy sequences which are expected in real-world applications. We present the Temporal Attention-Gated Model (TAGM) which is able to deal with noisy sequences. Our model assimilates ideas from attention models and gated recurrent networks. Specifically, we employ an attention model to measure the relevance of each time step of a sequence to the final decision. We then use the relevant segments based on their attention scores in a novel gated recurrent network to learn the hidden representation for the classification. More importantly, our attention weights provide a physically meaningful interpretation for the salience of each time step in the sequence. We demonstrate the merits of our model in both interpretability and classification performance on a variety of tasks, including speech recognition, textual sentiment analysis and event recognition.", "histories": [["v1", "Thu, 1 Dec 2016 19:11:24 GMT  (3920kb,D)", "https://arxiv.org/abs/1612.00385v1", null], ["v2", "Sat, 15 Apr 2017 12:53:28 GMT  (4716kb,D)", "http://arxiv.org/abs/1612.00385v2", "Accepted by CVPR 2017"]], "reviews": [], "SUBJECTS": "cs.CV cs.CL", "authors": ["wenjie pei", "tadas baltru\\v{s}aitis", "david m j tax", "louis-philippe morency"], "accepted": false, "id": "1612.00385"}, "pdf": {"name": "1612.00385.pdf", "metadata": {"source": "CRF", "title": "Temporal Attention-Gated Model for Robust Sequence Classification", "authors": ["Wenjie Pei", "Tadas Baltru\u0161aitis", "David M.J. Tax", "Louis-Philippe Morency"], "emails": ["W.Pei-1@tudelft.nl,", "tbaltrus@cs.cmu.edu,", "D.M.J.Tax@tudelft.nl,", "morency@cs.cmu.edu"], "sections": [{"heading": "1. Introduction", "text": "Most existing sequencer models are designed for well-maintained sequences and can therefore be very time-consuming."}, {"heading": "2. Related Work", "text": "This year, it is more than ever before in the history of the city."}, {"heading": "3. Temporal Attention-Gated Model", "text": "To input an unsegmented sequence of potentially noisy observations, our goal is: (1) calculate a Salience Score for each time step observation in our input sequence and (2) construct a hidden representation based on the Salience Scores that are best suited to the sequence classification task. To achieve these goals, we propose the Temporal Attention-Gated Model (TAGM), which consists of two modules: the Time Attention Module and Recurring Attention Units. Our TAGM model can be trained efficiently, and the graphical structure of the model is shown in Figure 2."}, {"heading": "3.1. Recurrent Attention-Gated Units", "text": "The aim of the recurring attention units is to learn a hidden sequence representation that integrates the attention values (derived from the temporal attention module discussed in the next section). In order to integrate the attention values into the recurring network units, we define an attention gate to control how much information flows from the input of the current time step based on the emphasis and relevance for the final task. Formally, considering an input sequence x1,..., T = {x1,.., xT} of length T, in which xt \"RD\" denotes the observation in the t-time step, the attention value in the t-time step is referred to as at, which represents a scalar value indicating the production of the current time step until the final decision.To this end, we define our core recurring process, in which the hidden state h. \"vt\" is hidden as a configuration \u00b7 modeled in the previous (1):"}, {"heading": "3.2. Temporal Attention Module", "text": "The aim of this module is to estimate the emphasis and relevance of each sequence observation, which should be based not only on the input observation in the current time step, but also on information from adjacent observations in both directions. To model this adjacent influence, we derive the attention value in Equation 1 using a bidirectional RNN: at = \u03c3 (m > (\u2212 \u2192 t; \u2190 \u2212 h t) + b) (4) Herein, m is the weight vector of our fusion layer, which integrates both directional levels of our bidirectional RNN, and b is the bias term. As an activation function, Equation 4 uses a sigmoid function to limit the attention weight so that it can lie between [0, 1]. \u2212 h t and --h t are the hidden representations of a bidirectional RNN model: \u2212 h t = g (\u2212 Wxt + b \u00b7 b \u00b7 b \u00b7 b \u00b7 b \u00b7 b \u00b7 b \u2212 n is another choice)."}, {"heading": "3.3. End-to-End Parameter Learning", "text": "Suppose we get a training set D = {(x (n) 1,..., T, y (n))} n = 1,..., N containing N sequences of length T and their corresponding designations y (n). x (n) t-RD denotes the observation in the t-th time step of the n-th sample and T can vary from sequence to sequence. Together we learn the two TAGM modules (temporal attention modules and recurring attention units) and the final sequence classifier by minimizing the conditional negative probability of the training data in terms of parameters: L = \u2212 N \u2211 n = 1logP (y (n) 1,..., T) (7) Since all three modules (including the final sequence classifier) are analytically differentiable, our TAGM model can easily be trained in an end-to-end manner."}, {"heading": "3.4. Comparison with LSTM and GRU", "text": "While our model resembles RNN variants such as GRU and LSTM, it is specifically designed to detect salicity and has four key differences compared to them: \u2022 We focus only on a scalar attention value to measure the relevance of the current time step, rather than generally modelling the multidimensional values of the gate for each hidden unit, as is done by GRU and LSTM. In this way, we can obtain an interpretable salicity detection (demonstrated by three tasks in Section 4). \u2022 We separate attention modeling and the recurring hidden representation of learning as two independent modules to reduce the degree of coupling. \u2022 One of the advantages is our ability to adapt the specific recursive structure for each module with different complexity according to requirements (e.g. the different size of hidden units in two modules of the TAGM in Table 1). \u2022 We use a bidirectional RNum, not taking into account both the preceding GRN and the preceding information."}, {"heading": "4. Experiments", "text": "We conducted experiments with TAGM on three publicly available datasets, which were selected to show generalizations across different tasks and modalities: (1) speech recognition on an audio dataset, (2) mood analysis on a text dataset, and (3) event detection on a video dataset. Experimental setup, which is experimentally shared. For all recurring networks mentioned in this thesis (TAGM, GRU, LSTM, and plain-RNN), the number of hidden units is matched by selecting the best configuration from the option set {64, 128, 256} using a validation set. The default value is validated from the option set {0.0, 0.25, 0.5} to avoid potential overmatch. We use RMSprop as an optimization algorithm for gradient descent with a gradient clipping between \u2212 5 and 5 [2] to match the TAGM function available for the learning effect and the learning effect in each region."}, {"heading": "4.1. Speech Recognition Experiments", "text": "First, we conduct preliminary experiments with a modified dataset built from the Arabic numerical dataset [9] to (1) evaluate the effectiveness of the two main modules of the TAGM; (2) compare the generalizability of three different gate setup repetition models (TAGM, GRU, and LSTM) with the different size of training data."}, {"heading": "4.1.1 Dataset", "text": "Each sequence consists of 13-dimensional Mel Frequency Cepstral Coefficents (MFCCs) sampled at 11,025 Hz, 16-bit using a hamming window. We append white noise to the beginning and end of each sample to simulate the problem with unsegmented sequences; the length of unrelated sub-sequences before and after the original audio clips is randomized to ensure that the model does not learn to focus only on the middle of the sequence."}, {"heading": "4.1.2 Experimental Setup", "text": "We use the same data division as Hammami and Bedda [9]: 6600 samples as a training set and 2200 samples as a test set. We continue to set aside 1100 samples from the training set as a validation set. There is no overlap between the subjects in the three sets. We compare the performance of our TAGM with three types of baseline models: Attention Module + Neural Network (AM-NN). In order to investigate the effects of our recurring attention unit, we include a baseline model that applies a feed-forward network1https: / / github.com / wenjiepei / TAGMdirectly to the temporal attention module. In this AM-NN model, v is defined as a weighted sum of input functions: v = T-t = 1 at \u00b7 xt, h = g (W \u00b7 v + b) (8) sequence classification is performed by converting it into a soft-max layer (see TAGR structure for our AGR)."}, {"heading": "4.1.3 Results and Discussion", "text": "Evaluation of Performance Table 1 represents the classification of multiple sequencers on Arabic datasets. To investigate the effect of manually added spatial information, we conduct experiments on clean and spacious versions of datasets. Our model achieves the best results in a one-sided recursive configuration. This probably leads to a better generalization of our model on the relatively small data collection and comparable performance on the level of pure data. Our model achieves the best results on the level of recursive configuration."}, {"heading": "4.2. Sentiment Analysis Experiments", "text": "Sentiment analysis is a popular research topic in the field of Natural Language Processing (NLP), which aims to identify the aspects underlying a span of text [25]. We conduct sentiment analysis experiments to evaluate the performance of our TAGM model in terms of text modality."}, {"heading": "4.2.1 Dataset", "text": "The Stanford Sentiment Treebank (SST) [36] is a corpus of film reviews. It consists of 11,855 sentences, each of which receives a score indicating sentimental attitudes toward the film reviews. It provides two types of annotations, sentiment annotations at the sentence level (with a total of 11,855 sentences) and phrase level (with a total of 215,154 sentences). Guardian-level and phrase-level annotations have two resolutions: binary classification task (positive or negative) and fine-grained task (5-level grades)."}, {"heading": "4.2.2 Experimental Setup", "text": "According to previous work [36], for each word of the sentences we use 300 d word vectors (300 dimensions) preschooled via the Common Crawl [28]. Our model is well suited for emotional analysis using sentence labels. Nevertheless, we also conduct experiments with sentence labels in order to have a fair and intuitive comparison with modern fundamentals. We follow the same data split as described by Socher et al. [36]: 8544 / 1101 / 2210 samples are used for training, validation and testing in the 5-grade task, with the corresponding splits in the binary classification task being 6920 / 872 / 1821."}, {"heading": "4.2.3 Results and Discussion", "text": "Evaluation of Classification Performance We perform two sets of experiments to evaluate the performance of our model compared to the base models. As our model is designed for modelling unsegmented and possibly low-noise sequences, it is better suited to use only tenent level labels, although tenent level labels are also available in SST datasets. Table 2 shows the experimental results of several sequential models trained only with tenent level labels. It is worth noting that our model achieves the best results in both the binary classification task and the fine-grained (5-grade) task. LSTM and GRU exceed the plain RNN model due to the information filtering capability provided by additional gates. It is worth noting that our model achieves better performance than LSTM with only half of the hidden parameters. To make a fair comparison with the existing sensor analysis models, we have to perform the second set of labels and labels in table 3."}, {"heading": "4.3. Event recognition Experiments", "text": "We then conduct video event detection experiments to evaluate our model in terms of visual modality."}, {"heading": "4.3.1 Dataset", "text": "The Columbia Consumer Video (CCV) Database [16] is an unrestricted video database collected from YouTube videos without post-editing. It consists of 9317 web videos with an average duration of 80 seconds (a total of 210 hours). With the exception of some negative background videos, each video is manually divided into one or more of 20 semantic categories such as \"basketball,\" \"ice skating,\" \"cycling,\" \"birthday,\" etc. It is a very sophisticated database due to the many noisy and irrelevant segments contained in these videos. 4.3.2 Experimental Setup According to Jiang et al [16] we use the same split for training and test sets: 4659 videos as a training set and 4658 as a test set. We compare our model with the basic method [15] on this dataset, which performs the classification separately with the support Vector Machine (SVM)."}, {"heading": "4.3.3 Results and Discussion", "text": "Evaluation of Classification Performance. We compare our model with the event detection system proposed by the authors of the datasets [15]. Table 4 shows the performance of multiple event detection models, with our TAGM clearly outperforming the other recurring models. BOW + SVM baseline uses the one-on-one strategy to train a separate classifier for each event, while our model trains all events in a single classifier. Our model still shows encouraging results, as it is a difficult but appealing task for TAGM to capture distinctive sections for 20 events simultaneously with complex scenes. In addition, our TAGM can provide a meaningful interpretation that the base models cannot provide. Sequence Salience Salience Detection. Detection of highlights for CCV databases is a difficult but appealing task due to the complex and long scenes in videos. \"Figure 6 shows some examples where TAGM will correctly capture the baseline event with the punctuation segments of the event most likely to be the attention-weighted event in the corner of the baseball."}, {"heading": "5. Conclusion", "text": "In this paper, we introduced the Temporal Attention-Gated Model (TAGM), a new model for classifying noisy and unsegmented sequences. Inspired by attention models and gated recurrent networks, the model is able to detect prominent parts of the sequence while ignoring irrelevant and noisy ones. The resulting hidden representation suffers less from the effects of noise and therefore performs better. In addition, the attention values learned provide a physically meaningful interpretation of the relevance of each step observation for the final decision. We demonstrated the generalization of our approach to three very different data sets and sequence classification tasks. As a future work, our model could be expanded to help summarize documents or videos."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["D. Bahdanau", "K. Cho", "Y. Bengio"], "venue": "ICLR", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2015}, {"title": "Advances in optimizing recurrent networks", "author": ["Y. Bengio", "N. Boulanger-Lewandowski", "R. Pascanu"], "venue": "ICASSP, pages 8624\u20138628. IEEE", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2013}, {"title": "Mind\u2019s eye: A recurrent visual representation for image caption generation", "author": ["X. Chen", "C.L. Zitnick"], "venue": "CVPR, pages 2422\u20132431. IEEE Computer Society", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2015}, {"title": "B", "author": ["K. Cho"], "venue": "van Merrienboer, D. Bahdanau, and Y. Bengio. On the properties of neural machine translation: Encoderdecoder approaches. In Proceedings of SSST@EMNLP 2014, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 103\u2013111", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2014}, {"title": "Neural conditional random fields", "author": ["T.-M.-T. Do", "T. Artieres"], "venue": "In Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2010}, {"title": "From captions to visual concepts and back", "author": ["H. Fang", "S. Gupta", "F.N. Iandola", "R.K. Srivastava", "L. Deng", "P. Dollr", "J. Gao", "X. He", "M. Mitchell", "J.C. Platt", "C.L. Zitnick", "G. Zweig"], "venue": "CVPR, pages 1473\u20131482. IEEE Computer Society", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2015}, {"title": "Generating sequences with recurrent neural networks", "author": ["A. Graves"], "venue": "CoRR, abs/1308.0850", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2013}, {"title": "Connectionist temporal classification: Labelling unsegmented sequence data with recurrent neural networks", "author": ["A. Graves", "S. Fernndez", "F. Gomez"], "venue": "In Proceedings of the International Conference on Machine Learning, ICML 2006, pages 369\u2013376", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2006}, {"title": "Improved tree model for Arabic speech recognition", "author": ["N. Hammami", "M. Bedda"], "venue": "Int. Conf. on Computer Science and Information Technology, pages 521\u2013526", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2010}, {"title": "Recurrent attention models for depth-based person identification", "author": ["A. Haque", "A. Alahi", "L. Fei-Fei"], "venue": "In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2016}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural Comput.,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1997}, {"title": "Deep recursive neural networks for compositionality in language", "author": ["O. \u0130rsoy", "C. Cardie"], "venue": "Z. Ghahramani, M. Welling, C. Cortes, N. Lawrence, and K. Weinberger, editors, Advances in Neural Information Processing Systems 27, pages 2096\u20132104. Curran Associates, Inc.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep unordered composition rivals syntactic methods for text classification", "author": ["M. Iyyer", "V. Manjunatha", "J. Boyd-Graber", "H. Daum\u00e9 III"], "venue": "Association for Computational Linguistics", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "A discriminative framework for detecting remot protein homologies", "author": ["T. Jaakkola", "M. Diekhans", "D. Haussler"], "venue": "Journal of Computational Biology, 7(1-2):95\u2013114", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2000}, {"title": "Super fast event recognition in internet videos", "author": ["Y.-G. Jiang", "Q. Dai", "T. Mei", "Y. Rui", "S.-F. Chang"], "venue": "IEEE Transactions on Multimedia, 17(8):1\u201313", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2015}, {"title": "Consumer video understanding: A benchmark database and  an evaluation of human and machine performance", "author": ["Y.-G. Jiang", "G. Ye", "S.-F. Chang", "D. Ellis", "A.C. Loui"], "venue": "Proceedings of ACM International Conference on Multimedia Retrieval (ICMR)", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2011}, {"title": "Large-scale video classification with convolutional neural networks", "author": ["A. Karpathy", "G. Toderici", "S. Shetty", "T. Leung", "R. Sukthankar", "L. Fei-Fei"], "venue": "CVPR", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2014}, {"title": "Convolutional neural networks for sentence classification", "author": ["Y. Kim"], "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1746\u20131751. Association for Computational Linguistics", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2014}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2012}, {"title": "Conditional random fields: Probabilistic models for segmenting and labeling sequence data", "author": ["J.D. Lafferty", "A. McCallum", "F.C.N. Pereira"], "venue": "Proceedings of the Eighteenth International Conference on Machine Learning, pages 282\u2013 289", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2001}, {"title": "Distributed representations of sentences and documents", "author": ["Q. Le", "T. Mikolov"], "venue": "T. Jebara and E. P. Xing, editors, Proceedings of the 31st International Conference on Machine Learning (ICML-14), pages 1188\u20131196. JMLR Workshop and Conference Proceedings", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2014}, {"title": "Effective approaches to attention-based neural machine translation", "author": ["M.-T. Luong", "H. Pham", "C.D. Manning"], "venue": "EMNLP", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2015}, {"title": "Extensions of recurrent neural network language model", "author": ["T. Mikolov", "S. Kombrink", "L. Burget", "J. Cernock", "S. Khudanpur"], "venue": "ICASSP, pages 5528\u20135531. IEEE", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2011}, {"title": "Rectified linear units improve restricted boltzmann machines", "author": ["V. Nair", "G.E. Hinton"], "venue": "J. Frnkranz and T. Joachims, editors, Proceedings of the 27th International Conference on Machine Learning (ICML-10), pages 807\u2013814. Omnipress", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2010}, {"title": "A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts", "author": ["B. Pang", "L. Lee"], "venue": "Proceedings of the 42Nd Annual Meeting on Association for Computational Linguistics, ACL \u201904, Stroudsburg, PA, USA", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2004}, {"title": "and L", "author": ["W. Pei", "H. Dibeklio\u01e7lu", "D.M.J. Tax"], "venue": "van der Maaten. Multivariate time-series classification using the hidden-unit logistic model. IEEE Transactions on Neural Networks and Learning Systems", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2017}, {"title": "Conditional neural fields", "author": ["J. Peng", "L. Bo", "J. Xu"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2009}, {"title": "Glove: Global vectors for word representation", "author": ["J. Pennington", "R. Socher", "C.D. Manning"], "venue": "Empirical Methods in Natural Language Processing (EMNLP), pages 1532\u20131543", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2014}, {"title": "Latent-dynamic discriminative models for continuous gesture recognition", "author": ["A. Quattoni", "T. Darrell"], "venue": "Proceedings of CVPR07, pages 1\u20138", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2007}, {"title": "Hidden conditional random fields", "author": ["A. Quattoni", "S. Wang", "L.-P. Morency", "M. Collins", "T. Darrell"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell.,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2007}, {"title": "Readings in speech recognition", "author": ["L.R. Rabiner"], "venue": "chapter A Tutorial on Hidden Markov Models and Selected Applications in Speech Recognition, pages 267\u2013296. Morgan Kaufmann Publishers Inc., San Francisco, CA, USA", "citeRegEx": "31", "shortCiteRegEx": null, "year": 1990}, {"title": "Temporal attention model for neural machine translation", "author": ["B. Sankaran", "H. Mi", "Y. Al-Onaizan", "A. Ittycheriah"], "venue": "CoRR, abs/1608.02927", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2016}, {"title": "A local learning algorithm for dynamic feedforward and recurrent networks", "author": ["J. SCHMIDHUBER"], "venue": "Connection Science, 1(4):403\u2013412", "citeRegEx": "33", "shortCiteRegEx": null, "year": 1989}, {"title": "Action recognition using visual attention", "author": ["S. Sharma", "R. Kiros", "R. Salakhutdinov"], "venue": "CoRR, abs/1511.04119", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2015}, {"title": "Semi-supervised recursive autoencoders for predicting sentiment distributions", "author": ["R. Socher", "J. Pennington", "E.H. Huang", "A.Y. Ng", "C.D. Manning"], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP \u201911, pages 151\u2013161, Stroudsburg, PA, USA", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2011}, {"title": "Recursive deep models for semantic compositionality over a sentiment treebank", "author": ["R. Socher", "A. Perelygin", "J. Wu", "J. Chuang", "C.D. Manning", "A.Y. Ng", "C. Potts"], "venue": "In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2013}, {"title": "Improved semantic representations from tree-structured long short-term memory networks", "author": ["K.S. Tai", "R. Socher", "C.D. Manning"], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing, ACL 2015, July 26-31, 2015, Beijing, China, Volume 1: Long Papers, pages 1556\u20131566. The Association for Computer Linguistics", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2015}, {"title": "Generative image modeling using spatial lstms", "author": ["L. Theis", "M. Bethge"], "venue": "Proceedings of the 28th International Conference on Neural Information Processing Systems, NIPS\u201915, pages 1927\u20131935, Cambridge, MA, USA", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2015}, {"title": "Hidden-unit conditional random fields", "author": ["L. van der Maaten", "M. Welling", "L.K. Saul"], "venue": "AISTATS, volume 15 of JMLR Proceedings,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2011}, {"title": "Generalization of backpropagation with application to a recurrent gas market model", "author": ["P.J. Werbos"], "venue": "Neural Networks, 1", "citeRegEx": "40", "shortCiteRegEx": null, "year": 1988}, {"title": "Show", "author": ["K. Xu", "J. Ba", "R. Kiros", "K. Cho", "A. Courville", "R. Salakhudinov", "R. Zemel", "Y. Bengio"], "venue": "attend and tell: Neural image caption generation with visual attention. In D. Blei and F. Bach, editors, Proceedings of the 32nd International Conference on Machine Learning (ICML-15), pages 2048\u2013 2057. JMLR Workshop and Conference Proceedings", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2015}, {"title": "Describing videos by exploiting temporal structure", "author": ["L. Yao", "A. Torabi", "K. Cho", "N. Ballas", "C. Pal", "H. Larochelle", "A. Courville"], "venue": "Computer Vision (ICCV), 2015 IEEE International Conference on. IEEE", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 16, "context": "Sequence classification models have extensive applications ranging from computer vision [17] to natural language processing [1].", "startOffset": 88, "endOffset": 92}, {"referenceID": 0, "context": "Sequence classification models have extensive applications ranging from computer vision [17] to natural language processing [1].", "startOffset": 124, "endOffset": 127}, {"referenceID": 3, "context": "A popular approach for sequence classification is gated recurrent networks like Gated Recurrent Units (GRU) [4] and Long Short-Term Memory (LSTM) [11].", "startOffset": 108, "endOffset": 111}, {"referenceID": 10, "context": "A popular approach for sequence classification is gated recurrent networks like Gated Recurrent Units (GRU) [4] and Long Short-Term Memory (LSTM) [11].", "startOffset": 146, "endOffset": 150}, {"referenceID": 30, "context": "These methods are typically based on the Hidden Markov Models (HMMs) [31].", "startOffset": 69, "endOffset": 73}, {"referenceID": 13, "context": "HMM can also be used as the base model for Fisher Kernel [14] to learn a sequence representation.", "startOffset": 57, "endOffset": 61}, {"referenceID": 19, "context": "Conditional random fields (CRF) [20] are discriminative models for sequence labeling which aims to assign one label for each sequence observation.", "startOffset": 32, "endOffset": 36}, {"referenceID": 28, "context": ", latent-dynamic CRFs [29], conditional neural fields [27], neural conditional random fields [5] and hidden-unit CRF model [39]).", "startOffset": 22, "endOffset": 26}, {"referenceID": 26, "context": ", latent-dynamic CRFs [29], conditional neural fields [27], neural conditional random fields [5] and hidden-unit CRF model [39]).", "startOffset": 54, "endOffset": 58}, {"referenceID": 4, "context": ", latent-dynamic CRFs [29], conditional neural fields [27], neural conditional random fields [5] and hidden-unit CRF model [39]).", "startOffset": 93, "endOffset": 96}, {"referenceID": 38, "context": ", latent-dynamic CRFs [29], conditional neural fields [27], neural conditional random fields [5] and hidden-unit CRF model [39]).", "startOffset": 123, "endOffset": 127}, {"referenceID": 29, "context": "Hidden-state CRF (HCRF) [30] employs a chain of k-nomial latent variables to model the latent structure and has been successfully used in the sequence labeling.", "startOffset": 24, "endOffset": 28}, {"referenceID": 25, "context": "Similarly, hidden unit logistic model (HULM) [26] utilizes binary stochastic hidden units to represent the exponential hidden states so as to model more complex latent decision boundaries.", "startOffset": 45, "endOffset": 49}, {"referenceID": 40, "context": "Doing so can not only improve the performance of the model but can also result in better interpretability [41].", "startOffset": 106, "endOffset": 110}, {"referenceID": 40, "context": "Attention models have been applied to image and video captioning [41, 3, 6, 42], machine translation [1, 22, 32], depthbased person identification [10] and speech recognition [8].", "startOffset": 65, "endOffset": 79}, {"referenceID": 2, "context": "Attention models have been applied to image and video captioning [41, 3, 6, 42], machine translation [1, 22, 32], depthbased person identification [10] and speech recognition [8].", "startOffset": 65, "endOffset": 79}, {"referenceID": 5, "context": "Attention models have been applied to image and video captioning [41, 3, 6, 42], machine translation [1, 22, 32], depthbased person identification [10] and speech recognition [8].", "startOffset": 65, "endOffset": 79}, {"referenceID": 41, "context": "Attention models have been applied to image and video captioning [41, 3, 6, 42], machine translation [1, 22, 32], depthbased person identification [10] and speech recognition [8].", "startOffset": 65, "endOffset": 79}, {"referenceID": 0, "context": "Attention models have been applied to image and video captioning [41, 3, 6, 42], machine translation [1, 22, 32], depthbased person identification [10] and speech recognition [8].", "startOffset": 101, "endOffset": 112}, {"referenceID": 21, "context": "Attention models have been applied to image and video captioning [41, 3, 6, 42], machine translation [1, 22, 32], depthbased person identification [10] and speech recognition [8].", "startOffset": 101, "endOffset": 112}, {"referenceID": 31, "context": "Attention models have been applied to image and video captioning [41, 3, 6, 42], machine translation [1, 22, 32], depthbased person identification [10] and speech recognition [8].", "startOffset": 101, "endOffset": 112}, {"referenceID": 9, "context": "Attention models have been applied to image and video captioning [41, 3, 6, 42], machine translation [1, 22, 32], depthbased person identification [10] and speech recognition [8].", "startOffset": 147, "endOffset": 151}, {"referenceID": 7, "context": "Attention models have been applied to image and video captioning [41, 3, 6, 42], machine translation [1, 22, 32], depthbased person identification [10] and speech recognition [8].", "startOffset": 175, "endOffset": 178}, {"referenceID": 33, "context": "[34].", "startOffset": 0, "endOffset": 4}, {"referenceID": 32, "context": "Recurrent Neural Networks (RNN) learn a representation for each time step by taking into account both the observation at current time step and the representation in the previous one [33].", "startOffset": 182, "endOffset": 186}, {"referenceID": 22, "context": "Recurrent networks have been successfully applied to various tasks including language modeling [23], image generation [38] and online handwriting generation [7].", "startOffset": 95, "endOffset": 99}, {"referenceID": 37, "context": "Recurrent networks have been successfully applied to various tasks including language modeling [23], image generation [38] and online handwriting generation [7].", "startOffset": 118, "endOffset": 122}, {"referenceID": 6, "context": "Recurrent networks have been successfully applied to various tasks including language modeling [23], image generation [38] and online handwriting generation [7].", "startOffset": 157, "endOffset": 160}, {"referenceID": 10, "context": "To address the gradient vanishing problem of plain-RNN when dealing with long sequences, LSTM [11] and GRU [4] were proposed.", "startOffset": 94, "endOffset": 98}, {"referenceID": 3, "context": "To address the gradient vanishing problem of plain-RNN when dealing with long sequences, LSTM [11] and GRU [4] were proposed.", "startOffset": 107, "endOffset": 110}, {"referenceID": 23, "context": "We use the rectified linear unit (ReLU)[24] as the activation function g.", "startOffset": 39, "endOffset": 43}, {"referenceID": 0, "context": "lie between [0, 1].", "startOffset": 12, "endOffset": 18}, {"referenceID": 39, "context": "The loss is back-propagated through top recurrent attention-gated units and temporal attention module successively using backpropagation through time algorithm [40].", "startOffset": 160, "endOffset": 164}, {"referenceID": 1, "context": "We employ RMSprop as the gradient descent optimization algorithm with gradient clipping between \u22125 and 5 [2].", "startOffset": 105, "endOffset": 108}, {"referenceID": 8, "context": "We first conduct preliminary experiments on a modified dataset constructed from the Arabic spoken digit dataset [9] to (1) evaluate the effectiveness of the two main modules of TAGM; (2) compare the generalizability of three different gate-setup recurrent models (TAGM, GRU and LSTM) with the varying size of the training data.", "startOffset": 112, "endOffset": 115}, {"referenceID": 8, "context": "We use the same data division as Hammami and Bedda [9]: 6600 samples as training set and 2200 samples as test set.", "startOffset": 51, "endOffset": 54}, {"referenceID": 19, "context": "HCRF and HULM are both extensions of CRF [20] by inserting hidden layers to model the non-linear latent structure in the data.", "startOffset": 41, "endOffset": 45}, {"referenceID": 25, "context": "Model #Hidden units #Parameters Accuracy HULM\u2217 [26] \u2212 \u2212 95.", "startOffset": 47, "endOffset": 51}, {"referenceID": 25, "context": "32 HCRF\u2217 [26] \u2212 \u2212 96.", "startOffset": 9, "endOffset": 13}, {"referenceID": 24, "context": "Sentiment analysis is a popular research topic in the field of natural language processing (NLP) which aims to identify the viewpoint(s) underlying a text span [25].", "startOffset": 160, "endOffset": 164}, {"referenceID": 35, "context": "The Stanford Sentiment Treebank (SST) [36] is a data corpus of movie review excerpts.", "startOffset": 38, "endOffset": 42}, {"referenceID": 35, "context": "Following previous work [36], we utilize 300-d Glove word vectors (300 dimensions) pretrained over the Common Crawl [28] as the features for each word of the sentences.", "startOffset": 24, "endOffset": 28}, {"referenceID": 27, "context": "Following previous work [36], we utilize 300-d Glove word vectors (300 dimensions) pretrained over the Common Crawl [28] as the features for each word of the sentences.", "startOffset": 116, "endOffset": 120}, {"referenceID": 12, "context": "Syntactic compositions DAN-ROOT [13] 85.", "startOffset": 32, "endOffset": 36}, {"referenceID": 12, "context": "compositions NBOW-RAND [13] 81.", "startOffset": 23, "endOffset": 27}, {"referenceID": 12, "context": "NBOW [13] 83.", "startOffset": 5, "endOffset": 9}, {"referenceID": 12, "context": "BiNB [13] 83.", "startOffset": 5, "endOffset": 9}, {"referenceID": 34, "context": "compositions RecNN [35] 82.", "startOffset": 19, "endOffset": 23}, {"referenceID": 35, "context": "RecNTN [36] 85.", "startOffset": 7, "endOffset": 11}, {"referenceID": 11, "context": "DRecNN [12] 86.", "startOffset": 7, "endOffset": 11}, {"referenceID": 12, "context": "DAN [13] 86.", "startOffset": 4, "endOffset": 8}, {"referenceID": 36, "context": "TreeLSTM [37] 86.", "startOffset": 9, "endOffset": 13}, {"referenceID": 17, "context": "CNN-MC [18] 88.", "startOffset": 7, "endOffset": 11}, {"referenceID": 20, "context": "PVEC [21] 87.", "startOffset": 5, "endOffset": 9}, {"referenceID": 35, "context": "[36]: 8544/1101/2210 samples are used for training, validation and testing respectively in the 5-class task.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "Columbia Consumer Video (CCV) Database [16] is an unconstrained video database collected from YouTube videos without any post-editing.", "startOffset": 39, "endOffset": 43}, {"referenceID": 15, "context": "2 Experimental Setup Following Jiang et al [16], we use the same split for training and test sets: 4659 videos as the training set and 4658 as the test set.", "startOffset": 43, "endOffset": 47}, {"referenceID": 14, "context": "We compare our model with the baseline method [15] on this dataset, which performs classification separately with Support Vector Machine (SVM) models trained on the bag-of-words representations for several popular features separately and then combines the results using late fusion.", "startOffset": 46, "endOffset": 50}, {"referenceID": 18, "context": ", the outputs (4,096 dimensions) of the seventh fully-connected layer of a pre-trained AlexNet model [19].", "startOffset": 101, "endOffset": 105}, {"referenceID": 15, "context": "We adopt mean Average Precision (mAP) as the evaluation metric, which is typically used for CCV dataset [16, 15].", "startOffset": 104, "endOffset": 112}, {"referenceID": 14, "context": "We adopt mean Average Precision (mAP) as the evaluation metric, which is typically used for CCV dataset [16, 15].", "startOffset": 104, "endOffset": 112}, {"referenceID": 14, "context": "We compare our model with the event recognition system proposed by dataset authors [15].", "startOffset": 83, "endOffset": 87}], "year": 2017, "abstractText": "Typical techniques for sequence classification are designed for well-segmented sequences which have been edited to remove noisy or irrelevant parts. Therefore, such methods cannot be easily applied on noisy sequences expected in real-world applications. In this paper, we present the Temporal Attention-Gated Model (TAGM) which integrates ideas from attention models and gated recurrent networks to better deal with noisy or unsegmented sequences. Specifically, we extend the concept of attention model to measure the relevance of each observation (time step) of a sequence. We then use a novel gated recurrent network to learn the hidden representation for the final prediction. An important advantage of our approach is interpretability since the temporal attention weights provide a meaningful value for the salience of each time step in the sequence. We demonstrate the merits of our TAGM approach, both for prediction accuracy and interpretability, on three different tasks: spoken digit recognition, text-based sentiment analysis and visual event recognition.", "creator": "LaTeX with hyperref package"}}}