{"id": "1612.00554", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Dec-2016", "title": "Higher Order Mutual Information Approximation for Feature Selection", "abstract": "Feature selection is a process of choosing a subset of relevant features so that the quality of prediction models can be improved. An extensive body of work exists on information-theoretic feature selection, based on maximizing Mutual Information (MI) between subsets of features and class labels. The prior methods use a lower order approximation, by treating the joint entropy as a summation of several single variable entropies. This leads to locally optimal selections and misses multi-way feature combinations. We present a higher order MI based approximation technique called Higher Order Feature Selection (HOFS). Instead of producing a single list of features, our method produces a ranked collection of feature subsets that maximizes MI, giving better comprehension (feature ranking) as to which features work best together when selected, due to their underlying interdependent structure. Our experiments demonstrate that the proposed method performs better than existing feature selection approaches while keeping similar running times and computational complexity.", "histories": [["v1", "Fri, 2 Dec 2016 03:34:44 GMT  (219kb,D)", "http://arxiv.org/abs/1612.00554v1", "14 page, 5 figures"]], "COMMENTS": "14 page, 5 figures", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["jilin wu", "soumyajit gupta", "chandrajit bajaj"], "accepted": false, "id": "1612.00554"}, "pdf": {"name": "1612.00554.pdf", "metadata": {"source": "CRF", "title": "Higher Order Mutual Information Approximation for Feature Selection", "authors": ["Jialin Wu", "Soumyajit Gupta", "Chandrajit Bajaj"], "emails": ["wujl13@mails.tsinghua.edu.cn", "smjtgupta@utexas.edu", "bajaj@cs.utexas.edu"], "sections": [{"heading": "1. Introduction", "text": "The question which arises is whether it is a matter of a manner and manner, in which it is a matter of a manner and manner, in which it is a matter of the dissemination of information, which is concerned in the manner and manner, as it is concerned in the manner and manner, as it is concerned in the manner and manner, as it is concerned in the manner and manner, as it is concerned in the manner and manner, as it is concerned in the manner and manner, as it is concerned in the manner and manner, as it is concerned in the manner and manner, as it is concerned in the language, in the language and in the language, as it is concerned in the language and in the language, as it is concerned in the language and in the language, as it is concerned in the language and in the language, as it is concerned in the language and in the language, as it is concerned in the language and in the language, as it is concerned in the language and in the language, as it is in the language and in the language, as it is in the language and in the language, as it is in the language and in the language, as it is in the language and in the language, as it is in the language and in the language, as it is in the language and in the language, as it is in the language and in the language and in the language, as it is in the language and in the language, as it is in the language and in the language and in the language, as it is in the language and in the language, as it is in the language and in the language and in the language and in the language, as it is in the language and in the language and in the language, as it is in the language and in the language and in the language and in the language, as it is in the language and in the language and in the language, as it is in the language in the language and in the language and in the language, as it is in the language in the language and in the language and in the language, as it is in the language in the language in the language in the language in the language and in the language and in the language, and in the language in the language in the language, and in the language in the language in the language, and in the language in the language in the language in the language of the language, and in the language in the language in the language of the"}, {"heading": "2. Feature Selection Background", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1. Notation", "text": "Let x specify a possible value of the discrete random variable (r.v.) X drawn from the alphabet set X. Also, let p (X) specify the probability density function (pdf) of X, and P (X = x) specify the probability that r.v. X takes the value x. If X is discrete, the probability can be estimated as a fraction of the observations p (x) = # xN, assuming the value x from the total set N. An event x x x is specified as a drawing of a value from the set. Let M also be the total number of characters in X, where Xi (i) [1: M] is the current character to be considered for selection, y specify its label, and vice versa specify the sparse set of selected characters with elements xft for t [1, T], where | 1 | = T specifies its cardinality."}, {"heading": "2.2. Definitions", "text": "Entropy and mutual information are two well-known concepts in information theory that are used to measure the information provided by random variables. Entropy H (X): It is a measure of the uncertainty of the random variables X, or the average amount of information captured with each event. The safer X is, the higher the value of H (X, Y): It is the measure of the uncertainty of a joint variable consisting of two random variables X and Y. The common entropy of a set of variables is greater or equal to all individual entropies of the variables in Set.H (X, Y): It is the measure of the uncertainty of a joint variable consisting of two random variables X and Y."}, {"heading": "2.3. Prior Work", "text": "In this context, it should be noted that the measures referred to by the Commission are measures that relate to the achievement of objectives and objectives. (...) The measures proposed by the Commission to reduce imbalances between countries (...) are taken by individual countries. (...) The measures proposed by the Commission to reduce imbalances between countries (...) are not implemented by individual countries. (...) The measures proposed by the EU to reduce imbalances (...) are not implemented by individual countries. (...) The measures proposed by the EU to reduce imbalances (...) are not implemented by individual countries. (...) The measures proposed by the EU to reduce imbalances between countries (...) are not implemented. (...) The measures proposed by the EU to reduce imbalances (...) are not implemented by individual countries (...). (...) The measures proposed by the EU to reduce imbalances between countries (...) are not implemented. (...)"}, {"heading": "3. MI based Feature Selection", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1. Forward Heuristic", "text": "Consider a monitored learning scenario in which x = {x1, x2,..., xM} is an M-dimensional input characteristic vector and y is the output marker. In filtering methods, the task of selecting information-based characteristics is to select T characteristics in such a way that mutual information between xB and y is maximized. Formally, most MI-based feature selection methods use a greedy search strategy that selects the characteristics step by step. ft = arg max i {I (\u00b7) denotes mutual information. Direct optimization of Eq. 5 is a NP-hard combinatorial problem [?]. Therefore, most MI-based feature selection methods use a greedy search strategy in which the characteristics are selected steply. ft = arg max i {I (\u00b7) denotes mutual information. Direct optimization of equation is a NP-hard combinatorial problem."}, {"heading": "3.2. Lower Order Approximation", "text": "Most common methods therefore propose a relevant term (features that contain information that is already contained in other features) and a redundant term (features that do not contain useful information relating to the class variable), assuming trait independence and class-related independence (e.g. Eq.8 and 9).H (Eq.8 and 9).H (Eq.9) Gao et al. [8] discusses the inconsistency of these two assumptions. The assumption of independence causes multi-path feature combinations to be overlooked because the interdependence between features is ignored. Consider a scenario in which a lower-order feature selection algorithm based on MI would fail. For example, consider three random variables x1 x 2, where Y = 3 x y (i.e. 1 = 1 x 2) and Y = 1 x 2 (Eq.1 = 1 x 2)."}, {"heading": "3.3. Independent Component Analysis", "text": "The starting point for ICA is the very simple assumption that input variables xi as a linear mixture of N independent components s1, s2,.., sN.xi = T \u2211 j = 1 \u03b1ijsj i = 1, 2,.., TWhen we write in a compact version, we have X = AS or S = WX, where W = A \u2212 1 Based on the estimation of the maximum probability, the ICA algorithm constructs a separation matrix W that provides the best matching of the input data. In this context, both W and S are unknown, and we further assume that the cumulative distribution function of the signal variables (cdf) matches a sigmoid function. g (s) = 11 + e \u2212 sThus, the separation matrix W (\u03b1) is defined as a consequence in Equation 10, and the protocol probability function can be decreased by stostic gradients using Wq = 11 + s\u2212 so the demixing matrix W = 1 (W) (T = 1)."}, {"heading": "4. Proposed Method: Higher Order Feature Selection", "text": "We are now considering finding interdependent trait combinations that would be avoided by introducing a lower MI approximation, which would together solve the problem of trait selection and trait ranking 1."}, {"heading": "4.1. Motivation", "text": "The problem of convergence of common mutual information (MI) as the summation of individual variables is limited by the interdependence between selected characteristics, which motivates us to determine, during the greedy search process (Eq.6) K, independent subsets of characteristics in which characteristics within a subset may be dependent on each other. Second, we determine the independent representation of K for each of the independent subsets of characteristics and then reconstruct H (X\u0435i) with H (S\u0441i). Potential problems of this framework lie in the accuracy of the restoration of H (X\u0441i) using H (S\u0441i) and the computational costs of the ICA algorithm. We solve both by the following mutual equilibrium and incremental ICA algorithms."}, {"heading": "4.2. Subset Independence", "text": "Instead of assuming the independence of individual features and class-related independence, we assume that the optimally selected feature subset can be subdivided into several feature subsets that are independent and class conditioned from each other (Eqs. 13 and 14). The rationality of the subset assumes that in most cases the label y can be better approximated as a function of X\u0102i, e.g.: y = f (Eq.) = f (X\u01021, X\u01022,,, X\u0102K) (12) 1Our code will soon be available online at https: / / cvcweb.ices.utexas.edu / cvcwp / software / where K specifies the number of selected subsets. Let's assume E = K i = 1 X\u0102i, with each X\u0430i subset denoting a feature subset and xt the next feature to be selected. Assume 1. P (X) = K (X) = 1 (1) = 13 (Assumption2) P."}, {"heading": "4.3. Entropy relation between Inputs and Signals", "text": "The common entropy of the feature combinations is calculated using Eq. 15H (XB) = q (XB) = j (j) = j (j) = p (XB) = p (XB) (15) However, the common pdf of Xi using finite samples is difficult to estimate as the size of the subset increases. It should be noted that each feature vector in SB (i) is independent of each other. Therefore, H (XB) \u2212 \u2212 si can be calculated using H (SB), which is a sum of H (sJ), which is the sum of H (sJ), where SJ (SB) is between SB (SB) and S (SB)."}, {"heading": "4.4. Forward Search", "text": "Based on the assumption 1 and 2 (Gl. 13 and Gl. 14), the forward heuristic search function Gg. 7: ft = arg max i / i (xi: y) + H (Gl. | xi) \u2212 H (Gl. | xi, y) = arg max i / i (xi: y) + K = 1 (H (Gl.: j | xi) \u2212 H (Gl. | xi, y))) (20) The calculation H (Gl. \u2212 1j | xi) and H (Gl. \u2212 1j | xi, y) is still a NP-hard problem and finite, discrete samples of input vectors X worsen the approximation of the link pdf. To solve this problem, we first transform them into non-conditional terms such as Gl. 21, 22. By assuming some general cumulative distribution functions of signal vectors si, we calculate these two terms in linear computational complexity with Gl."}, {"heading": "4.5. Inaccuracy Removal and Incremental ICA", "text": "The transformation from the input vectors Xxi i to the independent signal vectors Xxi i becomes a continuous distribution. (Finite discrete samples lead to inaccuracy.H (Xdiscrete) - H (Sdiscrete) - log (| W |) (23) Note that we are trying to find a mixture of independent signals si to represent xi. (Finite discrete samples lead to a balanced algorithm to reduce noise. (Finite discrete samples we try to find (Finite discrete samples). (Finite discrete samples result in H (Finite \u2212 1) - H (Finite \u2212 1 \u2212 1 \u2212 xi \u2212 1 \u2212 1, y) in Eq. (Finite search for a subtraction of two multivariable entropies we are motivated to design a balanced algorithm to reduce noise.) The calculation costs for implementing an algorithm A are also not ICbable."}, {"heading": "4.6. Time Complexity", "text": "A detailed step-by-step description of the HOFS algorithm can be found in Alg.???. Since the predictive heuristic search calculates the information gain between each unselected node and the selected node in each step, the total computational complexity for entropy O (MNT), where N is the total number of samples, M is the total number of characteristics and T the number of characteristics to be selected. The complexity of the ICA algorithm concerns the calculation of the gradients of the matrix W\u043ei (Eq. 11). Since W\u043ei is a lower triangular matrix and we only need the gradients of the elements in the last line, the reverse calculation can be done in O (MT 2K). Algorithm 1 Higher Order Feature Selection (HOFS) 1: Data: (Xi, yi), i = 1, 2,.., M 2: Input: T digit of the characteristics to be selected."}, {"heading": "4.7. Feature Subset Determination", "text": "We strive to find independent combinations of characteristics, so it is of the utmost importance to assess whether a new subset should be created for incoming characteristics or whether it should be inserted into an existing subset. Specifically, we calculate the maximum average correlation between each subset of XVeni. Argcovi = 1 | XVeni | u VVenVenVenVenVenVenVenVenVenVenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenvenven"}, {"heading": "5. Experiments", "text": "We conduct experiments by analyzing the quality of the incremental ICA algorithm and then compare our proposed HOFS approach with other popular MI-based, lower order feature selection algorithms on some publicly available standard datasets."}, {"heading": "5.1. Incremental ICA Quality", "text": "We test the feasibility of our incremental ICA algorithm using two measurements (Table 1): First, we calculate Pearson's average product-moment coefficient between the signal vectors according to ICA to show that by maximizing equivalent we obtain 10 uncorrelated vectors, ensuring that equivalent 16 is true. Low values (\u0445 0.055) show on average that the vectors in the ICA space are highly uncorrelated and thus almost independent of each other. Second, we calculate the average equilibrium ratio: Rbalance = 1K \u2211 i = 1 H (X\u03c6i) \u2212 H (X\u03c6i, y) H (siy) + log (a i Ti + 2, Ti + 2), where X = 1, 2, 3,... K is the spaced feature combination chosen, siy is the incremental signal vector for y in subset X\u03c6i. With Rbalance close to 1, we show that the error of H (X) is offset by H (subtraction)."}, {"heading": "5.2. Real-World Data", "text": "We compare our algorithm with other popular information theory methods for selecting characteristics, including VMI, mRMR, JMI, CMIM, and SPECCMI. We use 9 known datasets that are commonly used in feature selection studies. They were selected to have a wide variety of characteristics and multi-class problems (Table 1). We use the average error rate for cross-validation in the range of 10 to 100 (total number of characteristics if they are less than 100) characteristics to compare different algorithms under the same setting. For datasets with the number of samples N \u2265 100, a 10-fold cross-validation is applied, and otherwise cross-validation. The classifier is chosen to be linear SVM. We process data that follow the approach proposed in VMI [8]. Figure 2 shows the cross-validation error in Semeion datasets, with our method showing the lowest validation error based on the characteristics selected."}, {"heading": "6. Synthetic Model Experiment", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6.1. Inference from Graphical Model", "text": "We conduct experiments on a synthetic model (fig. 3) according to the tree structure. The root node Y in fig. 3a is a binary variable specifying the class name, while other variables xi are continuous Gaussian type, with the variance of the unit and the mean fixed to the value of its parent. We generate 100,000 samples from the model and compare the results of HOFS with VMI in table 5. The covariance structure between the 9 characteristics (fig. 3b) confirms the relationship generated by the tree graphic model. Although both methods select the same characteristics, HOFS selects them in the correct order and maintains the correct subsets."}, {"heading": "6.2. Handling Non-Numerical Features", "text": "In order to verify the effectiveness of the proposed HOFS for a non-numerical feature selection, 1000 samples with 20 heterogeneous features and 5 classes according to [21] were synthesized. There were 4 groups of features and each group had 5 features of both numerical and non-numerical features, which can be described in Table 7. The class name can be described by the feature group. In groups I, F1, F2, F3 are non-numerical features that can explain class 1 - class 3. F4, F5 are numerical features that can explain class 4 and class 5. Here, F1 is the most prominent feature for classifying the class name and there are redundancies within this group. Ideally, the selected feature subset {F1, F3, F5} or {F1, F4, F5} should be able to explain the negative features."}, {"heading": "7. Conclusion", "text": "Mutual Information (MI) defines a measurement of how informative the features are. Feature selection based on MI has evolved a lot over the past decade. However, calculating the global MI is a NP-hard problem, so most of these algorithms are forced to make a lower order estimate. We are introducing an improved method (HOFS) to estimate the global MI by integrating incremental ICA algorithms. Our method performs clearly better than most others due to its ability to select feature subsets that maximize the MI collectively while maintaining similar runtimes and computational complexity to current approaches. We want to expand our work to provide minimal cardinality features for a wide range of data sets, including geometric and hyper-spectral features."}], "references": [{"title": "Using mutual information for selecting features in supervised neural net learning", "author": ["R. Battiti"], "venue": "IEEE Transactions on neural networks, 5(4):537\u2013550,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1994}, {"title": "Application of high-dimensional feature selection: evaluation for genomic prediction in man", "author": ["M.L. Bermingham", "R. Pong-Wong", "A. Spiliopoulou", "C. Hayward", "I. Rudan", "H. Campbell", "A.F. Wright", "J.F. Wilson", "F. Agakov", "P. Navarro"], "venue": "Scientific reports,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2015}, {"title": "Conditional likelihood maximisation: a unifying framework for information theoretic feature selection", "author": ["G. Brown", "A. Pocock", "M.-J. Zhao", "M. Luj\u00e1n"], "venue": "Journal of Machine Learning Research, 13(Jan):27\u201366,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2012}, {"title": "Independent component analysis, a new concept", "author": ["P. Comon"], "venue": "Signal processing,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1994}, {"title": "Minimum redundancy feature selection from microarray gene expression data", "author": ["C. Ding", "H. Peng"], "venue": "Journal of bioinformatics and computational biology, 3(02):185\u2013205,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2005}, {"title": "Feature extraction: Foundations and applications", "author": ["W. Duch"], "venue": "Studies in Fuzziness and Soft Computing, chapter 3, pages 89\u2013117. Springer,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2006}, {"title": "Fast binary feature selection with conditional mutual information", "author": ["F. Fleuret"], "venue": "Journal of Machine Learning Research, 5(Nov):1531\u20131555,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2004}, {"title": "Variational information maximization for feature selection", "author": ["S. Gao", "G.V. Steeg", "A. Galstyan"], "venue": "arXiv preprint arXiv:1606.02827,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2016}, {"title": "An introduction to variable and feature selection", "author": ["I. Guyon", "A. Elisseeff"], "venue": "Journal of machine learning research, 3(Mar):1157\u20131182,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2003}, {"title": "Correlation-based feature selection of discrete and numeric class machine learning", "author": ["M.A. Hall"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2000}, {"title": "A practical approach to feature selection", "author": ["K. Kira", "L.A. Rendell"], "venue": "Proceedings of the ninth international workshop on Machine learning, pages 249\u2013256,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1992}, {"title": "Wrappers for feature subset selection", "author": ["R. Kohavi", "G.H. John"], "venue": "Artificial intelligence, 97(1):273\u2013324,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1997}, {"title": "Feature selection methods and algorithms", "author": ["L. Ladha", "T. Deepa"], "venue": "International journal on computer science and engineering, 1(3):1787\u20131797,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2011}, {"title": "Embedded methods", "author": ["T.N. Lal", "O. Chapelle", "J. Weston", "A. Elisseeff"], "venue": "Feature extraction, pages 137\u2013165. Springer,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2006}, {"title": "Feature selection and feature extraction for text categorization", "author": ["D.D. Lewis"], "venue": "Proceedings of the workshop on Speech and Natural Language, pages 212\u2013217. Association for Computational Linguistics,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1992}, {"title": "Information Technology and Applications: Proceedings of the 2014 International Conference on Information technology and Applications (ITA 2014), Xian, China, 8-9 August 2014", "author": ["X. Li"], "venue": "CRC Press,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}, {"title": "UCI machine learning repository", "author": ["M. Lichman"], "venue": "http://archive.ics.uci.edu/ml,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2013}, {"title": "Information-theoretic feature selection in microarray data using variable complementarity", "author": ["P.E. Meyer", "C. Schretter", "G. Bontempi"], "venue": "IEEE Journal of Selected Topics in Signal Processing, 2(3):261\u2013274,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2008}, {"title": "Effective global approaches for mutual information based feature selection", "author": ["X.V. Nguyen", "J. Chan", "S. Romano", "J. Bailey"], "venue": "Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 512\u2013521. ACM,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2014}, {"title": "Feature selection based on mutual information criteria of max-dependency, maxrelevance, and min-redundancy", "author": ["H. Peng", "F. Long", "C. Ding"], "venue": "IEEE Transactions on pattern analysis and machine intelligence, 27(8):1226\u20131238,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2005}, {"title": "Heterogeneous feature subset selection using mutual information-based feature transformation", "author": ["M. Wei", "T.W. Chow", "R.H. Chan"], "venue": "Neurocomputing, 168:706\u2013718,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2015}, {"title": "Data visualization and feature selection: New algorithms for nongaussian data", "author": ["H.H. Yang", "J.E. Moody"], "venue": "NIPS, volume 99, pages 687\u2013693. Citeseer,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 1999}], "referenceMentions": [{"referenceID": 12, "context": "The best subset contains the least number of dimensions that most contribute to accuracy [13].", "startOffset": 89, "endOffset": 93}, {"referenceID": 1, "context": "The central premise when using a feature selection technique is that the data contains many features that are either redundant or irrelevant, and can thus be removed without incurring much loss of information [2].", "startOffset": 209, "endOffset": 212}, {"referenceID": 11, "context": "Feature selection techniques can be broadly classified into two groups: classifier dependent (wrapper [12], embedded [14]) and classifier independent filter [11].", "startOffset": 102, "endOffset": 106}, {"referenceID": 13, "context": "Feature selection techniques can be broadly classified into two groups: classifier dependent (wrapper [12], embedded [14]) and classifier independent filter [11].", "startOffset": 117, "endOffset": 121}, {"referenceID": 10, "context": "Feature selection techniques can be broadly classified into two groups: classifier dependent (wrapper [12], embedded [14]) and classifier independent filter [11].", "startOffset": 157, "endOffset": 161}, {"referenceID": 8, "context": "In forward selection, variables are progressively incorporated into larger subsets, whereas in backward elimination one starts with the set of all variables and progressively eliminates the least promising ones [9].", "startOffset": 211, "endOffset": 214}, {"referenceID": 9, "context": "A very simple filter type feature selection algorithm is a basic correlation analysis [10], where only attributes which are correlated to the label are \u2217Authors contribute equally 0This work is supported by the NIH Grants #R41 GM116300, #R01 GM117594.", "startOffset": 86, "endOffset": 90}, {"referenceID": 2, "context": "[3] show that the redundant term should refer to conditional mutual information and summarizes a uniform framework for information theoretic feature selection.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[8], which can be seen as first order approximation of Information Gain, leading to the miss of multi-way feature combinations.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "X = AS, where each xi and si represents a random variable and A is called the mixing matrix [4].", "startOffset": 92, "endOffset": 95}, {"referenceID": 15, "context": "The connection between ICA algorithm and information theory has been well known for many years [16].", "startOffset": 95, "endOffset": 99}, {"referenceID": 14, "context": "Prior Work Lewis [15] and Duch [6] coined the term Mutual Information Maximization (MIM) as a feature scoring criteria, which measures the usefulness of a feature subset when used for classification.", "startOffset": 17, "endOffset": 21}, {"referenceID": 5, "context": "Prior Work Lewis [15] and Duch [6] coined the term Mutual Information Maximization (MIM) as a feature scoring criteria, which measures the usefulness of a feature subset when used for classification.", "startOffset": 31, "endOffset": 34}, {"referenceID": 0, "context": "JMIM (xi) = I(xk : y) Battiti [1] defined the Mutual Information Feature Selection (MIFS) criteria which ensures feature relevance and forces a penalty to ensure low correlations within the set of selected features, in a sequential manner.", "startOffset": 30, "endOffset": 33}, {"referenceID": 21, "context": "Yang and Moody [22] and Meyer et al.", "startOffset": 15, "endOffset": 19}, {"referenceID": 17, "context": "[18] proposed an alternate version of MIFS, called the Joint Mutual Information (JMI), which increases complementary information between features.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "[20] gave the Minimum-Redundancy Maximum-Relevance (MRMR) criteria which omits the conditional relevance term completely.", "startOffset": 0, "endOffset": 4}, {"referenceID": 6, "context": "Fleuret [7] introduced the Conditional Mutual Information Maximization (CMIM) criteria, which assumes that selected features are independent and class-conditionally independent given the unselected feature.", "startOffset": 8, "endOffset": 11}, {"referenceID": 18, "context": "[19] proposed a Global MI-based feature selection via spectral relaxation (SPECCMI) approach.", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "[8] produced Variational Information Maximization (VMI) which can be applied for feature selection over any general class of distributions and provide tractable lower bounds for mutual information.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "Readers can refer to [8] for a detailed derivation.", "startOffset": 21, "endOffset": 24}, {"referenceID": 7, "context": "[8] discusses the inconsistency of these two assumptions.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "Rbalance Lung [5] 56 32 3 0.", "startOffset": 14, "endOffset": 17}, {"referenceID": 16, "context": "1 Splice [17] 60 3175 3 0.", "startOffset": 9, "endOffset": 13}, {"referenceID": 16, "context": "07 Waveform [17] 40 5000 3 0.", "startOffset": 12, "endOffset": 16}, {"referenceID": 16, "context": "94 Semeion [17] 256 1593 10 0.", "startOffset": 11, "endOffset": 15}, {"referenceID": 16, "context": "94 Optdigits[17] 64 5620 10 0.", "startOffset": 12, "endOffset": 16}, {"referenceID": 16, "context": "89 Musk2 [17] 168 6598 2 0.", "startOffset": 9, "endOffset": 13}, {"referenceID": 16, "context": "98 Spambase[17] 57 4601 2 0.", "startOffset": 11, "endOffset": 15}, {"referenceID": 16, "context": "97 Promoter [17] 58 106 4 0.", "startOffset": 12, "endOffset": 16}, {"referenceID": 8, "context": "02 Madelon [9] 500 4400 2 0.", "startOffset": 11, "endOffset": 14}, {"referenceID": 7, "context": "We pre-process data following the approach proposed in VMI [8].", "startOffset": 59, "endOffset": 62}, {"referenceID": 16, "context": "Figure 2: Average Cross Validation Error of the aforementioned methods and ours, on the SEMEION [17] dataset.", "startOffset": 96, "endOffset": 100}, {"referenceID": 16, "context": "We show the average cross validation error results for the nine datasets [17] in Table 2.", "startOffset": 73, "endOffset": 77}, {"referenceID": 7, "context": "Dataset VMI[8] mRMR[20] JMI[22] CMIM[7] SPECCMI[19] Our Method Lung 7.", "startOffset": 11, "endOffset": 14}, {"referenceID": 19, "context": "Dataset VMI[8] mRMR[20] JMI[22] CMIM[7] SPECCMI[19] Our Method Lung 7.", "startOffset": 19, "endOffset": 23}, {"referenceID": 21, "context": "Dataset VMI[8] mRMR[20] JMI[22] CMIM[7] SPECCMI[19] Our Method Lung 7.", "startOffset": 27, "endOffset": 31}, {"referenceID": 6, "context": "Dataset VMI[8] mRMR[20] JMI[22] CMIM[7] SPECCMI[19] Our Method Lung 7.", "startOffset": 36, "endOffset": 39}, {"referenceID": 18, "context": "Dataset VMI[8] mRMR[20] JMI[22] CMIM[7] SPECCMI[19] Our Method Lung 7.", "startOffset": 47, "endOffset": 51}, {"referenceID": 7, "context": "Dataset VMI[8] mRMR[20] JMI[22] CMIM[7] SPECCMI[19] Our Method Lung 82.", "startOffset": 11, "endOffset": 14}, {"referenceID": 19, "context": "Dataset VMI[8] mRMR[20] JMI[22] CMIM[7] SPECCMI[19] Our Method Lung 82.", "startOffset": 19, "endOffset": 23}, {"referenceID": 21, "context": "Dataset VMI[8] mRMR[20] JMI[22] CMIM[7] SPECCMI[19] Our Method Lung 82.", "startOffset": 27, "endOffset": 31}, {"referenceID": 6, "context": "Dataset VMI[8] mRMR[20] JMI[22] CMIM[7] SPECCMI[19] Our Method Lung 82.", "startOffset": 36, "endOffset": 39}, {"referenceID": 18, "context": "Dataset VMI[8] mRMR[20] JMI[22] CMIM[7] SPECCMI[19] Our Method Lung 82.", "startOffset": 47, "endOffset": 51}, {"referenceID": 7, "context": "Dataset VMI[8] mRMR[20] JMI[22] CMIM[7] SPECCMI[19] Our Method Lung 23.", "startOffset": 11, "endOffset": 14}, {"referenceID": 19, "context": "Dataset VMI[8] mRMR[20] JMI[22] CMIM[7] SPECCMI[19] Our Method Lung 23.", "startOffset": 19, "endOffset": 23}, {"referenceID": 21, "context": "Dataset VMI[8] mRMR[20] JMI[22] CMIM[7] SPECCMI[19] Our Method Lung 23.", "startOffset": 27, "endOffset": 31}, {"referenceID": 6, "context": "Dataset VMI[8] mRMR[20] JMI[22] CMIM[7] SPECCMI[19] Our Method Lung 23.", "startOffset": 36, "endOffset": 39}, {"referenceID": 18, "context": "Dataset VMI[8] mRMR[20] JMI[22] CMIM[7] SPECCMI[19] Our Method Lung 23.", "startOffset": 47, "endOffset": 51}, {"referenceID": 20, "context": "Handling Non-Numerical Features To verify the effectiveness of the proposed HOFS for a non-/numerical feature selection, 1000 samples with 20 heterogeneous features and 5 classes were synthesized according to [21].", "startOffset": 209, "endOffset": 213}], "year": 2016, "abstractText": "Feature selection is a process of choosing a subset of relevant features so that the quality of prediction models can be improved. An extensive body of work exists on information-theoretic feature selection, based on maximizing Mutual Information (MI) between subsets of features and class labels. The prior methods use a lower order approximation, by treating the joint entropy as a summation of several single variable entropies. This leads to locally optimal selections and misses multi-way feature combinations. We present a higher order MI based approximation technique called Higher Order Feature Selection (HOFS). Instead of producing a single list of features, our method produces a ranked collection of feature subsets that maximizes MI, giving better comprehension (feature ranking) as to which features work best together when selected, due to their underlying interdependent structure. Our experiments demonstrate that the proposed method performs better than existing feature selection approaches while keeping similar running times and computational complexity.", "creator": "LaTeX with hyperref package"}}}