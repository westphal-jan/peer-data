{"id": "1612.00584", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Dec-2016", "title": "Alleviating Overfitting for Polysemous Words for Word Representation Estimation Using Lexicons", "abstract": "Though there are some works on improving distributed word representations using lexicons, the improper overfitting of the words that have multiple meanings is a remaining issue deteriorating the learning when lexicons are used, which needs to be solved. An alternative method is to allocate a vector per sense instead a vector per word. However, the word representations estimated in the former way are not as easy to use as the latter one. Our previous work uses a probabilistic method to alleviate the overfitting, but it is not robust with small corpus. In this paper, we propose a new neural network to estimate distributed word representations using a lexicon and a corpus. We add a lexicon layer in continuous bag-of-words model, and a threshold node after the output of the lexicon layer. The threshold rejects the \"bad\" outputs of the lexicon layer that are less likely to be the same with their inputs. In this way, it alleviates the overfitting of the polysemous words. The proposed neural network can be trained using negative sampling, which maximizing the log probabilities of target words given the context words, by distinguishing the target words from random noises. We compare the proposed neural network with continuous bag-of-words model, the other works improving it, and the previous works estimating distributed word representations using both a lexicon and a corpus. The experimental results show that the proposed neural network is more efficient and balanced for both semantic tasks and syntactic tasks than the previous works, and robust to the size of the corpus.", "histories": [["v1", "Fri, 2 Dec 2016 07:45:40 GMT  (383kb)", "http://arxiv.org/abs/1612.00584v1", "7 pages, under review as a conference paper at IEEE IJCNN 2017"], ["v2", "Thu, 9 Mar 2017 12:36:26 GMT  (1977kb)", "http://arxiv.org/abs/1612.00584v2", "Accepted by IEEE IJCNN 2017. Copyright transferred to IEEE"]], "COMMENTS": "7 pages, under review as a conference paper at IEEE IJCNN 2017", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["yuanzhi ke", "masafumi hagiwara"], "accepted": false, "id": "1612.00584"}, "pdf": {"name": "1612.00584.pdf", "metadata": {"source": "CRF", "title": "Alleviating Overfitting for Polysemous Words for Word Representation Estimation Using Lexicons", "authors": ["Yuanzhi Ke", "Masafumi Hagiwara"], "emails": ["enshi@soft.ics.keio.ac.jp", "hagiwara@soft.ics.keio.ac.jp"], "sections": [{"heading": null, "text": "It is not only a question of the expression, but also of the word, the word, the word, the word, the word, the word, the word, the word, the word, the word, the word, the word, the word, the word, the word, the word, the word, the word, the word, the word, the word, the word, the word, the word, the word, the word, the word, the word, the word, the word, the word, the word, the word, the word, the word, the word, the word, the word, the word, the word, the word, the word, the word, the word, the word, the word, the word, the word, the word, the word, the word, the word, the word, the word, the word, the word, the word, the word, the word, the word, the word, the word, the word, the word, the word, the word, the word, the word, the word, the word, the word, the word, the word, the word, the word, the word, the word, the word, the word"}, {"heading": "II. RELATED WORKS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. Continuous Bag-of-Words", "text": "Continuous Bag-of-Words (CBOW) with negative sampling [8] is an efficient algorithm for estimating distributed word representations. The goal of CBOW is to maximize the log probability of a target word, taking into account the vectors of the context words. Call the size of the vocabulary V, the size of the word vectors W. The model is like Fig. 1. Negative sampling is an efficient method for maximizing the log probability. It is a simplified noise-contrasting estimation [27]. It trains the model by distinguishing between target and randomly drawn noise. Call the vector of the target word vwO, the vector of a context word vwI, for each context word of the target the goal is the maximization: LoEnvironment (vO TvwI) + n = 1wi (w)."}, {"heading": "B. Continuous Bag of Fuzzy Paraphrases", "text": "The Continuous Bag of Fuzzy Paraphrases (CBOFP) model proposed by Ke et al. [26] is a model based on CBOW to learn word representations using both a lexicon and a corpus. It is able to alleviate the overmatch of word vectors for polysemic words. It surpasses previous work that lexicon uses to estimate distributed word representations, but is not robust for small corporations. Fig. 2 shows the structure of the model. CBOFP adds a lexicon level to CBOW. Unlike previous work with lexicon to estimate distributed word representations, in CBOFP each paraphrase is a blurred member of the word set with a degree of truth. The results of the lexicon layer are random. The failure product is controlled by a function of the paraphrase peraphrase that yields 0 or 1 from a Bernoulli distribution."}, {"heading": "III. THE PROPOSED METHOD", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. Structure", "text": "Our previous work CBOFP is weak on small corpus because it contains a probabilistic method to alleviate the overmatch of polysemic words. Such a method requires sufficient training data, so we are looking at another method that does not use a probabilistic method.Instead of randomly folding some of the lexicon level outputs, we are adding a node after the lexicon level, as in Fig. 3. The inputs are the context word of the target word. They are entered both in the hidden level that contains the vectors to be learned, and in the lexicon level.The lexicon level outputs the paraphrases of the inputs. The node after the lexicon level takes the score of the target word and holds a threshold.If the score of the circumscription is higher than the threshold, it returns true. Otherwise, it returns false."}, {"heading": "B. The Lexicon Layer", "text": "We use a paraphrase database called PPDB2.0 [28] - [30] to build our lexicon level used in previous work [22], [24], [26] and the paraphrases contained therein are automatically extracted from multilingual resources. It is reported that they are useful for many other tasks, such as detecting textual entanglements [31], [32], measuring semantic similarity [33] - [35], monolingual entanglement [36], [37] and generating natural language [38]. PPDB2.0 provides not only the paraphrases, but also the characteristics, alignment types and entanglement types. There are six types of entanglements in PPDB2.0, as in Table I. We consider the \"exclusion type,\" other related \"types,\" PPDB2.0, \"type\" independent, \"and only the\" natural entanglements. \""}, {"heading": "C. Learning the Word Representations", "text": "With the threshold, the goal to maximize becomesG, wi, wi, j, clog p (wi | wj) + Lwj, wk, Lwjf (Sjk) log p (wi | wk). (4) Here, G is the sentence of words in the corpus, C is the context of the word wi, Lwj is the paraphrase of the context word wj, Sjk is the value of the paraphrase wk in Lwj. The function f (Sjk) is defined as follows: f (Sjk) {1 if Sjk > 0 if Sjk < \u03b8. (5) Here, the threshold of the threshold is node.The log probability is maximized in the learning phrase by means of negative sampling. Similar to CBOW, we maximize the log probability in the equation (4) by maximizing the log probability (vwi Tvwj) + N = 1Ewi, word (Pww) or the layer of smoke (vw)."}, {"heading": "IV. EXPERIMENTS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. The Corpus Used in the Experiments", "text": "We are using text8 and a larger corpus called enwiki92 for the experiments to assess whether the proposed model for a smaller corpus is more robust than the previous work and more efficient for a larger corpus. Both text8 and enwiki9 are part of the English Wikipedia3 dump. Text8 contains 16,718,843 tokens, while enwiki9 contains 123,353,508 tokens. The vocabulary size of text8 is 71,291, while that of enwiki9 is 218,317. We see that text8 is one-tenth the size of enwiki9.2http: / / mattmahoney.net / dc / enwiki9.zip 3https: / / en.wikipedia.org /"}, {"heading": "B. The Task for Evaluation", "text": "For a quaternion of words (wA, wB, wC, wD) in which the relationship of wA and wB is similar to that of wC and wD, the goal is to predict wD on the basis of wA, wB and wC by searching for the word whose vector vB \u2212 vA + vC comes closest. The dataset has a semantic and a syntactic part. In the semantic part (wA, wB) and (wC, wD) have a similar semantic relationship, while in the syntactic part they have a syntactic relationship. Table II shows an example of the questions in the task. In the semantic part there are 8,869 questions and 10,675 questions in the syntactic part."}, {"heading": "C. Tuning", "text": "To find the correct value of the threshold, we need to let the proposed neural network learn the word representations for Text8 and enwiki9 and perform the word analog brainteaser with a different threshold. Results for Text8 are in Fig. 4. Results for enwiki9 are in Fig. 5. We see that the correlation between performance and threshold is not linear, and it is not very similar for different tasks or corpora. However, since all values of the paraphrases - the PPDB2.0 values in the version we use are below seven - are at 3.8, we can find the best threshold in the interval. From Fig. 4, we see that the best threshold for the semantic part and the entire dataset is using Text8. The best threshold for the syntactic part is 5.7. From Fig. 5, we see that the best threshold for the semantic part and the entire dataset is 1.5."}, {"heading": "D. Comparison and Results", "text": "We compare our proposed neural network with CBOW [8], CBOW enriched with subword information [11], GloVe [10], the work of Faruqui et al. [24], jointReps [23], RC-Net [25] and CBOFP [26]. For the first four, we used their publicly available online tools by the authors to learn word representations with text8 and enwiki9 and report the results. For jointReps and RC-Net, we4https: / code.google.com / archive / p / word2vec / failed to find an available tool that works correctly with text8 and enwikiki9. Thus, we use the results in their papers for jointReps and RC-Net."}, {"heading": "V. CONCLUSIONS", "text": "In order to reduce the overmatch of polyseed words, which was not well addressed in the previous work, we proposed a new neural network for estimating distributed word representations in this essay. In addition to the traditional continuous vocabulary model, we added a lexicon layer and a threshold node after output of the lexicon layer. The threshold is set manually, the neural network can be trained using negative samples. Experimental results show that the proposed neural network 5http: / / wacky.sslmit.unibo.it is more powerful and balanced than the previous models that lexicon use to estimate or improve distributed word representations. Furthermore, the proposed neural network in this essay is robust for small businesses, unlike our previous work CBOFP. Automatic matching of the threshold and other parameters remains a persistent problem. We will work on this in the future."}], "references": [{"title": "A neural probabilistic language model", "author": ["Y. Bengio", "R. Ducharme", "P. Vincent", "C. Jauvin"], "venue": "journal of machine learning research, vol. 3, no. Feb, pp. 1137\u20131155, 2003.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2003}, {"title": "Three new graphical models for statistical language modelling", "author": ["A. Mnih", "G. Hinton"], "venue": "Proceedings of the 24th international conference on Machine learning. ACM, 2007, pp. 641\u2013648.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2007}, {"title": "A scalable hierarchical distributed language model", "author": ["A. Mnih", "G.E. Hinton"], "venue": "Advances in neural information processing systems, 2009, pp. 1081\u20131088.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2009}, {"title": "Natural language processing (almost) from scratch", "author": ["R. Collobert", "J. Weston", "L. Bottou", "M. Karlen", "K. Kavukcuoglu", "P. Kuksa"], "venue": "Journal of Machine Learning Research, vol. 12, no. Aug, pp. 2493\u20132537, 2011.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2011}, {"title": "Improving word representations via global context and multiple word prototypes", "author": ["E.H. Huang", "R. Socher", "C.D. Manning", "A.Y. Ng"], "venue": "Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume 1. Association for Computational Linguistics, 2012, pp. 873\u2013882.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2012}, {"title": "Statistical language models based on neural networks", "author": ["T. Mikolov"], "venue": "Ph.D. dissertation, Brno University of Technology, 2012.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2012}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["T. Mikolov", "I. Sutskever", "K. Chen", "G.S. Corrado", "J. Dean"], "venue": "Advances in neural information processing systems, 2013, pp. 3111\u20133119.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2013}, {"title": "Efficient estimation of word representations in vector space", "author": ["T. Mikolov", "K. Chen", "G. Corrado", "J. Dean"], "venue": "ICLR Workshop, 2013.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2013}, {"title": "Linguistic regularities in continuous space word representations", "author": ["T. Mikolov", "W.-t. Yih", "G. Zweig"], "venue": "Proceedings of NAACL HLT, vol. 13, 2013, pp. 746\u2013751.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2013}, {"title": "Glove: Global vectors for word representation", "author": ["J. Pennington", "R. Socher", "C.D. Manning"], "venue": "Empirical Methods in Natural Language Processing (EMNLP), 2014, pp. 1532\u20131543.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2014}, {"title": "Enriching word vectors with subword information", "author": ["P. Bojanowski", "E. Grave", "A. Joulin", "T. Mikolov"], "venue": "arXiv preprint arXiv:1607.04606, 2016.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2016}, {"title": "Word representations: a simple and general method for semi-supervised learning", "author": ["J. Turian", "L. Ratinov", "Y. Bengio"], "venue": "Proceedings of the 48th annual meeting of the association for computational linguistics (ACL 2010). Association for Computational Linguistics, 2010, pp. 384\u2013394.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2010}, {"title": "Semantic compositionality through recursive matrix-vector spaces", "author": ["R. Socher", "B. Huval", "C.D. Manning", "A.Y. Ng"], "venue": "Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning. Association for Computational Linguistics, 2012, pp. 1201\u20131211.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2012}, {"title": "Distributed representations of sentences and documents.", "author": ["Q.V. Le", "T. Mikolov"], "venue": "in the 31st International Conference on Machine Learning (ICML 2014),", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2014}, {"title": "Convolutional neural networks for sentence classification", "author": ["Y. Kim"], "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), 2014.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2014}, {"title": "Bag of tricks for efficient text classification", "author": ["A. Joulin", "E. Grave", "P. Bojanowski", "T. Mikolov"], "venue": "arXiv preprint arXiv:1607.01759, 2016.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2016}, {"title": "Gaussian lda for topic models with word embeddings", "author": ["R. Das", "M. Zaheer", "C. Dyer"], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers). Association for Computational Linguistics, 2015.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2015}, {"title": "Generative topic embedding: a continuous representation of documents", "author": ["S. Li", "T.-S. Chua", "J. Zhu", "C. Miao"], "venue": "the 54th annual meeting of the Association for Computational Linguistics (ACL 2016). Association for Computational Linguistics, 2016.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2016}, {"title": "Recurrent neural network regularization", "author": ["W. Zaremba", "I. Sutskever", "O. Vinyals"], "venue": "CoRR, vol. abs/1409.2329, 2014.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2014}, {"title": "Sequence to sequence learning with neural networks", "author": ["I. Sutskever", "O. Vinyals", "Q.V. Le"], "venue": "Advances in Neural Information Processing Systems 27 (NIPS 2014), 2014.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2014}, {"title": "A unified model for word sense representation and disambiguation.", "author": ["X. Chen", "Z. Liu", "M. Sun"], "venue": "in EMNLP. Citeseer,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2014}, {"title": "Improving lexical embeddings with semantic knowledge.\u201d in the 52nd Annual Meeting of the Association for Computational Linguistics (ACL2014)", "author": ["M. Yu", "M. Dredze"], "venue": "Association for Computational Linguistics,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2014}, {"title": "Joint word representation learning using a corpus and a semantic lexicon", "author": ["D. Bollegala", "A. Mohammed", "T. Maehara", "K.-I. Kawarabayashi"], "venue": "Proceedings of the 30th AAAI Conference on Artificial Intelligence (AAAI\u201916), 2016.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2016}, {"title": "Retrofitting word vectors to semantic lexicons", "author": ["M. Faruqui", "J. Dodge", "S.K. Jauhar", "C. Dyer", "E. Hovy", "N.A. Smith"], "venue": "Proceedings of NAACL, 2015.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2015}, {"title": "Rc-net: A general framework for incorporating knowledge into word representations", "author": ["C. Xu", "Y. Bai", "J. Bian", "B. Gao", "G. Wang", "X. Liu", "T.-Y. Liu"], "venue": "Proceedings of the 23rd ACM International Conference on Conference on Information and Knowledge Management. ACM, 2014, pp. 1219\u20131228.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2014}, {"title": "Fuzzy paraphrases in learning word representations with a corpus and a lexicon", "author": ["Y. Ke", "M. Hagiwara"], "venue": "ArXiv e-prints, Nov. 2016.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2016}, {"title": "Noise-contrastive estimation of unnormalized statistical models, with applications to natural image statistics", "author": ["M.U. Gutmann", "A. Hyv\u00e4rinen"], "venue": "Journal of Machine Learning Research, vol. 13, no. Feb, pp. 307\u2013361, 2012.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2012}, {"title": "PPDB: The paraphrase database", "author": ["J. Ganitkevitch", "B. Van Durme", "C. Callison-Burch"], "venue": "Proceedings of NAACL-HLT. Atlanta, Georgia: Association for Computational Linguistics, June 2013, pp. 758\u2013764.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2013}, {"title": "Ppdb 2.0: Better paraphrase ranking, fine-grained entailment relations, word embeddings, and style classification", "author": ["E. Pavlick", "P. Rastogi", "J. Ganitkevich", "B.V. Durme", "C. Callison- Burch"], "venue": "Association for  Computational Linguistics. Beijing, China: Association for Computational Linguistics, July 2015.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2015}, {"title": "Adding semantics to data-driven paraphrasing", "author": ["E. Pavlick", "J. Bos", "M. Nissim", "C. Beller", "B.V. Durme", "C. Callison- Burch"], "venue": "Association for Computational Linguistics. Beijing, China: Association for Computational Linguistics, July 2015.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2015}, {"title": "Utexas: Natural language semantics using distributional semantics and probabilistic logic", "author": ["I. Beltagy", "S. Roller", "G. Boleda", "K. Erk", "R.J. Mooney"], "venue": "SemEval 2014, p. 796, 2014.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2014}, {"title": "The meaning factory: Formal semantics for recognizing textual entailment and determining semantic similarity", "author": ["J. Bjerva", "J. Bos", "R. Van der Goot", "M. Nissim"], "venue": "Proceedings of SemEval, 2014.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2014}, {"title": "Umbc ebiquity-core: Semantic textual similarity systems", "author": ["L. Han", "A. Kashyap", "T. Finin", "J. Mayfield", "J. Weese"], "venue": "Proceedings of the Second Joint Conference on Lexical and Computational Semantics, vol. 1, 2013, pp. 44\u201352.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2013}, {"title": "Discriminative improvements to distributional sentence similarity.", "author": ["Y. Ji", "J. Eisenstein"], "venue": "in EMNLP,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2013}, {"title": "Dls@ cu: Sentence similarity from word alignment", "author": ["M.A. Sultan", "S. Bethard", "T. Sumner"], "venue": "Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), 2014, pp. 241\u2013246.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2014}, {"title": "Back to basics for monolingual alignment: Exploiting word similarity and contextual evidence", "author": ["M.A. Sultan", "S. Bethard", "T. Sumner"], "venue": "Transactions of the Association for Computational Linguistics, vol. 2, pp. 219\u2013230, 2014.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2014}, {"title": "Semi-markov phrase-based monolingual alignment.", "author": ["X. Yao", "B. Van Durme", "C. Callison-Burch", "P. Clark"], "venue": "in EMNLP,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2013}, {"title": "Learning sentential paraphrases from bilingual parallel corpora for text-to-text generation", "author": ["J. Ganitkevitch", "C. Callison-Burch", "C. Napoles", "B. Van Durme"], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, 2011, pp. 1168\u20131179.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2011}], "referenceMentions": [{"referenceID": 0, "context": "Distributed text representations estimated using a neural network are useful to be applied to conventional natural language processing algorithms [1]\u2013[11].", "startOffset": 146, "endOffset": 149}, {"referenceID": 10, "context": "Distributed text representations estimated using a neural network are useful to be applied to conventional natural language processing algorithms [1]\u2013[11].", "startOffset": 150, "endOffset": 154}, {"referenceID": 11, "context": "Great improvement by the distributed text representations estimated this way has been reported in name entity recognition and chunking [12], text classification [13]\u2013[16], topic extraction [17], [18], and machine translation [19], [20] etc.", "startOffset": 135, "endOffset": 139}, {"referenceID": 12, "context": "Great improvement by the distributed text representations estimated this way has been reported in name entity recognition and chunking [12], text classification [13]\u2013[16], topic extraction [17], [18], and machine translation [19], [20] etc.", "startOffset": 161, "endOffset": 165}, {"referenceID": 15, "context": "Great improvement by the distributed text representations estimated this way has been reported in name entity recognition and chunking [12], text classification [13]\u2013[16], topic extraction [17], [18], and machine translation [19], [20] etc.", "startOffset": 166, "endOffset": 170}, {"referenceID": 16, "context": "Great improvement by the distributed text representations estimated this way has been reported in name entity recognition and chunking [12], text classification [13]\u2013[16], topic extraction [17], [18], and machine translation [19], [20] etc.", "startOffset": 189, "endOffset": 193}, {"referenceID": 17, "context": "Great improvement by the distributed text representations estimated this way has been reported in name entity recognition and chunking [12], text classification [13]\u2013[16], topic extraction [17], [18], and machine translation [19], [20] etc.", "startOffset": 195, "endOffset": 199}, {"referenceID": 18, "context": "Great improvement by the distributed text representations estimated this way has been reported in name entity recognition and chunking [12], text classification [13]\u2013[16], topic extraction [17], [18], and machine translation [19], [20] etc.", "startOffset": 225, "endOffset": 229}, {"referenceID": 19, "context": "Great improvement by the distributed text representations estimated this way has been reported in name entity recognition and chunking [12], text classification [13]\u2013[16], topic extraction [17], [18], and machine translation [19], [20] etc.", "startOffset": 231, "endOffset": 235}, {"referenceID": 15, "context": "For example, the conventional algorithms fail to correctly predicate the number of starts of 40% amazon reviews [16].", "startOffset": 112, "endOffset": 116}, {"referenceID": 0, "context": "The early approaches to estimate text representations use n-gram models [1], [4], [5].", "startOffset": 72, "endOffset": 75}, {"referenceID": 3, "context": "The early approaches to estimate text representations use n-gram models [1], [4], [5].", "startOffset": 77, "endOffset": 80}, {"referenceID": 4, "context": "The early approaches to estimate text representations use n-gram models [1], [4], [5].", "startOffset": 82, "endOffset": 85}, {"referenceID": 6, "context": "propose continuous bag-of-words and skip-gram models [7], [8].", "startOffset": 53, "endOffset": 56}, {"referenceID": 7, "context": "propose continuous bag-of-words and skip-gram models [7], [8].", "startOffset": 58, "endOffset": 61}, {"referenceID": 9, "context": "[10] propose an algorithm using both local information and global information in the corpus and report a higher performance.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[11] extend the models of Mikolov et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "[21] use the definitions in the lexicons to estimate representations for word senses and outperform the sense representations by Huang et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 4, "context": "[5].", "startOffset": 0, "endOffset": 3}, {"referenceID": 21, "context": "[22] and Bollegala et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "[23] estimate the word representations by not only maximizing the probability of target word given a context, but also minimizing the distance of the paraphrases in a lexicon at the same time.", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "[24] propose a method refining trained word representation vectors using lexicons.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "[25] estimate the word representations jointly by minimizing the distance of the tail word from the sum of the vectors of the head word and the relation for a triplet of words (head, relation, tail), and making words less similar to each other in a larger category.", "startOffset": 0, "endOffset": 4}, {"referenceID": 25, "context": "[26] propose a method considering the lexicon as a fuzzy set of paraphrases and using Bernoulli distribution subjected to the membership function of paraphrases to alleviate the problem.", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "Continuous Bag-of-Words (CBOW) with negative sampling [8] is an efficient algorithm to estimate distributed word representations.", "startOffset": 54, "endOffset": 57}, {"referenceID": 26, "context": "It is a simplified Noise Contrastive Estimation [27].", "startOffset": 48, "endOffset": 52}, {"referenceID": 25, "context": "Continuous Bag of Fuzzy Paraphrases (CBOFP) [26] proposed by Ke et al.", "startOffset": 44, "endOffset": 48}, {"referenceID": 27, "context": "The degree of truth is measured using the score provided by a paraphrase database called PPDB [28]\u2013[30].", "startOffset": 94, "endOffset": 98}, {"referenceID": 29, "context": "The degree of truth is measured using the score provided by a paraphrase database called PPDB [28]\u2013[30].", "startOffset": 99, "endOffset": 103}, {"referenceID": 27, "context": "0 [28]\u2013[30] to build our lexicon layer, which has been used in the previous works [22], [24], [26].", "startOffset": 2, "endOffset": 6}, {"referenceID": 29, "context": "0 [28]\u2013[30] to build our lexicon layer, which has been used in the previous works [22], [24], [26].", "startOffset": 7, "endOffset": 11}, {"referenceID": 21, "context": "0 [28]\u2013[30] to build our lexicon layer, which has been used in the previous works [22], [24], [26].", "startOffset": 82, "endOffset": 86}, {"referenceID": 23, "context": "0 [28]\u2013[30] to build our lexicon layer, which has been used in the previous works [22], [24], [26].", "startOffset": 88, "endOffset": 92}, {"referenceID": 25, "context": "0 [28]\u2013[30] to build our lexicon layer, which has been used in the previous works [22], [24], [26].", "startOffset": 94, "endOffset": 98}, {"referenceID": 30, "context": "It is reported useful in many other tasks such as recognizing textual entailment [31], [32], measuring the semantic similarity [33]\u2013[35], monolingual alignment [36], [37], and natural language generation [38].", "startOffset": 81, "endOffset": 85}, {"referenceID": 31, "context": "It is reported useful in many other tasks such as recognizing textual entailment [31], [32], measuring the semantic similarity [33]\u2013[35], monolingual alignment [36], [37], and natural language generation [38].", "startOffset": 87, "endOffset": 91}, {"referenceID": 32, "context": "It is reported useful in many other tasks such as recognizing textual entailment [31], [32], measuring the semantic similarity [33]\u2013[35], monolingual alignment [36], [37], and natural language generation [38].", "startOffset": 127, "endOffset": 131}, {"referenceID": 34, "context": "It is reported useful in many other tasks such as recognizing textual entailment [31], [32], measuring the semantic similarity [33]\u2013[35], monolingual alignment [36], [37], and natural language generation [38].", "startOffset": 132, "endOffset": 136}, {"referenceID": 35, "context": "It is reported useful in many other tasks such as recognizing textual entailment [31], [32], measuring the semantic similarity [33]\u2013[35], monolingual alignment [36], [37], and natural language generation [38].", "startOffset": 160, "endOffset": 164}, {"referenceID": 36, "context": "It is reported useful in many other tasks such as recognizing textual entailment [31], [32], measuring the semantic similarity [33]\u2013[35], monolingual alignment [36], [37], and natural language generation [38].", "startOffset": 166, "endOffset": 170}, {"referenceID": 37, "context": "It is reported useful in many other tasks such as recognizing textual entailment [31], [32], measuring the semantic similarity [33]\u2013[35], monolingual alignment [36], [37], and natural language generation [38].", "startOffset": 204, "endOffset": 208}, {"referenceID": 28, "context": "0 [29], [30].", "startOffset": 2, "endOffset": 6}, {"referenceID": 29, "context": "0 [29], [30].", "startOffset": 8, "endOffset": 12}, {"referenceID": 28, "context": "0 scores and human judgments are reported [29].", "startOffset": 42, "endOffset": 46}, {"referenceID": 7, "context": "[8] is used for evaluation in the experiments.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "We compare our proposed neural network with CBOW [8], CBOW enriched with subword information [11], GloVe [10], the work of Faruqui et al.", "startOffset": 49, "endOffset": 52}, {"referenceID": 10, "context": "We compare our proposed neural network with CBOW [8], CBOW enriched with subword information [11], GloVe [10], the work of Faruqui et al.", "startOffset": 93, "endOffset": 97}, {"referenceID": 9, "context": "We compare our proposed neural network with CBOW [8], CBOW enriched with subword information [11], GloVe [10], the work of Faruqui et al.", "startOffset": 105, "endOffset": 109}, {"referenceID": 23, "context": "[24], jointReps [23], RC-Net [25] and CBOFP [26].", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "[24], jointReps [23], RC-Net [25] and CBOFP [26].", "startOffset": 16, "endOffset": 20}, {"referenceID": 24, "context": "[24], jointReps [23], RC-Net [25] and CBOFP [26].", "startOffset": 29, "endOffset": 33}, {"referenceID": 25, "context": "[24], jointReps [23], RC-Net [25] and CBOFP [26].", "startOffset": 44, "endOffset": 48}, {"referenceID": 7, "context": "CBOW [8] 46.", "startOffset": 5, "endOffset": 8}, {"referenceID": 10, "context": "Enriched CBOW[11] 15.", "startOffset": 13, "endOffset": 17}, {"referenceID": 9, "context": "GloVe [10] 41.", "startOffset": 6, "endOffset": 10}, {"referenceID": 23, "context": "Faruqui [24] 34.", "startOffset": 8, "endOffset": 12}, {"referenceID": 25, "context": "CBOFP [26] 46.", "startOffset": 6, "endOffset": 10}, {"referenceID": 7, "context": "CBOW [8] 72.", "startOffset": 5, "endOffset": 8}, {"referenceID": 10, "context": "Enriched CBOW[11] 33.", "startOffset": 13, "endOffset": 17}, {"referenceID": 9, "context": "GloVe [10] 66.", "startOffset": 6, "endOffset": 10}, {"referenceID": 23, "context": "Faruqui [24] 53.", "startOffset": 8, "endOffset": 12}, {"referenceID": 22, "context": "JointReps [23] 61.", "startOffset": 10, "endOffset": 14}, {"referenceID": 24, "context": "RC-Net [25] 34.", "startOffset": 7, "endOffset": 11}, {"referenceID": 25, "context": "CBOFP [26] 73.", "startOffset": 6, "endOffset": 10}], "year": 2016, "abstractText": "Though there are some works on improving distributed word representations using lexicons, the improper overfitting of the words that have multiple meanings is a remaining issue deteriorating the learning when lexicons are used, which needs to be solved. An alternative method is to allocate a vector per sense instead a vector per word. However, the word representations estimated in the former way are not as easy to use as the latter one. Our previous work uses a probabilistic method to alleviate the overfitting, but it is not robust with small corpus. In this paper, we propose a new neural network to estimate distributed word representations using a lexicon and a corpus. We add a lexicon layer in continuous bag-of-words model, and a threshold node after the output of the lexicon layer. The threshold rejects the \u201cbad\u201d outputs of the lexicon layer that are less likely to be the same with their inputs. In this way, it alleviates the overfitting of the polysemous words. The proposed neural network can be trained using negative sampling, which maximizing the log probabilities of target words given the context words, by distinguishing the target words from random noises. We compare the proposed neural network with continuous bagof-words model, the other works improving it, and the previous works estimating distributed word representations using both a lexicon and a corpus. The experimental results show that the proposed neural network is more efficient and balanced for both semantic tasks and syntactic tasks than the previous works, and robust to the size of the corpus.", "creator": "gnuplot 4.6 patchlevel 3"}}}