{"id": "1612.00671", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Nov-2016", "title": "Reliable Evaluation of Neural Network for Multiclass Classification of Real-world Data", "abstract": "This paper presents a systematic evaluation of Neural Network (NN) for classification of real-world data. In the field of machine learning, it is often seen that a single parameter that is 'predictive accuracy' is being used for evaluating the performance of a classifier model. However, this parameter might not be considered reliable given a dataset with very high level of skewness. To demonstrate such behavior, seven different types of datasets have been used to evaluate a Multilayer Perceptron (MLP) using twelve(12) different parameters which include micro- and macro-level estimation. In the present study, the most common problem of prediction called 'multiclass' classification has been considered. The results that are obtained for different parameters for each of the dataset could demonstrate interesting findings to support the usability of these set of performance evaluation parameters.", "histories": [["v1", "Wed, 30 Nov 2016 19:58:44 GMT  (130kb)", "http://arxiv.org/abs/1612.00671v1", null]], "reviews": [], "SUBJECTS": "cs.NE cs.LG", "authors": ["siddharth dinesh", "tirtharaj dash"], "accepted": false, "id": "1612.00671"}, "pdf": {"name": "1612.00671.pdf", "metadata": {"source": "CRF", "title": "Reliable Evaluation of Neural Network for Multiclass Classification of Real-world Data", "authors": ["Siddharth Dinesh", "Tirtharaj Dash"], "emails": ["f2012519@goa.bits-pilani.ac.in", "tirtharaj@goa.bits-pilani.ac.in"], "sections": [{"heading": null, "text": "This year, it is closer than ever before in the history of the country."}, {"heading": "II. NN AND ITS TRAINING WITH GRADIENT DESCENT", "text": "It is a biologically inspired mathematical model that is used to approximate functions that depend on a number of input factors known as \"features.\" NN's computer-assisted processing closely follows information processing within the human brain, which has a complex network of neurons. The motivation behind the evaluation of NN in this work is that they have a high degree of adaptability that includes a better learning algorithm and its rigorous applications in many different real-life environments. In addition, there is a good degree of flexibility to set a learning algorithm with different parameters to improve the performance of NN. Generally, NN is a layered architecture in which neurons (nodes) are arranged in layers. In this work, multi-layered ed1Also known as artificial neural classes (ANN) neutral algorithms are used, which specifically implements a multi-layer perceptron (MLP) layer, in which the putty layer is made up of an inserted layer."}, {"heading": "III. DATASETS", "text": "To assess the performance of multi-class classification problems with NN, seven different comparative data sets from the real world were used in this paper. All data sets come from the UCI Machine Learning Repository [7]. However, we briefly explain all data sets in terms of their dimension. Further details on each data set can be found in [7]. Abalone Data Set: The aim of using abalone data sets is to predict the age of the abalone by the number of rings on the shell, specifying various descriptive attributes of the abalone. There are 4177 data sets, each with 8 entry marks and a class label.Breast Cancer Data Set: This data set is one of the most popular medical benchmarks in ML research. Patient records were obtained from the University of Wisconsin Hospital, Madison. There are a total of 699 data sets with 10 entry marks (one is an ID that is not used for compilation).E-coli data sets were used."}, {"heading": "IV. PERFORMANCE EVALUATION", "text": "All simulations of this work are performed in MATLAB R2015b with a PC system with Windows 10 operating system, a quad-core processor with the same clock speed of 1.70 GHz and a memory of 4 GB."}, {"heading": "A. Preparation of data for simulation", "text": "The real data from the UCI ML repository are unevenly distributed and therefore cannot be used directly during training and testing of the NN. Therefore, the input characteristics were first normalized in the range [0,1], then the normalized data set in training and an independent test set in the ratio 70: 30. The training and testing process was repeated for 10 independent runs (simulations) to determine the average performance of the NN and its deviation from the mean."}, {"heading": "B. Performance measures", "text": "The performance parameters evaluated for the multi-class classification are described as follows: For a class ci = \u03b2\u03b2i (SE), the performance of the classifier can be evaluated with tpi, fni, tni and fpi and can be calculated from the number of test instances belonging to ci. However, the quality of the overall error performance of the classification can be evaluated in two different ways (SEC and macro averaging).The macro averaging treats all classes equally, while the micro averaging classes with more data instances are favored.The calculation of various performance measures suitable for the evaluation of the NN for the multi-class problem can be done as follows: The generalization of the parameters shown in Table I is shown for many classes ci [5].For a class ci, fni, tni and fpi counts accordingly. Micro and macro averages indices are represented by \u00b5 and M accordingly."}, {"heading": "C. Results", "text": "The number of neurons in the hidden layer (nhidden) is one of the most important architectural parameters that directly influences the performance of NN during training and the collection of data to create a knowledge base. However, setting this parameter was an unsolved problem in ML research beforehand [3]. In this thesis, nhidden was set to 60, 80 and 100 and the results were recorded for each of the datasets. All the results obtained were summarized and presented as tables for different nhidden values. Table II shows results for nhidden = 60. Table III and Table IV also present results for nhidden = 80 and nhidden = 100, respectively. It should be noted that all the results presented in these three tables are averaged over ten (10) independent simulations for each of the datasets."}, {"heading": "D. Discussion", "text": "The discussion of the obtained results (as shown in Table II, Table III and Table IV) in this paper is based primarily on different performance parameters, rather than how the values are obtained. This paper summarizes various performance parameters that could be used to evaluate a classifier. Therefore, diversified datasets are used to evaluate the NN for the evaluation of the real world. However, it was found that the commonly known \"Accuracy Data\" parameters could not be considered a reliable parameter for the correct evaluation of a classifier. NN training performance is approximately the same for the abalone datasets with different hidden settings such as nhidden = 60, nhidden = 80 and nhidden = 100. Furthermore, the standard deviation in the MSEtrain is very low for all three cases. It is obvious that the number of hidden neurons increases the architectural complexity of the NN and the time of training."}, {"heading": "V. CONCLUSION", "text": "In this paper, a detailed evaluation of the NN classifier for classifying data from multiple classes was conducted. It has been shown that using predictive accuracy as a single parameter for evaluating an NN would not be wise given the high skew of the test data. Results obtained for different types of data sets clearly show that, although the accuracy is very high, there may be a fair chance that the positive or negative predictor rate falls far within any reliable range. In this paper, this type of property was observed in the performance of NN for the majority of the data sets tested, such as the Abalone dataset, Ecoli dataset, Glass dataset. Therefore, it would be prudent to use many different parameters for such classification problems in order to accurately evaluate a classifier."}], "references": [{"title": "Machine learning: Trends, perspectives, and prospects", "author": ["M.I. Jordan", "T.M. Mitchell"], "venue": "Science, vol. 349, no. 6245, pp. 255260, Jul. 2015.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2015}, {"title": "Machine learning applications in genetics and genomics", "author": ["M.W. Libbrecht", "W.S. Noble"], "venue": "Nature Reviews Genetics, vol. 16, no. 6, pp. 321332, May 2015.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2015}, {"title": "A study on intrusion detection using neural networks trained with evolutionary algorithms", "author": ["T. Dash"], "venue": "Soft Computing, Dec. 2015.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2015}, {"title": "Automatic navigation of wall following mobile robot using Adaptive resonance theory of type-1", "author": ["T. Dash"], "venue": "Biologically Inspired Cognitive Architectures, vol. 12, pp. 18, Apr. 2015.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2015}, {"title": "A systematic analysis of performance measures for classification tasks", "author": ["M. Sokolova", "G. Lapalme"], "venue": "Information Processing & Management, vol. 45, no. 4, pp. 427437, Jul. 2009.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2009}, {"title": "Theory of the backpropagation neural network", "author": ["R. Hecht-Nielsen"], "venue": "Neural Networks, vol. 1, p. 445, Jan. 1988.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1988}, {"title": "Hybrid gravitational search and particle swarm based fuzzy MLP for medical data classification", "author": ["T. Dash", "S.K. Nayak", "H.S. Behera"], "venue": "Computational Intelligence in Data Mining - Volume 1. Springer Science + Business Media, 2014, pp. 3543.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 0, "context": "ML techniques have been employed in many different real world problems such as fraud detection, intrusion detection, web search engines, e-mail spam filtering, sentiment analysis, credit scoring, equipment failures prediction, pattern and image recognition, genetics and genomics, robotics (see [1], [2], [3], [4]).", "startOffset": 295, "endOffset": 298}, {"referenceID": 1, "context": "ML techniques have been employed in many different real world problems such as fraud detection, intrusion detection, web search engines, e-mail spam filtering, sentiment analysis, credit scoring, equipment failures prediction, pattern and image recognition, genetics and genomics, robotics (see [1], [2], [3], [4]).", "startOffset": 300, "endOffset": 303}, {"referenceID": 2, "context": "ML techniques have been employed in many different real world problems such as fraud detection, intrusion detection, web search engines, e-mail spam filtering, sentiment analysis, credit scoring, equipment failures prediction, pattern and image recognition, genetics and genomics, robotics (see [1], [2], [3], [4]).", "startOffset": 305, "endOffset": 308}, {"referenceID": 3, "context": "ML techniques have been employed in many different real world problems such as fraud detection, intrusion detection, web search engines, e-mail spam filtering, sentiment analysis, credit scoring, equipment failures prediction, pattern and image recognition, genetics and genomics, robotics (see [1], [2], [3], [4]).", "startOffset": 310, "endOffset": 313}, {"referenceID": 4, "context": "For more information on these parameters, one can see the published work of Sokolova and Lapalme [5].", "startOffset": 97, "endOffset": 100}, {"referenceID": 4, "context": "We estimate various performance measures which could be used for multiclass classification problems based on the information provided in [5].", "startOffset": 137, "endOffset": 140}, {"referenceID": 5, "context": "The back-propagation based training algorithm [6] of the NN has been presently briefly in Algorithm 1 followed by a set of governing equations.", "startOffset": 46, "endOffset": 49}, {"referenceID": 0, "context": "Therefore, the input features were initially normalized in the range [0,1].", "startOffset": 69, "endOffset": 74}, {"referenceID": 4, "context": "Computation of various performance measures suitable for evaluating NN for multiclass classification problem can be obtained as follows which is a generalization of the parameters presented in Table I for many classes ci [5].", "startOffset": 221, "endOffset": 224}, {"referenceID": 4, "context": "Other crucial measures could be obtained from Equation (15) through to Equation (22) [5].", "startOffset": 85, "endOffset": 88}, {"referenceID": 2, "context": "However, the setting of this parameter apriori has been an unsolved problem in ML research [3].", "startOffset": 91, "endOffset": 94}, {"referenceID": 6, "context": "Evaluation of the NN with regard to ILPD dataset is quite good as compared to recent literature (see [8]) considering the fact that the accuracy and other parameters still follow a particular limit of deviation unlike the results obtained for Abalone, E-coli and Glass datasets.", "startOffset": 101, "endOffset": 104}], "year": 2016, "abstractText": "This paper presents a systematic evaluation of Neural Network (NN) for classification of real-world data. In the field of machine learning, it is often seen that a single parameter that is \u2018predictive accuracy\u2019 is being used for evaluating the performance of a classifier model. However, this parameter might not be considered reliable given a dataset with very high level of skewness. To demonstrate such behavior, seven different types of datasets have been used to evaluate a Multilayer Perceptron (MLP) using twelve(12) different parameters which include microand macro-level estimation. In the present study, the most common problem of prediction called \u2018multiclass\u2019 classification has been considered. The results that are obtained for different parameters for each of the dataset could demonstrate interesting findings to support the usability of these set of performance evaluation parameters.", "creator": "LaTeX with hyperref package"}}}