{"id": "1612.00694", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Dec-2016", "title": "ESE: Efficient Speech Recognition Engine with Sparse LSTM on FPGA", "abstract": "Long Short-Term Memory (LSTM) is widely used in speech recognition. In order to achieve higher prediction accuracy, machine learning scientists have built larger and larger models. Such large model is both computation intensive and memory intensive. Deploying such bulky model results in high power consumption and leads to high total cost of ownership (TCO) of a data center. In order to speedup the prediction and make it energy efficient, we first propose a load-balance-aware pruning method that can compress the LSTM model size by 20x (10x from pruning and 2x from quantization) with negligible loss of the prediction accuracy. The pruned model is friendly for parallel processing. Next, we propose scheduler that encodes and partitions the compressed model to each PE for parallelism, and schedule the complicated LSTM data flow. Finally, we design the hardware architecture, named Efficient Speech Recognition Engine (ESE) that works directly on the compressed model. Implemented on Xilinx XCKU060 FPGA running at 200MHz, ESE has a performance of 282 GOPS working directly on the compressed LSTM network, corresponding to 2.52 TOPS on the uncompressed one, and processes a full LSTM for speech recognition with a power dissipation of 41 Watts. Evaluated on the LSTM for speech recognition benchmark, ESE is 43x and 3x faster than Core i7 5930k CPU and Pascal Titan X GPU implementations. It achieves 40x and 11.5x higher energy efficiency compared with the CPU and GPU respectively.", "histories": [["v1", "Thu, 1 Dec 2016 13:16:00 GMT  (1424kb,D)", "http://arxiv.org/abs/1612.00694v1", "1st International Workshop on Efficient Methods for Deep Neural Networks at NIPS 2016, Barcelona, Spain. Full paper to appear at FPGA 2017"], ["v2", "Mon, 20 Feb 2017 06:28:58 GMT  (4527kb,D)", "http://arxiv.org/abs/1612.00694v2", "Accepted as full paper in FPGA'17, Monterey, CA; Also appeared at 1st International Workshop on Efficient Methods for Deep Neural Networks at NIPS 2016, Barcelona, Spain"]], "COMMENTS": "1st International Workshop on Efficient Methods for Deep Neural Networks at NIPS 2016, Barcelona, Spain. Full paper to appear at FPGA 2017", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["song han", "junlong kang", "huizi mao", "yiming hu", "xin li", "yubin li", "dongliang xie", "hong luo", "song yao", "yu wang", "huazhong yang", "william j dally"], "accepted": false, "id": "1612.00694"}, "pdf": {"name": "1612.00694.pdf", "metadata": {"source": "CRF", "title": "ESE: Efficient Speech Recognition Engine with Compressed LSTM on FPGA", "authors": ["Song Han", "Junlong Kang", "Huizi Mao", "Yiming Hu", "Xin Li", "Yubin Li", "Dongliang Xie", "Hong Luo", "Song Yao", "Yu Wang", "Huazhong Yang", "William J. Dally", "DeePhi Tech"], "emails": ["songhan@stanford.edu,", "dally@stanford.edu,", "song.yao@deephi.tech,", "yu-wang@mail.tsinghua.edu.cn"], "sections": [{"heading": "1 Introduction", "text": "eSi rf\u00fc ide rf\u00fc ide rf\u00fc eeirFngne\u00fceerlcnlhsrtee\u00fccnlhsrtee\u00fccnh rf\u00fc eid rf\u00fc eid rf\u00fc eid eid rf\u00fc eid-eaJng0e0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0"}, {"heading": "3 Computation Scheduling", "text": "Compressed LSTM models are highly irregular, and therefore accelerators on dense LSTMs cannot effectively take advantage of the benefits of economy [8, 9]. LSTM is a complicated data flow, we want to have more parallelism and at the same time fulfill the data dependency, but previous SPMV accelerators [10, 11, 12] or economical DNN accelerators [5] cannot achieve such planning. We propose an ESE scheduler, shown in Fig.4, in which calculation and data retrieval completely overlap. Operations in the first three lines fetch the pointers, weights and distortions from memory to prepare for computing. Operations in the fourth line are matrix vector multiplications. And operations in the fifth line are elementary multiplications (indigo blocks) or accumulations (orange blocks)."}, {"heading": "4 Hardware Architecture", "text": "Fig.5 (a) shows the overview architecture of the ESE system. It is a CPU + FPGA heterogeneous architecture for accelerating LSTM networks. Fig.5 (b) shows the architecture of a channel with multiple PEs. It consists of several main components: Activation vector queue (ActQueue). ActQueue consists of several FIFOs. Each FIFO stores elements of the input voice vector aj for each PE. ActQueue is shared by all PEs in one channel, while each FIFO belongs to each PE independently. ActQueue's Fuction is the decoupling of the load imbalance between different PES. Sparse Matrix Read (SpmatRead) and Sparse Matrix Read (Spmatector Read)."}, {"heading": "5 Experimental Results", "text": "We evaluated the acceleration and energy efficiency of ESE and GPU and compared them with dense LSTM. Our baseline LSTM runs on sparse LSTM and Pascal Titanium GPU. Experimental results of LSTM on ESE, CPU and GPU are shown in Table 1. The model is 10% without zeros. There are less than 12.2% without zeros."}, {"heading": "6 Conclusion", "text": "We introduce the Efficient Speech Recognition Engine (ESE), which works directly on the compressed LSTM model. ESE is optimized across the boundary between algorithm, software, and hardware: We first propose a method to compress the LSTM model 12 times without sacrificing predictive accuracy, which significantly saves the memory bandwidth of the FPGA implementation; then we design a scheduler that can map the complex LSTM operations on the FPGA and achieve parallelism; finally, we propose a hardware architecture that efficiently handles the irregularity caused by compression; working directly on the compressed model allows ESE to achieve 282 GOPS (equivalent to 2.52 TOPS for dense LSTM) on the Xilinx XCKU060 FPGA board. ESE outperforms the Core i7 CPU and Pascal Titan X GPU by 43 times, and 3 times faster, and 1 and 1 times faster than the Core i7 CPU, respectively."}, {"heading": "Acknowledgment", "text": "We would like to thank Wei Chen, Zhongliang Liu, Guanzhe Huang, Yong Liu, Yanfeng Wang, Xiaochuan Wang and other researchers from Sogou for their suggestions and providing real voice data for testing compression performance."}], "references": [{"title": "Deep speech: Scaling up end-to-end speech recognition", "author": ["Awni Hannun", "Carl Case", "Jared Casper", "Bryan Catanzaro", "Greg Diamos", "Erich Elsen", "Ryan Prenger", "Sanjeev Satheesh", "Shubho Sengupta", "Adam Coates", "Andrew Ng"], "venue": "arXiv, preprint arXiv:1412.5567,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2014}, {"title": "Deep speech 2: End-to-end speech recognition in english and mandarin", "author": ["Dario Amodei"], "venue": "arXiv, preprint arXiv:1512.02595,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2015}, {"title": "Long short-term memory recurrent neural network architectures for large scale acoustic modeling", "author": ["Hasim Sak"], "venue": "In INTERSPEECH,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2014}, {"title": "On the properties of neural machine translation: Encoder-decoder approaches", "author": ["Kyunghyun Cho", "Bart Van Merri\u00ebnboer", "Dzmitry Bahdanau", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1409.1259,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2014}, {"title": "Eie: efficient inference engine on compressed deep neural network", "author": ["Song Han", "Xingyu Liu", "Huizi Mao", "Jing Pu", "Ardavan Pedram", "Mark A Horowitz", "William J Dally"], "venue": "arXiv preprint arXiv:1602.01528,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2016}, {"title": "Learning both weights and connections for efficient neural networks", "author": ["Song Han", "Jeff Pool", "John Tran", "William J Dally"], "venue": "In Proceedings of Advances in Neural Information Processing Systems,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2015}, {"title": "Deep Compression: Compressing deep neural networks with pruning, trained quantization and huffman coding", "author": ["Song Han", "Huizi Mao", "William J Dally"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2016}, {"title": "Accelerating recurrent neural networks in analytics server: Comparison of fpga, cpu, gpu, and asic", "author": ["Eriko Nurvitadhi", "Jaewoong Sim", "David Sheffield", "Asit Mishra", "Srivatsan Krishnan", "Debbie Marr"], "venue": "In Field Programmable Logic (FPL),", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2016}, {"title": "Recurrent neural networks hardware implementation on", "author": ["Andre Xian Ming Chang", "Berin Martini", "Eugenio Culurciello"], "venue": "FPGA. CoRR,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2015}, {"title": "Sparse matrix-vector multiplication on fpgas", "author": ["Ling Zhuo", "Viktor K. Prasanna"], "venue": "In FPGA,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2005}, {"title": "A high memory bandwidth fpga accelerator for sparse matrixvector multiplication", "author": ["J. Fowers", "K. Ovtcharov", "K. Strauss"], "venue": "In FCCM,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2014}, {"title": "A scalable sparse matrix-vector multiplication kernel for energy-efficient sparse-blas on FPGAs", "author": ["Richard Dorrance", "Fengbo Ren"], "venue": "In FPGA,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2014}], "referenceMentions": [{"referenceID": 0, "context": "1 Introduction Deep neural network has surpassed the traditional acoustic model and become the state-of-the-art method for speech recognition [1, 2].", "startOffset": 142, "endOffset": 148}, {"referenceID": 1, "context": "1 Introduction Deep neural network has surpassed the traditional acoustic model and become the state-of-the-art method for speech recognition [1, 2].", "startOffset": 142, "endOffset": 148}, {"referenceID": 2, "context": "Long Short-Term Memory (LSTM) [3], Gated Recurrent Unit (GRU) [4] and vanilla recurrent neural networks (RNNs) are popular in speech recognition.", "startOffset": 30, "endOffset": 33}, {"referenceID": 3, "context": "Long Short-Term Memory (LSTM) [3], Gated Recurrent Unit (GRU) [4] and vanilla recurrent neural networks (RNNs) are popular in speech recognition.", "startOffset": 62, "endOffset": 65}, {"referenceID": 4, "context": "ESE takes the approach of EIE [5] one step further to address a more general problem of accelerating not only feed forward neural networks but also recurrent neural networks and LSTM.", "startOffset": 30, "endOffset": 33}, {"referenceID": 5, "context": "Figure 2: Load Balance Aware Pruning and its Benefit for Parallel Processing Previous pruning methods removed the redundant connections based on the absolute value of the weights [6, 7], which lead to a potential problem of unbalanced non-zero weights distribution.", "startOffset": 179, "endOffset": 185}, {"referenceID": 6, "context": "Figure 2: Load Balance Aware Pruning and its Benefit for Parallel Processing Previous pruning methods removed the redundant connections based on the absolute value of the weights [6, 7], which lead to a potential problem of unbalanced non-zero weights distribution.", "startOffset": 179, "endOffset": 185}, {"referenceID": 7, "context": "Compressed LSTM model is highly irregular, and thus accelerators on dense LSTMs cannot effectively take advantage of sparsity [8, 9].", "startOffset": 126, "endOffset": 132}, {"referenceID": 8, "context": "Compressed LSTM model is highly irregular, and thus accelerators on dense LSTMs cannot effectively take advantage of sparsity [8, 9].", "startOffset": 126, "endOffset": 132}, {"referenceID": 9, "context": "LSTM is a complicated dataflow, we want to have more parallelism and meet the data dependency at the same time, but previous spMV accelerator [10, 11, 12] or sparse DNN accelerator [5] cannot achieve such scheduling.", "startOffset": 142, "endOffset": 154}, {"referenceID": 10, "context": "LSTM is a complicated dataflow, we want to have more parallelism and meet the data dependency at the same time, but previous spMV accelerator [10, 11, 12] or sparse DNN accelerator [5] cannot achieve such scheduling.", "startOffset": 142, "endOffset": 154}, {"referenceID": 11, "context": "LSTM is a complicated dataflow, we want to have more parallelism and meet the data dependency at the same time, but previous spMV accelerator [10, 11, 12] or sparse DNN accelerator [5] cannot achieve such scheduling.", "startOffset": 142, "endOffset": 154}, {"referenceID": 4, "context": "LSTM is a complicated dataflow, we want to have more parallelism and meet the data dependency at the same time, but previous spMV accelerator [10, 11, 12] or sparse DNN accelerator [5] cannot achieve such scheduling.", "startOffset": 181, "endOffset": 184}], "year": 2016, "abstractText": "Long Short-Term Memory (LSTM) is widely used in speech recognition. In order to achieve higher prediction accuracy, machine learning scientists have built larger and larger models. Such large model is both computation intensive and memory intensive. Deploying such bulky model results in high power consumption given latency constraint and leads to high total cost of ownership (TCO) of a data center. In order to speedup the prediction and make it energy efficient, we first propose a load-balance-aware pruning method that can compress the LSTM model size by 20\u00d7 (10\u00d7 from pruning and 2\u00d7 from quantization) with negligible loss of the prediction accuracy. The pruned model is friendly for parallel processing. Next, we propose scheduler that encodes and partitions the compressed model to each PE for parallelism, and schedule the complicated LSTM data flow. Finally, we design the hardware architecture, named Efficient Speech Recognition Engine (ESE) that works directly on the compressed model. Implemented on Xilinx XCKU060 FPGA running at 200MHz, ESE has a performance of 282 GOPS working directly on the compressed LSTM network, corresponding to 2.52 TOPS on the uncompressed one, and processes a full LSTM for speech recognition with a power dissipation of 41 Watts. Evaluated on the LSTM for speech recognition benchmark, ESE is 43\u00d7 and 3\u00d7 faster than Core i7 5930k CPU and Pascal Titan X GPU implementations. It achieves 40\u00d7 and 11.5\u00d7 higher energy efficiency compared with the CPU and GPU respectively.", "creator": "LaTeX with hyperref package"}}}