{"id": "1612.00712", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Dec-2016", "title": "Probabilistic Neural Programs", "abstract": "We present probabilistic neural programs, a framework for program induction that permits flexible specification of both a computational model and inference algorithm while simultaneously enabling the use of deep neural networks. Probabilistic neural programs combine a computation graph for specifying a neural network with an operator for weighted nondeterministic choice. Thus, a program describes both a collection of decisions as well as the neural network architecture used to make each one. We evaluate our approach on a challenging diagram question answering task where probabilistic neural programs correctly execute nearly twice as many programs as a baseline model.", "histories": [["v1", "Fri, 2 Dec 2016 15:46:09 GMT  (763kb,D)", "http://arxiv.org/abs/1612.00712v1", "Appears in NAMPI workshop at NIPS 2016"]], "COMMENTS": "Appears in NAMPI workshop at NIPS 2016", "reviews": [], "SUBJECTS": "cs.NE cs.AI cs.LG", "authors": ["kenton w murray", "jayant krishnamurthy"], "accepted": false, "id": "1612.00712"}, "pdf": {"name": "1612.00712.pdf", "metadata": {"source": "CRF", "title": "Probabilistic Neural Programs", "authors": ["Kenton W. Murray", "Jayant Krishnamurthy"], "emails": ["kmurray4@nd.edu", "jayantk@allenai.org", "@NIPS"], "sections": [{"heading": "1 Introduction", "text": "In fact, most of them will be able to move to another world, in which they can move to another world."}, {"heading": "2 Probabilistic Neural Programs", "text": "As a matter of fact, we will be able to fix and correct the errors mentioned above."}, {"heading": "3 Diagram Question Answering with Probabilistic Neural Programs", "text": "We look at the problem of learning to perform program sketches in a food web using visual information from a diagram. This problem is motivated by recent work [8], which has shown that diagram questions can be answered to determine the answers. Diagram (left) is a food web that represents a collection of organisms in an ecosystem with arrows to indicate what each organism eats. Right side of the diagram shows questions regarding the diagram and its associated program sketches. Diagram (left) is a collection of organisms with arrows indicating what each organism eats."}, {"heading": "4 Experiments", "text": "We evaluate probabilistic neural programs based on the FOODWEBS dataset introduced by [8]. This dataset includes a training set of ~ 2,900 programs and a test set of ~ 1,000 programs. These programs are human-commented gold-standard interpretations of the questions in the dataset, which is consistent with the assumption that the translation of questions into programs is perfect. We train our probabilistic neural programs based on the correct execution traces of each program, which are also included in the dataset. We evaluate our models using two metrics. First, execution accuracy measures the percentage of programs in the test set that are executed fully correctly by the model. This measurement is difficult because correct execution of a program requires a number of selection decisions. Our 1,000 test programs had over 35,000 decisions, which means that full execution of a program on average means that 35 decisions in the test set are made correctly without making mistakes. Second, the accuracy of any decision, independently of any previous decisions, is measured correctly."}, {"heading": "5 Conclusion", "text": "We have presented probabilistic neural programs, a program induction framework that enables flexible specification of computational models and inference algorithms while allowing the use of deep learning. A program outline describes a collection of non-deterministic decisions to be made during execution, along with the neural architecture that can be used to evaluate these decisions. A sketch's network parameters can be trained using data from stochastic gradient descent. We show that probable neural programs improve accuracy in answering a diagram question, which can be formulated as learning to execute program sketches in a domain-specific computational model."}, {"heading": "Acknowledgements", "text": "The authors thank the reviewers for their comments and helpful discussions with Arturo Argueta and Oyvind Tafjord."}], "references": [{"title": "Tensorflow: Large-scale machine learning on heterogeneous distributed systems", "author": ["Mart\u00edn Abadi", "Ashish Agarwal", "Paul Barham", "Eugene Brevdo", "Zhifeng Chen", "Craig Citro", "Gregory S. Corrado", "Andy Davis", "Jeffrey Dean", "Matthieu Devin", "Sanjay Ghemawat", "Ian J. Goodfellow", "Andrew Harp", "Geoffrey Irving", "Michael Isard", "Yangqing Jia", "Rafal J\u00f3zefowicz", "Lukasz Kaiser", "Manjunath Kudlur", "Josh Levenberg", "Dan Man\u00e9", "Rajat Monga", "Sherry Moore", "Derek Gordon Murray", "Chris Olah", "Mike Schuster", "Jonathon Shlens", "Benoit Steiner", "Ilya Sutskever", "Kunal Talwar", "Paul A. Tucker", "Vincent Vanhoucke", "Vijay Vasudevan", "Fernanda B. Vi\u00e9gas", "Oriol Vinyals", "Pete Warden", "Martin Wattenberg", "Martin Wicke", "Yuan Yu", "Xiaoqiang Zheng"], "venue": "arXiv preprint arXiv:1603.04467,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2016}, {"title": "Globally normalized transition-based neural networks", "author": ["Daniel Andor", "Chris Alberti", "David Weiss", "Aliaksei Severyn", "Alessandro Presta", "Kuzman Ganchev", "Slav Petrov", "Michael Collins"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2016}, {"title": "Recurrent neural network grammars", "author": ["Chris Dyer", "Adhiguna Kuncoro", "Miguel Ballesteros", "Noah A. Smith"], "venue": "In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2016}, {"title": "Terpret: A probabilistic programming language for program induction", "author": ["Alexander L. Gaunt", "Marc Brockschmidt", "Rishabh Singh", "Nate Kushman", "Pushmeet Kohli", "Jonathan Taylor", "Daniel Tarlow"], "venue": "arXiv preprint arXiv:1608.04428,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2016}, {"title": "Church: a language for generative models", "author": ["Noah D. Goodman", "Vikash K. Mansinghka", "Daniel M. Roy", "Keith Bonawitz", "Joshua B. Tenenbaum"], "venue": "In Proc. of Uncertainty in Artificial Intelligence,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2008}, {"title": "Neural turing machines", "author": ["Alex Graves", "Greg Wayne", "Ivo Danihelka"], "venue": "arXiv preprint arXiv:1410.5401,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2014}, {"title": "Semantic parsing to probabilistic programs for situated question answering", "author": ["Jayant Krishnamurthy", "Oyvind Tafjord", "Aniruddha Kembhavi"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2016}, {"title": "Neural programmer: Inducing latent programs with gradient descent", "author": ["Arvind Neelakantan", "Quoc V. Le", "Ilya Sutskever"], "venue": "CoRR, abs/1511.04834,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2015}, {"title": "Maltparser: A language-independent system for data-driven dependency parsing", "author": ["Joakim Nivre", "Johan Hall", "Jens Nilsson", "Atanas Chanev", "G\u00fclsen Eryigit", "Sandra K\u00fcbler", "Svetoslav Marinov", "Erwin Marsi"], "venue": "Natural Language Engineering,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2007}, {"title": "Programming with a differentiable forth interpreter", "author": ["Sebastian Riedel", "Matko Bo\u0161njak", "Tim Rockt\u00e4schel"], "venue": "arXiv preprint arXiv:1605.06640,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2016}, {"title": "Combinatorial sketching for finite programs", "author": ["Armando Solar-Lezama", "Liviu Tancau", "Rastislav Bodik", "Sanjit Seshia", "Vijay Saraswat"], "venue": "In Proceedings of the 12th International Conference on Architectural Support for Programming Languages and Operating Systems, ASPLOS XII,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2006}], "referenceMentions": [{"referenceID": 10, "context": "First, in the noise-free setting, program synthesis approaches pose program induction as completing a program \u201csketch,\u201d which is a program containing nondeterministic choices (\u201choles\u201d) to be filled by the learning algorithm [13].", "startOffset": 224, "endOffset": 228}, {"referenceID": 4, "context": "Probabilistic programming languages generalize this approach to the noisy setting by permitting the sketch to specify a distribution over these choices as a function of prior parameters and further to condition this distribution on data, thereby training a Bayesian generative model to execute the sketch correctly [6].", "startOffset": 315, "endOffset": 318}, {"referenceID": 7, "context": "Second, neural abstract machines define continuous analogues of Turing machines or other general-purpose computational models by \u201clifting\u201d their discrete state and computation rules into a continuous representation [9, 11, 7, 12].", "startOffset": 215, "endOffset": 229}, {"referenceID": 5, "context": "Second, neural abstract machines define continuous analogues of Turing machines or other general-purpose computational models by \u201clifting\u201d their discrete state and computation rules into a continuous representation [9, 11, 7, 12].", "startOffset": 215, "endOffset": 229}, {"referenceID": 9, "context": "Second, neural abstract machines define continuous analogues of Turing machines or other general-purpose computational models by \u201clifting\u201d their discrete state and computation rules into a continuous representation [9, 11, 7, 12].", "startOffset": 215, "endOffset": 229}, {"referenceID": 3, "context": "These choices are suboptimal as (1) the bias/variance trade-off suggests that training a more expressive computational model will require more data than a less expressive one suited to the task at hand, and (2) recent work has suggested that discrete inference algorithms may outperform continuous approximations [5].", "startOffset": 313, "endOffset": 316}, {"referenceID": 0, "context": "Our approach builds on computation graph frameworks [1, 3] for specifying neural networks by adding an operator for weighted nondeterministic choice that is used to specify the computational model.", "startOffset": 52, "endOffset": 58}, {"referenceID": 1, "context": "This approach accords with the recent trend in structured prediction to combine greedy inference or beam search with powerful non-factoring models [2, 10, 4].", "startOffset": 147, "endOffset": 157}, {"referenceID": 8, "context": "This approach accords with the recent trend in structured prediction to combine greedy inference or beam search with powerful non-factoring models [2, 10, 4].", "startOffset": 147, "endOffset": 157}, {"referenceID": 2, "context": "This approach accords with the recent trend in structured prediction to combine greedy inference or beam search with powerful non-factoring models [2, 10, 4].", "startOffset": 147, "endOffset": 157}, {"referenceID": 6, "context": "This problem is motivated by recent work [8], which has demonstrated that diagram question answering can be formulated as translating natural language questions to program sketches in this model, then learning to execute these sketches.", "startOffset": 41, "endOffset": 44}, {"referenceID": 6, "context": "[8] has a more thorough description of the theory; our goal is to learn to make the choices in this theory.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[8] defines a set of hand-engineered features heuristically created from the outputs of this vision system.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "We evaluate probabilistic neural programs on the FOODWEBS dataset introduced by [8].", "startOffset": 80, "endOffset": 83}], "year": 2016, "abstractText": "We present probabilistic neural programs, a framework for program induction that permits flexible specification of both a computational model and inference algorithm while simultaneously enabling the use of deep neural networks. Probabilistic neural programs combine a computation graph for specifying a neural network with an operator for weighted nondeterministic choice. Thus, a program describes both a collection of decisions as well as the neural network architecture used to make each one. We evaluate our approach on a challenging diagram question answering task where probabilistic neural programs correctly execute nearly twice as many programs as a baseline model.", "creator": "LaTeX with hyperref package"}}}