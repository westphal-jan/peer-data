{"id": "1612.00729", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Dec-2016", "title": "Automated assessment of non-native learner essays: Investigating the role of linguistic features", "abstract": "Automatic essay scoring (AES) refers to the process of scoring free text responses to given prompts, considering human grader scores as the gold standard. Writing such essays is an essential component of many language and aptitude exams. Hence, AES became an active and established area of research, and there are many proprietary systems used in real life applications today. However, not much is known about which specific linguistic features are useful for prediction and how much of this is consistent across datasets. This article addresses that by exploring the role of various linguistic features in automatic essay scoring using two publicly available datasets of non-native English essays written in test taking scenarios. The linguistic properties are modeled by encoding lexical, syntactic, discourse and error types of learner language in the feature set. Predictive models are then developed using these features on both datasets and the most predictive features are compared. While the results show that the feature set used results in good predictive models with both datasets, the question \"what are the most predictive features?\" has a different answer for each dataset.", "histories": [["v1", "Fri, 2 Dec 2016 16:22:49 GMT  (29kb)", "http://arxiv.org/abs/1612.00729v1", "Article accepted for publication at: International Journal of Artificial Intelligence in Education (IJAIED). To appear in early 2017 (journal url:this http URL)"]], "COMMENTS": "Article accepted for publication at: International Journal of Artificial Intelligence in Education (IJAIED). To appear in early 2017 (journal url:this http URL)", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["sowmya vajjala"], "accepted": false, "id": "1612.00729"}, "pdf": {"name": "1612.00729.pdf", "metadata": {"source": "CRF", "title": "Automated assessment of non-native learner essays: Investigating the role of linguistic features", "authors": ["Sowmya Vajjala"], "emails": ["sowmya@iastate.edu"], "sections": [{"heading": null, "text": "ar Xiv: 161 2.00 729v 1 [cs.C L] 2D ecKeywords. Automated Writing Assessment, Essay Scoring, Natural Language Processing, Text Analysis, Linguistic Features, Student modeling1560-4292 / 08 / $17.00 c \u00a9 2016 - Springer Press and the authors. All rights reserved."}, {"heading": "Introduction", "text": "This year, it is so far that it will be able to reege.n"}, {"heading": "Related Work", "text": "In recent years, the number of those who are able to live and learn in the real world has increased significantly. In recent years, the number of those who live in the real world has decreased significantly, both in the real world and in the real world. In the real world, the number of those who live in the real world has increased significantly. In the real world, the number of those who live in the real world has multiplied. In the real world, the number of those who live in the real world is in the real world, in the real world, in the real world, in the real world, in the real world, in the real world, in the real world and in the real world, in the real world and in the real world, in the real world, in the real world and in the real world."}, {"heading": "Methods: Corpora", "text": "The experiments were conducted with two publicly available corporations that have human Scorpio annotations on language competence for essays written by non-native speakers. They are described below:"}, {"heading": "TOEFL11 Corpus", "text": "Our first corpus for the experiments reported in this paper is the TOEFL11 corpus of non-native English (Blanchard et al., 2013), a collection of essays written by TOEFL iBT R \u00a9 test participants in 2006-2007 in response to the independent writing task in the test. Students responded to one of the 8 calls, each of which differed on the topic of the question and not on the task itself. The entire corpus consists of 1,100 essays written by students with an 11 L1 background and belonging to three competences (low, medium, high), which were a result of the broken TOEFL scores, originally on a scale of 1-6. Essays were written in response to eight calls. The first version of this corpus, published during the First Native Language Identification Shared Task (Blanchard et al., 2013), is the precision of the experiments."}, {"heading": "FCE Corpus", "text": "Our second corpus is the First Certificate of English (FCE) corpus, published by Yannakoudakis et al. (2011), which published a Corpus of First Certificate of English exam transcripts, which is a subset of the larger Cambridge Learner Corpus (CLC), which contains texts written by English exam participants as Second or Other Language (ESOL), and the associated results on a scale of 1-40. These texts are typically letters and personal essays written in response to given instructions. As this corpus has a numerical scale, and with a much broader scale than TOEFL11 corpus, which had only three categories, we did not look into the equilibrium of the corpus for all evaluations, as this would have resulted in a very small corpus rendering it useless for machine learning purposes."}, {"heading": "Methods: Features", "text": "There is no publicly available documentation on how exactly the feature selections described in this article assign scores to each essay, other than the general guidelines. Typically, the AES research describes features that can be automatically extracted while encoding various linguistic aspects related to the written language. The feature selections described in this article also go in a similar direction, taking into account what we know from Second Language Acquisition (SLA) about the student's writing. L2 writing competence in SLA research has been discussed in the context of the terms complexity, accuracy and fluctuation (CAF) (Housen and Kuiken, 2009). This paper describes automatically extractable features that can encode these three aspects at different levels of language processing. To ensure complexity, features that examine the lexic richness and syntactical complexity of the language in Stanistik, as well as features from Stanistics analytics, are used."}, {"heading": "Word Level Features", "text": "s vocabulary. While there are many measures of lexical diversity, we grouped some of the measures that are not based on any linguistic representation except words alone into this group of features. This group consists of four variations of the typical ratio (TTR) with the formulas described in Lu (2012) that are TTR (num.types / num.tokens), correct TTR (num.types / \u221a num.tokens), root TTR (num.types / num.tokens) and correct TTR (num.tokens) features (num.tokens) that change most features of the paper form (num.types / ollens)), and Bilog TTR (num.types / num.tokens) with the correct TTR (num.tokens) features (num.tokens) that change most features of the paper form (num.types / ollens), and Bilog TTR for Lognumettypes / num.tokens Measure (Jarievavie.jkens / enum.Measure / httkhi)."}, {"heading": "POS Tag Density", "text": "This group consists of 27 characteristics, which are calculated based on the percentage of different POS tags in the text and some lexical variation measures used in SLA research to assess learning texts Lu (2012). We have taken a subset of the characteristics described in this paper, which are based on relationships between POS tags (noun, verb, adjective, adverb and modification formulas), all of which are based on POS tags, are referred to as POSFEATURES in the rest of the paper and are listed in Table 2, grouped by the motivations for these characteristics. All characteristics of this group, with the exception of the characteristics of word variation, are calculated based on the number of words in the text as denominator. The variation characteristics follow the formulas described in Lu (2012). While the lexical variation characteristics were used to examine their correlations with the spelling competence of L2, they were not used in the context of predicting our knowledge."}, {"heading": "Syntactic Complexity", "text": "This group consists of 28 features extracted from the syntactic parse trees of sentences in learning essays. 14 of these features have been used in the past to measure syntactic complexity in second language writing and its relationship to writing skills (Lu, 2010, 2011). These features were implemented based on the descriptions in Lu (2010) and compared with syntactic parse trees for the extraction of certain patterns using Tregex tree pattern comparison tools (Levy and Andrew, 2006). Although these 14 features have been used in the past for statistical analysis of second language writing, they have not been used in the formation of automated written assessment models. The remaining 14 features estimate the average number and size of different phrase groups (NP, VP, PP, WH phrases) and measurements of parse tree height (average height, number of sub-trees, components, SBARs, etc.) per set, this group is referred to as the rest of the EYES in the table."}, {"heading": "Discourse Properties", "text": "This also does not apply to the fluctuation aspect of the CAF framework in SLA research, which makes it a relevant aspect to consider in L2 writing ability prediction models. However, there is no single method to model relationships and different approaches that have been proposed so far in computational linguistics research. So there are a total of eight word overlap features that are considered in this paper, which overlap at different levels of linguistic analysis based on existing research in natural language processing. The implementation of these overlap features is based on the descriptions in the CohMetrix tool. There are a total of eight word overlaps, word, stem and argument overlaps in local roles (between adjacent sentences) and global roles (between any two sentences in a text) levels. The implementation of these overlap features is based on the descriptions in the Metrix tool (2012)."}, {"heading": "Errors", "text": "Errors are one of the most intuitive characteristics to assess a writer's skill. Some of the existing research on the subject used large language models with unique word and POS frames to model learning errors (e.g. Yannakoudakis et al., 2011).In this paper, a rules-based open source spelling and grammar checker tool called LanguageTool5 was used to calculate spelling and grammar characteristics. For each text, the following four error characteristics were extracted: the average number of spelling errors, non-spelling errors, and all errors per sentence, as well as the percentage of spelling errors for all errors in a text. This group of characteristics is referred to in this paper as ERRORFEATURES. 6"}, {"heading": "Document Length", "text": "The length of the document is one of the characteristics used in all AES systems, and it is known to be a strong predictor of competence. Therefore, we have included it as a feature that encodes it as the number of tokens in the document, which is referred to as DOCLEN for the rest of this work. It can be argued that the length of the document may not be a useful feature in language exams as they usually have a word limit. However, it has been used in all AES research and production systems and is known to correlate strongly with competence, and has therefore been used as one of the characteristics in this thesis. 5https: / / languagetool.org / 6The main reason for choosing the tool is the fact that it is in active development and provides both spelling and grammar checking in one place, eliminating the need to develop these modules for this study."}, {"heading": "Others: Prompt and L1", "text": "Apart from all the above features, prompt (the question for which learners responded with answers) and the learner's native language (L1) are considered additional (categorical) features for both datasets. Linguistic properties of writing impulses were also shown to influence student writing (Crossley et al., 2013a). As there is quick information for both datasets, this was included in the feature set. However, it should be noted that the prompt was used only as a categorical feature, without taking into account any features affecting the specific vocabulary in the learning essays.While L1 was not directly considered as a feature in the L2 essay scoring approaches, the possible influence of L1 on the L2 competence prediction was discussed in previous research (Chodorow and Burstein, 2004). Conversely, Lu and Ai (2015) showed that there are significant differences in the syntactical complexity of English L2."}, {"heading": "Experiments and Results", "text": "The experiments described in this paper fall primarily into two categories: development of predictive models and investigation of the most useful characteristics for predictive models. The general approach to conducting the experiments and interpreting the results is described below:"}, {"heading": "Approach", "text": "While the TOEFLSUBSET has three categories, the FCE dataset has a numerical score in the range of 1-40. Therefore, the following two approaches have been adapted to allow comparison between them: 1. Treat TOEFLSUBSET prediction as classification and FCE dataset as regression according to the way the datasets are designed, and then compare which characteristics are more predictive for both datasets, using a common feature selection approach that works for both classification and regression methods.2. Since both datasets are ordinary in nature, we can also convert TOEFLSUBSET datasets to a numerical scale (low = 1, medium = 2, high = 3) and compare both datasets with a common prediction algorithm that is used for both classification and regression methods.While it is possible to create a faulty version of FCE datasets, both data sets are not considered to be factual in nature."}, {"heading": "Classification with TOEFL11SUBSET", "text": "This year it has come to the point that it will only be a matter of time before it will happen, until it does."}, {"heading": "Regression with TOEFL11SUBSET", "text": "In fact, most of them will be able to orient themselves in a different direction from that in which they have gone."}, {"heading": "With FCE corpus", "text": "In fact, most of them are able to play by the rules that they have shown in recent years, and they are able to play by the rules that they have shown in previous years."}, {"heading": "Conclusions", "text": "This year it is so far that it will only take one year to move on to the next round."}, {"heading": "Future Work", "text": "One missing aspect in the models described here is that the actual use of words and phrases in the corpus has not been modeled at all in the features. While it is possible that the features currently being considered capture such information indirectly by using features based on current word / n-gram frequency measurements, the modelling of prompt specificity should be explicitly pursued in future work. Another useful direction for dealing with this aspect is to consider task-independent features (e.g. Zesch et al., 2015) for modeling the learner language. Modelling, when the essay actually answers the question posed in the prompt, is also an important aspect that must be treated as a categorical feature in the future, making AES a useful tool for evaluating responses to learning."}, {"heading": "Acknowledgements", "text": "I would like to thank all anonymous reviewers and editors of the issue for their useful comments, which have contributed significantly to improving this work from the first version. I would also like to thank Eyal Schejter for providing his code for extracting the features of the correlation chain and entity density."}], "references": [], "referenceMentions": [], "year": 2016, "abstractText": "Automatic essay scoring (AES) refers to the process of scoring free text responses to given prompts, considering human grader scores as the gold standard. Writing such essays is an essential component of many language and aptitude exams. Hence, AES became an active and established area of research, and there are many proprietary systems used in real life applications today. However, not much is known about which specific linguistic features are useful for prediction and how much of this is consistent across datasets. This article addresses that by exploring the role of various linguistic features in automatic essay scoring using two publicly available datasets of non-native English essays written in test taking scenarios. The linguistic properties are modeled by encoding lexical, syntactic, discourse and error types of learner language in the feature set. Predictive models are then developed using these features on both datasets and the most predictive features are compared. While the results show that the feature set used results in good predictive models with both datasets, the question \"what are the most predictive features?\" has a different answer for each dataset.", "creator": "LaTeX with hyperref package"}}}