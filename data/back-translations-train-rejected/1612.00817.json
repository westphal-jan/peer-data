{"id": "1612.00817", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Dec-2016", "title": "Summary - TerpreT: A Probabilistic Programming Language for Program Induction", "abstract": "We study machine learning formulations of inductive program synthesis; that is, given input-output examples, synthesize source code that maps inputs to corresponding outputs. Our key contribution is TerpreT, a domain-specific language for expressing program synthesis problems. A TerpreT model is composed of a specification of a program representation and an interpreter that describes how programs map inputs to outputs. The inference task is to observe a set of input-output examples and infer the underlying program. From a TerpreT model we automatically perform inference using four different back-ends: gradient descent (thus each TerpreT model can be seen as defining a differentiable interpreter), linear program (LP) relaxations for graphical models, discrete satisfiability solving, and the Sketch program synthesis system. TerpreT has two main benefits. First, it enables rapid exploration of a range of domains, program representations, and interpreter models. Second, it separates the model specification from the inference algorithm, allowing proper comparisons between different approaches to inference.", "histories": [["v1", "Fri, 2 Dec 2016 20:08:22 GMT  (6053kb,D)", "http://arxiv.org/abs/1612.00817v1", "7 pages, 2 figures, 4 tables in 1st Workshop on Neural Abstract Machines &amp; Program Induction (NAMPI), @NIPS 2016"]], "COMMENTS": "7 pages, 2 figures, 4 tables in 1st Workshop on Neural Abstract Machines &amp; Program Induction (NAMPI), @NIPS 2016", "reviews": [], "SUBJECTS": "cs.LG cs.AI cs.NE", "authors": ["alexander l gaunt", "marc brockschmidt", "rishabh singh", "nate kushman", "pushmeet kohli", "jonathan taylor", "daniel tarlow"], "accepted": false, "id": "1612.00817"}, "pdf": {"name": "1612.00817.pdf", "metadata": {"source": "CRF", "title": "TERPRET: A Probabilistic Programming Language for Program Induction", "authors": ["Alexander L. Gaunt", "Marc Brockschmidt", "Rishabh Singh", "Nate Kushman", "Pushmeet Kohli", "Jonathan Taylor", "Daniel Tarlow"], "emails": ["dtartlow}@microsoft.com", "jtaylor@perceptiveio.com", "@NIPS"], "sections": [{"heading": "1 Introduction", "text": "In fact, it is a way in which people are able to determine for themselves how they want to behave."}, {"heading": "3 Experimental Results", "text": "In this context, it should be noted that the solution to problems is not a solution to a problem, but a solution to a problem that has arisen in the past."}, {"heading": "4 Conclusion", "text": "We introduced TERPRET, a probabilistic programming language for specifying IPS problems. The flexibility of the TERPRET language in combination with the four inference backends allows a similar comparison between gradient-based and constraint-based techniques for inductive program synthesis. The primary effect of the experiments is that constraint solver outperforms other approaches in all the cases studied. However, our work measures (intentionally) only the ability to search efficiently in the program space. We remain optimistic about extensions of the TERPRET framework that allow differentiated interpreters to solve problems with perception data [Gaunt et al., 2016a], and the use of machine learning techniques to guide search-based techniques [Balog et al., 2016]."}], "references": [{"title": "Learning efficient algorithms with hierarchical attentive memory", "author": ["Marcin Andrychowicz", "Karol Kurach"], "venue": "arXiv preprint arXiv:1602.03218,", "citeRegEx": "Andrychowicz and Kurach.,? \\Q2016\\E", "shortCiteRegEx": "Andrychowicz and Kurach.", "year": 2016}, {"title": "Deepcoder: Learning to write programs", "author": ["Matej Balog", "Alexander L Gaunt", "Marc Brockschmidt", "Sebastian Nowozin", "Daniel Tarlow"], "venue": "arXiv preprint arXiv:1611.01989,", "citeRegEx": "Balog et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Balog et al\\.", "year": 2016}, {"title": "The inference of regular lisp programs from examples", "author": ["Alan W Biermann"], "venue": "IEEE transactions on Systems, Man, and Cybernetics,", "citeRegEx": "Biermann.,? \\Q1978\\E", "shortCiteRegEx": "Biermann.", "year": 1978}, {"title": "Adaptive neural compilation", "author": ["Rudy Bunel", "Alban Desmaison", "Pushmeet Kohli", "Philip H.S. Torr", "M. Pawan Kumar"], "venue": "CoRR, abs/1605.07969,", "citeRegEx": "Bunel et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Bunel et al\\.", "year": 2016}, {"title": "Lifelong perceptual programming by example", "author": ["Alexander L Gaunt", "Marc Brockschmidt", "Nate Kushman", "Daniel Tarlow"], "venue": "arXiv preprint arXiv:1611.02109,", "citeRegEx": "Gaunt et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Gaunt et al\\.", "year": 2016}, {"title": "Terpret: A probabilistic programming language for program induction", "author": ["Alexander L Gaunt", "Marc Brockschmidt", "Rishabh Singh", "Nate Kushman", "Pushmeet Kohli", "Jonathan Taylor", "Daniel Tarlow"], "venue": "arXiv preprint arXiv:1608.04428,", "citeRegEx": "Gaunt et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Gaunt et al\\.", "year": 2016}, {"title": "Higher order recurrent networks and grammatical inference", "author": ["C. Lee Giles", "Guo-Zheng Sun", "Hsing-Hen Chen", "Yee-Chun Lee", "Dong Chen"], "venue": "In Advances in Neural Information Processing Systems 2, [NIPS Conference,", "citeRegEx": "Giles et al\\.,? \\Q1989\\E", "shortCiteRegEx": "Giles et al\\.", "year": 1989}, {"title": "Learning to transduce with unbounded memory", "author": ["Edward Grefenstette", "Karl Moritz Hermann", "Mustafa Suleyman", "Phil Blunsom"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Grefenstette et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Grefenstette et al\\.", "year": 2015}, {"title": "Automating string processing in spreadsheets using input-output examples", "author": ["Sumit Gulwani"], "venue": "In ACM SIGPLAN Notices,", "citeRegEx": "Gulwani.,? \\Q2011\\E", "shortCiteRegEx": "Gulwani.", "year": 2011}, {"title": "Spreadsheet data manipulation using examples", "author": ["Sumit Gulwani", "William Harris", "Rishabh Singh"], "venue": "Communications of the ACM,", "citeRegEx": "Gulwani et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Gulwani et al\\.", "year": 2012}, {"title": "On the naturalness of software", "author": ["Abram Hindle", "Earl T Barr", "Zhendong Su", "Mark Gabel", "Premkumar Devanbu"], "venue": "In 2012 34th International Conference on Software Engineering (ICSE),", "citeRegEx": "Hindle et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hindle et al\\.", "year": 2012}, {"title": "Inferring algorithmic patterns with stack-augmented recurrent nets", "author": ["Armand Joulin", "Tomas Mikolov"], "venue": "In Advances in Neural Information Processing Systems 2, [NIPS Conference,", "citeRegEx": "Joulin and Mikolov.,? \\Q1989\\E", "shortCiteRegEx": "Joulin and Mikolov.", "year": 1989}, {"title": "Neural gpus learn algorithms", "author": ["\u0141ukasz Kaiser", "Ilya Sutskever"], "venue": "In Proceedings of the 4th International Conference on Learning Representations.,", "citeRegEx": "Kaiser and Sutskever.,? \\Q2016\\E", "shortCiteRegEx": "Kaiser and Sutskever.", "year": 2016}, {"title": "Neural random-access machines", "author": ["Karol Kurach", "Marcin Andrychowicz", "Ilya Sutskever"], "venue": "In Proceedings of the 4th International Conference on Learning Representations 2016,", "citeRegEx": "Kurach et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kurach et al\\.", "year": 2015}, {"title": "Llvm: A compilation framework for lifelong program analysis & transformation", "author": ["Chris Lattner", "Vikram Adve"], "venue": "In Code Generation and Optimization,", "citeRegEx": "Lattner and Adve.,? \\Q2004\\E", "shortCiteRegEx": "Lattner and Adve.", "year": 2004}, {"title": "Neural programmer: Inducing latent programs with gradient descent", "author": ["Arvind Neelakantan", "Quoc V. Le", "Ilya Sutskever"], "venue": "In Proceedings of the 4th International Conference on Learning Representations", "citeRegEx": "Neelakantan et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Neelakantan et al\\.", "year": 2016}, {"title": "Neural programmer-interpreters", "author": ["Scott E. Reed", "Nando de Freitas"], "venue": "In Proceedings of the 4th International Conference on Learning Representations", "citeRegEx": "Reed and Freitas.,? \\Q2016\\E", "shortCiteRegEx": "Reed and Freitas.", "year": 2016}, {"title": "Programming with a differentiable forth interpreter", "author": ["Sebastian Riedel", "Matko Bosnjak", "Tim Rockt\u00e4schel"], "venue": "CoRR, abs/1605.06640,", "citeRegEx": "Riedel et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Riedel et al\\.", "year": 2016}, {"title": "Program Synthesis By Sketching", "author": ["Armando Solar-Lezama"], "venue": "PhD thesis, EECS Dept., UC Berkeley,", "citeRegEx": "Solar.Lezama.,? \\Q2008\\E", "shortCiteRegEx": "Solar.Lezama.", "year": 2008}, {"title": "A methodology for lisp program construction from examples", "author": ["Phillip D Summers"], "venue": "Journal of the ACM (JACM),", "citeRegEx": "Summers.,? \\Q1977\\E", "shortCiteRegEx": "Summers.", "year": 1977}, {"title": "URL http://arxiv.org/ abs/1410.3916", "author": ["Jason Weston", "Sumit Chopra", "Antoine Bordes"], "venue": "Memory networks. In Proceedings of the 3rd International Conference on Learning Representations", "citeRegEx": "Weston et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Weston et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 9, "context": "Learning computer programs from input-output examples, or Inductive Program Synthesis (IPS), is a fundamental problem in computer science, dating back at least to Summers [1977] and Biermann [1978].", "startOffset": 163, "endOffset": 178}, {"referenceID": 1, "context": "Learning computer programs from input-output examples, or Inductive Program Synthesis (IPS), is a fundamental problem in computer science, dating back at least to Summers [1977] and Biermann [1978]. The field has produced many successes, with perhaps the most visible example being the FlashFill system in Microsoft Excel [Gulwani, 2011, Gulwani et al.", "startOffset": 182, "endOffset": 198}, {"referenceID": 13, "context": "Technique name Optimizer/Solver Description FMGD (Forward marginals, gradient descent) TensorFlow A gradient descent based approach which generalizes the approach used by Kurach et al. [2015].", "startOffset": 171, "endOffset": 192}, {"referenceID": 18, "context": "SKETCH [Solar-Lezama, 2008] SKETCH Cast the TERPRET model as a partial program (the interpreter) containing holes (the source code) to be inferred from a specification (input-output examples).", "startOffset": 7, "endOffset": 27}, {"referenceID": 10, "context": "In this work we focus on models that represent programs as simple, natural source code [Hindle et al., 2012], i.", "startOffset": 87, "endOffset": 108}, {"referenceID": 14, "context": "To address the first question we develop models inspired by intermediate representations used in compilers like LLVM [Lattner and Adve, 2004] that can be trained by gradient descent.", "startOffset": 117, "endOffset": 141}, {"referenceID": 3, "context": "We note two concurrent works, Adaptive Neural Compilation [Bunel et al., 2016] and Differentiable Forth [Riedel et al.", "startOffset": 58, "endOffset": 78}, {"referenceID": 17, "context": ", 2016] and Differentiable Forth [Riedel et al., 2016], which implement similar models.", "startOffset": 33, "endOffset": 54}, {"referenceID": 4, "context": "For a full grammar of the language and several longer examples, see the long version Gaunt et al. [2016b]. TERPRET obeys Python syntax, and we use the Python ast library to parse and compile TERPRET models.", "startOffset": 85, "endOffset": 106}, {"referenceID": 13, "context": "For the FMGD algorithm we run both a Vanilla form and an Optimized form with additional heuristics such as gradient clipping, gradient noise and an entropy bonus (Kurach et al. [2015]) to aid convergence.", "startOffset": 163, "endOffset": 184}], "year": 2016, "abstractText": "We study machine learning formulations of inductive program synthesis; that is, given input-output examples, synthesize source code that maps inputs to corresponding outputs. Our key contribution is TERPRET, a domain-specific language for expressing program synthesis problems. A TERPRET model is composed of a specification of a program representation and an interpreter that describes how programs map inputs to outputs. The inference task is to observe a set of inputoutput examples and infer the underlying program. From a TERPRET model we automatically perform inference using four different back-ends: gradient descent (thus each TERPRET model can be seen as defining a differentiable interpreter), linear program (LP) relaxations for graphical models, discrete satisfiability solving, and the SKETCH program synthesis system. TERPRET has two main benefits. First, it enables rapid exploration of a range of domains, program representations, and interpreter models. Second, it separates the model specification from the inference algorithm, allowing proper comparisons between different approaches to inference. We illustrate the value of TERPRET by developing several interpreter models and performing an extensive empirical comparison between alternative inference algorithms on a variety of program models. To our knowledge, this is the first work to compare gradient-based search over program space to traditional search-based alternatives. Our key empirical finding is that constraint solvers dominate the gradient descent and LP-based formulations.", "creator": "LaTeX with hyperref package"}}}