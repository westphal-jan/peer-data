{"id": "1612.00827", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Dec-2016", "title": "Learning Operations on a Stack with Neural Turing Machines", "abstract": "Multiple extensions of Recurrent Neural Networks (RNNs) have been proposed recently to address the difficulty of storing information over long time periods. In this paper, we experiment with the capacity of Neural Turing Machines (NTMs) to deal with these long-term dependencies on well-balanced strings of parentheses. We show that not only does the NTM emulate a stack with its heads and learn an algorithm to recognize such words, but it is also capable of strongly generalizing to much longer sequences.", "histories": [["v1", "Fri, 2 Dec 2016 20:31:44 GMT  (94kb)", "http://arxiv.org/abs/1612.00827v1", "1st Workshop on Neural Abstract Machines &amp; Program Induction (NAMPI), NIPS 2016, Barcelona, Spain"]], "COMMENTS": "1st Workshop on Neural Abstract Machines &amp; Program Induction (NAMPI), NIPS 2016, Barcelona, Spain", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["tristan deleu", "joseph dureau"], "accepted": false, "id": "1612.00827"}, "pdf": {"name": "1612.00827.pdf", "metadata": {"source": "CRF", "title": "Learning Operations on a Stack with Neural Turing Machines", "authors": ["Tristan Deleu"], "emails": ["tristan.deleu@snips.ai", "joseph.dureau@snips.ai", "@NIPS"], "sections": [{"heading": null, "text": "ar Xiv: 161 2.00 827v 1 [cs.L G] 2D ec"}, {"heading": "1 Introduction", "text": "Although neural networks excel at finding meaningful representations of data, they are limited in their ability to plan, reason, and store information over long periods of time. Tracking nested brackets in a language model, for example, is a particularly difficult problem for RNNs [9]. To do this, the network must somehow remember the number of incomparably open brackets. In this paper, we analyze the ability of Neural Turing Machines (NTMs) to detect well-balanced bracket strings. We show that the NTM architecture, while not working explicitly on a stack, is able to mimic this data structure with its heads. Such behavior was not observed in other simple algorithmic tasks [4]. After a brief reminder of the architecture of the Neural Turing Machine in Section 3, we show in Section 4 how the NTM is able to learn an algorithm to detect balanced brackets called dyckwords."}, {"heading": "2 Related Work", "text": "In the context of processing natural language, an alternative is to generate data from an artificial language based on a predefined grammar. Historically, these formal languages have been used to evaluate the theoretical foundations of RNNs [14]. Hochreiter and Schmidhuber [7] tested their new Long Short Term Memory (LSTM) on the embedded Reber language to show how their output gates can be beneficial. This behavior was later extended to a variety of context-free and context-sensitive languages [3]. In contrast to this earlier work, which focused on modeling sign languages, our task of interest here is the membership problem. This is a classification problem where positive examples are generated by a given grammar, and negative examples are generated using the same alphabet."}, {"heading": "3 Neural Turing Machines", "text": "The Neural Turing Machine (NTM) [4] is an instance of memory-enlarged neural networks consisting of a neural network controller that interacts with a large (albeit limited) memory band. NTM uses soft read and write heads to retrieve information from memory and store it in memory. Dynamics of these heads are controlled by one or more weight sets for the read head (s) and w w w w t for the write head (s), which are controlled by the controller (either a feed-forward network or an LSTM) and keep the entire architecture distinguishable. The read head returns a read vector rt as a weighted sum over the rows of the memory bank Mt: rt = iw r t (i) Mt (i) (1) (1) and the write heads modify the memory location Mt by first deleting a weighted version of a vector from each row in memory."}, {"heading": "4 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Dyck words", "text": "A Dyck word is a balanced sequence of opening and closing brackets. In addition to the important role they play in parsing, they have multiple connections to other combinatorial objects [15, 2]. In particular, a convenient and visual representation of a Dyck word is a path on the integer line (see Figure 2). To avoid ambiguities, we consider bracket sequences as words w, w, d, o = A. Each character u corresponds to an opening bracket and d to a closing bracket. The subset of A containing the Dyck words of length < 2n is called Dyck language and is denoted D < 2n."}, {"heading": "4.2 Experimental setup", "text": "We trained an NTM for a classification task in which positive examples [2] are consistently sampled from the Dyck language D < 12, and negative examples are non-Dyck words with the same number of letters u and d. We used the same experimental setup as described in [4], with a 1-layer feedback controller with 100 hidden units, 1 read head, 1 write head and a memory bank with 128 memory slots each with dimension 20. We used a ReLU nonlinearity for the key kt and added vector and a hard sigmoid for the deletion vector et. We trained the model with the Adam optimizer [10] with a learning rate of 0.001 and batch size 16."}, {"heading": "4.3 Stack emulation", "text": "The Dyck language is a context-free language that can be recognized by a push-down automaton. [1] Here, we are interested in the nature of the algorithm that the NTM can only derive from examples of this task. Specifically, we want to know if and how the NTM uses its memory to act as a stack without explicitly specifying the push-and-pop operations [8, 5]. In Figure 3, we show the behavior of the read and written heads on a Dyck word and a non-Dyck word, along with the probability that the model returns each prefix to be a Dyck word. We observe that the model actually emulates a stack with its read head. Each time the NTM reads an opening bracket u, the read head is moved up and vice versa when it reads a closing bracket d. This behavior is different from what was previously reported about other algorithmic tasks [4] where the content of memory played a central role."}, {"heading": "4.4 Strong generalization", "text": "When testing a model, it is often assumed that the training and test data come from the same (unknown) distribution, but this is not just called a strong generalization [12]. In Figure 4, we compare the generalization performance of the NTM with an LSTM. This LSTM was selected as the model that yields the best AUC on sequences in D < 200, selecting the number of hidden units in [2, 5, 10, 20, 50, 100, 200, 500] (best: 10). While the LSTM shows signs of strong generalization on sequences that are twice as long as what was given during training, the AUC begins to fall for much longer sequences. On the other hand, the NTM generalizes perfectly even for much longer sequences (up to 20 times longer than the training regime)."}, {"heading": "5 Conclusion", "text": "By experimenting with an artificial language called Dyck language, we have shown that neural Turing machines are not only able to use their memory for storage, but also to use their heads for computational purposes, allowing the NTM to generalize to input for much longer, effectively learning an algorithm (as opposed to just learning patterns in the data), and the size of the memory allocated to the NTM is the only limitation. An interesting line of research could then be to conduct a similar experiment on a model trained under a memory-constrained regime such as a single location, and to see how the NTM can mimic a stack under this greater constraint."}], "references": [{"title": "Context-Free Languages and Push- Down Automata", "author": ["Jean-Michel Autebert", "Jean Berstel", "Luc Boasson"], "venue": "In Handbook of Formal Languages,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1997}, {"title": "Analytic Combinatorics", "author": ["Philippe Flajolet", "Robert Sedgewick"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2009}, {"title": "LSTM recurrent networks learn simple context-free and context-sensitive languages", "author": ["Felix A Gers", "J\u00fcrgen Schmidhuber"], "venue": "Neural Networks, IEEE Transactions on,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2001}, {"title": "Learning to Transduce with Unbounded Memory", "author": ["Edward Grefenstette", "Karl Moritz Hermann", "Mustafa Suleyman", "Phil Blunsom"], "venue": "CoRR, abs/1506.02516,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2015}, {"title": "Dynamic Neural Turing Machine with Soft and Hard Addressing", "author": ["\u00c7aglar G\u00fcl\u00e7ehre", "Sarath Chandar", "Kyunghyun Cho", "Yoshua Bengio"], "venue": "Schemes. CoRR,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2016}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1997}, {"title": "Inferring Algorithmic Patterns with Stack-Augmented", "author": ["Armand Joulin", "Tomas Mikolov"], "venue": "Recurrent Nets. CoRR,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2015}, {"title": "Visualizing and understanding recurrent networks", "author": ["Andrej Karpathy", "Justin Johnson", "Fei-Fei Li"], "venue": "arXiv preprint arXiv:1506.02078,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2015}, {"title": "Adam: A Method for Stochastic Optimization", "author": ["Diederik P. Kingma", "Jimmy Ba"], "venue": "CoRR, abs/1412.6980,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2014}, {"title": "Key-Value Memory Networks for Directly Reading Documents", "author": ["Alexander Miller", "Adam Fisch", "Jesse Dodge", "Amir-Hossein Karimi", "Antoine Bordes", "Jason Weston"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2016}, {"title": "One-shot Learning with Memory-Augmented", "author": ["Adam Santoro", "Sergey Bartunov", "Matthew Botvinick", "Daan Wierstra", "Timothy P. Lillicrap"], "venue": "Neural Networks. CoRR,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2016}, {"title": "Enumerative combinatorics. Volume 2. Cambridge studies in advanced mathematics", "author": ["Richard P. Stanley"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1999}, {"title": "Dynamic Memory Networks for Visual and Textual Question Answering", "author": ["Caiming Xiong", "Stephen Merity", "Richard Socher"], "venue": "CoRR, abs/1603.01417,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2016}], "referenceMentions": [{"referenceID": 7, "context": "Keeping track of nested parentheses in a language model, for example, is a particularly challenging problem for RNNs [9].", "startOffset": 117, "endOffset": 120}, {"referenceID": 5, "context": "Hochreiter and Schmidhuber [7] tested their new Long Short-Term Memory (LSTM) on the embedded Reber language, to show how their output gates can be beneficial.", "startOffset": 27, "endOffset": 30}, {"referenceID": 2, "context": "This behaviour was later extended to a variety of context-free and context-sensitive languages [3].", "startOffset": 95, "endOffset": 98}, {"referenceID": 9, "context": "works [16, 11, 17] use a hierarchical attention mechanism on an associative array to solve text QA tasks involving reasoning.", "startOffset": 6, "endOffset": 18}, {"referenceID": 12, "context": "works [16, 11, 17] use a hierarchical attention mechanism on an associative array to solve text QA tasks involving reasoning.", "startOffset": 6, "endOffset": 18}, {"referenceID": 6, "context": "Closely related our work, Stack-augmented RNNs [8] are capable of inferring algorithmic patterns on some context-free and context-sensitive languages, including ab, abc, and abc.", "startOffset": 47, "endOffset": 50}, {"referenceID": 10, "context": "Even though recent works [13, 6] tend to drop the locationaddressing, we chose to use the original formulation of the NTM and keep both addressing mechanisms.", "startOffset": 25, "endOffset": 32}, {"referenceID": 4, "context": "Even though recent works [13, 6] tend to drop the locationaddressing, we chose to use the original formulation of the NTM and keep both addressing mechanisms.", "startOffset": 25, "endOffset": 32}, {"referenceID": 11, "context": "Besides the important role they play in parsing, they have multiple connections with other combinatorial objects [15, 2].", "startOffset": 113, "endOffset": 120}, {"referenceID": 1, "context": "Besides the important role they play in parsing, they have multiple connections with other combinatorial objects [15, 2].", "startOffset": 113, "endOffset": 120}, {"referenceID": 1, "context": "We trained a NTM for a classification task, where positive examples are uniformly sampled [2] from the Dyck language D<12, and negative examples are non-Dyck words w \u2208 A\u2217 of length < 12 with the same number of characters u and d.", "startOffset": 90, "endOffset": 93}, {"referenceID": 8, "context": "We trained the model using the Adam optimizer [10] with a learning rate of 0.", "startOffset": 46, "endOffset": 50}, {"referenceID": 0, "context": "The Dyck language is a context-free language that can be recognized by a pushdown automaton [1].", "startOffset": 92, "endOffset": 95}, {"referenceID": 6, "context": "More specifically, we want to know if, and how, the NTM uses its memory to act as a stack, without specifying the push and pop operations explicitely [8, 5].", "startOffset": 150, "endOffset": 156}, {"referenceID": 3, "context": "More specifically, we want to know if, and how, the NTM uses its memory to act as a stack, without specifying the push and pop operations explicitely [8, 5].", "startOffset": 150, "endOffset": 156}, {"referenceID": 1, "context": "This LSTM was selected as the model yielding the best AUC on sequences in D<200, with the number of hidden units selected in [2, 5, 10, 20, 50, 100, 200, 500] (best: 10).", "startOffset": 125, "endOffset": 158}, {"referenceID": 3, "context": "This LSTM was selected as the model yielding the best AUC on sequences in D<200, with the number of hidden units selected in [2, 5, 10, 20, 50, 100, 200, 500] (best: 10).", "startOffset": 125, "endOffset": 158}, {"referenceID": 8, "context": "This LSTM was selected as the model yielding the best AUC on sequences in D<200, with the number of hidden units selected in [2, 5, 10, 20, 50, 100, 200, 500] (best: 10).", "startOffset": 125, "endOffset": 158}], "year": 2016, "abstractText": "Multiple extensions of Recurrent Neural Networks (RNNs) have been proposed recently to address the difficulty of storing information over long time periods. In this paper, we experiment with the capacity of Neural Turing Machines (NTMs) to deal with these long-term dependencies on well-balanced strings of parentheses. We show that not only does the NTM emulate a stack with its heads and learn an algorithm to recognize such words, but it is also capable of strongly generalizing to much longer sequences.", "creator": "LaTeX with hyperref package"}}}