{"id": "1612.00835", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Dec-2016", "title": "Scribbler: Controlling Deep Image Synthesis with Sketch and Color", "abstract": "Recently, there have been several promising methods to generate realistic imagery from deep convolutional networks. These methods sidestep the traditional computer graphics rendering pipeline and instead generate imagery at the pixel level by learning from large collections of photos (e.g. faces or bedrooms). However, these methods are of limited utility because it is difficult for a user to control what the network produces. In this paper, we propose a deep adversarial image synthesis architecture that is conditioned on coarse sketches and sparse color strokes to generate realistic cars, bedrooms, or faces. We demonstrate a sketch based image synthesis system which allows users to 'scribble' over the sketch to indicate preferred color for objects. Our network can then generate convincing images that satisfy both the color and the sketch constraints of user. The network is feed-forward which allows users to see the effect of their edits in real time. We compare to recent work on sketch to image synthesis and show that our approach can generate more realistic, more diverse, and more controllable outputs. The architecture is also effective at user-guided colorization of grayscale images.", "histories": [["v1", "Fri, 2 Dec 2016 20:53:01 GMT  (8565kb,D)", "http://arxiv.org/abs/1612.00835v1", "13 pages, 14 figures"], ["v2", "Mon, 5 Dec 2016 20:06:57 GMT  (8565kb,D)", "http://arxiv.org/abs/1612.00835v2", "13 pages, 14 figures"]], "COMMENTS": "13 pages, 14 figures", "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["patsorn sangkloy", "jingwan lu", "chen fang", "fisher yu", "james hays"], "accepted": false, "id": "1612.00835"}, "pdf": {"name": "1612.00835.pdf", "metadata": {"source": "CRF", "title": "Scribbler: Controlling Deep Image Synthesis with Sketch and Color", "authors": ["Patsorn Sangkloy", "Jingwan Lu", "Chen Fang", "Fisher Yu", "James Hays"], "emails": [], "sections": [{"heading": null, "text": "Recently, there have been several promising methods for generating realistic images from deeply entangled networks. These methods bypass the traditional computer graphics rendering pipeline and instead produce pixel-level images by learning from large photo collections (such as faces or bedrooms). However, these methods are of limited use as it is difficult for the user to control what the network produces. In this post, we propose a deep, hostile image synthesis architecture based on rough sketches and sparse strokes of color to create realistic cars, bedrooms or faces. We demonstrate a sketch-based image synthesis system that allows the user to scribble over the sketch to specify the preferred color for objects. Our network can then generate compelling images that meet both the color and sketch limitations of the user. The network is a feed-forward that allows the user to view the effects of their editing in real-time, allowing us to compare and manipulate the sketch with our recent approach."}, {"heading": "1. Introduction", "text": "This year, it is more than ever before in the history of the city."}, {"heading": "2. Related Work", "text": "In fact, most of them are able to survive on their own."}, {"heading": "3. Overview", "text": "In this paper, we examine the extension of generative neural networks to include direct and fine-grained controls. We propose a generic feed-forward network that can be trained end-to-end to transform the user's control signals, such as a hand-drawn sketch and color strokes, directly into a high-resolution photo with realistic textual details.Our proposed network is essentially a profound generative model based on control signals. The network learns to transform the control signal into the pixel domain, learning to realistically fill in missing details and colors. Section 3.1 discusses the network structure shared by all applications presented in the paper. Section 3.2 introduces the objective functions, in particular the combination of loss of content and opposite loss, encouraging the result to photo-realistic behavior, while satisfying the fine-grained control of the user. Section 4 and 5 show how different color elements can be applied to each frame based on the proposed color elements, or to determine the shape of the multiple color elements."}, {"heading": "32 64 128 256 128 64 32", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1. Network Architecture", "text": "When creating an image based on a high-dimensional input in the same domain (i.e. from picture to picture), an encoder decoder type of the network architecture is typically adopted, for example in the form of sketch inversion [14], image coloring [53, 17], and sketch simplification [42]. In a typical network structure, the input is scaled down several times to a lower dimension, then undergoes a sequence of nonlinear transformations, and is finally extrapolated to the desired output size. Recently, he et al. [16] proposed the residual connection, where network blocks can only learn the residual component. Using residual blocks facilitates the training of deeper networks, which improves the ability of neural networks for more complex tasks."}, {"heading": "3.2. Objective Function", "text": "Given the pairs of training images (input, ground-truth), where the input image is derived from the ground-truth photo (synthetically produced sketches and color strokes in our case), the simplest and most common loss is the average difference per pixel L2 difference between the generated image and the ground-truth, which we call Lp. Previous work [14] showed that adding a loss of function to the objective function is also beneficial for image generation tasks. While pixel and function losses are generally used for explicitly correlated synthesized output with input, the use alone is often not enough to generate diverse, realistic images. More important, in our problem setup, conditioning on a potential user problem leaves us with a very rough control."}, {"heading": "4. Sketch-based Photo Synthesis", "text": "In this section, we examine how to apply the proposed feed-forward network to hallucinating content, colors, and textures to reconstruct a photo based on an input sketch of any style. To train such a deep neural network, we require many training sketch-photo pairs. Although high-quality hand-drawn sketches are readily available online, the corresponding photos on which sketches are drawn are not available. Therefore, we use high-quality synthetic algorithms to generate synthetic sketches from photos. To process real hand-drawn sketches at trial times, we apply various data augmentations to the training data to improve the universality of the network. In this essay, we experiment with three image classes - faces [28], cars, and bedrooms [52]. We believe that given similar amounts of training data and time, the proposed framework can be generalized well to other categories."}, {"heading": "4.1. Generation of Training Sketches", "text": "For each image category - face, car or bedroom - we apply the boundary detection filter XDoG [48] to 200k photos to generate the corresponding synthetic sketches. The input (and output) resolution of our network during the training phase is 128x128. To make the network invariably fit the exact positions of the objects, we randomly crop both the input and the ground-level images. For the face and bedroom categories, we first shrink the images to 256x256 before randomly trimming them to 128x128. For the car category, we scale the images to 170x170 before trimming, as most cars already occupy large areas of the screen, and enlarge them too much, this means that we lose the global spatial arrangement and the contexts around the cars. In addition to randomly cropping an image and its corresponding sketch, we also adjust the brightness level of the sketch to randomly get different levels of white sketches from a higher level."}, {"heading": "4.2. Network Generalization", "text": "Genuine hand-drawn sketches feature a wide variety of styles, from abstract pen-and-ink illustrations to elaborate pencil-like drawings with shades. The characteristics of the hand-drawn sketches could be very different from the synthetic sketches we algorithmically generated. Even with the various extensions, random cuts, random brightness adjustments, and random cutouts, the trained network could still match this particular style of sketches. To improve network generality, we extend the training data by adding multiple sketch styles. For the face category, we get 20k additional images, and for each image we randomly select one of the following four algorithms to synthesize a corresponding sketch. See sample sketches in Figure 4. \u2022 StyleNet [11] We use neural network-based transfer algorithms to transfer the texture style of a pencil drawing to the photo."}, {"heading": "4.3. Results and Discussions", "text": "For comparison purposes, we implemented the Sketch Inversion Architecture as described in [14]. We trained both the Sketch Inversion Network and our deeper network with the same training data and parameter settings. Figure 3 shows side by side comparisons of the results generated by Sketch Inversion (second row), our deeper network trained without (third row) and with opposite losses (fourth row) on three different image categories. Compared to Sketch Inversion, our deeper network produces sharper results even without adverse loss on complex bedroom scenes and better results in hallucinating missing details (eyebrow shapes and eyebrows) in the face of simplified sketches with few lines. Compared to Sketch Inversion, our deeper images with sharp edges, higher contrast and more realistic colors and lighting will help the network generate diversified results by producing similar skin colors over and over again."}, {"heading": "5. User-guided Colorization", "text": "The previous section focuses on the use of a grayscale sketch to guide the generation of color photos. Lack of color information in the input means that the problem is underdetermined, as a sketch can match photos that are colored in many different ways. Although the use of losses limits output to an approximately diverse number of natural images and thus limits color selection, it is still up to the generator (and the discriminator) to select a specific color. In this section, we will examine how users can directly control the colors in the output. To do this, we need to change the input to the network to include rough color information during the training (Section 5.1). We examined the addition of color controls in two applications, guided sketch coloring (Section 5.2) and guided image coloring (Section 5.3)."}, {"heading": "5.1. Generation of Training Color Strokes", "text": "One of the most intuitive ways to control the result of the coloring is to \"scribble\" some strokes to show the preferred color in a region.To train a network to detect these control signals at test time, we need to synthesize strokes for the training data. We create synthetic strokes based on the colors in the image with the basic truth. To mimic random user behavior, we blur the image with the basic truth and try a random number of strokes of random length and thickness in random places. We select the basic truth of the pixel color at the starting point of the brush stroke as the brush stroke and continue to grow until the maximum length is reached. If the difference between the current pixel color and the brush stroke color exceeds a certain threshold, we start the brush stroke with a new color that will be scanned at the current pixel. By random parameters of different brush strokes, we are able to synchronize the strokes in the position of the human being."}, {"heading": "5.2. Guided Sketch Colorization", "text": "Our previous objective function remains the same: we want the output to have the same content as the input (pixel and feature loss) and to appear realistic (adversarial loss).Pixel loss is crucial here, because it forces the network to be more precise in color by paying more attention to the color strokes. We modify the training data by placing color strokes over the input sketches. We then train the network as before with a parameter setting that emphasizes the loss of content and mitigates the adversarial loss, so that the results are more consistent with color limitations (Section 7.2).Figure 7 shows the results of the reconstruction of bedroom and auto scenes based on an input sketch and color strokes. Note that the colors of the strokes differ greatly from the colors in the basic image, but the network is still able to shift the input color to the relevant regions, taking into account the object boundaries."}, {"heading": "5.3. Guided Image Colorization", "text": "Recent work [17, 53] deals with the training of deep neural network models for the task of image coloring. However, the selection of colors in the output is entirely the responsibility of the network. In this section, we will examine the use of color lines (Section 5.1) to control the coloring process. We generate training data by extracting a single-channel grayscale image from the ground-level photo and combining it with the three-channel image that contains color strokes.Figure 8 shows different coloring results on an automotive image. In the face of a grayscale image, our system synthesizes realistic-looking cars based on strokes drawn with different colors in random locations. Note that most strokes are placed on the car body and therefore have no effect on the coloring of the other regions. Due to the adverse training, the sky is colored blue and the trees are colored green, regardless of the colors of the foreground object, so the network can always recognize the appropriate content while learning the appropriate regions."}, {"heading": "6. Applications", "text": "We believe that the ability to control the output generated by means of sketches and colors enables many useful applications, especially in the field of art."}, {"heading": "6.1. Interactive Image Generation Tools", "text": "With an input image at 256x256 resolution, it takes about 20ms for our network to turn it into a photo-like result. Real-time performance provides instant visual feedback after incremental edits in image generation and application editing. Sketches and color strokes can be used to assert fine-grained control for various design applications.After seeing the result, an interior designer can quickly sketch rough shapes of the objects, specify colors in different regions, and let our system fill in missing details and textures to generate a plausible bedroom scene.After seeing the result, the designer can interactively change the shapes and colors of the objects and receive instant visual feedback. Figure 7 illustrates the potential design workflow. Similarly, an auto designer can follow a similar workflow to design new cars and try out the looks in different background settings.Our portrait synthesis system provides an eye-shaping tool (see below for more graphic or virtual design)."}, {"heading": "6.2. Sketch and Color Guided Visual Search", "text": "Our image generation tools offer flexible visual search options. With a target scene in mind, you can sketch object boundaries and color constraints that allow our network to reconstruct a plausible arrangement, and then use the reconstructed image in a typical visual search tool to identify high-resolution images with similar content (see Figure 10)."}, {"heading": "7. Network Training Details", "text": "In view of the unpredictability of the opposing training, we find it helpful to divide the training into two phases."}, {"heading": "7.1. Optimizing for Content Loss", "text": "In the first stage, we set the opposing weight wadv from Equation 2 to 0 and let the network focus on minimizing the loss of content, which is a combination of pixel and feature loss. To achieve fine-grained control based on the input sketch, we choose the ReLU2-2 layer of the VGG-19 network [43] to calculate the feature loss, since higher feature representations tend to encourage the network to ignore important details such as the exact positions of the pupils. We set the weights of pixel loss and feature loss wp, wf to 1, and the weight of the TV loss wtv to 1e-5. We train the network for about 3 epochs with a batch size of 32 before moving on to the second stage of the training."}, {"heading": "7.2. Adding Adversarial Loss", "text": "When reconstructing photos from grayscale sketches (section 4), we switch off the pixel loss, maintain the feature loss and add the opposing loss with the following weight setting: wf = 1, wp = 0, wtv = 0, wadv \u2248 1e8. For coloring applications (section 5), we emphasize feature and pixel loss and deemphasize the opposing loss so that the output better follows the color controls: wf = 10, wp = 1, wtv = 0, wadv \u2248 1e5.We train the opposing discriminator alongside our generative network for three epochs at a learning rate between 1e-5 and 1e-6."}, {"heading": "8. Conclusion and Future Work", "text": "In this essay, we propose a profound generative framework that allows two types of user controls to guide the resulting generation, using rough sketches to guide the visual structure at the highest level, and using sparse strokes of color to control object color patterns. Despite the promising results, our current system suffers from several limitations. First, we sometimes observe blurred boundaries between object parts or regions of different colors that reduce the overall realism of the results. In other words, Figure 7 shows the problem of leaky color on the car, where the color of the car's hood fades into the background. Second, our system struggles between strictly following color / sketch controls and minimizing the opposite loss. In other words, if the user specifies a rare color, for example, violet for the car, red for trees, our network will map it to a different color that is considered more realistic by the opposite loss of color / sketch controls. In other words, if the user specifies a rare color, our network will map it to a different color, our network will map it to a different color that will support the scale of similar objects during the training process, as we would expect the scale of similar objects during the training process."}, {"heading": "Acknowledgments", "text": "We would like to thank Yijun Li for her support in producing synthetic training sketches from [11]. This work is supported by a grant from the Royal Thai Government to Patsorn Sangkloy, the NSF CAREER Prize 1149853 to James Hays and the NSF Prize 1561968."}], "references": [{"title": "Patchmatch: a randomized correspondence algorithm for structural image editing", "author": ["C. Barnes", "E. Shechtman", "A. Finkelstein", "D. Goldman"], "venue": "ACM Transactions on Graphics- TOG,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2009}, {"title": "Neural Photo Editing with Introspective Adversarial Networks", "author": ["A. Brock", "T. Lim", "J.M. Ritchie", "N. Weston"], "venue": "ArXiv e-prints,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2016}, {"title": "Semantic style transfer and turning two-bit doodles into fine artworks", "author": ["A.J. Champandard"], "venue": "arXiv preprint arXiv:1603.01768,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2016}, {"title": "Sketch2photo: internet image montage", "author": ["T. Chen", "M.-M. Cheng", "P. Tan", "A. Shamir", "S.-M. Hu"], "venue": "ACM Transactions on Graphics (TOG),", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2009}, {"title": "Generating images with perceptual similarity metrics based on deep networks", "author": ["A. Dosovitskiy", "T. Brox"], "venue": "In NIPS,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2016}, {"title": "Learning to generate chairs with convolutional neural networks", "author": ["A. Dosovitskiy", "J. Tobias Springenberg", "T. Brox"], "venue": "In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2015}, {"title": "Texture synthesis by nonparametric sampling", "author": ["A.A. Efros", "T.K. Leung"], "venue": "In Computer Vision,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1999}, {"title": "How do humans sketch objects", "author": ["M. Eitz", "J. Hays", "M. Alexa"], "venue": "ACM Trans. Graph.,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2012}, {"title": "Image style transfer using convolutional neural networks", "author": ["L.A. Gatys", "A.S. Ecker", "M. Bethge"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2016}, {"title": "Generative adversarial nets", "author": ["I. Goodfellow", "J. Pouget-Abadie", "M. Mirza", "B. Xu", "D. Warde-Farley", "S. Ozair", "A. Courville", "Y. Bengio"], "venue": "In Advances in Neural Information Processing Systems, pages 2672\u20132680,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2014}, {"title": "Draw: A recurrent neural network for image generation", "author": ["K. Gregor", "I. Danihelka", "A. Graves", "D.J. Rezende", "D. Wierstra"], "venue": "In ICML,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2015}, {"title": "Convolutional sketch inversion", "author": ["Y. G\u00fc\u00e7l\u00fct\u00fcrk", "U. G\u00fc\u00e7l\u00fc", "R. van Lier", "M.A. van Gerven"], "venue": "In Proceeding of the ECCV workshop on VISART Where Computer Vision Meets Art,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2016}, {"title": "Scene completion using millions of photographs", "author": ["J. Hays", "A.A. Efros"], "venue": "In ACM SIGGRAPH 2007 Papers, SIGGRAPH", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2007}, {"title": "Deep residual learning for image recognition", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "In IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2016}, {"title": "Let there be Color!: Joint End-to-end Learning of Global and Local Image Priors for Automatic Image Colorization with Simultaneous Classification", "author": ["S. Iizuka", "E. Simo-Serra", "H. Ishikawa"], "venue": "ACM Transactions on Graphics (Proc. of SIGGRAPH 2016),", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2016}, {"title": "Image-to-image translation with conditional adversarial networks. arxiv, 2016", "author": ["P. Isola", "J.-Y. Zhu", "T. Zhou", "A.A. Efros"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2016}, {"title": "Perceptual Losses for Real-Time Style Transfer and Super-Resolution", "author": ["J. Johnson", "A. Alahi", "L. Fei-Fei"], "venue": null, "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2016}, {"title": "Amortised MAP Inference for Image Superresolution", "author": ["C. Kaae S\u00f8nderby", "J. Caballero", "L. Theis", "W. Shi", "F. Husz\u00e1r"], "venue": "ArXiv e-prints,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2016}, {"title": "Auto-encoding variational bayes", "author": ["D.P. Kingma", "M. Welling"], "venue": "In The International Conference on Learning Representations (ICLR),", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2014}, {"title": "Learning representations for automatic colorization", "author": ["G. Larsson", "M. Maire", "G. Shakhnarovich"], "venue": "In European Conference on Computer Vision (ECCV),", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2016}, {"title": "Photo-realistic single image super-resolution using a generative adversarial network", "author": ["C. Ledig", "L. Theis", "F. Husz\u00e1r", "J. Caballero", "A. Aitken", "A. Tejani", "J. Totz", "Z. Wang", "W. Shi"], "venue": "arXiv preprint arXiv:1609.04802,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2016}, {"title": "Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations", "author": ["H. Lee", "R. Grosse", "R. Ranganath", "A.Y. Ng"], "venue": "In Proceedings of the 26th Annual International Conference on Machine Learning,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2009}, {"title": "Colorization using optimization", "author": ["A. Levin", "D. Lischinski", "Y. Weiss"], "venue": "In ACM Transactions on Graphics (TOG),", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2004}, {"title": "Precomputed real-time texture synthesis with markovian generative adversarial networks", "author": ["C. Li", "M. Wand"], "venue": "In European Conference on Computer Vision (ECCV),", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2016}, {"title": "Scribblesup: Scribble-supervised convolutional networks for semantic segmentation", "author": ["D. Lin", "J. Dai", "J. Jia", "K. He", "J. Sun"], "venue": "In IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2016}, {"title": "Deep learning face attributes in the wild", "author": ["Z. Liu", "P. Luo", "X. Wang", "X. Tang"], "venue": "In Proceedings of International Conference on Computer Vision (ICCV),", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2015}, {"title": "Plenoptic modeling: An imagebased rendering system", "author": ["L. McMillan", "G. Bishop"], "venue": "In Proceedings of the 22Nd Annual Conference on Computer Graphics and Interactive Techniques,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 1995}, {"title": "Synthesizing the preferred inputs for neurons in neural networks via deep generator networks", "author": ["A. Nguyen", "A. Dosovitskiy", "J. Yosinski", "T. Brox", "J. Clune"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2016}, {"title": "Plug & play generative networks: Conditional iterative generation of images in latent space", "author": ["A. Nguyen", "J. Yosinski", "Y. Bengio", "A. Dosovitskiy", "J. Clune"], "venue": "In arXiv pre-print", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2016}, {"title": "Deconvolution and checkerboard artifacts. http://distill.pub/2016/deconvcheckerboard/, 2016", "author": ["A. Odena", "V. Dumoulin", "C. Olah"], "venue": null, "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2016}, {"title": "Pixel recurrent neural networks", "author": ["A. v. d. Oord", "N. Kalchbrenner", "K. Kavukcuoglu"], "venue": "In Proceedings of the 33th International Conference on Machine Learning,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2016}, {"title": "Context encoders: Feature learning by inpainting", "author": ["D. Pathak", "P. Kr\u00e4henb\u00fchl", "J. Donahue", "T. Darrell", "A. Efros"], "venue": null, "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2016}, {"title": "Manga colorization", "author": ["Y. Qu", "T.-T. Wong", "P.-A. Heng"], "venue": "ACM Transactions on Graphics (SIGGRAPH", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2006}, {"title": "Unsupervised representation learning with deep convolutional generative adversarial networks", "author": ["A. Radford", "L. Metz", "S. Chintala"], "venue": "arXiv preprint arXiv:1511.06434,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2015}, {"title": "Learning what and where to draw", "author": ["S. Reed", "Z. Akata", "S. Mohan", "S. Tenka", "B. Schiele", "H. Lee"], "venue": "In NIPS,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2016}, {"title": "Generative adversarial text-to-image synthesis", "author": ["S. Reed", "Z. Akata", "X. Yan", "L. Logeswaran", "B. Schiele", "H. Lee"], "venue": "In Proceedings of The 33rd International Conference on Machine Learning,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2016}, {"title": "U-Net: Convolutional Networks for Biomedical Image Segmentation, pages 234\u2013241", "author": ["O. Ronneberger", "P. Fischer", "T. Brox"], "venue": null, "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2015}, {"title": "Deep Boltzmann machines", "author": ["R. Salakhutdinov", "G. Hinton"], "venue": "In Proceedings of the International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2009}, {"title": "The sketchy database: Learning to retrieve badly drawn bunnies", "author": ["P. Sangkloy", "N. Burnell", "C. Ham", "J. Hays"], "venue": "ACM Transactions on Graphics (proceedings of SIG- GRAPH),", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2016}, {"title": "Learning to Simplify: Fully Convolutional Networks for Rough Sketch Cleanup", "author": ["E. Simo-Serra", "S. Iizuka", "K. Sasaki", "H. Ishikawa"], "venue": "ACM Transactions on Graphics (SIG- GRAPH),", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2016}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "In Proceedings of the International Conference on Learning Representations (ICLR),", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2015}, {"title": "Lazybrush: Flexible painting tool for hand-drawn cartoons", "author": ["D. S\u1ef3kora", "J. Dingliana", "S. Collins"], "venue": "In Computer Graphics Forum,", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2009}, {"title": "Conditional image generation with pixelcnn decoders", "author": ["A. van den Oord", "N. Kalchbrenner", "L. Espeholt", "O. Vinyals", "A. Graves"], "venue": "In Advances In Neural Information Processing Systems,", "citeRegEx": "45", "shortCiteRegEx": "45", "year": 2016}, {"title": "Generative image modeling using style and structure adversarial networks", "author": ["X. Wang", "A. Gupta"], "venue": "In European Conference on Computer Vision (ECCV),", "citeRegEx": "46", "shortCiteRegEx": "46", "year": 2016}, {"title": "Face photo-sketch synthesis and recognition", "author": ["X. Wang", "X. Tang"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "47", "shortCiteRegEx": "47", "year": 2009}, {"title": "Xdog: an extended difference-of-gaussians compendium including advanced image stylization", "author": ["H. Winnem\u00f6Ller", "J.E. Kyprianidis", "S.C. Olsen"], "venue": "Computers & Graphics,", "citeRegEx": "48", "shortCiteRegEx": "48", "year": 2012}, {"title": "Attribute2image: Conditional image generation from visual attributes", "author": ["X. Yan", "J. Yang", "K. Sohn", "H. Lee"], "venue": "In Proceedings of European Conference on Computer Vision (ECCV),", "citeRegEx": "49", "shortCiteRegEx": "49", "year": 2016}, {"title": "Semantic image inpainting with perceptual and contextual losses", "author": ["R. Yeh", "C. Chen", "T.Y. Lim", "M. Hasegawa-Johnson", "M.N. Do"], "venue": "arXiv preprint arXiv:1607.07539,", "citeRegEx": "50", "shortCiteRegEx": "50", "year": 2016}, {"title": "Pixellevel domain transfer", "author": ["D. Yoo", "N. Kim", "S. Park", "A.S. Paek", "I.S. Kweon"], "venue": "In European Conference on Computer Vision (ECCV),", "citeRegEx": "51", "shortCiteRegEx": "51", "year": 2016}, {"title": "LSUN: construction of a large-scale image dataset using deep learning with humans", "author": ["F. Yu", "Y. Zhang", "S. Song", "A. Seff", "J. Xiao"], "venue": "in the loop. CoRR,", "citeRegEx": "52", "shortCiteRegEx": "52", "year": 2015}, {"title": "Colorful image colorization", "author": ["R. Zhang", "P. Isola", "A.A. Efros"], "venue": "ECCV, 2016", "citeRegEx": "53", "shortCiteRegEx": "53", "year": 2016}, {"title": "View synthesis by appearance flow", "author": ["T. Zhou", "S. Tulsiani", "W. Sun", "J. Malik", "A.A. Efros"], "venue": "In European Conference on Computer Vision (ECCV),", "citeRegEx": "54", "shortCiteRegEx": "54", "year": 2016}, {"title": "Generative visual manipulation on the natural image manifold", "author": ["J.-Y. Zhu", "P. Kr\u00e4henb\u00fchl", "E. Shechtman", "A.A. Efros"], "venue": "Proceedings of European Conference on Computer Vision (ECCV),", "citeRegEx": "55", "shortCiteRegEx": "55", "year": 2016}], "referenceMentions": [{"referenceID": 37, "context": "Recently, numerous image synthesis methods built on neural networks have emerged [40, 24, 12, 36, 21, 13].", "startOffset": 81, "endOffset": 105}, {"referenceID": 21, "context": "Recently, numerous image synthesis methods built on neural networks have emerged [40, 24, 12, 36, 21, 13].", "startOffset": 81, "endOffset": 105}, {"referenceID": 9, "context": "Recently, numerous image synthesis methods built on neural networks have emerged [40, 24, 12, 36, 21, 13].", "startOffset": 81, "endOffset": 105}, {"referenceID": 33, "context": "Recently, numerous image synthesis methods built on neural networks have emerged [40, 24, 12, 36, 21, 13].", "startOffset": 81, "endOffset": 105}, {"referenceID": 18, "context": "Recently, numerous image synthesis methods built on neural networks have emerged [40, 24, 12, 36, 21, 13].", "startOffset": 81, "endOffset": 105}, {"referenceID": 10, "context": "Recently, numerous image synthesis methods built on neural networks have emerged [40, 24, 12, 36, 21, 13].", "startOffset": 81, "endOffset": 105}, {"referenceID": 5, "context": "How can we enable everyday users (non-artists) to harness the power of deep image synthesis methods and produce realistic imagery? Several recent methods have explored controllable deep synthesis [8, 49, 54, 14, 55, 18, 45] and we focus on two complementary forms of control \u2013 sketches and color strokes.", "startOffset": 196, "endOffset": 223}, {"referenceID": 46, "context": "How can we enable everyday users (non-artists) to harness the power of deep image synthesis methods and produce realistic imagery? Several recent methods have explored controllable deep synthesis [8, 49, 54, 14, 55, 18, 45] and we focus on two complementary forms of control \u2013 sketches and color strokes.", "startOffset": 196, "endOffset": 223}, {"referenceID": 51, "context": "How can we enable everyday users (non-artists) to harness the power of deep image synthesis methods and produce realistic imagery? Several recent methods have explored controllable deep synthesis [8, 49, 54, 14, 55, 18, 45] and we focus on two complementary forms of control \u2013 sketches and color strokes.", "startOffset": 196, "endOffset": 223}, {"referenceID": 11, "context": "How can we enable everyday users (non-artists) to harness the power of deep image synthesis methods and produce realistic imagery? Several recent methods have explored controllable deep synthesis [8, 49, 54, 14, 55, 18, 45] and we focus on two complementary forms of control \u2013 sketches and color strokes.", "startOffset": 196, "endOffset": 223}, {"referenceID": 52, "context": "How can we enable everyday users (non-artists) to harness the power of deep image synthesis methods and produce realistic imagery? Several recent methods have explored controllable deep synthesis [8, 49, 54, 14, 55, 18, 45] and we focus on two complementary forms of control \u2013 sketches and color strokes.", "startOffset": 196, "endOffset": 223}, {"referenceID": 15, "context": "How can we enable everyday users (non-artists) to harness the power of deep image synthesis methods and produce realistic imagery? Several recent methods have explored controllable deep synthesis [8, 49, 54, 14, 55, 18, 45] and we focus on two complementary forms of control \u2013 sketches and color strokes.", "startOffset": 196, "endOffset": 223}, {"referenceID": 42, "context": "How can we enable everyday users (non-artists) to harness the power of deep image synthesis methods and produce realistic imagery? Several recent methods have explored controllable deep synthesis [8, 49, 54, 14, 55, 18, 45] and we focus on two complementary forms of control \u2013 sketches and color strokes.", "startOffset": 196, "endOffset": 223}, {"referenceID": 50, "context": "Color is a compelling form of control because many sketches or grayscale scenes are fundamentally ambiguous with respect to color [53], but it is easy for a user to intervene, e.", "startOffset": 130, "endOffset": 134}, {"referenceID": 11, "context": "Our approach is similar to Sketch Inversion [14], which also generates images from sketches, although we show the benefit of adversarial training, introduce color control signals, demonstrate results on image domains beyond faces, and demonstrate that users can perform simple edits to sketches to control the synthesis.", "startOffset": 44, "endOffset": 48}, {"referenceID": 52, "context": "[55] \u2013 they also demonstrate that GANs can be constrained by sketch and color strokes.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[18] significantly overlaps with our own.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "\u2022 We improve the quality of sketch-to-image synthesis compared to existing work [14].", "startOffset": 80, "endOffset": 84}, {"referenceID": 26, "context": "Previously, the most successful methods tended to be non-parametric approaches which found clever ways to reuse existing image fragments [29, 9, 15, 6, 3].", "startOffset": 137, "endOffset": 154}, {"referenceID": 6, "context": "Previously, the most successful methods tended to be non-parametric approaches which found clever ways to reuse existing image fragments [29, 9, 15, 6, 3].", "startOffset": 137, "endOffset": 154}, {"referenceID": 12, "context": "Previously, the most successful methods tended to be non-parametric approaches which found clever ways to reuse existing image fragments [29, 9, 15, 6, 3].", "startOffset": 137, "endOffset": 154}, {"referenceID": 3, "context": "Previously, the most successful methods tended to be non-parametric approaches which found clever ways to reuse existing image fragments [29, 9, 15, 6, 3].", "startOffset": 137, "endOffset": 154}, {"referenceID": 0, "context": "Previously, the most successful methods tended to be non-parametric approaches which found clever ways to reuse existing image fragments [29, 9, 15, 6, 3].", "startOffset": 137, "endOffset": 154}, {"referenceID": 9, "context": "In the last few years, parametric models built on deep convolutional networks have shown promising results [12, 8, 36, 21, 13].", "startOffset": 107, "endOffset": 126}, {"referenceID": 5, "context": "In the last few years, parametric models built on deep convolutional networks have shown promising results [12, 8, 36, 21, 13].", "startOffset": 107, "endOffset": 126}, {"referenceID": 33, "context": "In the last few years, parametric models built on deep convolutional networks have shown promising results [12, 8, 36, 21, 13].", "startOffset": 107, "endOffset": 126}, {"referenceID": 18, "context": "In the last few years, parametric models built on deep convolutional networks have shown promising results [12, 8, 36, 21, 13].", "startOffset": 107, "endOffset": 126}, {"referenceID": 10, "context": "In the last few years, parametric models built on deep convolutional networks have shown promising results [12, 8, 36, 21, 13].", "startOffset": 107, "endOffset": 126}, {"referenceID": 5, "context": "the ability to hallucinate unseen viewpoints of particular chairs based on the appearance changes of other chairs [8]).", "startOffset": 114, "endOffset": 117}, {"referenceID": 18, "context": "with Variational Autoencoders (VAEs) [21] or Generative Adversarial Networks (GANs) [12].", "startOffset": 37, "endOffset": 41}, {"referenceID": 9, "context": "with Variational Autoencoders (VAEs) [21] or Generative Adversarial Networks (GANs) [12].", "startOffset": 84, "endOffset": 88}, {"referenceID": 42, "context": "In general, deep image synthesis can be conditioned on any input vector [45], such as attributes [49], 3d viewpoint parameters and object identity [8], image and desired viewpoint [54], or grayscale image [53, 17, 22] .", "startOffset": 72, "endOffset": 76}, {"referenceID": 46, "context": "In general, deep image synthesis can be conditioned on any input vector [45], such as attributes [49], 3d viewpoint parameters and object identity [8], image and desired viewpoint [54], or grayscale image [53, 17, 22] .", "startOffset": 97, "endOffset": 101}, {"referenceID": 5, "context": "In general, deep image synthesis can be conditioned on any input vector [45], such as attributes [49], 3d viewpoint parameters and object identity [8], image and desired viewpoint [54], or grayscale image [53, 17, 22] .", "startOffset": 147, "endOffset": 150}, {"referenceID": 51, "context": "In general, deep image synthesis can be conditioned on any input vector [45], such as attributes [49], 3d viewpoint parameters and object identity [8], image and desired viewpoint [54], or grayscale image [53, 17, 22] .", "startOffset": 180, "endOffset": 184}, {"referenceID": 50, "context": "In general, deep image synthesis can be conditioned on any input vector [45], such as attributes [49], 3d viewpoint parameters and object identity [8], image and desired viewpoint [54], or grayscale image [53, 17, 22] .", "startOffset": 205, "endOffset": 217}, {"referenceID": 14, "context": "In general, deep image synthesis can be conditioned on any input vector [45], such as attributes [49], 3d viewpoint parameters and object identity [8], image and desired viewpoint [54], or grayscale image [53, 17, 22] .", "startOffset": 205, "endOffset": 217}, {"referenceID": 19, "context": "In general, deep image synthesis can be conditioned on any input vector [45], such as attributes [49], 3d viewpoint parameters and object identity [8], image and desired viewpoint [54], or grayscale image [53, 17, 22] .", "startOffset": 205, "endOffset": 217}, {"referenceID": 9, "context": "Generative Adversarial Networks (GANs) Among the most promising deep image synthesis techniques are Generative Adversarial Networks (GANs) [12, 36] in which a generative network attempts to fool a simultaneously trained discriminator network that classifies images as real or synthetic.", "startOffset": 139, "endOffset": 147}, {"referenceID": 33, "context": "Generative Adversarial Networks (GANs) Among the most promising deep image synthesis techniques are Generative Adversarial Networks (GANs) [12, 36] in which a generative network attempts to fool a simultaneously trained discriminator network that classifies images as real or synthetic.", "startOffset": 139, "endOffset": 147}, {"referenceID": 1, "context": "for image editing [4, 55] or network visualization [30, 31]), the space itself is not semantically well organized \u2013 the particular dimensions of the latent vector do not correspond to", "startOffset": 18, "endOffset": 25}, {"referenceID": 52, "context": "for image editing [4, 55] or network visualization [30, 31]), the space itself is not semantically well organized \u2013 the particular dimensions of the latent vector do not correspond to", "startOffset": 18, "endOffset": 25}, {"referenceID": 27, "context": "for image editing [4, 55] or network visualization [30, 31]), the space itself is not semantically well organized \u2013 the particular dimensions of the latent vector do not correspond to", "startOffset": 51, "endOffset": 59}, {"referenceID": 28, "context": "for image editing [4, 55] or network visualization [30, 31]), the space itself is not semantically well organized \u2013 the particular dimensions of the latent vector do not correspond to", "startOffset": 51, "endOffset": 59}, {"referenceID": 43, "context": "semantic attributes although mapping them to an intermediate structure image [46] can give us more insight.", "startOffset": 77, "endOffset": 81}, {"referenceID": 35, "context": "Conditional GANs Instead of synthesizing images from latent vectors, several works explore conditional GANs where the generator is conditioned on more meaningful inputs such as text [38, 37], low resolution images (superresolution) [23, 20], or incomplete images (inpainting) [34, 33, 50].", "startOffset": 182, "endOffset": 190}, {"referenceID": 34, "context": "Conditional GANs Instead of synthesizing images from latent vectors, several works explore conditional GANs where the generator is conditioned on more meaningful inputs such as text [38, 37], low resolution images (superresolution) [23, 20], or incomplete images (inpainting) [34, 33, 50].", "startOffset": 182, "endOffset": 190}, {"referenceID": 20, "context": "Conditional GANs Instead of synthesizing images from latent vectors, several works explore conditional GANs where the generator is conditioned on more meaningful inputs such as text [38, 37], low resolution images (superresolution) [23, 20], or incomplete images (inpainting) [34, 33, 50].", "startOffset": 232, "endOffset": 240}, {"referenceID": 17, "context": "Conditional GANs Instead of synthesizing images from latent vectors, several works explore conditional GANs where the generator is conditioned on more meaningful inputs such as text [38, 37], low resolution images (superresolution) [23, 20], or incomplete images (inpainting) [34, 33, 50].", "startOffset": 232, "endOffset": 240}, {"referenceID": 31, "context": "Conditional GANs Instead of synthesizing images from latent vectors, several works explore conditional GANs where the generator is conditioned on more meaningful inputs such as text [38, 37], low resolution images (superresolution) [23, 20], or incomplete images (inpainting) [34, 33, 50].", "startOffset": 276, "endOffset": 288}, {"referenceID": 30, "context": "Conditional GANs Instead of synthesizing images from latent vectors, several works explore conditional GANs where the generator is conditioned on more meaningful inputs such as text [38, 37], low resolution images (superresolution) [23, 20], or incomplete images (inpainting) [34, 33, 50].", "startOffset": 276, "endOffset": 288}, {"referenceID": 47, "context": "Conditional GANs Instead of synthesizing images from latent vectors, several works explore conditional GANs where the generator is conditioned on more meaningful inputs such as text [38, 37], low resolution images (superresolution) [23, 20], or incomplete images (inpainting) [34, 33, 50].", "startOffset": 276, "endOffset": 288}, {"referenceID": 48, "context": "Conditional GANs have also been used to transform images into different domains such as a product images [51] or different artistic styles [26].", "startOffset": 105, "endOffset": 109}, {"referenceID": 23, "context": "Conditional GANs have also been used to transform images into different domains such as a product images [51] or different artistic styles [26].", "startOffset": 139, "endOffset": 143}, {"referenceID": 35, "context": "[38] condition both the generator and discriminator on an embedding of input text.", "startOffset": 0, "endOffset": 4}, {"referenceID": 5, "context": "Examples of control signals include 3d pose of objects [8], natural language [38], semantic attributes [49], semantic segmentation [5], and object keypoints and bounding box [37].", "startOffset": 55, "endOffset": 58}, {"referenceID": 35, "context": "Examples of control signals include 3d pose of objects [8], natural language [38], semantic attributes [49], semantic segmentation [5], and object keypoints and bounding box [37].", "startOffset": 77, "endOffset": 81}, {"referenceID": 46, "context": "Examples of control signals include 3d pose of objects [8], natural language [38], semantic attributes [49], semantic segmentation [5], and object keypoints and bounding box [37].", "startOffset": 103, "endOffset": 107}, {"referenceID": 2, "context": "Examples of control signals include 3d pose of objects [8], natural language [38], semantic attributes [49], semantic segmentation [5], and object keypoints and bounding box [37].", "startOffset": 131, "endOffset": 134}, {"referenceID": 34, "context": "Examples of control signals include 3d pose of objects [8], natural language [38], semantic attributes [49], semantic segmentation [5], and object keypoints and bounding box [37].", "startOffset": 174, "endOffset": 178}, {"referenceID": 8, "context": "[11] could also be considered a mechanism to control deep image synthesis.", "startOffset": 0, "endOffset": 4}, {"referenceID": 52, "context": "[55] which optimizes for an image that is similar to an input sketch (potentially with color strokes) that lies on a learned natural image manifold.", "startOffset": 0, "endOffset": 4}, {"referenceID": 52, "context": "For the \u2018sketch brush\u2019 in [55], they get around this by optimizing for image with the same edges as user sketch that also lies within a natural image manifold as approximated by a pre-trained GAN.", "startOffset": 26, "endOffset": 30}, {"referenceID": 38, "context": "However, image edges are not necessarily a good proxy for human sketched strokes [41] and their method has no capacity to learn the mapping between user inputs and desired outputs.", "startOffset": 81, "endOffset": 85}, {"referenceID": 11, "context": "Sketch Inversion [14] is also closely related to our work although they do not address color control.", "startOffset": 17, "endOffset": 21}, {"referenceID": 22, "context": "Controllable Colorization Our color control strokes are inspired by Colorization Using Optimization [25] which interpolates sparse color strokes such that color changes tend to happen at intensity boundaries.", "startOffset": 100, "endOffset": 104}, {"referenceID": 32, "context": "Similar control strokes have been applied to sketch and manga imagery [35, 44], but the results remain nonphotorealistic and lack lighting and shading.", "startOffset": 70, "endOffset": 78}, {"referenceID": 41, "context": "Similar control strokes have been applied to sketch and manga imagery [35, 44], but the results remain nonphotorealistic and lack lighting and shading.", "startOffset": 70, "endOffset": 78}, {"referenceID": 24, "context": "We are unaware of sparse scribbles being used as input constraints to deep generative networks, although ScribbleSup [27] uses sparse scribbles to supervise the output of semantic segmentation networks.", "startOffset": 117, "endOffset": 121}, {"referenceID": 15, "context": "[18] also uses conditional GANs for sketch to photo and grayscale to color synthesis.", "startOffset": 0, "endOffset": 4}, {"referenceID": 36, "context": "Unlike our approach, they use a \u201cU-Net\u201d architecture [39] which allows later layers of the network to be conditioned on early layers where more spatial information is preserved.", "startOffset": 53, "endOffset": 57}, {"referenceID": 7, "context": "Their results are high quality and they are able to synthesize shoes and handbags from coarse sketches [10] even though their training data was simple image edges.", "startOffset": 103, "endOffset": 107}, {"referenceID": 15, "context": "[18] does not emphasize controllable synthesis.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "from image to image), typically an encoder-decoder type of network architecture is adopted, for example in sketch inversion [14], image colorization [53, 17], and sketch simplification [42].", "startOffset": 124, "endOffset": 128}, {"referenceID": 50, "context": "from image to image), typically an encoder-decoder type of network architecture is adopted, for example in sketch inversion [14], image colorization [53, 17], and sketch simplification [42].", "startOffset": 149, "endOffset": 157}, {"referenceID": 14, "context": "from image to image), typically an encoder-decoder type of network architecture is adopted, for example in sketch inversion [14], image colorization [53, 17], and sketch simplification [42].", "startOffset": 149, "endOffset": 157}, {"referenceID": 39, "context": "from image to image), typically an encoder-decoder type of network architecture is adopted, for example in sketch inversion [14], image colorization [53, 17], and sketch simplification [42].", "startOffset": 185, "endOffset": 189}, {"referenceID": 13, "context": "[16] proposed the residual connection that uses skip layers allowing network blocks to learn only the residual component.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "Starting from the network design in Sketch Inversion [14], we introduce several important modifications to improve the visual quality of output and accommodate higher resolution input and more challenging image categories, such as car and bedroom.", "startOffset": 53, "endOffset": 57}, {"referenceID": 29, "context": "In addition, we replace the deconvolutional layers with the bilinear upsampling step followed by two residual blocks, due to the recent finding that deconvolutional layers have the tendency to produce checkerboard artifacts commonly seen in deep generative models [32].", "startOffset": 264, "endOffset": 268}, {"referenceID": 11, "context": "Previous work [14] showed that adding a feature loss to the objective function is beneficial for image generation tasks.", "startOffset": 14, "endOffset": 18}, {"referenceID": 11, "context": "For image categories like face, the generated results tend to have similar skin tones [14].", "startOffset": 86, "endOffset": 90}, {"referenceID": 9, "context": "Generative adversarial networks (GAN), proposed by Goodfellow et al [12], have attracted considerable attention recently.", "startOffset": 68, "endOffset": 72}, {"referenceID": 4, "context": "Dosovitskiy et al [7] showed that complimenting the feature loss with an adversarial loss leads to more realistic results.", "startOffset": 18, "endOffset": 21}, {"referenceID": 31, "context": "We also avoided conditioning the discriminator on the input image, as this tends to increase the instability [34].", "startOffset": 109, "endOffset": 113}, {"referenceID": 16, "context": "Finally, we also add a total variation loss Ltv to encourage smoothness in the output [19].", "startOffset": 86, "endOffset": 90}, {"referenceID": 25, "context": "In this paper, we experiment with three image classes \u2013 faces [28], cars, and bedrooms [52].", "startOffset": 62, "endOffset": 66}, {"referenceID": 49, "context": "In this paper, we experiment with three image classes \u2013 faces [28], cars, and bedrooms [52].", "startOffset": 87, "endOffset": 91}, {"referenceID": 45, "context": "For each image category \u2013 face, car, or bedroom \u2013 we apply the boundary detection filter XDoG [48] on 200k photos to generate the corresponding synthetic sketches.", "startOffset": 94, "endOffset": 98}, {"referenceID": 8, "context": "\u2022 StyleNet [11] We apply neural network-based style transfer algorithm to transfer the texture style of a pencil drawing to the ground-truth photo.", "startOffset": 11, "endOffset": 15}, {"referenceID": 44, "context": "\u2022 CUHK Finally, we add the CUHK dataset, which contains 188 hand-drawn portrait sketches and their corresponding photos [47].", "startOffset": 120, "endOffset": 124}, {"referenceID": 11, "context": "For comparison purposes, we implemented the Sketch Inversion architecture as described in [14].", "startOffset": 90, "endOffset": 94}, {"referenceID": 50, "context": "Guided Image Colorization: a) grayscale input, b) original color image, c) deep colorization result [53], d) First and third rows: color strokes overlaid on top of the grayscale input (zoom in to see the color strokes).", "startOffset": 100, "endOffset": 104}, {"referenceID": 14, "context": "Recent work [17, 53] explores training deep neural network models for the image colorization tasks.", "startOffset": 12, "endOffset": 20}, {"referenceID": 50, "context": "Recent work [17, 53] explores training deep neural network models for the image colorization tasks.", "startOffset": 12, "endOffset": 20}, {"referenceID": 11, "context": "In addition to design, portrait reconstruction technology is useful for forensic purposes, for example the law enforcement department can use it to help identify suspects [14].", "startOffset": 171, "endOffset": 175}, {"referenceID": 40, "context": "To enforce a fine-grained control using the input sketch, we choose the ReLU2-2 layer of the VGG-19 net [43] to compute the feature loss, since higher level feature representations tend to encourage the network to ignore important details such as the exact locations of the pupils.", "startOffset": 104, "endOffset": 108}, {"referenceID": 8, "context": "We thank Yijun Li for assistance with generation of synthetic training sketches from [11].", "startOffset": 85, "endOffset": 89}], "year": 2017, "abstractText": "Recently, there have been several promising methods to generate realistic imagery from deep convolutional networks. These methods sidestep the traditional computer graphics rendering pipeline and instead generate imagery at the pixel level by learning from large collections of photos (e.g. faces or bedrooms). However, these methods are of limited utility because it is difficult for a user to control what the network produces. In this paper, we propose a deep adversarial image synthesis architecture that is conditioned on coarse sketches and sparse color strokes to generate realistic cars, bedrooms, or faces. We demonstrate a sketch based image synthesis system which allows users to scribble over the sketch to indicate preferred color for objects. Our network can then generate convincing images that satisfy both the color and the sketch constraints of user. The network is feed-forward which allows users to see the effect of their edits in real time. We compare to recent work on sketch to image synthesis and show that our approach can generate more realistic, more diverse, and more controllable outputs. The architecture is also effective at user-guided colorization of grayscale images.", "creator": "LaTeX with hyperref package"}}}