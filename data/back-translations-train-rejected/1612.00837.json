{"id": "1612.00837", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Dec-2016", "title": "Making the V in VQA Matter: Elevating the Role of Image Understanding in Visual Question Answering", "abstract": "Problems at the intersection of vision and language are of significant importance both as challenging research questions and for the rich set of applications they enable. However, inherent structure in our world and bias in our language tend to be a simpler signal for learning than visual modalities, resulting in models that ignore visual information, leading to an inflated sense of their capability.", "histories": [["v1", "Fri, 2 Dec 2016 20:57:07 GMT  (3642kb,D)", "https://arxiv.org/abs/1612.00837v1", null], ["v2", "Fri, 14 Apr 2017 18:20:13 GMT  (3637kb,D)", "http://arxiv.org/abs/1612.00837v2", null], ["v3", "Mon, 15 May 2017 17:58:49 GMT  (3637kb,D)", "http://arxiv.org/abs/1612.00837v3", null]], "reviews": [], "SUBJECTS": "cs.CV cs.AI cs.CL cs.LG", "authors": ["yash goyal", "tejas khot", "douglas summers-stay", "dhruv batra", "devi parikh"], "accepted": false, "id": "1612.00837"}, "pdf": {"name": "1612.00837.pdf", "metadata": {"source": "CRF", "title": "Making the V in VQA Matter: Elevating the Role of Image Understanding in Visual Question Answering", "authors": ["Yash Goyal", "Tejas Khot", "Douglas Summers-Stay", "Dhruv Batra", "Devi Parikh"], "emails": ["tjskhot}@vt.edu", "douglas.a.summers-stay.civ@mail.mil", "parikh}@gatech.edu"], "sections": [{"heading": null, "text": "We propose to counteract these language priorities for the task of answering visual questions (VQA) and to make vision (the V in VQA) an issue! Specifically, we balance the popular VQA dataset [3] by collecting complementary images so that each question in our balanced dataset is associated not only with a single image, but rather with a pair of similar images that lead to two different answers to the question. Our dataset is more constructively balanced than the original VQA dataset and contains about twice as many picture-question pairs. Our full balanced dataset is publicly available at http: / / visualqa.org / as part of the second iteration of the Visual Dataset and Challenge (VQA v2.0)."}, {"heading": "1. Introduction", "text": "In fact, it is the case that most people who are able are able to determine for themselves what they want and what they do not want."}, {"heading": "2. Related Work", "text": "A number of recent papers have suggested a visual question about answering datasets [3, 22, 26, 31, 10, 46, 38, 36] and models [9, 25, 43, 24, 27, 47, 45, 41, 35, 20, 29, 15, 42, 33, 17]. Our work builds on the top of the VQA dataset by Antol et al. [3], which is one of the most commonly used VQA datasets. We reduce the language distortions contained in this popular dataset, resulting in a dataset that is more balanced and about twice the size of the VQA dataset. We name a \"base model\" VQA model, an attention-based VQA model [25], and the winning model from the VQA Real Ended Challenge 2016. \""}, {"heading": "3. Dataset", "text": "We are building on the top of the VQA dataset presented by Antol et al. [3]. VQA Real Images Dataset contains just over 204K images of COCO [23], 614K freeform natural language questions (3 questions per image), and over 6 million free format answers (10 answers per question). While this dataset has caused significant progress in the VQA domain, as previously discussed, there have been strong language barriers. Our key idea to counteract this language bias is the following: for each (image, question, answer) triplet (me, Q, A) dataset, in which VQA dataset, our goal is to identify an image that I thatis similar to myself, the question that needs to be answered in order to enter this language bias, question, question, question, question, answer, answer, answer, answer, answer, answer, answer, answer, answer, triplet (me, answer, answer, answer, answer, answer, answer, answer, answer)."}, {"heading": "4. Benchmarking Existing VQA Models", "text": "In fact, we are able to set out in search of a solution that enables us to move to another world, to move to another world, to move to another world, to move to another world."}, {"heading": "5. Counter-example Explanations", "text": "We propose a new explanation modality: counter-examples. We propose a model that, when a question is asked about an image, not only provides an answer, but also provides sample images that are similar to the input image, but the model believes it has different answers to the input question. This would give the user confidence that the model actually \"understands\" the concept in question. For example, a VQA model can be perceived as more trustworthy if it adds \"other\" in addition to \"red\" and shows an example image that contains a fire hydrant that is not red.3"}, {"heading": "5.1. Model", "text": "In the first step, similar to a conventional VQA model, it takes an (image, question) pair (Q, I) as input and predicts an answer to Apred. In the second step, it uses this predicted answer Apred together with the question Q to retrieve an image that is similar to me, but a different answer than Apred to the question Q. To ensure similarity, the model selects a candidate closest to me, INN = {I1, I2, IK} as a counter-example to the counter-example.How can we find these \"negative explanations\"? One way to select the counter-example from INN is to follow the classic \"Hard Negative Mining\" strategy, which is popular in computer vision, simply to select the image that has the lowest P (Apred | Q), where i,..., K. We compare it with this strong base. \""}, {"heading": "5.2. Results", "text": "We see the original image I, the question asked Q, the answer Apred predicted by the5Note, that in theory one should use Apred as input during training instead of A. Finally, this corresponds to the expected application scenario in the test period. However, this alternative setup (where Apred is provided as input instead of A) leads to a peculiar and unnatural explanation training goal - specifically, the explanation head will still learn to explain A, as this is the answer for which we have collected negative explanations of human comments. It is simply unnatural to build this model, which answers one question with Apred, but learns to explain another answer to A. Note that this is an interesting scenario in which the current pressure on \"end-to-end\" training for everything breaks off. VQA head in our model, and the three negative explanations that are produced by the explanation head."}, {"heading": "6. Conclusion", "text": "To summarize, in this paper we address the strong language priorities for the task of Visual Question Answering 79 and highlight the role of image understanding required to succeed. We are developing a novel data acquisition interface to \"balance\" the popular VQA datasets [3] by collecting \"complementary\" images. For each question in the dataset, we have two complementary images that look similar but have different answers to the question. These efforts result in a dataset that is not only more balanced than the original VQA datasets by construction, but also about twice as large. We find both qualitatively and quantitatively that the \"tails\" of the response distribution in this balanced dataset are more difficult, which are the strong language priors that can be exploited by models. Our full balanced dataset is publicly available at http: / vvisualq.org / Answer 2.0 as the second part of the Visual Question Item and Explanation in fact."}], "references": [{"title": "Analyzing the Behavior of Visual Question Answering Models", "author": ["A. Agrawal", "D. Batra", "D. Parikh"], "venue": "EMNLP,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2016}, {"title": "Deep compositional question answering with neural module networks", "author": ["J. Andreas", "M. Rohrbach", "T. Darrell", "D. Klein"], "venue": "CVPR,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2016}, {"title": "VQA: Visual Question Answering", "author": ["S. Antol", "A. Agrawal", "J. Lu", "M. Mitchell", "D. Batra", "C.L. Zitnick", "D. Parikh"], "venue": "ICCV,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2015}, {"title": "Mind\u2019s Eye: A Recurrent Visual Representation for Image Caption Generation", "author": ["X. Chen", "C.L. Zitnick"], "venue": "CVPR,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2015}, {"title": "ImageNet: A Large-Scale Hierarchical Image Database", "author": ["J. Deng", "W. Dong", "R. Socher", "L.-J. Li", "K. Li", "L. Fei-Fei"], "venue": "CVPR,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2009}, {"title": "Exploring nearest neighbor approaches for image captioning", "author": ["J. Devlin", "S. Gupta", "R.B. Girshick", "M. Mitchell", "C.L. Zitnick"], "venue": "CoRR, abs/1505.04467,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2015}, {"title": "Long-term Recurrent Convolutional Networks for Visual Recognition and Description", "author": ["J. Donahue", "L.A. Hendricks", "S. Guadarrama", "M. Rohrbach", "S. Venugopalan", "K. Saenko", "T. Darrell"], "venue": "CVPR,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2015}, {"title": "From Captions to Visual Concepts and Back", "author": ["H. Fang", "S. Gupta", "F.N. Iandola", "R. Srivastava", "L. Deng", "P. Doll\u00e1r", "J. Gao", "X. He", "M. Mitchell", "J.C. Platt", "C.L. Zitnick", "G. Zweig"], "venue": "CVPR,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "Multimodal Compact Bilinear Pooling for Visual Question Answering and Visual Grounding", "author": ["A. Fukui", "D.H. Park", "D. Yang", "A. Rohrbach", "T. Darrell", "M. Rohrbach"], "venue": "EMNLP,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2016}, {"title": "Are you talking to a machine? dataset and methods for multilingual image question answering", "author": ["H. Gao", "J. Mao", "J. Zhou", "Z. Huang", "A. Yuille"], "venue": "NIPS,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "Towards Transparent AI Systems: Interpreting Visual Question Answering Models", "author": ["Y. Goyal", "A. Mohapatra", "D. Parikh", "D. Batra"], "venue": "ICML Workshop on Visualization for Deep Learning,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2016}, {"title": "Deep residual learning for image recognition", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "CVPR,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2016}, {"title": "Generating visual explanations", "author": ["L.A. Hendricks", "Z. Akata", "M. Rohrbach", "J. Donahue", "B. Schiele", "T. Darrell"], "venue": "ECCV,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2016}, {"title": "Focused evaluation for image description with binary forced-choice tasks", "author": ["M. Hodosh", "J. Hockenmaier"], "venue": "Workshop on Vision and Language, Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2016}, {"title": "A focused dynamic attention model for visual question answering", "author": ["I. Ilievski", "S. Yan", "J. Feng"], "venue": "CoRR, abs/1604.01485,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2016}, {"title": "Revisiting Visual Question Answering Baselines", "author": ["A. Jabri", "A. Joulin", "L. van der Maaten"], "venue": "In ECCV,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2016}, {"title": "Answer-type prediction for visual question answering", "author": ["K. Kafle", "C. Kanan"], "venue": "CVPR,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2016}, {"title": "Visual Question Answering: Datasets, Algorithms, and Future Challenges", "author": ["K. Kafle", "C. Kanan"], "venue": "CoRR, abs/1610.01465,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2016}, {"title": "Deep Visual-Semantic Alignments for Generating Image Descriptions", "author": ["A. Karpathy", "L. Fei-Fei"], "venue": "CVPR,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2015}, {"title": "Multimodal Residual Learning for Visual QA", "author": ["J.-H. Kim", "S.-W. Lee", "D.-H. Kwak", "M.-O. Heo", "J. Kim", "J.- W. Ha", "B.-T. Zhang"], "venue": "In NIPS,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2016}, {"title": "Unifying Visual-Semantic Embeddings with Multimodal Neural Language Models", "author": ["R. Kiros", "R. Salakhutdinov", "R.S. Zemel"], "venue": "TACL,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2015}, {"title": "Visual genome: Connecting language and vision using crowdsourced dense image annotations", "author": ["R. Krishna", "Y. Zhu", "O. Groth", "J. Johnson", "K. Hata", "J. Kravitz", "S. Chen", "Y. Kalantidis", "L.-J. Li", "D.A. Shamma"], "venue": "arXiv preprint arXiv:1602.07332,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2016}, {"title": "Microsoft COCO: Common Objects in Context", "author": ["T.-Y. Lin", "M. Maire", "S. Belongie", "J. Hays", "P. Perona", "D. Ramanan", "P. Doll\u00e1r", "C.L. Zitnick"], "venue": "ECCV,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2014}, {"title": "Deeper LSTM and normalized CNN Visual Question Answering model", "author": ["J. Lu", "X. Lin", "D. Batra", "D. Parikh"], "venue": "https://github.com/VT-vision-lab/ VQA_LSTM_CNN,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2015}, {"title": "Hierarchical Question-Image Co-Attention for Visual Question Answering", "author": ["J. Lu", "J. Yang", "D. Batra", "D. Parikh"], "venue": "NIPS,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2016}, {"title": "A Multi-World Approach to Question Answering about Real-World Scenes based on Uncertain Input", "author": ["M. Malinowski", "M. Fritz"], "venue": "NIPS,", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2014}, {"title": "Ask your neurons: A neural-based approach to answering questions about images", "author": ["M. Malinowski", "M. Rohrbach", "M. Fritz"], "venue": "ICCV,", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2015}, {"title": "Explain Images with Multimodal Recurrent Neural Networks", "author": ["J. Mao", "W. Xu", "Y. Yang", "J. Wang", "A.L. Yuille"], "venue": "NIPS,", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2014}, {"title": "Training recurrent answering units with joint loss minimization for vqa", "author": ["H. Noh", "B. Han"], "venue": "CoRR, abs/1606.03647,", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2016}, {"title": "Question Relevance in VQA: Identifying Non-Visual And False-Premise Questions", "author": ["A. Ray", "G. Christie", "M. Bansal", "D. Batra", "D. Parikh"], "venue": "EMNLP,", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2016}, {"title": "Exploring models and data for image question answering", "author": ["M. Ren", "R. Kiros", "R. Zemel"], "venue": "NIPS,", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2015}, {"title": "Why Should I Trust You?\u201d: Explaining the Predictions of Any Classifier", "author": ["M.T. Ribeiro", "S. Singh", "C. Guestrin"], "venue": "Knowledge Discovery and Data Mining (KDD),", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2016}, {"title": "Dualnet: Domain-invariant network for visual question answering", "author": ["K. Saito", "A. Shin", "Y. Ushiku", "T. Harada"], "venue": "CoRR, abs/1606.06108,", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2016}, {"title": "Grad-CAM: Why did you say that? Visual Explanations from Deep Networks via Gradientbased Localization", "author": ["R.R. Selvaraju", "A. Das", "R. Vedantam", "M. Cogswell", "D. Parikh", "D. Batra"], "venue": "arXiv preprint arXiv:1610.02391,", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2016}, {"title": "Where to look: Focus regions for visual question answering", "author": ["K.J. Shih", "S. Singh", "D. Hoiem"], "venue": "CVPR,", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2016}, {"title": "The Color of the Cat is Gray: 1 Million Full-Sentences Visual Question Answering (FSVQA)", "author": ["A. Shin", "Y. Ushiku", "T. Harada"], "venue": "arXiv preprint arXiv:1609.06657,", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2016}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "ICLR,", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2015}, {"title": "MovieQA: Understanding Stories in Movies through Question-Answering", "author": ["M. Tapaswi", "Y. Zhu", "R. Stiefelhagen", "A. Torralba", "R. Urtasun", "S. Fidler"], "venue": "CVPR,", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2016}, {"title": "Unbiased look at dataset bias", "author": ["A. Torralba", "A. Efros"], "venue": "CVPR,", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2011}, {"title": "Show and Tell: A Neural Image Caption Generator", "author": ["O. Vinyals", "A. Toshev", "S. Bengio", "D. Erhan"], "venue": "CVPR,", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2015}, {"title": "Explicit knowledge-based reasoning for visual question answering", "author": ["P. Wang", "Q. Wu", "C. Shen", "A. van den Hengel", "A.R. Dick"], "venue": "CoRR, abs/1511.02570,", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2015}, {"title": "Ask me anything: Free-form visual question answering based on knowledge from external sources", "author": ["Q. Wu", "P. Wang", "C. Shen", "A. van den Hengel", "A.R. Dick"], "venue": "In CVPR,", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2016}, {"title": "Dynamic memory networks for visual and textual question answering", "author": ["C. Xiong", "S. Merity", "R. Socher"], "venue": "ICML,", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2016}, {"title": "Ask, Attend and Answer: Exploring Question-Guided Spatial Attention for Visual Question Answering", "author": ["H. Xu", "K. Saenko"], "venue": "ECCV,", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2016}, {"title": "Stacked Attention Networks for Image Question Answering", "author": ["Z. Yang", "X. He", "J. Gao", "L. Deng", "A. Smola"], "venue": "CVPR,", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2016}, {"title": "Visual Madlibs: Fill-in-the-blank Description Generation and Question Answering", "author": ["L. Yu", "E. Park", "A.C. Berg", "T.L. Berg"], "venue": "ICCV,", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2015}, {"title": "Yin and Yang: Balancing and Answering Binary Visual Questions", "author": ["P. Zhang", "Y. Goyal", "D. Summers-Stay", "D. Batra", "D. Parikh"], "venue": "CVPR,", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2016}, {"title": "Learning Deep Features for Discriminative Localization", "author": ["B. Zhou", "A. Khosla", "A. Lapedriza", "A. Oliva", "A. Torralba"], "venue": "CVPR,", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2015}, {"title": "Simple Baseline for Visual Question Answering", "author": ["B. Zhou", "Y. Tian", "S. Sukhbaatar", "A. Szlam", "R. Fergus"], "venue": "CoRR, abs/1512.02167,", "citeRegEx": "49", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 2, "context": "We propose to counter these language priors for the task of Visual Question Answering (VQA) and make vision (the V in VQA) matter! Specifically, we balance the popular VQA dataset [3] by collecting complementary images such that every question in our balanced dataset is associated with not just a single image, but rather a pair of similar images that result in two different answers to the question.", "startOffset": 180, "endOffset": 183}, {"referenceID": 7, "context": "Language and vision problems such as image captioning [8, 4, 7, 19, 40, 21, 28] and visual question answering (VQA) [3, 26, 27, 10, 31] have gained popularity in recent years as the computer vision research community is progressing beyond \u201cbucketed\u201d recognition and towards solving multi-modal problems.", "startOffset": 54, "endOffset": 79}, {"referenceID": 3, "context": "Language and vision problems such as image captioning [8, 4, 7, 19, 40, 21, 28] and visual question answering (VQA) [3, 26, 27, 10, 31] have gained popularity in recent years as the computer vision research community is progressing beyond \u201cbucketed\u201d recognition and towards solving multi-modal problems.", "startOffset": 54, "endOffset": 79}, {"referenceID": 6, "context": "Language and vision problems such as image captioning [8, 4, 7, 19, 40, 21, 28] and visual question answering (VQA) [3, 26, 27, 10, 31] have gained popularity in recent years as the computer vision research community is progressing beyond \u201cbucketed\u201d recognition and towards solving multi-modal problems.", "startOffset": 54, "endOffset": 79}, {"referenceID": 18, "context": "Language and vision problems such as image captioning [8, 4, 7, 19, 40, 21, 28] and visual question answering (VQA) [3, 26, 27, 10, 31] have gained popularity in recent years as the computer vision research community is progressing beyond \u201cbucketed\u201d recognition and towards solving multi-modal problems.", "startOffset": 54, "endOffset": 79}, {"referenceID": 39, "context": "Language and vision problems such as image captioning [8, 4, 7, 19, 40, 21, 28] and visual question answering (VQA) [3, 26, 27, 10, 31] have gained popularity in recent years as the computer vision research community is progressing beyond \u201cbucketed\u201d recognition and towards solving multi-modal problems.", "startOffset": 54, "endOffset": 79}, {"referenceID": 20, "context": "Language and vision problems such as image captioning [8, 4, 7, 19, 40, 21, 28] and visual question answering (VQA) [3, 26, 27, 10, 31] have gained popularity in recent years as the computer vision research community is progressing beyond \u201cbucketed\u201d recognition and towards solving multi-modal problems.", "startOffset": 54, "endOffset": 79}, {"referenceID": 27, "context": "Language and vision problems such as image captioning [8, 4, 7, 19, 40, 21, 28] and visual question answering (VQA) [3, 26, 27, 10, 31] have gained popularity in recent years as the computer vision research community is progressing beyond \u201cbucketed\u201d recognition and towards solving multi-modal problems.", "startOffset": 54, "endOffset": 79}, {"referenceID": 2, "context": "Language and vision problems such as image captioning [8, 4, 7, 19, 40, 21, 28] and visual question answering (VQA) [3, 26, 27, 10, 31] have gained popularity in recent years as the computer vision research community is progressing beyond \u201cbucketed\u201d recognition and towards solving multi-modal problems.", "startOffset": 116, "endOffset": 135}, {"referenceID": 25, "context": "Language and vision problems such as image captioning [8, 4, 7, 19, 40, 21, 28] and visual question answering (VQA) [3, 26, 27, 10, 31] have gained popularity in recent years as the computer vision research community is progressing beyond \u201cbucketed\u201d recognition and towards solving multi-modal problems.", "startOffset": 116, "endOffset": 135}, {"referenceID": 26, "context": "Language and vision problems such as image captioning [8, 4, 7, 19, 40, 21, 28] and visual question answering (VQA) [3, 26, 27, 10, 31] have gained popularity in recent years as the computer vision research community is progressing beyond \u201cbucketed\u201d recognition and towards solving multi-modal problems.", "startOffset": 116, "endOffset": 135}, {"referenceID": 9, "context": "Language and vision problems such as image captioning [8, 4, 7, 19, 40, 21, 28] and visual question answering (VQA) [3, 26, 27, 10, 31] have gained popularity in recent years as the computer vision research community is progressing beyond \u201cbucketed\u201d recognition and towards solving multi-modal problems.", "startOffset": 116, "endOffset": 135}, {"referenceID": 30, "context": "Language and vision problems such as image captioning [8, 4, 7, 19, 40, 21, 28] and visual question answering (VQA) [3, 26, 27, 10, 31] have gained popularity in recent years as the computer vision research community is progressing beyond \u201cbucketed\u201d recognition and towards solving multi-modal problems.", "startOffset": 116, "endOffset": 135}, {"referenceID": 5, "context": "But recent works [6, 47, 49, 16, 18, 1] have pointed out that language also provides a strong prior that can result in good superficial performance, without the underlying models truly understanding the visual content.", "startOffset": 17, "endOffset": 39}, {"referenceID": 46, "context": "But recent works [6, 47, 49, 16, 18, 1] have pointed out that language also provides a strong prior that can result in good superficial performance, without the underlying models truly understanding the visual content.", "startOffset": 17, "endOffset": 39}, {"referenceID": 48, "context": "But recent works [6, 47, 49, 16, 18, 1] have pointed out that language also provides a strong prior that can result in good superficial performance, without the underlying models truly understanding the visual content.", "startOffset": 17, "endOffset": 39}, {"referenceID": 15, "context": "But recent works [6, 47, 49, 16, 18, 1] have pointed out that language also provides a strong prior that can result in good superficial performance, without the underlying models truly understanding the visual content.", "startOffset": 17, "endOffset": 39}, {"referenceID": 17, "context": "But recent works [6, 47, 49, 16, 18, 1] have pointed out that language also provides a strong prior that can result in good superficial performance, without the underlying models truly understanding the visual content.", "startOffset": 17, "endOffset": 39}, {"referenceID": 0, "context": "But recent works [6, 47, 49, 16, 18, 1] have pointed out that language also provides a strong prior that can result in good superficial performance, without the underlying models truly understanding the visual content.", "startOffset": 17, "endOffset": 39}, {"referenceID": 5, "context": "This phenomenon has been observed in image captioning [6] as well as visual question answering [47, 49, 16, 18, 1].", "startOffset": 54, "endOffset": 57}, {"referenceID": 46, "context": "This phenomenon has been observed in image captioning [6] as well as visual question answering [47, 49, 16, 18, 1].", "startOffset": 95, "endOffset": 114}, {"referenceID": 48, "context": "This phenomenon has been observed in image captioning [6] as well as visual question answering [47, 49, 16, 18, 1].", "startOffset": 95, "endOffset": 114}, {"referenceID": 15, "context": "This phenomenon has been observed in image captioning [6] as well as visual question answering [47, 49, 16, 18, 1].", "startOffset": 95, "endOffset": 114}, {"referenceID": 17, "context": "This phenomenon has been observed in image captioning [6] as well as visual question answering [47, 49, 16, 18, 1].", "startOffset": 95, "endOffset": 114}, {"referenceID": 0, "context": "This phenomenon has been observed in image captioning [6] as well as visual question answering [47, 49, 16, 18, 1].", "startOffset": 95, "endOffset": 114}, {"referenceID": 2, "context": "For instance, in the VQA [3] dataset, the most common sport answer \u201ctennis\u201d is the correct answer for 41% of the questions starting with \u201cWhat sport is\u201d, and \u201c2\u201d is the correct answer for 39% of the questions starting with \u201cHow many\u201d.", "startOffset": 25, "endOffset": 28}, {"referenceID": 46, "context": "[47] points out a particular \u2018visual priming bias\u2019 in the VQA dataset \u2013 specifically, subjects saw an image while asking questions about it.", "startOffset": 0, "endOffset": 4}, {"referenceID": 38, "context": "This can hinder progress in pushing state of art in the computer vision aspects of multi-modal AI [39, 47].", "startOffset": 98, "endOffset": 106}, {"referenceID": 46, "context": "This can hinder progress in pushing state of art in the computer vision aspects of multi-modal AI [39, 47].", "startOffset": 98, "endOffset": 106}, {"referenceID": 36, "context": "Our balanced VQA dataset is also particularly difficult because the picked complementary image I \u2032 is close to the original image I in the semantic (fc7) space of VGGNet [37] features.", "startOffset": 170, "endOffset": 174}, {"referenceID": 2, "context": "1 Million (image, question) pairs \u2013 almost double the size of the VQA [3] dataset \u2013 with approximately 13 Million associated answers on the \u223c200k images from COCO [23].", "startOffset": 70, "endOffset": 73}, {"referenceID": 22, "context": "1 Million (image, question) pairs \u2013 almost double the size of the VQA [3] dataset \u2013 with approximately 13 Million associated answers on the \u223c200k images from COCO [23].", "startOffset": 163, "endOffset": 167}, {"referenceID": 2, "context": "Our main contributions are as follows: (1) We balance the existing VQA dataset [3] by collecting complementary images such that almost every question in our balanced dataset is associated with not just a single image, but rather a pair of similar images that result in two different answers to the question.", "startOffset": 79, "endOffset": 82}, {"referenceID": 2, "context": "A number of recent works have proposed visual question answering datasets [3, 22, 26, 31, 10, 46, 38, 36] and models [9, 25, 2, 43, 24, 27, 47, 45, 44, 41, 35, 20, 29, 15, 42, 33, 17].", "startOffset": 74, "endOffset": 105}, {"referenceID": 21, "context": "A number of recent works have proposed visual question answering datasets [3, 22, 26, 31, 10, 46, 38, 36] and models [9, 25, 2, 43, 24, 27, 47, 45, 44, 41, 35, 20, 29, 15, 42, 33, 17].", "startOffset": 74, "endOffset": 105}, {"referenceID": 25, "context": "A number of recent works have proposed visual question answering datasets [3, 22, 26, 31, 10, 46, 38, 36] and models [9, 25, 2, 43, 24, 27, 47, 45, 44, 41, 35, 20, 29, 15, 42, 33, 17].", "startOffset": 74, "endOffset": 105}, {"referenceID": 30, "context": "A number of recent works have proposed visual question answering datasets [3, 22, 26, 31, 10, 46, 38, 36] and models [9, 25, 2, 43, 24, 27, 47, 45, 44, 41, 35, 20, 29, 15, 42, 33, 17].", "startOffset": 74, "endOffset": 105}, {"referenceID": 9, "context": "A number of recent works have proposed visual question answering datasets [3, 22, 26, 31, 10, 46, 38, 36] and models [9, 25, 2, 43, 24, 27, 47, 45, 44, 41, 35, 20, 29, 15, 42, 33, 17].", "startOffset": 74, "endOffset": 105}, {"referenceID": 45, "context": "A number of recent works have proposed visual question answering datasets [3, 22, 26, 31, 10, 46, 38, 36] and models [9, 25, 2, 43, 24, 27, 47, 45, 44, 41, 35, 20, 29, 15, 42, 33, 17].", "startOffset": 74, "endOffset": 105}, {"referenceID": 37, "context": "A number of recent works have proposed visual question answering datasets [3, 22, 26, 31, 10, 46, 38, 36] and models [9, 25, 2, 43, 24, 27, 47, 45, 44, 41, 35, 20, 29, 15, 42, 33, 17].", "startOffset": 74, "endOffset": 105}, {"referenceID": 35, "context": "A number of recent works have proposed visual question answering datasets [3, 22, 26, 31, 10, 46, 38, 36] and models [9, 25, 2, 43, 24, 27, 47, 45, 44, 41, 35, 20, 29, 15, 42, 33, 17].", "startOffset": 74, "endOffset": 105}, {"referenceID": 8, "context": "A number of recent works have proposed visual question answering datasets [3, 22, 26, 31, 10, 46, 38, 36] and models [9, 25, 2, 43, 24, 27, 47, 45, 44, 41, 35, 20, 29, 15, 42, 33, 17].", "startOffset": 117, "endOffset": 183}, {"referenceID": 24, "context": "A number of recent works have proposed visual question answering datasets [3, 22, 26, 31, 10, 46, 38, 36] and models [9, 25, 2, 43, 24, 27, 47, 45, 44, 41, 35, 20, 29, 15, 42, 33, 17].", "startOffset": 117, "endOffset": 183}, {"referenceID": 1, "context": "A number of recent works have proposed visual question answering datasets [3, 22, 26, 31, 10, 46, 38, 36] and models [9, 25, 2, 43, 24, 27, 47, 45, 44, 41, 35, 20, 29, 15, 42, 33, 17].", "startOffset": 117, "endOffset": 183}, {"referenceID": 42, "context": "A number of recent works have proposed visual question answering datasets [3, 22, 26, 31, 10, 46, 38, 36] and models [9, 25, 2, 43, 24, 27, 47, 45, 44, 41, 35, 20, 29, 15, 42, 33, 17].", "startOffset": 117, "endOffset": 183}, {"referenceID": 23, "context": "A number of recent works have proposed visual question answering datasets [3, 22, 26, 31, 10, 46, 38, 36] and models [9, 25, 2, 43, 24, 27, 47, 45, 44, 41, 35, 20, 29, 15, 42, 33, 17].", "startOffset": 117, "endOffset": 183}, {"referenceID": 26, "context": "A number of recent works have proposed visual question answering datasets [3, 22, 26, 31, 10, 46, 38, 36] and models [9, 25, 2, 43, 24, 27, 47, 45, 44, 41, 35, 20, 29, 15, 42, 33, 17].", "startOffset": 117, "endOffset": 183}, {"referenceID": 46, "context": "A number of recent works have proposed visual question answering datasets [3, 22, 26, 31, 10, 46, 38, 36] and models [9, 25, 2, 43, 24, 27, 47, 45, 44, 41, 35, 20, 29, 15, 42, 33, 17].", "startOffset": 117, "endOffset": 183}, {"referenceID": 44, "context": "A number of recent works have proposed visual question answering datasets [3, 22, 26, 31, 10, 46, 38, 36] and models [9, 25, 2, 43, 24, 27, 47, 45, 44, 41, 35, 20, 29, 15, 42, 33, 17].", "startOffset": 117, "endOffset": 183}, {"referenceID": 43, "context": "A number of recent works have proposed visual question answering datasets [3, 22, 26, 31, 10, 46, 38, 36] and models [9, 25, 2, 43, 24, 27, 47, 45, 44, 41, 35, 20, 29, 15, 42, 33, 17].", "startOffset": 117, "endOffset": 183}, {"referenceID": 40, "context": "A number of recent works have proposed visual question answering datasets [3, 22, 26, 31, 10, 46, 38, 36] and models [9, 25, 2, 43, 24, 27, 47, 45, 44, 41, 35, 20, 29, 15, 42, 33, 17].", "startOffset": 117, "endOffset": 183}, {"referenceID": 34, "context": "A number of recent works have proposed visual question answering datasets [3, 22, 26, 31, 10, 46, 38, 36] and models [9, 25, 2, 43, 24, 27, 47, 45, 44, 41, 35, 20, 29, 15, 42, 33, 17].", "startOffset": 117, "endOffset": 183}, {"referenceID": 19, "context": "A number of recent works have proposed visual question answering datasets [3, 22, 26, 31, 10, 46, 38, 36] and models [9, 25, 2, 43, 24, 27, 47, 45, 44, 41, 35, 20, 29, 15, 42, 33, 17].", "startOffset": 117, "endOffset": 183}, {"referenceID": 28, "context": "A number of recent works have proposed visual question answering datasets [3, 22, 26, 31, 10, 46, 38, 36] and models [9, 25, 2, 43, 24, 27, 47, 45, 44, 41, 35, 20, 29, 15, 42, 33, 17].", "startOffset": 117, "endOffset": 183}, {"referenceID": 14, "context": "A number of recent works have proposed visual question answering datasets [3, 22, 26, 31, 10, 46, 38, 36] and models [9, 25, 2, 43, 24, 27, 47, 45, 44, 41, 35, 20, 29, 15, 42, 33, 17].", "startOffset": 117, "endOffset": 183}, {"referenceID": 41, "context": "A number of recent works have proposed visual question answering datasets [3, 22, 26, 31, 10, 46, 38, 36] and models [9, 25, 2, 43, 24, 27, 47, 45, 44, 41, 35, 20, 29, 15, 42, 33, 17].", "startOffset": 117, "endOffset": 183}, {"referenceID": 32, "context": "A number of recent works have proposed visual question answering datasets [3, 22, 26, 31, 10, 46, 38, 36] and models [9, 25, 2, 43, 24, 27, 47, 45, 44, 41, 35, 20, 29, 15, 42, 33, 17].", "startOffset": 117, "endOffset": 183}, {"referenceID": 16, "context": "A number of recent works have proposed visual question answering datasets [3, 22, 26, 31, 10, 46, 38, 36] and models [9, 25, 2, 43, 24, 27, 47, 45, 44, 41, 35, 20, 29, 15, 42, 33, 17].", "startOffset": 117, "endOffset": 183}, {"referenceID": 2, "context": "[3], which is one of the most widely used VQA datasets.", "startOffset": 0, "endOffset": 3}, {"referenceID": 23, "context": "We benchmark one \u2018baseline\u2019 VQA model [24], one attention-based VQA model [25], and the winning model from the VQA Real Open Ended Challenge 2016 [9] on our balanced VQA dataset, and compare them to a language-only model.", "startOffset": 38, "endOffset": 42}, {"referenceID": 24, "context": "We benchmark one \u2018baseline\u2019 VQA model [24], one attention-based VQA model [25], and the winning model from the VQA Real Open Ended Challenge 2016 [9] on our balanced VQA dataset, and compare them to a language-only model.", "startOffset": 74, "endOffset": 78}, {"referenceID": 8, "context": "We benchmark one \u2018baseline\u2019 VQA model [24], one attention-based VQA model [25], and the winning model from the VQA Real Open Ended Challenge 2016 [9] on our balanced VQA dataset, and compare them to a language-only model.", "startOffset": 146, "endOffset": 149}, {"referenceID": 13, "context": "[14], who created a binary forced-choice image captioning task, where a machine must choose to caption an image with one of two similar captions.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[14] implemented hand-designed rules to create two similar captions for images, while we create a novel annotation interface to collect two similar images for questions in VQA.", "startOffset": 0, "endOffset": 4}, {"referenceID": 46, "context": "[47], who study this goal of balancing VQA in a fairly restricted setting \u2013 binary (yes/no) questions on abstract scenes made from clipart (part of the VQA abstract scenes dataset [3]).", "startOffset": 0, "endOffset": 4}, {"referenceID": 2, "context": "[47], who study this goal of balancing VQA in a fairly restricted setting \u2013 binary (yes/no) questions on abstract scenes made from clipart (part of the VQA abstract scenes dataset [3]).", "startOffset": 180, "endOffset": 183}, {"referenceID": 12, "context": "A number of recent works have proposed mechanisms for generating \u2018explanations\u2019 [13, 34, 48, 11, 32] for the predictions made by deep learning models, which are typically \u2018black-box\u2019 and noninterpretable.", "startOffset": 80, "endOffset": 100}, {"referenceID": 33, "context": "A number of recent works have proposed mechanisms for generating \u2018explanations\u2019 [13, 34, 48, 11, 32] for the predictions made by deep learning models, which are typically \u2018black-box\u2019 and noninterpretable.", "startOffset": 80, "endOffset": 100}, {"referenceID": 47, "context": "A number of recent works have proposed mechanisms for generating \u2018explanations\u2019 [13, 34, 48, 11, 32] for the predictions made by deep learning models, which are typically \u2018black-box\u2019 and noninterpretable.", "startOffset": 80, "endOffset": 100}, {"referenceID": 10, "context": "A number of recent works have proposed mechanisms for generating \u2018explanations\u2019 [13, 34, 48, 11, 32] for the predictions made by deep learning models, which are typically \u2018black-box\u2019 and noninterpretable.", "startOffset": 80, "endOffset": 100}, {"referenceID": 31, "context": "A number of recent works have proposed mechanisms for generating \u2018explanations\u2019 [13, 34, 48, 11, 32] for the predictions made by deep learning models, which are typically \u2018black-box\u2019 and noninterpretable.", "startOffset": 80, "endOffset": 100}, {"referenceID": 12, "context": "[13] generates a natural language explanation (sentence) for image categories.", "startOffset": 0, "endOffset": 4}, {"referenceID": 33, "context": "[34, 48, 11, 32] provide \u2018visual explanations\u2019 or spatial maps overlaid on images to highlight the regions that the model focused on while making its predictions.", "startOffset": 0, "endOffset": 16}, {"referenceID": 47, "context": "[34, 48, 11, 32] provide \u2018visual explanations\u2019 or spatial maps overlaid on images to highlight the regions that the model focused on while making its predictions.", "startOffset": 0, "endOffset": 16}, {"referenceID": 10, "context": "[34, 48, 11, 32] provide \u2018visual explanations\u2019 or spatial maps overlaid on images to highlight the regions that the model focused on while making its predictions.", "startOffset": 0, "endOffset": 16}, {"referenceID": 31, "context": "[34, 48, 11, 32] provide \u2018visual explanations\u2019 or spatial maps overlaid on images to highlight the regions that the model focused on while making its predictions.", "startOffset": 0, "endOffset": 16}, {"referenceID": 2, "context": "[3].", "startOffset": 0, "endOffset": 3}, {"referenceID": 22, "context": "VQA real images dataset contains just over 204K images from COCO [23], 614K free-form natural language questions (3 questions per image), and over 6 million free-form (but concise) answers (10 answers per question).", "startOffset": 65, "endOffset": 69}, {"referenceID": 36, "context": "We compute the 24 nearest neighbors by first representing each image with the activations from the penultimate (\u2018fc7\u2019) layer of a deep Convolutional Neural Network (CNN) \u2013 in particular VGGNet [37] \u2013 and then using `2distances to compute neighbors.", "startOffset": 193, "endOffset": 197}, {"referenceID": 2, "context": "Specifically, we show the picked image I \u2032 with the question Q to 10 new AMT workers, and collect 10 ground truth answers (similar to [3]).", "startOffset": 134, "endOffset": 137}, {"referenceID": 2, "context": "Figure 4: Distribution of answers per question type for a random sample of 60K questions from the original (unbalanced) VQA dataset [3] (top) and from our proposed balanced dataset (bottom).", "startOffset": 132, "endOffset": 135}, {"referenceID": 2, "context": "Following original VQA dataset [3], we divide our test set into 4 splits: test-dev, test-standard, test-challenge and testreserve.", "startOffset": 31, "endOffset": 34}, {"referenceID": 2, "context": "For more details, please refer to [3].", "startOffset": 34, "endOffset": 37}, {"referenceID": 2, "context": "As described above, we collected 10 answers for every complementary image and its corresponding question to be consistent with the VQA dataset [3].", "startOffset": 143, "endOffset": 146}, {"referenceID": 2, "context": "4 compares the distribution of answers per questiontype in our new balanced VQA dataset with the original (unbalanced) VQA dataset [3].", "startOffset": 131, "endOffset": 134}, {"referenceID": 23, "context": "Our first approach to training a VQA model that emphasizes the visual information over language-priors-alone is to re-train the existing state-of-art VQA models (with code publicly available [24, 25, 9]) on our new balanced VQA dataset.", "startOffset": 191, "endOffset": 202}, {"referenceID": 24, "context": "Our first approach to training a VQA model that emphasizes the visual information over language-priors-alone is to re-train the existing state-of-art VQA models (with code publicly available [24, 25, 9]) on our new balanced VQA dataset.", "startOffset": 191, "endOffset": 202}, {"referenceID": 8, "context": "Our first approach to training a VQA model that emphasizes the visual information over language-priors-alone is to re-train the existing state-of-art VQA models (with code publicly available [24, 25, 9]) on our new balanced VQA dataset.", "startOffset": 191, "endOffset": 202}, {"referenceID": 23, "context": "We experiment with the following models: Deeper LSTM Question + norm Image (d-LSTM+nI) [24]: This was the VQA model introduced in [3] together with the dataset.", "startOffset": 87, "endOffset": 91}, {"referenceID": 2, "context": "We experiment with the following models: Deeper LSTM Question + norm Image (d-LSTM+nI) [24]: This was the VQA model introduced in [3] together with the dataset.", "startOffset": 130, "endOffset": 133}, {"referenceID": 24, "context": "Hierarchical Co-attention (HieCoAtt) [25]: This is a recent attention-based VQA model that \u2018co-attends\u2019 to both the image and the question to predict an answer.", "startOffset": 37, "endOffset": 41}, {"referenceID": 8, "context": "Multimodal Compact Bilinear Pooling (MCB) [9]: This is the winning entry on the real images track of the VQA Challenge 2016.", "startOffset": 42, "endOffset": 45}, {"referenceID": 11, "context": "It should be noted that MCB uses image features from a more powerful CNN architecture ResNet [12] while the previous two models use image features from VGGNet [37].", "startOffset": 93, "endOffset": 97}, {"referenceID": 36, "context": "It should be noted that MCB uses image features from a more powerful CNN architecture ResNet [12] while the previous two models use image features from VGGNet [37].", "startOffset": 159, "endOffset": 163}, {"referenceID": 23, "context": "Language-only: This language-only baseline has a similar architecture as Deeper LSTM Question + norm Image [24] except that it only accepts the question as input and does not utilize any visual information.", "startOffset": 107, "endOffset": 111}, {"referenceID": 23, "context": "01 d-LSTM+n-I [24] 54.", "startOffset": 14, "endOffset": 18}, {"referenceID": 24, "context": "62 HieCoAtt [25] 57.", "startOffset": 12, "endOffset": 16}, {"referenceID": 8, "context": "57 MCB [9] 60.", "startOffset": 7, "endOffset": 10}, {"referenceID": 24, "context": "For the HieCoAtt [25] model, when trained on the unbalanced dataset, 13.", "startOffset": 17, "endOffset": 21}, {"referenceID": 8, "context": "Analysis of Accuracies for Different Answer Types: We further analyze the accuracy breakdown over answer types for Multimodal Compact Bilinear Pooling (MCB) [9] and Hierarchical Co-attention (HieCoAtt) [25] models.", "startOffset": 157, "endOffset": 160}, {"referenceID": 24, "context": "Analysis of Accuracies for Different Answer Types: We further analyze the accuracy breakdown over answer types for Multimodal Compact Bilinear Pooling (MCB) [9] and Hierarchical Co-attention (HieCoAtt) [25] models.", "startOffset": 202, "endOffset": 206}, {"referenceID": 23, "context": "37 d-LSTM+n-I [24] 54.", "startOffset": 14, "endOffset": 18}, {"referenceID": 8, "context": "83 MCB [9] 62.", "startOffset": 7, "endOffset": 10}, {"referenceID": 8, "context": "MCB [9] Yes/No 81.", "startOffset": 4, "endOffset": 7}, {"referenceID": 24, "context": "HieCoAtt [25] Yes/No 79.", "startOffset": 9, "endOffset": 13}, {"referenceID": 8, "context": "Table 3: Accuracy breakdown over answer types achieved by MCB [9] and HieCoAtt [25] models when trained/tested on unbalanced/balanced VQA datasets.", "startOffset": 62, "endOffset": 65}, {"referenceID": 24, "context": "Table 3: Accuracy breakdown over answer types achieved by MCB [9] and HieCoAtt [25] models when trained/tested on unbalanced/balanced VQA datasets.", "startOffset": 79, "endOffset": 83}, {"referenceID": 29, "context": "One could add a component of question relevance [30] to identify better counter-examples.", "startOffset": 48, "endOffset": 52}, {"referenceID": 23, "context": "This gives us a joint QI embedding, similar to the model in [24].", "startOffset": 60, "endOffset": 64}, {"referenceID": 23, "context": "Similar to [24], it consists of a fullyconnected layer fed into a softmax that predicts the prob-", "startOffset": 11, "endOffset": 15}, {"referenceID": 4, "context": "To provide robustness to this potential ambiguity in the counter-example chosen by humans, in a manner similar to the ImageNet [5] top-5 evaluation metric, we evaluate our approach using the Recall@5 metric.", "startOffset": 127, "endOffset": 130}, {"referenceID": 2, "context": "Random Distance VQA [3] Ours", "startOffset": 20, "endOffset": 23}, {"referenceID": 2, "context": "In Table 4, we can see that our explanation model significantly outperforms the random baseline, as well as the VQA [3] model.", "startOffset": 116, "endOffset": 119}, {"referenceID": 2, "context": "We develop a novel data-collection interface to \u2018balance\u2019 the popular VQA dataset [3] by collecting \u2018complementary\u2019 images.", "startOffset": 82, "endOffset": 85}], "year": 2017, "abstractText": "Problems at the intersection of vision and language are of significant importance both as challenging research questions and for the rich set of applications they enable. However, inherent structure in our world and bias in our language tend to be a simpler signal for learning than visual modalities, resulting in models that ignore visual information, leading to an inflated sense of their capability. We propose to counter these language priors for the task of Visual Question Answering (VQA) and make vision (the V in VQA) matter! Specifically, we balance the popular VQA dataset [3] by collecting complementary images such that every question in our balanced dataset is associated with not just a single image, but rather a pair of similar images that result in two different answers to the question. Our dataset is by construction more balanced than the original VQA dataset and has approximately twice the number of image-question pairs. Our complete balanced dataset is publicly available at http://visualqa.org/ as part of the 2nd iteration of the Visual Question Answering Dataset and Challenge (VQA v2.0). We further benchmark a number of state-of-art VQA models on our balanced dataset. All models perform significantly worse on our balanced dataset, suggesting that these models have indeed learned to exploit language priors. This finding provides the first concrete empirical evidence for what seems to be a qualitative sense among practitioners. Finally, our data collection protocol for identifying complementary images enables us to develop a novel interpretable model, which in addition to providing an answer to the given (image, question) pair, also provides a counterexample based explanation. Specifically, it identifies an image that is similar to the original image, but it believes has a different answer to the same question. This can help in building trust for machines among their users. \u2217The first two authors contributed equally. Who is wearing glasses? Where is the child sitting? Is the umbrella upside down? How many children are in the bed? woman man arms fridge", "creator": "LaTeX with hyperref package"}}}