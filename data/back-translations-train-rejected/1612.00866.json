{"id": "1612.00866", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Dec-2016", "title": "Creating a Real-Time, Reproducible Event Dataset", "abstract": "The generation of political event data has remained much the same since the mid-1990s, both in terms of data acquisition and the process of coding text into data. Since the 1990s, however, there have been significant improvements in open-source natural language processing software and in the availability of digitized news content. This paper presents a new, next-generation event dataset, named Phoenix, that builds from these and other advances. This dataset includes improvements in the underlying news collection process and event coding software, along with the creation of a general processing pipeline necessary to produce daily-updated data. This paper provides a face validity checks by briefly examining the data for the conflict in Syria, and a comparison between Phoenix and the Integrated Crisis Early Warning System data.", "histories": [["v1", "Fri, 2 Dec 2016 21:28:00 GMT  (1033kb,D)", "http://arxiv.org/abs/1612.00866v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["john beieler"], "accepted": false, "id": "1612.00866"}, "pdf": {"name": "1612.00866.pdf", "metadata": {"source": "CRF", "title": "Creating a Real-Time, Reproducible Event Dataset", "authors": ["John Beieler"], "emails": [], "sections": [{"heading": null, "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "1 Moving Event Data Forward", "text": "Automated encoding of political event data, or recording of who-what-who has done in the context of political action, has been around for about two decades, and the approach has remained largely the same during that time, with the underlying encoding procedures not updated to reflect changes in natural language processing technology (NLP). These NLP technologies have now reached such a level, and with accompanying open source software implementations, it is clear that their inclusion in the process of event data encoding represents obvious progress. Combined with changes in the way news content is obtained, the ability to store and process large volumes of text, and improvements based on two decades of experience with event data, it is clear that political event data is ready for a next generation of datasets. In this chapter, I offer the technical details for creating such a next- generated dataset. The technical details lead to a pipeline for the production of the event-political dataset Phoenix is a real-time dataset that is being prepared during the Phoenix Dataset update."}, {"heading": "2 The History of Event Data", "text": "Political event data have existed in various forms since the 1970s. Two of the most common political event datasets were the World Event Interaction Survey (WEIS) and the Conflict and Peace Data Bank (COPDAB) (Azar 1980; McClelland 1976). These two datasets were eventually replaced by the projects created by Philip Schrodt and various collaborators. Generally, these projects were characterized by the use of Conflict and Mediation Event Observations (CAMEO) encoding ontology and automated, machine encoding instead of human encoding (Gerner, Schrodt, Yilmaz and Abu-Jabr 2001, these projects quickly being characterized by the use of ICS event observations and Mediation Event Observations (CAMEO). CAMEO's ontology consists of 20 \"top-level\" categories that include actions such as \"Make Statement\" or \"Protest,\" and contains over 200 total event classifications that serve as the basis for most modern event dataseries."}, {"heading": "3 Event Data: The Next Generation", "text": "The original datasets such as WEIS and COPDAB were created by human encoders that read messages and encode events; future datasets such as KEDS and Phil Schrodt's Levant dataset were created using automated encryption software such as KEDS or TABARI; and news stories are downloaded from content aggregators such as Lexis Nexis or Factiva. Both parts of the encryption software used a technique called flat parsing (Schrodt 2001). Shallow parsing is best underpinned as opposed to a deep encryption method. Deep parsing uses and understands the entire syntactic structure of a sentence; this syntactic structure includes things like prepositional phrases, direct and indirect objects, and other grammatical structures. However, a flat parse focuses exclusively on how the name implies."}, {"heading": "4 Building A Pipeline", "text": "In the following sections, I will outline the manifold aspects of building a near-real-time pipeline for political event data creation. First, I will explain the considerations that have fed into the architecture of the software used to generate the data; second, I will outline the various advances that have been made in data collection and processing; and finally, I will discuss the challenges and obstacles of using such a software pipeline."}, {"heading": "4.1 Considerations", "text": "Three main aspects play a role in the development of the software around the Phoenix event data pipeline: modularity, composibility and reproducibility. In short, no part of the pipeline should be hard coded enough to operate within the pipeline, which means that other parts can easily be replaced by new and / or better alternatives, and the parts should function in a way that makes the precise steps used to create the final data set transparent and understandable to those within the broader event data community. To this end, the parts of the software are modular in nature; each part can stand alone without relying on another piece of software in the stack. These modular parts result in a system that is compatible. Since parts can stand alone, parts of the system can be replaced without significantly affecting the rest of the system. Finally, the modular and composible nature of the pipeline leads to a system that is inherently reproducible."}, {"heading": "4.2 Advances", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.2.1 PETRARCH", "text": "In fact, we will find ourselves in a position to stir up the aforementioned cerebral csrc\u00fcePnln. \"We,\" he says, \"are in a position to be in.\" We, \"he says,\" are in a position to be in. \"\" We, \"he says,\" are in a position to be in. \"\" We, \"he says,\" must be in a position to be in. \"\" We, \"he says,\" must be in a position to be in. \"\" We \"n\" n \"n,\" he says, he says."}, {"heading": "4.2.2 PETRARCH2", "text": "PETRARCH2 represents another iteration of the basic principles seen in PETRARCH, especially a deep dependence on information from a syntactic parse tree. The exact operational details of PETRARCH2 lie outside the scope of this chapter, with a full explanation of the algorithm available in Norris (2016), it should suffice to say that this second version of PETRARCH makes full use of the actual structure of the parse tree to determine the coding of events. In other words, PETRARCH focused mainly on parsing noun and verb phrases, without fully integrating the syntactic information."}, {"heading": "4.2.3 Realtime News Scraping", "text": "There are several ways to scrape news content off the web."}, {"heading": "4.2.4 Geolocation", "text": "The geolocation of event data is difficult from both a technological and ontological perspective. First, from 7https: / / github.com / johnb30 / atlasan ontological point of view, it is often difficult to decide which location to choose as the site for an event. A phrase like \"Speaking from the Rose Garden, President Obama denounced Russia's actions in Syria\" offers several possible locations: the Rose Garden, Syria, and possibly even Russia. It is also possible that an event has no location. This problem relates to the \"absurdity\" of an article. In the above example, the explanatory event that President Obama denounces Russia should probably be coded as having no location. Second, the technological problems that are involved in mentioning geolocalized locations."}, {"heading": "4.2.5 An Integrated Pipeline", "text": "In order for all parts of the pipeline to communicate with each other, a comprehensive pipeline is required to successfully coordinate the various tasks. Specifically, three main components of the software / technology must communicate with each other: PETRARCH, Stanford's CoreNLP software, and the MongoDB instance. For the real-time data component, the web scraper must also fit into this system; the total flow of this pipeline is illustrated in the figure below. The modular nature of this pipeline allows different parts to run independently of each other, for example, content can be obtained and processed via CoreNLP, with the resulting parse stored in a database. This derived parse information can then be fed into PETRARCH several times after updating the underlying dictionaries or software itself. Likewise, if the scraping software requires an update or move to a different architecture, as was the case with this project, the rest of the pipeline can proceed as normal, as the other parts are agnostic as the pipeline aspect."}, {"heading": "4.3 Challenges and Obstacles", "text": "This year is the highest in the history of the country."}, {"heading": "5 Production-Ready Versions", "text": "The pipeline described above is a relatively complicated software system; the various features described, such as modularity, result in a separate system that requires many details about a large number of components. To reduce this burden, I have developed or participated in open source software tools that help provide the various components of the pipeline."}, {"heading": "5.1 EL:DIABLO", "text": "EL: DIABLO14 is essentially a script for setting up a virtual machine with each of the software components properly configured, installed and linked. This virtual machine is a \"computer within a computer\" that allows the user to accurately replicate the production pipeline used to create the daily updated Phoenix data. This virtual machine serves two purposes: firstly, it enables the fulfilment of each of the main objectives described in Section 4.1; the components can remain modular, with the entire pipeline being reproducible without each user having to maintain a knowledge of how the entire pipeline works; secondly, the script for creating the virtual machine servers serves as documentation and an example of how to use the pipeline in an environment outside of a virtual machine."}, {"heading": "5.2 hypnos", "text": "For many applications, using the entire pipeline, such as EL: DIABLO, is a drastic overkill. For example, a user wants to process a set of existing texts or insert the main event data encryption software, PETRARCH or PETRARCH2, into an existing infrastructure. To support this, hypnos15 was created to use the two minimal components for encrypting event data: the event encoder and CoreNLP. These two components are wrapped in a REST API that allows users to submit HTTP requests, wrapped as Docker16 containers for easy deployment and 14https: / github.com / openeventdata / eldiablo 15https: / / github.com / caerusassociates / hypnos 16https: / / www.docker.com / transportability of applications, so users with a single command are able to easily create a control coding around the two main events."}, {"heading": "6 The Phoenix Dataset", "text": "The Phoenix Dataset is an attempt to take both the new advances in the event data described above, along with decades of knowledge of best practices, to create a new iteration of event data. It uses 450 English-language news pages that are scraped every hour for new content. New data is generated daily, encoded according to CAMEO Event Ontology, with an average of 2,200 events generated per day. Currently, the full dataset contains 254,060 total events distributed over 102 days of generated data. Based on publicly available information, the project also uses the most up-to-date actor data of all available machine-encoded event datasetaries.17The dataset currently contains 27 columns: \"EventID,\" \"Date,\" Year, \"Month,\" \"SourceActorEntity,\" \"SourceActorAttributes,\" \"Targetole.\""}, {"heading": "6.1 Description", "text": "To get a comprehensive picture of how the data is structured over time, Figure 1 presents a time series of daily events within the Phoenix dataset. There are three interesting aspects that are presented in this figure. Firstly, the number of events generated remains relatively stable over time. Secondly, there is an obvious weekly periodicity of data with lower numbers generated at weekends. Finally, there are points where the number of events generated falls to near zero. This is either the result of server failures or software failures in the webscraper and carries the risk of maintaining real-time software. Another useful context is the sources that generate a large portion of the events. Figure 2 shows this information. The World News Network of sites19 generates most events, about one-third. This is probably due to continuous updates and content that is relevant and defective within the ontology of CAMEO."}, {"heading": "6.2 Events", "text": "As already mentioned, events are encoded on two primary dimensions: event codes and actors. Most political event datasets are dominated by low-level political events that lack strong value, which are typically routine events such as statements that occur frequently. Figures 4 and 5 show the breakdown of event types within the current Phoenix data, both of which confirm this existing pattern. The addition of category 0 four-class events has been designed so that these events can be easily removed so that end-users can easily focus on more substantive political events. Following these events at lower level 21, this is slowly changing, as shown by the work documented at https: / / github.com / openeventdata / Dictionaries / pull / 9. Issue coding based on simple keyword lookups. Figure 5 shows that the most common topic in theme coding are terrorist organizations, followed by general security topics and the European Union, \"The hope that these codifications may continue to exist for those actors to have a clear view of the events."}, {"heading": "6.3 Actors", "text": "Predictably, most of the events are state actors, with IMGMOSISI's only outlier being the actor code for the Islamic State of Iraq and the Levant. This pattern also applies to entity codes, which can be either a state code or a few other important codes such as IGOs. It is possible to further break down the actor codes to examine role codes that take into account more specific functions that a particular actor performs within a particular country, such as military or business. Figure 8 shows that the most common role codes are state actors (GOV). Following the GOV role are military (MIL) and rebel codes (REB)."}, {"heading": "6.3.1 Syria", "text": "In order to better understand how the data set works, it is helpful to extract a specific case and examine a similar set of attributes as in the previous section 000. One of the major ongoing events in the international arena during the time currently covered by the Phoenix data set is the conflict in Syria. Given this, I extract all events that contain the Syrian country code, SYR, as a SourceActorEntity or TargetActorEntity. Figure 9 shows the plot of the daily aggregated event counting. In this action, it is possible to see actions such as the beginning of the U.S. intervention against ISIS, along with other significant events within the country. As with all event dates, it is important to note that the events shown do not represent the truth of events on the ground in Syria, but instead reflect the media coverage of the events mentioned. Thus, some of the peaks and troughs are the result of media coverage of ISIL, rather than an actual shift in reality."}, {"heading": "6.4 Phoenix vs. ICEWS", "text": "This section provides a comparison between the Phoenix dataset and the Integrated Crisis Early Warning System Event Dataset. Comparison is made at both the system and data level. That is, the following sections outline the differences and similarities in the way ICEWS and Phoenix produce data and how the generated data is compared. Phoenix data, as mentioned above, extends from June 2014 to the present day. ICEWS goes further back in time, with data from 1995 onwards, but public data is subject to a one-year embargo, meaning that at the time of this writing (fall 2016) there is approximately a year and a half of overlap between the two datasets. Therefore, the charts below show comparisons only during this period. A final note refers to the existence or absence of such \"gold standard\" datasets with which the two datasets can be compared. Wang, Kennedy, Lazer and Ramakrishnan (2016) address this issue by using data sets that are open source IPA indicators, although the IPA indicators are not available."}, {"heading": "6.4.1 System", "text": "The news stream ICEWS uses is composed of (Boschee 2016): [C] summer available news sources from about 300 different publishers, including a mix of internationally (e.g. Reuters, BBC) and nationally (e.g. O Globo, Fars News Agency) focused publishers. The W-ICEWS program filters the data stream to those news sources that focus more on socio-political topics and are less likely to focus on sports or entertainment. Furthermore, the ICEWS project uses the BBN ACCENT coder. As ACCENT is an entitlement software developed by BBN, not much currently exists in the way the coder works from an algorithmic perspective."}, {"heading": "6.4.2 Data", "text": "Figure 6.4.2 shows the plot of the total daily events generated by Phoenix and ICEWS between June 2014 and the end of 2015. Overall, the two sets of data generate a remarkably similar number of events given the different source materials and coding approaches, as mentioned in the previous section. ICEWS shows more stability over time than Phoenix, with Phoenix not becoming fully stable until 2015, due to the \"beta\" nature of much of the Phoenix underlying software until more focused developer support was available in 2015. The overall correlation between the two series is.31, although this number is likely to be affected by the large variations in the Phoenix dataset. When days fall with fewer than 1,000 events, the correlation moves up to.49. Figure 18 shows a pastoral comparison of the four quadrangles, with the \"neutral\" category not affected, as shown in Table 1. The most important takeaway is that the broad trends appear broadly the same, although it is important to note that the differences between the two categories are interesting and important, the two categories are interesting. \""}, {"heading": "7 Conclusion", "text": "The combination of different technological and software advances enables a new generation of political event data that differs significantly from previous iterations. In addition to advances in accuracy and reach, the marginal cost of generating event data is now close to zero. Even with previous automated encoding efforts, human intervention has been necessary to collect and format news content. With the addition of real-time web scraping, the entire system is moving closer to a set-it-and-forget-it model. The primary interaction required when the system is running is to periodically verify that relevant content is being scratched off and that no subtle errors are causing the system to crash. While this new generation offers an improvement over previous iterations, there is still much work to be done. Deeper integration with the open source NLP software is the most important place for future work. Currently, the PETRARCH system would provide a parovalent relationship between the information provided and the actual Noush information."}], "references": [{"title": "Improving Forecasts of International Events of Interest.", "author": ["Arva", "Bryan", "John Beieler", "Ben Fisher", "Gustavo Lara", "Philip A. Schrodt", "Wonjun Song", "Marsha Sowell", "Sam Stehle"], "venue": "Paper presented at the European Political Studies Association meetings,", "citeRegEx": "Arva et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Arva et al\\.", "year": 2013}, {"title": "The Conflict and Peace Data Bank (COPDAB) Project.", "author": ["Azar", "Edward E"], "venue": "Journal of Conflict Resolution", "citeRegEx": "Azar and E.,? \\Q1980\\E", "shortCiteRegEx": "Azar and E.", "year": 1980}, {"title": "Automatic Extraction of Events from Open Source Text for Predictive Forecasting", "author": ["E. Boschee", "P. Natarajan", "R. Weischedel."], "venue": "New York, NY, USA: Springer Science Business Media.", "citeRegEx": "Boschee et al\\.,? 2013", "shortCiteRegEx": "Boschee et al\\.", "year": 2013}, {"title": "ICEWS Coded Event Data.", "author": ["Boschee", "Elizabeth", "Lautenschlager", "Jennifer", "O\u2019Brien Sean", "Shellman Steve", "Starz James", "Ward Michael"], "venue": null, "citeRegEx": "Elizabeth et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Elizabeth et al\\.", "year": 2016}, {"title": "Real time, time series forecasting of inter-and intra-state political conflict.", "author": ["Brandt", "Patrick T", "John R Freeman", "Philip A Schrodt"], "venue": "Conflict Management and Peace Science", "citeRegEx": "Brandt et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Brandt et al\\.", "year": 2011}, {"title": "Evaluating Forecasts of Political Conflict Dynamics.", "author": ["Brandt", "Patrick T", "John R Freeman", "Philip A Schrodt"], "venue": "International Journal of Forecasting", "citeRegEx": "Brandt et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Brandt et al\\.", "year": 2014}, {"title": "Getoor, Lise and Ashwin Machanavajjhala", "author": ["American Political Science Association", "Boston", "August"], "venue": "2013. Entity Res-", "citeRegEx": "Association et al\\.,? 2002", "shortCiteRegEx": "Association et al\\.", "year": 2002}, {"title": "Whither the weather? climate change", "author": ["Nils Petter"], "venue": null, "citeRegEx": "Gleditsch and Petter.,? \\Q2012\\E", "shortCiteRegEx": "Gleditsch and Petter.", "year": 2012}, {"title": "A Conflict-Cooperation Scale for WEIS Events", "author": ["Goldstein", "Joshua S"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1992}, {"title": "The Stanford CoreNLP Natural Language Processing Toolkit", "author": ["Manning", "Christopher D.", "Mihai Surdeanu", "John Bauer", "Jenny Finkel", "Steven J. Bethard", "David McClosky."], "venue": "Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations. pp. 55\u201360.", "citeRegEx": "Manning et al\\.,? 2014", "shortCiteRegEx": "Manning et al\\.", "year": 2014}, {"title": "World Event/Interaction Survey Codebook", "author": ["McClelland", "Charles A."], "venue": "Ann Arbor: Inter-University Consortium for Political and Social Research.", "citeRegEx": "McClelland and A.,? 1976", "shortCiteRegEx": "McClelland and A.", "year": 1976}, {"title": "Crisis Early Warning and Decision Support: Contemporary Approaches and Thoughts on Future Research.", "author": ["O\u2019Brien", "Sean P"], "venue": "International Studies Review", "citeRegEx": "O.Brien and P.,? \\Q2010\\E", "shortCiteRegEx": "O.Brien and P.", "year": 2010}, {"title": "The Importance of Syntactic Parsing and Inference in Semantic Role Labeling.", "author": ["Punyakanok", "Vasin", "Dan Roth", "Wen tau Yih"], "venue": null, "citeRegEx": "Punyakanok et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Punyakanok et al\\.", "year": 2008}, {"title": "Automated Coding of International Event Data Using Sparse Parsing Techniques.", "author": ["Schrodt", "Philip A"], "venue": "Paper presented at the International Studies Association, Chicago,", "citeRegEx": "Schrodt and A.,? \\Q2001\\E", "shortCiteRegEx": "Schrodt and A.", "year": 2001}, {"title": "Precedents, Progress and Prospects in Political Event Data.", "author": ["Schrodt", "Philip A"], "venue": "Interactions", "citeRegEx": "Schrodt and A.,? \\Q2012\\E", "shortCiteRegEx": "Schrodt and A.", "year": 2012}, {"title": "Coding Sub-State Actors using the CAMEO (Conflict and Mediation Event Observations) Actor Coding Framework.", "author": ["Schrodt", "Philip A", "Omur Yilmaz", "Deborah J. Gerner", "Dennis Hermrick"], "venue": "International Studies Association,", "citeRegEx": "Schrodt et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Schrodt et al\\.", "year": 2008}, {"title": "Growing pains for global monitoring of societal events.", "author": ["Wang", "Wei", "Ryan Kennedy", "David Lazer", "Naren Ramakrishnan"], "venue": "Science 353(6307):1502\u20131503", "citeRegEx": "Wang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 8, "context": "The GoldsteinScore variable is the same, standard scale used in previous datasets (Goldstein 1992).18 The final column relating to event actions is codes for Issues. These issues are based on simple keyword lookups and serve as a mechanism to add further context to a CAMEO code. For instance, a statement (CAMEO code 01) might be about a specific topic such as 18As a note, the \u201cGoldstein scale\u201d used for CAMEO-coded data is not the same as the scaled presented in Goldstein (1992), which was designed for the WEIS coding ontology.", "startOffset": 94, "endOffset": 483}, {"referenceID": 8, "context": "ICEWS reaches further back into the past, with data starting in 1995, but the public data is subject to a one-year embargo. This means that at the time of this writing (Fall 2016) there is roughly a year and a half of overlap between the two datasets. Thus, the plots below show comparisons only during this time period. A final note relates to the existence, or lack thereof, of \"gold standard\" records against which to compare the two datasets. Wang, Kennedy, Lazer and Ramakrishnan (2016) addresses this issue through the use of records coded by the IARPA Open Source Indicators (OSI) program to serve as ground truth against which to compare ICEWS and GDELT.", "startOffset": 65, "endOffset": 492}], "year": 2016, "abstractText": "The generation of political event data has remained much the same since the mid-1990s, both in terms of data acquisition and the process of coding text into data. Since the 1990s, however, there have been significant improvements in open-source natural language processing software and in the availability of digitized news content. This paper presents a new, next-generation event dataset, named Phoenix, that builds from these and other advances. This dataset includes improvements in the underlying news collection process and event coding software, along with the creation of a general processing pipeline necessary to produce daily-updated data. This paper provides a face validity checks by briefly examining the data for the conflict in Syria, and a comparison between Phoenix and the Integrated Crisis Early Warning System data. 1 Moving Event Data Forward Automated coding of political event data, or the record of who-did-what-towhom within the context of political actions, has existed for roughly two decades. The approach has remained largely the same during this time, with the underlying coding procedures not updating to reflect changes in natural language processing (NLP) technology. These NLP technologies have now advanced to such a level, and with accompanying open-source software implementations, that their inclusion in the event-data coding process comes as an obvious advancement. When combined with changes in how news content is obtained, the ability to store and process large amounts of text, and enhancements based on two decades worth of event-data experience, it becomes clear that political event data is ready for a next generation dataset. In this chapter, I provide the technical details for creating such a nextgeneration dataset. The technical details lead to a pipeline for the production of the Phoenix event dataset. The Phoenix dataset is a daily updated, nearreal-time political event dataset. The coding process makes use of open-source NLP software, an abundance of online news content, and other technical advances made possible by open-source software. This enables a dataset that is transparent and replicable, while providing a more accurate coding process than previously possible. Additionally, the dataset\u2019s near-real-time nature also enables many applications that were previously impossible with batchupdated datasets, such as monitoring of ongoing events. Thus, this dataset", "creator": "LaTeX with hyperref package"}}}