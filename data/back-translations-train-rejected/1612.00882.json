{"id": "1612.00882", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Dec-2016", "title": "Success Probability of Exploration: a Concrete Analysis of Learning Efficiency", "abstract": "Exploration has been a crucial part of reinforcement learning, yet several important questions concerning exploration efficiency are still not answered satisfactorily by existing analytical frameworks. These questions include exploration parameter setting, situation analysis, and hardness of MDPs, all of which are unavoidable for practitioners. To bridge the gap between the theory and practice, we propose a new analytical framework called the success probability of exploration. We show that those important questions of exploration above can all be answered under our framework, and the answers provided by our framework meet the needs of practitioners better than the existing ones. More importantly, we introduce a concrete and practical approach to evaluating the success probabilities in certain MDPs without the need of actually running the learning algorithm. We then provide empirical results to verify our approach, and demonstrate how the success probability of exploration can be used to analyse and predict the behaviours and possible outcomes of exploration, which are the keys to the answer of the important questions of exploration.", "histories": [["v1", "Fri, 2 Dec 2016 22:38:37 GMT  (273kb,D)", "http://arxiv.org/abs/1612.00882v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["liangpeng zhang", "ke tang", "xin yao"], "accepted": false, "id": "1612.00882"}, "pdf": {"name": "1612.00882.pdf", "metadata": {"source": "CRF", "title": "Success Probability of Exploration: a Concrete Analysis of Learning Efficiency", "authors": ["Liangpeng Zhang", "Ke Tang", "Xin Yao"], "emails": ["udars@mail.ustc.edu.cn", "ketang@ustc.edu.cn", "x.yao@cs.bham.ac.uk"], "sections": [{"heading": null, "text": "Key words: enhanced learning, exploration efficiency, learning theory, analytical framework."}, {"heading": "1. Introduction", "text": "Most RL algorithms include a specific part, often referred to as an exploration strategy, which explicitly deals with exploration strategies. Numerous exploration strategies have been designed and proposed in the literature, and some of the most popular strategies among them are E-greedy, Boltzmann selection, Explicit exploration or exploit (Kearns and Singh, 2002), R-MAX (Brafman and Tennenholtz, 2002), and Bayesian approaches (Vlassis et al., 2012).ar Xiv: 161 2.00 882v 1 [cs.L GThere are several long-standing important questions regarding the exploration of three strategies that face each strategy."}, {"heading": "2. Preliminaries", "text": "In this thesis, we follow the standard reinforcement learning framework in Sutton and Barto (1998), where an agent continuously interacts with a stochastic environment, learns its dynamic properties, and searches for the optimal policies that could lead to maximum expected cumulative rewards. However, the environment is defined here as a finite discounted MarkovDecision Process (MDP) M = (S, A, R, \u03b3 s), where S and A finite sets of states and actions each P is the transition function that exhibits optimal properties for all s, S and A (s), P (s)."}, {"heading": "3. The Success Probability of Exploration", "text": "As discussed in the last section, the observations are not usually in vain, and therefore it is crucial to know the relationship between the costs in terms of observations and the result, namely the quality of the policies derived from these observations. In this section, we formulate this cost-result relationship first by the probability of success of the exploration, then compare it with the PAC analysis, and finally highlight some elementary but useful properties of the probability of success of the exploration."}, {"heading": "3.1 Formulating the Cost-outcome Relation", "text": "The simplest representation of the costs in this context is the total number of observations (0), each resulting in an additional result (0). It is the total number of time steps (0), since the agent receives exactly one observation in each time step. Furthermore, it is also the sum of the number of visits, which can be presented as strict, a, s, s, due to the fact that the costs can be presented as a weighted sum of the number of visits. \"In some more complicated constellations, however, this case can be converted into the unweighted observations for certain state action pairs, which could be more expensive than the others. In this case, the costs can be presented as a weighted sum of the number of visits. s, aNs, a, a0) However, this case can be converted into the unweighted observations by augmenting the original MDP with sequences of trivial transitions corresponding to the weights. For example, if the weight is 1 for all state action pair 0, which is a weight of 4, it is not mathematically equivalent to (where)."}, {"heading": "3.2 Comparison to the Traditional PAC Analysis", "text": "Our formulation is inspired by the notion of the Likely Approach to Analysis may be more helpful in practice formulation and approach to investigating the likelihood of success can not be seen. (PAC, Valiant (1984); Fiechter (1994); Kakade (2003), which tries to figure out how many observations are required to actually be optimal with a probability of at least 1 \u2212 \u03b4. However, there are several significant differences between our new formulation and the existing PAC notions when it comes to amplification learning. First, the traditional PAC analysis provides results in the form of asymptotic limits to sample complexity. However, there is a large gap between the best upper and lower limits (Szita and Szepesva (ri, 2010; Lattimore and Hutter, 2014) that either the upper limit is too loose or the specific hard problem used to derive the lower limit is still not difficult enough. No matter, the current gap makes it difficult to definitively compare the efficiency between algorithms."}, {"heading": "3.3 Basic Properties of the Success Probability of Exploration", "text": "In order to provide some clearer ideas for readers about the probability of success of exploration, we present some of its elementary but useful features in this subsection. (The first problem below is the relationship between P\u03b5 and P\u03c0.Lemma 6 (First-level Decomposition) P\u03b5\u03b8, \u03c4 = \u2211 E-level.Proof Weil's planning algorithms issue only one policy at a certain time, the success events E\u03b5 and E\u03c0 \"s for any other measure cannot happen simultaneously. Therefore, we have E\u03b5 = Barcelo E-level.Proof Weil's planning algorithms issue only one policy at a certain time, the success events E\u03c0 and E\u03c0\" s for any other measure cannot take place at the same time. Therefore, we have E\u03b5 = Barcelona \"Barcelona\" Barcelona \"Barcelona\" Barcelona \"Barcelona\" Barcelona \"Barcelona, Barcelona\" Barcelona \"Barcelona\" Barcelona, \"Barcelona\" and hence Pena Weil's \"E-level.E-Barcelona\" E-Barcelona, \"Barcelona\" Barcelona \"Barcelona\" E-Barcelona, \"Barcelona\" Barcelona \"Barcelona\" E-Barcelona, \"Barcelona's\" Barcelona, \"Barcelona\" Barcelona \"E-Barcelona,\" Barcelona, \"Barcelona,\" Barcelona \"E-Barcelona,\" Barcelona, \"Barcelona,\" Barcelona, \"Barcelona\" Barcelona, \"Barcelona,\" Barcelona \"Barcelona-Barcelona,\" Barcelona, \"Barcelona,\" E-Barcelona, \""}, {"heading": "4. The Solution to the Three Groups of Questions of Exploration", "text": "Once we have established a new cost-to-earnings perspective, it is critical to examine whether this new perspective is capable of reflecting our practical needs. In the following sections, we show that our probability of success in exploration can be used to answer the three sets of questions mentioned in Section 1. To focus on this purpose, we will leave the details of our approach to the concrete assessment of the probability of success of exploration in subsequent sections. First, let us assume that we have already derived the closed expression of the PTB, the MDP M Act, and Exploration Strategy A."}, {"heading": "4.1 (Q1) Questions of Exploration Parameter Setting", "text": "The first group of questions concerns the question of how to find the most suitable parameter setting for the given learning task, knowing that a minimum number of observations is required to guarantee the error probability (1 \u2212 P\u03b8, \u03c4) that does not exceed a certain threshold (see Section 3.2). It corresponds to the question that traditional PAC analysis tries to answer (\"What parameters can the PAC algorithm be with polynomial sample complexity?\") but has not led to a satisfactory result (see Section 3.2). Therefore, the probability of success of exploration should be more productive in answering these questions. Formally, we are interested in the best parameter setting, namely the definition of such phenomena. \u2212 If more observations are made and used to build the model, then M \u2212 again is more accurate, and since the probability of success of PTB here can be regarded as a function of determination, and can be written as a minimum function of PTB."}, {"heading": "4.2 (Q2) Questions of Situation Analysis", "text": "Suppose that we have been running the learning algorithm for a while, but have not achieved success for a better situation. We then need to perform a reasonable analysis of the situation, i.e. evaluate the learning process so far to find out whether we need to increase the sample size, change the parameters of the exploration strategy presented above, or even improve the presentation of states and measures. However, the need for a situation analysis arises even if a practitioner chooses the best parameter setting with appropriate ability to decide what should be done with the method presented above - optimization. The reason for this is that setting the parameter only guarantees a probability of success, and there are still chances of failure that cannot be ignored. If failure occurs, the practitioner must perform a situation analysis to decide what should be done with the information already received, as well as the remaining resource budgets. Although the need for a situation analysis by controlling the likelihood of failure to a value of a smaller success, this may be removed from a larger need for more frequent steps."}, {"heading": "4.3 (Q3) Questions of Hardness of Exploration", "text": "The hardness of reinforcement learning tasks is not obvious, especially for the exploration part, since it is associated with uncertainty > hardness for the PMPs >. Literature has shown that some benchmarks classified as non-trivial are in fact relatively simple (Mapercard et al., 2014), while some seemingly impossible tasks are efficiently solved by more simple algorithms (e.g. Mnih et al. (2015); Silver et al. (2016). Therefore, it is crucial to develop some metrics that can be used to compare the hardness of exploration strategies and MDPs. Furthermore, this metric should be directly linked to the learning costs so that practitioners can design their budgets accordingly. Therefore, the probability of success of the exploration reflects the internal properties of MDPs and the interaction between exploration strategies and MDPs, providing an effective solution to these issues. Given two MDPs M1 and M2, their probability of success is D1, D1, DA, DA, DMP (Expectation of Hardness Strategy), \u03b8 (PMA) and PM2, which may represent the hardness strategy for some MPs."}, {"heading": "5. The Chain Perspective of MDP", "text": "The last section has shown that the probability of success of exploration provides useful answers to all three sets of questions of exploration, and obtaining the closed form expression of the probability of success is the key. Working out the closed form expression itself, on the other hand, is not a trivial task as it involves complicated interactions between the MDPs and the learning algorithms. To avoid falling into the trap of ad hoc analysis, where one has to analyze each MDP action from the ground up, we present the chain perspective of the MDP, which helps to form a generalizable basis for later discussion. In short, the chain perspective of the MDP is to abstract a more complicated MDP than a chain of MDP actions consisting of several key elements, as follows: The state is a unique starting state s1 in a chain, and the learning agent always begins interacting with the environment of this status.Goal State & Goal reward gives a chain of the chain of the target state, where the number is the chain of the number of the target state."}, {"heading": "6. Solving the Success Probability in Chain MDPs", "text": "In this section, we explain our approach to the closed expression of the probability of success in chain MDPs."}, {"heading": "6.1 The Dependency Graph of Relevant Variables", "text": "Given that the probability of success implies almost everything that is learned in the affirmation phase, it is crucial to determine the correlation of all relevant variables as a first step. The resulting dependency curve is in Figure 3. If there is a directed edge from node X to Y, it means that variable X depends on variable Y. When a variable is written in square brackets, it represents a tuple of all variables of the same kind. < E\u03c0 > denotes the tuple of the probability of success of all possible strategies. The thick arrows from probability of success E to the set of optimal strategies E and the tuple of all success events E < E\u03c0 > correspond to Lemma 6, the breakdown of the probability of success of the first step, where the probability of E events is equal to the sum of the probabilities of E events."}, {"heading": "6.2 The Prototypes of Chain MDPs", "text": "As indicated by the dependency curve, there are four critical factors involved in the probability of success of the exploration: the degree of near-optimality \u03b5, MDP M, the exploration strategy A and its parameter setting A, and the total number of steps (or observations). It is unrealistic to express the essence of the MDP (including for the chain MDPs) and the exploration strategy in only a limited number of numerical variables, so that the result of the measures derived in this paper can be extended as less than possible. This subsection focuses on the four prototypes of the chain MDPs. These prototypes should capture the key features that have critical implications for the difficulty of exploration."}, {"heading": "6.3 The Prototype Exploration Strategy", "text": "It is difficult to find a representative strategy that is able to reflect the basic characteristics of both vanilla strategies, such as: \"It is often known as\" Optimism in the face of uncertainty \"or\" The Principle of Optimism \"Kaelbling et al. (1996), which was already mentioned in the discussion on the second level of decomposition (Lemma 11). The main idea of the Optimism Principle is that the agent should adopt any state action principle as long as there is insufficient evidence against this optimistic assumption of state action strategies. This principle forces the agent to try every action several times in each state in order to ensure a basis of observations for all state action strategies visited. The Principle of Optimism is widely used among the Pita strategies MDP-MDP strategies (2003)."}, {"heading": "6.4 The Traverse Events", "text": "The reason for this is that the traverse events affect only a small fraction of the state-action pairs and are therefore easier to decide than the number of visits of all state-action pairs. In the scenario in which the optimistic prototype strategy A0 (m) is applied to the agent in the prototype chain MC's, the following dilemma can be worked out rather effortlessly: Lemma 14 For every political decision, a traverse event Eztrav occurs when and only when the agent successfully arrives at the destination."}, {"heading": "6.5 The Visit Numbers", "text": "In this section, we examine the visitor numbers < Ns, a > and < Ns, a, s \"> in light of the fact that the Traverse Event Etrav took place. < N, a, si, a\" s, a \"s, a\", \"a\", \"\" \"\" \"\", \"\" a, \"\" \",\" \"\" \"\", \"\" \"\", \"\" \"\", \"\" \"\", \"\" \"\" \",\" \"\" \",\" \"\" \",\" \"\" \",\" \"\" \"\", \"\" \"\" \",\" \"\" \"\", \"\" \",\" \",\" \"\", \"\", \"\", \"\", \"\", \",\", \"\", \",\", \"\", \",\", \"\", \",\", \",\" \",\", \",\", \"\", \"\", \",\" \",\" \",\", \"\", \"\", \"\", \",\" \"\" \"\", \",\" \"\", \"\" \",\" \"\", \"\" \"\", \",\" \"\", \"\""}, {"heading": "6.6 The Actual and Estimated State Value Functions", "text": "In this subsection, we derive the expressions of state value functions for chain prototypes. < p = > p = > p = > p (Figure 3), the actual state value functions V \u2212 p \u2212 p \u2212 s depend only on the MDP M, and therefore this is a relatively simple part compared to the others. According to the Bellman equation (see Equation 1), the actual state value V \u03c0 (s) for a state is s \u00b2 s \u00b2 s \u00b2 s as an expression for all relevant transition probabilities P (s) and rewards R (s), s \u00b2 s \u00b2, s \u00b2 s \u00b2). Therefore, the main task here is to solve the Bellman equations for all chain states s1..., sn to convert the expressions of state values into p1, pn \u2212 1, rG, and rD.After expressing the actual state values V \u00b2, it is trivial to convert them into estimated values V."}, {"heading": "6.7 Solving the Success Probabilities", "text": "We have worked out the expressions of the quantity probability P (Etrav | MC, A0 (m), \u03c4m), which apply to all quantities. < M (Etrav), A0 (m), D (n), D (m), D (m), D (n), D (n), D (n), D (n), D (n), D (n), D (n), D (n), D (n), D (n), D (n), D (n), D (n), D (n), D (n), D (n), D (n), D), D (n), D (n), D (n), D (n), D (n), D (D), D (D), D (n), D (D), D (D), D (n), D (D), D (n), D (n), D (n), D (n), D (n), D (n), D (n), D (n), D (n), D (n), D (n), D (n)."}, {"heading": "7. A Practical Approximation", "text": "Although we already have an arithmetic solution, the actual calculation process for \u03c0 \u2212 k success probabilities is still quite complicated. (For this reason, we present a useful approximation of \u03c0 \u2212 k success probabilities in this section, which is both easier to calculate and more intuitive. By examining (c) and (d) in term 25, we can find that in addition to the approximation of k = 0 in MC (G = 1), the expressions of V (sk + 1) are linear to F (k + 1). Since F'k itself takes the form of the product of the order of p'i (1 \u2212 p'i), this may inspire us to approximate the cumulative distributions of V'p'i. (sk + 1) in log-normal distributions of V'k's (sk + 1) in log-normal distributions."}, {"heading": "8. Empirical Verification", "text": "We conducted several experiments to verify our main results, notably Theorem 16 > > for the probability of the traverse event, Lemma 20 for the expected visitor numbers, and Theorem 37 for the approximation of the \u03c0 probability of success. Experiments were conducted by executing the Optimistic Prototype Strategy (OPS) in prototype chains. Therefore, when the number of time steps reaches a threshold, i.e. when the optimistic exploration ends, an additional value iteration process is executed that was collected as far as its input to decide the final spending policy. Therefore, when the number of time steps reaches a threshold, i.e. when the optimistic exploration ends, an additional value iteration process is performed in OPS that is collected as far as its input to decide the final spending policy."}, {"heading": "8.1 Verification for Traverse Probability", "text": "The first theoretical result is the expression for the traverse probability in Theorem 16. What the theorem claims is fairly simple: in all four prototype chains, the traverse probability for A0 (m) is at the top, is primarily the fact that the exploration parameters H or the target productivity G of the chains we set in all four prototypes MC with the length n = 40 and < pi > 0.3. The exploration parameters m of the OPS were set from 5 to 15. Each pair of (MC, m) was executed for 1000 times, and each time the maximum increment of 300, 000. The experimental results are shown in Figure 5."}, {"heading": "8.2 Verification for Visit Numbers", "text": "The second theoretical result to be investigated is Lemma 20, the expressions for the expected visitor numbers < N > s, a > at \u03c4m, since the traverse event occurred. < N and the third rule in Definition 17 should also be checked to see if this abstraction is able to maintain the spread of visitor numbers. < N and the third rule in Definition 17 should also be checked to see if this abstraction is able to preserve the spread of visitor numbers. < N and the third deviation is a prerequisite of these results, while the visitor numbers of the forward-looking actions are trivial m, while the visitor numbers of the forward-looking actions, N and i, have different expressions in different prototypical chains. An interesting observation from these expressions is that N + i in MC (H = 1) are invariant in position i, while in MC (H = 3) they are linear in i. If the future transitional probabilities < then pi are identical."}, {"heading": "8.3 Verification for Success Probability", "text": "According to Theorem 12, the \u03b5 probability of success P\u03b5 = > 1 verse verse is merely a simple sum of the \u03c0 probabilities of success P\u03c0 over the set of relevant policies. < M = 0.5% probabilities of success themselves are the product of the traverse probability P (E\u03c0trav) and the conditional probability P (E\u03c0trav), and the former has already been verified in Section 8.1, the latter being the main concern in this subsection. As stated in the terms of the \u03c0 \u2212 k probability of success (Lemma 28 and Lemma 29), these probabilities are equivalent to the probabilities, some of which are exceeded or exceeded by certain estimated state values V-k (sk + 1). Section 7 also provides a log-normal approximation of the cumulative distribution of V-k (sk + 1) to simplify the calculation of the theoretical probabilities of success."}, {"heading": "9. Applying the Theory", "text": "Back in Sections 6 and 7, it has been a long journey to climb the dependency graph (Figure 3) to its peak to obtain the probability of success of Exploration P. However, the actual method of extracting P is not all that complicated, as much of the previous analysis is used to justify the approach rather than being part of the necessary steps. In this section, we will show an example of how the theory proposed in this paper can be applied to more general MDPs, and then summarize the analysis process in a short practical guide."}, {"heading": "9.1 An Instance of Application to General MDPs", "text": "In this subsection, we will show that our analytical approach is applicable not only to the prototypes, but also to the general MDPs. The MDP examined in this section is shown in Figure 13 (a). It is an instance of the labyrinth, where the general objective for the agent is to reach the target from the starting point without falling into the trap. Specifically, the agent starts from the cell marked \"S.\" In each step, the agent can select one of the four directions and try to go in that direction. There is a probability that the attempt to move will be successful, and if it fails, the agent will still be in the same cell. This can be seen as an abstraction of the real world source of uncertainty, such as the imperfection of state representation due to discretion, or the imperfection of the execution of actions."}, {"heading": "9.2 A Short Practice Guide to Our Approach", "text": "The general steps for obtaining and analyzing the probability of success of the exploration are summarized as follows: \u2022 1) Abstract a chain MDP as in Section 5 and Section 9.1. \u2022 Try to keep as much of the critical features affecting the exploration strategy as possible. \u2022 It does not have to be one of the prototypes in Section 6.2, it is highly recommended to perform an abstraction of the main results in Section 6.3."}, {"heading": "10. Conclusion and Discussion", "text": "Although several analytical frameworks have been established in the literature, they generally lack the ability to satisfy practical needs. This paper is an attempt to bridge the gap under a new framework, namely the probability of success of exploration, so that practice can actually benefit from the theory, rather than simply ignoring it and relying on experience and expertise. Looking back at the previous sections, we have formulated the probability of success of exploration, presented its basic characteristics, elaborated our concrete approach to its evaluation, and confirmed our approach with empirical results. We have also shown that our novel framework does not suffer from the problems as the previous sections, and demonstrated that it can be used to comprehensively solve the three groups of questions mentioned in Section 1, although our framework and approach does not cover every problem that RL practitioners encounter every day."}, {"heading": "Acknowledgments", "text": "We would like to acknowledge that..."}], "references": [{"title": "Exploration and apprenticeship learning in reinforcement learning", "author": ["Pieter Abbeel", "Andrew Y Ng"], "venue": "In Proceedings of the 22nd international conference on Machine learning,", "citeRegEx": "Abbeel and Ng.,? \\Q2005\\E", "shortCiteRegEx": "Abbeel and Ng.", "year": 2005}, {"title": "Logarithmic online regret bounds for undiscounted reinforcement learning", "author": ["Peter Auer", "Ronald Ortner"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Auer and Ortner.,? \\Q2007\\E", "shortCiteRegEx": "Auer and Ortner.", "year": 2007}, {"title": "R-max\u2013a general polynomial time algorithm for near-optimal reinforcement learning", "author": ["Ronen I. Brafman", "Moshe Tennenholtz"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Brafman and Tennenholtz.,? \\Q2002\\E", "shortCiteRegEx": "Brafman and Tennenholtz.", "year": 2002}, {"title": "Bayesian Q-learning", "author": ["Richard Dearden", "Nir Friedman", "Stuart J. Russell"], "venue": "In Proceedings of the Fifteenth National Conference on Artificial Intelligence", "citeRegEx": "Dearden et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Dearden et al\\.", "year": 1998}, {"title": "Action-gap phenomenon in reinforcement learning", "author": ["Amir-massoud Farahmand"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Farahmand.,? \\Q2011\\E", "shortCiteRegEx": "Farahmand.", "year": 2011}, {"title": "Efficient reinforcement learning", "author": ["Claude-Nicolas Fiechter"], "venue": "In Proceedings of the seventh annual conference on Computational learning theory,", "citeRegEx": "Fiechter.,? \\Q1994\\E", "shortCiteRegEx": "Fiechter.", "year": 1994}, {"title": "Near-optimal regret bounds for reinforcement learning", "author": ["Thomas Jaksch", "Ronald Ortner", "Peter Auer"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Jaksch et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Jaksch et al\\.", "year": 2010}, {"title": "Reinforcement learning: A survey", "author": ["Leslie P. Kaelbling", "Michael L. Littman", "Andrew W. Moore"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Kaelbling et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Kaelbling et al\\.", "year": 1996}, {"title": "On the sample complexity of reinforcement learning", "author": ["Sham M. Kakade"], "venue": "PhD thesis,", "citeRegEx": "Kakade.,? \\Q2003\\E", "shortCiteRegEx": "Kakade.", "year": 2003}, {"title": "Near-optimal reinforcement learning in polynomial time", "author": ["Michael Kearns", "Satinder Singh"], "venue": "Machine Learning,", "citeRegEx": "Kearns and Singh.,? \\Q2002\\E", "shortCiteRegEx": "Kearns and Singh.", "year": 2002}, {"title": "Bandit based monte-carlo planning", "author": ["Levente Kocsis", "Csaba Szepesv\u00e1ri"], "venue": "In European conference on machine learning,", "citeRegEx": "Kocsis and Szepesv\u00e1ri.,? \\Q2006\\E", "shortCiteRegEx": "Kocsis and Szepesv\u00e1ri.", "year": 2006}, {"title": "Near-Bayesian exploration in polynomial time", "author": ["J. Zico Kolter", "Andrew Y. Ng"], "venue": "In Proceedings of the 26th International Conference on Machine Learning,", "citeRegEx": "Kolter and Ng.,? \\Q2009\\E", "shortCiteRegEx": "Kolter and Ng.", "year": 2009}, {"title": "Near-optimal PAC bounds for discounted MDPs", "author": ["Tor Lattimore", "Marcus Hutter"], "venue": "Theoretical Computer Science,", "citeRegEx": "Lattimore and Hutter.,? \\Q2014\\E", "shortCiteRegEx": "Lattimore and Hutter.", "year": 2014}, {"title": "Sample complexity bounds of exploration", "author": ["Lihong Li"], "venue": "In Reinforcement Learning,", "citeRegEx": "Li.,? \\Q2012\\E", "shortCiteRegEx": "Li.", "year": 2012}, {"title": "On the complexity of solving markov decision problems", "author": ["Michael L. Littman", "Thomas L. Dean", "Leslie P. Kaelbling"], "venue": "In Proceedings of the Eleventh conference on Uncertainty in artificial intelligence,", "citeRegEx": "Littman et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Littman et al\\.", "year": 1995}, {"title": "How hard is my MDP? The distribution-norm to the rescue", "author": ["Odalric-Ambrym Maillard", "Timothy A. Mann", "Shie Mannor"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Maillard et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Maillard et al\\.", "year": 2014}, {"title": "Human-level control through deep reinforcement learning", "author": ["Volodymyr Mnih", "Koray Kavukcuoglu", "David Silver", "Andrei A. Rusu", "Joel Veness", "Marc G. Bellemare", "Alex Graves", "Martin Riedmiller", "Andreas K. Fidjeland", "Georg Ostrovski"], "venue": "Nature, 518(7540):529\u2013533,", "citeRegEx": "Mnih et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2015}, {"title": "A note on the delta method", "author": ["Gary W. Oehlert"], "venue": "The American Statistician,", "citeRegEx": "Oehlert.,? \\Q1992\\E", "shortCiteRegEx": "Oehlert.", "year": 1992}, {"title": "Online regret bounds for undiscounted continuous reinforcement learning", "author": ["Ronald Ortner", "Daniil Ryabko"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Ortner and Ryabko.,? \\Q2012\\E", "shortCiteRegEx": "Ortner and Ryabko.", "year": 2012}, {"title": "Learning curves in machine learning. In Encyclopedia of Machine Learning, pages 577\u2013580", "author": ["Claudia Perlich"], "venue": null, "citeRegEx": "Perlich.,? \\Q2011\\E", "shortCiteRegEx": "Perlich.", "year": 2011}, {"title": "V-max: tempered optimism for better PAC reinforcement learning", "author": ["Karun Rao", "Shimon Whiteson"], "venue": "In Proceedings of the 11th International Conference on Autonomous Agents and Multiagent Systems-Volume", "citeRegEx": "Rao and Whiteson.,? \\Q2012\\E", "shortCiteRegEx": "Rao and Whiteson.", "year": 2012}, {"title": "Reinforcement learning for robot soccer", "author": ["Martin Riedmiller", "Thomas Gabel", "Roland Hafner", "Sascha Lange"], "venue": "Autonomous Robots,", "citeRegEx": "Riedmiller et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Riedmiller et al\\.", "year": 2009}, {"title": "Mastering the game of go with deep neural networks and tree", "author": ["David Silver", "Aja Huang", "Chris J. Maddison", "Arthur Guez", "Laurent Sifre", "George Van Den Driessche", "Julian Schrittwieser", "Ioannis Antonoglou", "Veda Panneershelvam", "Marc Lanctot"], "venue": "search. Nature,", "citeRegEx": "Silver et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Silver et al\\.", "year": 2016}, {"title": "An empirical evaluation of interval estimation for markov decision processes", "author": ["Alexander L. Strehl", "Michael L. Littman"], "venue": "In Tools with Artificial Intelligence (ICTAI),", "citeRegEx": "Strehl and Littman.,? \\Q2004\\E", "shortCiteRegEx": "Strehl and Littman.", "year": 2004}, {"title": "A theoretical analysis of model-based interval estimation", "author": ["Alexander L. Strehl", "Michael L. Littman"], "venue": "In Proceedings of the 22nd International Conference on Machine learning,", "citeRegEx": "Strehl and Littman.,? \\Q2005\\E", "shortCiteRegEx": "Strehl and Littman.", "year": 2005}, {"title": "Reinforcement learning in finite MDPs: PAC analysis", "author": ["Alexander L. Strehl", "Lihong Li", "Michael L. Littman"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Strehl et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Strehl et al\\.", "year": 2009}, {"title": "Introduction to Reinforcement Learning", "author": ["Richard S. Sutton", "Andrew G. Barto"], "venue": null, "citeRegEx": "Sutton and Barto.,? \\Q1998\\E", "shortCiteRegEx": "Sutton and Barto.", "year": 1998}, {"title": "Model-based reinforcement learning with nearly tight exploration complexity bounds", "author": ["Istv\u00e1n Szita", "Csaba Szepesv\u00e1ri"], "venue": "In Proceedings of the 27th International Conference on Machine Learning,", "citeRegEx": "Szita and Szepesv\u00e1ri.,? \\Q2010\\E", "shortCiteRegEx": "Szita and Szepesv\u00e1ri.", "year": 2010}, {"title": "A theory of the learnable", "author": ["Leslie G. Valiant"], "venue": "Communications of the ACM,", "citeRegEx": "Valiant.,? \\Q1984\\E", "shortCiteRegEx": "Valiant.", "year": 1984}, {"title": "Bayesian reinforcement learning", "author": ["Nikos Vlassis", "Mohammad Ghavamzadeh", "Shie Mannor", "Pascal Poupart"], "venue": "In Reinforcement Learning,", "citeRegEx": "Vlassis et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Vlassis et al\\.", "year": 2012}, {"title": "Complexity and cooperation in Q-learning", "author": ["Steven D. Whitehead"], "venue": "In Proceedings of the Eighth International Workshop on Machine Learning,", "citeRegEx": "Whitehead.,? \\Q1991\\E", "shortCiteRegEx": "Whitehead.", "year": 1991}, {"title": "Increasingly cautious optimism for practical PAC-MDP exploration", "author": ["Liangpeng Zhang", "Ke Tang", "Xin Yao"], "venue": "In Proceedings of the 24th International Joint Conference on Artificial Intelligence,", "citeRegEx": "Zhang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 26, "context": "Exploration is an essential process for Reinforcement Learning (RL) agents to resolve uncertainty (Sutton and Barto, 1998).", "startOffset": 98, "endOffset": 122}, {"referenceID": 9, "context": "Numerous exploration strategies have been designed and proposed in the literature, and some of the most popular ones among them are \u03b5-greedy, Boltzmann action selection, Explicit Explore or Exploit (Kearns and Singh, 2002), R-MAX (Brafman and Tennenholtz, 2002), Upper Confidence RL (Jaksch et al.", "startOffset": 198, "endOffset": 222}, {"referenceID": 2, "context": "Numerous exploration strategies have been designed and proposed in the literature, and some of the most popular ones among them are \u03b5-greedy, Boltzmann action selection, Explicit Explore or Exploit (Kearns and Singh, 2002), R-MAX (Brafman and Tennenholtz, 2002), Upper Confidence RL (Jaksch et al.", "startOffset": 230, "endOffset": 261}, {"referenceID": 6, "context": "Numerous exploration strategies have been designed and proposed in the literature, and some of the most popular ones among them are \u03b5-greedy, Boltzmann action selection, Explicit Explore or Exploit (Kearns and Singh, 2002), R-MAX (Brafman and Tennenholtz, 2002), Upper Confidence RL (Jaksch et al., 2010), and Bayesian approaches (Vlassis et al.", "startOffset": 283, "endOffset": 304}, {"referenceID": 29, "context": ", 2010), and Bayesian approaches (Vlassis et al., 2012).", "startOffset": 33, "endOffset": 55}, {"referenceID": 26, "context": "1 from the textbook (Sutton and Barto, 1998) will usually be tried first.", "startOffset": 20, "endOffset": 44}, {"referenceID": 28, "context": "In addition to than the straightforward approach above, the first group of questions, exploration parameter setting, are mostly investigated under the framework of PAC analysis (Valiant, 1984; Fiechter, 1994; Kakade, 2003; Strehl et al., 2009) and the regret bound analysis (Auer and Ortner, 2007; Jaksch et al.", "startOffset": 177, "endOffset": 243}, {"referenceID": 5, "context": "In addition to than the straightforward approach above, the first group of questions, exploration parameter setting, are mostly investigated under the framework of PAC analysis (Valiant, 1984; Fiechter, 1994; Kakade, 2003; Strehl et al., 2009) and the regret bound analysis (Auer and Ortner, 2007; Jaksch et al.", "startOffset": 177, "endOffset": 243}, {"referenceID": 8, "context": "In addition to than the straightforward approach above, the first group of questions, exploration parameter setting, are mostly investigated under the framework of PAC analysis (Valiant, 1984; Fiechter, 1994; Kakade, 2003; Strehl et al., 2009) and the regret bound analysis (Auer and Ortner, 2007; Jaksch et al.", "startOffset": 177, "endOffset": 243}, {"referenceID": 25, "context": "In addition to than the straightforward approach above, the first group of questions, exploration parameter setting, are mostly investigated under the framework of PAC analysis (Valiant, 1984; Fiechter, 1994; Kakade, 2003; Strehl et al., 2009) and the regret bound analysis (Auer and Ortner, 2007; Jaksch et al.", "startOffset": 177, "endOffset": 243}, {"referenceID": 1, "context": ", 2009) and the regret bound analysis (Auer and Ortner, 2007; Jaksch et al., 2010).", "startOffset": 38, "endOffset": 82}, {"referenceID": 6, "context": ", 2009) and the regret bound analysis (Auer and Ortner, 2007; Jaksch et al., 2010).", "startOffset": 38, "endOffset": 82}, {"referenceID": 27, "context": "MoR-MAX (Szita and Szepesv\u00e1ri, 2010), V-MAX (Rao and Whiteson, 2012), ICR and ICV (Zhang et al.", "startOffset": 8, "endOffset": 36}, {"referenceID": 20, "context": "MoR-MAX (Szita and Szepesv\u00e1ri, 2010), V-MAX (Rao and Whiteson, 2012), ICR and ICV (Zhang et al.", "startOffset": 44, "endOffset": 68}, {"referenceID": 31, "context": "MoR-MAX (Szita and Szepesv\u00e1ri, 2010), V-MAX (Rao and Whiteson, 2012), ICR and ICV (Zhang et al., 2015)), Model-Based Interval Estimation (Strehl and Littman, 2005), and UCRL\u03b3 (Lattimore and Hutter, 2014), have been proved to have sample complexity bounds polynomial to the scale parameters of the learning task.", "startOffset": 82, "endOffset": 102}, {"referenceID": 24, "context": ", 2015)), Model-Based Interval Estimation (Strehl and Littman, 2005), and UCRL\u03b3 (Lattimore and Hutter, 2014), have been proved to have sample complexity bounds polynomial to the scale parameters of the learning task.", "startOffset": 42, "endOffset": 68}, {"referenceID": 12, "context": ", 2015)), Model-Based Interval Estimation (Strehl and Littman, 2005), and UCRL\u03b3 (Lattimore and Hutter, 2014), have been proved to have sample complexity bounds polynomial to the scale parameters of the learning task.", "startOffset": 80, "endOffset": 108}, {"referenceID": 6, "context": "The UCRL families are also proved to have regret bounds sublinear to the horizon of the cumulative rewards (Jaksch et al., 2010; Ortner and Ryabko, 2012).", "startOffset": 107, "endOffset": 153}, {"referenceID": 18, "context": "The UCRL families are also proved to have regret bounds sublinear to the horizon of the cumulative rewards (Jaksch et al., 2010; Ortner and Ryabko, 2012).", "startOffset": 107, "endOffset": 153}, {"referenceID": 25, "context": "For example, the PAC theory for R-MAX (Strehl et al., 2009) requires its parameter m to be set polynomial to the scale parameters of the learning task so that its sample complexity can be polynomial as well.", "startOffset": 38, "endOffset": 59}, {"referenceID": 23, "context": "However, in practice m is usually fixed to some value around 10-20 (Strehl and Littman, 2004) regardless of the scale of the task, which violates the basic condition of the PAC theory.", "startOffset": 67, "endOffset": 93}, {"referenceID": 19, "context": "the plot of generalization error of the learned model against the size of the training dataset or the number of executed iterations (Perlich, 2011).", "startOffset": 132, "endOffset": 147}, {"referenceID": 26, "context": "This approach can also be applied to Reinforcement Learning by plotting the current total reward or the expected cumulative reward over time (Sutton and Barto, 1998).", "startOffset": 141, "endOffset": 165}, {"referenceID": 4, "context": "There has been some works related to the hardness questions, such as action gap (Farahmand, 2011) and distribution-norm (Maillard et al.", "startOffset": 80, "endOffset": 97}, {"referenceID": 15, "context": "There has been some works related to the hardness questions, such as action gap (Farahmand, 2011) and distribution-norm (Maillard et al., 2014).", "startOffset": 120, "endOffset": 143}, {"referenceID": 5, "context": "The UCRL families are also proved to have regret bounds sublinear to the horizon of the cumulative rewards (Jaksch et al., 2010; Ortner and Ryabko, 2012). The main drawbacks of these analyses is that their theoretical results are not sufficiently relevant to the practical needs. For example, the PAC theory for R-MAX (Strehl et al., 2009) requires its parameter m to be set polynomial to the scale parameters of the learning task so that its sample complexity can be polynomial as well. However, in practice m is usually fixed to some value around 10-20 (Strehl and Littman, 2004) regardless of the scale of the task, which violates the basic condition of the PAC theory. Meanwhile, the PAC theory does not provide any prediction of the performance of R-MAX with its m fixed to small values like 10-20. This results in a strange dilemma where practitioners have to choose one between theoretical performance guarantee and actual efficiency, and in most cases the latter is chosen, leaving the former invalid in practice. In Zhang et al. (2015), some workarounds are proposed so that the practitioners are not forced to discard theoretical guarantees in exchange for efficiency.", "startOffset": 108, "endOffset": 1045}, {"referenceID": 26, "context": "In this paper we follow the standard reinforcement learning framework in Sutton and Barto (1998), where an agent continuously interacts with a stochastic environment, learns its dynamic properties, and searches for the optimal policy that could lead to maximum expected cumulative rewards.", "startOffset": 73, "endOffset": 97}, {"referenceID": 14, "context": "Some popular planning algorithms, for example Value Iteration (Puterman, 1994), have been proved that their calculated state values converge to the true optimal values in the limit, or to the near-optimal ones in polynomial time under some assumptions (Littman et al., 1995).", "startOffset": 252, "endOffset": 274}, {"referenceID": 26, "context": "The model-free learning algorithms such as Temporal Difference and Q-Learning (Sutton and Barto, 1998), on the other hand, do not build models explicitly, but use Equations 1, 2 or their modified versions to update the estimated values directly.", "startOffset": 78, "endOffset": 102}, {"referenceID": 26, "context": "Although many reinforcement learning algorithms are guaranteed to converge to optimal policies if all state-action pairs have been visited infinitely many times (Sutton and Barto, 1998), in reality the resources for acquiring observations are far less than infinite.", "startOffset": 161, "endOffset": 185}, {"referenceID": 27, "context": "There lies a big gap between the best upper and lower bounds (Szita and Szepesv\u00e1ri, 2010; Lattimore and Hutter, 2014) been discovered.", "startOffset": 61, "endOffset": 117}, {"referenceID": 12, "context": "There lies a big gap between the best upper and lower bounds (Szita and Szepesv\u00e1ri, 2010; Lattimore and Hutter, 2014) been discovered.", "startOffset": 61, "endOffset": 117}, {"referenceID": 11, "context": "This leads to a paradoxical situation where, if one decide to set the exploration parameters according to the PAC theories, then the learning agent is very likely to over-explore as if it is in the most difficult MDP, resulting in poor actual performance despite its PAC guarantee (Kolter and Ng, 2009; Zhang et al., 2015).", "startOffset": 281, "endOffset": 322}, {"referenceID": 31, "context": "This leads to a paradoxical situation where, if one decide to set the exploration parameters according to the PAC theories, then the learning agent is very likely to over-explore as if it is in the most difficult MDP, resulting in poor actual performance despite its PAC guarantee (Kolter and Ng, 2009; Zhang et al., 2015).", "startOffset": 281, "endOffset": 322}, {"referenceID": 23, "context": "Our formulation is inspired by the notion of Probably Approximately Correct (PAC, Valiant (1984); Fiechter (1994); Kakade (2003)) which tries to figure out how many observations are needed to be \u03b5-optimal with probability at least 1 \u2212 \u03b4.", "startOffset": 82, "endOffset": 97}, {"referenceID": 5, "context": "Our formulation is inspired by the notion of Probably Approximately Correct (PAC, Valiant (1984); Fiechter (1994); Kakade (2003)) which tries to figure out how many observations are needed to be \u03b5-optimal with probability at least 1 \u2212 \u03b4.", "startOffset": 98, "endOffset": 114}, {"referenceID": 5, "context": "Our formulation is inspired by the notion of Probably Approximately Correct (PAC, Valiant (1984); Fiechter (1994); Kakade (2003)) which tries to figure out how many observations are needed to be \u03b5-optimal with probability at least 1 \u2212 \u03b4.", "startOffset": 98, "endOffset": 129}, {"referenceID": 8, "context": "The PAC optimality in Fiechter (1994) refers to a local \u03b5-optimality in the fixed start state, while in the PAC-MDP analyses (Kakade, 2003; Strehl et al., 2009) it refers to a local \u03b5-optimality along the states the agent actually visits during learning.", "startOffset": 125, "endOffset": 160}, {"referenceID": 25, "context": "The PAC optimality in Fiechter (1994) refers to a local \u03b5-optimality in the fixed start state, while in the PAC-MDP analyses (Kakade, 2003; Strehl et al., 2009) it refers to a local \u03b5-optimality along the states the agent actually visits during learning.", "startOffset": 125, "endOffset": 160}, {"referenceID": 5, "context": "The PAC optimality in Fiechter (1994) refers to a local \u03b5-optimality in the fixed start state, while in the PAC-MDP analyses (Kakade, 2003; Strehl et al.", "startOffset": 22, "endOffset": 38}, {"referenceID": 5, "context": "The PAC optimality in Fiechter (1994) refers to a local \u03b5-optimality in the fixed start state, while in the PAC-MDP analyses (Kakade, 2003; Strehl et al., 2009) it refers to a local \u03b5-optimality along the states the agent actually visits during learning. In our formulation, the output policy has to be \u03b5-optimal in all states of the MDP in order to be \u03b5-successful. Therefore, an \u03b5-success must be \u03b5-optimal in the PAC framework of Fiechter (1994) and Kakade (2003), but the converse is not necessarily correct.", "startOffset": 22, "endOffset": 449}, {"referenceID": 5, "context": "The PAC optimality in Fiechter (1994) refers to a local \u03b5-optimality in the fixed start state, while in the PAC-MDP analyses (Kakade, 2003; Strehl et al., 2009) it refers to a local \u03b5-optimality along the states the agent actually visits during learning. In our formulation, the output policy has to be \u03b5-optimal in all states of the MDP in order to be \u03b5-successful. Therefore, an \u03b5-success must be \u03b5-optimal in the PAC framework of Fiechter (1994) and Kakade (2003), but the converse is not necessarily correct.", "startOffset": 22, "endOffset": 467}, {"referenceID": 6, "context": "Additionally, this lemma provides an intuition about why the optimism principle proposed by Kaelbling et al. (1996) is so broadly accepted in designing the exploration strategies.", "startOffset": 92, "endOffset": 116}, {"referenceID": 0, "context": "Abbeel and Ng (2005); Riedmiller et al.", "startOffset": 0, "endOffset": 21}, {"referenceID": 0, "context": "Abbeel and Ng (2005); Riedmiller et al. (2009); Mnih et al.", "startOffset": 0, "endOffset": 47}, {"referenceID": 0, "context": "Abbeel and Ng (2005); Riedmiller et al. (2009); Mnih et al. (2015)), their successes are often more dependent on the state/action feature engineering, prior knowledge, the (near-)deterministic environment, and generalization techniques.", "startOffset": 0, "endOffset": 67}, {"referenceID": 15, "context": "Literature have shown that some benchmarks considered non-trivial are actually relatively easy (Maillard et al., 2014), while some seemingly impossible tasks are solved efficiently by rather simple algorithms (e.", "startOffset": 95, "endOffset": 118}, {"referenceID": 13, "context": "Literature have shown that some benchmarks considered non-trivial are actually relatively easy (Maillard et al., 2014), while some seemingly impossible tasks are solved efficiently by rather simple algorithms (e.g. Mnih et al. (2015); Silver et al.", "startOffset": 0, "endOffset": 234}, {"referenceID": 13, "context": "Literature have shown that some benchmarks considered non-trivial are actually relatively easy (Maillard et al., 2014), while some seemingly impossible tasks are solved efficiently by rather simple algorithms (e.g. Mnih et al. (2015); Silver et al. (2016)).", "startOffset": 0, "endOffset": 256}, {"referenceID": 3, "context": "Dearden et al. (1998); Strehl and Littman (2004); Kolter and Ng (2009)) is shown in Figure 1.", "startOffset": 0, "endOffset": 22}, {"referenceID": 3, "context": "Dearden et al. (1998); Strehl and Littman (2004); Kolter and Ng (2009)) is shown in Figure 1.", "startOffset": 0, "endOffset": 49}, {"referenceID": 3, "context": "Dearden et al. (1998); Strehl and Littman (2004); Kolter and Ng (2009)) is shown in Figure 1.", "startOffset": 0, "endOffset": 71}, {"referenceID": 29, "context": "In Whitehead (1991), it has been proved that in a homogeneous problem solving task, the expected number of observations required by a Q-Learning agent with an \u03b5-greedy exploration strategy to find an optimal policy is exponential to the number of steps required by the optimal policy to arrive at the goal state.", "startOffset": 3, "endOffset": 20}, {"referenceID": 13, "context": "Another expression of this theorem, proposed by Li (2012), is as follows: in a chain MDP, a Q-Learning agent with an \u03b5-greedy exploration strategy, starting from the start state, needs observations exponential to the length of the chain in order to reach the goal state.", "startOffset": 48, "endOffset": 58}, {"referenceID": 26, "context": "It is difficult to find a representative strategy that is able to reflect the fundamental properties of both vanilla strategies such as \u03b5-greedy and Boltzmann selection rule (Sutton and Barto, 1998), and the more advanced strategies.", "startOffset": 174, "endOffset": 198}, {"referenceID": 8, "context": "The optimism principle is widely adopted among PAC-MDP strategies (Kakade, 2003; Szita and Szepesv\u00e1ri, 2010; Lattimore and Hutter, 2014; Zhang et al., 2015) as well as the strategies with regret bound guarantees (Jaksch et al.", "startOffset": 66, "endOffset": 156}, {"referenceID": 27, "context": "The optimism principle is widely adopted among PAC-MDP strategies (Kakade, 2003; Szita and Szepesv\u00e1ri, 2010; Lattimore and Hutter, 2014; Zhang et al., 2015) as well as the strategies with regret bound guarantees (Jaksch et al.", "startOffset": 66, "endOffset": 156}, {"referenceID": 12, "context": "The optimism principle is widely adopted among PAC-MDP strategies (Kakade, 2003; Szita and Szepesv\u00e1ri, 2010; Lattimore and Hutter, 2014; Zhang et al., 2015) as well as the strategies with regret bound guarantees (Jaksch et al.", "startOffset": 66, "endOffset": 156}, {"referenceID": 31, "context": "The optimism principle is widely adopted among PAC-MDP strategies (Kakade, 2003; Szita and Szepesv\u00e1ri, 2010; Lattimore and Hutter, 2014; Zhang et al., 2015) as well as the strategies with regret bound guarantees (Jaksch et al.", "startOffset": 66, "endOffset": 156}, {"referenceID": 6, "context": ", 2015) as well as the strategies with regret bound guarantees (Jaksch et al., 2010).", "startOffset": 63, "endOffset": 84}, {"referenceID": 10, "context": "It is also applied in some Monte-Carlo tree search algorithms such as UCT (Kocsis and Szepesv\u00e1ri, 2006).", "startOffset": 74, "endOffset": 103}, {"referenceID": 29, "context": "There also exists a different family of non-optimistic exploration strategies, namely the Bayesian strategies (Vlassis et al., 2012).", "startOffset": 110, "endOffset": 132}, {"referenceID": 6, "context": "It is often known as \u201coptimism in the face of uncertainty\u201d or \u201cthe optimism principle\u201d Kaelbling et al. (1996), which has been already mentioned in the discussion of the second-level decomposition (Lemma 11).", "startOffset": 87, "endOffset": 111}, {"referenceID": 2, "context": "This prototype strategy is based on the famous baseline PAC-MDP strategy R-MAX (Brafman and Tennenholtz, 2002; Kakade, 2003).", "startOffset": 79, "endOffset": 124}, {"referenceID": 8, "context": "This prototype strategy is based on the famous baseline PAC-MDP strategy R-MAX (Brafman and Tennenholtz, 2002; Kakade, 2003).", "startOffset": 79, "endOffset": 124}, {"referenceID": 17, "context": "The following lemma, a special case of the delta method (Oehlert, 1992), provides a useful tool for approximating the mean and variance of functions random variables.", "startOffset": 56, "endOffset": 71}, {"referenceID": 2, "context": "The OPS was implemented based on R-MAX algorithm (Brafman and Tennenholtz, 2002; Kakade, 2003).", "startOffset": 49, "endOffset": 94}, {"referenceID": 8, "context": "The OPS was implemented based on R-MAX algorithm (Brafman and Tennenholtz, 2002; Kakade, 2003).", "startOffset": 49, "endOffset": 94}], "year": 2016, "abstractText": "Exploration has been a crucial part of reinforcement learning, yet several important questions concerning exploration efficiency are still not answered satisfactorily by existing analytical frameworks. These questions include exploration parameter setting, situation analysis, and hardness of MDPs, all of which are unavoidable for practitioners. To bridge the gap between the theory and practice, we propose a new analytical framework called the success probability of exploration. We show that those important questions of exploration above can all be answered under our framework, and the answers provided by our framework meet the needs of practitioners better than the existing ones. More importantly, we introduce a concrete and practical approach to evaluating the success probabilities in certain MDPs without the need of actually running the learning algorithm. We then provide empirical results to verify our approach, and demonstrate how the success probability of exploration can be used to analyse and predict the behaviours and possible outcomes of exploration, which are the keys to the answer of the important questions of exploration.", "creator": "LaTeX with hyperref package"}}}