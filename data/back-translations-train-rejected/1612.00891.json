{"id": "1612.00891", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Dec-2016", "title": "Parameter Compression of Recurrent Neural Networks and Degradation of Short-term Memory", "abstract": "The significant computational costs of deploying neural networks in large-scale or resource constrained environments, such as data centers and mobile devices, has spurred interest in model compression, which can achieve a reduction in both arithmetic operations and storage memory. Several techniques have been proposed for reducing or compressing the parameters for feed-forward and convolutional neural networks, but less is understood about the effect of parameter compression on recurrent neural networks (RNN). In particular, the extent to which the recurrent parameters can be compressed and the impact on short-term memory performance, is not well understood. In this paper, we study the effect of complexity reduction, through singular value decomposition rank reduction, on RNN and minimal gated recurrent unit (MGRU) networks for several tasks. We show that considerable rank reduction is possible when compressing recurrent weights, even without fine tuning. Furthermore, we propose a perturbation model for the effect of general perturbations, such as a compression, on the recurrent parameters of RNNs. The model is tested against a noiseless memorization experiment that elucidates the short-term memory performance. In this way, we demonstrate that the effect of compression of recurrent parameters is dependent on the degree of temporal coherence present in the data and task. This work can guide on-the-fly RNN compression for novel environments or tasks, and provides insight for applying RNN compression in low-power devices, such as hearing aids.", "histories": [["v1", "Fri, 2 Dec 2016 23:11:10 GMT  (1940kb)", "http://arxiv.org/abs/1612.00891v1", "Submitted to IJCNN 2017"], ["v2", "Fri, 24 Feb 2017 18:22:30 GMT  (1954kb)", "http://arxiv.org/abs/1612.00891v2", "Accepted to IJCNN 2017. Final camera ready paper"]], "COMMENTS": "Submitted to IJCNN 2017", "reviews": [], "SUBJECTS": "cs.CV cs.LG cs.NE", "authors": ["jonathan a cox"], "accepted": false, "id": "1612.00891"}, "pdf": {"name": "1612.00891.pdf", "metadata": {"source": "CRF", "title": "Parameter Compression of Recurrent Neural Networks and Degredation of Short-term Memory", "authors": ["Jonathan A. Cox"], "emails": ["joncox@alum.mit.edu"], "sections": [{"heading": null, "text": "In fact, most of them are able to survive on their own."}, {"heading": "A. Language Model", "text": "In the first experiment, we train a recursive language model to predict the next word in a sequence by minimizing cross-entropy across the entire vocabulary, as in [19]. We use the complete works of Shakespeare as the corpus for this task, and apply the Stanford Treebank Tokenizer (PTBTokenizer) library to tokenize the corpus [22]. The resulting corpus has a vocabulary of 26,430 words. All models and experiments consist of a single recursive layer and are trained with the Adam Optimizer [23], applying dropout to the hidden layer outputs. For the speech modeling task, we use a stack size of 20 (by splitting the corpus into equal parts), and train the network over continuous, ordered passes through the corpus for 30 epochs. Both the RNN and the MGRU networks have a single recursive unit replaced with the 500."}, {"heading": "B. MNIST Classifier", "text": "The second experiment is performed using a single-layer recurring MNIST classifier. In this case, the data is presented to the RNN as a 28-dimensional column vector per time step in 28 time steps. In fact, the RNN considers the image one \"scan line\" at a time and must make sense of the overall picture. Output of the recurring hidden layer, which also includes 500 units, is temporally pooled and sent to a fully networked starting layer with Softmax activation and cross-entropy loss across the 10 classes. The rank of the feed connections is a maximum of 28, which is the dimensionality of the input, while for the recurring connections it is a maximum of 500. Unlike the language model, the rank of both the feed and recurring weights is reduced by similar ratios of about 6x along the 98.5% isoline. However, as we will see from the next experiment, the degree of reduction depends on the task that can be permanently compressed in this memory model and if the duration is not decisive."}, {"heading": "C. Noiseless Memorization and Temporal Coherence", "text": "The last experiments were carried out to verify the performance scale of the fault model for short-term memory networks. < < < > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > >"}], "references": [{"title": "A Fully Convolutional Neural Network for Speech Enhancement", "author": ["S.R. Park", "J. Lee"], "venue": "ArXiv Prepr. ArXiv160907132, 2016.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2016}, {"title": "Compression of deep convolutional neural networks for fast and low power mobile applications", "author": ["Y.-D. Kim", "E. Park", "S. Yoo", "T. Choi", "L. Yang", "D. Shin"], "venue": "ArXiv Prepr. ArXiv151106530, 2015.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2015}, {"title": "SqueezeNet: AlexNet-level accuracy with 50x fewer  parameters and< 1MB model size", "author": ["F.N. Iandola", "M.W. Moskewicz", "K. Ashraf", "S. Han", "W.J. Dally", "K. Keutzer"], "venue": "ArXiv Prepr. ArXiv160207360, 2016.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2016}, {"title": "Sequence to sequence learning with neural networks", "author": ["I. Sutskever", "O. Vinyals", "Q.V. Le"], "venue": "Advances in neural information processing systems, 2014, pp. 3104\u20133112.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2014}, {"title": "Exploiting linear structure within convolutional networks for efficient evaluation", "author": ["E.L. Denton", "W. Zaremba", "J. Bruna", "Y. LeCun", "R. Fergus"], "venue": "Advances in Neural Information Processing Systems, 2014, pp. 1269\u20131277.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2014}, {"title": "Predicting parameters in deep learning", "author": ["M. Denil", "B. Shakibi", "L. Dinh", "N. de Freitas", "others"], "venue": "Advances in Neural Information Processing Systems, 2013, pp. 2148\u20132156.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2013}, {"title": "Speeding up convolutional neural networks with low rank expansions", "author": ["M. Jaderberg", "A. Vedaldi", "A. Zisserman"], "venue": "ArXiv Prepr. ArXiv14053866, 2014.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2014}, {"title": "Minimal gated unit for recurrent neural networks", "author": ["G.-B. Zhou", "J. Wu", "C.-L. Zhang", "Z.-H. Zhou"], "venue": "Int. J. Autom. Comput., vol. 13, no. 3, pp. 226\u2013234, 2016.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2016}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural Comput., vol. 9, no. 8, pp. 1735\u20131780, 1997.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1997}, {"title": "On the Properties of Neural Machine Translation: Encoder\u2013Decoder Approaches", "author": ["K. Cho", "B. van Merri\u00ebnboer", "D. Bahdanau", "Y. Bengio"], "venue": "Syntax Semant. Struct. Stat. Transl., p. 103, 2014.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2014}, {"title": "Blending LSTMs into CNNs", "author": ["K.J. Geras"], "venue": "ArXiv Prepr. ArXiv151106433, 2015.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "Model compression applied to small-footprint keyword spotting", "author": ["G. Tucker", "M. Wu", "M. Sun", "S. Panchapagesan", "G. Fu", "S. Vitaladevuni"], "venue": "Proc. Interspeech, 2016.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2016}, {"title": "Google\u2019s Neural Machine Translation System: Bridging the Gap between Human and Machine Translation", "author": ["Y. Wu"], "venue": "ArXiv Prepr. ArXiv160908144, 2016.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2016}, {"title": "Show, Attend and Tell: Neural Image Caption Generation with Visual Attention", "author": ["K. Xu"], "venue": "Proceedings of the 32nd International Conference on Machine Learning (ICML-15), 2015, pp. 2048\u20132057.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2015}, {"title": "End-to-end people detection in crowded scenes", "author": ["R. Stewart", "M. Andriluka"], "venue": "ArXiv Prepr. ArXiv150604878, 2015.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2015}, {"title": "Architectural Complexity Measures of Recurrent Neural Networks", "author": ["S. Zhang"], "venue": "ArXiv Prepr. ArXiv160208210, 2016.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2016}, {"title": "Learning recurrent neural networks with hessian-free optimization", "author": ["J. Martens", "I. Sutskever"], "venue": "Proceedings of the 28th International Conference on Machine Learning (ICML-11), 2011, pp. 1033\u20131040.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2011}, {"title": "LSTM: A search space odyssey", "author": ["K. Greff", "R.K. Srivastava", "J. Koutn\u00edk", "B.R. Steunebrink", "J. Schmidhuber"], "venue": "ArXiv Prepr. ArXiv150304069, 2015.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2015}, {"title": "Recurrent neural network regularization", "author": ["W. Zaremba", "I. Sutskever", "O. Vinyals"], "venue": "ArXiv Prepr. ArXiv14092329, 2014.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2014}, {"title": "Sparse autoencoder", "author": ["A. Ng"], "venue": "CS294A Lect. Notes, vol. 72, pp. 1\u201319, 2011.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2011}, {"title": "On the importance of initialization and momentum in deep learning", "author": ["I. Sutskever", "J. Martens", "G.E. Dahl", "G.E. Hinton"], "venue": "ICML, vol. 3, no. 28, pp. 1139\u20131147, 2013.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2013}, {"title": "Adam: A method for stochastic optimization", "author": ["D. Kingma", "J. Ba"], "venue": "ArXiv Prepr. ArXiv14126980, 2014.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 0, "context": "INTRODUCTION There has been considerable interest in the deployment of artificial neural network models for a wide range of applications, including biomedical devices [1], drones, mobile phones [2] and autonomous vehicles [3].", "startOffset": 167, "endOffset": 170}, {"referenceID": 1, "context": "INTRODUCTION There has been considerable interest in the deployment of artificial neural network models for a wide range of applications, including biomedical devices [1], drones, mobile phones [2] and autonomous vehicles [3].", "startOffset": 194, "endOffset": 197}, {"referenceID": 2, "context": "INTRODUCTION There has been considerable interest in the deployment of artificial neural network models for a wide range of applications, including biomedical devices [1], drones, mobile phones [2] and autonomous vehicles [3].", "startOffset": 222, "endOffset": 225}, {"referenceID": 2, "context": "However, such models are often extremely computationally complex, and can have hundreds of millions of parameters for both recurrent and convolutional neural networks [3], [4].", "startOffset": 167, "endOffset": 170}, {"referenceID": 3, "context": "However, such models are often extremely computationally complex, and can have hundreds of millions of parameters for both recurrent and convolutional neural networks [3], [4].", "startOffset": 172, "endOffset": 175}, {"referenceID": 4, "context": "Related methods applied to convolutional neural networks (CNN) have demonstrated a reduction of 3-5x [5]\u2013[7].", "startOffset": 101, "endOffset": 104}, {"referenceID": 6, "context": "Related methods applied to convolutional neural networks (CNN) have demonstrated a reduction of 3-5x [5]\u2013[7].", "startOffset": 105, "endOffset": 108}, {"referenceID": 7, "context": "Although there has been work investigating pruning, compression and rank reduction of feed-forward and convolutional neural networks, it is not well understood how complexity reduction impacts recurrent neural networks (RNN), and memory-cell based architectures such as the long short-term memory (LSTM), gated recurrent unit (GRU) or the recently proposed minimal gated recurrent unit (MGRU) [8]\u2013 [10].", "startOffset": 393, "endOffset": 396}, {"referenceID": 9, "context": "Although there has been work investigating pruning, compression and rank reduction of feed-forward and convolutional neural networks, it is not well understood how complexity reduction impacts recurrent neural networks (RNN), and memory-cell based architectures such as the long short-term memory (LSTM), gated recurrent unit (GRU) or the recently proposed minimal gated recurrent unit (MGRU) [8]\u2013 [10].", "startOffset": 398, "endOffset": 402}, {"referenceID": 10, "context": "[11] has investigated using model compression to train a CNN from an LSTM, it is not always possible or desirable to transform an RNN into a CNN for practical and theoretical reasons (for instance, when very long-range time dependencies are inherent in the task).", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "Nevertheless, there is significant interest in complexity reduction of RNNs, as they have witnessed large-scale adoption in industrial systems [12], [13].", "startOffset": 143, "endOffset": 147}, {"referenceID": 12, "context": "Nevertheless, there is significant interest in complexity reduction of RNNs, as they have witnessed large-scale adoption in industrial systems [12], [13].", "startOffset": 149, "endOffset": 153}, {"referenceID": 13, "context": "In general, during inference, it is often not known in advance how long an RNN must be unfolded, such as during image captioning or object detection [14], [15].", "startOffset": 149, "endOffset": 153}, {"referenceID": 14, "context": "In general, during inference, it is often not known in advance how long an RNN must be unfolded, such as during image captioning or object detection [14], [15].", "startOffset": 155, "endOffset": 159}, {"referenceID": 15, "context": "While this capability makes RNNs extremely powerful and expressive, applying and understanding complexity reduction is more challenging [16].", "startOffset": 136, "endOffset": 140}, {"referenceID": 9, "context": "However, it has recently been shown that there is redundancy in the LSTM structure, which has led to new architectures, such as the GRU [10].", "startOffset": 136, "endOffset": 140}, {"referenceID": 16, "context": "There has also been renewed interest in RNNs owing to more powerful optimization algorithms, such as Hessian-free optimization [17].", "startOffset": 127, "endOffset": 131}, {"referenceID": 17, "context": "It has been suggested that the core attribute of the LSTM is the memory cell architecture, and that comparable performance is obtained with fewer [18], and possibly even a single, flow control gate, as for the MGRU [8].", "startOffset": 146, "endOffset": 150}, {"referenceID": 7, "context": "It has been suggested that the core attribute of the LSTM is the memory cell architecture, and that comparable performance is obtained with fewer [18], and possibly even a single, flow control gate, as for the MGRU [8].", "startOffset": 215, "endOffset": 218}, {"referenceID": 18, "context": "As a result, they are especially sensitive to corruptions and perturbations, which is why performance suffers when Dropout is applied to the recurrent connections [19].", "startOffset": 163, "endOffset": 167}, {"referenceID": 19, "context": "Consider a standard RNN with tanh activation nonlinearity, which is biased near zero and has small activations, perhaps through a sparsity activation penalty [20].", "startOffset": 158, "endOffset": 162}, {"referenceID": 16, "context": "To understand the effect of a small perturbation on shortterm memory performance, consider the noiseless memorization experiment described by Martens and Sutskever [17] and shown in Fig.", "startOffset": 164, "endOffset": 168}, {"referenceID": 20, "context": "Nevertheless, there is a tradeoff between the desire to set \uf028 \uf029 1 r W \uf072 \uf03e to encourage short-term memory and to reduce the amplification of error due to a perturbation [21].", "startOffset": 168, "endOffset": 172}, {"referenceID": 13, "context": "The particular examples of image caption generation and object detection in crowded scenes illustrate this point [14], [15], since the output sequence is highly dependent on the input data found in the field.", "startOffset": 113, "endOffset": 117}, {"referenceID": 14, "context": "The particular examples of image caption generation and object detection in crowded scenes illustrate this point [14], [15], since the output sequence is highly dependent on the input data found in the field.", "startOffset": 119, "endOffset": 123}, {"referenceID": 18, "context": "Language Model In the first experiment, we train a recurrent language model to predict the next word in a sequence by minimizing the crossentropy error over the full vocabulary, as in [19].", "startOffset": 184, "endOffset": 188}, {"referenceID": 21, "context": "All models and experiments consist of a single recurrent layer and are trained using the Adam optimizer [23] with Dropout applied to the hidden layer outputs.", "startOffset": 104, "endOffset": 108}, {"referenceID": 16, "context": "The unfolded RNN for the noiseless memorization experiment as described in [17].", "startOffset": 75, "endOffset": 79}, {"referenceID": 18, "context": "Moreover, we have observed that practical models (see [19]) have even greater redundancy, and tend to be highly over-parameterized in comparison to this simplified example.", "startOffset": 54, "endOffset": 58}, {"referenceID": 16, "context": "2 [17], with 100 recurrent hidden units.", "startOffset": 2, "endOffset": 6}], "year": 2016, "abstractText": "The significant computational costs of deploying neural networks in large-scale or resource constrained environments, such as data centers and mobile devices, has spurred interest in model compression, which can achieve a reduction in both arithmetic operations and storage memory. Several techniques have been proposed for reducing or compressing the parameters for feed-forward and convolutional neural networks, but less is understood about the effect of parameter compression on recurrent neural networks (RNN). In particular, the extent to which the recurrent parameters can be compressed and the impact on short-term memory performance, is not well understood. In this paper, we study the effect of complexity reduction, through singular value decomposition rank reduction, on RNN and minimal gated recurrent unit (MGRU) networks for several tasks. We show that considerable rank reduction is possible when compressing recurrent weights, even without fine tuning. Furthermore, we propose a perturbation model for the effect of general perturbations, such as a compression, on the recurrent parameters of RNNs. The model is tested against a noiseless memorization experiment that elucidates the short-term memory performance. In this way, we demonstrate that the effect of compression of recurrent parameters is dependent on the degree of temporal coherence present in the data and task. This work can guide on-the-fly RNN compression for novel environments or tasks, and provides insight for applying RNN compression in low-power devices, such as hearing aids. Keywords\u2014compression; recurrent neural networks; complexity reduction; SVD; RNN; MGRU; MRU", "creator": null}}}