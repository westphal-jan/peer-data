{"id": "1612.00901", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Dec-2016", "title": "Commonly Uncommon: Semantic Sparsity in Situation Recognition", "abstract": "Semantic sparsity is a common challenge in structured visual classification problems; when the output space is complex, the vast majority of the possible predictions are rarely, if ever, seen in the training set. This paper studies semantic sparsity in situation recognition, the task of producing structured summaries of what is happening in images, including activities, objects and the roles objects play within the activity. For this problem, we find empirically that most object-role combinations are rare, and current state-of-the-art models significantly underperform in this sparse data regime. We avoid many such errors by (1) introducing a novel tensor composition function that learns to share examples across role-noun combinations and (2) semantically augmenting our training data with automatically gathered examples of rarely observed outputs using web data. When integrated within a complete CRF-based structured prediction model, the tensor-based approach outperforms existing state of the art by a relative improvement of 2.11% and 4.40% on top-5 verb and noun-role accuracy, respectively. Adding 5 million images with our semantic augmentation techniques gives further relative improvements of 6.23% and 9.57% on top-5 verb and noun-role accuracy.", "histories": [["v1", "Sat, 3 Dec 2016 00:31:52 GMT  (3527kb,D)", "http://arxiv.org/abs/1612.00901v1", null]], "reviews": [], "SUBJECTS": "cs.CV cs.AI", "authors": ["mark yatskar", "vicente ordonez", "luke zettlemoyer", "ali farhadi"], "accepted": false, "id": "1612.00901"}, "pdf": {"name": "1612.00901.pdf", "metadata": {"source": "CRF", "title": "Commonly Uncommon: Semantic Sparsity in Situation Recognition", "authors": ["Mark Yatskar", "Vicente Ordonez", "Luke Zettlemoyer", "Ali Farhadi"], "emails": ["ali]@cs.washington.edu,", "vicente@cs.virginia.edu"], "sections": [{"heading": "1. Introduction", "text": "In fact, most of them are able to determine for themselves what they want to do and what they want to do."}, {"heading": "2. Background", "text": "In fact, semantic role processing, in which verbs are automatically paired with their arguments in a sentence (e.g. see [8]), each semantic role corresponds to a question about an event (e.g. in the first picture of Figure 1), semantic role processing corresponds to \"Who is doing the carry?\" and agent part corresponds to \"How is the item is carried?.\" We examine the situation recognition in imSitu [44], a large-scale dataset of human annotatic situations with over 500 activities, 1700 roles, 125000 images.imSitu images are collected to detect different situations."}, {"heading": "3. Methods", "text": "In this section we present our compositional CRFs and semantic data augmentation techniques."}, {"heading": "3.1. Compositional Conditional Random Field", "text": "D (D). D (D). D (D). (D). D (D). D (D). D (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D. (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). ("}, {"heading": "3.2. Semantic Data Augmentation", "text": "Each situation can be thought of as a simple declarative sentence about an activity that takes place in an image. For example, while the first situation in Figure 1 could be expressed as \"man carrying baby on his chest outside,\" by knowing the prototypical arrangement of semantic roles around verbs and inserting prepositions, this relationship can be used to reduce the semantic sparseness by using images that can contain the elements of a situation. We convert annotated situations into phrases for semantic augmentation by enumerating all possible parts of realized situations that occur in the imSitu training scenery (see Section 4 for implementation details). In the first situation in Figure 1, for example, we obtain the phrases for semantic augmentation by fully enumerating all possible parts of realized situations that occur in the imSitu training scenery (see Section 4 for implementation details)."}, {"heading": "4. Experimental Setup", "text": "We could not afford to settle for as long as possible for a model in which we used the educational potential of our VGG network as a global representation, in which we seemed to improve the values of m and o, but were too slow to improve the results so that we blanked them out. In experiments in which we used image regression in conjunction with a compositional potential, we removed regression parameters associated with combinations seen less than 10 times in the imSitu training to compare our models to two alternative methods for effective division between nouns."}, {"heading": "5. Results", "text": "This year, it has come to the point where it only takes one year to achieve a result that is capable of proving itself."}, {"heading": "6. Related Work", "text": "Learning to handle semantic thrift is closely related to zero-shot or K-shot learning. Attribute-based learning [24, 25, 12], transmodal transfer [39, 28, 15, 26] and the use of text priorities [32, 18] have been suggested, but they are studying classification or other simplified constellations. In the structured case, picture caption models [45, 22, 7, 11, 33, 20, 35, 31] that suffer from a lack of diversity and generalization [42] have been observed. Current efforts to gain insights into such topics extract subject-verb-object (SVO) triplets from captions and count predictive errors in rare tuples [3]. Our use of imSitu to study semantic thrift deals with the need for intermedial processing of terms and generalizes to verbs with more than two arguments."}, {"heading": "7. Conclusion", "text": "We investigated situation detection, a prototypical example of a structured classification problem with significant semantic scarcity. Despite the fact that the vast majority of possible output configurations are rarely observed in the training data, we demonstrated that it was possible to introduce new compositional models that effectively share examples between required outputs and semantic data augmentation techniques that significantly improved performance. In the future, it will be important to introduce similar techniques for related problems with semantic scarcity and to generalize these ideas to zero-shot learning."}], "references": [{"title": "Neural module networks", "author": ["J. Andreas", "M. Rohrbach", "T. Darrell", "D. Klein"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2016}, {"title": "Vqa: Visual question answering", "author": ["S. Antol", "A. Agrawal", "J. Lu", "M. Mitchell", "D. Batra", "C.L. Zitnick", "D. Parikh"], "venue": "In International Conference on Computer Vision,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2015}, {"title": "Learning to generalize to new compositions in image understanding", "author": ["Y. Atzmon", "J. Berant", "V. Kezami", "A. Globerson", "G. Chechik"], "venue": "arXiv preprint arXiv:1608.07639,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2016}, {"title": "Distributional memory: A general framework for corpus-based semantics", "author": ["M. Baroni", "A. Lenci"], "venue": "Computational Linguistics,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2010}, {"title": "Webly supervised learning of convolutional networks", "author": ["X. Chen", "A. Gupta"], "venue": "In Proceedings of the IEEE International Conference on Computer Vision,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2015}, {"title": "Extracting visual knowledge from web data", "author": ["X. Chen", "A. Shrivastava", "A. Gupta. Neil"], "venue": "In Proceedings of the IEEE International Conference on Computer Vision,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2013}, {"title": "Learning a recurrent visual representation for image caption generation", "author": ["X. Chen"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2014}, {"title": "Semi-Supervised and Latent-Variable Models of Natural Language Semantics", "author": ["D. Das"], "venue": "PhD thesis,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2012}, {"title": "Exploring nearest neighbor approaches for image captioning", "author": ["J. Devlin", "S. Gupta", "R. Girshick", "M. Mitchell", "C.L. Zitnick"], "venue": "arXiv preprint arXiv:1505.04467,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2015}, {"title": "Learning everything about anything: Webly-supervised visual concept learning", "author": ["S. Divvala", "A. Farhadi", "C. Guestrin"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2014}, {"title": "From captions to visual concepts and back", "author": ["H. Fang"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2014}, {"title": "Describing objects by their attributes", "author": ["A. Farhadi"], "venue": "In CVPR,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2009}, {"title": "Background to framenet", "author": ["C.J. Fillmore"], "venue": "International Journal of lexicography,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2003}, {"title": "Semantic role labelling with neural network factors", "author": ["N. FitzGerald"], "venue": "In EMNLP,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2015}, {"title": "Devise: A deep visual-semantic embedding model", "author": ["A. Frome"], "venue": "In NIPS,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2013}, {"title": "Compact bilinear pooling", "author": ["Y. Gao", "O. Beijbom", "N. Zhang", "T. Darrell"], "venue": "arXiv preprint arXiv:1511.06062,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2015}, {"title": "Automatic labeling of semantic roles", "author": ["D. Gildea", "D. Jurafsky"], "venue": "Computational linguistics,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2002}, {"title": "Youtube2text: Recognizing and describing arbitrary activities using semantic hierarchies and zero-shot recognition", "author": ["S. Guadarrama"], "venue": "In ICCV,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2013}, {"title": "Visual semantic role labeling", "author": ["S. Gupta", "J. Malik"], "venue": "arXiv preprint arXiv:1505.04474,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2015}, {"title": "Framing image description as a ranking task: Data, models and evaluation", "author": ["M. Hodosh"], "venue": "metrics. JAIR,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2013}, {"title": "Caffe: Convolutional architecture for fast feature embedding", "author": ["Y. Jia"], "venue": null, "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2014}, {"title": "Deep visual-semantic alignments for generating image descriptions", "author": ["A. Karpathy"], "venue": null, "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2014}, {"title": "Referitgame: Referring to objects in photographs of natural scenes", "author": ["S. Kazemzadeh", "V. Ordonez", "M. Matten", "T.L. Berg"], "venue": "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2014}, {"title": "Attributebased classification for zero-shot visual object categorization", "author": ["C.H. Lampert", "H. Nickisch", "S. Harmeling"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2014}, {"title": "Learning to detect unseen object classes by between-class attribute transfer", "author": ["C.H. Lampert"], "venue": "In CVPR,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2009}, {"title": "Is this a wampimuk", "author": ["A. Lazaridou"], "venue": "In ACL,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2014}, {"title": "Low-rank tensors for scoring dependency structures", "author": ["T. Lei", "Y. Zhang", "R. Barzilay", "T. Jaakkola"], "venue": "Association for Computational Linguistics,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2014}, {"title": "Predicting deep zeroshot convolutional neural networks using textual descriptions", "author": ["J. Lei Ba", "K. Swersky", "S. Fidler"], "venue": "In Proceedings of the IEEE International Conference on Computer Vision,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2015}, {"title": "Microsoft coco: Common objects in context", "author": ["T.-Y. Lin", "M. Maire", "S. Belongie", "J. Hays", "P. Perona", "D. Ramanan", "P. Doll\u00e1r", "C.L. Zitnick"], "venue": "In European Conference on Computer Vision", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2014}, {"title": "Bilinear cnn models for fine-grained visual recognition", "author": ["T.-Y. Lin", "A. RoyChowdhury", "S. Maji"], "venue": "In Proceedings of the IEEE International Conference on Computer Vision,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2015}, {"title": "Microsoft coco: Common objects in context", "author": ["T.-Y. Lin"], "venue": "In ECCV", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2014}, {"title": "Visual relationship detection with language priors", "author": ["C. Lu", "R. Krishna", "M. Bernstein", "L. Fei-Fei"], "venue": "In European Conference on Computer Vision,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2016}, {"title": "Explain images with multimodal recurrent neural networks", "author": ["J. Mao"], "venue": null, "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2014}, {"title": "Wordnet: a lexical database for english", "author": ["G.A. Miller"], "venue": "Communications of the ACM,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 1995}, {"title": "Im2text: Describing images using 1 million captioned photographs", "author": ["V. Ordonez"], "venue": "In NIPS,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2011}, {"title": "Describing common human visual actions in images", "author": ["M. Ronchi", "P. Perona"], "venue": "In British Machine Vision Conference (BMVC),", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2015}, {"title": "Imagenet large scale visual recognition challenge", "author": ["O. Russakovsky", "J. Deng", "H. Su", "J. Krause", "S. Satheesh", "S. Ma", "Z. Huang", "A. Karpathy", "A. Khosla", "M. Bernstein"], "venue": "International Journal of Computer Vision,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2014}, {"title": "Viske: Visual knowledge extraction and question answering by visual verification of relation phrases", "author": ["F. Sadeghi", "S.K. Divvala", "A. Farhadi"], "venue": "In Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2015}, {"title": "Grounded models of semantic representation", "author": ["C. Silberer"], "venue": "In EMNLP,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2012}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "CoRR, abs/1409.1556,", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2014}, {"title": "Recursive deep models for semantic compositionality over a sentiment treebank", "author": ["R. Socher", "A. Perelygin", "J.Y. Wu", "J. Chuang", "C.D. Manning", "A.Y. Ng", "C. Potts"], "venue": "In Proceedings of the conference on empirical methods in natural language processing (EMNLP),", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2013}, {"title": "Show and tell: A neural image caption generator", "author": ["O. Vinyals"], "venue": null, "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2014}, {"title": "Grounded semantic role labeling", "author": ["S. Yang", "Q. Gao", "C. Liu", "C. Xiong", "S.-C. Zhu", "Y.J. Chai"], "venue": "In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2016}, {"title": "Situation recognition: Visual semantic role labeling for image understanding", "author": ["M. Yatskar", "L. Zettlemoyer", "A. Farhadi"], "venue": "In Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2016}, {"title": "See no evil, say no evil: Description generation from densely labeled images", "author": ["M. Yatskar"], "venue": "*SEM,", "citeRegEx": "45", "shortCiteRegEx": "45", "year": 2014}, {"title": "Simple baseline for visual question answering", "author": ["B. Zhou", "Y. Tian", "S. Sukhbaatar", "A. Szlam", "R. Fergus"], "venue": "arXiv preprint arXiv:1512.02167,", "citeRegEx": "46", "shortCiteRegEx": "46", "year": 2015}], "referenceMentions": [{"referenceID": 28, "context": "Many visual classification problems, such as image captioning [29], visual question answering [2], referring expressions [23], and situation recognition [44] have structured, semantically interpretable output spaces.", "startOffset": 62, "endOffset": 66}, {"referenceID": 1, "context": "Many visual classification problems, such as image captioning [29], visual question answering [2], referring expressions [23], and situation recognition [44] have structured, semantically interpretable output spaces.", "startOffset": 94, "endOffset": 97}, {"referenceID": 22, "context": "Many visual classification problems, such as image captioning [29], visual question answering [2], referring expressions [23], and situation recognition [44] have structured, semantically interpretable output spaces.", "startOffset": 121, "endOffset": 125}, {"referenceID": 43, "context": "Many visual classification problems, such as image captioning [29], visual question answering [2], referring expressions [23], and situation recognition [44] have structured, semantically interpretable output spaces.", "startOffset": 153, "endOffset": 157}, {"referenceID": 36, "context": "In contrast to classification tasks such as ImageNet [37], these problems typically suffer from semantic sparsity; there is a combinatorial number of possible outputs, no dataset can cover them all, and performance of existing models degrades significantly when evaluated on rare or unseen inputs [3, 46, 9, 44].", "startOffset": 53, "endOffset": 57}, {"referenceID": 2, "context": "In contrast to classification tasks such as ImageNet [37], these problems typically suffer from semantic sparsity; there is a combinatorial number of possible outputs, no dataset can cover them all, and performance of existing models degrades significantly when evaluated on rare or unseen inputs [3, 46, 9, 44].", "startOffset": 297, "endOffset": 311}, {"referenceID": 45, "context": "In contrast to classification tasks such as ImageNet [37], these problems typically suffer from semantic sparsity; there is a combinatorial number of possible outputs, no dataset can cover them all, and performance of existing models degrades significantly when evaluated on rare or unseen inputs [3, 46, 9, 44].", "startOffset": 297, "endOffset": 311}, {"referenceID": 8, "context": "In contrast to classification tasks such as ImageNet [37], these problems typically suffer from semantic sparsity; there is a combinatorial number of possible outputs, no dataset can cover them all, and performance of existing models degrades significantly when evaluated on rare or unseen inputs [3, 46, 9, 44].", "startOffset": 297, "endOffset": 311}, {"referenceID": 43, "context": "In contrast to classification tasks such as ImageNet [37], these problems typically suffer from semantic sparsity; there is a combinatorial number of possible outputs, no dataset can cover them all, and performance of existing models degrades significantly when evaluated on rare or unseen inputs [3, 46, 9, 44].", "startOffset": 297, "endOffset": 311}, {"referenceID": 43, "context": "Situation recognition [44] is the task of producing structured summaries of what is happening in images, including activities, objects and the roles those objects play within the activity.", "startOffset": 22, "endOffset": 26}, {"referenceID": 43, "context": "This is a prototypical instance of semantic sparsity: rare outputs constitute a large portion of required predictions (35% in the imSitu dataset [44], see Figure 2), and current state-of-the-art per-", "startOffset": 145, "endOffset": 149}, {"referenceID": 43, "context": "Like previous work [44], we use a deep neural network to directly predict factors in the CRF.", "startOffset": 19, "endOffset": 23}, {"referenceID": 43, "context": "Figure 3: Verb and role-noun prediction accuracy of a baseline CRF [44] on the imSitu dev set as a function of the frequency of the least observed role-noun pair in the training set.", "startOffset": 67, "endOffset": 71}, {"referenceID": 43, "context": "Experiments on the imSitu dataset [44] demonstrate that our new compositional CRF and semantic augmentation techniques reduce the effects of semantic sparsity, with strong gains for relatively rare configurations.", "startOffset": 34, "endOffset": 38}, {"referenceID": 18, "context": "Situation Recognition Situation recognition has been recently proposed to model events within images [19, 36, 43, 44], in order to answer questions beyond just \u201cWhat activity is happening?\u201d such as \u201cWho is doing it?\u201d, \u201cWhat are they doing it to?\u201d, \u201cWhat are they doing it with?\u201d.", "startOffset": 101, "endOffset": 117}, {"referenceID": 35, "context": "Situation Recognition Situation recognition has been recently proposed to model events within images [19, 36, 43, 44], in order to answer questions beyond just \u201cWhat activity is happening?\u201d such as \u201cWho is doing it?\u201d, \u201cWhat are they doing it to?\u201d, \u201cWhat are they doing it with?\u201d.", "startOffset": 101, "endOffset": 117}, {"referenceID": 42, "context": "Situation Recognition Situation recognition has been recently proposed to model events within images [19, 36, 43, 44], in order to answer questions beyond just \u201cWhat activity is happening?\u201d such as \u201cWho is doing it?\u201d, \u201cWhat are they doing it to?\u201d, \u201cWhat are they doing it with?\u201d.", "startOffset": 101, "endOffset": 117}, {"referenceID": 43, "context": "Situation Recognition Situation recognition has been recently proposed to model events within images [19, 36, 43, 44], in order to answer questions beyond just \u201cWhat activity is happening?\u201d such as \u201cWho is doing it?\u201d, \u201cWhat are they doing it to?\u201d, \u201cWhat are they doing it with?\u201d.", "startOffset": 101, "endOffset": 117}, {"referenceID": 16, "context": "In general, formulations build on semantic role labelling [17], a problem in natural language processing where verbs are automatically paired with their arguments in a sentence (for example, see [8]).", "startOffset": 58, "endOffset": 62}, {"referenceID": 7, "context": "In general, formulations build on semantic role labelling [17], a problem in natural language processing where verbs are automatically paired with their arguments in a sentence (for example, see [8]).", "startOffset": 195, "endOffset": 198}, {"referenceID": 43, "context": "We study situation recognition in imSitu [44], a largescale dataset of human annotated situations containing over 500 activities, 1,700 roles, 11,000 nouns, 125,000 images.", "startOffset": 41, "endOffset": 45}, {"referenceID": 12, "context": "The verb set V and frame set F are derived from FrameNet [13], a lexicon for semantic role labeling, while the noun set N is drawn from WordNet [34].", "startOffset": 57, "endOffset": 61}, {"referenceID": 33, "context": "The verb set V and frame set F are derived from FrameNet [13], a lexicon for semantic role labeling, while the noun set N is drawn from WordNet [34].", "startOffset": 144, "endOffset": 148}, {"referenceID": 43, "context": "Conditional Random Field Our CRF for predicting a situation, S = (v,Rf ), given an image i, decomposes over the verb v and semantic role-value pairs (e, ne) in the realized frame Rf = {(e, ne) : e \u2208 Ef}, similarly to previous work [44].", "startOffset": 231, "endOffset": 235}, {"referenceID": 39, "context": "Compositional Tensor Potential In previous work, the CRF potentials (Equation 2 and 3 ) are computed using a global image representation, a p-dimensional image vector gi \u2208 R, derived by the VGG convolutional neural network [40].", "startOffset": 223, "endOffset": 227}, {"referenceID": 20, "context": "Models All models were implemented in Caffe [21] and use a pretrained VGG network [40] for the base image representation with the final two fully connected layers replaced with two fully connected layers of dimensionality 1024.", "startOffset": 44, "endOffset": 48}, {"referenceID": 39, "context": "Models All models were implemented in Caffe [21] and use a pretrained VGG network [40] for the base image representation with the final two fully connected layers replaced with two fully connected layers of dimensionality 1024.", "startOffset": 82, "endOffset": 86}, {"referenceID": 13, "context": "The model is motivated by compositional models used for semantic role labeling [14] and allows us to trade-off the need to reduce parameters associated with nouns and expressivity.", "startOffset": 79, "endOffset": 83}, {"referenceID": 43, "context": "im Si tu 1 Baseline: Image Regression [44] 32.", "startOffset": 38, "endOffset": 42}, {"referenceID": 43, "context": "im Si tu 1 Baseline: image regression [44] 19.", "startOffset": 38, "endOffset": 42}, {"referenceID": 43, "context": "Evaluation We use the standard data split for imSitu[44] with 75k train, 25k development, and 25k test images.", "startOffset": 52, "endOffset": 56}, {"referenceID": 43, "context": "top-1 predicted verb top-5 predicted verbs ground truth verbs verb value value-all verb value value-all value value-all mean imSitu Baseline: Image Regression [44] 32.", "startOffset": 159, "endOffset": 163}, {"referenceID": 43, "context": "top-1 predicted verb top-5 predicted verbs ground truth verbs verb value value-all verb value value-all value value-all mean imSitu Baseline: Image Regression [44] 20.", "startOffset": 159, "endOffset": 163}, {"referenceID": 23, "context": "Attribute-based learning [24, 25, 12], cross-modal transfer [39, 28, 15, 26] and using text priors [32, 18] have all been proposed but they study classification or other simplified settings.", "startOffset": 25, "endOffset": 37}, {"referenceID": 24, "context": "Attribute-based learning [24, 25, 12], cross-modal transfer [39, 28, 15, 26] and using text priors [32, 18] have all been proposed but they study classification or other simplified settings.", "startOffset": 25, "endOffset": 37}, {"referenceID": 11, "context": "Attribute-based learning [24, 25, 12], cross-modal transfer [39, 28, 15, 26] and using text priors [32, 18] have all been proposed but they study classification or other simplified settings.", "startOffset": 25, "endOffset": 37}, {"referenceID": 38, "context": "Attribute-based learning [24, 25, 12], cross-modal transfer [39, 28, 15, 26] and using text priors [32, 18] have all been proposed but they study classification or other simplified settings.", "startOffset": 60, "endOffset": 76}, {"referenceID": 27, "context": "Attribute-based learning [24, 25, 12], cross-modal transfer [39, 28, 15, 26] and using text priors [32, 18] have all been proposed but they study classification or other simplified settings.", "startOffset": 60, "endOffset": 76}, {"referenceID": 14, "context": "Attribute-based learning [24, 25, 12], cross-modal transfer [39, 28, 15, 26] and using text priors [32, 18] have all been proposed but they study classification or other simplified settings.", "startOffset": 60, "endOffset": 76}, {"referenceID": 25, "context": "Attribute-based learning [24, 25, 12], cross-modal transfer [39, 28, 15, 26] and using text priors [32, 18] have all been proposed but they study classification or other simplified settings.", "startOffset": 60, "endOffset": 76}, {"referenceID": 31, "context": "Attribute-based learning [24, 25, 12], cross-modal transfer [39, 28, 15, 26] and using text priors [32, 18] have all been proposed but they study classification or other simplified settings.", "startOffset": 99, "endOffset": 107}, {"referenceID": 17, "context": "Attribute-based learning [24, 25, 12], cross-modal transfer [39, 28, 15, 26] and using text priors [32, 18] have all been proposed but they study classification or other simplified settings.", "startOffset": 99, "endOffset": 107}, {"referenceID": 44, "context": "For the structured case, image captioning models [45, 22, 7, 11, 33, 20, 35, 31] have been observed to suffer from a lack of diversity and", "startOffset": 49, "endOffset": 80}, {"referenceID": 21, "context": "For the structured case, image captioning models [45, 22, 7, 11, 33, 20, 35, 31] have been observed to suffer from a lack of diversity and", "startOffset": 49, "endOffset": 80}, {"referenceID": 6, "context": "For the structured case, image captioning models [45, 22, 7, 11, 33, 20, 35, 31] have been observed to suffer from a lack of diversity and", "startOffset": 49, "endOffset": 80}, {"referenceID": 10, "context": "For the structured case, image captioning models [45, 22, 7, 11, 33, 20, 35, 31] have been observed to suffer from a lack of diversity and", "startOffset": 49, "endOffset": 80}, {"referenceID": 32, "context": "For the structured case, image captioning models [45, 22, 7, 11, 33, 20, 35, 31] have been observed to suffer from a lack of diversity and", "startOffset": 49, "endOffset": 80}, {"referenceID": 19, "context": "For the structured case, image captioning models [45, 22, 7, 11, 33, 20, 35, 31] have been observed to suffer from a lack of diversity and", "startOffset": 49, "endOffset": 80}, {"referenceID": 34, "context": "For the structured case, image captioning models [45, 22, 7, 11, 33, 20, 35, 31] have been observed to suffer from a lack of diversity and", "startOffset": 49, "endOffset": 80}, {"referenceID": 30, "context": "For the structured case, image captioning models [45, 22, 7, 11, 33, 20, 35, 31] have been observed to suffer from a lack of diversity and", "startOffset": 49, "endOffset": 80}, {"referenceID": 41, "context": "generalization [42].", "startOffset": 15, "endOffset": 19}, {"referenceID": 2, "context": "Recent efforts to gain insight on such issues extract subject-verb-object (SVO) triplets from captions and count prediction failures on rare tuples [3].", "startOffset": 148, "endOffset": 151}, {"referenceID": 40, "context": "Compositional models have been explored in a number of applications in natural language processing, such as sentiment analysis [41], dependency parsing [27], text similarity [4], and visual question answering [1] as effective tools for combining natural language elements for prediction.", "startOffset": 127, "endOffset": 131}, {"referenceID": 26, "context": "Compositional models have been explored in a number of applications in natural language processing, such as sentiment analysis [41], dependency parsing [27], text similarity [4], and visual question answering [1] as effective tools for combining natural language elements for prediction.", "startOffset": 152, "endOffset": 156}, {"referenceID": 3, "context": "Compositional models have been explored in a number of applications in natural language processing, such as sentiment analysis [41], dependency parsing [27], text similarity [4], and visual question answering [1] as effective tools for combining natural language elements for prediction.", "startOffset": 174, "endOffset": 177}, {"referenceID": 0, "context": "Compositional models have been explored in a number of applications in natural language processing, such as sentiment analysis [41], dependency parsing [27], text similarity [4], and visual question answering [1] as effective tools for combining natural language elements for prediction.", "startOffset": 209, "endOffset": 212}, {"referenceID": 29, "context": "Recently, bilinear pooling [30] and compact bilinear pooling [16] have been proposed as second-order feature representations for tasks such as fine grained recognition and visual question answer.", "startOffset": 27, "endOffset": 31}, {"referenceID": 15, "context": "Recently, bilinear pooling [30] and compact bilinear pooling [16] have been proposed as second-order feature representations for tasks such as fine grained recognition and visual question answer.", "startOffset": 61, "endOffset": 65}, {"referenceID": 5, "context": "Using the web as a resource for image understanding has been studied through NEIL [6], a system which continuously queries for concepts discovered in text, and Levan [10], which can create detectors from user specified queries.", "startOffset": 82, "endOffset": 85}, {"referenceID": 9, "context": "Using the web as a resource for image understanding has been studied through NEIL [6], a system which continuously queries for concepts discovered in text, and Levan [10], which can create detectors from user specified queries.", "startOffset": 166, "endOffset": 170}, {"referenceID": 4, "context": "Web supervision has also been explored for pretraining convolutional neural networks [5] or for finegrained bird classification [5] and common sense reasoning [38].", "startOffset": 85, "endOffset": 88}, {"referenceID": 4, "context": "Web supervision has also been explored for pretraining convolutional neural networks [5] or for finegrained bird classification [5] and common sense reasoning [38].", "startOffset": 128, "endOffset": 131}, {"referenceID": 37, "context": "Web supervision has also been explored for pretraining convolutional neural networks [5] or for finegrained bird classification [5] and common sense reasoning [38].", "startOffset": 159, "endOffset": 163}], "year": 2016, "abstractText": "Semantic sparsity is a common challenge in structured visual classification problems; when the output space is complex, the vast majority of the possible predictions are rarely, if ever, seen in the training set. This paper studies semantic sparsity in situation recognition, the task of producing structured summaries of what is happening in images, including activities, objects and the roles objects play within the activity. For this problem, we find empirically that most object-role combinations are rare, and current state-of-the-art models significantly underperform in this sparse data regime. We avoid many such errors by (1) introducing a novel tensor composition function that learns to share examples across role-noun combinations and (2) semantically augmenting our training data with automatically gathered examples of rarely observed outputs using web data. When integrated within a complete CRF-based structured prediction model, the tensor-based approach outperforms existing state of the art by a relative improvement of 2.11% and 4.40% on top-5 verb and noun-role accuracy, respectively. Adding 5 million images with our semantic augmentation techniques gives further relative improvements of 6.23% and 9.57% on top-5 verb and noun-role accuracy.", "creator": "LaTeX with hyperref package"}}}