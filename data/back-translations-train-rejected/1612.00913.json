{"id": "1612.00913", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Dec-2016", "title": "End-to-End Joint Learning of Natural Language Understanding and Dialogue Manager", "abstract": "Natural language understanding and dialogue policy learning are both essential in conversational systems that predict the next system actions in response to a current user utterance. Conventional approaches aggregate separate models of natural language understanding (NLU) and system action prediction (SAP) as a pipeline that is sensitive to noisy outputs of error-prone NLU. To address the issues, we propose an end-to-end deep recurrent neural network with limited contextual dialogue memory by jointly training NLU and SAP on DSTC4 multi-domain human-human dialogues. Experiments show that our proposed model significantly outperforms the state-of-the-art pipeline models for both NLU and SAP, which indicates that our joint model is capable of mitigating the affects of noisy NLU outputs, and NLU model can be refined by error flows backpropagating from the extra supervised signals of system actions.", "histories": [["v1", "Sat, 3 Dec 2016 02:13:18 GMT  (103kb)", "http://arxiv.org/abs/1612.00913v1", null], ["v2", "Wed, 4 Jan 2017 08:39:09 GMT  (102kb)", "http://arxiv.org/abs/1612.00913v2", "Accepted in The 42nd IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP2017)"]], "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["xuesong yang", "yun-nung chen", "dilek hakkani-tur", "paul crook", "xiujun li", "jianfeng gao", "li deng"], "accepted": false, "id": "1612.00913"}, "pdf": {"name": "1612.00913.pdf", "metadata": {"source": "CRF", "title": "END-TO-END JOINT LEARNING OF NATURAL LANGUAGE UNDERSTANDING AND DIALOGUE MANAGER", "authors": ["Xuesong Yang", "Yun-Nung Chen", "Dilek Hakkani-T\u00fcr", "Paul Crook", "Xiujun Li", "Jianfeng Gao", "Li Deng"], "emails": [], "sections": [{"heading": null, "text": "ar Xiv: 161 2.00 913v 1 [cs.C L] 3D ecIndex Terms - Language Understanding, Spoken Dialog Systems, End-to-End, Dialog Manager, Deep Learning"}, {"heading": "1. INTRODUCTION", "text": "In fact, it is such that we are in a position to claim that we are in a position to claim that we are in a position to claim that we are in the world, that we are in a position to claim that we are in the world, that we are in the world, in which we are located, and that we are in the world, in which we are located, in which we live, in which we live, in which we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we live, that we are living, that we are living, that we live, that we are living, that we live, that we live, that"}, {"heading": "2. END-TO-END JOINT MODEL", "text": "The common model can be considered an SAP model stacked on top of a history of NLU models in Figure 3. The NLU model was designed by sharing bidirectional long-term short-term memory layers (biLSTM) with slot tagging and intent prediction."}, {"heading": "2.1. Sequence to Sequence Model with biLSTM Cells", "text": "For a sequence of input vectors x = {xt} T1, a recurrent unit H = \u2192 functions = = sequence of hidden vectors = {ht} T1, and output sequence y = {yt} T 1 by iterating the following equations, ht = H (xt, ht \u2212 1) = \u03c3 (Wxhxt + Uhhhht \u2212 1) y-sequence y (argmax (softmax (Whyht) \u2212 \u2212 \u2212 sc = zm) e zi, \u03c3 is an activation function, and Wxh, Uhh and Why are weight matrices (Wxhht \u2212 1) y-sequence (argats \u2212 t = argbit \u2212 \u2212 \u2212 s \u2212 sc \u2212 s \u2212 s \u2212 s \u2212 s \u2212 s \u2212 s \u2212 s \u2212 s \u2212 s \u2212 s \u2212 sc \u2212 s \u2212 s \u2212 s \u2212 sc \u2212 s \u2212 sc \u2212 s \u2212 s \u2212 sc \u2212 s \u2212 sc \u2212 s \u2212 s \u2212 sc \u2212 s \u2212 s \u2212 s \u2212 s \u2212 s \u2212 s \u2212 s \u2212 s \u2212 s \u2212 s \u2212 s \u2212 sc \u2212 sc \u2212 sc \u2212 s \u2212 s \u2212 s \u2212 s \u2212 s \u2212 s \u2212 s \u2212 s \u2212 s \u2212 s \u2212 s \u2212 s \u2212 s \u2212 s \u2212 s \u2212 s \u2212 s \u2212 s \u2212 s \u2212 s \u2212 s \u2212 s \u2212 s \u2212 s \u2212 s \u2212 s \u2212 s \u2212 s \u2212 s \u2212 s \u2212 s \u2212 s \u2212 s \u2212 s \u2212 s \u2212 s \u2212 s \u2212 s \u2212 s \u2212 s \u2212 s \u2212 s \u2212 s \u2212 s \u2212 s \u2212 s \u2212 s \u2212 s \u2212 s \u2212 s \u2212 s \u2212 s \u2212 s \u2212 s \u2212 s \u2212 s \u2212 s \u2212 s \u2212 s \u2212 s \u2212 s \u2212 s \u2212 s \u2212 s \u2212 s \u2212 s \u2212 s \u2212 s \u2212 s \u2212 s \u2212 s \u2212 s \u2212 s \u2212 s \u2212 s \u2212 s \u2212 s \u2212 s \u2212 s \u2212 s \u2212 s \u2212 s \u2212 s \u2212 s \u2212 s \u2212 s \u2212 s \u2212 s \u2212 s \u2212 s \u2212 s \u2212 s \u2212 s \u2212 s \u2212 s \u2212 s \u2212 s \u2212 s \u2212 s \u2212 s \u2212 s \u2212 s \u2212 s \u2212 s \u2212 s \u2212 s \u2212 s \u2212 s \u2212 s \u2212 s \u2212 s \u2212 s \u2212 s \u2212 s \u2212 s \u2212 s \u2212 s \u2212 s \u2212 s \u2212 s \u2212 s \u2212 s \u2212 s \u2212 s \u2212 s \u2212 s"}, {"heading": "2.2. Joint Modeling", "text": "The proposed common model is an RNN classifier using bidirectional LSTM cells H -1 (1) -1 (1) -1 (1) -1 (1) -1 (1) -1 (1) -1 (1) -1 (1) -1 (1) -1 (1) -1 (1) -1 (1) -1 (1) -1 (1) -1 (1) -1 (1) -1 (1) -1 (1) -1 (1) -2 (1) -1 (1) -1 (1) -1 (1) -1 (1) -1 (1) -1 (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1 (1) (1) (1) (1) (1 (1) (1) (1) (1 (1) (1) (1 (1) (1) (1) (1 (1) (1) (1 (1) (1 (1) (1) (1 (1) (1) (1 (1) (1 (1) (1 (1) (1) (1) (1 (1 (1) (1) (1 (1) (1) (1) (1 (1 (1) (1) (1) (1) (1 (1 (1) (1 (1) (1) (1) (1) (1 (1 (1) (1 (1 (1) (1) (1) (1 (1) (1) (1) (1 (1 (1) (1) (1) (1 (1) (1) (1 (1) (1 (1 (1) (1) (1) (1 (1)"}, {"heading": "3. EXPERIMENTS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1. Training Configurations", "text": "The size of each hidden recurring layer is 256 and the size of the output vector of the NLU units is M + N. We assume that the common model can only access the prehistory with I = 5. The dimension of the Word embedding is 512. The failure rate is 0.5. We apply 300 training periods and do not use an early-stop strategy. The model is evaluated in each epoch on the basis of dev sets. The best models for three tasks are individually selected from different measurements; the smallest average F1 value is used for filling slots; frame-level accuracy (it only counts if the entire frame parse is correct) is used for predicting user intentions as well as for predicting system actions whose decision thresholds are also matched to dev sets."}, {"heading": "3.2. Corpus", "text": "We select DSTC4 corpus1 for experiments. This corpus gathered the human-human dialogues of tourist information for Singapore based on Skype calls covering five areas - accommodation, attraction, food, shopping and transportation. Each tourist and guidebook tends to express itself in a series of multiple phrases. In this essay, the guide is defined as a system. We transform raw data into examples that match our experiments. Each example contains a user statement and associated slot tags in IOB format [28], user intentions and system actions as responses. Labels of system actions are defined as concatenations of categories and attributes of language files, such as QST WHEN. We also add NULL as a waiting response from guides when expressed in multiple phrases. Successive leadership actions in response to a single tourist statement are grouped as multiple labels. The entire corpus is divided into train dev / user views, table / invisible words, as in category K."}, {"heading": "3.3. Evaluation Results", "text": "We compare our proposed joint model with the following models for three tasks: slot filling, intention prediction, and system action prediction.1http: / / www.colips.org / workshop / dstc4 / data.htmlEvaluation results from end-to-end models are illustrated in Table 3. However, our proposed joint model far outperforms all other end-to-end models in terms of frame-level accuracy. The joint model and the biLSTMs pipeline achieved an absolute increase over the baseline model at 15.03% and 4.25%, respectively. Both models beat the SVMs oracle values. The biLSTMs pipeline model will be worse than the biLSTM oracle because it transfers the errors of NLU to the SAP model. Nevertheless, the joint model achieves 10.88% more than the pipeline model and 3.17% than the biLSTM oracle. These promising improvements can be interpreted as the high-level improvements of a common pipeline, requiring that the large number of secondary school units can mitigate the major drawbacks of a pipeline."}, {"heading": "4. CONCLUSION", "text": "We proposed an end-to-end deep recurrent neural network with limited contextual dialog memory that could be trained jointly by three monitored signals of user-slot filling, predicting intentions, and predicting system actions. Experiments on cross-domain human-to-human dialogues showed that our proposed model had excellent advantages in natural speech comprehension and dialogue management, achieving significantly better frame-level accuracy than the state-of-the-art technology by which pipelines separate models of NLU and SAP. Promising performance showed that contextual dialog memory made a significant contribution to the dialog manager, and highly expressive feature representations beyond the traditional aggregation of slot tags and intentions could be captured in our common model, mitigating the effects of the loud output of NLU."}], "references": [{"title": "Spoken language understanding: Systems for extracting semantic information from speech", "author": ["Gokhan T\u00fcr", "Renato De Mori"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2011}, {"title": "Pomdp-based statistical spoken dialog systems: A review", "author": ["Steve Young", "Milica Ga\u0161i\u0107", "Blaise Thomson", "Jason D Williams"], "venue": "the IEEE, vol. 101, no. 5, pp. 1160\u20131179, 2013.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2013}, {"title": "End-toend reinforcement learning of dialogue agents for information access", "author": ["Bhuwan Dhingra", "Lihong Li", "Xiujun Li", "Jianfeng Gao", "Yun-Nung Chen", "Faisal Ahmed", "Li Deng"], "venue": "arXiv preprint arXiv:1609.00777, 2016.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2016}, {"title": "Spoken language understanding", "author": ["Ye-Yi Wang", "Li Deng", "Alex Acero"], "venue": "IEEE Signal Processing Magazine, vol. 22, no. 5, pp. 16\u201331, 2005.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2005}, {"title": "Generative and discriminative algorithms for spoken language understanding", "author": ["Christian Raymond", "Giuseppe Riccardi"], "venue": "INTERSPEECH, 2007.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2007}, {"title": "Conditional random fields: Probabilistic models for segmenting and labeling sequence data", "author": ["John Lafferty", "Andrew McCallum", "Fernando Pereira"], "venue": "ICML, 2001, vol. 1, pp. 282\u2013289.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2001}, {"title": "Optimizing SVMs for complex call classification", "author": ["Patrick Haffner", "Gokhan Tur", "Jerry H Wright"], "venue": "ICASSP. IEEE, 2003, vol. 1, pp. I\u2013632.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2003}, {"title": "Speech utterance classification", "author": ["Ciprian Chelba", "Milind Mahajan", "Alex Acero"], "venue": "ICASSP, 2003.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2003}, {"title": "Deep belief nets for natural language callrouting", "author": ["Ruhi Sarikaya", "Geoffrey E Hinton", "Bhuvana Ramabhadran"], "venue": "ICASSP, 2011.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2011}, {"title": "Application of deep belief networks for natural language understanding", "author": ["Ruhi Sarikaya", "Geoffrey E Hinton", "Anoop Deoras"], "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 22, no. 4, pp. 778\u2013784, 2014.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2014}, {"title": "Knowledge as a teacher: Knowledge-guided structural attention networks", "author": ["Yun-Nung Chen", "Dilek Hakkani-Tur", "Gokhan Tur", "Asli Celikyilmaz", "Jianfeng Gao", "Li Deng"], "venue": "arXiv preprint arXiv:1609.03286, 2016.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2016}, {"title": "Syntax or semantics? knowledge-guided joint semantic frame parsing", "author": ["Yun-Nung Chen", "Dilek Hakkani-T\u00fcr", "Gokan Tur", "Asli Celikyilmaz", "Jianfeng Gao", "Li Deng"], "venue": "SLT, 2016.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2016}, {"title": "Spoken language understanding using long short-term memory neural networks", "author": ["Kaisheng Yao", "Baolin Peng", "Yu Zhang", "Dong Yu", "Geoffrey Zweig", "Yangyang Shi"], "venue": "SLT, 2014.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2014}, {"title": "Using recurrent neural networks for slot filling in spoken language understanding", "author": ["Gr\u00e9goire Mesnil", "Yann Dauphin", "Kaisheng Yao", "Yoshua Bengio", "Li Deng", "Dilek Hakkani-Tur", "Xiaodong He", "Larry Heck", "Gokhan Tur", "Dong Yu"], "venue": "IEEE/ACM Transactions on Au-  dio, Speech, and Language Processing, vol. 23, no. 3, pp. 530\u2013539, 2015.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2015}, {"title": "Recurrent conditional random field for language understanding", "author": ["Kaisheng Yao", "Baolin Peng", "Geoffrey Zweig", "Dong Yu", "Xiaolong Li", "Feng Gao"], "venue": "ICASSP, 2014.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2014}, {"title": "Contextual domain classification in spoken language understanding systems using recurrent neural network", "author": ["Puyang Xu", "Ruhi Sarikaya"], "venue": "ICASSP, 2014.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2014}, {"title": "Towards deeper understanding: Deep convex networks for semantic utterance classification", "author": ["Gokhan Tur", "Li Deng", "Dilek Hakkani-T\u00fcr", "Xiaodong He"], "venue": "ICASSP, 2012.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2012}, {"title": "Easy contextual intent prediction and slot detection", "author": ["Aditya Bhargava", "Asli Celikyilmaz", "Dilek Hakkani-T\u00fcr", "Ruhi Sarikaya"], "venue": "ICASSP, 2013.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2013}, {"title": "Convolutional neural network based triangular CRF for joint intent detection and slot filling", "author": ["Puyang Xu", "Ruhi Sarikaya"], "venue": "ASRU, 2013.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2013}, {"title": "Contextual spoken language understanding using recurrent neural networks", "author": ["Yangyang Shi", "Kaisheng Yao", "Hu Chen", "Yi-Cheng Pan", "Mei-Yuh Hwang", "Baolin Peng"], "venue": "ICASSP, 2015.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2015}, {"title": "End-to-end memory networks with knowledge carryover for multi-turn spoken language understanding", "author": ["Yun-Nung Chen", "Dilek Hakkani-T\u00fcr", "Gokhan Tur", "Jianfeng Gao", "Li Deng"], "venue": "INTERSPEECH, 2016.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2016}, {"title": "Word-based dialog state tracking with recurrent neural networks", "author": ["Matthew Henderson", "Blaise Thomson", "Steve Young"], "venue": "SIGDIAL, 2014, pp. 292\u2013299.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2014}, {"title": "On-line policy optimisation of Bayesian spoken dialogue systems via human interaction", "author": ["M Ga\u0161i\u0107", "Catherine Breslin", "Matthew Henderson", "Dongho Kim", "Martin Szummer", "Blaise Thomson", "Pirros Tsiakoulis", "Steve Young"], "venue": "ICASSP. IEEE, 2013, pp. 8367\u20138371.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2013}, {"title": "A network-based endto-end trainable task-oriented dialogue system", "author": ["Tsung-Hsien Wen", "Milica Gasic", "Nikola Mrksic", "Lina M Rojas-Barahona", "Pei-Hao Su", "Stefan Ultes", "David Vandyke", "Steve Young"], "venue": "arXiv preprint arXiv:1604.04562, 2016.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2016}, {"title": "Long shortterm memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation, vol. 9, no. 8, pp. 1735\u20131780, 1997.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 1997}, {"title": "Bidirectional recurrent neural networks", "author": ["Mike Schuster", "Kuldip K Paliwal"], "venue": "IEEE Transactions on Signal Processing, vol. 45, no. 11, pp. 2673\u20132681, 1997.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 1997}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba"], "venue": "arXiv preprint arXiv:1412.6980, 2014.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2014}, {"title": "Multi-domain joint semantic frame parsing using bidirectional RNN-LSTM", "author": ["Dilek Hakkani-T\u00fcr", "Gokhan Tur", "Asli Celikyilmaz", "Yun- Nung Chen", "Jianfeng Gao", "Li Deng", "Ye-Yi Wang"], "venue": "INTERSPEECH, 2016.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 0, "context": "NLU typically detects dialog domains by parsing user utterances followed by user intent classification and filling associated slots according to a domain-specific semantic template [1]; DM keeps monitoring the belief distribution over all possible user states underlying current user behaviors and predicts system actions as responses [2, 3].", "startOffset": 181, "endOffset": 184}, {"referenceID": 1, "context": "NLU typically detects dialog domains by parsing user utterances followed by user intent classification and filling associated slots according to a domain-specific semantic template [1]; DM keeps monitoring the belief distribution over all possible user states underlying current user behaviors and predicts system actions as responses [2, 3].", "startOffset": 335, "endOffset": 341}, {"referenceID": 2, "context": "NLU typically detects dialog domains by parsing user utterances followed by user intent classification and filling associated slots according to a domain-specific semantic template [1]; DM keeps monitoring the belief distribution over all possible user states underlying current user behaviors and predicts system actions as responses [2, 3].", "startOffset": 335, "endOffset": 341}, {"referenceID": 3, "context": "hidden Markov models (HMMs) and conditional random field (CRF) [4\u20136] are widely used in slot tagging tasks; maximum entropy and support vector machines with linear kernel (LinearSVM) [7, 8] are applied to user intent prediction.", "startOffset": 63, "endOffset": 68}, {"referenceID": 4, "context": "hidden Markov models (HMMs) and conditional random field (CRF) [4\u20136] are widely used in slot tagging tasks; maximum entropy and support vector machines with linear kernel (LinearSVM) [7, 8] are applied to user intent prediction.", "startOffset": 63, "endOffset": 68}, {"referenceID": 5, "context": "hidden Markov models (HMMs) and conditional random field (CRF) [4\u20136] are widely used in slot tagging tasks; maximum entropy and support vector machines with linear kernel (LinearSVM) [7, 8] are applied to user intent prediction.", "startOffset": 63, "endOffset": 68}, {"referenceID": 6, "context": "hidden Markov models (HMMs) and conditional random field (CRF) [4\u20136] are widely used in slot tagging tasks; maximum entropy and support vector machines with linear kernel (LinearSVM) [7, 8] are applied to user intent prediction.", "startOffset": 183, "endOffset": 189}, {"referenceID": 7, "context": "hidden Markov models (HMMs) and conditional random field (CRF) [4\u20136] are widely used in slot tagging tasks; maximum entropy and support vector machines with linear kernel (LinearSVM) [7, 8] are applied to user intent prediction.", "startOffset": 183, "endOffset": 189}, {"referenceID": 8, "context": "Deep learning techniques making incredible progress on learning expressive feature representations have achieved better solutions to NLU modeling in ATIS domain [9\u201312].", "startOffset": 161, "endOffset": 167}, {"referenceID": 9, "context": "Deep learning techniques making incredible progress on learning expressive feature representations have achieved better solutions to NLU modeling in ATIS domain [9\u201312].", "startOffset": 161, "endOffset": 167}, {"referenceID": 10, "context": "Deep learning techniques making incredible progress on learning expressive feature representations have achieved better solutions to NLU modeling in ATIS domain [9\u201312].", "startOffset": 161, "endOffset": 167}, {"referenceID": 11, "context": "Deep learning techniques making incredible progress on learning expressive feature representations have achieved better solutions to NLU modeling in ATIS domain [9\u201312].", "startOffset": 161, "endOffset": 167}, {"referenceID": 12, "context": "The performance was improved significantly by incorporating recurrent neural networks (RNN) and CRF model [13\u201315].", "startOffset": 106, "endOffset": 113}, {"referenceID": 13, "context": "The performance was improved significantly by incorporating recurrent neural networks (RNN) and CRF model [13\u201315].", "startOffset": 106, "endOffset": 113}, {"referenceID": 14, "context": "The performance was improved significantly by incorporating recurrent neural networks (RNN) and CRF model [13\u201315].", "startOffset": 106, "endOffset": 113}, {"referenceID": 15, "context": "Convolutional neural networks are also used for domain/intent classification [16, 17].", "startOffset": 77, "endOffset": 85}, {"referenceID": 16, "context": "Convolutional neural networks are also used for domain/intent classification [16, 17].", "startOffset": 77, "endOffset": 85}, {"referenceID": 17, "context": "Flexible architectures of neural networks provide a way of jointly training with intent classification and slot filling [18, 19].", "startOffset": 120, "endOffset": 128}, {"referenceID": 18, "context": "Flexible architectures of neural networks provide a way of jointly training with intent classification and slot filling [18, 19].", "startOffset": 120, "endOffset": 128}, {"referenceID": 19, "context": "Contextual information of previous queries and domain/intent prediction was also incorporated into RNN structures [20, 21].", "startOffset": 114, "endOffset": 122}, {"referenceID": 20, "context": "Contextual information of previous queries and domain/intent prediction was also incorporated into RNN structures [20, 21].", "startOffset": 114, "endOffset": 122}, {"referenceID": 1, "context": "Most successful approaches cast the dialog manager as a partially observable Markov decision process [2], which uses hand-crafted features to represent the state and action space, and requires a large number of annotated conversations [22] or human interactions [23, 24].", "startOffset": 101, "endOffset": 104}, {"referenceID": 21, "context": "Most successful approaches cast the dialog manager as a partially observable Markov decision process [2], which uses hand-crafted features to represent the state and action space, and requires a large number of annotated conversations [22] or human interactions [23, 24].", "startOffset": 235, "endOffset": 239}, {"referenceID": 22, "context": "Most successful approaches cast the dialog manager as a partially observable Markov decision process [2], which uses hand-crafted features to represent the state and action space, and requires a large number of annotated conversations [22] or human interactions [23, 24].", "startOffset": 262, "endOffset": 270}, {"referenceID": 23, "context": "Most successful approaches cast the dialog manager as a partially observable Markov decision process [2], which uses hand-crafted features to represent the state and action space, and requires a large number of annotated conversations [22] or human interactions [23, 24].", "startOffset": 262, "endOffset": 270}, {"referenceID": 24, "context": "LSTM cells are used as recurrent units because LSTM can mitigate problems of vanishing or exploding gradients in long-term dependencies via self-regularization [25].", "startOffset": 160, "endOffset": 164}, {"referenceID": 25, "context": "Since preceding and following lexical contexts are important in analysis of user utterances, bi-directional LSTM cells (biLSTM) [26] is used.", "startOffset": 128, "endOffset": 132}, {"referenceID": 26, "context": "For the training procedure, we choose a mini-batch stochastic gradient descent method Adam [27] with the batch size of 32 examples.", "startOffset": 91, "endOffset": 95}, {"referenceID": 27, "context": "Each example includes an user utterance and its associated slot tags in IOB format [28], user intents, and system actions as the responses.", "startOffset": 83, "endOffset": 87}], "year": 2016, "abstractText": "Natural language understanding and dialogue policy learning are both essential in conversational systems that predict the next system actions in response to a current user utterance. Conventional approaches aggregate separate models of natural language understanding (NLU) and system action prediction (SAP) as a pipeline that is sensitive to noisy outputs of error-prone NLU. To address the issues, we propose an end-to-end deep recurrent neural network with limited contextual dialogue memory by jointly training NLU and SAP on DSTC4 multi-domain human-human dialogues. Experiments show that our proposed model significantly outperforms the state-of-the-art pipeline models for both NLU and SAP, which indicates that our joint model is capable of mitigating the affects of noisy NLU outputs, and NLU model can be refined by error flows backpropagating from the extra supervised signals of system actions.", "creator": "LaTeX with hyperref package"}}}