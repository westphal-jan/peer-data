{"id": "1612.00959", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Dec-2016", "title": "RecSys Challenge 2016: job recommendations based on preselection of offers and gradient boosting", "abstract": "We present the Mim-Solution's approach to the RecSys Challenge 2016, which ranked 2nd. The goal of the competition was to prepare job recommendations for the users of the website Xing.com.", "histories": [["v1", "Sat, 3 Dec 2016 11:35:37 GMT  (15kb)", "http://arxiv.org/abs/1612.00959v1", "6 pages, 1 figure, 2 tables, Description of 2nd place winning solution of RecSys 2016 Challange. To be published in RecSys'16 Challange Proceedings"]], "COMMENTS": "6 pages, 1 figure, 2 tables, Description of 2nd place winning solution of RecSys 2016 Challange. To be published in RecSys'16 Challange Proceedings", "reviews": [], "SUBJECTS": "cs.AI cs.IR", "authors": ["andrzej pacuk", "piotr sankowski", "karol w\\k{e}grzycki", "adam witkowski", "piotr wygocki"], "accepted": false, "id": "1612.00959"}, "pdf": {"name": "1612.00959.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["apacuk@mimuw.edu.pl", "sank@mimuw.edu.pl", "k.wegrzycki@mimuw.edu.pl", "a.witkowski@mimuw.edu.pl", "wygos@mimuw.edu.pl"], "sections": [{"heading": null, "text": "ar Xiv: 161 2.00 959v 1 [cs.A I] 3 Dec 201 6We present the approach of the Mim-Solution for the RecSys Challenge 2016, which took second place. The aim of the competition was to prepare job recommendations for the users of the website Xing.com. Our two-phase algorithm consists of candidate selection followed by candidate ranking. We evaluated the candidates according to the predicted probability that the user interacts positively with the job offer. We used Gradient Boosting Decision Trees as a regression tool."}, {"heading": "1 Introduction", "text": "There are several ways a user can interact with a job offer (keyword): by clicking, as well as by bookmarking interesting information that responds to offers and finally deletes a recommendation. The task in the challenge was to predict for a given XING user 30 points in which that user will positively interact with a job offer (keyword).Dataset The dataset consisted of user characteristics and articles, user interactions (clicked by users), user interactions (clicked by users), user interactions (clicked by users), user interactions (clicked by users), user interactions (clicked by users), user interactions (clicked by users).Dataset The dataset consisted of user characteristics (clicked by users), user interactions (clicked by users), user interactions (clicked by users), user (clicked by users), user (clicked by users)."}, {"heading": "2 Our solution", "text": "Our solution consists of two parts: 1. For each user, we calculate a set of candidate items that are much smaller than the entire group I (when creating a template, only those candidates were considered); 2. For each (user, candidate) pair (u, i), we learn the probability that the user will interact with that item. We submitted 30 items for each user with the highest predicted probability of interaction, without items that the user has deleted. To determine the probabilities P [i] G (u)], we used Gradient Boosting Decision Trees (GBDT) to significantly reduce the time required to train a model and prepare a template. Testing all possible user item pairs (150,000 x 327,000 pairs) was not feasible considering our resources."}, {"heading": "2.1 Training set", "text": "Since our task was to predict user interactions in the week following the end of the available data, we trained our model on all data except the last available week and used that data in the last week to calculate the Truth G training site to evaluate the model, allowing us to calculate the score and determine whether we are making progress without submitting an official submitter.There was an overlap between the training data and the test data (the full data set).Both candidates and features were calculated separately for the training set and the full dataset.3https: / / github.com / dmlc / xgboost"}, {"heading": "2.2 Candidate items selection", "text": "To solve this problem, we selected a number of promising elements for each user, which we called candidates. To define candidates, we need a few terms of similarity. The Jaccard coefficient between two groups A and B is J (A, B). The Jaccard coefficient between two groups A and B (A, B) is the user coefficient between the groups of elements from their positive interaction (impressions). For example, Int-sim (u, u) = J (Intu, u) = Int-sim (u, u \") (Imp-sim (u, u\") is the user coefficient between groups of elements from their positive interaction (impressions)."}, {"heading": "2.3 Learning the probabilities", "text": "We wanted to construct a model that calculated the probability P [i-G (u)] by specifying a (u, i) pair and the values of the characteristics for that pair. To estimate the probabilities, we used XGBoost4, a machine learning library called GBDT.4https: / / github.com / dmlc / xgboostFor the training of the ranking model, we divided all users with at least one point on the training site Truth into two sentences of the same size. Users from the first sentence were used to train the XGBoost model, and the users from the second sentence were used to validate the model. The training file (and the validation file) contained for each user: \u2022 all training candidates who occurred on the training site Truth (positive candidates) \u2022 and up to 5 training candidates who did not occur on the training site Truth (negative candidates). Shortly after the deadline for the submission of the training application, we observed one solution for each of the 1 and the 4 candidates."}, {"heading": "2.4 Features", "text": "Many of these characteristics were highly correlated, but we observed that redundant characteristics did not decrease the quality of the model. Many of these characteristics differ only in: \u2022 used different event sources: impressions instead of interactions, only positive / negative interactions, only user interactions, \u2022 instead of the Jaccard coefficient, which we also used for the size of sets. \u2022 used characteristics are only available in different aggregate functions: maximum, minimum, sum, number or unique number of entries (number without duplicate entries). Therefore, we will only describe the important groups of characteristics we used. There are 12 such groups Table 2 summarizes the meaning of the features we used, which were inspired by papers from previous RecSys Challenge. [2, 3] and similar competitions hosted on Kaggle platform6. Event-based characteristics are percentages of elements from Intu that had some ownership."}, {"heading": "2.5 Blending", "text": "In a final step in the construction of our solution, we merged our best models. As they were all similar and based on XGBoost, for each (u, i) pair we took the arithmetic mean of probabilities P [i \u0441G (u)] calculated by these models. Finally, we sorted the candidates for each target user according to these averaged probabilities and selected the 30 best ones."}, {"heading": "3 Conclusions", "text": "We presented Mim-Solution's approach to the 2016 th RecSys Challenge. We used XGBoost to predict the likelihood that a user will be interested in a job offer, but only for pre-selected offers. This enabled us to train and evaluate the model efficiently and to use complex and robust features. Although our score was good (28.5% of the best possible score measured by our train data), there was definitely a lot of room for improvement. A simple improvement, which is already mentioned in the paper, is the enlargement of the training set. There was also a lot to improve when ordering candidates, but we suspect that there was twice as much room for improvement in the expansion of the candidate pool (we achieved 77.5% or 37% of the best possible results in sorting and selecting candidates).This work is supported by the ERC project PAAl-POC 680912."}], "references": [{"title": "Xgboost: A scalable tree boosting system", "author": ["T. Chen", "C. Guestrin"], "venue": "Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2016}, {"title": "Recsys challenge 2015: Ensemble learning with categorical features", "author": ["P. Romov", "E. Sokolov"], "venue": "In Proceedings of the 2015 International ACM Recommender Systems Challenge, RecSys \u201915 Challenge,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2015}, {"title": "Two-stage approach to item recommendation from user sessions", "author": ["M. Volkovs"], "venue": "In Proceedings of the 2015 International ACM Recommender Systems Challenge, RecSys \u201915 Challenge,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "To learn the probabilities P [i \u2208 G(u)], we have used Gradient Boosting Decision Trees (GBDT) [1], optimizing the logloss measure.", "startOffset": 94, "endOffset": 97}, {"referenceID": 1, "context": "Ideas for features were inspired by papers of previous RecSys Challenge [2, 3] and similar competitions hosted on Kaggle platform.", "startOffset": 72, "endOffset": 78}, {"referenceID": 2, "context": "Ideas for features were inspired by papers of previous RecSys Challenge [2, 3] and similar competitions hosted on Kaggle platform.", "startOffset": 72, "endOffset": 78}], "year": 2016, "abstractText": "We present the Mim-Solution\u2019s approach to the RecSys Challenge 2016, which ranked 2nd. The goal of the competition was to prepare job recommendations for the users of the website Xing.com. Our two phase algorithm consists of candidate selection followed by the candidate ranking. We ranked the candidates by the predicted probability that the user will positively interact with the job offer. We have used Gradient Boosting Decision Trees as the regression tool.", "creator": "LaTeX with hyperref package"}}}