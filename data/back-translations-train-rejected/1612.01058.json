{"id": "1612.01058", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Dec-2016", "title": "Algorithmic Songwriting with ALYSIA", "abstract": "This paper introduces ALYSIA: Automated LYrical SongwrIting Application. ALYSIA is based on a machine learning model using Random Forests, and we discuss its success at pitch and rhythm prediction. Next, we show how ALYSIA was used to create original pop songs that were subsequently recorded and produced. Finally, we discuss our vision for the future of Automated Songwriting for both co-creative and autonomous systems.", "histories": [["v1", "Sun, 4 Dec 2016 03:36:51 GMT  (1507kb,D)", "http://arxiv.org/abs/1612.01058v1", null]], "reviews": [], "SUBJECTS": "cs.AI cs.LG cs.MM cs.SD", "authors": ["margareta ackerman", "david loker"], "accepted": false, "id": "1612.01058"}, "pdf": {"name": "1612.01058.pdf", "metadata": {"source": "CRF", "title": "Algorithmic Songwriting with ALYSIA", "authors": ["Margareta Ackerman", "David Loker"], "emails": ["margareta.ackerman@sjsu.edu", "dloker@gmail.com"], "sections": [{"heading": "1 Introduction", "text": "In fact, most of them are able to survive on their own, without being able to survive on their own."}, {"heading": "2 Previous work", "text": "In fact, most of them will be able to move to another world in which they will be able to move."}, {"heading": "3 The making of ALYSIA", "text": "ALYSIA is a fully data-driven system based on a predictive model. We chose two models: the first predicts the duration of notes, which we call the rhythm model; the second predicts the scale of a note, with possible accidentals, which we call the melody model. Both models rely on a similar set of characteristics, whereby depending on which model is executed first, a model has access to additional information. In our case, the melody model benefits from the predictions of the rhythm model. Note that combing the models would increase the number of output classes, which would require a larger corpus. The most important component of our model is the set of characteristics. The goal is to construct features that allow the model to learn the mechanics and techniques behind the structure of songs and their composition. Since we provide melodies for texts provided by the user, our training set consists of vocal lines of the music and not the melodies (each consisting of five)."}, {"heading": "3.1 Feature Extraction", "text": "To build a model, we first extract a set of features from the Music XML (MXL) files.For each note, we extract the following features: - First Measure - A Boolean variable that indicates whether the current note belongs to the first bar of the piece or not - Key Signature - The key signature that applies to the current note - Time Signature - The bar rate that applies to the current note - Offset - The number of bars since the beginning of the music - Offset within Measure - The number of bars since the beginning of the bar - Duration - The length of the note - Scale Degree - The scale of the note (1-7) - Accidental - The random number of bars (flat, sharp or none) - Beat Strength - The strength of the bar as defined by music21. We include the continuous and categorical version of the syllable (Beat Factor of the syllable)."}, {"heading": "3.2 Model Building and Evaluation", "text": "With R, we train two models with random forests. Random forests were selected for several reasons. They are well suited for a large number of categorical variables. In addition, they allow nonlinearity in combining characteristics without an6 Brown Corpus: http: / / www.hit.uib.no / icame / brown / bcm.html 7 CMUDict v0.07, Carnegie Mellon University.explosion in the required size of the training set to avoid overadjustment. Nonlinearity makes random forests more powerful than linear models such as logistic regression. We randomly divide the data with stratified sampling into a training set (75% of the data) with the rest for testing. The accuracy of the rhythm model on test data was 86.79%. Figure 2 shows the confusion matrix. The sequence labels correspond to the duration found in the test sets, and column labels are the predicted labels."}, {"heading": "3.3 User Lyrics to Features", "text": "To create music for new lyrics with our models, we need to extract the same lyrical characteristics that we used for our models using a Python script that prints a character matrix for importing to R."}, {"heading": "3.4 Song Generation", "text": "With R, we loop through the text attribute that specifies one sentence at a time. For each line, we read the attribute from the user-specified text and create the rhythm, followed by the melody. Note that we create one note for each syllable in the text passages. We also keep track of 5 previous notes, as these are very important characteristics in the model. Finally, we create a file in which the generated melodies are described in detail for each line. Depending on how many melodies are requested per line, there may be many."}, {"heading": "3.5 Corpus", "text": "Our corpus consists of 12,358 observations on 59 features, which we extract from MXL files containing 24 pop songs. Only the melody lines and lyrics were used for each song. Seventy-five percent of the data is used for training, while the remaining data is used for evaluation, i.e. scale with accent for the melody model and note duration for the rhythm model. Seventy-five percent of the data is used for training, while the remaining data is used for evaluation. Please note that MXL files containing text are much more difficult to obtain than MP3 files, which limits the size of our data."}, {"heading": "3.6 Parameters", "text": "When used in generation mode, ALYSIA allows the user to set the following parameters: Explore / Exploit This parameter determines how much we rely on the model. Specifically, our model prints a distribution of all possible results for each scale / note duration that we generate; this parameter determines how many independent drawings we make from this distribution; the final resulting tone is the most frequent draw value, with the bindings interrupted by the original result distribution; that is, when scales 1 and 2 are bound after four drawings, we take the scale that was originally more likely for this data point in the distribution output of the model. Given this definition, a higher value of the exploration / exploitation parameters means that we exploit it it, because we almost always output the scale degree or duration that occurs with the highest probability. This parameter allows us to generate the duration and duration experiments per comparison, which we would prefer to generate the melody output most likely to be in the specific antenna point."}, {"heading": "4 The Co-Creative Process of Songwriting with ALYSIA", "text": "In this section, we describe the co-creative process of songwriting with ALYSIA. This is a new approach to writing songs that require minimal to no musical training at all. ALYSIA makes it easy to explore melodic lines and reduces songwriting to the ability to select melodies based on one's own musical tastes. ALYSIA delivers the text in separate lines, then the system returns the specified number of melodies (notes / rhythm combinations) for which the predefined lyrics can be sung. The number of melodies per line is set by the user. ALYSIA delivers the melodies in both MXL and MIDI format. The MIDI form is particularly helpful as it allows the user to play the desired lyrics and uses the MIDI language directly to produce the song. We describe the workflow we use with ALYSIA. We typically ask for 15 to 30 melodic variations per line."}, {"heading": "5 Songs made with ALYSIA", "text": "The first two songs are based on lyrics written by the authors themselves, and the last two borrow their lyrics from a song from 1917. The songs were recorded and produced as follows: All recordings are on http: / / bit.ly / 2eQHado.The primary technical difference between these songs was the exploration and exploitation of people who were engaged in the exploration and exploitation of parameters that lead to more exploration, while everywhere and I had Rainbows dealing with a higher exploitation of people. Why I still do not know why I ever wrote a song that is the livelihood of ALYSIA was done with the aim of testing the viability of people."}, {"heading": "6 Discussion: Co-Creative and Autonomous Songwriting", "text": "In fact, most of them are able to determine for themselves what they want and what they want to do."}, {"heading": "1. Jose D Ferna\u0301ndez and Francisco Vico. Ai methods in algorithmic composition: A", "text": "Comprehensive Study. Journal of Artificial Intelligence Research, pp. 513-582, 2013. 2. David Gauntlett. Making is connecting. John Wiley & Sons, 2013. 3. Anna Jordanous. Has computerized creativity made it successfully \"beyond the boundaries of theater\" into musical theater? In Proceedings of the International Conference on Computational Creativity, pp. 87-94, 2012. 4. Kristine Monteith, Tony Martinez and Dan Ventura. Automatic production of melodic accompanying texts. In Proceedings of the International Conference on Computational Creativity, pp. 87-94, 2012. 5. Eric Nichols. Lyric Rhythm Suggestions. Ann Arbor, MI: Michigan Publishing, University of Michigan Library, 2009. 6. Hugo Gonc'alo Oliveira. Tra-la-lyrics 2.0: Automatic production of song lyrics in a semantic domain. Journal of Artificial General Intelligence, 6 (1): 87-110, 2015."}], "references": [{"title": "Ai methods in algorithmic composition: A comprehensive survey", "author": ["Jose D Fern\u00e1ndez", "Francisco Vico"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2013}, {"title": "Making is connecting", "author": ["David Gauntlett"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2013}, {"title": "Has computational creativity successfully made it \u2018beyond the fence\u2019in musical theatre", "author": ["Anna Jordanous"], "venue": "In Proceedings of the 7th International Conference on Computational Creativity,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2016}, {"title": "Automatic generation of melodic accompaniments for lyrics", "author": ["Kristine Monteith", "Tony Martinez", "Dan Ventura"], "venue": "In Proceedings of the International Conference on Computational Creativity,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2012}, {"title": "Lyric-based rhythm suggestion", "author": ["Eric Nichols"], "venue": "Ann Arbor, MI: Michigan Publishing, University of Michigan Library,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2009}, {"title": "Tra-la-lyrics 2.0: Automatic generation of song lyrics on a semantic domain", "author": ["Hugo Gon\u00e7alo Oliveira"], "venue": "Journal of Artificial General Intelligence,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2015}, {"title": "Smug: Scientific music generator", "author": ["Marco Scirea", "Gabriella AB Barros", "Noor Shaker", "Julian Togelius"], "venue": "In Proceedings of the Sixth International Conference on Computational Creativity June,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2015}, {"title": "Automatical composition of lyrical songs", "author": ["Jukka M Toivanen", "Hannu Toivonen", "Alessandro Valitutti"], "venue": "In The Fourth International Conference on Computational Creativity,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2013}], "referenceMentions": [{"referenceID": 1, "context": "Helping people be creative, particularly in a manner that aids their social engagement both on and offline, can have substantial positive impact on their sense of well-being and happiness [2].", "startOffset": 188, "endOffset": 191}, {"referenceID": 0, "context": "See a survey by Fern\u00e1ndez and Vico [1] for an excellent overview.", "startOffset": 35, "endOffset": 38}, {"referenceID": 7, "context": "Sucus-Apparatusf [8] by Toivanen et al, is a system that was used for the generation of Finnish art songs.", "startOffset": 17, "endOffset": 20}, {"referenceID": 7, "context": "Yet, as with most previous systems, the major weakness is a lack of clear phrase structure [8].", "startOffset": 91, "endOffset": 94}, {"referenceID": 6, "context": "Another full-cycle automated songwriting system comes from the work of Scirea et al [7].", "startOffset": 84, "endOffset": 87}, {"referenceID": 3, "context": "[4] study the generation of melodic accompaniments.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "In other related works, Nichols [5] studies a sub-component of the problem we address here, particularly lyric-based rhythm suggestion.", "startOffset": 32, "endOffset": 35}, {"referenceID": 5, "context": "Lastly, in the work of Oliveira [6], the pipeline is inverted and lyrics are generated based on the rhythm of the provided melody.", "startOffset": 32, "endOffset": 35}, {"referenceID": 4, "context": "\u2013 Word Rarity* - A function of word frequency, as defined by [5].", "startOffset": 61, "endOffset": 64}, {"referenceID": 4, "context": "The features marked with an * were used by [5] as part of a rule-based evaluation criteria for rhythm suggestion.", "startOffset": 43, "endOffset": 46}, {"referenceID": 2, "context": "The unique challenges of songwriting were observed during the creation of the first musical to rely on computational creativity systems, Beyond the Fence, which played in London in the early months of 2016 [3].", "startOffset": 206, "endOffset": 209}], "year": 2016, "abstractText": "This paper introduces ALYSIA: Automated LYrical SongwrIting Application. ALYSIA is based on a machine learning model using Random Forests, and we discuss its success at pitch and rhythm prediction. Next, we show how ALYSIA was used to create original pop songs that were subsequently recorded and produced. Finally, we discuss our vision for the future of Automated Songwriting for both co-creative and autonomous systems.", "creator": "LaTeX with hyperref package"}}}