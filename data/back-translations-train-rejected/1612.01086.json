{"id": "1612.01086", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Dec-2016", "title": "Deep Learning of Robotic Tasks without a Simulator using Strong and Weak Human Supervision", "abstract": "We propose a scheme for training a computerized agent to perform complex human tasks such as highway steering. The scheme resembles natural teaching-learning processes used by humans to teach themselves and each other complex tasks, and consists of the following four stages. In the first stage the agent learns by itself an informative low-dimensional representations of raw input signals in an unsupervised learning manner. In the second stage the agent learns to mimic the human instructor using supervised learning so as to reach a basic performance level; the third stage is devoted to learning an instantaneous reward model. Here, the (human) instructor observes (possibly in real time) the agent performing the task and provides reward feedback. During this stage the agent monitors both itself and the instructor feedback and learns a reward model using supervised learning. This stage terminates when the reward model is sufficiently accurate. In the last stage a reinforcement learning algorithm is deployed to optimize the agent policy. The guidance reward signal in the reinforcement learning algorithm relies on the previously learned reward model. As a proof of concept for the proposed scheme, we designed a system consisting of deep convolutional neural networks, and applied it to successfully learn a computerized agent capable of autonomous highway steering over the well-known racing game Assetto Corsa.", "histories": [["v1", "Sun, 4 Dec 2016 08:28:38 GMT  (955kb,D)", "http://arxiv.org/abs/1612.01086v1", null], ["v2", "Tue, 14 Mar 2017 17:08:16 GMT  (2095kb,D)", "http://arxiv.org/abs/1612.01086v2", null], ["v3", "Sun, 26 Mar 2017 08:43:23 GMT  (2095kb,D)", "http://arxiv.org/abs/1612.01086v3", null]], "reviews": [], "SUBJECTS": "cs.AI cs.LG cs.RO", "authors": ["bar hilleli", "ran el-yaniv"], "accepted": false, "id": "1612.01086"}, "pdf": {"name": "1612.01086.pdf", "metadata": {"source": "CRF", "title": "Deep Learning of Robotic Tasks using Strong and Weak Human Supervision", "authors": [], "emails": ["barh@campus.technion.ac.il", "rani@cs.technion.ac.il"], "sections": [{"heading": "1 Introduction", "text": "This year, it is more than ever before in the history of the country in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is not a country, but in which it is a country, in which it is a country, in which it is a city and in which it is a country."}, {"heading": "2 Imitation learning", "text": "The question is to what extent this is a way in which the ability to achieve reasonable performance is limited; the performance of an agent trained in this way is an old idea that was conceived about decades ago (Hayes & Demiris, 1994; Argall et al., 2009). If the training sample is not sufficiently diverse / representative, the agent will not be subjected to unexpectedly difficult conditions and may suffer from very poor and unpredictable performance when such conditions are encountered in production. Finally, labeled samples that we receive from human demonstrations are vulnerable to the labeling of noise that we observe these limitations, we use imitation learning in our scheme only to achieve a basic level of performance that allows the agent to perform the required task without harming himself or the environment and receiving increased reward."}, {"heading": "3 Deep reward shaping", "text": "The idea is to define complementary reward signals to make it easier to learn an RL problem. Developing a reward function can be a complicated task that requires experience and a fair amount of specific domain knowledge. Therefore, other methods of creating a reward function without the domain competence requirement used in other countries to obtain a reward function are required. The reward function is learned from expert demonstrations (Abbeel & Ng, 2004). The reward is based on the fact that an expert demonstration implicitly encodes the reward function of the task, and its goal is to restore a reward function that best explains the expert's behavior. (2014) suggests learning a reward model in a monitored manner and using the reward between reward learning and reinforcement to continuously improve the reward model and the agent's policies."}, {"heading": "4 Reinforcement learning using DDQN", "text": "In this section, we assume that there is basic familiarity with reinforcement learning after the initiation of the optimal policy following adherence to the optimal guidelines. See, for example, (Sutton & Barto, Q = Q = Q, 1998) In the final level of the RL, we use the already trained reward networks and apply them within a standard RL method. Remember that performance is measured in relation to the learned reward model, the main objective of this level is to achieve a significantly better level of performance than what we have achieved in the level of mimicry reward by allowing the agent to teach himself. In other words, starting with a policy, we would like to apply an RL algorithm to learn a policy that is optimal in terms of the expected (discounted) reward. While any RL method can be used in our scheme, we use a variant of the Q-learning algorithm (Dakins & Watyan, Action Function aimed at finding optimal value in 1992)."}, {"heading": "5 Experimental setting and technicalities", "text": "This year, the time has come for such a process to take place in the first half of the year, in which such a process will take place."}, {"heading": "6 Concluding remarks", "text": "We presented a multi-level generic framework that utilizes a natural synergy between multiple learning principles to train an agent to perform a complex task. We expect the proposed framework to be useful in various areas of application and have demonstrated its strength in terms of autonomous motorway driving problems. Second, an instantaneous reward network of human instruction is trained to successfully implement all three components of the proposed framework. First, we form a CNN agent aimed at achieving a basic initial driving style. Third, the agent uses the learned reward networks as an orientation signal in an RL process."}], "references": [{"title": "Apprenticeship Learning via Inverse Reinforcement Learning", "author": ["P. Abbeel", "A.Y. Ng"], "venue": "In Proceedings of the Twenty-First International Conference on Machine Learning,", "citeRegEx": "Abbeel and Ng.,? \\Q2004\\E", "shortCiteRegEx": "Abbeel and Ng.", "year": 2004}, {"title": "A survey of robot learning from demonstration", "author": ["B.D. Argall", "S. Chernova", "M. Veloso", "B. Browning"], "venue": "Robotics and Autonomous Systems,", "citeRegEx": "Argall et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Argall et al\\.", "year": 2009}, {"title": "Deepdriving: Learning Affordance for Direct Perception in Autonomous Driving", "author": ["C. Chen", "A. Seff", "A. Kornhauser", "J. Xiao"], "venue": "In Proceedings of the IEEE International Conference on Computer Vision, pp", "citeRegEx": "Chen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2015}, {"title": "An analysis of single-layer networks in unsupervised feature learning", "author": ["Adam Coates", "Honglak Lee", "Andrew Y Ng"], "venue": "Ann Arbor,", "citeRegEx": "Coates et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Coates et al\\.", "year": 2010}, {"title": "Active Reward Learning", "author": ["C. Daniel", "M. Viering", "J. Metz", "O. Kroemer", "J. Peters"], "venue": "In Proceedings of Robotics Science & Systems,", "citeRegEx": "Daniel et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Daniel et al\\.", "year": 2014}, {"title": "Double Q-learning", "author": ["H. Van Hasselt"], "venue": "In Advances in Neural Information Processing Systems, pp. 2613\u20132621,", "citeRegEx": "Hasselt.,? \\Q2010\\E", "shortCiteRegEx": "Hasselt.", "year": 2010}, {"title": "Deep Reinforcement Learning with Double Q-Learning", "author": ["H. Van Hasselt", "A. Guez", "D. Silver"], "venue": "In Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence,", "citeRegEx": "Hasselt et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Hasselt et al\\.", "year": 2016}, {"title": "A Robot Controller Using Learning by Imitation", "author": ["G.M. Hayes", "J. Demiris"], "venue": "Technical report,", "citeRegEx": "Hayes and Demiris.,? \\Q1994\\E", "shortCiteRegEx": "Hayes and Demiris.", "year": 1994}, {"title": "A survey of the Hough transform", "author": ["John Illingworth", "Josef Kittler"], "venue": "Computer vision, graphics, and image processing,", "citeRegEx": "Illingworth and Kittler.,? \\Q1988\\E", "shortCiteRegEx": "Illingworth and Kittler.", "year": 1988}, {"title": "Adam: A Method for Stochastic Optimization", "author": ["D. Kingma", "J. Ba"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "Kingma and Ba.,? \\Q2014\\E", "shortCiteRegEx": "Kingma and Ba.", "year": 2014}, {"title": "Reinforcement Learning in Robotics: A survey", "author": ["J. Kober", "J.A. Bagnell", "J. Peters"], "venue": "The International Journal of Robotics Research,", "citeRegEx": "Kober et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Kober et al\\.", "year": 2013}, {"title": "Imagenet Classification with Deep Convolutional Neural Networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Theory and Application of Reward Shaping in Reinforcement Learning", "author": ["A.D. Laud"], "venue": "PhD thesis, University of Illinois at Urbana-Champaign,", "citeRegEx": "Laud.,? \\Q2004\\E", "shortCiteRegEx": "Laud.", "year": 2004}, {"title": "Learning to Overtake in TORCS Using Simple Reinforcement Learning", "author": ["D. Loiacono", "A. Prete", "P.L. Lanzi", "L. Cardamone"], "venue": "In IEEE Congress on Evolutionary Computation,", "citeRegEx": "Loiacono et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Loiacono et al\\.", "year": 2010}, {"title": "Human-level control through deep reinforcement learning", "author": ["V. Mnih", "K. Kavukcuoglu", "D. Silver", "A.A. Rusu", "J. Veness", "M.G. Bellemare", "A. Graves", "A. Riedmiller", "A.K. Fidjeland", "G. Ostrovski"], "venue": "Nature, pp. 529\u2013533,", "citeRegEx": "Mnih et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2015}, {"title": "Controller for TORCS created by imitation", "author": ["J. Munoz", "G. Gutierrez", "A. Sanchis"], "venue": "In IEEE Symposium on Computational Intelligence and Games, pp", "citeRegEx": "Munoz et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Munoz et al\\.", "year": 2009}, {"title": "Policy invariance under reward transformations: Theory and application to reward shaping", "author": ["A.Y. Ng", "D. Harada", "S.J. Russell"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Ng et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Ng et al\\.", "year": 1999}, {"title": "Multimodal deep learning", "author": ["Jiquan Ngiam", "Aditya Khosla", "Mingyu Kim", "Juhan Nam", "Honglak Lee", "Andrew Y Ng"], "venue": "In Proceedings of the 28th international conference on machine learning", "citeRegEx": "Ngiam et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Ngiam et al\\.", "year": 2011}, {"title": "A Reduction of Imitation Learning and Structured Prediction to No-Regret Online Learning", "author": ["S. Ross", "G.J. Gordon", "J.A. Bagnell"], "venue": "In International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "Ross et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Ross et al\\.", "year": 2011}, {"title": "Dropout: a simple way to prevent neural networks from overfitting", "author": ["N. Srivastava", "G. Hinton", "A. Krizhevsky", "I. Sutskever", "R. Salakhutdinov"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Srivastava et al\\.,? \\Q1929\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 1929}, {"title": "Reinforcement Learning: An Introduction", "author": ["R.S. Sutton", "A.G. Barto"], "venue": "MIT press Cambridge,", "citeRegEx": "Sutton and Barto.,? \\Q1998\\E", "shortCiteRegEx": "Sutton and Barto.", "year": 1998}, {"title": "Using Human Demonstrations to Improve Reinforcement Learning", "author": ["M.E. Taylor", "H.B. Suay", "S. Chernova"], "venue": "In AAAI Spring Symposium: Help Me Help You: Bridging the Gaps in Human-Agent Collaboration,", "citeRegEx": "Taylor et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Taylor et al\\.", "year": 2011}, {"title": "TORCS, the open racing car simulator. Software available at http://torcs", "author": ["B. Wymann", "E. Espi\u00e9", "C. Guionneau", "C. Dimitrakakis", "R. Coulom", "A. Sumner"], "venue": "sourceforge. net,", "citeRegEx": "Wymann et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Wymann et al\\.", "year": 2000}, {"title": "Query-Efficient Imitation Learning for End-to-End Autonomous Driving", "author": ["J. Zhang", "K. Cho"], "venue": "arXiv preprint:1605.06450,", "citeRegEx": "Zhang and Cho.,? \\Q2016\\E", "shortCiteRegEx": "Zhang and Cho.", "year": 2016}], "referenceMentions": [{"referenceID": 1, "context": "In imitation learning, which is also known as \u2018behavioral cloning\u2019 or \u2018learning from demonstrations\u2019, a human demonstrator performs the desired task with the goal of teaching a (robotic) agent to mimic her actions (Argall et al., 2009).", "startOffset": 214, "endOffset": 235}, {"referenceID": 1, "context": "In imitation learning, which is also known as \u2018behavioral cloning\u2019 or \u2018learning from demonstrations\u2019, a human demonstrator performs the desired task with the goal of teaching a (robotic) agent to mimic her actions (Argall et al., 2009). The demonstrations are used to learn a mapping from a given world state, s, received via sensors, to a desired action, a, consisting of instructions to the agent\u2019s controllers. Throughout this paper s will be referred also as the \u201draw signal.\u201d The objective in IL is to minimize the risk, in the supervised learning sense. In RL the goal is to enable the agent to find its own policy, one that maximizes a value function defined in terms of certain guidance reward signals received during its interaction with the environment. IL and RL can be combined, as was recently proposed by Taylor et al. (2011). The idea is to start the reinforcement learning with an initial policy learned during a preceding IL stage.", "startOffset": 215, "endOffset": 840}, {"referenceID": 17, "context": "Any set of such external features can be easily integrated into our scheme using known methods such as those described in (Ngiam et al., 2011).", "startOffset": 122, "endOffset": 142}, {"referenceID": 1, "context": "(2010), imitation learning (Argall et al., 2009) and reinforcement learning (Kober et al.", "startOffset": 27, "endOffset": 48}, {"referenceID": 10, "context": ", 2009) and reinforcement learning (Kober et al., 2013).", "startOffset": 35, "endOffset": 55}, {"referenceID": 21, "context": "The two closest works to ours are (Taylor et al., 2011) and (Daniel et al.", "startOffset": 34, "endOffset": 55}, {"referenceID": 4, "context": ", 2011) and (Daniel et al., 2014).", "startOffset": 12, "endOffset": 33}, {"referenceID": 2, "context": "See, for example surveys on unsupervised feature learning Coates et al. (2010), imitation learning (Argall et al.", "startOffset": 58, "endOffset": 79}, {"referenceID": 14, "context": "The choice of CNNs for all three tasks is based on their proven ability to extract informative features from images in the context of classification and control tasks (Mnih et al., 2015; Krizhevsky et al., 2012), thus obviating the exhausting task of manually defining features.", "startOffset": 167, "endOffset": 211}, {"referenceID": 11, "context": "The choice of CNNs for all three tasks is based on their proven ability to extract informative features from images in the context of classification and control tasks (Mnih et al., 2015; Krizhevsky et al., 2012), thus obviating the exhausting task of manually defining features.", "startOffset": 167, "endOffset": 211}, {"referenceID": 22, "context": ", TORCS (Wymann et al., 2000)) containing valuable parameters such as the car\u2019s distance from the roadside or its angle with respect to the road.", "startOffset": 8, "endOffset": 29}, {"referenceID": 13, "context": ", in computer game simulators) (Zhang & Cho, 2016; Loiacono et al., 2010; Munoz et al., 2009; Chen et al., 2015).", "startOffset": 31, "endOffset": 112}, {"referenceID": 15, "context": ", in computer game simulators) (Zhang & Cho, 2016; Loiacono et al., 2010; Munoz et al., 2009; Chen et al., 2015).", "startOffset": 31, "endOffset": 112}, {"referenceID": 2, "context": ", in computer game simulators) (Zhang & Cho, 2016; Loiacono et al., 2010; Munoz et al., 2009; Chen et al., 2015).", "startOffset": 31, "endOffset": 112}, {"referenceID": 16, "context": "This method is known as reward shaping, where an additional reward signal is used to guide the learning agent (Ng et al., 1999).", "startOffset": 110, "endOffset": 127}, {"referenceID": 5, "context": "Third, the Double Deep Q-learning (Hasselt, 2010; Hasselt et al., 2016) (DDQN) RL algorithm is used to train a Q-network.", "startOffset": 34, "endOffset": 71}, {"referenceID": 6, "context": "Third, the Double Deep Q-learning (Hasselt, 2010; Hasselt et al., 2016) (DDQN) RL algorithm is used to train a Q-network.", "startOffset": 34, "endOffset": 71}, {"referenceID": 8, "context": ", 2015; Krizhevsky et al., 2012), thus obviating the exhausting task of manually defining features. For example, in the work of Mnih et al. (2015) a CNN was successfully trained to predict desired control actions given high-dimensional pixel data in the Atari 2600 domain.", "startOffset": 8, "endOffset": 147}, {"referenceID": 1, "context": "learning using mimicry is an old idea, conceived decades ago (Hayes & Demiris, 1994; Argall et al., 2009).", "startOffset": 61, "endOffset": 105}, {"referenceID": 18, "context": "A training sample of state-action pairs is gathered in the following two-stage procedure, which can be viewed as one iteration of the Dagger algorithm applied with \u03b2 = 0 (Ross et al., 2011).", "startOffset": 170, "endOffset": 189}, {"referenceID": 12, "context": "The problem of designing suitable reward functions to guide an agent to successfully learn a desired task is known as reward shaping (Laud, 2004).", "startOffset": 133, "endOffset": 145}, {"referenceID": 14, "context": "We used the DDQN algorithm with replay memory and target values calculated from parameters of the previous iteration, as in the work of Mnih et al. (2015). The loss function we used is therefore,", "startOffset": 136, "endOffset": 155}], "year": 2016, "abstractText": "We propose a scheme for training a computerized agent to perform complex human tasks such as highway steering. The scheme resembles natural teaching-learning processes used by humans to teach themselves and each other complex tasks, and consists of the following four stages. In the first stage the agent learns by itself an informative low-dimensional representations of raw input signals in an unsupervised learning manner. In the second stage the agent learns to mimic the human instructor using supervised learning so as to reach a basic performance level; the third stage is devoted to learning an instantaneous reward model. Here, the (human) instructor observes (possibly in real time) the agent performing the task and provides reward feedback. During this stage the agent monitors both itself and the instructor feedback and learns a reward model using supervised learning. This stage terminates when the reward model is sufficiently accurate. In the last stage a reinforcement learning algorithm is deployed to optimize the agent policy. The guidance reward signal in the reinforcement learning algorithm relies on the previously learned reward model. As a proof of concept for the proposed scheme, we designed a system consisting of deep convolutional neural networks, and applied it to successfully learn a computerized agent capable of autonomous highway steering over the well-known racing game Assetto Corsa.", "creator": "LaTeX with hyperref package"}}}