{"id": "1612.07119", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Dec-2016", "title": "FINN: A Framework for Fast, Scalable Binarized Neural Network Inference", "abstract": "Research has shown that convolutional neural networks contain significant redundancy, and high classification accuracy can be obtained even when weights and activations are reduced from floating point to binary values. In this paper, we present FINN, a framework for building fast and flexible FPGA accelerators using a flexible heterogeneous streaming architecture. By utilizing a novel set of optimizations that enable efficient mapping of binarized neural networks to hardware, we implement fully connected, convolutional and pooling layers, with per-layer compute resources being tailored to user-provided throughput requirements. On a ZC706 embedded FPGA platform drawing less than 25 W total system power, we demonstrate up to 12.3 million image classifications per second with 0.31 {\\mu}s latency on the MNIST dataset with 95.8% accuracy, and 21906 image classifications per second with 283 {\\mu}s latency on the CIFAR-10 and SVHN datasets with respectively 80.1% and 94.9% accuracy. To the best of our knowledge, ours are the fastest classification rates reported to date on these benchmarks.", "histories": [["v1", "Thu, 1 Dec 2016 22:19:47 GMT  (867kb,D)", "http://arxiv.org/abs/1612.07119v1", "To appear in the 25th International Symposium on Field-Programmable Gate Arrays, February 2017"]], "COMMENTS": "To appear in the 25th International Symposium on Field-Programmable Gate Arrays, February 2017", "reviews": [], "SUBJECTS": "cs.CV cs.AR cs.LG", "authors": ["yaman umuroglu", "nicholas j fraser", "giulio gambardella", "michaela blott", "philip leong", "magnus jahre", "kees vissers"], "accepted": false, "id": "1612.07119"}, "pdf": {"name": "1612.07119.pdf", "metadata": {"source": "CRF", "title": "FINN: A Framework for Fast, Scalable Binarized Neural Network Inference", "authors": ["Yaman Umuroglu", "Nicholas J. Fraser", "Giulio Gambardella", "Michaela Blott", "Philip Leong", "Kees Vissers"], "emails": ["yamanu@idi.ntnu.no"], "sections": [{"heading": "1. INTRODUCTION", "text": "This year, it has reached the point where there is only one person who is able to establish himself in the region."}, {"heading": "2. BACKGROUND", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Convolutional Neural Networks", "text": "This work focuses on supervised learning, where the goal is to find a function, g (xi), approaching a mapping function in which {xi, yi} is an input / output pair known as a training example. A multi-layer perceptron is a kind of artificial neural network that has its neurons arranged in several layers, with neurons taking the output of all neurons of the previous layer as inputs. Mathematically, the output, al, n, for which nth neuron in the lth layer of a fully connected network is calculated as follows: al, n = fact (Sl = 0 wl, n, sal \u2212 1, s + bl, n), where wl, s, weight of the s th synapse is connected to the input of the nth neuron in the lth layer of a fully connected network, is calculated as follows: al, n = fact (Sl = fact, n = fact, n = fact, sp, n, x, x, where n is connected to the output of the n wl, n)."}, {"heading": "2.2 Binary Neural Networks", "text": "Although floating-point numbers are a natural choice for dealing with the small updates that occur during neural network training, the resulting parameters can contain a lot of redundant information [8]. One of several possible dimensions of redundancy is precision [26]. An extreme case is BNs, in which some or all arithmetics involved in calculating the results are limited to single-bit values. We consider three aspects of binarization for neural network layers: binary input activations, binary synapse weights, and binary output activations. If all three components are binary, we refer to this as complete binarization, and the cases with one or two components as partial binarization. Kim and Smaragdis [12] consider full binarization with a predetermined portion of the synapses to be zero weight, and all other synapses weighing one."}, {"heading": "2.3 Neural Networks in Hardware", "text": "This year, it is only a matter of time before an agreement is reached."}, {"heading": "3. BNN PERFORMANCE AND ACCURACY", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Estimating Performance Using Rooflines", "text": "To estimate and compare the performance of BNN with fixedpoint CNN, we use a Roofline model [29] that takes into account memory bandwidth, maximum computing power, and arithmetic intensity (the number of mathematical operations performed for each byte of read or written off-chip memory).The intersection of the Roofline curve with a vertical line for a certain arithmetic intensity yields the theoretical peak power point that is either computer-bound or memory-bound. In particular, we consider the binarized [31.21] and 8-bit fixed-point curve [25] implementations of the popular AlexNet [14], both requiring 1.4 billion operations (GOPS) to classify an image. Using the methodology described in [17], we develop a Roofline model for a Xilinx Zynq UltraScale + ZU19EG FPGA1. The resulting Roofline model is shown in Figure 1."}, {"heading": "3.2 Accuracy\u2013Computation Tradeoffs", "text": "A compromise exists between network size, precision and accuracy [26], so if you want to achieve a certain classification accuracy for a given problem, which approach leads to the most efficient solution? 1) A regular ANN with floating-point precision? 2) A larger network, but a BNN? To gain more insight into this problem, we conducted a series of experiments on the MNIST dataset al, which compares the accuracy of floating-point and binary precision for the same topology. Binary networks are achieved by replacing regular layers with their binary equivalents, as described by Courbariaux et al. [5] We also binarize the input images for the BNN, as our experiments show that input binarization works well for MNIST. Since the space of possible network topologies that can be trained is infinite, we have adopted the approach in which we proceed to [26] simplify the problem."}, {"heading": "4. BNNs ON RECONFIGURABLE LOGIC", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Architecture", "text": "We have a heterogeneous streaming architecture as shown in Figure 2, for this work. We build a custom architecture for a specific topology, rather than a scheduling of operations on a fixed architecture. Separate computing engines are used for each layer that communicate via on-chip data streams. Each machine begins to calculate once the previous machine starts to produce output. In addition, due to the compact model size of BNNNs, all neural network parameters are kept in the on-chip memory. This avoids most accesses to off-chip memory, minimizes latency (the time to classify an image) by overlapping computation and communication, and minimizes the initiation interval: a new image can enter the accelerator once the first computing array is finished with the previous image."}, {"heading": "4.2 BNN-specific Operator Optimizations", "text": "We assume that the methodology described in [5] is used to train all BNNs in this paper, while all BNN levels have the following properties (unless otherwise specified): \u2022 The use of 1-bit values for all input activations, weights, and output activations (full binarization) where an unset bit -1 and a specified threshold represents + 1. \u2022 Batch normalization before the activation function requires the following activation function: Sign (x) = {+ 1 if x < 0} 4.2.1 Popcount for accumulation The regular and value-limited nature of BNN com-putations allows the calculation of binary dot products with fewer hardware resources. Let Y be the number of input synapses (or fan-in) for a given neuron, with the number of + 1 synchronized values."}, {"heading": "4.3 FINN Design Flow and Hardware Library", "text": "The user delivers an FPS target next to a theano-trained BNN to set it in the world while applying the optimizations from Section 4.2, which then generates a synthetizable C + + description of a heterogeneous streaming architecture.The architecture consists of the building blocks of the Finn hardware library described in the following subsections. 4.3.1 The matrix vector threshold unit images from the matrix vector threshold unit are used by the matrix vector threshold architecture.The vast majority of compiled operations in a BNN can be expressed as matrix vector operations, followed by the ratio vector vector vector vector operations."}, {"heading": "4.4 Folding", "text": "This year it is more than ever before in the history of the city."}, {"heading": "5. EVALUATION", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 Experimental Setup", "text": "This year, it is more than ever before in the history of the city."}, {"heading": "5.2 Results", "text": "This year it is so far that it will be able to retaliate, \"he said.\" It's as if it will be able to retaliate, \"he said.\" It's as if it will be able to retaliate, \"he said."}, {"heading": "5.3 Comparison to prior work", "text": "From an application perspective, we suggest that the best way to compare different platforms is simply to compare their accuracy, FPS, and power consumption when working on the same benchmark datasets (MNIST, CIFAR10, and SVHN). This comparison is provided in Table 4 and is divided into three sections: our results, previous work on low-precision networks (< 4 bits), and previous work on high-precision (> 4 bits) networks.When it comes to pure image throughput, our designs outperform all others. In the MNIST dataset, we achieve an FPS that is more than 48 / 6 bits above the closest throughput design [1] for our SFC-max / LFC-max designs. While our SFC-Max design has a lower accuracy than the networks implemented by Alemdar et al. [1], our LFC-Max design was able to increase our data throughput [10 / LFC-max-max-max-max-values] by 1.6, or more than the VAR for highest throughput [6]."}, {"heading": "6. CONCLUSION", "text": "They are particularly well suited for FPGA implementations as the parameters fit fully into OCM and arithmetic is simplified, enabling high computing power. A novel parameterisable data flow architecture and the optimizations presented enable unprecedented classification rates, minimal power consumption and latency, while providing the flexibility and scalability needed to accelerate larger and more complex networks. We therefore believe that this technology is ideally suited for embedded applications that require real-time response, including monitoring, robotics and augmented reality. Future work will focus on supporting non-binary low precision, implementing larger networks such as AlexNet, higher performance volumes and more in-depth design space research. Finally, Finn believes that all BNN parameters can fit into the available OCM of a single FPGA."}, {"heading": "Acknowledgments", "text": "The authors would like to thank the NTNU HPC Laboratory and colleagues from Xilinx Research Labs for their support, which was supported by the Australian Research Councils Linkage Projects funding program (project number LP130101034)."}, {"heading": "7. REFERENCES", "text": "[1] H. Alemdar, N. Caldwell, V. Leroy, A. Prost-Boucle, and F. Pe-trot. Ternary Neural Networks for Resource-Efficient AI Applications. CoRR, abs / 1609.00222, 2016. [2] R. Andri, L. Cavigelli, D. Rossi, and L. Benini. YodaNN: An ultra-low power convolutional neural network accelerator based on binary weights. CoRR, abs / 1606.05487, 2016. [3] K. Chellapilla, S. Puri, and P. Kinard. High performance convolutional neural networks for document processing. In Proc. ICFHR. Suvisoft, 2006. [4] Y.-H. Chen, J. Emer, and V. Sze. Eyeriss: A spatial architecture for energy-efficient dataflow for convolutional neural networks."}], "references": [{"title": "Ternary Neural Networks for Resource-Efficient AI Applications", "author": ["H. Alemdar", "N. Caldwell", "V. Leroy", "A. Prost-Boucle", "F. P\u00e9trot"], "venue": "CoRR, abs/1609.00222,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2016}, {"title": "YodaNN: An ultra-low power convolutional neural network accelerator based on binary weights", "author": ["R. Andri", "L. Cavigelli", "D. Rossi", "L. Benini"], "venue": "CoRR, abs/1606.05487,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2016}, {"title": "High performance convolutional neural networks for document processing", "author": ["K. Chellapilla", "S. Puri", "P. Simard"], "venue": "Proc. ICFHR. Suvisoft,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2006}, {"title": "Eyeriss: A spatial architecture for energy-efficient dataflow for convolutional neural networks", "author": ["Y.-H. Chen", "J. Emer", "V. Sze"], "venue": "Proc. ACM/IEEE ISCA. IEEE,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2016}, {"title": "Binarynet: Training deep neural networks with weights and activations constrained to +1 or -1", "author": ["M. Courbariaux", "Y. Bengio"], "venue": "CoRR, abs/1602.02830,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2016}, {"title": "Convolutional Networks for Fast, Energy-Efficient Neuromorphic Computing", "author": ["S.K. Esser", "P.A. Merolla", "J.V. Arthur", "A.S. Cassidy", "R. Appuswamy", "A. Andreopoulos", "D.J. Berg", "J.L. McKinstry", "T. Melano", "D.R. Barch"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2016}, {"title": "CNP: An FPGA-based processor for convolutional networks", "author": ["C. Farabet", "C. Poulet", "J.Y. Han", "Y. LeCun"], "venue": "Proc. IEEE FPL, pages 32\u201337. IEEE,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2009}, {"title": "Deep Compression: Compressing Deep Neural Network with Pruning, Trained Quantization and Huffman coding", "author": ["S. Han", "H. Mao", "W.J. Dally"], "venue": "CoRR, abs/1510.00149,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "CaffePresso: An Optimized Library for Deep Learning on Embedded Accelerator-based platforms", "author": ["G. Hegde", "Siddhartha", "N. Ramasamy", "N. Kapre"], "venue": "In Proc. CASES,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2016}, {"title": "SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and< 1MB model size", "author": ["F.N. Iandola", "M.W. Moskewicz", "K. Ashraf", "S. Han", "W.J. Dally", "K. Keutzer"], "venue": "CoRR, abs/1602.07630,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2016}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["S. Ioffe", "C. Szegedy"], "venue": "Proc. ICML, pages 448\u2013456,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "Bitwise neural networks", "author": ["M. Kim", "P. Smaragdis"], "venue": "CoRR, abs/1601.06071,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2016}, {"title": "Learning multiple layers of features from tiny images", "author": ["A. Krizhevsky", "G. Hinton"], "venue": "Technical Report,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2009}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "Proc. NIPS, pages 1097\u20131105,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2012}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proc. of the IEEE, 86(11):2278\u20132324,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1998}, {"title": "Artificial neural networks in hardware: A survey of two decades of progress", "author": ["J. Misra", "I. Saha"], "venue": "Neurocomputing, 74(1\u20133):239\u2013255,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2010}, {"title": "A Semi-Automated Tool Flow for Roofline Anaylsis of OpenCL Kernels on Accelerators", "author": ["S. Muralidharan", "K. O\u2019Brien", "C. Lalanne"], "venue": "Proc. Workshop on H2RC,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2015}, {"title": "Reading digits in natural images with unsupervised feature learning", "author": ["Y. Netzer", "T. Wang", "A. Coates", "A. Bissacco", "B. Wu", "A.Y. Ng"], "venue": "NIPS Workshop on Deep Learning and Unsupervised Feature Learning,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2011}, {"title": "Accelerating deep  convolutional neural networks using specialized hardware", "author": ["K. Ovtcharov", "O. Ruwase", "J.-Y. Kim", "J. Fowers", "K. Strauss", "E. Chung"], "venue": null, "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2015}, {"title": "FPGA based implementation of deep neural networks using on-chip memory only", "author": ["J. Park", "W. Sung"], "venue": "Proc. IEEE ICASSP, pages 1011\u20131015. IEEE,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2016}, {"title": "XNOR-Net: ImageNet Classification Using Binary Convolutional Neural Networks", "author": ["M. Rastegari", "V. Ordonez", "J. Redmon", "A. Farhadi"], "venue": "ECCV,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2016}, {"title": "ImageNet Large Scale Visual Recognition Challenge", "author": ["O. Russakovsky", "J. Deng", "H. Su", "J. Krause", "S. Satheesh", "S. Ma", "Z. Huang", "A. Karpathy", "A. Khosla", "M. Bernstein", "A.C. Berg", "L. Fei-Fei"], "venue": "IJCV, 115(3):211\u2013252,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep learning in neural networks: An overview", "author": ["J. Schmidhuber"], "venue": "Neural Networks, 61:85\u2013117,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2015}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "CoRR, abs/1409.1556,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2014}, {"title": "Throughput-Optimized OpenCL-based FPGA Accelerator for Large-Scale Convolutional Neural Networks", "author": ["N. Suda", "V. Chandra", "G. Dasika", "A. Mohanty", "Y. Ma", "S.B.K. Vrudhula", "J. Seo", "Y. Cao"], "venue": "Proc. ACM/SIGDA ISFPGA, pages 16\u201325,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2016}, {"title": "Resiliency of deep neural networks under quantization", "author": ["W. Sung", "S. Shin", "K. Hwang"], "venue": "CoRR, abs/1511.06488,", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2015}, {"title": "fpgaConvNet: A Framework for Mapping Convolutional Neural Networks on FPGAs", "author": ["S.I. Venieris", "C.-S. Bouganis"], "venue": "Proc. IEEE FCCM, pages 40\u201347. IEEE,", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2016}, {"title": "Planet - photo geolocation with convolutional neural networks", "author": ["T. Weyand", "I. Kostrikov", "J. Philbin"], "venue": "CoRR, abs/1602.05314,", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2016}, {"title": "Roofline: an insightful visual performance model for multicore architectures", "author": ["S. Williams", "A. Waterman", "D.A. Patterson"], "venue": "Commun. ACM, 52(4):65\u201376,", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2009}, {"title": "Optimizing FPGA-based accelerator design for deep convolutional neural networks", "author": ["C. Zhang", "P. Li", "G. Sun", "Y. Guan", "B. Xiao", "J. Cong"], "venue": "Proc. ACM/SIGDA ISFPGA, pages 161\u2013170. ACM,", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2015}, {"title": "DoReFa-Net: Training low bitwidth convolutional neural networks with low bitwidth gradients", "author": ["S. Zhou", "Z. Ni", "X. Zhou", "H. Wen", "Y. Wu", "Y. Zou"], "venue": "CoRR, abs/1606.06160,", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 13, "context": "Convolutional Neural Networks (CNNs) have dramatically improved in recent years, their performance now exceeding that of other visual recognition algorithms [14], and even surpassing human accuracy on certain problems [23, 28].", "startOffset": 157, "endOffset": 161}, {"referenceID": 22, "context": "Convolutional Neural Networks (CNNs) have dramatically improved in recent years, their performance now exceeding that of other visual recognition algorithms [14], and even surpassing human accuracy on certain problems [23, 28].", "startOffset": 218, "endOffset": 226}, {"referenceID": 27, "context": "Convolutional Neural Networks (CNNs) have dramatically improved in recent years, their performance now exceeding that of other visual recognition algorithms [14], and even surpassing human accuracy on certain problems [23, 28].", "startOffset": 218, "endOffset": 226}, {"referenceID": 13, "context": "For instance, AlexNet [14] (the winning entry for ImageNet Large Scale Visual Recognition Competition (ILSVRC) [22] in 2012) required 244 MB of parameters and 1.", "startOffset": 22, "endOffset": 26}, {"referenceID": 21, "context": "For instance, AlexNet [14] (the winning entry for ImageNet Large Scale Visual Recognition Competition (ILSVRC) [22] in 2012) required 244 MB of parameters and 1.", "startOffset": 111, "endOffset": 115}, {"referenceID": 23, "context": "4 billon floating point operations (GFLOP) per image, while VGG-16 [24] from ILSVRC 2014 required 552MB of parameters and 30.", "startOffset": 67, "endOffset": 71}, {"referenceID": 4, "context": "Recently, it has been shown [5, 26, 21, 12, 31] that neural networks can classify accurately using one- or two-bit quantization for weights and activations.", "startOffset": 28, "endOffset": 47}, {"referenceID": 25, "context": "Recently, it has been shown [5, 26, 21, 12, 31] that neural networks can classify accurately using one- or two-bit quantization for weights and activations.", "startOffset": 28, "endOffset": 47}, {"referenceID": 20, "context": "Recently, it has been shown [5, 26, 21, 12, 31] that neural networks can classify accurately using one- or two-bit quantization for weights and activations.", "startOffset": 28, "endOffset": 47}, {"referenceID": 11, "context": "Recently, it has been shown [5, 26, 21, 12, 31] that neural networks can classify accurately using one- or two-bit quantization for weights and activations.", "startOffset": 28, "endOffset": 47}, {"referenceID": 30, "context": "Recently, it has been shown [5, 26, 21, 12, 31] that neural networks can classify accurately using one- or two-bit quantization for weights and activations.", "startOffset": 28, "endOffset": 47}, {"referenceID": 4, "context": "[5], are particularly appealing since they can be implemented almost entirely with binary operations, with the potential to attain performance in the teraoperations per second (TOPS) range on FPGAs.", "startOffset": 0, "endOffset": 3}, {"referenceID": 14, "context": "Convolutional neural networks [15] (CNNs) are a variant of multilayer perceptrons, in which a layer only receives inputs from a small receptive field of the previous layer.", "startOffset": 30, "endOffset": 34}, {"referenceID": 14, "context": ", edges, corners) to be found [15].", "startOffset": 30, "endOffset": 34}, {"referenceID": 7, "context": "Although floating point numbers are a natural choice for handling the small updates that occur during neural network training, the resulting parameters can contain a lot of redundant information [8].", "startOffset": 195, "endOffset": 198}, {"referenceID": 25, "context": "One of several possible dimensions possessing redundancy is precision [26].", "startOffset": 70, "endOffset": 74}, {"referenceID": 11, "context": "Kim and Smaragdis [12] consider full binarization with a predetermined portion of the synapses having zero weight, and all other synapses with a weight of one.", "startOffset": 18, "endOffset": 22}, {"referenceID": 20, "context": "[21] applies convolutional BNNs on the ImageNet dataset with topologies inspired by AlexNet, ResNet and GoogLeNet, reporting top-1 accuracies of up to 51.", "startOffset": 0, "endOffset": 4}, {"referenceID": 30, "context": "[31] explores reduced precision during the forward pass as well as the backward pass, and note that this opens interesting possibilities for training neural networks on FPGAs.", "startOffset": 0, "endOffset": 4}, {"referenceID": 4, "context": "[5] describes how to train fully-connected and convolutional networks with full binarization and batch normalization layers, reporting competitive accuracy on the MNIST, SVHN and CIFAR-10 datasets.", "startOffset": 0, "endOffset": 3}, {"referenceID": 15, "context": "We refer the reader to the work by Misra and Saha [16] for a comprehensive survey.", "startOffset": 50, "endOffset": 54}, {"referenceID": 18, "context": "We cover a recent and representative set of works here, roughly dividing them into four categories based on their basic architecture: 1) a single processing engine [19, 30, 4, 2], usually in the form of a systolic array, which processes each layer sequentially; 2) a streaming architecture [27, 1], consisting of one processing engine per network layer; 3) a vector processor [7] with instructions specific to accelerating the primitives operations of convolutions; and 4) a neurosynaptic processor [6], which implements many digital neurons and their interconnecting weights.", "startOffset": 164, "endOffset": 178}, {"referenceID": 29, "context": "We cover a recent and representative set of works here, roughly dividing them into four categories based on their basic architecture: 1) a single processing engine [19, 30, 4, 2], usually in the form of a systolic array, which processes each layer sequentially; 2) a streaming architecture [27, 1], consisting of one processing engine per network layer; 3) a vector processor [7] with instructions specific to accelerating the primitives operations of convolutions; and 4) a neurosynaptic processor [6], which implements many digital neurons and their interconnecting weights.", "startOffset": 164, "endOffset": 178}, {"referenceID": 3, "context": "We cover a recent and representative set of works here, roughly dividing them into four categories based on their basic architecture: 1) a single processing engine [19, 30, 4, 2], usually in the form of a systolic array, which processes each layer sequentially; 2) a streaming architecture [27, 1], consisting of one processing engine per network layer; 3) a vector processor [7] with instructions specific to accelerating the primitives operations of convolutions; and 4) a neurosynaptic processor [6], which implements many digital neurons and their interconnecting weights.", "startOffset": 164, "endOffset": 178}, {"referenceID": 1, "context": "We cover a recent and representative set of works here, roughly dividing them into four categories based on their basic architecture: 1) a single processing engine [19, 30, 4, 2], usually in the form of a systolic array, which processes each layer sequentially; 2) a streaming architecture [27, 1], consisting of one processing engine per network layer; 3) a vector processor [7] with instructions specific to accelerating the primitives operations of convolutions; and 4) a neurosynaptic processor [6], which implements many digital neurons and their interconnecting weights.", "startOffset": 164, "endOffset": 178}, {"referenceID": 26, "context": "We cover a recent and representative set of works here, roughly dividing them into four categories based on their basic architecture: 1) a single processing engine [19, 30, 4, 2], usually in the form of a systolic array, which processes each layer sequentially; 2) a streaming architecture [27, 1], consisting of one processing engine per network layer; 3) a vector processor [7] with instructions specific to accelerating the primitives operations of convolutions; and 4) a neurosynaptic processor [6], which implements many digital neurons and their interconnecting weights.", "startOffset": 290, "endOffset": 297}, {"referenceID": 0, "context": "We cover a recent and representative set of works here, roughly dividing them into four categories based on their basic architecture: 1) a single processing engine [19, 30, 4, 2], usually in the form of a systolic array, which processes each layer sequentially; 2) a streaming architecture [27, 1], consisting of one processing engine per network layer; 3) a vector processor [7] with instructions specific to accelerating the primitives operations of convolutions; and 4) a neurosynaptic processor [6], which implements many digital neurons and their interconnecting weights.", "startOffset": 290, "endOffset": 297}, {"referenceID": 6, "context": "We cover a recent and representative set of works here, roughly dividing them into four categories based on their basic architecture: 1) a single processing engine [19, 30, 4, 2], usually in the form of a systolic array, which processes each layer sequentially; 2) a streaming architecture [27, 1], consisting of one processing engine per network layer; 3) a vector processor [7] with instructions specific to accelerating the primitives operations of convolutions; and 4) a neurosynaptic processor [6], which implements many digital neurons and their interconnecting weights.", "startOffset": 376, "endOffset": 379}, {"referenceID": 5, "context": "We cover a recent and representative set of works here, roughly dividing them into four categories based on their basic architecture: 1) a single processing engine [19, 30, 4, 2], usually in the form of a systolic array, which processes each layer sequentially; 2) a streaming architecture [27, 1], consisting of one processing engine per network layer; 3) a vector processor [7] with instructions specific to accelerating the primitives operations of convolutions; and 4) a neurosynaptic processor [6], which implements many digital neurons and their interconnecting weights.", "startOffset": 499, "endOffset": 502}, {"referenceID": 29, "context": "[30] describes a single processing engine style architecture, using theoretical roofline models tool to design accelerators optimized for the execution of each layer.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "[19] implement a similar style architecture, but achieved a 3\u00d7 speedup over Zhang et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 29, "context": "[30].", "startOffset": 0, "endOffset": 4}, {"referenceID": 3, "context": "[4] use 16-bit fixed point rather than floating point, and combine several different data reuse strategies.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "[2] have a similar design as Zhang et al.", "startOffset": 0, "endOffset": 3}, {"referenceID": 29, "context": "[30]", "startOffset": 0, "endOffset": 4}, {"referenceID": 26, "context": "Streaming architectures: Venieris and Bouganis [27] proposed a synchronous dataflow (SDF) model for mapping CNNs to FPGAs, which is a similar approach to ours.", "startOffset": 47, "endOffset": 51}, {"referenceID": 0, "context": "[1] implement fully-connected ternary-weight neural networks with streaming and report up to 255K frames per second on the MNIST dataset, but concentrate on the training aspect for those networks.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[7] describe a programmable ConvNet Processor (CNP), which is a RISC vector processor with specific macro-instructions for CNNs including 2D convolutions, 2D spatial pooling, dot product and an elementwise non-linear mapping function.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "Neurosynaptic processors: TrueNorth [6] is a low power, parallel ASIC with 4096 neurosynaptic cores, each implementing 256 binary inputs, 256 neurons and a 256 \u00d7 256 array of synapses.", "startOffset": 36, "endOffset": 39}, {"referenceID": 14, "context": "This currently comes at the cost of a small drop in accuracy for larger networks, however we believe a) there are use cases that do not require the highest level of accuracy, or can be solved with smaller networks (such as classification of playing cards or handwritten digits [15]) and b) that the accuracy can be improved by increasing network sizes [26].", "startOffset": 277, "endOffset": 281}, {"referenceID": 25, "context": "This currently comes at the cost of a small drop in accuracy for larger networks, however we believe a) there are use cases that do not require the highest level of accuracy, or can be solved with smaller networks (such as classification of playing cards or handwritten digits [15]) and b) that the accuracy can be improved by increasing network sizes [26].", "startOffset": 352, "endOffset": 356}, {"referenceID": 28, "context": "To estimate and compare BNN performance with fixedpoint CNN, we use a roofline model [29] which considers memory bandwidth, peak computational performance and arithmetic intensity (the number of mathematical operations performed for each byte of off-chip memory read or written).", "startOffset": 85, "endOffset": 89}, {"referenceID": 30, "context": "In particular, we consider the binarized [31, 21] and 8-bit fixed-point [25] implementations of the popular AlexNet [14], both of which require 1.", "startOffset": 41, "endOffset": 49}, {"referenceID": 20, "context": "In particular, we consider the binarized [31, 21] and 8-bit fixed-point [25] implementations of the popular AlexNet [14], both of which require 1.", "startOffset": 41, "endOffset": 49}, {"referenceID": 24, "context": "In particular, we consider the binarized [31, 21] and 8-bit fixed-point [25] implementations of the popular AlexNet [14], both of which require 1.", "startOffset": 72, "endOffset": 76}, {"referenceID": 13, "context": "In particular, we consider the binarized [31, 21] and 8-bit fixed-point [25] implementations of the popular AlexNet [14], both of which require 1.", "startOffset": 116, "endOffset": 120}, {"referenceID": 16, "context": "Using the methodology described in [17], we develop a roofline model for a Xilinx Zynq UltraScale+ ZU19EG FPGA.", "startOffset": 35, "endOffset": 39}, {"referenceID": 4, "context": "For instance, [5] mentions 6 cycles per 32 synapses (64 binary operations) on recent NVIDIA GPUs, which would yield a computational peak of about 26 TOPS on a Tesla K40 with 2880 cores running at 875 MHz, and 16666 images per second for binarized AlexNet.", "startOffset": 14, "endOffset": 17}, {"referenceID": 25, "context": "A tradeoff between network size, precision and accuracy exists [26] so if one would like to achieve a certain classification accuracy for a particular problem, which approach leads to the most efficient solution? 1) A regular ANN with floating point precision? 2) A larger network, but a BNN? To gain more insight into this issue, we conducted a set of experiments on the MNIST dataset that compare accuracy of floating point and binary precision for the same topology.", "startOffset": 63, "endOffset": 67}, {"referenceID": 4, "context": "[5].", "startOffset": 0, "endOffset": 3}, {"referenceID": 25, "context": "Since the space of possible network topologies that can be trained is infinite, we adopted the approach in [26] to simplify the problem.", "startOffset": 107, "endOffset": 111}, {"referenceID": 25, "context": "[26], as the network size increases, the difference in accuracy between low precision networks and floating point networks decreases; and 2) in order to achieve the same level of accuracy as floating point networks, BNNs require 2\u201311\u00d7 more parameters and operations.", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "accuracy [10].", "startOffset": 9, "endOffset": 13}, {"referenceID": 4, "context": "We assume that the methodology described in [5] is used for training all BNNs in this paper, where all BNN layers have the following properties (unless otherwise stated):", "startOffset": 44, "endOffset": 47}, {"referenceID": 10, "context": "All BNN layers use batch normalization [11] on convolutional or fully connected layer outputs, then apply the sign function to determine the output activation.", "startOffset": 39, "endOffset": 43}, {"referenceID": 4, "context": "The networks described in [5] perform pooling prior to activations, i.", "startOffset": 26, "endOffset": 29}, {"referenceID": 3, "context": "In terms of the taxonomy described in [4], this architecture is both weight stationary (since each weight remains local to the PE) and output stationary (since each popcount computation remains local to the PE).", "startOffset": 38, "endOffset": 41}, {"referenceID": 2, "context": "Convolutions can be lowered to matrix-matrix multiplications [3], which is the approach followed in this work.", "startOffset": 61, "endOffset": 64}, {"referenceID": 4, "context": "Future work will look into what impact this has on the accuracy of trained networks, but early experiments suggest that there is very little difference in accuracy, with respect to [5].", "startOffset": 181, "endOffset": 184}, {"referenceID": 26, "context": "[27] describes a method for folding neural networks expressed as streaming dataflow graphs, with focus on formalizing the folding and design space exploration.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "To evaluate Finn, we created a number of prototypes that accelerate BNNs inference on the MNIST [15] (28\u00d7 28 handwritten digits), CIFAR-10 [13] (32 \u00d7 32 color images in 10 categories) and cropped SVHN [18] (32 \u00d7 32 images of Street View House Numbers) datasets.", "startOffset": 96, "endOffset": 100}, {"referenceID": 12, "context": "To evaluate Finn, we created a number of prototypes that accelerate BNNs inference on the MNIST [15] (28\u00d7 28 handwritten digits), CIFAR-10 [13] (32 \u00d7 32 color images in 10 categories) and cropped SVHN [18] (32 \u00d7 32 images of Street View House Numbers) datasets.", "startOffset": 139, "endOffset": 143}, {"referenceID": 17, "context": "To evaluate Finn, we created a number of prototypes that accelerate BNNs inference on the MNIST [15] (28\u00d7 28 handwritten digits), CIFAR-10 [13] (32 \u00d7 32 color images in 10 categories) and cropped SVHN [18] (32 \u00d7 32 images of Street View House Numbers) datasets.", "startOffset": 201, "endOffset": 205}, {"referenceID": 4, "context": "\u2022 CNV is a convolutional network topology inspired by BinaryNet [5] and VGG-16 [24].", "startOffset": 64, "endOffset": 67}, {"referenceID": 23, "context": "\u2022 CNV is a convolutional network topology inspired by BinaryNet [5] and VGG-16 [24].", "startOffset": 79, "endOffset": 83}, {"referenceID": 0, "context": "For the MNIST dataset, we achieve an FPS which is over 48/6\u00d7 over the nearest highest throughput design [1] for our SFC-max/LFC-max designs respectively.", "startOffset": 104, "endOffset": 107}, {"referenceID": 0, "context": "[1] our LFC-max design outperforms their nearest accuracy design by over 6/1.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "For other datasets, our CNV-max design outperforms TrueNorth [6] for FPS by over 17/8\u00d7 for CIFAR-10 / SVHN datasets respectively, while achieving 9.", "startOffset": 61, "endOffset": 64}, {"referenceID": 18, "context": "[19], and 2.", "startOffset": 0, "endOffset": 4}, {"referenceID": 8, "context": "[9].", "startOffset": 0, "endOffset": 3}, {"referenceID": 26, "context": "Supporting external memory, multi-FPGAs implementations and reconfiguration [27] could improve the utility of our approach.", "startOffset": 76, "endOffset": 80}], "year": 2016, "abstractText": "Research has shown that convolutional neural networks contain significant redundancy, and high classification accuracy can be obtained even when weights and activations are reduced from floating point to binary values. In this paper, we present Finn, a framework for building fast and flexible FPGA accelerators using a flexible heterogeneous streaming architecture. By utilizing a novel set of optimizations that enable efficient mapping of binarized neural networks to hardware, we implement fully connected, convolutional and pooling layers, with per-layer compute resources being tailored to user-provided throughput requirements. On a ZC706 embedded FPGA platform drawing less than 25 W total system power, we demonstrate up to 12.3 million image classifications per second with 0.31 \u03bcs latency on the MNIST dataset with 95.8% accuracy, and 21906 image classifications per second with 283 \u03bcs latency on the CIFAR-10 and SVHN datasets with respectively 80.1% and 94.9% accuracy. To the best of our knowledge, ours are the fastest classification rates reported to date on these benchmarks.", "creator": "TeX"}}}