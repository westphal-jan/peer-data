{"id": "1701.01574", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Jan-2017", "title": "Real Multi-Sense or Pseudo Multi-Sense: An Approach to Improve Word Representation", "abstract": "Previous researches have shown that learning multiple representations for polysemous words can improve the performance of word embeddings on many tasks. However, this leads to another problem. Several vectors of a word may actually point to the same meaning, namely pseudo multi-sense. In this paper, we introduce the concept of pseudo multi-sense, and then propose an algorithm to detect such cases. With the consideration of the detected pseudo multi-sense cases, we try to refine the existing word embeddings to eliminate the influence of pseudo multi-sense. Moreover, we apply our algorithm on previous released multi-sense word embeddings and tested it on artificial word similarity tasks and the analogy task. The result of the experiments shows that diminishing pseudo multi-sense can improve the quality of word representations. Thus, our method is actually an efficient way to reduce linguistic complexity.", "histories": [["v1", "Fri, 6 Jan 2017 08:52:41 GMT  (18kb)", "http://arxiv.org/abs/1701.01574v1", "11 pages in CL4LC 2016"]], "COMMENTS": "11 pages in CL4LC 2016", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["haoyue shi", "caihua li", "junfeng hu"], "accepted": false, "id": "1701.01574"}, "pdf": {"name": "1701.01574.pdf", "metadata": {"source": "CRF", "title": "Real Multi-Sense or Pseudo Multi-Sense: An Approach to Improve Word Representation", "authors": ["Haoyue Shi", "Caihua Li", "Junfeng Hu"], "emails": ["hyshi@pku.edu.cn", "peterli@pku.edu.cn", "hujf@pku.edu.cn"], "sections": [{"heading": null, "text": "ar Xiv: 170 1.01 574v 1 [cs.C L] 6J an2 017"}, {"heading": "1 Introduction", "text": "In fact, it is the case that most of them are in a position to go into a different world, in which they are able to move, in which they move, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they, in which they live, in which they live, in which they, in which they live, in which they live, in which they live, in which they live."}, {"heading": "2 Background and related work", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Distributional word representations", "text": "Since Bengio et al. (2003) applied neural networks to language models that treat word embeddings as parameters, thus enabling us to learn the language model and word embeddings simultaneously, many researchers have proposed other neural network models (Mnih and Hinton, 2007; Collobert and Weston, 2008; Mikolov et al., 2013a) to improve both efficiency and accuracy. Furthermore, hierarchical Softmax by Morin and Bengio (2005), contrasting noise estimates by Mnih and Kavukcuoglu (2013), and negative sampling by Mikolov et al. (2013c) enable precise word embeddings to be learned in a short time."}, {"heading": "2.2 Multi-sense word embeddings", "text": "Most vector-space models (VSMs) represent a word with only one vector that clearly does not capture homonymy and polysemy, and so Huang et al. (2012) proposed a method to generate context embeddings in the following ways: First, they generate word embeddings and calculate the context embeddings, then cluster the context embeddings, and the result is used to redefine each occurrence of each word in the corpus. Third, the model they proposed is applied to the marked corpus to generate the multi-sense embeddings. Chen et al. (2014) took into account external knowledge bases and developed a model to learn a separate vector for each sense predefined by WordNet (Miller, 1995). Neelakantan et al. (2015) improved multi-sense embeddings models by dropping the assumption that each word should have an equal number of words, rather than a narrow number of them."}, {"heading": "2.3 WordNet and WordNet domain knowledge", "text": "WordNet (Miller, 1995) is a large lexical database of English. Nouns, verbs, adjectives and adverbs are grouped into groups of cognitive synonyms, namely synsets, each expressing a specific concept. Synsets are represented by a word, a pos tag and a label, and linked by conceptual-semantic and lexical relationships (hypernymia / hyponymy). Chen et al. (2014) used WordNet to improve word embeddings. Magnini and Cavaglia (2000) and Bentivogli et al. (2004) presented a WordNet domain hierarchy, which is a language-independent resource composed of 164 domain labels."}, {"heading": "2.4 Vector space projection", "text": "Although bilingual data always play an important role in the modern statistical machine translation system, they were unable to map the missing word and phrase entries between two languages until Mikolov et al. (2013b) proposed a simple but effective method for expanding dictionaries and translation tables. The basic idea of this novel method is to learn a linear projection between languages using a small bilingual dictionary, but little assumptions were made about the languages, which has proved to be able to accurately project the vector representation of any word from the source space onto the target space. Our vector space projection algorithm is very similar to this."}, {"heading": "3 Pseudo multi-sense detection and elimination by vector space projection", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Domain based pseudo multi-sense detection", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1.1 Direct domain similarity", "text": "Given a word and its context, we humans can easily determine the domains to which that word belongs. WordNet makes it convenient for the user to obtain the domains of all the synsets of a word. To determine the domain of a sense given the multi-sense word embedding, we can intuitively define the probability that the kest sense of the word w belongs to the domain d asPD (w, k, d)."}, {"heading": "3.1.2 Semantic hierarchical similarity", "text": "In the knowledge base we have applied, domain knowledge is sometimes insufficient to detect pseudo-ambiguities (especially for some abstract words); for example, it is difficult to determine which area the word extract belongs to. Besides, based on pseudo-ambiguities as a supplement (2012), the Extended WordNet Domain (w) cannot reach the precision of 100%. Therefore, we tend to apply semantic hierarchy, especially hypernism relationships, to improve our pseudo-ambiguity, since hypernism somehow contains domain information. With WordNet, we can also obtain the semantic relationships (e.g. hypernism, hyponymy, synonymy) of synthetics; by looking at the DAG structure of semantic relationships, for hypernism of a particular word that is closer to hypernism the more information it contains."}, {"heading": "3.2 Pseudo multi-sense elimination", "text": "Once we have the existing word embeddings, we assume that we have a recognized pseudo-multisense group G = {wk1, wk2,..., wkn} in which wk1, wk2,..., wkn are the senses of the word w that have the same meaning. Thus, we can find a representative vector for the group. Let us be the corresponding vectors of wki, and vr (G) the representative vector for group G. Such a vector vr (G) can be randomly selected from {vs (w, k1), vs (w, k2),..., vs (w, kn), or simply the middle vector of them. Other methods for calculating vr (G) are also worth trying, if reasonably so. Inspired by Mikolov et al. (2013b), we assume that there is a transition matrix that can be vented by all pseudo-multisense groups, G, wki, G, wki."}, {"heading": "4 Experiments", "text": "We use our method on the published word embeddings of Huang et al. (2012) and Neelakantan et al. (2015), both trained on the same Wikipedia corpus, and show the performance of our method based on the next adjacent task, word similarity tasks and the analogy task. In the following parts, MSSG and NP-MSSG are word embeddings by Neelakantan et al. (2015); 50d and 300d are the dimensions of vector space. The vector space released by Huang et al. (2012) is 50-dimensional."}, {"heading": "4.1 Nearest Neighbors", "text": "As we have suspected, earlier methods of multi-sense embedding would produce a lot of pseudo-multi-sense examples. For the sake of simplicity, we focus only on the semantic relationship in the qualitative evaluation part. We extracted the most likely hypernym for each sense of some example words from Eq (4) using the semantic relations provided by WordNet (Miller, 1995). If different representations of a word have the same hypernym, we consider them to be pseudo-multiple senses. In Table 1, for each sense of each example word, we show the closest neighbors with multiple word embedding and our result of pseudo-multi-sense embedding. For most representations, we obtained reasonable hypernyms according to their closest neighbors. However, there are also some unexpected cases from the result based on the word vectors of hyperdomination published by Huang et al. (2012), while no such cases are found in the neelakant vectors released."}, {"heading": "4.2 Word Similarity", "text": "For each word in embedded vector space, we first determine the pseudo-multisense with Eq (5), then try to minimize the distance between vectors belonging to the same pseudo-multisense group, arguing that they actually represent the same meaning in vector space.L = \u2211 (x, xr) | | \u03a6x \u2212 xr | | 2 (7), where x is a vector belonging to a pseudo-multisense group and xr is the representative vector of the corresponding group. In our experiments, we tried both random sampling and the calculation of mean vectors in order to obtain such a representative vector. We then try to minimize the distance between vectors belonging to the same pseudo-multisense group, since we argue that they actually represent the same meaning in vector space.L = \u2211 (x, xr) | \u03a6x \u2212 xr | | 2 (7), where x is a vector belonging to a pseudo-multisense group and xr is the representative vector of the corresponding group."}, {"heading": "4.2.1 Similarity Metrics", "text": "The similarity here is a metric between words used to evaluate the performance of word embeddings used to compare with human judgments, unlike the similarities we introduced in Section 3, which are used to capture pseudo-plural senses. Neelakantan et al. (2015) introduced three metrics to calculate the similarity between words in multiple senses, namely avgSim, avgSimC and localSim, defined by the following equations. avgSim (w, w, i) = 1K1K \"K\" K \"i = 1K\" J = 1s (vs, i), vs (w, i), vs (w, j)) (8), where K and K \"\u2032 are the sense numbers for w and w,\" vs (w, i, i) is the vector of the ith word w, and S (vs, i) is the similarity between vectors vs, \"in the context,\" i, \"and\" Kali. \""}, {"heading": "4.2.2 WordSim-353", "text": "WordSim-353 is a standard dataset for evaluating the quality of word vectors introduced by Finkelstein et al. (2001) and comprising 353 pairs of nouns (without context). Each pair receives 13 to 16 human judgments on similarity and kinship on a scale from 0 to 10. For example, pair (stock exchange, market) gets the score of 8.08, while pair (stock, egg) only gets the score of 1.81. In this dataset, since the word context is not given, we can only calculate the avgSim for each word pair to evaluate our method. The result is shown in Table 2."}, {"heading": "4.2.3 SCWS", "text": "The Stanford Contextual Word Similarity (SCWS) dataset proposed by Huang et al. (2012) is also a standard dataset for quantitative evaluation of word embedding performance. It contains 2,003 pairs of words and the context in which they occur. As Neelakantan et al. (2015), we also report in their paper on the Spearman ranking correlation between the output similarities of a model and human judgments. We also tried to determine both random samples and the mean vector to determine the representative vector for each pseudo-multisense group. The result of our experiments is shown in Table 3."}, {"heading": "4.3 Analogy", "text": "Analogy task is another method to evaluate the performance of word embeddings. In the sense of word embeddings, there should be an algebraic relationship v (A) \u2212 v (B) = v (C) \u2212 v (D), where v (A) is the vector of word A in word embeddings (Mikolov et al., 2013a). On the basis of such a relationship, we conduct the following experiment, which shows that our method is capable of improving the quality of word embeddings in the multisense range (A). To compare the quality of different versions of word embeddings, our experiment runs on the Semantic-Syntactic Word Relationship Dataset, which contains five types of semantic questions and nine types of syntactic questions, as shown in Table 4, including 19544 such word embeddings kkwally.For every quadruple in the test dataset, we mark it as Win the sense of 1, 2, 2, wwector, 4, 4, 4, 4, 4, 4, 4, 4, 4."}, {"heading": "5 Conclusion and future work", "text": "In this essay, we introduced the concept of pseudo-multisense, i.e. the word-embedding model often embeds a meaning in several senses to describe the common problem in multi-sense embeddings. Then, we proposed a method based on both domains and semantic relationships to recognize such cases. Furthermore, we trained a global transition matrix based on the detected pseudo-multisense from the given word embeddings, which is used to actually eliminate the distance between the senses. The evaluation of our pseudo-multisense eliminated vector showed that the detection and elimination of pseudo-multisense significantly improved the ability of each vector in the word embeddings to represent an exact meaning. We suggest that the following research guidelines could be taken into account. \u2022 For the detection of pseudo-multisense eliminated vector showed that tactical information and other information that we have to extract the existing sense of multiple could be a reasonable idea from the existing one."}], "references": [{"title": "A neural probabilistic language model. journal of machine learning research, 3(Feb):1137\u20131155", "author": ["Bengio et al.2003] Yoshua Bengio", "R\u00e9jean Ducharme", "Pascal Vincent", "Christian Jauvin"], "venue": null, "citeRegEx": "Bengio et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2003}, {"title": "Revising the wordnet domains hierarchy: semantics, coverage and balancing", "author": ["Pamela Forner", "Bernardo Magnini", "Emanuele Pianta"], "venue": "In Proceedings of the Workshop on Multilingual Linguistic Ressources,", "citeRegEx": "Bentivogli et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Bentivogli et al\\.", "year": 2004}, {"title": "A unified model for word sense representation and disambiguation", "author": ["Chen et al.2014] Xinxiong Chen", "Zhiyuan Liu", "Maosong Sun"], "venue": "In EMNLP,", "citeRegEx": "Chen et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2014}, {"title": "Syntax-aware multi-sense word embeddings for deep compositional models of meaning", "author": ["Cheng", "Kartsaklis2015] Jianpeng Cheng", "Dimitri Kartsaklis"], "venue": "arXiv preprint arXiv:1508.02354", "citeRegEx": "Cheng et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Cheng et al\\.", "year": 2015}, {"title": "A unified architecture for natural language processing: Deep neural networks with multitask learning", "author": ["Collobert", "Weston2008] Ronan Collobert", "Jason Weston"], "venue": "In Proceedings of the 25th international conference on Machine learning,", "citeRegEx": "Collobert et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2008}, {"title": "Placing search in context: The concept revisited", "author": ["Evgeniy Gabrilovich", "Yossi Matias", "Ehud Rivlin", "Zach Solan", "Gadi Wolfman", "Eytan Ruppin"], "venue": "In Proceedings of the 10th international conference on World Wide Web,", "citeRegEx": "Finkelstein et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Finkelstein et al\\.", "year": 2001}, {"title": "A graph-based method to improve wordnet domains", "author": ["German Rigau", "Mauro Castillo"], "venue": "In International Conference on Intelligent Text Processing and Computational Linguistics,", "citeRegEx": "Gonz\u00e1lez et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Gonz\u00e1lez et al\\.", "year": 2012}, {"title": "Improving word representations via global context and multiple word prototypes", "author": ["Eric H Huang", "Richard Socher", "Christopher D Manning", "Andrew Y Ng"], "venue": "In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume", "citeRegEx": "Huang et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2012}, {"title": "Do multi-sense embeddings improve natural language understanding? arXiv preprint arXiv:1506.01070", "author": ["Li", "Jurafsky2015] Jiwei Li", "Dan Jurafsky"], "venue": null, "citeRegEx": "Li et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Li et al\\.", "year": 2015}, {"title": "Topical word embeddings", "author": ["Liu et al.2015] Yang Liu", "Zhiyuan Liu", "Tat-Seng Chua", "Maosong Sun"], "venue": "In AAAI,", "citeRegEx": "Liu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2015}, {"title": "Integrating subject field codes into wordnet", "author": ["Magnini", "Cavaglia2000] Bernardo Magnini", "Gabriela Cavaglia"], "venue": null, "citeRegEx": "Magnini et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Magnini et al\\.", "year": 2000}, {"title": "Recurrent neural network based language model", "author": ["Martin Karafi\u00e1t", "Lukas Burget", "Jan Cernock\u1ef3", "Sanjeev Khudanpur"], "venue": "In Interspeech,", "citeRegEx": "Mikolov et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2010}, {"title": "Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781", "author": ["Kai Chen", "Greg Corrado", "Jeffrey Dean"], "venue": null, "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Exploiting similarities among languages for machine translation", "author": ["Quoc V Le", "Ilya Sutskever"], "venue": "arXiv preprint arXiv:1309.4168", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Linguistic regularities in continuous space word representations", "author": ["Wen-tau Yih", "Geoffrey Zweig"], "venue": "In HLT-NAACL,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Wordnet: a lexical database for english", "author": ["George A Miller"], "venue": "Communications of the ACM,", "citeRegEx": "Miller.,? \\Q1995\\E", "shortCiteRegEx": "Miller.", "year": 1995}, {"title": "Three new graphical models for statistical language modelling", "author": ["Mnih", "Hinton2007] Andriy Mnih", "Geoffrey Hinton"], "venue": "In Proceedings of the 24th international conference on Machine learning,", "citeRegEx": "Mnih et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2007}, {"title": "Learning word embeddings efficiently with noise-contrastive estimation", "author": ["Mnih", "Kavukcuoglu2013] Andriy Mnih", "Koray Kavukcuoglu"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Mnih et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2013}, {"title": "Hierarchical probabilistic neural network language model", "author": ["Morin", "Bengio2005] Frederic Morin", "Yoshua Bengio"], "venue": "In Aistats,", "citeRegEx": "Morin et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Morin et al\\.", "year": 2005}, {"title": "Efficient non-parametric estimation of multiple embeddings per word in vector space. arXiv preprint arXiv:1504.06654", "author": ["Jeevan Shankar", "Alexandre Passos", "Andrew McCallum"], "venue": null, "citeRegEx": "Neelakantan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Neelakantan et al\\.", "year": 2015}, {"title": "A simple and efficient method to generate word sense representations. arXiv preprint arXiv:1412.6045", "author": ["Pina", "Johansson2014] Luis Nieto Pina", "Richard Johansson"], "venue": null, "citeRegEx": "Pina et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pina et al\\.", "year": 2014}, {"title": "Multi-prototype vector-space models of word meaning", "author": ["Reisinger", "Mooney2010] Joseph Reisinger", "Raymond J Mooney"], "venue": "In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics,", "citeRegEx": "Reisinger et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Reisinger et al\\.", "year": 2010}], "referenceMentions": [{"referenceID": 0, "context": "An intuitive idea is to encode one word into a single vector, which contains the semantic information of the word in corpus (Bengio et al., 2003; Collobert and Weston, 2008; Mnih and Hinton, 2007; Mikolov et al., 2010).", "startOffset": 124, "endOffset": 218}, {"referenceID": 11, "context": "An intuitive idea is to encode one word into a single vector, which contains the semantic information of the word in corpus (Bengio et al., 2003; Collobert and Weston, 2008; Mnih and Hinton, 2007; Mikolov et al., 2010).", "startOffset": 124, "endOffset": 218}, {"referenceID": 7, "context": "Considering the polysemous words, some previous approaches have learned multiple embeddings for a word, discriminating different senses by their context, related syntax and topics (Reisinger and Mooney, 2010; Huang et al., 2012; Chen et al., 2014; Pina and Johansson, 2014; Neelakantan et al., 2015; Cheng and Kartsaklis, 2015; Liu et al., 2015).", "startOffset": 180, "endOffset": 345}, {"referenceID": 2, "context": "Considering the polysemous words, some previous approaches have learned multiple embeddings for a word, discriminating different senses by their context, related syntax and topics (Reisinger and Mooney, 2010; Huang et al., 2012; Chen et al., 2014; Pina and Johansson, 2014; Neelakantan et al., 2015; Cheng and Kartsaklis, 2015; Liu et al., 2015).", "startOffset": 180, "endOffset": 345}, {"referenceID": 19, "context": "Considering the polysemous words, some previous approaches have learned multiple embeddings for a word, discriminating different senses by their context, related syntax and topics (Reisinger and Mooney, 2010; Huang et al., 2012; Chen et al., 2014; Pina and Johansson, 2014; Neelakantan et al., 2015; Cheng and Kartsaklis, 2015; Liu et al., 2015).", "startOffset": 180, "endOffset": 345}, {"referenceID": 9, "context": "Considering the polysemous words, some previous approaches have learned multiple embeddings for a word, discriminating different senses by their context, related syntax and topics (Reisinger and Mooney, 2010; Huang et al., 2012; Chen et al., 2014; Pina and Johansson, 2014; Neelakantan et al., 2015; Cheng and Kartsaklis, 2015; Liu et al., 2015).", "startOffset": 180, "endOffset": 345}, {"referenceID": 0, "context": "An intuitive idea is to encode one word into a single vector, which contains the semantic information of the word in corpus (Bengio et al., 2003; Collobert and Weston, 2008; Mnih and Hinton, 2007; Mikolov et al., 2010). There is a consensus that natural languages always include lots of polysemous words. For example, when the word star appears together with words like planet, satellite, it may roughly denote a kind of celestial body; when star appears with words like movie, song, drama, it may stand for a famous person. For most cases, we human beings can easily point out which sense a word belongs to based on its context. Considering the polysemous words, some previous approaches have learned multiple embeddings for a word, discriminating different senses by their context, related syntax and topics (Reisinger and Mooney, 2010; Huang et al., 2012; Chen et al., 2014; Pina and Johansson, 2014; Neelakantan et al., 2015; Cheng and Kartsaklis, 2015; Liu et al., 2015). The authors also provided methods to disambiguate among the multiple representations. Li and Jurafsky (2015) have demonstrated that multi-sense word embeddings could be helpful to improve the performance on many NLP and NLU tasks.", "startOffset": 125, "endOffset": 1086}, {"referenceID": 0, "context": "An intuitive idea is to encode one word into a single vector, which contains the semantic information of the word in corpus (Bengio et al., 2003; Collobert and Weston, 2008; Mnih and Hinton, 2007; Mikolov et al., 2010). There is a consensus that natural languages always include lots of polysemous words. For example, when the word star appears together with words like planet, satellite, it may roughly denote a kind of celestial body; when star appears with words like movie, song, drama, it may stand for a famous person. For most cases, we human beings can easily point out which sense a word belongs to based on its context. Considering the polysemous words, some previous approaches have learned multiple embeddings for a word, discriminating different senses by their context, related syntax and topics (Reisinger and Mooney, 2010; Huang et al., 2012; Chen et al., 2014; Pina and Johansson, 2014; Neelakantan et al., 2015; Cheng and Kartsaklis, 2015; Liu et al., 2015). The authors also provided methods to disambiguate among the multiple representations. Li and Jurafsky (2015) have demonstrated that multi-sense word embeddings could be helpful to improve the performance on many NLP and NLU tasks. However, this leads to another problem. It\u2019s much more difficult for computer than human beings to detect whether two appearances of a same word stand for the same sense. Moreover, the contexts may be totally different even if these appearances belong to the same meaning based on human judgement. Previous multi-sense word embedding approaches often tend to embed a word in such situation into more than one vector by mistake (actually, they have the same meaning and should be embedded into only one vector). Consider three different representations of word bear learnt by the method introduced by Neelakantan et al. (2015), which are shown by their nearest neighbors in the vector space MSSG-50d.", "startOffset": 125, "endOffset": 1834}, {"referenceID": 5, "context": "(2013a), and also on WordSim-353 (Finkelstein et al., 2001) and SCWS (Huang et al.", "startOffset": 33, "endOffset": 59}, {"referenceID": 7, "context": ", 2001) and SCWS (Huang et al., 2012) dataset which include human judgements on similarity between pairs of words.", "startOffset": 17, "endOffset": 37}, {"referenceID": 9, "context": "We also suggest this approach can improve the performance on real world NLU tasks by evaluating the algorithm on the analogy test dataset introduced by Mikolov et al. (2013a), and also on WordSim-353 (Finkelstein et al.", "startOffset": 152, "endOffset": 175}, {"referenceID": 0, "context": "1 Distributional word representations Since Bengio et al. (2003) applied neural network to language model, which treats word embeddings as parameters and thus it allows us to learn the language model and word embeddings at the same time, many researchers have proposed other neural network models (Mnih and Hinton, 2007; Collobert and Weston, 2008; Mikolov et al.", "startOffset": 44, "endOffset": 65}, {"referenceID": 0, "context": "1 Distributional word representations Since Bengio et al. (2003) applied neural network to language model, which treats word embeddings as parameters and thus it allows us to learn the language model and word embeddings at the same time, many researchers have proposed other neural network models (Mnih and Hinton, 2007; Collobert and Weston, 2008; Mikolov et al., 2013a) to improve in both efficiency and accuracy. What\u2019s more, hierarchical softmax by Morin and Bengio (2005), noise contrastive estimation by Mnih and Kavukcuoglu (2013) and negative sampling by Mikolov et al.", "startOffset": 44, "endOffset": 477}, {"referenceID": 0, "context": "1 Distributional word representations Since Bengio et al. (2003) applied neural network to language model, which treats word embeddings as parameters and thus it allows us to learn the language model and word embeddings at the same time, many researchers have proposed other neural network models (Mnih and Hinton, 2007; Collobert and Weston, 2008; Mikolov et al., 2013a) to improve in both efficiency and accuracy. What\u2019s more, hierarchical softmax by Morin and Bengio (2005), noise contrastive estimation by Mnih and Kavukcuoglu (2013) and negative sampling by Mikolov et al.", "startOffset": 44, "endOffset": 538}, {"referenceID": 0, "context": "1 Distributional word representations Since Bengio et al. (2003) applied neural network to language model, which treats word embeddings as parameters and thus it allows us to learn the language model and word embeddings at the same time, many researchers have proposed other neural network models (Mnih and Hinton, 2007; Collobert and Weston, 2008; Mikolov et al., 2013a) to improve in both efficiency and accuracy. What\u2019s more, hierarchical softmax by Morin and Bengio (2005), noise contrastive estimation by Mnih and Kavukcuoglu (2013) and negative sampling by Mikolov et al. (2013c) make it possible to learn accurate word embeddings in a short time.", "startOffset": 44, "endOffset": 586}, {"referenceID": 15, "context": "(2014) took external knowledge base into consideration and built a model to learn a separate vector for each sense pre-defined by WordNet (Miller, 1995).", "startOffset": 138, "endOffset": 152}, {"referenceID": 6, "context": "And thus, Huang et al. (2012) proposed a method to generate the context embeddings in the following way.", "startOffset": 10, "endOffset": 30}, {"referenceID": 2, "context": "Chen et al. (2014) took external knowledge base into consideration and built a model to learn a separate vector for each sense pre-defined by WordNet (Miller, 1995).", "startOffset": 0, "endOffset": 19}, {"referenceID": 2, "context": "Chen et al. (2014) took external knowledge base into consideration and built a model to learn a separate vector for each sense pre-defined by WordNet (Miller, 1995). Neelakantan et al. (2015) improved multi-sense word embedding model by dropping the assumption that each word should have the same number of senses, and proposed a non-parametric model to automatically discover a varying number of senses per word type.", "startOffset": 0, "endOffset": 192}, {"referenceID": 2, "context": "Chen et al. (2014) took external knowledge base into consideration and built a model to learn a separate vector for each sense pre-defined by WordNet (Miller, 1995). Neelakantan et al. (2015) improved multi-sense word embedding model by dropping the assumption that each word should have the same number of senses, and proposed a non-parametric model to automatically discover a varying number of senses per word type. Cheng and Kartsaklis (2015) proposed a syntax-aware approach for multi-sense word embeddings.", "startOffset": 0, "endOffset": 447}, {"referenceID": 15, "context": "3 WordNet and WordNet domain knowledge WordNet (Miller, 1995) is a large lexical database of English.", "startOffset": 47, "endOffset": 61}, {"referenceID": 1, "context": "Chen et al. (2014) used WordNet to improve word embeddings.", "startOffset": 0, "endOffset": 19}, {"referenceID": 1, "context": "Chen et al. (2014) used WordNet to improve word embeddings. Magnini and Cavaglia (2000) and Bentivogli et al.", "startOffset": 0, "endOffset": 88}, {"referenceID": 1, "context": "Magnini and Cavaglia (2000) and Bentivogli et al. (2004) presented a WordNet Domains Hierarchy, which is a language-independent resourse composed of 164 domain labels.", "startOffset": 32, "endOffset": 57}, {"referenceID": 11, "context": "4 Vector space projection Even though bilingual data always plays an important role in the modern statistical machine translation system, it had failed to map the missing word and phrase entries between two languages until Mikolov et al. (2013b) proposed a simple but effective method to extend dictionaries and translation tables.", "startOffset": 223, "endOffset": 246}, {"referenceID": 6, "context": "What\u2019s more, based on Gonz\u00e1lez et al. (2012), the Extended WordNet Domain cannot reach the precision of 100%.", "startOffset": 22, "endOffset": 45}, {"referenceID": 11, "context": "Inspired by Mikolov et al. (2013b), we assume there is a transition matrix, by which for all pseudo multi-sense group G, \u2200wki \u2208 G, vwki can be projected to vr(G).", "startOffset": 12, "endOffset": 35}, {"referenceID": 7, "context": "We apply our method to the released word embeddings by Huang et al. (2012) and Neelakantan et al.", "startOffset": 55, "endOffset": 75}, {"referenceID": 7, "context": "We apply our method to the released word embeddings by Huang et al. (2012) and Neelakantan et al. (2015), which were both trained on the same Wikipedia corpus, and display the performance of our method based on the nearest neighbor task, word similarity tasks and the analogy task.", "startOffset": 55, "endOffset": 105}, {"referenceID": 7, "context": "We apply our method to the released word embeddings by Huang et al. (2012) and Neelakantan et al. (2015), which were both trained on the same Wikipedia corpus, and display the performance of our method based on the nearest neighbor task, word similarity tasks and the analogy task. In the following parts, MSSG and NP-MSSG are word embeddings released by Neelakantan et al. (2015); 50d and 300d are the dimensions of the vector space.", "startOffset": 55, "endOffset": 381}, {"referenceID": 7, "context": "We apply our method to the released word embeddings by Huang et al. (2012) and Neelakantan et al. (2015), which were both trained on the same Wikipedia corpus, and display the performance of our method based on the nearest neighbor task, word similarity tasks and the analogy task. In the following parts, MSSG and NP-MSSG are word embeddings released by Neelakantan et al. (2015); 50d and 300d are the dimensions of the vector space. The vector space released by Huang et al. (2012) are 50-dimensional.", "startOffset": 55, "endOffset": 484}, {"referenceID": 15, "context": "We extracted the most probable hypernym for each sense of some sample words by Eq(4), using the synset semantic relations provided by WordNet (Miller, 1995).", "startOffset": 142, "endOffset": 156}, {"referenceID": 7, "context": "However, there are also some unexpected cases from the result based on the word vectors released by Huang et al. (2012), while no such cases are found in the vectors released by Neelakantan et al.", "startOffset": 100, "endOffset": 120}, {"referenceID": 7, "context": "However, there are also some unexpected cases from the result based on the word vectors released by Huang et al. (2012), while no such cases are found in the vectors released by Neelakantan et al. (2015). For example, we got [whole.", "startOffset": 100, "endOffset": 204}, {"referenceID": 7, "context": "However, there are also some unexpected cases from the result based on the word vectors released by Huang et al. (2012), while no such cases are found in the vectors released by Neelakantan et al. (2015). For example, we got [whole.n.02] as the hypernym of the three sample words (which seems too general since whole can be the hypernym of nearly all entities), and [person.n.01] as a hypernym of ROCK (which seems not very reasonable according to the nearest neighbors). By intuition, we suggest that is because of the quality of the word embeddings. Possibly, the level of confidence to extract domains and hypernyms for each sense could be a metric for evaluating the quality of word embeddings. From this point of view, the word embeddings released by Neelakantan et al. (2015) are also with higher quality.", "startOffset": 100, "endOffset": 782}, {"referenceID": 19, "context": "Neelakantan et al. (2015) introduced three metrics to compute the similarity between words in multisense word embeddings, which are avgSim, avgSimC and localSim, defined by the following equations.", "startOffset": 0, "endOffset": 26}, {"referenceID": 5, "context": "2 WordSim-353 WordSim-353 is a standard dataset for evaluating the quality of word vectors introduced by Finkelstein et al. (2001), which includes 353 pairs of nouns (without context).", "startOffset": 105, "endOffset": 131}, {"referenceID": 7, "context": "3 SCWS Stanford Contextual Word Similarity (SCWS) dataset proposed by Huang et al. (2012) is also a standard dataset to evaluate the performance of word embeddings quantitatively.", "startOffset": 70, "endOffset": 90}, {"referenceID": 7, "context": "3 SCWS Stanford Contextual Word Similarity (SCWS) dataset proposed by Huang et al. (2012) is also a standard dataset to evaluate the performance of word embeddings quantitatively. It contains 2,003 pairs of words and the context they occur in. Then as Neelakantan et al. (2015) did in their work, we also report the Spearman rank correlation between a model\u2019s output similarities and the human judgements.", "startOffset": 70, "endOffset": 278}], "year": 2017, "abstractText": "Previous researches have shown that learning multiple representations for polysemous words can improve the performance of word embeddings on many tasks. However, this leads to another problem. Several vectors of a word may actually point to the same meaning, namely pseudo multi-sense. In this paper, we introduce the concept of pseudo multi-sense, and then propose an algorithm to detect such cases. With the consideration of the detected pseudo multi-sense cases, we try to refine the existing word embeddings to eliminate the influence of pseudo multi-sense. Moreover, we apply our algorithm on previous released multi-sense word embeddings and tested it on artificial word similarity tasks and the analogy task. The result of the experiments shows that diminishing pseudo multi-sense can improve the quality of word representations. Thus, our method is actually an efficient way to reduce linguistic complexity.", "creator": "LaTeX with hyperref package"}}}