{"id": "1701.01623", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Jan-2017", "title": "Cross-Lingual Dependency Parsing with Late Decoding for Truly Low-Resource Languages", "abstract": "In cross-lingual dependency annotation projection, information is often lost during transfer because of early decoding. We present an end-to-end graph-based neural network dependency parser that can be trained to reproduce matrices of edge scores, which can be directly projected across word alignments. We show that our approach to cross-lingual dependency parsing is not only simpler, but also achieves an absolute improvement of 2.25% averaged across 10 languages compared to the previous state of the art.", "histories": [["v1", "Fri, 6 Jan 2017 12:54:48 GMT  (98kb,D)", "http://arxiv.org/abs/1701.01623v1", "To be published at EACL 2017"]], "COMMENTS": "To be published at EACL 2017", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["michael sejr schlichtkrull", "anders s{\\o}gaard"], "accepted": false, "id": "1701.01623"}, "pdf": {"name": "1701.01623.pdf", "metadata": {"source": "CRF", "title": "Cross-Lingual Dependency Parsing with Late Decoding for Truly Low-Resource Languages", "authors": ["Michael Sejr Schlichtkrull", "Anders S\u00f8gaard"], "emails": ["m.s.schlichtkrull@uva.nl", "soegaard@di.ku.dk"], "sections": [{"heading": "1 Introduction", "text": "Most research on dependence on the Bible has focused on learning tree trunks, i.e. collections of manually annotated, well-trained syntactic trees. In this paper, we develop and evaluate a graph-based parser that does not require the training data to be well-shaped trees. We show that such a parser has an important application in interlingual education, the annotation is a method for developing low-resource parsers that rely on aligned translations from resource-rich source languages rather than applying linguistic resources such as treebanks or dictionaries. The Bible has been fully translated into 542 languages and partially translated into another 2344 languages."}, {"heading": "2 Model", "text": "The goal of this section is to construct a first-order graph-based parser that is able to learn directly from potentially incomplete edge result matrices produced by another graph-based parser. Our approach is to treat the encoding phase of the parser as a tensor transformation problem, mapping edge feature tensors to edge result matrices, allowing our model to apply approximate sets of scoring matrices generated directly by another parser through nonlinear regression. The core component of the model is a sequence of recurrent neuronal network transformations applied to the axes of an input string variety. Formally, any type of scoring matrix G = (V, E) can be expressed as binary regression."}, {"heading": "3 Cross-lingual parsing", "text": "Hwa et al. (2005) is a pioneering paper on cross-border dependencies, but they use very detailed heuristics to ensure that the projected syntactic structures are well formed. (2016) is the latest continuation of their work, which represents a new approach to cross-border projection when the parallel data is of low quality, as the decoder introduces errors when the linguistic target values are missing. (2016) Despite the averaging language, there is a lack of input weight matrices, especially when the source and target languages are very far away. Below, we show that we can avoid errors by working directly on the projected edges."}, {"heading": "4 Experiments", "text": "In the second experiment, we compare Tensor-LSTM with the TurboParser (Martins et al., 2010) in multiple languages from the Universal Dependencies dataset. In the second experiment, we evaluate Tensor-LSTM in a linguistic context. Basically, we use the delicalized parser from McDonald et al. (2011) and the approach from Agic \u0301 et al. (2016) with TurboParser. To demonstrate the effectiveness of circumventing the decoding step, we perform the lingual evaluation of Tensor-LSTM by means of cross-entropy loss at early decoding and by means of mean square loss at later decoding."}, {"heading": "4.1 Model selection and training", "text": "Our features consist of 500-dimensional word embeddings trained on translations of the Bible. The word embeddings were then trained using Skipgram with negative sampling on a word-by-sentence PMI matrix derived from the Edinburgh Bible Corpus (Levy et al., 2017). Our embeddings are not trainable, but fixed representations throughout the learning process. Unknown tokens were represented by zero vectors. We combined the word embeddings with onehot encodings of POS tags projected via word alignments using the method of Agic et al. (2016) In order to verify the value of the POS features, we conducted preliminary experiments on English development data. When we included POS tags, we found small, non-significant improvements for monolingual parsing, but significant improvements for multilingual parsing. The weights were calculated using the standardized values Glo values in values values in values and values in values."}, {"heading": "4.2 Results", "text": "In the monolingual environment, we compare our parser with TurboParser (Martins et al., 2010) - a fast, powerful graph-based parser used as a component in many larger systems. TurboParser is also the system of choice for the interlingual pipeline of Agic \u0301 et al. (2016) investigated in the secondary experiment. It is therefore interesting to make a direct comparison between the two. Results can be found in Table 1. Note that for a parser to be directly applicable to the annotation projection investigated in the secondary experiment, it must be a first-order graph-based parser. In the monolingual environment, the best results reported so far (an average of 84.74) for the above selection of tree trunks were obtained by the Parsito system (Straka et al., 2015), a transition-based parser using a dynamic oracle."}, {"heading": "5 Discussion", "text": "In fact, the fact is that most of them will be able to play by the rules they have given themselves, and that they will be able to play by the rules they have given themselves."}, {"heading": "6 Related Work", "text": "Their work uses a revolutionary neural network to evaluate local edge scores based on global information, whereas in Zhang and Zhao (2015) and Pei et al. (2015) neural networks are used to simultaneously evaluate first order scores and high order scores for graph-based parsing. Bidirectional LSTM models have been successfully applied to feature generation (Kiperwasser and Goldberg, 2016). Such LSTM-based functions could be applied and trained in conjunction with Tensor-LSTM to apply global information in both analysis and feature generation (Kiperwasser and Goldberg, 2016)."}, {"heading": "7 Conclusion", "text": "We introduced a novel graphics-based dependency parsing algorithm based on an extension of sequence LSTM to the more general tensor LSTM. We demonstrated how the parser performs with a cross-entropy loss function comparable to the state-of-the-art in monolingual parsing. Furthermore, we demonstrated that the flexibility of our parser enables learning from poorly shaped data and from the output of other parsers. With this property, we applied our parser to a cross-language annotation projection problem for languages with really low resources and showed an average target-language unmarked attachment score of 48.54, which, to the best of our knowledge, represents the best results to date for the task."}, {"heading": "Acknowledgments", "text": "The second author was supported by ERC Starting Grant No. 313695."}], "references": [{"title": "Multilingual projection for parsing truly low-resource languages", "author": ["\u017deljko Agi\u0107", "Anders Johannsen", "Barbara Plank", "H\u00e9ctor Mart\u0131\u0301nez Alonso", "Natalie Schluter", "Anders S\u00f8gaard"], "venue": "Transactions of the Association for Computational Linguistics,", "citeRegEx": "Agi\u0107 et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Agi\u0107 et al\\.", "year": 2016}, {"title": "Where to apply dropout in recurrent neural networks for handwriting recognition", "author": ["Theodore Bluche", "Christopher Kermorvant", "Jerome Louradour"], "venue": "In Document Analysis and Recognition (ICDAR),", "citeRegEx": "Bluche et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bluche et al\\.", "year": 2015}, {"title": "Low resource dependency parsing: Cross-lingual parameter sharing in a neural network parser", "author": ["Long Duong", "Trevor Cohn", "Steven Bird", "Paul Cook."], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics", "citeRegEx": "Duong et al\\.,? 2015", "shortCiteRegEx": "Duong et al\\.", "year": 2015}, {"title": "Optimum branchings", "author": ["Jack Edmonds."], "venue": "Mathematics and the Decision Sciences, Part 1, pages 335\u2013345. American Mathematical Society.", "citeRegEx": "Edmonds.,? 1968", "shortCiteRegEx": "Edmonds.", "year": 1968}, {"title": "A deep architecture for non-projective dependency parsing", "author": ["Erick R Fonseca", "Avenida Trabalhador S\u00e3o-carlense", "Sandra M Alu\u0131\u0301sio"], "venue": "In Proceedings of the 2015 NAACL-HLT Workshop on Vector Space Modeling for NLP,", "citeRegEx": "Fonseca et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Fonseca et al\\.", "year": 2015}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["Xavier Glorot", "Yoshua Bengio."], "venue": "Proceedings of the 2010 International conference on Artificial Intelligence and Statistics, pages 249\u2013256. Society for Artificial Intelligence", "citeRegEx": "Glorot and Bengio.,? 2010", "shortCiteRegEx": "Glorot and Bengio.", "year": 2010}, {"title": "Multi-dimensional recurrent neural networks", "author": ["Alex Graves", "Santiago Fern\u00e1ndez", "J\u00fcrgen Schmidhuber."], "venue": "arXiv preprint arXiv:0705.2011.", "citeRegEx": "Graves et al\\.,? 2007", "shortCiteRegEx": "Graves et al\\.", "year": 2007}, {"title": "Cross-lingual dependency parsing based on distributed representations", "author": ["Jiang Guo", "Wanxiang Che", "David Yarowsky", "Haifeng Wang", "Ting Liu."], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the", "citeRegEx": "Guo et al\\.,? 2015", "shortCiteRegEx": "Guo et al\\.", "year": 2015}, {"title": "Gradient flow in recurrent nets: the difficulty of learning long-term dependencies", "author": ["Sepp Hochreiter", "Yoshua Bengio", "Paolo Frasconi", "J\u00fcrgen Schmidhuber."], "venue": "A Field Guide to Dynamic Recurrent Neural Networks. IEEE press.", "citeRegEx": "Hochreiter et al\\.,? 2001", "shortCiteRegEx": "Hochreiter et al\\.", "year": 2001}, {"title": "Bootstrapping parsers via syntactic projection across parallel texts", "author": ["Rebecca Hwa", "Philip Resnik", "Amy Weinberg", "Clara Cabezas", "Okan Kolak."], "venue": "Natural language engineering, 11(03):311\u2013325.", "citeRegEx": "Hwa et al\\.,? 2005", "shortCiteRegEx": "Hwa et al\\.", "year": 2005}, {"title": "An empirical exploration of recurrent network architectures", "author": ["Rafal Jozefowicz", "Wojciech Zaremba", "Ilya Sutskever."], "venue": "Proceedings of the 32nd International Conference on Machine Learning, pages 2342\u20132350. International Machine Learn-", "citeRegEx": "Jozefowicz et al\\.,? 2015", "shortCiteRegEx": "Jozefowicz et al\\.", "year": 2015}, {"title": "Grid long short-term memory", "author": ["Nal Kalchbrenner", "Ivo Danihelka", "Alex Graves."], "venue": "arXiv preprint arXiv:1507.01526.", "citeRegEx": "Kalchbrenner et al\\.,? 2015", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2015}, {"title": "Simple and accurate dependency parsing using bidirectional lstm feature representations", "author": ["Eliyahu Kiperwasser", "Yoav Goldberg."], "venue": "arXiv preprint arXiv:1603.04351.", "citeRegEx": "Kiperwasser and Goldberg.,? 2016", "shortCiteRegEx": "Kiperwasser and Goldberg.", "year": 2016}, {"title": "A strong baseline for learning cross-lingual word representations from sentence alignments", "author": ["Omer Levy", "Anders S\u00f8gaard", "Yoav Goldberg."], "venue": "EACL.", "citeRegEx": "Levy et al\\.,? 2017", "shortCiteRegEx": "Levy et al\\.", "year": 2017}, {"title": "Soft cross-lingual syntax projection for dependency parsing", "author": ["Zhenghua Li", "Min Zhang", "Wenliang Chen."], "venue": "Proceedings of the 25th International Conference on Computational Linguistics, pages 783\u2013793. Association for Computational Linguis-", "citeRegEx": "Li et al\\.,? 2014", "shortCiteRegEx": "Li et al\\.", "year": 2014}, {"title": "Unsupervised dependency parsing with transferring distribution via parallel guidance and entropy regularization", "author": ["Xuezhe Ma", "Fei Xia."], "venue": "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1337\u2013", "citeRegEx": "Ma and Xia.,? 2014", "shortCiteRegEx": "Ma and Xia.", "year": 2014}, {"title": "Turbo parsers: Dependency parsing by approximate variational inference", "author": ["Andr\u00e9 FT Martins", "Noah A Smith", "Eric P Xing", "Pedro MQ Aguiar", "M\u00e1rio AT Figueiredo."], "venue": "Proceedings of the 2010 Conference on Empirical Methods in Natural", "citeRegEx": "Martins et al\\.,? 2010", "shortCiteRegEx": "Martins et al\\.", "year": 2010}, {"title": "Non-projective dependency parsing using spanning tree algorithms", "author": ["Ryan McDonald", "Fernando Pereira", "Kiril Ribarov", "Jan Haji\u010d."], "venue": "Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Pro-", "citeRegEx": "McDonald et al\\.,? 2005", "shortCiteRegEx": "McDonald et al\\.", "year": 2005}, {"title": "Multi-source transfer of delexicalized dependency parsers", "author": ["Ryan McDonald", "Slav Petrov", "Keith Hall."], "venue": "Proceedings of the 2011 Conference on", "citeRegEx": "McDonald et al\\.,? 2011", "shortCiteRegEx": "McDonald et al\\.", "year": 2011}, {"title": "Universal dependency annotation for multilingual parsing", "author": ["Ryan T McDonald", "Joakim Nivre", "Yvonne QuirmbachBrundage", "Yoav Goldberg", "Dipanjan Das", "Kuzman Ganchev", "Keith B Hall", "Slav Petrov", "Hao Zhang", "Oscar T\u00e4ckstr\u00f6m"], "venue": null, "citeRegEx": "McDonald et al\\.,? \\Q2013\\E", "shortCiteRegEx": "McDonald et al\\.", "year": 2013}, {"title": "Adding gradient noise improves learning for very deep networks", "author": ["Arvind Neelakantan", "Luke Vilnis", "Quoc V Le", "Ilya Sutskever", "Lukasz Kaiser", "Karol Kurach", "James Martens."], "venue": "arXiv preprint arXiv:1511.06807.", "citeRegEx": "Neelakantan et al\\.,? 2015", "shortCiteRegEx": "Neelakantan et al\\.", "year": 2015}, {"title": "Bayesian models for multilingual word alignment", "author": ["Robert \u00d6stling."], "venue": "Ph.D. thesis, Department of Linguistics, Stockholm University.", "citeRegEx": "\u00d6stling.,? 2015", "shortCiteRegEx": "\u00d6stling.", "year": 2015}, {"title": "On the difficulty of training recurrent neural networks", "author": ["Razvan Pascanu", "Tomas Mikolov", "Yoshua Bengio."], "venue": "arXiv preprint arXiv:1211.5063.", "citeRegEx": "Pascanu et al\\.,? 2012", "shortCiteRegEx": "Pascanu et al\\.", "year": 2012}, {"title": "An effective neural network model for graph-based dependency parsing", "author": ["Wenzhe Pei", "Tao Ge", "Baobao Chang."], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Confer-", "citeRegEx": "Pei et al\\.,? 2015", "shortCiteRegEx": "Pei et al\\.", "year": 2015}, {"title": "Density-driven cross-lingual transfer of dependency parsers", "author": ["Mohammad Sadegh Rasooli", "Michael Collins."], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 328\u2013338. Association for Com-", "citeRegEx": "Rasooli and Collins.,? 2015", "shortCiteRegEx": "Rasooli and Collins.", "year": 2015}, {"title": "Parsing universal dependency treebanks using neural networks and search-based oracle", "author": ["Milan Straka", "Jan Haji\u010d", "Jana Strakov\u00e1", "Jan Haji\u010d jr."], "venue": "Proceedings of the 14th International Workshop on Treebanks and Linguistic Theories, pages 208\u2013220.", "citeRegEx": "Straka et al\\.,? 2015", "shortCiteRegEx": "Straka et al\\.", "year": 2015}, {"title": "Rediscovering annotation projection for cross-lingual parser induction", "author": ["J\u00f6rg Tiedemann."], "venue": "Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers, pages 1854\u20131864. Association for Com-", "citeRegEx": "Tiedemann.,? 2014", "shortCiteRegEx": "Tiedemann.", "year": 2014}, {"title": "Cross-lingual dependency parsing with universal dependencies and predicted pos labels", "author": ["J\u00f6rg Tiedemann."], "venue": "Proceedings of the Third International Conference on Dependency Linguistics, pages 340\u2013 349.", "citeRegEx": "Tiedemann.,? 2015", "shortCiteRegEx": "Tiedemann.", "year": 2015}, {"title": "Parallel corpora for medium density languages", "author": ["D\u00e1niel Varga", "P\u00e9ter Hal\u00e1csy", "Andr\u00e1s Kornai", "Viktor Nagy", "L\u00e1szl\u00f3 N\u00e9meth", "Viktor Tr\u00f3n."], "venue": "Proceedings of the 2005 Conference on Recent Advances in Natural Language Processing. Associa-", "citeRegEx": "Varga et al\\.,? 2005", "shortCiteRegEx": "Varga et al\\.", "year": 2005}, {"title": "Hierarchical low-rank tensors for multilingual transfer parsing", "author": ["Yuan Zhang", "Regina Barzilay."], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1857\u20131867. Association for Computational Linguis-", "citeRegEx": "Zhang and Barzilay.,? 2015", "shortCiteRegEx": "Zhang and Barzilay.", "year": 2015}, {"title": "High-order graph-based neural dependency parsing", "author": ["Zhisong Zhang", "Hai Zhao."], "venue": "Proceedings of the 29th Pacific Asia Conference on Language, Information and Computation, pages 114\u2013 123. Association for Computational Linguistics.", "citeRegEx": "Zhang and Zhao.,? 2015", "shortCiteRegEx": "Zhang and Zhao.", "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "In Agi\u0107 et al. (2016), a projection scheme is proposed wherein labels are collected from many sources, projected into a target language, and", "startOffset": 3, "endOffset": 22}, {"referenceID": 19, "context": "eral languages, demonstrating results comparable to the state of the art on the Universal Dependencies (McDonald et al., 2013) dataset.", "startOffset": 103, "endOffset": 126}, {"referenceID": 17, "context": "In the arc-factored (first-order), graph-based model, P is a composite function P = D \u25e6 E where the encoder E : Rw\u00d7f \u2192 Rw\u00d7(w+1) is a real-valued scoring function and the decoder D : Rw\u00d7(w+1) \u2192 Z 2 is a minimum spanning tree algorithm (McDonald et al., 2005).", "startOffset": 234, "endOffset": 257}, {"referenceID": 16, "context": "The classical arc-factored parsing algorithm of McDonald et al. (2005) corresponds to applying a function O : R2f+1 \u2192 R pointwise to S S\u2217, then decoding the resulting w \u00d7 (w + 1)-matrix.", "startOffset": 48, "endOffset": 71}, {"referenceID": 8, "context": "The Long Short-Term Memory (LSTM) unit is a function LSTM(x, ht\u22121, ct\u22121) = (ht, ct) defined through the use of several intermediary steps, following Hochreiter et al. (2001). A concatenated input vector I = x \u2295 hprev is constructed, where \u2295 represents vector concatenation.", "startOffset": 149, "endOffset": 174}, {"referenceID": 3, "context": "In both cases, predictiontime decoding is done with Chu-Liu-Edmonds algorithm (Edmonds, 1968) following McDonald et al.", "startOffset": 78, "endOffset": 93}, {"referenceID": 3, "context": "In both cases, predictiontime decoding is done with Chu-Liu-Edmonds algorithm (Edmonds, 1968) following McDonald et al. (2005).", "startOffset": 60, "endOffset": 127}, {"referenceID": 0, "context": "Agi\u0107 et al. (2016) is the latest continuation of their work, pre-", "startOffset": 0, "endOffset": 19}, {"referenceID": 0, "context": "Agi\u0107 et al. (2016) construct target-language treebanks by aggregating scores from multiple source languages, before decoding.", "startOffset": 0, "endOffset": 19}, {"referenceID": 0, "context": "Following Agi\u0107 et al. (2016), a sentence-wise voting function is then constructed as the highest contribution from a source-language edge:", "startOffset": 10, "endOffset": 29}, {"referenceID": 0, "context": "We experiment with two methods of learning from the projected data \u2013 decoding with Chu-LiuEdmonds algorithm and then training as proposed in Agi\u0107 et al. (2016), or directly learning to reproduce the matrices of edge scores.", "startOffset": 141, "endOffset": 160}, {"referenceID": 27, "context": "troduced in Varga et al. (2005) and the token-level model presented in \u00d6stling (2015).", "startOffset": 12, "endOffset": 32}, {"referenceID": 21, "context": "(2005) and the token-level model presented in \u00d6stling (2015).", "startOffset": 46, "endOffset": 61}, {"referenceID": 16, "context": "We compare Tensor-LSTM to the TurboParser (Martins et al., 2010) on several languages from the Universal Dependencies dataset.", "startOffset": 42, "endOffset": 64}, {"referenceID": 0, "context": "(2011), and the approach of Agi\u0107 et al. (2016) using TurboParser.", "startOffset": 28, "endOffset": 47}, {"referenceID": 13, "context": "The word embeddings were trained using skipgram with negative sampling on a word-by-sentence PMI matrix induced from the Edinburgh Bible Corpus, following (Levy et al., 2017).", "startOffset": 155, "endOffset": 174}, {"referenceID": 0, "context": "We combined the word embeddings with onehot-encodings of POS-tags, projected across word alignments following the method of Agi\u0107 et al. (2016). To verify the value of the POS-features, we conducted preliminary experiments on English development data.", "startOffset": 124, "endOffset": 143}, {"referenceID": 10, "context": "Following Jozefowicz et al. (2015), we add 1 to the initial forget gate bias.", "startOffset": 10, "endOffset": 35}, {"referenceID": 20, "context": "lowing Neelakantan et al. (2015), we added a noise factor n \u223c N (0, 1 (1+t)0.", "startOffset": 7, "endOffset": 33}, {"referenceID": 1, "context": "2 (Bluche et al., 2015).", "startOffset": 2, "endOffset": 23}, {"referenceID": 1, "context": "2 (Bluche et al., 2015). As proposed in Pascanu et al. (2012), we employed a gradient clipping factor of 15.", "startOffset": 3, "endOffset": 62}, {"referenceID": 16, "context": "In the monolingual setting, we compare our parser to TurboParser (Martins et al., 2010) \u2013 a fast, ca-", "startOffset": 65, "endOffset": 87}, {"referenceID": 0, "context": "TurboParser is also the system of choice for the cross-lingual pipeline of Agi\u0107 et al. (2016). It is therefore interesting to make a direct comparison between the two.", "startOffset": 75, "endOffset": 94}, {"referenceID": 25, "context": "74, on average) for the above selection of treebanks were by the Parsito system (Straka et al., 2015), a transition-based parser using a dynamic oracle.", "startOffset": 80, "endOffset": 101}, {"referenceID": 16, "context": "For the cross-lingual annotation projection experiments, we use the delexicalized system suggested by McDonald et al. (2011) as a baseline.", "startOffset": 102, "endOffset": 125}, {"referenceID": 0, "context": "We also compare against the annotation projection scheme using TurboParser suggested in Agi\u0107 et al. (2016), representing the previous state of the art for truly low-resource cross-lingual dependency parsing.", "startOffset": 88, "endOffset": 107}, {"referenceID": 16, "context": "We include the results of two baselines \u2013 the delexicalized system of McDonald et al. (2011) and the Turbo-based projection scheme of Agi\u0107 et al.", "startOffset": 70, "endOffset": 93}, {"referenceID": 0, "context": "(2011) and the Turbo-based projection scheme of Agi\u0107 et al. (2016). English and German development data was used for hyperparameter tuning (marked *).", "startOffset": 48, "endOffset": 67}, {"referenceID": 11, "context": "Kalchbrenner et al. (2015) may yield further per-", "startOffset": 0, "endOffset": 27}, {"referenceID": 4, "context": "ture representations, with the notable exception of Fonseca et al. (2015). In their paper, a convolutional neural network is used to evaluate local edge scores based on global information.", "startOffset": 52, "endOffset": 74}, {"referenceID": 4, "context": "ture representations, with the notable exception of Fonseca et al. (2015). In their paper, a convolutional neural network is used to evaluate local edge scores based on global information. In Zhang and Zhao (2015) and Pei et al.", "startOffset": 52, "endOffset": 214}, {"referenceID": 4, "context": "ture representations, with the notable exception of Fonseca et al. (2015). In their paper, a convolutional neural network is used to evaluate local edge scores based on global information. In Zhang and Zhao (2015) and Pei et al. (2015), neu-", "startOffset": 52, "endOffset": 236}, {"referenceID": 12, "context": "Bidirectional LSTM-models have been successfully applied to feature generation (Kiperwasser and Goldberg, 2016).", "startOffset": 79, "endOffset": 111}, {"referenceID": 6, "context": "An extension of LSTM to tensor-structured data has been explored in Graves et al. (2007), and further improved upon in Kalchbrenner et al.", "startOffset": 68, "endOffset": 89}, {"referenceID": 6, "context": "An extension of LSTM to tensor-structured data has been explored in Graves et al. (2007), and further improved upon in Kalchbrenner et al. (2015) in the form of GridLSTM.", "startOffset": 68, "endOffset": 146}, {"referenceID": 9, "context": "Annotation projection for dependency parsing has been explored in a number of papers, starting with Hwa et al. (2005). In Tiedemann (2014) and Tiedemann (2015) the process in extended and evaluated across many languages.", "startOffset": 100, "endOffset": 118}, {"referenceID": 9, "context": "Annotation projection for dependency parsing has been explored in a number of papers, starting with Hwa et al. (2005). In Tiedemann (2014) and Tiedemann (2015) the process in extended and evaluated across many languages.", "startOffset": 100, "endOffset": 139}, {"referenceID": 9, "context": "Annotation projection for dependency parsing has been explored in a number of papers, starting with Hwa et al. (2005). In Tiedemann (2014) and Tiedemann (2015) the process in extended and evaluated across many languages.", "startOffset": 100, "endOffset": 160}, {"referenceID": 9, "context": "Annotation projection for dependency parsing has been explored in a number of papers, starting with Hwa et al. (2005). In Tiedemann (2014) and Tiedemann (2015) the process in extended and evaluated across many languages. Li et al. (2014) follows the method of Hwa et al.", "startOffset": 100, "endOffset": 238}, {"referenceID": 9, "context": "Annotation projection for dependency parsing has been explored in a number of papers, starting with Hwa et al. (2005). In Tiedemann (2014) and Tiedemann (2015) the process in extended and evaluated across many languages. Li et al. (2014) follows the method of Hwa et al. (2005) and adds a probabilistic target-language classifier to determine and filter out high-uncertainty trees.", "startOffset": 100, "endOffset": 278}, {"referenceID": 9, "context": "Annotation projection for dependency parsing has been explored in a number of papers, starting with Hwa et al. (2005). In Tiedemann (2014) and Tiedemann (2015) the process in extended and evaluated across many languages. Li et al. (2014) follows the method of Hwa et al. (2005) and adds a probabilistic target-language classifier to determine and filter out high-uncertainty trees. In Ma and Xia (2014), performance on projected data is used as an additional objective for unsupervised learning through a combined loss function.", "startOffset": 100, "endOffset": 403}, {"referenceID": 0, "context": "In Agi\u0107 et al. (2016) this problem is addressed, and a parser is constructed which utilizes averaging over edge posteriors for many source languages to compensate for low-quality projected data.", "startOffset": 3, "endOffset": 22}, {"referenceID": 7, "context": "lingual dependency parsing has been the focus of several other recent papers (Guo et al., 2015; Zhang and Barzilay, 2015; Duong et al., 2015; Rasooli and Collins, 2015).", "startOffset": 77, "endOffset": 168}, {"referenceID": 29, "context": "lingual dependency parsing has been the focus of several other recent papers (Guo et al., 2015; Zhang and Barzilay, 2015; Duong et al., 2015; Rasooli and Collins, 2015).", "startOffset": 77, "endOffset": 168}, {"referenceID": 2, "context": "lingual dependency parsing has been the focus of several other recent papers (Guo et al., 2015; Zhang and Barzilay, 2015; Duong et al., 2015; Rasooli and Collins, 2015).", "startOffset": 77, "endOffset": 168}, {"referenceID": 24, "context": "lingual dependency parsing has been the focus of several other recent papers (Guo et al., 2015; Zhang and Barzilay, 2015; Duong et al., 2015; Rasooli and Collins, 2015).", "startOffset": 77, "endOffset": 168}, {"referenceID": 2, "context": ", 2015; Zhang and Barzilay, 2015; Duong et al., 2015; Rasooli and Collins, 2015). In Guo et al. (2015), distributed, language-independent feature represen-", "startOffset": 34, "endOffset": 103}, {"referenceID": 28, "context": "Zhang and Barzilay (2015) introduce a tensor-based feature representation capable of incorporating prior knowledge about feature interactions learned from source languages.", "startOffset": 0, "endOffset": 26}, {"referenceID": 2, "context": "In Duong et al. (2015), a neural network parser is built wherein higher-level layers", "startOffset": 3, "endOffset": 23}, {"referenceID": 24, "context": "Finally, Rasooli and Collins (2015) leverage dense information in high-quality sentence translations to improve performance.", "startOffset": 9, "endOffset": 36}], "year": 2017, "abstractText": "In cross-lingual dependency annotation projection, information is often lost during transfer because of early decoding. We present an end-to-end graph-based neural network dependency parser that can be trained to reproduce matrices of edge scores, which can be directly projected across word alignments. We show that our approach to cross-lingual dependency parsing is not only simpler, but also achieves an absolute improvement of 2.25% averaged across 10 languages compared to the previous state of the art.", "creator": "TeX"}}}